You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
INTERNAL Error: Attempted to dereference unique_ptr that is NULL
### What happens?

The below test case (after some seconds creduce) causes an INTERNAL error in DuckDB. This [test case](https://github.com/user-attachments/files/17001454/backup.txt) before further reduced could sometimes cause segmentation fault.


### To Reproduce

```sql
CREATE TABLE t0(c0 TIMESTAMP, c1 VARCHAR[]);
CREATE TABLE t1(c0 FLOAT, c1 TIMESTAMP, c2 FLOAT);
INSERT INTO t0 VALUES('2023-10-10 00:00:00+00:00', NULL);
INSERT INTO t0 VALUES('2025-12-25 12:00:00+02:00', []), ('2004-07-27 10:00:00+02', []);
INSERT INTO t0(c1, c0) VALUES([], '2023-01-01 00:00:00+00:00'), ([], '2021-01-01 00:00:00+01');
INSERT INTO t0(c1, c0) VALUES([], '2021-01-01 00:00:00+00');
INSERT INTO t1 VALUES(2.71, '1999-12-31 23:59:59', 1.41421356237);
INSERT INTO t1 VALUES(1.61803, '1970-01-01 00:00:00', 1.61803);
INSERT INTO t1(c0) VALUES(1064961652.34), (123.45);
INSERT INTO t1(c0) VALUES('514332609.12');
INSERT INTO t1(c0, c2, c1) VALUES(2.71828, 2.345, '1995-05-23 08:45:00'), ('1308880808', 12.34, '2021-01-01 15:30:45');
INSERT INTO t1(c0) VALUES(92857950), (840458867);
INSERT INTO t1 VALUES('3.14', '1999-12-31 23:59:59', 3.1415);

SELECT * FROM t0 RIGHT JOIN t1 ON(CAST(t1.c1 AS TIMESTAMP) BETWEEN t0.c0 AND t0.c0);
-- INTERNAL Error: Attempted to dereference unique_ptr that is NULL! This error signals an assertion failure within DuckDB. This usually occurs due to unexpected conditions or errors in the program's logic. For more information, see https://duckdb.org/docs/dev/internal_errors
```

### OS:

Ubuntu 22.04

### DuckDB Version:

v1.1.1-dev122 b369bcb4e0

### DuckDB Client:

CLI

### Hardware:

_No response_

### Full Name:

Suyang Zhong

### Affiliation:

NUS

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a nightly build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://duckdb.org/docs/installation) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of extension/jemalloc/jemalloc/README.md]
1: # Updating jemalloc
2: 
3: Clone [this](https://github.com/jemalloc/jemalloc), and check out the branch you need.
4: 
5: For convenience:
6: ```sh
7: export DUCKDB_DIR=<duckdb_dir>
8: ```
9: 
10: Copy jemalloc source files:
11: ```sh
12: cd <jemalloc_dir>
13: ./configure --with-jemalloc-prefix="duckdb_je_" --with-private-namespace="duckdb_" --without-export
14: cp -r src/* $DUCKDB_DIR/extension/jemalloc/jemalloc/src/
15: cp -r include/* $DUCKDB_DIR/extension/jemalloc/jemalloc/include/
16: cp COPYING $DUCKDB_DIR/extension/jemalloc/jemalloc/LICENSE
17: ```
18: 
19: Remove junk:
20: ```sh
21: cd $DUCKDB_DIR/extension/jemalloc/jemalloc
22: find . -name "*.in" -type f -delete
23: find . -name "*.sh" -type f -delete
24: find . -name "*.awk" -type f -delete
25: find . -name "*.txt" -type f -delete
26: find . -name "*.py" -type f -delete
27: ```
28: 
29: Restore these files:
30: ```sh
31: git checkout -- \
32:   include/jemalloc/internal/jemalloc_internal_defs.h \
33:   include/jemalloc/jemalloc.h \
34:   CMakeLists.txt
35: ```
36: 
37: The logarithm of the size of a pointer is defined in `jemalloc.h` and `jemalloc_defs.h`, around line 50.
38: This is not portable, but we can make it portable if we replace all of it with this:
39: ```c++
40: #ifdef _MSC_VER
41: #  ifdef _WIN64
42: #    define LG_SIZEOF_PTR_WIN 3
43: #  else
44: #    define LG_SIZEOF_PTR_WIN 2
45: #  endif
46: #endif
47: 
48: /* sizeof(void *) == 2^LG_SIZEOF_PTR. */
49: #include <limits.h>
50: #ifdef _MSC_VER
51: #  define LG_SIZEOF_PTR LG_SIZEOF_PTR_WIN
52: #elif INTPTR_MAX == INT64_MAX
53: #  define LG_SIZEOF_PTR 3
54: #else
55: #  define LG_SIZEOF_PTR 2
56: #endif
57: ```
58: 
59: Add this to `jemalloc.h`:
60: ```c++
61: // DuckDB uses a 5s decay
62: #define DUCKDB_JEMALLOC_DECAY 5
63: ```
64: 
65: We also supply our own config string in `jemalloc.c`.
66: Define this just after the `#include`s.
67: ```c++
68: #define JE_MALLOC_CONF_BUFFER_SIZE 200
69: char JE_MALLOC_CONF_BUFFER[JE_MALLOC_CONF_BUFFER_SIZE];
70: ```
71: This is what `jemalloc_constructor` in `jemalloc.c` should look like:
72: ```c++
73: JEMALLOC_ATTR(constructor)
74: static void
75: jemalloc_constructor(void) {
76: 	unsigned long long cpu_count = malloc_ncpus();
77: 	unsigned long long bgt_count = cpu_count / 16;
78: 	if (bgt_count == 0) {
79: 		bgt_count = 1;
80: 	}
81: 	// decay is in ms
82: 	unsigned long long decay = DUCKDB_JEMALLOC_DECAY * 1000;
83: #ifdef DEBUG
84: 	snprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, "junk:true,oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu", decay, decay, cpu_count, bgt_count);
85: #else
86: 	snprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, "oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu", decay, decay, cpu_count, bgt_count);
87: #endif
88: 	je_malloc_conf = JE_MALLOC_CONF_BUFFER;
89: 	malloc_init();
90: }
91: ```
92: 
93: Make `strerror_r` portable using this hack in `malloc_io.c`, just above the `buferror` function:
94: ```c++
95: // taken from https://ae1020.github.io/fixing-strerror_r-posix-debacle/
96: int strerror_fixed(int err, char *buf, size_t buflen) {
97:     assert(buflen != 0);
98: 
99:     buf[0] = (char)255;  // never valid in UTF-8 sequences
100:     int old_errno = errno;
101:     intptr_t r = (intptr_t)strerror_r(err, buf, buflen);
102:     int new_errno = errno;
103: 
104:     if (r == -1 || new_errno != old_errno) {
105:         //
106:         // errno was changed, so probably the return value is just -1 or
107:         // something else that doesn't provide info.
108:         //
109:         malloc_snprintf(buf, buflen, "errno %d in strerror_r call", new_errno);
110:     }
111:     else if (r == 0) {
112:         //
113:         // The GNU version always succeds and should never return 0 (NULL).
114:         //
115:         // "The XSI-compliant strerror_r() function returns 0 on success.
116:         // On error, a (positive) error number is returned (since glibc
117:         // 2.13), or -1 is returned and errno is set to indicate the error
118:         // (glibc versions before 2.13)."
119:         //
120:         // Documentation isn't clear on whether the buffer is terminated if
121:         // the message is too long, or ERANGE always returned.  Terminate.
122:         //
123:         buf[buflen - 1] = '\0';
124:     }
125:     else if (r == EINVAL) {  // documented result from XSI strerror_r
126:         malloc_snprintf(buf, buflen, "bad errno %d for strerror_r()", err);
127:     }
128:     else if (r == ERANGE) {  // documented result from XSI strerror_r
129:         malloc_snprintf(buf, buflen, "bad buflen for errno %d", err);
130:     }
131:     else if (r == (intptr_t)buf) {
132:         //
133:         // The GNU version gives us our error back as a pointer if it
134:         // filled the buffer successfully.  Sanity check that.
135:         //
136:         if (buf[0] == (char)255) {
137:             assert(false);
138:             strncpy(buf, "strerror_r didn't update buffer", buflen);
139:         }
140:     }
141:     else if (r < 256) {  // extremely unlikely to be string buffer pointer
142:         assert(false);
143:         strncpy(buf, "Unknown XSI strerror_r error result code", buflen);
144:     }
145:     else {
146:         // The GNU version never fails, but may return an immutable string
147:         // instead of filling the buffer. Unknown errors get an
148:         // "unknown error" message.  The result is always null terminated.
149:         //
150:         // (This is the risky part, if `r` is not a valid pointer but some
151:         // weird large int return result from XSI strerror_r.)
152:         //
153:         strncpy(buf, (const char*)r, buflen);
154:     }
155: 	return 0;
156: }
157: ```
158: 
159: Edit the following in `pages.c`:
160: ```c++
161: // explicitly initialize this buffer to prevent reading uninitialized memory if the file is somehow empty
162: // 0 is the default setting for linux if it hasn't been changed so that's what we initialize to
163: char buf[1] = {'0'};
164: // in this function
165: static bool
166: os_overcommits_proc(void)
167: ```
168: 
169: Modify this function to only print in DEBUG mode in `malloc_io.c`.
170: ```c++
171: void
172: malloc_write(const char *s) {
173: #ifdef DEBUG
174: 	if (je_malloc_message != NULL) {
175: 		je_malloc_message(NULL, s);
176: 	} else {
177: 		wrtmessage(NULL, s);
178: 	}
179: #endif
180: }
181: ```
182: 
183: Almost no symbols are leaked due to `private_namespace.h`.
184: The `exported_symbols_check.py` script still found a few, so these lines need to be added to `private_namespace.h`:
185: ```c++
186: // DuckDB: added these so we can pass "exported_symbols_check.py"
187: #define JE_MALLOC_CONF_BUFFER JEMALLOC_N(JE_MALLOC_CONF_BUFFER)
188: #define arena_name_get JEMALLOC_N(arena_name_get)
189: #define arena_name_set JEMALLOC_N(arena_name_set)
190: #define b0_alloc_tcache_stack JEMALLOC_N(b0_alloc_tcache_stack)
191: #define b0_dalloc_tcache_stack JEMALLOC_N(b0_dalloc_tcache_stack)
192: #define base_alloc_rtree JEMALLOC_N(base_alloc_rtree)
193: #define cache_bin_stack_use_thp JEMALLOC_N(cache_bin_stack_use_thp)
194: #define disabled_bin JEMALLOC_N(disabled_bin)
195: #define global_do_not_change_tcache_maxclass JEMALLOC_N(global_do_not_change_tcache_maxclass)
196: #define global_do_not_change_tcache_nbins JEMALLOC_N(global_do_not_change_tcache_nbins)
197: #define invalid_conf_abort JEMALLOC_N(invalid_conf_abort)
198: #define je_free_aligned_sized JEMALLOC_N(je_free_aligned_sized)
199: #define je_free_sized JEMALLOC_N(je_free_sized)
200: #define _malloc_thread_cleanup JEMALLOC_N(_malloc_thread_cleanup)
201: #define _malloc_tsd_cleanup_register JEMALLOC_N(_malloc_tsd_cleanup_register)
202: #define multi_setting_parse_next JEMALLOC_N(multi_setting_parse_next)
203: #define opt_calloc_madvise_threshold JEMALLOC_N(opt_calloc_madvise_threshold)
204: #define opt_debug_double_free_max_scan JEMALLOC_N(opt_debug_double_free_max_scan)
205: #define opt_malloc_conf_env_var JEMALLOC_N(opt_malloc_conf_env_var)
206: #define opt_malloc_conf_symlink JEMALLOC_N(opt_malloc_conf_symlink)
207: #define opt_prof_bt_max JEMALLOC_N(opt_prof_bt_max)
208: #define opt_prof_pid_namespace JEMALLOC_N(opt_prof_pid_namespace)
209: #define os_page JEMALLOC_N(os_page)
210: #define pa_shard_nactive JEMALLOC_N(pa_shard_nactive)
211: #define pa_shard_ndirty JEMALLOC_N(pa_shard_ndirty)
212: #define pa_shard_nmuzzy JEMALLOC_N(pa_shard_nmuzzy)
213: #define prof_sample_free_hook_get JEMALLOC_N(prof_sample_free_hook_get)
214: #define prof_sample_free_hook_set JEMALLOC_N(prof_sample_free_hook_set)
215: #define prof_sample_hook_get JEMALLOC_N(prof_sample_hook_get)
216: #define prof_sample_hook_set JEMALLOC_N(prof_sample_hook_set)
217: #define pthread_create_wrapper JEMALLOC_N(pthread_create_wrapper)
218: #define tcache_bin_ncached_max_read JEMALLOC_N(tcache_bin_ncached_max_read)
219: #define tcache_bins_ncached_max_write JEMALLOC_N(tcache_bins_ncached_max_write)
220: #define tcache_enabled_set JEMALLOC_N(tcache_enabled_set)
221: #define thread_tcache_max_set JEMALLOC_N(thread_tcache_max_set)
222: #define tsd_tls JEMALLOC_N(tsd_tls)
223: #define batcher_pop_begin JEMALLOC_N(batcher_pop_begin)
224: #define batcher_pop_get_pushes JEMALLOC_N(batcher_pop_get_pushes)
225: #define batcher_postfork_child JEMALLOC_N(batcher_postfork_child)
226: #define batcher_postfork_parent JEMALLOC_N(batcher_postfork_parent)
227: #define batcher_prefork JEMALLOC_N(batcher_prefork)
228: #define batcher_push_begin JEMALLOC_N(batcher_push_begin)
229: #define bin_info_nbatched_bins JEMALLOC_N(bin_info_nbatched_bins)
230: #define bin_info_nbatched_sizes JEMALLOC_N(bin_info_nbatched_sizes)
231: #define bin_info_nunbatched_bins JEMALLOC_N(bin_info_nunbatched_bins)
232: #define opt_bin_info_max_batched_size JEMALLOC_N(opt_bin_info_max_batched_size)
233: #define opt_bin_info_remote_free_max JEMALLOC_N(opt_bin_info_remote_free_max)
234: #define opt_bin_info_remote_free_max_batch JEMALLOC_N(opt_bin_info_remote_free_max_batch)
235: ```
[end of extension/jemalloc/jemalloc/README.md]
[start of extension/jemalloc/jemalloc/src/jemalloc.c]
1: #include "jemalloc/internal/jemalloc_preamble.h"
2: #include "jemalloc/internal/jemalloc_internal_includes.h"
3: 
4: #include "jemalloc/internal/assert.h"
5: #include "jemalloc/internal/atomic.h"
6: #include "jemalloc/internal/buf_writer.h"
7: #include "jemalloc/internal/ctl.h"
8: #include "jemalloc/internal/emap.h"
9: #include "jemalloc/internal/extent_dss.h"
10: #include "jemalloc/internal/extent_mmap.h"
11: #include "jemalloc/internal/fxp.h"
12: #include "jemalloc/internal/san.h"
13: #include "jemalloc/internal/hook.h"
14: #include "jemalloc/internal/jemalloc_internal_types.h"
15: #include "jemalloc/internal/log.h"
16: #include "jemalloc/internal/malloc_io.h"
17: #include "jemalloc/internal/mutex.h"
18: #include "jemalloc/internal/nstime.h"
19: #include "jemalloc/internal/rtree.h"
20: #include "jemalloc/internal/safety_check.h"
21: #include "jemalloc/internal/sc.h"
22: #include "jemalloc/internal/spin.h"
23: #include "jemalloc/internal/sz.h"
24: #include "jemalloc/internal/ticker.h"
25: #include "jemalloc/internal/thread_event.h"
26: #include "jemalloc/internal/util.h"
27: 
28: /******************************************************************************/
29: /* Data. */
30: 
31: /* Runtime configuration options. */
32: #define JE_MALLOC_CONF_BUFFER_SIZE 200
33: char JE_MALLOC_CONF_BUFFER[JE_MALLOC_CONF_BUFFER_SIZE];
34: 
35: const char	*je_malloc_conf
36: #ifndef _WIN32
37:     JEMALLOC_ATTR(weak)
38: #endif
39:     ;
40: /*
41:  * The usual rule is that the closer to runtime you are, the higher priority
42:  * your configuration settings are (so the jemalloc config options get lower
43:  * priority than the per-binary setting, which gets lower priority than the /etc
44:  * setting, which gets lower priority than the environment settings).
45:  *
46:  * But it's a fairly common use case in some testing environments for a user to
47:  * be able to control the binary, but nothing else (e.g. a performancy canary
48:  * uses the production OS and environment variables, but can run any binary in
49:  * those circumstances).  For these use cases, it's handy to have an in-binary
50:  * mechanism for overriding environment variable settings, with the idea that if
51:  * the results are positive they get promoted to the official settings, and
52:  * moved from the binary to the environment variable.
53:  *
54:  * We don't actually want this to be widespread, so we'll give it a silly name
55:  * and not mention it in headers or documentation.
56:  */
57: const char	*je_malloc_conf_2_conf_harder
58: #ifndef _WIN32
59:     JEMALLOC_ATTR(weak)
60: #endif
61:     ;
62: 
63: const char *opt_malloc_conf_symlink = NULL;
64: const char *opt_malloc_conf_env_var = NULL;
65: 
66: bool	opt_abort =
67: #ifdef JEMALLOC_DEBUG
68:     true
69: #else
70:     false
71: #endif
72:     ;
73: bool	opt_abort_conf =
74: #ifdef JEMALLOC_DEBUG
75:     true
76: #else
77:     false
78: #endif
79:     ;
80: /* Intentionally default off, even with debug builds. */
81: bool	opt_confirm_conf = false;
82: const char	*opt_junk =
83: #if (defined(JEMALLOC_DEBUG) && defined(JEMALLOC_FILL))
84:     "true"
85: #else
86:     "false"
87: #endif
88:     ;
89: bool	opt_junk_alloc =
90: #if (defined(JEMALLOC_DEBUG) && defined(JEMALLOC_FILL))
91:     true
92: #else
93:     false
94: #endif
95:     ;
96: bool	opt_junk_free =
97: #if (defined(JEMALLOC_DEBUG) && defined(JEMALLOC_FILL))
98:     true
99: #else
100:     false
101: #endif
102:     ;
103: bool	opt_trust_madvise =
104: #ifdef JEMALLOC_PURGE_MADVISE_DONTNEED_ZEROS
105:     false
106: #else
107:     true
108: #endif
109:     ;
110: 
111: bool opt_cache_oblivious =
112: #ifdef JEMALLOC_CACHE_OBLIVIOUS
113:     true
114: #else
115:     false
116: #endif
117:     ;
118: 
119: zero_realloc_action_t opt_zero_realloc_action =
120: #ifdef JEMALLOC_ZERO_REALLOC_DEFAULT_FREE
121:     zero_realloc_action_free
122: #else
123:     zero_realloc_action_alloc
124: #endif
125:     ;
126: 
127: atomic_zu_t zero_realloc_count = ATOMIC_INIT(0);
128: 
129: const char *const zero_realloc_mode_names[] = {
130: 	"alloc",
131: 	"free",
132: 	"abort",
133: };
134: 
135: /*
136:  * These are the documented values for junk fill debugging facilities -- see the
137:  * man page.
138:  */
139: static const uint8_t junk_alloc_byte = 0xa5;
140: static const uint8_t junk_free_byte = 0x5a;
141: 
142: static void default_junk_alloc(void *ptr, size_t usize) {
143: 	memset(ptr, junk_alloc_byte, usize);
144: }
145: 
146: static void default_junk_free(void *ptr, size_t usize) {
147: 	memset(ptr, junk_free_byte, usize);
148: }
149: 
150: void (*JET_MUTABLE junk_alloc_callback)(void *ptr, size_t size) = &default_junk_alloc;
151: void (*JET_MUTABLE junk_free_callback)(void *ptr, size_t size) = &default_junk_free;
152: void (*JET_MUTABLE invalid_conf_abort)(void) = &abort;
153: 
154: bool	opt_utrace = false;
155: bool	opt_xmalloc = false;
156: bool	opt_experimental_infallible_new = false;
157: bool	opt_zero = false;
158: unsigned	opt_narenas = 0;
159: static fxp_t		opt_narenas_ratio = FXP_INIT_INT(4);
160: 
161: unsigned	ncpus;
162: 
163: unsigned opt_debug_double_free_max_scan =
164:     SAFETY_CHECK_DOUBLE_FREE_MAX_SCAN_DEFAULT;
165: 
166: size_t opt_calloc_madvise_threshold = 0;
167: 
168: /* Protects arenas initialization. */
169: static malloc_mutex_t arenas_lock;
170: 
171: /* The global hpa, and whether it's on. */
172: bool opt_hpa = false;
173: hpa_shard_opts_t opt_hpa_opts = HPA_SHARD_OPTS_DEFAULT;
174: sec_opts_t opt_hpa_sec_opts = SEC_OPTS_DEFAULT;
175: 
176: /*
177:  * Arenas that are used to service external requests.  Not all elements of the
178:  * arenas array are necessarily used; arenas are created lazily as needed.
179:  *
180:  * arenas[0..narenas_auto) are used for automatic multiplexing of threads and
181:  * arenas.  arenas[narenas_auto..narenas_total) are only used if the application
182:  * takes some action to create them and allocate from them.
183:  *
184:  * Points to an arena_t.
185:  */
186: JEMALLOC_ALIGNED(CACHELINE)
187: atomic_p_t		arenas[MALLOCX_ARENA_LIMIT];
188: static atomic_u_t	narenas_total; /* Use narenas_total_*(). */
189: /* Below three are read-only after initialization. */
190: static arena_t		*a0; /* arenas[0]. */
191: unsigned		narenas_auto;
192: unsigned		manual_arena_base;
193: 
194: malloc_init_t malloc_init_state = malloc_init_uninitialized;
195: 
196: /* False should be the common case.  Set to true to trigger initialization. */
197: bool			malloc_slow = true;
198: 
199: /* When malloc_slow is true, set the corresponding bits for sanity check. */
200: enum {
201: 	flag_opt_junk_alloc	= (1U),
202: 	flag_opt_junk_free	= (1U << 1),
203: 	flag_opt_zero		= (1U << 2),
204: 	flag_opt_utrace		= (1U << 3),
205: 	flag_opt_xmalloc	= (1U << 4)
206: };
207: static uint8_t	malloc_slow_flags;
208: 
209: #ifdef JEMALLOC_THREADED_INIT
210: /* Used to let the initializing thread recursively allocate. */
211: #  define NO_INITIALIZER	((unsigned long)0)
212: #  define INITIALIZER		pthread_self()
213: #  define IS_INITIALIZER	(malloc_initializer == pthread_self())
214: static pthread_t		malloc_initializer = NO_INITIALIZER;
215: #else
216: #  define NO_INITIALIZER	false
217: #  define INITIALIZER		true
218: #  define IS_INITIALIZER	malloc_initializer
219: static bool			malloc_initializer = NO_INITIALIZER;
220: #endif
221: 
222: /* Used to avoid initialization races. */
223: #ifdef _WIN32
224: #if _WIN32_WINNT >= 0x0600
225: static malloc_mutex_t	init_lock = SRWLOCK_INIT;
226: #else
227: static malloc_mutex_t	init_lock;
228: static bool init_lock_initialized = false;
229: 
230: JEMALLOC_ATTR(constructor)
231: static void WINAPI
232: _init_init_lock(void) {
233: 	/*
234: 	 * If another constructor in the same binary is using mallctl to e.g.
235: 	 * set up extent hooks, it may end up running before this one, and
236: 	 * malloc_init_hard will crash trying to lock the uninitialized lock. So
237: 	 * we force an initialization of the lock in malloc_init_hard as well.
238: 	 * We don't try to care about atomicity of the accessed to the
239: 	 * init_lock_initialized boolean, since it really only matters early in
240: 	 * the process creation, before any separate thread normally starts
241: 	 * doing anything.
242: 	 */
243: 	if (!init_lock_initialized) {
244: 		malloc_mutex_init(&init_lock, "init", WITNESS_RANK_INIT,
245: 		    malloc_mutex_rank_exclusive);
246: 	}
247: 	init_lock_initialized = true;
248: }
249: 
250: #ifdef _MSC_VER
251: #  pragma section(".CRT$XCU", read)
252: JEMALLOC_SECTION(".CRT$XCU") JEMALLOC_ATTR(used)
253: static const void (WINAPI *init_init_lock)(void) = _init_init_lock;
254: #endif
255: #endif
256: #else
257: static malloc_mutex_t	init_lock = MALLOC_MUTEX_INITIALIZER;
258: #endif
259: 
260: typedef struct {
261: 	void	*p;	/* Input pointer (as in realloc(p, s)). */
262: 	size_t	s;	/* Request size. */
263: 	void	*r;	/* Result pointer. */
264: } malloc_utrace_t;
265: 
266: #ifdef JEMALLOC_UTRACE
267: #  define UTRACE(a, b, c) do {						\
268: 	if (unlikely(opt_utrace)) {					\
269: 		int utrace_serrno = errno;				\
270: 		malloc_utrace_t ut;					\
271: 		ut.p = (a);						\
272: 		ut.s = (b);						\
273: 		ut.r = (c);						\
274: 		UTRACE_CALL(&ut, sizeof(ut));				\
275: 		errno = utrace_serrno;					\
276: 	}								\
277: } while (0)
278: #else
279: #  define UTRACE(a, b, c)
280: #endif
281: 
282: /* Whether encountered any invalid config options. */
283: static bool had_conf_error = false;
284: 
285: /******************************************************************************/
286: /*
287:  * Function prototypes for static functions that are referenced prior to
288:  * definition.
289:  */
290: 
291: static bool	malloc_init_hard_a0(void);
292: static bool	malloc_init_hard(void);
293: 
294: /******************************************************************************/
295: /*
296:  * Begin miscellaneous support functions.
297:  */
298: 
299: JEMALLOC_ALWAYS_INLINE bool
300: malloc_init_a0(void) {
301: 	if (unlikely(malloc_init_state == malloc_init_uninitialized)) {
302: 		return malloc_init_hard_a0();
303: 	}
304: 	return false;
305: }
306: 
307: JEMALLOC_ALWAYS_INLINE bool
308: malloc_init(void) {
309: 	if (unlikely(!malloc_initialized()) && malloc_init_hard()) {
310: 		return true;
311: 	}
312: 	return false;
313: }
314: 
315: /*
316:  * The a0*() functions are used instead of i{d,}alloc() in situations that
317:  * cannot tolerate TLS variable access.
318:  */
319: 
320: static void *
321: a0ialloc(size_t size, bool zero, bool is_internal) {
322: 	if (unlikely(malloc_init_a0())) {
323: 		return NULL;
324: 	}
325: 
326: 	return iallocztm(TSDN_NULL, size, sz_size2index(size), zero, NULL,
327: 	    is_internal, arena_get(TSDN_NULL, 0, true), true);
328: }
329: 
330: static void
331: a0idalloc(void *ptr, bool is_internal) {
332: 	idalloctm(TSDN_NULL, ptr, NULL, NULL, is_internal, true);
333: }
334: 
335: void *
336: a0malloc(size_t size) {
337: 	return a0ialloc(size, false, true);
338: }
339: 
340: void
341: a0dalloc(void *ptr) {
342: 	a0idalloc(ptr, true);
343: }
344: 
345: /*
346:  * FreeBSD's libc uses the bootstrap_*() functions in bootstrap-sensitive
347:  * situations that cannot tolerate TLS variable access (TLS allocation and very
348:  * early internal data structure initialization).
349:  */
350: 
351: void *
352: bootstrap_malloc(size_t size) {
353: 	if (unlikely(size == 0)) {
354: 		size = 1;
355: 	}
356: 
357: 	return a0ialloc(size, false, false);
358: }
359: 
360: void *
361: bootstrap_calloc(size_t num, size_t size) {
362: 	size_t num_size;
363: 
364: 	num_size = num * size;
365: 	if (unlikely(num_size == 0)) {
366: 		assert(num == 0 || size == 0);
367: 		num_size = 1;
368: 	}
369: 
370: 	return a0ialloc(num_size, true, false);
371: }
372: 
373: void
374: bootstrap_free(void *ptr) {
375: 	if (unlikely(ptr == NULL)) {
376: 		return;
377: 	}
378: 
379: 	a0idalloc(ptr, false);
380: }
381: 
382: void
383: arena_set(unsigned ind, arena_t *arena) {
384: 	atomic_store_p(&arenas[ind], arena, ATOMIC_RELEASE);
385: }
386: 
387: static void
388: narenas_total_set(unsigned narenas) {
389: 	atomic_store_u(&narenas_total, narenas, ATOMIC_RELEASE);
390: }
391: 
392: static void
393: narenas_total_inc(void) {
394: 	atomic_fetch_add_u(&narenas_total, 1, ATOMIC_RELEASE);
395: }
396: 
397: unsigned
398: narenas_total_get(void) {
399: 	return atomic_load_u(&narenas_total, ATOMIC_ACQUIRE);
400: }
401: 
402: /* Create a new arena and insert it into the arenas array at index ind. */
403: static arena_t *
404: arena_init_locked(tsdn_t *tsdn, unsigned ind, const arena_config_t *config) {
405: 	arena_t *arena;
406: 
407: 	assert(ind <= narenas_total_get());
408: 	if (ind >= MALLOCX_ARENA_LIMIT) {
409: 		return NULL;
410: 	}
411: 	if (ind == narenas_total_get()) {
412: 		narenas_total_inc();
413: 	}
414: 
415: 	/*
416: 	 * Another thread may have already initialized arenas[ind] if it's an
417: 	 * auto arena.
418: 	 */
419: 	arena = arena_get(tsdn, ind, false);
420: 	if (arena != NULL) {
421: 		assert(arena_is_auto(arena));
422: 		return arena;
423: 	}
424: 
425: 	/* Actually initialize the arena. */
426: 	arena = arena_new(tsdn, ind, config);
427: 
428: 	return arena;
429: }
430: 
431: static void
432: arena_new_create_background_thread(tsdn_t *tsdn, unsigned ind) {
433: 	if (ind == 0) {
434: 		return;
435: 	}
436: 
437: 	if (have_background_thread) {
438: 		if (background_thread_create(tsdn_tsd(tsdn), ind)) {
439: 			malloc_printf("<jemalloc>: error in background thread "
440: 				      "creation for arena %u. Abort.\n", ind);
441: 			abort();
442: 		}
443: 	}
444: }
445: 
446: arena_t *
447: arena_init(tsdn_t *tsdn, unsigned ind, const arena_config_t *config) {
448: 	arena_t *arena;
449: 
450: 	malloc_mutex_lock(tsdn, &arenas_lock);
451: 	arena = arena_init_locked(tsdn, ind, config);
452: 	malloc_mutex_unlock(tsdn, &arenas_lock);
453: 
454: 	arena_new_create_background_thread(tsdn, ind);
455: 
456: 	return arena;
457: }
458: 
459: static void
460: arena_bind(tsd_t *tsd, unsigned ind, bool internal) {
461: 	arena_t *arena = arena_get(tsd_tsdn(tsd), ind, false);
462: 	arena_nthreads_inc(arena, internal);
463: 
464: 	if (internal) {
465: 		tsd_iarena_set(tsd, arena);
466: 	} else {
467: 		tsd_arena_set(tsd, arena);
468: 		/*
469: 		 * While shard acts as a random seed, the cast below should
470: 		 * not make much difference.
471: 		 */
472: 		uint8_t shard = (uint8_t)atomic_fetch_add_u(
473: 		    &arena->binshard_next, 1, ATOMIC_RELAXED);
474: 		tsd_binshards_t *bins = tsd_binshardsp_get(tsd);
475: 		for (unsigned i = 0; i < SC_NBINS; i++) {
476: 			assert(bin_infos[i].n_shards > 0 &&
477: 			    bin_infos[i].n_shards <= BIN_SHARDS_MAX);
478: 			bins->binshard[i] = shard % bin_infos[i].n_shards;
479: 		}
480: 	}
481: }
482: 
483: void
484: arena_migrate(tsd_t *tsd, arena_t *oldarena, arena_t *newarena) {
485: 	assert(oldarena != NULL);
486: 	assert(newarena != NULL);
487: 
488: 	arena_nthreads_dec(oldarena, false);
489: 	arena_nthreads_inc(newarena, false);
490: 	tsd_arena_set(tsd, newarena);
491: 
492: 	if (arena_nthreads_get(oldarena, false) == 0) {
493: 		/* Purge if the old arena has no associated threads anymore. */
494: 		arena_decay(tsd_tsdn(tsd), oldarena,
495: 		    /* is_background_thread */ false, /* all */ true);
496: 	}
497: }
498: 
499: static void
500: arena_unbind(tsd_t *tsd, unsigned ind, bool internal) {
501: 	arena_t *arena;
502: 
503: 	arena = arena_get(tsd_tsdn(tsd), ind, false);
504: 	arena_nthreads_dec(arena, internal);
505: 
506: 	if (internal) {
507: 		tsd_iarena_set(tsd, NULL);
508: 	} else {
509: 		tsd_arena_set(tsd, NULL);
510: 	}
511: }
512: 
513: /* Slow path, called only by arena_choose(). */
514: arena_t *
515: arena_choose_hard(tsd_t *tsd, bool internal) {
516: 	arena_t *ret JEMALLOC_CC_SILENCE_INIT(NULL);
517: 
518: 	if (have_percpu_arena && PERCPU_ARENA_ENABLED(opt_percpu_arena)) {
519: 		unsigned choose = percpu_arena_choose();
520: 		ret = arena_get(tsd_tsdn(tsd), choose, true);
521: 		assert(ret != NULL);
522: 		arena_bind(tsd, arena_ind_get(ret), false);
523: 		arena_bind(tsd, arena_ind_get(ret), true);
524: 
525: 		return ret;
526: 	}
527: 
528: 	if (narenas_auto > 1) {
529: 		unsigned i, j, choose[2], first_null;
530: 		bool is_new_arena[2];
531: 
532: 		/*
533: 		 * Determine binding for both non-internal and internal
534: 		 * allocation.
535: 		 *
536: 		 *   choose[0]: For application allocation.
537: 		 *   choose[1]: For internal metadata allocation.
538: 		 */
539: 
540: 		for (j = 0; j < 2; j++) {
541: 			choose[j] = 0;
542: 			is_new_arena[j] = false;
543: 		}
544: 
545: 		first_null = narenas_auto;
546: 		malloc_mutex_lock(tsd_tsdn(tsd), &arenas_lock);
547: 		assert(arena_get(tsd_tsdn(tsd), 0, false) != NULL);
548: 		for (i = 1; i < narenas_auto; i++) {
549: 			if (arena_get(tsd_tsdn(tsd), i, false) != NULL) {
550: 				/*
551: 				 * Choose the first arena that has the lowest
552: 				 * number of threads assigned to it.
553: 				 */
554: 				for (j = 0; j < 2; j++) {
555: 					if (arena_nthreads_get(arena_get(
556: 					    tsd_tsdn(tsd), i, false), !!j) <
557: 					    arena_nthreads_get(arena_get(
558: 					    tsd_tsdn(tsd), choose[j], false),
559: 					    !!j)) {
560: 						choose[j] = i;
561: 					}
562: 				}
563: 			} else if (first_null == narenas_auto) {
564: 				/*
565: 				 * Record the index of the first uninitialized
566: 				 * arena, in case all extant arenas are in use.
567: 				 *
568: 				 * NB: It is possible for there to be
569: 				 * discontinuities in terms of initialized
570: 				 * versus uninitialized arenas, due to the
571: 				 * "thread.arena" mallctl.
572: 				 */
573: 				first_null = i;
574: 			}
575: 		}
576: 
577: 		for (j = 0; j < 2; j++) {
578: 			if (arena_nthreads_get(arena_get(tsd_tsdn(tsd),
579: 			    choose[j], false), !!j) == 0 || first_null ==
580: 			    narenas_auto) {
581: 				/*
582: 				 * Use an unloaded arena, or the least loaded
583: 				 * arena if all arenas are already initialized.
584: 				 */
585: 				if (!!j == internal) {
586: 					ret = arena_get(tsd_tsdn(tsd),
587: 					    choose[j], false);
588: 				}
589: 			} else {
590: 				arena_t *arena;
591: 
592: 				/* Initialize a new arena. */
593: 				choose[j] = first_null;
594: 				arena = arena_init_locked(tsd_tsdn(tsd),
595: 				    choose[j], &arena_config_default);
596: 				if (arena == NULL) {
597: 					malloc_mutex_unlock(tsd_tsdn(tsd),
598: 					    &arenas_lock);
599: 					return NULL;
600: 				}
601: 				is_new_arena[j] = true;
602: 				if (!!j == internal) {
603: 					ret = arena;
604: 				}
605: 			}
606: 			arena_bind(tsd, choose[j], !!j);
607: 		}
608: 		malloc_mutex_unlock(tsd_tsdn(tsd), &arenas_lock);
609: 
610: 		for (j = 0; j < 2; j++) {
611: 			if (is_new_arena[j]) {
612: 				assert(choose[j] > 0);
613: 				arena_new_create_background_thread(
614: 				    tsd_tsdn(tsd), choose[j]);
615: 			}
616: 		}
617: 
618: 	} else {
619: 		ret = arena_get(tsd_tsdn(tsd), 0, false);
620: 		arena_bind(tsd, 0, false);
621: 		arena_bind(tsd, 0, true);
622: 	}
623: 
624: 	return ret;
625: }
626: 
627: void
628: iarena_cleanup(tsd_t *tsd) {
629: 	arena_t *iarena;
630: 
631: 	iarena = tsd_iarena_get(tsd);
632: 	if (iarena != NULL) {
633: 		arena_unbind(tsd, arena_ind_get(iarena), true);
634: 	}
635: }
636: 
637: void
638: arena_cleanup(tsd_t *tsd) {
639: 	arena_t *arena;
640: 
641: 	arena = tsd_arena_get(tsd);
642: 	if (arena != NULL) {
643: 		arena_unbind(tsd, arena_ind_get(arena), false);
644: 	}
645: }
646: 
647: static void
648: stats_print_atexit(void) {
649: 	if (config_stats) {
650: 		tsdn_t *tsdn;
651: 		unsigned narenas, i;
652: 
653: 		tsdn = tsdn_fetch();
654: 
655: 		/*
656: 		 * Merge stats from extant threads.  This is racy, since
657: 		 * individual threads do not lock when recording tcache stats
658: 		 * events.  As a consequence, the final stats may be slightly
659: 		 * out of date by the time they are reported, if other threads
660: 		 * continue to allocate.
661: 		 */
662: 		for (i = 0, narenas = narenas_total_get(); i < narenas; i++) {
663: 			arena_t *arena = arena_get(tsdn, i, false);
664: 			if (arena != NULL) {
665: 				tcache_slow_t *tcache_slow;
666: 
667: 				malloc_mutex_lock(tsdn, &arena->tcache_ql_mtx);
668: 				ql_foreach(tcache_slow, &arena->tcache_ql,
669: 				    link) {
670: 					tcache_stats_merge(tsdn,
671: 					    tcache_slow->tcache, arena);
672: 				}
673: 				malloc_mutex_unlock(tsdn,
674: 				    &arena->tcache_ql_mtx);
675: 			}
676: 		}
677: 	}
678: 	je_malloc_stats_print(NULL, NULL, opt_stats_print_opts);
679: }
680: 
681: /*
682:  * Ensure that we don't hold any locks upon entry to or exit from allocator
683:  * code (in a "broad" sense that doesn't count a reentrant allocation as an
684:  * entrance or exit).
685:  */
686: JEMALLOC_ALWAYS_INLINE void
687: check_entry_exit_locking(tsdn_t *tsdn) {
688: 	if (!config_debug) {
689: 		return;
690: 	}
691: 	if (tsdn_null(tsdn)) {
692: 		return;
693: 	}
694: 	tsd_t *tsd = tsdn_tsd(tsdn);
695: 	/*
696: 	 * It's possible we hold locks at entry/exit if we're in a nested
697: 	 * allocation.
698: 	 */
699: 	int8_t reentrancy_level = tsd_reentrancy_level_get(tsd);
700: 	if (reentrancy_level != 0) {
701: 		return;
702: 	}
703: 	witness_assert_lockless(tsdn_witness_tsdp_get(tsdn));
704: }
705: 
706: /*
707:  * End miscellaneous support functions.
708:  */
709: /******************************************************************************/
710: /*
711:  * Begin initialization functions.
712:  */
713: 
714: static char *
715: jemalloc_getenv(const char *name) {
716: #ifdef JEMALLOC_FORCE_GETENV
717: 	return getenv(name);
718: #else
719: #  ifdef JEMALLOC_HAVE_SECURE_GETENV
720: 	return secure_getenv(name);
721: #  else
722: #    ifdef JEMALLOC_HAVE_ISSETUGID
723: 	if (issetugid() != 0) {
724: 		return NULL;
725: 	}
726: #    endif
727: 	return getenv(name);
728: #  endif
729: #endif
730: }
731: 
732: static unsigned
733: malloc_ncpus(void) {
734: 	long result;
735: 
736: #ifdef _WIN32
737: 	SYSTEM_INFO si;
738: 	GetSystemInfo(&si);
739: 	result = si.dwNumberOfProcessors;
740: #elif defined(CPU_COUNT)
741: 	/*
742: 	 * glibc >= 2.6 has the CPU_COUNT macro.
743: 	 *
744: 	 * glibc's sysconf() uses isspace().  glibc allocates for the first time
745: 	 * *before* setting up the isspace tables.  Therefore we need a
746: 	 * different method to get the number of CPUs.
747: 	 *
748: 	 * The getaffinity approach is also preferred when only a subset of CPUs
749: 	 * is available, to avoid using more arenas than necessary.
750: 	 */
751: 	{
752: #  if defined(__FreeBSD__) || defined(__DragonFly__)
753: 		cpuset_t set;
754: #  else
755: 		cpu_set_t set;
756: #  endif
757: #  if defined(JEMALLOC_HAVE_SCHED_SETAFFINITY)
758: 		sched_getaffinity(0, sizeof(set), &set);
759: #  else
760: 		pthread_getaffinity_np(pthread_self(), sizeof(set), &set);
761: #  endif
762: 		result = CPU_COUNT(&set);
763: 	}
764: #else
765: 	result = sysconf(_SC_NPROCESSORS_ONLN);
766: #endif
767: 	return ((result == -1) ? 1 : (unsigned)result);
768: }
769: 
770: /*
771:  * Ensure that number of CPUs is determistinc, i.e. it is the same based on:
772:  * - sched_getaffinity()
773:  * - _SC_NPROCESSORS_ONLN
774:  * - _SC_NPROCESSORS_CONF
775:  * Since otherwise tricky things is possible with percpu arenas in use.
776:  */
777: static bool
778: malloc_cpu_count_is_deterministic(void)
779: {
780: #ifdef _WIN32
781: 	return true;
782: #else
783: 	long cpu_onln = sysconf(_SC_NPROCESSORS_ONLN);
784: 	long cpu_conf = sysconf(_SC_NPROCESSORS_CONF);
785: 	if (cpu_onln != cpu_conf) {
786: 		return false;
787: 	}
788: #  if defined(CPU_COUNT)
789: #    if defined(__FreeBSD__) || defined(__DragonFly__)
790: 	cpuset_t set;
791: #    else
792: 	cpu_set_t set;
793: #    endif /* __FreeBSD__ */
794: #    if defined(JEMALLOC_HAVE_SCHED_SETAFFINITY)
795: 	sched_getaffinity(0, sizeof(set), &set);
796: #    else /* !JEMALLOC_HAVE_SCHED_SETAFFINITY */
797: 	pthread_getaffinity_np(pthread_self(), sizeof(set), &set);
798: #    endif /* JEMALLOC_HAVE_SCHED_SETAFFINITY */
799: 	long cpu_affinity = CPU_COUNT(&set);
800: 	if (cpu_affinity != cpu_conf) {
801: 		return false;
802: 	}
803: #  endif /* CPU_COUNT */
804: 	return true;
805: #endif
806: }
807: 
808: static void
809: init_opt_stats_opts(const char *v, size_t vlen, char *dest) {
810: 	size_t opts_len = strlen(dest);
811: 	assert(opts_len <= stats_print_tot_num_options);
812: 
813: 	for (size_t i = 0; i < vlen; i++) {
814: 		switch (v[i]) {
815: #define OPTION(o, v, d, s) case o: break;
816: 			STATS_PRINT_OPTIONS
817: #undef OPTION
818: 		default: continue;
819: 		}
820: 
821: 		if (strchr(dest, v[i]) != NULL) {
822: 			/* Ignore repeated. */
823: 			continue;
824: 		}
825: 
826: 		dest[opts_len++] = v[i];
827: 		dest[opts_len] = '\0';
828: 		assert(opts_len <= stats_print_tot_num_options);
829: 	}
830: 	assert(opts_len == strlen(dest));
831: }
832: 
833: static void
834: malloc_conf_format_error(const char *msg, const char *begin, const char *end) {
835: 	size_t len = end - begin + 1;
836: 	len = len > BUFERROR_BUF ? BUFERROR_BUF : len;
837: 
838: 	malloc_printf("<jemalloc>: %s -- %.*s\n", msg, (int)len, begin);
839: }
840: 
841: static bool
842: malloc_conf_next(char const **opts_p, char const **k_p, size_t *klen_p,
843:     char const **v_p, size_t *vlen_p) {
844: 	bool accept;
845: 	const char *opts = *opts_p;
846: 
847: 	*k_p = opts;
848: 
849: 	for (accept = false; !accept;) {
850: 		switch (*opts) {
851: 		case 'A': case 'B': case 'C': case 'D': case 'E': case 'F':
852: 		case 'G': case 'H': case 'I': case 'J': case 'K': case 'L':
853: 		case 'M': case 'N': case 'O': case 'P': case 'Q': case 'R':
854: 		case 'S': case 'T': case 'U': case 'V': case 'W': case 'X':
855: 		case 'Y': case 'Z':
856: 		case 'a': case 'b': case 'c': case 'd': case 'e': case 'f':
857: 		case 'g': case 'h': case 'i': case 'j': case 'k': case 'l':
858: 		case 'm': case 'n': case 'o': case 'p': case 'q': case 'r':
859: 		case 's': case 't': case 'u': case 'v': case 'w': case 'x':
860: 		case 'y': case 'z':
861: 		case '0': case '1': case '2': case '3': case '4': case '5':
862: 		case '6': case '7': case '8': case '9':
863: 		case '_':
864: 			opts++;
865: 			break;
866: 		case ':':
867: 			opts++;
868: 			*klen_p = (uintptr_t)opts - 1 - (uintptr_t)*k_p;
869: 			*v_p = opts;
870: 			accept = true;
871: 			break;
872: 		case '\0':
873: 			if (opts != *opts_p) {
874: 				malloc_conf_format_error(
875: 				    "Conf string ends with key",
876: 				    *opts_p, opts - 1);
877: 				had_conf_error = true;
878: 			}
879: 			return true;
880: 		default:
881: 			malloc_conf_format_error(
882: 			    "Malformed conf string", *opts_p, opts);
883: 			had_conf_error = true;
884: 			return true;
885: 		}
886: 	}
887: 
888: 	for (accept = false; !accept;) {
889: 		switch (*opts) {
890: 		case ',':
891: 			opts++;
892: 			/*
893: 			 * Look ahead one character here, because the next time
894: 			 * this function is called, it will assume that end of
895: 			 * input has been cleanly reached if no input remains,
896: 			 * but we have optimistically already consumed the
897: 			 * comma if one exists.
898: 			 */
899: 			if (*opts == '\0') {
900: 				malloc_conf_format_error(
901: 				    "Conf string ends with comma",
902: 				    *opts_p, opts - 1);
903: 				had_conf_error = true;
904: 			}
905: 			*vlen_p = (uintptr_t)opts - 1 - (uintptr_t)*v_p;
906: 			accept = true;
907: 			break;
908: 		case '\0':
909: 			*vlen_p = (uintptr_t)opts - (uintptr_t)*v_p;
910: 			accept = true;
911: 			break;
912: 		default:
913: 			opts++;
914: 			break;
915: 		}
916: 	}
917: 
918: 	*opts_p = opts;
919: 	return false;
920: }
921: 
922: static void
923: malloc_abort_invalid_conf(void) {
924: 	assert(opt_abort_conf);
925: 	malloc_printf("<jemalloc>: Abort (abort_conf:true) on invalid conf "
926: 	    "value (see above).\n");
927: 	invalid_conf_abort();
928: }
929: 
930: static void
931: malloc_conf_error(const char *msg, const char *k, size_t klen, const char *v,
932:     size_t vlen) {
933: 	malloc_printf("<jemalloc>: %s: %.*s:%.*s\n", msg, (int)klen, k,
934: 	    (int)vlen, v);
935: 	/* If abort_conf is set, error out after processing all options. */
936: 	const char *experimental = "experimental_";
937: 	if (strncmp(k, experimental, strlen(experimental)) == 0) {
938: 		/* However, tolerate experimental features. */
939: 		return;
940: 	}
941: 	had_conf_error = true;
942: }
943: 
944: static void
945: malloc_slow_flag_init(void) {
946: 	/*
947: 	 * Combine the runtime options into malloc_slow for fast path.  Called
948: 	 * after processing all the options.
949: 	 */
950: 	malloc_slow_flags |= (opt_junk_alloc ? flag_opt_junk_alloc : 0)
951: 	    | (opt_junk_free ? flag_opt_junk_free : 0)
952: 	    | (opt_zero ? flag_opt_zero : 0)
953: 	    | (opt_utrace ? flag_opt_utrace : 0)
954: 	    | (opt_xmalloc ? flag_opt_xmalloc : 0);
955: 
956: 	malloc_slow = (malloc_slow_flags != 0);
957: }
958: 
959: /* Number of sources for initializing malloc_conf */
960: #define MALLOC_CONF_NSOURCES 5
961: 
962: static const char *
963: obtain_malloc_conf(unsigned which_source, char readlink_buf[PATH_MAX + 1]) {
964: 	if (config_debug) {
965: 		static unsigned read_source = 0;
966: 		/*
967: 		 * Each source should only be read once, to minimize # of
968: 		 * syscalls on init.
969: 		 */
970: 		assert(read_source == which_source);
971: 		read_source++;
972: 	}
973: 	assert(which_source < MALLOC_CONF_NSOURCES);
974: 
975: 	const char *ret;
976: 	switch (which_source) {
977: 	case 0:
978: 		ret = config_malloc_conf;
979: 		break;
980: 	case 1:
981: 		if (je_malloc_conf != NULL) {
982: 			/* Use options that were compiled into the program. */
983: 			ret = je_malloc_conf;
984: 		} else {
985: 			/* No configuration specified. */
986: 			ret = NULL;
987: 		}
988: 		break;
989: 	case 2: {
990: 		ssize_t linklen = 0;
991: #ifndef _WIN32
992: 		int saved_errno = errno;
993: 		const char *linkname =
994: #  ifdef JEMALLOC_PREFIX
995: 		    "/etc/"JEMALLOC_PREFIX"malloc.conf"
996: #  else
997: 		    "/etc/malloc.conf"
998: #  endif
999: 		    ;
1000: 
1001: 		/*
1002: 		 * Try to use the contents of the "/etc/malloc.conf" symbolic
1003: 		 * link's name.
1004: 		 */
1005: #ifndef JEMALLOC_READLINKAT
1006: 		linklen = readlink(linkname, readlink_buf, PATH_MAX);
1007: #else
1008: 		linklen = readlinkat(AT_FDCWD, linkname, readlink_buf, PATH_MAX);
1009: #endif
1010: 		if (linklen == -1) {
1011: 			/* No configuration specified. */
1012: 			linklen = 0;
1013: 			/* Restore errno. */
1014: 			set_errno(saved_errno);
1015: 		}
1016: #endif
1017: 		readlink_buf[linklen] = '\0';
1018: 		ret = readlink_buf;
1019: 		break;
1020: 	} case 3: {
1021: 		const char *envname =
1022: #ifdef JEMALLOC_PREFIX
1023: 		    JEMALLOC_CPREFIX"MALLOC_CONF"
1024: #else
1025: 		    "MALLOC_CONF"
1026: #endif
1027: 		    ;
1028: 
1029: 		if ((ret = jemalloc_getenv(envname)) != NULL) {
1030: 			opt_malloc_conf_env_var = ret;
1031: 		} else {
1032: 			/* No configuration specified. */
1033: 			ret = NULL;
1034: 		}
1035: 		break;
1036: 	} case 4: {
1037: 		ret = je_malloc_conf_2_conf_harder;
1038: 		break;
1039: 	} default:
1040: 		not_reached();
1041: 		ret = NULL;
1042: 	}
1043: 	return ret;
1044: }
1045: 
1046: static void
1047: validate_hpa_settings(void) {
1048: 	if (!hpa_supported() || !opt_hpa || opt_hpa_opts.dirty_mult == (fxp_t)-1) {
1049: 		return;
1050: 	}
1051: 	size_t hpa_threshold = fxp_mul_frac(HUGEPAGE, opt_hpa_opts.dirty_mult) +
1052: 	    opt_hpa_opts.hugification_threshold;
1053: 	if (hpa_threshold > HUGEPAGE) {
1054: 		return;
1055: 	}
1056: 
1057: 	had_conf_error = true;
1058: 	char hpa_dirty_mult[FXP_BUF_SIZE];
1059: 	char hugification_threshold[FXP_BUF_SIZE];
1060: 	char normalization_message[256] = {0};
1061: 	fxp_print(opt_hpa_opts.dirty_mult, hpa_dirty_mult);
1062: 	fxp_print(fxp_div(FXP_INIT_INT((unsigned)
1063: 	    (opt_hpa_opts.hugification_threshold >> LG_PAGE)),
1064: 	    FXP_INIT_INT(HUGEPAGE_PAGES)), hugification_threshold);
1065: 	if (!opt_abort_conf) {
1066: 		char normalized_hugification_threshold[FXP_BUF_SIZE];
1067: 		opt_hpa_opts.hugification_threshold +=
1068: 		    HUGEPAGE - hpa_threshold;
1069: 		fxp_print(fxp_div(FXP_INIT_INT((unsigned)
1070: 		    (opt_hpa_opts.hugification_threshold >> LG_PAGE)),
1071: 		    FXP_INIT_INT(HUGEPAGE_PAGES)),
1072: 		    normalized_hugification_threshold);
1073: 		malloc_snprintf(normalization_message,
1074: 		    sizeof(normalization_message), "<jemalloc>: Normalizing "
1075: 		    "HPA settings to avoid pathological behavior, setting "
1076: 		    "hpa_hugification_threshold_ratio: to %s.\n",
1077: 		    normalized_hugification_threshold);
1078: 	}
1079: 	malloc_printf(
1080: 	    "<jemalloc>: Invalid combination of options "
1081: 	    "hpa_hugification_threshold_ratio: %s and hpa_dirty_mult: %s. "
1082: 	    "These values should sum to > 1.0.\n%s", hugification_threshold,
1083: 	    hpa_dirty_mult, normalization_message);
1084: }
1085: 
1086: static void
1087: malloc_conf_init_helper(sc_data_t *sc_data, unsigned bin_shard_sizes[SC_NBINS],
1088:     bool initial_call, const char *opts_cache[MALLOC_CONF_NSOURCES],
1089:     char readlink_buf[PATH_MAX + 1]) {
1090: 	static const char *opts_explain[MALLOC_CONF_NSOURCES] = {
1091: 		"string specified via --with-malloc-conf",
1092: 		"string pointed to by the global variable malloc_conf",
1093: 		"\"name\" of the file referenced by the symbolic link named "
1094: 		    "/etc/malloc.conf",
1095: 		"value of the environment variable MALLOC_CONF",
1096: 		"string pointed to by the global variable "
1097: 		    "malloc_conf_2_conf_harder",
1098: 	};
1099: 	unsigned i;
1100: 	const char *opts, *k, *v;
1101: 	size_t klen, vlen;
1102: 
1103: 	for (i = 0; i < MALLOC_CONF_NSOURCES; i++) {
1104: 		/* Get runtime configuration. */
1105: 		if (initial_call) {
1106: 			opts_cache[i] = obtain_malloc_conf(i, readlink_buf);
1107: 		}
1108: 		opts = opts_cache[i];
1109: 		if (!initial_call && opt_confirm_conf) {
1110: 			malloc_printf(
1111: 			    "<jemalloc>: malloc_conf #%u (%s): \"%s\"\n",
1112: 			    i + 1, opts_explain[i], opts != NULL ? opts : "");
1113: 		}
1114: 		if (opts == NULL) {
1115: 			continue;
1116: 		}
1117: 
1118: 		while (*opts != '\0' && !malloc_conf_next(&opts, &k, &klen, &v,
1119: 		    &vlen)) {
1120: 
1121: #define CONF_ERROR(msg, k, klen, v, vlen)				\
1122: 			if (!initial_call) {				\
1123: 				malloc_conf_error(			\
1124: 				    msg, k, klen, v, vlen);		\
1125: 				cur_opt_valid = false;			\
1126: 			}
1127: #define CONF_CONTINUE	{						\
1128: 				if (!initial_call && opt_confirm_conf	\
1129: 				    && cur_opt_valid) {			\
1130: 					malloc_printf("<jemalloc>: -- "	\
1131: 					    "Set conf value: %.*s:%.*s"	\
1132: 					    "\n", (int)klen, k,		\
1133: 					    (int)vlen, v);		\
1134: 				}					\
1135: 				continue;				\
1136: 			}
1137: #define CONF_MATCH(n)							\
1138: 	(sizeof(n)-1 == klen && strncmp(n, k, klen) == 0)
1139: #define CONF_MATCH_VALUE(n)						\
1140: 	(sizeof(n)-1 == vlen && strncmp(n, v, vlen) == 0)
1141: #define CONF_HANDLE_BOOL(o, n)						\
1142: 			if (CONF_MATCH(n)) {				\
1143: 				if (CONF_MATCH_VALUE("true")) {		\
1144: 					o = true;			\
1145: 				} else if (CONF_MATCH_VALUE("false")) {	\
1146: 					o = false;			\
1147: 				} else {				\
1148: 					CONF_ERROR("Invalid conf value",\
1149: 					    k, klen, v, vlen);		\
1150: 				}					\
1151: 				CONF_CONTINUE;				\
1152: 			}
1153:       /*
1154:        * One of the CONF_MIN macros below expands, in one of the use points,
1155:        * to "unsigned integer < 0", which is always false, triggering the
1156:        * GCC -Wtype-limits warning, which we disable here and re-enable below.
1157:        */
1158:       JEMALLOC_DIAGNOSTIC_PUSH
1159:       JEMALLOC_DIAGNOSTIC_IGNORE_TYPE_LIMITS
1160: 
1161: #define CONF_DONT_CHECK_MIN(um, min)	false
1162: #define CONF_CHECK_MIN(um, min)	((um) < (min))
1163: #define CONF_DONT_CHECK_MAX(um, max)	false
1164: #define CONF_CHECK_MAX(um, max)	((um) > (max))
1165: 
1166: #define CONF_VALUE_READ(max_t, result)					\
1167: 	      char *end;						\
1168: 	      set_errno(0);						\
1169: 	      result = (max_t)malloc_strtoumax(v, &end, 0);
1170: #define CONF_VALUE_READ_FAIL()						\
1171: 	      (get_errno() != 0 || (uintptr_t)end - (uintptr_t)v != vlen)
1172: 
1173: #define CONF_HANDLE_T(t, max_t, o, n, min, max, check_min, check_max, clip) \
1174: 			if (CONF_MATCH(n)) {				\
1175: 				max_t mv;				\
1176: 				CONF_VALUE_READ(max_t, mv)		\
1177: 				if (CONF_VALUE_READ_FAIL()) {		\
1178: 					CONF_ERROR("Invalid conf value",\
1179: 					    k, klen, v, vlen);		\
1180: 				} else if (clip) {			\
1181: 					if (check_min(mv, (t)(min))) {	\
1182: 						o = (t)(min);		\
1183: 					} else if (			\
1184: 					    check_max(mv, (t)(max))) {	\
1185: 						o = (t)(max);		\
1186: 					} else {			\
1187: 						o = (t)mv;		\
1188: 					}				\
1189: 				} else {				\
1190: 					if (check_min(mv, (t)(min)) ||	\
1191: 					    check_max(mv, (t)(max))) {	\
1192: 						CONF_ERROR(		\
1193: 						    "Out-of-range "	\
1194: 						    "conf value",	\
1195: 						    k, klen, v, vlen);	\
1196: 					} else {			\
1197: 						o = (t)mv;		\
1198: 					}				\
1199: 				}					\
1200: 				CONF_CONTINUE;				\
1201: 			}
1202: #define CONF_HANDLE_T_U(t, o, n, min, max, check_min, check_max, clip)	\
1203: 	      CONF_HANDLE_T(t, uintmax_t, o, n, min, max, check_min,	\
1204: 			    check_max, clip)
1205: #define CONF_HANDLE_T_SIGNED(t, o, n, min, max, check_min, check_max, clip)\
1206: 	      CONF_HANDLE_T(t, intmax_t, o, n, min, max, check_min,	\
1207: 			    check_max, clip)
1208: 
1209: #define CONF_HANDLE_UNSIGNED(o, n, min, max, check_min, check_max,	\
1210:     clip)								\
1211: 			CONF_HANDLE_T_U(unsigned, o, n, min, max,	\
1212: 			    check_min, check_max, clip)
1213: #define CONF_HANDLE_SIZE_T(o, n, min, max, check_min, check_max, clip)	\
1214: 			CONF_HANDLE_T_U(size_t, o, n, min, max,		\
1215: 			    check_min, check_max, clip)
1216: #define CONF_HANDLE_INT64_T(o, n, min, max, check_min, check_max, clip)	\
1217: 			CONF_HANDLE_T_SIGNED(int64_t, o, n, min, max,	\
1218: 			    check_min, check_max, clip)
1219: #define CONF_HANDLE_UINT64_T(o, n, min, max, check_min, check_max, clip)\
1220: 			CONF_HANDLE_T_U(uint64_t, o, n, min, max,	\
1221: 			    check_min, check_max, clip)
1222: #define CONF_HANDLE_SSIZE_T(o, n, min, max)				\
1223: 			CONF_HANDLE_T_SIGNED(ssize_t, o, n, min, max,	\
1224: 			    CONF_CHECK_MIN, CONF_CHECK_MAX, false)
1225: #define CONF_HANDLE_CHAR_P(o, n, d)					\
1226: 			if (CONF_MATCH(n)) {				\
1227: 				size_t cpylen = (vlen <=		\
1228: 				    sizeof(o)-1) ? vlen :		\
1229: 				    sizeof(o)-1;			\
1230: 				strncpy(o, v, cpylen);			\
1231: 				o[cpylen] = '\0';			\
1232: 				CONF_CONTINUE;				\
1233: 			}
1234: 
1235: 			bool cur_opt_valid = true;
1236: 
1237: 			CONF_HANDLE_BOOL(opt_confirm_conf, "confirm_conf")
1238: 			if (initial_call) {
1239: 				continue;
1240: 			}
1241: 
1242: 			CONF_HANDLE_BOOL(opt_abort, "abort")
1243: 			CONF_HANDLE_BOOL(opt_abort_conf, "abort_conf")
1244: 			CONF_HANDLE_BOOL(opt_cache_oblivious, "cache_oblivious")
1245: 			CONF_HANDLE_BOOL(opt_trust_madvise, "trust_madvise")
1246: 			if (strncmp("metadata_thp", k, klen) == 0) {
1247: 				int m;
1248: 				bool match = false;
1249: 				for (m = 0; m < metadata_thp_mode_limit; m++) {
1250: 					if (strncmp(metadata_thp_mode_names[m],
1251: 					    v, vlen) == 0) {
1252: 						opt_metadata_thp = m;
1253: 						match = true;
1254: 						break;
1255: 					}
1256: 				}
1257: 				if (!match) {
1258: 					CONF_ERROR("Invalid conf value",
1259: 					    k, klen, v, vlen);
1260: 				}
1261: 				CONF_CONTINUE;
1262: 			}
1263: 			CONF_HANDLE_BOOL(opt_retain, "retain")
1264: 			if (strncmp("dss", k, klen) == 0) {
1265: 				int m;
1266: 				bool match = false;
1267: 				for (m = 0; m < dss_prec_limit; m++) {
1268: 					if (strncmp(dss_prec_names[m], v, vlen)
1269: 					    == 0) {
1270: 						if (extent_dss_prec_set(m)) {
1271: 							CONF_ERROR(
1272: 							    "Error setting dss",
1273: 							    k, klen, v, vlen);
1274: 						} else {
1275: 							opt_dss =
1276: 							    dss_prec_names[m];
1277: 							match = true;
1278: 							break;
1279: 						}
1280: 					}
1281: 				}
1282: 				if (!match) {
1283: 					CONF_ERROR("Invalid conf value",
1284: 					    k, klen, v, vlen);
1285: 				}
1286: 				CONF_CONTINUE;
1287: 			}
1288: 			if (CONF_MATCH("narenas")) {
1289: 				if (CONF_MATCH_VALUE("default")) {
1290: 					opt_narenas = 0;
1291: 					CONF_CONTINUE;
1292: 				} else {
1293: 					CONF_HANDLE_UNSIGNED(opt_narenas,
1294: 					    "narenas", 1, UINT_MAX,
1295: 					    CONF_CHECK_MIN, CONF_DONT_CHECK_MAX,
1296: 					    /* clip */ false)
1297: 				}
1298: 			}
1299: 			if (CONF_MATCH("narenas_ratio")) {
1300: 				char *end;
1301: 				bool err = fxp_parse(&opt_narenas_ratio, v,
1302: 				    &end);
1303: 				if (err || (size_t)(end - v) != vlen) {
1304: 					CONF_ERROR("Invalid conf value",
1305: 					    k, klen, v, vlen);
1306: 				}
1307: 				CONF_CONTINUE;
1308: 			}
1309: 			if (CONF_MATCH("bin_shards")) {
1310: 				const char *bin_shards_segment_cur = v;
1311: 				size_t vlen_left = vlen;
1312: 				do {
1313: 					size_t size_start;
1314: 					size_t size_end;
1315: 					size_t nshards;
1316: 					bool err = multi_setting_parse_next(
1317: 					    &bin_shards_segment_cur, &vlen_left,
1318: 					    &size_start, &size_end, &nshards);
1319: 					if (err || bin_update_shard_size(
1320: 					    bin_shard_sizes, size_start,
1321: 					    size_end, nshards)) {
1322: 						CONF_ERROR(
1323: 						    "Invalid settings for "
1324: 						    "bin_shards", k, klen, v,
1325: 						    vlen);
1326: 						break;
1327: 					}
1328: 				} while (vlen_left > 0);
1329: 				CONF_CONTINUE;
1330: 			}
1331: 			CONF_HANDLE_SIZE_T(opt_bin_info_max_batched_size,
1332: 			    "max_batched_size", 0, SIZE_T_MAX,
1333: 			    CONF_DONT_CHECK_MIN, CONF_DONT_CHECK_MAX,
1334: 			    /* clip */ true)
1335: 			CONF_HANDLE_SIZE_T(opt_bin_info_remote_free_max_batch,
1336: 			    "remote_free_max_batch", 0,
1337: 			    BIN_REMOTE_FREE_ELEMS_MAX,
1338: 			    CONF_DONT_CHECK_MIN, CONF_CHECK_MAX,
1339: 			    /* clip */ true)
1340: 			CONF_HANDLE_SIZE_T(opt_bin_info_remote_free_max,
1341: 			    "remote_free_max", 0,
1342: 			    BIN_REMOTE_FREE_ELEMS_MAX,
1343: 			    CONF_DONT_CHECK_MIN, CONF_CHECK_MAX,
1344: 			    /* clip */ true)
1345: 
1346: 			if (CONF_MATCH("tcache_ncached_max")) {
1347: 				bool err = tcache_bin_info_default_init(
1348: 				    v, vlen);
1349: 				if (err) {
1350: 					CONF_ERROR("Invalid settings for "
1351: 					    "tcache_ncached_max", k, klen, v,
1352: 					    vlen);
1353: 				}
1354: 				CONF_CONTINUE;
1355: 			}
1356: 			CONF_HANDLE_INT64_T(opt_mutex_max_spin,
1357: 			    "mutex_max_spin", -1, INT64_MAX, CONF_CHECK_MIN,
1358: 			    CONF_DONT_CHECK_MAX, false);
1359: 			CONF_HANDLE_SSIZE_T(opt_dirty_decay_ms,
1360: 			    "dirty_decay_ms", -1, NSTIME_SEC_MAX * KQU(1000) <
1361: 			    QU(SSIZE_MAX) ? NSTIME_SEC_MAX * KQU(1000) :
1362: 			    SSIZE_MAX);
1363: 			CONF_HANDLE_SSIZE_T(opt_muzzy_decay_ms,
1364: 			    "muzzy_decay_ms", -1, NSTIME_SEC_MAX * KQU(1000) <
1365: 			    QU(SSIZE_MAX) ? NSTIME_SEC_MAX * KQU(1000) :
1366: 			    SSIZE_MAX);
1367: 			CONF_HANDLE_BOOL(opt_stats_print, "stats_print")
1368: 			if (CONF_MATCH("stats_print_opts")) {
1369: 				init_opt_stats_opts(v, vlen,
1370: 				    opt_stats_print_opts);
1371: 				CONF_CONTINUE;
1372: 			}
1373: 			CONF_HANDLE_INT64_T(opt_stats_interval,
1374: 			    "stats_interval", -1, INT64_MAX,
1375: 			    CONF_CHECK_MIN, CONF_DONT_CHECK_MAX, false)
1376: 			if (CONF_MATCH("stats_interval_opts")) {
1377: 				init_opt_stats_opts(v, vlen,
1378: 				    opt_stats_interval_opts);
1379: 				CONF_CONTINUE;
1380: 			}
1381: 			if (config_fill) {
1382: 				if (CONF_MATCH("junk")) {
1383: 					if (CONF_MATCH_VALUE("true")) {
1384: 						opt_junk = "true";
1385: 						opt_junk_alloc = opt_junk_free =
1386: 						    true;
1387: 					} else if (CONF_MATCH_VALUE("false")) {
1388: 						opt_junk = "false";
1389: 						opt_junk_alloc = opt_junk_free =
1390: 						    false;
1391: 					} else if (CONF_MATCH_VALUE("alloc")) {
1392: 						opt_junk = "alloc";
1393: 						opt_junk_alloc = true;
1394: 						opt_junk_free = false;
1395: 					} else if (CONF_MATCH_VALUE("free")) {
1396: 						opt_junk = "free";
1397: 						opt_junk_alloc = false;
1398: 						opt_junk_free = true;
1399: 					} else {
1400: 						CONF_ERROR(
1401: 						    "Invalid conf value",
1402: 						    k, klen, v, vlen);
1403: 					}
1404: 					CONF_CONTINUE;
1405: 				}
1406: 				CONF_HANDLE_BOOL(opt_zero, "zero")
1407: 			}
1408: 			if (config_utrace) {
1409: 				CONF_HANDLE_BOOL(opt_utrace, "utrace")
1410: 			}
1411: 			if (config_xmalloc) {
1412: 				CONF_HANDLE_BOOL(opt_xmalloc, "xmalloc")
1413: 			}
1414: 			if (config_enable_cxx) {
1415: 				CONF_HANDLE_BOOL(
1416: 				    opt_experimental_infallible_new,
1417: 				    "experimental_infallible_new")
1418: 			}
1419: 
1420: 			CONF_HANDLE_BOOL(opt_tcache, "tcache")
1421: 			CONF_HANDLE_SIZE_T(opt_tcache_max, "tcache_max",
1422: 			    0, TCACHE_MAXCLASS_LIMIT, CONF_DONT_CHECK_MIN,
1423: 			    CONF_CHECK_MAX, /* clip */ true)
1424: 			if (CONF_MATCH("lg_tcache_max")) {
1425: 				size_t m;
1426: 				CONF_VALUE_READ(size_t, m)
1427: 				if (CONF_VALUE_READ_FAIL()) {
1428: 					CONF_ERROR("Invalid conf value",
1429: 					    k, klen, v, vlen);
1430: 				} else {
1431: 					/* clip if necessary */
1432: 					if (m > TCACHE_LG_MAXCLASS_LIMIT) {
1433: 						m = TCACHE_LG_MAXCLASS_LIMIT;
1434: 					}
1435: 					opt_tcache_max = (size_t)1 << m;
1436: 				}
1437: 				CONF_CONTINUE;
1438: 			}
1439: 			/*
1440: 			 * Anyone trying to set a value outside -16 to 16 is
1441: 			 * deeply confused.
1442: 			 */
1443: 			CONF_HANDLE_SSIZE_T(opt_lg_tcache_nslots_mul,
1444: 			    "lg_tcache_nslots_mul", -16, 16)
1445: 			/* Ditto with values past 2048. */
1446: 			CONF_HANDLE_UNSIGNED(opt_tcache_nslots_small_min,
1447: 			    "tcache_nslots_small_min", 1, 2048,
1448: 			    CONF_CHECK_MIN, CONF_CHECK_MAX, /* clip */ true)
1449: 			CONF_HANDLE_UNSIGNED(opt_tcache_nslots_small_max,
1450: 			    "tcache_nslots_small_max", 1, 2048,
1451: 			    CONF_CHECK_MIN, CONF_CHECK_MAX, /* clip */ true)
1452: 			CONF_HANDLE_UNSIGNED(opt_tcache_nslots_large,
1453: 			    "tcache_nslots_large", 1, 2048,
1454: 			    CONF_CHECK_MIN, CONF_CHECK_MAX, /* clip */ true)
1455: 			CONF_HANDLE_SIZE_T(opt_tcache_gc_incr_bytes,
1456: 			    "tcache_gc_incr_bytes", 1024, SIZE_T_MAX,
1457: 			    CONF_CHECK_MIN, CONF_DONT_CHECK_MAX,
1458: 			    /* clip */ true)
1459: 			CONF_HANDLE_SIZE_T(opt_tcache_gc_delay_bytes,
1460: 			    "tcache_gc_delay_bytes", 0, SIZE_T_MAX,
1461: 			    CONF_DONT_CHECK_MIN, CONF_DONT_CHECK_MAX,
1462: 			    /* clip */ false)
1463: 			CONF_HANDLE_UNSIGNED(opt_lg_tcache_flush_small_div,
1464: 			    "lg_tcache_flush_small_div", 1, 16,
1465: 			    CONF_CHECK_MIN, CONF_CHECK_MAX, /* clip */ true)
1466: 			CONF_HANDLE_UNSIGNED(opt_lg_tcache_flush_large_div,
1467: 			    "lg_tcache_flush_large_div", 1, 16,
1468: 			    CONF_CHECK_MIN, CONF_CHECK_MAX, /* clip */ true)
1469: 			CONF_HANDLE_UNSIGNED(opt_debug_double_free_max_scan,
1470: 			    "debug_double_free_max_scan", 0, UINT_MAX,
1471: 			    CONF_DONT_CHECK_MIN, CONF_DONT_CHECK_MAX,
1472: 			    /* clip */ false)
1473: 			CONF_HANDLE_SIZE_T(opt_calloc_madvise_threshold,
1474: 			    "calloc_madvise_threshold", 0, SC_LARGE_MAXCLASS,
1475: 			    CONF_DONT_CHECK_MIN, CONF_CHECK_MAX, /* clip */ false)
1476: 
1477: 			/*
1478: 			 * The runtime option of oversize_threshold remains
1479: 			 * undocumented.  It may be tweaked in the next major
1480: 			 * release (6.0).  The default value 8M is rather
1481: 			 * conservative / safe.  Tuning it further down may
1482: 			 * improve fragmentation a bit more, but may also cause
1483: 			 * contention on the huge arena.
1484: 			 */
1485: 			CONF_HANDLE_SIZE_T(opt_oversize_threshold,
1486: 			    "oversize_threshold", 0, SC_LARGE_MAXCLASS,
1487: 			    CONF_DONT_CHECK_MIN, CONF_CHECK_MAX, false)
1488: 			CONF_HANDLE_SIZE_T(opt_lg_extent_max_active_fit,
1489: 			    "lg_extent_max_active_fit", 0,
1490: 			    (sizeof(size_t) << 3), CONF_DONT_CHECK_MIN,
1491: 			    CONF_CHECK_MAX, false)
1492: 
1493: 			if (strncmp("percpu_arena", k, klen) == 0) {
1494: 				bool match = false;
1495: 				for (int m = percpu_arena_mode_names_base; m <
1496: 				    percpu_arena_mode_names_limit; m++) {
1497: 					if (strncmp(percpu_arena_mode_names[m],
1498: 					    v, vlen) == 0) {
1499: 						if (!have_percpu_arena) {
1500: 							CONF_ERROR(
1501: 							    "No getcpu support",
1502: 							    k, klen, v, vlen);
1503: 						}
1504: 						opt_percpu_arena = m;
1505: 						match = true;
1506: 						break;
1507: 					}
1508: 				}
1509: 				if (!match) {
1510: 					CONF_ERROR("Invalid conf value",
1511: 					    k, klen, v, vlen);
1512: 				}
1513: 				CONF_CONTINUE;
1514: 			}
1515: 			CONF_HANDLE_BOOL(opt_background_thread,
1516: 			    "background_thread");
1517: 			CONF_HANDLE_SIZE_T(opt_max_background_threads,
1518: 					   "max_background_threads", 1,
1519: 					   opt_max_background_threads,
1520: 					   CONF_CHECK_MIN, CONF_CHECK_MAX,
1521: 					   true);
1522: 			CONF_HANDLE_BOOL(opt_hpa, "hpa")
1523: 			CONF_HANDLE_SIZE_T(opt_hpa_opts.slab_max_alloc,
1524: 			    "hpa_slab_max_alloc", PAGE, HUGEPAGE,
1525: 			    CONF_CHECK_MIN, CONF_CHECK_MAX, true);
1526: 
1527: 			/*
1528: 			 * Accept either a ratio-based or an exact hugification
1529: 			 * threshold.
1530: 			 */
1531: 			CONF_HANDLE_SIZE_T(opt_hpa_opts.hugification_threshold,
1532: 			    "hpa_hugification_threshold", PAGE, HUGEPAGE,
1533: 			    CONF_CHECK_MIN, CONF_CHECK_MAX, true);
1534: 			if (CONF_MATCH("hpa_hugification_threshold_ratio")) {
1535: 				fxp_t ratio;
1536: 				char *end;
1537: 				bool err = fxp_parse(&ratio, v,
1538: 				    &end);
1539: 				if (err || (size_t)(end - v) != vlen
1540: 				    || ratio > FXP_INIT_INT(1)) {
1541: 					CONF_ERROR("Invalid conf value",
1542: 					    k, klen, v, vlen);
1543: 				} else {
1544: 					opt_hpa_opts.hugification_threshold =
1545: 					    fxp_mul_frac(HUGEPAGE, ratio);
1546: 				}
1547: 				CONF_CONTINUE;
1548: 			}
1549: 
1550: 			CONF_HANDLE_UINT64_T(
1551: 			    opt_hpa_opts.hugify_delay_ms, "hpa_hugify_delay_ms",
1552: 			    0, 0, CONF_DONT_CHECK_MIN, CONF_DONT_CHECK_MAX,
1553: 			    false);
1554: 
1555: 			CONF_HANDLE_UINT64_T(
1556: 			    opt_hpa_opts.min_purge_interval_ms,
1557: 			    "hpa_min_purge_interval_ms", 0, 0,
1558: 			    CONF_DONT_CHECK_MIN, CONF_DONT_CHECK_MAX, false);
1559: 
1560: 			CONF_HANDLE_BOOL(
1561: 			    opt_hpa_opts.strict_min_purge_interval,
1562: 			    "hpa_strict_min_purge_interval");
1563: 
1564: 			if (CONF_MATCH("hpa_dirty_mult")) {
1565: 				if (CONF_MATCH_VALUE("-1")) {
1566: 					opt_hpa_opts.dirty_mult = (fxp_t)-1;
1567: 					CONF_CONTINUE;
1568: 				}
1569: 				fxp_t ratio;
1570: 				char *end;
1571: 				bool err = fxp_parse(&ratio, v,
1572: 				    &end);
1573: 				if (err || (size_t)(end - v) != vlen) {
1574: 					CONF_ERROR("Invalid conf value",
1575: 					    k, klen, v, vlen);
1576: 				} else {
1577: 					opt_hpa_opts.dirty_mult = ratio;
1578: 				}
1579: 				CONF_CONTINUE;
1580: 			}
1581: 
1582: 			CONF_HANDLE_SIZE_T(opt_hpa_sec_opts.nshards,
1583: 			    "hpa_sec_nshards", 0, 0, CONF_CHECK_MIN,
1584: 			    CONF_DONT_CHECK_MAX, true);
1585: 			CONF_HANDLE_SIZE_T(opt_hpa_sec_opts.max_alloc,
1586: 			    "hpa_sec_max_alloc", PAGE, 0, CONF_CHECK_MIN,
1587: 			    CONF_DONT_CHECK_MAX, true);
1588: 			CONF_HANDLE_SIZE_T(opt_hpa_sec_opts.max_bytes,
1589: 			    "hpa_sec_max_bytes", PAGE, 0, CONF_CHECK_MIN,
1590: 			    CONF_DONT_CHECK_MAX, true);
1591: 			CONF_HANDLE_SIZE_T(opt_hpa_sec_opts.bytes_after_flush,
1592: 			    "hpa_sec_bytes_after_flush", PAGE, 0,
1593: 			    CONF_CHECK_MIN, CONF_DONT_CHECK_MAX, true);
1594: 			CONF_HANDLE_SIZE_T(opt_hpa_sec_opts.batch_fill_extra,
1595: 			    "hpa_sec_batch_fill_extra", 0, HUGEPAGE_PAGES,
1596: 			    CONF_CHECK_MIN, CONF_CHECK_MAX, true);
1597: 
1598: 			if (CONF_MATCH("slab_sizes")) {
1599: 				if (CONF_MATCH_VALUE("default")) {
1600: 					sc_data_init(sc_data);
1601: 					CONF_CONTINUE;
1602: 				}
1603: 				bool err;
1604: 				const char *slab_size_segment_cur = v;
1605: 				size_t vlen_left = vlen;
1606: 				do {
1607: 					size_t slab_start;
1608: 					size_t slab_end;
1609: 					size_t pgs;
1610: 					err = multi_setting_parse_next(
1611: 					    &slab_size_segment_cur,
1612: 					    &vlen_left, &slab_start, &slab_end,
1613: 					    &pgs);
1614: 					if (!err) {
1615: 						sc_data_update_slab_size(
1616: 						    sc_data, slab_start,
1617: 						    slab_end, (int)pgs);
1618: 					} else {
1619: 						CONF_ERROR("Invalid settings "
1620: 						    "for slab_sizes",
1621: 						    k, klen, v, vlen);
1622: 					}
1623: 				} while (!err && vlen_left > 0);
1624: 				CONF_CONTINUE;
1625: 			}
1626: 			if (config_prof) {
1627: 				CONF_HANDLE_BOOL(opt_prof, "prof")
1628: 				CONF_HANDLE_CHAR_P(opt_prof_prefix,
1629: 				    "prof_prefix", "jeprof")
1630: 				CONF_HANDLE_BOOL(opt_prof_active, "prof_active")
1631: 				CONF_HANDLE_BOOL(opt_prof_thread_active_init,
1632: 				    "prof_thread_active_init")
1633: 				CONF_HANDLE_SIZE_T(opt_lg_prof_sample,
1634: 				    "lg_prof_sample", 0, (sizeof(uint64_t) << 3)
1635: 				    - 1, CONF_DONT_CHECK_MIN, CONF_CHECK_MAX,
1636: 				    true)
1637: 				CONF_HANDLE_BOOL(opt_prof_accum, "prof_accum")
1638: 				CONF_HANDLE_UNSIGNED(opt_prof_bt_max, "prof_bt_max",
1639: 				    1, PROF_BT_MAX_LIMIT, CONF_CHECK_MIN, CONF_CHECK_MAX,
1640: 				    /* clip */ true)
1641: 				CONF_HANDLE_SSIZE_T(opt_lg_prof_interval,
1642: 				    "lg_prof_interval", -1,
1643: 				    (sizeof(uint64_t) << 3) - 1)
1644: 				CONF_HANDLE_BOOL(opt_prof_gdump, "prof_gdump")
1645: 				CONF_HANDLE_BOOL(opt_prof_final, "prof_final")
1646: 				CONF_HANDLE_BOOL(opt_prof_leak, "prof_leak")
1647: 				CONF_HANDLE_BOOL(opt_prof_leak_error,
1648: 				    "prof_leak_error")
1649: 				CONF_HANDLE_BOOL(opt_prof_log, "prof_log")
1650: 				CONF_HANDLE_BOOL(opt_prof_pid_namespace, "prof_pid_namespace")
1651: 				CONF_HANDLE_SSIZE_T(opt_prof_recent_alloc_max,
1652: 				    "prof_recent_alloc_max", -1, SSIZE_MAX)
1653: 				CONF_HANDLE_BOOL(opt_prof_stats, "prof_stats")
1654: 				CONF_HANDLE_BOOL(opt_prof_sys_thread_name,
1655: 				    "prof_sys_thread_name")
1656: 				if (CONF_MATCH("prof_time_resolution")) {
1657: 					if (CONF_MATCH_VALUE("default")) {
1658: 						opt_prof_time_res =
1659: 						    prof_time_res_default;
1660: 					} else if (CONF_MATCH_VALUE("high")) {
1661: 						if (!config_high_res_timer) {
1662: 							CONF_ERROR(
1663: 							    "No high resolution"
1664: 							    " timer support",
1665: 							    k, klen, v, vlen);
1666: 						} else {
1667: 							opt_prof_time_res =
1668: 							    prof_time_res_high;
1669: 						}
1670: 					} else {
1671: 						CONF_ERROR("Invalid conf value",
1672: 						    k, klen, v, vlen);
1673: 					}
1674: 					CONF_CONTINUE;
1675: 				}
1676: 				/*
1677: 				 * Undocumented.  When set to false, don't
1678: 				 * correct for an unbiasing bug in jeprof
1679: 				 * attribution.  This can be handy if you want
1680: 				 * to get consistent numbers from your binary
1681: 				 * across different jemalloc versions, even if
1682: 				 * those numbers are incorrect.  The default is
1683: 				 * true.
1684: 				 */
1685: 				CONF_HANDLE_BOOL(opt_prof_unbias, "prof_unbias")
1686: 			}
1687: 			if (config_log) {
1688: 				if (CONF_MATCH("log")) {
1689: 					size_t cpylen = (
1690: 					    vlen <= sizeof(log_var_names) ?
1691: 					    vlen : sizeof(log_var_names) - 1);
1692: 					strncpy(log_var_names, v, cpylen);
1693: 					log_var_names[cpylen] = '\0';
1694: 					CONF_CONTINUE;
1695: 				}
1696: 			}
1697: 			if (CONF_MATCH("thp")) {
1698: 				bool match = false;
1699: 				for (int m = 0; m < thp_mode_names_limit; m++) {
1700: 					if (strncmp(thp_mode_names[m],v, vlen)
1701: 					    == 0) {
1702: 						if (!have_madvise_huge && !have_memcntl) {
1703: 							CONF_ERROR(
1704: 							    "No THP support",
1705: 							    k, klen, v, vlen);
1706: 						}
1707: 						opt_thp = m;
1708: 						match = true;
1709: 						break;
1710: 					}
1711: 				}
1712: 				if (!match) {
1713: 					CONF_ERROR("Invalid conf value",
1714: 					    k, klen, v, vlen);
1715: 				}
1716: 				CONF_CONTINUE;
1717: 			}
1718: 			if (CONF_MATCH("zero_realloc")) {
1719: 				if (CONF_MATCH_VALUE("alloc")) {
1720: 					opt_zero_realloc_action
1721: 					    = zero_realloc_action_alloc;
1722: 				} else if (CONF_MATCH_VALUE("free")) {
1723: 					opt_zero_realloc_action
1724: 					    = zero_realloc_action_free;
1725: 				} else if (CONF_MATCH_VALUE("abort")) {
1726: 					opt_zero_realloc_action
1727: 					    = zero_realloc_action_abort;
1728: 				} else {
1729: 					CONF_ERROR("Invalid conf value",
1730: 					    k, klen, v, vlen);
1731: 				}
1732: 				CONF_CONTINUE;
1733: 			}
1734: 			if (config_uaf_detection &&
1735: 			    CONF_MATCH("lg_san_uaf_align")) {
1736: 				ssize_t a;
1737: 				CONF_VALUE_READ(ssize_t, a)
1738: 				if (CONF_VALUE_READ_FAIL() || a < -1) {
1739: 					CONF_ERROR("Invalid conf value",
1740: 					    k, klen, v, vlen);
1741: 				}
1742: 				if (a == -1) {
1743: 					opt_lg_san_uaf_align = -1;
1744: 					CONF_CONTINUE;
1745: 				}
1746: 
1747: 				/* clip if necessary */
1748: 				ssize_t max_allowed = (sizeof(size_t) << 3) - 1;
1749: 				ssize_t min_allowed = LG_PAGE;
1750: 				if (a > max_allowed) {
1751: 					a = max_allowed;
1752: 				} else if (a < min_allowed) {
1753: 					a = min_allowed;
1754: 				}
1755: 
1756: 				opt_lg_san_uaf_align = a;
1757: 				CONF_CONTINUE;
1758: 			}
1759: 
1760: 			CONF_HANDLE_SIZE_T(opt_san_guard_small,
1761: 			    "san_guard_small", 0, SIZE_T_MAX,
1762: 			    CONF_DONT_CHECK_MIN, CONF_DONT_CHECK_MAX, false)
1763: 			CONF_HANDLE_SIZE_T(opt_san_guard_large,
1764: 			    "san_guard_large", 0, SIZE_T_MAX,
1765: 			    CONF_DONT_CHECK_MIN, CONF_DONT_CHECK_MAX, false)
1766: 
1767: 			CONF_ERROR("Invalid conf pair", k, klen, v, vlen);
1768: #undef CONF_ERROR
1769: #undef CONF_CONTINUE
1770: #undef CONF_MATCH
1771: #undef CONF_MATCH_VALUE
1772: #undef CONF_HANDLE_BOOL
1773: #undef CONF_DONT_CHECK_MIN
1774: #undef CONF_CHECK_MIN
1775: #undef CONF_DONT_CHECK_MAX
1776: #undef CONF_CHECK_MAX
1777: #undef CONF_HANDLE_T
1778: #undef CONF_HANDLE_T_U
1779: #undef CONF_HANDLE_T_SIGNED
1780: #undef CONF_HANDLE_UNSIGNED
1781: #undef CONF_HANDLE_SIZE_T
1782: #undef CONF_HANDLE_SSIZE_T
1783: #undef CONF_HANDLE_CHAR_P
1784:     /* Re-enable diagnostic "-Wtype-limits" */
1785:     JEMALLOC_DIAGNOSTIC_POP
1786: 		}
1787: 		validate_hpa_settings();
1788: 		if (opt_abort_conf && had_conf_error) {
1789: 			malloc_abort_invalid_conf();
1790: 		}
1791: 	}
1792: 	atomic_store_b(&log_init_done, true, ATOMIC_RELEASE);
1793: }
1794: 
1795: static bool
1796: malloc_conf_init_check_deps(void) {
1797: 	if (opt_prof_leak_error && !opt_prof_final) {
1798: 		malloc_printf("<jemalloc>: prof_leak_error is set w/o "
1799: 		    "prof_final.\n");
1800: 		return true;
1801: 	}
1802: 	/* To emphasize in the stats output that opt is disabled when !debug. */
1803: 	if (!config_debug) {
1804: 		opt_debug_double_free_max_scan = 0;
1805: 	}
1806: 
1807: 	return false;
1808: }
1809: 
1810: static void
1811: malloc_conf_init(sc_data_t *sc_data, unsigned bin_shard_sizes[SC_NBINS],
1812:     char readlink_buf[PATH_MAX + 1]) {
1813: 	const char *opts_cache[MALLOC_CONF_NSOURCES] = {NULL, NULL, NULL, NULL,
1814: 		NULL};
1815: 
1816: 	/* The first call only set the confirm_conf option and opts_cache */
1817: 	malloc_conf_init_helper(NULL, NULL, true, opts_cache, readlink_buf);
1818: 	malloc_conf_init_helper(sc_data, bin_shard_sizes, false, opts_cache,
1819: 	    NULL);
1820: 	if (malloc_conf_init_check_deps()) {
1821: 		/* check_deps does warning msg only; abort below if needed. */
1822: 		if (opt_abort_conf) {
1823: 			malloc_abort_invalid_conf();
1824: 		}
1825: 	}
1826: }
1827: 
1828: #undef MALLOC_CONF_NSOURCES
1829: 
1830: static bool
1831: malloc_init_hard_needed(void) {
1832: 	if (malloc_initialized() || (IS_INITIALIZER && malloc_init_state ==
1833: 	    malloc_init_recursible)) {
1834: 		/*
1835: 		 * Another thread initialized the allocator before this one
1836: 		 * acquired init_lock, or this thread is the initializing
1837: 		 * thread, and it is recursively allocating.
1838: 		 */
1839: 		return false;
1840: 	}
1841: #ifdef JEMALLOC_THREADED_INIT
1842: 	if (malloc_initializer != NO_INITIALIZER && !IS_INITIALIZER) {
1843: 		/* Busy-wait until the initializing thread completes. */
1844: 		spin_t spinner = SPIN_INITIALIZER;
1845: 		do {
1846: 			malloc_mutex_unlock(TSDN_NULL, &init_lock);
1847: 			spin_adaptive(&spinner);
1848: 			malloc_mutex_lock(TSDN_NULL, &init_lock);
1849: 		} while (!malloc_initialized());
1850: 		return false;
1851: 	}
1852: #endif
1853: 	return true;
1854: }
1855: 
1856: static bool
1857: malloc_init_hard_a0_locked(void) {
1858: 	malloc_initializer = INITIALIZER;
1859: 
1860: 	JEMALLOC_DIAGNOSTIC_PUSH
1861: 	JEMALLOC_DIAGNOSTIC_IGNORE_MISSING_STRUCT_FIELD_INITIALIZERS
1862: 	sc_data_t sc_data = {0};
1863: 	JEMALLOC_DIAGNOSTIC_POP
1864: 
1865: 	/*
1866: 	 * Ordering here is somewhat tricky; we need sc_boot() first, since that
1867: 	 * determines what the size classes will be, and then
1868: 	 * malloc_conf_init(), since any slab size tweaking will need to be done
1869: 	 * before sz_boot and bin_info_boot, which assume that the values they
1870: 	 * read out of sc_data_global are final.
1871: 	 */
1872: 	sc_boot(&sc_data);
1873: 	unsigned bin_shard_sizes[SC_NBINS];
1874: 	bin_shard_sizes_boot(bin_shard_sizes);
1875: 	/*
1876: 	 * prof_boot0 only initializes opt_prof_prefix.  We need to do it before
1877: 	 * we parse malloc_conf options, in case malloc_conf parsing overwrites
1878: 	 * it.
1879: 	 */
1880: 	if (config_prof) {
1881: 		prof_boot0();
1882: 	}
1883: 	char readlink_buf[PATH_MAX + 1];
1884: 	readlink_buf[0] = '\0';
1885: 	malloc_conf_init(&sc_data, bin_shard_sizes, readlink_buf);
1886: 	san_init(opt_lg_san_uaf_align);
1887: 	sz_boot(&sc_data, opt_cache_oblivious);
1888: 	bin_info_boot(&sc_data, bin_shard_sizes);
1889: 
1890: 	if (opt_stats_print) {
1891: 		/* Print statistics at exit. */
1892: 		if (atexit(stats_print_atexit) != 0) {
1893: 			malloc_write("<jemalloc>: Error in atexit()\n");
1894: 			if (opt_abort) {
1895: 				abort();
1896: 			}
1897: 		}
1898: 	}
1899: 
1900: 	if (stats_boot()) {
1901: 		return true;
1902: 	}
1903: 	if (pages_boot()) {
1904: 		return true;
1905: 	}
1906: 	if (base_boot(TSDN_NULL)) {
1907: 		return true;
1908: 	}
1909: 	/* emap_global is static, hence zeroed. */
1910: 	if (emap_init(&arena_emap_global, b0get(), /* zeroed */ true)) {
1911: 		return true;
1912: 	}
1913: 	if (extent_boot()) {
1914: 		return true;
1915: 	}
1916: 	if (ctl_boot()) {
1917: 		return true;
1918: 	}
1919: 	if (config_prof) {
1920: 		prof_boot1();
1921: 	}
1922: 	if (opt_hpa && !hpa_supported()) {
1923: 		malloc_printf("<jemalloc>: HPA not supported in the current "
1924: 		    "configuration; %s.",
1925: 		    opt_abort_conf ? "aborting" : "disabling");
1926: 		if (opt_abort_conf) {
1927: 			malloc_abort_invalid_conf();
1928: 		} else {
1929: 			opt_hpa = false;
1930: 		}
1931: 	}
1932: 	if (arena_boot(&sc_data, b0get(), opt_hpa)) {
1933: 		return true;
1934: 	}
1935: 	if (tcache_boot(TSDN_NULL, b0get())) {
1936: 		return true;
1937: 	}
1938: 	if (malloc_mutex_init(&arenas_lock, "arenas", WITNESS_RANK_ARENAS,
1939: 	    malloc_mutex_rank_exclusive)) {
1940: 		return true;
1941: 	}
1942: 	hook_boot();
1943: 	/*
1944: 	 * Create enough scaffolding to allow recursive allocation in
1945: 	 * malloc_ncpus().
1946: 	 */
1947: 	narenas_auto = 1;
1948: 	manual_arena_base = narenas_auto + 1;
1949: 	memset(arenas, 0, sizeof(arena_t *) * narenas_auto);
1950: 	/*
1951: 	 * Initialize one arena here.  The rest are lazily created in
1952: 	 * arena_choose_hard().
1953: 	 */
1954: 	if (arena_init(TSDN_NULL, 0, &arena_config_default) == NULL) {
1955: 		return true;
1956: 	}
1957: 	a0 = arena_get(TSDN_NULL, 0, false);
1958: 
1959: 	if (opt_hpa && !hpa_supported()) {
1960: 		malloc_printf("<jemalloc>: HPA not supported in the current "
1961: 		    "configuration; %s.",
1962: 		    opt_abort_conf ? "aborting" : "disabling");
1963: 		if (opt_abort_conf) {
1964: 			malloc_abort_invalid_conf();
1965: 		} else {
1966: 			opt_hpa = false;
1967: 		}
1968: 	} else if (opt_hpa) {
1969: 		hpa_shard_opts_t hpa_shard_opts = opt_hpa_opts;
1970: 		hpa_shard_opts.deferral_allowed = background_thread_enabled();
1971: 		if (pa_shard_enable_hpa(TSDN_NULL, &a0->pa_shard,
1972: 		    &hpa_shard_opts, &opt_hpa_sec_opts)) {
1973: 			return true;
1974: 		}
1975: 	}
1976: 
1977: 	malloc_init_state = malloc_init_a0_initialized;
1978: 
1979: 	size_t buf_len = strlen(readlink_buf);
1980: 	if (buf_len > 0) {
1981: 		void *readlink_allocated = a0ialloc(buf_len + 1, false, true);
1982: 		if (readlink_allocated != NULL) {
1983: 			memcpy(readlink_allocated, readlink_buf, buf_len + 1);
1984: 			opt_malloc_conf_symlink = readlink_allocated;
1985: 		}
1986: 	}
1987: 
1988: 	return false;
1989: }
1990: 
1991: static bool
1992: malloc_init_hard_a0(void) {
1993: 	bool ret;
1994: 
1995: 	malloc_mutex_lock(TSDN_NULL, &init_lock);
1996: 	ret = malloc_init_hard_a0_locked();
1997: 	malloc_mutex_unlock(TSDN_NULL, &init_lock);
1998: 	return ret;
1999: }
2000: 
2001: /* Initialize data structures which may trigger recursive allocation. */
2002: static bool
2003: malloc_init_hard_recursible(void) {
2004: 	malloc_init_state = malloc_init_recursible;
2005: 
2006: 	ncpus = malloc_ncpus();
2007: 	if (opt_percpu_arena != percpu_arena_disabled) {
2008: 		bool cpu_count_is_deterministic =
2009: 		    malloc_cpu_count_is_deterministic();
2010: 		if (!cpu_count_is_deterministic) {
2011: 			/*
2012: 			 * If # of CPU is not deterministic, and narenas not
2013: 			 * specified, disables per cpu arena since it may not
2014: 			 * detect CPU IDs properly.
2015: 			 */
2016: 			if (opt_narenas == 0) {
2017: 				opt_percpu_arena = percpu_arena_disabled;
2018: 				malloc_write("<jemalloc>: Number of CPUs "
2019: 				    "detected is not deterministic. Per-CPU "
2020: 				    "arena disabled.\n");
2021: 				if (opt_abort_conf) {
2022: 					malloc_abort_invalid_conf();
2023: 				}
2024: 				if (opt_abort) {
2025: 					abort();
2026: 				}
2027: 			}
2028: 		}
2029: 	}
2030: 
2031: #if (defined(JEMALLOC_HAVE_PTHREAD_ATFORK) && !defined(JEMALLOC_MUTEX_INIT_CB) \
2032:     && !defined(JEMALLOC_ZONE) && !defined(_WIN32) && \
2033:     !defined(__native_client__))
2034: 	/* LinuxThreads' pthread_atfork() allocates. */
2035: 	if (pthread_atfork(jemalloc_prefork, jemalloc_postfork_parent,
2036: 	    jemalloc_postfork_child) != 0) {
2037: 		malloc_write("<jemalloc>: Error in pthread_atfork()\n");
2038: 		if (opt_abort) {
2039: 			abort();
2040: 		}
2041: 		return true;
2042: 	}
2043: #endif
2044: 
2045: 	if (background_thread_boot0()) {
2046: 		return true;
2047: 	}
2048: 
2049: 	return false;
2050: }
2051: 
2052: static unsigned
2053: malloc_narenas_default(void) {
2054: 	assert(ncpus > 0);
2055: 	/*
2056: 	 * For SMP systems, create more than one arena per CPU by
2057: 	 * default.
2058: 	 */
2059: 	if (ncpus > 1) {
2060: 		fxp_t fxp_ncpus = FXP_INIT_INT(ncpus);
2061: 		fxp_t goal = fxp_mul(fxp_ncpus, opt_narenas_ratio);
2062: 		uint32_t int_goal = fxp_round_nearest(goal);
2063: 		if (int_goal == 0) {
2064: 			return 1;
2065: 		}
2066: 		return int_goal;
2067: 	} else {
2068: 		return 1;
2069: 	}
2070: }
2071: 
2072: static percpu_arena_mode_t
2073: percpu_arena_as_initialized(percpu_arena_mode_t mode) {
2074: 	assert(!malloc_initialized());
2075: 	assert(mode <= percpu_arena_disabled);
2076: 
2077: 	if (mode != percpu_arena_disabled) {
2078: 		mode += percpu_arena_mode_enabled_base;
2079: 	}
2080: 
2081: 	return mode;
2082: }
2083: 
2084: static bool
2085: malloc_init_narenas(void) {
2086: 	assert(ncpus > 0);
2087: 
2088: 	if (opt_percpu_arena != percpu_arena_disabled) {
2089: 		if (!have_percpu_arena || malloc_getcpu() < 0) {
2090: 			opt_percpu_arena = percpu_arena_disabled;
2091: 			malloc_printf("<jemalloc>: perCPU arena getcpu() not "
2092: 			    "available. Setting narenas to %u.\n", opt_narenas ?
2093: 			    opt_narenas : malloc_narenas_default());
2094: 			if (opt_abort) {
2095: 				abort();
2096: 			}
2097: 		} else {
2098: 			if (ncpus >= MALLOCX_ARENA_LIMIT) {
2099: 				malloc_printf("<jemalloc>: narenas w/ percpu"
2100: 				    "arena beyond limit (%d)\n", ncpus);
2101: 				if (opt_abort) {
2102: 					abort();
2103: 				}
2104: 				return true;
2105: 			}
2106: 			/* NB: opt_percpu_arena isn't fully initialized yet. */
2107: 			if (percpu_arena_as_initialized(opt_percpu_arena) ==
2108: 			    per_phycpu_arena && ncpus % 2 != 0) {
2109: 				malloc_printf("<jemalloc>: invalid "
2110: 				    "configuration -- per physical CPU arena "
2111: 				    "with odd number (%u) of CPUs (no hyper "
2112: 				    "threading?).\n", ncpus);
2113: 				if (opt_abort)
2114: 					abort();
2115: 			}
2116: 			unsigned n = percpu_arena_ind_limit(
2117: 			    percpu_arena_as_initialized(opt_percpu_arena));
2118: 			if (opt_narenas < n) {
2119: 				/*
2120: 				 * If narenas is specified with percpu_arena
2121: 				 * enabled, actual narenas is set as the greater
2122: 				 * of the two. percpu_arena_choose will be free
2123: 				 * to use any of the arenas based on CPU
2124: 				 * id. This is conservative (at a small cost)
2125: 				 * but ensures correctness.
2126: 				 *
2127: 				 * If for some reason the ncpus determined at
2128: 				 * boot is not the actual number (e.g. because
2129: 				 * of affinity setting from numactl), reserving
2130: 				 * narenas this way provides a workaround for
2131: 				 * percpu_arena.
2132: 				 */
2133: 				opt_narenas = n;
2134: 			}
2135: 		}
2136: 	}
2137: 	if (opt_narenas == 0) {
2138: 		opt_narenas = malloc_narenas_default();
2139: 	}
2140: 	assert(opt_narenas > 0);
2141: 
2142: 	narenas_auto = opt_narenas;
2143: 	/*
2144: 	 * Limit the number of arenas to the indexing range of MALLOCX_ARENA().
2145: 	 */
2146: 	if (narenas_auto >= MALLOCX_ARENA_LIMIT) {
2147: 		narenas_auto = MALLOCX_ARENA_LIMIT - 1;
2148: 		malloc_printf("<jemalloc>: Reducing narenas to limit (%d)\n",
2149: 		    narenas_auto);
2150: 	}
2151: 	narenas_total_set(narenas_auto);
2152: 	if (arena_init_huge(a0)) {
2153: 		narenas_total_inc();
2154: 	}
2155: 	manual_arena_base = narenas_total_get();
2156: 
2157: 	return false;
2158: }
2159: 
2160: static void
2161: malloc_init_percpu(void) {
2162: 	opt_percpu_arena = percpu_arena_as_initialized(opt_percpu_arena);
2163: }
2164: 
2165: static bool
2166: malloc_init_hard_finish(void) {
2167: 	if (malloc_mutex_boot()) {
2168: 		return true;
2169: 	}
2170: 
2171: 	malloc_init_state = malloc_init_initialized;
2172: 	malloc_slow_flag_init();
2173: 
2174: 	return false;
2175: }
2176: 
2177: static void
2178: malloc_init_hard_cleanup(tsdn_t *tsdn, bool reentrancy_set) {
2179: 	malloc_mutex_assert_owner(tsdn, &init_lock);
2180: 	malloc_mutex_unlock(tsdn, &init_lock);
2181: 	if (reentrancy_set) {
2182: 		assert(!tsdn_null(tsdn));
2183: 		tsd_t *tsd = tsdn_tsd(tsdn);
2184: 		assert(tsd_reentrancy_level_get(tsd) > 0);
2185: 		post_reentrancy(tsd);
2186: 	}
2187: }
2188: 
2189: static bool
2190: malloc_init_hard(void) {
2191: 	tsd_t *tsd;
2192: 
2193: #if defined(_WIN32) && _WIN32_WINNT < 0x0600
2194: 	_init_init_lock();
2195: #endif
2196: 	malloc_mutex_lock(TSDN_NULL, &init_lock);
2197: 
2198: #define UNLOCK_RETURN(tsdn, ret, reentrancy)		\
2199: 	malloc_init_hard_cleanup(tsdn, reentrancy);	\
2200: 	return ret;
2201: 
2202: 	if (!malloc_init_hard_needed()) {
2203: 		UNLOCK_RETURN(TSDN_NULL, false, false)
2204: 	}
2205: 
2206: 	if (malloc_init_state != malloc_init_a0_initialized &&
2207: 	    malloc_init_hard_a0_locked()) {
2208: 		UNLOCK_RETURN(TSDN_NULL, true, false)
2209: 	}
2210: 
2211: 	malloc_mutex_unlock(TSDN_NULL, &init_lock);
2212: 	/* Recursive allocation relies on functional tsd. */
2213: 	tsd = malloc_tsd_boot0();
2214: 	if (tsd == NULL) {
2215: 		return true;
2216: 	}
2217: 	if (malloc_init_hard_recursible()) {
2218: 		return true;
2219: 	}
2220: 
2221: 	malloc_mutex_lock(tsd_tsdn(tsd), &init_lock);
2222: 	/* Set reentrancy level to 1 during init. */
2223: 	pre_reentrancy(tsd, NULL);
2224: 	/* Initialize narenas before prof_boot2 (for allocation). */
2225: 	if (malloc_init_narenas()
2226: 	    || background_thread_boot1(tsd_tsdn(tsd), b0get())) {
2227: 		UNLOCK_RETURN(tsd_tsdn(tsd), true, true)
2228: 	}
2229: 	if (config_prof && prof_boot2(tsd, b0get())) {
2230: 		UNLOCK_RETURN(tsd_tsdn(tsd), true, true)
2231: 	}
2232: 
2233: 	malloc_init_percpu();
2234: 
2235: 	if (malloc_init_hard_finish()) {
2236: 		UNLOCK_RETURN(tsd_tsdn(tsd), true, true)
2237: 	}
2238: 	post_reentrancy(tsd);
2239: 	malloc_mutex_unlock(tsd_tsdn(tsd), &init_lock);
2240: 
2241: 	witness_assert_lockless(witness_tsd_tsdn(
2242: 	    tsd_witness_tsdp_get_unsafe(tsd)));
2243: 	malloc_tsd_boot1();
2244: 	/* Update TSD after tsd_boot1. */
2245: 	tsd = tsd_fetch();
2246: 	if (opt_background_thread) {
2247: 		assert(have_background_thread);
2248: 		/*
2249: 		 * Need to finish init & unlock first before creating background
2250: 		 * threads (pthread_create depends on malloc).  ctl_init (which
2251: 		 * sets isthreaded) needs to be called without holding any lock.
2252: 		 */
2253: 		background_thread_ctl_init(tsd_tsdn(tsd));
2254: 		if (background_thread_create(tsd, 0)) {
2255: 			return true;
2256: 		}
2257: 	}
2258: #undef UNLOCK_RETURN
2259: 	return false;
2260: }
2261: 
2262: /*
2263:  * End initialization functions.
2264:  */
2265: /******************************************************************************/
2266: /*
2267:  * Begin allocation-path internal functions and data structures.
2268:  */
2269: 
2270: /*
2271:  * Settings determined by the documented behavior of the allocation functions.
2272:  */
2273: typedef struct static_opts_s static_opts_t;
2274: struct static_opts_s {
2275: 	/* Whether or not allocation size may overflow. */
2276: 	bool may_overflow;
2277: 
2278: 	/*
2279: 	 * Whether or not allocations (with alignment) of size 0 should be
2280: 	 * treated as size 1.
2281: 	 */
2282: 	bool bump_empty_aligned_alloc;
2283: 	/*
2284: 	 * Whether to assert that allocations are not of size 0 (after any
2285: 	 * bumping).
2286: 	 */
2287: 	bool assert_nonempty_alloc;
2288: 
2289: 	/*
2290: 	 * Whether or not to modify the 'result' argument to malloc in case of
2291: 	 * error.
2292: 	 */
2293: 	bool null_out_result_on_error;
2294: 	/* Whether to set errno when we encounter an error condition. */
2295: 	bool set_errno_on_error;
2296: 
2297: 	/*
2298: 	 * The minimum valid alignment for functions requesting aligned storage.
2299: 	 */
2300: 	size_t min_alignment;
2301: 
2302: 	/* The error string to use if we oom. */
2303: 	const char *oom_string;
2304: 	/* The error string to use if the passed-in alignment is invalid. */
2305: 	const char *invalid_alignment_string;
2306: 
2307: 	/*
2308: 	 * False if we're configured to skip some time-consuming operations.
2309: 	 *
2310: 	 * This isn't really a malloc "behavior", but it acts as a useful
2311: 	 * summary of several other static (or at least, static after program
2312: 	 * initialization) options.
2313: 	 */
2314: 	bool slow;
2315: 	/*
2316: 	 * Return size.
2317: 	 */
2318: 	bool usize;
2319: };
2320: 
2321: JEMALLOC_ALWAYS_INLINE void
2322: static_opts_init(static_opts_t *static_opts) {
2323: 	static_opts->may_overflow = false;
2324: 	static_opts->bump_empty_aligned_alloc = false;
2325: 	static_opts->assert_nonempty_alloc = false;
2326: 	static_opts->null_out_result_on_error = false;
2327: 	static_opts->set_errno_on_error = false;
2328: 	static_opts->min_alignment = 0;
2329: 	static_opts->oom_string = "";
2330: 	static_opts->invalid_alignment_string = "";
2331: 	static_opts->slow = false;
2332: 	static_opts->usize = false;
2333: }
2334: 
2335: typedef struct dynamic_opts_s dynamic_opts_t;
2336: struct dynamic_opts_s {
2337: 	void **result;
2338: 	size_t usize;
2339: 	size_t num_items;
2340: 	size_t item_size;
2341: 	size_t alignment;
2342: 	bool zero;
2343: 	unsigned tcache_ind;
2344: 	unsigned arena_ind;
2345: };
2346: 
2347: JEMALLOC_ALWAYS_INLINE void
2348: dynamic_opts_init(dynamic_opts_t *dynamic_opts) {
2349: 	dynamic_opts->result = NULL;
2350: 	dynamic_opts->usize = 0;
2351: 	dynamic_opts->num_items = 0;
2352: 	dynamic_opts->item_size = 0;
2353: 	dynamic_opts->alignment = 0;
2354: 	dynamic_opts->zero = false;
2355: 	dynamic_opts->tcache_ind = TCACHE_IND_AUTOMATIC;
2356: 	dynamic_opts->arena_ind = ARENA_IND_AUTOMATIC;
2357: }
2358: 
2359: /*
2360:  * ind parameter is optional and is only checked and filled if alignment == 0;
2361:  * return true if result is out of range.
2362:  */
2363: JEMALLOC_ALWAYS_INLINE bool
2364: aligned_usize_get(size_t size, size_t alignment, size_t *usize, szind_t *ind,
2365:     bool bump_empty_aligned_alloc) {
2366: 	assert(usize != NULL);
2367: 	if (alignment == 0) {
2368: 		if (ind != NULL) {
2369: 			*ind = sz_size2index(size);
2370: 			if (unlikely(*ind >= SC_NSIZES)) {
2371: 				return true;
2372: 			}
2373: 			*usize = sz_index2size(*ind);
2374: 			assert(*usize > 0 && *usize <= SC_LARGE_MAXCLASS);
2375: 			return false;
2376: 		}
2377: 		*usize = sz_s2u(size);
2378: 	} else {
2379: 		if (bump_empty_aligned_alloc && unlikely(size == 0)) {
2380: 			size = 1;
2381: 		}
2382: 		*usize = sz_sa2u(size, alignment);
2383: 	}
2384: 	if (unlikely(*usize == 0 || *usize > SC_LARGE_MAXCLASS)) {
2385: 		return true;
2386: 	}
2387: 	return false;
2388: }
2389: 
2390: JEMALLOC_ALWAYS_INLINE bool
2391: zero_get(bool guarantee, bool slow) {
2392: 	if (config_fill && slow && unlikely(opt_zero)) {
2393: 		return true;
2394: 	} else {
2395: 		return guarantee;
2396: 	}
2397: }
2398: 
2399: /* Return true if a manual arena is specified and arena_get() OOMs. */
2400: JEMALLOC_ALWAYS_INLINE bool
2401: arena_get_from_ind(tsd_t *tsd, unsigned arena_ind, arena_t **arena_p) {
2402: 	if (arena_ind == ARENA_IND_AUTOMATIC) {
2403: 		/*
2404: 		 * In case of automatic arena management, we defer arena
2405: 		 * computation until as late as we can, hoping to fill the
2406: 		 * allocation out of the tcache.
2407: 		 */
2408: 		*arena_p = NULL;
2409: 	} else {
2410: 		*arena_p = arena_get(tsd_tsdn(tsd), arena_ind, true);
2411: 		if (unlikely(*arena_p == NULL) && arena_ind >= narenas_auto) {
2412: 			return true;
2413: 		}
2414: 	}
2415: 	return false;
2416: }
2417: 
2418: /* ind is ignored if dopts->alignment > 0. */
2419: JEMALLOC_ALWAYS_INLINE void *
2420: imalloc_no_sample(static_opts_t *sopts, dynamic_opts_t *dopts, tsd_t *tsd,
2421:     size_t size, size_t usize, szind_t ind, bool slab) {
2422: 	/* Fill in the tcache. */
2423: 	tcache_t *tcache = tcache_get_from_ind(tsd, dopts->tcache_ind,
2424: 	    sopts->slow, /* is_alloc */ true);
2425: 
2426: 	/* Fill in the arena. */
2427: 	arena_t *arena;
2428: 	if (arena_get_from_ind(tsd, dopts->arena_ind, &arena)) {
2429: 		return NULL;
2430: 	}
2431: 
2432: 	if (unlikely(dopts->alignment != 0)) {
2433: 		return ipalloct_explicit_slab(tsd_tsdn(tsd), usize,
2434: 		    dopts->alignment, dopts->zero, slab, tcache, arena);
2435: 	}
2436: 
2437: 	return iallocztm_explicit_slab(tsd_tsdn(tsd), size, ind, dopts->zero,
2438: 	    slab, tcache, false, arena, sopts->slow);
2439: }
2440: 
2441: JEMALLOC_ALWAYS_INLINE void *
2442: imalloc_sample(static_opts_t *sopts, dynamic_opts_t *dopts, tsd_t *tsd,
2443:     size_t usize, szind_t ind) {
2444: 	void *ret;
2445: 
2446: 	dopts->alignment = prof_sample_align(usize, dopts->alignment);
2447: 	/*
2448: 	 * If the allocation is small enough that it would normally be allocated
2449: 	 * on a slab, we need to take additional steps to ensure that it gets
2450: 	 * its own extent instead.
2451: 	 */
2452: 	if (sz_can_use_slab(usize)) {
2453: 		assert((dopts->alignment & PROF_SAMPLE_ALIGNMENT_MASK) == 0);
2454: 		size_t bumped_usize = sz_sa2u(usize, dopts->alignment);
2455: 		szind_t bumped_ind = sz_size2index(bumped_usize);
2456: 		dopts->tcache_ind = TCACHE_IND_NONE;
2457: 		ret = imalloc_no_sample(sopts, dopts, tsd, bumped_usize,
2458: 		    bumped_usize, bumped_ind, /* slab */ false);
2459: 		if (unlikely(ret == NULL)) {
2460: 			return NULL;
2461: 		}
2462: 		arena_prof_promote(tsd_tsdn(tsd), ret, usize, bumped_usize);
2463: 	} else {
2464: 		ret = imalloc_no_sample(sopts, dopts, tsd, usize, usize, ind,
2465: 		    /* slab */ false);
2466: 	}
2467: 	assert(prof_sample_aligned(ret));
2468: 
2469: 	return ret;
2470: }
2471: 
2472: /*
2473:  * Returns true if the allocation will overflow, and false otherwise.  Sets
2474:  * *size to the product either way.
2475:  */
2476: JEMALLOC_ALWAYS_INLINE bool
2477: compute_size_with_overflow(bool may_overflow, dynamic_opts_t *dopts,
2478:     size_t *size) {
2479: 	/*
2480: 	 * This function is just num_items * item_size, except that we may have
2481: 	 * to check for overflow.
2482: 	 */
2483: 
2484: 	if (!may_overflow) {
2485: 		assert(dopts->num_items == 1);
2486: 		*size = dopts->item_size;
2487: 		return false;
2488: 	}
2489: 
2490: 	/* A size_t with its high-half bits all set to 1. */
2491: 	static const size_t high_bits = SIZE_T_MAX << (sizeof(size_t) * 8 / 2);
2492: 
2493: 	*size = dopts->item_size * dopts->num_items;
2494: 
2495: 	if (unlikely(*size == 0)) {
2496: 		return (dopts->num_items != 0 && dopts->item_size != 0);
2497: 	}
2498: 
2499: 	/*
2500: 	 * We got a non-zero size, but we don't know if we overflowed to get
2501: 	 * there.  To avoid having to do a divide, we'll be clever and note that
2502: 	 * if both A and B can be represented in N/2 bits, then their product
2503: 	 * can be represented in N bits (without the possibility of overflow).
2504: 	 */
2505: 	if (likely((high_bits & (dopts->num_items | dopts->item_size)) == 0)) {
2506: 		return false;
2507: 	}
2508: 	if (likely(*size / dopts->item_size == dopts->num_items)) {
2509: 		return false;
2510: 	}
2511: 	return true;
2512: }
2513: 
2514: JEMALLOC_ALWAYS_INLINE int
2515: imalloc_body(static_opts_t *sopts, dynamic_opts_t *dopts, tsd_t *tsd) {
2516: 	/* Where the actual allocated memory will live. */
2517: 	void *allocation = NULL;
2518: 	/* Filled in by compute_size_with_overflow below. */
2519: 	size_t size = 0;
2520: 	/*
2521: 	 * The zero initialization for ind is actually dead store, in that its
2522: 	 * value is reset before any branch on its value is taken.  Sometimes
2523: 	 * though, it's convenient to pass it as arguments before this point.
2524: 	 * To avoid undefined behavior then, we initialize it with dummy stores.
2525: 	 */
2526: 	szind_t ind = 0;
2527: 	/* usize will always be properly initialized. */
2528: 	size_t usize;
2529: 
2530: 	/* Reentrancy is only checked on slow path. */
2531: 	int8_t reentrancy_level;
2532: 
2533: 	/* Compute the amount of memory the user wants. */
2534: 	if (unlikely(compute_size_with_overflow(sopts->may_overflow, dopts,
2535: 	    &size))) {
2536: 		goto label_oom;
2537: 	}
2538: 
2539: 	if (unlikely(dopts->alignment < sopts->min_alignment
2540: 	    || (dopts->alignment & (dopts->alignment - 1)) != 0)) {
2541: 		goto label_invalid_alignment;
2542: 	}
2543: 
2544: 	/* This is the beginning of the "core" algorithm. */
2545: 	dopts->zero = zero_get(dopts->zero, sopts->slow);
2546: 	if (aligned_usize_get(size, dopts->alignment, &usize, &ind,
2547: 	    sopts->bump_empty_aligned_alloc)) {
2548: 		goto label_oom;
2549: 	}
2550: 	dopts->usize = usize;
2551: 	/* Validate the user input. */
2552: 	if (sopts->assert_nonempty_alloc) {
2553: 		assert (size != 0);
2554: 	}
2555: 
2556: 	check_entry_exit_locking(tsd_tsdn(tsd));
2557: 
2558: 	/*
2559: 	 * If we need to handle reentrancy, we can do it out of a
2560: 	 * known-initialized arena (i.e. arena 0).
2561: 	 */
2562: 	reentrancy_level = tsd_reentrancy_level_get(tsd);
2563: 	if (sopts->slow && unlikely(reentrancy_level > 0)) {
2564: 		/*
2565: 		 * We should never specify particular arenas or tcaches from
2566: 		 * within our internal allocations.
2567: 		 */
2568: 		assert(dopts->tcache_ind == TCACHE_IND_AUTOMATIC ||
2569: 		    dopts->tcache_ind == TCACHE_IND_NONE);
2570: 		assert(dopts->arena_ind == ARENA_IND_AUTOMATIC);
2571: 		dopts->tcache_ind = TCACHE_IND_NONE;
2572: 		/* We know that arena 0 has already been initialized. */
2573: 		dopts->arena_ind = 0;
2574: 	}
2575: 
2576: 	/*
2577: 	 * If dopts->alignment > 0, then ind is still 0, but usize was computed
2578: 	 * in the previous if statement.  Down the positive alignment path,
2579: 	 * imalloc_no_sample and imalloc_sample will ignore ind.
2580: 	 */
2581: 
2582: 	/* If profiling is on, get our profiling context. */
2583: 	if (config_prof && opt_prof) {
2584: 		bool prof_active = prof_active_get_unlocked();
2585: 		bool sample_event = te_prof_sample_event_lookahead(tsd, usize);
2586: 		prof_tctx_t *tctx = prof_alloc_prep(tsd, prof_active,
2587: 		    sample_event);
2588: 
2589: 		emap_alloc_ctx_t alloc_ctx;
2590: 		if (likely(tctx == PROF_TCTX_SENTINEL)) {
2591: 			alloc_ctx.slab = sz_can_use_slab(usize);
2592: 			allocation = imalloc_no_sample(
2593: 			    sopts, dopts, tsd, usize, usize, ind,
2594: 			    alloc_ctx.slab);
2595: 		} else if (tctx != NULL) {
2596: 			allocation = imalloc_sample(
2597: 			    sopts, dopts, tsd, usize, ind);
2598: 			alloc_ctx.slab = false;
2599: 		} else {
2600: 			allocation = NULL;
2601: 		}
2602: 
2603: 		if (unlikely(allocation == NULL)) {
2604: 			prof_alloc_rollback(tsd, tctx);
2605: 			goto label_oom;
2606: 		}
2607: 		prof_malloc(tsd, allocation, size, usize, &alloc_ctx, tctx);
2608: 	} else {
2609: 		assert(!opt_prof);
2610: 		allocation = imalloc_no_sample(sopts, dopts, tsd, size, usize,
2611: 		    ind, sz_can_use_slab(usize));
2612: 		if (unlikely(allocation == NULL)) {
2613: 			goto label_oom;
2614: 		}
2615: 	}
2616: 
2617: 	/*
2618: 	 * Allocation has been done at this point.  We still have some
2619: 	 * post-allocation work to do though.
2620: 	 */
2621: 
2622: 	thread_alloc_event(tsd, usize);
2623: 
2624: 	assert(dopts->alignment == 0
2625: 	    || ((uintptr_t)allocation & (dopts->alignment - 1)) == ZU(0));
2626: 
2627: 	assert(usize == isalloc(tsd_tsdn(tsd), allocation));
2628: 
2629: 	if (config_fill && sopts->slow && !dopts->zero
2630: 	    && unlikely(opt_junk_alloc)) {
2631: 		junk_alloc_callback(allocation, usize);
2632: 	}
2633: 
2634: 	if (sopts->slow) {
2635: 		UTRACE(0, size, allocation);
2636: 	}
2637: 
2638: 	/* Success! */
2639: 	check_entry_exit_locking(tsd_tsdn(tsd));
2640: 	*dopts->result = allocation;
2641: 	return 0;
2642: 
2643: label_oom:
2644: 	if (unlikely(sopts->slow) && config_xmalloc && unlikely(opt_xmalloc)) {
2645: 		malloc_write(sopts->oom_string);
2646: 		abort();
2647: 	}
2648: 
2649: 	if (sopts->slow) {
2650: 		UTRACE(NULL, size, NULL);
2651: 	}
2652: 
2653: 	check_entry_exit_locking(tsd_tsdn(tsd));
2654: 
2655: 	if (sopts->set_errno_on_error) {
2656: 		set_errno(ENOMEM);
2657: 	}
2658: 
2659: 	if (sopts->null_out_result_on_error) {
2660: 		*dopts->result = NULL;
2661: 	}
2662: 
2663: 	return ENOMEM;
2664: 
2665: 	/*
2666: 	 * This label is only jumped to by one goto; we move it out of line
2667: 	 * anyways to avoid obscuring the non-error paths, and for symmetry with
2668: 	 * the oom case.
2669: 	 */
2670: label_invalid_alignment:
2671: 	if (config_xmalloc && unlikely(opt_xmalloc)) {
2672: 		malloc_write(sopts->invalid_alignment_string);
2673: 		abort();
2674: 	}
2675: 
2676: 	if (sopts->set_errno_on_error) {
2677: 		set_errno(EINVAL);
2678: 	}
2679: 
2680: 	if (sopts->slow) {
2681: 		UTRACE(NULL, size, NULL);
2682: 	}
2683: 
2684: 	check_entry_exit_locking(tsd_tsdn(tsd));
2685: 
2686: 	if (sopts->null_out_result_on_error) {
2687: 		*dopts->result = NULL;
2688: 	}
2689: 
2690: 	return EINVAL;
2691: }
2692: 
2693: JEMALLOC_ALWAYS_INLINE bool
2694: imalloc_init_check(static_opts_t *sopts, dynamic_opts_t *dopts) {
2695: 	if (unlikely(!malloc_initialized()) && unlikely(malloc_init())) {
2696: 		if (config_xmalloc && unlikely(opt_xmalloc)) {
2697: 			malloc_write(sopts->oom_string);
2698: 			abort();
2699: 		}
2700: 		UTRACE(NULL, dopts->num_items * dopts->item_size, NULL);
2701: 		set_errno(ENOMEM);
2702: 		*dopts->result = NULL;
2703: 
2704: 		return false;
2705: 	}
2706: 
2707: 	return true;
2708: }
2709: 
2710: /* Returns the errno-style error code of the allocation. */
2711: JEMALLOC_ALWAYS_INLINE int
2712: imalloc(static_opts_t *sopts, dynamic_opts_t *dopts) {
2713: 	if (tsd_get_allocates() && !imalloc_init_check(sopts, dopts)) {
2714: 		return ENOMEM;
2715: 	}
2716: 
2717: 	/* We always need the tsd.  Let's grab it right away. */
2718: 	tsd_t *tsd = tsd_fetch();
2719: 	assert(tsd);
2720: 	if (likely(tsd_fast(tsd))) {
2721: 		/* Fast and common path. */
2722: 		tsd_assert_fast(tsd);
2723: 		sopts->slow = false;
2724: 		return imalloc_body(sopts, dopts, tsd);
2725: 	} else {
2726: 		if (!tsd_get_allocates() && !imalloc_init_check(sopts, dopts)) {
2727: 			return ENOMEM;
2728: 		}
2729: 
2730: 		sopts->slow = true;
2731: 		return imalloc_body(sopts, dopts, tsd);
2732: 	}
2733: }
2734: 
2735: JEMALLOC_NOINLINE
2736: void *
2737: malloc_default(size_t size) {
2738: 	void *ret;
2739: 	static_opts_t sopts;
2740: 	dynamic_opts_t dopts;
2741: 
2742: 	/*
2743: 	 * This variant has logging hook on exit but not on entry.  It's callled
2744: 	 * only by je_malloc, below, which emits the entry one for us (and, if
2745: 	 * it calls us, does so only via tail call).
2746: 	 */
2747: 
2748: 	static_opts_init(&sopts);
2749: 	dynamic_opts_init(&dopts);
2750: 
2751: 	sopts.null_out_result_on_error = true;
2752: 	sopts.set_errno_on_error = true;
2753: 	sopts.oom_string = "<jemalloc>: Error in malloc(): out of memory\n";
2754: 
2755: 	dopts.result = &ret;
2756: 	dopts.num_items = 1;
2757: 	dopts.item_size = size;
2758: 
2759: 	imalloc(&sopts, &dopts);
2760: 	/*
2761: 	 * Note that this branch gets optimized away -- it immediately follows
2762: 	 * the check on tsd_fast that sets sopts.slow.
2763: 	 */
2764: 	if (sopts.slow) {
2765: 		uintptr_t args[3] = {size};
2766: 		hook_invoke_alloc(hook_alloc_malloc, ret, (uintptr_t)ret, args);
2767: 	}
2768: 
2769: 	return ret;
2770: }
2771: 
2772: /******************************************************************************/
2773: /*
2774:  * Begin malloc(3)-compatible functions.
2775:  */
2776: 
2777: JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
2778: void JEMALLOC_NOTHROW *
2779: JEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE(1)
2780: je_malloc(size_t size) {
2781: 	LOG("core.malloc.entry", "size: %zu", size);
2782: 
2783: 	void * ret = imalloc_fastpath(size, &malloc_default);
2784: 
2785: 	LOG("core.malloc.exit", "result: %p", ret);
2786: 	return ret;
2787: }
2788: 
2789: JEMALLOC_EXPORT int JEMALLOC_NOTHROW
2790: JEMALLOC_ATTR(nonnull(1))
2791: je_posix_memalign(void **memptr, size_t alignment, size_t size) {
2792: 	int ret;
2793: 	static_opts_t sopts;
2794: 	dynamic_opts_t dopts;
2795: 
2796: 	LOG("core.posix_memalign.entry", "mem ptr: %p, alignment: %zu, "
2797: 	    "size: %zu", memptr, alignment, size);
2798: 
2799: 	static_opts_init(&sopts);
2800: 	dynamic_opts_init(&dopts);
2801: 
2802: 	sopts.bump_empty_aligned_alloc = true;
2803: 	sopts.min_alignment = sizeof(void *);
2804: 	sopts.oom_string =
2805: 	    "<jemalloc>: Error allocating aligned memory: out of memory\n";
2806: 	sopts.invalid_alignment_string =
2807: 	    "<jemalloc>: Error allocating aligned memory: invalid alignment\n";
2808: 
2809: 	dopts.result = memptr;
2810: 	dopts.num_items = 1;
2811: 	dopts.item_size = size;
2812: 	dopts.alignment = alignment;
2813: 
2814: 	ret = imalloc(&sopts, &dopts);
2815: 	if (sopts.slow) {
2816: 		uintptr_t args[3] = {(uintptr_t)memptr, (uintptr_t)alignment,
2817: 			(uintptr_t)size};
2818: 		hook_invoke_alloc(hook_alloc_posix_memalign, *memptr,
2819: 		    (uintptr_t)ret, args);
2820: 	}
2821: 
2822: 	LOG("core.posix_memalign.exit", "result: %d, alloc ptr: %p", ret,
2823: 	    *memptr);
2824: 
2825: 	return ret;
2826: }
2827: 
2828: JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
2829: void JEMALLOC_NOTHROW *
2830: JEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE(2)
2831: je_aligned_alloc(size_t alignment, size_t size) {
2832: 	void *ret;
2833: 
2834: 	static_opts_t sopts;
2835: 	dynamic_opts_t dopts;
2836: 
2837: 	LOG("core.aligned_alloc.entry", "alignment: %zu, size: %zu\n",
2838: 	    alignment, size);
2839: 
2840: 	static_opts_init(&sopts);
2841: 	dynamic_opts_init(&dopts);
2842: 
2843: 	sopts.bump_empty_aligned_alloc = true;
2844: 	sopts.null_out_result_on_error = true;
2845: 	sopts.set_errno_on_error = true;
2846: 	sopts.min_alignment = 1;
2847: 	sopts.oom_string =
2848: 	    "<jemalloc>: Error allocating aligned memory: out of memory\n";
2849: 	sopts.invalid_alignment_string =
2850: 	    "<jemalloc>: Error allocating aligned memory: invalid alignment\n";
2851: 
2852: 	dopts.result = &ret;
2853: 	dopts.num_items = 1;
2854: 	dopts.item_size = size;
2855: 	dopts.alignment = alignment;
2856: 
2857: 	imalloc(&sopts, &dopts);
2858: 	if (sopts.slow) {
2859: 		uintptr_t args[3] = {(uintptr_t)alignment, (uintptr_t)size};
2860: 		hook_invoke_alloc(hook_alloc_aligned_alloc, ret,
2861: 		    (uintptr_t)ret, args);
2862: 	}
2863: 
2864: 	LOG("core.aligned_alloc.exit", "result: %p", ret);
2865: 
2866: 	return ret;
2867: }
2868: 
2869: JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
2870: void JEMALLOC_NOTHROW *
2871: JEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE2(1, 2)
2872: je_calloc(size_t num, size_t size) {
2873: 	void *ret;
2874: 	static_opts_t sopts;
2875: 	dynamic_opts_t dopts;
2876: 
2877: 	LOG("core.calloc.entry", "num: %zu, size: %zu", num, size);
2878: 
2879: 	static_opts_init(&sopts);
2880: 	dynamic_opts_init(&dopts);
2881: 
2882: 	sopts.may_overflow = true;
2883: 	sopts.null_out_result_on_error = true;
2884: 	sopts.set_errno_on_error = true;
2885: 	sopts.oom_string = "<jemalloc>: Error in calloc(): out of memory\n";
2886: 
2887: 	dopts.result = &ret;
2888: 	dopts.num_items = num;
2889: 	dopts.item_size = size;
2890: 	dopts.zero = true;
2891: 
2892: 	imalloc(&sopts, &dopts);
2893: 	if (sopts.slow) {
2894: 		uintptr_t args[3] = {(uintptr_t)num, (uintptr_t)size};
2895: 		hook_invoke_alloc(hook_alloc_calloc, ret, (uintptr_t)ret, args);
2896: 	}
2897: 
2898: 	LOG("core.calloc.exit", "result: %p", ret);
2899: 
2900: 	return ret;
2901: }
2902: 
2903: JEMALLOC_ALWAYS_INLINE void
2904: ifree(tsd_t *tsd, void *ptr, tcache_t *tcache, bool slow_path) {
2905: 	if (!slow_path) {
2906: 		tsd_assert_fast(tsd);
2907: 	}
2908: 	check_entry_exit_locking(tsd_tsdn(tsd));
2909: 	if (tsd_reentrancy_level_get(tsd) != 0) {
2910: 		assert(slow_path);
2911: 	}
2912: 
2913: 	assert(ptr != NULL);
2914: 	assert(malloc_initialized() || IS_INITIALIZER);
2915: 
2916: 	emap_alloc_ctx_t alloc_ctx;
2917: 	emap_alloc_ctx_lookup(tsd_tsdn(tsd), &arena_emap_global, ptr,
2918: 	    &alloc_ctx);
2919: 	assert(alloc_ctx.szind != SC_NSIZES);
2920: 
2921: 	size_t usize = sz_index2size(alloc_ctx.szind);
2922: 	if (config_prof && opt_prof) {
2923: 		prof_free(tsd, ptr, usize, &alloc_ctx);
2924: 	}
2925: 
2926: 	if (likely(!slow_path)) {
2927: 		idalloctm(tsd_tsdn(tsd), ptr, tcache, &alloc_ctx, false,
2928: 		    false);
2929: 	} else {
2930: 		if (config_fill && slow_path && opt_junk_free) {
2931: 			junk_free_callback(ptr, usize);
2932: 		}
2933: 		idalloctm(tsd_tsdn(tsd), ptr, tcache, &alloc_ctx, false,
2934: 		    true);
2935: 	}
2936: 	thread_dalloc_event(tsd, usize);
2937: }
2938: 
2939: JEMALLOC_ALWAYS_INLINE void
2940: isfree(tsd_t *tsd, void *ptr, size_t usize, tcache_t *tcache, bool slow_path) {
2941: 	if (!slow_path) {
2942: 		tsd_assert_fast(tsd);
2943: 	}
2944: 	check_entry_exit_locking(tsd_tsdn(tsd));
2945: 	if (tsd_reentrancy_level_get(tsd) != 0) {
2946: 		assert(slow_path);
2947: 	}
2948: 
2949: 	assert(ptr != NULL);
2950: 	assert(malloc_initialized() || IS_INITIALIZER);
2951: 
2952: 	emap_alloc_ctx_t alloc_ctx;
2953: 	if (!config_prof) {
2954: 		alloc_ctx.szind = sz_size2index(usize);
2955: 		alloc_ctx.slab = (alloc_ctx.szind < SC_NBINS);
2956: 	} else {
2957: 		if (likely(!prof_sample_aligned(ptr))) {
2958: 			/*
2959: 			 * When the ptr is not page aligned, it was not sampled.
2960: 			 * usize can be trusted to determine szind and slab.
2961: 			 */
2962: 			alloc_ctx.szind = sz_size2index(usize);
2963: 			alloc_ctx.slab = (alloc_ctx.szind < SC_NBINS);
2964: 		} else if (opt_prof) {
2965: 			emap_alloc_ctx_lookup(tsd_tsdn(tsd), &arena_emap_global,
2966: 			    ptr, &alloc_ctx);
2967: 
2968: 			if (config_opt_safety_checks) {
2969: 				/* Small alloc may have !slab (sampled). */
2970: 				if (unlikely(alloc_ctx.szind !=
2971: 				    sz_size2index(usize))) {
2972: 					safety_check_fail_sized_dealloc(
2973: 					    /* current_dealloc */ true, ptr,
2974: 					    /* true_size */ sz_index2size(
2975: 					    alloc_ctx.szind),
2976: 					    /* input_size */ usize);
2977: 				}
2978: 			}
2979: 		} else {
2980: 			alloc_ctx.szind = sz_size2index(usize);
2981: 			alloc_ctx.slab = (alloc_ctx.szind < SC_NBINS);
2982: 		}
2983: 	}
2984: 	bool fail = maybe_check_alloc_ctx(tsd, ptr, &alloc_ctx);
2985: 	if (fail) {
2986: 		/*
2987: 		 * This is a heap corruption bug.  In real life we'll crash; for
2988: 		 * the unit test we just want to avoid breaking anything too
2989: 		 * badly to get a test result out.  Let's leak instead of trying
2990: 		 * to free.
2991: 		 */
2992: 		return;
2993: 	}
2994: 
2995: 	if (config_prof && opt_prof) {
2996: 		prof_free(tsd, ptr, usize, &alloc_ctx);
2997: 	}
2998: 	if (likely(!slow_path)) {
2999: 		isdalloct(tsd_tsdn(tsd), ptr, usize, tcache, &alloc_ctx,
3000: 		    false);
3001: 	} else {
3002: 		if (config_fill && slow_path && opt_junk_free) {
3003: 			junk_free_callback(ptr, usize);
3004: 		}
3005: 		isdalloct(tsd_tsdn(tsd), ptr, usize, tcache, &alloc_ctx,
3006: 		    true);
3007: 	}
3008: 	thread_dalloc_event(tsd, usize);
3009: }
3010: 
3011: JEMALLOC_NOINLINE
3012: void
3013: free_default(void *ptr) {
3014: 	UTRACE(ptr, 0, 0);
3015: 	if (likely(ptr != NULL)) {
3016: 		/*
3017: 		 * We avoid setting up tsd fully (e.g. tcache, arena binding)
3018: 		 * based on only free() calls -- other activities trigger the
3019: 		 * minimal to full transition.  This is because free() may
3020: 		 * happen during thread shutdown after tls deallocation: if a
3021: 		 * thread never had any malloc activities until then, a
3022: 		 * fully-setup tsd won't be destructed properly.
3023: 		 */
3024: 		tsd_t *tsd = tsd_fetch_min();
3025: 		check_entry_exit_locking(tsd_tsdn(tsd));
3026: 
3027: 		if (likely(tsd_fast(tsd))) {
3028: 			tcache_t *tcache = tcache_get_from_ind(tsd,
3029: 			    TCACHE_IND_AUTOMATIC, /* slow */ false,
3030: 			    /* is_alloc */ false);
3031: 			ifree(tsd, ptr, tcache, /* slow */ false);
3032: 		} else {
3033: 			tcache_t *tcache = tcache_get_from_ind(tsd,
3034: 			    TCACHE_IND_AUTOMATIC, /* slow */ true,
3035: 			    /* is_alloc */ false);
3036: 			uintptr_t args_raw[3] = {(uintptr_t)ptr};
3037: 			hook_invoke_dalloc(hook_dalloc_free, ptr, args_raw);
3038: 			ifree(tsd, ptr, tcache, /* slow */ true);
3039: 		}
3040: 
3041: 		check_entry_exit_locking(tsd_tsdn(tsd));
3042: 	}
3043: }
3044: 
3045: JEMALLOC_EXPORT void JEMALLOC_NOTHROW
3046: je_free(void *ptr) {
3047: 	LOG("core.free.entry", "ptr: %p", ptr);
3048: 
3049: 	je_free_impl(ptr);
3050: 
3051: 	LOG("core.free.exit", "");
3052: }
3053: 
3054: JEMALLOC_EXPORT void JEMALLOC_NOTHROW
3055: je_free_sized(void *ptr, size_t size) {
3056: 	LOG("core.free_sized.entry", "ptr: %p, size: %zu", ptr, size);
3057: 
3058: 	je_sdallocx_noflags(ptr, size);
3059: 
3060: 	LOG("core.free_sized.exit", "");
3061: }
3062: 
3063: JEMALLOC_EXPORT void JEMALLOC_NOTHROW
3064: je_free_aligned_sized(void *ptr, size_t alignment, size_t size) {
3065: 	return je_sdallocx(ptr, size, /* flags */ MALLOCX_ALIGN(alignment));
3066: }
3067: 
3068: /*
3069:  * End malloc(3)-compatible functions.
3070:  */
3071: /******************************************************************************/
3072: /*
3073:  * Begin non-standard override functions.
3074:  */
3075: 
3076: #ifdef JEMALLOC_OVERRIDE_MEMALIGN
3077: JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
3078: void JEMALLOC_NOTHROW *
3079: JEMALLOC_ATTR(malloc)
3080: je_memalign(size_t alignment, size_t size) {
3081: 	void *ret;
3082: 	static_opts_t sopts;
3083: 	dynamic_opts_t dopts;
3084: 
3085: 	LOG("core.memalign.entry", "alignment: %zu, size: %zu\n", alignment,
3086: 	    size);
3087: 
3088: 	static_opts_init(&sopts);
3089: 	dynamic_opts_init(&dopts);
3090: 
3091: 	sopts.bump_empty_aligned_alloc = true;
3092: 	sopts.min_alignment = 1;
3093: 	sopts.oom_string =
3094: 	    "<jemalloc>: Error allocating aligned memory: out of memory\n";
3095: 	sopts.invalid_alignment_string =
3096: 	    "<jemalloc>: Error allocating aligned memory: invalid alignment\n";
3097: 	sopts.null_out_result_on_error = true;
3098: 
3099: 	dopts.result = &ret;
3100: 	dopts.num_items = 1;
3101: 	dopts.item_size = size;
3102: 	dopts.alignment = alignment;
3103: 
3104: 	imalloc(&sopts, &dopts);
3105: 	if (sopts.slow) {
3106: 		uintptr_t args[3] = {alignment, size};
3107: 		hook_invoke_alloc(hook_alloc_memalign, ret, (uintptr_t)ret,
3108: 		    args);
3109: 	}
3110: 
3111: 	LOG("core.memalign.exit", "result: %p", ret);
3112: 	return ret;
3113: }
3114: #endif
3115: 
3116: #ifdef JEMALLOC_OVERRIDE_VALLOC
3117: JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
3118: void JEMALLOC_NOTHROW *
3119: JEMALLOC_ATTR(malloc)
3120: je_valloc(size_t size) {
3121: 	void *ret;
3122: 
3123: 	static_opts_t sopts;
3124: 	dynamic_opts_t dopts;
3125: 
3126: 	LOG("core.valloc.entry", "size: %zu\n", size);
3127: 
3128: 	static_opts_init(&sopts);
3129: 	dynamic_opts_init(&dopts);
3130: 
3131: 	sopts.null_out_result_on_error = true;
3132: 	sopts.min_alignment = PAGE;
3133: 	sopts.oom_string =
3134: 	    "<jemalloc>: Error allocating aligned memory: out of memory\n";
3135: 	sopts.invalid_alignment_string =
3136: 	    "<jemalloc>: Error allocating aligned memory: invalid alignment\n";
3137: 
3138: 	dopts.result = &ret;
3139: 	dopts.num_items = 1;
3140: 	dopts.item_size = size;
3141: 	dopts.alignment = PAGE;
3142: 
3143: 	imalloc(&sopts, &dopts);
3144: 	if (sopts.slow) {
3145: 		uintptr_t args[3] = {size};
3146: 		hook_invoke_alloc(hook_alloc_valloc, ret, (uintptr_t)ret, args);
3147: 	}
3148: 
3149: 	LOG("core.valloc.exit", "result: %p\n", ret);
3150: 	return ret;
3151: }
3152: #endif
3153: 
3154: #ifdef JEMALLOC_OVERRIDE_PVALLOC
3155: JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
3156: void JEMALLOC_NOTHROW *
3157: JEMALLOC_ATTR(malloc)
3158: je_pvalloc(size_t size) {
3159: 	void *ret;
3160: 
3161: 	static_opts_t sopts;
3162: 	dynamic_opts_t dopts;
3163: 
3164: 	LOG("core.pvalloc.entry", "size: %zu\n", size);
3165: 
3166: 	static_opts_init(&sopts);
3167: 	dynamic_opts_init(&dopts);
3168: 
3169: 	sopts.null_out_result_on_error = true;
3170: 	sopts.min_alignment = PAGE;
3171: 	sopts.oom_string =
3172: 	    "<jemalloc>: Error allocating aligned memory: out of memory\n";
3173: 	sopts.invalid_alignment_string =
3174: 	    "<jemalloc>: Error allocating aligned memory: invalid alignment\n";
3175: 
3176: 	dopts.result = &ret;
3177: 	dopts.num_items = 1;
3178: 	/*
3179: 	 * This is the only difference from je_valloc - size is rounded up to
3180: 	 * a PAGE multiple.
3181: 	 */
3182: 	dopts.item_size = PAGE_CEILING(size);
3183: 	dopts.alignment = PAGE;
3184: 
3185: 	imalloc(&sopts, &dopts);
3186: 	if (sopts.slow) {
3187: 		uintptr_t args[3] = {size};
3188: 		hook_invoke_alloc(hook_alloc_pvalloc, ret, (uintptr_t)ret,
3189: 		    args);
3190: 	}
3191: 
3192: 	LOG("core.pvalloc.exit", "result: %p\n", ret);
3193: 	return ret;
3194: }
3195: #endif
3196: 
3197: #if defined(JEMALLOC_IS_MALLOC) && defined(JEMALLOC_GLIBC_MALLOC_HOOK)
3198: /*
3199:  * glibc provides the RTLD_DEEPBIND flag for dlopen which can make it possible
3200:  * to inconsistently reference libc's malloc(3)-compatible functions
3201:  * (https://bugzilla.mozilla.org/show_bug.cgi?id=493541).
3202:  *
3203:  * These definitions interpose hooks in glibc.  The functions are actually
3204:  * passed an extra argument for the caller return address, which will be
3205:  * ignored.
3206:  */
3207: #include <features.h> // defines __GLIBC__ if we are compiling against glibc
3208: 
3209: JEMALLOC_EXPORT void (*__free_hook)(void *ptr) = je_free;
3210: JEMALLOC_EXPORT void *(*__malloc_hook)(size_t size) = je_malloc;
3211: JEMALLOC_EXPORT void *(*__realloc_hook)(void *ptr, size_t size) = je_realloc;
3212: #  ifdef JEMALLOC_GLIBC_MEMALIGN_HOOK
3213: JEMALLOC_EXPORT void *(*__memalign_hook)(size_t alignment, size_t size) =
3214:     je_memalign;
3215: #  endif
3216: 
3217: #  ifdef __GLIBC__
3218: /*
3219:  * To enable static linking with glibc, the libc specific malloc interface must
3220:  * be implemented also, so none of glibc's malloc.o functions are added to the
3221:  * link.
3222:  */
3223: #    define ALIAS(je_fn)	__attribute__((alias (#je_fn), used))
3224: /* To force macro expansion of je_ prefix before stringification. */
3225: #    define PREALIAS(je_fn)	ALIAS(je_fn)
3226: #    ifdef JEMALLOC_OVERRIDE___LIBC_CALLOC
3227: void *__libc_calloc(size_t n, size_t size) PREALIAS(je_calloc);
3228: #    endif
3229: #    ifdef JEMALLOC_OVERRIDE___LIBC_FREE
3230: void __libc_free(void* ptr) PREALIAS(je_free);
3231: #    endif
3232: #    ifdef JEMALLOC_OVERRIDE___LIBC_FREE_SIZED
3233: void __libc_free_sized(void* ptr, size_t size) PREALIAS(je_free_sized);
3234: #    endif
3235: #    ifdef JEMALLOC_OVERRIDE___LIBC_FREE_ALIGNED_SIZED
3236: void __libc_free_aligned_sized(
3237:     void* ptr, size_t alignment, size_t size) PREALIAS(je_free_aligned_sized);
3238: #    endif
3239: #    ifdef JEMALLOC_OVERRIDE___LIBC_MALLOC
3240: void *__libc_malloc(size_t size) PREALIAS(je_malloc);
3241: #    endif
3242: #    ifdef JEMALLOC_OVERRIDE___LIBC_MEMALIGN
3243: void *__libc_memalign(size_t align, size_t s) PREALIAS(je_memalign);
3244: #    endif
3245: #    ifdef JEMALLOC_OVERRIDE___LIBC_REALLOC
3246: void *__libc_realloc(void* ptr, size_t size) PREALIAS(je_realloc);
3247: #    endif
3248: #    ifdef JEMALLOC_OVERRIDE___LIBC_VALLOC
3249: void *__libc_valloc(size_t size) PREALIAS(je_valloc);
3250: #    endif
3251: #    ifdef JEMALLOC_OVERRIDE___LIBC_PVALLOC
3252: void *__libc_pvalloc(size_t size) PREALIAS(je_pvalloc);
3253: #    endif
3254: #    ifdef JEMALLOC_OVERRIDE___POSIX_MEMALIGN
3255: int __posix_memalign(void** r, size_t a, size_t s) PREALIAS(je_posix_memalign);
3256: #    endif
3257: #    undef PREALIAS
3258: #    undef ALIAS
3259: #  endif
3260: #endif
3261: 
3262: /*
3263:  * End non-standard override functions.
3264:  */
3265: /******************************************************************************/
3266: /*
3267:  * Begin non-standard functions.
3268:  */
3269: 
3270: JEMALLOC_ALWAYS_INLINE unsigned
3271: mallocx_tcache_get(int flags) {
3272: 	if (likely((flags & MALLOCX_TCACHE_MASK) == 0)) {
3273: 		return TCACHE_IND_AUTOMATIC;
3274: 	} else if ((flags & MALLOCX_TCACHE_MASK) == MALLOCX_TCACHE_NONE) {
3275: 		return TCACHE_IND_NONE;
3276: 	} else {
3277: 		return MALLOCX_TCACHE_GET(flags);
3278: 	}
3279: }
3280: 
3281: JEMALLOC_ALWAYS_INLINE unsigned
3282: mallocx_arena_get(int flags) {
3283: 	if (unlikely((flags & MALLOCX_ARENA_MASK) != 0)) {
3284: 		return MALLOCX_ARENA_GET(flags);
3285: 	} else {
3286: 		return ARENA_IND_AUTOMATIC;
3287: 	}
3288: }
3289: 
3290: #ifdef JEMALLOC_EXPERIMENTAL_SMALLOCX_API
3291: 
3292: #define JEMALLOC_SMALLOCX_CONCAT_HELPER(x, y) x ## y
3293: #define JEMALLOC_SMALLOCX_CONCAT_HELPER2(x, y)  \
3294:   JEMALLOC_SMALLOCX_CONCAT_HELPER(x, y)
3295: 
3296: typedef struct {
3297: 	void *ptr;
3298: 	size_t size;
3299: } smallocx_return_t;
3300: 
3301: JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
3302: smallocx_return_t JEMALLOC_NOTHROW
3303: /*
3304:  * The attribute JEMALLOC_ATTR(malloc) cannot be used due to:
3305:  *  - https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86488
3306:  */
3307: JEMALLOC_SMALLOCX_CONCAT_HELPER2(je_smallocx_, JEMALLOC_VERSION_GID_IDENT)
3308:   (size_t size, int flags) {
3309: 	/*
3310: 	 * Note: the attribute JEMALLOC_ALLOC_SIZE(1) cannot be
3311: 	 * used here because it makes writing beyond the `size`
3312: 	 * of the `ptr` undefined behavior, but the objective
3313: 	 * of this function is to allow writing beyond `size`
3314: 	 * up to `smallocx_return_t::size`.
3315: 	 */
3316: 	smallocx_return_t ret;
3317: 	static_opts_t sopts;
3318: 	dynamic_opts_t dopts;
3319: 
3320: 	LOG("core.smallocx.entry", "size: %zu, flags: %d", size, flags);
3321: 
3322: 	static_opts_init(&sopts);
3323: 	dynamic_opts_init(&dopts);
3324: 
3325: 	sopts.assert_nonempty_alloc = true;
3326: 	sopts.null_out_result_on_error = true;
3327: 	sopts.oom_string = "<jemalloc>: Error in mallocx(): out of memory\n";
3328: 	sopts.usize = true;
3329: 
3330: 	dopts.result = &ret.ptr;
3331: 	dopts.num_items = 1;
3332: 	dopts.item_size = size;
3333: 	if (unlikely(flags != 0)) {
3334: 		dopts.alignment = MALLOCX_ALIGN_GET(flags);
3335: 		dopts.zero = MALLOCX_ZERO_GET(flags);
3336: 		dopts.tcache_ind = mallocx_tcache_get(flags);
3337: 		dopts.arena_ind = mallocx_arena_get(flags);
3338: 	}
3339: 
3340: 	imalloc(&sopts, &dopts);
3341: 	assert(dopts.usize == je_nallocx(size, flags));
3342: 	ret.size = dopts.usize;
3343: 
3344: 	LOG("core.smallocx.exit", "result: %p, size: %zu", ret.ptr, ret.size);
3345: 	return ret;
3346: }
3347: #undef JEMALLOC_SMALLOCX_CONCAT_HELPER
3348: #undef JEMALLOC_SMALLOCX_CONCAT_HELPER2
3349: #endif
3350: 
3351: JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
3352: void JEMALLOC_NOTHROW *
3353: JEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE(1)
3354: je_mallocx(size_t size, int flags) {
3355: 	void *ret;
3356: 	static_opts_t sopts;
3357: 	dynamic_opts_t dopts;
3358: 
3359: 	LOG("core.mallocx.entry", "size: %zu, flags: %d", size, flags);
3360: 
3361: 	static_opts_init(&sopts);
3362: 	dynamic_opts_init(&dopts);
3363: 
3364: 	sopts.assert_nonempty_alloc = true;
3365: 	sopts.null_out_result_on_error = true;
3366: 	sopts.oom_string = "<jemalloc>: Error in mallocx(): out of memory\n";
3367: 
3368: 	dopts.result = &ret;
3369: 	dopts.num_items = 1;
3370: 	dopts.item_size = size;
3371: 	if (unlikely(flags != 0)) {
3372: 		dopts.alignment = MALLOCX_ALIGN_GET(flags);
3373: 		dopts.zero = MALLOCX_ZERO_GET(flags);
3374: 		dopts.tcache_ind = mallocx_tcache_get(flags);
3375: 		dopts.arena_ind = mallocx_arena_get(flags);
3376: 	}
3377: 
3378: 	imalloc(&sopts, &dopts);
3379: 	if (sopts.slow) {
3380: 		uintptr_t args[3] = {size, flags};
3381: 		hook_invoke_alloc(hook_alloc_mallocx, ret, (uintptr_t)ret,
3382: 		    args);
3383: 	}
3384: 
3385: 	LOG("core.mallocx.exit", "result: %p", ret);
3386: 	return ret;
3387: }
3388: 
3389: static void *
3390: irallocx_prof_sample(tsdn_t *tsdn, void *old_ptr, size_t old_usize,
3391:     size_t usize, size_t alignment, bool zero, tcache_t *tcache, arena_t *arena,
3392:     prof_tctx_t *tctx, hook_ralloc_args_t *hook_args) {
3393: 	void *p;
3394: 
3395: 	if (tctx == NULL) {
3396: 		return NULL;
3397: 	}
3398: 
3399: 	alignment = prof_sample_align(usize, alignment);
3400: 	/*
3401: 	 * If the allocation is small enough that it would normally be allocated
3402: 	 * on a slab, we need to take additional steps to ensure that it gets
3403: 	 * its own extent instead.
3404: 	 */
3405: 	if (sz_can_use_slab(usize)) {
3406: 		size_t bumped_usize = sz_sa2u(usize, alignment);
3407: 		p = iralloct_explicit_slab(tsdn, old_ptr, old_usize,
3408: 		    bumped_usize, alignment, zero, /* slab */ false,
3409: 		    tcache, arena, hook_args);
3410: 		if (p == NULL) {
3411: 			return NULL;
3412: 		}
3413: 		arena_prof_promote(tsdn, p, usize, bumped_usize);
3414: 	} else {
3415: 		p = iralloct_explicit_slab(tsdn, old_ptr, old_usize, usize,
3416: 		    alignment, zero, /* slab */ false, tcache, arena,
3417: 		    hook_args);
3418: 	}
3419: 	assert(prof_sample_aligned(p));
3420: 
3421: 	return p;
3422: }
3423: 
3424: JEMALLOC_ALWAYS_INLINE void *
3425: irallocx_prof(tsd_t *tsd, void *old_ptr, size_t old_usize, size_t size,
3426:     size_t alignment, size_t usize, bool zero, tcache_t *tcache,
3427:     arena_t *arena, emap_alloc_ctx_t *alloc_ctx,
3428:     hook_ralloc_args_t *hook_args) {
3429: 	prof_info_t old_prof_info;
3430: 	prof_info_get_and_reset_recent(tsd, old_ptr, alloc_ctx, &old_prof_info);
3431: 	bool prof_active = prof_active_get_unlocked();
3432: 	bool sample_event = te_prof_sample_event_lookahead(tsd, usize);
3433: 	prof_tctx_t *tctx = prof_alloc_prep(tsd, prof_active, sample_event);
3434: 	void *p;
3435: 	if (unlikely(tctx != PROF_TCTX_SENTINEL)) {
3436: 		p = irallocx_prof_sample(tsd_tsdn(tsd), old_ptr, old_usize,
3437: 		    usize, alignment, zero, tcache, arena, tctx, hook_args);
3438: 	} else {
3439: 		p = iralloct(tsd_tsdn(tsd), old_ptr, old_usize, size, alignment,
3440: 		    usize, zero, tcache, arena, hook_args);
3441: 	}
3442: 	if (unlikely(p == NULL)) {
3443: 		prof_alloc_rollback(tsd, tctx);
3444: 		return NULL;
3445: 	}
3446: 	assert(usize == isalloc(tsd_tsdn(tsd), p));
3447: 	prof_realloc(tsd, p, size, usize, tctx, prof_active, old_ptr,
3448: 	    old_usize, &old_prof_info, sample_event);
3449: 
3450: 	return p;
3451: }
3452: 
3453: static void *
3454: do_rallocx(void *ptr, size_t size, int flags, bool is_realloc) {
3455: 	void *p;
3456: 	tsd_t *tsd;
3457: 	size_t usize;
3458: 	size_t old_usize;
3459: 	size_t alignment = MALLOCX_ALIGN_GET(flags);
3460: 	arena_t *arena;
3461: 
3462: 	assert(ptr != NULL);
3463: 	assert(size != 0);
3464: 	assert(malloc_initialized() || IS_INITIALIZER);
3465: 	tsd = tsd_fetch();
3466: 	check_entry_exit_locking(tsd_tsdn(tsd));
3467: 
3468: 	bool zero = zero_get(MALLOCX_ZERO_GET(flags), /* slow */ true);
3469: 
3470: 	unsigned arena_ind = mallocx_arena_get(flags);
3471: 	if (arena_get_from_ind(tsd, arena_ind, &arena)) {
3472: 		goto label_oom;
3473: 	}
3474: 
3475: 	unsigned tcache_ind = mallocx_tcache_get(flags);
3476: 	tcache_t *tcache = tcache_get_from_ind(tsd, tcache_ind,
3477: 	    /* slow */ true, /* is_alloc */ true);
3478: 
3479: 	emap_alloc_ctx_t alloc_ctx;
3480: 	emap_alloc_ctx_lookup(tsd_tsdn(tsd), &arena_emap_global, ptr,
3481: 	    &alloc_ctx);
3482: 	assert(alloc_ctx.szind != SC_NSIZES);
3483: 	old_usize = sz_index2size(alloc_ctx.szind);
3484: 	assert(old_usize == isalloc(tsd_tsdn(tsd), ptr));
3485: 	if (aligned_usize_get(size, alignment, &usize, NULL, false)) {
3486: 		goto label_oom;
3487: 	}
3488: 
3489: 	hook_ralloc_args_t hook_args = {is_realloc, {(uintptr_t)ptr, size,
3490: 		flags, 0}};
3491: 	if (config_prof && opt_prof) {
3492: 		p = irallocx_prof(tsd, ptr, old_usize, size, alignment, usize,
3493: 		    zero, tcache, arena, &alloc_ctx, &hook_args);
3494: 		if (unlikely(p == NULL)) {
3495: 			goto label_oom;
3496: 		}
3497: 	} else {
3498: 		p = iralloct(tsd_tsdn(tsd), ptr, old_usize, size, alignment,
3499: 		    usize, zero, tcache, arena, &hook_args);
3500: 		if (unlikely(p == NULL)) {
3501: 			goto label_oom;
3502: 		}
3503: 		assert(usize == isalloc(tsd_tsdn(tsd), p));
3504: 	}
3505: 	assert(alignment == 0 || ((uintptr_t)p & (alignment - 1)) == ZU(0));
3506: 	thread_alloc_event(tsd, usize);
3507: 	thread_dalloc_event(tsd, old_usize);
3508: 
3509: 	UTRACE(ptr, size, p);
3510: 	check_entry_exit_locking(tsd_tsdn(tsd));
3511: 
3512: 	if (config_fill && unlikely(opt_junk_alloc) && usize > old_usize
3513: 	    && !zero) {
3514: 		size_t excess_len = usize - old_usize;
3515: 		void *excess_start = (void *)((byte_t *)p + old_usize);
3516: 		junk_alloc_callback(excess_start, excess_len);
3517: 	}
3518: 
3519: 	return p;
3520: label_oom:
3521: 	if (is_realloc) {
3522: 		set_errno(ENOMEM);
3523: 	}
3524: 	if (config_xmalloc && unlikely(opt_xmalloc)) {
3525: 		malloc_write("<jemalloc>: Error in rallocx(): out of memory\n");
3526: 		abort();
3527: 	}
3528: 	UTRACE(ptr, size, 0);
3529: 	check_entry_exit_locking(tsd_tsdn(tsd));
3530: 
3531: 	return NULL;
3532: }
3533: 
3534: JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
3535: void JEMALLOC_NOTHROW *
3536: JEMALLOC_ALLOC_SIZE(2)
3537: je_rallocx(void *ptr, size_t size, int flags) {
3538: 	LOG("core.rallocx.entry", "ptr: %p, size: %zu, flags: %d", ptr,
3539: 	    size, flags);
3540: 	void *ret = do_rallocx(ptr, size, flags, false);
3541: 	LOG("core.rallocx.exit", "result: %p", ret);
3542: 	return ret;
3543: }
3544: 
3545: static void *
3546: do_realloc_nonnull_zero(void *ptr) {
3547: 	if (config_stats) {
3548: 		atomic_fetch_add_zu(&zero_realloc_count, 1, ATOMIC_RELAXED);
3549: 	}
3550: 	if (opt_zero_realloc_action == zero_realloc_action_alloc) {
3551: 		/*
3552: 		 * The user might have gotten an alloc setting while expecting a
3553: 		 * free setting.  If that's the case, we at least try to
3554: 		 * reduce the harm, and turn off the tcache while allocating, so
3555: 		 * that we'll get a true first fit.
3556: 		 */
3557: 		return do_rallocx(ptr, 1, MALLOCX_TCACHE_NONE, true);
3558: 	} else if (opt_zero_realloc_action == zero_realloc_action_free) {
3559: 		UTRACE(ptr, 0, 0);
3560: 		tsd_t *tsd = tsd_fetch();
3561: 		check_entry_exit_locking(tsd_tsdn(tsd));
3562: 
3563: 		tcache_t *tcache = tcache_get_from_ind(tsd,
3564: 		    TCACHE_IND_AUTOMATIC, /* slow */ true,
3565: 		    /* is_alloc */ false);
3566: 		uintptr_t args[3] = {(uintptr_t)ptr, 0};
3567: 		hook_invoke_dalloc(hook_dalloc_realloc, ptr, args);
3568: 		ifree(tsd, ptr, tcache, true);
3569: 
3570: 		check_entry_exit_locking(tsd_tsdn(tsd));
3571: 		return NULL;
3572: 	} else {
3573: 		safety_check_fail("Called realloc(non-null-ptr, 0) with "
3574: 		    "zero_realloc:abort set\n");
3575: 		/* In real code, this will never run; the safety check failure
3576: 		 * will call abort.  In the unit test, we just want to bail out
3577: 		 * without corrupting internal state that the test needs to
3578: 		 * finish.
3579: 		 */
3580: 		return NULL;
3581: 	}
3582: }
3583: 
3584: JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
3585: void JEMALLOC_NOTHROW *
3586: JEMALLOC_ALLOC_SIZE(2)
3587: je_realloc(void *ptr, size_t size) {
3588: 	LOG("core.realloc.entry", "ptr: %p, size: %zu\n", ptr, size);
3589: 
3590: 	if (likely(ptr != NULL && size != 0)) {
3591: 		void *ret = do_rallocx(ptr, size, 0, true);
3592: 		LOG("core.realloc.exit", "result: %p", ret);
3593: 		return ret;
3594: 	} else if (ptr != NULL && size == 0) {
3595: 		void *ret = do_realloc_nonnull_zero(ptr);
3596: 		LOG("core.realloc.exit", "result: %p", ret);
3597: 		return ret;
3598: 	} else {
3599: 		/* realloc(NULL, size) is equivalent to malloc(size). */
3600: 		void *ret;
3601: 
3602: 		static_opts_t sopts;
3603: 		dynamic_opts_t dopts;
3604: 
3605: 		static_opts_init(&sopts);
3606: 		dynamic_opts_init(&dopts);
3607: 
3608: 		sopts.null_out_result_on_error = true;
3609: 		sopts.set_errno_on_error = true;
3610: 		sopts.oom_string =
3611: 		    "<jemalloc>: Error in realloc(): out of memory\n";
3612: 
3613: 		dopts.result = &ret;
3614: 		dopts.num_items = 1;
3615: 		dopts.item_size = size;
3616: 
3617: 		imalloc(&sopts, &dopts);
3618: 		if (sopts.slow) {
3619: 			uintptr_t args[3] = {(uintptr_t)ptr, size};
3620: 			hook_invoke_alloc(hook_alloc_realloc, ret,
3621: 			    (uintptr_t)ret, args);
3622: 		}
3623: 		LOG("core.realloc.exit", "result: %p", ret);
3624: 		return ret;
3625: 	}
3626: }
3627: 
3628: JEMALLOC_ALWAYS_INLINE size_t
3629: ixallocx_helper(tsdn_t *tsdn, void *ptr, size_t old_usize, size_t size,
3630:     size_t extra, size_t alignment, bool zero) {
3631: 	size_t newsize;
3632: 
3633: 	if (ixalloc(tsdn, ptr, old_usize, size, extra, alignment, zero,
3634: 	    &newsize)) {
3635: 		return old_usize;
3636: 	}
3637: 
3638: 	return newsize;
3639: }
3640: 
3641: static size_t
3642: ixallocx_prof_sample(tsdn_t *tsdn, void *ptr, size_t old_usize, size_t size,
3643:     size_t extra, size_t alignment, bool zero, prof_tctx_t *tctx) {
3644: 	/* Sampled allocation needs to be page aligned. */
3645: 	if (tctx == NULL || !prof_sample_aligned(ptr)) {
3646: 		return old_usize;
3647: 	}
3648: 
3649: 	return ixallocx_helper(tsdn, ptr, old_usize, size, extra, alignment,
3650: 	    zero);
3651: }
3652: 
3653: JEMALLOC_ALWAYS_INLINE size_t
3654: ixallocx_prof(tsd_t *tsd, void *ptr, size_t old_usize, size_t size,
3655:     size_t extra, size_t alignment, bool zero, emap_alloc_ctx_t *alloc_ctx) {
3656: 	/*
3657: 	 * old_prof_info is only used for asserting that the profiling info
3658: 	 * isn't changed by the ixalloc() call.
3659: 	 */
3660: 	prof_info_t old_prof_info;
3661: 	prof_info_get(tsd, ptr, alloc_ctx, &old_prof_info);
3662: 
3663: 	/*
3664: 	 * usize isn't knowable before ixalloc() returns when extra is non-zero.
3665: 	 * Therefore, compute its maximum possible value and use that in
3666: 	 * prof_alloc_prep() to decide whether to capture a backtrace.
3667: 	 * prof_realloc() will use the actual usize to decide whether to sample.
3668: 	 */
3669: 	size_t usize_max;
3670: 	if (aligned_usize_get(size + extra, alignment, &usize_max, NULL,
3671: 	    false)) {
3672: 		/*
3673: 		 * usize_max is out of range, and chances are that allocation
3674: 		 * will fail, but use the maximum possible value and carry on
3675: 		 * with prof_alloc_prep(), just in case allocation succeeds.
3676: 		 */
3677: 		usize_max = SC_LARGE_MAXCLASS;
3678: 	}
3679: 	bool prof_active = prof_active_get_unlocked();
3680: 	bool sample_event = te_prof_sample_event_lookahead(tsd, usize_max);
3681: 	prof_tctx_t *tctx = prof_alloc_prep(tsd, prof_active, sample_event);
3682: 
3683: 	size_t usize;
3684: 	if (unlikely(tctx != PROF_TCTX_SENTINEL)) {
3685: 		usize = ixallocx_prof_sample(tsd_tsdn(tsd), ptr, old_usize,
3686: 		    size, extra, alignment, zero, tctx);
3687: 	} else {
3688: 		usize = ixallocx_helper(tsd_tsdn(tsd), ptr, old_usize, size,
3689: 		    extra, alignment, zero);
3690: 	}
3691: 
3692: 	/*
3693: 	 * At this point we can still safely get the original profiling
3694: 	 * information associated with the ptr, because (a) the edata_t object
3695: 	 * associated with the ptr still lives and (b) the profiling info
3696: 	 * fields are not touched.  "(a)" is asserted in the outer je_xallocx()
3697: 	 * function, and "(b)" is indirectly verified below by checking that
3698: 	 * the alloc_tctx field is unchanged.
3699: 	 */
3700: 	prof_info_t prof_info;
3701: 	if (usize == old_usize) {
3702: 		prof_info_get(tsd, ptr, alloc_ctx, &prof_info);
3703: 		prof_alloc_rollback(tsd, tctx);
3704: 	} else {
3705: 		prof_info_get_and_reset_recent(tsd, ptr, alloc_ctx, &prof_info);
3706: 		assert(usize <= usize_max);
3707: 		sample_event = te_prof_sample_event_lookahead(tsd, usize);
3708: 		prof_realloc(tsd, ptr, size, usize, tctx, prof_active, ptr,
3709: 		    old_usize, &prof_info, sample_event);
3710: 	}
3711: 
3712: 	assert(old_prof_info.alloc_tctx == prof_info.alloc_tctx);
3713: 	return usize;
3714: }
3715: 
3716: JEMALLOC_EXPORT size_t JEMALLOC_NOTHROW
3717: je_xallocx(void *ptr, size_t size, size_t extra, int flags) {
3718: 	tsd_t *tsd;
3719: 	size_t usize, old_usize;
3720: 	size_t alignment = MALLOCX_ALIGN_GET(flags);
3721: 	bool zero = zero_get(MALLOCX_ZERO_GET(flags), /* slow */ true);
3722: 
3723: 	LOG("core.xallocx.entry", "ptr: %p, size: %zu, extra: %zu, "
3724: 	    "flags: %d", ptr, size, extra, flags);
3725: 
3726: 	assert(ptr != NULL);
3727: 	assert(size != 0);
3728: 	assert(SIZE_T_MAX - size >= extra);
3729: 	assert(malloc_initialized() || IS_INITIALIZER);
3730: 	tsd = tsd_fetch();
3731: 	check_entry_exit_locking(tsd_tsdn(tsd));
3732: 
3733: 	/*
3734: 	 * old_edata is only for verifying that xallocx() keeps the edata_t
3735: 	 * object associated with the ptr (though the content of the edata_t
3736: 	 * object can be changed).
3737: 	 */
3738: 	edata_t *old_edata = emap_edata_lookup(tsd_tsdn(tsd),
3739: 	    &arena_emap_global, ptr);
3740: 
3741: 	emap_alloc_ctx_t alloc_ctx;
3742: 	emap_alloc_ctx_lookup(tsd_tsdn(tsd), &arena_emap_global, ptr,
3743: 	    &alloc_ctx);
3744: 	assert(alloc_ctx.szind != SC_NSIZES);
3745: 	old_usize = sz_index2size(alloc_ctx.szind);
3746: 	assert(old_usize == isalloc(tsd_tsdn(tsd), ptr));
3747: 	/*
3748: 	 * The API explicitly absolves itself of protecting against (size +
3749: 	 * extra) numerical overflow, but we may need to clamp extra to avoid
3750: 	 * exceeding SC_LARGE_MAXCLASS.
3751: 	 *
3752: 	 * Ordinarily, size limit checking is handled deeper down, but here we
3753: 	 * have to check as part of (size + extra) clamping, since we need the
3754: 	 * clamped value in the above helper functions.
3755: 	 */
3756: 	if (unlikely(size > SC_LARGE_MAXCLASS)) {
3757: 		usize = old_usize;
3758: 		goto label_not_resized;
3759: 	}
3760: 	if (unlikely(SC_LARGE_MAXCLASS - size < extra)) {
3761: 		extra = SC_LARGE_MAXCLASS - size;
3762: 	}
3763: 
3764: 	if (config_prof && opt_prof) {
3765: 		usize = ixallocx_prof(tsd, ptr, old_usize, size, extra,
3766: 		    alignment, zero, &alloc_ctx);
3767: 	} else {
3768: 		usize = ixallocx_helper(tsd_tsdn(tsd), ptr, old_usize, size,
3769: 		    extra, alignment, zero);
3770: 	}
3771: 
3772: 	/*
3773: 	 * xallocx() should keep using the same edata_t object (though its
3774: 	 * content can be changed).
3775: 	 */
3776: 	assert(emap_edata_lookup(tsd_tsdn(tsd), &arena_emap_global, ptr)
3777: 	    == old_edata);
3778: 
3779: 	if (unlikely(usize == old_usize)) {
3780: 		goto label_not_resized;
3781: 	}
3782: 	thread_alloc_event(tsd, usize);
3783: 	thread_dalloc_event(tsd, old_usize);
3784: 
3785: 	if (config_fill && unlikely(opt_junk_alloc) && usize > old_usize &&
3786: 	    !zero) {
3787: 		size_t excess_len = usize - old_usize;
3788: 		void *excess_start = (void *)((byte_t *)ptr + old_usize);
3789: 		junk_alloc_callback(excess_start, excess_len);
3790: 	}
3791: label_not_resized:
3792: 	if (unlikely(!tsd_fast(tsd))) {
3793: 		uintptr_t args[4] = {(uintptr_t)ptr, size, extra, flags};
3794: 		hook_invoke_expand(hook_expand_xallocx, ptr, old_usize,
3795: 		    usize, (uintptr_t)usize, args);
3796: 	}
3797: 
3798: 	UTRACE(ptr, size, ptr);
3799: 	check_entry_exit_locking(tsd_tsdn(tsd));
3800: 
3801: 	LOG("core.xallocx.exit", "result: %zu", usize);
3802: 	return usize;
3803: }
3804: 
3805: JEMALLOC_EXPORT size_t JEMALLOC_NOTHROW
3806: JEMALLOC_ATTR(pure)
3807: je_sallocx(const void *ptr, int flags) {
3808: 	size_t usize;
3809: 	tsdn_t *tsdn;
3810: 
3811: 	LOG("core.sallocx.entry", "ptr: %p, flags: %d", ptr, flags);
3812: 
3813: 	assert(malloc_initialized() || IS_INITIALIZER);
3814: 	assert(ptr != NULL);
3815: 
3816: 	tsdn = tsdn_fetch();
3817: 	check_entry_exit_locking(tsdn);
3818: 
3819: 	if (config_debug || force_ivsalloc) {
3820: 		usize = ivsalloc(tsdn, ptr);
3821: 		assert(force_ivsalloc || usize != 0);
3822: 	} else {
3823: 		usize = isalloc(tsdn, ptr);
3824: 	}
3825: 
3826: 	check_entry_exit_locking(tsdn);
3827: 
3828: 	LOG("core.sallocx.exit", "result: %zu", usize);
3829: 	return usize;
3830: }
3831: 
3832: JEMALLOC_EXPORT void JEMALLOC_NOTHROW
3833: je_dallocx(void *ptr, int flags) {
3834: 	LOG("core.dallocx.entry", "ptr: %p, flags: %d", ptr, flags);
3835: 
3836: 	assert(ptr != NULL);
3837: 	assert(malloc_initialized() || IS_INITIALIZER);
3838: 
3839: 	tsd_t *tsd = tsd_fetch_min();
3840: 	bool fast = tsd_fast(tsd);
3841: 	check_entry_exit_locking(tsd_tsdn(tsd));
3842: 
3843: 	unsigned tcache_ind = mallocx_tcache_get(flags);
3844: 	tcache_t *tcache = tcache_get_from_ind(tsd, tcache_ind, !fast,
3845: 	    /* is_alloc */ false);
3846: 
3847: 	UTRACE(ptr, 0, 0);
3848: 	if (likely(fast)) {
3849: 		tsd_assert_fast(tsd);
3850: 		ifree(tsd, ptr, tcache, false);
3851: 	} else {
3852: 		uintptr_t args_raw[3] = {(uintptr_t)ptr, flags};
3853: 		hook_invoke_dalloc(hook_dalloc_dallocx, ptr, args_raw);
3854: 		ifree(tsd, ptr, tcache, true);
3855: 	}
3856: 	check_entry_exit_locking(tsd_tsdn(tsd));
3857: 
3858: 	LOG("core.dallocx.exit", "");
3859: }
3860: 
3861: JEMALLOC_ALWAYS_INLINE size_t
3862: inallocx(tsdn_t *tsdn, size_t size, int flags) {
3863: 	check_entry_exit_locking(tsdn);
3864: 	size_t usize;
3865: 	/* In case of out of range, let the user see it rather than fail. */
3866: 	aligned_usize_get(size, MALLOCX_ALIGN_GET(flags), &usize, NULL, false);
3867: 	check_entry_exit_locking(tsdn);
3868: 	return usize;
3869: }
3870: 
3871: JEMALLOC_NOINLINE void
3872: sdallocx_default(void *ptr, size_t size, int flags) {
3873: 	assert(ptr != NULL);
3874: 	assert(malloc_initialized() || IS_INITIALIZER);
3875: 
3876: 	tsd_t *tsd = tsd_fetch_min();
3877: 	bool fast = tsd_fast(tsd);
3878: 	size_t usize = inallocx(tsd_tsdn(tsd), size, flags);
3879: 	check_entry_exit_locking(tsd_tsdn(tsd));
3880: 
3881: 	unsigned tcache_ind = mallocx_tcache_get(flags);
3882: 	tcache_t *tcache = tcache_get_from_ind(tsd, tcache_ind, !fast,
3883: 	    /* is_alloc */ false);
3884: 
3885: 	UTRACE(ptr, 0, 0);
3886: 	if (likely(fast)) {
3887: 		tsd_assert_fast(tsd);
3888: 		isfree(tsd, ptr, usize, tcache, false);
3889: 	} else {
3890: 		uintptr_t args_raw[3] = {(uintptr_t)ptr, size, flags};
3891: 		hook_invoke_dalloc(hook_dalloc_sdallocx, ptr, args_raw);
3892: 		isfree(tsd, ptr, usize, tcache, true);
3893: 	}
3894: 	check_entry_exit_locking(tsd_tsdn(tsd));
3895: }
3896: 
3897: JEMALLOC_EXPORT void JEMALLOC_NOTHROW
3898: je_sdallocx(void *ptr, size_t size, int flags) {
3899: 	LOG("core.sdallocx.entry", "ptr: %p, size: %zu, flags: %d", ptr,
3900: 		size, flags);
3901: 
3902: 	je_sdallocx_impl(ptr, size, flags);
3903: 
3904: 	LOG("core.sdallocx.exit", "");
3905: }
3906: 
3907: JEMALLOC_EXPORT size_t JEMALLOC_NOTHROW
3908: JEMALLOC_ATTR(pure)
3909: je_nallocx(size_t size, int flags) {
3910: 	size_t usize;
3911: 	tsdn_t *tsdn;
3912: 
3913: 	assert(size != 0);
3914: 
3915: 	if (unlikely(malloc_init())) {
3916: 		LOG("core.nallocx.exit", "result: %zu", ZU(0));
3917: 		return 0;
3918: 	}
3919: 
3920: 	tsdn = tsdn_fetch();
3921: 	check_entry_exit_locking(tsdn);
3922: 
3923: 	usize = inallocx(tsdn, size, flags);
3924: 	if (unlikely(usize > SC_LARGE_MAXCLASS)) {
3925: 		LOG("core.nallocx.exit", "result: %zu", ZU(0));
3926: 		return 0;
3927: 	}
3928: 
3929: 	check_entry_exit_locking(tsdn);
3930: 	LOG("core.nallocx.exit", "result: %zu", usize);
3931: 	return usize;
3932: }
3933: 
3934: JEMALLOC_EXPORT int JEMALLOC_NOTHROW
3935: je_mallctl(const char *name, void *oldp, size_t *oldlenp, void *newp,
3936:     size_t newlen) {
3937: 	int ret;
3938: 	tsd_t *tsd;
3939: 
3940: 	LOG("core.mallctl.entry", "name: %s", name);
3941: 
3942: 	if (unlikely(malloc_init())) {
3943: 		LOG("core.mallctl.exit", "result: %d", EAGAIN);
3944: 		return EAGAIN;
3945: 	}
3946: 
3947: 	tsd = tsd_fetch();
3948: 	check_entry_exit_locking(tsd_tsdn(tsd));
3949: 	ret = ctl_byname(tsd, name, oldp, oldlenp, newp, newlen);
3950: 	check_entry_exit_locking(tsd_tsdn(tsd));
3951: 
3952: 	LOG("core.mallctl.exit", "result: %d", ret);
3953: 	return ret;
3954: }
3955: 
3956: JEMALLOC_EXPORT int JEMALLOC_NOTHROW
3957: je_mallctlnametomib(const char *name, size_t *mibp, size_t *miblenp) {
3958: 	int ret;
3959: 
3960: 	LOG("core.mallctlnametomib.entry", "name: %s", name);
3961: 
3962: 	if (unlikely(malloc_init())) {
3963: 		LOG("core.mallctlnametomib.exit", "result: %d", EAGAIN);
3964: 		return EAGAIN;
3965: 	}
3966: 
3967: 	tsd_t *tsd = tsd_fetch();
3968: 	check_entry_exit_locking(tsd_tsdn(tsd));
3969: 	ret = ctl_nametomib(tsd, name, mibp, miblenp);
3970: 	check_entry_exit_locking(tsd_tsdn(tsd));
3971: 
3972: 	LOG("core.mallctlnametomib.exit", "result: %d", ret);
3973: 	return ret;
3974: }
3975: 
3976: JEMALLOC_EXPORT int JEMALLOC_NOTHROW
3977: je_mallctlbymib(const size_t *mib, size_t miblen, void *oldp, size_t *oldlenp,
3978:   void *newp, size_t newlen) {
3979: 	int ret;
3980: 	tsd_t *tsd;
3981: 
3982: 	LOG("core.mallctlbymib.entry", "");
3983: 
3984: 	if (unlikely(malloc_init())) {
3985: 		LOG("core.mallctlbymib.exit", "result: %d", EAGAIN);
3986: 		return EAGAIN;
3987: 	}
3988: 
3989: 	tsd = tsd_fetch();
3990: 	check_entry_exit_locking(tsd_tsdn(tsd));
3991: 	ret = ctl_bymib(tsd, mib, miblen, oldp, oldlenp, newp, newlen);
3992: 	check_entry_exit_locking(tsd_tsdn(tsd));
3993: 	LOG("core.mallctlbymib.exit", "result: %d", ret);
3994: 	return ret;
3995: }
3996: 
3997: #define STATS_PRINT_BUFSIZE 65536
3998: JEMALLOC_EXPORT void JEMALLOC_NOTHROW
3999: je_malloc_stats_print(void (*write_cb)(void *, const char *), void *cbopaque,
4000:     const char *opts) {
4001: 	tsdn_t *tsdn;
4002: 
4003: 	LOG("core.malloc_stats_print.entry", "");
4004: 
4005: 	tsdn = tsdn_fetch();
4006: 	check_entry_exit_locking(tsdn);
4007: 
4008: 	if (config_debug) {
4009: 		stats_print(write_cb, cbopaque, opts);
4010: 	} else {
4011: 		buf_writer_t buf_writer;
4012: 		buf_writer_init(tsdn, &buf_writer, write_cb, cbopaque, NULL,
4013: 		    STATS_PRINT_BUFSIZE);
4014: 		stats_print(buf_writer_cb, &buf_writer, opts);
4015: 		buf_writer_terminate(tsdn, &buf_writer);
4016: 	}
4017: 
4018: 	check_entry_exit_locking(tsdn);
4019: 	LOG("core.malloc_stats_print.exit", "");
4020: }
4021: #undef STATS_PRINT_BUFSIZE
4022: 
4023: JEMALLOC_ALWAYS_INLINE size_t
4024: je_malloc_usable_size_impl(JEMALLOC_USABLE_SIZE_CONST void *ptr) {
4025: 	assert(malloc_initialized() || IS_INITIALIZER);
4026: 
4027: 	tsdn_t *tsdn = tsdn_fetch();
4028: 	check_entry_exit_locking(tsdn);
4029: 
4030: 	size_t ret;
4031: 	if (unlikely(ptr == NULL)) {
4032: 		ret = 0;
4033: 	} else {
4034: 		if (config_debug || force_ivsalloc) {
4035: 			ret = ivsalloc(tsdn, ptr);
4036: 			assert(force_ivsalloc || ret != 0);
4037: 		} else {
4038: 			ret = isalloc(tsdn, ptr);
4039: 		}
4040: 	}
4041: 	check_entry_exit_locking(tsdn);
4042: 
4043: 	return ret;
4044: }
4045: 
4046: JEMALLOC_EXPORT size_t JEMALLOC_NOTHROW
4047: je_malloc_usable_size(JEMALLOC_USABLE_SIZE_CONST void *ptr) {
4048: 	LOG("core.malloc_usable_size.entry", "ptr: %p", ptr);
4049: 
4050: 	size_t ret = je_malloc_usable_size_impl(ptr);
4051: 
4052: 	LOG("core.malloc_usable_size.exit", "result: %zu", ret);
4053: 	return ret;
4054: }
4055: 
4056: #ifdef JEMALLOC_HAVE_MALLOC_SIZE
4057: JEMALLOC_EXPORT size_t JEMALLOC_NOTHROW
4058: je_malloc_size(const void *ptr) {
4059: 	LOG("core.malloc_size.entry", "ptr: %p", ptr);
4060: 
4061: 	size_t ret = je_malloc_usable_size_impl(ptr);
4062: 
4063: 	LOG("core.malloc_size.exit", "result: %zu", ret);
4064: 	return ret;
4065: }
4066: #endif
4067: 
4068: static void
4069: batch_alloc_prof_sample_assert(tsd_t *tsd, size_t batch, size_t usize) {
4070: 	assert(config_prof && opt_prof);
4071: 	bool prof_sample_event = te_prof_sample_event_lookahead(tsd,
4072: 	    batch * usize);
4073: 	assert(!prof_sample_event);
4074: 	size_t surplus;
4075: 	prof_sample_event = te_prof_sample_event_lookahead_surplus(tsd,
4076: 	    (batch + 1) * usize, &surplus);
4077: 	assert(prof_sample_event);
4078: 	assert(surplus < usize);
4079: }
4080: 
4081: size_t
4082: batch_alloc(void **ptrs, size_t num, size_t size, int flags) {
4083: 	LOG("core.batch_alloc.entry",
4084: 	    "ptrs: %p, num: %zu, size: %zu, flags: %d", ptrs, num, size, flags);
4085: 
4086: 	tsd_t *tsd = tsd_fetch();
4087: 	check_entry_exit_locking(tsd_tsdn(tsd));
4088: 
4089: 	size_t filled = 0;
4090: 
4091: 	if (unlikely(tsd == NULL || tsd_reentrancy_level_get(tsd) > 0)) {
4092: 		goto label_done;
4093: 	}
4094: 
4095: 	size_t alignment = MALLOCX_ALIGN_GET(flags);
4096: 	size_t usize;
4097: 	if (aligned_usize_get(size, alignment, &usize, NULL, false)) {
4098: 		goto label_done;
4099: 	}
4100: 	szind_t ind = sz_size2index(usize);
4101: 	bool zero = zero_get(MALLOCX_ZERO_GET(flags), /* slow */ true);
4102: 
4103: 	/*
4104: 	 * The cache bin and arena will be lazily initialized; it's hard to
4105: 	 * know in advance whether each of them needs to be initialized.
4106: 	 */
4107: 	cache_bin_t *bin = NULL;
4108: 	arena_t *arena = NULL;
4109: 
4110: 	size_t nregs = 0;
4111: 	if (likely(ind < SC_NBINS)) {
4112: 		nregs = bin_infos[ind].nregs;
4113: 		assert(nregs > 0);
4114: 	}
4115: 
4116: 	while (filled < num) {
4117: 		size_t batch = num - filled;
4118: 		size_t surplus = SIZE_MAX; /* Dead store. */
4119: 		bool prof_sample_event = config_prof && opt_prof
4120: 		    && prof_active_get_unlocked()
4121: 		    && te_prof_sample_event_lookahead_surplus(tsd,
4122: 		    batch * usize, &surplus);
4123: 
4124: 		if (prof_sample_event) {
4125: 			/*
4126: 			 * Adjust so that the batch does not trigger prof
4127: 			 * sampling.
4128: 			 */
4129: 			batch -= surplus / usize + 1;
4130: 			batch_alloc_prof_sample_assert(tsd, batch, usize);
4131: 		}
4132: 
4133: 		size_t progress = 0;
4134: 
4135: 		if (likely(ind < SC_NBINS) && batch >= nregs) {
4136: 			if (arena == NULL) {
4137: 				unsigned arena_ind = mallocx_arena_get(flags);
4138: 				if (arena_get_from_ind(tsd, arena_ind,
4139: 				    &arena)) {
4140: 					goto label_done;
4141: 				}
4142: 				if (arena == NULL) {
4143: 					arena = arena_choose(tsd, NULL);
4144: 				}
4145: 				if (unlikely(arena == NULL)) {
4146: 					goto label_done;
4147: 				}
4148: 			}
4149: 			size_t arena_batch = batch - batch % nregs;
4150: 			size_t n = arena_fill_small_fresh(tsd_tsdn(tsd), arena,
4151: 			    ind, ptrs + filled, arena_batch, zero);
4152: 			progress += n;
4153: 			filled += n;
4154: 		}
4155: 
4156: 		unsigned tcache_ind = mallocx_tcache_get(flags);
4157: 		tcache_t *tcache = tcache_get_from_ind(tsd, tcache_ind,
4158: 		    /* slow */ true, /* is_alloc */ true);
4159: 		if (likely(tcache != NULL &&
4160: 		    ind < tcache_nbins_get(tcache->tcache_slow) &&
4161: 		    !tcache_bin_disabled(ind, &tcache->bins[ind],
4162: 		    tcache->tcache_slow)) && progress < batch) {
4163: 			if (bin == NULL) {
4164: 				bin = &tcache->bins[ind];
4165: 			}
4166: 			/*
4167: 			 * If we don't have a tcache bin, we don't want to
4168: 			 * immediately give up, because there's the possibility
4169: 			 * that the user explicitly requested to bypass the
4170: 			 * tcache, or that the user explicitly turned off the
4171: 			 * tcache; in such cases, we go through the slow path,
4172: 			 * i.e. the mallocx() call at the end of the while loop.
4173: 			 */
4174: 			if (bin != NULL) {
4175: 				size_t bin_batch = batch - progress;
4176: 				/*
4177: 				 * n can be less than bin_batch, meaning that
4178: 				 * the cache bin does not have enough memory.
4179: 				 * In such cases, we rely on the slow path,
4180: 				 * i.e. the mallocx() call at the end of the
4181: 				 * while loop, to fill in the cache, and in the
4182: 				 * next iteration of the while loop, the tcache
4183: 				 * will contain a lot of memory, and we can
4184: 				 * harvest them here.  Compared to the
4185: 				 * alternative approach where we directly go to
4186: 				 * the arena bins here, the overhead of our
4187: 				 * current approach should usually be minimal,
4188: 				 * since we never try to fetch more memory than
4189: 				 * what a slab contains via the tcache.  An
4190: 				 * additional benefit is that the tcache will
4191: 				 * not be empty for the next allocation request.
4192: 				 */
4193: 				size_t n = cache_bin_alloc_batch(bin, bin_batch,
4194: 				    ptrs + filled);
4195: 				if (config_stats) {
4196: 					bin->tstats.nrequests += n;
4197: 				}
4198: 				if (zero) {
4199: 					for (size_t i = 0; i < n; ++i) {
4200: 						memset(ptrs[filled + i], 0,
4201: 						    usize);
4202: 					}
4203: 				}
4204: 				if (config_prof && opt_prof
4205: 				    && unlikely(ind >= SC_NBINS)) {
4206: 					for (size_t i = 0; i < n; ++i) {
4207: 						prof_tctx_reset_sampled(tsd,
4208: 						    ptrs[filled + i]);
4209: 					}
4210: 				}
4211: 				progress += n;
4212: 				filled += n;
4213: 			}
4214: 		}
4215: 
4216: 		/*
4217: 		 * For thread events other than prof sampling, trigger them as
4218: 		 * if there's a single allocation of size (n * usize).  This is
4219: 		 * fine because:
4220: 		 * (a) these events do not alter the allocation itself, and
4221: 		 * (b) it's possible that some event would have been triggered
4222: 		 *     multiple times, instead of only once, if the allocations
4223: 		 *     were handled individually, but it would do no harm (or
4224: 		 *     even be beneficial) to coalesce the triggerings.
4225: 		 */
4226: 		thread_alloc_event(tsd, progress * usize);
4227: 
4228: 		if (progress < batch || prof_sample_event) {
4229: 			void *p = je_mallocx(size, flags);
4230: 			if (p == NULL) { /* OOM */
4231: 				break;
4232: 			}
4233: 			if (progress == batch) {
4234: 				assert(prof_sampled(tsd, p));
4235: 			}
4236: 			ptrs[filled++] = p;
4237: 		}
4238: 	}
4239: 
4240: label_done:
4241: 	check_entry_exit_locking(tsd_tsdn(tsd));
4242: 	LOG("core.batch_alloc.exit", "result: %zu", filled);
4243: 	return filled;
4244: }
4245: 
4246: /*
4247:  * End non-standard functions.
4248:  */
4249: /******************************************************************************/
4250: /*
4251:  * The following functions are used by threading libraries for protection of
4252:  * malloc during fork().
4253:  */
4254: 
4255: /*
4256:  * If an application creates a thread before doing any allocation in the main
4257:  * thread, then calls fork(2) in the main thread followed by memory allocation
4258:  * in the child process, a race can occur that results in deadlock within the
4259:  * child: the main thread may have forked while the created thread had
4260:  * partially initialized the allocator.  Ordinarily jemalloc prevents
4261:  * fork/malloc races via the following functions it registers during
4262:  * initialization using pthread_atfork(), but of course that does no good if
4263:  * the allocator isn't fully initialized at fork time.  The following library
4264:  * constructor is a partial solution to this problem.  It may still be possible
4265:  * to trigger the deadlock described above, but doing so would involve forking
4266:  * via a library constructor that runs before jemalloc's runs.
4267:  */
4268: #ifndef JEMALLOC_JET
4269: JEMALLOC_ATTR(constructor)
4270: static void
4271: jemalloc_constructor(void) {
4272: 	unsigned long long cpu_count = malloc_ncpus();
4273: 	unsigned long long bgt_count = cpu_count / 16;
4274: 	if (bgt_count == 0) {
4275: 		bgt_count = 1;
4276: 	}
4277: 	// decay is in ms
4278: 	unsigned long long decay = DUCKDB_JEMALLOC_DECAY * 1000;
4279: #ifdef DEBUG
4280: 	snprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, "junk:true,oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu", decay, decay, cpu_count, bgt_count);
4281: #else
4282: 	snprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, "oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu", decay, decay, cpu_count, bgt_count);
4283: #endif
4284: 	je_malloc_conf = JE_MALLOC_CONF_BUFFER;
4285: 	malloc_init();
4286: }
4287: #endif
4288: 
4289: #ifndef JEMALLOC_MUTEX_INIT_CB
4290: void
4291: jemalloc_prefork(void)
4292: #else
4293: JEMALLOC_EXPORT void
4294: _malloc_prefork(void)
4295: #endif
4296: {
4297: 	tsd_t *tsd;
4298: 	unsigned i, j, narenas;
4299: 	arena_t *arena;
4300: 
4301: #ifdef JEMALLOC_MUTEX_INIT_CB
4302: 	if (!malloc_initialized()) {
4303: 		return;
4304: 	}
4305: #endif
4306: 	assert(malloc_initialized());
4307: 
4308: 	tsd = tsd_fetch();
4309: 
4310: 	narenas = narenas_total_get();
4311: 
4312: 	witness_prefork(tsd_witness_tsdp_get(tsd));
4313: 	/* Acquire all mutexes in a safe order. */
4314: 	ctl_prefork(tsd_tsdn(tsd));
4315: 	tcache_prefork(tsd_tsdn(tsd));
4316: 	malloc_mutex_prefork(tsd_tsdn(tsd), &arenas_lock);
4317: 	if (have_background_thread) {
4318: 		background_thread_prefork0(tsd_tsdn(tsd));
4319: 	}
4320: 	prof_prefork0(tsd_tsdn(tsd));
4321: 	if (have_background_thread) {
4322: 		background_thread_prefork1(tsd_tsdn(tsd));
4323: 	}
4324: 	/* Break arena prefork into stages to preserve lock order. */
4325: 	for (i = 0; i < 9; i++) {
4326: 		for (j = 0; j < narenas; j++) {
4327: 			if ((arena = arena_get(tsd_tsdn(tsd), j, false)) !=
4328: 			    NULL) {
4329: 				switch (i) {
4330: 				case 0:
4331: 					arena_prefork0(tsd_tsdn(tsd), arena);
4332: 					break;
4333: 				case 1:
4334: 					arena_prefork1(tsd_tsdn(tsd), arena);
4335: 					break;
4336: 				case 2:
4337: 					arena_prefork2(tsd_tsdn(tsd), arena);
4338: 					break;
4339: 				case 3:
4340: 					arena_prefork3(tsd_tsdn(tsd), arena);
4341: 					break;
4342: 				case 4:
4343: 					arena_prefork4(tsd_tsdn(tsd), arena);
4344: 					break;
4345: 				case 5:
4346: 					arena_prefork5(tsd_tsdn(tsd), arena);
4347: 					break;
4348: 				case 6:
4349: 					arena_prefork6(tsd_tsdn(tsd), arena);
4350: 					break;
4351: 				case 7:
4352: 					arena_prefork7(tsd_tsdn(tsd), arena);
4353: 					break;
4354: 				case 8:
4355: 					arena_prefork8(tsd_tsdn(tsd), arena);
4356: 					break;
4357: 				default: not_reached();
4358: 				}
4359: 			}
4360: 		}
4361: 
4362: 	}
4363: 	prof_prefork1(tsd_tsdn(tsd));
4364: 	stats_prefork(tsd_tsdn(tsd));
4365: 	tsd_prefork(tsd);
4366: }
4367: 
4368: #ifndef JEMALLOC_MUTEX_INIT_CB
4369: void
4370: jemalloc_postfork_parent(void)
4371: #else
4372: JEMALLOC_EXPORT void
4373: _malloc_postfork(void)
4374: #endif
4375: {
4376: 	tsd_t *tsd;
4377: 	unsigned i, narenas;
4378: 
4379: #ifdef JEMALLOC_MUTEX_INIT_CB
4380: 	if (!malloc_initialized()) {
4381: 		return;
4382: 	}
4383: #endif
4384: 	assert(malloc_initialized());
4385: 
4386: 	tsd = tsd_fetch();
4387: 
4388: 	tsd_postfork_parent(tsd);
4389: 
4390: 	witness_postfork_parent(tsd_witness_tsdp_get(tsd));
4391: 	/* Release all mutexes, now that fork() has completed. */
4392: 	stats_postfork_parent(tsd_tsdn(tsd));
4393: 	for (i = 0, narenas = narenas_total_get(); i < narenas; i++) {
4394: 		arena_t *arena;
4395: 
4396: 		if ((arena = arena_get(tsd_tsdn(tsd), i, false)) != NULL) {
4397: 			arena_postfork_parent(tsd_tsdn(tsd), arena);
4398: 		}
4399: 	}
4400: 	prof_postfork_parent(tsd_tsdn(tsd));
4401: 	if (have_background_thread) {
4402: 		background_thread_postfork_parent(tsd_tsdn(tsd));
4403: 	}
4404: 	malloc_mutex_postfork_parent(tsd_tsdn(tsd), &arenas_lock);
4405: 	tcache_postfork_parent(tsd_tsdn(tsd));
4406: 	ctl_postfork_parent(tsd_tsdn(tsd));
4407: }
4408: 
4409: void
4410: jemalloc_postfork_child(void) {
4411: 	tsd_t *tsd;
4412: 	unsigned i, narenas;
4413: 
4414: 	assert(malloc_initialized());
4415: 
4416: 	tsd = tsd_fetch();
4417: 
4418: 	tsd_postfork_child(tsd);
4419: 
4420: 	witness_postfork_child(tsd_witness_tsdp_get(tsd));
4421: 	/* Release all mutexes, now that fork() has completed. */
4422: 	stats_postfork_child(tsd_tsdn(tsd));
4423: 	for (i = 0, narenas = narenas_total_get(); i < narenas; i++) {
4424: 		arena_t *arena;
4425: 
4426: 		if ((arena = arena_get(tsd_tsdn(tsd), i, false)) != NULL) {
4427: 			arena_postfork_child(tsd_tsdn(tsd), arena);
4428: 		}
4429: 	}
4430: 	prof_postfork_child(tsd_tsdn(tsd));
4431: 	if (have_background_thread) {
4432: 		background_thread_postfork_child(tsd_tsdn(tsd));
4433: 	}
4434: 	malloc_mutex_postfork_child(tsd_tsdn(tsd), &arenas_lock);
4435: 	tcache_postfork_child(tsd_tsdn(tsd));
4436: 	ctl_postfork_child(tsd_tsdn(tsd));
4437: }
4438: 
4439: /******************************************************************************/
[end of extension/jemalloc/jemalloc/src/jemalloc.c]
[start of extension/json/include/json_common.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // json_common.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/operator/cast_operators.hpp"
12: #include "duckdb/common/operator/decimal_cast_operators.hpp"
13: #include "duckdb/common/operator/string_cast.hpp"
14: #include "duckdb/planner/expression/bound_function_expression.hpp"
15: #include "yyjson.hpp"
16: 
17: using namespace duckdb_yyjson; // NOLINT
18: 
19: namespace duckdb {
20: 
21: //! JSON allocator is a custom allocator for yyjson that prevents many tiny allocations
22: class JSONAllocator {
23: public:
24: 	explicit JSONAllocator(Allocator &allocator)
25: 	    : arena_allocator(allocator), yyjson_allocator({Allocate, Reallocate, Free, &arena_allocator}) {
26: 	}
27: 
28: 	inline yyjson_alc *GetYYAlc() {
29: 		return &yyjson_allocator;
30: 	}
31: 
32: 	void Reset() {
33: 		arena_allocator.Reset();
34: 	}
35: 
36: private:
37: 	static inline void *Allocate(void *ctx, size_t size) {
38: 		auto alloc = (ArenaAllocator *)ctx;
39: 		return alloc->AllocateAligned(size);
40: 	}
41: 
42: 	static inline void *Reallocate(void *ctx, void *ptr, size_t old_size, size_t size) {
43: 		auto alloc = (ArenaAllocator *)ctx;
44: 		return alloc->ReallocateAligned(data_ptr_cast(ptr), old_size, size);
45: 	}
46: 
47: 	static inline void Free(void *ctx, void *ptr) {
48: 		// NOP because ArenaAllocator can't free
49: 	}
50: 
51: private:
52: 	ArenaAllocator arena_allocator;
53: 	yyjson_alc yyjson_allocator;
54: };
55: 
56: //! JSONKey / json_key_map_t speeds up mapping from JSON key to column ID
57: struct JSONKey {
58: 	const char *ptr;
59: 	size_t len;
60: };
61: 
62: struct JSONKeyHash {
63: 	inline std::size_t operator()(const JSONKey &k) const {
64: 		size_t result;
65: 		if (k.len >= sizeof(size_t)) {
66: 			memcpy(&result, k.ptr + k.len - sizeof(size_t), sizeof(size_t));
67: 		} else {
68: 			result = 0;
69: 			FastMemcpy(&result, k.ptr, k.len);
70: 		}
71: 		return result;
72: 	}
73: };
74: 
75: struct JSONKeyEquality {
76: 	inline bool operator()(const JSONKey &a, const JSONKey &b) const {
77: 		if (a.len != b.len) {
78: 			return false;
79: 		}
80: 		return FastMemcmp(a.ptr, b.ptr, a.len) == 0;
81: 	}
82: };
83: 
84: template <typename T>
85: using json_key_map_t = unordered_map<JSONKey, T, JSONKeyHash, JSONKeyEquality>;
86: using json_key_set_t = unordered_set<JSONKey, JSONKeyHash, JSONKeyEquality>;
87: 
88: //! Common JSON functionality for most JSON functions
89: struct JSONCommon {
90: public:
91: 	//! Read/Write flags
92: 	static constexpr auto READ_FLAG = YYJSON_READ_ALLOW_INF_AND_NAN | YYJSON_READ_ALLOW_TRAILING_COMMAS;
93: 	static constexpr auto READ_STOP_FLAG = READ_FLAG | YYJSON_READ_STOP_WHEN_DONE;
94: 	static constexpr auto READ_INSITU_FLAG = READ_STOP_FLAG | YYJSON_READ_INSITU;
95: 	static constexpr auto WRITE_FLAG = YYJSON_WRITE_ALLOW_INF_AND_NAN;
96: 	static constexpr auto WRITE_PRETTY_FLAG = YYJSON_WRITE_ALLOW_INF_AND_NAN | YYJSON_WRITE_PRETTY;
97: 
98: public:
99: 	//! Constant JSON type strings
100: 	static constexpr char const *TYPE_STRING_NULL = "NULL";
101: 	static constexpr char const *TYPE_STRING_BOOLEAN = "BOOLEAN";
102: 	static constexpr char const *TYPE_STRING_BIGINT = "BIGINT";
103: 	static constexpr char const *TYPE_STRING_UBIGINT = "UBIGINT";
104: 	static constexpr char const *TYPE_STRING_DOUBLE = "DOUBLE";
105: 	static constexpr char const *TYPE_STRING_VARCHAR = "VARCHAR";
106: 	static constexpr char const *TYPE_STRING_ARRAY = "ARRAY";
107: 	static constexpr char const *TYPE_STRING_OBJECT = "OBJECT";
108: 
109: 	static inline const char *ValTypeToString(yyjson_val *val) {
110: 		switch (yyjson_get_tag(val)) {
111: 		case YYJSON_TYPE_NULL | YYJSON_SUBTYPE_NONE:
112: 			return JSONCommon::TYPE_STRING_NULL;
113: 		case YYJSON_TYPE_STR | YYJSON_SUBTYPE_NOESC:
114: 		case YYJSON_TYPE_STR | YYJSON_SUBTYPE_NONE:
115: 			return JSONCommon::TYPE_STRING_VARCHAR;
116: 		case YYJSON_TYPE_ARR | YYJSON_SUBTYPE_NONE:
117: 			return JSONCommon::TYPE_STRING_ARRAY;
118: 		case YYJSON_TYPE_OBJ | YYJSON_SUBTYPE_NONE:
119: 			return JSONCommon::TYPE_STRING_OBJECT;
120: 		case YYJSON_TYPE_BOOL | YYJSON_SUBTYPE_TRUE:
121: 		case YYJSON_TYPE_BOOL | YYJSON_SUBTYPE_FALSE:
122: 			return JSONCommon::TYPE_STRING_BOOLEAN;
123: 		case YYJSON_TYPE_NUM | YYJSON_SUBTYPE_UINT:
124: 			return JSONCommon::TYPE_STRING_UBIGINT;
125: 		case YYJSON_TYPE_NUM | YYJSON_SUBTYPE_SINT:
126: 			return JSONCommon::TYPE_STRING_BIGINT;
127: 		case YYJSON_TYPE_NUM | YYJSON_SUBTYPE_REAL:
128: 			return JSONCommon::TYPE_STRING_DOUBLE;
129: 		default:
130: 			throw InternalException("Unexpected yyjson tag in ValTypeToString");
131: 		}
132: 	}
133: 
134: 	static inline string_t ValTypeToStringT(yyjson_val *val) {
135: 		return string_t(ValTypeToString(val));
136: 	}
137: 
138: 	static inline LogicalTypeId ValTypeToLogicalTypeId(yyjson_val *val) {
139: 		switch (yyjson_get_tag(val)) {
140: 		case YYJSON_TYPE_NULL | YYJSON_SUBTYPE_NONE:
141: 			return LogicalTypeId::SQLNULL;
142: 		case YYJSON_TYPE_STR | YYJSON_SUBTYPE_NOESC:
143: 		case YYJSON_TYPE_STR | YYJSON_SUBTYPE_NONE:
144: 			return LogicalTypeId::VARCHAR;
145: 		case YYJSON_TYPE_ARR | YYJSON_SUBTYPE_NONE:
146: 			return LogicalTypeId::LIST;
147: 		case YYJSON_TYPE_OBJ | YYJSON_SUBTYPE_NONE:
148: 			return LogicalTypeId::STRUCT;
149: 		case YYJSON_TYPE_BOOL | YYJSON_SUBTYPE_TRUE:
150: 		case YYJSON_TYPE_BOOL | YYJSON_SUBTYPE_FALSE:
151: 			return LogicalTypeId::BOOLEAN;
152: 		case YYJSON_TYPE_NUM | YYJSON_SUBTYPE_UINT:
153: 			return LogicalTypeId::UBIGINT;
154: 		case YYJSON_TYPE_NUM | YYJSON_SUBTYPE_SINT:
155: 			return LogicalTypeId::BIGINT;
156: 		case YYJSON_TYPE_NUM | YYJSON_SUBTYPE_REAL:
157: 			return LogicalTypeId::DOUBLE;
158: 		default:
159: 			throw InternalException("Unexpected yyjson tag in ValTypeToLogicalTypeId");
160: 		}
161: 	}
162: 
163: public:
164: 	//===--------------------------------------------------------------------===//
165: 	// Document creation / reading / writing
166: 	//===--------------------------------------------------------------------===//
167: 	template <class T>
168: 	static T *AllocateArray(yyjson_alc *alc, idx_t count) {
169: 		return reinterpret_cast<T *>(alc->malloc(alc->ctx, sizeof(T) * count));
170: 	}
171: 
172: 	template <class T>
173: 	static T *AllocateArray(yyjson_mut_doc *doc, idx_t count) {
174: 		return AllocateArray<T>(&doc->alc, count);
175: 	}
176: 
177: 	static inline yyjson_mut_doc *CreateDocument(yyjson_alc *alc) {
178: 		D_ASSERT(alc);
179: 		return yyjson_mut_doc_new(alc);
180: 	}
181: 	static inline yyjson_doc *ReadDocumentUnsafe(char *data, idx_t size, const yyjson_read_flag flg, yyjson_alc *alc,
182: 	                                             yyjson_read_err *err = nullptr) {
183: 		D_ASSERT(alc);
184: 		return yyjson_read_opts(data, size, flg, alc, err);
185: 	}
186: 	static inline yyjson_doc *ReadDocumentUnsafe(const string_t &input, const yyjson_read_flag flg, yyjson_alc *alc,
187: 	                                             yyjson_read_err *err = nullptr) {
188: 		return ReadDocumentUnsafe(input.GetDataWriteable(), input.GetSize(), flg, alc, err);
189: 	}
190: 	static inline yyjson_doc *ReadDocument(char *data, idx_t size, const yyjson_read_flag flg, yyjson_alc *alc) {
191: 		yyjson_read_err error;
192: 		auto result = ReadDocumentUnsafe(data, size, flg, alc, &error);
193: 		if (error.code != YYJSON_READ_SUCCESS) {
194: 			ThrowParseError(data, size, error);
195: 		}
196: 		return result;
197: 	}
198: 	static inline yyjson_doc *ReadDocument(const string_t &input, const yyjson_read_flag flg, yyjson_alc *alc) {
199: 		return ReadDocument(input.GetDataWriteable(), input.GetSize(), flg, alc);
200: 	}
201: 
202: 	static string FormatParseError(const char *data, idx_t length, yyjson_read_err &error, const string &extra = "") {
203: 		D_ASSERT(error.code != YYJSON_READ_SUCCESS);
204: 		// Truncate, so we don't print megabytes worth of JSON
205: 		string input = length > 50 ? string(data, 47) + "..." : string(data, length);
206: 		// Have to replace \r, otherwise output is unreadable
207: 		input = StringUtil::Replace(input, "\r", "\\r");
208: 		return StringUtil::Format("Malformed JSON at byte %lld of input: %s. %s Input: %s", error.pos, error.msg, extra,
209: 		                          input);
210: 	}
211: 	static void ThrowParseError(const char *data, idx_t length, yyjson_read_err &error, const string &extra = "") {
212: 		throw InvalidInputException(FormatParseError(data, length, error, extra));
213: 	}
214: 
215: 	template <class YYJSON_VAL_T>
216: 	static inline char *WriteVal(YYJSON_VAL_T *val, yyjson_alc *alc, idx_t &len) {
217: 		throw InternalException("Unknown yyjson val type");
218: 	}
219: 	template <class YYJSON_VAL_T>
220: 	static inline string_t WriteVal(YYJSON_VAL_T *val, yyjson_alc *alc) {
221: 		D_ASSERT(alc);
222: 		idx_t len;
223: 		auto data = WriteVal<YYJSON_VAL_T>(val, alc, len);
224: 		return string_t(data, len);
225: 	}
226: 
227: 	//! Slow and easy ToString for errors
228: 	static string ValToString(yyjson_val *val, idx_t max_len = DConstants::INVALID_INDEX);
229: 	//! Throw an error with the printed yyjson_val
230: 	static void ThrowValFormatError(string error_string, yyjson_val *val);
231: 
232: public:
233: 	//===--------------------------------------------------------------------===//
234: 	// JSON pointer / path
235: 	//===--------------------------------------------------------------------===//
236: 	enum class JSONPathType : uint8_t {
237: 		//! Extract a single value
238: 		REGULAR = 0,
239: 		//! Extract multiple values (when we have a '*' wildcard in the JSON Path)
240: 		WILDCARD = 1,
241: 	};
242: 
243: 	//! Get JSON value using JSON path query (safe, checks the path query)
244: 	static inline yyjson_val *Get(yyjson_val *val, const string_t &path_str) {
245: 		auto ptr = path_str.GetData();
246: 		auto len = path_str.GetSize();
247: 		if (len == 0) {
248: 			return GetUnsafe(val, ptr, len);
249: 		}
250: 		switch (*ptr) {
251: 		case '/': {
252: 			// '/' notation must be '\0'-terminated
253: 			auto str = string(ptr, len);
254: 			return GetUnsafe(val, str.c_str(), len);
255: 		}
256: 		case '$': {
257: 			if (ValidatePath(ptr, len, false) == JSONPathType::WILDCARD) {
258: 				throw InvalidInputException(
259: 				    "JSON path cannot contain wildcards if the path is not a constant parameter");
260: 			}
261: 			return GetUnsafe(val, ptr, len);
262: 		}
263: 		default:
264: 			auto str = "/" + string(ptr, len);
265: 			return GetUnsafe(val, str.c_str(), len + 1);
266: 		}
267: 	}
268: 
269: 	//! Get JSON value using JSON path query (unsafe)
270: 	static inline yyjson_val *GetUnsafe(yyjson_val *val, const char *ptr, const idx_t &len) {
271: 		if (len == 0) {
272: 			return nullptr;
273: 		}
274: 		switch (*ptr) {
275: 		case '/':
276: 			return GetPointer(val, ptr, len);
277: 		case '$':
278: 			return GetPath(val, ptr, len);
279: 		default:
280: 			throw InternalException("JSON pointer/path does not start with '/' or '$'");
281: 		}
282: 	}
283: 
284: 	//! Get JSON value using JSON path query (unsafe)
285: 	static void GetWildcardPath(yyjson_val *val, const char *ptr, const idx_t &len, vector<yyjson_val *> &vals);
286: 
287: 	//! Validate JSON Path ($.field[index]... syntax), returns true if there are wildcards in the path
288: 	static JSONPathType ValidatePath(const char *ptr, const idx_t &len, const bool binder);
289: 
290: private:
291: 	//! Get JSON pointer (/field/index/... syntax)
292: 	static inline yyjson_val *GetPointer(yyjson_val *val, const char *ptr, const idx_t &len) {
293: 		yyjson_ptr_err err;
294: 		return len == 1 ? val : unsafe_yyjson_ptr_getx(val, ptr, len, &err);
295: 	}
296: 	//! Get JSON path ($.field[index]... syntax)
297: 	static yyjson_val *GetPath(yyjson_val *val, const char *ptr, const idx_t &len);
298: };
299: 
300: template <>
301: inline char *JSONCommon::WriteVal(yyjson_val *val, yyjson_alc *alc, idx_t &len) {
302: 	return yyjson_val_write_opts(val, JSONCommon::WRITE_FLAG, alc, reinterpret_cast<size_t *>(&len), nullptr);
303: }
304: template <>
305: inline char *JSONCommon::WriteVal(yyjson_mut_val *val, yyjson_alc *alc, idx_t &len) {
306: 	return yyjson_mut_val_write_opts(val, JSONCommon::WRITE_FLAG, alc, reinterpret_cast<size_t *>(&len), nullptr);
307: }
308: 
309: } // namespace duckdb
[end of extension/json/include/json_common.hpp]
[start of extension/json/include/json_executors.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // json_executors.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/execution/expression_executor.hpp"
12: #include "json_functions.hpp"
13: 
14: namespace duckdb {
15: 
16: template <class T>
17: using json_function_t = std::function<T(yyjson_val *, yyjson_alc *, Vector &, ValidityMask &, idx_t)>;
18: 
19: struct JSONExecutors {
20: public:
21: 	//! Single-argument JSON read function, i.e. json_type('[1, 2, 3]')
22: 	template <class T>
23: 	static void UnaryExecute(DataChunk &args, ExpressionState &state, Vector &result, const json_function_t<T> fun) {
24: 		auto &lstate = JSONFunctionLocalState::ResetAndGet(state);
25: 		auto alc = lstate.json_allocator.GetYYAlc();
26: 
27: 		auto &inputs = args.data[0];
28: 		UnaryExecutor::ExecuteWithNulls<string_t, T>(
29: 		    inputs, result, args.size(), [&](string_t input, ValidityMask &mask, idx_t idx) {
30: 			    auto doc = JSONCommon::ReadDocument(input, JSONCommon::READ_FLAG, alc);
31: 			    return fun(doc->root, alc, result, mask, idx);
32: 		    });
33: 	}
34: 
35: 	//! Two-argument JSON read function (with path query), i.e. json_type('[1, 2, 3]', '$[0]')
36: 	template <class T, bool SET_NULL_IF_NOT_FOUND = true>
37: 	static void BinaryExecute(DataChunk &args, ExpressionState &state, Vector &result, const json_function_t<T> fun) {
38: 		auto &func_expr = state.expr.Cast<BoundFunctionExpression>();
39: 		const auto &info = func_expr.bind_info->Cast<JSONReadFunctionData>();
40: 		auto &lstate = JSONFunctionLocalState::ResetAndGet(state);
41: 		auto alc = lstate.json_allocator.GetYYAlc();
42: 
43: 		auto &inputs = args.data[0];
44: 		if (info.constant) { // Constant path
45: 			const char *ptr = info.ptr;
46: 			const idx_t &len = info.len;
47: 			if (info.path_type == JSONCommon::JSONPathType::REGULAR) {
48: 				UnaryExecutor::ExecuteWithNulls<string_t, T>(
49: 				    inputs, result, args.size(), [&](string_t input, ValidityMask &mask, idx_t idx) {
50: 					    auto doc =
51: 					        JSONCommon::ReadDocument(input, JSONCommon::READ_FLAG, lstate.json_allocator.GetYYAlc());
52: 					    auto val = JSONCommon::GetUnsafe(doc->root, ptr, len);
53: 					    if (SET_NULL_IF_NOT_FOUND && !val) {
54: 						    mask.SetInvalid(idx);
55: 						    return T {};
56: 					    } else {
57: 						    return fun(val, alc, result, mask, idx);
58: 					    }
59: 				    });
60: 			} else {
61: 				D_ASSERT(info.path_type == JSONCommon::JSONPathType::WILDCARD);
62: 				vector<yyjson_val *> vals;
63: 				UnaryExecutor::Execute<string_t, list_entry_t>(inputs, result, args.size(), [&](string_t input) {
64: 					vals.clear();
65: 
66: 					auto doc = JSONCommon::ReadDocument(input, JSONCommon::READ_FLAG, lstate.json_allocator.GetYYAlc());
67: 					JSONCommon::GetWildcardPath(doc->root, ptr, len, vals);
68: 
69: 					auto current_size = ListVector::GetListSize(result);
70: 					auto new_size = current_size + vals.size();
71: 					if (ListVector::GetListCapacity(result) < new_size) {
72: 						ListVector::Reserve(result, new_size);
73: 					}
74: 
75: 					auto &child_entry = ListVector::GetEntry(result);
76: 					auto child_vals = FlatVector::GetData<T>(child_entry);
77: 					auto &child_validity = FlatVector::Validity(child_entry);
78: 					for (idx_t i = 0; i < vals.size(); i++) {
79: 						auto &val = vals[i];
80: 						D_ASSERT(val != nullptr); // Wildcard extract shouldn't give back nullptrs
81: 						child_vals[current_size + i] = fun(val, alc, result, child_validity, current_size + i);
82: 					}
83: 
84: 					ListVector::SetListSize(result, new_size);
85: 
86: 					return list_entry_t {current_size, vals.size()};
87: 				});
88: 			}
89: 		} else { // Columnref path
90: 			D_ASSERT(info.path_type == JSONCommon::JSONPathType::REGULAR);
91: 			auto &paths = args.data[1];
92: 			BinaryExecutor::ExecuteWithNulls<string_t, string_t, T>(
93: 			    inputs, paths, result, args.size(), [&](string_t input, string_t path, ValidityMask &mask, idx_t idx) {
94: 				    auto doc = JSONCommon::ReadDocument(input, JSONCommon::READ_FLAG, lstate.json_allocator.GetYYAlc());
95: 				    auto val = JSONCommon::Get(doc->root, path);
96: 				    if (SET_NULL_IF_NOT_FOUND && !val) {
97: 					    mask.SetInvalid(idx);
98: 					    return T {};
99: 				    } else {
100: 					    return fun(val, alc, result, mask, idx);
101: 				    }
102: 			    });
103: 		}
104: 		if (args.AllConstant()) {
105: 			result.SetVectorType(VectorType::CONSTANT_VECTOR);
106: 		}
107: 	}
108: 
109: 	//! JSON read function with list of path queries, i.e. json_type('[1, 2, 3]', ['$[0]', '$[1]'])
110: 	template <class T, bool SET_NULL_IF_NOT_FOUND = true>
111: 	static void ExecuteMany(DataChunk &args, ExpressionState &state, Vector &result, const json_function_t<T> fun) {
112: 		auto &func_expr = state.expr.Cast<BoundFunctionExpression>();
113: 		const auto &info = func_expr.bind_info->Cast<JSONReadManyFunctionData>();
114: 		auto &lstate = JSONFunctionLocalState::ResetAndGet(state);
115: 		auto alc = lstate.json_allocator.GetYYAlc();
116: 		D_ASSERT(info.ptrs.size() == info.lens.size());
117: 
118: 		const auto count = args.size();
119: 		const idx_t num_paths = info.ptrs.size();
120: 		const idx_t list_size = count * num_paths;
121: 
122: 		UnifiedVectorFormat input_data;
123: 		auto &input_vector = args.data[0];
124: 		input_vector.ToUnifiedFormat(count, input_data);
125: 		auto inputs = UnifiedVectorFormat::GetData<string_t>(input_data);
126: 
127: 		ListVector::Reserve(result, list_size);
128: 		auto list_entries = FlatVector::GetData<list_entry_t>(result);
129: 		auto &list_validity = FlatVector::Validity(result);
130: 
131: 		auto &child = ListVector::GetEntry(result);
132: 		auto child_data = FlatVector::GetData<T>(child);
133: 		auto &child_validity = FlatVector::Validity(child);
134: 
135: 		idx_t offset = 0;
136: 		yyjson_val *val;
137: 		for (idx_t i = 0; i < count; i++) {
138: 			auto idx = input_data.sel->get_index(i);
139: 			if (!input_data.validity.RowIsValid(idx)) {
140: 				list_validity.SetInvalid(i);
141: 				continue;
142: 			}
143: 
144: 			auto doc = JSONCommon::ReadDocument(inputs[idx], JSONCommon::READ_FLAG, lstate.json_allocator.GetYYAlc());
145: 			for (idx_t path_i = 0; path_i < num_paths; path_i++) {
146: 				auto child_idx = offset + path_i;
147: 				val = JSONCommon::GetUnsafe(doc->root, info.ptrs[path_i], info.lens[path_i]);
148: 				if (SET_NULL_IF_NOT_FOUND && !val) {
149: 					child_validity.SetInvalid(child_idx);
150: 				} else {
151: 					child_data[child_idx] = fun(val, alc, child, child_validity, child_idx);
152: 				}
153: 			}
154: 
155: 			list_entries[i].offset = offset;
156: 			list_entries[i].length = num_paths;
157: 			offset += num_paths;
158: 		}
159: 		ListVector::SetListSize(result, offset);
160: 
161: 		if (args.AllConstant()) {
162: 			result.SetVectorType(VectorType::CONSTANT_VECTOR);
163: 		}
164: 	}
165: };
166: 
167: } // namespace duckdb
[end of extension/json/include/json_executors.hpp]
[start of extension/json/json_extension.cpp]
1: #define DUCKDB_EXTENSION_MAIN
2: #include "json_extension.hpp"
3: 
4: #include "duckdb/catalog/catalog_entry/macro_catalog_entry.hpp"
5: #include "duckdb/catalog/default/default_functions.hpp"
6: #include "duckdb/common/string_util.hpp"
7: #include "duckdb/function/copy_function.hpp"
8: #include "duckdb/main/extension_util.hpp"
9: #include "duckdb/parser/expression/constant_expression.hpp"
10: #include "duckdb/parser/expression/function_expression.hpp"
11: #include "duckdb/parser/parsed_data/create_pragma_function_info.hpp"
12: #include "duckdb/parser/parsed_data/create_type_info.hpp"
13: #include "duckdb/parser/tableref/table_function_ref.hpp"
14: #include "json_common.hpp"
15: #include "json_functions.hpp"
16: 
17: namespace duckdb {
18: 
19: static DefaultMacro json_macros[] = {
20:     {DEFAULT_SCHEMA, "json_group_array", {"x", nullptr}, {{nullptr, nullptr}}, "to_json(list(x))"},
21:     {DEFAULT_SCHEMA,
22:      "json_group_object",
23:      {"name", "value", nullptr},
24:      {{nullptr, nullptr}},
25:      "to_json(map(list(name), list(value)))"},
26:     {DEFAULT_SCHEMA,
27:      "json_group_structure",
28:      {"x", nullptr},
29:      {{nullptr, nullptr}},
30:      "json_structure(json_group_array(x))->'0'"},
31:     {DEFAULT_SCHEMA, "json", {"x", nullptr}, {{nullptr, nullptr}}, "json_extract(x, '$')"},
32:     {nullptr, nullptr, {nullptr}, {{nullptr, nullptr}}, nullptr}};
33: 
34: void JsonExtension::Load(DuckDB &db) {
35: 	auto &db_instance = *db.instance;
36: 	// JSON type
37: 	auto json_type = LogicalType::JSON();
38: 	ExtensionUtil::RegisterType(db_instance, LogicalType::JSON_TYPE_NAME, std::move(json_type));
39: 
40: 	// JSON casts
41: 	JSONFunctions::RegisterSimpleCastFunctions(DBConfig::GetConfig(db_instance).GetCastFunctions());
42: 	JSONFunctions::RegisterJSONCreateCastFunctions(DBConfig::GetConfig(db_instance).GetCastFunctions());
43: 	JSONFunctions::RegisterJSONTransformCastFunctions(DBConfig::GetConfig(db_instance).GetCastFunctions());
44: 
45: 	// JSON scalar functions
46: 	for (auto &fun : JSONFunctions::GetScalarFunctions()) {
47: 		ExtensionUtil::RegisterFunction(db_instance, fun);
48: 	}
49: 
50: 	// JSON table functions
51: 	for (auto &fun : JSONFunctions::GetTableFunctions()) {
52: 		ExtensionUtil::RegisterFunction(db_instance, fun);
53: 	}
54: 
55: 	// JSON pragma functions
56: 	for (auto &fun : JSONFunctions::GetPragmaFunctions()) {
57: 		ExtensionUtil::RegisterFunction(db_instance, fun);
58: 	}
59: 
60: 	// JSON replacement scan
61: 	auto &config = DBConfig::GetConfig(*db.instance);
62: 	config.replacement_scans.emplace_back(JSONFunctions::ReadJSONReplacement);
63: 
64: 	// JSON copy function
65: 	auto copy_fun = JSONFunctions::GetJSONCopyFunction();
66: 	ExtensionUtil::RegisterFunction(db_instance, std::move(copy_fun));
67: 
68: 	// JSON macro's
69: 	for (idx_t index = 0; json_macros[index].name != nullptr; index++) {
70: 		auto info = DefaultFunctionGenerator::CreateInternalMacroInfo(json_macros[index]);
71: 		ExtensionUtil::RegisterFunction(db_instance, *info);
72: 	}
73: }
74: 
75: std::string JsonExtension::Name() {
76: 	return "json";
77: }
78: 
79: std::string JsonExtension::Version() const {
80: #ifdef EXT_VERSION_JSON
81: 	return EXT_VERSION_JSON;
82: #else
83: 	return "";
84: #endif
85: }
86: 
87: } // namespace duckdb
88: 
89: extern "C" {
90: 
91: DUCKDB_EXTENSION_API void json_init(duckdb::DatabaseInstance &db) {
92: 	duckdb::DuckDB db_wrapper(db);
93: 	db_wrapper.LoadExtension<duckdb::JsonExtension>();
94: }
95: 
96: DUCKDB_EXTENSION_API const char *json_version() {
97: 	return duckdb::DuckDB::LibraryVersion();
98: }
99: }
100: 
101: #ifndef DUCKDB_EXTENSION_MAIN
102: #error DUCKDB_EXTENSION_MAIN not defined
103: #endif
[end of extension/json/json_extension.cpp]
[start of extension/json/json_functions.cpp]
1: #include "json_functions.hpp"
2: 
3: #include "duckdb/common/file_system.hpp"
4: #include "duckdb/execution/expression_executor.hpp"
5: #include "duckdb/function/cast/cast_function_set.hpp"
6: #include "duckdb/function/cast/default_casts.hpp"
7: #include "duckdb/function/replacement_scan.hpp"
8: #include "duckdb/parser/expression/constant_expression.hpp"
9: #include "duckdb/parser/expression/function_expression.hpp"
10: #include "duckdb/parser/parsed_data/create_pragma_function_info.hpp"
11: #include "duckdb/parser/tableref/table_function_ref.hpp"
12: 
13: namespace duckdb {
14: 
15: using JSONPathType = JSONCommon::JSONPathType;
16: 
17: static JSONPathType CheckPath(const Value &path_val, string &path, size_t &len) {
18: 	if (path_val.IsNull()) {
19: 		throw BinderException("JSON path cannot be NULL");
20: 	}
21: 	const auto path_str_val = path_val.DefaultCastAs(LogicalType::VARCHAR);
22: 	auto path_str = path_str_val.GetValueUnsafe<string_t>();
23: 	len = path_str.GetSize();
24: 	auto ptr = path_str.GetData();
25: 	// Empty strings and invalid $ paths yield an error
26: 	if (len == 0) {
27: 		throw BinderException("Empty JSON path");
28: 	}
29: 	JSONPathType path_type = JSONPathType::REGULAR;
30: 	if (*ptr == '$') {
31: 		path_type = JSONCommon::ValidatePath(ptr, len, true);
32: 	}
33: 	// Copy over string to the bind data
34: 	if (*ptr == '/' || *ptr == '$') {
35: 		path = string(ptr, len);
36: 	} else {
37: 		path = "/" + string(ptr, len);
38: 		len++;
39: 	}
40: 	return path_type;
41: }
42: 
43: JSONReadFunctionData::JSONReadFunctionData(bool constant, string path_p, idx_t len, JSONPathType path_type_p)
44:     : constant(constant), path(std::move(path_p)), path_type(path_type_p), ptr(path.c_str()), len(len) {
45: }
46: 
47: unique_ptr<FunctionData> JSONReadFunctionData::Copy() const {
48: 	return make_uniq<JSONReadFunctionData>(constant, path, len, path_type);
49: }
50: 
51: bool JSONReadFunctionData::Equals(const FunctionData &other_p) const {
52: 	auto &other = other_p.Cast<JSONReadFunctionData>();
53: 	return constant == other.constant && path == other.path && len == other.len && path_type == other.path_type;
54: }
55: 
56: unique_ptr<FunctionData> JSONReadFunctionData::Bind(ClientContext &context, ScalarFunction &bound_function,
57:                                                     vector<unique_ptr<Expression>> &arguments) {
58: 	D_ASSERT(bound_function.arguments.size() == 2);
59: 	bool constant = false;
60: 	string path;
61: 	size_t len = 0;
62: 	JSONPathType path_type = JSONPathType::REGULAR;
63: 	if (arguments[1]->IsFoldable()) {
64: 		const auto path_val = ExpressionExecutor::EvaluateScalar(context, *arguments[1]);
65: 		if (!path_val.IsNull()) {
66: 			constant = true;
67: 			path_type = CheckPath(path_val, path, len);
68: 		}
69: 	}
70: 	bound_function.arguments[1] = LogicalType::VARCHAR;
71: 	if (path_type == JSONCommon::JSONPathType::WILDCARD) {
72: 		bound_function.return_type = LogicalType::LIST(bound_function.return_type);
73: 	}
74: 	return make_uniq<JSONReadFunctionData>(constant, std::move(path), len, path_type);
75: }
76: 
77: JSONReadManyFunctionData::JSONReadManyFunctionData(vector<string> paths_p, vector<size_t> lens_p)
78:     : paths(std::move(paths_p)), lens(std::move(lens_p)) {
79: 	for (const auto &path : paths) {
80: 		ptrs.push_back(path.c_str());
81: 	}
82: }
83: 
84: unique_ptr<FunctionData> JSONReadManyFunctionData::Copy() const {
85: 	return make_uniq<JSONReadManyFunctionData>(paths, lens);
86: }
87: 
88: bool JSONReadManyFunctionData::Equals(const FunctionData &other_p) const {
89: 	auto &other = other_p.Cast<JSONReadManyFunctionData>();
90: 	return paths == other.paths && lens == other.lens;
91: }
92: 
93: unique_ptr<FunctionData> JSONReadManyFunctionData::Bind(ClientContext &context, ScalarFunction &bound_function,
94:                                                         vector<unique_ptr<Expression>> &arguments) {
95: 	D_ASSERT(bound_function.arguments.size() == 2);
96: 	if (arguments[1]->HasParameter()) {
97: 		throw ParameterNotResolvedException();
98: 	}
99: 	if (!arguments[1]->IsFoldable()) {
100: 		throw BinderException("List of paths must be constant");
101: 	}
102: 
103: 	vector<string> paths;
104: 	vector<size_t> lens;
105: 	auto paths_val = ExpressionExecutor::EvaluateScalar(context, *arguments[1]);
106: 
107: 	for (auto &path_val : ListValue::GetChildren(paths_val)) {
108: 		paths.emplace_back("");
109: 		lens.push_back(0);
110: 		if (CheckPath(path_val, paths.back(), lens.back()) == JSONPathType::WILDCARD) {
111: 			throw BinderException("Cannot have wildcards in JSON path when supplying multiple paths");
112: 		}
113: 	}
114: 
115: 	return make_uniq<JSONReadManyFunctionData>(std::move(paths), std::move(lens));
116: }
117: 
118: JSONFunctionLocalState::JSONFunctionLocalState(Allocator &allocator) : json_allocator(allocator) {
119: }
120: JSONFunctionLocalState::JSONFunctionLocalState(ClientContext &context)
121:     : JSONFunctionLocalState(BufferAllocator::Get(context)) {
122: }
123: 
124: unique_ptr<FunctionLocalState> JSONFunctionLocalState::Init(ExpressionState &state, const BoundFunctionExpression &expr,
125:                                                             FunctionData *bind_data) {
126: 	return make_uniq<JSONFunctionLocalState>(state.GetContext());
127: }
128: 
129: unique_ptr<FunctionLocalState> JSONFunctionLocalState::InitCastLocalState(CastLocalStateParameters &parameters) {
130: 	return parameters.context ? make_uniq<JSONFunctionLocalState>(*parameters.context)
131: 	                          : make_uniq<JSONFunctionLocalState>(Allocator::DefaultAllocator());
132: }
133: 
134: JSONFunctionLocalState &JSONFunctionLocalState::ResetAndGet(ExpressionState &state) {
135: 	auto &lstate = ExecuteFunctionState::GetFunctionState(state)->Cast<JSONFunctionLocalState>();
136: 	lstate.json_allocator.Reset();
137: 	return lstate;
138: }
139: 
140: vector<ScalarFunctionSet> JSONFunctions::GetScalarFunctions() {
141: 	vector<ScalarFunctionSet> functions;
142: 
143: 	// Extract functions
144: 	AddAliases({"json_extract", "json_extract_path"}, GetExtractFunction(), functions);
145: 	AddAliases({"json_extract_string", "json_extract_path_text", "->>"}, GetExtractStringFunction(), functions);
146: 
147: 	// Create functions
148: 	functions.push_back(GetArrayFunction());
149: 	functions.push_back(GetObjectFunction());
150: 	AddAliases({"to_json", "json_quote"}, GetToJSONFunction(), functions);
151: 	functions.push_back(GetArrayToJSONFunction());
152: 	functions.push_back(GetRowToJSONFunction());
153: 	functions.push_back(GetMergePatchFunction());
154: 
155: 	// Structure/Transform
156: 	functions.push_back(GetStructureFunction());
157: 	AddAliases({"json_transform", "from_json"}, GetTransformFunction(), functions);
158: 	AddAliases({"json_transform_strict", "from_json_strict"}, GetTransformStrictFunction(), functions);
159: 
160: 	// Other
161: 	functions.push_back(GetArrayLengthFunction());
162: 	functions.push_back(GetContainsFunction());
163: 	functions.push_back(GetExistsFunction());
164: 	functions.push_back(GetKeysFunction());
165: 	functions.push_back(GetTypeFunction());
166: 	functions.push_back(GetValidFunction());
167: 	functions.push_back(GetValueFunction());
168: 	functions.push_back(GetSerializePlanFunction());
169: 	functions.push_back(GetSerializeSqlFunction());
170: 	functions.push_back(GetDeserializeSqlFunction());
171: 
172: 	functions.push_back(GetPrettyPrintFunction());
173: 
174: 	return functions;
175: }
176: 
177: vector<PragmaFunctionSet> JSONFunctions::GetPragmaFunctions() {
178: 	vector<PragmaFunctionSet> functions;
179: 	functions.push_back(GetExecuteJsonSerializedSqlPragmaFunction());
180: 	return functions;
181: }
182: 
183: vector<TableFunctionSet> JSONFunctions::GetTableFunctions() {
184: 	vector<TableFunctionSet> functions;
185: 
186: 	// Reads JSON as string
187: 	functions.push_back(GetReadJSONObjectsFunction());
188: 	functions.push_back(GetReadNDJSONObjectsFunction());
189: 	functions.push_back(GetReadJSONObjectsAutoFunction());
190: 
191: 	// Read JSON as columnar data
192: 	functions.push_back(GetReadJSONFunction());
193: 	functions.push_back(GetReadNDJSONFunction());
194: 	functions.push_back(GetReadJSONAutoFunction());
195: 	functions.push_back(GetReadNDJSONAutoFunction());
196: 	functions.push_back(GetExecuteJsonSerializedSqlFunction());
197: 
198: 	return functions;
199: }
200: 
201: unique_ptr<TableRef> JSONFunctions::ReadJSONReplacement(ClientContext &context, ReplacementScanInput &input,
202:                                                         optional_ptr<ReplacementScanData> data) {
203: 	auto table_name = ReplacementScan::GetFullPath(input);
204: 	if (!ReplacementScan::CanReplace(table_name, {"json", "jsonl", "ndjson"})) {
205: 		return nullptr;
206: 	}
207: 	auto table_function = make_uniq<TableFunctionRef>();
208: 	vector<unique_ptr<ParsedExpression>> children;
209: 	children.push_back(make_uniq<ConstantExpression>(Value(table_name)));
210: 	table_function->function = make_uniq<FunctionExpression>("read_json_auto", std::move(children));
211: 
212: 	if (!FileSystem::HasGlob(table_name)) {
213: 		auto &fs = FileSystem::GetFileSystem(context);
214: 		table_function->alias = fs.ExtractBaseName(table_name);
215: 	}
216: 
217: 	return std::move(table_function);
218: }
219: 
220: static bool CastVarcharToJSON(Vector &source, Vector &result, idx_t count, CastParameters &parameters) {
221: 	auto &lstate = parameters.local_state->Cast<JSONFunctionLocalState>();
222: 	lstate.json_allocator.Reset();
223: 	auto alc = lstate.json_allocator.GetYYAlc();
224: 
225: 	bool success = true;
226: 	UnaryExecutor::ExecuteWithNulls<string_t, string_t>(
227: 	    source, result, count, [&](string_t input, ValidityMask &mask, idx_t idx) {
228: 		    auto data = input.GetDataWriteable();
229: 		    const auto length = input.GetSize();
230: 
231: 		    yyjson_read_err error;
232: 		    auto doc = JSONCommon::ReadDocumentUnsafe(data, length, JSONCommon::READ_FLAG, alc, &error);
233: 
234: 		    if (!doc) {
235: 			    mask.SetInvalid(idx);
236: 			    if (success) {
237: 				    HandleCastError::AssignError(JSONCommon::FormatParseError(data, length, error), parameters);
238: 				    success = false;
239: 			    }
240: 		    }
241: 
242: 		    return input;
243: 	    });
244: 	StringVector::AddHeapReference(result, source);
245: 	return success;
246: }
247: 
248: void JSONFunctions::RegisterSimpleCastFunctions(CastFunctionSet &casts) {
249: 	// JSON to VARCHAR is basically free
250: 	casts.RegisterCastFunction(LogicalType::JSON(), LogicalType::VARCHAR, DefaultCasts::ReinterpretCast, 1);
251: 
252: 	// VARCHAR to JSON requires a parse so it's not free. Let's make it 1 more than a cast to STRUCT
253: 	auto varchar_to_json_cost = casts.ImplicitCastCost(LogicalType::SQLNULL, LogicalTypeId::STRUCT) + 1;
254: 	BoundCastInfo varchar_to_json_info(CastVarcharToJSON, nullptr, JSONFunctionLocalState::InitCastLocalState);
255: 	casts.RegisterCastFunction(LogicalType::VARCHAR, LogicalType::JSON(), std::move(varchar_to_json_info),
256: 	                           varchar_to_json_cost);
257: 
258: 	// Register NULL to JSON with a different cost than NULL to VARCHAR so the binder can disambiguate functions
259: 	auto null_to_json_cost = casts.ImplicitCastCost(LogicalType::SQLNULL, LogicalTypeId::VARCHAR) + 1;
260: 	casts.RegisterCastFunction(LogicalType::SQLNULL, LogicalType::JSON(), DefaultCasts::TryVectorNullCast,
261: 	                           null_to_json_cost);
262: }
263: 
264: } // namespace duckdb
[end of extension/json/json_functions.cpp]
[start of src/common/types/column/column_data_allocator.cpp]
1: #include "duckdb/common/types/column/column_data_allocator.hpp"
2: 
3: #include "duckdb/common/types/column/column_data_collection_segment.hpp"
4: #include "duckdb/storage/buffer/block_handle.hpp"
5: #include "duckdb/storage/buffer_manager.hpp"
6: 
7: namespace duckdb {
8: 
9: ColumnDataAllocator::ColumnDataAllocator(Allocator &allocator) : type(ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR) {
10: 	alloc.allocator = &allocator;
11: }
12: 
13: ColumnDataAllocator::ColumnDataAllocator(BufferManager &buffer_manager)
14:     : type(ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR) {
15: 	alloc.buffer_manager = &buffer_manager;
16: }
17: 
18: ColumnDataAllocator::ColumnDataAllocator(ClientContext &context, ColumnDataAllocatorType allocator_type)
19:     : type(allocator_type) {
20: 	switch (type) {
21: 	case ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR:
22: 	case ColumnDataAllocatorType::HYBRID:
23: 		alloc.buffer_manager = &BufferManager::GetBufferManager(context);
24: 		break;
25: 	case ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR:
26: 		alloc.allocator = &Allocator::Get(context);
27: 		break;
28: 	default:
29: 		throw InternalException("Unrecognized column data allocator type");
30: 	}
31: }
32: 
33: ColumnDataAllocator::ColumnDataAllocator(ColumnDataAllocator &other) {
34: 	type = other.GetType();
35: 	switch (type) {
36: 	case ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR:
37: 	case ColumnDataAllocatorType::HYBRID:
38: 		alloc.allocator = other.alloc.allocator;
39: 		break;
40: 	case ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR:
41: 		alloc.buffer_manager = other.alloc.buffer_manager;
42: 		break;
43: 	default:
44: 		throw InternalException("Unrecognized column data allocator type");
45: 	}
46: }
47: 
48: BufferHandle ColumnDataAllocator::Pin(uint32_t block_id) {
49: 	D_ASSERT(type == ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR || type == ColumnDataAllocatorType::HYBRID);
50: 	shared_ptr<BlockHandle> handle;
51: 	if (shared) {
52: 		// we only need to grab the lock when accessing the vector, because vector access is not thread-safe:
53: 		// the vector can be resized by another thread while we try to access it
54: 		lock_guard<mutex> guard(lock);
55: 		handle = blocks[block_id].handle;
56: 	} else {
57: 		handle = blocks[block_id].handle;
58: 	}
59: 	return alloc.buffer_manager->Pin(handle);
60: }
61: 
62: BufferHandle ColumnDataAllocator::AllocateBlock(idx_t size) {
63: 	D_ASSERT(type == ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR || type == ColumnDataAllocatorType::HYBRID);
64: 	auto max_size = MaxValue<idx_t>(size, GetBufferManager().GetBlockSize());
65: 	BlockMetaData data;
66: 	data.size = 0;
67: 	data.capacity = NumericCast<uint32_t>(max_size);
68: 	auto pin = alloc.buffer_manager->Allocate(MemoryTag::COLUMN_DATA, max_size, false);
69: 	data.handle = pin.GetBlockHandle();
70: 	blocks.push_back(std::move(data));
71: 	allocated_size += max_size;
72: 	return pin;
73: }
74: 
75: void ColumnDataAllocator::AllocateEmptyBlock(idx_t size) {
76: 	auto allocation_amount = MaxValue<idx_t>(NextPowerOfTwo(size), 4096);
77: 	if (!blocks.empty()) {
78: 		idx_t last_capacity = blocks.back().capacity;
79: 		auto next_capacity = MinValue<idx_t>(last_capacity * 2, last_capacity + Storage::DEFAULT_BLOCK_SIZE);
80: 		allocation_amount = MaxValue<idx_t>(next_capacity, allocation_amount);
81: 	}
82: 	D_ASSERT(type == ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR);
83: 	BlockMetaData data;
84: 	data.size = 0;
85: 	data.capacity = NumericCast<uint32_t>(allocation_amount);
86: 	data.handle = nullptr;
87: 	blocks.push_back(std::move(data));
88: 	allocated_size += allocation_amount;
89: }
90: 
91: void ColumnDataAllocator::AssignPointer(uint32_t &block_id, uint32_t &offset, data_ptr_t pointer) {
92: 	auto pointer_value = uintptr_t(pointer);
93: 	if (sizeof(uintptr_t) == sizeof(uint32_t)) {
94: 		block_id = uint32_t(pointer_value);
95: 	} else if (sizeof(uintptr_t) == sizeof(uint64_t)) {
96: 		block_id = uint32_t(pointer_value & 0xFFFFFFFF);
97: 		offset = uint32_t(pointer_value >> 32);
98: 	} else {
99: 		throw InternalException("ColumnDataCollection: Architecture not supported!?");
100: 	}
101: }
102: 
103: void ColumnDataAllocator::AllocateBuffer(idx_t size, uint32_t &block_id, uint32_t &offset,
104:                                          ChunkManagementState *chunk_state) {
105: 	D_ASSERT(allocated_data.empty());
106: 	if (blocks.empty() || blocks.back().Capacity() < size) {
107: 		auto pinned_block = AllocateBlock(size);
108: 		if (chunk_state) {
109: 			D_ASSERT(!blocks.empty());
110: 			auto new_block_id = blocks.size() - 1;
111: 			chunk_state->handles[new_block_id] = std::move(pinned_block);
112: 		}
113: 	}
114: 	auto &block = blocks.back();
115: 	D_ASSERT(size <= block.capacity - block.size);
116: 	block_id = NumericCast<uint32_t>(blocks.size() - 1);
117: 	if (chunk_state && chunk_state->handles.find(block_id) == chunk_state->handles.end()) {
118: 		// not guaranteed to be pinned already by this thread (if shared allocator)
119: 		chunk_state->handles[block_id] = alloc.buffer_manager->Pin(blocks[block_id].handle);
120: 	}
121: 	offset = block.size;
122: 	block.size += size;
123: }
124: 
125: void ColumnDataAllocator::AllocateMemory(idx_t size, uint32_t &block_id, uint32_t &offset,
126:                                          ChunkManagementState *chunk_state) {
127: 	D_ASSERT(blocks.size() == allocated_data.size());
128: 	if (blocks.empty() || blocks.back().Capacity() < size) {
129: 		AllocateEmptyBlock(size);
130: 		auto &last_block = blocks.back();
131: 		auto allocated = alloc.allocator->Allocate(last_block.capacity);
132: 		allocated_data.push_back(std::move(allocated));
133: 	}
134: 	auto &block = blocks.back();
135: 	D_ASSERT(size <= block.capacity - block.size);
136: 	AssignPointer(block_id, offset, allocated_data.back().get() + block.size);
137: 	block.size += size;
138: }
139: 
140: void ColumnDataAllocator::AllocateData(idx_t size, uint32_t &block_id, uint32_t &offset,
141:                                        ChunkManagementState *chunk_state) {
142: 	switch (type) {
143: 	case ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR:
144: 	case ColumnDataAllocatorType::HYBRID:
145: 		if (shared) {
146: 			lock_guard<mutex> guard(lock);
147: 			AllocateBuffer(size, block_id, offset, chunk_state);
148: 		} else {
149: 			AllocateBuffer(size, block_id, offset, chunk_state);
150: 		}
151: 		break;
152: 	case ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR:
153: 		D_ASSERT(!shared);
154: 		AllocateMemory(size, block_id, offset, chunk_state);
155: 		break;
156: 	default:
157: 		throw InternalException("Unrecognized allocator type");
158: 	}
159: }
160: 
161: void ColumnDataAllocator::Initialize(ColumnDataAllocator &other) {
162: 	D_ASSERT(other.HasBlocks());
163: 	blocks.push_back(other.blocks.back());
164: }
165: 
166: data_ptr_t ColumnDataAllocator::GetDataPointer(ChunkManagementState &state, uint32_t block_id, uint32_t offset) {
167: 	if (type == ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR) {
168: 		// in-memory allocator: construct pointer from block_id and offset
169: 		if (sizeof(uintptr_t) == sizeof(uint32_t)) {
170: 			uintptr_t pointer_value = uintptr_t(block_id);
171: 			return (data_ptr_t)pointer_value; // NOLINT - convert from pointer value back to pointer
172: 		} else if (sizeof(uintptr_t) == sizeof(uint64_t)) {
173: 			uintptr_t pointer_value = (uintptr_t(offset) << 32) | uintptr_t(block_id);
174: 			return (data_ptr_t)pointer_value; // NOLINT - convert from pointer value back to pointer
175: 		} else {
176: 			throw InternalException("ColumnDataCollection: Architecture not supported!?");
177: 		}
178: 	}
179: 	D_ASSERT(state.handles.find(block_id) != state.handles.end());
180: 	return state.handles[block_id].Ptr() + offset;
181: }
182: 
183: void ColumnDataAllocator::UnswizzlePointers(ChunkManagementState &state, Vector &result, idx_t v_offset, uint16_t count,
184:                                             uint32_t block_id, uint32_t offset) {
185: 	D_ASSERT(result.GetType().InternalType() == PhysicalType::VARCHAR);
186: 	lock_guard<mutex> guard(lock);
187: 
188: 	auto &validity = FlatVector::Validity(result);
189: 	auto strings = FlatVector::GetData<string_t>(result);
190: 
191: 	// find first non-inlined string
192: 	auto i = NumericCast<uint32_t>(v_offset);
193: 	const uint32_t end = NumericCast<uint32_t>(v_offset + count);
194: 	for (; i < end; i++) {
195: 		if (!validity.RowIsValid(i)) {
196: 			continue;
197: 		}
198: 		if (!strings[i].IsInlined()) {
199: 			break;
200: 		}
201: 	}
202: 	// at least one string must be non-inlined, otherwise this function should not be called
203: 	D_ASSERT(i < end);
204: 
205: 	auto base_ptr = char_ptr_cast(GetDataPointer(state, block_id, offset));
206: 	if (strings[i].GetData() == base_ptr) {
207: 		// pointers are still valid
208: 		return;
209: 	}
210: 
211: 	// pointer mismatch! pointers are invalid, set them correctly
212: 	for (; i < end; i++) {
213: 		if (!validity.RowIsValid(i)) {
214: 			continue;
215: 		}
216: 		if (strings[i].IsInlined()) {
217: 			continue;
218: 		}
219: 		strings[i].SetPointer(base_ptr);
220: 		base_ptr += strings[i].GetSize();
221: 	}
222: }
223: 
224: void ColumnDataAllocator::SetDestroyBufferUponUnpin(uint32_t block_id) {
225: 	blocks[block_id].handle->SetDestroyBufferUpon(DestroyBufferUpon::UNPIN);
226: }
227: 
228: Allocator &ColumnDataAllocator::GetAllocator() {
229: 	if (type == ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR) {
230: 		return *alloc.allocator;
231: 	}
232: 	return alloc.buffer_manager->GetBufferAllocator();
233: }
234: 
235: BufferManager &ColumnDataAllocator::GetBufferManager() {
236: 	if (type == ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR) {
237: 		throw InternalException("cannot obtain the buffer manager for in memory allocations");
238: 	}
239: 	return *alloc.buffer_manager;
240: }
241: 
242: void ColumnDataAllocator::InitializeChunkState(ChunkManagementState &state, ChunkMetaData &chunk) {
243: 	if (type != ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR && type != ColumnDataAllocatorType::HYBRID) {
244: 		// nothing to pin
245: 		return;
246: 	}
247: 	// release any handles that are no longer required
248: 	bool found_handle;
249: 	do {
250: 		found_handle = false;
251: 		for (auto it = state.handles.begin(); it != state.handles.end(); it++) {
252: 			if (chunk.block_ids.find(NumericCast<uint32_t>(it->first)) != chunk.block_ids.end()) {
253: 				// still required: do not release
254: 				continue;
255: 			}
256: 			state.handles.erase(it);
257: 			found_handle = true;
258: 			break;
259: 		}
260: 	} while (found_handle);
261: 
262: 	// grab any handles that are now required
263: 	for (auto &block_id : chunk.block_ids) {
264: 		if (state.handles.find(block_id) != state.handles.end()) {
265: 			// already pinned: don't need to do anything
266: 			continue;
267: 		}
268: 		state.handles[block_id] = Pin(block_id);
269: 	}
270: }
271: 
272: uint32_t BlockMetaData::Capacity() {
273: 	D_ASSERT(size <= capacity);
274: 	return capacity - size;
275: }
276: 
277: } // namespace duckdb
[end of src/common/types/column/column_data_allocator.cpp]
[start of src/common/types/row/tuple_data_segment.cpp]
1: #include "duckdb/common/types/row/tuple_data_segment.hpp"
2: 
3: #include "duckdb/common/types/row/tuple_data_allocator.hpp"
4: 
5: namespace duckdb {
6: 
7: TupleDataChunkPart::TupleDataChunkPart(mutex &lock_p) : lock(lock_p) {
8: }
9: 
10: void TupleDataChunkPart::SetHeapEmpty() {
11: 	heap_block_index = INVALID_INDEX;
12: 	heap_block_offset = INVALID_INDEX;
13: 	total_heap_size = 0;
14: 	base_heap_ptr = nullptr;
15: }
16: 
17: void SwapTupleDataChunkPart(TupleDataChunkPart &a, TupleDataChunkPart &b) {
18: 	std::swap(a.row_block_index, b.row_block_index);
19: 	std::swap(a.row_block_offset, b.row_block_offset);
20: 	std::swap(a.heap_block_index, b.heap_block_index);
21: 	std::swap(a.heap_block_offset, b.heap_block_offset);
22: 	std::swap(a.base_heap_ptr, b.base_heap_ptr);
23: 	std::swap(a.total_heap_size, b.total_heap_size);
24: 	std::swap(a.count, b.count);
25: 	std::swap(a.lock, b.lock);
26: }
27: 
28: TupleDataChunkPart::TupleDataChunkPart(TupleDataChunkPart &&other) noexcept : lock((other.lock)) {
29: 	SwapTupleDataChunkPart(*this, other);
30: }
31: 
32: TupleDataChunkPart &TupleDataChunkPart::operator=(TupleDataChunkPart &&other) noexcept {
33: 	SwapTupleDataChunkPart(*this, other);
34: 	return *this;
35: }
36: 
37: TupleDataChunk::TupleDataChunk() : count(0), lock(make_unsafe_uniq<mutex>()) {
38: 	parts.reserve(2);
39: }
40: 
41: static inline void SwapTupleDataChunk(TupleDataChunk &a, TupleDataChunk &b) noexcept {
42: 	std::swap(a.parts, b.parts);
43: 	std::swap(a.row_block_ids, b.row_block_ids);
44: 	std::swap(a.heap_block_ids, b.heap_block_ids);
45: 	std::swap(a.count, b.count);
46: 	std::swap(a.lock, b.lock);
47: }
48: 
49: TupleDataChunk::TupleDataChunk(TupleDataChunk &&other) noexcept {
50: 	SwapTupleDataChunk(*this, other);
51: }
52: 
53: TupleDataChunk &TupleDataChunk::operator=(TupleDataChunk &&other) noexcept {
54: 	SwapTupleDataChunk(*this, other);
55: 	return *this;
56: }
57: 
58: void TupleDataChunk::AddPart(TupleDataChunkPart &&part, const TupleDataLayout &layout) {
59: 	count += part.count;
60: 	row_block_ids.insert(part.row_block_index);
61: 	if (!layout.AllConstant() && part.total_heap_size > 0) {
62: 		heap_block_ids.insert(part.heap_block_index);
63: 	}
64: 	part.lock = *lock;
65: 	parts.emplace_back(std::move(part));
66: }
67: 
68: void TupleDataChunk::Verify() const {
69: #ifdef DEBUG
70: 	idx_t total_count = 0;
71: 	for (const auto &part : parts) {
72: 		total_count += part.count;
73: 	}
74: 	D_ASSERT(this->count == total_count);
75: 	D_ASSERT(this->count <= STANDARD_VECTOR_SIZE);
76: #endif
77: }
78: 
79: void TupleDataChunk::MergeLastChunkPart(const TupleDataLayout &layout) {
80: 	if (parts.size() < 2) {
81: 		return;
82: 	}
83: 
84: 	auto &second_to_last = parts[parts.size() - 2];
85: 	auto &last = parts[parts.size() - 1];
86: 
87: 	auto rows_align =
88: 	    last.row_block_index == second_to_last.row_block_index &&
89: 	    last.row_block_offset == second_to_last.row_block_offset + second_to_last.count * layout.GetRowWidth();
90: 
91: 	if (!rows_align) { // If rows don't align we can never merge
92: 		return;
93: 	}
94: 
95: 	if (layout.AllConstant()) { // No heap and rows align - merge
96: 		second_to_last.count += last.count;
97: 		parts.pop_back();
98: 		return;
99: 	}
100: 
101: 	if (last.heap_block_index == second_to_last.heap_block_index &&
102: 	    last.heap_block_offset == second_to_last.heap_block_index + second_to_last.total_heap_size &&
103: 	    last.base_heap_ptr == second_to_last.base_heap_ptr) { // There is a heap and it aligns - merge
104: 		second_to_last.total_heap_size += last.total_heap_size;
105: 		second_to_last.count += last.count;
106: 		parts.pop_back();
107: 	}
108: }
109: 
110: TupleDataSegment::TupleDataSegment(shared_ptr<TupleDataAllocator> allocator_p)
111:     : allocator(std::move(allocator_p)), count(0), data_size(0) {
112: }
113: 
114: TupleDataSegment::~TupleDataSegment() {
115: 	lock_guard<mutex> guard(pinned_handles_lock);
116: 	if (allocator) {
117: 		allocator->SetDestroyBufferUponUnpin(); // Prevent blocks from being added to eviction queue
118: 	}
119: 	pinned_row_handles.clear();
120: 	pinned_heap_handles.clear();
121: 	allocator.reset();
122: }
123: 
124: void SwapTupleDataSegment(TupleDataSegment &a, TupleDataSegment &b) {
125: 	std::swap(a.allocator, b.allocator);
126: 	std::swap(a.chunks, b.chunks);
127: 	std::swap(a.count, b.count);
128: 	std::swap(a.data_size, b.data_size);
129: 	std::swap(a.pinned_row_handles, b.pinned_row_handles);
130: 	std::swap(a.pinned_heap_handles, b.pinned_heap_handles);
131: }
132: 
133: TupleDataSegment::TupleDataSegment(TupleDataSegment &&other) noexcept {
134: 	SwapTupleDataSegment(*this, other);
135: }
136: 
137: TupleDataSegment &TupleDataSegment::operator=(TupleDataSegment &&other) noexcept {
138: 	SwapTupleDataSegment(*this, other);
139: 	return *this;
140: }
141: 
142: idx_t TupleDataSegment::ChunkCount() const {
143: 	return chunks.size();
144: }
145: 
146: idx_t TupleDataSegment::SizeInBytes() const {
147: 	return data_size;
148: }
149: 
150: void TupleDataSegment::Unpin() {
151: 	lock_guard<mutex> guard(pinned_handles_lock);
152: 	pinned_row_handles.clear();
153: 	pinned_heap_handles.clear();
154: }
155: 
156: void TupleDataSegment::Verify() const {
157: #ifdef DEBUG
158: 	const auto &layout = allocator->GetLayout();
159: 
160: 	idx_t total_count = 0;
161: 	idx_t total_size = 0;
162: 	for (const auto &chunk : chunks) {
163: 		chunk.Verify();
164: 		total_count += chunk.count;
165: 
166: 		total_size += chunk.count * layout.GetRowWidth();
167: 		if (!layout.AllConstant()) {
168: 			for (const auto &part : chunk.parts) {
169: 				total_size += part.total_heap_size;
170: 			}
171: 		}
172: 	}
173: 	D_ASSERT(total_count == this->count);
174: 	D_ASSERT(total_size == this->data_size);
175: #endif
176: }
177: 
178: void TupleDataSegment::VerifyEverythingPinned() const {
179: #ifdef DEBUG
180: 	D_ASSERT(pinned_row_handles.size() == allocator->RowBlockCount());
181: 	D_ASSERT(pinned_heap_handles.size() == allocator->HeapBlockCount());
182: #endif
183: }
184: 
185: } // namespace duckdb
[end of src/common/types/row/tuple_data_segment.cpp]
[start of src/execution/operator/join/physical_piecewise_merge_join.cpp]
1: #include "duckdb/execution/operator/join/physical_piecewise_merge_join.hpp"
2: 
3: #include "duckdb/common/fast_mem.hpp"
4: #include "duckdb/common/operator/comparison_operators.hpp"
5: #include "duckdb/common/row_operations/row_operations.hpp"
6: #include "duckdb/common/sort/comparators.hpp"
7: #include "duckdb/common/sort/sort.hpp"
8: #include "duckdb/common/vector_operations/vector_operations.hpp"
9: #include "duckdb/execution/expression_executor.hpp"
10: #include "duckdb/execution/operator/join/outer_join_marker.hpp"
11: #include "duckdb/main/client_context.hpp"
12: #include "duckdb/parallel/event.hpp"
13: #include "duckdb/parallel/thread_context.hpp"
14: 
15: namespace duckdb {
16: 
17: PhysicalPiecewiseMergeJoin::PhysicalPiecewiseMergeJoin(LogicalComparisonJoin &op, unique_ptr<PhysicalOperator> left,
18:                                                        unique_ptr<PhysicalOperator> right, vector<JoinCondition> cond,
19:                                                        JoinType join_type, idx_t estimated_cardinality)
20:     : PhysicalRangeJoin(op, PhysicalOperatorType::PIECEWISE_MERGE_JOIN, std::move(left), std::move(right),
21:                         std::move(cond), join_type, estimated_cardinality) {
22: 
23: 	for (auto &cond : conditions) {
24: 		D_ASSERT(cond.left->return_type == cond.right->return_type);
25: 		join_key_types.push_back(cond.left->return_type);
26: 
27: 		// Convert the conditions to sort orders
28: 		auto left = cond.left->Copy();
29: 		auto right = cond.right->Copy();
30: 		switch (cond.comparison) {
31: 		case ExpressionType::COMPARE_LESSTHAN:
32: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO:
33: 			lhs_orders.emplace_back(OrderType::ASCENDING, OrderByNullType::NULLS_LAST, std::move(left));
34: 			rhs_orders.emplace_back(OrderType::ASCENDING, OrderByNullType::NULLS_LAST, std::move(right));
35: 			break;
36: 		case ExpressionType::COMPARE_GREATERTHAN:
37: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
38: 			lhs_orders.emplace_back(OrderType::DESCENDING, OrderByNullType::NULLS_LAST, std::move(left));
39: 			rhs_orders.emplace_back(OrderType::DESCENDING, OrderByNullType::NULLS_LAST, std::move(right));
40: 			break;
41: 		case ExpressionType::COMPARE_NOTEQUAL:
42: 		case ExpressionType::COMPARE_DISTINCT_FROM:
43: 			// Allowed in multi-predicate joins, but can't be first/sort.
44: 			D_ASSERT(!lhs_orders.empty());
45: 			lhs_orders.emplace_back(OrderType::INVALID, OrderByNullType::NULLS_LAST, std::move(left));
46: 			rhs_orders.emplace_back(OrderType::INVALID, OrderByNullType::NULLS_LAST, std::move(right));
47: 			break;
48: 
49: 		default:
50: 			// COMPARE EQUAL not supported with merge join
51: 			throw NotImplementedException("Unimplemented join type for merge join");
52: 		}
53: 	}
54: }
55: 
56: //===--------------------------------------------------------------------===//
57: // Sink
58: //===--------------------------------------------------------------------===//
59: class MergeJoinLocalState : public LocalSinkState {
60: public:
61: 	explicit MergeJoinLocalState(ClientContext &context, const PhysicalRangeJoin &op, const idx_t child)
62: 	    : table(context, op, child) {
63: 	}
64: 
65: 	//! The local sort state
66: 	PhysicalRangeJoin::LocalSortedTable table;
67: };
68: 
69: class MergeJoinGlobalState : public GlobalSinkState {
70: public:
71: 	using GlobalSortedTable = PhysicalRangeJoin::GlobalSortedTable;
72: 
73: public:
74: 	MergeJoinGlobalState(ClientContext &context, const PhysicalPiecewiseMergeJoin &op) {
75: 		RowLayout rhs_layout;
76: 		rhs_layout.Initialize(op.children[1]->types);
77: 		vector<BoundOrderByNode> rhs_order;
78: 		rhs_order.emplace_back(op.rhs_orders[0].Copy());
79: 		table = make_uniq<GlobalSortedTable>(context, rhs_order, rhs_layout, op);
80: 	}
81: 
82: 	inline idx_t Count() const {
83: 		return table->count;
84: 	}
85: 
86: 	void Sink(DataChunk &input, MergeJoinLocalState &lstate) {
87: 		auto &global_sort_state = table->global_sort_state;
88: 		auto &local_sort_state = lstate.table.local_sort_state;
89: 
90: 		// Sink the data into the local sort state
91: 		lstate.table.Sink(input, global_sort_state);
92: 
93: 		// When sorting data reaches a certain size, we sort it
94: 		if (local_sort_state.SizeInBytes() >= table->memory_per_thread) {
95: 			local_sort_state.Sort(global_sort_state, true);
96: 		}
97: 	}
98: 
99: 	unique_ptr<GlobalSortedTable> table;
100: };
101: 
102: unique_ptr<GlobalSinkState> PhysicalPiecewiseMergeJoin::GetGlobalSinkState(ClientContext &context) const {
103: 	return make_uniq<MergeJoinGlobalState>(context, *this);
104: }
105: 
106: unique_ptr<LocalSinkState> PhysicalPiecewiseMergeJoin::GetLocalSinkState(ExecutionContext &context) const {
107: 	// We only sink the RHS
108: 	return make_uniq<MergeJoinLocalState>(context.client, *this, 1U);
109: }
110: 
111: SinkResultType PhysicalPiecewiseMergeJoin::Sink(ExecutionContext &context, DataChunk &chunk,
112:                                                 OperatorSinkInput &input) const {
113: 	auto &gstate = input.global_state.Cast<MergeJoinGlobalState>();
114: 	auto &lstate = input.local_state.Cast<MergeJoinLocalState>();
115: 
116: 	gstate.Sink(chunk, lstate);
117: 
118: 	return SinkResultType::NEED_MORE_INPUT;
119: }
120: 
121: SinkCombineResultType PhysicalPiecewiseMergeJoin::Combine(ExecutionContext &context,
122:                                                           OperatorSinkCombineInput &input) const {
123: 	auto &gstate = input.global_state.Cast<MergeJoinGlobalState>();
124: 	auto &lstate = input.local_state.Cast<MergeJoinLocalState>();
125: 	gstate.table->Combine(lstate.table);
126: 	auto &client_profiler = QueryProfiler::Get(context.client);
127: 
128: 	context.thread.profiler.Flush(*this);
129: 	client_profiler.Flush(context.thread.profiler);
130: 
131: 	return SinkCombineResultType::FINISHED;
132: }
133: 
134: //===--------------------------------------------------------------------===//
135: // Finalize
136: //===--------------------------------------------------------------------===//
137: SinkFinalizeType PhysicalPiecewiseMergeJoin::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
138:                                                       OperatorSinkFinalizeInput &input) const {
139: 	auto &gstate = input.global_state.Cast<MergeJoinGlobalState>();
140: 	auto &global_sort_state = gstate.table->global_sort_state;
141: 
142: 	if (PropagatesBuildSide(join_type)) {
143: 		// for FULL/RIGHT OUTER JOIN, initialize found_match to false for every tuple
144: 		gstate.table->IntializeMatches();
145: 	}
146: 	if (global_sort_state.sorted_blocks.empty() && EmptyResultIfRHSIsEmpty()) {
147: 		// Empty input!
148: 		return SinkFinalizeType::NO_OUTPUT_POSSIBLE;
149: 	}
150: 
151: 	// Sort the current input child
152: 	gstate.table->Finalize(pipeline, event);
153: 
154: 	return SinkFinalizeType::READY;
155: }
156: 
157: //===--------------------------------------------------------------------===//
158: // Operator
159: //===--------------------------------------------------------------------===//
160: class PiecewiseMergeJoinState : public CachingOperatorState {
161: public:
162: 	using LocalSortedTable = PhysicalRangeJoin::LocalSortedTable;
163: 
164: 	PiecewiseMergeJoinState(ClientContext &context, const PhysicalPiecewiseMergeJoin &op, bool force_external)
165: 	    : context(context), allocator(Allocator::Get(context)), op(op),
166: 	      buffer_manager(BufferManager::GetBufferManager(context)), force_external(force_external),
167: 	      left_outer(IsLeftOuterJoin(op.join_type)), left_position(0), first_fetch(true), finished(true),
168: 	      right_position(0), right_chunk_index(0), rhs_executor(context) {
169: 		vector<LogicalType> condition_types;
170: 		for (auto &order : op.lhs_orders) {
171: 			condition_types.push_back(order.expression->return_type);
172: 		}
173: 		left_outer.Initialize(STANDARD_VECTOR_SIZE);
174: 		lhs_layout.Initialize(op.children[0]->types);
175: 		lhs_payload.Initialize(allocator, op.children[0]->types);
176: 
177: 		lhs_order.emplace_back(op.lhs_orders[0].Copy());
178: 
179: 		// Set up shared data for multiple predicates
180: 		sel.Initialize(STANDARD_VECTOR_SIZE);
181: 		condition_types.clear();
182: 		for (auto &order : op.rhs_orders) {
183: 			rhs_executor.AddExpression(*order.expression);
184: 			condition_types.push_back(order.expression->return_type);
185: 		}
186: 		rhs_keys.Initialize(allocator, condition_types);
187: 	}
188: 
189: 	ClientContext &context;
190: 	Allocator &allocator;
191: 	const PhysicalPiecewiseMergeJoin &op;
192: 	BufferManager &buffer_manager;
193: 	bool force_external;
194: 
195: 	// Block sorting
196: 	DataChunk lhs_payload;
197: 	OuterJoinMarker left_outer;
198: 	vector<BoundOrderByNode> lhs_order;
199: 	RowLayout lhs_layout;
200: 	unique_ptr<LocalSortedTable> lhs_local_table;
201: 	unique_ptr<GlobalSortState> lhs_global_state;
202: 	unique_ptr<PayloadScanner> scanner;
203: 
204: 	// Simple scans
205: 	idx_t left_position;
206: 
207: 	// Complex scans
208: 	bool first_fetch;
209: 	bool finished;
210: 	idx_t right_position;
211: 	idx_t right_chunk_index;
212: 	idx_t right_base;
213: 	idx_t prev_left_index;
214: 
215: 	// Secondary predicate shared data
216: 	SelectionVector sel;
217: 	DataChunk rhs_keys;
218: 	DataChunk rhs_input;
219: 	ExpressionExecutor rhs_executor;
220: 	vector<BufferHandle> payload_heap_handles;
221: 
222: public:
223: 	void ResolveJoinKeys(DataChunk &input) {
224: 		// sort by join key
225: 		lhs_global_state = make_uniq<GlobalSortState>(buffer_manager, lhs_order, lhs_layout);
226: 		lhs_local_table = make_uniq<LocalSortedTable>(context, op, 0U);
227: 		lhs_local_table->Sink(input, *lhs_global_state);
228: 
229: 		// Set external (can be forced with the PRAGMA)
230: 		lhs_global_state->external = force_external;
231: 		lhs_global_state->AddLocalState(lhs_local_table->local_sort_state);
232: 		lhs_global_state->PrepareMergePhase();
233: 		while (lhs_global_state->sorted_blocks.size() > 1) {
234: 			MergeSorter merge_sorter(*lhs_global_state, buffer_manager);
235: 			merge_sorter.PerformInMergeRound();
236: 			lhs_global_state->CompleteMergeRound();
237: 		}
238: 
239: 		// Scan the sorted payload
240: 		D_ASSERT(lhs_global_state->sorted_blocks.size() == 1);
241: 
242: 		scanner = make_uniq<PayloadScanner>(*lhs_global_state->sorted_blocks[0]->payload_data, *lhs_global_state);
243: 		lhs_payload.Reset();
244: 		scanner->Scan(lhs_payload);
245: 
246: 		// Recompute the sorted keys from the sorted input
247: 		lhs_local_table->keys.Reset();
248: 		lhs_local_table->executor.Execute(lhs_payload, lhs_local_table->keys);
249: 	}
250: 
251: 	void Finalize(const PhysicalOperator &op, ExecutionContext &context) override {
252: 		if (lhs_local_table) {
253: 			context.thread.profiler.Flush(op);
254: 		}
255: 	}
256: };
257: 
258: unique_ptr<OperatorState> PhysicalPiecewiseMergeJoin::GetOperatorState(ExecutionContext &context) const {
259: 	auto &config = ClientConfig::GetConfig(context.client);
260: 	return make_uniq<PiecewiseMergeJoinState>(context.client, *this, config.force_external);
261: }
262: 
263: static inline idx_t SortedBlockNotNull(const idx_t base, const idx_t count, const idx_t not_null) {
264: 	return MinValue(base + count, MaxValue(base, not_null)) - base;
265: }
266: 
267: static int MergeJoinComparisonValue(ExpressionType comparison) {
268: 	switch (comparison) {
269: 	case ExpressionType::COMPARE_LESSTHAN:
270: 	case ExpressionType::COMPARE_GREATERTHAN:
271: 		return -1;
272: 	case ExpressionType::COMPARE_LESSTHANOREQUALTO:
273: 	case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
274: 		return 0;
275: 	default:
276: 		throw InternalException("Unimplemented comparison type for merge join!");
277: 	}
278: }
279: 
280: struct BlockMergeInfo {
281: 	GlobalSortState &state;
282: 	//! The block being scanned
283: 	const idx_t block_idx;
284: 	//! The number of not-NULL values in the block (they are at the end)
285: 	const idx_t not_null;
286: 	//! The current offset in the block
287: 	idx_t &entry_idx;
288: 	SelectionVector result;
289: 
290: 	BlockMergeInfo(GlobalSortState &state, idx_t block_idx, idx_t &entry_idx, idx_t not_null)
291: 	    : state(state), block_idx(block_idx), not_null(not_null), entry_idx(entry_idx), result(STANDARD_VECTOR_SIZE) {
292: 	}
293: };
294: 
295: static void MergeJoinPinSortingBlock(SBScanState &scan, const idx_t block_idx) {
296: 	scan.SetIndices(block_idx, 0);
297: 	scan.PinRadix(block_idx);
298: 
299: 	auto &sd = *scan.sb->blob_sorting_data;
300: 	if (block_idx < sd.data_blocks.size()) {
301: 		scan.PinData(sd);
302: 	}
303: }
304: 
305: static data_ptr_t MergeJoinRadixPtr(SBScanState &scan, const idx_t entry_idx) {
306: 	scan.entry_idx = entry_idx;
307: 	return scan.RadixPtr();
308: }
309: 
310: static idx_t MergeJoinSimpleBlocks(PiecewiseMergeJoinState &lstate, MergeJoinGlobalState &rstate, bool *found_match,
311:                                    const ExpressionType comparison) {
312: 	const auto cmp = MergeJoinComparisonValue(comparison);
313: 
314: 	// The sort parameters should all be the same
315: 	auto &lsort = *lstate.lhs_global_state;
316: 	auto &rsort = rstate.table->global_sort_state;
317: 	D_ASSERT(lsort.sort_layout.all_constant == rsort.sort_layout.all_constant);
318: 	const auto all_constant = lsort.sort_layout.all_constant;
319: 	D_ASSERT(lsort.external == rsort.external);
320: 	const auto external = lsort.external;
321: 
322: 	// There should only be one sorted block if they have been sorted
323: 	D_ASSERT(lsort.sorted_blocks.size() == 1);
324: 	SBScanState lread(lsort.buffer_manager, lsort);
325: 	lread.sb = lsort.sorted_blocks[0].get();
326: 
327: 	const idx_t l_block_idx = 0;
328: 	idx_t l_entry_idx = 0;
329: 	const auto lhs_not_null = lstate.lhs_local_table->count - lstate.lhs_local_table->has_null;
330: 	MergeJoinPinSortingBlock(lread, l_block_idx);
331: 	auto l_ptr = MergeJoinRadixPtr(lread, l_entry_idx);
332: 
333: 	D_ASSERT(rsort.sorted_blocks.size() == 1);
334: 	SBScanState rread(rsort.buffer_manager, rsort);
335: 	rread.sb = rsort.sorted_blocks[0].get();
336: 
337: 	const auto cmp_size = lsort.sort_layout.comparison_size;
338: 	const auto entry_size = lsort.sort_layout.entry_size;
339: 
340: 	idx_t right_base = 0;
341: 	for (idx_t r_block_idx = 0; r_block_idx < rread.sb->radix_sorting_data.size(); r_block_idx++) {
342: 		// we only care about the BIGGEST value in each of the RHS data blocks
343: 		// because we want to figure out if the LHS values are less than [or equal] to ANY value
344: 		// get the biggest value from the RHS chunk
345: 		MergeJoinPinSortingBlock(rread, r_block_idx);
346: 
347: 		auto &rblock = *rread.sb->radix_sorting_data[r_block_idx];
348: 		const auto r_not_null =
349: 		    SortedBlockNotNull(right_base, rblock.count, rstate.table->count - rstate.table->has_null);
350: 		if (r_not_null == 0) {
351: 			break;
352: 		}
353: 		const auto r_entry_idx = r_not_null - 1;
354: 		right_base += rblock.count;
355: 
356: 		auto r_ptr = MergeJoinRadixPtr(rread, r_entry_idx);
357: 
358: 		// now we start from the current lpos value and check if we found a new value that is [<= OR <] the max RHS
359: 		// value
360: 		while (true) {
361: 			int comp_res;
362: 			if (all_constant) {
363: 				comp_res = FastMemcmp(l_ptr, r_ptr, cmp_size);
364: 			} else {
365: 				lread.entry_idx = l_entry_idx;
366: 				rread.entry_idx = r_entry_idx;
367: 				comp_res = Comparators::CompareTuple(lread, rread, l_ptr, r_ptr, lsort.sort_layout, external);
368: 			}
369: 
370: 			if (comp_res <= cmp) {
371: 				// found a match for lpos, set it in the found_match vector
372: 				found_match[l_entry_idx] = true;
373: 				l_entry_idx++;
374: 				l_ptr += entry_size;
375: 				if (l_entry_idx >= lhs_not_null) {
376: 					// early out: we exhausted the entire LHS and they all match
377: 					return 0;
378: 				}
379: 			} else {
380: 				// we found no match: any subsequent value from the LHS we scan now will be bigger and thus also not
381: 				// match move to the next RHS chunk
382: 				break;
383: 			}
384: 		}
385: 	}
386: 	return 0;
387: }
388: 
389: void PhysicalPiecewiseMergeJoin::ResolveSimpleJoin(ExecutionContext &context, DataChunk &input, DataChunk &chunk,
390:                                                    OperatorState &state_p) const {
391: 	auto &state = state_p.Cast<PiecewiseMergeJoinState>();
392: 	auto &gstate = sink_state->Cast<MergeJoinGlobalState>();
393: 
394: 	state.ResolveJoinKeys(input);
395: 	auto &lhs_table = *state.lhs_local_table;
396: 
397: 	// perform the actual join
398: 	bool found_match[STANDARD_VECTOR_SIZE];
399: 	memset(found_match, 0, sizeof(found_match));
400: 	MergeJoinSimpleBlocks(state, gstate, found_match, conditions[0].comparison);
401: 
402: 	// use the sorted payload
403: 	const auto lhs_not_null = lhs_table.count - lhs_table.has_null;
404: 	auto &payload = state.lhs_payload;
405: 
406: 	// now construct the result based on the join result
407: 	switch (join_type) {
408: 	case JoinType::MARK: {
409: 		// The only part of the join keys that is actually used is the validity mask.
410: 		// Since the payload is sorted, we can just set the tail end of the validity masks to invalid.
411: 		for (auto &key : lhs_table.keys.data) {
412: 			key.Flatten(lhs_table.keys.size());
413: 			auto &mask = FlatVector::Validity(key);
414: 			if (mask.AllValid()) {
415: 				continue;
416: 			}
417: 			mask.SetAllValid(lhs_not_null);
418: 			for (idx_t i = lhs_not_null; i < lhs_table.count; ++i) {
419: 				mask.SetInvalid(i);
420: 			}
421: 		}
422: 		// So we make a set of keys that have the validity mask set for the
423: 		PhysicalJoin::ConstructMarkJoinResult(lhs_table.keys, payload, chunk, found_match, gstate.table->has_null);
424: 		break;
425: 	}
426: 	case JoinType::SEMI:
427: 		PhysicalJoin::ConstructSemiJoinResult(payload, chunk, found_match);
428: 		break;
429: 	case JoinType::ANTI:
430: 		PhysicalJoin::ConstructAntiJoinResult(payload, chunk, found_match);
431: 		break;
432: 	default:
433: 		throw NotImplementedException("Unimplemented join type for merge join");
434: 	}
435: }
436: 
437: static idx_t MergeJoinComplexBlocks(BlockMergeInfo &l, BlockMergeInfo &r, const ExpressionType comparison,
438:                                     idx_t &prev_left_index) {
439: 	const auto cmp = MergeJoinComparisonValue(comparison);
440: 
441: 	// The sort parameters should all be the same
442: 	D_ASSERT(l.state.sort_layout.all_constant == r.state.sort_layout.all_constant);
443: 	const auto all_constant = r.state.sort_layout.all_constant;
444: 	D_ASSERT(l.state.external == r.state.external);
445: 	const auto external = l.state.external;
446: 
447: 	// There should only be one sorted block if they have been sorted
448: 	D_ASSERT(l.state.sorted_blocks.size() == 1);
449: 	SBScanState lread(l.state.buffer_manager, l.state);
450: 	lread.sb = l.state.sorted_blocks[0].get();
451: 	D_ASSERT(lread.sb->radix_sorting_data.size() == 1);
452: 	MergeJoinPinSortingBlock(lread, l.block_idx);
453: 	auto l_start = MergeJoinRadixPtr(lread, 0);
454: 	auto l_ptr = MergeJoinRadixPtr(lread, l.entry_idx);
455: 
456: 	D_ASSERT(r.state.sorted_blocks.size() == 1);
457: 	SBScanState rread(r.state.buffer_manager, r.state);
458: 	rread.sb = r.state.sorted_blocks[0].get();
459: 
460: 	if (r.entry_idx >= r.not_null) {
461: 		return 0;
462: 	}
463: 
464: 	MergeJoinPinSortingBlock(rread, r.block_idx);
465: 	auto r_ptr = MergeJoinRadixPtr(rread, r.entry_idx);
466: 
467: 	const auto cmp_size = l.state.sort_layout.comparison_size;
468: 	const auto entry_size = l.state.sort_layout.entry_size;
469: 
470: 	idx_t result_count = 0;
471: 	while (true) {
472: 		if (l.entry_idx < prev_left_index) {
473: 			// left side smaller: found match
474: 			l.result.set_index(result_count, sel_t(l.entry_idx));
475: 			r.result.set_index(result_count, sel_t(r.entry_idx));
476: 			result_count++;
477: 			// move left side forward
478: 			l.entry_idx++;
479: 			l_ptr += entry_size;
480: 			if (result_count == STANDARD_VECTOR_SIZE) {
481: 				// out of space!
482: 				break;
483: 			}
484: 			continue;
485: 		}
486: 		if (l.entry_idx < l.not_null) {
487: 			int comp_res;
488: 			if (all_constant) {
489: 				comp_res = FastMemcmp(l_ptr, r_ptr, cmp_size);
490: 			} else {
491: 				lread.entry_idx = l.entry_idx;
492: 				rread.entry_idx = r.entry_idx;
493: 				comp_res = Comparators::CompareTuple(lread, rread, l_ptr, r_ptr, l.state.sort_layout, external);
494: 			}
495: 			if (comp_res <= cmp) {
496: 				// left side smaller: found match
497: 				l.result.set_index(result_count, sel_t(l.entry_idx));
498: 				r.result.set_index(result_count, sel_t(r.entry_idx));
499: 				result_count++;
500: 				// move left side forward
501: 				l.entry_idx++;
502: 				l_ptr += entry_size;
503: 				if (result_count == STANDARD_VECTOR_SIZE) {
504: 					// out of space!
505: 					break;
506: 				}
507: 				continue;
508: 			}
509: 		}
510: 
511: 		prev_left_index = l.entry_idx;
512: 		// right side smaller or equal, or left side exhausted: move
513: 		// right pointer forward reset left side to start
514: 		r.entry_idx++;
515: 		if (r.entry_idx >= r.not_null) {
516: 			break;
517: 		}
518: 		r_ptr += entry_size;
519: 
520: 		l_ptr = l_start;
521: 		l.entry_idx = 0;
522: 	}
523: 
524: 	return result_count;
525: }
526: 
527: OperatorResultType PhysicalPiecewiseMergeJoin::ResolveComplexJoin(ExecutionContext &context, DataChunk &input,
528:                                                                   DataChunk &chunk, OperatorState &state_p) const {
529: 	auto &state = state_p.Cast<PiecewiseMergeJoinState>();
530: 	auto &gstate = sink_state->Cast<MergeJoinGlobalState>();
531: 	auto &rsorted = *gstate.table->global_sort_state.sorted_blocks[0];
532: 	const auto left_cols = input.ColumnCount();
533: 	const auto tail_cols = conditions.size() - 1;
534: 
535: 	state.payload_heap_handles.clear();
536: 	do {
537: 		if (state.first_fetch) {
538: 			state.ResolveJoinKeys(input);
539: 
540: 			state.right_chunk_index = 0;
541: 			state.right_base = 0;
542: 			state.left_position = 0;
543: 			state.prev_left_index = 0;
544: 			state.right_position = 0;
545: 			state.first_fetch = false;
546: 			state.finished = false;
547: 		}
548: 		if (state.finished) {
549: 			if (state.left_outer.Enabled()) {
550: 				// left join: before we move to the next chunk, see if we need to output any vectors that didn't
551: 				// have a match found
552: 				state.left_outer.ConstructLeftJoinResult(state.lhs_payload, chunk);
553: 				state.left_outer.Reset();
554: 			}
555: 			state.first_fetch = true;
556: 			state.finished = false;
557: 			return OperatorResultType::NEED_MORE_INPUT;
558: 		}
559: 
560: 		auto &lhs_table = *state.lhs_local_table;
561: 		const auto lhs_not_null = lhs_table.count - lhs_table.has_null;
562: 		BlockMergeInfo left_info(*state.lhs_global_state, 0, state.left_position, lhs_not_null);
563: 
564: 		const auto &rblock = *rsorted.radix_sorting_data[state.right_chunk_index];
565: 		const auto rhs_not_null =
566: 		    SortedBlockNotNull(state.right_base, rblock.count, gstate.table->count - gstate.table->has_null);
567: 		BlockMergeInfo right_info(gstate.table->global_sort_state, state.right_chunk_index, state.right_position,
568: 		                          rhs_not_null);
569: 
570: 		idx_t result_count =
571: 		    MergeJoinComplexBlocks(left_info, right_info, conditions[0].comparison, state.prev_left_index);
572: 		if (result_count == 0) {
573: 			// exhausted this chunk on the right side
574: 			// move to the next right chunk
575: 			state.left_position = 0;
576: 			state.right_position = 0;
577: 			state.right_base += rsorted.radix_sorting_data[state.right_chunk_index]->count;
578: 			state.right_chunk_index++;
579: 			if (state.right_chunk_index >= rsorted.radix_sorting_data.size()) {
580: 				state.finished = true;
581: 			}
582: 		} else {
583: 			// found matches: extract them
584: 			chunk.Reset();
585: 			for (idx_t c = 0; c < state.lhs_payload.ColumnCount(); ++c) {
586: 				chunk.data[c].Slice(state.lhs_payload.data[c], left_info.result, result_count);
587: 			}
588: 			state.payload_heap_handles.push_back(SliceSortedPayload(chunk, right_info.state, right_info.block_idx,
589: 			                                                        right_info.result, result_count, left_cols));
590: 			chunk.SetCardinality(result_count);
591: 
592: 			auto sel = FlatVector::IncrementalSelectionVector();
593: 			if (tail_cols) {
594: 				// If there are more expressions to compute,
595: 				// split the result chunk into the left and right halves
596: 				// so we can compute the values for comparison.
597: 				chunk.Split(state.rhs_input, left_cols);
598: 				state.rhs_executor.SetChunk(state.rhs_input);
599: 				state.rhs_keys.Reset();
600: 
601: 				auto tail_count = result_count;
602: 				for (size_t cmp_idx = 1; cmp_idx < conditions.size(); ++cmp_idx) {
603: 					Vector left(lhs_table.keys.data[cmp_idx]);
604: 					left.Slice(left_info.result, result_count);
605: 
606: 					auto &right = state.rhs_keys.data[cmp_idx];
607: 					state.rhs_executor.ExecuteExpression(cmp_idx, right);
608: 
609: 					if (tail_count < result_count) {
610: 						left.Slice(*sel, tail_count);
611: 						right.Slice(*sel, tail_count);
612: 					}
613: 					tail_count =
614: 					    SelectJoinTail(conditions[cmp_idx].comparison, left, right, sel, tail_count, &state.sel);
615: 					sel = &state.sel;
616: 				}
617: 				chunk.Fuse(state.rhs_input);
618: 
619: 				if (tail_count < result_count) {
620: 					result_count = tail_count;
621: 					chunk.Slice(*sel, result_count);
622: 				}
623: 			}
624: 
625: 			// found matches: mark the found matches if required
626: 			if (state.left_outer.Enabled()) {
627: 				for (idx_t i = 0; i < result_count; i++) {
628: 					state.left_outer.SetMatch(left_info.result[sel->get_index(i)]);
629: 				}
630: 			}
631: 			if (gstate.table->found_match) {
632: 				//	Absolute position of the block + start position inside that block
633: 				for (idx_t i = 0; i < result_count; i++) {
634: 					gstate.table->found_match[state.right_base + right_info.result[sel->get_index(i)]] = true;
635: 				}
636: 			}
637: 			chunk.SetCardinality(result_count);
638: 			chunk.Verify();
639: 		}
640: 	} while (chunk.size() == 0);
641: 	return OperatorResultType::HAVE_MORE_OUTPUT;
642: }
643: 
644: OperatorResultType PhysicalPiecewiseMergeJoin::ExecuteInternal(ExecutionContext &context, DataChunk &input,
645:                                                                DataChunk &chunk, GlobalOperatorState &gstate_p,
646:                                                                OperatorState &state) const {
647: 	auto &gstate = sink_state->Cast<MergeJoinGlobalState>();
648: 
649: 	if (gstate.Count() == 0) {
650: 		// empty RHS
651: 		if (!EmptyResultIfRHSIsEmpty()) {
652: 			ConstructEmptyJoinResult(join_type, gstate.table->has_null, input, chunk);
653: 			return OperatorResultType::NEED_MORE_INPUT;
654: 		} else {
655: 			return OperatorResultType::FINISHED;
656: 		}
657: 	}
658: 
659: 	input.Verify();
660: 	switch (join_type) {
661: 	case JoinType::SEMI:
662: 	case JoinType::ANTI:
663: 	case JoinType::MARK:
664: 		// simple joins can have max STANDARD_VECTOR_SIZE matches per chunk
665: 		ResolveSimpleJoin(context, input, chunk, state);
666: 		return OperatorResultType::NEED_MORE_INPUT;
667: 	case JoinType::LEFT:
668: 	case JoinType::INNER:
669: 	case JoinType::RIGHT:
670: 	case JoinType::OUTER:
671: 		return ResolveComplexJoin(context, input, chunk, state);
672: 	default:
673: 		throw NotImplementedException("Unimplemented type for piecewise merge loop join!");
674: 	}
675: }
676: 
677: //===--------------------------------------------------------------------===//
678: // Source
679: //===--------------------------------------------------------------------===//
680: class PiecewiseJoinScanState : public GlobalSourceState {
681: public:
682: 	explicit PiecewiseJoinScanState(const PhysicalPiecewiseMergeJoin &op) : op(op), right_outer_position(0) {
683: 	}
684: 
685: 	mutex lock;
686: 	const PhysicalPiecewiseMergeJoin &op;
687: 	unique_ptr<PayloadScanner> scanner;
688: 	idx_t right_outer_position;
689: 
690: public:
691: 	idx_t MaxThreads() override {
692: 		auto &sink = op.sink_state->Cast<MergeJoinGlobalState>();
693: 		return sink.Count() / (STANDARD_VECTOR_SIZE * idx_t(10));
694: 	}
695: };
696: 
697: unique_ptr<GlobalSourceState> PhysicalPiecewiseMergeJoin::GetGlobalSourceState(ClientContext &context) const {
698: 	return make_uniq<PiecewiseJoinScanState>(*this);
699: }
700: 
701: SourceResultType PhysicalPiecewiseMergeJoin::GetData(ExecutionContext &context, DataChunk &result,
702:                                                      OperatorSourceInput &input) const {
703: 	D_ASSERT(PropagatesBuildSide(join_type));
704: 	// check if we need to scan any unmatched tuples from the RHS for the full/right outer join
705: 	auto &sink = sink_state->Cast<MergeJoinGlobalState>();
706: 	auto &state = input.global_state.Cast<PiecewiseJoinScanState>();
707: 
708: 	lock_guard<mutex> l(state.lock);
709: 	if (!state.scanner) {
710: 		// Initialize scanner (if not yet initialized)
711: 		auto &sort_state = sink.table->global_sort_state;
712: 		if (sort_state.sorted_blocks.empty()) {
713: 			return SourceResultType::FINISHED;
714: 		}
715: 		state.scanner = make_uniq<PayloadScanner>(*sort_state.sorted_blocks[0]->payload_data, sort_state);
716: 	}
717: 
718: 	// if the LHS is exhausted in a FULL/RIGHT OUTER JOIN, we scan the found_match for any chunks we
719: 	// still need to output
720: 	const auto found_match = sink.table->found_match.get();
721: 
722: 	DataChunk rhs_chunk;
723: 	rhs_chunk.Initialize(Allocator::Get(context.client), sink.table->global_sort_state.payload_layout.GetTypes());
724: 	SelectionVector rsel(STANDARD_VECTOR_SIZE);
725: 	for (;;) {
726: 		// Read the next sorted chunk
727: 		state.scanner->Scan(rhs_chunk);
728: 
729: 		const auto count = rhs_chunk.size();
730: 		if (count == 0) {
731: 			return result.size() == 0 ? SourceResultType::FINISHED : SourceResultType::HAVE_MORE_OUTPUT;
732: 		}
733: 
734: 		idx_t result_count = 0;
735: 		// figure out which tuples didn't find a match in the RHS
736: 		for (idx_t i = 0; i < count; i++) {
737: 			if (!found_match[state.right_outer_position + i]) {
738: 				rsel.set_index(result_count++, i);
739: 			}
740: 		}
741: 		state.right_outer_position += count;
742: 
743: 		if (result_count > 0) {
744: 			// if there were any tuples that didn't find a match, output them
745: 			const idx_t left_column_count = children[0]->types.size();
746: 			for (idx_t col_idx = 0; col_idx < left_column_count; ++col_idx) {
747: 				result.data[col_idx].SetVectorType(VectorType::CONSTANT_VECTOR);
748: 				ConstantVector::SetNull(result.data[col_idx], true);
749: 			}
750: 			const idx_t right_column_count = children[1]->types.size();
751: 			;
752: 			for (idx_t col_idx = 0; col_idx < right_column_count; ++col_idx) {
753: 				result.data[left_column_count + col_idx].Slice(rhs_chunk.data[col_idx], rsel, result_count);
754: 			}
755: 			result.SetCardinality(result_count);
756: 			break;
757: 		}
758: 	}
759: 
760: 	return result.size() == 0 ? SourceResultType::FINISHED : SourceResultType::HAVE_MORE_OUTPUT;
761: }
762: 
763: } // namespace duckdb
[end of src/execution/operator/join/physical_piecewise_merge_join.cpp]
[start of src/include/duckdb/common/types/column/column_data_allocator.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/types/column/column_data_allocator.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/column/column_data_collection.hpp"
12: 
13: namespace duckdb {
14: 
15: struct ChunkMetaData;
16: struct VectorMetaData;
17: 
18: struct BlockMetaData {
19: 	//! The underlying block handle
20: 	shared_ptr<BlockHandle> handle;
21: 	//! How much space is currently used within the block
22: 	uint32_t size;
23: 	//! How much space is available in the block
24: 	uint32_t capacity;
25: 
26: 	uint32_t Capacity();
27: };
28: 
29: class ColumnDataAllocator {
30: public:
31: 	explicit ColumnDataAllocator(Allocator &allocator);
32: 	explicit ColumnDataAllocator(BufferManager &buffer_manager);
33: 	ColumnDataAllocator(ClientContext &context, ColumnDataAllocatorType allocator_type);
34: 	ColumnDataAllocator(ColumnDataAllocator &allocator);
35: 
36: 	//! Returns an allocator object to allocate with. This returns the allocator in IN_MEMORY_ALLOCATOR, and a buffer
37: 	//! allocator in case of BUFFER_MANAGER_ALLOCATOR.
38: 	Allocator &GetAllocator();
39: 	//! Returns the buffer manager, if this is not an in-memory allocation.
40: 	BufferManager &GetBufferManager();
41: 	//! Returns the allocator type
42: 	ColumnDataAllocatorType GetType() {
43: 		return type;
44: 	}
45: 	void MakeShared() {
46: 		shared = true;
47: 	}
48: 	bool IsShared() const {
49: 		return shared;
50: 	}
51: 	idx_t BlockCount() const {
52: 		return blocks.size();
53: 	}
54: 	idx_t SizeInBytes() const {
55: 		idx_t total_size = 0;
56: 		for (const auto &block : blocks) {
57: 			total_size += block.size;
58: 		}
59: 		return total_size;
60: 	}
61: 	idx_t AllocationSize() const {
62: 		return allocated_size;
63: 	}
64: 
65: public:
66: 	void AllocateData(idx_t size, uint32_t &block_id, uint32_t &offset, ChunkManagementState *chunk_state);
67: 
68: 	void Initialize(ColumnDataAllocator &other);
69: 	void InitializeChunkState(ChunkManagementState &state, ChunkMetaData &meta_data);
70: 	data_ptr_t GetDataPointer(ChunkManagementState &state, uint32_t block_id, uint32_t offset);
71: 	void UnswizzlePointers(ChunkManagementState &state, Vector &result, idx_t v_offset, uint16_t count,
72: 	                       uint32_t block_id, uint32_t offset);
73: 
74: 	//! Prevents the block with the given id from being added to the eviction queue
75: 	void SetDestroyBufferUponUnpin(uint32_t block_id);
76: 
77: private:
78: 	void AllocateEmptyBlock(idx_t size);
79: 	BufferHandle AllocateBlock(idx_t size);
80: 	BufferHandle Pin(uint32_t block_id);
81: 
82: 	bool HasBlocks() const {
83: 		return !blocks.empty();
84: 	}
85: 
86: private:
87: 	void AllocateBuffer(idx_t size, uint32_t &block_id, uint32_t &offset, ChunkManagementState *chunk_state);
88: 	void AllocateMemory(idx_t size, uint32_t &block_id, uint32_t &offset, ChunkManagementState *chunk_state);
89: 	void AssignPointer(uint32_t &block_id, uint32_t &offset, data_ptr_t pointer);
90: 
91: private:
92: 	ColumnDataAllocatorType type;
93: 	union {
94: 		//! The allocator object (if this is a IN_MEMORY_ALLOCATOR)
95: 		Allocator *allocator;
96: 		//! The buffer manager (if this is a BUFFER_MANAGER_ALLOCATOR)
97: 		BufferManager *buffer_manager;
98: 	} alloc;
99: 	//! The set of blocks used by the column data collection
100: 	vector<BlockMetaData> blocks;
101: 	//! The set of allocated data
102: 	vector<AllocatedData> allocated_data;
103: 	//! Whether this ColumnDataAllocator is shared across ColumnDataCollections that allocate in parallel
104: 	bool shared = false;
105: 	//! Lock used in case this ColumnDataAllocator is shared across threads
106: 	mutex lock;
107: 	//! Total allocated size
108: 	idx_t allocated_size = 0;
109: };
110: 
111: } // namespace duckdb
[end of src/include/duckdb/common/types/column/column_data_allocator.hpp]
[start of src/include/duckdb/storage/buffer/buffer_pool.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/buffer/buffer_pool.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/array.hpp"
12: #include "duckdb/common/enums/memory_tag.hpp"
13: #include "duckdb/common/file_buffer.hpp"
14: #include "duckdb/common/mutex.hpp"
15: #include "duckdb/common/typedefs.hpp"
16: #include "duckdb/storage/buffer/block_handle.hpp"
17: 
18: namespace duckdb {
19: 
20: class TemporaryMemoryManager;
21: struct EvictionQueue;
22: 
23: struct BufferEvictionNode {
24: 	BufferEvictionNode() {
25: 	}
26: 	BufferEvictionNode(weak_ptr<BlockHandle> handle_p, idx_t eviction_seq_num);
27: 
28: 	weak_ptr<BlockHandle> handle;
29: 	idx_t handle_sequence_number;
30: 
31: 	bool CanUnload(BlockHandle &handle_p);
32: 	shared_ptr<BlockHandle> TryGetBlockHandle();
33: };
34: 
35: //! The BufferPool is in charge of handling memory management for one or more databases. It defines memory limits
36: //! and implements priority eviction among all users of the pool.
37: class BufferPool {
38: 	friend class BlockHandle;
39: 	friend class BlockManager;
40: 	friend class BufferManager;
41: 	friend class StandardBufferManager;
42: 
43: public:
44: 	BufferPool(idx_t maximum_memory, bool track_eviction_timestamps, idx_t allocator_bulk_deallocation_flush_threshold);
45: 	virtual ~BufferPool();
46: 
47: 	//! Set a new memory limit to the buffer pool, throws an exception if the new limit is too low and not enough
48: 	//! blocks can be evicted
49: 	void SetLimit(idx_t limit, const char *exception_postscript);
50: 
51: 	//! If bulk deallocation larger than this occurs, flush outstanding allocations
52: 	void SetAllocatorBulkDeallocationFlushThreshold(idx_t threshold);
53: 
54: 	void UpdateUsedMemory(MemoryTag tag, int64_t size);
55: 
56: 	idx_t GetUsedMemory() const;
57: 
58: 	idx_t GetMaxMemory() const;
59: 
60: 	virtual idx_t GetQueryMaxMemory() const;
61: 
62: 	TemporaryMemoryManager &GetTemporaryMemoryManager();
63: 
64: protected:
65: 	//! Evict blocks until the currently used memory + extra_memory fit, returns false if this was not possible
66: 	//! (i.e. not enough blocks could be evicted)
67: 	//! If the "buffer" argument is specified AND the system can find a buffer to re-use for the given allocation size
68: 	//! "buffer" will be made to point to the re-usable memory. Note that this is not guaranteed.
69: 	//! Returns a pair. result.first indicates if eviction was successful. result.second contains the
70: 	//! reservation handle, which can be moved to the BlockHandle that will own the reservation.
71: 	struct EvictionResult {
72: 		bool success;
73: 		TempBufferPoolReservation reservation;
74: 	};
75: 	virtual EvictionResult EvictBlocks(MemoryTag tag, idx_t extra_memory, idx_t memory_limit,
76: 	                                   unique_ptr<FileBuffer> *buffer = nullptr);
77: 	virtual EvictionResult EvictBlocksInternal(EvictionQueue &queue, MemoryTag tag, idx_t extra_memory,
78: 	                                           idx_t memory_limit, unique_ptr<FileBuffer> *buffer = nullptr);
79: 
80: 	//! Purge all blocks that haven't been pinned within the last N seconds
81: 	idx_t PurgeAgedBlocks(uint32_t max_age_sec);
82: 	idx_t PurgeAgedBlocksInternal(EvictionQueue &queue, uint32_t max_age_sec, int64_t now, int64_t limit);
83: 	//! Garbage collect dead nodes in the eviction queue.
84: 	void PurgeQueue(FileBufferType type);
85: 	//! Add a buffer handle to the eviction queue. Returns true, if the queue is
86: 	//! ready to be purged, and false otherwise.
87: 	bool AddToEvictionQueue(shared_ptr<BlockHandle> &handle);
88: 	//! Gets the eviction queue for the specified type
89: 	EvictionQueue &GetEvictionQueueForType(FileBufferType type);
90: 	//! Increments the dead nodes for the queue with specified type
91: 	void IncrementDeadNodes(FileBufferType type);
92: 
93: protected:
94: 	enum class MemoryUsageCaches {
95: 		FLUSH,
96: 		NO_FLUSH,
97: 	};
98: 
99: 	struct MemoryUsage {
100: 		//! The maximum difference between memory statistics and actual usage is 2MB (64 * 32k)
101: 		static constexpr idx_t MEMORY_USAGE_CACHE_COUNT = 64;
102: 		static constexpr idx_t MEMORY_USAGE_CACHE_THRESHOLD = 32 << 10;
103: 		static constexpr idx_t TOTAL_MEMORY_USAGE_INDEX = MEMORY_TAG_COUNT;
104: 		using MemoryUsageCounters = array<atomic<int64_t>, MEMORY_TAG_COUNT + 1>;
105: 
106: 		//! global memory usage counters
107: 		MemoryUsageCounters memory_usage;
108: 		//! cache memory usage to improve performance
109: 		array<MemoryUsageCounters, MEMORY_USAGE_CACHE_COUNT> memory_usage_caches;
110: 
111: 		MemoryUsage();
112: 
113: 		idx_t GetUsedMemory(MemoryUsageCaches cache) {
114: 			return GetUsedMemory(TOTAL_MEMORY_USAGE_INDEX, cache);
115: 		}
116: 
117: 		idx_t GetUsedMemory(MemoryTag tag, MemoryUsageCaches cache) {
118: 			return GetUsedMemory((idx_t)tag, cache);
119: 		}
120: 
121: 		idx_t GetUsedMemory(idx_t index, MemoryUsageCaches cache) {
122: 			if (cache == MemoryUsageCaches::NO_FLUSH) {
123: 				auto used_memory = memory_usage[index].load(std::memory_order_relaxed);
124: 				return used_memory > 0 ? static_cast<idx_t>(used_memory) : 0;
125: 			}
126: 			int64_t cached = 0;
127: 			for (auto &cache : memory_usage_caches) {
128: 				cached += cache[index].exchange(0, std::memory_order_relaxed);
129: 			}
130: 			auto used_memory = memory_usage[index].fetch_add(cached, std::memory_order_relaxed) + cached;
131: 			return used_memory > 0 ? static_cast<idx_t>(used_memory) : 0;
132: 		}
133: 
134: 		void UpdateUsedMemory(MemoryTag tag, int64_t size);
135: 	};
136: 
137: 	//! The lock for changing the memory limit
138: 	mutex limit_lock;
139: 	//! The maximum amount of memory that the buffer manager can keep (in bytes)
140: 	atomic<idx_t> maximum_memory;
141: 	//! If bulk deallocation larger than this occurs, flush outstanding allocations
142: 	atomic<idx_t> allocator_bulk_deallocation_flush_threshold;
143: 	//! Record timestamps of buffer manager unpin() events. Usable by custom eviction policies.
144: 	bool track_eviction_timestamps;
145: 	//! Eviction queues
146: 	vector<unique_ptr<EvictionQueue>> queues;
147: 	//! Memory manager for concurrently used temporary memory, e.g., for physical operators
148: 	unique_ptr<TemporaryMemoryManager> temporary_memory_manager;
149: 	//! To improve performance, MemoryUsage maintains counter caches based on current cpu or thread id,
150: 	//! and only updates the global counter when the cache value exceeds a threshold.
151: 	//! Therefore, the statistics may have slight differences from the actual memory usage.
152: 	mutable MemoryUsage memory_usage;
153: };
154: 
155: } // namespace duckdb
[end of src/include/duckdb/storage/buffer/buffer_pool.hpp]
[start of src/optimizer/filter_pushdown.cpp]
1: #include "duckdb/optimizer/filter_pushdown.hpp"
2: 
3: #include "duckdb/optimizer/filter_combiner.hpp"
4: #include "duckdb/optimizer/optimizer.hpp"
5: #include "duckdb/planner/expression_iterator.hpp"
6: #include "duckdb/planner/operator/logical_comparison_join.hpp"
7: #include "duckdb/planner/operator/logical_filter.hpp"
8: #include "duckdb/planner/operator/logical_join.hpp"
9: #include "duckdb/planner/operator/logical_projection.hpp"
10: #include "duckdb/planner/operator/logical_window.hpp"
11: 
12: namespace duckdb {
13: 
14: using Filter = FilterPushdown::Filter;
15: 
16: void FilterPushdown::CheckMarkToSemi(LogicalOperator &op, unordered_set<idx_t> &table_bindings) {
17: 	switch (op.type) {
18: 	case LogicalOperatorType::LOGICAL_COMPARISON_JOIN: {
19: 		auto &join = op.Cast<LogicalComparisonJoin>();
20: 		if (join.join_type != JoinType::MARK) {
21: 			break;
22: 		}
23: 		// if the projected table bindings include the mark join index,
24: 		if (table_bindings.find(join.mark_index) != table_bindings.end()) {
25: 			join.convert_mark_to_semi = false;
26: 		}
27: 		break;
28: 	}
29: 	// you need to store table.column index.
30: 	// if you get to a projection, you need to change the table_bindings passed so they reflect the
31: 	// table index of the original expression they originated from.
32: 	case LogicalOperatorType::LOGICAL_PROJECTION: {
33: 		// when we encounter a projection, replace the table_bindings with
34: 		// the tables in the projection
35: 		auto plan_bindings = op.GetColumnBindings();
36: 		auto &proj = op.Cast<LogicalProjection>();
37: 		auto proj_bindings = proj.GetColumnBindings();
38: 		unordered_set<idx_t> new_table_bindings;
39: 		for (auto &binding : proj_bindings) {
40: 			auto col_index = binding.column_index;
41: 			auto &expr = proj.expressions.at(col_index);
42: 			vector<ColumnBinding> bindings_to_keep;
43: 			ExpressionIterator::EnumerateExpression(expr, [&](Expression &child) {
44: 				if (expr->expression_class == ExpressionClass::BOUND_COLUMN_REF) {
45: 					auto &col_ref = expr->Cast<BoundColumnRefExpression>();
46: 					bindings_to_keep.push_back(col_ref.binding);
47: 				}
48: 			});
49: 			for (auto &expr_binding : bindings_to_keep) {
50: 				new_table_bindings.insert(expr_binding.table_index);
51: 			}
52: 			table_bindings = new_table_bindings;
53: 		}
54: 		break;
55: 	}
56: 	default:
57: 		break;
58: 	}
59: 
60: 	// recurse into the children to find mark joins and project their indexes.
61: 	for (auto &child : op.children) {
62: 		CheckMarkToSemi(*child, table_bindings);
63: 	}
64: }
65: 
66: FilterPushdown::FilterPushdown(Optimizer &optimizer, bool convert_mark_joins)
67:     : optimizer(optimizer), combiner(optimizer.context), convert_mark_joins(convert_mark_joins) {
68: }
69: 
70: unique_ptr<LogicalOperator> FilterPushdown::Rewrite(unique_ptr<LogicalOperator> op) {
71: 	D_ASSERT(!combiner.HasFilters());
72: 	switch (op->type) {
73: 	case LogicalOperatorType::LOGICAL_AGGREGATE_AND_GROUP_BY:
74: 		return PushdownAggregate(std::move(op));
75: 	case LogicalOperatorType::LOGICAL_FILTER:
76: 		return PushdownFilter(std::move(op));
77: 	case LogicalOperatorType::LOGICAL_CROSS_PRODUCT:
78: 		return PushdownCrossProduct(std::move(op));
79: 	case LogicalOperatorType::LOGICAL_COMPARISON_JOIN:
80: 	case LogicalOperatorType::LOGICAL_ANY_JOIN:
81: 	case LogicalOperatorType::LOGICAL_ASOF_JOIN:
82: 	case LogicalOperatorType::LOGICAL_DELIM_JOIN:
83: 		return PushdownJoin(std::move(op));
84: 	case LogicalOperatorType::LOGICAL_PROJECTION:
85: 		return PushdownProjection(std::move(op));
86: 	case LogicalOperatorType::LOGICAL_INTERSECT:
87: 	case LogicalOperatorType::LOGICAL_EXCEPT:
88: 	case LogicalOperatorType::LOGICAL_UNION:
89: 		return PushdownSetOperation(std::move(op));
90: 	case LogicalOperatorType::LOGICAL_DISTINCT:
91: 		return PushdownDistinct(std::move(op));
92: 	case LogicalOperatorType::LOGICAL_ORDER_BY:
93: 		// we can just push directly through these operations without any rewriting
94: 		op->children[0] = Rewrite(std::move(op->children[0]));
95: 		return op;
96: 	case LogicalOperatorType::LOGICAL_MATERIALIZED_CTE:
97: 		op->children[1] = Rewrite(std::move(op->children[1]));
98: 		return op;
99: 	case LogicalOperatorType::LOGICAL_GET:
100: 		return PushdownGet(std::move(op));
101: 	case LogicalOperatorType::LOGICAL_LIMIT:
102: 		return PushdownLimit(std::move(op));
103: 	case LogicalOperatorType::LOGICAL_WINDOW:
104: 		return PushdownWindow(std::move(op));
105: 	default:
106: 		return FinishPushdown(std::move(op));
107: 	}
108: }
109: 
110: ClientContext &FilterPushdown::GetContext() {
111: 	return optimizer.GetContext();
112: }
113: 
114: unique_ptr<LogicalOperator> FilterPushdown::PushdownJoin(unique_ptr<LogicalOperator> op) {
115: 	D_ASSERT(op->type == LogicalOperatorType::LOGICAL_COMPARISON_JOIN ||
116: 	         op->type == LogicalOperatorType::LOGICAL_ASOF_JOIN || op->type == LogicalOperatorType::LOGICAL_ANY_JOIN ||
117: 	         op->type == LogicalOperatorType::LOGICAL_DELIM_JOIN);
118: 	auto &join = op->Cast<LogicalJoin>();
119: 	if (!join.left_projection_map.empty() || !join.right_projection_map.empty()) {
120: 		// cannot push down further otherwise the projection maps won't be preserved
121: 		return FinishPushdown(std::move(op));
122: 	}
123: 
124: 	unordered_set<idx_t> left_bindings, right_bindings;
125: 	LogicalJoin::GetTableReferences(*op->children[0], left_bindings);
126: 	LogicalJoin::GetTableReferences(*op->children[1], right_bindings);
127: 
128: 	switch (join.join_type) {
129: 	case JoinType::INNER:
130: 		return PushdownInnerJoin(std::move(op), left_bindings, right_bindings);
131: 	case JoinType::LEFT:
132: 		return PushdownLeftJoin(std::move(op), left_bindings, right_bindings);
133: 	case JoinType::MARK:
134: 		return PushdownMarkJoin(std::move(op), left_bindings, right_bindings);
135: 	case JoinType::SINGLE:
136: 		return PushdownSingleJoin(std::move(op), left_bindings, right_bindings);
137: 	case JoinType::SEMI:
138: 	case JoinType::ANTI:
139: 		return PushdownSemiAntiJoin(std::move(op));
140: 	default:
141: 		// unsupported join type: stop pushing down
142: 		return FinishPushdown(std::move(op));
143: 	}
144: }
145: void FilterPushdown::PushFilters() {
146: 	for (auto &f : filters) {
147: 		auto result = combiner.AddFilter(std::move(f->filter));
148: 		D_ASSERT(result != FilterResult::UNSUPPORTED);
149: 		(void)result;
150: 	}
151: 	filters.clear();
152: }
153: 
154: FilterResult FilterPushdown::AddFilter(unique_ptr<Expression> expr) {
155: 	PushFilters();
156: 	// split up the filters by AND predicate
157: 	vector<unique_ptr<Expression>> expressions;
158: 	expressions.push_back(std::move(expr));
159: 	LogicalFilter::SplitPredicates(expressions);
160: 	// push the filters into the combiner
161: 	for (auto &child_expr : expressions) {
162: 		if (combiner.AddFilter(std::move(child_expr)) == FilterResult::UNSATISFIABLE) {
163: 			return FilterResult::UNSATISFIABLE;
164: 		}
165: 	}
166: 	return FilterResult::SUCCESS;
167: }
168: 
169: void FilterPushdown::GenerateFilters() {
170: 	if (!filters.empty()) {
171: 		D_ASSERT(!combiner.HasFilters());
172: 		return;
173: 	}
174: 	combiner.GenerateFilters([&](unique_ptr<Expression> filter) {
175: 		auto f = make_uniq<Filter>();
176: 		f->filter = std::move(filter);
177: 		f->ExtractBindings();
178: 		filters.push_back(std::move(f));
179: 	});
180: }
181: 
182: unique_ptr<LogicalOperator> FilterPushdown::AddLogicalFilter(unique_ptr<LogicalOperator> op,
183:                                                              vector<unique_ptr<Expression>> expressions) {
184: 	if (expressions.empty()) {
185: 		// No left expressions, so needn't to add an extra filter operator.
186: 		return op;
187: 	}
188: 	auto filter = make_uniq<LogicalFilter>();
189: 	if (op->has_estimated_cardinality) {
190: 		// set the filter's estimated cardinality as the child op's.
191: 		// if the filter is created during the filter pushdown optimization, the estimated cardinality will be later
192: 		// overridden during the join order optimization to a more accurate one.
193: 		// if the filter is created during the statistics propagation, the estimated cardinality won't be set unless set
194: 		// here. assuming the filters introduced during the statistics propagation have little effect in reducing the
195: 		// cardinality, we adopt the the cardinality of the child. this could be improved by MinMax info from the
196: 		// statistics propagation
197: 		filter->SetEstimatedCardinality(op->estimated_cardinality);
198: 	}
199: 	filter->expressions = std::move(expressions);
200: 	filter->children.push_back(std::move(op));
201: 	return std::move(filter);
202: }
203: 
204: unique_ptr<LogicalOperator> FilterPushdown::PushFinalFilters(unique_ptr<LogicalOperator> op) {
205: 	vector<unique_ptr<Expression>> expressions;
206: 	for (auto &f : filters) {
207: 		expressions.push_back(std::move(f->filter));
208: 	}
209: 
210: 	return AddLogicalFilter(std::move(op), std::move(expressions));
211: }
212: 
213: unique_ptr<LogicalOperator> FilterPushdown::FinishPushdown(unique_ptr<LogicalOperator> op) {
214: 	// unhandled type, first perform filter pushdown in its children
215: 	for (auto &child : op->children) {
216: 		FilterPushdown pushdown(optimizer, convert_mark_joins);
217: 		child = pushdown.Rewrite(std::move(child));
218: 	}
219: 	// now push any existing filters
220: 	return PushFinalFilters(std::move(op));
221: }
222: 
223: void FilterPushdown::Filter::ExtractBindings() {
224: 	bindings.clear();
225: 	LogicalJoin::GetExpressionBindings(*filter, bindings);
226: }
227: 
228: } // namespace duckdb
[end of src/optimizer/filter_pushdown.cpp]
[start of src/optimizer/join_order/cardinality_estimator.cpp]
1: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
2: #include "duckdb/common/enums/join_type.hpp"
3: #include "duckdb/common/limits.hpp"
4: #include "duckdb/common/printer.hpp"
5: #include "duckdb/planner/expression_iterator.hpp"
6: #include "duckdb/function/table/table_scan.hpp"
7: #include "duckdb/optimizer/join_order/join_node.hpp"
8: #include "duckdb/optimizer/join_order/query_graph_manager.hpp"
9: #include "duckdb/planner/operator/logical_comparison_join.hpp"
10: #include "duckdb/storage/data_table.hpp"
11: 
12: namespace duckdb {
13: 
14: // The filter was made on top of a logical sample or other projection,
15: // but no specific columns are referenced. See issue 4978 number 4.
16: bool CardinalityEstimator::EmptyFilter(FilterInfo &filter_info) {
17: 	if (!filter_info.left_set && !filter_info.right_set) {
18: 		return true;
19: 	}
20: 	return false;
21: }
22: 
23: void CardinalityEstimator::AddRelationTdom(FilterInfo &filter_info) {
24: 	D_ASSERT(filter_info.set.get().count >= 1);
25: 	for (const RelationsToTDom &r2tdom : relations_to_tdoms) {
26: 		auto &i_set = r2tdom.equivalent_relations;
27: 		if (i_set.find(filter_info.left_binding) != i_set.end()) {
28: 			// found an equivalent filter
29: 			return;
30: 		}
31: 	}
32: 
33: 	auto key = ColumnBinding(filter_info.left_binding.table_index, filter_info.left_binding.column_index);
34: 	RelationsToTDom new_r2tdom(column_binding_set_t({key}));
35: 
36: 	relations_to_tdoms.emplace_back(new_r2tdom);
37: }
38: 
39: bool CardinalityEstimator::SingleColumnFilter(duckdb::FilterInfo &filter_info) {
40: 	if (filter_info.left_set && filter_info.right_set && filter_info.set.get().count > 1) {
41: 		// Both set and are from different relations
42: 		return false;
43: 	}
44: 	if (EmptyFilter(filter_info)) {
45: 		return false;
46: 	}
47: 	if (filter_info.join_type == JoinType::SEMI || filter_info.join_type == JoinType::ANTI) {
48: 		return false;
49: 	}
50: 	return true;
51: }
52: 
53: vector<idx_t> CardinalityEstimator::DetermineMatchingEquivalentSets(optional_ptr<FilterInfo> filter_info) {
54: 	vector<idx_t> matching_equivalent_sets;
55: 	idx_t equivalent_relation_index = 0;
56: 
57: 	for (const RelationsToTDom &r2tdom : relations_to_tdoms) {
58: 		auto &i_set = r2tdom.equivalent_relations;
59: 		if (i_set.find(filter_info->left_binding) != i_set.end()) {
60: 			matching_equivalent_sets.push_back(equivalent_relation_index);
61: 		} else if (i_set.find(filter_info->right_binding) != i_set.end()) {
62: 			// don't add both left and right to the matching_equivalent_sets
63: 			// since both left and right get added to that index anyway.
64: 			matching_equivalent_sets.push_back(equivalent_relation_index);
65: 		}
66: 		equivalent_relation_index++;
67: 	}
68: 	return matching_equivalent_sets;
69: }
70: 
71: void CardinalityEstimator::AddToEquivalenceSets(optional_ptr<FilterInfo> filter_info,
72:                                                 vector<idx_t> matching_equivalent_sets) {
73: 	D_ASSERT(matching_equivalent_sets.size() <= 2);
74: 	if (matching_equivalent_sets.size() > 1) {
75: 		// an equivalence relation is connecting two sets of equivalence relations
76: 		// so push all relations from the second set into the first. Later we will delete
77: 		// the second set.
78: 		for (ColumnBinding i : relations_to_tdoms.at(matching_equivalent_sets[1]).equivalent_relations) {
79: 			relations_to_tdoms.at(matching_equivalent_sets[0]).equivalent_relations.insert(i);
80: 		}
81: 		for (auto &column_name : relations_to_tdoms.at(matching_equivalent_sets[1]).column_names) {
82: 			relations_to_tdoms.at(matching_equivalent_sets[0]).column_names.push_back(column_name);
83: 		}
84: 		relations_to_tdoms.at(matching_equivalent_sets[1]).equivalent_relations.clear();
85: 		relations_to_tdoms.at(matching_equivalent_sets[1]).column_names.clear();
86: 		relations_to_tdoms.at(matching_equivalent_sets[0]).filters.push_back(filter_info);
87: 		// add all values of one set to the other, delete the empty one
88: 	} else if (matching_equivalent_sets.size() == 1) {
89: 		auto &tdom_i = relations_to_tdoms.at(matching_equivalent_sets.at(0));
90: 		tdom_i.equivalent_relations.insert(filter_info->left_binding);
91: 		tdom_i.equivalent_relations.insert(filter_info->right_binding);
92: 		tdom_i.filters.push_back(filter_info);
93: 	} else if (matching_equivalent_sets.empty()) {
94: 		column_binding_set_t tmp;
95: 		tmp.insert(filter_info->left_binding);
96: 		tmp.insert(filter_info->right_binding);
97: 		relations_to_tdoms.emplace_back(tmp);
98: 		relations_to_tdoms.back().filters.push_back(filter_info);
99: 	}
100: }
101: 
102: void CardinalityEstimator::InitEquivalentRelations(const vector<unique_ptr<FilterInfo>> &filter_infos) {
103: 	// For each filter, we fill keep track of the index of the equivalent relation set
104: 	// the left and right relation needs to be added to.
105: 	for (auto &filter : filter_infos) {
106: 		if (SingleColumnFilter(*filter)) {
107: 			// Filter on one relation, (i.e. string or range filter on a column).
108: 			// Grab the first relation and add it to  the equivalence_relations
109: 			AddRelationTdom(*filter);
110: 			continue;
111: 		} else if (EmptyFilter(*filter)) {
112: 			continue;
113: 		}
114: 		D_ASSERT(filter->left_set->count >= 1);
115: 		D_ASSERT(filter->right_set->count >= 1);
116: 
117: 		auto matching_equivalent_sets = DetermineMatchingEquivalentSets(filter.get());
118: 		AddToEquivalenceSets(filter.get(), matching_equivalent_sets);
119: 	}
120: 	RemoveEmptyTotalDomains();
121: }
122: 
123: void CardinalityEstimator::RemoveEmptyTotalDomains() {
124: 	auto remove_start = std::remove_if(relations_to_tdoms.begin(), relations_to_tdoms.end(),
125: 	                                   [](RelationsToTDom &r_2_tdom) { return r_2_tdom.equivalent_relations.empty(); });
126: 	relations_to_tdoms.erase(remove_start, relations_to_tdoms.end());
127: }
128: 
129: double CardinalityEstimator::GetNumerator(JoinRelationSet &set) {
130: 	double numerator = 1;
131: 	for (idx_t i = 0; i < set.count; i++) {
132: 		auto &single_node_set = set_manager.GetJoinRelation(set.relations[i]);
133: 		auto card_helper = relation_set_2_cardinality[single_node_set.ToString()];
134: 		numerator *= card_helper.cardinality_before_filters == 0 ? 1 : card_helper.cardinality_before_filters;
135: 	}
136: 	return numerator;
137: }
138: 
139: bool EdgeConnects(FilterInfoWithTotalDomains &edge, Subgraph2Denominator &subgraph) {
140: 	if (edge.filter_info->left_set) {
141: 		if (JoinRelationSet::IsSubset(*subgraph.relations, *edge.filter_info->left_set)) {
142: 			// cool
143: 			return true;
144: 		}
145: 	}
146: 	if (edge.filter_info->right_set) {
147: 		if (JoinRelationSet::IsSubset(*subgraph.relations, *edge.filter_info->right_set)) {
148: 			return true;
149: 		}
150: 	}
151: 	return false;
152: }
153: 
154: vector<FilterInfoWithTotalDomains> GetEdges(vector<RelationsToTDom> &relations_to_tdom,
155:                                             JoinRelationSet &requested_set) {
156: 	vector<FilterInfoWithTotalDomains> res;
157: 	for (auto &relation_2_tdom : relations_to_tdom) {
158: 		for (auto &filter : relation_2_tdom.filters) {
159: 			if (JoinRelationSet::IsSubset(requested_set, filter->set)) {
160: 				FilterInfoWithTotalDomains new_edge(filter, relation_2_tdom);
161: 				res.push_back(new_edge);
162: 			}
163: 		}
164: 	}
165: 	return res;
166: }
167: 
168: vector<idx_t> SubgraphsConnectedByEdge(FilterInfoWithTotalDomains &edge, vector<Subgraph2Denominator> &subgraphs) {
169: 	vector<idx_t> res;
170: 	if (subgraphs.empty()) {
171: 		return res;
172: 	} else {
173: 		// check the combinations of subgraphs and see if the edge connects two of them,
174: 		// if so, return the indexes of the two subgraphs within the vector
175: 		for (idx_t outer = 0; outer != subgraphs.size(); outer++) {
176: 			// check if the edge connects two subgraphs.
177: 			for (idx_t inner = outer + 1; inner != subgraphs.size(); inner++) {
178: 				if (EdgeConnects(edge, subgraphs.at(outer)) && EdgeConnects(edge, subgraphs.at(inner))) {
179: 					// order is important because we will delete the inner subgraph later
180: 					res.push_back(outer);
181: 					res.push_back(inner);
182: 					return res;
183: 				}
184: 			}
185: 			// if the edge does not connect two subgraphs, see if the edge connects with just outer
186: 			// merge subgraph.at(outer) with the RelationSet(s) that edge connects
187: 			if (EdgeConnects(edge, subgraphs.at(outer))) {
188: 				res.push_back(outer);
189: 				return res;
190: 			}
191: 		}
192: 	}
193: 	// this edge connects only the relations it connects. Return an empty result so a new subgraph is created.
194: 	return res;
195: }
196: 
197: JoinRelationSet &CardinalityEstimator::UpdateNumeratorRelations(Subgraph2Denominator left, Subgraph2Denominator right,
198:                                                                 FilterInfoWithTotalDomains &filter) {
199: 	switch (filter.filter_info->join_type) {
200: 	case JoinType::SEMI:
201: 	case JoinType::ANTI: {
202: 		if (JoinRelationSet::IsSubset(*left.relations, *filter.filter_info->left_set) &&
203: 		    JoinRelationSet::IsSubset(*right.relations, *filter.filter_info->right_set)) {
204: 			return *left.numerator_relations;
205: 		}
206: 		return *right.numerator_relations;
207: 	}
208: 	default:
209: 		// cross product or inner join
210: 		return set_manager.Union(*left.numerator_relations, *right.numerator_relations);
211: 	}
212: }
213: 
214: double CardinalityEstimator::CalculateUpdatedDenom(Subgraph2Denominator left, Subgraph2Denominator right,
215:                                                    FilterInfoWithTotalDomains &filter) {
216: 	double new_denom = left.denom * right.denom;
217: 	switch (filter.filter_info->join_type) {
218: 	case JoinType::INNER: {
219: 		bool set = false;
220: 		ExpressionType comparison_type = ExpressionType::COMPARE_EQUAL;
221: 		ExpressionIterator::EnumerateExpression(filter.filter_info->filter, [&](Expression &expr) {
222: 			if (expr.expression_class == ExpressionClass::BOUND_COMPARISON) {
223: 				comparison_type = expr.type;
224: 				set = true;
225: 				return;
226: 			}
227: 		});
228: 		if (!set) {
229: 			new_denom *= filter.has_tdom_hll ? (double)filter.tdom_hll : (double)filter.tdom_no_hll;
230: 			// no comparison is taking place, so the denominator is just the product of the left and right
231: 			return new_denom;
232: 		}
233: 		// extra_ratio helps represents how many tuples will be filtered out if the comparison evaluates to
234: 		// false. set to 1 to assume cross product.
235: 		double extra_ratio = 1;
236: 		switch (comparison_type) {
237: 		case ExpressionType::COMPARE_EQUAL:
238: 		case ExpressionType::COMPARE_NOT_DISTINCT_FROM:
239: 			// extra ration stays 1
240: 			extra_ratio = filter.has_tdom_hll ? (double)filter.tdom_hll : (double)filter.tdom_no_hll;
241: 			break;
242: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO:
243: 		case ExpressionType::COMPARE_LESSTHAN:
244: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
245: 		case ExpressionType::COMPARE_GREATERTHAN:
246: 			// start with the selectivity of equality
247: 			extra_ratio = filter.has_tdom_hll ? (double)filter.tdom_hll : (double)filter.tdom_no_hll;
248: 			// now assume every tuple will match 2.5 times (on average)
249: 			extra_ratio *= static_cast<double>(1) / CardinalityEstimator::DEFAULT_LT_GT_MULTIPLIER;
250: 			break;
251: 		case ExpressionType::COMPARE_NOTEQUAL:
252: 		case ExpressionType::COMPARE_DISTINCT_FROM:
253: 			// basically assume cross product.
254: 			extra_ratio = 1;
255: 			break;
256: 		default:
257: 			break;
258: 		}
259: 		new_denom *= extra_ratio;
260: 		return new_denom;
261: 	}
262: 	case JoinType::SEMI:
263: 	case JoinType::ANTI: {
264: 		if (JoinRelationSet::IsSubset(*left.relations, *filter.filter_info->left_set) &&
265: 		    JoinRelationSet::IsSubset(*right.relations, *filter.filter_info->right_set)) {
266: 			new_denom = left.denom * CardinalityEstimator::DEFAULT_SEMI_ANTI_SELECTIVITY;
267: 			return new_denom;
268: 		}
269: 		new_denom = right.denom * CardinalityEstimator::DEFAULT_SEMI_ANTI_SELECTIVITY;
270: 		return new_denom;
271: 	}
272: 	default:
273: 		// cross product
274: 		return new_denom;
275: 	}
276: }
277: 
278: DenomInfo CardinalityEstimator::GetDenominator(JoinRelationSet &set) {
279: 	vector<Subgraph2Denominator> subgraphs;
280: 
281: 	// Finding the denominator is tricky. You need to go through the tdoms in decreasing order
282: 	// Then loop through all filters in the equivalence set of the tdom to see if both the
283: 	// left and right relations are in the new set, if so you can use that filter.
284: 	// You must also make sure that the filters all relations in the given set, so we use subgraphs
285: 	// that should eventually merge into one connected graph that joins all the relations
286: 	// TODO: Implement a method to cache subgraphs so you don't have to build them up every
287: 	// time the cardinality of a new set is requested
288: 
289: 	// relations_to_tdoms has already been sorted by largest to smallest total domain
290: 	// then we look through the filters for the relations_to_tdoms,
291: 	// and we start to choose the filters that join relations in the set.
292: 
293: 	// edges are guaranteed to be in order of largest tdom to smallest tdom.
294: 	auto edges = GetEdges(relations_to_tdoms, set);
295: 	for (auto &edge : edges) {
296: 		auto subgraph_connections = SubgraphsConnectedByEdge(edge, subgraphs);
297: 
298: 		if (subgraph_connections.empty()) {
299: 			// create a subgraph out of left and right, then merge right into left and add left to subgraphs.
300: 			// this helps cover a case where there are no subgraphs yet, and the only join filter is a SEMI JOIN
301: 			auto left_subgraph = Subgraph2Denominator();
302: 			auto right_subgraph = Subgraph2Denominator();
303: 			left_subgraph.relations = edge.filter_info->left_set;
304: 			left_subgraph.numerator_relations = edge.filter_info->left_set;
305: 			right_subgraph.relations = edge.filter_info->right_set;
306: 			right_subgraph.numerator_relations = edge.filter_info->right_set;
307: 			left_subgraph.numerator_relations = &UpdateNumeratorRelations(left_subgraph, right_subgraph, edge);
308: 			left_subgraph.relations = edge.filter_info->set.get();
309: 			left_subgraph.denom = CalculateUpdatedDenom(left_subgraph, right_subgraph, edge);
310: 			subgraphs.push_back(left_subgraph);
311: 		} else if (subgraph_connections.size() == 1) {
312: 			auto left_subgraph = &subgraphs.at(subgraph_connections.at(0));
313: 			auto right_subgraph = Subgraph2Denominator();
314: 			right_subgraph.relations = edge.filter_info->right_set;
315: 			right_subgraph.numerator_relations = edge.filter_info->right_set;
316: 			if (JoinRelationSet::IsSubset(*left_subgraph->relations, *right_subgraph.relations)) {
317: 				right_subgraph.relations = edge.filter_info->left_set;
318: 				right_subgraph.numerator_relations = edge.filter_info->left_set;
319: 			}
320: 
321: 			if (JoinRelationSet::IsSubset(*left_subgraph->relations, *edge.filter_info->left_set) &&
322: 			    JoinRelationSet::IsSubset(*left_subgraph->relations, *edge.filter_info->right_set)) {
323: 				// here we have an edge that connects the same subgraph to the same subgraph. Just continue. no need to
324: 				// update the denom
325: 				continue;
326: 			}
327: 			left_subgraph->numerator_relations = &UpdateNumeratorRelations(*left_subgraph, right_subgraph, edge);
328: 			left_subgraph->relations = &set_manager.Union(*left_subgraph->relations, *right_subgraph.relations);
329: 			left_subgraph->denom = CalculateUpdatedDenom(*left_subgraph, right_subgraph, edge);
330: 		} else if (subgraph_connections.size() == 2) {
331: 			// The two subgraphs in the subgraph_connections can be merged by this edge.
332: 			D_ASSERT(subgraph_connections.at(0) < subgraph_connections.at(1));
333: 			auto subgraph_to_merge_into = &subgraphs.at(subgraph_connections.at(0));
334: 			auto subgraph_to_delete = &subgraphs.at(subgraph_connections.at(1));
335: 			subgraph_to_merge_into->relations =
336: 			    &set_manager.Union(*subgraph_to_merge_into->relations, *subgraph_to_delete->relations);
337: 			subgraph_to_merge_into->numerator_relations =
338: 			    &UpdateNumeratorRelations(*subgraph_to_merge_into, *subgraph_to_delete, edge);
339: 			subgraph_to_delete->relations = nullptr;
340: 			subgraph_to_merge_into->denom = CalculateUpdatedDenom(*subgraph_to_merge_into, *subgraph_to_delete, edge);
341: 			auto remove_start = std::remove_if(subgraphs.begin(), subgraphs.end(),
342: 			                                   [](Subgraph2Denominator &s) { return !s.relations; });
343: 			subgraphs.erase(remove_start, subgraphs.end());
344: 		}
345: 		if (subgraphs.size() == 1 && subgraphs.at(0).relations->ToString() == set.ToString()) {
346: 			// the first subgraph has connected all the desired relations, no need to iterate
347: 			// through the rest of the edges.
348: 			break;
349: 		}
350: 	}
351: 
352: 	// It's possible cross-products were added and are not present in the filters in the relation_2_tdom
353: 	// structures. When that's the case, merge all remaining subgraphs.
354: 	if (subgraphs.size() > 1) {
355: 		auto final_subgraph = subgraphs.at(0);
356: 		for (auto merge_with = subgraphs.begin() + 1; merge_with != subgraphs.end(); merge_with++) {
357: 			D_ASSERT(final_subgraph.relations && merge_with->relations);
358: 			final_subgraph.relations = &set_manager.Union(*final_subgraph.relations, *merge_with->relations);
359: 			D_ASSERT(final_subgraph.numerator_relations && merge_with->numerator_relations);
360: 			final_subgraph.numerator_relations =
361: 			    &set_manager.Union(*final_subgraph.numerator_relations, *merge_with->numerator_relations);
362: 			final_subgraph.denom *= merge_with->denom;
363: 		}
364: 	}
365: 	// can happen if a table has cardinality 0, a tdom is set to 0, or if a cross product is used.
366: 	if (subgraphs.empty() || subgraphs.at(0).denom == 0) {
367: 		// denominator is 1 and numerators are a cross product of cardinalities.
368: 		return DenomInfo(set, 1, 1);
369: 	}
370: 	return DenomInfo(*subgraphs.at(0).numerator_relations, 1, subgraphs.at(0).denom);
371: }
372: 
373: template <>
374: double CardinalityEstimator::EstimateCardinalityWithSet(JoinRelationSet &new_set) {
375: 
376: 	if (relation_set_2_cardinality.find(new_set.ToString()) != relation_set_2_cardinality.end()) {
377: 		return relation_set_2_cardinality[new_set.ToString()].cardinality_before_filters;
378: 	}
379: 
380: 	// can happen if a table has cardinality 0, or a tdom is set to 0
381: 	auto denom = GetDenominator(new_set);
382: 	auto numerator = GetNumerator(denom.numerator_relations);
383: 
384: 	double result = numerator / denom.denominator;
385: 	auto new_entry = CardinalityHelper(result);
386: 	relation_set_2_cardinality[new_set.ToString()] = new_entry;
387: 	return result;
388: }
389: 
390: template <>
391: idx_t CardinalityEstimator::EstimateCardinalityWithSet(JoinRelationSet &new_set) {
392: 	auto cardinality_as_double = EstimateCardinalityWithSet<double>(new_set);
393: 	auto max = NumericLimits<idx_t>::Maximum();
394: 	if (cardinality_as_double >= (double)max) {
395: 		return max;
396: 	}
397: 	return (idx_t)cardinality_as_double;
398: }
399: 
400: bool SortTdoms(const RelationsToTDom &a, const RelationsToTDom &b) {
401: 	if (a.has_tdom_hll && b.has_tdom_hll) {
402: 		return a.tdom_hll > b.tdom_hll;
403: 	}
404: 	if (a.has_tdom_hll) {
405: 		return a.tdom_hll > b.tdom_no_hll;
406: 	}
407: 	if (b.has_tdom_hll) {
408: 		return a.tdom_no_hll > b.tdom_hll;
409: 	}
410: 	return a.tdom_no_hll > b.tdom_no_hll;
411: }
412: 
413: void CardinalityEstimator::InitCardinalityEstimatorProps(optional_ptr<JoinRelationSet> set, RelationStats &stats) {
414: 	// Get the join relation set
415: 	D_ASSERT(stats.stats_initialized);
416: 	auto relation_cardinality = stats.cardinality;
417: 
418: 	auto card_helper = CardinalityHelper((double)relation_cardinality);
419: 	relation_set_2_cardinality[set->ToString()] = card_helper;
420: 
421: 	UpdateTotalDomains(set, stats);
422: 
423: 	// sort relations from greatest tdom to lowest tdom.
424: 	std::sort(relations_to_tdoms.begin(), relations_to_tdoms.end(), SortTdoms);
425: }
426: 
427: void CardinalityEstimator::UpdateTotalDomains(optional_ptr<JoinRelationSet> set, RelationStats &stats) {
428: 	D_ASSERT(set->count == 1);
429: 	auto relation_id = set->relations[0];
430: 	//! Initialize the distinct count for all columns used in joins with the current relation.
431: 	//	D_ASSERT(stats.column_distinct_count.size() >= 1);
432: 
433: 	for (idx_t i = 0; i < stats.column_distinct_count.size(); i++) {
434: 		//! for every column used in a filter in the relation, get the distinct count via HLL, or assume it to be
435: 		//! the cardinality
436: 		// Update the relation_to_tdom set with the estimated distinct count (or tdom) calculated above
437: 		auto key = ColumnBinding(relation_id, i);
438: 		for (auto &relation_to_tdom : relations_to_tdoms) {
439: 			column_binding_set_t i_set = relation_to_tdom.equivalent_relations;
440: 			if (i_set.find(key) == i_set.end()) {
441: 				continue;
442: 			}
443: 			auto distinct_count = stats.column_distinct_count.at(i);
444: 			if (distinct_count.from_hll && relation_to_tdom.has_tdom_hll) {
445: 				relation_to_tdom.tdom_hll = MaxValue(relation_to_tdom.tdom_hll, distinct_count.distinct_count);
446: 			} else if (distinct_count.from_hll && !relation_to_tdom.has_tdom_hll) {
447: 				relation_to_tdom.has_tdom_hll = true;
448: 				relation_to_tdom.tdom_hll = distinct_count.distinct_count;
449: 			} else {
450: 				relation_to_tdom.tdom_no_hll = MinValue(distinct_count.distinct_count, relation_to_tdom.tdom_no_hll);
451: 			}
452: 			break;
453: 		}
454: 	}
455: }
456: 
457: // LCOV_EXCL_START
458: 
459: void CardinalityEstimator::AddRelationNamesToTdoms(vector<RelationStats> &stats) {
460: #ifdef DEBUG
461: 	for (auto &total_domain : relations_to_tdoms) {
462: 		for (auto &binding : total_domain.equivalent_relations) {
463: 			D_ASSERT(binding.table_index < stats.size());
464: 			string column_name;
465: 			if (binding.column_index < stats[binding.table_index].column_names.size()) {
466: 				column_name = stats[binding.table_index].column_names[binding.column_index];
467: 			} else {
468: 				column_name = "[unknown]";
469: 			}
470: 			total_domain.column_names.push_back(column_name);
471: 		}
472: 	}
473: #endif
474: }
475: 
476: void CardinalityEstimator::PrintRelationToTdomInfo() {
477: 	for (auto &total_domain : relations_to_tdoms) {
478: 		string domain = "Following columns have the same distinct count: ";
479: 		for (auto &column_name : total_domain.column_names) {
480: 			domain += column_name + ", ";
481: 		}
482: 		bool have_hll = total_domain.has_tdom_hll;
483: 		domain += "\n TOTAL DOMAIN = " + to_string(have_hll ? total_domain.tdom_hll : total_domain.tdom_no_hll);
484: 		Printer::Print(domain);
485: 	}
486: }
487: 
488: // LCOV_EXCL_STOP
489: 
490: } // namespace duckdb
[end of src/optimizer/join_order/cardinality_estimator.cpp]
[start of src/planner/binder.cpp]
1: #include "duckdb/planner/binder.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/aggregate_function_catalog_entry.hpp"
4: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
6: #include "duckdb/common/enum_util.hpp"
7: #include "duckdb/common/helper.hpp"
8: #include "duckdb/main/config.hpp"
9: #include "duckdb/main/database.hpp"
10: #include "duckdb/parser/expression/function_expression.hpp"
11: #include "duckdb/parser/expression/subquery_expression.hpp"
12: #include "duckdb/parser/parsed_expression_iterator.hpp"
13: #include "duckdb/parser/query_node/list.hpp"
14: #include "duckdb/parser/query_node/select_node.hpp"
15: #include "duckdb/parser/statement/list.hpp"
16: #include "duckdb/parser/tableref/list.hpp"
17: #include "duckdb/parser/tableref/table_function_ref.hpp"
18: #include "duckdb/planner/bound_query_node.hpp"
19: #include "duckdb/planner/expression.hpp"
20: #include "duckdb/planner/expression_binder/returning_binder.hpp"
21: #include "duckdb/planner/operator/logical_projection.hpp"
22: #include "duckdb/planner/operator/logical_sample.hpp"
23: #include "duckdb/planner/query_node/list.hpp"
24: #include "duckdb/planner/tableref/list.hpp"
25: 
26: #include <algorithm>
27: 
28: namespace duckdb {
29: 
30: Binder &Binder::GetRootBinder() {
31: 	reference<Binder> root = *this;
32: 	while (root.get().parent) {
33: 		root = *root.get().parent;
34: 	}
35: 	return root.get();
36: }
37: 
38: idx_t Binder::GetBinderDepth() const {
39: 	const_reference<Binder> root = *this;
40: 	idx_t depth = 1;
41: 	while (root.get().parent) {
42: 		depth++;
43: 		root = *root.get().parent;
44: 	}
45: 	return depth;
46: }
47: 
48: shared_ptr<Binder> Binder::CreateBinder(ClientContext &context, optional_ptr<Binder> parent, BinderType binder_type) {
49: 	auto depth = parent ? parent->GetBinderDepth() : 0;
50: 	if (depth > context.config.max_expression_depth) {
51: 		throw BinderException("Max expression depth limit of %lld exceeded. Use \"SET max_expression_depth TO x\" to "
52: 		                      "increase the maximum expression depth.",
53: 		                      context.config.max_expression_depth);
54: 	}
55: 	return shared_ptr<Binder>(new Binder(context, parent ? parent->shared_from_this() : nullptr, binder_type));
56: }
57: 
58: Binder::Binder(ClientContext &context, shared_ptr<Binder> parent_p, BinderType binder_type)
59:     : context(context), bind_context(*this), parent(std::move(parent_p)), bound_tables(0), binder_type(binder_type),
60:       entry_retriever(context) {
61: 	if (parent) {
62: 		entry_retriever.SetCallback(parent->entry_retriever.GetCallback());
63: 
64: 		// We have to inherit macro and lambda parameter bindings and from the parent binder, if there is a parent.
65: 		macro_binding = parent->macro_binding;
66: 		lambda_bindings = parent->lambda_bindings;
67: 
68: 		if (binder_type == BinderType::REGULAR_BINDER) {
69: 			// We have to inherit CTE bindings from the parent bind_context, if there is a parent.
70: 			bind_context.SetCTEBindings(parent->bind_context.GetCTEBindings());
71: 			bind_context.cte_references = parent->bind_context.cte_references;
72: 			parameters = parent->parameters;
73: 		}
74: 	}
75: }
76: 
77: unique_ptr<BoundCTENode> Binder::BindMaterializedCTE(CommonTableExpressionMap &cte_map) {
78: 	// Extract materialized CTEs from cte_map
79: 	vector<unique_ptr<CTENode>> materialized_ctes;
80: 	for (auto &cte : cte_map.map) {
81: 		auto &cte_entry = cte.second;
82: 		if (cte_entry->materialized == CTEMaterialize::CTE_MATERIALIZE_ALWAYS) {
83: 			auto mat_cte = make_uniq<CTENode>();
84: 			mat_cte->ctename = cte.first;
85: 			mat_cte->query = cte_entry->query->node->Copy();
86: 			mat_cte->aliases = cte_entry->aliases;
87: 			materialized_ctes.push_back(std::move(mat_cte));
88: 		}
89: 	}
90: 
91: 	if (materialized_ctes.empty()) {
92: 		return nullptr;
93: 	}
94: 
95: 	unique_ptr<CTENode> cte_root = nullptr;
96: 	while (!materialized_ctes.empty()) {
97: 		unique_ptr<CTENode> node_result;
98: 		node_result = std::move(materialized_ctes.back());
99: 		node_result->cte_map = cte_map.Copy();
100: 		if (cte_root) {
101: 			node_result->child = std::move(cte_root);
102: 		} else {
103: 			node_result->child = nullptr;
104: 		}
105: 		cte_root = std::move(node_result);
106: 		materialized_ctes.pop_back();
107: 	}
108: 
109: 	AddCTEMap(cte_map);
110: 	auto bound_cte = BindCTE(cte_root->Cast<CTENode>());
111: 
112: 	return bound_cte;
113: }
114: 
115: template <class T>
116: BoundStatement Binder::BindWithCTE(T &statement) {
117: 	BoundStatement bound_statement;
118: 	auto bound_cte = BindMaterializedCTE(statement.template Cast<T>().cte_map);
119: 	if (bound_cte) {
120: 		reference<BoundCTENode> tail_ref = *bound_cte;
121: 
122: 		while (tail_ref.get().child && tail_ref.get().child->type == QueryNodeType::CTE_NODE) {
123: 			tail_ref = tail_ref.get().child->Cast<BoundCTENode>();
124: 		}
125: 
126: 		auto &tail = tail_ref.get();
127: 		bound_statement = tail.child_binder->Bind(statement.template Cast<T>());
128: 
129: 		tail.types = bound_statement.types;
130: 		tail.names = bound_statement.names;
131: 
132: 		for (auto &c : tail.query_binder->correlated_columns) {
133: 			tail.child_binder->AddCorrelatedColumn(c);
134: 		}
135: 		MoveCorrelatedExpressions(*tail.child_binder);
136: 
137: 		auto plan = std::move(bound_statement.plan);
138: 		bound_statement.plan = CreatePlan(*bound_cte, std::move(plan));
139: 	} else {
140: 		bound_statement = Bind(statement.template Cast<T>());
141: 	}
142: 	return bound_statement;
143: }
144: 
145: BoundStatement Binder::Bind(SQLStatement &statement) {
146: 	root_statement = &statement;
147: 	switch (statement.type) {
148: 	case StatementType::SELECT_STATEMENT:
149: 		return Bind(statement.Cast<SelectStatement>());
150: 	case StatementType::INSERT_STATEMENT:
151: 		return BindWithCTE(statement.Cast<InsertStatement>());
152: 	case StatementType::COPY_STATEMENT:
153: 		return Bind(statement.Cast<CopyStatement>(), CopyToType::COPY_TO_FILE);
154: 	case StatementType::DELETE_STATEMENT:
155: 		return BindWithCTE(statement.Cast<DeleteStatement>());
156: 	case StatementType::UPDATE_STATEMENT:
157: 		return BindWithCTE(statement.Cast<UpdateStatement>());
158: 	case StatementType::RELATION_STATEMENT:
159: 		return Bind(statement.Cast<RelationStatement>());
160: 	case StatementType::CREATE_STATEMENT:
161: 		return Bind(statement.Cast<CreateStatement>());
162: 	case StatementType::DROP_STATEMENT:
163: 		return Bind(statement.Cast<DropStatement>());
164: 	case StatementType::ALTER_STATEMENT:
165: 		return Bind(statement.Cast<AlterStatement>());
166: 	case StatementType::TRANSACTION_STATEMENT:
167: 		return Bind(statement.Cast<TransactionStatement>());
168: 	case StatementType::PRAGMA_STATEMENT:
169: 		return Bind(statement.Cast<PragmaStatement>());
170: 	case StatementType::EXPLAIN_STATEMENT:
171: 		return Bind(statement.Cast<ExplainStatement>());
172: 	case StatementType::VACUUM_STATEMENT:
173: 		return Bind(statement.Cast<VacuumStatement>());
174: 	case StatementType::CALL_STATEMENT:
175: 		return Bind(statement.Cast<CallStatement>());
176: 	case StatementType::EXPORT_STATEMENT:
177: 		return Bind(statement.Cast<ExportStatement>());
178: 	case StatementType::SET_STATEMENT:
179: 		return Bind(statement.Cast<SetStatement>());
180: 	case StatementType::LOAD_STATEMENT:
181: 		return Bind(statement.Cast<LoadStatement>());
182: 	case StatementType::EXTENSION_STATEMENT:
183: 		return Bind(statement.Cast<ExtensionStatement>());
184: 	case StatementType::PREPARE_STATEMENT:
185: 		return Bind(statement.Cast<PrepareStatement>());
186: 	case StatementType::EXECUTE_STATEMENT:
187: 		return Bind(statement.Cast<ExecuteStatement>());
188: 	case StatementType::LOGICAL_PLAN_STATEMENT:
189: 		return Bind(statement.Cast<LogicalPlanStatement>());
190: 	case StatementType::ATTACH_STATEMENT:
191: 		return Bind(statement.Cast<AttachStatement>());
192: 	case StatementType::DETACH_STATEMENT:
193: 		return Bind(statement.Cast<DetachStatement>());
194: 	case StatementType::COPY_DATABASE_STATEMENT:
195: 		return Bind(statement.Cast<CopyDatabaseStatement>());
196: 	case StatementType::UPDATE_EXTENSIONS_STATEMENT:
197: 		return Bind(statement.Cast<UpdateExtensionsStatement>());
198: 	default: // LCOV_EXCL_START
199: 		throw NotImplementedException("Unimplemented statement type \"%s\" for Bind",
200: 		                              StatementTypeToString(statement.type));
201: 	} // LCOV_EXCL_STOP
202: }
203: 
204: void Binder::AddCTEMap(CommonTableExpressionMap &cte_map) {
205: 	for (auto &cte_it : cte_map.map) {
206: 		AddCTE(cte_it.first, *cte_it.second);
207: 	}
208: }
209: 
210: static void GetTableRefCountsNode(case_insensitive_map_t<idx_t> &cte_ref_counts, QueryNode &node);
211: 
212: static void GetTableRefCountsExpr(case_insensitive_map_t<idx_t> &cte_ref_counts, ParsedExpression &expr) {
213: 	if (expr.type == ExpressionType::SUBQUERY) {
214: 		auto &subquery = expr.Cast<SubqueryExpression>();
215: 		GetTableRefCountsNode(cte_ref_counts, *subquery.subquery->node);
216: 	} else {
217: 		ParsedExpressionIterator::EnumerateChildren(
218: 		    expr, [&](ParsedExpression &expr) { GetTableRefCountsExpr(cte_ref_counts, expr); });
219: 	}
220: }
221: 
222: static void GetTableRefCountsNode(case_insensitive_map_t<idx_t> &cte_ref_counts, QueryNode &node) {
223: 	ParsedExpressionIterator::EnumerateQueryNodeChildren(
224: 	    node, [&](unique_ptr<ParsedExpression> &child) { GetTableRefCountsExpr(cte_ref_counts, *child); },
225: 	    [&](TableRef &ref) {
226: 		    if (ref.type != TableReferenceType::BASE_TABLE) {
227: 			    return;
228: 		    }
229: 		    auto cte_ref_counts_it = cte_ref_counts.find(ref.Cast<BaseTableRef>().table_name);
230: 		    if (cte_ref_counts_it != cte_ref_counts.end()) {
231: 			    cte_ref_counts_it->second++;
232: 		    }
233: 	    });
234: }
235: 
236: static bool ParsedExpressionIsAggregate(Binder &binder, const ParsedExpression &expr) {
237: 	if (expr.GetExpressionClass() == ExpressionClass::FUNCTION) {
238: 		auto &function = expr.Cast<FunctionExpression>();
239: 		QueryErrorContext error_context;
240: 		auto entry = binder.GetCatalogEntry(CatalogType::SCALAR_FUNCTION_ENTRY, function.catalog, function.schema,
241: 		                                    function.function_name, OnEntryNotFound::RETURN_NULL, error_context);
242: 		if (entry && entry->type == CatalogType::AGGREGATE_FUNCTION_ENTRY) {
243: 			return true;
244: 		}
245: 	}
246: 	bool is_aggregate = false;
247: 	ParsedExpressionIterator::EnumerateChildren(
248: 	    expr, [&](const ParsedExpression &child) { is_aggregate |= ParsedExpressionIsAggregate(binder, child); });
249: 	return is_aggregate;
250: }
251: 
252: bool Binder::OptimizeCTEs(QueryNode &node) {
253: 	D_ASSERT(context.config.enable_optimizer);
254: 
255: 	// only applies to nodes that have at least one CTE
256: 	auto &cte_map = node.cte_map.map;
257: 	if (cte_map.empty()) {
258: 		return false;
259: 	}
260: 
261: 	// initialize counts with the CTE names
262: 	case_insensitive_map_t<idx_t> cte_ref_counts;
263: 	for (auto &cte : cte_map) {
264: 		cte_ref_counts[cte.first];
265: 	}
266: 
267: 	// count the references of each CTE
268: 	GetTableRefCountsNode(cte_ref_counts, node);
269: 
270: 	// determine for each CTE whether it should be materialized
271: 	bool result = false;
272: 	for (auto &cte : cte_map) {
273: 		if (cte.second->materialized != CTEMaterialize::CTE_MATERIALIZE_DEFAULT) {
274: 			continue; // only triggers when nothing is specified
275: 		}
276: 		if (bind_context.GetCTEBinding(cte.first)) {
277: 			continue; // there's a CTE in the bind context with an overlapping name, we can't also materialize this
278: 		}
279: 
280: 		auto cte_ref_counts_it = cte_ref_counts.find(cte.first);
281: 		D_ASSERT(cte_ref_counts_it != cte_ref_counts.end());
282: 
283: 		// only applies to CTEs that are referenced more than once
284: 		if (cte_ref_counts_it->second <= 1) {
285: 			continue;
286: 		}
287: 
288: 		// if the cte is a SELECT node
289: 		if (cte.second->query->node->type != QueryNodeType::SELECT_NODE) {
290: 			continue;
291: 		}
292: 
293: 		// we materialize if the CTE ends in an aggregation
294: 		auto &cte_node = cte.second->query->node->Cast<SelectNode>();
295: 		bool materialize = !cte_node.groups.group_expressions.empty() || !cte_node.groups.grouping_sets.empty();
296: 		// or has a distinct modifier
297: 		for (auto &modifier : cte_node.modifiers) {
298: 			if (materialize) {
299: 				break;
300: 			}
301: 			if (modifier->type == ResultModifierType::DISTINCT_MODIFIER) {
302: 				materialize = true;
303: 			}
304: 		}
305: 		for (auto &sel : cte_node.select_list) {
306: 			if (materialize) {
307: 				break;
308: 			}
309: 			materialize |= ParsedExpressionIsAggregate(*this, *sel);
310: 		}
311: 
312: 		if (materialize) {
313: 			cte.second->materialized = CTEMaterialize::CTE_MATERIALIZE_ALWAYS;
314: 			result = true;
315: 		}
316: 	}
317: 	return result;
318: }
319: 
320: unique_ptr<BoundQueryNode> Binder::BindNode(QueryNode &node) {
321: 	// first we visit the set of CTEs and add them to the bind context
322: 	AddCTEMap(node.cte_map);
323: 	// now we bind the node
324: 	unique_ptr<BoundQueryNode> result;
325: 	switch (node.type) {
326: 	case QueryNodeType::SELECT_NODE:
327: 		result = BindNode(node.Cast<SelectNode>());
328: 		break;
329: 	case QueryNodeType::RECURSIVE_CTE_NODE:
330: 		result = BindNode(node.Cast<RecursiveCTENode>());
331: 		break;
332: 	case QueryNodeType::CTE_NODE:
333: 		result = BindNode(node.Cast<CTENode>());
334: 		break;
335: 	default:
336: 		D_ASSERT(node.type == QueryNodeType::SET_OPERATION_NODE);
337: 		result = BindNode(node.Cast<SetOperationNode>());
338: 		break;
339: 	}
340: 	return result;
341: }
342: 
343: BoundStatement Binder::Bind(QueryNode &node) {
344: 	BoundStatement result;
345: 	if (context.db->config.options.disabled_optimizers.find(OptimizerType::MATERIALIZED_CTE) ==
346: 	        context.db->config.options.disabled_optimizers.end() &&
347: 	    context.config.enable_optimizer && OptimizeCTEs(node)) {
348: 		switch (node.type) {
349: 		case QueryNodeType::SELECT_NODE:
350: 			result = BindWithCTE(node.Cast<SelectNode>());
351: 			break;
352: 		case QueryNodeType::RECURSIVE_CTE_NODE:
353: 			result = BindWithCTE(node.Cast<RecursiveCTENode>());
354: 			break;
355: 		case QueryNodeType::CTE_NODE:
356: 			result = BindWithCTE(node.Cast<CTENode>());
357: 			break;
358: 		default:
359: 			D_ASSERT(node.type == QueryNodeType::SET_OPERATION_NODE);
360: 			result = BindWithCTE(node.Cast<SetOperationNode>());
361: 			break;
362: 		}
363: 	} else {
364: 		auto bound_node = BindNode(node);
365: 
366: 		result.names = bound_node->names;
367: 		result.types = bound_node->types;
368: 
369: 		// and plan it
370: 		result.plan = CreatePlan(*bound_node);
371: 	}
372: 	return result;
373: }
374: 
375: unique_ptr<LogicalOperator> Binder::CreatePlan(BoundQueryNode &node) {
376: 	switch (node.type) {
377: 	case QueryNodeType::SELECT_NODE:
378: 		return CreatePlan(node.Cast<BoundSelectNode>());
379: 	case QueryNodeType::SET_OPERATION_NODE:
380: 		return CreatePlan(node.Cast<BoundSetOperationNode>());
381: 	case QueryNodeType::RECURSIVE_CTE_NODE:
382: 		return CreatePlan(node.Cast<BoundRecursiveCTENode>());
383: 	case QueryNodeType::CTE_NODE:
384: 		return CreatePlan(node.Cast<BoundCTENode>());
385: 	default:
386: 		throw InternalException("Unsupported bound query node type");
387: 	}
388: }
389: 
390: unique_ptr<BoundTableRef> Binder::Bind(TableRef &ref) {
391: 	unique_ptr<BoundTableRef> result;
392: 	switch (ref.type) {
393: 	case TableReferenceType::BASE_TABLE:
394: 		result = Bind(ref.Cast<BaseTableRef>());
395: 		break;
396: 	case TableReferenceType::JOIN:
397: 		result = Bind(ref.Cast<JoinRef>());
398: 		break;
399: 	case TableReferenceType::SUBQUERY:
400: 		result = Bind(ref.Cast<SubqueryRef>());
401: 		break;
402: 	case TableReferenceType::EMPTY_FROM:
403: 		result = Bind(ref.Cast<EmptyTableRef>());
404: 		break;
405: 	case TableReferenceType::TABLE_FUNCTION:
406: 		result = Bind(ref.Cast<TableFunctionRef>());
407: 		break;
408: 	case TableReferenceType::EXPRESSION_LIST:
409: 		result = Bind(ref.Cast<ExpressionListRef>());
410: 		break;
411: 	case TableReferenceType::COLUMN_DATA:
412: 		result = Bind(ref.Cast<ColumnDataRef>());
413: 		break;
414: 	case TableReferenceType::PIVOT:
415: 		result = Bind(ref.Cast<PivotRef>());
416: 		break;
417: 	case TableReferenceType::SHOW_REF:
418: 		result = Bind(ref.Cast<ShowRef>());
419: 		break;
420: 	case TableReferenceType::DELIM_GET:
421: 		result = Bind(ref.Cast<DelimGetRef>());
422: 		break;
423: 	case TableReferenceType::CTE:
424: 	case TableReferenceType::INVALID:
425: 	default:
426: 		throw InternalException("Unknown table ref type (%s)", EnumUtil::ToString(ref.type));
427: 	}
428: 	result->sample = std::move(ref.sample);
429: 	return result;
430: }
431: 
432: unique_ptr<LogicalOperator> Binder::CreatePlan(BoundTableRef &ref) {
433: 	unique_ptr<LogicalOperator> root;
434: 	switch (ref.type) {
435: 	case TableReferenceType::BASE_TABLE:
436: 		root = CreatePlan(ref.Cast<BoundBaseTableRef>());
437: 		break;
438: 	case TableReferenceType::SUBQUERY:
439: 		root = CreatePlan(ref.Cast<BoundSubqueryRef>());
440: 		break;
441: 	case TableReferenceType::JOIN:
442: 		root = CreatePlan(ref.Cast<BoundJoinRef>());
443: 		break;
444: 	case TableReferenceType::TABLE_FUNCTION:
445: 		root = CreatePlan(ref.Cast<BoundTableFunction>());
446: 		break;
447: 	case TableReferenceType::EMPTY_FROM:
448: 		root = CreatePlan(ref.Cast<BoundEmptyTableRef>());
449: 		break;
450: 	case TableReferenceType::EXPRESSION_LIST:
451: 		root = CreatePlan(ref.Cast<BoundExpressionListRef>());
452: 		break;
453: 	case TableReferenceType::COLUMN_DATA:
454: 		root = CreatePlan(ref.Cast<BoundColumnDataRef>());
455: 		break;
456: 	case TableReferenceType::CTE:
457: 		root = CreatePlan(ref.Cast<BoundCTERef>());
458: 		break;
459: 	case TableReferenceType::PIVOT:
460: 		root = CreatePlan(ref.Cast<BoundPivotRef>());
461: 		break;
462: 	case TableReferenceType::DELIM_GET:
463: 		root = CreatePlan(ref.Cast<BoundDelimGetRef>());
464: 		break;
465: 	case TableReferenceType::INVALID:
466: 	default:
467: 		throw InternalException("Unsupported bound table ref type (%s)", EnumUtil::ToString(ref.type));
468: 	}
469: 	// plan the sample clause
470: 	if (ref.sample) {
471: 		root = make_uniq<LogicalSample>(std::move(ref.sample), std::move(root));
472: 	}
473: 	return root;
474: }
475: 
476: void Binder::AddCTE(const string &name, CommonTableExpressionInfo &info) {
477: 	D_ASSERT(!name.empty());
478: 	auto entry = CTE_bindings.find(name);
479: 	if (entry != CTE_bindings.end()) {
480: 		throw InternalException("Duplicate CTE \"%s\" in query!", name);
481: 	}
482: 	CTE_bindings.insert(make_pair(name, reference<CommonTableExpressionInfo>(info)));
483: }
484: 
485: vector<reference<CommonTableExpressionInfo>> Binder::FindCTE(const string &name, bool skip) {
486: 	auto entry = CTE_bindings.find(name);
487: 	vector<reference<CommonTableExpressionInfo>> ctes;
488: 	if (entry != CTE_bindings.end()) {
489: 		if (!skip || entry->second.get().query->node->type == QueryNodeType::RECURSIVE_CTE_NODE) {
490: 			ctes.push_back(entry->second);
491: 		}
492: 	}
493: 	if (parent && binder_type == BinderType::REGULAR_BINDER) {
494: 		auto parent_ctes = parent->FindCTE(name, name == alias);
495: 		ctes.insert(ctes.end(), parent_ctes.begin(), parent_ctes.end());
496: 	}
497: 	return ctes;
498: }
499: 
500: bool Binder::CTEIsAlreadyBound(CommonTableExpressionInfo &cte) {
501: 	if (bound_ctes.find(cte) != bound_ctes.end()) {
502: 		return true;
503: 	}
504: 	if (parent && binder_type == BinderType::REGULAR_BINDER) {
505: 		return parent->CTEIsAlreadyBound(cte);
506: 	}
507: 	return false;
508: }
509: 
510: void Binder::AddBoundView(ViewCatalogEntry &view) {
511: 	// check if the view is already bound
512: 	auto current = this;
513: 	while (current) {
514: 		if (current->bound_views.find(view) != current->bound_views.end()) {
515: 			throw BinderException("infinite recursion detected: attempting to recursively bind view \"%s\"", view.name);
516: 		}
517: 		current = current->parent.get();
518: 	}
519: 	bound_views.insert(view);
520: }
521: 
522: idx_t Binder::GenerateTableIndex() {
523: 	auto &root_binder = GetRootBinder();
524: 	return root_binder.bound_tables++;
525: }
526: 
527: StatementProperties &Binder::GetStatementProperties() {
528: 	auto &root_binder = GetRootBinder();
529: 	return root_binder.prop;
530: }
531: 
532: void Binder::PushExpressionBinder(ExpressionBinder &binder) {
533: 	GetActiveBinders().push_back(binder);
534: }
535: 
536: void Binder::PopExpressionBinder() {
537: 	D_ASSERT(HasActiveBinder());
538: 	GetActiveBinders().pop_back();
539: }
540: 
541: void Binder::SetActiveBinder(ExpressionBinder &binder) {
542: 	D_ASSERT(HasActiveBinder());
543: 	GetActiveBinders().back() = binder;
544: }
545: 
546: ExpressionBinder &Binder::GetActiveBinder() {
547: 	return GetActiveBinders().back();
548: }
549: 
550: bool Binder::HasActiveBinder() {
551: 	return !GetActiveBinders().empty();
552: }
553: 
554: vector<reference<ExpressionBinder>> &Binder::GetActiveBinders() {
555: 	reference<Binder> root = *this;
556: 	while (root.get().parent && root.get().binder_type == BinderType::REGULAR_BINDER) {
557: 		root = *root.get().parent;
558: 	}
559: 	auto &root_binder = root.get();
560: 	return root_binder.active_binders;
561: }
562: 
563: void Binder::AddUsingBindingSet(unique_ptr<UsingColumnSet> set) {
564: 	auto &root_binder = GetRootBinder();
565: 	root_binder.bind_context.AddUsingBindingSet(std::move(set));
566: }
567: 
568: void Binder::MoveCorrelatedExpressions(Binder &other) {
569: 	MergeCorrelatedColumns(other.correlated_columns);
570: 	other.correlated_columns.clear();
571: }
572: 
573: void Binder::MergeCorrelatedColumns(vector<CorrelatedColumnInfo> &other) {
574: 	for (idx_t i = 0; i < other.size(); i++) {
575: 		AddCorrelatedColumn(other[i]);
576: 	}
577: }
578: 
579: void Binder::AddCorrelatedColumn(const CorrelatedColumnInfo &info) {
580: 	// we only add correlated columns to the list if they are not already there
581: 	if (std::find(correlated_columns.begin(), correlated_columns.end(), info) == correlated_columns.end()) {
582: 		correlated_columns.push_back(info);
583: 	}
584: }
585: 
586: bool Binder::HasMatchingBinding(const string &table_name, const string &column_name, ErrorData &error) {
587: 	string empty_schema;
588: 	return HasMatchingBinding(empty_schema, table_name, column_name, error);
589: }
590: 
591: bool Binder::HasMatchingBinding(const string &schema_name, const string &table_name, const string &column_name,
592:                                 ErrorData &error) {
593: 	string empty_catalog;
594: 	return HasMatchingBinding(empty_catalog, schema_name, table_name, column_name, error);
595: }
596: 
597: bool Binder::HasMatchingBinding(const string &catalog_name, const string &schema_name, const string &table_name,
598:                                 const string &column_name, ErrorData &error) {
599: 	optional_ptr<Binding> binding;
600: 	D_ASSERT(!lambda_bindings);
601: 	if (macro_binding && table_name == macro_binding->alias) {
602: 		binding = optional_ptr<Binding>(macro_binding.get());
603: 	} else {
604: 		binding = bind_context.GetBinding(table_name, error);
605: 	}
606: 
607: 	if (!binding) {
608: 		return false;
609: 	}
610: 	if (!catalog_name.empty() || !schema_name.empty()) {
611: 		auto catalog_entry = binding->GetStandardEntry();
612: 		if (!catalog_entry) {
613: 			return false;
614: 		}
615: 		if (!catalog_name.empty() && catalog_entry->catalog.GetName() != catalog_name) {
616: 			return false;
617: 		}
618: 		if (!schema_name.empty() && catalog_entry->schema.name != schema_name) {
619: 			return false;
620: 		}
621: 		if (catalog_entry->name != table_name) {
622: 			return false;
623: 		}
624: 	}
625: 	bool binding_found;
626: 	binding_found = binding->HasMatchingBinding(column_name);
627: 	if (!binding_found) {
628: 		error = binding->ColumnNotFoundError(column_name);
629: 	}
630: 	return binding_found;
631: }
632: 
633: void Binder::SetBindingMode(BindingMode mode) {
634: 	auto &root_binder = GetRootBinder();
635: 	root_binder.mode = mode;
636: }
637: 
638: BindingMode Binder::GetBindingMode() {
639: 	auto &root_binder = GetRootBinder();
640: 	return root_binder.mode;
641: }
642: 
643: void Binder::SetCanContainNulls(bool can_contain_nulls_p) {
644: 	can_contain_nulls = can_contain_nulls_p;
645: }
646: 
647: void Binder::SetAlwaysRequireRebind() {
648: 	auto &properties = GetStatementProperties();
649: 	properties.always_require_rebind = true;
650: }
651: 
652: void Binder::AddTableName(string table_name) {
653: 	auto &root_binder = GetRootBinder();
654: 	root_binder.table_names.insert(std::move(table_name));
655: }
656: 
657: void Binder::AddReplacementScan(const string &table_name, unique_ptr<TableRef> replacement) {
658: 	auto &root_binder = GetRootBinder();
659: 	auto it = root_binder.replacement_scans.find(table_name);
660: 	replacement->column_name_alias.clear();
661: 	replacement->alias.clear();
662: 	if (it == root_binder.replacement_scans.end()) {
663: 		root_binder.replacement_scans[table_name] = std::move(replacement);
664: 	} else {
665: 		// A replacement scan by this name was previously registered, we can just use it
666: 	}
667: }
668: 
669: const unordered_set<string> &Binder::GetTableNames() {
670: 	auto &root_binder = GetRootBinder();
671: 	return root_binder.table_names;
672: }
673: 
674: case_insensitive_map_t<unique_ptr<TableRef>> &Binder::GetReplacementScans() {
675: 	auto &root_binder = GetRootBinder();
676: 	return root_binder.replacement_scans;
677: }
678: 
679: // FIXME: this is extremely naive
680: void VerifyNotExcluded(ParsedExpression &expr) {
681: 	if (expr.type == ExpressionType::COLUMN_REF) {
682: 		auto &column_ref = expr.Cast<ColumnRefExpression>();
683: 		if (!column_ref.IsQualified()) {
684: 			return;
685: 		}
686: 		auto &table_name = column_ref.GetTableName();
687: 		if (table_name == "excluded") {
688: 			throw NotImplementedException("'excluded' qualified columns are not supported in the RETURNING clause yet");
689: 		}
690: 		return;
691: 	}
692: 	ParsedExpressionIterator::EnumerateChildren(
693: 	    expr, [&](const ParsedExpression &child) { VerifyNotExcluded((ParsedExpression &)child); });
694: }
695: 
696: BoundStatement Binder::BindReturning(vector<unique_ptr<ParsedExpression>> returning_list, TableCatalogEntry &table,
697:                                      const string &alias, idx_t update_table_index,
698:                                      unique_ptr<LogicalOperator> child_operator, BoundStatement result) {
699: 
700: 	vector<LogicalType> types;
701: 	vector<std::string> names;
702: 
703: 	auto binder = Binder::CreateBinder(context);
704: 
705: 	vector<column_t> bound_columns;
706: 	idx_t column_count = 0;
707: 	for (auto &col : table.GetColumns().Logical()) {
708: 		names.push_back(col.Name());
709: 		types.push_back(col.Type());
710: 		if (!col.Generated()) {
711: 			bound_columns.push_back(column_count);
712: 		}
713: 		column_count++;
714: 	}
715: 
716: 	binder->bind_context.AddBaseTable(update_table_index, alias.empty() ? table.name : alias, names, types,
717: 	                                  bound_columns, &table, false);
718: 	ReturningBinder returning_binder(*binder, context);
719: 
720: 	vector<unique_ptr<Expression>> projection_expressions;
721: 	LogicalType result_type;
722: 	vector<unique_ptr<ParsedExpression>> new_returning_list;
723: 	binder->ExpandStarExpressions(returning_list, new_returning_list);
724: 	for (auto &returning_expr : new_returning_list) {
725: 		VerifyNotExcluded(*returning_expr);
726: 		auto expr = returning_binder.Bind(returning_expr, &result_type);
727: 		result.names.push_back(expr->GetName());
728: 		result.types.push_back(result_type);
729: 		projection_expressions.push_back(std::move(expr));
730: 	}
731: 	if (new_returning_list.empty()) {
732: 		throw BinderException("RETURNING list is empty!");
733: 	}
734: 	auto projection = make_uniq<LogicalProjection>(GenerateTableIndex(), std::move(projection_expressions));
735: 	projection->AddChild(std::move(child_operator));
736: 	D_ASSERT(result.types.size() == result.names.size());
737: 	result.plan = std::move(projection);
738: 	// If an insert/delete/update statement returns data, there are sometimes issues with streaming results
739: 	// where the data modification doesn't take place until the streamed result is exhausted. Once a row is
740: 	// returned, it should be guaranteed that the row has been inserted.
741: 	// see https://github.com/duckdb/duckdb/issues/8310
742: 	auto &properties = GetStatementProperties();
743: 	properties.allow_stream_result = false;
744: 	properties.return_type = StatementReturnType::QUERY_RESULT;
745: 	return result;
746: }
747: 
748: optional_ptr<CatalogEntry> Binder::GetCatalogEntry(CatalogType type, const string &catalog, const string &schema,
749:                                                    const string &name, OnEntryNotFound on_entry_not_found,
750:                                                    QueryErrorContext &error_context) {
751: 	return entry_retriever.GetEntry(type, catalog, schema, name, on_entry_not_found, error_context);
752: }
753: 
754: } // namespace duckdb
[end of src/planner/binder.cpp]
[start of src/storage/buffer/buffer_pool.cpp]
1: #include "duckdb/storage/buffer/buffer_pool.hpp"
2: 
3: #include "duckdb/common/allocator.hpp"
4: #include "duckdb/common/chrono.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/typedefs.hpp"
7: #include "duckdb/parallel/concurrentqueue.hpp"
8: #include "duckdb/parallel/task_scheduler.hpp"
9: #include "duckdb/storage/temporary_memory_manager.hpp"
10: 
11: namespace duckdb {
12: 
13: BufferEvictionNode::BufferEvictionNode(weak_ptr<BlockHandle> handle_p, idx_t eviction_seq_num)
14:     : handle(std::move(handle_p)), handle_sequence_number(eviction_seq_num) {
15: 	D_ASSERT(!handle.expired());
16: }
17: 
18: bool BufferEvictionNode::CanUnload(BlockHandle &handle_p) {
19: 	if (handle_sequence_number != handle_p.eviction_seq_num) {
20: 		// handle was used in between
21: 		return false;
22: 	}
23: 	return handle_p.CanUnload();
24: }
25: 
26: shared_ptr<BlockHandle> BufferEvictionNode::TryGetBlockHandle() {
27: 	auto handle_p = handle.lock();
28: 	if (!handle_p) {
29: 		// BlockHandle has been destroyed
30: 		return nullptr;
31: 	}
32: 	if (!CanUnload(*handle_p)) {
33: 		// handle was used in between
34: 		return nullptr;
35: 	}
36: 	// this is the latest node in the queue with this handle
37: 	return handle_p;
38: }
39: 
40: typedef duckdb_moodycamel::ConcurrentQueue<BufferEvictionNode> eviction_queue_t;
41: 
42: struct EvictionQueue {
43: public:
44: 	EvictionQueue() : evict_queue_insertions(0), total_dead_nodes(0) {
45: 	}
46: 
47: public:
48: 	//! Add a buffer handle to the eviction queue. Returns true, if the queue is
49: 	//! ready to be purged, and false otherwise.
50: 	bool AddToEvictionQueue(BufferEvictionNode &&node);
51: 	//! Tries to dequeue an element from the eviction queue, but only after acquiring the purge queue lock.
52: 	bool TryDequeueWithLock(BufferEvictionNode &node);
53: 	//! Garbage collect dead nodes in the eviction queue.
54: 	void Purge();
55: 	template <typename FN>
56: 	void IterateUnloadableBlocks(FN fn);
57: 
58: 	//! Increment the dead node counter in the purge queue.
59: 	inline void IncrementDeadNodes() {
60: 		total_dead_nodes++;
61: 	}
62: 	//! Decrement the dead node counter in the purge queue.
63: 	inline void DecrementDeadNodes() {
64: 		total_dead_nodes--;
65: 	}
66: 
67: private:
68: 	//! Bulk purge dead nodes from the eviction queue. Then, enqueue those that are still alive.
69: 	void PurgeIteration(const idx_t purge_size);
70: 
71: public:
72: 	//! The concurrent queue
73: 	eviction_queue_t q;
74: 
75: private:
76: 	//! We trigger a purge of the eviction queue every INSERT_INTERVAL insertions
77: 	constexpr static idx_t INSERT_INTERVAL = 4096;
78: 	//! We multiply the base purge size by this value.
79: 	constexpr static idx_t PURGE_SIZE_MULTIPLIER = 2;
80: 	//! We multiply the purge size by this value to determine early-outs. This is the minimum queue size.
81: 	//! We never purge below this point.
82: 	constexpr static idx_t EARLY_OUT_MULTIPLIER = 4;
83: 	//! We multiply the approximate alive nodes by this value to test whether our total dead nodes
84: 	//! exceed their allowed ratio. Must be greater than 1.
85: 	constexpr static idx_t ALIVE_NODE_MULTIPLIER = 4;
86: 
87: private:
88: 	//! Total number of insertions into the eviction queue. This guides the schedule for calling PurgeQueue.
89: 	atomic<idx_t> evict_queue_insertions;
90: 	//! Total dead nodes in the eviction queue. There are two scenarios in which a node dies: (1) we destroy its block
91: 	//! handle, or (2) we insert a newer version into the eviction queue.
92: 	atomic<idx_t> total_dead_nodes;
93: 
94: 	//! Locked, if a queue purge is currently active or we're trying to forcefully evict a node.
95: 	//! Only lets a single thread enter the purge phase.
96: 	mutex purge_lock;
97: 	//! A pre-allocated vector of eviction nodes. We reuse this to keep the allocation overhead of purges small.
98: 	vector<BufferEvictionNode> purge_nodes;
99: };
100: 
101: bool EvictionQueue::AddToEvictionQueue(BufferEvictionNode &&node) {
102: 	q.enqueue(std::move(node));
103: 	return ++evict_queue_insertions % INSERT_INTERVAL == 0;
104: }
105: 
106: bool EvictionQueue::TryDequeueWithLock(BufferEvictionNode &node) {
107: 	lock_guard<mutex> lock(purge_lock);
108: 	return q.try_dequeue(node);
109: }
110: 
111: void EvictionQueue::Purge() {
112: 	// only one thread purges the queue, all other threads early-out
113: 	if (!purge_lock.try_lock()) {
114: 		return;
115: 	}
116: 	lock_guard<mutex> lock {purge_lock, std::adopt_lock};
117: 
118: 	// we purge INSERT_INTERVAL * PURGE_SIZE_MULTIPLIER nodes
119: 	idx_t purge_size = INSERT_INTERVAL * PURGE_SIZE_MULTIPLIER;
120: 
121: 	// get an estimate of the queue size as-of now
122: 	idx_t approx_q_size = q.size_approx();
123: 
124: 	// early-out, if the queue is not big enough to justify purging
125: 	// - we want to keep the LRU characteristic alive
126: 	if (approx_q_size < purge_size * EARLY_OUT_MULTIPLIER) {
127: 		return;
128: 	}
129: 
130: 	// There are two types of situations.
131: 
132: 	// For most scenarios, purging INSERT_INTERVAL * PURGE_SIZE_MULTIPLIER nodes is enough.
133: 	// Purging more nodes than we insert also counters oscillation for scenarios where most nodes are dead.
134: 	// If we always purge slightly more, we trigger a purge less often, as we purge below the trigger.
135: 
136: 	// However, if the pressure on the queue becomes too contested, we need to purge more aggressively,
137: 	// i.e., we actively seek a specific number of dead nodes to purge. We use the total number of existing dead nodes.
138: 	// We detect this situation by observing the queue's ratio between alive vs. dead nodes. If the ratio of alive vs.
139: 	// dead nodes grows faster than we can purge, we keep purging until we hit one of the following conditions.
140: 
141: 	// 2.1. We're back at an approximate queue size less than purge_size * EARLY_OUT_MULTIPLIER.
142: 	// 2.2. We're back at a ratio of 1*alive_node:ALIVE_NODE_MULTIPLIER*dead_nodes.
143: 	// 2.3. We've purged the entire queue: max_purges is zero. This is a worst-case scenario,
144: 	// guaranteeing that we always exit the loop.
145: 
146: 	idx_t max_purges = approx_q_size / purge_size;
147: 	while (max_purges != 0) {
148: 		PurgeIteration(purge_size);
149: 
150: 		// update relevant sizes and potentially early-out
151: 		approx_q_size = q.size_approx();
152: 
153: 		// early-out according to (2.1)
154: 		if (approx_q_size < purge_size * EARLY_OUT_MULTIPLIER) {
155: 			break;
156: 		}
157: 
158: 		idx_t approx_dead_nodes = total_dead_nodes;
159: 		approx_dead_nodes = approx_dead_nodes > approx_q_size ? approx_q_size : approx_dead_nodes;
160: 		idx_t approx_alive_nodes = approx_q_size - approx_dead_nodes;
161: 
162: 		// early-out according to (2.2)
163: 		if (approx_alive_nodes * (ALIVE_NODE_MULTIPLIER - 1) > approx_dead_nodes) {
164: 			break;
165: 		}
166: 
167: 		max_purges--;
168: 	}
169: }
170: 
171: void EvictionQueue::PurgeIteration(const idx_t purge_size) {
172: 	// if this purge is significantly smaller or bigger than the previous purge, then
173: 	// we need to resize the purge_nodes vector. Note that this barely happens, as we
174: 	// purge queue_insertions * PURGE_SIZE_MULTIPLIER nodes
175: 	idx_t previous_purge_size = purge_nodes.size();
176: 	if (purge_size < previous_purge_size / 2 || purge_size > previous_purge_size) {
177: 		purge_nodes.resize(purge_size);
178: 	}
179: 
180: 	// bulk purge
181: 	idx_t actually_dequeued = q.try_dequeue_bulk(purge_nodes.begin(), purge_size);
182: 
183: 	// retrieve all alive nodes that have been wrongly dequeued
184: 	idx_t alive_nodes = 0;
185: 	for (idx_t i = 0; i < actually_dequeued; i++) {
186: 		auto &node = purge_nodes[i];
187: 		auto handle = node.TryGetBlockHandle();
188: 		if (handle) {
189: 			q.enqueue(std::move(node));
190: 			alive_nodes++;
191: 		}
192: 	}
193: 
194: 	total_dead_nodes -= actually_dequeued - alive_nodes;
195: }
196: 
197: BufferPool::BufferPool(idx_t maximum_memory, bool track_eviction_timestamps,
198:                        idx_t allocator_bulk_deallocation_flush_threshold)
199:     : maximum_memory(maximum_memory),
200:       allocator_bulk_deallocation_flush_threshold(allocator_bulk_deallocation_flush_threshold),
201:       track_eviction_timestamps(track_eviction_timestamps),
202:       temporary_memory_manager(make_uniq<TemporaryMemoryManager>()) {
203: 	queues.reserve(FILE_BUFFER_TYPE_COUNT);
204: 	for (idx_t i = 0; i < FILE_BUFFER_TYPE_COUNT; i++) {
205: 		queues.push_back(make_uniq<EvictionQueue>());
206: 	}
207: }
208: BufferPool::~BufferPool() {
209: }
210: 
211: bool BufferPool::AddToEvictionQueue(shared_ptr<BlockHandle> &handle) {
212: 	auto &queue = GetEvictionQueueForType(handle->buffer->type);
213: 
214: 	// The block handle is locked during this operation (Unpin),
215: 	// or the block handle is still a local variable (ConvertToPersistent)
216: 	D_ASSERT(handle->readers == 0);
217: 	auto ts = ++handle->eviction_seq_num;
218: 	if (track_eviction_timestamps) {
219: 		handle->lru_timestamp_msec =
220: 		    std::chrono::time_point_cast<std::chrono::milliseconds>(std::chrono::steady_clock::now())
221: 		        .time_since_epoch()
222: 		        .count();
223: 	}
224: 
225: 	if (ts != 1) {
226: 		// we add a newer version, i.e., we kill exactly one previous version
227: 		queue.IncrementDeadNodes();
228: 	}
229: 
230: 	// Get the eviction queue for the buffer type and add it
231: 	return queue.AddToEvictionQueue(BufferEvictionNode(weak_ptr<BlockHandle>(handle), ts));
232: }
233: 
234: EvictionQueue &BufferPool::GetEvictionQueueForType(FileBufferType type) {
235: 	return *queues[uint8_t(type) - 1];
236: }
237: 
238: void BufferPool::IncrementDeadNodes(FileBufferType type) {
239: 	GetEvictionQueueForType(type).IncrementDeadNodes();
240: }
241: 
242: void BufferPool::UpdateUsedMemory(MemoryTag tag, int64_t size) {
243: 	memory_usage.UpdateUsedMemory(tag, size);
244: }
245: 
246: idx_t BufferPool::GetUsedMemory() const {
247: 	return memory_usage.GetUsedMemory(MemoryUsageCaches::FLUSH);
248: }
249: 
250: idx_t BufferPool::GetMaxMemory() const {
251: 	return maximum_memory;
252: }
253: 
254: idx_t BufferPool::GetQueryMaxMemory() const {
255: 	return GetMaxMemory();
256: }
257: 
258: TemporaryMemoryManager &BufferPool::GetTemporaryMemoryManager() {
259: 	return *temporary_memory_manager;
260: }
261: 
262: BufferPool::EvictionResult BufferPool::EvictBlocks(MemoryTag tag, idx_t extra_memory, idx_t memory_limit,
263:                                                    unique_ptr<FileBuffer> *buffer) {
264: 	// First, we try to evict persistent table data
265: 	auto block_result =
266: 	    EvictBlocksInternal(GetEvictionQueueForType(FileBufferType::BLOCK), tag, extra_memory, memory_limit, buffer);
267: 	if (block_result.success) {
268: 		return block_result;
269: 	}
270: 
271: 	// If that does not succeed, we try to evict temporary data
272: 	auto managed_buffer_result = EvictBlocksInternal(GetEvictionQueueForType(FileBufferType::MANAGED_BUFFER), tag,
273: 	                                                 extra_memory, memory_limit, buffer);
274: 	if (managed_buffer_result.success) {
275: 		return managed_buffer_result;
276: 	}
277: 
278: 	// Finally, we try to evict tiny buffers
279: 	return EvictBlocksInternal(GetEvictionQueueForType(FileBufferType::TINY_BUFFER), tag, extra_memory, memory_limit,
280: 	                           buffer);
281: }
282: 
283: BufferPool::EvictionResult BufferPool::EvictBlocksInternal(EvictionQueue &queue, MemoryTag tag, idx_t extra_memory,
284:                                                            idx_t memory_limit, unique_ptr<FileBuffer> *buffer) {
285: 	TempBufferPoolReservation r(tag, *this, extra_memory);
286: 	bool found = false;
287: 
288: 	if (memory_usage.GetUsedMemory(MemoryUsageCaches::NO_FLUSH) <= memory_limit) {
289: 		if (Allocator::SupportsFlush() && extra_memory > allocator_bulk_deallocation_flush_threshold) {
290: 			Allocator::FlushAll();
291: 		}
292: 		return {true, std::move(r)};
293: 	}
294: 
295: 	queue.IterateUnloadableBlocks([&](BufferEvictionNode &, const shared_ptr<BlockHandle> &handle) {
296: 		// hooray, we can unload the block
297: 		if (buffer && handle->buffer->AllocSize() == extra_memory) {
298: 			// we can re-use the memory directly
299: 			*buffer = handle->UnloadAndTakeBlock();
300: 			found = true;
301: 			return false;
302: 		}
303: 
304: 		// release the memory and mark the block as unloaded
305: 		handle->Unload();
306: 
307: 		if (memory_usage.GetUsedMemory(MemoryUsageCaches::NO_FLUSH) <= memory_limit) {
308: 			found = true;
309: 			return false;
310: 		}
311: 
312: 		// Continue iteration
313: 		return true;
314: 	});
315: 
316: 	if (!found) {
317: 		r.Resize(0);
318: 	} else if (Allocator::SupportsFlush() && extra_memory > allocator_bulk_deallocation_flush_threshold) {
319: 		Allocator::FlushAll();
320: 	}
321: 
322: 	return {found, std::move(r)};
323: }
324: 
325: idx_t BufferPool::PurgeAgedBlocks(uint32_t max_age_sec) {
326: 	int64_t now = std::chrono::time_point_cast<std::chrono::milliseconds>(std::chrono::steady_clock::now())
327: 	                  .time_since_epoch()
328: 	                  .count();
329: 	int64_t limit = now - (static_cast<int64_t>(max_age_sec) * 1000);
330: 	idx_t purged_bytes = 0;
331: 	for (auto &queue : queues) {
332: 		purged_bytes += PurgeAgedBlocksInternal(*queue, max_age_sec, now, limit);
333: 	}
334: 	return purged_bytes;
335: }
336: 
337: idx_t BufferPool::PurgeAgedBlocksInternal(EvictionQueue &queue, uint32_t max_age_sec, int64_t now, int64_t limit) {
338: 	idx_t purged_bytes = 0;
339: 	queue.IterateUnloadableBlocks([&](BufferEvictionNode &node, const shared_ptr<BlockHandle> &handle) {
340: 		// We will unload this block regardless. But stop the iteration immediately afterward if this
341: 		// block is younger than the age threshold.
342: 		bool is_fresh = handle->lru_timestamp_msec >= limit && handle->lru_timestamp_msec <= now;
343: 		purged_bytes += handle->GetMemoryUsage();
344: 		handle->Unload();
345: 		return is_fresh;
346: 	});
347: 	return purged_bytes;
348: }
349: 
350: template <typename FN>
351: void EvictionQueue::IterateUnloadableBlocks(FN fn) {
352: 	for (;;) {
353: 		// get a block to unpin from the queue
354: 		BufferEvictionNode node;
355: 		if (!q.try_dequeue(node)) {
356: 			// we could not dequeue any eviction node, so we try one more time,
357: 			// but more aggressively
358: 			if (!TryDequeueWithLock(node)) {
359: 				return;
360: 			}
361: 		}
362: 
363: 		// get a reference to the underlying block pointer
364: 		auto handle = node.TryGetBlockHandle();
365: 		if (!handle) {
366: 			DecrementDeadNodes();
367: 			continue;
368: 		}
369: 
370: 		// we might be able to free this block: grab the mutex and check if we can free it
371: 		lock_guard<mutex> lock(handle->lock);
372: 		if (!node.CanUnload(*handle)) {
373: 			// something changed in the mean-time, bail out
374: 			DecrementDeadNodes();
375: 			continue;
376: 		}
377: 
378: 		if (!fn(node, handle)) {
379: 			break;
380: 		}
381: 	}
382: }
383: 
384: void BufferPool::PurgeQueue(FileBufferType type) {
385: 	GetEvictionQueueForType(type).Purge();
386: }
387: 
388: void BufferPool::SetLimit(idx_t limit, const char *exception_postscript) {
389: 	lock_guard<mutex> l_lock(limit_lock);
390: 	// try to evict until the limit is reached
391: 	if (!EvictBlocks(MemoryTag::EXTENSION, 0, limit).success) {
392: 		throw OutOfMemoryException(
393: 		    "Failed to change memory limit to %lld: could not free up enough memory for the new limit%s", limit,
394: 		    exception_postscript);
395: 	}
396: 	idx_t old_limit = maximum_memory;
397: 	// set the global maximum memory to the new limit if successful
398: 	maximum_memory = limit;
399: 	// evict again
400: 	if (!EvictBlocks(MemoryTag::EXTENSION, 0, limit).success) {
401: 		// failed: go back to old limit
402: 		maximum_memory = old_limit;
403: 		throw OutOfMemoryException(
404: 		    "Failed to change memory limit to %lld: could not free up enough memory for the new limit%s", limit,
405: 		    exception_postscript);
406: 	}
407: 	if (Allocator::SupportsFlush()) {
408: 		Allocator::FlushAll();
409: 	}
410: }
411: 
412: void BufferPool::SetAllocatorBulkDeallocationFlushThreshold(idx_t threshold) {
413: 	allocator_bulk_deallocation_flush_threshold = threshold;
414: }
415: 
416: BufferPool::MemoryUsage::MemoryUsage() {
417: 	for (auto &v : memory_usage) {
418: 		v = 0;
419: 	}
420: 	for (auto &cache : memory_usage_caches) {
421: 		for (auto &v : cache) {
422: 			v = 0;
423: 		}
424: 	}
425: }
426: 
427: void BufferPool::MemoryUsage::UpdateUsedMemory(MemoryTag tag, int64_t size) {
428: 	auto tag_idx = (idx_t)tag;
429: 	if ((idx_t)AbsValue(size) < MEMORY_USAGE_CACHE_THRESHOLD) {
430: 		// update cache and update global counter when cache exceeds threshold
431: 		// Get corresponding cache slot based on current CPU core index
432: 		// Two threads may access the same cache simultaneously,
433: 		// ensuring correctness through atomic operations
434: 		auto cache_idx = (idx_t)TaskScheduler::GetEstimatedCPUId() % MEMORY_USAGE_CACHE_COUNT;
435: 		auto &cache = memory_usage_caches[cache_idx];
436: 		auto new_tag_size = cache[tag_idx].fetch_add(size, std::memory_order_relaxed) + size;
437: 		if ((idx_t)AbsValue(new_tag_size) >= MEMORY_USAGE_CACHE_THRESHOLD) {
438: 			// cached tag memory usage exceeds threshold
439: 			auto tag_size = cache[tag_idx].exchange(0, std::memory_order_relaxed);
440: 			memory_usage[tag_idx].fetch_add(tag_size, std::memory_order_relaxed);
441: 		}
442: 		auto new_total_size = cache[TOTAL_MEMORY_USAGE_INDEX].fetch_add(size, std::memory_order_relaxed) + size;
443: 		if ((idx_t)AbsValue(new_total_size) >= MEMORY_USAGE_CACHE_THRESHOLD) {
444: 			// cached total memory usage exceeds threshold
445: 			auto total_size = cache[TOTAL_MEMORY_USAGE_INDEX].exchange(0, std::memory_order_relaxed);
446: 			memory_usage[TOTAL_MEMORY_USAGE_INDEX].fetch_add(total_size, std::memory_order_relaxed);
447: 		}
448: 	} else {
449: 		// update global counter
450: 		memory_usage[tag_idx].fetch_add(size, std::memory_order_relaxed);
451: 		memory_usage[TOTAL_MEMORY_USAGE_INDEX].fetch_add(size, std::memory_order_relaxed);
452: 	}
453: }
454: 
455: } // namespace duckdb
[end of src/storage/buffer/buffer_pool.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: