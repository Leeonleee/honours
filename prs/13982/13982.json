{
  "repo": "duckdb/duckdb",
  "pull_number": 13982,
  "instance_id": "duckdb__duckdb-13982",
  "issue_numbers": [
    "13938"
  ],
  "base_commit": "44bba02cea5d316e38f6edbad7fad3a1f913d63f",
  "patch": "diff --git a/extension/jemalloc/jemalloc/README.md b/extension/jemalloc/jemalloc/README.md\nindex 292d2e4bbae6..9bf08821e76a 100644\n--- a/extension/jemalloc/jemalloc/README.md\n+++ b/extension/jemalloc/jemalloc/README.md\n@@ -81,9 +81,9 @@ jemalloc_constructor(void) {\n \t// decay is in ms\n \tunsigned long long decay = DUCKDB_JEMALLOC_DECAY * 1000;\n #ifdef DEBUG\n-\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"junk:true,oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, cpu_count, bgt_count);\n+\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"junk:true,oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, cpu_count / 2, bgt_count);\n #else\n-\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, cpu_count, bgt_count);\n+\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, cpu_count / 2, bgt_count);\n #endif\n \tje_malloc_conf = JE_MALLOC_CONF_BUFFER;\n \tmalloc_init();\ndiff --git a/extension/jemalloc/jemalloc/src/jemalloc.c b/extension/jemalloc/jemalloc/src/jemalloc.c\nindex bac0880f9deb..9d57c2e42e73 100644\n--- a/extension/jemalloc/jemalloc/src/jemalloc.c\n+++ b/extension/jemalloc/jemalloc/src/jemalloc.c\n@@ -4277,9 +4277,9 @@ jemalloc_constructor(void) {\n \t// decay is in ms\n \tunsigned long long decay = DUCKDB_JEMALLOC_DECAY * 1000;\n #ifdef DEBUG\n-\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"junk:true,oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, cpu_count, bgt_count);\n+\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"junk:true,oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, cpu_count / 2, bgt_count);\n #else\n-\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, cpu_count, bgt_count);\n+\tsnprintf(JE_MALLOC_CONF_BUFFER, JE_MALLOC_CONF_BUFFER_SIZE, \"oversize_threshold:268435456,dirty_decay_ms:%llu,muzzy_decay_ms:%llu,narenas:%llu,max_background_threads:%llu\", decay, decay, cpu_count / 2, bgt_count);\n #endif\n \tje_malloc_conf = JE_MALLOC_CONF_BUFFER;\n \tmalloc_init();\ndiff --git a/extension/json/include/json_common.hpp b/extension/json/include/json_common.hpp\nindex bcc8d7c23db7..872058023c22 100644\n--- a/extension/json/include/json_common.hpp\n+++ b/extension/json/include/json_common.hpp\n@@ -241,12 +241,16 @@ struct JSONCommon {\n \t};\n \n \t//! Get JSON value using JSON path query (safe, checks the path query)\n-\tstatic inline yyjson_val *Get(yyjson_val *val, const string_t &path_str) {\n+\tstatic inline yyjson_val *Get(yyjson_val *val, const string_t &path_str, bool integral_argument) {\n \t\tauto ptr = path_str.GetData();\n \t\tauto len = path_str.GetSize();\n \t\tif (len == 0) {\n \t\t\treturn GetUnsafe(val, ptr, len);\n \t\t}\n+\t\tif (integral_argument) {\n+\t\t\tauto str = \"$[\" + path_str.GetString() + \"]\";\n+\t\t\treturn GetUnsafe(val, str.c_str(), str.length());\n+\t\t}\n \t\tswitch (*ptr) {\n \t\tcase '/': {\n \t\t\t// '/' notation must be '\\0'-terminated\n@@ -260,9 +264,15 @@ struct JSONCommon {\n \t\t\t}\n \t\t\treturn GetUnsafe(val, ptr, len);\n \t\t}\n-\t\tdefault:\n-\t\t\tauto str = \"/\" + string(ptr, len);\n-\t\t\treturn GetUnsafe(val, str.c_str(), len + 1);\n+\t\tdefault: {\n+\t\t\tstring path;\n+\t\t\tif (memchr(ptr, '\"', len)) {\n+\t\t\t\tpath = \"/\" + string(ptr, len);\n+\t\t\t} else {\n+\t\t\t\tpath = \"$.\\\"\" + path_str.GetString() + \"\\\"\";\n+\t\t\t}\n+\t\t\treturn GetUnsafe(val, path.c_str(), path.length());\n+\t\t}\n \t\t}\n \t}\n \ndiff --git a/extension/json/include/json_executors.hpp b/extension/json/include/json_executors.hpp\nindex 0eeff5e40bc6..3290a95ede6a 100644\n--- a/extension/json/include/json_executors.hpp\n+++ b/extension/json/include/json_executors.hpp\n@@ -8,6 +8,7 @@\n \n #pragma once\n \n+#include \"duckdb/common/vector_operations/vector_operations.hpp\"\n #include \"duckdb/execution/expression_executor.hpp\"\n #include \"json_functions.hpp\"\n \n@@ -88,11 +89,18 @@ struct JSONExecutors {\n \t\t\t}\n \t\t} else { // Columnref path\n \t\t\tD_ASSERT(info.path_type == JSONCommon::JSONPathType::REGULAR);\n-\t\t\tauto &paths = args.data[1];\n+\t\t\tunique_ptr<Vector> casted_paths;\n+\t\t\tif (args.data[1].GetType().id() == LogicalTypeId::VARCHAR) {\n+\t\t\t\tcasted_paths = make_uniq<Vector>(args.data[1]);\n+\t\t\t} else {\n+\t\t\t\tcasted_paths = make_uniq<Vector>(LogicalTypeId::VARCHAR);\n+\t\t\t\tVectorOperations::DefaultCast(args.data[1], *casted_paths, args.size(), true);\n+\t\t\t}\n \t\t\tBinaryExecutor::ExecuteWithNulls<string_t, string_t, T>(\n-\t\t\t    inputs, paths, result, args.size(), [&](string_t input, string_t path, ValidityMask &mask, idx_t idx) {\n+\t\t\t    inputs, *casted_paths, result, args.size(),\n+\t\t\t    [&](string_t input, string_t path, ValidityMask &mask, idx_t idx) {\n \t\t\t\t    auto doc = JSONCommon::ReadDocument(input, JSONCommon::READ_FLAG, lstate.json_allocator.GetYYAlc());\n-\t\t\t\t    auto val = JSONCommon::Get(doc->root, path);\n+\t\t\t\t    auto val = JSONCommon::Get(doc->root, path, args.data[1].GetType().IsIntegral());\n \t\t\t\t    if (SET_NULL_IF_NOT_FOUND && !val) {\n \t\t\t\t\t    mask.SetInvalid(idx);\n \t\t\t\t\t    return T {};\ndiff --git a/extension/json/json_extension.cpp b/extension/json/json_extension.cpp\nindex b594c26d9c15..e0665260a5bf 100644\n--- a/extension/json/json_extension.cpp\n+++ b/extension/json/json_extension.cpp\n@@ -27,7 +27,7 @@ static DefaultMacro json_macros[] = {\n      \"json_group_structure\",\n      {\"x\", nullptr},\n      {{nullptr, nullptr}},\n-     \"json_structure(json_group_array(x))->'0'\"},\n+     \"json_structure(json_group_array(x))->0\"},\n     {DEFAULT_SCHEMA, \"json\", {\"x\", nullptr}, {{nullptr, nullptr}}, \"json_extract(x, '$')\"},\n     {nullptr, nullptr, {nullptr}, {{nullptr, nullptr}}, nullptr}};\n \ndiff --git a/extension/json/json_functions.cpp b/extension/json/json_functions.cpp\nindex 0ad6837690b6..2b8b7828e17c 100644\n--- a/extension/json/json_functions.cpp\n+++ b/extension/json/json_functions.cpp\n@@ -21,21 +21,25 @@ static JSONPathType CheckPath(const Value &path_val, string &path, size_t &len)\n \tconst auto path_str_val = path_val.DefaultCastAs(LogicalType::VARCHAR);\n \tauto path_str = path_str_val.GetValueUnsafe<string_t>();\n \tlen = path_str.GetSize();\n-\tauto ptr = path_str.GetData();\n+\tconst auto ptr = path_str.GetData();\n \t// Empty strings and invalid $ paths yield an error\n \tif (len == 0) {\n \t\tthrow BinderException(\"Empty JSON path\");\n \t}\n \tJSONPathType path_type = JSONPathType::REGULAR;\n-\tif (*ptr == '$') {\n-\t\tpath_type = JSONCommon::ValidatePath(ptr, len, true);\n-\t}\n \t// Copy over string to the bind data\n \tif (*ptr == '/' || *ptr == '$') {\n \t\tpath = string(ptr, len);\n-\t} else {\n+\t} else if (path_val.type().IsIntegral()) {\n+\t\tpath = \"$[\" + string(ptr, len) + \"]\";\n+\t} else if (memchr(ptr, '\"', len)) {\n \t\tpath = \"/\" + string(ptr, len);\n-\t\tlen++;\n+\t} else {\n+\t\tpath = \"$.\\\"\" + string(ptr, len) + \"\\\"\";\n+\t}\n+\tlen = path.length();\n+\tif (*path.c_str() == '$') {\n+\t\tpath_type = JSONCommon::ValidatePath(path.c_str(), len, true);\n \t}\n \treturn path_type;\n }\n@@ -67,7 +71,11 @@ unique_ptr<FunctionData> JSONReadFunctionData::Bind(ClientContext &context, Scal\n \t\t\tpath_type = CheckPath(path_val, path, len);\n \t\t}\n \t}\n-\tbound_function.arguments[1] = LogicalType::VARCHAR;\n+\tif (arguments[1]->return_type.IsIntegral()) {\n+\t\tbound_function.arguments[1] = LogicalType::BIGINT;\n+\t} else {\n+\t\tbound_function.arguments[1] = LogicalType::VARCHAR;\n+\t}\n \tif (path_type == JSONCommon::JSONPathType::WILDCARD) {\n \t\tbound_function.return_type = LogicalType::LIST(bound_function.return_type);\n \t}\n@@ -117,6 +125,7 @@ unique_ptr<FunctionData> JSONReadManyFunctionData::Bind(ClientContext &context,\n \n JSONFunctionLocalState::JSONFunctionLocalState(Allocator &allocator) : json_allocator(allocator) {\n }\n+\n JSONFunctionLocalState::JSONFunctionLocalState(ClientContext &context)\n     : JSONFunctionLocalState(BufferAllocator::Get(context)) {\n }\ndiff --git a/src/common/types/column/column_data_allocator.cpp b/src/common/types/column/column_data_allocator.cpp\nindex 0f2c63849e6e..bec0751e0d6c 100644\n--- a/src/common/types/column/column_data_allocator.cpp\n+++ b/src/common/types/column/column_data_allocator.cpp\n@@ -2,6 +2,7 @@\n \n #include \"duckdb/common/types/column/column_data_collection_segment.hpp\"\n #include \"duckdb/storage/buffer/block_handle.hpp\"\n+#include \"duckdb/storage/buffer/buffer_pool.hpp\"\n #include \"duckdb/storage/buffer_manager.hpp\"\n \n namespace duckdb {\n@@ -45,6 +46,21 @@ ColumnDataAllocator::ColumnDataAllocator(ColumnDataAllocator &other) {\n \t}\n }\n \n+ColumnDataAllocator::~ColumnDataAllocator() {\n+\tif (type == ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR) {\n+\t\treturn;\n+\t}\n+\tfor (auto &block : blocks) {\n+\t\tblock.handle->SetDestroyBufferUpon(DestroyBufferUpon::UNPIN);\n+\t}\n+\tconst auto data_size = SizeInBytes();\n+\tblocks.clear();\n+\tif (Allocator::SupportsFlush() &&\n+\t    data_size > alloc.buffer_manager->GetBufferPool().GetAllocatorBulkDeallocationFlushThreshold()) {\n+\t\tAllocator::FlushAll();\n+\t}\n+}\n+\n BufferHandle ColumnDataAllocator::Pin(uint32_t block_id) {\n \tD_ASSERT(type == ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR || type == ColumnDataAllocatorType::HYBRID);\n \tshared_ptr<BlockHandle> handle;\ndiff --git a/src/common/types/row/tuple_data_segment.cpp b/src/common/types/row/tuple_data_segment.cpp\nindex 82e25001ee2b..ddec1323846e 100644\n--- a/src/common/types/row/tuple_data_segment.cpp\n+++ b/src/common/types/row/tuple_data_segment.cpp\n@@ -1,6 +1,7 @@\n #include \"duckdb/common/types/row/tuple_data_segment.hpp\"\n \n #include \"duckdb/common/types/row/tuple_data_allocator.hpp\"\n+#include \"duckdb/storage/buffer/buffer_pool.hpp\"\n \n namespace duckdb {\n \n@@ -118,6 +119,10 @@ TupleDataSegment::~TupleDataSegment() {\n \t}\n \tpinned_row_handles.clear();\n \tpinned_heap_handles.clear();\n+\tif (Allocator::SupportsFlush() && allocator &&\n+\t    data_size > allocator->GetBufferManager().GetBufferPool().GetAllocatorBulkDeallocationFlushThreshold()) {\n+\t\tAllocator::FlushAll();\n+\t}\n \tallocator.reset();\n }\n \ndiff --git a/src/execution/operator/join/physical_piecewise_merge_join.cpp b/src/execution/operator/join/physical_piecewise_merge_join.cpp\nindex d7b30423e3a0..8216d91ab18b 100644\n--- a/src/execution/operator/join/physical_piecewise_merge_join.cpp\n+++ b/src/execution/operator/join/physical_piecewise_merge_join.cpp\n@@ -618,7 +618,12 @@ OperatorResultType PhysicalPiecewiseMergeJoin::ResolveComplexJoin(ExecutionConte\n \n \t\t\t\tif (tail_count < result_count) {\n \t\t\t\t\tresult_count = tail_count;\n-\t\t\t\t\tchunk.Slice(*sel, result_count);\n+\t\t\t\t\tif (result_count == 0) {\n+\t\t\t\t\t\t// Need to reset here otherwise we may use the non-flat chunk when constructing LEFT/OUTER\n+\t\t\t\t\t\tchunk.Reset();\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tchunk.Slice(*sel, result_count);\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \ndiff --git a/src/include/duckdb/common/types/column/column_data_allocator.hpp b/src/include/duckdb/common/types/column/column_data_allocator.hpp\nindex dc49d2db0ad9..194b40ca3ea1 100644\n--- a/src/include/duckdb/common/types/column/column_data_allocator.hpp\n+++ b/src/include/duckdb/common/types/column/column_data_allocator.hpp\n@@ -32,6 +32,7 @@ class ColumnDataAllocator {\n \texplicit ColumnDataAllocator(BufferManager &buffer_manager);\n \tColumnDataAllocator(ClientContext &context, ColumnDataAllocatorType allocator_type);\n \tColumnDataAllocator(ColumnDataAllocator &allocator);\n+\t~ColumnDataAllocator();\n \n \t//! Returns an allocator object to allocate with. This returns the allocator in IN_MEMORY_ALLOCATOR, and a buffer\n \t//! allocator in case of BUFFER_MANAGER_ALLOCATOR.\ndiff --git a/src/include/duckdb/storage/buffer/buffer_pool.hpp b/src/include/duckdb/storage/buffer/buffer_pool.hpp\nindex 955f1aa4ecee..50166a51fa05 100644\n--- a/src/include/duckdb/storage/buffer/buffer_pool.hpp\n+++ b/src/include/duckdb/storage/buffer/buffer_pool.hpp\n@@ -50,6 +50,7 @@ class BufferPool {\n \n \t//! If bulk deallocation larger than this occurs, flush outstanding allocations\n \tvoid SetAllocatorBulkDeallocationFlushThreshold(idx_t threshold);\n+\tidx_t GetAllocatorBulkDeallocationFlushThreshold();\n \n \tvoid UpdateUsedMemory(MemoryTag tag, int64_t size);\n \ndiff --git a/src/optimizer/filter_pushdown.cpp b/src/optimizer/filter_pushdown.cpp\nindex 0744c67091f8..4af2a075ff53 100644\n--- a/src/optimizer/filter_pushdown.cpp\n+++ b/src/optimizer/filter_pushdown.cpp\n@@ -93,9 +93,14 @@ unique_ptr<LogicalOperator> FilterPushdown::Rewrite(unique_ptr<LogicalOperator>\n \t\t// we can just push directly through these operations without any rewriting\n \t\top->children[0] = Rewrite(std::move(op->children[0]));\n \t\treturn op;\n-\tcase LogicalOperatorType::LOGICAL_MATERIALIZED_CTE:\n+\tcase LogicalOperatorType::LOGICAL_MATERIALIZED_CTE: {\n+\t\t// we can't push filters into the materialized CTE (LHS), but we do want to recurse into it\n+\t\tFilterPushdown pushdown(optimizer, convert_mark_joins);\n+\t\top->children[0] = pushdown.Rewrite(std::move(op->children[0]));\n+\t\t// we can push filters into the rest of the query plan (RHS)\n \t\top->children[1] = Rewrite(std::move(op->children[1]));\n \t\treturn op;\n+\t}\n \tcase LogicalOperatorType::LOGICAL_GET:\n \t\treturn PushdownGet(std::move(op));\n \tcase LogicalOperatorType::LOGICAL_LIMIT:\ndiff --git a/src/optimizer/join_order/cardinality_estimator.cpp b/src/optimizer/join_order/cardinality_estimator.cpp\nindex 7b3151d4049d..fb65c0baf739 100644\n--- a/src/optimizer/join_order/cardinality_estimator.cpp\n+++ b/src/optimizer/join_order/cardinality_estimator.cpp\n@@ -2,10 +2,10 @@\n #include \"duckdb/common/enums/join_type.hpp\"\n #include \"duckdb/common/limits.hpp\"\n #include \"duckdb/common/printer.hpp\"\n-#include \"duckdb/planner/expression_iterator.hpp\"\n #include \"duckdb/function/table/table_scan.hpp\"\n #include \"duckdb/optimizer/join_order/join_node.hpp\"\n #include \"duckdb/optimizer/join_order/query_graph_manager.hpp\"\n+#include \"duckdb/planner/expression_iterator.hpp\"\n #include \"duckdb/planner/operator/logical_comparison_join.hpp\"\n #include \"duckdb/storage/data_table.hpp\"\n \n@@ -291,10 +291,18 @@ DenomInfo CardinalityEstimator::GetDenominator(JoinRelationSet &set) {\n \t// and we start to choose the filters that join relations in the set.\n \n \t// edges are guaranteed to be in order of largest tdom to smallest tdom.\n+\tunordered_set<idx_t> unused_edge_tdoms;\n \tauto edges = GetEdges(relations_to_tdoms, set);\n \tfor (auto &edge : edges) {\n-\t\tauto subgraph_connections = SubgraphsConnectedByEdge(edge, subgraphs);\n+\t\tif (subgraphs.size() == 1 && subgraphs.at(0).relations->ToString() == set.ToString()) {\n+\t\t\t// the first subgraph has connected all the desired relations, just skip the rest of the edges\n+\t\t\tif (edge.has_tdom_hll) {\n+\t\t\t\tunused_edge_tdoms.insert(edge.tdom_hll);\n+\t\t\t}\n+\t\t\tcontinue;\n+\t\t}\n \n+\t\tauto subgraph_connections = SubgraphsConnectedByEdge(edge, subgraphs);\n \t\tif (subgraph_connections.empty()) {\n \t\t\t// create a subgraph out of left and right, then merge right into left and add left to subgraphs.\n \t\t\t// this helps cover a case where there are no subgraphs yet, and the only join filter is a SEMI JOIN\n@@ -342,13 +350,11 @@ DenomInfo CardinalityEstimator::GetDenominator(JoinRelationSet &set) {\n \t\t\t                                   [](Subgraph2Denominator &s) { return !s.relations; });\n \t\t\tsubgraphs.erase(remove_start, subgraphs.end());\n \t\t}\n-\t\tif (subgraphs.size() == 1 && subgraphs.at(0).relations->ToString() == set.ToString()) {\n-\t\t\t// the first subgraph has connected all the desired relations, no need to iterate\n-\t\t\t// through the rest of the edges.\n-\t\t\tbreak;\n-\t\t}\n \t}\n \n+\t// Slight penalty to cardinality for unused edges\n+\tauto denom_multiplier = 1.0 + static_cast<double>(unused_edge_tdoms.size());\n+\n \t// It's possible cross-products were added and are not present in the filters in the relation_2_tdom\n \t// structures. When that's the case, merge all remaining subgraphs.\n \tif (subgraphs.size() > 1) {\n@@ -367,7 +373,7 @@ DenomInfo CardinalityEstimator::GetDenominator(JoinRelationSet &set) {\n \t\t// denominator is 1 and numerators are a cross product of cardinalities.\n \t\treturn DenomInfo(set, 1, 1);\n \t}\n-\treturn DenomInfo(*subgraphs.at(0).numerator_relations, 1, subgraphs.at(0).denom);\n+\treturn DenomInfo(*subgraphs.at(0).numerator_relations, 1, subgraphs.at(0).denom * denom_multiplier);\n }\n \n template <>\ndiff --git a/src/planner/binder.cpp b/src/planner/binder.cpp\nindex ea751bade659..9f0abf3986ea 100644\n--- a/src/planner/binder.cpp\n+++ b/src/planner/binder.cpp\n@@ -342,7 +342,8 @@ unique_ptr<BoundQueryNode> Binder::BindNode(QueryNode &node) {\n \n BoundStatement Binder::Bind(QueryNode &node) {\n \tBoundStatement result;\n-\tif (context.db->config.options.disabled_optimizers.find(OptimizerType::MATERIALIZED_CTE) ==\n+\tif (node.type != QueryNodeType::CTE_NODE && // Issue #13850 - Don't auto-materialize if users materialize (for now)\n+\t    context.db->config.options.disabled_optimizers.find(OptimizerType::MATERIALIZED_CTE) ==\n \t        context.db->config.options.disabled_optimizers.end() &&\n \t    context.config.enable_optimizer && OptimizeCTEs(node)) {\n \t\tswitch (node.type) {\ndiff --git a/src/storage/buffer/buffer_pool.cpp b/src/storage/buffer/buffer_pool.cpp\nindex 3e7e11c496dc..9ed31f6f3262 100644\n--- a/src/storage/buffer/buffer_pool.cpp\n+++ b/src/storage/buffer/buffer_pool.cpp\n@@ -413,6 +413,10 @@ void BufferPool::SetAllocatorBulkDeallocationFlushThreshold(idx_t threshold) {\n \tallocator_bulk_deallocation_flush_threshold = threshold;\n }\n \n+idx_t BufferPool::GetAllocatorBulkDeallocationFlushThreshold() {\n+\treturn allocator_bulk_deallocation_flush_threshold;\n+}\n+\n BufferPool::MemoryUsage::MemoryUsage() {\n \tfor (auto &v : memory_usage) {\n \t\tv = 0;\n",
  "test_patch": "diff --git a/test/issues/general/test_13938.test b/test/issues/general/test_13938.test\nnew file mode 100644\nindex 000000000000..0d91b3f296f4\n--- /dev/null\n+++ b/test/issues/general/test_13938.test\n@@ -0,0 +1,45 @@\n+# name: test/issues/general/test_13938.test\n+# description: Issue 13938 - INTERNAL Error: Attempted to dereference unique_ptr that is NULL\n+# group: [general]\n+\n+statement ok\n+CREATE TABLE t0(c0 TIMESTAMP, c1 VARCHAR[]);\n+\n+statement ok\n+CREATE TABLE t1(c0 FLOAT, c1 TIMESTAMP, c2 FLOAT);\n+\n+statement ok\n+INSERT INTO t0 VALUES('2023-10-10 00:00:00+00:00', NULL);\n+\n+statement ok\n+INSERT INTO t0 VALUES('2025-12-25 12:00:00+02:00', []), ('2004-07-27 10:00:00+02', []);\n+\n+statement ok\n+INSERT INTO t0(c1, c0) VALUES([], '2023-01-01 00:00:00+00:00'), ([], '2021-01-01 00:00:00+01');\n+\n+statement ok\n+INSERT INTO t0(c1, c0) VALUES([], '2021-01-01 00:00:00+00');\n+\n+statement ok\n+INSERT INTO t1 VALUES(2.71, '1999-12-31 23:59:59', 1.41421356237);\n+\n+statement ok\n+INSERT INTO t1 VALUES(1.61803, '1970-01-01 00:00:00', 1.61803);\n+\n+statement ok\n+INSERT INTO t1(c0) VALUES(1064961652.34), (123.45);\n+\n+statement ok\n+INSERT INTO t1(c0) VALUES('514332609.12');\n+\n+statement ok\n+INSERT INTO t1(c0, c2, c1) VALUES(2.71828, 2.345, '1995-05-23 08:45:00'), ('1308880808', 12.34, '2021-01-01 15:30:45');\n+\n+statement ok\n+INSERT INTO t1(c0) VALUES(92857950), (840458867);\n+\n+statement ok\n+INSERT INTO t1 VALUES('3.14', '1999-12-31 23:59:59', 3.1415);\n+\n+statement ok\n+SELECT * FROM t0 RIGHT JOIN t1 ON(CAST(t1.c1 AS TIMESTAMP) BETWEEN t0.c0 AND t0.c0);\ndiff --git a/test/sql/cte/materialized/annotated_and_auto_materialized.test b/test/sql/cte/materialized/annotated_and_auto_materialized.test\nnew file mode 100644\nindex 000000000000..6ba4008d512c\n--- /dev/null\n+++ b/test/sql/cte/materialized/annotated_and_auto_materialized.test\n@@ -0,0 +1,39 @@\n+# name: test/sql/cte/materialized/annotated_and_auto_materialized.test\n+# description: Issue 13850 - Binder error when manually materializing a CTE\n+# group: [materialized]\n+\n+statement ok\n+create table batch (\n+    entity text,\n+    start_ts timestamp,\n+    duration interval\n+);\n+\n+statement ok\n+create table active_events (\n+    entity text,\n+    start_ts timestamp,\n+    end_ts timestamp\n+);\n+\n+statement ok\n+explain create table new_active_events as\n+with\n+  new_events as materialized (  -- Does not make much sense in this example, but my original query was a union of a bunch of things\n+      select * from batch\n+  ), combined_deduplicated_events as (\n+      select\n+          entity,\n+          min(start_ts) as start_ts,\n+          max(end_ts) as end_ts\n+      from\n+          active_events\n+      group by\n+          entity\n+  ), all_events as (\n+      select  * from combined_deduplicated_events\n+  )\n+select\n+  *\n+from\n+  new_events;\n\\ No newline at end of file\ndiff --git a/test/sql/json/issues/issue13948.test b/test/sql/json/issues/issue13948.test\nnew file mode 100644\nindex 000000000000..827ead417d4f\n--- /dev/null\n+++ b/test/sql/json/issues/issue13948.test\n@@ -0,0 +1,79 @@\n+# name: test/sql/json/issues/issue13948.test\n+# description: Test issue 13948 - Json property name with special characters produce inconsistent results with json -> 'propertyname' and json_extract\n+# group: [issues]\n+\n+require json\n+\n+statement ok\n+pragma enable_verification\n+\n+query I\n+SELECT '{\"Status / SubStatus\": \"test\"}' -> 'Status / SubStatus';\n+----\n+\"test\"\n+\n+query I\n+WITH path AS (\n+    SELECT 'Status / SubStatus' p\n+)\n+SELECT '{\"Status / SubStatus\": \"test\"}' -> p\n+FROM path\n+----\n+\"test\"\n+\n+# TODO at some point we should escape supplied JSON paths automatically so that this works\n+query I\n+SELECT '{\"\\\"Status / SubStatus\\\"\": \"test\"}' -> '\"Status / SubStatus\"';\n+----\n+NULL\n+\n+query I\n+WITH path AS (\n+    SELECT NULL p\n+)\n+SELECT '{\"\\\"Status / SubStatus\\\"\": \"test\"}' -> p\n+FROM path\n+----\n+NULL\n+\n+query I\n+SELECT '{\"Status / SubStatus\": \"test\"}' -> '$.\"Status / SubStatus\"';\n+----\n+\"test\"\n+\n+query I\n+WITH path AS (\n+    SELECT '$.\"Status / SubStatus\"' p\n+)\n+SELECT '{\"Status / SubStatus\": \"test\"}' -> p\n+FROM path\n+----\n+\"test\"\n+\n+query I\n+SELECT '[1, 2, 3]'->0\n+----\n+1\n+\n+query I\n+WITH path AS (\n+    SELECT 0 p\n+)\n+SELECT '[1, 2, 3]' -> p\n+FROM path\n+----\n+1\n+\n+query I\n+SELECT '[1, 2, 3]'->'0'\n+----\n+NULL\n+\n+query I\n+WITH path AS (\n+    SELECT '0' p\n+)\n+SELECT '[1, 2, 3]' -> p\n+FROM path\n+----\n+NULL\ndiff --git a/test/sql/json/scalar/test_json_extract.test b/test/sql/json/scalar/test_json_extract.test\nindex d41b6a4e909e..276cfc229e31 100644\n--- a/test/sql/json/scalar/test_json_extract.test\n+++ b/test/sql/json/scalar/test_json_extract.test\n@@ -85,7 +85,7 @@ select '{\"my_field\": {\"my_nested_field\": [\"goose\", \"duckduckduckduck\"]}}'::JSON-\n duckduckduckduck\n \n query T\n-select json_extract('[1, 2, 42]', '2')\n+select json_extract('[1, 2, 42]', 2)\n ----\n 42\n \ndiff --git a/test/sql/json/scalar/test_json_value.test b/test/sql/json/scalar/test_json_value.test\nindex 30e0c1acd873..d0ae5622b542 100644\n--- a/test/sql/json/scalar/test_json_value.test\n+++ b/test/sql/json/scalar/test_json_value.test\n@@ -39,7 +39,7 @@ select json_value('{\"my_field\": {\"my_nested_field\": [\"goose\", \"duckduckduckduck\"\n \"duckduckduckduck\"\n \n query T\n-select json_value('[1, 2, 42]', '2')\n+select json_value('[1, 2, 42]', 2)\n ----\n 42\n \ndiff --git a/test/sql/optimizer/plan/test_filter_pushdown_materialized_cte.test b/test/sql/optimizer/plan/test_filter_pushdown_materialized_cte.test\nnew file mode 100644\nindex 000000000000..d71d18a78a75\n--- /dev/null\n+++ b/test/sql/optimizer/plan/test_filter_pushdown_materialized_cte.test\n@@ -0,0 +1,34 @@\n+# name: test/sql/optimizer/plan/test_filter_pushdown_materialized_cte.test\n+# description: Test filter pushdown in materialized CTEs (internal issue #3041)\n+# group: [plan]\n+\n+require tpcds\n+\n+statement ok\n+call dsdgen(sf=0.01)\n+\n+statement ok\n+pragma explain_output='OPTIMIZED_ONLY'\n+\n+query II\n+EXPLAIN WITH ss AS MATERIALIZED\n+  ( SELECT i_manufact_id,\n+           sum(ss_ext_sales_price) total_sales\n+   FROM store_sales,\n+        date_dim,\n+        customer_address,\n+        item\n+   WHERE i_manufact_id IN\n+       (SELECT i_manufact_id\n+        FROM item\n+        WHERE i_category IN ('Electronics'))\n+     AND ss_item_sk = i_item_sk\n+     AND ss_sold_date_sk = d_date_sk\n+     AND d_year = 1998\n+     AND d_moy = 5\n+     AND ss_addr_sk = ca_address_sk\n+     AND ca_gmt_offset = -5\n+   GROUP BY i_manufact_id)\n+FROM ss\n+----\n+logical_opt\t<!REGEX>:.*CROSS_PRODUCT.*\n",
  "problem_statement": "INTERNAL Error: Attempted to dereference unique_ptr that is NULL\n### What happens?\n\nThe below test case (after some seconds creduce) causes an INTERNAL error in DuckDB. This [test case](https://github.com/user-attachments/files/17001454/backup.txt) before further reduced could sometimes cause segmentation fault.\r\n\n\n### To Reproduce\n\n```sql\r\nCREATE TABLE t0(c0 TIMESTAMP, c1 VARCHAR[]);\r\nCREATE TABLE t1(c0 FLOAT, c1 TIMESTAMP, c2 FLOAT);\r\nINSERT INTO t0 VALUES('2023-10-10 00:00:00+00:00', NULL);\r\nINSERT INTO t0 VALUES('2025-12-25 12:00:00+02:00', []), ('2004-07-27 10:00:00+02', []);\r\nINSERT INTO t0(c1, c0) VALUES([], '2023-01-01 00:00:00+00:00'), ([], '2021-01-01 00:00:00+01');\r\nINSERT INTO t0(c1, c0) VALUES([], '2021-01-01 00:00:00+00');\r\nINSERT INTO t1 VALUES(2.71, '1999-12-31 23:59:59', 1.41421356237);\r\nINSERT INTO t1 VALUES(1.61803, '1970-01-01 00:00:00', 1.61803);\r\nINSERT INTO t1(c0) VALUES(1064961652.34), (123.45);\r\nINSERT INTO t1(c0) VALUES('514332609.12');\r\nINSERT INTO t1(c0, c2, c1) VALUES(2.71828, 2.345, '1995-05-23 08:45:00'), ('1308880808', 12.34, '2021-01-01 15:30:45');\r\nINSERT INTO t1(c0) VALUES(92857950), (840458867);\r\nINSERT INTO t1 VALUES('3.14', '1999-12-31 23:59:59', 3.1415);\r\n\r\nSELECT * FROM t0 RIGHT JOIN t1 ON(CAST(t1.c1 AS TIMESTAMP) BETWEEN t0.c0 AND t0.c0);\r\n-- INTERNAL Error: Attempted to dereference unique_ptr that is NULL! This error signals an assertion failure within DuckDB. This usually occurs due to unexpected conditions or errors in the program's logic. For more information, see https://duckdb.org/docs/dev/internal_errors\r\n```\n\n### OS:\n\nUbuntu 22.04\n\n### DuckDB Version:\n\nv1.1.1-dev122 b369bcb4e0\n\n### DuckDB Client:\n\nCLI\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nSuyang Zhong\n\n### Affiliation:\n\nNUS\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a nightly build\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\n",
  "hints_text": "@suyZhong Reproduced it, thanks!",
  "created_at": "2024-09-17T09:41:17Z"
}