You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Specific query not finishing since v1.1.0 and filling up all temp disk space
### What happens?

We have a specific query (involving a join and subqueries for counting), of which a minimal reproducible example has been derived below. It operates on two tables, one with over a million rows, the other 4-5% of that. The query works fine in DuckDB 1.0.0 and finishes very fast (at most a few seconds). However, in later releases it never seems to finish -- instead, a temporary directory gets created that keeps growing (and the progress bar remains stuck at 50%).

A `git bisect` helped point out that the commit introducing the problem seems to be https://github.com/duckdb/duckdb/commit/91b0fb71d17090e9f68332e65673ab899d691bca .

### To Reproduce

Generate dummy data (two Parquet files) using the following Python script (with numpy, pandas, and duckdb):

```python
import numpy as np
import pandas as pd
import duckdb


np.random.seed(12345)

dates = np.random.randint(low=1514764800.0, high=1704067200.0, size=(1000000,))
categories = np.random.randint(low=0, high=200, size=(1000000,))
dates.sort()

records = pd.DataFrame({
    "creation_dt": dates,
    "category": categories,
}).reset_index().rename(columns={"index": "id"})
records["creation_dt"] = records["creation_dt"].map(lambda x: pd.Timestamp(x * 1e9))
records["category"] = records["category"].map('{:02X}'.format)

labels = records.sample(frac=0.05, random_state=23456)
label_delays = np.random.randint(low=1 * 60 * 60, high=125 * 24 * 60 * 60, size=(len(labels.index),))
labels["label_dt"] = labels["creation_dt"] + pd.to_timedelta(label_delays, "s")
labels["label"] = 1

db = duckdb.connect()
db.sql("""
    copy (
       select id, creation_dt::timestamp as creation_dt, creation_dt::date as creation_day, category from records
    ) to 'records.parquet'
""")
db.sql("""
    copy (
       select id, label_dt::timestamp as label_dt, label from labels
    ) to 'labels.parquet'
""")
```

`records.parquet` has a million rows looking like this:

```
┌───────┬─────────────────────┬──────────────┬──────────┐
│  id   │     creation_dt     │ creation_day │ category │
│ int64 │      timestamp      │     date     │ varchar  │
├───────┼─────────────────────┼──────────────┼──────────┤
│     0 │ 2018-01-01 00:02:15 │ 2018-01-01   │ A5       │
│     1 │ 2018-01-01 00:17:21 │ 2018-01-01   │ 62       │
│     2 │ 2018-01-01 00:22:10 │ 2018-01-01   │ 07       │
└───────┴─────────────────────┴──────────────┴──────────┘
```

5% of which has a corresponding row in `labels.parquet` looking like this:

```
┌────────┬─────────────────────┬───────┐
│   id   │      label_dt       │ label │
│ int64  │      timestamp      │ int64 │
├────────┼─────────────────────┼───────┤
│ 970100 │ 2024-02-13 10:14:11 │     1 │
│ 524709 │ 2021-05-15 17:11:48 │     1 │
│ 800619 │ 2022-11-28 23:57:17 │     1 │
└────────┴─────────────────────┴───────┘
```



Then run the following SQL (on an in-memory database or on a file, same result). The query should result into a table that has a row for each combination of `creation_day` and `category` appearing in `records` with a third column `num_labeled_30d` that contains a count of how many records with that `category` have been labeled in the 30 days prior to `creation_day`.

```sql
create or replace table records as
from 'records.parquet';


create or replace table labels as
from 'labels.parquet';

with
day_cat_rows as
  (select category,
          creation_day
   from records
   group by category,
            creation_day),
recs as
  (select category,
          records.creation_dt,
          labels.label_dt,
          labels.label
   from records
   left join labels on labels.id = records.id),
counts as
  (select day_cat_rows.creation_day,
          category,

     (select count(1)
      from recs
      where recs.creation_dt > day_cat_rows.creation_day - '30 days'::interval
        and recs.creation_dt <= day_cat_rows.creation_day
        and recs.category = day_cat_rows.category
        and recs.label_dt <= day_cat_rows.creation_day
        and recs.label = 1) as num_labeled_30d,
   from day_cat_rows)
select *
from counts;
```

DuckDB 1.0.0 will give an output that looks like this:

```
┌──────────────┬──────────┬─────────────────┐
│ creation_day │ category │ num_labeled_30d │
│     date     │ varchar  │      int64      │
├──────────────┼──────────┼─────────────────┤
│ 2023-11-25   │ 05       │               2 │
│ 2023-11-25   │ 3A       │               0 │
│ 2023-11-25   │ 09       │               0 │
│ 2023-11-25   │ 62       │               0 │
│ 2023-11-25   │ 32       │               1 │
│ 2023-11-25   │ 83       │               1 │
│ 2023-11-25   │ 92       │               0 │
│ 2023-11-25   │ 21       │               0 │
│ 2023-11-25   │ BA       │               2 │
│ 2023-11-25   │ 96       │               0 │
│ 2023-11-25   │ 6F       │               1 │
│ 2023-11-25   │ 64       │               0 │
│ 2023-11-25   │ 24       │               1 │
│ 2023-11-25   │ 14       │               0 │
│ 2023-11-25   │ 5D       │               0 │
│ 2023-11-25   │ 6A       │               0 │
│ 2023-11-25   │ 4D       │               0 │
│ 2023-11-25   │ B7       │               0 │
│ 2023-11-25   │ 9E       │               1 │
│ 2023-11-25   │ 1F       │               0 │
│     ·        │ ·        │               · │
│     ·        │ ·        │               · │
│     ·        │ ·        │               · │
│ 2021-09-07   │ 3F       │               1 │
│ 2021-09-07   │ 74       │               0 │
│ 2021-09-07   │ 06       │               0 │
│ 2021-09-07   │ 52       │               0 │
│ 2021-09-07   │ 2F       │               0 │
│ 2021-09-07   │ 8B       │               0 │
│ 2021-09-07   │ 17       │               0 │
│ 2021-09-07   │ 89       │               1 │
│ 2021-09-07   │ 39       │               1 │
│ 2021-09-07   │ 5A       │               1 │
│ 2021-09-07   │ 7E       │               0 │
│ 2021-09-07   │ 82       │               0 │
│ 2021-09-07   │ 7B       │               0 │
│ 2021-09-07   │ 90       │               0 │
│ 2021-09-08   │ 6D       │               1 │
│ 2021-09-08   │ A0       │               1 │
│ 2021-09-08   │ 67       │               2 │
│ 2021-09-08   │ 94       │               0 │
│ 2021-09-08   │ 61       │               0 │
│ 2021-09-08   │ 60       │               1 │
├──────────────┴──────────┴─────────────────┤
│ 393154 rows (40 shown)          3 columns │
└───────────────────────────────────────────┘
```
But in later releases, it does not finish.

### OS:

macOS 15.3, arm64

### DuckDB Version:

1.1.0 and later

### DuckDB Client:

CLI

### Hardware:

M1 Pro chip, 32 GB RAM

### Full Name:

Thomas Daniels

### Affiliation:

DNS Belgium

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a source build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [x] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [x] Yes, I have
Specific query not finishing since v1.1.0 and filling up all temp disk space
### What happens?

We have a specific query (involving a join and subqueries for counting), of which a minimal reproducible example has been derived below. It operates on two tables, one with over a million rows, the other 4-5% of that. The query works fine in DuckDB 1.0.0 and finishes very fast (at most a few seconds). However, in later releases it never seems to finish -- instead, a temporary directory gets created that keeps growing (and the progress bar remains stuck at 50%).

A `git bisect` helped point out that the commit introducing the problem seems to be https://github.com/duckdb/duckdb/commit/91b0fb71d17090e9f68332e65673ab899d691bca .

### To Reproduce

Generate dummy data (two Parquet files) using the following Python script (with numpy, pandas, and duckdb):

```python
import numpy as np
import pandas as pd
import duckdb


np.random.seed(12345)

dates = np.random.randint(low=1514764800.0, high=1704067200.0, size=(1000000,))
categories = np.random.randint(low=0, high=200, size=(1000000,))
dates.sort()

records = pd.DataFrame({
    "creation_dt": dates,
    "category": categories,
}).reset_index().rename(columns={"index": "id"})
records["creation_dt"] = records["creation_dt"].map(lambda x: pd.Timestamp(x * 1e9))
records["category"] = records["category"].map('{:02X}'.format)

labels = records.sample(frac=0.05, random_state=23456)
label_delays = np.random.randint(low=1 * 60 * 60, high=125 * 24 * 60 * 60, size=(len(labels.index),))
labels["label_dt"] = labels["creation_dt"] + pd.to_timedelta(label_delays, "s")
labels["label"] = 1

db = duckdb.connect()
db.sql("""
    copy (
       select id, creation_dt::timestamp as creation_dt, creation_dt::date as creation_day, category from records
    ) to 'records.parquet'
""")
db.sql("""
    copy (
       select id, label_dt::timestamp as label_dt, label from labels
    ) to 'labels.parquet'
""")
```

`records.parquet` has a million rows looking like this:

```
┌───────┬─────────────────────┬──────────────┬──────────┐
│  id   │     creation_dt     │ creation_day │ category │
│ int64 │      timestamp      │     date     │ varchar  │
├───────┼─────────────────────┼──────────────┼──────────┤
│     0 │ 2018-01-01 00:02:15 │ 2018-01-01   │ A5       │
│     1 │ 2018-01-01 00:17:21 │ 2018-01-01   │ 62       │
│     2 │ 2018-01-01 00:22:10 │ 2018-01-01   │ 07       │
└───────┴─────────────────────┴──────────────┴──────────┘
```

5% of which has a corresponding row in `labels.parquet` looking like this:

```
┌────────┬─────────────────────┬───────┐
│   id   │      label_dt       │ label │
│ int64  │      timestamp      │ int64 │
├────────┼─────────────────────┼───────┤
│ 970100 │ 2024-02-13 10:14:11 │     1 │
│ 524709 │ 2021-05-15 17:11:48 │     1 │
│ 800619 │ 2022-11-28 23:57:17 │     1 │
└────────┴─────────────────────┴───────┘
```



Then run the following SQL (on an in-memory database or on a file, same result). The query should result into a table that has a row for each combination of `creation_day` and `category` appearing in `records` with a third column `num_labeled_30d` that contains a count of how many records with that `category` have been labeled in the 30 days prior to `creation_day`.

```sql
create or replace table records as
from 'records.parquet';


create or replace table labels as
from 'labels.parquet';

with
day_cat_rows as
  (select category,
          creation_day
   from records
   group by category,
            creation_day),
recs as
  (select category,
          records.creation_dt,
          labels.label_dt,
          labels.label
   from records
   left join labels on labels.id = records.id),
counts as
  (select day_cat_rows.creation_day,
          category,

     (select count(1)
      from recs
      where recs.creation_dt > day_cat_rows.creation_day - '30 days'::interval
        and recs.creation_dt <= day_cat_rows.creation_day
        and recs.category = day_cat_rows.category
        and recs.label_dt <= day_cat_rows.creation_day
        and recs.label = 1) as num_labeled_30d,
   from day_cat_rows)
select *
from counts;
```

DuckDB 1.0.0 will give an output that looks like this:

```
┌──────────────┬──────────┬─────────────────┐
│ creation_day │ category │ num_labeled_30d │
│     date     │ varchar  │      int64      │
├──────────────┼──────────┼─────────────────┤
│ 2023-11-25   │ 05       │               2 │
│ 2023-11-25   │ 3A       │               0 │
│ 2023-11-25   │ 09       │               0 │
│ 2023-11-25   │ 62       │               0 │
│ 2023-11-25   │ 32       │               1 │
│ 2023-11-25   │ 83       │               1 │
│ 2023-11-25   │ 92       │               0 │
│ 2023-11-25   │ 21       │               0 │
│ 2023-11-25   │ BA       │               2 │
│ 2023-11-25   │ 96       │               0 │
│ 2023-11-25   │ 6F       │               1 │
│ 2023-11-25   │ 64       │               0 │
│ 2023-11-25   │ 24       │               1 │
│ 2023-11-25   │ 14       │               0 │
│ 2023-11-25   │ 5D       │               0 │
│ 2023-11-25   │ 6A       │               0 │
│ 2023-11-25   │ 4D       │               0 │
│ 2023-11-25   │ B7       │               0 │
│ 2023-11-25   │ 9E       │               1 │
│ 2023-11-25   │ 1F       │               0 │
│     ·        │ ·        │               · │
│     ·        │ ·        │               · │
│     ·        │ ·        │               · │
│ 2021-09-07   │ 3F       │               1 │
│ 2021-09-07   │ 74       │               0 │
│ 2021-09-07   │ 06       │               0 │
│ 2021-09-07   │ 52       │               0 │
│ 2021-09-07   │ 2F       │               0 │
│ 2021-09-07   │ 8B       │               0 │
│ 2021-09-07   │ 17       │               0 │
│ 2021-09-07   │ 89       │               1 │
│ 2021-09-07   │ 39       │               1 │
│ 2021-09-07   │ 5A       │               1 │
│ 2021-09-07   │ 7E       │               0 │
│ 2021-09-07   │ 82       │               0 │
│ 2021-09-07   │ 7B       │               0 │
│ 2021-09-07   │ 90       │               0 │
│ 2021-09-08   │ 6D       │               1 │
│ 2021-09-08   │ A0       │               1 │
│ 2021-09-08   │ 67       │               2 │
│ 2021-09-08   │ 94       │               0 │
│ 2021-09-08   │ 61       │               0 │
│ 2021-09-08   │ 60       │               1 │
├──────────────┴──────────┴─────────────────┤
│ 393154 rows (40 shown)          3 columns │
└───────────────────────────────────────────┘
```
But in later releases, it does not finish.

### OS:

macOS 15.3, arm64

### DuckDB Version:

1.1.0 and later

### DuckDB Client:

CLI

### Hardware:

M1 Pro chip, 32 GB RAM

### Full Name:

Thomas Daniels

### Affiliation:

DNS Belgium

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a source build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [x] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [x] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/stable/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/stable/clients/cli/overview) and has clients for [Python](https://duckdb.org/docs/stable/clients/python/overview), [R](https://duckdb.org/docs/stable/clients/r), [Java](https://duckdb.org/docs/stable/clients/java), [Wasm](https://duckdb.org/docs/stable/clients/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdb.org/docs/stable/clients/r#duckplyr-dplyr-api).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/stable/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://duckdb.org/docs/installation/) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/stable/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/stable/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/docs/stable/dev/building/overview) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of src/execution/physical_plan/plan_delim_join.cpp]
1: #include "duckdb/common/enum_util.hpp"
2: #include "duckdb/execution/operator/aggregate/physical_hash_aggregate.hpp"
3: #include "duckdb/execution/operator/join/physical_hash_join.hpp"
4: #include "duckdb/execution/operator/join/physical_left_delim_join.hpp"
5: #include "duckdb/execution/operator/join/physical_right_delim_join.hpp"
6: #include "duckdb/execution/operator/projection/physical_projection.hpp"
7: #include "duckdb/execution/physical_plan_generator.hpp"
8: #include "duckdb/planner/expression/bound_reference_expression.hpp"
9: 
10: #include "duckdb/execution/operator/scan/physical_column_data_scan.hpp"
11: 
12: namespace duckdb {
13: 
14: static void GatherDelimScans(PhysicalOperator &op, vector<const_reference<PhysicalOperator>> &delim_scans,
15:                              idx_t delim_index) {
16: 	if (op.type == PhysicalOperatorType::DELIM_SCAN) {
17: 		auto &scan = op.Cast<PhysicalColumnDataScan>();
18: 		scan.delim_index = optional_idx(delim_index);
19: 		delim_scans.push_back(op);
20: 	}
21: 	for (auto &child : op.children) {
22: 		GatherDelimScans(child, delim_scans, delim_index);
23: 	}
24: }
25: 
26: PhysicalOperator &PhysicalPlanGenerator::PlanDelimJoin(LogicalComparisonJoin &op) {
27: 	// first create the underlying join
28: 	auto &plan = PlanComparisonJoin(op);
29: 	// this should create a join, not a cross product
30: 	D_ASSERT(plan.type != PhysicalOperatorType::CROSS_PRODUCT);
31: 	// duplicate eliminated join
32: 	// first gather the scans on the duplicate eliminated data set from the delim side
33: 	const idx_t delim_idx = op.delim_flipped ? 0 : 1;
34: 	vector<const_reference<PhysicalOperator>> delim_scans;
35: 	GatherDelimScans(plan.children[delim_idx], delim_scans, ++this->delim_index);
36: 	if (delim_scans.empty()) {
37: 		// no duplicate eliminated scans in the delim side!
38: 		// in this case we don't need to create a delim join
39: 		// just push the normal join
40: 		return plan;
41: 	}
42: 	vector<LogicalType> delim_types;
43: 	vector<unique_ptr<Expression>> distinct_groups, distinct_expressions;
44: 	for (auto &delim_expr : op.duplicate_eliminated_columns) {
45: 		D_ASSERT(delim_expr->GetExpressionType() == ExpressionType::BOUND_REF);
46: 		auto &bound_ref = delim_expr->Cast<BoundReferenceExpression>();
47: 		delim_types.push_back(bound_ref.return_type);
48: 		distinct_groups.push_back(make_uniq<BoundReferenceExpression>(bound_ref.return_type, bound_ref.index));
49: 	}
50: 
51: 	// we still have to create the DISTINCT clause that is used to generate the duplicate eliminated chunk
52: 	auto &distinct = Make<PhysicalHashAggregate>(context, delim_types, std::move(distinct_expressions),
53: 	                                             std::move(distinct_groups), op.estimated_cardinality);
54: 
55: 	// Create the duplicate eliminated join.
56: 	if (op.delim_flipped) {
57: 		return Make<PhysicalRightDelimJoin>(*this, op.types, plan, distinct, delim_scans, op.estimated_cardinality,
58: 		                                    optional_idx(this->delim_index));
59: 	}
60: 	return Make<PhysicalLeftDelimJoin>(*this, op.types, plan, distinct, delim_scans, op.estimated_cardinality,
61: 	                                   optional_idx(this->delim_index));
62: }
63: 
64: } // namespace duckdb
[end of src/execution/physical_plan/plan_delim_join.cpp]
[start of src/include/duckdb/optimizer/join_order/cardinality_estimator.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/optimizer/join_order/cardinality_estimator.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: #pragma once
9: 
10: #include "duckdb/planner/column_binding_map.hpp"
11: #include "duckdb/optimizer/join_order/query_graph.hpp"
12: 
13: #include "duckdb/optimizer/join_order/relation_statistics_helper.hpp"
14: 
15: namespace duckdb {
16: 
17: class FilterInfo;
18: 
19: struct DenomInfo {
20: 	DenomInfo(JoinRelationSet &numerator_relations, double filter_strength, double denominator)
21: 	    : numerator_relations(numerator_relations), filter_strength(filter_strength), denominator(denominator) {
22: 	}
23: 
24: 	JoinRelationSet &numerator_relations;
25: 	double filter_strength;
26: 	double denominator;
27: };
28: 
29: struct RelationsToTDom {
30: 	//! column binding sets that are equivalent in a join plan.
31: 	//! if you have A.x = B.y and B.y = C.z, then one set is {A.x, B.y, C.z}.
32: 	column_binding_set_t equivalent_relations;
33: 	//!	the estimated total domains of the equivalent relations determined using HLL
34: 	idx_t tdom_hll;
35: 	//! the estimated total domains of each relation without using HLL
36: 	idx_t tdom_no_hll;
37: 	bool has_tdom_hll;
38: 	vector<optional_ptr<FilterInfo>> filters;
39: 	vector<string> column_names;
40: 
41: 	explicit RelationsToTDom(const column_binding_set_t &column_binding_set)
42: 	    : equivalent_relations(column_binding_set), tdom_hll(0), tdom_no_hll(NumericLimits<idx_t>::Maximum()),
43: 	      has_tdom_hll(false) {};
44: };
45: 
46: class FilterInfoWithTotalDomains {
47: public:
48: 	FilterInfoWithTotalDomains(optional_ptr<FilterInfo> filter_info, RelationsToTDom &relation2tdom)
49: 	    : filter_info(filter_info), tdom_hll(relation2tdom.tdom_hll), tdom_no_hll(relation2tdom.tdom_no_hll),
50: 	      has_tdom_hll(relation2tdom.has_tdom_hll) {
51: 	}
52: 
53: 	optional_ptr<FilterInfo> filter_info;
54: 	//!	the estimated total domains of the equivalent relations determined using HLL
55: 	idx_t tdom_hll;
56: 	//! the estimated total domains of each relation without using HLL
57: 	idx_t tdom_no_hll;
58: 	bool has_tdom_hll;
59: };
60: 
61: struct Subgraph2Denominator {
62: 	optional_ptr<JoinRelationSet> relations;
63: 	optional_ptr<JoinRelationSet> numerator_relations;
64: 	double denom;
65: 
66: 	Subgraph2Denominator() : relations(nullptr), numerator_relations(nullptr), denom(1) {};
67: };
68: 
69: class CardinalityHelper {
70: public:
71: 	CardinalityHelper() {
72: 	}
73: 	explicit CardinalityHelper(double cardinality_before_filters)
74: 	    : cardinality_before_filters(cardinality_before_filters) {};
75: 
76: public:
77: 	// must be a double. Otherwise we can lose significance between different join orders.
78: 	// our cardinality estimator severely underestimates cardinalities for 3+ joins. However,
79: 	// if one join order has an estimate of 0.8, and another has an estimate of 0.6, rounding
80: 	// them means there is no estimated difference, when in reality there could be a very large
81: 	// difference.
82: 	double cardinality_before_filters;
83: 
84: 	vector<string> table_names_joined;
85: 	vector<string> column_names;
86: };
87: 
88: class CardinalityEstimator {
89: public:
90: 	static constexpr double DEFAULT_SEMI_ANTI_SELECTIVITY = 5;
91: 	static constexpr double DEFAULT_LT_GT_MULTIPLIER = 2.5;
92: 	explicit CardinalityEstimator() {};
93: 
94: private:
95: 	vector<RelationsToTDom> relations_to_tdoms;
96: 	unordered_map<string, CardinalityHelper> relation_set_2_cardinality;
97: 	JoinRelationSetManager set_manager;
98: 	vector<RelationStats> relation_stats;
99: 
100: public:
101: 	void RemoveEmptyTotalDomains();
102: 	void UpdateTotalDomains(optional_ptr<JoinRelationSet> set, RelationStats &stats);
103: 	void InitEquivalentRelations(const vector<unique_ptr<FilterInfo>> &filter_infos);
104: 
105: 	void InitCardinalityEstimatorProps(optional_ptr<JoinRelationSet> set, RelationStats &stats);
106: 
107: 	//! cost model needs estimated cardinalities to the fraction since the formula captures
108: 	//! distinct count selectivities and multiplicities. Hence the template
109: 	template <class T>
110: 	T EstimateCardinalityWithSet(JoinRelationSet &new_set);
111: 
112: 	//! used for debugging.
113: 	void AddRelationNamesToTdoms(vector<RelationStats> &stats);
114: 	void PrintRelationToTdomInfo();
115: 
116: private:
117: 	double GetNumerator(JoinRelationSet &set);
118: 	DenomInfo GetDenominator(JoinRelationSet &set);
119: 
120: 	bool SingleColumnFilter(FilterInfo &filter_info);
121: 	vector<idx_t> DetermineMatchingEquivalentSets(optional_ptr<FilterInfo> filter_info);
122: 	//! Given a filter, add the column bindings to the matching equivalent set at the index
123: 	//! given in matching equivalent sets.
124: 	//! If there are multiple equivalence sets, they are merged.
125: 	void AddToEquivalenceSets(optional_ptr<FilterInfo> filter_info, vector<idx_t> matching_equivalent_sets);
126: 
127: 	double CalculateUpdatedDenom(Subgraph2Denominator left, Subgraph2Denominator right,
128: 	                             FilterInfoWithTotalDomains &filter);
129: 	JoinRelationSet &UpdateNumeratorRelations(Subgraph2Denominator left, Subgraph2Denominator right,
130: 	                                          FilterInfoWithTotalDomains &filter);
131: 
132: 	void AddRelationTdom(FilterInfo &filter_info);
133: 	bool EmptyFilter(FilterInfo &filter_info);
134: };
135: 
136: } // namespace duckdb
[end of src/include/duckdb/optimizer/join_order/cardinality_estimator.hpp]
[start of src/optimizer/join_order/cardinality_estimator.cpp]
1: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
2: #include "duckdb/common/enums/join_type.hpp"
3: #include "duckdb/common/limits.hpp"
4: #include "duckdb/common/printer.hpp"
5: #include "duckdb/function/table/table_scan.hpp"
6: #include "duckdb/optimizer/join_order/join_node.hpp"
7: #include "duckdb/optimizer/join_order/query_graph_manager.hpp"
8: #include "duckdb/planner/expression_iterator.hpp"
9: #include "duckdb/planner/operator/logical_comparison_join.hpp"
10: #include "duckdb/storage/data_table.hpp"
11: 
12: namespace duckdb {
13: 
14: // The filter was made on top of a logical sample or other projection,
15: // but no specific columns are referenced. See issue 4978 number 4.
16: bool CardinalityEstimator::EmptyFilter(FilterInfo &filter_info) {
17: 	if (!filter_info.left_set && !filter_info.right_set) {
18: 		return true;
19: 	}
20: 	return false;
21: }
22: 
23: void CardinalityEstimator::AddRelationTdom(FilterInfo &filter_info) {
24: 	D_ASSERT(filter_info.set.get().count >= 1);
25: 	for (const RelationsToTDom &r2tdom : relations_to_tdoms) {
26: 		auto &i_set = r2tdom.equivalent_relations;
27: 		if (i_set.find(filter_info.left_binding) != i_set.end()) {
28: 			// found an equivalent filter
29: 			return;
30: 		}
31: 	}
32: 
33: 	auto key = ColumnBinding(filter_info.left_binding.table_index, filter_info.left_binding.column_index);
34: 	RelationsToTDom new_r2tdom(column_binding_set_t({key}));
35: 
36: 	relations_to_tdoms.emplace_back(new_r2tdom);
37: }
38: 
39: bool CardinalityEstimator::SingleColumnFilter(duckdb::FilterInfo &filter_info) {
40: 	if (filter_info.left_set && filter_info.right_set && filter_info.set.get().count > 1) {
41: 		// Both set and are from different relations
42: 		return false;
43: 	}
44: 	if (EmptyFilter(filter_info)) {
45: 		return false;
46: 	}
47: 	if (filter_info.join_type == JoinType::SEMI || filter_info.join_type == JoinType::ANTI) {
48: 		return false;
49: 	}
50: 	return true;
51: }
52: 
53: vector<idx_t> CardinalityEstimator::DetermineMatchingEquivalentSets(optional_ptr<FilterInfo> filter_info) {
54: 	vector<idx_t> matching_equivalent_sets;
55: 	idx_t equivalent_relation_index = 0;
56: 
57: 	for (const RelationsToTDom &r2tdom : relations_to_tdoms) {
58: 		auto &i_set = r2tdom.equivalent_relations;
59: 		if (i_set.find(filter_info->left_binding) != i_set.end()) {
60: 			matching_equivalent_sets.push_back(equivalent_relation_index);
61: 		} else if (i_set.find(filter_info->right_binding) != i_set.end()) {
62: 			// don't add both left and right to the matching_equivalent_sets
63: 			// since both left and right get added to that index anyway.
64: 			matching_equivalent_sets.push_back(equivalent_relation_index);
65: 		}
66: 		equivalent_relation_index++;
67: 	}
68: 	return matching_equivalent_sets;
69: }
70: 
71: void CardinalityEstimator::AddToEquivalenceSets(optional_ptr<FilterInfo> filter_info,
72:                                                 vector<idx_t> matching_equivalent_sets) {
73: 	D_ASSERT(matching_equivalent_sets.size() <= 2);
74: 	if (matching_equivalent_sets.size() > 1) {
75: 		// an equivalence relation is connecting two sets of equivalence relations
76: 		// so push all relations from the second set into the first. Later we will delete
77: 		// the second set.
78: 		for (ColumnBinding i : relations_to_tdoms.at(matching_equivalent_sets[1]).equivalent_relations) {
79: 			relations_to_tdoms.at(matching_equivalent_sets[0]).equivalent_relations.insert(i);
80: 		}
81: 		for (auto &column_name : relations_to_tdoms.at(matching_equivalent_sets[1]).column_names) {
82: 			relations_to_tdoms.at(matching_equivalent_sets[0]).column_names.push_back(column_name);
83: 		}
84: 		relations_to_tdoms.at(matching_equivalent_sets[1]).equivalent_relations.clear();
85: 		relations_to_tdoms.at(matching_equivalent_sets[1]).column_names.clear();
86: 		relations_to_tdoms.at(matching_equivalent_sets[0]).filters.push_back(filter_info);
87: 		// add all values of one set to the other, delete the empty one
88: 	} else if (matching_equivalent_sets.size() == 1) {
89: 		auto &tdom_i = relations_to_tdoms.at(matching_equivalent_sets.at(0));
90: 		tdom_i.equivalent_relations.insert(filter_info->left_binding);
91: 		tdom_i.equivalent_relations.insert(filter_info->right_binding);
92: 		tdom_i.filters.push_back(filter_info);
93: 	} else if (matching_equivalent_sets.empty()) {
94: 		column_binding_set_t tmp;
95: 		tmp.insert(filter_info->left_binding);
96: 		tmp.insert(filter_info->right_binding);
97: 		relations_to_tdoms.emplace_back(tmp);
98: 		relations_to_tdoms.back().filters.push_back(filter_info);
99: 	}
100: }
101: 
102: void CardinalityEstimator::InitEquivalentRelations(const vector<unique_ptr<FilterInfo>> &filter_infos) {
103: 	// For each filter, we fill keep track of the index of the equivalent relation set
104: 	// the left and right relation needs to be added to.
105: 	for (auto &filter : filter_infos) {
106: 		if (SingleColumnFilter(*filter)) {
107: 			// Filter on one relation, (i.e. string or range filter on a column).
108: 			// Grab the first relation and add it to  the equivalence_relations
109: 			AddRelationTdom(*filter);
110: 			continue;
111: 		} else if (EmptyFilter(*filter)) {
112: 			continue;
113: 		}
114: 		D_ASSERT(filter->left_set->count >= 1);
115: 		D_ASSERT(filter->right_set->count >= 1);
116: 
117: 		auto matching_equivalent_sets = DetermineMatchingEquivalentSets(filter.get());
118: 		AddToEquivalenceSets(filter.get(), matching_equivalent_sets);
119: 	}
120: 	RemoveEmptyTotalDomains();
121: }
122: 
123: void CardinalityEstimator::RemoveEmptyTotalDomains() {
124: 	auto remove_start = std::remove_if(relations_to_tdoms.begin(), relations_to_tdoms.end(),
125: 	                                   [](RelationsToTDom &r_2_tdom) { return r_2_tdom.equivalent_relations.empty(); });
126: 	relations_to_tdoms.erase(remove_start, relations_to_tdoms.end());
127: }
128: 
129: double CardinalityEstimator::GetNumerator(JoinRelationSet &set) {
130: 	double numerator = 1;
131: 	for (idx_t i = 0; i < set.count; i++) {
132: 		auto &single_node_set = set_manager.GetJoinRelation(set.relations[i]);
133: 		auto card_helper = relation_set_2_cardinality[single_node_set.ToString()];
134: 		numerator *= card_helper.cardinality_before_filters == 0 ? 1 : card_helper.cardinality_before_filters;
135: 	}
136: 	return numerator;
137: }
138: 
139: bool EdgeConnects(FilterInfoWithTotalDomains &edge, Subgraph2Denominator &subgraph) {
140: 	if (edge.filter_info->left_set) {
141: 		if (JoinRelationSet::IsSubset(*subgraph.relations, *edge.filter_info->left_set)) {
142: 			// cool
143: 			return true;
144: 		}
145: 	}
146: 	if (edge.filter_info->right_set) {
147: 		if (JoinRelationSet::IsSubset(*subgraph.relations, *edge.filter_info->right_set)) {
148: 			return true;
149: 		}
150: 	}
151: 	return false;
152: }
153: 
154: vector<FilterInfoWithTotalDomains> GetEdges(vector<RelationsToTDom> &relations_to_tdom,
155:                                             JoinRelationSet &requested_set) {
156: 	vector<FilterInfoWithTotalDomains> res;
157: 	for (auto &relation_2_tdom : relations_to_tdom) {
158: 		for (auto &filter : relation_2_tdom.filters) {
159: 			if (JoinRelationSet::IsSubset(requested_set, filter->set)) {
160: 				FilterInfoWithTotalDomains new_edge(filter, relation_2_tdom);
161: 				res.push_back(new_edge);
162: 			}
163: 		}
164: 	}
165: 	return res;
166: }
167: 
168: vector<idx_t> SubgraphsConnectedByEdge(FilterInfoWithTotalDomains &edge, vector<Subgraph2Denominator> &subgraphs) {
169: 	vector<idx_t> res;
170: 	if (subgraphs.empty()) {
171: 		return res;
172: 	} else {
173: 		// check the combinations of subgraphs and see if the edge connects two of them,
174: 		// if so, return the indexes of the two subgraphs within the vector
175: 		for (idx_t outer = 0; outer != subgraphs.size(); outer++) {
176: 			// check if the edge connects two subgraphs.
177: 			for (idx_t inner = outer + 1; inner != subgraphs.size(); inner++) {
178: 				if (EdgeConnects(edge, subgraphs.at(outer)) && EdgeConnects(edge, subgraphs.at(inner))) {
179: 					// order is important because we will delete the inner subgraph later
180: 					res.push_back(outer);
181: 					res.push_back(inner);
182: 					return res;
183: 				}
184: 			}
185: 			// if the edge does not connect two subgraphs, see if the edge connects with just outer
186: 			// merge subgraph.at(outer) with the RelationSet(s) that edge connects
187: 			if (EdgeConnects(edge, subgraphs.at(outer))) {
188: 				res.push_back(outer);
189: 				return res;
190: 			}
191: 		}
192: 	}
193: 	// this edge connects only the relations it connects. Return an empty result so a new subgraph is created.
194: 	return res;
195: }
196: 
197: JoinRelationSet &CardinalityEstimator::UpdateNumeratorRelations(Subgraph2Denominator left, Subgraph2Denominator right,
198:                                                                 FilterInfoWithTotalDomains &filter) {
199: 	switch (filter.filter_info->join_type) {
200: 	case JoinType::SEMI:
201: 	case JoinType::ANTI: {
202: 		if (JoinRelationSet::IsSubset(*left.relations, *filter.filter_info->left_set) &&
203: 		    JoinRelationSet::IsSubset(*right.relations, *filter.filter_info->right_set)) {
204: 			return *left.numerator_relations;
205: 		}
206: 		return *right.numerator_relations;
207: 	}
208: 	default:
209: 		// cross product or inner join
210: 		return set_manager.Union(*left.numerator_relations, *right.numerator_relations);
211: 	}
212: }
213: 
214: double CardinalityEstimator::CalculateUpdatedDenom(Subgraph2Denominator left, Subgraph2Denominator right,
215:                                                    FilterInfoWithTotalDomains &filter) {
216: 	double new_denom = left.denom * right.denom;
217: 	switch (filter.filter_info->join_type) {
218: 	case JoinType::INNER: {
219: 		bool set = false;
220: 		ExpressionType comparison_type = ExpressionType::COMPARE_EQUAL;
221: 		ExpressionIterator::EnumerateExpression(filter.filter_info->filter, [&](Expression &expr) {
222: 			if (expr.GetExpressionClass() == ExpressionClass::BOUND_COMPARISON) {
223: 				comparison_type = expr.GetExpressionType();
224: 				set = true;
225: 				return;
226: 			}
227: 		});
228: 		if (!set) {
229: 			new_denom *=
230: 			    filter.has_tdom_hll ? static_cast<double>(filter.tdom_hll) : static_cast<double>(filter.tdom_no_hll);
231: 			// no comparison is taking place, so the denominator is just the product of the left and right
232: 			return new_denom;
233: 		}
234: 		// extra_ratio helps represents how many tuples will be filtered out if the comparison evaluates to
235: 		// false. set to 1 to assume cross product.
236: 		double extra_ratio = 1;
237: 		switch (comparison_type) {
238: 		case ExpressionType::COMPARE_EQUAL:
239: 		case ExpressionType::COMPARE_NOT_DISTINCT_FROM:
240: 			// extra ration stays 1
241: 			extra_ratio = filter.has_tdom_hll ? (double)filter.tdom_hll : (double)filter.tdom_no_hll;
242: 			break;
243: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO:
244: 		case ExpressionType::COMPARE_LESSTHAN:
245: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
246: 		case ExpressionType::COMPARE_GREATERTHAN:
247: 			// start with the selectivity of equality
248: 			extra_ratio = filter.has_tdom_hll ? (double)filter.tdom_hll : (double)filter.tdom_no_hll;
249: 			// now assume every tuple will match 2.5 times (on average)
250: 			extra_ratio *= static_cast<double>(1) / CardinalityEstimator::DEFAULT_LT_GT_MULTIPLIER;
251: 			break;
252: 		case ExpressionType::COMPARE_NOTEQUAL:
253: 		case ExpressionType::COMPARE_DISTINCT_FROM:
254: 			// basically assume cross product.
255: 			extra_ratio = 1;
256: 			break;
257: 		default:
258: 			break;
259: 		}
260: 		new_denom *= extra_ratio;
261: 		return new_denom;
262: 	}
263: 	case JoinType::SEMI:
264: 	case JoinType::ANTI: {
265: 		if (JoinRelationSet::IsSubset(*left.relations, *filter.filter_info->left_set) &&
266: 		    JoinRelationSet::IsSubset(*right.relations, *filter.filter_info->right_set)) {
267: 			new_denom = left.denom * CardinalityEstimator::DEFAULT_SEMI_ANTI_SELECTIVITY;
268: 			return new_denom;
269: 		}
270: 		new_denom = right.denom * CardinalityEstimator::DEFAULT_SEMI_ANTI_SELECTIVITY;
271: 		return new_denom;
272: 	}
273: 	default:
274: 		// cross product
275: 		return new_denom;
276: 	}
277: }
278: 
279: DenomInfo CardinalityEstimator::GetDenominator(JoinRelationSet &set) {
280: 	vector<Subgraph2Denominator> subgraphs;
281: 
282: 	// Finding the denominator is tricky. You need to go through the tdoms in decreasing order
283: 	// Then loop through all filters in the equivalence set of the tdom to see if both the
284: 	// left and right relations are in the new set, if so you can use that filter.
285: 	// You must also make sure that the filters all relations in the given set, so we use subgraphs
286: 	// that should eventually merge into one connected graph that joins all the relations
287: 	// TODO: Implement a method to cache subgraphs so you don't have to build them up every
288: 	// time the cardinality of a new set is requested
289: 
290: 	// relations_to_tdoms has already been sorted by largest to smallest total domain
291: 	// then we look through the filters for the relations_to_tdoms,
292: 	// and we start to choose the filters that join relations in the set.
293: 
294: 	// edges are guaranteed to be in order of largest tdom to smallest tdom.
295: 	unordered_set<idx_t> unused_edge_tdoms;
296: 	auto edges = GetEdges(relations_to_tdoms, set);
297: 	for (auto &edge : edges) {
298: 		if (subgraphs.size() == 1 && subgraphs.at(0).relations->ToString() == set.ToString()) {
299: 			// the first subgraph has connected all the desired relations, just skip the rest of the edges
300: 			if (edge.has_tdom_hll) {
301: 				unused_edge_tdoms.insert(edge.tdom_hll);
302: 			}
303: 			continue;
304: 		}
305: 
306: 		auto subgraph_connections = SubgraphsConnectedByEdge(edge, subgraphs);
307: 		if (subgraph_connections.empty()) {
308: 			// create a subgraph out of left and right, then merge right into left and add left to subgraphs.
309: 			// this helps cover a case where there are no subgraphs yet, and the only join filter is a SEMI JOIN
310: 			auto left_subgraph = Subgraph2Denominator();
311: 			auto right_subgraph = Subgraph2Denominator();
312: 			left_subgraph.relations = edge.filter_info->left_set;
313: 			left_subgraph.numerator_relations = edge.filter_info->left_set;
314: 			right_subgraph.relations = edge.filter_info->right_set;
315: 			right_subgraph.numerator_relations = edge.filter_info->right_set;
316: 			left_subgraph.numerator_relations = &UpdateNumeratorRelations(left_subgraph, right_subgraph, edge);
317: 			left_subgraph.relations = edge.filter_info->set.get();
318: 			left_subgraph.denom = CalculateUpdatedDenom(left_subgraph, right_subgraph, edge);
319: 			subgraphs.push_back(left_subgraph);
320: 		} else if (subgraph_connections.size() == 1) {
321: 			auto left_subgraph = &subgraphs.at(subgraph_connections.at(0));
322: 			auto right_subgraph = Subgraph2Denominator();
323: 			right_subgraph.relations = edge.filter_info->right_set;
324: 			right_subgraph.numerator_relations = edge.filter_info->right_set;
325: 			if (JoinRelationSet::IsSubset(*left_subgraph->relations, *right_subgraph.relations)) {
326: 				right_subgraph.relations = edge.filter_info->left_set;
327: 				right_subgraph.numerator_relations = edge.filter_info->left_set;
328: 			}
329: 
330: 			if (JoinRelationSet::IsSubset(*left_subgraph->relations, *edge.filter_info->left_set) &&
331: 			    JoinRelationSet::IsSubset(*left_subgraph->relations, *edge.filter_info->right_set)) {
332: 				// here we have an edge that connects the same subgraph to the same subgraph. Just continue. no need to
333: 				// update the denom
334: 				continue;
335: 			}
336: 			left_subgraph->numerator_relations = &UpdateNumeratorRelations(*left_subgraph, right_subgraph, edge);
337: 			left_subgraph->relations = &set_manager.Union(*left_subgraph->relations, *right_subgraph.relations);
338: 			left_subgraph->denom = CalculateUpdatedDenom(*left_subgraph, right_subgraph, edge);
339: 		} else if (subgraph_connections.size() == 2) {
340: 			// The two subgraphs in the subgraph_connections can be merged by this edge.
341: 			D_ASSERT(subgraph_connections.at(0) < subgraph_connections.at(1));
342: 			auto subgraph_to_merge_into = &subgraphs.at(subgraph_connections.at(0));
343: 			auto subgraph_to_delete = &subgraphs.at(subgraph_connections.at(1));
344: 			subgraph_to_merge_into->relations =
345: 			    &set_manager.Union(*subgraph_to_merge_into->relations, *subgraph_to_delete->relations);
346: 			subgraph_to_merge_into->numerator_relations =
347: 			    &UpdateNumeratorRelations(*subgraph_to_merge_into, *subgraph_to_delete, edge);
348: 			subgraph_to_merge_into->denom = CalculateUpdatedDenom(*subgraph_to_merge_into, *subgraph_to_delete, edge);
349: 			subgraph_to_delete->relations = nullptr;
350: 			auto remove_start = std::remove_if(subgraphs.begin(), subgraphs.end(),
351: 			                                   [](Subgraph2Denominator &s) { return !s.relations; });
352: 			subgraphs.erase(remove_start, subgraphs.end());
353: 		}
354: 	}
355: 
356: 	// Slight penalty to cardinality for unused edges
357: 	auto denom_multiplier = 1.0 + static_cast<double>(unused_edge_tdoms.size());
358: 
359: 	// It's possible cross-products were added and are not present in the filters in the relation_2_tdom
360: 	// structures. When that's the case, merge all remaining subgraphs.
361: 	if (subgraphs.size() > 1) {
362: 		auto final_subgraph = subgraphs.at(0);
363: 		for (auto merge_with = subgraphs.begin() + 1; merge_with != subgraphs.end(); merge_with++) {
364: 			D_ASSERT(final_subgraph.relations && merge_with->relations);
365: 			final_subgraph.relations = &set_manager.Union(*final_subgraph.relations, *merge_with->relations);
366: 			D_ASSERT(final_subgraph.numerator_relations && merge_with->numerator_relations);
367: 			final_subgraph.numerator_relations =
368: 			    &set_manager.Union(*final_subgraph.numerator_relations, *merge_with->numerator_relations);
369: 			final_subgraph.denom *= merge_with->denom;
370: 		}
371: 	}
372: 	// can happen if a table has cardinality 0, a tdom is set to 0, or if a cross product is used.
373: 	if (subgraphs.empty() || subgraphs.at(0).denom == 0) {
374: 		// denominator is 1 and numerators are a cross product of cardinalities.
375: 		return DenomInfo(set, 1, 1);
376: 	}
377: 	return DenomInfo(*subgraphs.at(0).numerator_relations, 1, subgraphs.at(0).denom * denom_multiplier);
378: }
379: 
380: template <>
381: double CardinalityEstimator::EstimateCardinalityWithSet(JoinRelationSet &new_set) {
382: 
383: 	if (relation_set_2_cardinality.find(new_set.ToString()) != relation_set_2_cardinality.end()) {
384: 		return relation_set_2_cardinality[new_set.ToString()].cardinality_before_filters;
385: 	}
386: 
387: 	// can happen if a table has cardinality 0, or a tdom is set to 0
388: 	auto denom = GetDenominator(new_set);
389: 	auto numerator = GetNumerator(denom.numerator_relations);
390: 
391: 	double result = numerator / denom.denominator;
392: 	auto new_entry = CardinalityHelper(result);
393: 	relation_set_2_cardinality[new_set.ToString()] = new_entry;
394: 	return result;
395: }
396: 
397: template <>
398: idx_t CardinalityEstimator::EstimateCardinalityWithSet(JoinRelationSet &new_set) {
399: 	auto cardinality_as_double = EstimateCardinalityWithSet<double>(new_set);
400: 	auto max = NumericLimits<idx_t>::Maximum();
401: 	if (cardinality_as_double >= (double)max) {
402: 		return max;
403: 	}
404: 	return (idx_t)cardinality_as_double;
405: }
406: 
407: bool SortTdoms(const RelationsToTDom &a, const RelationsToTDom &b) {
408: 	if (a.has_tdom_hll && b.has_tdom_hll) {
409: 		return a.tdom_hll > b.tdom_hll;
410: 	}
411: 	if (a.has_tdom_hll) {
412: 		return a.tdom_hll > b.tdom_no_hll;
413: 	}
414: 	if (b.has_tdom_hll) {
415: 		return a.tdom_no_hll > b.tdom_hll;
416: 	}
417: 	return a.tdom_no_hll > b.tdom_no_hll;
418: }
419: 
420: void CardinalityEstimator::InitCardinalityEstimatorProps(optional_ptr<JoinRelationSet> set, RelationStats &stats) {
421: 	// Get the join relation set
422: 	D_ASSERT(stats.stats_initialized);
423: 	auto relation_cardinality = stats.cardinality;
424: 
425: 	auto card_helper = CardinalityHelper((double)relation_cardinality);
426: 	relation_set_2_cardinality[set->ToString()] = card_helper;
427: 
428: 	UpdateTotalDomains(set, stats);
429: 
430: 	// sort relations from greatest tdom to lowest tdom.
431: 	std::sort(relations_to_tdoms.begin(), relations_to_tdoms.end(), SortTdoms);
432: }
433: 
434: void CardinalityEstimator::UpdateTotalDomains(optional_ptr<JoinRelationSet> set, RelationStats &stats) {
435: 	D_ASSERT(set->count == 1);
436: 	auto relation_id = set->relations[0];
437: 	//! Initialize the distinct count for all columns used in joins with the current relation.
438: 	//	D_ASSERT(stats.column_distinct_count.size() >= 1);
439: 
440: 	for (idx_t i = 0; i < stats.column_distinct_count.size(); i++) {
441: 		//! for every column used in a filter in the relation, get the distinct count via HLL, or assume it to be
442: 		//! the cardinality
443: 		// Update the relation_to_tdom set with the estimated distinct count (or tdom) calculated above
444: 		auto key = ColumnBinding(relation_id, i);
445: 		for (auto &relation_to_tdom : relations_to_tdoms) {
446: 			column_binding_set_t i_set = relation_to_tdom.equivalent_relations;
447: 			if (i_set.find(key) == i_set.end()) {
448: 				continue;
449: 			}
450: 			auto distinct_count = stats.column_distinct_count.at(i);
451: 			if (distinct_count.from_hll && relation_to_tdom.has_tdom_hll) {
452: 				relation_to_tdom.tdom_hll = MaxValue(relation_to_tdom.tdom_hll, distinct_count.distinct_count);
453: 			} else if (distinct_count.from_hll && !relation_to_tdom.has_tdom_hll) {
454: 				relation_to_tdom.has_tdom_hll = true;
455: 				relation_to_tdom.tdom_hll = distinct_count.distinct_count;
456: 			} else {
457: 				relation_to_tdom.tdom_no_hll = MinValue(distinct_count.distinct_count, relation_to_tdom.tdom_no_hll);
458: 			}
459: 			break;
460: 		}
461: 	}
462: }
463: 
464: // LCOV_EXCL_START
465: 
466: void CardinalityEstimator::AddRelationNamesToTdoms(vector<RelationStats> &stats) {
467: #ifdef DEBUG
468: 	for (auto &total_domain : relations_to_tdoms) {
469: 		for (auto &binding : total_domain.equivalent_relations) {
470: 			D_ASSERT(binding.table_index < stats.size());
471: 			string column_name;
472: 			if (binding.column_index < stats[binding.table_index].column_names.size()) {
473: 				column_name = stats[binding.table_index].column_names[binding.column_index];
474: 			} else {
475: 				column_name = "[unknown]";
476: 			}
477: 			total_domain.column_names.push_back(column_name);
478: 		}
479: 	}
480: #endif
481: }
482: 
483: void CardinalityEstimator::PrintRelationToTdomInfo() {
484: 	for (auto &total_domain : relations_to_tdoms) {
485: 		string domain = "Following columns have the same distinct count: ";
486: 		for (auto &column_name : total_domain.column_names) {
487: 			domain += column_name + ", ";
488: 		}
489: 		bool have_hll = total_domain.has_tdom_hll;
490: 		domain += "\n TOTAL DOMAIN = " + to_string(have_hll ? total_domain.tdom_hll : total_domain.tdom_no_hll);
491: 		Printer::Print(domain);
492: 	}
493: }
494: 
495: // LCOV_EXCL_STOP
496: 
497: } // namespace duckdb
[end of src/optimizer/join_order/cardinality_estimator.cpp]
[start of src/optimizer/join_order/relation_manager.cpp]
1: #include "duckdb/optimizer/join_order/relation_manager.hpp"
2: 
3: #include "duckdb/common/enums/join_type.hpp"
4: #include "duckdb/common/enums/logical_operator_type.hpp"
5: #include "duckdb/common/string_util.hpp"
6: #include "duckdb/optimizer/join_order/join_order_optimizer.hpp"
7: #include "duckdb/optimizer/join_order/relation_statistics_helper.hpp"
8: #include "duckdb/parser/expression_map.hpp"
9: #include "duckdb/planner/expression/list.hpp"
10: #include "duckdb/planner/expression_iterator.hpp"
11: #include "duckdb/planner/operator/list.hpp"
12: 
13: namespace duckdb {
14: 
15: const vector<RelationStats> RelationManager::GetRelationStats() {
16: 	vector<RelationStats> ret;
17: 	for (idx_t i = 0; i < relations.size(); i++) {
18: 		ret.push_back(relations[i]->stats);
19: 	}
20: 	return ret;
21: }
22: 
23: vector<unique_ptr<SingleJoinRelation>> RelationManager::GetRelations() {
24: 	return std::move(relations);
25: }
26: 
27: idx_t RelationManager::NumRelations() {
28: 	return relations.size();
29: }
30: 
31: void RelationManager::AddAggregateOrWindowRelation(LogicalOperator &op, optional_ptr<LogicalOperator> parent,
32:                                                    const RelationStats &stats, LogicalOperatorType op_type) {
33: 	auto relation = make_uniq<SingleJoinRelation>(op, parent, stats);
34: 	auto relation_id = relations.size();
35: 
36: 	auto op_bindings = op.GetColumnBindings();
37: 	for (auto &binding : op_bindings) {
38: 		if (relation_mapping.find(binding.table_index) == relation_mapping.end()) {
39: 			relation_mapping[binding.table_index] = relation_id;
40: 		}
41: 	}
42: 	relations.push_back(std::move(relation));
43: 	op.estimated_cardinality = stats.cardinality;
44: 	op.has_estimated_cardinality = true;
45: }
46: 
47: void RelationManager::AddRelation(LogicalOperator &op, optional_ptr<LogicalOperator> parent,
48:                                   const RelationStats &stats) {
49: 
50: 	// if parent is null, then this is a root relation
51: 	// if parent is not null, it should have multiple children
52: 	D_ASSERT(!parent || parent->children.size() >= 2);
53: 	auto relation = make_uniq<SingleJoinRelation>(op, parent, stats);
54: 	auto relation_id = relations.size();
55: 
56: 	auto table_indexes = op.GetTableIndex();
57: 	if (table_indexes.empty()) {
58: 		// relation represents a non-reorderable relation, most likely a join relation
59: 		// Get the tables referenced in the non-reorderable relation and add them to the relation mapping
60: 		// This should all table references, even if there are nested non-reorderable joins.
61: 		unordered_set<idx_t> table_references;
62: 		LogicalJoin::GetTableReferences(op, table_references);
63: 		D_ASSERT(table_references.size() > 0);
64: 		for (auto &reference : table_references) {
65: 			D_ASSERT(relation_mapping.find(reference) == relation_mapping.end());
66: 			relation_mapping[reference] = relation_id;
67: 		}
68: 	} else if (op.type == LogicalOperatorType::LOGICAL_UNNEST) {
69: 		// logical unnest has a logical_unnest index, but other bindings can refer to
70: 		// columns that are not unnested.
71: 		auto bindings = op.GetColumnBindings();
72: 		for (auto &binding : bindings) {
73: 			relation_mapping[binding.table_index] = relation_id;
74: 		}
75: 	} else {
76: 		// Relations should never return more than 1 table index
77: 		D_ASSERT(table_indexes.size() == 1);
78: 		idx_t table_index = table_indexes.at(0);
79: 		D_ASSERT(relation_mapping.find(table_index) == relation_mapping.end());
80: 		relation_mapping[table_index] = relation_id;
81: 	}
82: 	relations.push_back(std::move(relation));
83: 	op.estimated_cardinality = stats.cardinality;
84: 	op.has_estimated_cardinality = true;
85: }
86: 
87: bool RelationManager::CrossProductWithRelationAllowed(idx_t relation_id) {
88: 	return no_cross_product_relations.find(relation_id) == no_cross_product_relations.end();
89: }
90: 
91: static bool OperatorNeedsRelation(LogicalOperatorType op_type) {
92: 	switch (op_type) {
93: 	case LogicalOperatorType::LOGICAL_PROJECTION:
94: 	case LogicalOperatorType::LOGICAL_EXPRESSION_GET:
95: 	case LogicalOperatorType::LOGICAL_GET:
96: 	case LogicalOperatorType::LOGICAL_UNNEST:
97: 	case LogicalOperatorType::LOGICAL_DELIM_GET:
98: 	case LogicalOperatorType::LOGICAL_AGGREGATE_AND_GROUP_BY:
99: 	case LogicalOperatorType::LOGICAL_WINDOW:
100: 	case LogicalOperatorType::LOGICAL_SAMPLE:
101: 		return true;
102: 	default:
103: 		return false;
104: 	}
105: }
106: 
107: static bool OperatorIsNonReorderable(LogicalOperatorType op_type) {
108: 	switch (op_type) {
109: 	case LogicalOperatorType::LOGICAL_UNION:
110: 	case LogicalOperatorType::LOGICAL_EXCEPT:
111: 	case LogicalOperatorType::LOGICAL_INTERSECT:
112: 	case LogicalOperatorType::LOGICAL_ANY_JOIN:
113: 	case LogicalOperatorType::LOGICAL_ASOF_JOIN:
114: 		return true;
115: 	default:
116: 		return false;
117: 	}
118: }
119: 
120: bool ExpressionContainsColumnRef(Expression &expression) {
121: 	if (expression.GetExpressionType() == ExpressionType::BOUND_COLUMN_REF) {
122: 		// Here you have a filter on a single column in a table. Return a binding for the column
123: 		// being filtered on so the filter estimator knows what HLL count to pull
124: #ifdef DEBUG
125: 		auto &colref = expression.Cast<BoundColumnRefExpression>();
126: 		(void)colref.depth;
127: 		D_ASSERT(colref.depth == 0);
128: 		D_ASSERT(colref.binding.table_index != DConstants::INVALID_INDEX);
129: #endif
130: 		// map the base table index to the relation index used by the JoinOrderOptimizer
131: 		return true;
132: 	}
133: 	// TODO: handle inequality filters with functions.
134: 	auto children_ret = false;
135: 	ExpressionIterator::EnumerateChildren(expression,
136: 	                                      [&](Expression &expr) { children_ret = ExpressionContainsColumnRef(expr); });
137: 	return children_ret;
138: }
139: 
140: static bool JoinIsReorderable(LogicalOperator &op) {
141: 	if (op.type == LogicalOperatorType::LOGICAL_CROSS_PRODUCT) {
142: 		return true;
143: 	} else if (op.type == LogicalOperatorType::LOGICAL_COMPARISON_JOIN) {
144: 		auto &join = op.Cast<LogicalComparisonJoin>();
145: 		switch (join.join_type) {
146: 		case JoinType::INNER:
147: 		case JoinType::SEMI:
148: 		case JoinType::ANTI:
149: 			for (auto &cond : join.conditions) {
150: 				if (ExpressionContainsColumnRef(*cond.left) && ExpressionContainsColumnRef(*cond.right)) {
151: 					return true;
152: 				}
153: 			}
154: 			return false;
155: 		default:
156: 			return false;
157: 		}
158: 	}
159: 	return false;
160: }
161: 
162: static bool HasNonReorderableChild(LogicalOperator &op) {
163: 	LogicalOperator *tmp = &op;
164: 	while (tmp->children.size() == 1) {
165: 		if (OperatorNeedsRelation(tmp->type) || OperatorIsNonReorderable(tmp->type)) {
166: 			return true;
167: 		}
168: 		tmp = tmp->children[0].get();
169: 		if (tmp->type == LogicalOperatorType::LOGICAL_COMPARISON_JOIN) {
170: 			if (!JoinIsReorderable(*tmp)) {
171: 				return true;
172: 			}
173: 		}
174: 	}
175: 	return tmp->children.empty();
176: }
177: 
178: static void ModifyStatsIfLimit(optional_ptr<LogicalOperator> limit_op, RelationStats &stats) {
179: 	if (!limit_op) {
180: 		return;
181: 	}
182: 	auto &limit = limit_op->Cast<LogicalLimit>();
183: 	if (limit.limit_val.Type() == LimitNodeType::CONSTANT_VALUE) {
184: 		stats.cardinality = MinValue(limit.limit_val.GetConstantValue(), stats.cardinality);
185: 	}
186: }
187: 
188: bool RelationManager::ExtractJoinRelations(JoinOrderOptimizer &optimizer, LogicalOperator &input_op,
189:                                            vector<reference<LogicalOperator>> &filter_operators,
190:                                            optional_ptr<LogicalOperator> parent) {
191: 	optional_ptr<LogicalOperator> op = &input_op;
192: 	vector<reference<LogicalOperator>> datasource_filters;
193: 	optional_ptr<LogicalOperator> limit_op = nullptr;
194: 	// pass through single child operators
195: 	while (op->children.size() == 1 && !OperatorNeedsRelation(op->type)) {
196: 		if (op->type == LogicalOperatorType::LOGICAL_FILTER) {
197: 			if (HasNonReorderableChild(*op)) {
198: 				datasource_filters.push_back(*op);
199: 			}
200: 			filter_operators.push_back(*op);
201: 		}
202: 		if (op->type == LogicalOperatorType::LOGICAL_LIMIT) {
203: 			limit_op = op;
204: 		}
205: 		op = op->children[0].get();
206: 	}
207: 	bool non_reorderable_operation = false;
208: 	if (OperatorIsNonReorderable(op->type)) {
209: 		// set operation, optimize separately in children
210: 		non_reorderable_operation = true;
211: 	}
212: 
213: 	if (op->type == LogicalOperatorType::LOGICAL_COMPARISON_JOIN) {
214: 		if (JoinIsReorderable(*op)) {
215: 			// extract join conditions from inner join
216: 			filter_operators.push_back(*op);
217: 		} else {
218: 			non_reorderable_operation = true;
219: 		}
220: 	}
221: 	if (non_reorderable_operation) {
222: 		// we encountered a non-reordable operation (setop or non-inner join)
223: 		// we do not reorder non-inner joins yet, however we do want to expand the potential join graph around them
224: 		// non-inner joins are also tricky because we can't freely make conditions through them
225: 		// e.g. suppose we have (left LEFT OUTER JOIN right WHERE right IS NOT NULL), the join can generate
226: 		// new NULL values in the right side, so pushing this condition through the join leads to incorrect results
227: 		// for this reason, we just start a new JoinOptimizer pass in each of the children of the join
228: 		// stats.cardinality will be initiated to highest cardinality of the children.
229: 		vector<RelationStats> children_stats;
230: 		for (auto &child : op->children) {
231: 			auto stats = RelationStats();
232: 			auto child_optimizer = optimizer.CreateChildOptimizer();
233: 			child = child_optimizer.Optimize(std::move(child), &stats);
234: 			children_stats.push_back(stats);
235: 		}
236: 
237: 		auto combined_stats = RelationStatisticsHelper::CombineStatsOfNonReorderableOperator(*op, children_stats);
238: 		op->SetEstimatedCardinality(combined_stats.cardinality);
239: 		if (!datasource_filters.empty()) {
240: 			combined_stats.cardinality = (idx_t)MaxValue(
241: 			    double(combined_stats.cardinality) * RelationStatisticsHelper::DEFAULT_SELECTIVITY, (double)1);
242: 		}
243: 		AddRelation(input_op, parent, combined_stats);
244: 		return true;
245: 	}
246: 
247: 	switch (op->type) {
248: 	case LogicalOperatorType::LOGICAL_AGGREGATE_AND_GROUP_BY: {
249: 		// optimize children
250: 		RelationStats child_stats;
251: 		auto child_optimizer = optimizer.CreateChildOptimizer();
252: 		op->children[0] = child_optimizer.Optimize(std::move(op->children[0]), &child_stats);
253: 		auto &aggr = op->Cast<LogicalAggregate>();
254: 		auto operator_stats = RelationStatisticsHelper::ExtractAggregationStats(aggr, child_stats);
255: 		// the extracted cardinality should be set for aggregate
256: 		aggr.SetEstimatedCardinality(operator_stats.cardinality);
257: 		if (!datasource_filters.empty()) {
258: 			operator_stats.cardinality = LossyNumericCast<idx_t>(static_cast<double>(operator_stats.cardinality) *
259: 			                                                     RelationStatisticsHelper::DEFAULT_SELECTIVITY);
260: 		}
261: 		ModifyStatsIfLimit(limit_op.get(), child_stats);
262: 		AddAggregateOrWindowRelation(input_op, parent, operator_stats, op->type);
263: 		return true;
264: 	}
265: 	case LogicalOperatorType::LOGICAL_WINDOW: {
266: 		// optimize children
267: 		RelationStats child_stats;
268: 		auto child_optimizer = optimizer.CreateChildOptimizer();
269: 		op->children[0] = child_optimizer.Optimize(std::move(op->children[0]), &child_stats);
270: 		auto &window = op->Cast<LogicalWindow>();
271: 		auto operator_stats = RelationStatisticsHelper::ExtractWindowStats(window, child_stats);
272: 		// the extracted cardinality should be set for window
273: 		window.SetEstimatedCardinality(operator_stats.cardinality);
274: 		if (!datasource_filters.empty()) {
275: 			operator_stats.cardinality = LossyNumericCast<idx_t>(static_cast<double>(operator_stats.cardinality) *
276: 			                                                     RelationStatisticsHelper::DEFAULT_SELECTIVITY);
277: 		}
278: 		ModifyStatsIfLimit(limit_op.get(), child_stats);
279: 		AddAggregateOrWindowRelation(input_op, parent, operator_stats, op->type);
280: 		return true;
281: 	}
282: 	case LogicalOperatorType::LOGICAL_UNNEST: {
283: 		// optimize children of unnest
284: 		RelationStats child_stats;
285: 		auto child_optimizer = optimizer.CreateChildOptimizer();
286: 		op->children[0] = child_optimizer.Optimize(std::move(op->children[0]), &child_stats);
287: 		// the extracted cardinality should be set for window
288: 		if (!datasource_filters.empty()) {
289: 			child_stats.cardinality = LossyNumericCast<idx_t>(static_cast<double>(child_stats.cardinality) *
290: 			                                                  RelationStatisticsHelper::DEFAULT_SELECTIVITY);
291: 		}
292: 		ModifyStatsIfLimit(limit_op.get(), child_stats);
293: 		AddRelation(input_op, parent, child_stats);
294: 		return true;
295: 	}
296: 	case LogicalOperatorType::LOGICAL_COMPARISON_JOIN: {
297: 		auto &join = op->Cast<LogicalComparisonJoin>();
298: 		// Adding relations of the left side to the current join order optimizer
299: 		bool can_reorder_left = ExtractJoinRelations(optimizer, *op->children[0], filter_operators, op);
300: 		bool can_reorder_right = true;
301: 		// For semi & anti joins, you only reorder relations in the left side of the join.
302: 		// We do not want to reorder a relation A into the right side because then all column bindings A from A will be
303: 		// lost after the semi or anti join
304: 
305: 		// We cannot reorder a relation B out of the right side because any filter/join in the right side
306: 		// between a relation B and another RHS relation will be invalid. The semi join will remove
307: 		// all right column bindings,
308: 
309: 		// So we treat the right side of left join as its own relation so no relations
310: 		// are pushed into the right side, or taken out of the right side.
311: 		if (join.join_type == JoinType::SEMI || join.join_type == JoinType::ANTI) {
312: 			RelationStats child_stats;
313: 			// optimize the child and copy the stats
314: 			auto child_optimizer = optimizer.CreateChildOptimizer();
315: 			op->children[1] = child_optimizer.Optimize(std::move(op->children[1]), &child_stats);
316: 			AddRelation(*op->children[1], op, child_stats);
317: 			// remember that if a cross product needs to be forced, it cannot be forced
318: 			// across the children of a semi or anti join
319: 			no_cross_product_relations.insert(relations.size() - 1);
320: 			auto right_child_bindings = op->children[1]->GetColumnBindings();
321: 			for (auto &bindings : right_child_bindings) {
322: 				relation_mapping[bindings.table_index] = relations.size() - 1;
323: 			}
324: 		} else {
325: 			can_reorder_right = ExtractJoinRelations(optimizer, *op->children[1], filter_operators, op);
326: 		}
327: 		return can_reorder_left && can_reorder_right;
328: 	}
329: 	case LogicalOperatorType::LOGICAL_CROSS_PRODUCT: {
330: 		bool can_reorder_left = ExtractJoinRelations(optimizer, *op->children[0], filter_operators, op);
331: 		bool can_reorder_right = ExtractJoinRelations(optimizer, *op->children[1], filter_operators, op);
332: 		return can_reorder_left && can_reorder_right;
333: 	}
334: 	case LogicalOperatorType::LOGICAL_DUMMY_SCAN: {
335: 		auto &dummy_scan = op->Cast<LogicalDummyScan>();
336: 		auto stats = RelationStatisticsHelper::ExtractDummyScanStats(dummy_scan, context);
337: 		AddRelation(input_op, parent, stats);
338: 		return true;
339: 	}
340: 	case LogicalOperatorType::LOGICAL_EXPRESSION_GET: {
341: 		// base table scan, add to set of relations.
342: 		// create empty stats for dummy scan or logical expression get
343: 		auto &expression_get = op->Cast<LogicalExpressionGet>();
344: 		auto stats = RelationStatisticsHelper::ExtractExpressionGetStats(expression_get, context);
345: 		AddRelation(input_op, parent, stats);
346: 		return true;
347: 	}
348: 	case LogicalOperatorType::LOGICAL_GET: {
349: 		// TODO: Get stats from a logical GET
350: 		auto &get = op->Cast<LogicalGet>();
351: 		auto stats = RelationStatisticsHelper::ExtractGetStats(get, context);
352: 		// if there is another logical filter that could not be pushed down into the
353: 		// table scan, apply another selectivity.
354: 		get.SetEstimatedCardinality(stats.cardinality);
355: 		if (!datasource_filters.empty()) {
356: 			stats.cardinality =
357: 			    (idx_t)MaxValue(double(stats.cardinality) * RelationStatisticsHelper::DEFAULT_SELECTIVITY, (double)1);
358: 		}
359: 		ModifyStatsIfLimit(limit_op.get(), stats);
360: 		AddRelation(input_op, parent, stats);
361: 		return true;
362: 	}
363: 	case LogicalOperatorType::LOGICAL_PROJECTION: {
364: 		RelationStats child_stats;
365: 		// optimize the child and copy the stats
366: 		auto child_optimizer = optimizer.CreateChildOptimizer();
367: 		op->children[0] = child_optimizer.Optimize(std::move(op->children[0]), &child_stats);
368: 		auto &proj = op->Cast<LogicalProjection>();
369: 		// Projection can create columns so we need to add them here
370: 		auto proj_stats = RelationStatisticsHelper::ExtractProjectionStats(proj, child_stats);
371: 		proj.SetEstimatedCardinality(proj_stats.cardinality);
372: 		ModifyStatsIfLimit(limit_op.get(), proj_stats);
373: 		AddRelation(input_op, parent, proj_stats);
374: 		return true;
375: 	}
376: 	case LogicalOperatorType::LOGICAL_EMPTY_RESULT: {
377: 		// optimize the child and copy the stats
378: 		auto &empty_result = op->Cast<LogicalEmptyResult>();
379: 		// Projection can create columns so we need to add them here
380: 		auto stats = RelationStatisticsHelper::ExtractEmptyResultStats(empty_result);
381: 		empty_result.SetEstimatedCardinality(stats.cardinality);
382: 		AddRelation(input_op, parent, stats);
383: 		return true;
384: 	}
385: 	case LogicalOperatorType::LOGICAL_MATERIALIZED_CTE:
386: 	case LogicalOperatorType::LOGICAL_RECURSIVE_CTE: {
387: 		RelationStats lhs_stats;
388: 		// optimize the lhs child and copy the stats
389: 		auto lhs_optimizer = optimizer.CreateChildOptimizer();
390: 		op->children[0] = lhs_optimizer.Optimize(std::move(op->children[0]), &lhs_stats);
391: 		// optimize the rhs child
392: 		auto rhs_optimizer = optimizer.CreateChildOptimizer();
393: 		auto table_index = op->type == LogicalOperatorType::LOGICAL_MATERIALIZED_CTE
394: 		                       ? op->Cast<LogicalMaterializedCTE>().table_index
395: 		                       : op->Cast<LogicalRecursiveCTE>().table_index;
396: 		rhs_optimizer.AddMaterializedCTEStats(table_index, std::move(lhs_stats));
397: 		op->children[1] = rhs_optimizer.Optimize(std::move(op->children[1]));
398: 		return false;
399: 	}
400: 	case LogicalOperatorType::LOGICAL_CTE_REF: {
401: 		auto &cte_ref = op->Cast<LogicalCTERef>();
402: 		if (cte_ref.materialized_cte != CTEMaterialize::CTE_MATERIALIZE_ALWAYS) {
403: 			return false;
404: 		}
405: 		auto cte_stats = optimizer.GetMaterializedCTEStats(cte_ref.cte_index);
406: 		cte_ref.SetEstimatedCardinality(cte_stats.cardinality);
407: 		AddRelation(input_op, parent, cte_stats);
408: 		return true;
409: 	}
410: 	case LogicalOperatorType::LOGICAL_DELIM_JOIN: {
411: 		auto &delim_join = op->Cast<LogicalComparisonJoin>();
412: 
413: 		// optimize LHS (duplicate-eliminated) child
414: 		RelationStats lhs_stats;
415: 		auto lhs_optimizer = optimizer.CreateChildOptimizer();
416: 		op->children[0] = lhs_optimizer.Optimize(std::move(op->children[0]), &lhs_stats);
417: 
418: 		// create dummy aggregation for the duplicate elimination
419: 		auto dummy_aggr = make_uniq<LogicalAggregate>(DConstants::INVALID_INDEX - 1, DConstants::INVALID_INDEX,
420: 		                                              vector<unique_ptr<Expression>>());
421: 		for (auto &delim_col : delim_join.duplicate_eliminated_columns) {
422: 			dummy_aggr->groups.push_back(delim_col->Copy());
423: 		}
424: 		auto lhs_delim_stats = RelationStatisticsHelper::ExtractAggregationStats(*dummy_aggr, lhs_stats);
425: 
426: 		// optimize the other child, which will now have access to the stats
427: 		RelationStats rhs_stats;
428: 		auto rhs_optimizer = optimizer.CreateChildOptimizer();
429: 		rhs_optimizer.AddDelimScanStats(lhs_delim_stats);
430: 		op->children[1] = rhs_optimizer.Optimize(std::move(op->children[1]), rhs_stats);
431: 
432: 		return false;
433: 	}
434: 	case LogicalOperatorType::LOGICAL_DELIM_GET: {
435: 		// Used to not be possible to reorder these. We added reordering (without stats) before,
436: 		// but ran into terrible join orders (see internal issue #596), so we removed it again
437: 		// We now have proper statistics for DelimGets, and get an even better query plan for #596
438: 		auto delim_scan_stats = optimizer.GetDelimScanStats();
439: 		op->SetEstimatedCardinality(delim_scan_stats.cardinality);
440: 		AddAggregateOrWindowRelation(input_op, parent, delim_scan_stats, op->type);
441: 		return true;
442: 	}
443: 	default:
444: 		return false;
445: 	}
446: }
447: 
448: bool RelationManager::ExtractBindings(Expression &expression, unordered_set<idx_t> &bindings) {
449: 	if (expression.GetExpressionType() == ExpressionType::BOUND_COLUMN_REF) {
450: 		auto &colref = expression.Cast<BoundColumnRefExpression>();
451: 		D_ASSERT(colref.depth == 0);
452: 		D_ASSERT(colref.binding.table_index != DConstants::INVALID_INDEX);
453: 		// map the base table index to the relation index used by the JoinOrderOptimizer
454: 		if (expression.GetAlias() == "SUBQUERY" &&
455: 		    relation_mapping.find(colref.binding.table_index) == relation_mapping.end()) {
456: 			// most likely a BoundSubqueryExpression that was created from an uncorrelated subquery
457: 			// Here we return true and don't fill the bindings, the expression can be reordered.
458: 			// A filter will be created using this expression, and pushed back on top of the parent
459: 			// operator during plan reconstruction
460: 			return true;
461: 		}
462: 		if (relation_mapping.find(colref.binding.table_index) != relation_mapping.end()) {
463: 			bindings.insert(relation_mapping[colref.binding.table_index]);
464: 		}
465: 	}
466: 	if (expression.GetExpressionType() == ExpressionType::BOUND_REF) {
467: 		// bound expression
468: 		bindings.clear();
469: 		return false;
470: 	}
471: 	D_ASSERT(expression.GetExpressionType() != ExpressionType::SUBQUERY);
472: 	bool can_reorder = true;
473: 	ExpressionIterator::EnumerateChildren(expression, [&](Expression &expr) {
474: 		if (!ExtractBindings(expr, bindings)) {
475: 			can_reorder = false;
476: 			return;
477: 		}
478: 	});
479: 	return can_reorder;
480: }
481: 
482: vector<unique_ptr<FilterInfo>> RelationManager::ExtractEdges(LogicalOperator &op,
483:                                                              vector<reference<LogicalOperator>> &filter_operators,
484:                                                              JoinRelationSetManager &set_manager) {
485: 	// now that we know we are going to perform join ordering we actually extract the filters, eliminating duplicate
486: 	// filters in the process
487: 	vector<unique_ptr<FilterInfo>> filters_and_bindings;
488: 	expression_set_t filter_set;
489: 	for (auto &filter_op : filter_operators) {
490: 		auto &f_op = filter_op.get();
491: 		if (f_op.type == LogicalOperatorType::LOGICAL_COMPARISON_JOIN ||
492: 		    f_op.type == LogicalOperatorType::LOGICAL_ASOF_JOIN) {
493: 			auto &join = f_op.Cast<LogicalComparisonJoin>();
494: 			D_ASSERT(join.expressions.empty());
495: 			if (join.join_type == JoinType::SEMI || join.join_type == JoinType::ANTI) {
496: 
497: 				auto conjunction_expression = make_uniq<BoundConjunctionExpression>(ExpressionType::CONJUNCTION_AND);
498: 				// create a conjunction expression for the semi join.
499: 				// It's possible multiple LHS relations have a condition in
500: 				// this semi join. Suppose we have ((A ⨝ B) ⋉ C). (example in test_4950.test)
501: 				// If the semi join condition has A.x = C.y AND B.x = C.z then we need to prevent a reordering
502: 				// that looks like ((A ⋉ C) ⨝ B)), since all columns from C will be lost after it joins with A,
503: 				// and the condition B.x = C.z will no longer be possible.
504: 				// if we make a conjunction expressions and populate the left set and right set with all
505: 				// the relations from the conditions in the conjunction expression, we can prevent invalid
506: 				// reordering.
507: 				for (auto &cond : join.conditions) {
508: 					auto comparison = make_uniq<BoundComparisonExpression>(cond.comparison, std::move(cond.left),
509: 					                                                       std::move(cond.right));
510: 					conjunction_expression->children.push_back(std::move(comparison));
511: 				}
512: 
513: 				// create the filter info so all required LHS relations are present when reconstructing the
514: 				// join
515: 				optional_ptr<JoinRelationSet> left_set;
516: 				optional_ptr<JoinRelationSet> right_set;
517: 				optional_ptr<JoinRelationSet> full_set;
518: 				// here we create a left_set that unions all relations from the left side of
519: 				// every expression and a right_set that unions all relations frmo the right side of a
520: 				// every expression (although this should always be 1).
521: 				for (auto &bound_expr : conjunction_expression->children) {
522: 					D_ASSERT(bound_expr->GetExpressionClass() == ExpressionClass::BOUND_COMPARISON);
523: 					auto &comp = bound_expr->Cast<BoundComparisonExpression>();
524: 					unordered_set<idx_t> right_bindings, left_bindings;
525: 					ExtractBindings(*comp.right, right_bindings);
526: 					ExtractBindings(*comp.left, left_bindings);
527: 
528: 					if (!left_set) {
529: 						left_set = set_manager.GetJoinRelation(left_bindings);
530: 					} else {
531: 						left_set = set_manager.Union(set_manager.GetJoinRelation(left_bindings), *left_set);
532: 					}
533: 					if (!right_set) {
534: 						right_set = set_manager.GetJoinRelation(right_bindings);
535: 					} else {
536: 						right_set = set_manager.Union(set_manager.GetJoinRelation(right_bindings), *right_set);
537: 					}
538: 				}
539: 				full_set = set_manager.Union(*left_set, *right_set);
540: 				D_ASSERT(left_set && left_set->count > 0);
541: 				D_ASSERT(right_set && right_set->count == 1);
542: 				D_ASSERT(full_set && full_set->count > 0);
543: 
544: 				// now we push the conjunction expressions
545: 				// In QueryGraphManager::GenerateJoins we extract each condition again and create a standalone join
546: 				// condition.
547: 				auto filter_info = make_uniq<FilterInfo>(std::move(conjunction_expression), *full_set,
548: 				                                         filters_and_bindings.size(), join.join_type);
549: 				filter_info->SetLeftSet(left_set);
550: 				filter_info->SetRightSet(right_set);
551: 
552: 				filters_and_bindings.push_back(std::move(filter_info));
553: 			} else {
554: 				// can extract every inner join condition individually.
555: 				for (auto &cond : join.conditions) {
556: 					auto comparison = make_uniq<BoundComparisonExpression>(cond.comparison, std::move(cond.left),
557: 					                                                       std::move(cond.right));
558: 					if (filter_set.find(*comparison) == filter_set.end()) {
559: 						filter_set.insert(*comparison);
560: 						unordered_set<idx_t> bindings;
561: 						ExtractBindings(*comparison, bindings);
562: 						auto &set = set_manager.GetJoinRelation(bindings);
563: 						auto filter_info = make_uniq<FilterInfo>(std::move(comparison), set,
564: 						                                         filters_and_bindings.size(), join.join_type);
565: 						filters_and_bindings.push_back(std::move(filter_info));
566: 					}
567: 				}
568: 			}
569: 			join.conditions.clear();
570: 		} else {
571: 			vector<unique_ptr<Expression>> leftover_expressions;
572: 			for (auto &expression : f_op.expressions) {
573: 				if (filter_set.find(*expression) == filter_set.end()) {
574: 					filter_set.insert(*expression);
575: 					unordered_set<idx_t> bindings;
576: 					ExtractBindings(*expression, bindings);
577: 					if (bindings.empty()) {
578: 						// the filter is on a column that is not in our relational map. (example: limit_rownum)
579: 						// in this case we do not create a FilterInfo for it. (duckdb-internal/#1493)s
580: 						leftover_expressions.push_back(std::move(expression));
581: 						continue;
582: 					}
583: 					auto &set = set_manager.GetJoinRelation(bindings);
584: 					auto filter_info = make_uniq<FilterInfo>(std::move(expression), set, filters_and_bindings.size());
585: 					filters_and_bindings.push_back(std::move(filter_info));
586: 				}
587: 			}
588: 			f_op.expressions = std::move(leftover_expressions);
589: 		}
590: 	}
591: 
592: 	return filters_and_bindings;
593: }
594: 
595: // LCOV_EXCL_START
596: 
597: void RelationManager::PrintRelationStats() {
598: #ifdef DEBUG
599: 	string to_print;
600: 	for (idx_t i = 0; i < relations.size(); i++) {
601: 		auto &relation = relations.at(i);
602: 		auto &stats = relation->stats;
603: 		D_ASSERT(stats.column_names.size() == stats.column_distinct_count.size());
604: 		for (idx_t i = 0; i < stats.column_names.size(); i++) {
605: 			to_print = stats.column_names.at(i) + " has estimated distinct count " +
606: 			           to_string(stats.column_distinct_count.at(i).distinct_count);
607: 			Printer::Print(to_print);
608: 		}
609: 		to_print = stats.table_name + " has estimated cardinality " + to_string(stats.cardinality);
610: 		to_print += " and relation id " + to_string(i) + "\n";
611: 		Printer::Print(to_print);
612: 	}
613: #endif
614: }
615: 
616: // LCOV_EXCL_STOP
617: 
618: } // namespace duckdb
[end of src/optimizer/join_order/relation_manager.cpp]
[start of src/optimizer/join_order/relation_statistics_helper.cpp]
1: #include "duckdb/optimizer/join_order/relation_statistics_helper.hpp"
2: #include "duckdb/planner/expression/list.hpp"
3: #include "duckdb/planner/operator/list.hpp"
4: #include "duckdb/planner/filter/conjunction_filter.hpp"
5: #include "duckdb/planner/expression_iterator.hpp"
6: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
7: #include "duckdb/function/table/table_scan.hpp"
8: #include "duckdb/planner/operator/logical_get.hpp"
9: #include "duckdb/storage/data_table.hpp"
10: #include "duckdb/planner/filter/constant_filter.hpp"
11: 
12: namespace duckdb {
13: 
14: static ExpressionBinding GetChildColumnBinding(Expression &expr) {
15: 	auto ret = ExpressionBinding();
16: 	switch (expr.GetExpressionClass()) {
17: 	case ExpressionClass::BOUND_FUNCTION: {
18: 		// TODO: Other expression classes that can have 0 children?
19: 		auto &func = expr.Cast<BoundFunctionExpression>();
20: 		// no children some sort of gen_random_uuid() or equivalent.
21: 		if (func.children.empty()) {
22: 			ret.found_expression = true;
23: 			ret.expression_is_constant = true;
24: 			return ret;
25: 		}
26: 		break;
27: 	}
28: 	case ExpressionClass::BOUND_COLUMN_REF: {
29: 		ret.found_expression = true;
30: 		auto &new_col_ref = expr.Cast<BoundColumnRefExpression>();
31: 		ret.child_binding = ColumnBinding(new_col_ref.binding.table_index, new_col_ref.binding.column_index);
32: 		return ret;
33: 	}
34: 	case ExpressionClass::BOUND_LAMBDA_REF:
35: 	case ExpressionClass::BOUND_CONSTANT:
36: 	case ExpressionClass::BOUND_DEFAULT:
37: 	case ExpressionClass::BOUND_PARAMETER:
38: 	case ExpressionClass::BOUND_REF:
39: 		ret.found_expression = true;
40: 		ret.expression_is_constant = true;
41: 		return ret;
42: 	default:
43: 		break;
44: 	}
45: 	ExpressionIterator::EnumerateChildren(expr, [&](unique_ptr<Expression> &child) {
46: 		auto recursive_result = GetChildColumnBinding(*child);
47: 		if (recursive_result.found_expression) {
48: 			ret = recursive_result;
49: 		}
50: 	});
51: 	// we didn't find a Bound Column Ref
52: 	return ret;
53: }
54: 
55: RelationStats RelationStatisticsHelper::ExtractGetStats(LogicalGet &get, ClientContext &context) {
56: 	auto return_stats = RelationStats();
57: 
58: 	auto base_table_cardinality = get.EstimateCardinality(context);
59: 	auto cardinality_after_filters = base_table_cardinality;
60: 	unique_ptr<BaseStatistics> column_statistics;
61: 
62: 	auto catalog_table = get.GetTable();
63: 	auto name = string("some table");
64: 	if (catalog_table) {
65: 		name = catalog_table->name;
66: 		return_stats.table_name = name;
67: 	}
68: 
69: 	// if we can get the catalog table, then our column statistics will be accurate
70: 	// parquet readers etc. will still return statistics, but they initialize distinct column
71: 	// counts to 0.
72: 	// TODO: fix this, some file formats can encode distinct counts, we don't want to rely on
73: 	//  getting a catalog table to know that we can use statistics.
74: 	bool have_catalog_table_statistics = false;
75: 	if (get.GetTable()) {
76: 		have_catalog_table_statistics = true;
77: 	}
78: 
79: 	// first push back basic distinct counts for each column (if we have them).
80: 	auto &column_ids = get.GetColumnIds();
81: 	for (idx_t i = 0; i < column_ids.size(); i++) {
82: 		auto column_id = column_ids[i].GetPrimaryIndex();
83: 		bool have_distinct_count_stats = false;
84: 		if (get.function.statistics) {
85: 			column_statistics = get.function.statistics(context, get.bind_data.get(), column_id);
86: 			if (column_statistics && have_catalog_table_statistics) {
87: 				auto distinct_count = MaxValue<idx_t>(1, column_statistics->GetDistinctCount());
88: 				auto column_distinct_count = DistinctCount({distinct_count, true});
89: 				return_stats.column_distinct_count.push_back(column_distinct_count);
90: 				return_stats.column_names.push_back(name + "." + get.names.at(column_id));
91: 				have_distinct_count_stats = true;
92: 			}
93: 		}
94: 		if (!have_distinct_count_stats) {
95: 			// currently treating the cardinality as the distinct count.
96: 			// the cardinality estimator will update these distinct counts based
97: 			// on the extra columns that are joined on.
98: 			auto column_distinct_count = DistinctCount({cardinality_after_filters, false});
99: 			return_stats.column_distinct_count.push_back(column_distinct_count);
100: 			auto column_name = string("column");
101: 			if (column_id < get.names.size()) {
102: 				column_name = get.names.at(column_id);
103: 			}
104: 			return_stats.column_names.push_back(get.GetName() + "." + column_name);
105: 		}
106: 	}
107: 
108: 	if (!get.table_filters.filters.empty()) {
109: 		column_statistics = nullptr;
110: 		bool has_non_optional_filters = false;
111: 		for (auto &it : get.table_filters.filters) {
112: 			if (get.bind_data && get.function.statistics) {
113: 				column_statistics = get.function.statistics(context, get.bind_data.get(), it.first);
114: 			}
115: 
116: 			if (column_statistics) {
117: 				idx_t cardinality_with_filter =
118: 				    InspectTableFilter(base_table_cardinality, it.first, *it.second, *column_statistics);
119: 				cardinality_after_filters = MinValue(cardinality_after_filters, cardinality_with_filter);
120: 			}
121: 
122: 			if (it.second->filter_type != TableFilterType::OPTIONAL_FILTER) {
123: 				has_non_optional_filters = true;
124: 			}
125: 		}
126: 		// if the above code didn't find an equality filter (i.e country_code = "[us]")
127: 		// and there are other table filters (i.e cost > 50), use default selectivity.
128: 		bool has_equality_filter = (cardinality_after_filters != base_table_cardinality);
129: 		if (!has_equality_filter && has_non_optional_filters) {
130: 			cardinality_after_filters = MaxValue<idx_t>(
131: 			    LossyNumericCast<idx_t>(double(base_table_cardinality) * RelationStatisticsHelper::DEFAULT_SELECTIVITY),
132: 			    1U);
133: 		}
134: 		if (base_table_cardinality == 0) {
135: 			cardinality_after_filters = 0;
136: 		}
137: 	}
138: 	return_stats.cardinality = cardinality_after_filters;
139: 	// update the estimated cardinality of the get as well.
140: 	// This is not updated during plan reconstruction.
141: 	get.estimated_cardinality = cardinality_after_filters;
142: 	get.has_estimated_cardinality = true;
143: 	D_ASSERT(base_table_cardinality >= cardinality_after_filters);
144: 	return_stats.stats_initialized = true;
145: 	return return_stats;
146: }
147: 
148: RelationStats RelationStatisticsHelper::ExtractDelimGetStats(LogicalDelimGet &delim_get, ClientContext &context) {
149: 	RelationStats stats;
150: 	stats.table_name = delim_get.GetName();
151: 	idx_t card = delim_get.EstimateCardinality(context);
152: 	stats.cardinality = card;
153: 	stats.stats_initialized = true;
154: 	for (auto &binding : delim_get.GetColumnBindings()) {
155: 		stats.column_distinct_count.push_back(DistinctCount({1, false}));
156: 		stats.column_names.push_back("column" + to_string(binding.column_index));
157: 	}
158: 	return stats;
159: }
160: 
161: RelationStats RelationStatisticsHelper::ExtractProjectionStats(LogicalProjection &proj, RelationStats &child_stats) {
162: 	auto proj_stats = RelationStats();
163: 	proj_stats.cardinality = child_stats.cardinality;
164: 	proj_stats.table_name = proj.GetName();
165: 	for (auto &expr : proj.expressions) {
166: 		proj_stats.column_names.push_back(expr->GetName());
167: 		auto res = GetChildColumnBinding(*expr);
168: 		D_ASSERT(res.found_expression);
169: 		if (res.expression_is_constant) {
170: 			proj_stats.column_distinct_count.push_back(DistinctCount({1, true}));
171: 		} else {
172: 			auto column_index = res.child_binding.column_index;
173: 			if (column_index >= child_stats.column_distinct_count.size() && expr->ToString() == "count_star()") {
174: 				// only one value for a count star
175: 				proj_stats.column_distinct_count.push_back(DistinctCount({1, true}));
176: 			} else {
177: 				// TODO: add this back in
178: 				//	D_ASSERT(column_index < stats.column_distinct_count.size());
179: 				if (column_index < child_stats.column_distinct_count.size()) {
180: 					proj_stats.column_distinct_count.push_back(child_stats.column_distinct_count.at(column_index));
181: 				} else {
182: 					proj_stats.column_distinct_count.push_back(DistinctCount({proj_stats.cardinality, false}));
183: 				}
184: 			}
185: 		}
186: 	}
187: 	proj_stats.stats_initialized = true;
188: 	return proj_stats;
189: }
190: 
191: RelationStats RelationStatisticsHelper::ExtractDummyScanStats(LogicalDummyScan &dummy_scan, ClientContext &context) {
192: 	auto stats = RelationStats();
193: 	idx_t card = dummy_scan.EstimateCardinality(context);
194: 	stats.cardinality = card;
195: 	for (idx_t i = 0; i < dummy_scan.GetColumnBindings().size(); i++) {
196: 		stats.column_distinct_count.push_back(DistinctCount({card, false}));
197: 		stats.column_names.push_back("dummy_scan_column");
198: 	}
199: 	stats.stats_initialized = true;
200: 	stats.table_name = "dummy scan";
201: 	return stats;
202: }
203: 
204: void RelationStatisticsHelper::CopyRelationStats(RelationStats &to, const RelationStats &from) {
205: 	to.column_distinct_count = from.column_distinct_count;
206: 	to.column_names = from.column_names;
207: 	to.cardinality = from.cardinality;
208: 	to.table_name = from.table_name;
209: 	to.stats_initialized = from.stats_initialized;
210: }
211: 
212: RelationStats RelationStatisticsHelper::CombineStatsOfReorderableOperator(vector<ColumnBinding> &bindings,
213:                                                                           vector<RelationStats> relation_stats) {
214: 	RelationStats stats;
215: 	idx_t max_card = 0;
216: 	for (auto &child_stats : relation_stats) {
217: 		for (idx_t i = 0; i < child_stats.column_distinct_count.size(); i++) {
218: 			stats.column_distinct_count.push_back(child_stats.column_distinct_count.at(i));
219: 			stats.column_names.push_back(child_stats.column_names.at(i));
220: 		}
221: 		stats.table_name += "joined with " + child_stats.table_name;
222: 		max_card = MaxValue(max_card, child_stats.cardinality);
223: 	}
224: 	stats.stats_initialized = true;
225: 	stats.cardinality = max_card;
226: 	return stats;
227: }
228: 
229: RelationStats RelationStatisticsHelper::CombineStatsOfNonReorderableOperator(LogicalOperator &op,
230:                                                                              vector<RelationStats> child_stats) {
231: 	D_ASSERT(child_stats.size() == 2);
232: 	RelationStats ret;
233: 	idx_t child_1_card = child_stats[0].stats_initialized ? child_stats[0].cardinality : 0;
234: 	idx_t child_2_card = child_stats[1].stats_initialized ? child_stats[1].cardinality : 0;
235: 	ret.cardinality = MaxValue(child_1_card, child_2_card);
236: 	switch (op.type) {
237: 	case LogicalOperatorType::LOGICAL_COMPARISON_JOIN: {
238: 		auto &join = op.Cast<LogicalComparisonJoin>();
239: 		switch (join.join_type) {
240: 		case JoinType::RIGHT_ANTI:
241: 		case JoinType::RIGHT_SEMI:
242: 			ret.cardinality = child_2_card;
243: 			break;
244: 		case JoinType::ANTI:
245: 		case JoinType::SEMI:
246: 		case JoinType::SINGLE:
247: 		case JoinType::MARK:
248: 			ret.cardinality = child_1_card;
249: 			break;
250: 		default:
251: 			break;
252: 		}
253: 		break;
254: 	}
255: 	case LogicalOperatorType::LOGICAL_UNION: {
256: 		auto &setop = op.Cast<LogicalSetOperation>();
257: 		if (setop.setop_all) {
258: 			// setop returns all records
259: 			ret.cardinality = child_1_card + child_2_card;
260: 		} else {
261: 			ret.cardinality = MaxValue(child_1_card, child_2_card);
262: 		}
263: 		break;
264: 	}
265: 	case LogicalOperatorType::LOGICAL_INTERSECT: {
266: 		ret.cardinality = MinValue(child_1_card, child_2_card);
267: 		break;
268: 	}
269: 	case LogicalOperatorType::LOGICAL_EXCEPT: {
270: 		ret.cardinality = child_1_card;
271: 		break;
272: 	}
273: 	default:
274: 		break;
275: 	}
276: 
277: 	ret.stats_initialized = true;
278: 	ret.filter_strength = 1;
279: 	ret.table_name = child_stats[0].table_name + " joined with " + child_stats[1].table_name;
280: 	for (auto &stats : child_stats) {
281: 		// MARK joins are nonreorderable. They won't return initialized stats
282: 		// continue in this case.
283: 		if (!stats.stats_initialized) {
284: 			continue;
285: 		}
286: 		for (auto &distinct_count : stats.column_distinct_count) {
287: 			ret.column_distinct_count.push_back(distinct_count);
288: 		}
289: 		for (auto &column_name : stats.column_names) {
290: 			ret.column_names.push_back(column_name);
291: 		}
292: 	}
293: 	return ret;
294: }
295: 
296: RelationStats RelationStatisticsHelper::ExtractExpressionGetStats(LogicalExpressionGet &expression_get,
297:                                                                   ClientContext &context) {
298: 	auto stats = RelationStats();
299: 	idx_t card = expression_get.EstimateCardinality(context);
300: 	stats.cardinality = card;
301: 	for (idx_t i = 0; i < expression_get.GetColumnBindings().size(); i++) {
302: 		stats.column_distinct_count.push_back(DistinctCount({card, false}));
303: 		stats.column_names.push_back("expression_get_column");
304: 	}
305: 	stats.stats_initialized = true;
306: 	stats.table_name = "expression_get";
307: 	return stats;
308: }
309: 
310: RelationStats RelationStatisticsHelper::ExtractWindowStats(LogicalWindow &window, RelationStats &child_stats) {
311: 	RelationStats stats;
312: 	stats.cardinality = child_stats.cardinality;
313: 	stats.column_distinct_count = child_stats.column_distinct_count;
314: 	stats.column_names = child_stats.column_names;
315: 	stats.stats_initialized = true;
316: 	auto num_child_columns = window.GetColumnBindings().size();
317: 
318: 	for (idx_t column_index = child_stats.column_distinct_count.size(); column_index < num_child_columns;
319: 	     column_index++) {
320: 		stats.column_distinct_count.push_back(DistinctCount({child_stats.cardinality, false}));
321: 		stats.column_names.push_back("window");
322: 	}
323: 	return stats;
324: }
325: 
326: RelationStats RelationStatisticsHelper::ExtractAggregationStats(LogicalAggregate &aggr, RelationStats &child_stats) {
327: 	RelationStats stats;
328: 	// TODO: look at child distinct count to better estimate cardinality.
329: 	stats.cardinality = child_stats.cardinality;
330: 	stats.column_distinct_count = child_stats.column_distinct_count;
331: 	double new_card = -1;
332: 	for (auto &g_set : aggr.grouping_sets) {
333: 		for (auto &ind : g_set) {
334: 			if (aggr.groups[ind]->GetExpressionClass() != ExpressionClass::BOUND_COLUMN_REF) {
335: 				continue;
336: 			}
337: 			auto bound_col = &aggr.groups[ind]->Cast<BoundColumnRefExpression>();
338: 			auto col_index = bound_col->binding.column_index;
339: 			if (col_index >= child_stats.column_distinct_count.size()) {
340: 				// it is possible the column index of the grouping_set is not in the child stats.
341: 				// this can happen when delim joins are present, since delim scans are not currently
342: 				// reorderable. Meaning they don't add a relation or column_ids that could potentially
343: 				// be grouped by. Hopefully this can be fixed with duckdb-internal#606
344: 				continue;
345: 			}
346: 			double distinct_count = double(child_stats.column_distinct_count[col_index].distinct_count);
347: 			if (new_card < distinct_count) {
348: 				new_card = distinct_count;
349: 			}
350: 		}
351: 	}
352: 	if (new_card < 0 || new_card >= double(child_stats.cardinality)) {
353: 		// We have no good statistics on distinct count.
354: 		// most likely we are running on parquet files. Therefore we divide by 2.
355: 		new_card = (double)child_stats.cardinality / 2;
356: 	}
357: 	// an ungrouped aggregate has 1 row
358: 	stats.cardinality = aggr.groups.empty() ? 1 : LossyNumericCast<idx_t>(new_card);
359: 	stats.column_names = child_stats.column_names;
360: 	stats.stats_initialized = true;
361: 	auto num_child_columns = aggr.GetColumnBindings().size();
362: 
363: 	for (idx_t column_index = child_stats.column_distinct_count.size(); column_index < num_child_columns;
364: 	     column_index++) {
365: 		stats.column_distinct_count.push_back(DistinctCount({child_stats.cardinality, false}));
366: 		stats.column_names.push_back("aggregate");
367: 	}
368: 	return stats;
369: }
370: 
371: RelationStats RelationStatisticsHelper::ExtractEmptyResultStats(LogicalEmptyResult &empty) {
372: 	RelationStats stats;
373: 	for (idx_t i = 0; i < empty.GetColumnBindings().size(); i++) {
374: 		stats.column_distinct_count.push_back(DistinctCount({0, false}));
375: 		stats.column_names.push_back("empty_result_column");
376: 	}
377: 	stats.stats_initialized = true;
378: 	return stats;
379: }
380: 
381: idx_t RelationStatisticsHelper::InspectTableFilter(idx_t cardinality, idx_t column_index, TableFilter &filter,
382:                                                    BaseStatistics &base_stats) {
383: 	auto cardinality_after_filters = cardinality;
384: 	switch (filter.filter_type) {
385: 	case TableFilterType::CONJUNCTION_AND: {
386: 		auto &and_filter = filter.Cast<ConjunctionAndFilter>();
387: 		for (auto &child_filter : and_filter.child_filters) {
388: 			cardinality_after_filters = MinValue(
389: 			    cardinality_after_filters, InspectTableFilter(cardinality, column_index, *child_filter, base_stats));
390: 		}
391: 		return cardinality_after_filters;
392: 	}
393: 	case TableFilterType::CONSTANT_COMPARISON: {
394: 		auto &comparison_filter = filter.Cast<ConstantFilter>();
395: 		if (comparison_filter.comparison_type != ExpressionType::COMPARE_EQUAL) {
396: 			return cardinality_after_filters;
397: 		}
398: 		auto column_count = base_stats.GetDistinctCount();
399: 		// column_count = 0 when there is no column count (i.e parquet scans)
400: 		if (column_count > 0) {
401: 			// we want the ceil of cardinality/column_count. We also want to avoid compiler errors
402: 			cardinality_after_filters = (cardinality + column_count - 1) / column_count;
403: 		}
404: 		return cardinality_after_filters;
405: 	}
406: 	default:
407: 		return cardinality_after_filters;
408: 	}
409: }
410: 
411: // TODO: Currently only simple AND filters are pushed into table scans.
412: //  When OR filters are pushed this function can be added
413: // idx_t RelationStatisticsHelper::InspectConjunctionOR(idx_t cardinality, idx_t column_index, ConjunctionOrFilter
414: // &filter,
415: //                                                     BaseStatistics &base_stats) {
416: //	auto has_equality_filter = false;
417: //	auto cardinality_after_filters = cardinality;
418: //	for (auto &child_filter : filter.child_filters) {
419: //		if (child_filter->filter_type != TableFilterType::CONSTANT_COMPARISON) {
420: //			continue;
421: //		}
422: //		auto &comparison_filter = child_filter->Cast<ConstantFilter>();
423: //		if (comparison_filter.comparison_type == ExpressionType::COMPARE_EQUAL) {
424: //			auto column_count = base_stats.GetDistinctCount();
425: //			auto increment = MaxValue<idx_t>(((cardinality + column_count - 1) / column_count), 1);
426: //			if (has_equality_filter) {
427: //				cardinality_after_filters += increment;
428: //			} else {
429: //				cardinality_after_filters = increment;
430: //			}
431: //			has_equality_filter = true;
432: //		}
433: //		if (child_filter->filter_type == TableFilterType::CONJUNCTION_AND) {
434: //			auto &and_filter = child_filter->Cast<ConjunctionAndFilter>();
435: //			cardinality_after_filters = RelationStatisticsHelper::InspectConjunctionAND(
436: //			    cardinality_after_filters, column_index, and_filter, base_stats);
437: //			continue;
438: //		}
439: //	}
440: //	D_ASSERT(cardinality_after_filters > 0);
441: //	return cardinality_after_filters;
442: //}
443: 
444: } // namespace duckdb
[end of src/optimizer/join_order/relation_statistics_helper.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: