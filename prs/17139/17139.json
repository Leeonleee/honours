{
  "repo": "duckdb/duckdb",
  "pull_number": 17139,
  "instance_id": "duckdb__duckdb-17139",
  "issue_numbers": [
    "16213",
    "16213"
  ],
  "base_commit": "88c4366deacd3c98b6f3c54ebe3306552a374877",
  "patch": "diff --git a/src/execution/physical_plan/plan_delim_join.cpp b/src/execution/physical_plan/plan_delim_join.cpp\nindex 27a9de5e65b6..02424b0b125d 100644\n--- a/src/execution/physical_plan/plan_delim_join.cpp\n+++ b/src/execution/physical_plan/plan_delim_join.cpp\n@@ -49,8 +49,9 @@ PhysicalOperator &PhysicalPlanGenerator::PlanDelimJoin(LogicalComparisonJoin &op\n \t}\n \n \t// we still have to create the DISTINCT clause that is used to generate the duplicate eliminated chunk\n-\tauto &distinct = Make<PhysicalHashAggregate>(context, delim_types, std::move(distinct_expressions),\n-\t                                             std::move(distinct_groups), op.estimated_cardinality);\n+\tauto &distinct =\n+\t    Make<PhysicalHashAggregate>(context, delim_types, std::move(distinct_expressions), std::move(distinct_groups),\n+\t                                delim_scans[0].get().estimated_cardinality);\n \n \t// Create the duplicate eliminated join.\n \tif (op.delim_flipped) {\ndiff --git a/src/include/duckdb/optimizer/join_order/cardinality_estimator.hpp b/src/include/duckdb/optimizer/join_order/cardinality_estimator.hpp\nindex 8aec1cd02c46..5390357102b2 100644\n--- a/src/include/duckdb/optimizer/join_order/cardinality_estimator.hpp\n+++ b/src/include/duckdb/optimizer/join_order/cardinality_estimator.hpp\n@@ -88,7 +88,6 @@ class CardinalityHelper {\n class CardinalityEstimator {\n public:\n \tstatic constexpr double DEFAULT_SEMI_ANTI_SELECTIVITY = 5;\n-\tstatic constexpr double DEFAULT_LT_GT_MULTIPLIER = 2.5;\n \texplicit CardinalityEstimator() {};\n \n private:\ndiff --git a/src/optimizer/join_order/cardinality_estimator.cpp b/src/optimizer/join_order/cardinality_estimator.cpp\nindex 4b5e22adad1e..2539bf37ac39 100644\n--- a/src/optimizer/join_order/cardinality_estimator.cpp\n+++ b/src/optimizer/join_order/cardinality_estimator.cpp\n@@ -9,6 +9,8 @@\n #include \"duckdb/planner/operator/logical_comparison_join.hpp\"\n #include \"duckdb/storage/data_table.hpp\"\n \n+#include <math.h>\n+\n namespace duckdb {\n \n // The filter was made on top of a logical sample or other projection,\n@@ -216,16 +218,14 @@ double CardinalityEstimator::CalculateUpdatedDenom(Subgraph2Denominator left, Su\n \tdouble new_denom = left.denom * right.denom;\n \tswitch (filter.filter_info->join_type) {\n \tcase JoinType::INNER: {\n-\t\tbool set = false;\n-\t\tExpressionType comparison_type = ExpressionType::COMPARE_EQUAL;\n+\t\t// Collect comparison types\n+\t\tExpressionType comparison_type = ExpressionType::INVALID;\n \t\tExpressionIterator::EnumerateExpression(filter.filter_info->filter, [&](Expression &expr) {\n \t\t\tif (expr.GetExpressionClass() == ExpressionClass::BOUND_COMPARISON) {\n \t\t\t\tcomparison_type = expr.GetExpressionType();\n-\t\t\t\tset = true;\n-\t\t\t\treturn;\n \t\t\t}\n \t\t});\n-\t\tif (!set) {\n+\t\tif (comparison_type == ExpressionType::INVALID) {\n \t\t\tnew_denom *=\n \t\t\t    filter.has_tdom_hll ? static_cast<double>(filter.tdom_hll) : static_cast<double>(filter.tdom_no_hll);\n \t\t\t// no comparison is taking place, so the denominator is just the product of the left and right\n@@ -237,22 +237,20 @@ double CardinalityEstimator::CalculateUpdatedDenom(Subgraph2Denominator left, Su\n \t\tswitch (comparison_type) {\n \t\tcase ExpressionType::COMPARE_EQUAL:\n \t\tcase ExpressionType::COMPARE_NOT_DISTINCT_FROM:\n-\t\t\t// extra ration stays 1\n-\t\t\textra_ratio = filter.has_tdom_hll ? (double)filter.tdom_hll : (double)filter.tdom_no_hll;\n+\t\t\t// extra ratio stays 1\n+\t\t\textra_ratio =\n+\t\t\t    filter.has_tdom_hll ? static_cast<double>(filter.tdom_hll) : static_cast<double>(filter.tdom_no_hll);\n \t\t\tbreak;\n \t\tcase ExpressionType::COMPARE_LESSTHANOREQUALTO:\n \t\tcase ExpressionType::COMPARE_LESSTHAN:\n \t\tcase ExpressionType::COMPARE_GREATERTHANOREQUALTO:\n \t\tcase ExpressionType::COMPARE_GREATERTHAN:\n-\t\t\t// start with the selectivity of equality\n-\t\t\textra_ratio = filter.has_tdom_hll ? (double)filter.tdom_hll : (double)filter.tdom_no_hll;\n-\t\t\t// now assume every tuple will match 2.5 times (on average)\n-\t\t\textra_ratio *= static_cast<double>(1) / CardinalityEstimator::DEFAULT_LT_GT_MULTIPLIER;\n-\t\t\tbreak;\n \t\tcase ExpressionType::COMPARE_NOTEQUAL:\n \t\tcase ExpressionType::COMPARE_DISTINCT_FROM:\n-\t\t\t// basically assume cross product.\n-\t\t\textra_ratio = 1;\n+\t\t\t// Assume this blows up, but use the tdom to bound it a bit\n+\t\t\textra_ratio =\n+\t\t\t    filter.has_tdom_hll ? static_cast<double>(filter.tdom_hll) : static_cast<double>(filter.tdom_no_hll);\n+\t\t\textra_ratio = pow(extra_ratio, 2.0 / 3.0);\n \t\t\tbreak;\n \t\tdefault:\n \t\t\tbreak;\ndiff --git a/src/optimizer/join_order/relation_manager.cpp b/src/optimizer/join_order/relation_manager.cpp\nindex d4f7032d676d..cfc829da7808 100644\n--- a/src/optimizer/join_order/relation_manager.cpp\n+++ b/src/optimizer/join_order/relation_manager.cpp\n@@ -418,7 +418,9 @@ bool RelationManager::ExtractJoinRelations(JoinOrderOptimizer &optimizer, Logica\n \t\t// create dummy aggregation for the duplicate elimination\n \t\tauto dummy_aggr = make_uniq<LogicalAggregate>(DConstants::INVALID_INDEX - 1, DConstants::INVALID_INDEX,\n \t\t                                              vector<unique_ptr<Expression>>());\n+\t\tdummy_aggr->grouping_sets.emplace_back();\n \t\tfor (auto &delim_col : delim_join.duplicate_eliminated_columns) {\n+\t\t\tdummy_aggr->grouping_sets.back().insert(dummy_aggr->groups.size());\n \t\t\tdummy_aggr->groups.push_back(delim_col->Copy());\n \t\t}\n \t\tauto lhs_delim_stats = RelationStatisticsHelper::ExtractAggregationStats(*dummy_aggr, lhs_stats);\n@@ -429,6 +431,36 @@ bool RelationManager::ExtractJoinRelations(JoinOrderOptimizer &optimizer, Logica\n \t\trhs_optimizer.AddDelimScanStats(lhs_delim_stats);\n \t\top->children[1] = rhs_optimizer.Optimize(std::move(op->children[1]), rhs_stats);\n \n+\t\tRelationStats dj_stats;\n+\t\tswitch (delim_join.join_type) {\n+\t\tcase JoinType::LEFT:\n+\t\tcase JoinType::INNER:\n+\t\tcase JoinType::OUTER:\n+\t\tcase JoinType::SINGLE:\n+\t\tcase JoinType::MARK:\n+\t\tcase JoinType::SEMI:\n+\t\tcase JoinType::ANTI:\n+\t\t\tdj_stats = lhs_stats;\n+\t\t\tbreak;\n+\t\tcase JoinType::RIGHT:\n+\t\tcase JoinType::RIGHT_SEMI:\n+\t\tcase JoinType::RIGHT_ANTI:\n+\t\t\tdj_stats = rhs_stats;\n+\t\t\tbreak;\n+\t\tdefault:\n+\t\t\tthrow NotImplementedException(\"Unsupported join type\");\n+\t\t}\n+\n+\t\tif (delim_join.join_type == JoinType::SEMI || delim_join.join_type == JoinType::ANTI ||\n+\t\t    delim_join.join_type == JoinType::RIGHT_SEMI || delim_join.join_type == JoinType::RIGHT_ANTI) {\n+\t\t\tdj_stats.cardinality =\n+\t\t\t    MaxValue<idx_t>(LossyNumericCast<idx_t>(static_cast<double>(dj_stats.cardinality) /\n+\t\t\t                                            CardinalityEstimator::DEFAULT_SEMI_ANTI_SELECTIVITY),\n+\t\t\t                    1);\n+\t\t}\n+\n+\t\tAddAggregateOrWindowRelation(input_op, parent, dj_stats, op->type);\n+\n \t\treturn false;\n \t}\n \tcase LogicalOperatorType::LOGICAL_DELIM_GET: {\ndiff --git a/src/optimizer/join_order/relation_statistics_helper.cpp b/src/optimizer/join_order/relation_statistics_helper.cpp\nindex 6340d9c02cd8..33edb135e7d0 100644\n--- a/src/optimizer/join_order/relation_statistics_helper.cpp\n+++ b/src/optimizer/join_order/relation_statistics_helper.cpp\n@@ -9,6 +9,8 @@\n #include \"duckdb/storage/data_table.hpp\"\n #include \"duckdb/planner/filter/constant_filter.hpp\"\n \n+#include <math.h>\n+\n namespace duckdb {\n \n static ExpressionBinding GetChildColumnBinding(Expression &expr) {\n@@ -328,8 +330,9 @@ RelationStats RelationStatisticsHelper::ExtractAggregationStats(LogicalAggregate\n \t// TODO: look at child distinct count to better estimate cardinality.\n \tstats.cardinality = child_stats.cardinality;\n \tstats.column_distinct_count = child_stats.column_distinct_count;\n-\tdouble new_card = -1;\n+\tvector<double> distinct_counts;\n \tfor (auto &g_set : aggr.grouping_sets) {\n+\t\tvector<double> set_distinct_counts;\n \t\tfor (auto &ind : g_set) {\n \t\t\tif (aggr.groups[ind]->GetExpressionClass() != ExpressionClass::BOUND_COLUMN_REF) {\n \t\t\t\tcontinue;\n@@ -343,26 +346,59 @@ RelationStats RelationStatisticsHelper::ExtractAggregationStats(LogicalAggregate\n \t\t\t\t// be grouped by. Hopefully this can be fixed with duckdb-internal#606\n \t\t\t\tcontinue;\n \t\t\t}\n-\t\t\tdouble distinct_count = double(child_stats.column_distinct_count[col_index].distinct_count);\n-\t\t\tif (new_card < distinct_count) {\n-\t\t\t\tnew_card = distinct_count;\n-\t\t\t}\n+\t\t\tdouble distinct_count = static_cast<double>(child_stats.column_distinct_count[col_index].distinct_count);\n+\t\t\tset_distinct_counts.push_back(distinct_count == 0 ? 1 : distinct_count);\n+\t\t}\n+\t\t// We use the grouping set with the most group key columns for cardinality estimation\n+\t\tif (set_distinct_counts.size() > distinct_counts.size()) {\n+\t\t\tdistinct_counts = std::move(set_distinct_counts);\n \t\t}\n \t}\n-\tif (new_card < 0 || new_card >= double(child_stats.cardinality)) {\n+\n+\tdouble new_card;\n+\tif (distinct_counts.empty()) {\n \t\t// We have no good statistics on distinct count.\n \t\t// most likely we are running on parquet files. Therefore we divide by 2.\n-\t\tnew_card = (double)child_stats.cardinality / 2;\n+\t\tnew_card = static_cast<double>(child_stats.cardinality) / 2.0;\n+\t} else {\n+\t\t// Multiply distinct counts\n+\t\tdouble product = 1;\n+\t\tfor (const auto &distinct_count : distinct_counts) {\n+\t\t\tproduct *= distinct_count;\n+\t\t}\n+\n+\t\t// Assume slight correlation for each grouping column\n+\t\tconst auto correction = pow(0.95, static_cast<double>(distinct_counts.size() - 1));\n+\t\tproduct *= correction;\n+\n+\t\t// Estimate using the \"Occupancy Problem\",\n+\t\t// where \"product\" is number of bins, and \"child_stats.cardinality\" is number of balls\n+\t\tconst auto mult = 1.0 - exp(-static_cast<double>(child_stats.cardinality) / product);\n+\t\tif (mult == 0) { // Can become 0 with very large estimates due to double imprecision\n+\t\t\tnew_card = static_cast<double>(child_stats.cardinality);\n+\t\t} else {\n+\t\t\tnew_card = product * mult;\n+\t\t}\n+\t\tnew_card = MinValue(new_card, static_cast<double>(child_stats.cardinality));\n \t}\n+\n \t// an ungrouped aggregate has 1 row\n \tstats.cardinality = aggr.groups.empty() ? 1 : LossyNumericCast<idx_t>(new_card);\n \tstats.column_names = child_stats.column_names;\n \tstats.stats_initialized = true;\n-\tauto num_child_columns = aggr.GetColumnBindings().size();\n+\tconst auto aggr_column_bindings = aggr.GetColumnBindings();\n+\tauto num_child_columns = aggr_column_bindings.size();\n \n-\tfor (idx_t column_index = child_stats.column_distinct_count.size(); column_index < num_child_columns;\n-\t     column_index++) {\n-\t\tstats.column_distinct_count.push_back(DistinctCount({child_stats.cardinality, false}));\n+\tfor (idx_t column_index = 0; column_index < num_child_columns; column_index++) {\n+\t\tconst auto &binding = aggr_column_bindings[column_index];\n+\t\tif (binding.table_index == aggr.group_index && column_index < distinct_counts.size()) {\n+\t\t\t// Group column that we have the HLL of\n+\t\t\tstats.column_distinct_count.push_back(\n+\t\t\t    DistinctCount({LossyNumericCast<idx_t>(distinct_counts[column_index]), true}));\n+\t\t} else {\n+\t\t\t// Non-group column, or we don't have the HLL\n+\t\t\tstats.column_distinct_count.push_back(DistinctCount({child_stats.cardinality, false}));\n+\t\t}\n \t\tstats.column_names.push_back(\"aggregate\");\n \t}\n \treturn stats;\n",
  "test_patch": "diff --git a/test/api/adbc/test_adbc.cpp b/test/api/adbc/test_adbc.cpp\nindex eb45ccd4d623..7eef329a9dd4 100644\n--- a/test/api/adbc/test_adbc.cpp\n+++ b/test/api/adbc/test_adbc.cpp\n@@ -1433,12 +1433,13 @@ TEST_CASE(\"Test AdbcConnectionGetObjects\", \"[adbc]\") {\n \t\tAdbcConnectionGetObjects(&db.adbc_connection, ADBC_OBJECT_DEPTH_COLUMNS, nullptr, nullptr, \"bla\", nullptr,\n \t\t                         nullptr, &arrow_stream, &adbc_error);\n \t\tdb.CreateTable(\"result\", arrow_stream);\n-\t\tres = db.Query(\"Select * from result order by catalog_name asc\");\n+\t\tres = db.Query(\"Select catalog_name, list_sort(catalog_db_schemas) as catalog_db_schemas from result order by \"\n+\t\t               \"catalog_name asc\");\n \t\tREQUIRE((res->ColumnCount() == 2));\n \t\tREQUIRE((res->RowCount() == 3));\n \t\tREQUIRE((res->GetValue(1, 0).ToString() ==\n-\t\t         \"[{'db_schema_name': pg_catalog, 'db_schema_tables': NULL}, {'db_schema_name': information_schema, \"\n-\t\t         \"'db_schema_tables': NULL}, {'db_schema_name': main, 'db_schema_tables': NULL}]\"));\n+\t\t         \"[{'db_schema_name': information_schema, 'db_schema_tables': NULL}, {'db_schema_name': main, \"\n+\t\t         \"'db_schema_tables': NULL}, {'db_schema_name': pg_catalog, 'db_schema_tables': NULL}]\"));\n \t\tdb.Query(\"Drop table result;\");\n \n \t\tAdbcConnectionGetObjects(&db.adbc_connection, ADBC_OBJECT_DEPTH_COLUMNS, nullptr, nullptr, nullptr, nullptr,\ndiff --git a/test/issues/general/test_16213.test_slow b/test/issues/general/test_16213.test_slow\nnew file mode 100644\nindex 000000000000..447123fbb332\n--- /dev/null\n+++ b/test/issues/general/test_16213.test_slow\n@@ -0,0 +1,56 @@\n+# name: test/issues/general/test_16213.test_slow\n+# description: Issue 16213 - Specific query not finishing since v1.1.0 and filling up all temp disk space\n+# group: [general]\n+\n+require icu\n+\n+# replicate date generation in issue, but in SQL\n+statement ok\n+create table records as\n+\tselect\n+\t\trange id,\n+\t\tto_timestamp(1514764800 + range / 1_000_000 * (1704067200 - 1514764800)) as creation_dt,\n+\t\tcreation_dt::date as creation_day,\n+\t\tprintf('%02X', range % 200) category,\n+\tfrom range(1_000_000);\n+\n+statement ok\n+create table labels as\n+\tselect\n+\t\tid,\n+\t\tcreation_dt + (1 * 60 * 60 + random() * (125 * 24 * 60 * 60 - 1 * 60 * 60) || ' seconds')::interval as label_dt,\n+\t\t1::bigint as label,\n+\tfrom (\n+\t\tfrom records\n+\t\tusing sample 50_000\n+\t);\n+\n+# this should not time out\n+statement ok\n+with\n+day_cat_rows as\n+  (select category,\n+          creation_day\n+   from records\n+   group by category,\n+            creation_day),\n+recs as\n+  (select category,\n+          records.creation_dt,\n+          labels.label_dt,\n+          labels.label\n+   from records\n+   left join labels on labels.id = records.id),\n+counts as\n+  (select day_cat_rows.creation_day,\n+          category,\n+     (select count(1)\n+      from recs\n+      where recs.creation_dt > day_cat_rows.creation_day - '30 days'::interval\n+        and recs.creation_dt <= day_cat_rows.creation_day\n+        and recs.category = day_cat_rows.category\n+        and recs.label_dt <= day_cat_rows.creation_day\n+        and recs.label = 1) as num_labeled_30d,\n+   from day_cat_rows)\n+select *\n+from counts;\n\\ No newline at end of file\n",
  "problem_statement": "Specific query not finishing since v1.1.0 and filling up all temp disk space\n### What happens?\n\nWe have a specific query (involving a join and subqueries for counting), of which a minimal reproducible example has been derived below. It operates on two tables, one with over a million rows, the other 4-5% of that. The query works fine in DuckDB 1.0.0 and finishes very fast (at most a few seconds). However, in later releases it never seems to finish -- instead, a temporary directory gets created that keeps growing (and the progress bar remains stuck at 50%).\n\nA `git bisect` helped point out that the commit introducing the problem seems to be https://github.com/duckdb/duckdb/commit/91b0fb71d17090e9f68332e65673ab899d691bca .\n\n### To Reproduce\n\nGenerate dummy data (two Parquet files) using the following Python script (with numpy, pandas, and duckdb):\n\n```python\nimport numpy as np\nimport pandas as pd\nimport duckdb\n\n\nnp.random.seed(12345)\n\ndates = np.random.randint(low=1514764800.0, high=1704067200.0, size=(1000000,))\ncategories = np.random.randint(low=0, high=200, size=(1000000,))\ndates.sort()\n\nrecords = pd.DataFrame({\n    \"creation_dt\": dates,\n    \"category\": categories,\n}).reset_index().rename(columns={\"index\": \"id\"})\nrecords[\"creation_dt\"] = records[\"creation_dt\"].map(lambda x: pd.Timestamp(x * 1e9))\nrecords[\"category\"] = records[\"category\"].map('{:02X}'.format)\n\nlabels = records.sample(frac=0.05, random_state=23456)\nlabel_delays = np.random.randint(low=1 * 60 * 60, high=125 * 24 * 60 * 60, size=(len(labels.index),))\nlabels[\"label_dt\"] = labels[\"creation_dt\"] + pd.to_timedelta(label_delays, \"s\")\nlabels[\"label\"] = 1\n\ndb = duckdb.connect()\ndb.sql(\"\"\"\n    copy (\n       select id, creation_dt::timestamp as creation_dt, creation_dt::date as creation_day, category from records\n    ) to 'records.parquet'\n\"\"\")\ndb.sql(\"\"\"\n    copy (\n       select id, label_dt::timestamp as label_dt, label from labels\n    ) to 'labels.parquet'\n\"\"\")\n```\n\n`records.parquet` has a million rows looking like this:\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  id   \u2502     creation_dt     \u2502 creation_day \u2502 category \u2502\n\u2502 int64 \u2502      timestamp      \u2502     date     \u2502 varchar  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     0 \u2502 2018-01-01 00:02:15 \u2502 2018-01-01   \u2502 A5       \u2502\n\u2502     1 \u2502 2018-01-01 00:17:21 \u2502 2018-01-01   \u2502 62       \u2502\n\u2502     2 \u2502 2018-01-01 00:22:10 \u2502 2018-01-01   \u2502 07       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n5% of which has a corresponding row in `labels.parquet` looking like this:\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   id   \u2502      label_dt       \u2502 label \u2502\n\u2502 int64  \u2502      timestamp      \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 970100 \u2502 2024-02-13 10:14:11 \u2502     1 \u2502\n\u2502 524709 \u2502 2021-05-15 17:11:48 \u2502     1 \u2502\n\u2502 800619 \u2502 2022-11-28 23:57:17 \u2502     1 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n\n\nThen run the following SQL (on an in-memory database or on a file, same result). The query should result into a table that has a row for each combination of `creation_day` and `category` appearing in `records` with a third column `num_labeled_30d` that contains a count of how many records with that `category` have been labeled in the 30 days prior to `creation_day`.\n\n```sql\ncreate or replace table records as\nfrom 'records.parquet';\n\n\ncreate or replace table labels as\nfrom 'labels.parquet';\n\nwith\nday_cat_rows as\n  (select category,\n          creation_day\n   from records\n   group by category,\n            creation_day),\nrecs as\n  (select category,\n          records.creation_dt,\n          labels.label_dt,\n          labels.label\n   from records\n   left join labels on labels.id = records.id),\ncounts as\n  (select day_cat_rows.creation_day,\n          category,\n\n     (select count(1)\n      from recs\n      where recs.creation_dt > day_cat_rows.creation_day - '30 days'::interval\n        and recs.creation_dt <= day_cat_rows.creation_day\n        and recs.category = day_cat_rows.category\n        and recs.label_dt <= day_cat_rows.creation_day\n        and recs.label = 1) as num_labeled_30d,\n   from day_cat_rows)\nselect *\nfrom counts;\n```\n\nDuckDB 1.0.0 will give an output that looks like this:\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 creation_day \u2502 category \u2502 num_labeled_30d \u2502\n\u2502     date     \u2502 varchar  \u2502      int64      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2023-11-25   \u2502 05       \u2502               2 \u2502\n\u2502 2023-11-25   \u2502 3A       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 09       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 62       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 32       \u2502               1 \u2502\n\u2502 2023-11-25   \u2502 83       \u2502               1 \u2502\n\u2502 2023-11-25   \u2502 92       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 21       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 BA       \u2502               2 \u2502\n\u2502 2023-11-25   \u2502 96       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 6F       \u2502               1 \u2502\n\u2502 2023-11-25   \u2502 64       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 24       \u2502               1 \u2502\n\u2502 2023-11-25   \u2502 14       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 5D       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 6A       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 4D       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 B7       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 9E       \u2502               1 \u2502\n\u2502 2023-11-25   \u2502 1F       \u2502               0 \u2502\n\u2502     \u00b7        \u2502 \u00b7        \u2502               \u00b7 \u2502\n\u2502     \u00b7        \u2502 \u00b7        \u2502               \u00b7 \u2502\n\u2502     \u00b7        \u2502 \u00b7        \u2502               \u00b7 \u2502\n\u2502 2021-09-07   \u2502 3F       \u2502               1 \u2502\n\u2502 2021-09-07   \u2502 74       \u2502               0 \u2502\n\u2502 2021-09-07   \u2502 06       \u2502               0 \u2502\n\u2502 2021-09-07   \u2502 52       \u2502               0 \u2502\n\u2502 2021-09-07   \u2502 2F       \u2502               0 \u2502\n\u2502 2021-09-07   \u2502 8B       \u2502               0 \u2502\n\u2502 2021-09-07   \u2502 17       \u2502               0 \u2502\n\u2502 2021-09-07   \u2502 89       \u2502               1 \u2502\n\u2502 2021-09-07   \u2502 39       \u2502               1 \u2502\n\u2502 2021-09-07   \u2502 5A       \u2502               1 \u2502\n\u2502 2021-09-07   \u2502 7E       \u2502               0 \u2502\n\u2502 2021-09-07   \u2502 82       \u2502               0 \u2502\n\u2502 2021-09-07   \u2502 7B       \u2502               0 \u2502\n\u2502 2021-09-07   \u2502 90       \u2502               0 \u2502\n\u2502 2021-09-08   \u2502 6D       \u2502               1 \u2502\n\u2502 2021-09-08   \u2502 A0       \u2502               1 \u2502\n\u2502 2021-09-08   \u2502 67       \u2502               2 \u2502\n\u2502 2021-09-08   \u2502 94       \u2502               0 \u2502\n\u2502 2021-09-08   \u2502 61       \u2502               0 \u2502\n\u2502 2021-09-08   \u2502 60       \u2502               1 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 393154 rows (40 shown)          3 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\nBut in later releases, it does not finish.\n\n### OS:\n\nmacOS 15.3, arm64\n\n### DuckDB Version:\n\n1.1.0 and later\n\n### DuckDB Client:\n\nCLI\n\n### Hardware:\n\nM1 Pro chip, 32 GB RAM\n\n### Full Name:\n\nThomas Daniels\n\n### Affiliation:\n\nDNS Belgium\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a source build\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [x] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [x] Yes, I have\nSpecific query not finishing since v1.1.0 and filling up all temp disk space\n### What happens?\n\nWe have a specific query (involving a join and subqueries for counting), of which a minimal reproducible example has been derived below. It operates on two tables, one with over a million rows, the other 4-5% of that. The query works fine in DuckDB 1.0.0 and finishes very fast (at most a few seconds). However, in later releases it never seems to finish -- instead, a temporary directory gets created that keeps growing (and the progress bar remains stuck at 50%).\n\nA `git bisect` helped point out that the commit introducing the problem seems to be https://github.com/duckdb/duckdb/commit/91b0fb71d17090e9f68332e65673ab899d691bca .\n\n### To Reproduce\n\nGenerate dummy data (two Parquet files) using the following Python script (with numpy, pandas, and duckdb):\n\n```python\nimport numpy as np\nimport pandas as pd\nimport duckdb\n\n\nnp.random.seed(12345)\n\ndates = np.random.randint(low=1514764800.0, high=1704067200.0, size=(1000000,))\ncategories = np.random.randint(low=0, high=200, size=(1000000,))\ndates.sort()\n\nrecords = pd.DataFrame({\n    \"creation_dt\": dates,\n    \"category\": categories,\n}).reset_index().rename(columns={\"index\": \"id\"})\nrecords[\"creation_dt\"] = records[\"creation_dt\"].map(lambda x: pd.Timestamp(x * 1e9))\nrecords[\"category\"] = records[\"category\"].map('{:02X}'.format)\n\nlabels = records.sample(frac=0.05, random_state=23456)\nlabel_delays = np.random.randint(low=1 * 60 * 60, high=125 * 24 * 60 * 60, size=(len(labels.index),))\nlabels[\"label_dt\"] = labels[\"creation_dt\"] + pd.to_timedelta(label_delays, \"s\")\nlabels[\"label\"] = 1\n\ndb = duckdb.connect()\ndb.sql(\"\"\"\n    copy (\n       select id, creation_dt::timestamp as creation_dt, creation_dt::date as creation_day, category from records\n    ) to 'records.parquet'\n\"\"\")\ndb.sql(\"\"\"\n    copy (\n       select id, label_dt::timestamp as label_dt, label from labels\n    ) to 'labels.parquet'\n\"\"\")\n```\n\n`records.parquet` has a million rows looking like this:\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  id   \u2502     creation_dt     \u2502 creation_day \u2502 category \u2502\n\u2502 int64 \u2502      timestamp      \u2502     date     \u2502 varchar  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     0 \u2502 2018-01-01 00:02:15 \u2502 2018-01-01   \u2502 A5       \u2502\n\u2502     1 \u2502 2018-01-01 00:17:21 \u2502 2018-01-01   \u2502 62       \u2502\n\u2502     2 \u2502 2018-01-01 00:22:10 \u2502 2018-01-01   \u2502 07       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n5% of which has a corresponding row in `labels.parquet` looking like this:\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   id   \u2502      label_dt       \u2502 label \u2502\n\u2502 int64  \u2502      timestamp      \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 970100 \u2502 2024-02-13 10:14:11 \u2502     1 \u2502\n\u2502 524709 \u2502 2021-05-15 17:11:48 \u2502     1 \u2502\n\u2502 800619 \u2502 2022-11-28 23:57:17 \u2502     1 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n\n\nThen run the following SQL (on an in-memory database or on a file, same result). The query should result into a table that has a row for each combination of `creation_day` and `category` appearing in `records` with a third column `num_labeled_30d` that contains a count of how many records with that `category` have been labeled in the 30 days prior to `creation_day`.\n\n```sql\ncreate or replace table records as\nfrom 'records.parquet';\n\n\ncreate or replace table labels as\nfrom 'labels.parquet';\n\nwith\nday_cat_rows as\n  (select category,\n          creation_day\n   from records\n   group by category,\n            creation_day),\nrecs as\n  (select category,\n          records.creation_dt,\n          labels.label_dt,\n          labels.label\n   from records\n   left join labels on labels.id = records.id),\ncounts as\n  (select day_cat_rows.creation_day,\n          category,\n\n     (select count(1)\n      from recs\n      where recs.creation_dt > day_cat_rows.creation_day - '30 days'::interval\n        and recs.creation_dt <= day_cat_rows.creation_day\n        and recs.category = day_cat_rows.category\n        and recs.label_dt <= day_cat_rows.creation_day\n        and recs.label = 1) as num_labeled_30d,\n   from day_cat_rows)\nselect *\nfrom counts;\n```\n\nDuckDB 1.0.0 will give an output that looks like this:\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 creation_day \u2502 category \u2502 num_labeled_30d \u2502\n\u2502     date     \u2502 varchar  \u2502      int64      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2023-11-25   \u2502 05       \u2502               2 \u2502\n\u2502 2023-11-25   \u2502 3A       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 09       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 62       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 32       \u2502               1 \u2502\n\u2502 2023-11-25   \u2502 83       \u2502               1 \u2502\n\u2502 2023-11-25   \u2502 92       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 21       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 BA       \u2502               2 \u2502\n\u2502 2023-11-25   \u2502 96       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 6F       \u2502               1 \u2502\n\u2502 2023-11-25   \u2502 64       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 24       \u2502               1 \u2502\n\u2502 2023-11-25   \u2502 14       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 5D       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 6A       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 4D       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 B7       \u2502               0 \u2502\n\u2502 2023-11-25   \u2502 9E       \u2502               1 \u2502\n\u2502 2023-11-25   \u2502 1F       \u2502               0 \u2502\n\u2502     \u00b7        \u2502 \u00b7        \u2502               \u00b7 \u2502\n\u2502     \u00b7        \u2502 \u00b7        \u2502               \u00b7 \u2502\n\u2502     \u00b7        \u2502 \u00b7        \u2502               \u00b7 \u2502\n\u2502 2021-09-07   \u2502 3F       \u2502               1 \u2502\n\u2502 2021-09-07   \u2502 74       \u2502               0 \u2502\n\u2502 2021-09-07   \u2502 06       \u2502               0 \u2502\n\u2502 2021-09-07   \u2502 52       \u2502               0 \u2502\n\u2502 2021-09-07   \u2502 2F       \u2502               0 \u2502\n\u2502 2021-09-07   \u2502 8B       \u2502               0 \u2502\n\u2502 2021-09-07   \u2502 17       \u2502               0 \u2502\n\u2502 2021-09-07   \u2502 89       \u2502               1 \u2502\n\u2502 2021-09-07   \u2502 39       \u2502               1 \u2502\n\u2502 2021-09-07   \u2502 5A       \u2502               1 \u2502\n\u2502 2021-09-07   \u2502 7E       \u2502               0 \u2502\n\u2502 2021-09-07   \u2502 82       \u2502               0 \u2502\n\u2502 2021-09-07   \u2502 7B       \u2502               0 \u2502\n\u2502 2021-09-07   \u2502 90       \u2502               0 \u2502\n\u2502 2021-09-08   \u2502 6D       \u2502               1 \u2502\n\u2502 2021-09-08   \u2502 A0       \u2502               1 \u2502\n\u2502 2021-09-08   \u2502 67       \u2502               2 \u2502\n\u2502 2021-09-08   \u2502 94       \u2502               0 \u2502\n\u2502 2021-09-08   \u2502 61       \u2502               0 \u2502\n\u2502 2021-09-08   \u2502 60       \u2502               1 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 393154 rows (40 shown)          3 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\nBut in later releases, it does not finish.\n\n### OS:\n\nmacOS 15.3, arm64\n\n### DuckDB Version:\n\n1.1.0 and later\n\n### DuckDB Client:\n\nCLI\n\n### Hardware:\n\nM1 Pro chip, 32 GB RAM\n\n### Full Name:\n\nThomas Daniels\n\n### Affiliation:\n\nDNS Belgium\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a source build\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [x] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [x] Yes, I have\n",
  "hints_text": "Same result on a Mac with 2 GHz Quad-Core Intel Core i5 processor running MacOS Version 15.3 (24D60)\nJust tried this query as well. \nSetting SET disabled_optimizers = 'join_order,build_side_probe_side'; \nor changing the recs cte part  to reduce the #rows and materialize seems to solve the problem for now.\n\nwith\nday_cat_rows as \n  (select category,\n          creation_day\n   from records\n   group by category,\n            creation_day),\nrecs as materialized\n  (select category,\n          records.creation_dt,\n          labels.label_dt\n   from records\n   inner join labels on labels.id = records.id and labels.label = 1),\ncounts as\n  (select day_cat_rows.creation_day,\n          category,\n\n     (select count(1)\n      from recs\n      where recs.creation_dt > day_cat_rows.creation_day - '30 days'::interval\n        and recs.creation_dt <= day_cat_rows.creation_day\n        and recs.category = day_cat_rows.category\n        and recs.label_dt <= day_cat_rows.creation_day\n      ) as num_labeled_30d,\n   from day_cat_rows)\nselect *\nfrom counts\norder by creation_day \n\nthanks for the suggestion @cmettler, just adding the `materialized` keyword appears to make it work (both for the query in this issue and our original query). I'll leave the issue open because I suppose there is still an underlying issue that makes the original query stuck without materializing `recs`\nSame result on a Mac with 2 GHz Quad-Core Intel Core i5 processor running MacOS Version 15.3 (24D60)\nJust tried this query as well. \nSetting SET disabled_optimizers = 'join_order,build_side_probe_side'; \nor changing the recs cte part  to reduce the #rows and materialize seems to solve the problem for now.\n\nwith\nday_cat_rows as \n  (select category,\n          creation_day\n   from records\n   group by category,\n            creation_day),\nrecs as materialized\n  (select category,\n          records.creation_dt,\n          labels.label_dt\n   from records\n   inner join labels on labels.id = records.id and labels.label = 1),\ncounts as\n  (select day_cat_rows.creation_day,\n          category,\n\n     (select count(1)\n      from recs\n      where recs.creation_dt > day_cat_rows.creation_day - '30 days'::interval\n        and recs.creation_dt <= day_cat_rows.creation_day\n        and recs.category = day_cat_rows.category\n        and recs.label_dt <= day_cat_rows.creation_day\n      ) as num_labeled_30d,\n   from day_cat_rows)\nselect *\nfrom counts\norder by creation_day \n\nthanks for the suggestion @cmettler, just adding the `materialized` keyword appears to make it work (both for the query in this issue and our original query). I'll leave the issue open because I suppose there is still an underlying issue that makes the original query stuck without materializing `recs`",
  "created_at": "2025-04-16T08:02:24Z"
}