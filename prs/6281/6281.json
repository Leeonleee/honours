{
  "repo": "duckdb/duckdb",
  "pull_number": 6281,
  "instance_id": "duckdb__duckdb-6281",
  "issue_numbers": [
    "6249"
  ],
  "base_commit": "8ef0b5c610123c14443ac23ceb6b824441b7ace0",
  "patch": "diff --git a/data/json/top_level_two_arrays.json b/data/json/top_level_two_arrays.json\nnew file mode 100644\nindex 000000000000..1e96d0bf4d0d\n--- /dev/null\n+++ b/data/json/top_level_two_arrays.json\n@@ -0,0 +1,2 @@\n+[{\"conclusion\":\"cancelled\"}, {\"conclusion\":\"cancelled\"}]\n+[{\"conclusion\":\"cancelled\"}, {\"conclusion\":\"cancelled\"}]\ndiff --git a/extension/json/buffered_json_reader.cpp b/extension/json/buffered_json_reader.cpp\nindex a38ee0a3eaba..3e4e7b196d1f 100644\n--- a/extension/json/buffered_json_reader.cpp\n+++ b/extension/json/buffered_json_reader.cpp\n@@ -25,7 +25,12 @@ JSONBufferHandle::JSONBufferHandle(idx_t buffer_index_p, idx_t readers_p, Alloca\n JSONFileHandle::JSONFileHandle(unique_ptr<FileHandle> file_handle_p, Allocator &allocator_p)\n     : file_handle(std::move(file_handle_p)), allocator(allocator_p), can_seek(file_handle->CanSeek()),\n       plain_file_source(file_handle->OnDiskFile() && can_seek), file_size(file_handle->GetFileSize()), read_position(0),\n-      cached_size(0) {\n+      requested_reads(0), actual_reads(0), cached_size(0) {\n+}\n+\n+void JSONFileHandle::Close() {\n+\tfile_handle->Close();\n+\tcached_buffers.clear();\n }\n \n idx_t JSONFileHandle::FileSize() const {\n@@ -36,10 +41,6 @@ idx_t JSONFileHandle::Remaining() const {\n \treturn file_size - read_position;\n }\n \n-bool JSONFileHandle::PlainFileSource() const {\n-\treturn plain_file_source;\n-}\n-\n bool JSONFileHandle::CanSeek() const {\n \treturn can_seek;\n }\n@@ -53,6 +54,9 @@ idx_t JSONFileHandle::GetPositionAndSize(idx_t &position, idx_t requested_size)\n \tposition = read_position;\n \tauto actual_size = MinValue<idx_t>(requested_size, Remaining());\n \tread_position += actual_size;\n+\tif (actual_size != 0) {\n+\t\trequested_reads++;\n+\t}\n \treturn actual_size;\n }\n \n@@ -60,11 +64,13 @@ void JSONFileHandle::ReadAtPosition(const char *pointer, idx_t size, idx_t posit\n \tD_ASSERT(size != 0);\n \tif (plain_file_source) {\n \t\tfile_handle->Read((void *)pointer, size, position);\n+\t\tactual_reads++;\n \t\treturn;\n \t}\n \n \tif (sample_run) { // Cache the buffer\n \t\tfile_handle->Read((void *)pointer, size, position);\n+\t\tactual_reads++;\n \t\tcached_buffers.emplace_back(allocator.Allocate(size));\n \t\tmemcpy(cached_buffers.back().get(), pointer, size);\n \t\tcached_size += size;\n@@ -73,9 +79,11 @@ void JSONFileHandle::ReadAtPosition(const char *pointer, idx_t size, idx_t posit\n \n \tif (!cached_buffers.empty() || position < cached_size) {\n \t\tReadFromCache(pointer, size, position);\n+\t\tactual_reads++;\n \t}\n \tif (size != 0) {\n \t\tfile_handle->Read((void *)pointer, size, position);\n+\t\tactual_reads++;\n \t}\n }\n \n@@ -143,6 +151,16 @@ void BufferedJSONReader::OpenJSONFile() {\n \tfile_handle = make_unique<JSONFileHandle>(std::move(regular_file_handle), BufferAllocator::Get(context));\n }\n \n+void BufferedJSONReader::CloseJSONFile() {\n+\twhile (true) {\n+\t\tlock_guard<mutex> guard(lock);\n+\t\tif (file_handle->RequestedReadsComplete()) {\n+\t\t\tfile_handle->Close();\n+\t\t\tbreak;\n+\t\t}\n+\t}\n+}\n+\n bool BufferedJSONReader::IsOpen() {\n \treturn file_handle != nullptr;\n }\n@@ -246,9 +264,15 @@ void BufferedJSONReader::Reset() {\n \n void JSONFileHandle::Reset() {\n \tread_position = 0;\n+\trequested_reads = 0;\n+\tactual_reads = 0;\n \tif (plain_file_source) {\n \t\tfile_handle->Reset();\n \t}\n }\n \n+bool JSONFileHandle::RequestedReadsComplete() {\n+\treturn requested_reads == actual_reads;\n+}\n+\n } // namespace duckdb\ndiff --git a/extension/json/include/buffered_json_reader.hpp b/extension/json/include/buffered_json_reader.hpp\nindex ffc8fda5f592..f98e3feddc9a 100644\n--- a/extension/json/include/buffered_json_reader.hpp\n+++ b/extension/json/include/buffered_json_reader.hpp\n@@ -58,11 +58,11 @@ struct JSONBufferHandle {\n struct JSONFileHandle {\n public:\n \tJSONFileHandle(unique_ptr<FileHandle> file_handle, Allocator &allocator);\n+\tvoid Close();\n \n \tidx_t FileSize() const;\n \tidx_t Remaining() const;\n \n-\tbool PlainFileSource() const;\n \tbool CanSeek() const;\n \tvoid Seek(idx_t position);\n \n@@ -71,6 +71,7 @@ struct JSONFileHandle {\n \tidx_t Read(const char *pointer, idx_t requested_size, bool sample_run);\n \n \tvoid Reset();\n+\tbool RequestedReadsComplete();\n \n private:\n \tidx_t ReadFromCache(const char *&pointer, idx_t &size, idx_t &position);\n@@ -87,6 +88,8 @@ struct JSONFileHandle {\n \n \t//! Read properties\n \tidx_t read_position;\n+\tidx_t requested_reads;\n+\tatomic<idx_t> actual_reads;\n \n \t//! Cached buffers for resetting when reading stream\n \tvector<AllocatedData> cached_buffers;\n@@ -98,6 +101,7 @@ class BufferedJSONReader {\n \tBufferedJSONReader(ClientContext &context, BufferedJSONReaderOptions options, string file_path);\n \n \tvoid OpenJSONFile();\n+\tvoid CloseJSONFile();\n \tbool IsOpen();\n \n \tBufferedJSONReaderOptions &GetOptions();\ndiff --git a/extension/json/include/json_scan.hpp b/extension/json/include/json_scan.hpp\nindex 7b87a7b44639..e1a1c9c5dd98 100644\n--- a/extension/json/include/json_scan.hpp\n+++ b/extension/json/include/json_scan.hpp\n@@ -26,6 +26,16 @@ enum class JSONScanType : uint8_t {\n \tSAMPLE = 3,\n };\n \n+enum class JSONScanTopLevelType : uint8_t {\n+\tINVALID = 0,\n+\t//! Sequential objects, e.g., NDJSON\n+\tOBJECTS = 1,\n+\t//! Top-level array containing objects\n+\tARRAY_OF_OBJECTS = 2,\n+\t//! Other, e.g., array of integer, or just strings\n+\tOTHER = 3\n+};\n+\n //! Even though LogicalTypeId is just a uint8_t, this is still needed ...\n struct LogicalTypeIdHash {\n \tinline std::size_t operator()(const LogicalTypeId &id) const {\n@@ -105,7 +115,7 @@ struct JSONScanData : public TableFunctionData {\n \t//! Max depth we go to detect nested JSON schema (defaults to unlimited)\n \tidx_t max_depth = NumericLimits<idx_t>::Maximum();\n \t//! Whether we're parsing objects (usually), or something else like arrays\n-\tbool objects = true;\n+\tJSONScanTopLevelType top_level_type = JSONScanTopLevelType::OBJECTS;\n \t//! Forced date/timestamp formats\n \tstring date_format;\n \tstring timestamp_format;\n@@ -181,9 +191,14 @@ struct JSONScanLocalState {\n \tyyjson_alc *GetAllocator();\n \tvoid ThrowTransformError(idx_t count, idx_t object_index, const string &error_message);\n \n+\tidx_t scan_count;\n \tJSONLine lines[STANDARD_VECTOR_SIZE];\n \tyyjson_val *objects[STANDARD_VECTOR_SIZE];\n \n+\tidx_t array_idx;\n+\tidx_t array_offset;\n+\tyyjson_val *array_objects[STANDARD_VECTOR_SIZE];\n+\n \tidx_t batch_index;\n \n \t//! Options when transforming the JSON to columnar data\n@@ -192,6 +207,7 @@ struct JSONScanLocalState {\n \n private:\n \tyyjson_val *ParseLine(char *line_start, idx_t line_size, idx_t remaining, JSONLine &line);\n+\tidx_t GetObjectsFromArray();\n \n private:\n \t//! Bind data\n@@ -300,7 +316,6 @@ struct JSONScan {\n \t\ttable_function.serialize = JSONScanSerialize;\n \t\ttable_function.deserialize = JSONScanDeserialize;\n \n-\t\t// TODO: might be able to do some of these\n \t\ttable_function.projection_pushdown = false;\n \t\ttable_function.filter_pushdown = false;\n \t\ttable_function.filter_prune = false;\ndiff --git a/extension/json/json_functions.cpp b/extension/json/json_functions.cpp\nindex adbf8bb6e065..96e6019c41af 100644\n--- a/extension/json/json_functions.cpp\n+++ b/extension/json/json_functions.cpp\n@@ -166,6 +166,12 @@ vector<CreateTableFunctionInfo> JSONFunctions::GetTableFunctions() {\n unique_ptr<TableRef> JSONFunctions::ReadJSONReplacement(ClientContext &context, const string &table_name,\n                                                         ReplacementScanData *data) {\n \tauto lower_name = StringUtil::Lower(table_name);\n+\t// remove any compression\n+\tif (StringUtil::EndsWith(lower_name, \".gz\")) {\n+\t\tlower_name = lower_name.substr(0, lower_name.size() - 3);\n+\t} else if (StringUtil::EndsWith(lower_name, \".zst\")) {\n+\t\tlower_name = lower_name.substr(0, lower_name.size() - 4);\n+\t}\n \tif (!StringUtil::EndsWith(lower_name, \".json\") && !StringUtil::Contains(lower_name, \".json?\") &&\n \t    !StringUtil::EndsWith(lower_name, \".ndjson\") && !StringUtil::Contains(lower_name, \".ndjson?\")) {\n \t\treturn nullptr;\ndiff --git a/extension/json/json_functions/json_transform.cpp b/extension/json/json_functions/json_transform.cpp\nindex a37473d0feac..f4bef879ec46 100644\n--- a/extension/json/json_functions/json_transform.cpp\n+++ b/extension/json/json_functions/json_transform.cpp\n@@ -523,6 +523,21 @@ static bool TransformArray(yyjson_val *arrays[], yyjson_alc *alc, Vector &result\n \treturn success;\n }\n \n+bool TransformToJSON(yyjson_val *vals[], yyjson_alc *alc, Vector &result, const idx_t count) {\n+\tauto data = (string_t *)FlatVector::GetData(result);\n+\tauto &validity = FlatVector::Validity(result);\n+\tfor (idx_t i = 0; i < count; i++) {\n+\t\tconst auto &val = vals[i];\n+\t\tif (!val) {\n+\t\t\tvalidity.SetInvalid(i);\n+\t\t} else {\n+\t\t\tdata[i] = JSONCommon::WriteVal(val, alc);\n+\t\t}\n+\t}\n+\t// Can always transform to JSON\n+\treturn true;\n+}\n+\n bool JSONTransform::Transform(yyjson_val *vals[], yyjson_alc *alc, Vector &result, const idx_t count,\n                               JSONTransformOptions &options) {\n \tauto result_type = result.GetType();\n@@ -531,6 +546,10 @@ bool JSONTransform::Transform(yyjson_val *vals[], yyjson_alc *alc, Vector &resul\n \t\treturn TransformFromStringWithFormat(vals, result, count, options);\n \t}\n \n+\tif (JSONCommon::LogicalTypeIsJSON(result_type)) {\n+\t\treturn TransformToJSON(vals, alc, result, count);\n+\t}\n+\n \tswitch (result_type.id()) {\n \tcase LogicalTypeId::SQLNULL:\n \t\treturn true;\ndiff --git a/extension/json/json_functions/read_json.cpp b/extension/json/json_functions/read_json.cpp\nindex 249e701c6f8f..585973a5530a 100644\n--- a/extension/json/json_functions/read_json.cpp\n+++ b/extension/json/json_functions/read_json.cpp\n@@ -13,32 +13,17 @@ void JSONScan::AutoDetect(ClientContext &context, JSONScanData &bind_data, vecto\n \tJSONScanLocalState lstate(context, gstate);\n \tArenaAllocator allocator(BufferAllocator::Get(context));\n \n-\tstatic const unordered_map<LogicalTypeId, vector<const char *>, LogicalTypeIdHash> FORMAT_TEMPLATES = {\n-\t    {LogicalTypeId::DATE, {\"%m-%d-%Y\", \"%m-%d-%y\", \"%d-%m-%Y\", \"%d-%m-%y\", \"%Y-%m-%d\", \"%y-%m-%d\"}},\n-\t    {LogicalTypeId::TIMESTAMP,\n-\t     {\"%Y-%m-%d %H:%M:%S.%f\", \"%m-%d-%Y %I:%M:%S %p\", \"%m-%d-%y %I:%M:%S %p\", \"%d-%m-%Y %H:%M:%S\",\n-\t      \"%d-%m-%y %H:%M:%S\", \"%Y-%m-%d %H:%M:%S\", \"%y-%m-%d %H:%M:%S\", \"%Y-%m-%dT%H:%M:%SZ\"}},\n-\t};\n-\n-\t// Populate possible date/timestamp formats, assume this is consistent across columns\n-\tfor (auto &kv : FORMAT_TEMPLATES) {\n-\t\tconst auto &type = kv.first;\n-\t\tif (bind_data.date_format_map.HasFormats(type)) {\n-\t\t\tcontinue; // Already populated\n-\t\t}\n-\t\tconst auto &format_strings = kv.second;\n-\t\tfor (auto &format_string : format_strings) {\n-\t\t\tbind_data.date_format_map.AddFormat(type, format_string);\n-\t\t}\n-\t}\n-\n \t// Read for the specified sample size\n \tJSONStructureNode node;\n+\tbool more_than_one = false;\n \tVector string_vector(LogicalType::VARCHAR);\n \tidx_t remaining = bind_data.sample_size;\n \twhile (remaining != 0) {\n \t\tallocator.Reset();\n \t\tauto read_count = lstate.ReadNext(gstate);\n+\t\tif (read_count > 1) {\n+\t\t\tmore_than_one = true;\n+\t\t}\n \t\tif (read_count == 0) {\n \t\t\tbreak;\n \t\t}\n@@ -54,15 +39,29 @@ void JSONScan::AutoDetect(ClientContext &context, JSONScanData &bind_data, vecto\n \t\tnode.InitializeCandidateTypes(bind_data.max_depth);\n \t\tnode.RefineCandidateTypes(lstate.objects, next, string_vector, allocator, bind_data.date_format_map);\n \t\tremaining -= next;\n+\n+\t\tif (gstate.file_index == 10) {\n+\t\t\t// We really shouldn't open more than 10 files when sampling\n+\t\t\tbreak;\n+\t\t}\n \t}\n \tbind_data.type = original_scan_type;\n \tbind_data.transform_options.date_format_map = &bind_data.date_format_map;\n \n-\tconst auto type = JSONStructure::StructureToType(context, node, bind_data.max_depth);\n+\tauto type = JSONStructure::StructureToType(context, node, bind_data.max_depth);\n+\tif (type.id() == LogicalTypeId::STRUCT) {\n+\t\tbind_data.top_level_type = JSONScanTopLevelType::OBJECTS;\n+\t} else if (!more_than_one && type.id() == LogicalTypeId::LIST &&\n+\t           ListType::GetChildType(type).id() == LogicalTypeId::STRUCT) {\n+\t\tbind_data.top_level_type = JSONScanTopLevelType::ARRAY_OF_OBJECTS;\n+\t\tbind_data.options.format = JSONFormat::UNSTRUCTURED;\n+\t\ttype = ListType::GetChildType(type);\n+\t}\n+\n \tif (type.id() != LogicalTypeId::STRUCT) {\n \t\treturn_types.emplace_back(type);\n \t\tnames.emplace_back(\"json\");\n-\t\tbind_data.objects = false;\n+\t\tbind_data.top_level_type = JSONScanTopLevelType::OTHER;\n \t} else {\n \t\tconst auto &child_types = StructType::GetChildTypes(type);\n \t\treturn_types.reserve(child_types.size());\n@@ -189,9 +188,11 @@ static void ReadJSONFunction(ClientContext &context, TableFunctionInput &data_p,\n \tauto &gstate = ((JSONGlobalTableFunctionState &)*data_p.global_state).state;\n \tauto &lstate = ((JSONLocalTableFunctionState &)*data_p.local_state).state;\n \n-\t// Fetch next lines\n \tconst auto count = lstate.ReadNext(gstate);\n-\tconst auto objects = lstate.objects;\n+\tconst auto objects = gstate.bind_data.top_level_type == JSONScanTopLevelType::ARRAY_OF_OBJECTS\n+\t                         ? lstate.array_objects\n+\t                         : lstate.objects;\n+\toutput.SetCardinality(count);\n \n \tvector<Vector *> result_vectors;\n \tresult_vectors.reserve(output.ColumnCount());\n@@ -202,13 +203,14 @@ static void ReadJSONFunction(ClientContext &context, TableFunctionInput &data_p,\n \n \t// Pass current reader to transform options so we can get line number information if an error occurs\n \tbool success;\n-\tif (gstate.bind_data.objects) {\n-\t\tsuccess = JSONTransform::TransformObject(objects, lstate.GetAllocator(), count, gstate.bind_data.names,\n-\t\t                                         result_vectors, lstate.transform_options);\n-\t} else {\n+\tif (gstate.bind_data.top_level_type == JSONScanTopLevelType::OTHER) {\n \t\tsuccess = JSONTransform::Transform(objects, lstate.GetAllocator(), *result_vectors[0], count,\n \t\t                                   lstate.transform_options);\n+\t} else {\n+\t\tsuccess = JSONTransform::TransformObject(objects, lstate.GetAllocator(), count, gstate.bind_data.names,\n+\t\t                                         result_vectors, lstate.transform_options);\n \t}\n+\n \tif (!success) {\n \t\tstring hint = gstate.bind_data.auto_detect\n \t\t                  ? \"\\nTry increasing 'sample_size', reducing 'maximum_depth', specifying 'columns' manually, \"\n@@ -217,7 +219,6 @@ static void ReadJSONFunction(ClientContext &context, TableFunctionInput &data_p,\n \t\tlstate.ThrowTransformError(count, lstate.transform_options.object_index,\n \t\t                           lstate.transform_options.error_message + hint);\n \t}\n-\toutput.SetCardinality(count);\n }\n \n TableFunction JSONFunctions::GetReadJSONTableFunction(bool list_parameter, shared_ptr<JSONScanInfo> function_info) {\n@@ -235,6 +236,7 @@ TableFunction JSONFunctions::GetReadJSONTableFunction(bool list_parameter, share\n \ttable_function.named_parameters[\"timestamp_format\"] = LogicalType::VARCHAR;\n \n \ttable_function.projection_pushdown = true;\n+\t// TODO: might be able to do filter pushdown/prune too\n \n \ttable_function.function_info = std::move(function_info);\n \ndiff --git a/extension/json/json_scan.cpp b/extension/json/json_scan.cpp\nindex 32d7fd5b4bab..30df5c35e83b 100644\n--- a/extension/json/json_scan.cpp\n+++ b/extension/json/json_scan.cpp\n@@ -48,8 +48,11 @@ unique_ptr<FunctionData> JSONScanData::Bind(ClientContext &context, TableFunctio\n \t\t\t\toptions.format = JSONFormat::UNSTRUCTURED;\n \t\t\t} else if (format == \"newline_delimited\") {\n \t\t\t\toptions.format = JSONFormat::NEWLINE_DELIMITED;\n+\t\t\t} else if (format == \"array_of_objects\") {\n+\t\t\t\tresult->top_level_type = JSONScanTopLevelType::ARRAY_OF_OBJECTS;\n \t\t\t} else {\n-\t\t\t\tthrow BinderException(\"format must be one of ['auto', 'unstructured', 'newline_delimited']\");\n+\t\t\t\tthrow BinderException(\n+\t\t\t\t    \"format must be one of ['auto', 'unstructured', 'newline_delimited', 'array_of_objects']\");\n \t\t\t}\n \t\t} else if (loption == \"compression\") {\n \t\t\tauto compression = StringUtil::Lower(StringValue::Get(kv.second));\n@@ -67,6 +70,10 @@ unique_ptr<FunctionData> JSONScanData::Bind(ClientContext &context, TableFunctio\n \t\t}\n \t}\n \n+\tif (result->top_level_type == JSONScanTopLevelType::ARRAY_OF_OBJECTS) {\n+\t\tresult->options.format = JSONFormat::UNSTRUCTURED;\n+\t}\n+\n \treturn std::move(result);\n }\n \n@@ -98,6 +105,27 @@ void JSONScanData::InitializeFormats() {\n \tif (!timestamp_format.empty()) {\n \t\tdate_format_map.AddFormat(LogicalTypeId::TIMESTAMP, timestamp_format);\n \t}\n+\n+\tif (auto_detect) {\n+\t\tstatic const unordered_map<LogicalTypeId, vector<const char *>, LogicalTypeIdHash> FORMAT_TEMPLATES = {\n+\t\t    {LogicalTypeId::DATE, {\"%m-%d-%Y\", \"%m-%d-%y\", \"%d-%m-%Y\", \"%d-%m-%y\", \"%Y-%m-%d\", \"%y-%m-%d\"}},\n+\t\t    {LogicalTypeId::TIMESTAMP,\n+\t\t     {\"%Y-%m-%d %H:%M:%S.%f\", \"%m-%d-%Y %I:%M:%S %p\", \"%m-%d-%y %I:%M:%S %p\", \"%d-%m-%Y %H:%M:%S\",\n+\t\t      \"%d-%m-%y %H:%M:%S\", \"%Y-%m-%d %H:%M:%S\", \"%y-%m-%d %H:%M:%S\", \"%Y-%m-%dT%H:%M:%SZ\"}},\n+\t\t};\n+\n+\t\t// Populate possible date/timestamp formats, assume this is consistent across columns\n+\t\tfor (auto &kv : FORMAT_TEMPLATES) {\n+\t\t\tconst auto &type = kv.first;\n+\t\t\tif (date_format_map.HasFormats(type)) {\n+\t\t\t\tcontinue; // Already populated\n+\t\t\t}\n+\t\t\tconst auto &format_strings = kv.second;\n+\t\t\tfor (auto &format_string : format_strings) {\n+\t\t\t\tdate_format_map.AddFormat(type, format_string);\n+\t\t\t}\n+\t\t}\n+\t}\n }\n \n void JSONScanData::Serialize(FieldWriter &writer) {\n@@ -112,9 +140,17 @@ void JSONScanData::Serialize(FieldWriter &writer) {\n \twriter.WriteList<string>(names);\n \twriter.WriteList<idx_t>(valid_cols);\n \twriter.WriteField<idx_t>(max_depth);\n-\twriter.WriteField<bool>(objects);\n-\twriter.WriteString(date_format);\n-\twriter.WriteString(timestamp_format);\n+\twriter.WriteField<JSONScanTopLevelType>(top_level_type);\n+\tif (!date_format.empty()) {\n+\t\twriter.WriteString(date_format);\n+\t} else {\n+\t\twriter.WriteString(date_format_map.GetFormat(LogicalTypeId::DATE).format_specifier);\n+\t}\n+\tif (!timestamp_format.empty()) {\n+\t\twriter.WriteString(timestamp_format);\n+\t} else {\n+\t\twriter.WriteString(date_format_map.GetFormat(LogicalTypeId::TIMESTAMP).format_specifier);\n+\t}\n }\n \n void JSONScanData::Deserialize(FieldReader &reader) {\n@@ -129,9 +165,12 @@ void JSONScanData::Deserialize(FieldReader &reader) {\n \tnames = reader.ReadRequiredList<string>();\n \tvalid_cols = reader.ReadRequiredList<idx_t>();\n \tmax_depth = reader.ReadRequired<idx_t>();\n-\tobjects = reader.ReadRequired<bool>();\n+\ttop_level_type = reader.ReadRequired<JSONScanTopLevelType>();\n \tdate_format = reader.ReadRequired<string>();\n \ttimestamp_format = reader.ReadRequired<string>();\n+\n+\tInitializeFormats();\n+\ttransform_options.date_format_map = &date_format_map;\n }\n \n JSONScanGlobalState::JSONScanGlobalState(ClientContext &context, JSONScanData &bind_data_p)\n@@ -150,9 +189,9 @@ JSONScanGlobalState::JSONScanGlobalState(ClientContext &context, JSONScanData &b\n }\n \n JSONScanLocalState::JSONScanLocalState(ClientContext &context, JSONScanGlobalState &gstate)\n-    : batch_index(DConstants::INVALID_INDEX), bind_data(gstate.bind_data),\n+    : scan_count(0), array_idx(0), array_offset(0), batch_index(DConstants::INVALID_INDEX), bind_data(gstate.bind_data),\n       json_allocator(BufferAllocator::Get(context)), current_reader(nullptr), current_buffer_handle(nullptr),\n-      buffer_size(0), buffer_offset(0), prev_buffer_remainder(0) {\n+      is_last(false), buffer_size(0), buffer_offset(0), prev_buffer_remainder(0) {\n \n \t// Buffer to reconstruct JSON objects when they cross a buffer boundary\n \treconstruct_buffer = gstate.allocator.Allocate(gstate.bind_data.maximum_object_size + YYJSON_PADDING_SIZE);\n@@ -174,11 +213,6 @@ unique_ptr<GlobalTableFunctionState> JSONGlobalTableFunctionState::Init(ClientCo\n \t// Perform projection pushdown\n \tif (bind_data.type == JSONScanType::READ_JSON) {\n \t\tD_ASSERT(input.column_ids.size() <= bind_data.names.size()); // Can't project to have more columns\n-\t\tif (bind_data.auto_detect && input.column_ids.size() < bind_data.names.size()) {\n-\t\t\t// If we are auto-detecting, but don't need all columns present in the file,\n-\t\t\t// then we don't need to throw an error if we encounter an unseen column\n-\t\t\tbind_data.transform_options.error_unknown_key = false;\n-\t\t}\n \t\tvector<string> names;\n \t\tnames.reserve(input.column_ids.size());\n \t\tfor (idx_t i = 0; i < input.column_ids.size(); i++) {\n@@ -189,6 +223,11 @@ unique_ptr<GlobalTableFunctionState> JSONGlobalTableFunctionState::Init(ClientCo\n \t\t\tnames.push_back(std::move(bind_data.names[id]));\n \t\t\tbind_data.valid_cols.push_back(i);\n \t\t}\n+\t\tif (names.size() < bind_data.names.size()) {\n+\t\t\t// If we are auto-detecting, but don't need all columns present in the file,\n+\t\t\t// then we don't need to throw an error if we encounter an unseen column\n+\t\t\tbind_data.transform_options.error_unknown_key = false;\n+\t\t}\n \t\tbind_data.names = std::move(names);\n \t}\n \treturn result;\n@@ -231,6 +270,10 @@ static inline void SkipWhitespace(const char *buffer_ptr, idx_t &buffer_offset,\n idx_t JSONScanLocalState::ReadNext(JSONScanGlobalState &gstate) {\n \tjson_allocator.Reset();\n \n+\tif (gstate.bind_data.top_level_type == JSONScanTopLevelType::ARRAY_OF_OBJECTS && array_idx < scan_count) {\n+\t\treturn GetObjectsFromArray();\n+\t}\n+\n \tidx_t count = 0;\n \tif (buffer_offset == buffer_size) {\n \t\tif (!ReadNextBuffer(gstate)) {\n@@ -254,10 +297,20 @@ idx_t JSONScanLocalState::ReadNext(JSONScanGlobalState &gstate) {\n \tdefault:\n \t\tthrow InternalException(\"Unknown JSON format\");\n \t}\n+\tscan_count = count;\n \n \t// Skip over any remaining whitespace for the next scan\n \tSkipWhitespace(buffer_ptr, buffer_offset, buffer_size);\n \n+\tif (gstate.bind_data.top_level_type == JSONScanTopLevelType::ARRAY_OF_OBJECTS) {\n+\t\tif (scan_count > 1) {\n+\t\t\tthrow InvalidInputException(\"File must have exactly one array of objects when format='array_of_objects'\");\n+\t\t}\n+\t\tarray_idx = 0;\n+\t\tarray_offset = 0;\n+\t\treturn GetObjectsFromArray();\n+\t}\n+\n \treturn count;\n }\n \n@@ -332,10 +385,39 @@ yyjson_val *JSONScanLocalState::ParseLine(char *line_start, idx_t line_size, idx\n \t}\n }\n \n+idx_t JSONScanLocalState::GetObjectsFromArray() {\n+\tidx_t arr_count = 0;\n+\n+\tsize_t idx, max;\n+\tyyjson_val *val;\n+\tfor (; array_idx < scan_count; array_idx++, array_offset = 0) {\n+\t\tif (objects[array_idx]) {\n+\t\t\tyyjson_arr_foreach(objects[array_idx], idx, max, val) {\n+\t\t\t\tif (idx < array_offset) {\n+\t\t\t\t\tcontinue;\n+\t\t\t\t}\n+\t\t\t\tarray_objects[arr_count++] = val;\n+\t\t\t\tif (arr_count == STANDARD_VECTOR_SIZE) {\n+\t\t\t\t\tbreak;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tarray_offset = idx + 1;\n+\t\t\tif (arr_count == STANDARD_VECTOR_SIZE) {\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn arr_count;\n+}\n+\n bool JSONScanLocalState::ReadNextBuffer(JSONScanGlobalState &gstate) {\n \tif (current_reader) {\n \t\tD_ASSERT(current_buffer_handle);\n \t\tcurrent_reader->SetBufferLineOrObjectCount(current_buffer_handle->buffer_index, lines_or_objects_in_buffer);\n+\t\tif (is_last && gstate.bind_data.type != JSONScanType::SAMPLE) {\n+\t\t\t// Close files that are done if we're not sampling\n+\t\t\tcurrent_reader->CloseJSONFile();\n+\t\t}\n \t}\n \n \tAllocatedData buffer;\n@@ -396,7 +478,9 @@ bool JSONScanLocalState::ReadNextBuffer(JSONScanGlobalState &gstate) {\n \t\t// Unopened file\n \t\tcurrent_reader->OpenJSONFile();\n \t\tbatch_index = gstate.batch_index++;\n-\t\tif (options.format == JSONFormat::UNSTRUCTURED) {\n+\t\tif (options.format == JSONFormat::UNSTRUCTURED || (options.format == JSONFormat::NEWLINE_DELIMITED &&\n+\t\t                                                   options.compression != FileCompressionType::UNCOMPRESSED &&\n+\t\t                                                   gstate.file_index < gstate.json_readers.size())) {\n \t\t\tgstate.file_index++; // UNSTRUCTURED necessitates single-threaded read\n \t\t}\n \t\tif (options.format != JSONFormat::AUTO_DETECT) {\n@@ -450,9 +534,6 @@ bool JSONScanLocalState::ReadNextBuffer(JSONScanGlobalState &gstate) {\n \tauto json_buffer_handle = make_unique<JSONBufferHandle>(buffer_index, readers, std::move(buffer), buffer_size);\n \tcurrent_buffer_handle = json_buffer_handle.get();\n \tcurrent_reader->InsertBuffer(buffer_index, std::move(json_buffer_handle));\n-\tif (!current_reader->GetFileHandle().PlainFileSource() && gstate.bind_data.type == JSONScanType::SAMPLE) {\n-\t\t// TODO: store buffer\n-\t}\n \n \tbuffer_offset = 0;\n \tprev_buffer_remainder = 0;\n@@ -508,16 +589,18 @@ void JSONScanLocalState::ReadNextBufferSeek(JSONScanGlobalState &gstate, idx_t &\n }\n \n void JSONScanLocalState::ReadNextBufferNoSeek(JSONScanGlobalState &gstate, idx_t &buffer_index) {\n-\tauto &file_handle = current_reader->GetFileHandle();\n-\n \tidx_t request_size = gstate.buffer_capacity - prev_buffer_remainder - YYJSON_PADDING_SIZE;\n \tidx_t read_size;\n \t{\n \t\tlock_guard<mutex> reader_guard(current_reader->lock);\n \t\tbuffer_index = current_reader->GetBufferIndex();\n \n-\t\tread_size = file_handle.Read(buffer_ptr + prev_buffer_remainder, request_size,\n-\t\t                             gstate.bind_data.type == JSONScanType::SAMPLE);\n+\t\tif (current_reader->IsOpen()) {\n+\t\t\tread_size = current_reader->GetFileHandle().Read(buffer_ptr + prev_buffer_remainder, request_size,\n+\t\t\t                                                 gstate.bind_data.type == JSONScanType::SAMPLE);\n+\t\t} else {\n+\t\t\tread_size = 0;\n+\t\t}\n \t\tis_last = read_size < request_size;\n \n \t\tif (!gstate.bind_data.ignore_errors && read_size == 0 && prev_buffer_remainder != 0) {\n@@ -583,6 +666,11 @@ void JSONScanLocalState::ReconstructFirstObject(JSONScanGlobalState &gstate) {\n }\n \n void JSONScanLocalState::ReadUnstructured(idx_t &count) {\n+\t// yyjson does not always return YYJSON_READ_ERROR_UNEXPECTED_END properly\n+\t// if a different error code happens within the last 50 bytes\n+\t// we assume it should be YYJSON_READ_ERROR_UNEXPECTED_END instead\n+\tstatic constexpr idx_t END_BOUND = 50;\n+\n \tconst auto max_obj_size = reconstruct_buffer.GetSize();\n \tyyjson_read_err error;\n \tfor (; count < STANDARD_VECTOR_SIZE; count++) {\n@@ -608,8 +696,7 @@ void JSONScanLocalState::ReadUnstructured(idx_t &count) {\n \t\t} else if (error.pos > max_obj_size) {\n \t\t\tcurrent_reader->ThrowParseError(current_buffer_handle->buffer_index, lines_or_objects_in_buffer, error,\n \t\t\t                                \"Try increasing \\\"maximum_object_size\\\".\");\n-\n-\t\t} else if (error.code == YYJSON_READ_ERROR_UNEXPECTED_END && !is_last) {\n+\t\t} else if (!is_last && (error.code == YYJSON_READ_ERROR_UNEXPECTED_END || remaining - error.pos < END_BOUND)) {\n \t\t\t// Copy remaining to reconstruct_buffer\n \t\t\tconst auto reconstruct_ptr = reconstruct_buffer.get();\n \t\t\tmemcpy(reconstruct_ptr, obj_copy_start, remaining);\ndiff --git a/src/common/types.cpp b/src/common/types.cpp\nindex 95ff8f5c29a8..22395eb94f68 100644\n--- a/src/common/types.cpp\n+++ b/src/common/types.cpp\n@@ -504,11 +504,32 @@ LogicalType TransformStringToLogicalType(const string &str) {\n \treturn Parser::ParseColumnList(\"dummy \" + str).GetColumn(LogicalIndex(0)).Type();\n }\n \n+LogicalType GetUserTypeRecursive(const LogicalType &type, ClientContext &context) {\n+\tif (type.id() == LogicalTypeId::USER && type.HasAlias()) {\n+\t\treturn Catalog::GetSystemCatalog(context).GetType(context, SYSTEM_CATALOG, DEFAULT_SCHEMA, type.GetAlias());\n+\t}\n+\t// Look for LogicalTypeId::USER in nested types\n+\tif (type.id() == LogicalTypeId::STRUCT) {\n+\t\tchild_list_t<LogicalType> children;\n+\t\tchildren.reserve(StructType::GetChildCount(type));\n+\t\tfor (auto &child : StructType::GetChildTypes(type)) {\n+\t\t\tchildren.emplace_back(child.first, GetUserTypeRecursive(child.second, context));\n+\t\t}\n+\t\treturn LogicalType::STRUCT(std::move(children));\n+\t}\n+\tif (type.id() == LogicalTypeId::LIST) {\n+\t\treturn LogicalType::LIST(GetUserTypeRecursive(ListType::GetChildType(type), context));\n+\t}\n+\tif (type.id() == LogicalTypeId::MAP) {\n+\t\treturn LogicalType::MAP(GetUserTypeRecursive(MapType::KeyType(type), context),\n+\t\t                        GetUserTypeRecursive(MapType::ValueType(type), context));\n+\t}\n+\t// Not LogicalTypeId::USER or a nested type\n+\treturn type;\n+}\n+\n LogicalType TransformStringToLogicalType(const string &str, ClientContext &context) {\n-\tauto type = TransformStringToLogicalType(str);\n-\treturn type.id() == LogicalTypeId::USER\n-\t           ? Catalog::GetSystemCatalog(context).GetType(context, SYSTEM_CATALOG, DEFAULT_SCHEMA, str)\n-\t           : type;\n+\treturn GetUserTypeRecursive(TransformStringToLogicalType(str), context);\n }\n \n bool LogicalType::IsIntegral() const {\n@@ -888,18 +909,23 @@ void LogicalType::SetAlias(string alias) {\n }\n \n string LogicalType::GetAlias() const {\n-\tif (!type_info_) {\n-\t\treturn string();\n-\t} else {\n+\tif (id() == LogicalTypeId::USER) {\n+\t\treturn UserType::GetTypeName(*this);\n+\t}\n+\tif (type_info_) {\n \t\treturn type_info_->alias;\n \t}\n+\treturn string();\n }\n \n bool LogicalType::HasAlias() const {\n-\tif (!type_info_) {\n-\t\treturn false;\n+\tif (id() == LogicalTypeId::USER) {\n+\t\treturn !UserType::GetTypeName(*this).empty();\n+\t}\n+\tif (type_info_ && !type_info_->alias.empty()) {\n+\t\treturn true;\n \t}\n-\treturn !type_info_->alias.empty();\n+\treturn false;\n }\n \n void LogicalType::SetCatalog(LogicalType &type, TypeCatalogEntry *catalog_entry) {\ndiff --git a/src/parser/statement/copy_statement.cpp b/src/parser/statement/copy_statement.cpp\nindex de29e26e7af4..898bc2722ce4 100644\n--- a/src/parser/statement/copy_statement.cpp\n+++ b/src/parser/statement/copy_statement.cpp\n@@ -11,16 +11,6 @@ CopyStatement::CopyStatement(const CopyStatement &other) : SQLStatement(other),\n \t}\n }\n \n-string ConvertOptionValueToString(const Value &val) {\n-\tauto type = val.type().id();\n-\tswitch (type) {\n-\tcase LogicalTypeId::VARCHAR:\n-\t\treturn KeywordHelper::WriteOptionallyQuoted(val.ToString());\n-\tdefault:\n-\t\treturn val.ToString();\n-\t}\n-}\n-\n string CopyStatement::CopyOptionsToString(const string &format,\n                                           const case_insensitive_map_t<vector<Value>> &options) const {\n \tif (format.empty() && options.empty()) {\n@@ -45,15 +35,14 @@ string CopyStatement::CopyOptionsToString(const string &format,\n \t\t\t// Options like HEADER don't need an explicit value\n \t\t\t// just providing the name already sets it to true\n \t\t} else if (values.size() == 1) {\n-\t\t\tresult += ConvertOptionValueToString(values[0]);\n+\t\t\tresult += values[0].ToSQLString();\n \t\t} else {\n \t\t\tresult += \"( \";\n \t\t\tfor (idx_t i = 0; i < values.size(); i++) {\n-\t\t\t\tauto &value = values[i];\n \t\t\t\tif (i) {\n \t\t\t\t\tresult += \", \";\n \t\t\t\t}\n-\t\t\t\tresult += KeywordHelper::WriteOptionallyQuoted(value.ToString());\n+\t\t\t\tresult += values[i].ToSQLString();\n \t\t\t}\n \t\t\tresult += \" )\";\n \t\t}\n",
  "test_patch": "diff --git a/test/sql/json/read_json.test b/test/sql/json/read_json.test\nindex 61e9417c8d08..ca2e235768cc 100644\n--- a/test/sql/json/read_json.test\n+++ b/test/sql/json/read_json.test\n@@ -4,6 +4,9 @@\n \n require json\n \n+statement ok\n+pragma enable_verification\n+\n statement error\n SELECT * FROM read_json('data/json/example_n.ndjson')\n ----\n@@ -123,7 +126,7 @@ SELECT * FROM 'data/json/example_n.ndjson'\n 4\tBroadcast News\n 5\tRaising Arizona\n \n-# we can detect lists too, default depth is 2\n+# we can detect lists too\n query III\n SELECT id, typeof(name), unnest(name) FROM 'data/json/with_list.json'\n ----\n@@ -147,23 +150,23 @@ SELECT id, typeof(name), unnest(name) FROM 'data/json/with_list.json'\n query III\n SELECT id, typeof(name), unnest(name) FROM read_json_auto('data/json/with_list.json', maximum_depth=1)\n ----\n-1\tJSON[]\tO\n-1\tJSON[]\tBrother,\n-1\tJSON[]\tWhere\n-1\tJSON[]\tArt\n-1\tJSON[]\tThou?\n-2\tJSON[]\tHome\n-2\tJSON[]\tfor\n-2\tJSON[]\tthe\n-2\tJSON[]\tHolidays\n-3\tJSON[]\tThe\n-3\tJSON[]\tFirm\n-4\tJSON[]\tBroadcast\n-4\tJSON[]\tNews\n-5\tJSON[]\tRaising\n-5\tJSON[]\tArizona\n-\n-# with depth 1 we don't bother detecting anything, everything defaults to JSON (even the \"id\" column in this case)\n+1\tJSON[]\t\"O\"\n+1\tJSON[]\t\"Brother,\"\n+1\tJSON[]\t\"Where\"\n+1\tJSON[]\t\"Art\"\n+1\tJSON[]\t\"Thou?\"\n+2\tJSON[]\t\"Home\"\n+2\tJSON[]\t\"for\"\n+2\tJSON[]\t\"the\"\n+2\tJSON[]\t\"Holidays\"\n+3\tJSON[]\t\"The\"\n+3\tJSON[]\t\"Firm\"\n+4\tJSON[]\t\"Broadcast\"\n+4\tJSON[]\t\"News\"\n+5\tJSON[]\t\"Raising\"\n+5\tJSON[]\t\"Arizona\"\n+\n+# with depth 0 we don't bother detecting anything, everything defaults to JSON (even the \"id\" column in this case)\n query II\n SELECT typeof(id), typeof(name) FROM read_json_auto('data/json/with_list.json', maximum_depth=0)\n ----\n@@ -183,6 +186,41 @@ d5c52052-5f8e-473f-bc8d-176342643ef5\tUUID\n ae24e69e-e0bf-4e85-9848-27d35df85b8b\tUUID\n 63928b16-1814-436f-8b30-b3c40cc31d51\tUUID\n \n+# top-level array of objects\n+query I\n+select * from read_json('data/json/top_level_array.json', columns={conclusion: 'VARCHAR'}, format='array_of_objects')\n+----\n+cancelled\n+cancelled\n+\n+# test that we can read a list of longer than STANDARD_VECTOR_SIZE properly\n+statement ok\n+copy (select list(to_json({duck: 42})) from range(10000)) to '__TEST_DIR__/my_file.json' (format csv, quote '')\n+\n+query T\n+select count(*) from read_json('__TEST_DIR__/my_file.json', columns={duck: 'INTEGER'}, format='array_of_objects')\n+----\n+10000\n+\n+query T\n+select sum(duck) = 42*10000 from read_json('__TEST_DIR__/my_file.json', columns={duck: 'INTEGER'}, format='array_of_objects')\n+----\n+true\n+\n+# read_json_auto also understands array of objects\n+query T\n+select count(*) from '__TEST_DIR__/my_file.json'\n+----\n+10000\n+\n+query T\n+select sum(duck) = 42*10000 from '__TEST_DIR__/my_file.json'\n+----\n+true\n+\n+statement ok\n+pragma disable_verification\n+\n require httpfs\n \n query II\ndiff --git a/test/sql/json/read_json_auto.test_slow b/test/sql/json/read_json_auto.test_slow\nindex 20e2fffc99aa..916050245341 100644\n--- a/test/sql/json/read_json_auto.test_slow\n+++ b/test/sql/json/read_json_auto.test_slow\n@@ -2,6 +2,9 @@\n # description: Read json files - schema detection\n # group: [json]\n \n+statement ok\n+pragma enable_verification\n+\n require json\n \n # some arrow tests (python/pyarrow/tests/test_json.py) on their github\n@@ -26,6 +29,11 @@ select * from '__TEST_DIR__/my_file.json'\n 2\n 3\n \n+query I\n+select count(*) from '__TEST_DIR__/my_file.json'\n+----\n+3\n+\n statement ok\n copy (select * from (values ('{\"a\": 1,\"b\": 2, \"c\": 3}'), ('{\"a\": 4,\"b\": 5, \"c\": 6}'))) to '__TEST_DIR__/my_file.json' (format csv, quote '')\n \n@@ -76,12 +84,39 @@ select * from '__TEST_DIR__/my_file.json'\n bar\t0\n baz\t1\n \n-# we can read even if the top-level json is not object\n+# we can read objects from a top-level list\n query I\n select * from 'data/json/top_level_array.json'\n ----\n+cancelled\n+cancelled\n+\n+query I\n+select count(*) from 'data/json/top_level_array.json'\n+----\n+2\n+\n+# for maximum_depth=0 this is an array of objects\n+query I\n+select * from read_json_auto('data/json/top_level_array.json', maximum_depth=0)\n+----\n+[{\"conclusion\":\"cancelled\"}, {\"conclusion\":\"cancelled\"}]\n+\n+# for 1 it's 1 column of JSON\n+query I\n+select * from read_json_auto('data/json/top_level_array.json', maximum_depth=1)\n+----\n+\"cancelled\"\n+\"cancelled\"\n+\n+# however, if there are multiple top-level arrays, we default to reading them as lists\n+query I\n+select * from 'data/json/top_level_two_arrays.json'\n+----\n+[{'conclusion': cancelled}, {'conclusion': cancelled}]\n [{'conclusion': cancelled}, {'conclusion': cancelled}]\n \n+\n # issue Mark found when analyzing a JSON dump of our CI - projection pushdown wasn't working properly\n statement ok\n select * from 'data/json/projection_pushdown_example.json' WHERE status <> 'completed'\n@@ -115,7 +150,7 @@ Invalid Input Error: JSON transform error in file \"data/json/inconsistent_schema\n query II\n select * from read_json_auto('data/json/inconsistent_schemas.ndjson', sample_size=2)\n ----\n-1\tNULL\n+\"1\"\tNULL\n 2\tHome for the Holidays\n [3]\tThe Firm\n 4\tBroadcast News\n@@ -131,17 +166,20 @@ select typeof(id) from '__TEST_DIR__/my_file.json'\n BIGINT\n BIGINT\n \n+statement ok\n+pragma disable_verification\n+\n require httpfs\n \n # this is one big object - yyjson uses it as a benchmark\n query II\n-select typeof(\"type\"), typeof(features) from read_json_auto('https://raw.githubusercontent.com/miloyip/nativejson-benchmark/master/data/canada.json', maximum_object_size=4194304, maximum_depth=2);\n+select typeof(\"type\"), typeof(features) from read_json_auto('https://github.com/duckdb/duckdb-data/releases/download/v1.0/canada.json', maximum_object_size=4194304, maximum_depth=2);\n ----\n VARCHAR\tSTRUCT(type JSON, properties JSON, geometry JSON)[]\n \n # let's crank up maximum_depth and see if we can fully unnest this big object\n query II\n-select typeof(\"type\"), typeof(features) from read_json_auto('https://raw.githubusercontent.com/miloyip/nativejson-benchmark/master/data/canada.json', maximum_object_size=4194304, maximum_depth=7);\n+select typeof(\"type\"), typeof(features) from read_json_auto('https://github.com/duckdb/duckdb-data/releases/download/v1.0/canada.json', maximum_object_size=4194304, maximum_depth=7);\n ----\n VARCHAR\tSTRUCT(type VARCHAR, properties STRUCT(name VARCHAR), geometry STRUCT(type VARCHAR, coordinates DOUBLE[][][]))[]\n \n@@ -150,6 +188,6 @@ VARCHAR\tSTRUCT(type VARCHAR, properties STRUCT(name VARCHAR), geometry STRUCT(ty\n # the \"coordinates\" array in \"features.geometry\" is huge, let's just check the length - not all the values\n query IIIII\n select type, features[1].type, features[1].properties.name, features[1].geometry.type, length(features[1].geometry.coordinates)\n-from read_json_auto('https://raw.githubusercontent.com/miloyip/nativejson-benchmark/master/data/canada.json', maximum_object_size=4194304, maximum_depth=7);\n+from read_json_auto('https://github.com/duckdb/duckdb-data/releases/download/v1.0/canada.json', maximum_object_size=4194304, maximum_depth=7);\n ----\n FeatureCollection\tFeature\tCanada\tPolygon\t480\ndiff --git a/test/sql/json/read_json_dates.test b/test/sql/json/read_json_dates.test\nindex 993a4260f798..25229640dc6c 100644\n--- a/test/sql/json/read_json_dates.test\n+++ b/test/sql/json/read_json_dates.test\n@@ -2,6 +2,9 @@\n # description: Read json files - date detection\n # group: [json]\n \n+statement ok\n+pragma enable_verification\n+\n require json\n \n # create date and timestamp tables\ndiff --git a/test/sql/json/read_json_many_files.test_slow b/test/sql/json/read_json_many_files.test_slow\nnew file mode 100644\nindex 000000000000..3aa3b28aaa40\n--- /dev/null\n+++ b/test/sql/json/read_json_many_files.test_slow\n@@ -0,0 +1,23 @@\n+# name: test/sql/json/read_json_many_files.test_slow\n+# description: Read > 1000 json files (issue #6249)\n+# group: [json]\n+\n+require json\n+\n+statement ok\n+create table input as select 'Laurens' as name;\n+\n+loop i 0 2000\n+\n+statement ok\n+copy input to '__TEST_DIR__/input${i}.json';\n+\n+endloop\n+\n+statement ok\n+pragma threads=1\n+\n+query T\n+select count(*) from read_json_auto('__TEST_DIR__/input*.json');\n+----\n+2000\n",
  "problem_statement": "read_json_auto: duckdb.IOException: IO Error: Cannot open file \"xxx\": Too many open files\n### What happens?\n\nWhen reading about 1000 json files, I recieve the following error:\r\n\r\n    read_json_auto: duckdb.IOException: IO Error: Cannot open file \"xxx\": Too many open files\r\n\r\nThe same works when reading 1000 csv files (see example below).\n\n### To Reproduce\n\n```python\r\nimport shutil\r\nfrom pathlib import Path\r\n\r\nimport duckdb\r\nimport pandas as pd\r\n\r\nif __name__ == \"__main__\":\r\n    mode = \"json\"  # \"json\", \"csv\"\r\n\r\n    data_dir = Path(\"data\")\r\n    if data_dir.exists():\r\n        shutil.rmtree(data_dir)\r\n    data_dir.mkdir(exist_ok=True)\r\n\r\n    for i in range(1000):\r\n        df = pd.DataFrame({\"a\": [1, 2, 3]})\r\n\r\n        if mode == \"json\":\r\n            df.to_json(data_dir / f\"file_{i}.json\", lines=True, orient=\"records\")\r\n        else:\r\n            df.to_csv(data_dir / f\"file_{i}.csv\")\r\n\r\n    data = duckdb.sql(f\"SELECT COUNT(*) FROM read_{mode}_auto('{data_dir}/*.{mode}');\")\r\n\r\n    print(data.fetchone())\r\n```\r\nWhen setting `mode=\"csv\"`, the code works as expected.\n\n### OS:\n\nmacOS\n\n### DuckDB Version:\n\n0.7.0\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nTimon Schmelzer\n\n### Affiliation:\n\ngrandcentrix\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "",
  "created_at": "2023-02-14T15:06:30Z"
}