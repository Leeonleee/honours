{
  "repo": "duckdb/duckdb",
  "pull_number": 7112,
  "instance_id": "duckdb__duckdb-7112",
  "issue_numbers": [
    "7083",
    "7083"
  ],
  "base_commit": "544ca6d4fa2bc9451c07b514309b1fdf8a05e574",
  "patch": "diff --git a/extension/parquet/column_reader.cpp b/extension/parquet/column_reader.cpp\nindex faa1705b1f6a..c58a6cba9014 100644\n--- a/extension/parquet/column_reader.cpp\n+++ b/extension/parquet/column_reader.cpp\n@@ -647,6 +647,7 @@ void StringColumnReader::PrepareDeltaLengthByteArray(ResizeableBuffer &buffer) {\n \tauto length_data = (uint32_t *)length_buffer->ptr;\n \tbyte_array_data = make_uniq<Vector>(LogicalType::VARCHAR, value_count);\n \tbyte_array_count = value_count;\n+\tdelta_offset = 0;\n \tauto string_data = FlatVector::GetData<string_t>(*byte_array_data);\n \tfor (idx_t i = 0; i < value_count; i++) {\n \t\tauto str_len = length_data[i];\n@@ -674,6 +675,7 @@ void StringColumnReader::PrepareDeltaByteArray(ResizeableBuffer &buffer) {\n \tauto suffix_data = (uint32_t *)suffix_buffer->ptr;\n \tbyte_array_data = make_uniq<Vector>(LogicalType::VARCHAR, prefix_count);\n \tbyte_array_count = prefix_count;\n+\tdelta_offset = 0;\n \tauto string_data = FlatVector::GetData<string_t>(*byte_array_data);\n \tfor (idx_t i = 0; i < prefix_count; i++) {\n \t\tauto str_len = prefix_data[i] + suffix_data[i];\n@@ -715,6 +717,7 @@ void StringColumnReader::DeltaByteArray(uint8_t *defines, idx_t num_values, parq\n \t\t\tdelta_offset++;\n \t\t}\n \t}\n+\tStringVector::AddHeapReference(result, *byte_array_data);\n }\n \n class ParquetStringVectorBuffer : public VectorBuffer {\n",
  "test_patch": "diff --git a/test/sql/copy/parquet/delta_byte_array_length_mismatch.test b/test/sql/copy/parquet/delta_byte_array_length_mismatch.test\nindex 464c6eb27808..320c76f5ce31 100644\n--- a/test/sql/copy/parquet/delta_byte_array_length_mismatch.test\n+++ b/test/sql/copy/parquet/delta_byte_array_length_mismatch.test\n@@ -6,7 +6,5 @@ require parquet\n \n require httpfs\n \n-statement error\n+statement ok\n SELECT * FROM parquet_scan('https://github.com/duckdb/duckdb-data/releases/download/v1.0/delta_byte_array_length_mismatch.parquet')\n-----\n-length mismatch\ndiff --git a/test/sql/copy/parquet/delta_byte_array_multiple_pages.test b/test/sql/copy/parquet/delta_byte_array_multiple_pages.test\nnew file mode 100644\nindex 000000000000..0e04ca58c09d\n--- /dev/null\n+++ b/test/sql/copy/parquet/delta_byte_array_multiple_pages.test\n@@ -0,0 +1,23 @@\n+# name: test/sql/copy/parquet/delta_byte_array_multiple_pages.test\n+# description: Test delta byte array parquet file with multiple pages\n+# group: [parquet]\n+\n+require parquet\n+\n+require httpfs\n+\n+statement ok\n+CREATE TABLE delta_byte_array AS SELECT * FROM parquet_scan('https://github.com/duckdb/duckdb-data/releases/download/v1.0/delta_byte_array_multiple_pages.parquet')\n+\n+query I\n+SELECT COUNT(*) FROM delta_byte_array\n+----\n+100000\n+\n+query II\n+SELECT min(strlen(json_column)), max(strlen(json_column)) FROM delta_byte_array\n+----\n+54\t54\n+\n+\n+\n",
  "problem_statement": "IO Error when reading Parquet file that works fine on Pandas.\n### What happens?\n\nI'm transcoding some public data dumps from StackExchange from XML to Parquet using Rust's `parquet` crate (`v37.0.0`) from the official `arrow-rs` repository. Some of my tables work fine with DuckDB but others don't and the reason for that is not very clear, especially since all of them work (or seem to work) on Pandas.\r\n\r\nThe error is the following, and on all Parquet's where this happens it's an off-by-one as is this one:\r\n```error\r\nError: IO Error: DELTA_BYTE_ARRAY - length mismatch between values and byte array lengths\r\n(attempted read of 14337 from 14336 entries) - corrupt file?\r\n```\n\n### To Reproduce\n\n1. `CREATE VIEW users AS SELECT * FROM read_parquet('users.parquet');`\r\n2. `SUMMARIZE SELECT * FROM users;`\r\n\r\nIf you give me some way of sending the file over to you I can definitely do that, given that this is all information from public data dumps anyways. It's a 550MB download (the smallest of my tables I can repro this on).\r\n\r\nI've also attached my code that does the transcoding, it's in the ZIP file below and `cargo build --release` on the latest Rust stable should build it just fine. In the ZIP there are also some sample files for which the transcoding works fine and DuckDB will parse the output Parquet without issue.\r\n\r\nTo transcode something, run the following (if there's a parsing error try adding `--bom` as well):\r\n```bash\r\ncargo run -- convert xml --schema=tags.dumparq --output=tags.parquet --input=Tags.xml --pluck=row\r\n```\r\n\r\n[transcoding.zip](https://github.com/duckdb/duckdb/files/11225626/transcoding.zip)\n\n### OS:\n\nLinux amd64\n\n### DuckDB Version:\n\nLatest Stable but also validated against the master nightly build\n\n### DuckDB Client:\n\nCLI\n\n### Full Name:\n\nLuiz Berti\n\n### Affiliation:\n\nUnaffiliated\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\nIO Error when reading Parquet file that works fine on Pandas.\n### What happens?\n\nI'm transcoding some public data dumps from StackExchange from XML to Parquet using Rust's `parquet` crate (`v37.0.0`) from the official `arrow-rs` repository. Some of my tables work fine with DuckDB but others don't and the reason for that is not very clear, especially since all of them work (or seem to work) on Pandas.\r\n\r\nThe error is the following, and on all Parquet's where this happens it's an off-by-one as is this one:\r\n```error\r\nError: IO Error: DELTA_BYTE_ARRAY - length mismatch between values and byte array lengths\r\n(attempted read of 14337 from 14336 entries) - corrupt file?\r\n```\n\n### To Reproduce\n\n1. `CREATE VIEW users AS SELECT * FROM read_parquet('users.parquet');`\r\n2. `SUMMARIZE SELECT * FROM users;`\r\n\r\nIf you give me some way of sending the file over to you I can definitely do that, given that this is all information from public data dumps anyways. It's a 550MB download (the smallest of my tables I can repro this on).\r\n\r\nI've also attached my code that does the transcoding, it's in the ZIP file below and `cargo build --release` on the latest Rust stable should build it just fine. In the ZIP there are also some sample files for which the transcoding works fine and DuckDB will parse the output Parquet without issue.\r\n\r\nTo transcode something, run the following (if there's a parsing error try adding `--bom` as well):\r\n```bash\r\ncargo run -- convert xml --schema=tags.dumparq --output=tags.parquet --input=Tags.xml --pluck=row\r\n```\r\n\r\n[transcoding.zip](https://github.com/duckdb/duckdb/files/11225626/transcoding.zip)\n\n### OS:\n\nLinux amd64\n\n### DuckDB Version:\n\nLatest Stable but also validated against the master nightly build\n\n### DuckDB Client:\n\nCLI\n\n### Full Name:\n\nLuiz Berti\n\n### Affiliation:\n\nUnaffiliated\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "I was just about to log something for this :) - the error message comes from https://github.com/duckdb/duckdb/pull/6412 so possibly related? I'm able to reproduce with a string column (binary / byte array) that spans multiple pages with `DELTA_BYTE_ARRAY` encoding.\r\n\r\nGiven the query\r\n```\r\nselect * from '<the_parquet_file>'\r\n```\r\n\r\n[10k_rows_single_page.parquet](https://github.com/ryanrupp/duckdb/blob/parquet-delta-byte-array-encoding-issue/10k_rows_single_page.parquet) ==> works\r\n[100k_rows_multiple_pages.parquet](https://github.com/ryanrupp/duckdb/blob/parquet-delta-byte-array-encoding-issue/100k_rows_multiple_pages.parquet) ==> `Error: IO Error: DELTA_BYTE_ARRAY - length mismatch between values and byte array lengths (attempted read of 16301 from 16300 entries) - corrupt file?`\r\n\r\nUsing `parquet` CLI\r\n\r\n```\r\nparquet pages 100k_rows_multiple_pages.parquet \r\n\r\nColumn: row_number\r\n--------------------------------------------------------------------------------\r\n  page   type  enc  count   avg size   size       rows     nulls   min / max\r\n  0-0    data  _ _  100000  8.00 B     781.254 kB 100000   0       \"1.0\" / \"100000.0\"\r\n\r\n\r\nColumn: json_column\r\n--------------------------------------------------------------------------------\r\n  page   type  enc  count   avg size   size       rows     nulls   min / max\r\n  0-0    data  _ D  16300   38.53 B    613.261 kB 16300    0       \r\n  0-1    data  _ D  16300   38.53 B    613.276 kB 16300    0       \r\n  0-2    data  _ D  16300   38.53 B    613.267 kB 16300    0       \r\n  0-3    data  _ D  16300   38.53 B    613.339 kB 16300    0       \r\n  0-4    data  _ D  16300   38.52 B    613.187 kB 16300    0       \r\n  0-5    data  _ D  16300   38.53 B    613.369 kB 16300    0       \r\n  0-6    data  _ D  2200    38.55 B    82.830 kB  2200     0       \r\n```\r\n\r\nvs the single page version that works\r\n\r\n```\r\nColumn: row_number\r\n--------------------------------------------------------------------------------\r\n  page   type  enc  count   avg size   size       rows     nulls   min / max\r\n  0-0    data  _ _  10000   8.00 B     78.129 kB  10000    0       \"1.0\" / \"10000.0\"\r\n\r\n\r\nColumn: json_column\r\n--------------------------------------------------------------------------------\r\n  page   type  enc  count   avg size   size       rows     nulls   min / max\r\n  0-0    data  _ D  10000   38.53 B    376.225 kB 10000    0    \r\n```\r\n\r\nAlso you can avoid hitting this by keeping under that page number to an extent (maybe before fetching the next page triggers?) i.e.\r\n\r\n```\r\n-- works\r\nselect * from '100k_rows_multiple_pages.parquet' limit 14336;\r\n\r\n-- fails with: Error: IO Error: DELTA_BYTE_ARRAY - length mismatch between values and byte array lengths (attempted read of 17139 from 17138 entries) - corrupt file?\r\nselect * from '100k_rows_multiple_pages.parquet' limit 14337;\r\n```\r\n\r\nThese files were generated `parquet-mr` `1.12.3` and can be read by various other query engines.\r\n\nI was just about to log something for this :) - the error message comes from https://github.com/duckdb/duckdb/pull/6412 so possibly related? I'm able to reproduce with a string column (binary / byte array) that spans multiple pages with `DELTA_BYTE_ARRAY` encoding.\r\n\r\nGiven the query\r\n```\r\nselect * from '<the_parquet_file>'\r\n```\r\n\r\n[10k_rows_single_page.parquet](https://github.com/ryanrupp/duckdb/blob/parquet-delta-byte-array-encoding-issue/10k_rows_single_page.parquet) ==> works\r\n[100k_rows_multiple_pages.parquet](https://github.com/ryanrupp/duckdb/blob/parquet-delta-byte-array-encoding-issue/100k_rows_multiple_pages.parquet) ==> `Error: IO Error: DELTA_BYTE_ARRAY - length mismatch between values and byte array lengths (attempted read of 16301 from 16300 entries) - corrupt file?`\r\n\r\nUsing `parquet` CLI\r\n\r\n```\r\nparquet pages 100k_rows_multiple_pages.parquet \r\n\r\nColumn: row_number\r\n--------------------------------------------------------------------------------\r\n  page   type  enc  count   avg size   size       rows     nulls   min / max\r\n  0-0    data  _ _  100000  8.00 B     781.254 kB 100000   0       \"1.0\" / \"100000.0\"\r\n\r\n\r\nColumn: json_column\r\n--------------------------------------------------------------------------------\r\n  page   type  enc  count   avg size   size       rows     nulls   min / max\r\n  0-0    data  _ D  16300   38.53 B    613.261 kB 16300    0       \r\n  0-1    data  _ D  16300   38.53 B    613.276 kB 16300    0       \r\n  0-2    data  _ D  16300   38.53 B    613.267 kB 16300    0       \r\n  0-3    data  _ D  16300   38.53 B    613.339 kB 16300    0       \r\n  0-4    data  _ D  16300   38.52 B    613.187 kB 16300    0       \r\n  0-5    data  _ D  16300   38.53 B    613.369 kB 16300    0       \r\n  0-6    data  _ D  2200    38.55 B    82.830 kB  2200     0       \r\n```\r\n\r\nvs the single page version that works\r\n\r\n```\r\nColumn: row_number\r\n--------------------------------------------------------------------------------\r\n  page   type  enc  count   avg size   size       rows     nulls   min / max\r\n  0-0    data  _ _  10000   8.00 B     78.129 kB  10000    0       \"1.0\" / \"10000.0\"\r\n\r\n\r\nColumn: json_column\r\n--------------------------------------------------------------------------------\r\n  page   type  enc  count   avg size   size       rows     nulls   min / max\r\n  0-0    data  _ D  10000   38.53 B    376.225 kB 10000    0    \r\n```\r\n\r\nAlso you can avoid hitting this by keeping under that page number to an extent (maybe before fetching the next page triggers?) i.e.\r\n\r\n```\r\n-- works\r\nselect * from '100k_rows_multiple_pages.parquet' limit 14336;\r\n\r\n-- fails with: Error: IO Error: DELTA_BYTE_ARRAY - length mismatch between values and byte array lengths (attempted read of 17139 from 17138 entries) - corrupt file?\r\nselect * from '100k_rows_multiple_pages.parquet' limit 14337;\r\n```\r\n\r\nThese files were generated `parquet-mr` `1.12.3` and can be read by various other query engines.\r\n",
  "created_at": "2023-04-17T11:41:48Z"
}