You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Case insensitivity for header names in read_csv_auto
Currently the header names in read_csv_auto are read as-is, which means case is preserved.  For example, consider this CSV file:

**test.csv**
```
FlightDate|UniqueCarrier|OriginCityName|DestCityName
1988-01-01|AA|New York, NY|Los Angeles, CA
1988-01-02|AA|New York, NY|Los Angeles, CA
1988-01-03|AA|New York, NY|Los Angeles, CA
```

The file is correctly loaded:
```sql
SELECT * FROM read_csv_auto('test.csv');
```

|FlightDate|UniqueCarrier| OriginCityName  | DestCityName  |
|---------:|------------:|----------------:|--------------:|
|1988-01-01|AA           |New York, NY     |Los Angeles, CA|
|1988-01-02|AA           |New York, NY     |Los Angeles, CA|
|1988-01-03|AA           |New York, NY     |Los Angeles, CA|

However, the names must be quoted to use them in the query:

```sql
SELECT flightdate FROM read_csv_auto('test.csv');
-- error!
SELECT FlightDate FROM read_csv_auto('test.csv');
-- error!
SELECT "FlightDate" FROM read_csv_auto('test.csv');
-- correct
```

I think we should lowercase them before loading so the behavior is more "as expected".


</issue>
<code>
[start of README.md]
1: <img align="left" src="logo/duckdb-logo.png" height="120">
2: 
3: # DuckDB, the SQLite for Analytics
4: [![Travis](https://api.travis-ci.org/cwida/duckdb.svg?branch=master)](https://travis-ci.org/cwida/duckdb)
5: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
6: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
7: 
8: <br>
9: 
10: 
11: # Requirements
12: DuckDB requires [CMake](https://cmake.org) to be installed and a `C++11` compliant compiler. GCC 4.9 and newer, Clang 3.9 and newer and VisualStudio 2017 are tested on each revision.
13: 
14: ## Compiling
15: Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You may run `make unit` and `make allunit` to verify that your version works properly after making changes.
16: 
17: # Usage
18: A command line utility based on `sqlite3` can be found in either `build/release/duckdb` (release, the default) or `build/debug/duckdb` (debug).
19: 
20: # Embedding
21: As DuckDB is an embedded database, there is no database server to launch or client to connect to a running server. However, the database server can be embedded directly into an application using the C or C++ bindings. The main build process creates the shared library `build/release/src/libduckdb.[so|dylib|dll]` that can be linked against. A static library is built as well.
22: 
23: For examples on how to embed DuckDB into your application, see the [examples](https://github.com/cwida/duckdb/tree/master/examples) folder.
24: 
25: ## Benchmarks
26: After compiling, benchmarks can be executed from the root directory by executing `./build/release/benchmark/benchmark_runner`.
27: 
28: ## Standing on the Shoulders of Giants
29: DuckDB is implemented in C++ 11, should compile with GCC and clang, uses CMake to build and [Catch2](https://github.com/catchorg/Catch2) for testing. DuckDB uses some components from various Open-Source databases and draws inspiration from scientific publications. Here is an overview:
30: 
31: * Parser: We use the PostgreSQL parser that was [repackaged as a stand-alone library](https://github.com/lfittl/libpg_query). The translation to our own parse tree is inspired by [Peloton](https://pelotondb.io).
32: * Shell: We have adapted the [SQLite shell](https://sqlite.org/cli.html) to work with DuckDB.
33: * Tests: We use the [SQL Logic Tests from SQLite](https://www.sqlite.org/sqllogictest/doc/trunk/about.wiki) to test DuckDB.
34: * Query fuzzing: We use [SQLsmith](https://github.com/anse1/sqlsmith) to generate random queries for additional testing.
35: * Date Math: We use the date math component from [MonetDB](https://www.monetdb.org).
36: * SQL Window Functions: DuckDB's window functions implementation uses Segment Tree Aggregation as described in the paper "Efficient Processing of Window Functions in Analytical SQL Queries" by Viktor Leis, Kan Kundhikanjana, Alfons Kemper and Thomas Neumann.
37: * Execution engine: The vectorized execution engine is inspired by the paper "MonetDB/X100: Hyper-Pipelining Query Execution" by Peter Boncz, Marcin Zukowski and Niels Nes.
38: * Optimizer: DuckDB's optimizer draws inspiration from the papers "Dynamic programming strikes back" by Guido Moerkotte and Thomas Neumman as well as "Unnesting Arbitrary Queries" by Thomas Neumann and Alfons Kemper.
39: * Concurrency control: Our MVCC implementation is inspired by the paper "Fast Serializable Multi-Version Concurrency Control for Main-Memory Database Systems" by Thomas Neumann, Tobias Mühlbauer and Alfons Kemper.
40: * Regular Expression: DuckDB uses Google's [RE2](https://github.com/google/re2) regular expression engine.
41: 
42: ## Other pages
43: * [Continuous Benchmarking (CB™)](https://www.duckdb.org/benchmarks/index.html), runs TPC-H, TPC-DS and some microbenchmarks on every commit
[end of README.md]
[start of src/catalog/catalog_entry/table_catalog_entry.cpp]
1: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/serializer.hpp"
7: #include "duckdb/main/connection.hpp"
8: #include "duckdb/main/database.hpp"
9: #include "duckdb/parser/constraints/list.hpp"
10: #include "duckdb/parser/parsed_data/alter_table_info.hpp"
11: #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
12: #include "duckdb/planner/constraints/bound_unique_constraint.hpp"
13: #include "duckdb/planner/constraints/bound_check_constraint.hpp"
14: #include "duckdb/planner/expression/bound_constant_expression.hpp"
15: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
16: #include "duckdb/storage/storage_manager.hpp"
17: #include "duckdb/planner/binder.hpp"
18: 
19: #include "duckdb/execution/index/art/art.hpp"
20: #include "duckdb/parser/expression/columnref_expression.hpp"
21: #include "duckdb/planner/expression/bound_reference_expression.hpp"
22: #include "duckdb/parser/parsed_expression_iterator.hpp"
23: #include "duckdb/planner/expression_binder/alter_binder.hpp"
24: 
25: #include <algorithm>
26: 
27: using namespace duckdb;
28: using namespace std;
29: 
30: TableCatalogEntry::TableCatalogEntry(Catalog *catalog, SchemaCatalogEntry *schema, BoundCreateTableInfo *info,
31:                                      std::shared_ptr<DataTable> inherited_storage)
32:     : StandardEntry(CatalogType::TABLE, schema, catalog, info->Base().table), storage(inherited_storage),
33:       columns(move(info->Base().columns)), constraints(move(info->Base().constraints)),
34:       bound_constraints(move(info->bound_constraints)), name_map(info->name_map) {
35: 	this->temporary = info->Base().temporary;
36: 	// add the "rowid" alias, if there is no rowid column specified in the table
37: 	if (name_map.find("rowid") == name_map.end()) {
38: 		name_map["rowid"] = COLUMN_IDENTIFIER_ROW_ID;
39: 	}
40: 	if (!storage) {
41: 		// create the physical storage
42: 		storage = make_shared<DataTable>(catalog->storage, schema->name, name, GetTypes(), move(info->data));
43: 
44: 		// create the unique indexes for the UNIQUE and PRIMARY KEY constraints
45: 		for (idx_t i = 0; i < bound_constraints.size(); i++) {
46: 			auto &constraint = bound_constraints[i];
47: 			if (constraint->type == ConstraintType::UNIQUE) {
48: 				// unique constraint: create a unique index
49: 				auto &unique = (BoundUniqueConstraint &)*constraint;
50: 				// fetch types and create expressions for the index from the columns
51: 				vector<column_t> column_ids;
52: 				vector<unique_ptr<Expression>> unbound_expressions;
53: 				vector<unique_ptr<Expression>> bound_expressions;
54: 				idx_t key_nr = 0;
55: 				for (auto &key : unique.keys) {
56: 					TypeId column_type = GetInternalType(columns[key].type);
57: 					assert(key < columns.size());
58: 
59: 					unbound_expressions.push_back(
60: 					    make_unique<BoundColumnRefExpression>(column_type, ColumnBinding(0, column_ids.size())));
61: 					bound_expressions.push_back(make_unique<BoundReferenceExpression>(column_type, key_nr++));
62: 					column_ids.push_back(key);
63: 				}
64: 				// create an adaptive radix tree around the expressions
65: 				auto art = make_unique<ART>(column_ids, move(unbound_expressions), true);
66: 				storage->AddIndex(move(art), bound_expressions);
67: 			}
68: 		}
69: 	}
70: }
71: 
72: bool TableCatalogEntry::ColumnExists(const string &name) {
73: 	return name_map.find(name) != name_map.end();
74: }
75: 
76: unique_ptr<CatalogEntry> TableCatalogEntry::AlterEntry(ClientContext &context, AlterInfo *info) {
77: 	if (info->type != AlterType::ALTER_TABLE) {
78: 		throw CatalogException("Can only modify table with ALTER TABLE statement");
79: 	}
80: 	auto table_info = (AlterTableInfo *)info;
81: 	switch (table_info->alter_table_type) {
82: 	case AlterTableType::RENAME_COLUMN: {
83: 		auto rename_info = (RenameColumnInfo *)table_info;
84: 		return RenameColumn(context, *rename_info);
85: 	}
86: 	case AlterTableType::RENAME_TABLE: {
87: 		auto rename_info = (RenameTableInfo *)table_info;
88: 		auto copied_table = Copy(context);
89: 		copied_table->name = rename_info->new_table_name;
90: 		return copied_table;
91: 	}
92: 	case AlterTableType::ADD_COLUMN: {
93: 		auto add_info = (AddColumnInfo *)table_info;
94: 		return AddColumn(context, *add_info);
95: 	}
96: 	case AlterTableType::REMOVE_COLUMN: {
97: 		auto remove_info = (RemoveColumnInfo *)table_info;
98: 		return RemoveColumn(context, *remove_info);
99: 	}
100: 	case AlterTableType::SET_DEFAULT: {
101: 		auto set_default_info = (SetDefaultInfo *)table_info;
102: 		return SetDefault(context, *set_default_info);
103: 	}
104: 	case AlterTableType::ALTER_COLUMN_TYPE: {
105: 		auto change_type_info = (ChangeColumnTypeInfo *)table_info;
106: 		return ChangeColumnType(context, *change_type_info);
107: 	}
108: 	default:
109: 		throw InternalException("Unrecognized alter table type!");
110: 	}
111: }
112: 
113: static void RenameExpression(ParsedExpression &expr, RenameColumnInfo &info) {
114: 	if (expr.type == ExpressionType::COLUMN_REF) {
115: 		auto &colref = (ColumnRefExpression &)expr;
116: 		if (colref.column_name == info.name) {
117: 			colref.column_name = info.new_name;
118: 		}
119: 	}
120: 	ParsedExpressionIterator::EnumerateChildren(
121: 	    expr, [&](const ParsedExpression &child) { RenameExpression((ParsedExpression &)child, info); });
122: }
123: 
124: unique_ptr<CatalogEntry> TableCatalogEntry::RenameColumn(ClientContext &context, RenameColumnInfo &info) {
125: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
126: 	create_info->temporary = temporary;
127: 	bool found = false;
128: 	for (idx_t i = 0; i < columns.size(); i++) {
129: 		ColumnDefinition copy = columns[i].Copy();
130: 
131: 		create_info->columns.push_back(move(copy));
132: 		if (info.name == columns[i].name) {
133: 			assert(!found);
134: 			create_info->columns[i].name = info.new_name;
135: 			found = true;
136: 		}
137: 	}
138: 	if (!found) {
139: 		throw CatalogException("Table does not have a column with name \"%s\"", info.name.c_str());
140: 	}
141: 	for (idx_t c_idx = 0; c_idx < constraints.size(); c_idx++) {
142: 		auto copy = constraints[c_idx]->Copy();
143: 		switch (copy->type) {
144: 		case ConstraintType::NOT_NULL:
145: 			// NOT NULL constraint: no adjustments necessary
146: 			break;
147: 		case ConstraintType::CHECK: {
148: 			// CHECK constraint: need to rename column references that refer to the renamed column
149: 			auto &check = (CheckConstraint &)*copy;
150: 			RenameExpression(*check.expression, info);
151: 			break;
152: 		}
153: 		case ConstraintType::UNIQUE: {
154: 			// UNIQUE constraint: possibly need to rename columns
155: 			auto &unique = (UniqueConstraint &)*copy;
156: 			for (idx_t i = 0; i < unique.columns.size(); i++) {
157: 				if (unique.columns[i] == info.name) {
158: 					unique.columns[i] = info.new_name;
159: 				}
160: 			}
161: 			break;
162: 		}
163: 		default:
164: 			throw CatalogException("Unsupported constraint for entry!");
165: 		}
166: 		create_info->constraints.push_back(move(copy));
167: 	}
168: 	Binder binder(context);
169: 	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
170: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
171: }
172: 
173: unique_ptr<CatalogEntry> TableCatalogEntry::AddColumn(ClientContext &context, AddColumnInfo &info) {
174: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
175: 	create_info->temporary = temporary;
176: 	for (idx_t i = 0; i < columns.size(); i++) {
177: 		create_info->columns.push_back(columns[i].Copy());
178: 	}
179: 	info.new_column.oid = columns.size();
180: 	create_info->columns.push_back(info.new_column.Copy());
181: 
182: 	Binder binder(context);
183: 	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
184: 	auto new_storage =
185: 	    make_shared<DataTable>(context, *storage, info.new_column, bound_create_info->bound_defaults.back().get());
186: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
187: 	                                      new_storage);
188: }
189: 
190: unique_ptr<CatalogEntry> TableCatalogEntry::RemoveColumn(ClientContext &context, RemoveColumnInfo &info) {
191: 	idx_t removed_index = INVALID_INDEX;
192: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
193: 	create_info->temporary = temporary;
194: 	for (idx_t i = 0; i < columns.size(); i++) {
195: 		if (columns[i].name == info.removed_column) {
196: 			assert(removed_index == INVALID_INDEX);
197: 			removed_index = i;
198: 			continue;
199: 		}
200: 		create_info->columns.push_back(columns[i].Copy());
201: 	}
202: 	if (removed_index == INVALID_INDEX) {
203: 		if (!info.if_exists) {
204: 			throw CatalogException("Table does not have a column with name \"%s\"", info.removed_column.c_str());
205: 		}
206: 		return nullptr;
207: 	}
208: 	if (create_info->columns.size() == 0) {
209: 		throw CatalogException("Cannot drop column: table only has one column remaining!");
210: 	}
211: 	// handle constraints for the new table
212: 	assert(constraints.size() == bound_constraints.size());
213: 	for (idx_t constr_idx = 0; constr_idx < constraints.size(); constr_idx++) {
214: 		auto &constraint = constraints[constr_idx];
215: 		auto &bound_constraint = bound_constraints[constr_idx];
216: 		switch (bound_constraint->type) {
217: 		case ConstraintType::NOT_NULL: {
218: 			auto &not_null_constraint = (BoundNotNullConstraint &)*bound_constraint;
219: 			if (not_null_constraint.index != removed_index) {
220: 				// the constraint is not about this column: we need to copy it
221: 				// we might need to shift the index back by one though, to account for the removed column
222: 				idx_t new_index = not_null_constraint.index;
223: 				if (not_null_constraint.index > removed_index) {
224: 					new_index -= 1;
225: 				}
226: 				create_info->constraints.push_back(make_unique<NotNullConstraint>(new_index));
227: 			}
228: 			break;
229: 		}
230: 		case ConstraintType::CHECK: {
231: 			// CHECK constraint
232: 			auto &bound_check = (BoundCheckConstraint &)*bound_constraint;
233: 			// check if the removed column is part of the check constraint
234: 			if (bound_check.bound_columns.find(removed_index) != bound_check.bound_columns.end()) {
235: 				if (bound_check.bound_columns.size() > 1) {
236: 					// CHECK constraint that concerns mult
237: 					throw CatalogException(
238: 					    "Cannot drop column \"%s\" because there is a CHECK constraint that depends on it",
239: 					    info.removed_column.c_str());
240: 				} else {
241: 					// CHECK constraint that ONLY concerns this column, strip the constraint
242: 				}
243: 			} else {
244: 				// check constraint does not concern the removed column: simply re-add it
245: 				create_info->constraints.push_back(constraint->Copy());
246: 			}
247: 			break;
248: 		}
249: 		case ConstraintType::UNIQUE: {
250: 			auto copy = constraint->Copy();
251: 			auto &unique = (UniqueConstraint &) *copy;
252: 			if (unique.index != INVALID_INDEX) {
253: 				if (unique.index == removed_index) {
254: 					throw CatalogException(
255: 					    "Cannot drop column \"%s\" because there is a UNIQUE constraint that depends on it",
256: 					    info.removed_column.c_str());
257: 				} else if (unique.index > removed_index) {
258: 					unique.index--;
259: 				}
260: 			}
261: 			create_info->constraints.push_back(move(copy));
262: 			break;
263: 		}
264: 		default:
265: 			throw InternalException("Unsupported constraint for entry!");
266: 		}
267: 	}
268: 
269: 	Binder binder(context);
270: 	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
271: 	auto new_storage = make_shared<DataTable>(context, *storage, removed_index);
272: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
273: 	                                      new_storage);
274: }
275: 
276: unique_ptr<CatalogEntry> TableCatalogEntry::SetDefault(ClientContext &context, SetDefaultInfo &info) {
277: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
278: 	bool found = false;
279: 	for (idx_t i = 0; i < columns.size(); i++) {
280: 		auto copy = columns[i].Copy();
281: 		if (info.column_name == copy.name) {
282: 			// set the default value of this column
283: 			copy.default_value = info.expression ? info.expression->Copy() : nullptr;
284: 			found = true;
285: 		}
286: 		create_info->columns.push_back(move(copy));
287: 	}
288: 	if (!found) {
289: 		throw BinderException("Table \"%s\" does not have a column with name \"%s\"", info.table.c_str(),
290: 		                      info.column_name.c_str());
291: 	}
292: 
293: 	for (idx_t i = 0; i < constraints.size(); i++) {
294: 		auto constraint = constraints[i]->Copy();
295: 		create_info->constraints.push_back(move(constraint));
296: 	}
297: 
298: 	Binder binder(context);
299: 	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
300: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
301: }
302: 
303: unique_ptr<CatalogEntry> TableCatalogEntry::ChangeColumnType(ClientContext &context, ChangeColumnTypeInfo &info) {
304: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
305: 	idx_t change_idx = INVALID_INDEX;
306: 	for (idx_t i = 0; i < columns.size(); i++) {
307: 		auto copy = columns[i].Copy();
308: 		if (info.column_name == copy.name) {
309: 			// set the default value of this column
310: 			change_idx = i;
311: 			copy.type = info.target_type;
312: 		}
313: 		create_info->columns.push_back(move(copy));
314: 	}
315: 	if (change_idx == INVALID_INDEX) {
316: 		throw BinderException("Table \"%s\" does not have a column with name \"%s\"", info.table.c_str(),
317: 		                      info.column_name.c_str());
318: 	}
319: 
320: 	for (idx_t i = 0; i < constraints.size(); i++) {
321: 		auto constraint = constraints[i]->Copy();
322: 		switch (constraint->type) {
323: 		case ConstraintType::CHECK: {
324: 			auto &bound_check = (BoundCheckConstraint &)*bound_constraints[i];
325: 			if (bound_check.bound_columns.find(change_idx) != bound_check.bound_columns.end()) {
326: 				throw BinderException("Cannot change the type of a column that has a CHECK constraint specified");
327: 			}
328: 			break;
329: 		}
330: 		case ConstraintType::NOT_NULL:
331: 			break;
332: 		case ConstraintType::UNIQUE: {
333: 			auto &bound_unique = (BoundUniqueConstraint &)*bound_constraints[i];
334: 			if (bound_unique.keys.find(change_idx) != bound_unique.keys.end()) {
335: 				throw BinderException(
336: 				    "Cannot change the type of a column that has a UNIQUE or PRIMARY KEY constraint specified");
337: 			}
338: 			break;
339: 		}
340: 		default:
341: 			throw InternalException("Unsupported constraint for entry!");
342: 		}
343: 		create_info->constraints.push_back(move(constraint));
344: 	}
345: 
346: 	Binder binder(context);
347: 	// bind the specified expression
348: 	vector<column_t> bound_columns;
349: 	AlterBinder expr_binder(binder, context, name, columns, bound_columns, info.target_type);
350: 	auto expression = info.expression->Copy();
351: 	auto bound_expression = expr_binder.Bind(expression);
352: 	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
353: 	if (bound_columns.size() == 0) {
354: 		bound_columns.push_back(COLUMN_IDENTIFIER_ROW_ID);
355: 	}
356: 
357: 	auto new_storage =
358: 	    make_shared<DataTable>(context, *storage, change_idx, info.target_type, move(bound_columns), *bound_expression);
359: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
360: 	                                      new_storage);
361: }
362: 
363: ColumnDefinition &TableCatalogEntry::GetColumn(const string &name) {
364: 	auto entry = name_map.find(name);
365: 	if (entry == name_map.end() || entry->second == COLUMN_IDENTIFIER_ROW_ID) {
366: 		throw CatalogException("Column with name %s does not exist!", name.c_str());
367: 	}
368: 	return columns[entry->second];
369: }
370: 
371: vector<TypeId> TableCatalogEntry::GetTypes() {
372: 	vector<TypeId> types;
373: 	for (auto &it : columns) {
374: 		types.push_back(GetInternalType(it.type));
375: 	}
376: 	return types;
377: }
378: 
379: vector<TypeId> TableCatalogEntry::GetTypes(const vector<column_t> &column_ids) {
380: 	vector<TypeId> result;
381: 	for (auto &index : column_ids) {
382: 		if (index == COLUMN_IDENTIFIER_ROW_ID) {
383: 			result.push_back(TypeId::INT64);
384: 		} else {
385: 			result.push_back(GetInternalType(columns[index].type));
386: 		}
387: 	}
388: 	return result;
389: }
390: 
391: void TableCatalogEntry::Serialize(Serializer &serializer) {
392: 	serializer.WriteString(schema->name);
393: 	serializer.WriteString(name);
394: 	assert(columns.size() <= std::numeric_limits<uint32_t>::max());
395: 	serializer.Write<uint32_t>((uint32_t)columns.size());
396: 	for (auto &column : columns) {
397: 		column.Serialize(serializer);
398: 	}
399: 	assert(constraints.size() <= std::numeric_limits<uint32_t>::max());
400: 	serializer.Write<uint32_t>((uint32_t)constraints.size());
401: 	for (auto &constraint : constraints) {
402: 		constraint->Serialize(serializer);
403: 	}
404: }
405: 
406: unique_ptr<CreateTableInfo> TableCatalogEntry::Deserialize(Deserializer &source) {
407: 	auto info = make_unique<CreateTableInfo>();
408: 
409: 	info->schema = source.Read<string>();
410: 	info->table = source.Read<string>();
411: 	auto column_count = source.Read<uint32_t>();
412: 
413: 	for (uint32_t i = 0; i < column_count; i++) {
414: 		auto column = ColumnDefinition::Deserialize(source);
415: 		info->columns.push_back(move(column));
416: 	}
417: 	auto constraint_count = source.Read<uint32_t>();
418: 
419: 	for (uint32_t i = 0; i < constraint_count; i++) {
420: 		auto constraint = Constraint::Deserialize(source);
421: 		info->constraints.push_back(move(constraint));
422: 	}
423: 	return info;
424: }
425: 
426: unique_ptr<CatalogEntry> TableCatalogEntry::Copy(ClientContext &context) {
427: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
428: 	for (idx_t i = 0; i < columns.size(); i++) {
429: 		create_info->columns.push_back(columns[i].Copy());
430: 	}
431: 
432: 	for (idx_t i = 0; i < constraints.size(); i++) {
433: 		auto constraint = constraints[i]->Copy();
434: 		create_info->constraints.push_back(move(constraint));
435: 	}
436: 
437: 	Binder binder(context);
438: 	auto bound_create_info = binder.BindCreateTableInfo(move(create_info));
439: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
440: }
441: 
442: void TableCatalogEntry::SetAsRoot() {
443: 	storage->SetAsRoot();
444: }
[end of src/catalog/catalog_entry/table_catalog_entry.cpp]
[start of src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/catalog/catalog_entry/table_catalog_entry.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/catalog/standard_entry.hpp"
12: #include "duckdb/common/unordered_map.hpp"
13: #include "duckdb/parser/column_definition.hpp"
14: #include "duckdb/parser/constraint.hpp"
15: #include "duckdb/planner/bound_constraint.hpp"
16: #include "duckdb/planner/expression.hpp"
17: 
18: namespace duckdb {
19: 
20: class ColumnStatistics;
21: class DataTable;
22: struct CreateTableInfo;
23: struct BoundCreateTableInfo;
24: 
25: struct RenameColumnInfo;
26: struct AddColumnInfo;
27: struct RemoveColumnInfo;
28: struct SetDefaultInfo;
29: struct ChangeColumnTypeInfo;
30: 
31: //! A table catalog entry
32: class TableCatalogEntry : public StandardEntry {
33: public:
34: 	//! Create a real TableCatalogEntry and initialize storage for it
35: 	TableCatalogEntry(Catalog *catalog, SchemaCatalogEntry *schema, BoundCreateTableInfo *info,
36: 	                  std::shared_ptr<DataTable> inherited_storage = nullptr);
37: 
38: 	//! A reference to the underlying storage unit used for this table
39: 	std::shared_ptr<DataTable> storage;
40: 	//! A list of columns that are part of this table
41: 	vector<ColumnDefinition> columns;
42: 	//! A list of constraints that are part of this table
43: 	vector<unique_ptr<Constraint>> constraints;
44: 	//! A list of constraints that are part of this table
45: 	vector<unique_ptr<BoundConstraint>> bound_constraints;
46: 	//! A map of column name to column index
47: 	unordered_map<string, column_t> name_map;
48: 
49: public:
50: 	unique_ptr<CatalogEntry> AlterEntry(ClientContext &context, AlterInfo *info) override;
51: 	//! Returns whether or not a column with the given name exists
52: 	bool ColumnExists(const string &name);
53: 	//! Returns a reference to the column of the specified name. Throws an
54: 	//! exception if the column does not exist.
55: 	ColumnDefinition &GetColumn(const string &name);
56: 	//! Returns a list of types of the table
57: 	vector<TypeId> GetTypes();
58: 	//! Returns a list of types of the specified columns of the table
59: 	vector<TypeId> GetTypes(const vector<column_t> &column_ids);
60: 
61: 	//! Serialize the meta information of the TableCatalogEntry a serializer
62: 	virtual void Serialize(Serializer &serializer);
63: 	//! Deserializes to a CreateTableInfo
64: 	static unique_ptr<CreateTableInfo> Deserialize(Deserializer &source);
65: 
66: 	unique_ptr<CatalogEntry> Copy(ClientContext &context) override;
67: 
68: 	void SetAsRoot() override;
69: 
70: private:
71: 	unique_ptr<CatalogEntry> RenameColumn(ClientContext &context, RenameColumnInfo &info);
72: 	unique_ptr<CatalogEntry> AddColumn(ClientContext &context, AddColumnInfo &info);
73: 	unique_ptr<CatalogEntry> RemoveColumn(ClientContext &context, RemoveColumnInfo &info);
74: 	unique_ptr<CatalogEntry> SetDefault(ClientContext &context, SetDefaultInfo &info);
75: 	unique_ptr<CatalogEntry> ChangeColumnType(ClientContext &context, ChangeColumnTypeInfo &info);
76: };
77: } // namespace duckdb
[end of src/include/duckdb/catalog/catalog_entry/table_catalog_entry.hpp]
[start of src/include/duckdb/planner/table_binding.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/table_binding.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/unordered_map.hpp"
13: #include "duckdb/parser/column_definition.hpp"
14: #include "duckdb/parser/parsed_expression.hpp"
15: #include "duckdb/planner/expression_binder.hpp"
16: 
17: namespace duckdb {
18: class BindContext;
19: class BoundQueryNode;
20: class ColumnRefExpression;
21: class SubqueryRef;
22: class LogicalGet;
23: class TableCatalogEntry;
24: class TableFunctionCatalogEntry;
25: class BoundTableFunction;
26: 
27: enum class BindingType : uint8_t { TABLE = 0, SUBQUERY = 1, TABLE_FUNCTION = 2, GENERIC = 3 };
28: 
29: //! A Binding represents a binding to a table, table-producing function or subquery with a specified table index. Used
30: //! in the binder.
31: struct Binding {
32: 	Binding(BindingType type, const string &alias, idx_t index) : type(type), alias(alias), index(index) {
33: 	}
34: 	virtual ~Binding() = default;
35: 
36: 	BindingType type;
37: 	string alias;
38: 	idx_t index;
39: 
40: public:
41: 	virtual bool HasMatchingBinding(const string &column_name) = 0;
42: 	virtual BindResult Bind(ColumnRefExpression &colref, idx_t depth) = 0;
43: 	virtual void GenerateAllColumnExpressions(BindContext &context,
44: 	                                          vector<unique_ptr<ParsedExpression>> &select_list) = 0;
45: };
46: 
47: //! Represents a binding to a base table
48: struct TableBinding : public Binding {
49: 	TableBinding(const string &alias, TableCatalogEntry &table, LogicalGet &get, idx_t index);
50: 
51: 	TableCatalogEntry &table;
52: 	LogicalGet &get;
53: 
54: public:
55: 	bool HasMatchingBinding(const string &column_name) override;
56: 	BindResult Bind(ColumnRefExpression &colref, idx_t depth) override;
57: 	void GenerateAllColumnExpressions(BindContext &context, vector<unique_ptr<ParsedExpression>> &select_list) override;
58: };
59: 
60: //! Represents a generic binding with types and names
61: struct GenericBinding : public Binding {
62: 	GenericBinding(const string &alias, vector<SQLType> types, vector<string> names, idx_t index);
63: 
64: 	vector<SQLType> types;
65: 	//! Column names of the subquery
66: 	vector<string> names;
67: 	//! Name -> index for the names
68: 	unordered_map<string, uint64_t> name_map;
69: 
70: public:
71: 	bool HasMatchingBinding(const string &column_name) override;
72: 	BindResult Bind(ColumnRefExpression &colref, idx_t depth) override;
73: 	void GenerateAllColumnExpressions(BindContext &context, vector<unique_ptr<ParsedExpression>> &select_list) override;
74: };
75: 
76: } // namespace duckdb
[end of src/include/duckdb/planner/table_binding.hpp]
[start of src/planner/table_binding.cpp]
1: #include "duckdb/planner/table_binding.hpp"
2: 
3: #include "duckdb/common/string_util.hpp"
4: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/table_function_catalog_entry.hpp"
6: #include "duckdb/parser/expression/columnref_expression.hpp"
7: #include "duckdb/parser/tableref/subqueryref.hpp"
8: #include "duckdb/planner/bind_context.hpp"
9: #include "duckdb/planner/bound_query_node.hpp"
10: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
11: #include "duckdb/planner/operator/logical_get.hpp"
12: 
13: using namespace duckdb;
14: using namespace std;
15: 
16: TableBinding::TableBinding(const string &alias, TableCatalogEntry &table, LogicalGet &get, idx_t index)
17:     : Binding(BindingType::TABLE, alias, index), table(table), get(get) {
18: }
19: 
20: bool TableBinding::HasMatchingBinding(const string &column_name) {
21: 	return table.ColumnExists(column_name);
22: }
23: 
24: BindResult TableBinding::Bind(ColumnRefExpression &colref, idx_t depth) {
25: 	auto entry = table.name_map.find(colref.column_name);
26: 	if (entry == table.name_map.end()) {
27: 		return BindResult(StringUtil::Format("Table \"%s\" does not have a column named \"%s\"",
28: 		                                     colref.table_name.c_str(), colref.column_name.c_str()));
29: 	}
30: 	auto col_index = entry->second;
31: 	// fetch the type of the column
32: 	SQLType col_type;
33: 	if (entry->second == COLUMN_IDENTIFIER_ROW_ID) {
34: 		// row id: BIGINT type
35: 		col_type = SQLType::BIGINT;
36: 	} else {
37: 		// normal column: fetch type from base column
38: 		auto &col = table.columns[col_index];
39: 		col_type = col.type;
40: 	}
41: 
42: 	auto &column_ids = get.column_ids;
43: 	// check if the entry already exists in the column list for the table
44: 	ColumnBinding binding;
45: 
46: 	binding.column_index = column_ids.size();
47: 	for (idx_t i = 0; i < column_ids.size(); i++) {
48: 		if (column_ids[i] == col_index) {
49: 			binding.column_index = i;
50: 			break;
51: 		}
52: 	}
53: 	if (binding.column_index == column_ids.size()) {
54: 		// column binding not found: add it to the list of bindings
55: 		column_ids.push_back(col_index);
56: 	}
57: 	binding.table_index = index;
58: 	return BindResult(
59: 	    make_unique<BoundColumnRefExpression>(colref.GetName(), GetInternalType(col_type), binding, depth), col_type);
60: }
61: 
62: void TableBinding::GenerateAllColumnExpressions(BindContext &context,
63:                                                 vector<unique_ptr<ParsedExpression>> &select_list) {
64: 	for (auto &column : table.columns) {
65: 		if (context.BindingIsHidden(alias, column.name)) {
66: 			continue;
67: 		}
68: 		assert(!column.name.empty());
69: 		select_list.push_back(make_unique<ColumnRefExpression>(column.name, alias));
70: 	}
71: }
72: 
73: GenericBinding::GenericBinding(const string &alias, vector<SQLType> coltypes, vector<string> colnames, idx_t index)
74:     : Binding(BindingType::GENERIC, alias, index), types(move(coltypes)), names(move(colnames)) {
75: 	assert(types.size() == names.size());
76: 	for (idx_t i = 0; i < names.size(); i++) {
77: 		auto &name = names[i];
78: 		assert(!name.empty());
79: 		if (name_map.find(name) != name_map.end()) {
80: 			throw BinderException("table \"%s\" has duplicate column name \"%s\"", alias.c_str(), name.c_str());
81: 		}
82: 		name_map[name] = i;
83: 	}
84: }
85: 
86: bool GenericBinding::HasMatchingBinding(const string &column_name) {
87: 	auto entry = name_map.find(column_name);
88: 	return entry != name_map.end();
89: }
90: 
91: BindResult GenericBinding::Bind(ColumnRefExpression &colref, idx_t depth) {
92: 	auto column_entry = name_map.find(colref.column_name);
93: 	if (column_entry == name_map.end()) {
94: 		return BindResult(StringUtil::Format("Values list \"%s\" does not have a column named \"%s\"", alias.c_str(),
95: 		                                     colref.column_name.c_str()));
96: 	}
97: 	ColumnBinding binding;
98: 	binding.table_index = index;
99: 	binding.column_index = column_entry->second;
100: 	SQLType sql_type = types[column_entry->second];
101: 	return BindResult(
102: 	    make_unique<BoundColumnRefExpression>(colref.GetName(), GetInternalType(sql_type), binding, depth), sql_type);
103: }
104: 
105: void GenericBinding::GenerateAllColumnExpressions(BindContext &context,
106:                                                   vector<unique_ptr<ParsedExpression>> &select_list) {
107: 	for (auto &column_name : names) {
108: 		assert(!column_name.empty());
109: 		if (context.BindingIsHidden(alias, column_name)) {
110: 			continue;
111: 		}
112: 		select_list.push_back(make_unique<ColumnRefExpression>(column_name, alias));
113: 	}
114: }
[end of src/planner/table_binding.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: