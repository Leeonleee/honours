{
  "repo": "duckdb/duckdb",
  "pull_number": 8383,
  "instance_id": "duckdb__duckdb-8383",
  "issue_numbers": [
    "5547"
  ],
  "base_commit": "4c6509191ed38c24077f722fd283cd2cfe3bacc2",
  "patch": "diff --git a/src/function/table/arrow.cpp b/src/function/table/arrow.cpp\nindex 04dbf0798300..de27b9f4d74c 100644\n--- a/src/function/table/arrow.cpp\n+++ b/src/function/table/arrow.cpp\n@@ -123,7 +123,7 @@ LogicalType ArrowTableFunction::GetArrowLogicalType(\n \t\tchild_list_t<LogicalType> child_types;\n \t\tfor (idx_t type_idx = 0; type_idx < (idx_t)schema.n_children; type_idx++) {\n \t\t\tauto child_type = GetArrowLogicalType(*schema.children[type_idx], arrow_convert_data, col_idx);\n-\t\t\tchild_types.push_back({schema.children[type_idx]->name, child_type});\n+\t\t\tchild_types.emplace_back(schema.children[type_idx]->name, child_type);\n \t\t}\n \t\treturn LogicalType::STRUCT(child_types);\n \ndiff --git a/src/function/table/arrow_conversion.cpp b/src/function/table/arrow_conversion.cpp\nindex 3e32acbdedcd..ec893828a1eb 100644\n--- a/src/function/table/arrow_conversion.cpp\n+++ b/src/function/table/arrow_conversion.cpp\n@@ -93,7 +93,7 @@ static void SetValidityMask(Vector &vector, ArrowArray &array, ArrowScanLocalSta\n static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n                                 std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,\n                                 idx_t col_idx, ArrowConvertDataIndices &arrow_convert_idx, int64_t nested_offset = -1,\n-                                ValidityMask *parent_mask = nullptr);\n+                                ValidityMask *parent_mask = nullptr, uint64_t parent_offset = 0);\n \n static void ArrowToDuckDBList(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n                               std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,\n@@ -265,12 +265,13 @@ static void SetVectorString(Vector &vector, idx_t size, char *cdata, T *offsets)\n \t}\n }\n \n-static void DirectConversion(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state,\n-                             int64_t nested_offset) {\n+static void DirectConversion(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, int64_t nested_offset,\n+                             uint64_t parent_offset) {\n \tauto internal_type = GetTypeIdSize(vector.GetType().InternalType());\n-\tauto data_ptr = ArrowBufferData<data_t>(array, 1) + internal_type * (scan_state.chunk_offset + array.offset);\n+\tauto data_ptr =\n+\t    ArrowBufferData<data_t>(array, 1) + internal_type * (scan_state.chunk_offset + array.offset + parent_offset);\n \tif (nested_offset != -1) {\n-\t\tdata_ptr = ArrowBufferData<data_t>(array, 1) + internal_type * (array.offset + nested_offset);\n+\t\tdata_ptr = ArrowBufferData<data_t>(array, 1) + internal_type * (array.offset + nested_offset + parent_offset);\n \t}\n \tFlatVector::SetData(vector, data_ptr);\n }\n@@ -359,7 +360,7 @@ static void IntervalConversionMonthDayNanos(Vector &vector, ArrowArray &array, A\n static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n                                 std::unordered_map<idx_t, unique_ptr<ArrowConvertData>> &arrow_convert_data,\n                                 idx_t col_idx, ArrowConvertDataIndices &arrow_convert_idx, int64_t nested_offset,\n-                                ValidityMask *parent_mask) {\n+                                ValidityMask *parent_mask, uint64_t parent_offset) {\n \tswitch (vector.GetType().id()) {\n \tcase LogicalTypeId::SQLNULL:\n \t\tvector.Reference(Value());\n@@ -407,7 +408,7 @@ static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanLoca\n \tcase LogicalTypeId::TIMESTAMP_SEC:\n \tcase LogicalTypeId::TIMESTAMP_MS:\n \tcase LogicalTypeId::TIMESTAMP_NS: {\n-\t\tDirectConversion(vector, array, scan_state, nested_offset);\n+\t\tDirectConversion(vector, array, scan_state, nested_offset, parent_offset);\n \t\tbreak;\n \t}\n \tcase LogicalTypeId::VARCHAR: {\n@@ -432,7 +433,7 @@ static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanLoca\n \t\tauto precision = arrow_convert_data[col_idx]->date_time_precision[arrow_convert_idx.datetime_precision_index++];\n \t\tswitch (precision) {\n \t\tcase ArrowDateTimeType::DAYS: {\n-\t\t\tDirectConversion(vector, array, scan_state, nested_offset);\n+\t\t\tDirectConversion(vector, array, scan_state, nested_offset, parent_offset);\n \t\t\tbreak;\n \t\t}\n \t\tcase ArrowDateTimeType::MILLISECONDS: {\n@@ -495,7 +496,7 @@ static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanLoca\n \t\t\tbreak;\n \t\t}\n \t\tcase ArrowDateTimeType::MICROSECONDS: {\n-\t\t\tDirectConversion(vector, array, scan_state, nested_offset);\n+\t\t\tDirectConversion(vector, array, scan_state, nested_offset, parent_offset);\n \t\t\tbreak;\n \t\t}\n \t\tcase ArrowDateTimeType::NANOSECONDS: {\n@@ -640,7 +641,8 @@ static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanLoca\n \t\t\t\t}\n \t\t\t}\n \t\t\tColumnArrowToDuckDB(*child_entries[type_idx], *array.children[type_idx], scan_state, size,\n-\t\t\t                    arrow_convert_data, col_idx, arrow_convert_idx, nested_offset, &struct_validity_mask);\n+\t\t\t                    arrow_convert_data, col_idx, arrow_convert_idx, nested_offset, &struct_validity_mask,\n+\t\t\t                    array.offset);\n \t\t}\n \t\tbreak;\n \t}\n",
  "test_patch": "diff --git a/tools/pythonpkg/tests/fast/arrow/test_5547.py b/tools/pythonpkg/tests/fast/arrow/test_5547.py\nnew file mode 100644\nindex 000000000000..5a376ad7f57e\n--- /dev/null\n+++ b/tools/pythonpkg/tests/fast/arrow/test_5547.py\n@@ -0,0 +1,37 @@\n+import duckdb\n+import pandas as pd\n+from pandas.testing import assert_frame_equal\n+import pytest\n+\n+pa = pytest.importorskip('pyarrow')\n+\n+\n+def test_5547():\n+    num_rows = 2**17 + 1\n+\n+    tbl = pa.Table.from_pandas(\n+        pd.DataFrame.from_records(\n+            [\n+                dict(\n+                    id=i,\n+                    nested=dict(\n+                        a=i,\n+                    ),\n+                )\n+                for i in range(num_rows)\n+            ]\n+        )\n+    )\n+\n+    con = duckdb.connect()\n+    expected = tbl.to_pandas()\n+    result = con.execute(\n+        \"\"\"\n+    SELECT *\n+    FROM tbl\n+    \"\"\"\n+    ).df()\n+\n+    assert_frame_equal(expected, result)\n+\n+    con.close()\n",
  "problem_statement": "Incorrect nested data converted in Arrow <-> DuckDB conversion when read through Arrow DataSet (Python)\n### What happens?\n\nI have a parquet file with a nested struct column. When querying this file and extracting the struct column, it appears to return the wrong data. It has done this when using a Arrow Dataset / Table as inputs.\r\n\r\nI have uploaded the specific file in question down below. The nested aspect might be a coincidence - I have no idea what's going wrong.\n\n### To Reproduce\n\nDataset (zipped parquet file - could not upload parquet alone):\r\n[obfuscated.zip](https://github.com/duckdb/duckdb/files/10118562/obfuscated.zip)\r\n\r\n## Environment\r\n```\r\nPackage         Version\r\n--------------- ------------\r\nduckdb          0.6.1.dev191\r\nnumpy           1.24.0rc1\r\npandas          1.5.2\r\npip             22.3.1\r\npyarrow         10.0.1\r\npython-dateutil 2.8.2\r\npytz            2022.6\r\nsetuptools      65.5.1\r\nsix             1.16.0\r\nwheel           0.37.1\r\n```\r\n\r\n\r\n## Python code\r\n```python\r\nimport pyarrow.dataset as ds\r\nimport duckdb\r\n\r\ndata = ds.dataset(\"~/obfuscated.parquet\")\r\n\r\ncon = duckdb.connect()\r\n\r\ndisplay(\"Select then filter\")\r\ndisplay(con.execute(\"\"\"\r\nSELECT\r\n    col1,\r\n    col2,\r\n    col3,\r\n    col4,\r\n    col5,\r\n    nested_col.*,\r\nFROM data\r\nORDER BY ALL\r\n\"\"\").df().loc[lambda df: (df.col1 == 749) & (df.col2 == 747) & (df.col3 == 5) & (df.col4 == 1)])\r\n\r\ndisplay(\"Filter then select\")\r\ndisplay(con.execute(\"\"\"\r\nSELECT\r\n    col1,\r\n    col2,\r\n    col3,\r\n    col4,\r\n    col5,\r\n    nested_col.*,\r\nFROM data\r\nWHERE col1 = 749\r\n    AND col2 = 747\r\n    AND col3 = 5\r\n    AND col4 = 1\r\nORDER BY ALL\r\n\"\"\").df())\r\n\r\ncon.close()\r\n```\r\n\r\n## Output\r\n![image](https://user-images.githubusercontent.com/91922857/204685867-c90a5e4e-22f8-4b23-ab14-264399dfe278.png)\r\n\r\nThe expected output is the second one above. This issue only shows with Arrow Dataset/Table inputs. The issue doesn't show when using a pandas DataFrame as inputs.\n\n### OS:\n\nLinux x64\n\n### DuckDB Version:\n\nduckdb-0.6.1.dev191\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nEdward Davis\n\n### Affiliation:\n\nVeitch Lister Consulting\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "Thanks for the report!\r\n\r\nThis looks like a bug in the arrow conversion code. @pdet can you have a look?\nHi, I have found a minimal example: see below. I noticed some weird things happening when the length of the pyarrow table exceeds 2^17. A similar issue came about when using polars (see pola-rs/polars#5796).\r\n\r\nMy understanding isn't great but looks like it's an upstream issue where arrow returns duplicate chunks through the FFI. \r\n\r\n```python\r\nimport duckdb\r\nimport pyarrow as pa\r\nimport pandas as pd\r\n\r\nnum_rows = 2**17 + 1\r\n\r\ntbl = pa.Table.from_pandas(pd.DataFrame.from_records([\r\n        dict(\r\n            id=i, \r\n            nested=dict(\r\n                a=i,\r\n            )\r\n        )\r\n        for i in range(num_rows)\r\n]))\r\n\r\ncon = duckdb.connect()\r\n\r\nprint(con.execute(\"\"\"\r\nSELECT *\r\nFROM tbl\r\n\"\"\").df())\r\n\r\n# Last row shows id=131072,  nested={'a': 0}  instead of id=131072,  nested={'a': 131072} \r\n# Does not occur for num_rows <= 2^17\r\n\r\ncon.close()\r\n```\r\n\nI'm not sure whether this issue has the same root cause, but I am also getting incorrect results on boolean fields when I read a Parquet file in. I originally thought it was an issue with Arrow's write, but pyarrow and fastparquet both read it correctly.\r\n\r\nThe original Arrow ticket is here and has all of the data/details needed to reproduce: https://github.com/apache/arrow/issues/33605\n I'm also getting an issue with random boolean fields being flipped. \r\n\r\nIn my case, the boolean doesn't have to be nested (although I noticed the nested booleans first) \r\n\r\nI write my dataset twice via spark (same data, but partitioned differently). \r\nWhen I query both datasets via duckdb, 2887 out of 481917 rows have a column mismatch! \r\n\r\nWhen I read both datasets via pandas, or via spark, or via pyarrow directly, both datasets match perfectly. \r\n___\r\nThis is extremely unnerving and a huge blow to our confidence in duckdb producing accurate results.\r\n\r\nI recommend categorizing this as a critical level issue/bug\n> I'm not sure whether this issue has the same root cause, but I am also getting incorrect results on boolean fields when I read a Parquet file in. I originally thought it was an issue with Arrow's write, but pyarrow and fastparquet both read it correctly.\r\n> \r\n> The original Arrow ticket is here and has all of the data/details needed to reproduce: [apache/arrow#33605](https://github.com/apache/arrow/issues/33605)\r\n\r\nThanks for the detailed report - I have pushed a fix in #5926. In the future it would be helpful if you could file it as an actual issue rather than as a comment in an unrelated issue. Although I can see the confusion given the original title this issue is about a bug in the Arrow <> DuckDB conversion code that originated on Arrows' side. I have renamed the issue to clear up future confusion.\r\n\r\n\n@alex-shchetkov\r\n> This is extremely unnerving and a huge blow to our confidence in duckdb producing accurate results.\r\n> I recommend categorizing this as a critical level issue/bug\r\n\r\nPlease note that DuckDB is provided as-is at no charge under the MIT license which states (in caps) that 'THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, [...] FITNESS FOR A PARTICULAR PURPOSE [...]'. \r\n\r\nDespite this, we are doing our best to support our users here and elsewhere. But best-effort does not a guarantee that we will address a particular issue at all or in any particular time frame. Those kind of guarantees are usually part of a commercial support agreement. \r\n\r\nI would also like to ask you to re-think the tone that you are using in your comment. It comes over somewhat aggressive. Please refer to our code of conduct for some additional guidance.\r\n\r\nIt would be more productive if you would provide reproducible steps so we can work together in addressing the issue.\r\n\nHi there, I've also encountered the issue that @edavisau has pointed to, and came up with a similar minimal example (before seeing this issue!):\r\n```python\r\nimport duckdb\r\nimport pyarrow as pa\r\n\r\n\r\ndef test_duckdb_struct():\r\n    # threshold is important 2^17, so go just over\r\n    col1 = [i for i in range(0, 131075)]\r\n    # \"a\" in the struct matches the value for col1\r\n    col2 = [{\"a\": i} for i in col1]\r\n    table = pa.Table.from_pydict(\r\n        {\"col1\": col1, \"col2\": col2},\r\n        schema=pa.schema(\r\n            [(\"col1\", pa.float64()), (\"col2\", pa.struct({\"a\": pa.float64()}))]\r\n        ),\r\n    )\r\n\r\n    conn = duckdb.connect(\":memory:\")\r\n    conn.register(\"my_table\", table)\r\n    # If working correctly I would expect an empty list to be printed here\r\n    print(\r\n        conn.execute(\"SELECT * FROM my_table WHERE col1 <> col2.a LIMIT 10\")\r\n        .fetch_arrow_table()\r\n        .to_pylist()\r\n    )\r\n```\r\n\r\nWhat's worth noting is that the struct column `col2` seems to have rotated back to the start, since we see `col2.a` values 0, 1, 2.\r\n\r\nI'm not sure that this test helps, but reifying the table seems to show correct data:\r\n```python\r\nprint([o for o in table.to_pylist() if o[\"col1\"] != o[\"col2\"][\"a\"]]) # prints empty list\r\n```\r\n\r\nAll run on duckdb 0.8.2.dev2068\nIn case it's of interest a workaround exists using the same approach as in the test suite of subclassing `pyarrow.dataset.Dataset` https://github.com/duckdb/duckdb/blob/09e2d957342607904124ebab892be70b0ecf9a10/tools/pythonpkg/tests/fast/arrow/test_dataset.py#L100\r\n\r\nSetting the batch size in the scanner to a large value such that we only have a single batch side steps the issue.  Standard disclaimer that this isn't ideal because it prevents batchwise processing and caps your max dataset row count:\r\n```python\r\ndef test_duckdb_struct_workaround():\r\n    # threshold is important 2^17, so go just over\r\n    col1 = [i for i in range(0, 131075)]\r\n    # \"a\" in the struct matches the value for col1\r\n    col2 = [{\"a\": i} for i in col1]\r\n    table = pa.Table.from_pydict(\r\n        {\"col1\": col1, \"col2\": col2},\r\n        schema=pa.schema(\r\n            [(\"col1\", pa.float64()), (\"col2\", pa.struct({\"a\": pa.float64()}))]\r\n        ),\r\n    )\r\n\r\n    class CustomDataset(ds.Dataset):\r\n        def __init__(self, dataset):\r\n            self.original = dataset\r\n\r\n        def scanner(self, **kwargs):\r\n            return self.original.scanner(**kwargs, batch_size=1_000_000)\r\n\r\n        @property\r\n        def schema(self):\r\n            return self.original.schema\r\n\r\n    dataset = CustomDataset(ds.dataset(table))\r\n\r\n    conn = duckdb.connect(\":memory:\")\r\n    conn.register(\"my_table\", dataset)\r\n    # If working correctly I would expect an empty list to be printed here\r\n    print(\r\n        conn.execute(\"SELECT * FROM my_table WHERE col1 <> col2.a LIMIT 10\")\r\n        .fetch_arrow_table()\r\n        .to_pylist()\r\n    )\r\n\r\n    # Check _something_ happens\r\n    print(\r\n        conn.execute(\"SELECT COUNT(*), SUM(col1), SUM(col2.a) FROM my_table\")\r\n        .fetch_arrow_table()\r\n        .to_pylist()\r\n    )\r\n\r\n    # See this value matches SUM(col1) in the above SQL execution\r\n    print(sum(col1))\r\n```\n@Tishj perhaps you can have a look when you have time?\nIn case helpful, I _think_ @pdet has a branch which has addressed this:\r\nhttps://github.com/duckdb/duckdb/compare/master...pdet:duckdb:bug_5547\r\n\r\nWithout understanding in detail, the `parent_offset` looks like a fix",
  "created_at": "2023-07-27T07:54:43Z"
}