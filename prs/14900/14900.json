{
  "repo": "duckdb/duckdb",
  "pull_number": 14900,
  "instance_id": "duckdb__duckdb-14900",
  "issue_numbers": [
    "14896"
  ],
  "base_commit": "d29642dc9dc922e57f1f12852bf92015a0ea5e68",
  "patch": "diff --git a/src/execution/operator/order/physical_top_n.cpp b/src/execution/operator/order/physical_top_n.cpp\nindex 1b3ae11aed7a..dbee3bf617f7 100644\n--- a/src/execution/operator/order/physical_top_n.cpp\n+++ b/src/execution/operator/order/physical_top_n.cpp\n@@ -90,14 +90,53 @@ class TopNHeap {\n \tvoid InitializeScan(TopNScanState &state, bool exclude_offset);\n \tvoid Scan(TopNScanState &state, DataChunk &chunk);\n \n+\tvoid AddSmallHeap(DataChunk &input, Vector &sort_keys_vec, const string &boundary_val,\n+\t                  const string_t &global_boundary_val);\n+\tvoid AddLargeHeap(DataChunk &input, Vector &sort_keys_vec, const string &boundary_val,\n+\t                  const string_t &global_boundary_val);\n+\n public:\n \tidx_t ReduceThreshold() const {\n \t\treturn MaxValue<idx_t>(STANDARD_VECTOR_SIZE * 5ULL, 2ULL * heap_size);\n \t}\n \n-\tidx_t HeapAllocSize() const {\n+\tidx_t InitialHeapAllocSize() const {\n \t\treturn MinValue<idx_t>(STANDARD_VECTOR_SIZE * 100ULL, ReduceThreshold()) + STANDARD_VECTOR_SIZE;\n \t}\n+\n+private:\n+\tinline bool EntryShouldBeAdded(const string_t &sort_key) {\n+\t\tif (heap.size() < heap_size) {\n+\t\t\t// heap is full - check the latest entry\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (sort_key < heap.front().sort_key) {\n+\t\t\t// sort key is smaller than current max value\n+\t\t\treturn true;\n+\t\t}\n+\t\t// heap is full and there is no room for the entry\n+\t\treturn false;\n+\t}\n+\n+\tinline bool EntryShouldBeAdded(const string_t &sort_key, const string &boundary_val,\n+\t                               const string_t &global_boundary_val) {\n+\t\t// first compare against the global boundary value (if there is any)\n+\t\tif (!boundary_val.empty() && sort_key > global_boundary_val) {\n+\t\t\t// this entry is out-of-range for the global boundary val\n+\t\t\t// it will never be in the final result even if it fits in this heap\n+\t\t\treturn false;\n+\t\t}\n+\t\treturn EntryShouldBeAdded(sort_key);\n+\t}\n+\n+\tinline void AddEntryToHeap(const TopNEntry &entry) {\n+\t\tif (heap.size() >= heap_size) {\n+\t\t\tstd::pop_heap(heap.begin(), heap.end());\n+\t\t\theap.pop_back();\n+\t\t}\n+\t\theap.push_back(entry);\n+\t\tstd::push_heap(heap.begin(), heap.end());\n+\t}\n };\n \n //===--------------------------------------------------------------------===//\n@@ -115,9 +154,10 @@ TopNHeap::TopNHeap(ClientContext &context, Allocator &allocator, const vector<Lo\n \t\tsort_types.push_back(expr->return_type);\n \t\texecutor.AddExpression(*expr);\n \t}\n+\theap.reserve(InitialHeapAllocSize());\n \tvector<LogicalType> sort_keys_type {LogicalType::BLOB};\n \tsort_keys.Initialize(allocator, sort_keys_type);\n-\theap_data.Initialize(allocator, payload_types, HeapAllocSize());\n+\theap_data.Initialize(allocator, payload_types, InitialHeapAllocSize());\n \tpayload_chunk.Initialize(allocator, payload_types);\n \tsort_chunk.Initialize(allocator, sort_types);\n }\n@@ -132,28 +172,8 @@ TopNHeap::TopNHeap(ExecutionContext &context, const vector<LogicalType> &payload\n     : TopNHeap(context.client, Allocator::Get(context.client), payload_types, orders, limit, offset) {\n }\n \n-void TopNHeap::Sink(DataChunk &input, optional_ptr<TopNBoundaryValue> global_boundary) {\n-\t// compute the ordering values for the new chunk\n-\tsort_chunk.Reset();\n-\texecutor.Execute(input, sort_chunk);\n-\n-\t// construct the sort key from the sort chunk\n-\tvector<OrderModifiers> modifiers;\n-\tfor (auto &order : orders) {\n-\t\tmodifiers.emplace_back(order.type, order.null_order);\n-\t}\n-\tsort_keys.Reset();\n-\tauto &sort_keys_vec = sort_keys.data[0];\n-\tCreateSortKeyHelpers::CreateSortKey(sort_chunk, modifiers, sort_keys_vec);\n-\n-\t// fetch the current global boundary (if any)\n-\tstring boundary_val;\n-\tstring_t global_boundary_val;\n-\tif (global_boundary) {\n-\t\tboundary_val = global_boundary->GetBoundaryValue();\n-\t\tglobal_boundary_val = string_t(boundary_val);\n-\t}\n-\n+void TopNHeap::AddSmallHeap(DataChunk &input, Vector &sort_keys_vec, const string &boundary_val,\n+                            const string_t &global_boundary_val) {\n \t// insert the sort keys into the priority queue\n \tconstexpr idx_t BASE_INDEX = NumericLimits<uint32_t>::Maximum();\n \n@@ -161,37 +181,20 @@ void TopNHeap::Sink(DataChunk &input, optional_ptr<TopNBoundaryValue> global_bou\n \tauto sort_key_values = FlatVector::GetData<string_t>(sort_keys_vec);\n \tfor (idx_t r = 0; r < input.size(); r++) {\n \t\tauto &sort_key = sort_key_values[r];\n-\t\tif (!boundary_val.empty() && sort_key > global_boundary_val) {\n+\t\tif (!EntryShouldBeAdded(sort_key, boundary_val, global_boundary_val)) {\n \t\t\tcontinue;\n \t\t}\n-\t\tif (heap.size() >= heap_size) {\n-\t\t\t// heap is full - check the latest entry\n-\t\t\tif (sort_key > heap.front().sort_key) {\n-\t\t\t\t// current max in the heap is smaller than the new key - skip this entry\n-\t\t\t\tcontinue;\n-\t\t\t}\n-\t\t}\n \t\t// replace the previous top entry with the new entry\n \t\tTopNEntry entry;\n \t\tentry.sort_key = sort_key;\n \t\tentry.index = BASE_INDEX + r;\n-\t\tif (heap.size() >= heap_size) {\n-\t\t\tstd::pop_heap(heap.begin(), heap.end());\n-\t\t\theap.pop_back();\n-\t\t}\n-\t\theap.push_back(entry);\n-\t\tstd::push_heap(heap.begin(), heap.end());\n+\t\tAddEntryToHeap(entry);\n \t\tany_added = true;\n \t}\n \tif (!any_added) {\n \t\t// early-out: no matches\n \t\treturn;\n \t}\n-\t// if we modified the heap we might be able to update the global boundary\n-\t// note that the global boundary only applies to FULL heaps\n-\tif (heap.size() >= heap_size && global_boundary) {\n-\t\tglobal_boundary->UpdateValue(heap.front().sort_key);\n-\t}\n \n \t// for all matching entries we need to copy over the corresponding payload values\n \tidx_t match_count = 0;\n@@ -214,20 +217,98 @@ void TopNHeap::Sink(DataChunk &input, optional_ptr<TopNBoundaryValue> global_bou\n \theap_data.Append(input, true, &matching_sel, match_count);\n }\n \n+void TopNHeap::AddLargeHeap(DataChunk &input, Vector &sort_keys_vec, const string &boundary_val,\n+                            const string_t &global_boundary_val) {\n+\tauto sort_key_values = FlatVector::GetData<string_t>(sort_keys_vec);\n+\tidx_t base_index = heap_data.size();\n+\tidx_t match_count = 0;\n+\tfor (idx_t r = 0; r < input.size(); r++) {\n+\t\tauto &sort_key = sort_key_values[r];\n+\t\tif (!EntryShouldBeAdded(sort_key, boundary_val, global_boundary_val)) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\t// replace the previous top entry with the new entry\n+\t\tTopNEntry entry;\n+\t\tentry.sort_key = sort_key.IsInlined() ? sort_key : sort_key_heap.AddBlob(sort_key);\n+\t\tentry.index = base_index + match_count;\n+\t\tAddEntryToHeap(entry);\n+\t\tmatching_sel.set_index(match_count++, r);\n+\t}\n+\tif (match_count == 0) {\n+\t\t// early-out: no matches\n+\t\treturn;\n+\t}\n+\n+\t// copy over the input rows to the payload chunk\n+\theap_data.Append(input, true, &matching_sel, match_count);\n+}\n+\n+void TopNHeap::Sink(DataChunk &input, optional_ptr<TopNBoundaryValue> global_boundary) {\n+\tstatic constexpr idx_t SMALL_HEAP_THRESHOLD = 100;\n+\n+\t// compute the ordering values for the new chunk\n+\tsort_chunk.Reset();\n+\texecutor.Execute(input, sort_chunk);\n+\n+\t// construct the sort key from the sort chunk\n+\tvector<OrderModifiers> modifiers;\n+\tfor (auto &order : orders) {\n+\t\tmodifiers.emplace_back(order.type, order.null_order);\n+\t}\n+\tsort_keys.Reset();\n+\tauto &sort_keys_vec = sort_keys.data[0];\n+\tCreateSortKeyHelpers::CreateSortKey(sort_chunk, modifiers, sort_keys_vec);\n+\n+\t// fetch the current global boundary (if any)\n+\tstring boundary_val;\n+\tstring_t global_boundary_val;\n+\tif (global_boundary) {\n+\t\tboundary_val = global_boundary->GetBoundaryValue();\n+\t\tglobal_boundary_val = string_t(boundary_val);\n+\t}\n+\n+\tif (heap_size <= SMALL_HEAP_THRESHOLD) {\n+\t\tAddSmallHeap(input, sort_keys_vec, boundary_val, global_boundary_val);\n+\t} else {\n+\t\tAddLargeHeap(input, sort_keys_vec, boundary_val, global_boundary_val);\n+\t}\n+\n+\t// if we modified the heap we might be able to update the global boundary\n+\t// note that the global boundary only applies to FULL heaps\n+\tif (heap.size() >= heap_size && global_boundary) {\n+\t\tglobal_boundary->UpdateValue(heap.front().sort_key);\n+\t}\n+}\n+\n void TopNHeap::Combine(TopNHeap &other) {\n \tother.Finalize();\n \n-\t// FIXME: heaps can be merged directly instead of doing it like this\n-\t// that only really speeds things up if heaps are very large, however\n-\tTopNScanState state;\n-\tother.InitializeScan(state, false);\n-\twhile (true) {\n-\t\tpayload_chunk.Reset();\n-\t\tother.Scan(state, payload_chunk);\n-\t\tif (payload_chunk.size() == 0) {\n-\t\t\tbreak;\n+\tidx_t match_count = 0;\n+\t// merge the heap of other into this\n+\tfor (idx_t i = 0; i < other.heap.size(); i++) {\n+\t\t// heap is full - check the latest entry\n+\t\tauto &other_entry = other.heap[i];\n+\t\tauto &sort_key = other_entry.sort_key;\n+\t\tif (!EntryShouldBeAdded(sort_key)) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\t// add this entry\n+\t\tTopNEntry new_entry;\n+\t\tnew_entry.sort_key = sort_key.IsInlined() ? sort_key : sort_key_heap.AddBlob(sort_key);\n+\t\tnew_entry.index = heap_data.size() + match_count;\n+\t\tAddEntryToHeap(new_entry);\n+\n+\t\tmatching_sel.set_index(match_count++, other_entry.index);\n+\t\tif (match_count >= STANDARD_VECTOR_SIZE) {\n+\t\t\t// flush\n+\t\t\theap_data.Append(other.heap_data, true, &matching_sel, match_count);\n+\t\t\tmatch_count = 0;\n \t\t}\n-\t\tSink(payload_chunk);\n+\t}\n+\tif (match_count > 0) {\n+\t\t// flush\n+\t\theap_data.Append(other.heap_data, true, &matching_sel, match_count);\n+\t\tmatch_count = 0;\n \t}\n \tReduce();\n }\n@@ -236,14 +317,14 @@ void TopNHeap::Finalize() {\n }\n \n void TopNHeap::Reduce() {\n-\tif (payload_chunk.size() < ReduceThreshold()) {\n+\tif (heap_data.size() < ReduceThreshold()) {\n \t\t// only reduce when we pass the reduce threshold\n \t\treturn;\n \t}\n \t// we have too many values in the heap - reduce them\n \tStringHeap new_sort_heap;\n-\tDataChunk new_payload_chunk;\n-\tnew_payload_chunk.Initialize(allocator, payload_types, HeapAllocSize());\n+\tDataChunk new_heap_data;\n+\tnew_heap_data.Initialize(allocator, payload_types, heap.size());\n \n \tSelectionVector new_payload_sel(heap.size());\n \tfor (idx_t i = 0; i < heap.size(); i++) {\n@@ -258,21 +339,22 @@ void TopNHeap::Reduce() {\n \t}\n \n \t// copy over the data from the current payload chunk to the new payload chunk\n-\tpayload_chunk.Copy(new_payload_chunk, new_payload_sel, heap.size());\n+\tnew_heap_data.Slice(heap_data, new_payload_sel, heap.size());\n+\tnew_heap_data.Flatten();\n \n-\tnew_sort_heap.Move(sort_key_heap);\n-\tpayload_chunk.Reference(new_payload_chunk);\n+\tsort_key_heap.Move(new_sort_heap);\n+\theap_data.Reference(new_heap_data);\n }\n \n void TopNHeap::InitializeScan(TopNScanState &state, bool exclude_offset) {\n \tauto heap_copy = heap;\n \t// traverse the rest of the heap\n+\tstate.scan_order.resize(heap_copy.size());\n \twhile (!heap_copy.empty()) {\n \t\tstd::pop_heap(heap_copy.begin(), heap_copy.end());\n-\t\tstate.scan_order.push_back(UnsafeNumericCast<sel_t>(heap_copy.back().index));\n+\t\tstate.scan_order[heap_copy.size() - 1] = UnsafeNumericCast<sel_t>(heap_copy.back().index);\n \t\theap_copy.pop_back();\n \t}\n-\tstd::reverse(state.scan_order.begin(), state.scan_order.end());\n \tstate.pos = exclude_offset ? offset : 0;\n }\n \n",
  "test_patch": "diff --git a/test/optimizer/compressed_materialization.test_coverage b/test/optimizer/compressed_materialization.test_slow\nsimilarity index 98%\nrename from test/optimizer/compressed_materialization.test_coverage\nrename to test/optimizer/compressed_materialization.test_slow\nindex 4088c1efab2d..a39f17f8a4af 100644\n--- a/test/optimizer/compressed_materialization.test_coverage\n+++ b/test/optimizer/compressed_materialization.test_slow\n@@ -1,4 +1,4 @@\n-# name: test/optimizer/compressed_materialization.test_coverage\n+# name: test/optimizer/compressed_materialization.test_slow\n # description: Compressed materialization test\n # group: [optimizer]\n \n@@ -18,7 +18,7 @@ Binder Error: Compressed materialization functions are for internal use only!\n statement ok\n create table t0 as select range%400000 a, range%400000 b from range(500000);\n \n-query III\n+query III rowsort\n select * from (\n       select *, row_number() OVER () as row_number from (\n           SELECT * FROM t0 ORDER BY 1) ta\ndiff --git a/test/sql/topn/tpch_top_n.test_slow b/test/sql/topn/tpch_top_n.test_slow\nnew file mode 100644\nindex 000000000000..a3657bb05faf\n--- /dev/null\n+++ b/test/sql/topn/tpch_top_n.test_slow\n@@ -0,0 +1,78 @@\n+# name: test/sql/topn/tpch_top_n.test_slow\n+# description: Test Top N NULLS FIRST/LAST with few rows\n+# group: [topn]\n+\n+require tpch\n+\n+statement ok\n+CALL dbgen(sf=1);\n+\n+query I\n+select l_quantity\n+from lineitem\n+where l_linestatus = 'O'\n+order by l_quantity limit 10 offset 100;\n+----\n+1.00\n+1.00\n+1.00\n+1.00\n+1.00\n+1.00\n+1.00\n+1.00\n+1.00\n+1.00\n+\n+query I\n+select l_quantity\n+from lineitem\n+where l_linestatus = 'O'\n+order by l_quantity limit 10 offset 1000000;\n+----\n+17.00\n+17.00\n+17.00\n+17.00\n+17.00\n+17.00\n+17.00\n+17.00\n+17.00\n+17.00\n+\n+query I\n+select sum(l_quantity)\n+from lineitem\n+group by l_orderkey\n+order by sum(l_quantity) desc\n+limit 10 offset 100;\n+----\n+297.00\n+296.00\n+296.00\n+296.00\n+296.00\n+296.00\n+296.00\n+296.00\n+295.00\n+295.00\n+\n+query I\n+select sum(l_quantity)\n+from lineitem\n+group by l_orderkey\n+order by sum(l_quantity) desc\n+limit 10 offset 100000;\n+----\n+195.00\n+195.00\n+195.00\n+195.00\n+195.00\n+195.00\n+195.00\n+195.00\n+195.00\n+195.00\n",
  "problem_statement": "A confusion about Top-N operator\n### What happens?\n\nIn `TopNHeap::Reduce`, the reduce threshold is determined by `payload_chunk.size()`. However, the `payload_chunk` is only used as a temporary data holder in `TopNHeap::Combine`. It's `heap_data` right?\n\n### To Reproduce\n\n```cpp\r\nvoid TopNHeap::Combine(TopNHeap &other) {\r\n\tother.Finalize();\r\n\r\n\t// FIXME: heaps can be merged directly instead of doing it like this\r\n\t// that only really speeds things up if heaps are very large, however\r\n\tTopNScanState state;\r\n\tother.InitializeScan(state, false);\r\n\twhile (true) {\r\n\t\tpayload_chunk.Reset();\r\n\t\tother.Scan(state, payload_chunk);\r\n\t\tif (payload_chunk.size() == 0) {\r\n\t\t\tbreak;\r\n\t\t}\r\n\t\tSink(payload_chunk);\r\n\t}\r\n\tReduce();\r\n}\r\n\r\nvoid TopNHeap::Reduce() {\r\n\tif (payload_chunk.size() < ReduceThreshold()) {\r\n\t\t// only reduce when we pass the reduce threshold\r\n\t\treturn;\r\n\t}\r\n\t...\r\n}\r\n```\n\n### OS:\n\nx86_64\n\n### DuckDB Version:\n\nnewest commit id: b470dea7ee47dc2debcc37a4e94976f8eff6670c\n\n### DuckDB Client:\n\nnone\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nYiliang Qiu\n\n### Affiliation:\n\nBaidu Inc.\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have not tested with any build\n\n### Did you include all relevant data sets for reproducing the issue?\n\nNo - Other reason (please specify in the issue body)\n\n### Did you include all code required to reproduce the issue?\n\n- [x] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [x] Yes, I have\n",
  "hints_text": "There are some addition to note: I benchmarked Top-N operator in v1.0.0 and newest commit and found that the performance of top-n in newest commit is degradation in some cases.\r\n\r\nbenchmark env:\r\n- cpu: 96core\r\n- memory: 754gb\r\n- dataset: tpch sf100\r\n- sql:\r\n```sql\r\n-- q1\r\nselect l_orderkey\r\nfrom lineitem\r\nwhere l_linestatus = 'O'\r\norder by l_quantity limit 100 offset 100;\r\n\r\n-- q2\r\nselect l_orderkey\r\nfrom lineitem\r\nwhere l_linestatus = 'O'\r\norder by l_quantity limit 100 offset 1000000;\r\n\r\n-- q3\r\nselect l_orderkey, sum(l_quantity)\r\nfrom lineitem\r\ngroup by l_orderkey\r\norder by sum(l_quantity) desc\r\nlimit 100 offset 100;\r\n\r\n-- q4\r\nselect l_orderkey, sum(l_quantity)\r\nfrom lineitem\r\ngroup by l_orderkey\r\norder by sum(l_quantity) desc\r\nlimit 100 offset 1000000;\r\n```\r\n\r\nresult (unit: sec):\r\n| sql | v1.0.0 | main |\r\n|:--------|:-------|:--------|\r\n| q1 | 0.223 | 0.834 |\r\n| q2 | 19.009 | 82.766 |\r\n| q3 | 4.190 | 1.498 |\r\n| q4 | 27.769 | 78.018 |",
  "created_at": "2024-11-19T13:37:56Z"
}