You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Segfault when using struct column and inserting null in it
**What does happen?**

```sql
create table test(s struct(a int));
insert into test(s) values (null);
```

I get `fish: “duckdb pouet.db” terminated by signal SIGSEGV (Address boundary error)`

**What should happen?**
A `null` value should be inserted into a new row.

**Environment (please complete the following information):**
 - OS: Ubuntu 21.04
 - DuckDB Version 0.2.6 and also current master

**Before submitting**
- [x] Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?
- [x] Have you tried this on the latest `master` branch? In case you cannot compile, you may find some binaries here: https://github.com/duckdb/duckdb/releases/tag/master-builds


</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
2: 
3: ![.github/workflows/main.yml](https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
6: 
7: 
8: ## Installation
9: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
10: 
11: ## Development
12: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
13: 
14: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
15: 
16: 
[end of README.md]
[start of src/common/types.cpp]
1: #include "duckdb/common/types.hpp"
2: 
3: #include "duckdb/common/exception.hpp"
4: #include "duckdb/common/types/hash.hpp"
5: #include "duckdb/common/serializer.hpp"
6: #include "duckdb/common/string_util.hpp"
7: #include "duckdb/common/types/hash.hpp"
8: #include "duckdb/common/types/string_type.hpp"
9: #include "duckdb/common/types/decimal.hpp"
10: 
11: #include <cmath>
12: 
13: namespace duckdb {
14: 
15: LogicalType::LogicalType() : id_(LogicalTypeId::INVALID), width_(0), scale_(0), collation_(string()) {
16: 	physical_type_ = GetInternalType();
17: }
18: LogicalType::LogicalType(LogicalTypeId id) : id_(id), width_(0), scale_(0), collation_(string()) {
19: 	physical_type_ = GetInternalType();
20: }
21: 
22: LogicalType::LogicalType(LogicalTypeId id, string collation)
23:     : id_(id), width_(0), scale_(0), collation_(move(collation)) {
24: 	physical_type_ = GetInternalType();
25: }
26: LogicalType::LogicalType(LogicalTypeId id, uint8_t width, uint8_t scale)
27:     : id_(id), width_(width), scale_(scale), collation_(string()) {
28: 	physical_type_ = GetInternalType();
29: }
30: LogicalType::LogicalType(LogicalTypeId id, child_list_t<LogicalType> child_types)
31:     : id_(id), width_(0), scale_(0), collation_(string()), child_types_(move(child_types)) {
32: 	physical_type_ = GetInternalType();
33: }
34: LogicalType::LogicalType(LogicalTypeId id, uint8_t width, uint8_t scale, string collation,
35:                          child_list_t<LogicalType> child_types)
36:     : id_(id), width_(width), scale_(scale), collation_(move(collation)), child_types_(move(child_types)) {
37: 	physical_type_ = GetInternalType();
38: }
39: 
40: hash_t LogicalType::Hash() const {
41: 	return duckdb::Hash<uint8_t>((uint8_t)id_);
42: }
43: 
44: PhysicalType LogicalType::GetInternalType() {
45: 	switch (id_) {
46: 	case LogicalTypeId::BOOLEAN:
47: 		return PhysicalType::BOOL;
48: 	case LogicalTypeId::TINYINT:
49: 		return PhysicalType::INT8;
50: 	case LogicalTypeId::UTINYINT:
51: 		return PhysicalType::UINT8;
52: 	case LogicalTypeId::SMALLINT:
53: 		return PhysicalType::INT16;
54: 	case LogicalTypeId::USMALLINT:
55: 		return PhysicalType::UINT16;
56: 	case LogicalTypeId::SQLNULL:
57: 	case LogicalTypeId::DATE:
58: 	case LogicalTypeId::INTEGER:
59: 		return PhysicalType::INT32;
60: 	case LogicalTypeId::UINTEGER:
61: 		return PhysicalType::UINT32;
62: 	case LogicalTypeId::BIGINT:
63: 	case LogicalTypeId::TIME:
64: 	case LogicalTypeId::TIMESTAMP:
65: 	case LogicalTypeId::TIMESTAMP_SEC:
66: 	case LogicalTypeId::TIMESTAMP_NS:
67: 	case LogicalTypeId::TIMESTAMP_MS:
68: 		return PhysicalType::INT64;
69: 	case LogicalTypeId::UBIGINT:
70: 		return PhysicalType::UINT64;
71: 	case LogicalTypeId::HUGEINT:
72: 		return PhysicalType::INT128;
73: 	case LogicalTypeId::FLOAT:
74: 		return PhysicalType::FLOAT;
75: 	case LogicalTypeId::DOUBLE:
76: 		return PhysicalType::DOUBLE;
77: 	case LogicalTypeId::DECIMAL:
78: 		if (width_ <= Decimal::MAX_WIDTH_INT16) {
79: 			return PhysicalType::INT16;
80: 		} else if (width_ <= Decimal::MAX_WIDTH_INT32) {
81: 			return PhysicalType::INT32;
82: 		} else if (width_ <= Decimal::MAX_WIDTH_INT64) {
83: 			return PhysicalType::INT64;
84: 		} else if (width_ <= Decimal::MAX_WIDTH_INT128) {
85: 			return PhysicalType::INT128;
86: 		} else {
87: 			throw NotImplementedException("Widths bigger than 38 are not supported");
88: 		}
89: 	case LogicalTypeId::VARCHAR:
90: 	case LogicalTypeId::CHAR:
91: 	case LogicalTypeId::BLOB:
92: 		return PhysicalType::VARCHAR;
93: 	case LogicalTypeId::INTERVAL:
94: 		return PhysicalType::INTERVAL;
95: 	case LogicalTypeId::STRUCT:
96: 		return PhysicalType::STRUCT;
97: 	case LogicalTypeId::LIST:
98: 		return PhysicalType::LIST;
99: 	case LogicalTypeId::MAP:
100: 		return PhysicalType::MAP;
101: 	case LogicalTypeId::HASH:
102: 		return PhysicalType::HASH;
103: 	case LogicalTypeId::POINTER:
104: 		return PhysicalType::POINTER;
105: 	case LogicalTypeId::VALIDITY:
106: 		return PhysicalType::BIT;
107: 	case LogicalTypeId::TABLE:
108: 	case LogicalTypeId::ANY:
109: 	case LogicalTypeId::INVALID:
110: 	case LogicalTypeId::UNKNOWN:
111: 		return PhysicalType::INVALID;
112: 	default:
113: 		throw ConversionException("Invalid LogicalType %s", ToString());
114: 	}
115: }
116: 
117: const LogicalType LogicalType::INVALID = LogicalType(LogicalTypeId::INVALID);
118: const LogicalType LogicalType::SQLNULL = LogicalType(LogicalTypeId::SQLNULL);
119: const LogicalType LogicalType::BOOLEAN = LogicalType(LogicalTypeId::BOOLEAN);
120: const LogicalType LogicalType::TINYINT = LogicalType(LogicalTypeId::TINYINT);
121: const LogicalType LogicalType::UTINYINT = LogicalType(LogicalTypeId::UTINYINT);
122: const LogicalType LogicalType::SMALLINT = LogicalType(LogicalTypeId::SMALLINT);
123: const LogicalType LogicalType::USMALLINT = LogicalType(LogicalTypeId::USMALLINT);
124: const LogicalType LogicalType::INTEGER = LogicalType(LogicalTypeId::INTEGER);
125: const LogicalType LogicalType::UINTEGER = LogicalType(LogicalTypeId::UINTEGER);
126: const LogicalType LogicalType::BIGINT = LogicalType(LogicalTypeId::BIGINT);
127: const LogicalType LogicalType::UBIGINT = LogicalType(LogicalTypeId::UBIGINT);
128: const LogicalType LogicalType::HUGEINT = LogicalType(LogicalTypeId::HUGEINT);
129: const LogicalType LogicalType::FLOAT = LogicalType(LogicalTypeId::FLOAT);
130: const LogicalType LogicalType::DECIMAL = LogicalType(LogicalTypeId::DECIMAL);
131: const LogicalType LogicalType::DOUBLE = LogicalType(LogicalTypeId::DOUBLE);
132: const LogicalType LogicalType::DATE = LogicalType(LogicalTypeId::DATE);
133: 
134: const LogicalType LogicalType::TIMESTAMP = LogicalType(LogicalTypeId::TIMESTAMP);
135: const LogicalType LogicalType::TIMESTAMP_MS = LogicalType(LogicalTypeId::TIMESTAMP_MS);
136: const LogicalType LogicalType::TIMESTAMP_NS = LogicalType(LogicalTypeId::TIMESTAMP_NS);
137: const LogicalType LogicalType::TIMESTAMP_S = LogicalType(LogicalTypeId::TIMESTAMP_SEC);
138: 
139: const LogicalType LogicalType::TIME = LogicalType(LogicalTypeId::TIME);
140: const LogicalType LogicalType::HASH = LogicalType(LogicalTypeId::HASH);
141: const LogicalType LogicalType::POINTER = LogicalType(LogicalTypeId::POINTER);
142: 
143: const LogicalType LogicalType::VARCHAR = LogicalType(LogicalTypeId::VARCHAR);
144: 
145: const LogicalType LogicalType::BLOB = LogicalType(LogicalTypeId::BLOB);
146: const LogicalType LogicalType::INTERVAL = LogicalType(LogicalTypeId::INTERVAL);
147: 
148: // TODO these are incomplete and should maybe not exist as such
149: const LogicalType LogicalType::STRUCT = LogicalType(LogicalTypeId::STRUCT);
150: const LogicalType LogicalType::LIST = LogicalType(LogicalTypeId::LIST);
151: const LogicalType LogicalType::MAP = LogicalType(LogicalTypeId::MAP);
152: const LogicalType LogicalType::TABLE = LogicalType(LogicalTypeId::TABLE);
153: 
154: const LogicalType LogicalType::ANY = LogicalType(LogicalTypeId::ANY);
155: 
156: const vector<LogicalType> LogicalType::NUMERIC = {LogicalType::TINYINT,   LogicalType::SMALLINT, LogicalType::INTEGER,
157:                                                   LogicalType::BIGINT,    LogicalType::HUGEINT,  LogicalType::FLOAT,
158:                                                   LogicalType::DOUBLE,    LogicalType::DECIMAL,  LogicalType::UTINYINT,
159:                                                   LogicalType::USMALLINT, LogicalType::UINTEGER, LogicalType::UBIGINT};
160: 
161: const vector<LogicalType> LogicalType::INTEGRAL = {LogicalType::TINYINT,   LogicalType::SMALLINT, LogicalType::INTEGER,
162:                                                    LogicalType::BIGINT,    LogicalType::HUGEINT,  LogicalType::UTINYINT,
163:                                                    LogicalType::USMALLINT, LogicalType::UINTEGER, LogicalType::UBIGINT};
164: 
165: const vector<LogicalType> LogicalType::ALL_TYPES = {
166:     LogicalType::BOOLEAN,   LogicalType::TINYINT,   LogicalType::SMALLINT, LogicalType::INTEGER, LogicalType::BIGINT,
167:     LogicalType::DATE,      LogicalType::TIMESTAMP, LogicalType::DOUBLE,   LogicalType::FLOAT,   LogicalType::VARCHAR,
168:     LogicalType::BLOB,      LogicalType::INTERVAL,  LogicalType::HUGEINT,  LogicalType::DECIMAL, LogicalType::UTINYINT,
169:     LogicalType::USMALLINT, LogicalType::UINTEGER,  LogicalType::UBIGINT};
170: // TODO add LIST/STRUCT here
171: 
172: const LogicalType LOGICAL_ROW_TYPE = LogicalType::BIGINT;
173: const PhysicalType ROW_TYPE = PhysicalType::INT64;
174: 
175: string TypeIdToString(PhysicalType type) {
176: 	switch (type) {
177: 	case PhysicalType::BOOL:
178: 		return "BOOL";
179: 	case PhysicalType::INT8:
180: 		return "INT8";
181: 	case PhysicalType::INT16:
182: 		return "INT16";
183: 	case PhysicalType::INT32:
184: 		return "INT32";
185: 	case PhysicalType::INT64:
186: 		return "INT64";
187: 	case PhysicalType::UINT8:
188: 		return "UINT8";
189: 	case PhysicalType::UINT16:
190: 		return "UINT16";
191: 	case PhysicalType::UINT32:
192: 		return "UINT32";
193: 	case PhysicalType::UINT64:
194: 		return "UINT64";
195: 	case PhysicalType::INT128:
196: 		return "INT128";
197: 	case PhysicalType::HASH:
198: 		return "HASH";
199: 	case PhysicalType::POINTER:
200: 		return "POINTER";
201: 	case PhysicalType::FLOAT:
202: 		return "FLOAT";
203: 	case PhysicalType::DOUBLE:
204: 		return "DOUBLE";
205: 	case PhysicalType::VARCHAR:
206: 		return "VARCHAR";
207: 	case PhysicalType::INTERVAL:
208: 		return "INTERVAL";
209: 	case PhysicalType::STRUCT:
210: 		return "STRUCT<?>";
211: 	case PhysicalType::LIST:
212: 		return "LIST<?>";
213: 	case PhysicalType::MAP:
214: 		return "MAP<?>";
215: 	case PhysicalType::INVALID:
216: 		return "INVALID";
217: 	case PhysicalType::BIT:
218: 		return "BIT";
219: 	default:
220: 		throw ConversionException("Invalid PhysicalType %s", type);
221: 	}
222: }
223: 
224: idx_t GetTypeIdSize(PhysicalType type) {
225: 	switch (type) {
226: 	case PhysicalType::BIT:
227: 	case PhysicalType::BOOL:
228: 		return sizeof(bool);
229: 	case PhysicalType::INT8:
230: 		return sizeof(int8_t);
231: 	case PhysicalType::INT16:
232: 		return sizeof(int16_t);
233: 	case PhysicalType::INT32:
234: 		return sizeof(int32_t);
235: 	case PhysicalType::INT64:
236: 		return sizeof(int64_t);
237: 	case PhysicalType::UINT8:
238: 		return sizeof(uint8_t);
239: 	case PhysicalType::UINT16:
240: 		return sizeof(uint16_t);
241: 	case PhysicalType::UINT32:
242: 		return sizeof(uint32_t);
243: 	case PhysicalType::UINT64:
244: 		return sizeof(uint64_t);
245: 	case PhysicalType::INT128:
246: 		return sizeof(hugeint_t);
247: 	case PhysicalType::FLOAT:
248: 		return sizeof(float);
249: 	case PhysicalType::DOUBLE:
250: 		return sizeof(double);
251: 	case PhysicalType::HASH:
252: 		return sizeof(hash_t);
253: 	case PhysicalType::POINTER:
254: 		return sizeof(uintptr_t);
255: 	case PhysicalType::VARCHAR:
256: 		return sizeof(string_t);
257: 	case PhysicalType::INTERVAL:
258: 		return sizeof(interval_t);
259: 	case PhysicalType::MAP:
260: 	case PhysicalType::STRUCT:
261: 		return 0; // no own payload
262: 	case PhysicalType::LIST:
263: 		return 16; // offset + len
264: 
265: 	default:
266: 		throw ConversionException("Invalid PhysicalType %s", type);
267: 	}
268: }
269: 
270: bool TypeIsConstantSize(PhysicalType type) {
271: 	return (type >= PhysicalType::BOOL && type <= PhysicalType::DOUBLE) ||
272: 	       (type >= PhysicalType::FIXED_SIZE_BINARY && type <= PhysicalType::INTERVAL) || type == PhysicalType::HASH ||
273: 	       type == PhysicalType::POINTER || type == PhysicalType::INTERVAL || type == PhysicalType::INT128;
274: }
275: bool TypeIsIntegral(PhysicalType type) {
276: 	return (type >= PhysicalType::UINT8 && type <= PhysicalType::INT64) || type == PhysicalType::HASH ||
277: 	       type == PhysicalType::POINTER || type == PhysicalType::INT128;
278: }
279: bool TypeIsNumeric(PhysicalType type) {
280: 	return (type >= PhysicalType::UINT8 && type <= PhysicalType::DOUBLE) || type == PhysicalType::INT128;
281: }
282: bool TypeIsInteger(PhysicalType type) {
283: 	return (type >= PhysicalType::UINT8 && type <= PhysicalType::INT64) || type == PhysicalType::INT128;
284: }
285: 
286: void LogicalType::Serialize(Serializer &serializer) const {
287: 	serializer.Write<LogicalTypeId>(id_);
288: 	serializer.Write<uint8_t>(width_);
289: 	serializer.Write<uint8_t>(scale_);
290: 	serializer.WriteString(collation_);
291: 	serializer.Write<uint16_t>(child_types_.size());
292: 	for (auto &entry : child_types_) {
293: 		serializer.WriteString(entry.first);
294: 		entry.second.Serialize(serializer);
295: 	}
296: }
297: 
298: LogicalType LogicalType::Deserialize(Deserializer &source) {
299: 	auto id = source.Read<LogicalTypeId>();
300: 	auto width = source.Read<uint8_t>();
301: 	auto scale = source.Read<uint8_t>();
302: 	auto collation = source.Read<string>();
303: 	child_list_t<LogicalType> children;
304: 	auto child_count = source.Read<uint16_t>();
305: 	for (uint16_t i = 0; i < child_count; i++) {
306: 		string name = source.Read<string>();
307: 		LogicalType child_type = LogicalType::Deserialize(source);
308: 		children.push_back(make_pair(move(name), move(child_type)));
309: 	}
310: 	return LogicalType(id, width, scale, collation, move(children));
311: }
312: 
313: string LogicalTypeIdToString(LogicalTypeId id) {
314: 	switch (id) {
315: 	case LogicalTypeId::BOOLEAN:
316: 		return "BOOLEAN";
317: 	case LogicalTypeId::TINYINT:
318: 		return "TINYINT";
319: 	case LogicalTypeId::SMALLINT:
320: 		return "SMALLINT";
321: 	case LogicalTypeId::INTEGER:
322: 		return "INTEGER";
323: 	case LogicalTypeId::BIGINT:
324: 		return "BIGINT";
325: 	case LogicalTypeId::HUGEINT:
326: 		return "HUGEINT";
327: 	case LogicalTypeId::UTINYINT:
328: 		return "UTINYINT";
329: 	case LogicalTypeId::USMALLINT:
330: 		return "USMALLINT";
331: 	case LogicalTypeId::UINTEGER:
332: 		return "UINTEGER";
333: 	case LogicalTypeId::UBIGINT:
334: 		return "UBIGINT";
335: 	case LogicalTypeId::DATE:
336: 		return "DATE";
337: 	case LogicalTypeId::TIME:
338: 		return "TIME";
339: 	case LogicalTypeId::TIMESTAMP:
340: 		return "TIMESTAMP";
341: 	case LogicalTypeId::TIMESTAMP_MS:
342: 		return "TIMESTAMP (MS)";
343: 	case LogicalTypeId::TIMESTAMP_NS:
344: 		return "TIMESTAMP (NS)";
345: 	case LogicalTypeId::TIMESTAMP_SEC:
346: 		return "TIMESTAMP (SEC)";
347: 	case LogicalTypeId::FLOAT:
348: 		return "FLOAT";
349: 	case LogicalTypeId::DOUBLE:
350: 		return "DOUBLE";
351: 	case LogicalTypeId::DECIMAL:
352: 		return "DECIMAL";
353: 	case LogicalTypeId::VARCHAR:
354: 		return "VARCHAR";
355: 	case LogicalTypeId::BLOB:
356: 		return "BLOB";
357: 	case LogicalTypeId::CHAR:
358: 		return "CHAR";
359: 	case LogicalTypeId::INTERVAL:
360: 		return "INTERVAL";
361: 	case LogicalTypeId::SQLNULL:
362: 		return "NULL";
363: 	case LogicalTypeId::ANY:
364: 		return "ANY";
365: 	case LogicalTypeId::VALIDITY:
366: 		return "VALIDITY";
367: 	case LogicalTypeId::STRUCT:
368: 		return "STRUCT<?>";
369: 	case LogicalTypeId::LIST:
370: 		return "LIST<?>";
371: 	case LogicalTypeId::MAP:
372: 		return "MAP<?>";
373: 	case LogicalTypeId::HASH:
374: 		return "HASH";
375: 	case LogicalTypeId::POINTER:
376: 		return "POINTER";
377: 	case LogicalTypeId::TABLE:
378: 		return "TABLE";
379: 	case LogicalTypeId::INVALID:
380: 		return "INVALID";
381: 	case LogicalTypeId::UNKNOWN:
382: 		return "UNKNOWN";
383: 	}
384: 	return "UNDEFINED";
385: }
386: 
387: string LogicalType::ToString() const {
388: 	switch (id_) {
389: 	case LogicalTypeId::STRUCT: {
390: 		string ret = "STRUCT<";
391: 		for (size_t i = 0; i < child_types_.size(); i++) {
392: 			ret += child_types_[i].first + ": " + child_types_[i].second.ToString();
393: 			if (i < child_types_.size() - 1) {
394: 				ret += ", ";
395: 			}
396: 		}
397: 		ret += ">";
398: 		return ret;
399: 	}
400: 	case LogicalTypeId::LIST: {
401: 		if (child_types_.empty()) {
402: 			return "LIST<?>";
403: 		}
404: 		if (child_types_.size() != 1) {
405: 			throw Exception("List needs a single child element");
406: 		}
407: 		return "LIST<" + child_types_[0].second.ToString() + ">";
408: 	}
409: 	case LogicalTypeId::MAP: {
410: 		if (child_types_.empty()) {
411: 			return "MAP<?>";
412: 		}
413: 		if (child_types_.size() != 2) {
414: 			throw Exception("Map needs exactly two child elements");
415: 		}
416: 		return "MAP<" + child_types_[0].second.child_types()[0].second.ToString() + ", " +
417: 		       child_types_[1].second.child_types()[0].second.ToString() + ">";
418: 	}
419: 	case LogicalTypeId::DECIMAL: {
420: 		if (width_ == 0) {
421: 			return "DECIMAL";
422: 		}
423: 		return StringUtil::Format("DECIMAL(%d,%d)", width_, scale_);
424: 	}
425: 	default:
426: 		return LogicalTypeIdToString(id_);
427: 	}
428: }
429: 
430: LogicalType TransformStringToLogicalType(const string &str) {
431: 	auto lower_str = StringUtil::Lower(str);
432: 	// Transform column type
433: 	if (lower_str == "int" || lower_str == "int4" || lower_str == "signed" || lower_str == "integer" ||
434: 	    lower_str == "integral" || lower_str == "int32") {
435: 		return LogicalType::INTEGER;
436: 	} else if (lower_str == "varchar" || lower_str == "bpchar" || lower_str == "text" || lower_str == "string" ||
437: 	           lower_str == "char") {
438: 		return LogicalType::VARCHAR;
439: 	} else if (lower_str == "bytea" || lower_str == "blob" || lower_str == "varbinary" || lower_str == "binary") {
440: 		return LogicalType::BLOB;
441: 	} else if (lower_str == "int8" || lower_str == "bigint" || lower_str == "int64" || lower_str == "long") {
442: 		return LogicalType::BIGINT;
443: 	} else if (lower_str == "int2" || lower_str == "smallint" || lower_str == "short" || lower_str == "int16") {
444: 		return LogicalType::SMALLINT;
445: 	} else if (lower_str == "timestamp" || lower_str == "datetime" || lower_str == "timestamp_us") {
446: 		return LogicalType::TIMESTAMP;
447: 	} else if (lower_str == "timestamp_ms") {
448: 		return LogicalType::TIMESTAMP_MS;
449: 	} else if (lower_str == "timestamp_ns") {
450: 		return LogicalType::TIMESTAMP_NS;
451: 	} else if (lower_str == "timestamp_s") {
452: 		return LogicalType::TIMESTAMP_S;
453: 	} else if (lower_str == "bool" || lower_str == "boolean" || lower_str == "logical") {
454: 		return LogicalType(LogicalTypeId::BOOLEAN);
455: 	} else if (lower_str == "real" || lower_str == "float4" || lower_str == "float") {
456: 		return LogicalType::FLOAT;
457: 	} else if (lower_str == "decimal" || lower_str == "dec" || lower_str == "numeric") {
458: 		return LogicalType(LogicalTypeId::DECIMAL, 18, 3);
459: 	} else if (lower_str == "double" || lower_str == "float8" || lower_str == "decimal") {
460: 		return LogicalType::DOUBLE;
461: 	} else if (lower_str == "tinyint" || lower_str == "int1") {
462: 		return LogicalType::TINYINT;
463: 	} else if (lower_str == "date") {
464: 		return LogicalType::DATE;
465: 	} else if (lower_str == "time") {
466: 		return LogicalType::TIME;
467: 	} else if (lower_str == "interval") {
468: 		return LogicalType::INTERVAL;
469: 	} else if (lower_str == "hugeint" || lower_str == "int128") {
470: 		return LogicalType::HUGEINT;
471: 	} else if (lower_str == "struct" || lower_str == "row") {
472: 		return LogicalType::STRUCT;
473: 	} else if (lower_str == "map") {
474: 		return LogicalType::MAP;
475: 	} else if (lower_str == "utinyint") {
476: 		return LogicalType::UTINYINT;
477: 	} else if (lower_str == "usmallint") {
478: 		return LogicalType::USMALLINT;
479: 	} else if (lower_str == "uinteger") {
480: 		return LogicalType::UINTEGER;
481: 	} else if (lower_str == "ubigint") {
482: 		return LogicalType::UBIGINT;
483: 	} else {
484: 		throw NotImplementedException("DataType %s not supported yet...\n", str);
485: 	}
486: }
487: 
488: bool LogicalType::IsIntegral() const {
489: 	switch (id_) {
490: 	case LogicalTypeId::TINYINT:
491: 	case LogicalTypeId::SMALLINT:
492: 	case LogicalTypeId::INTEGER:
493: 	case LogicalTypeId::BIGINT:
494: 	case LogicalTypeId::UTINYINT:
495: 	case LogicalTypeId::USMALLINT:
496: 	case LogicalTypeId::UINTEGER:
497: 	case LogicalTypeId::UBIGINT:
498: 	case LogicalTypeId::HUGEINT:
499: 		return true;
500: 	default:
501: 		return false;
502: 	}
503: }
504: 
505: bool LogicalType::IsNumeric() const {
506: 	switch (id_) {
507: 	case LogicalTypeId::TINYINT:
508: 	case LogicalTypeId::SMALLINT:
509: 	case LogicalTypeId::INTEGER:
510: 	case LogicalTypeId::BIGINT:
511: 	case LogicalTypeId::HUGEINT:
512: 	case LogicalTypeId::FLOAT:
513: 	case LogicalTypeId::DOUBLE:
514: 	case LogicalTypeId::DECIMAL:
515: 	case LogicalTypeId::UTINYINT:
516: 	case LogicalTypeId::USMALLINT:
517: 	case LogicalTypeId::UINTEGER:
518: 	case LogicalTypeId::UBIGINT:
519: 		return true;
520: 	default:
521: 		return false;
522: 	}
523: }
524: 
525: bool LogicalType::GetDecimalProperties(uint8_t &width, uint8_t &scale) const {
526: 	switch (id_) {
527: 	case LogicalTypeId::SQLNULL:
528: 		width = 0;
529: 		scale = 0;
530: 		break;
531: 	case LogicalTypeId::BOOLEAN:
532: 		width = 1;
533: 		scale = 0;
534: 		break;
535: 	case LogicalTypeId::TINYINT:
536: 		// tinyint: [-127, 127] = DECIMAL(3,0)
537: 		width = 3;
538: 		scale = 0;
539: 		break;
540: 	case LogicalTypeId::SMALLINT:
541: 		// smallint: [-32767, 32767] = DECIMAL(5,0)
542: 		width = 5;
543: 		scale = 0;
544: 		break;
545: 	case LogicalTypeId::INTEGER:
546: 		// integer: [-2147483647, 2147483647] = DECIMAL(10,0)
547: 		width = 10;
548: 		scale = 0;
549: 		break;
550: 	case LogicalTypeId::BIGINT:
551: 		// bigint: [-9223372036854775807, 9223372036854775807] = DECIMAL(19,0)
552: 		width = 19;
553: 		scale = 0;
554: 		break;
555: 	case LogicalTypeId::UTINYINT:
556: 		// UInt8 — [0 : 255]
557: 		width = 3;
558: 		scale = 0;
559: 		break;
560: 	case LogicalTypeId::USMALLINT:
561: 		// UInt16 — [0 : 65535]
562: 		width = 5;
563: 		scale = 0;
564: 		break;
565: 	case LogicalTypeId::UINTEGER:
566: 		// UInt32 — [0 : 4294967295]
567: 		width = 10;
568: 		scale = 0;
569: 		break;
570: 	case LogicalTypeId::UBIGINT:
571: 		// UInt64 — [0 : 18446744073709551615]
572: 		width = 20;
573: 		scale = 0;
574: 		break;
575: 	case LogicalTypeId::HUGEINT:
576: 		// hugeint: max size decimal (38, 0)
577: 		// note that a hugeint is not guaranteed to fit in this
578: 		width = 38;
579: 		scale = 0;
580: 		break;
581: 	case LogicalTypeId::DECIMAL:
582: 		width = width_;
583: 		scale = scale_;
584: 		break;
585: 	default:
586: 		return false;
587: 	}
588: 	return true;
589: }
590: 
591: bool LogicalType::IsMoreGenericThan(LogicalType &other) const {
592: 	if (other.id() == id_) {
593: 		return false;
594: 	}
595: 
596: 	if (other.id() == LogicalTypeId::SQLNULL) {
597: 		return true;
598: 	}
599: 
600: 	// all integer types can cast from INTEGER
601: 	// this is because INTEGER is the smallest type considered by the automatic csv sniffer
602: 	switch (id_) {
603: 	case LogicalTypeId::SMALLINT:
604: 		switch (other.id()) {
605: 		case LogicalTypeId::BOOLEAN:
606: 		case LogicalTypeId::TINYINT:
607: 			return true;
608: 		default:
609: 			return false;
610: 		}
611: 	case LogicalTypeId::INTEGER:
612: 		switch (other.id()) {
613: 		case LogicalTypeId::BOOLEAN:
614: 		case LogicalTypeId::TINYINT:
615: 		case LogicalTypeId::SMALLINT:
616: 			return true;
617: 		default:
618: 			return false;
619: 		}
620: 	case LogicalTypeId::BIGINT:
621: 		switch (other.id()) {
622: 		case LogicalTypeId::BOOLEAN:
623: 		case LogicalTypeId::TINYINT:
624: 		case LogicalTypeId::SMALLINT:
625: 		case LogicalTypeId::INTEGER:
626: 			return true;
627: 		default:
628: 			return false;
629: 		}
630: 	case LogicalTypeId::HUGEINT:
631: 		switch (other.id()) {
632: 		case LogicalTypeId::BOOLEAN:
633: 		case LogicalTypeId::TINYINT:
634: 		case LogicalTypeId::SMALLINT:
635: 		case LogicalTypeId::INTEGER:
636: 		case LogicalTypeId::BIGINT:
637: 			return true;
638: 		default:
639: 			return false;
640: 		}
641: 	case LogicalTypeId::FLOAT:
642: 		switch (other.id()) {
643: 		case LogicalTypeId::BOOLEAN:
644: 		case LogicalTypeId::TINYINT:
645: 		case LogicalTypeId::SMALLINT:
646: 		case LogicalTypeId::INTEGER:
647: 		case LogicalTypeId::BIGINT:
648: 			return true;
649: 		default:
650: 			return false;
651: 		}
652: 	case LogicalTypeId::DOUBLE:
653: 		switch (other.id()) {
654: 		case LogicalTypeId::BOOLEAN:
655: 		case LogicalTypeId::TINYINT:
656: 		case LogicalTypeId::SMALLINT:
657: 		case LogicalTypeId::INTEGER:
658: 		case LogicalTypeId::BIGINT:
659: 		case LogicalTypeId::FLOAT:
660: 			return true;
661: 		default:
662: 			return false;
663: 		}
664: 	case LogicalTypeId::DATE:
665: 		return false;
666: 	case LogicalTypeId::TIMESTAMP: {
667: 		switch (other.id()) {
668: 		case LogicalTypeId::TIMESTAMP_NS:
669: 		case LogicalTypeId::TIME:
670: 		case LogicalTypeId::DATE:
671: 			return true;
672: 		default:
673: 			return false;
674: 		}
675: 	}
676: 	case LogicalTypeId::TIMESTAMP_NS: {
677: 		switch (other.id()) {
678: 		case LogicalTypeId::TIMESTAMP:
679: 		case LogicalTypeId::TIMESTAMP_MS:
680: 		case LogicalTypeId::TIMESTAMP_SEC:
681: 		case LogicalTypeId::TIME:
682: 		case LogicalTypeId::DATE:
683: 			return true;
684: 		default:
685: 			return false;
686: 		}
687: 	}
688: 	case LogicalTypeId::TIMESTAMP_MS: {
689: 		switch (other.id()) {
690: 		case LogicalTypeId::TIMESTAMP_SEC:
691: 		case LogicalTypeId::TIMESTAMP:
692: 		case LogicalTypeId::TIME:
693: 		case LogicalTypeId::DATE:
694: 			return true;
695: 		default:
696: 			return false;
697: 		}
698: 	}
699: 	case LogicalTypeId::TIMESTAMP_SEC: {
700: 		switch (other.id()) {
701: 		case LogicalTypeId::TIME:
702: 		case LogicalTypeId::DATE:
703: 			return true;
704: 		default:
705: 			return false;
706: 		}
707: 	}
708: 
709: 	case LogicalTypeId::VARCHAR:
710: 		return true;
711: 	default:
712: 		return false;
713: 	}
714: }
715: 
716: LogicalType LogicalType::MaxLogicalType(const LogicalType &left, const LogicalType &right) {
717: 	if (left.id() < right.id()) {
718: 		return right;
719: 	} else if (right.id() < left.id()) {
720: 		return left;
721: 	} else {
722: 		if (left.id() == LogicalTypeId::VARCHAR) {
723: 			// varchar: use type that has collation (if any)
724: 			if (right.collation().empty()) {
725: 				return left;
726: 			} else {
727: 				return right;
728: 			}
729: 		} else if (left.id() == LogicalTypeId::DECIMAL) {
730: 			// use max width/scale of the two types
731: 			return LogicalType(LogicalTypeId::DECIMAL, MaxValue<uint8_t>(left.width(), right.width()),
732: 			                   MaxValue<uint8_t>(left.scale(), right.scale()));
733: 		} else if (left.id() == LogicalTypeId::LIST) {
734: 			// list: perform max recursively on child type
735: 			child_list_t<LogicalType> child_types;
736: 			child_types.push_back(
737: 			    make_pair(left.child_types()[0].first,
738: 			              MaxLogicalType(left.child_types()[0].second, right.child_types()[0].second)));
739: 			return LogicalType(LogicalTypeId::LIST, move(child_types));
740: 		} else {
741: 			// types are equal but no extra specifier: just return the type
742: 			// FIXME: LIST and STRUCT?
743: 			return left;
744: 		}
745: 	}
746: }
747: 
748: void LogicalType::Verify() const {
749: #ifdef DEBUG
750: 	if (id_ == LogicalTypeId::DECIMAL) {
751: 		D_ASSERT(width_ >= 1 && width_ <= Decimal::MAX_WIDTH_DECIMAL);
752: 		D_ASSERT(scale_ >= 0 && scale_ <= width_);
753: 	}
754: #endif
755: }
756: 
757: bool ApproxEqual(float ldecimal, float rdecimal) {
758: 	float epsilon = std::fabs(rdecimal) * 0.01;
759: 	return std::fabs(ldecimal - rdecimal) <= epsilon;
760: }
761: 
762: bool ApproxEqual(double ldecimal, double rdecimal) {
763: 	double epsilon = std::fabs(rdecimal) * 0.01;
764: 	return std::fabs(ldecimal - rdecimal) <= epsilon;
765: }
766: 
767: } // namespace duckdb
[end of src/common/types.cpp]
[start of src/common/types/chunk_collection.cpp]
1: #include "duckdb/common/types/chunk_collection.hpp"
2: 
3: #include "duckdb/common/exception.hpp"
4: #include "duckdb/common/printer.hpp"
5: #include "duckdb/common/value_operations/value_operations.hpp"
6: #include "duckdb/common/operator/comparison_operators.hpp"
7: #include "duckdb/common/assert.hpp"
8: 
9: #include <algorithm>
10: #include <cstring>
11: #include <queue>
12: 
13: namespace duckdb {
14: 
15: void ChunkCollection::Verify() {
16: #ifdef DEBUG
17: 	for (auto &chunk : chunks) {
18: 		chunk->Verify();
19: 	}
20: #endif
21: }
22: 
23: void ChunkCollection::Append(ChunkCollection &other) {
24: 	for (auto &chunk : other.chunks) {
25: 		Append(*chunk);
26: 	}
27: }
28: 
29: void ChunkCollection::Merge(ChunkCollection &other) {
30: 	if (other.count == 0) {
31: 		return;
32: 	}
33: 	if (count == 0) {
34: 		chunks = move(other.chunks);
35: 		types = move(other.types);
36: 		count = other.count;
37: 		return;
38: 	}
39: 	unique_ptr<DataChunk> old_back;
40: 	if (!chunks.empty() && chunks.back()->size() != STANDARD_VECTOR_SIZE) {
41: 		old_back = move(chunks.back());
42: 		chunks.pop_back();
43: 		count -= old_back->size();
44: 	}
45: 	for (auto &chunk : other.chunks) {
46: 		chunks.push_back(move(chunk));
47: 	}
48: 	count += other.count;
49: 	if (old_back) {
50: 		Append(*old_back);
51: 	}
52: 	Verify();
53: }
54: 
55: void ChunkCollection::Append(DataChunk &new_chunk) {
56: 	if (new_chunk.size() == 0) {
57: 		return;
58: 	}
59: 	new_chunk.Verify();
60: 
61: 	// we have to ensure that every chunk in the ChunkCollection is completely
62: 	// filled, otherwise our O(1) lookup in GetValue and SetValue does not work
63: 	// first fill the latest chunk, if it exists
64: 	count += new_chunk.size();
65: 
66: 	idx_t remaining_data = new_chunk.size();
67: 	idx_t offset = 0;
68: 	if (chunks.empty()) {
69: 		// first chunk
70: 		types = new_chunk.GetTypes();
71: 	} else {
72: 		// the types of the new chunk should match the types of the previous one
73: 		D_ASSERT(types.size() == new_chunk.ColumnCount());
74: 		auto new_types = new_chunk.GetTypes();
75: 		for (idx_t i = 0; i < types.size(); i++) {
76: 			if (new_types[i] != types[i]) {
77: 				throw TypeMismatchException(new_types[i], types[i], "Type mismatch when combining rows");
78: 			}
79: 			if (types[i].InternalType() == PhysicalType::LIST) {
80: 				for (auto &chunk :
81: 				     chunks) { // need to check all the chunks because they can have only-null list entries
82: 					auto &chunk_vec = chunk->data[i];
83: 					auto &new_vec = new_chunk.data[i];
84: 					if (ListVector::HasEntry(chunk_vec) && ListVector::HasEntry(new_vec)) {
85: 						auto &chunk_type = chunk_vec.GetType();
86: 						auto &new_type = new_vec.GetType();
87: 						if (chunk_type != new_type) {
88: 							throw TypeMismatchException(chunk_type, new_type, "Type mismatch when combining lists");
89: 						}
90: 					}
91: 				}
92: 			}
93: 			// TODO check structs, too
94: 		}
95: 
96: 		// first append data to the current chunk
97: 		DataChunk &last_chunk = *chunks.back();
98: 		idx_t added_data = MinValue<idx_t>(remaining_data, STANDARD_VECTOR_SIZE - last_chunk.size());
99: 		if (added_data > 0) {
100: 			// copy <added_data> elements to the last chunk
101: 			new_chunk.Normalify();
102: 			// have to be careful here: setting the cardinality without calling normalify can cause incorrect partial
103: 			// decompression
104: 			idx_t old_count = new_chunk.size();
105: 			new_chunk.SetCardinality(added_data);
106: 
107: 			last_chunk.Append(new_chunk);
108: 			remaining_data -= added_data;
109: 			// reset the chunk to the old data
110: 			new_chunk.SetCardinality(old_count);
111: 			offset = added_data;
112: 		}
113: 	}
114: 
115: 	if (remaining_data > 0) {
116: 		// create a new chunk and fill it with the remainder
117: 		auto chunk = make_unique<DataChunk>();
118: 		chunk->Initialize(types);
119: 		new_chunk.Copy(*chunk, offset);
120: 		chunks.push_back(move(chunk));
121: 	}
122: }
123: 
124: // returns an int similar to a C comparator:
125: // -1 if left < right
126: // 0 if left == right
127: // 1 if left > right
128: 
129: template <class TYPE>
130: static int8_t TemplatedCompareValue(Vector &left_vec, Vector &right_vec, idx_t left_idx, idx_t right_idx) {
131: 	D_ASSERT(left_vec.GetType() == right_vec.GetType());
132: 	auto left_val = FlatVector::GetData<TYPE>(left_vec)[left_idx];
133: 	auto right_val = FlatVector::GetData<TYPE>(right_vec)[right_idx];
134: 	if (Equals::Operation<TYPE>(left_val, right_val)) {
135: 		return 0;
136: 	}
137: 	if (LessThan::Operation<TYPE>(left_val, right_val)) {
138: 		return -1;
139: 	}
140: 	return 1;
141: }
142: 
143: // return type here is int32 because strcmp() on some platforms returns rather large values
144: static int32_t CompareValue(Vector &left_vec, Vector &right_vec, idx_t vector_idx_left, idx_t vector_idx_right,
145:                             OrderByNullType null_order) {
146: 	auto left_null = FlatVector::IsNull(left_vec, vector_idx_left);
147: 	auto right_null = FlatVector::IsNull(right_vec, vector_idx_right);
148: 
149: 	if (left_null && right_null) {
150: 		return 0;
151: 	} else if (right_null) {
152: 		return null_order == OrderByNullType::NULLS_FIRST ? 1 : -1;
153: 	} else if (left_null) {
154: 		return null_order == OrderByNullType::NULLS_FIRST ? -1 : 1;
155: 	}
156: 
157: 	switch (left_vec.GetType().InternalType()) {
158: 	case PhysicalType::BOOL:
159: 	case PhysicalType::INT8:
160: 		return TemplatedCompareValue<int8_t>(left_vec, right_vec, vector_idx_left, vector_idx_right);
161: 	case PhysicalType::INT16:
162: 		return TemplatedCompareValue<int16_t>(left_vec, right_vec, vector_idx_left, vector_idx_right);
163: 	case PhysicalType::INT32:
164: 		return TemplatedCompareValue<int32_t>(left_vec, right_vec, vector_idx_left, vector_idx_right);
165: 	case PhysicalType::INT64:
166: 		return TemplatedCompareValue<int64_t>(left_vec, right_vec, vector_idx_left, vector_idx_right);
167: 	case PhysicalType::UINT8:
168: 		return TemplatedCompareValue<uint8_t>(left_vec, right_vec, vector_idx_left, vector_idx_right);
169: 	case PhysicalType::UINT16:
170: 		return TemplatedCompareValue<uint16_t>(left_vec, right_vec, vector_idx_left, vector_idx_right);
171: 	case PhysicalType::UINT32:
172: 		return TemplatedCompareValue<uint32_t>(left_vec, right_vec, vector_idx_left, vector_idx_right);
173: 	case PhysicalType::UINT64:
174: 		return TemplatedCompareValue<uint64_t>(left_vec, right_vec, vector_idx_left, vector_idx_right);
175: 	case PhysicalType::INT128:
176: 		return TemplatedCompareValue<hugeint_t>(left_vec, right_vec, vector_idx_left, vector_idx_right);
177: 	case PhysicalType::FLOAT:
178: 		return TemplatedCompareValue<float>(left_vec, right_vec, vector_idx_left, vector_idx_right);
179: 	case PhysicalType::DOUBLE:
180: 		return TemplatedCompareValue<double>(left_vec, right_vec, vector_idx_left, vector_idx_right);
181: 	case PhysicalType::VARCHAR:
182: 		return TemplatedCompareValue<string_t>(left_vec, right_vec, vector_idx_left, vector_idx_right);
183: 	case PhysicalType::INTERVAL:
184: 		return TemplatedCompareValue<interval_t>(left_vec, right_vec, vector_idx_left, vector_idx_right);
185: 	default:
186: 		throw NotImplementedException("Type for comparison");
187: 	}
188: }
189: 
190: static int CompareTuple(ChunkCollection *sort_by, vector<OrderType> &desc, vector<OrderByNullType> &null_order,
191:                         idx_t left, idx_t right) {
192: 	D_ASSERT(sort_by);
193: 
194: 	idx_t chunk_idx_left = left / STANDARD_VECTOR_SIZE;
195: 	idx_t chunk_idx_right = right / STANDARD_VECTOR_SIZE;
196: 	idx_t vector_idx_left = left % STANDARD_VECTOR_SIZE;
197: 	idx_t vector_idx_right = right % STANDARD_VECTOR_SIZE;
198: 
199: 	auto &left_chunk = sort_by->GetChunk(chunk_idx_left);
200: 	auto &right_chunk = sort_by->GetChunk(chunk_idx_right);
201: 
202: 	for (idx_t col_idx = 0; col_idx < desc.size(); col_idx++) {
203: 		auto order_type = desc[col_idx];
204: 
205: 		auto &left_vec = left_chunk.data[col_idx];
206: 		auto &right_vec = right_chunk.data[col_idx];
207: 
208: 		D_ASSERT(left_vec.GetVectorType() == VectorType::FLAT_VECTOR);
209: 		D_ASSERT(right_vec.GetVectorType() == VectorType::FLAT_VECTOR);
210: 		D_ASSERT(left_vec.GetType() == right_vec.GetType());
211: 
212: 		auto comp_res = CompareValue(left_vec, right_vec, vector_idx_left, vector_idx_right, null_order[col_idx]);
213: 
214: 		if (comp_res == 0) {
215: 			continue;
216: 		}
217: 		return comp_res < 0 ? (order_type == OrderType::ASCENDING ? -1 : 1)
218: 		                    : (order_type == OrderType::ASCENDING ? 1 : -1);
219: 	}
220: 	return 0;
221: }
222: 
223: static int64_t QuicksortInitial(ChunkCollection *sort_by, vector<OrderType> &desc, vector<OrderByNullType> &null_order,
224:                                 idx_t *result) {
225: 	// select pivot
226: 	int64_t pivot = 0;
227: 	int64_t low = 0, high = sort_by->Count() - 1;
228: 	// now insert elements
229: 	for (idx_t i = 1; i < sort_by->Count(); i++) {
230: 		if (CompareTuple(sort_by, desc, null_order, i, pivot) <= 0) {
231: 			result[low++] = i;
232: 		} else {
233: 			result[high--] = i;
234: 		}
235: 	}
236: 	D_ASSERT(low == high);
237: 	result[low] = pivot;
238: 	return low;
239: }
240: 
241: struct QuicksortInfo {
242: 	QuicksortInfo(int64_t left_p, int64_t right_p) : left(left_p), right(right_p) {
243: 	}
244: 
245: 	int64_t left;
246: 	int64_t right;
247: };
248: 
249: struct QuicksortStack {
250: 	std::queue<QuicksortInfo> info_queue;
251: 
252: 	QuicksortInfo Pop() {
253: 		auto element = info_queue.front();
254: 		info_queue.pop();
255: 		return element;
256: 	}
257: 
258: 	bool IsEmpty() {
259: 		return info_queue.empty();
260: 	}
261: 
262: 	void Enqueue(int64_t left, int64_t right) {
263: 		if (left >= right) {
264: 			return;
265: 		}
266: 		info_queue.emplace(left, right);
267: 	}
268: };
269: 
270: static void QuicksortInPlace(ChunkCollection *sort_by, vector<OrderType> &desc, vector<OrderByNullType> &null_order,
271:                              idx_t *result, QuicksortInfo info, QuicksortStack &stack) {
272: 	auto left = info.left;
273: 	auto right = info.right;
274: 
275: 	D_ASSERT(left < right);
276: 
277: 	int64_t middle = left + (right - left) / 2;
278: 	int64_t pivot = result[middle];
279: 	// move the mid point value to the front.
280: 	int64_t i = left + 1;
281: 	int64_t j = right;
282: 
283: 	std::swap(result[middle], result[left]);
284: 	bool all_equal = true;
285: 	while (i <= j) {
286: 		if (result) {
287: 			while (i <= j) {
288: 				int cmp = CompareTuple(sort_by, desc, null_order, result[i], pivot);
289: 				if (cmp < 0) {
290: 					all_equal = false;
291: 				} else if (cmp > 0) {
292: 					all_equal = false;
293: 					break;
294: 				}
295: 				i++;
296: 			}
297: 		}
298: 
299: 		while (i <= j && CompareTuple(sort_by, desc, null_order, result[j], pivot) > 0) {
300: 			j--;
301: 		}
302: 
303: 		if (i < j) {
304: 			std::swap(result[i], result[j]);
305: 		}
306: 	}
307: 	std::swap(result[i - 1], result[left]);
308: 	int64_t part = i - 1;
309: 
310: 	if (all_equal) {
311: 		return;
312: 	}
313: 
314: 	stack.Enqueue(left, part - 1);
315: 	stack.Enqueue(part + 1, right);
316: }
317: 
318: void ChunkCollection::Sort(vector<OrderType> &desc, vector<OrderByNullType> &null_order, idx_t result[]) {
319: 	D_ASSERT(result);
320: 	if (count == 0) {
321: 		return;
322: 	}
323: 	// start off with an initial quicksort
324: 	int64_t part = QuicksortInitial(this, desc, null_order, result);
325: 
326: 	// now continuously perform
327: 	QuicksortStack stack;
328: 	stack.Enqueue(0, part);
329: 	stack.Enqueue(part + 1, count - 1);
330: 	while (!stack.IsEmpty()) {
331: 		auto element = stack.Pop();
332: 		QuicksortInPlace(this, desc, null_order, result, element, stack);
333: 	}
334: }
335: 
336: // FIXME make this more efficient by not using the Value API
337: // just use memcpy in the vectors
338: // assert that there is no selection list
339: void ChunkCollection::Reorder(idx_t order_org[]) {
340: 	auto order = unique_ptr<idx_t[]>(new idx_t[count]);
341: 	memcpy(order.get(), order_org, sizeof(idx_t) * count);
342: 
343: 	// adapted from https://stackoverflow.com/a/7366196/2652376
344: 
345: 	auto val_buf = vector<Value>();
346: 	val_buf.resize(ColumnCount());
347: 
348: 	idx_t j, k;
349: 	for (idx_t i = 0; i < count; i++) {
350: 		for (idx_t col_idx = 0; col_idx < ColumnCount(); col_idx++) {
351: 			val_buf[col_idx] = GetValue(col_idx, i);
352: 		}
353: 		j = i;
354: 		while (true) {
355: 			k = order[j];
356: 			order[j] = j;
357: 			if (k == i) {
358: 				break;
359: 			}
360: 			for (idx_t col_idx = 0; col_idx < ColumnCount(); col_idx++) {
361: 				SetValue(col_idx, j, GetValue(col_idx, k));
362: 			}
363: 			j = k;
364: 		}
365: 		for (idx_t col_idx = 0; col_idx < ColumnCount(); col_idx++) {
366: 			SetValue(col_idx, j, val_buf[col_idx]);
367: 		}
368: 	}
369: }
370: 
371: template <class TYPE>
372: static void TemplatedSetValues(ChunkCollection *src_coll, Vector &tgt_vec, idx_t order[], idx_t col_idx,
373:                                idx_t start_offset, idx_t remaining_data) {
374: 	D_ASSERT(src_coll);
375: 
376: 	for (idx_t row_idx = 0; row_idx < remaining_data; row_idx++) {
377: 		idx_t chunk_idx_src = order[start_offset + row_idx] / STANDARD_VECTOR_SIZE;
378: 		idx_t vector_idx_src = order[start_offset + row_idx] % STANDARD_VECTOR_SIZE;
379: 
380: 		auto &src_chunk = src_coll->GetChunk(chunk_idx_src);
381: 		Vector &src_vec = src_chunk.data[col_idx];
382: 		auto source_data = FlatVector::GetData<TYPE>(src_vec);
383: 		auto target_data = FlatVector::GetData<TYPE>(tgt_vec);
384: 
385: 		if (FlatVector::IsNull(src_vec, vector_idx_src)) {
386: 			FlatVector::SetNull(tgt_vec, row_idx, true);
387: 		} else {
388: 			target_data[row_idx] = source_data[vector_idx_src];
389: 		}
390: 	}
391: }
392: 
393: // TODO: reorder functionality is similar, perhaps merge
394: void ChunkCollection::MaterializeSortedChunk(DataChunk &target, idx_t order[], idx_t start_offset) {
395: 	idx_t remaining_data = MinValue<idx_t>(STANDARD_VECTOR_SIZE, count - start_offset);
396: 	D_ASSERT(target.GetTypes() == types);
397: 
398: 	target.SetCardinality(remaining_data);
399: 	for (idx_t col_idx = 0; col_idx < ColumnCount(); col_idx++) {
400: 		switch (types[col_idx].InternalType()) {
401: 		case PhysicalType::BOOL:
402: 		case PhysicalType::INT8:
403: 			TemplatedSetValues<int8_t>(this, target.data[col_idx], order, col_idx, start_offset, remaining_data);
404: 			break;
405: 		case PhysicalType::INT16:
406: 			TemplatedSetValues<int16_t>(this, target.data[col_idx], order, col_idx, start_offset, remaining_data);
407: 			break;
408: 		case PhysicalType::INT32:
409: 			TemplatedSetValues<int32_t>(this, target.data[col_idx], order, col_idx, start_offset, remaining_data);
410: 			break;
411: 		case PhysicalType::INT64:
412: 			TemplatedSetValues<int64_t>(this, target.data[col_idx], order, col_idx, start_offset, remaining_data);
413: 			break;
414: 		case PhysicalType::UINT8:
415: 			TemplatedSetValues<uint8_t>(this, target.data[col_idx], order, col_idx, start_offset, remaining_data);
416: 			break;
417: 		case PhysicalType::UINT16:
418: 			TemplatedSetValues<uint16_t>(this, target.data[col_idx], order, col_idx, start_offset, remaining_data);
419: 			break;
420: 		case PhysicalType::UINT32:
421: 			TemplatedSetValues<uint32_t>(this, target.data[col_idx], order, col_idx, start_offset, remaining_data);
422: 			break;
423: 		case PhysicalType::UINT64:
424: 			TemplatedSetValues<uint64_t>(this, target.data[col_idx], order, col_idx, start_offset, remaining_data);
425: 			break;
426: 		case PhysicalType::INT128:
427: 			TemplatedSetValues<hugeint_t>(this, target.data[col_idx], order, col_idx, start_offset, remaining_data);
428: 			break;
429: 		case PhysicalType::FLOAT:
430: 			TemplatedSetValues<float>(this, target.data[col_idx], order, col_idx, start_offset, remaining_data);
431: 			break;
432: 		case PhysicalType::DOUBLE:
433: 			TemplatedSetValues<double>(this, target.data[col_idx], order, col_idx, start_offset, remaining_data);
434: 			break;
435: 		case PhysicalType::VARCHAR:
436: 			TemplatedSetValues<string_t>(this, target.data[col_idx], order, col_idx, start_offset, remaining_data);
437: 			break;
438: 		case PhysicalType::INTERVAL:
439: 			TemplatedSetValues<interval_t>(this, target.data[col_idx], order, col_idx, start_offset, remaining_data);
440: 			break;
441: 		case PhysicalType::MAP:
442: 		case PhysicalType::LIST:
443: 		case PhysicalType::STRUCT: {
444: 			for (idx_t row_idx = 0; row_idx < remaining_data; row_idx++) {
445: 				idx_t chunk_idx_src = order[start_offset + row_idx] / STANDARD_VECTOR_SIZE;
446: 				idx_t vector_idx_src = order[start_offset + row_idx] % STANDARD_VECTOR_SIZE;
447: 
448: 				auto &src_chunk = chunks[chunk_idx_src];
449: 				Vector &src_vec = src_chunk->data[col_idx];
450: 				auto &tgt_vec = target.data[col_idx];
451: 				if (FlatVector::IsNull(src_vec, vector_idx_src)) {
452: 					FlatVector::SetNull(tgt_vec, row_idx, true);
453: 				} else {
454: 					tgt_vec.SetValue(row_idx, src_vec.GetValue(vector_idx_src));
455: 				}
456: 			}
457: 		} break;
458: 		default:
459: 			throw NotImplementedException("Type is unsupported in MaterializeSortedChunk()");
460: 		}
461: 	}
462: 	target.Verify();
463: }
464: 
465: Value ChunkCollection::GetValue(idx_t column, idx_t index) {
466: 	return chunks[LocateChunk(index)]->GetValue(column, index % STANDARD_VECTOR_SIZE);
467: }
468: 
469: vector<Value> ChunkCollection::GetRow(idx_t index) {
470: 	vector<Value> values;
471: 	values.resize(ColumnCount());
472: 
473: 	for (idx_t p_idx = 0; p_idx < ColumnCount(); p_idx++) {
474: 		values[p_idx] = GetValue(p_idx, index);
475: 	}
476: 	return values;
477: }
478: 
479: void ChunkCollection::SetValue(idx_t column, idx_t index, const Value &value) {
480: 	chunks[LocateChunk(index)]->SetValue(column, index % STANDARD_VECTOR_SIZE, value);
481: }
482: 
483: void ChunkCollection::Print() {
484: 	Printer::Print(ToString());
485: }
486: 
487: bool ChunkCollection::Equals(ChunkCollection &other) {
488: 	if (count != other.count) {
489: 		return false;
490: 	}
491: 	if (ColumnCount() != other.ColumnCount()) {
492: 		return false;
493: 	}
494: 	if (types != other.types) {
495: 		return false;
496: 	}
497: 	// if count is equal amount of chunks should be equal
498: 	for (idx_t row_idx = 0; row_idx < count; row_idx++) {
499: 		for (idx_t col_idx = 0; col_idx < ColumnCount(); col_idx++) {
500: 			auto lvalue = GetValue(col_idx, row_idx);
501: 			auto rvalue = other.GetValue(col_idx, row_idx);
502: 			if (!Value::ValuesAreEqual(lvalue, rvalue)) {
503: 				return false;
504: 			}
505: 		}
506: 	}
507: 	return true;
508: }
509: static void Heapify(ChunkCollection *input, vector<OrderType> &desc, vector<OrderByNullType> &null_order, idx_t *heap,
510:                     idx_t heap_size, idx_t current_index) {
511: 	if (current_index >= heap_size) {
512: 		return;
513: 	}
514: 	idx_t left_child_index = current_index * 2 + 1;
515: 	idx_t right_child_index = current_index * 2 + 2;
516: 	idx_t swap_index = current_index;
517: 
518: 	if (left_child_index < heap_size) {
519: 		swap_index = CompareTuple(input, desc, null_order, heap[swap_index], heap[left_child_index]) <= 0
520: 		                 ? left_child_index
521: 		                 : swap_index;
522: 	}
523: 
524: 	if (right_child_index < heap_size) {
525: 		swap_index = CompareTuple(input, desc, null_order, heap[swap_index], heap[right_child_index]) <= 0
526: 		                 ? right_child_index
527: 		                 : swap_index;
528: 	}
529: 
530: 	if (swap_index != current_index) {
531: 		std::swap(heap[current_index], heap[swap_index]);
532: 		Heapify(input, desc, null_order, heap, heap_size, swap_index);
533: 	}
534: }
535: 
536: static void HeapCreate(ChunkCollection *input, vector<OrderType> &desc, vector<OrderByNullType> &null_order,
537:                        idx_t *heap, idx_t heap_size) {
538: 	for (idx_t i = 0; i < heap_size; i++) {
539: 		heap[i] = i;
540: 	}
541: 
542: 	// build heap
543: 	for (int64_t i = heap_size / 2 - 1; i >= 0; i--) {
544: 		Heapify(input, desc, null_order, heap, heap_size, i);
545: 	}
546: 
547: 	// Run through all the rows.
548: 	for (idx_t i = heap_size; i < input->Count(); i++) {
549: 		if (CompareTuple(input, desc, null_order, i, heap[0]) <= 0) {
550: 			heap[0] = i;
551: 			Heapify(input, desc, null_order, heap, heap_size, 0);
552: 		}
553: 	}
554: }
555: 
556: void ChunkCollection::Heap(vector<OrderType> &desc, vector<OrderByNullType> &null_order, idx_t heap[],
557:                            idx_t heap_size) {
558: 	D_ASSERT(heap);
559: 	if (count == 0) {
560: 		return;
561: 	}
562: 
563: 	HeapCreate(this, desc, null_order, heap, heap_size);
564: 
565: 	// Heap is ready. Now do a heapsort
566: 	for (int64_t i = heap_size - 1; i >= 0; i--) {
567: 		std::swap(heap[i], heap[0]);
568: 		Heapify(this, desc, null_order, heap, i, 0);
569: 	}
570: }
571: 
572: idx_t ChunkCollection::MaterializeHeapChunk(DataChunk &target, idx_t order[], idx_t start_offset, idx_t heap_size) {
573: 	idx_t remaining_data = MinValue<idx_t>(STANDARD_VECTOR_SIZE, heap_size - start_offset);
574: 	D_ASSERT(target.GetTypes() == types);
575: 
576: 	target.SetCardinality(remaining_data);
577: 	for (idx_t col_idx = 0; col_idx < ColumnCount(); col_idx++) {
578: 		switch (types[col_idx].InternalType()) {
579: 		case PhysicalType::BOOL:
580: 		case PhysicalType::INT8:
581: 			TemplatedSetValues<int8_t>(this, target.data[col_idx], order, col_idx, start_offset, remaining_data);
582: 			break;
583: 		case PhysicalType::INT16:
584: 			TemplatedSetValues<int16_t>(this, target.data[col_idx], order, col_idx, start_offset, remaining_data);
585: 			break;
586: 		case PhysicalType::INT32:
587: 			TemplatedSetValues<int32_t>(this, target.data[col_idx], order, col_idx, start_offset, remaining_data);
588: 			break;
589: 		case PhysicalType::INT64:
590: 			TemplatedSetValues<int64_t>(this, target.data[col_idx], order, col_idx, start_offset, remaining_data);
591: 			break;
592: 		case PhysicalType::INT128:
593: 			TemplatedSetValues<hugeint_t>(this, target.data[col_idx], order, col_idx, start_offset, remaining_data);
594: 			break;
595: 		case PhysicalType::FLOAT:
596: 			TemplatedSetValues<float>(this, target.data[col_idx], order, col_idx, start_offset, remaining_data);
597: 			break;
598: 		case PhysicalType::DOUBLE:
599: 			TemplatedSetValues<double>(this, target.data[col_idx], order, col_idx, start_offset, remaining_data);
600: 			break;
601: 		case PhysicalType::VARCHAR:
602: 			TemplatedSetValues<string_t>(this, target.data[col_idx], order, col_idx, start_offset, remaining_data);
603: 			break;
604: 		// TODO this is ugly and sloooow!
605: 		case PhysicalType::MAP:
606: 		case PhysicalType::STRUCT:
607: 		case PhysicalType::LIST: {
608: 			for (idx_t row_idx = 0; row_idx < remaining_data; row_idx++) {
609: 				idx_t chunk_idx_src = order[start_offset + row_idx] / STANDARD_VECTOR_SIZE;
610: 				idx_t vector_idx_src = order[start_offset + row_idx] % STANDARD_VECTOR_SIZE;
611: 
612: 				auto &src_chunk = chunks[chunk_idx_src];
613: 				Vector &src_vec = src_chunk->data[col_idx];
614: 				auto &tgt_vec = target.data[col_idx];
615: 				if (FlatVector::IsNull(src_vec, vector_idx_src)) {
616: 					FlatVector::SetNull(tgt_vec, row_idx, true);
617: 				} else {
618: 					tgt_vec.SetValue(row_idx, src_vec.GetValue(vector_idx_src));
619: 				}
620: 			}
621: 		} break;
622: 
623: 		default:
624: 			throw NotImplementedException("Type is unsupported in MaterializeHeapChunk()");
625: 		}
626: 	}
627: 	target.Verify();
628: 	return start_offset + remaining_data;
629: }
630: 
631: } // namespace duckdb
[end of src/common/types/chunk_collection.cpp]
[start of src/common/types/row_data_collection.cpp]
1: #include "duckdb/common/types/row_data_collection.hpp"
2: 
3: #include "duckdb/common/bit_operations.hpp"
4: #include "duckdb/common/types/chunk_collection.hpp"
5: 
6: namespace duckdb {
7: 
8: RowDataCollection::RowDataCollection(BufferManager &buffer_manager, idx_t block_capacity, idx_t entry_size)
9:     : buffer_manager(buffer_manager), count(0), block_capacity(block_capacity), entry_size(entry_size),
10:       is_little_endian(IsLittleEndian()) {
11: 	D_ASSERT(block_capacity * entry_size >= Storage::BLOCK_ALLOC_SIZE);
12: }
13: 
14: template <class T>
15: void RowDataCollection::TemplatedSerializeVectorSortable(VectorData &vdata, const SelectionVector &sel, idx_t add_count,
16:                                                          data_ptr_t key_locations[], const bool desc,
17:                                                          const bool has_null, const bool nulls_first) {
18: 	auto source = (T *)vdata.data;
19: 	if (has_null) {
20: 		auto &validity = vdata.validity;
21: 		const data_t valid = nulls_first ? 1 : 0;
22: 		const data_t invalid = 1 - valid;
23: 
24: 		for (idx_t i = 0; i < add_count; i++) {
25: 			auto idx = sel.get_index(i);
26: 			auto source_idx = vdata.sel->get_index(idx);
27: 			// write validity and according value
28: 			if (validity.RowIsValid(source_idx)) {
29: 				key_locations[i][0] = valid;
30: 				EncodeData<T>(key_locations[i] + 1, source[source_idx], is_little_endian);
31: 				// invert bits if desc
32: 				if (desc) {
33: 					for (idx_t s = 1; s < sizeof(T) + 1; s++) {
34: 						*(key_locations[i] + s) = ~*(key_locations[i] + s);
35: 					}
36: 				}
37: 			} else {
38: 				key_locations[i][0] = invalid;
39: 				memset(key_locations[i] + 1, '\0', sizeof(T));
40: 			}
41: 			key_locations[i] += sizeof(T) + 1;
42: 		}
43: 	} else {
44: 		for (idx_t i = 0; i < add_count; i++) {
45: 			auto idx = sel.get_index(i);
46: 			auto source_idx = vdata.sel->get_index(idx);
47: 			// write value
48: 			EncodeData<T>(key_locations[i], source[source_idx], is_little_endian);
49: 			// invert bits if desc
50: 			if (desc) {
51: 				for (idx_t s = 1; s < sizeof(T); s++) {
52: 					*(key_locations[i] + s) = ~*(key_locations[i] + s);
53: 				}
54: 			}
55: 			key_locations[i] += sizeof(T);
56: 		}
57: 	}
58: }
59: 
60: void RowDataCollection::SerializeStringVectorSortable(VectorData &vdata, const SelectionVector &sel, idx_t add_count,
61:                                                       data_ptr_t key_locations[], const bool desc, const bool has_null,
62:                                                       const bool nulls_first, const idx_t prefix_len) {
63: 	auto source = (string_t *)vdata.data;
64: 	if (has_null) {
65: 		auto &validity = vdata.validity;
66: 		const data_t valid = nulls_first ? 1 : 0;
67: 		const data_t invalid = 1 - valid;
68: 
69: 		for (idx_t i = 0; i < add_count; i++) {
70: 			auto idx = sel.get_index(i);
71: 			auto source_idx = vdata.sel->get_index(idx);
72: 			// write validity and according value
73: 			if (validity.RowIsValid(source_idx)) {
74: 				key_locations[i][0] = valid;
75: 				EncodeStringDataPrefix(key_locations[i] + 1, source[source_idx], prefix_len);
76: 				// invert bits if desc
77: 				if (desc) {
78: 					for (idx_t s = 1; s < prefix_len + 1; s++) {
79: 						*(key_locations[i] + s) = ~*(key_locations[i] + s);
80: 					}
81: 				}
82: 			} else {
83: 				key_locations[i][0] = invalid;
84: 				memset(key_locations[i] + 1, '\0', prefix_len);
85: 			}
86: 			key_locations[i] += prefix_len + 1;
87: 		}
88: 	} else {
89: 		for (idx_t i = 0; i < add_count; i++) {
90: 			auto idx = sel.get_index(i);
91: 			auto source_idx = vdata.sel->get_index(idx);
92: 			// write value
93: 			EncodeStringDataPrefix(key_locations[i], source[source_idx], prefix_len);
94: 			// invert bits if desc
95: 			if (desc) {
96: 				for (idx_t s = 1; s < prefix_len; s++) {
97: 					*(key_locations[i] + s) = ~*(key_locations[i] + s);
98: 				}
99: 			}
100: 			key_locations[i] += prefix_len;
101: 		}
102: 	}
103: }
104: 
105: void RowDataCollection::SerializeVectorSortable(Vector &v, idx_t vcount, const SelectionVector &sel, idx_t ser_count,
106:                                                 data_ptr_t key_locations[], bool desc, bool has_null, bool nulls_first,
107:                                                 idx_t prefix_len) {
108: 	VectorData vdata;
109: 	v.Orrify(vcount, vdata);
110: 	switch (v.GetType().InternalType()) {
111: 	case PhysicalType::BOOL:
112: 	case PhysicalType::INT8:
113: 		TemplatedSerializeVectorSortable<int8_t>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first);
114: 		break;
115: 	case PhysicalType::INT16:
116: 		TemplatedSerializeVectorSortable<int16_t>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first);
117: 		break;
118: 	case PhysicalType::INT32:
119: 		TemplatedSerializeVectorSortable<int32_t>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first);
120: 		break;
121: 	case PhysicalType::INT64:
122: 		TemplatedSerializeVectorSortable<int64_t>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first);
123: 		break;
124: 	case PhysicalType::UINT8:
125: 		TemplatedSerializeVectorSortable<uint8_t>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first);
126: 		break;
127: 	case PhysicalType::UINT16:
128: 		TemplatedSerializeVectorSortable<uint16_t>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first);
129: 		break;
130: 	case PhysicalType::UINT32:
131: 		TemplatedSerializeVectorSortable<uint32_t>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first);
132: 		break;
133: 	case PhysicalType::UINT64:
134: 		TemplatedSerializeVectorSortable<uint64_t>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first);
135: 		break;
136: 	case PhysicalType::INT128:
137: 		TemplatedSerializeVectorSortable<hugeint_t>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first);
138: 		break;
139: 	case PhysicalType::FLOAT:
140: 		TemplatedSerializeVectorSortable<float>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first);
141: 		break;
142: 	case PhysicalType::DOUBLE:
143: 		TemplatedSerializeVectorSortable<double>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first);
144: 		break;
145: 	case PhysicalType::HASH:
146: 		TemplatedSerializeVectorSortable<hash_t>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first);
147: 		break;
148: 	case PhysicalType::INTERVAL:
149: 		TemplatedSerializeVectorSortable<interval_t>(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first);
150: 		break;
151: 	case PhysicalType::VARCHAR:
152: 		SerializeStringVectorSortable(vdata, sel, ser_count, key_locations, desc, has_null, nulls_first, prefix_len);
153: 		break;
154: 	default:
155: 		throw NotImplementedException("Cannot ORDER BY column with type %s", v.GetType().ToString());
156: 	}
157: }
158: 
159: void RowDataCollection::ComputeStringEntrySizes(Vector &v, idx_t entry_sizes[], idx_t vcount, idx_t offset) {
160: 	VectorData vdata;
161: 	v.Orrify(vcount, vdata);
162: 
163: 	const idx_t string_prefix_len = string_t::PREFIX_LENGTH;
164: 	auto strings = (string_t *)vdata.data;
165: 	for (idx_t i = 0; i < vcount; i++) {
166: 		idx_t str_idx = vdata.sel->get_index(i) + offset;
167: 		if (vdata.validity.RowIsValid(str_idx)) {
168: 			entry_sizes[i] += string_prefix_len + strings[str_idx].GetSize();
169: 		}
170: 	}
171: }
172: 
173: void RowDataCollection::ComputeStructEntrySizes(Vector &v, idx_t entry_sizes[], idx_t vcount, idx_t offset) {
174: 	VectorData vdata;
175: 	v.Orrify(vcount, vdata);
176: 
177: 	// obtain child vectors
178: 	idx_t num_children;
179: 	vector<Vector> struct_vectors;
180: 	if (v.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
181: 		auto &child = DictionaryVector::Child(v);
182: 		auto &dict_sel = DictionaryVector::SelVector(v);
183: 		auto &children = StructVector::GetEntries(child);
184: 		num_children = children.size();
185: 		for (auto &struct_child : children) {
186: 			Vector struct_vector;
187: 			struct_vector.Slice(*struct_child, dict_sel, vcount);
188: 			struct_vectors.push_back(move(struct_vector));
189: 		}
190: 	} else {
191: 		auto &children = StructVector::GetEntries(v);
192: 		num_children = children.size();
193: 		for (auto &struct_child : children) {
194: 			Vector struct_vector;
195: 			struct_vector.Reference(*struct_child);
196: 			struct_vectors.push_back(move(struct_vector));
197: 		}
198: 	}
199: 	// add struct validitymask size
200: 	const idx_t struct_validitymask_size = (num_children + 7) / 8;
201: 	for (idx_t i = 0; i < vcount; i++) {
202: 		// FIXME: don't serialize if the struct is NULL?
203: 		entry_sizes[i] += struct_validitymask_size;
204: 	}
205: 	// compute size of child vectors
206: 	for (auto &struct_vector : struct_vectors) {
207: 		ComputeEntrySizes(struct_vector, entry_sizes, vcount, offset);
208: 	}
209: }
210: 
211: static list_entry_t *GetListData(Vector &v) {
212: 	if (v.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
213: 		auto &child = DictionaryVector::Child(v);
214: 		return GetListData(child);
215: 	}
216: 	return FlatVector::GetData<list_entry_t>(v);
217: }
218: 
219: void RowDataCollection::ComputeListEntrySizes(Vector &v, idx_t entry_sizes[], idx_t vcount, idx_t offset) {
220: 	VectorData vdata;
221: 	v.Orrify(vcount, vdata);
222: 
223: 	auto list_data = GetListData(v);
224: 	auto &child_vector = ListVector::GetEntry(v);
225: 	idx_t list_entry_sizes[STANDARD_VECTOR_SIZE];
226: 	for (idx_t i = 0; i < vcount; i++) {
227: 		idx_t idx = vdata.sel->get_index(i) + offset;
228: 		if (vdata.validity.RowIsValid(idx)) {
229: 			auto list_entry = list_data[idx];
230: 
231: 			// make room for list length, list validitymask
232: 			entry_sizes[i] += sizeof(list_entry.length);
233: 			entry_sizes[i] += (list_entry.length + 7) / 8;
234: 
235: 			// serialize size of each entry (if non-constant size)
236: 			if (!TypeIsConstantSize(v.GetType().child_types()[0].second.InternalType())) {
237: 				entry_sizes[i] += list_entry.length * sizeof(list_entry.length);
238: 			}
239: 
240: 			// compute size of each the elements in list_entry and sum them
241: 			auto entry_remaining = list_entry.length;
242: 			auto entry_offset = list_entry.offset;
243: 			while (entry_remaining > 0) {
244: 				// the list entry can span multiple vectors
245: 				auto next = MinValue((idx_t)STANDARD_VECTOR_SIZE, entry_remaining);
246: 
247: 				// compute and add to the total
248: 				std::fill_n(list_entry_sizes, next, 0);
249: 				ComputeEntrySizes(child_vector, list_entry_sizes, next, entry_offset);
250: 				for (idx_t list_idx = 0; list_idx < next; list_idx++) {
251: 					entry_sizes[i] += list_entry_sizes[list_idx];
252: 				}
253: 
254: 				// update for next iteration
255: 				entry_remaining -= next;
256: 				entry_offset += next;
257: 			}
258: 		}
259: 	}
260: }
261: 
262: void RowDataCollection::ComputeEntrySizes(Vector &v, idx_t entry_sizes[], idx_t vcount, idx_t offset) {
263: 	auto physical_type = v.GetType().InternalType();
264: 	if (TypeIsConstantSize(physical_type)) {
265: 		const auto type_size = GetTypeIdSize(physical_type);
266: 		for (idx_t i = 0; i < vcount; i++) {
267: 			entry_sizes[i] += type_size;
268: 		}
269: 	} else {
270: 		switch (physical_type) {
271: 		case PhysicalType::VARCHAR:
272: 			ComputeStringEntrySizes(v, entry_sizes, vcount, offset);
273: 			break;
274: 		case PhysicalType::MAP:
275: 		case PhysicalType::STRUCT:
276: 			ComputeStructEntrySizes(v, entry_sizes, vcount, offset);
277: 			break;
278: 		case PhysicalType::LIST:
279: 			ComputeListEntrySizes(v, entry_sizes, vcount, offset);
280: 			break;
281: 		default:
282: 			throw NotImplementedException("Column with variable size type %s cannot be serialized to row-format",
283: 			                              v.GetType().ToString());
284: 		}
285: 	}
286: }
287: 
288: void RowDataCollection::ComputeEntrySizes(DataChunk &input, idx_t entry_sizes[], idx_t entry_size) {
289: 	// fill array with constant portion of payload entry size
290: 	std::fill_n(entry_sizes, input.size(), entry_size);
291: 
292: 	// compute size of the constant portion of the payload columns
293: 	VectorData vdata;
294: 	for (idx_t col_idx = 0; col_idx < input.data.size(); col_idx++) {
295: 		auto physical_type = input.data[col_idx].GetType().InternalType();
296: 		if (TypeIsConstantSize(physical_type)) {
297: 			continue;
298: 		}
299: 		ComputeEntrySizes(input.data[col_idx], entry_sizes, input.size());
300: 	}
301: }
302: 
303: template <class T>
304: static void TemplatedSerializeVData(VectorData &vdata, const SelectionVector &sel, idx_t count, idx_t col_idx,
305:                                     data_ptr_t *key_locations, data_ptr_t *validitymask_locations, idx_t offset) {
306: 	auto source = (T *)vdata.data;
307: 	if (!validitymask_locations) {
308: 		for (idx_t i = 0; i < count; i++) {
309: 			auto idx = sel.get_index(i);
310: 			auto source_idx = vdata.sel->get_index(idx) + offset;
311: 
312: 			auto target = (T *)key_locations[i];
313: 			Store<T>(source[source_idx], (data_ptr_t)target);
314: 			key_locations[i] += sizeof(T);
315: 		}
316: 	} else {
317: 		const auto byte_offset = col_idx / 8;
318: 		const auto bit = ~(1UL << (col_idx % 8));
319: 		for (idx_t i = 0; i < count; i++) {
320: 			auto idx = sel.get_index(i);
321: 			auto source_idx = vdata.sel->get_index(idx) + offset;
322: 
323: 			auto target = (T *)key_locations[i];
324: 			Store<T>(source[source_idx], (data_ptr_t)target);
325: 			key_locations[i] += sizeof(T);
326: 
327: 			// set the validitymask
328: 			if (!vdata.validity.RowIsValid(source_idx)) {
329: 				*(validitymask_locations[i] + byte_offset) &= bit;
330: 			}
331: 		}
332: 	}
333: }
334: 
335: void RowDataCollection::SerializeVectorData(VectorData &vdata, PhysicalType type, const SelectionVector &sel,
336:                                             idx_t ser_count, idx_t col_idx, data_ptr_t key_locations[],
337:                                             data_ptr_t validitymask_locations[], idx_t offset) {
338: 	switch (type) {
339: 	case PhysicalType::BOOL:
340: 	case PhysicalType::INT8:
341: 		TemplatedSerializeVData<int8_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
342: 		break;
343: 	case PhysicalType::INT16:
344: 		TemplatedSerializeVData<int16_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
345: 		break;
346: 	case PhysicalType::INT32:
347: 		TemplatedSerializeVData<int32_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
348: 		break;
349: 	case PhysicalType::INT64:
350: 		TemplatedSerializeVData<int64_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
351: 		break;
352: 	case PhysicalType::UINT8:
353: 		TemplatedSerializeVData<uint8_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
354: 		break;
355: 	case PhysicalType::UINT16:
356: 		TemplatedSerializeVData<uint16_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations,
357: 		                                  offset);
358: 		break;
359: 	case PhysicalType::UINT32:
360: 		TemplatedSerializeVData<uint32_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations,
361: 		                                  offset);
362: 		break;
363: 	case PhysicalType::UINT64:
364: 		TemplatedSerializeVData<uint64_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations,
365: 		                                  offset);
366: 		break;
367: 	case PhysicalType::INT128:
368: 		TemplatedSerializeVData<hugeint_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations,
369: 		                                   offset);
370: 		break;
371: 	case PhysicalType::FLOAT:
372: 		TemplatedSerializeVData<float>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
373: 		break;
374: 	case PhysicalType::DOUBLE:
375: 		TemplatedSerializeVData<double>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
376: 		break;
377: 	case PhysicalType::HASH:
378: 		TemplatedSerializeVData<hash_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
379: 		break;
380: 	case PhysicalType::INTERVAL:
381: 		TemplatedSerializeVData<interval_t>(vdata, sel, ser_count, col_idx, key_locations, validitymask_locations,
382: 		                                    offset);
383: 		break;
384: 	default:
385: 		throw NotImplementedException("FIXME: unimplemented serialize to of constant type column to row-format");
386: 	}
387: }
388: 
389: void RowDataCollection::SerializeStringVector(Vector &v, idx_t vcount, const SelectionVector &sel, idx_t ser_count,
390:                                               idx_t col_idx, data_ptr_t key_locations[],
391:                                               data_ptr_t validitymask_locations[], idx_t offset) {
392: 	VectorData vdata;
393: 	v.Orrify(vcount, vdata);
394: 
395: 	const idx_t string_prefix_len = string_t::PREFIX_LENGTH;
396: 	auto strings = (string_t *)vdata.data;
397: 	if (!validitymask_locations) {
398: 		for (idx_t i = 0; i < ser_count; i++) {
399: 			auto idx = sel.get_index(i);
400: 			auto source_idx = vdata.sel->get_index(idx) + offset;
401: 			if (vdata.validity.RowIsValid(source_idx)) {
402: 				auto &string_entry = strings[source_idx];
403: 				// store string size
404: 				Store<uint32_t>(string_entry.GetSize(), key_locations[i]);
405: 				key_locations[i] += string_prefix_len;
406: 				// store the string
407: 				memcpy(key_locations[i], string_entry.GetDataUnsafe(), string_entry.GetSize());
408: 				key_locations[i] += string_entry.GetSize();
409: 			}
410: 		}
411: 	} else {
412: 		auto byte_offset = col_idx / 8;
413: 		const auto bit = ~(1UL << (col_idx % 8));
414: 		for (idx_t i = 0; i < ser_count; i++) {
415: 			auto idx = sel.get_index(i);
416: 			auto source_idx = vdata.sel->get_index(idx) + offset;
417: 			if (vdata.validity.RowIsValid(source_idx)) {
418: 				auto &string_entry = strings[source_idx];
419: 				// store string size
420: 				Store<uint32_t>(string_entry.GetSize(), key_locations[i]);
421: 				key_locations[i] += string_prefix_len;
422: 				// store the string
423: 				memcpy(key_locations[i], string_entry.GetDataUnsafe(), string_entry.GetSize());
424: 				key_locations[i] += string_entry.GetSize();
425: 			} else {
426: 				// set the validitymask
427: 				*(validitymask_locations[i] + byte_offset) &= bit;
428: 			}
429: 		}
430: 	}
431: }
432: 
433: void RowDataCollection::SerializeStructVector(Vector &v, idx_t vcount, const SelectionVector &sel, idx_t ser_count,
434:                                               idx_t col_idx, data_ptr_t key_locations[],
435:                                               data_ptr_t validitymask_locations[], idx_t offset) {
436: 	VectorData vdata;
437: 	v.Orrify(vcount, vdata);
438: 
439: 	idx_t num_children;
440: 	vector<Vector> struct_vectors;
441: 	if (v.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
442: 		auto &child = DictionaryVector::Child(v);
443: 		auto &dict_sel = DictionaryVector::SelVector(v);
444: 		auto &children = StructVector::GetEntries(child);
445: 		num_children = children.size();
446: 		for (auto &struct_child : children) {
447: 			Vector struct_vector;
448: 			struct_vector.Slice(*struct_child, dict_sel, vcount);
449: 			struct_vectors.push_back(move(struct_vector));
450: 		}
451: 	} else {
452: 		auto &children = StructVector::GetEntries(v);
453: 		num_children = children.size();
454: 		for (auto &struct_child : children) {
455: 			Vector struct_vector;
456: 			struct_vector.Reference(*struct_child);
457: 			struct_vectors.push_back(move(struct_vector));
458: 		}
459: 	}
460: 
461: 	// the whole struct itself can be NULL
462: 	auto byte_offset = col_idx / 8;
463: 	const auto bit = ~(1UL << (col_idx % 8));
464: 
465: 	// struct must have a validitymask for its fields
466: 	const idx_t struct_validitymask_size = (num_children + 7) / 8;
467: 	data_ptr_t struct_validitymask_locations[STANDARD_VECTOR_SIZE];
468: 	for (idx_t i = 0; i < ser_count; i++) {
469: 		// initialize the struct validity mask
470: 		struct_validitymask_locations[i] = key_locations[i];
471: 		memset(struct_validitymask_locations[i], -1, struct_validitymask_size);
472: 		key_locations[i] += struct_validitymask_size;
473: 
474: 		// set whether the whole struct is null
475: 		auto idx = sel.get_index(i);
476: 		auto source_idx = vdata.sel->get_index(idx) + offset;
477: 		if (validitymask_locations && !vdata.validity.RowIsValid(source_idx)) {
478: 			*(validitymask_locations[i] + byte_offset) &= bit;
479: 		}
480: 	}
481: 
482: 	// now serialize the struct vectors
483: 	for (idx_t i = 0; i < struct_vectors.size(); i++) {
484: 		auto &struct_vector = struct_vectors[i];
485: 		SerializeVector(struct_vector, vcount, sel, ser_count, i, key_locations, struct_validitymask_locations, offset);
486: 	}
487: }
488: 
489: void RowDataCollection::SerializeListVector(Vector &v, idx_t vcount, const SelectionVector &sel, idx_t ser_count,
490:                                             idx_t col_idx, data_ptr_t key_locations[],
491:                                             data_ptr_t validitymask_locations[], idx_t offset) {
492: 	VectorData vdata;
493: 	v.Orrify(vcount, vdata);
494: 
495: 	auto byte_offset = col_idx / 8;
496: 	const auto bit = ~(1UL << (col_idx % 8));
497: 
498: 	auto list_data = GetListData(v);
499: 	auto &child_vector = ListVector::GetEntry(v);
500: 
501: 	VectorData list_vdata;
502: 	child_vector.Orrify(ListVector::GetListSize(v), list_vdata);
503: 	auto child_type = v.GetType().child_types()[0].second.InternalType();
504: 
505: 	idx_t list_entry_sizes[STANDARD_VECTOR_SIZE];
506: 	data_ptr_t list_entry_locations[STANDARD_VECTOR_SIZE];
507: 
508: 	for (idx_t i = 0; i < ser_count; i++) {
509: 		auto idx = sel.get_index(i);
510: 		auto source_idx = vdata.sel->get_index(idx) + offset;
511: 		if (!vdata.validity.RowIsValid(source_idx)) {
512: 			if (validitymask_locations) {
513: 				// set the validitymask
514: 				*(validitymask_locations[i] + byte_offset) &= bit;
515: 			}
516: 			continue;
517: 		}
518: 		auto list_entry = list_data[source_idx];
519: 
520: 		// store list length
521: 		Store<uint64_t>(list_entry.length, key_locations[i]);
522: 		key_locations[i] += sizeof(list_entry.length);
523: 
524: 		// make room for the validitymask
525: 		data_ptr_t list_validitymask_location = key_locations[i];
526: 		idx_t entry_offset_in_byte = 0;
527: 		idx_t validitymask_size = (list_entry.length + 7) / 8;
528: 		memset(list_validitymask_location, -1, validitymask_size);
529: 		key_locations[i] += validitymask_size;
530: 
531: 		// serialize size of each entry (if non-constant size)
532: 		data_ptr_t var_entry_size_ptr = nullptr;
533: 		if (!TypeIsConstantSize(child_type)) {
534: 			var_entry_size_ptr = key_locations[i];
535: 			key_locations[i] += list_entry.length * sizeof(idx_t);
536: 		}
537: 
538: 		auto entry_remaining = list_entry.length;
539: 		auto entry_offset = list_entry.offset;
540: 		while (entry_remaining > 0) {
541: 			// the list entry can span multiple vectors
542: 			auto next = MinValue((idx_t)STANDARD_VECTOR_SIZE, entry_remaining);
543: 
544: 			// serialize list validity
545: 			for (idx_t entry_idx = 0; entry_idx < next; entry_idx++) {
546: 				auto list_idx = list_vdata.sel->get_index(entry_idx) + entry_offset;
547: 				if (!list_vdata.validity.RowIsValid(list_idx)) {
548: 					*(list_validitymask_location) &= ~(1UL << entry_offset_in_byte);
549: 				}
550: 				if (++entry_offset_in_byte == 8) {
551: 					list_validitymask_location++;
552: 					entry_offset_in_byte = 0;
553: 				}
554: 			}
555: 
556: 			if (TypeIsConstantSize(child_type)) {
557: 				// constant size list entries: set list entry locations
558: 				const idx_t type_size = GetTypeIdSize(child_type);
559: 				for (idx_t entry_idx = 0; entry_idx < next; entry_idx++) {
560: 					list_entry_locations[entry_idx] = key_locations[i];
561: 					key_locations[i] += type_size;
562: 				}
563: 			} else {
564: 				// variable size list entries: compute entry sizes and set list entry locations
565: 				std::fill_n(list_entry_sizes, next, 0);
566: 				ComputeEntrySizes(child_vector, list_entry_sizes, next, entry_offset);
567: 				for (idx_t entry_idx = 0; entry_idx < next; entry_idx++) {
568: 					list_entry_locations[entry_idx] = key_locations[i];
569: 					key_locations[i] += list_entry_sizes[entry_idx];
570: 					Store<idx_t>(list_entry_sizes[entry_idx], var_entry_size_ptr);
571: 					var_entry_size_ptr += sizeof(idx_t);
572: 				}
573: 			}
574: 
575: 			// now serialize to the locations
576: 			SerializeVector(child_vector, ListVector::GetListSize(v), sel, next, 0, list_entry_locations, nullptr,
577: 			                entry_offset);
578: 
579: 			// update for next iteration
580: 			entry_remaining -= next;
581: 			entry_offset += next;
582: 		}
583: 	}
584: }
585: 
586: void RowDataCollection::SerializeVector(Vector &v, idx_t vcount, const SelectionVector &sel, idx_t ser_count,
587:                                         idx_t col_idx, data_ptr_t key_locations[], data_ptr_t validitymask_locations[],
588:                                         idx_t offset) {
589: 	if (TypeIsConstantSize(v.GetType().InternalType())) {
590: 		VectorData vdata;
591: 		v.Orrify(vcount, vdata);
592: 		SerializeVectorData(vdata, v.GetType().InternalType(), sel, ser_count, col_idx, key_locations,
593: 		                    validitymask_locations, offset);
594: 	} else {
595: 		switch (v.GetType().InternalType()) {
596: 		case PhysicalType::VARCHAR:
597: 			SerializeStringVector(v, vcount, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
598: 			break;
599: 		case PhysicalType::MAP:
600: 		case PhysicalType::STRUCT:
601: 			SerializeStructVector(v, vcount, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
602: 			break;
603: 		case PhysicalType::LIST:
604: 			SerializeListVector(v, vcount, sel, ser_count, col_idx, key_locations, validitymask_locations, offset);
605: 			break;
606: 		default:
607: 			throw NotImplementedException("Serialization of variable length vector with type %s",
608: 			                              v.GetType().ToString());
609: 		}
610: 	}
611: }
612: 
613: idx_t RowDataCollection::AppendToBlock(RowDataBlock &block, BufferHandle &handle,
614:                                        vector<BlockAppendEntry> &append_entries, idx_t remaining, idx_t entry_sizes[]) {
615: 	idx_t append_count = 0;
616: 	data_ptr_t dataptr;
617: 	if (entry_sizes) {
618: 		// compute how many entries fit if entry size if variable
619: 		dataptr = handle.node->buffer + block.byte_offset;
620: 		for (idx_t i = 0; i < remaining; i++) {
621: 			if (block.byte_offset + entry_sizes[i] > block_capacity * entry_size) {
622: 				while (entry_sizes[i] > block_capacity * entry_size) {
623: 					// if an entry does not fit, increase entry size until it does
624: 					entry_size *= 2;
625: 				}
626: 				break;
627: 			}
628: 			append_count++;
629: 			block.byte_offset += entry_sizes[i];
630: 		}
631: 	} else {
632: 		append_count = MinValue<idx_t>(remaining, block.capacity - block.count);
633: 		dataptr = handle.node->buffer + block.count * entry_size;
634: 	}
635: 	append_entries.emplace_back(dataptr, append_count);
636: 	block.count += append_count;
637: 	return append_count;
638: }
639: 
640: void RowDataCollection::Build(idx_t added_count, data_ptr_t key_locations[], idx_t entry_sizes[]) {
641: 	vector<unique_ptr<BufferHandle>> handles;
642: 	vector<BlockAppendEntry> append_entries;
643: 
644: 	// first allocate space of where to serialize the keys and payload columns
645: 	idx_t remaining = added_count;
646: 	{
647: 		// first append to the last block (if any)
648: 		lock_guard<mutex> append_lock(rc_lock);
649: 		count += added_count;
650: 		if (!blocks.empty()) {
651: 			auto &last_block = blocks.back();
652: 			if (last_block.count < last_block.capacity) {
653: 				// last block has space: pin the buffer of this block
654: 				auto handle = buffer_manager.Pin(last_block.block);
655: 				// now append to the block
656: 				idx_t append_count = AppendToBlock(last_block, *handle, append_entries, remaining, entry_sizes);
657: 				remaining -= append_count;
658: 				handles.push_back(move(handle));
659: 			}
660: 		}
661: 		while (remaining > 0) {
662: 			// now for the remaining data, allocate new buffers to store the data and append there
663: 			RowDataBlock new_block(buffer_manager, block_capacity, entry_size);
664: 			auto handle = buffer_manager.Pin(new_block.block);
665: 
666: 			// offset the entry sizes array if we have added entries already
667: 			idx_t *offset_entry_sizes = entry_sizes ? entry_sizes + added_count - remaining : nullptr;
668: 
669: 			idx_t append_count = AppendToBlock(new_block, *handle, append_entries, remaining, offset_entry_sizes);
670: 			remaining -= append_count;
671: 
672: 			if (new_block.count > 0) {
673: 				// it can be that no tuples fit the block (huge entry e.g. large string)
674: 				// in this case we do not add them
675: 				blocks.push_back(move(new_block));
676: 				handles.push_back(move(handle));
677: 			}
678: 		}
679: 	}
680: 	// now set up the key_locations based on the append entries
681: 	idx_t append_idx = 0;
682: 	for (auto &append_entry : append_entries) {
683: 		idx_t next = append_idx + append_entry.count;
684: 		if (entry_sizes) {
685: 			for (; append_idx < next; append_idx++) {
686: 				key_locations[append_idx] = append_entry.baseptr;
687: 				append_entry.baseptr += entry_sizes[append_idx];
688: 			}
689: 		} else {
690: 			for (; append_idx < next; append_idx++) {
691: 				key_locations[append_idx] = append_entry.baseptr;
692: 				append_entry.baseptr += entry_size;
693: 			}
694: 		}
695: 	}
696: }
697: 
698: template <class T>
699: static void TemplatedDeserializeIntoVector(Vector &v, idx_t count, idx_t col_idx, data_ptr_t *key_locations) {
700: 	auto target = FlatVector::GetData<T>(v);
701: 	// fixed-size inner loop to allow unrolling
702: 	idx_t i;
703: 	for (i = 0; i + 7 < count; i += 8) {
704: 		for (idx_t j = 0; j < 8; j++) {
705: 			target[i + j] = Load<T>(key_locations[i + j]);
706: 			key_locations[i + j] += sizeof(T);
707: 		}
708: 	}
709: 	// finishing up
710: 	for (; i < count; i++) {
711: 		target[i] = Load<T>(key_locations[i]);
712: 		key_locations[i] += sizeof(T);
713: 	}
714: }
715: 
716: void RowDataCollection::DeserializeIntoStringVector(Vector &v, const idx_t &vcount, const idx_t &col_idx,
717:                                                     data_ptr_t *key_locations, data_ptr_t *validitymask_locations) {
718: 	const auto &validity = FlatVector::Validity(v);
719: 	const idx_t string_prefix_len = string_t::PREFIX_LENGTH;
720: 	auto target = FlatVector::GetData<string_t>(v);
721: 	// fixed size inner loop to allow unrolling
722: 	idx_t i = 0;
723: 	if (validity.AllValid()) {
724: 		for (; i + 7 < vcount; i += 8) {
725: 			for (idx_t j = 0; j < 8; j++) {
726: 				auto len = Load<uint32_t>(key_locations[i + j]);
727: 				key_locations[i + j] += string_prefix_len;
728: 				target[i + j] = StringVector::AddStringOrBlob(v, string_t((const char *)key_locations[i + j], len));
729: 				key_locations[i + j] += len;
730: 			}
731: 		}
732: 	}
733: 	// finishing up
734: 	for (; i < vcount; i++) {
735: 		if (!validity.RowIsValid(i)) {
736: 			continue;
737: 		}
738: 		auto len = Load<uint32_t>(key_locations[i]);
739: 		key_locations[i] += string_prefix_len;
740: 		target[i] = StringVector::AddStringOrBlob(v, string_t((const char *)key_locations[i], len));
741: 		key_locations[i] += len;
742: 	}
743: }
744: 
745: void RowDataCollection::DeserializeIntoStructVector(Vector &v, const idx_t &vcount, const idx_t &col_idx,
746:                                                     data_ptr_t *key_locations, data_ptr_t *validitymask_locations) {
747: 	// struct must have a validitymask for its fields
748: 	auto &child_types = v.GetType().child_types();
749: 	const idx_t struct_validitymask_size = (child_types.size() + 7) / 8;
750: 	data_ptr_t struct_validitymask_locations[STANDARD_VECTOR_SIZE];
751: 	for (idx_t i = 0; i < vcount; i++) {
752: 		// use key_locations as the validitymask, and create struct_key_locations
753: 		struct_validitymask_locations[i] = key_locations[i];
754: 		key_locations[i] += struct_validitymask_size;
755: 	}
756: 
757: 	// now deserialize into the struct vectors
758: 	auto &children = StructVector::GetEntries(v);
759: 	for (idx_t i = 0; i < child_types.size(); i++) {
760: 		DeserializeIntoVector(*children[i], vcount, i, key_locations, struct_validitymask_locations);
761: 	}
762: }
763: 
764: void RowDataCollection::DeserializeIntoListVector(Vector &v, const idx_t &vcount, const idx_t &col_idx,
765:                                                   data_ptr_t *key_locations, data_ptr_t *validitymask_locations) {
766: 	const auto &validity = FlatVector::Validity(v);
767: 
768: 	auto child_type = v.GetType().child_types()[0].second;
769: 	auto list_data = GetListData(v);
770: 	data_ptr_t list_entry_locations[STANDARD_VECTOR_SIZE];
771: 
772: 	ListVector::Initialize(v);
773: 	uint64_t entry_offset = ListVector::GetListSize(v);
774: 	for (idx_t i = 0; i < vcount; i++) {
775: 		if (!validity.RowIsValid(i)) {
776: 			continue;
777: 		}
778: 		// read list length
779: 		auto entry_remaining = Load<uint64_t>(key_locations[i]);
780: 		key_locations[i] += sizeof(uint64_t);
781: 		// set list entry attributes
782: 		list_data[i].length = entry_remaining;
783: 		list_data[i].offset = entry_offset;
784: 		// skip over the validity mask
785: 		data_ptr_t validitymask_location = key_locations[i];
786: 		idx_t offset_in_byte = 0;
787: 		key_locations[i] += (entry_remaining + 7) / 8;
788: 		// entry sizes
789: 		data_ptr_t var_entry_size_ptr = nullptr;
790: 		if (!TypeIsConstantSize(child_type.InternalType())) {
791: 			var_entry_size_ptr = key_locations[i];
792: 			key_locations[i] += entry_remaining * sizeof(idx_t);
793: 		}
794: 
795: 		// now read the list data
796: 		while (entry_remaining > 0) {
797: 			auto next = MinValue(entry_remaining, (idx_t)STANDARD_VECTOR_SIZE);
798: 
799: 			// initialize a new vector to append
800: 			Vector append_vector(v.GetType());
801: 			append_vector.SetVectorType(v.GetVectorType());
802: 			ListVector::Initialize(append_vector);
803: 			auto &list_vec_to_append = ListVector::GetEntry(append_vector);
804: 
805: 			// set validity
806: 			//! Since we are constructing the vector, this will always be a flat vector.
807: 			auto &append_validity = FlatVector::Validity(list_vec_to_append);
808: 			for (idx_t entry_idx = 0; entry_idx < next; entry_idx++) {
809: 				append_validity.Set(entry_idx, *(validitymask_location) & (1 << offset_in_byte));
810: 				if (++offset_in_byte == 8) {
811: 					validitymask_location++;
812: 					offset_in_byte = 0;
813: 				}
814: 			}
815: 
816: 			// compute entry sizes and set locations where the list entries are
817: 			if (TypeIsConstantSize(child_type.InternalType())) {
818: 				// constant size list entries
819: 				const idx_t type_size = GetTypeIdSize(child_type.InternalType());
820: 				for (idx_t entry_idx = 0; entry_idx < next; entry_idx++) {
821: 					list_entry_locations[entry_idx] = key_locations[i];
822: 					key_locations[i] += type_size;
823: 				}
824: 			} else {
825: 				// variable size list entries
826: 				for (idx_t entry_idx = 0; entry_idx < next; entry_idx++) {
827: 					list_entry_locations[entry_idx] = key_locations[i];
828: 					key_locations[i] += Load<idx_t>(var_entry_size_ptr);
829: 					var_entry_size_ptr += sizeof(idx_t);
830: 				}
831: 			}
832: 
833: 			// now deserialize and add to listvector
834: 			DeserializeIntoVector(list_vec_to_append, next, 0, list_entry_locations, nullptr);
835: 			ListVector::Append(v, list_vec_to_append, next);
836: 
837: 			// update for next iteration
838: 			entry_remaining -= next;
839: 			entry_offset += next;
840: 		}
841: 	}
842: }
843: 
844: void RowDataCollection::DeserializeIntoVector(Vector &v, const idx_t &vcount, const idx_t &col_idx,
845:                                               data_ptr_t key_locations[], data_ptr_t validitymask_locations[]) {
846: 	auto &validity = FlatVector::Validity(v);
847: 	if (validitymask_locations) {
848: 		// validity mask is not yet set: deserialize it
849: 		const auto byte_offset = col_idx / 8;
850: 		const auto bit = 1 << (col_idx % 8);
851: 
852: 		// fixed-size inner loop to allow unrolling
853: 		idx_t i;
854: 		for (i = 0; i + 7 < vcount; i += 8) {
855: 			for (idx_t j = 0; j < 8; j++) {
856: 				bool valid = *(validitymask_locations[i + j] + byte_offset) & bit;
857: 				validity.Set(i + j, valid);
858: 			}
859: 		}
860: 
861: 		// finishing up
862: 		for (i = 0; i < vcount; i++) {
863: 			bool valid = *(validitymask_locations[i] + byte_offset) & bit;
864: 			validity.Set(i, valid);
865: 		}
866: 	}
867: 
868: 	auto type = v.GetType().InternalType();
869: 	switch (type) {
870: 	case PhysicalType::BOOL:
871: 	case PhysicalType::INT8:
872: 		TemplatedDeserializeIntoVector<int8_t>(v, vcount, col_idx, key_locations);
873: 		break;
874: 	case PhysicalType::INT16:
875: 		TemplatedDeserializeIntoVector<int16_t>(v, vcount, col_idx, key_locations);
876: 		break;
877: 	case PhysicalType::INT32:
878: 		TemplatedDeserializeIntoVector<int32_t>(v, vcount, col_idx, key_locations);
879: 		break;
880: 	case PhysicalType::INT64:
881: 		TemplatedDeserializeIntoVector<int64_t>(v, vcount, col_idx, key_locations);
882: 		break;
883: 	case PhysicalType::UINT8:
884: 		TemplatedDeserializeIntoVector<uint8_t>(v, vcount, col_idx, key_locations);
885: 		break;
886: 	case PhysicalType::UINT16:
887: 		TemplatedDeserializeIntoVector<uint16_t>(v, vcount, col_idx, key_locations);
888: 		break;
889: 	case PhysicalType::UINT32:
890: 		TemplatedDeserializeIntoVector<uint32_t>(v, vcount, col_idx, key_locations);
891: 		break;
892: 	case PhysicalType::UINT64:
893: 		TemplatedDeserializeIntoVector<uint64_t>(v, vcount, col_idx, key_locations);
894: 		break;
895: 	case PhysicalType::INT128:
896: 		TemplatedDeserializeIntoVector<hugeint_t>(v, vcount, col_idx, key_locations);
897: 		break;
898: 	case PhysicalType::FLOAT:
899: 		TemplatedDeserializeIntoVector<float>(v, vcount, col_idx, key_locations);
900: 		break;
901: 	case PhysicalType::DOUBLE:
902: 		TemplatedDeserializeIntoVector<double>(v, vcount, col_idx, key_locations);
903: 		break;
904: 	case PhysicalType::HASH:
905: 		TemplatedDeserializeIntoVector<hash_t>(v, vcount, col_idx, key_locations);
906: 		break;
907: 	case PhysicalType::INTERVAL:
908: 		TemplatedDeserializeIntoVector<interval_t>(v, vcount, col_idx, key_locations);
909: 		break;
910: 	case PhysicalType::VARCHAR:
911: 		DeserializeIntoStringVector(v, vcount, col_idx, key_locations, validitymask_locations);
912: 		break;
913: 	case PhysicalType::MAP:
914: 	case PhysicalType::STRUCT:
915: 		DeserializeIntoStructVector(v, vcount, col_idx, key_locations, validitymask_locations);
916: 		break;
917: 	case PhysicalType::LIST:
918: 		DeserializeIntoListVector(v, vcount, col_idx, key_locations, validitymask_locations);
919: 		break;
920: 	default:
921: 		throw NotImplementedException("Unimplemented deserialize from row-format");
922: 	}
923: }
924: 
925: } // namespace duckdb
[end of src/common/types/row_data_collection.cpp]
[start of src/common/types/vector.cpp]
1: #include "duckdb/common/types/vector.hpp"
2: 
3: #include "duckdb/common/algorithm.hpp"
4: #include "duckdb/common/assert.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/pair.hpp"
7: #include "duckdb/common/printer.hpp"
8: #include "duckdb/common/serializer.hpp"
9: #include "duckdb/common/to_string.hpp"
10: #include "duckdb/common/types/chunk_collection.hpp"
11: #include "duckdb/common/types/null_value.hpp"
12: #include "duckdb/common/types/sel_cache.hpp"
13: #include "duckdb/common/vector_operations/vector_operations.hpp"
14: #include "duckdb/storage/buffer/buffer_handle.hpp"
15: #include "duckdb/common/operator/comparison_operators.hpp"
16: 
17: #include <cstring> // strlen() on Solaris
18: 
19: namespace duckdb {
20: 
21: Vector::Vector(const LogicalType &type, bool create_data, bool zero_data) : data(nullptr) {
22: 	buffer = make_buffer<VectorBuffer>(VectorType::FLAT_VECTOR, type);
23: 	if (create_data) {
24: 		Initialize(type, zero_data);
25: 	}
26: }
27: 
28: Vector::Vector(const LogicalType &type) : Vector(type, true, false) {
29: }
30: 
31: Vector::Vector(const LogicalType &type, data_ptr_t dataptr) : data(dataptr) {
32: 	buffer = make_buffer<VectorBuffer>(VectorType::FLAT_VECTOR, type);
33: 	if (dataptr && type.id() == LogicalTypeId::INVALID) {
34: 		throw InvalidTypeException(type, "Cannot create a vector of type INVALID!");
35: 	}
36: }
37: 
38: Vector::Vector(const Value &value) {
39: 	buffer = make_buffer<VectorBuffer>(VectorType::CONSTANT_VECTOR);
40: 	Reference(value);
41: }
42: 
43: Vector::Vector() : data(nullptr) {
44: 	buffer = make_buffer<VectorBuffer>(VectorType::FLAT_VECTOR, LogicalTypeId::INVALID);
45: }
46: 
47: Vector::Vector(Vector &&other) noexcept
48:     : data(other.data), validity(move(other.validity)), buffer(move(other.buffer)), auxiliary(move(other.auxiliary)) {
49: }
50: 
51: void Vector::Reference(const Value &value) {
52: 	buffer = VectorBuffer::CreateConstantVector(VectorType::CONSTANT_VECTOR, value.type());
53: 	if (value.type().id() == LogicalTypeId::STRUCT || value.type().id() == LogicalTypeId::MAP) {
54: 		auto struct_buffer = make_unique<VectorStructBuffer>();
55: 		auto &child_types = value.type().child_types();
56: 		auto &child_vectors = struct_buffer->GetChildren();
57: 		for (idx_t i = 0; i < child_types.size(); i++) {
58: 			auto vector = make_unique<Vector>(value.is_null ? Value(child_types[i].second) : value.struct_value[i]);
59: 			child_vectors.push_back(move(vector));
60: 		}
61: 		auxiliary = move(struct_buffer);
62: 		if (value.is_null) {
63: 			SetValue(0, value);
64: 		}
65: 	} else {
66: 		auxiliary.reset();
67: 		data = buffer->GetData();
68: 		SetValue(0, value);
69: 	}
70: }
71: 
72: void Vector::Reference(Vector &other) {
73: 	buffer = other.buffer;
74: 	auxiliary = other.auxiliary;
75: 	data = other.data;
76: 	validity = other.validity;
77: }
78: 
79: void Vector::Slice(Vector &other, idx_t offset) {
80: 	if (other.GetVectorType() == VectorType::CONSTANT_VECTOR) {
81: 		Reference(other);
82: 		return;
83: 	}
84: 	D_ASSERT(GetVectorType() == VectorType::FLAT_VECTOR);
85: 	D_ASSERT(other.GetVectorType() == VectorType::FLAT_VECTOR);
86: 
87: 	// create a reference to the other vector
88: 	Reference(other);
89: 	if (offset > 0) {
90: 		data = data + GetTypeIdSize(GetType().InternalType()) * offset;
91: 		validity.Slice(other.validity, offset);
92: 	}
93: }
94: 
95: void Vector::Slice(Vector &other, const SelectionVector &sel, idx_t count) {
96: 	Reference(other);
97: 	Slice(sel, count);
98: }
99: 
100: void Vector::Slice(const SelectionVector &sel, idx_t count) {
101: 	if (GetVectorType() == VectorType::CONSTANT_VECTOR) {
102: 		// dictionary on a constant is just a constant
103: 		return;
104: 	}
105: 	if (GetVectorType() == VectorType::DICTIONARY_VECTOR) {
106: 		// already a dictionary, slice the current dictionary
107: 		auto &current_sel = DictionaryVector::SelVector(*this);
108: 		auto sliced_dictionary = current_sel.Slice(sel, count);
109: 		buffer = make_buffer<DictionaryBuffer>(move(sliced_dictionary), GetType(), GetVectorType());
110: 		return;
111: 	}
112: 	auto child_ref = make_buffer<VectorChildBuffer>();
113: 	child_ref->data.Reference(*this);
114: 
115: 	auto dict_buffer = make_buffer<DictionaryBuffer>(sel, GetType(), VectorType::DICTIONARY_VECTOR);
116: 	buffer = move(dict_buffer);
117: 	auxiliary = move(child_ref);
118: }
119: 
120: void Vector::Slice(const SelectionVector &sel, idx_t count, SelCache &cache) {
121: 	if (GetVectorType() == VectorType::DICTIONARY_VECTOR) {
122: 		// dictionary vector: need to merge dictionaries
123: 		// check if we have a cached entry
124: 		auto &current_sel = DictionaryVector::SelVector(*this);
125: 		auto target_data = current_sel.data();
126: 		auto entry = cache.cache.find(target_data);
127: 		if (entry != cache.cache.end()) {
128: 			// cached entry exists: use that
129: 			this->buffer = make_buffer<DictionaryBuffer>(((DictionaryBuffer &)*entry->second).GetSelVector(),
130: 			                                             buffer->GetType(), buffer->GetVectorType());
131: 		} else {
132: 			Slice(sel, count);
133: 			cache.cache[target_data] = this->buffer;
134: 		}
135: 	} else {
136: 		Slice(sel, count);
137: 	}
138: }
139: 
140: void Vector::Initialize(const LogicalType &new_type, bool zero_data) {
141: 	if (new_type.id() != LogicalTypeId::INVALID) {
142: 		SetType(new_type);
143: 	}
144: 	auxiliary.reset();
145: 	validity.Reset();
146: 	auto &type = GetType();
147: 	if (type.id() == LogicalTypeId::STRUCT || type.id() == LogicalTypeId::MAP) {
148: 		auto struct_buffer = make_unique<VectorStructBuffer>();
149: 		auto &child_types = type.child_types();
150: 		auto &child_vectors = struct_buffer->GetChildren();
151: 		for (auto &child_type : child_types) {
152: 			auto vector = make_unique<Vector>(child_type.second);
153: 			child_vectors.push_back(move(vector));
154: 		}
155: 
156: 		auxiliary = move(struct_buffer);
157: 	}
158: 	if (GetTypeIdSize(type.InternalType()) > 0) {
159: 		buffer = VectorBuffer::CreateStandardVector(VectorType::FLAT_VECTOR, type);
160: 		data = buffer->GetData();
161: 		if (zero_data) {
162: 			memset(data, 0, STANDARD_VECTOR_SIZE * GetTypeIdSize(type.InternalType()));
163: 		}
164: 	} else {
165: 		buffer = VectorBuffer::CreateStandardVector(VectorType::FLAT_VECTOR, type);
166: 	}
167: }
168: 
169: struct DataArrays {
170: 	Vector &vec;
171: 	data_ptr_t data;
172: 	VectorBuffer *buffer;
173: 	idx_t type_size;
174: 	bool is_nested;
175: 	DataArrays(Vector &vec, data_ptr_t data, VectorBuffer *buffer, idx_t type_size, bool is_nested)
176: 	    : vec(vec), data(data), buffer(buffer), type_size(type_size), is_nested(is_nested) {};
177: };
178: 
179: void FindChildren(std::vector<DataArrays> &to_resize, VectorBuffer &auxiliary) {
180: 	if (auxiliary.GetBufferType() == VectorBufferType::LIST_BUFFER) {
181: 		auto &buffer = (VectorListBuffer &)auxiliary;
182: 		auto &child = buffer.GetChild();
183: 		auto data = child.GetData();
184: 		if (!data) {
185: 			//! Nested type
186: 			DataArrays arrays(child, data, child.GetBuffer().get(), GetTypeIdSize(child.GetType().InternalType()),
187: 			                  true);
188: 			to_resize.emplace_back(arrays);
189: 			FindChildren(to_resize, *child.GetAuxiliary());
190: 		} else {
191: 			DataArrays arrays(child, data, child.GetBuffer().get(), GetTypeIdSize(child.GetType().InternalType()),
192: 			                  false);
193: 			to_resize.emplace_back(arrays);
194: 		}
195: 	} else if (auxiliary.GetBufferType() == VectorBufferType::STRUCT_BUFFER) {
196: 		auto &buffer = (VectorStructBuffer &)auxiliary;
197: 		auto &children = buffer.GetChildren();
198: 		for (auto &child : children) {
199: 			auto data = child->GetData();
200: 			if (!data) {
201: 				//! Nested type
202: 				DataArrays arrays(*child, data, child->GetBuffer().get(),
203: 				                  GetTypeIdSize(child->GetType().InternalType()), true);
204: 				to_resize.emplace_back(arrays);
205: 				FindChildren(to_resize, *child->GetAuxiliary());
206: 			} else {
207: 				DataArrays arrays(*child, data, child->GetBuffer().get(),
208: 				                  GetTypeIdSize(child->GetType().InternalType()), false);
209: 				to_resize.emplace_back(arrays);
210: 			}
211: 		}
212: 	}
213: }
214: void Vector::Resize(idx_t cur_size, idx_t new_size) {
215: 	std::vector<DataArrays> to_resize;
216: 	if (!data) {
217: 		//! this is a nested structure
218: 		DataArrays arrays(*this, data, buffer.get(), GetTypeIdSize(GetType().InternalType()), true);
219: 		to_resize.emplace_back(arrays);
220: 		FindChildren(to_resize, *auxiliary);
221: 	} else {
222: 		DataArrays arrays(*this, data, buffer.get(), GetTypeIdSize(GetType().InternalType()), false);
223: 		to_resize.emplace_back(arrays);
224: 	}
225: 	for (auto &data_to_resize : to_resize) {
226: 		if (!data_to_resize.is_nested) {
227: 			auto new_data = unique_ptr<data_t[]>(new data_t[new_size * data_to_resize.type_size]);
228: 			memcpy(new_data.get(), data_to_resize.data, cur_size * data_to_resize.type_size * sizeof(data_t));
229: 			data_to_resize.buffer->SetData(move(new_data));
230: 			data_to_resize.vec.data = data_to_resize.buffer->GetData();
231: 		}
232: 		data_to_resize.vec.validity.Resize(cur_size, new_size);
233: 	}
234: }
235: 
236: void Vector::SetValue(idx_t index, const Value &val) {
237: 	if (GetVectorType() == VectorType::DICTIONARY_VECTOR) {
238: 		// dictionary: apply dictionary and forward to child
239: 		auto &sel_vector = DictionaryVector::SelVector(*this);
240: 		auto &child = DictionaryVector::Child(*this);
241: 		return child.SetValue(sel_vector.get_index(index), val);
242: 	}
243: 	if (val.type() != GetType()) {
244: 		SetValue(index, val.CastAs(GetType()));
245: 		return;
246: 	}
247: 
248: 	validity.EnsureWritable();
249: 	validity.Set(index, !val.is_null);
250: 	if (val.is_null) {
251: 		return;
252: 	}
253: 
254: 	switch (GetType().id()) {
255: 	case LogicalTypeId::BOOLEAN:
256: 		((bool *)data)[index] = val.value_.boolean;
257: 		break;
258: 	case LogicalTypeId::TINYINT:
259: 		((int8_t *)data)[index] = val.value_.tinyint;
260: 		break;
261: 	case LogicalTypeId::SMALLINT:
262: 		((int16_t *)data)[index] = val.value_.smallint;
263: 		break;
264: 	case LogicalTypeId::DATE:
265: 	case LogicalTypeId::INTEGER:
266: 		((int32_t *)data)[index] = val.value_.integer;
267: 		break;
268: 	case LogicalTypeId::TIMESTAMP:
269: 	case LogicalTypeId::TIMESTAMP_SEC:
270: 	case LogicalTypeId::TIMESTAMP_MS:
271: 	case LogicalTypeId::TIMESTAMP_NS:
272: 	case LogicalTypeId::HASH:
273: 	case LogicalTypeId::TIME:
274: 	case LogicalTypeId::BIGINT:
275: 		((int64_t *)data)[index] = val.value_.bigint;
276: 		break;
277: 	case LogicalTypeId::UTINYINT:
278: 		((uint8_t *)data)[index] = val.value_.utinyint;
279: 		break;
280: 	case LogicalTypeId::USMALLINT:
281: 		((uint16_t *)data)[index] = val.value_.usmallint;
282: 		break;
283: 	case LogicalTypeId::UINTEGER:
284: 		((uint32_t *)data)[index] = val.value_.uinteger;
285: 		break;
286: 	case LogicalTypeId::UBIGINT:
287: 		((uint64_t *)data)[index] = val.value_.ubigint;
288: 		break;
289: 	case LogicalTypeId::HUGEINT:
290: 		((hugeint_t *)data)[index] = val.value_.hugeint;
291: 		break;
292: 	case LogicalTypeId::DECIMAL:
293: 		D_ASSERT(GetType().width() == val.type().width() && GetType().scale() == val.type().scale());
294: 		switch (GetType().InternalType()) {
295: 		case PhysicalType::INT16:
296: 			((int16_t *)data)[index] = val.value_.smallint;
297: 			break;
298: 		case PhysicalType::INT32:
299: 			((int32_t *)data)[index] = val.value_.integer;
300: 			break;
301: 		case PhysicalType::INT64:
302: 			((int64_t *)data)[index] = val.value_.bigint;
303: 			break;
304: 		case PhysicalType::INT128:
305: 			((hugeint_t *)data)[index] = val.value_.hugeint;
306: 			break;
307: 		default:
308: 			throw NotImplementedException("Widths bigger than 38 are not supported");
309: 		}
310: 		break;
311: 	case LogicalTypeId::FLOAT:
312: 		((float *)data)[index] = val.value_.float_;
313: 		break;
314: 	case LogicalTypeId::DOUBLE:
315: 		((double *)data)[index] = val.value_.double_;
316: 		break;
317: 	case LogicalTypeId::POINTER:
318: 		((uintptr_t *)data)[index] = val.value_.pointer;
319: 		break;
320: 	case LogicalTypeId::INTERVAL:
321: 		((interval_t *)data)[index] = val.value_.interval;
322: 		break;
323: 	case LogicalTypeId::VARCHAR:
324: 	case LogicalTypeId::BLOB:
325: 		((string_t *)data)[index] = StringVector::AddStringOrBlob(*this, val.str_value);
326: 		break;
327: 	case LogicalTypeId::MAP:
328: 	case LogicalTypeId::STRUCT: {
329: 		auto &children = StructVector::GetEntries(*this);
330: 		D_ASSERT(children.size() == val.struct_value.size());
331: 
332: 		for (size_t i = 0; i < val.struct_value.size(); i++) {
333: 			auto &struct_child = val.struct_value[i];
334: 			D_ASSERT(GetVectorType() == VectorType::CONSTANT_VECTOR || GetVectorType() == VectorType::FLAT_VECTOR);
335: 			auto &vec_child = children[i];
336: 			vec_child->SetValue(index, struct_child);
337: 		}
338: 		break;
339: 	}
340: 	case LogicalTypeId::LIST: {
341: 		if (!auxiliary) {
342: 			auto vec_list = make_unique<Vector>(GetType().child_types()[0].second);
343: 			ListVector::SetEntry(*this, move(vec_list));
344: 		}
345: 		auto offset = ListVector::GetListSize(*this);
346: 		if (!val.list_value.empty()) {
347: 			for (idx_t i = 0; i < val.list_value.size(); i++) {
348: 				Value v(val.list_value[i]);
349: 				ListVector::PushBack(*this, v);
350: 			}
351: 		}
352: 		//! now set the pointer
353: 		auto &entry = ((list_entry_t *)data)[index];
354: 		entry.length = val.list_value.size();
355: 		entry.offset = offset;
356: 		break;
357: 	}
358: 	default:
359: 		throw NotImplementedException("Unimplemented type for Vector::SetValue");
360: 	}
361: }
362: 
363: Value Vector::GetValue(idx_t index) const {
364: 	switch (GetVectorType()) {
365: 	case VectorType::CONSTANT_VECTOR:
366: 		index = 0;
367: 		break;
368: 	case VectorType::FLAT_VECTOR:
369: 		break;
370: 		// dictionary: apply dictionary and forward to child
371: 	case VectorType::DICTIONARY_VECTOR: {
372: 		auto &sel_vector = DictionaryVector::SelVector(*this);
373: 		auto &child = DictionaryVector::Child(*this);
374: 		return child.GetValue(sel_vector.get_index(index));
375: 	}
376: 	case VectorType::SEQUENCE_VECTOR: {
377: 		int64_t start, increment;
378: 		SequenceVector::GetSequence(*this, start, increment);
379: 		return Value::Numeric(GetType(), start + increment * index);
380: 	}
381: 	default:
382: 		throw NotImplementedException("Unimplemented vector type for Vector::GetValue");
383: 	}
384: 
385: 	if (!validity.RowIsValid(index)) {
386: 		return Value(GetType());
387: 	}
388: 	switch (GetType().id()) {
389: 	case LogicalTypeId::BOOLEAN:
390: 		return Value::BOOLEAN(((bool *)data)[index]);
391: 	case LogicalTypeId::TINYINT:
392: 		return Value::TINYINT(((int8_t *)data)[index]);
393: 	case LogicalTypeId::SMALLINT:
394: 		return Value::SMALLINT(((int16_t *)data)[index]);
395: 	case LogicalTypeId::INTEGER:
396: 		return Value::INTEGER(((int32_t *)data)[index]);
397: 	case LogicalTypeId::DATE:
398: 		return Value::DATE(((date_t *)data)[index]);
399: 	case LogicalTypeId::TIME:
400: 		return Value::TIME(((dtime_t *)data)[index]);
401: 	case LogicalTypeId::BIGINT:
402: 		return Value::BIGINT(((int64_t *)data)[index]);
403: 	case LogicalTypeId::UTINYINT:
404: 		return Value::UTINYINT(((uint8_t *)data)[index]);
405: 	case LogicalTypeId::USMALLINT:
406: 		return Value::USMALLINT(((uint16_t *)data)[index]);
407: 	case LogicalTypeId::UINTEGER:
408: 		return Value::UINTEGER(((uint32_t *)data)[index]);
409: 	case LogicalTypeId::UBIGINT:
410: 		return Value::UBIGINT(((uint64_t *)data)[index]);
411: 	case LogicalTypeId::TIMESTAMP:
412: 		return Value::TIMESTAMP(((timestamp_t *)data)[index]);
413: 	case LogicalTypeId::TIMESTAMP_NS:
414: 		return Value::TimestampNs(((timestamp_t *)data)[index]);
415: 	case LogicalTypeId::TIMESTAMP_MS:
416: 		return Value::TimestampMs(((timestamp_t *)data)[index]);
417: 	case LogicalTypeId::TIMESTAMP_SEC:
418: 		return Value::TimestampSec(((timestamp_t *)data)[index]);
419: 	case LogicalTypeId::HUGEINT:
420: 		return Value::HUGEINT(((hugeint_t *)data)[index]);
421: 	case LogicalTypeId::DECIMAL: {
422: 		switch (GetType().InternalType()) {
423: 		case PhysicalType::INT16:
424: 			return Value::DECIMAL(((int16_t *)data)[index], GetType().width(), GetType().scale());
425: 		case PhysicalType::INT32:
426: 			return Value::DECIMAL(((int32_t *)data)[index], GetType().width(), GetType().scale());
427: 		case PhysicalType::INT64:
428: 			return Value::DECIMAL(((int64_t *)data)[index], GetType().width(), GetType().scale());
429: 		case PhysicalType::INT128:
430: 			return Value::DECIMAL(((hugeint_t *)data)[index], GetType().width(), GetType().scale());
431: 		default:
432: 			throw NotImplementedException("Widths bigger than 38 are not supported");
433: 		}
434: 	}
435: 	case LogicalTypeId::HASH:
436: 		return Value::HASH(((hash_t *)data)[index]);
437: 	case LogicalTypeId::POINTER:
438: 		return Value::POINTER(((uintptr_t *)data)[index]);
439: 	case LogicalTypeId::FLOAT:
440: 		return Value::FLOAT(((float *)data)[index]);
441: 	case LogicalTypeId::DOUBLE:
442: 		return Value::DOUBLE(((double *)data)[index]);
443: 	case LogicalTypeId::INTERVAL:
444: 		return Value::INTERVAL(((interval_t *)data)[index]);
445: 	case LogicalTypeId::VARCHAR: {
446: 		auto str = ((string_t *)data)[index];
447: 		return Value(str.GetString());
448: 	}
449: 	case LogicalTypeId::BLOB: {
450: 		auto str = ((string_t *)data)[index];
451: 		return Value::BLOB((const_data_ptr_t)str.GetDataUnsafe(), str.GetSize());
452: 	}
453: 	case LogicalTypeId::MAP:
454: 	case LogicalTypeId::STRUCT: {
455: 		Value ret(GetType());
456: 		ret.is_null = false;
457: 		// we can derive the value schema from the vector schema
458: 		auto &child_entries = StructVector::GetEntries(*this);
459: 		for (auto &struct_child : child_entries) {
460: 			ret.struct_value.push_back(struct_child->GetValue(index));
461: 		}
462: 		return ret;
463: 	}
464: 	case LogicalTypeId::LIST: {
465: 		Value ret(GetType());
466: 		ret.is_null = false;
467: 		auto offlen = ((list_entry_t *)data)[index];
468: 		auto &child_vec = ListVector::GetEntry(*this);
469: 		for (idx_t i = offlen.offset; i < offlen.offset + offlen.length; i++) {
470: 			ret.list_value.push_back(child_vec.GetValue(i));
471: 		}
472: 		return ret;
473: 	}
474: 	default:
475: 		throw NotImplementedException("Unimplemented type for value access");
476: 	}
477: }
478: 
479: string VectorTypeToString(VectorType type) {
480: 	switch (type) {
481: 	case VectorType::FLAT_VECTOR:
482: 		return "FLAT";
483: 	case VectorType::SEQUENCE_VECTOR:
484: 		return "SEQUENCE";
485: 	case VectorType::DICTIONARY_VECTOR:
486: 		return "DICTIONARY";
487: 	case VectorType::CONSTANT_VECTOR:
488: 		return "CONSTANT";
489: 	default:
490: 		return "UNKNOWN";
491: 	}
492: }
493: 
494: string Vector::ToString(idx_t count) const {
495: 	string retval =
496: 	    VectorTypeToString(GetVectorType()) + " " + GetType().ToString() + ": " + to_string(count) + " = [ ";
497: 	switch (GetVectorType()) {
498: 	case VectorType::FLAT_VECTOR:
499: 	case VectorType::DICTIONARY_VECTOR:
500: 		for (idx_t i = 0; i < count; i++) {
501: 			retval += GetValue(i).ToString() + (i == count - 1 ? "" : ", ");
502: 		}
503: 		break;
504: 	case VectorType::CONSTANT_VECTOR:
505: 		retval += GetValue(0).ToString();
506: 		break;
507: 	case VectorType::SEQUENCE_VECTOR: {
508: 		int64_t start, increment;
509: 		SequenceVector::GetSequence(*this, start, increment);
510: 		for (idx_t i = 0; i < count; i++) {
511: 			retval += to_string(start + increment * i) + (i == count - 1 ? "" : ", ");
512: 		}
513: 		break;
514: 	}
515: 	default:
516: 		retval += "UNKNOWN VECTOR TYPE";
517: 		break;
518: 	}
519: 	retval += "]";
520: 	return retval;
521: }
522: 
523: void Vector::Print(idx_t count) {
524: 	Printer::Print(ToString(count));
525: }
526: 
527: string Vector::ToString() const {
528: 	string retval = VectorTypeToString(GetVectorType()) + " " + GetType().ToString() + ": (UNKNOWN COUNT) [ ";
529: 	switch (GetVectorType()) {
530: 	case VectorType::FLAT_VECTOR:
531: 	case VectorType::DICTIONARY_VECTOR:
532: 		break;
533: 	case VectorType::CONSTANT_VECTOR:
534: 		retval += GetValue(0).ToString();
535: 		break;
536: 	case VectorType::SEQUENCE_VECTOR: {
537: 		break;
538: 	}
539: 	default:
540: 		retval += "UNKNOWN VECTOR TYPE";
541: 		break;
542: 	}
543: 	retval += "]";
544: 	return retval;
545: }
546: 
547: void Vector::Print() {
548: 	Printer::Print(ToString());
549: }
550: 
551: template <class T>
552: static void TemplatedFlattenConstantVector(data_ptr_t data, data_ptr_t old_data, idx_t count) {
553: 	auto constant = Load<T>(old_data);
554: 	auto output = (T *)data;
555: 	for (idx_t i = 0; i < count; i++) {
556: 		output[i] = constant;
557: 	}
558: }
559: 
560: void Vector::Normalify(idx_t count) {
561: 	switch (GetVectorType()) {
562: 	case VectorType::FLAT_VECTOR:
563: 		// already a flat vector
564: 		break;
565: 	case VectorType::DICTIONARY_VECTOR: {
566: 		// create a new flat vector of this type
567: 		Vector other(GetType());
568: 		// now copy the data of this vector to the other vector, removing the selection vector in the process
569: 		VectorOperations::Copy(*this, other, count, 0, 0);
570: 		// create a reference to the data in the other vector
571: 		this->Reference(other);
572: 		break;
573: 	}
574: 	case VectorType::CONSTANT_VECTOR: {
575: 		bool is_null = ConstantVector::IsNull(*this);
576: 		// allocate a new buffer for the vector
577: 		auto old_buffer = move(buffer);
578: 		auto old_data = data;
579: 		buffer = VectorBuffer::CreateStandardVector(VectorType::FLAT_VECTOR, old_buffer->GetType());
580: 		data = buffer->GetData();
581: 		if (is_null) {
582: 			// constant NULL, set nullmask
583: 			validity.EnsureWritable();
584: 			validity.SetAllInvalid(count);
585: 			return;
586: 		}
587: 		// non-null constant: have to repeat the constant
588: 		switch (GetType().InternalType()) {
589: 		case PhysicalType::BOOL:
590: 			TemplatedFlattenConstantVector<bool>(data, old_data, count);
591: 			break;
592: 		case PhysicalType::INT8:
593: 			TemplatedFlattenConstantVector<int8_t>(data, old_data, count);
594: 			break;
595: 		case PhysicalType::INT16:
596: 			TemplatedFlattenConstantVector<int16_t>(data, old_data, count);
597: 			break;
598: 		case PhysicalType::INT32:
599: 			TemplatedFlattenConstantVector<int32_t>(data, old_data, count);
600: 			break;
601: 		case PhysicalType::INT64:
602: 			TemplatedFlattenConstantVector<int64_t>(data, old_data, count);
603: 			break;
604: 		case PhysicalType::UINT8:
605: 			TemplatedFlattenConstantVector<uint8_t>(data, old_data, count);
606: 			break;
607: 		case PhysicalType::UINT16:
608: 			TemplatedFlattenConstantVector<uint16_t>(data, old_data, count);
609: 			break;
610: 		case PhysicalType::UINT32:
611: 			TemplatedFlattenConstantVector<uint32_t>(data, old_data, count);
612: 			break;
613: 		case PhysicalType::UINT64:
614: 			TemplatedFlattenConstantVector<uint64_t>(data, old_data, count);
615: 			break;
616: 		case PhysicalType::INT128:
617: 			TemplatedFlattenConstantVector<hugeint_t>(data, old_data, count);
618: 			break;
619: 		case PhysicalType::FLOAT:
620: 			TemplatedFlattenConstantVector<float>(data, old_data, count);
621: 			break;
622: 		case PhysicalType::DOUBLE:
623: 			TemplatedFlattenConstantVector<double>(data, old_data, count);
624: 			break;
625: 		case PhysicalType::HASH:
626: 			TemplatedFlattenConstantVector<hash_t>(data, old_data, count);
627: 			break;
628: 		case PhysicalType::POINTER:
629: 			TemplatedFlattenConstantVector<uintptr_t>(data, old_data, count);
630: 			break;
631: 		case PhysicalType::INTERVAL:
632: 			TemplatedFlattenConstantVector<interval_t>(data, old_data, count);
633: 			break;
634: 		case PhysicalType::VARCHAR:
635: 			TemplatedFlattenConstantVector<string_t>(data, old_data, count);
636: 			break;
637: 		case PhysicalType::LIST: {
638: 			TemplatedFlattenConstantVector<list_entry_t>(data, old_data, count);
639: 			break;
640: 		}
641: 		case PhysicalType::MAP:
642: 		case PhysicalType::STRUCT: {
643: 			auto &child_entries = StructVector::GetEntries(*this);
644: 			for (auto &child : child_entries) {
645: 				D_ASSERT(child->GetVectorType() == VectorType::CONSTANT_VECTOR);
646: 				child->Normalify(count);
647: 			}
648: 		} break;
649: 		default:
650: 			throw NotImplementedException("Unimplemented type for VectorOperations::Normalify");
651: 		}
652: 		break;
653: 	}
654: 	case VectorType::SEQUENCE_VECTOR: {
655: 		int64_t start, increment;
656: 		SequenceVector::GetSequence(*this, start, increment);
657: 
658: 		buffer = VectorBuffer::CreateStandardVector(VectorType::FLAT_VECTOR, GetType());
659: 		data = buffer->GetData();
660: 		VectorOperations::GenerateSequence(*this, count, start, increment);
661: 		break;
662: 	}
663: 	default:
664: 		throw NotImplementedException("FIXME: unimplemented type for normalify");
665: 	}
666: }
667: 
668: void Vector::Normalify(const SelectionVector &sel, idx_t count) {
669: 	switch (GetVectorType()) {
670: 	case VectorType::FLAT_VECTOR:
671: 		// already a flat vector
672: 		break;
673: 	case VectorType::SEQUENCE_VECTOR: {
674: 		int64_t start, increment;
675: 		SequenceVector::GetSequence(*this, start, increment);
676: 
677: 		buffer = VectorBuffer::CreateStandardVector(VectorType::FLAT_VECTOR, GetType());
678: 		data = buffer->GetData();
679: 		VectorOperations::GenerateSequence(*this, count, sel, start, increment);
680: 		break;
681: 	}
682: 	default:
683: 		throw NotImplementedException("Unimplemented type for normalify with selection vector");
684: 	}
685: }
686: 
687: void Vector::Orrify(idx_t count, VectorData &data) {
688: 	switch (GetVectorType()) {
689: 	case VectorType::DICTIONARY_VECTOR: {
690: 		auto &sel = DictionaryVector::SelVector(*this);
691: 		auto &child = DictionaryVector::Child(*this);
692: 		if (child.GetVectorType() == VectorType::FLAT_VECTOR) {
693: 			data.sel = &sel;
694: 			data.data = FlatVector::GetData(child);
695: 			data.validity = FlatVector::Validity(child);
696: 		} else {
697: 			// dictionary with non-flat child: create a new reference to the child and normalify it
698: 			auto new_aux = make_buffer<VectorChildBuffer>();
699: 			new_aux->data.Reference(child);
700: 			new_aux->data.Normalify(sel, count);
701: 
702: 			data.sel = &sel;
703: 			data.data = FlatVector::GetData(new_aux->data);
704: 			data.validity = FlatVector::Validity(new_aux->data);
705: 			this->auxiliary = move(new_aux);
706: 		}
707: 		break;
708: 	}
709: 	case VectorType::CONSTANT_VECTOR:
710: 		data.sel = &ConstantVector::ZERO_SELECTION_VECTOR;
711: 		data.data = ConstantVector::GetData(*this);
712: 		data.validity = ConstantVector::Validity(*this);
713: 		break;
714: 	default:
715: 		Normalify(count);
716: 		data.sel = &FlatVector::INCREMENTAL_SELECTION_VECTOR;
717: 		data.data = FlatVector::GetData(*this);
718: 		data.validity = FlatVector::Validity(*this);
719: 		break;
720: 	}
721: }
722: 
723: void Vector::Sequence(int64_t start, int64_t increment) {
724: 	this->buffer = make_buffer<VectorBuffer>(VectorType::SEQUENCE_VECTOR, GetType(), sizeof(int64_t) * 2);
725: 	auto data = (int64_t *)buffer->GetData();
726: 	data[0] = start;
727: 	data[1] = increment;
728: 	validity.Reset();
729: 	auxiliary.reset();
730: }
731: 
732: void Vector::Serialize(idx_t count, Serializer &serializer) {
733: 	auto &type = GetType();
734: 
735: 	VectorData vdata;
736: 	Orrify(count, vdata);
737: 
738: 	const auto write_validity = (count > 0) && !vdata.validity.AllValid();
739: 	serializer.Write<bool>(write_validity);
740: 	if (write_validity) {
741: 		ValidityMask flat_mask(count);
742: 		for (idx_t i = 0; i < count; ++i) {
743: 			auto row_idx = vdata.sel->get_index(i);
744: 			flat_mask.Set(i, vdata.validity.RowIsValid(row_idx));
745: 		}
746: 		serializer.WriteData((const_data_ptr_t)flat_mask.GetData(), flat_mask.ValidityMaskSize(count));
747: 	}
748: 
749: 	if (TypeIsConstantSize(type.InternalType())) {
750: 		// constant size type: simple copy
751: 		idx_t write_size = GetTypeIdSize(type.InternalType()) * count;
752: 		auto ptr = unique_ptr<data_t[]>(new data_t[write_size]);
753: 		VectorOperations::WriteToStorage(*this, count, ptr.get());
754: 		serializer.WriteData(ptr.get(), write_size);
755: 	} else {
756: 		switch (type.InternalType()) {
757: 		case PhysicalType::VARCHAR: {
758: 			auto strings = (string_t *)vdata.data;
759: 			for (idx_t i = 0; i < count; i++) {
760: 				auto idx = vdata.sel->get_index(i);
761: 				auto source = !vdata.validity.RowIsValid(idx) ? NullValue<string_t>() : strings[idx];
762: 				serializer.WriteStringLen((const_data_ptr_t)source.GetDataUnsafe(), source.GetSize());
763: 			}
764: 			break;
765: 		}
766: 		default:
767: 			throw NotImplementedException("Unimplemented variable width type for Vector::Serialize!");
768: 		}
769: 	}
770: }
771: 
772: void Vector::Deserialize(idx_t count, Deserializer &source) {
773: 	auto &type = GetType();
774: 
775: 	auto &validity = FlatVector::Validity(*this);
776: 	validity.Reset();
777: 	const auto has_validity = source.Read<bool>();
778: 	if (has_validity) {
779: 		validity.Initialize(count);
780: 		source.ReadData((data_ptr_t)validity.GetData(), validity.ValidityMaskSize(count));
781: 	}
782: 
783: 	if (TypeIsConstantSize(type.InternalType())) {
784: 		// constant size type: read fixed amount of data from
785: 		auto column_size = GetTypeIdSize(type.InternalType()) * count;
786: 		auto ptr = unique_ptr<data_t[]>(new data_t[column_size]);
787: 		source.ReadData(ptr.get(), column_size);
788: 
789: 		VectorOperations::ReadFromStorage(ptr.get(), count, *this);
790: 	} else {
791: 		auto strings = FlatVector::GetData<string_t>(*this);
792: 		for (idx_t i = 0; i < count; i++) {
793: 			// read the strings
794: 			auto str = source.Read<string>();
795: 			// now add the string to the StringHeap of the vector
796: 			// and write the pointer into the vector
797: 			if (validity.RowIsValid(i)) {
798: 				strings[i] = StringVector::AddStringOrBlob(*this, str);
799: 			}
800: 		}
801: 	}
802: }
803: 
804: void Vector::UTFVerify(const SelectionVector &sel, idx_t count) {
805: #ifdef DEBUG
806: 	if (count == 0) {
807: 		return;
808: 	}
809: 	if (GetType().InternalType() == PhysicalType::VARCHAR) {
810: 		// we just touch all the strings and let the sanitizer figure out if any
811: 		// of them are deallocated/corrupt
812: 		switch (GetVectorType()) {
813: 		case VectorType::CONSTANT_VECTOR: {
814: 			auto string = ConstantVector::GetData<string_t>(*this);
815: 			if (!ConstantVector::IsNull(*this)) {
816: 				string->Verify();
817: 			}
818: 			break;
819: 		}
820: 		case VectorType::FLAT_VECTOR: {
821: 			auto strings = FlatVector::GetData<string_t>(*this);
822: 			for (idx_t i = 0; i < count; i++) {
823: 				auto oidx = sel.get_index(i);
824: 				if (validity.RowIsValid(oidx)) {
825: 					strings[oidx].Verify();
826: 				}
827: 			}
828: 			break;
829: 		}
830: 		default:
831: 			break;
832: 		}
833: 	}
834: #endif
835: }
836: 
837: void Vector::UTFVerify(idx_t count) {
838: 	UTFVerify(FlatVector::INCREMENTAL_SELECTION_VECTOR, count);
839: }
840: 
841: void Vector::Verify(const SelectionVector &sel, idx_t count) {
842: #ifdef DEBUG
843: 	if (count == 0) {
844: 		return;
845: 	}
846: 	if (GetVectorType() == VectorType::DICTIONARY_VECTOR) {
847: 		auto &child = DictionaryVector::Child(*this);
848: 		D_ASSERT(child.GetVectorType() != VectorType::DICTIONARY_VECTOR);
849: 		auto &dict_sel = DictionaryVector::SelVector(*this);
850: 		for (idx_t i = 0; i < count; i++) {
851: 			auto oidx = sel.get_index(i);
852: 			auto idx = dict_sel.get_index(oidx);
853: 			D_ASSERT(idx < STANDARD_VECTOR_SIZE);
854: 		}
855: 		// merge the selection vectors and verify the child
856: 		auto new_buffer = dict_sel.Slice(sel, count);
857: 		SelectionVector new_sel(new_buffer);
858: 		child.Verify(new_sel, count);
859: 		return;
860: 	}
861: 	if (TypeIsConstantSize(GetType().InternalType()) &&
862: 	    (GetVectorType() == VectorType::CONSTANT_VECTOR || GetVectorType() == VectorType::FLAT_VECTOR)) {
863: 		D_ASSERT(!auxiliary);
864: 	}
865: 	if (GetType().InternalType() == PhysicalType::DOUBLE) {
866: 		// verify that there are no INF or NAN values
867: 		switch (GetVectorType()) {
868: 		case VectorType::CONSTANT_VECTOR: {
869: 			auto dbl = ConstantVector::GetData<double>(*this);
870: 			if (!ConstantVector::IsNull(*this)) {
871: 				D_ASSERT(Value::DoubleIsValid(*dbl));
872: 			}
873: 			break;
874: 		}
875: 		case VectorType::FLAT_VECTOR: {
876: 			auto doubles = FlatVector::GetData<double>(*this);
877: 			for (idx_t i = 0; i < count; i++) {
878: 				auto oidx = sel.get_index(i);
879: 				if (validity.RowIsValid(oidx)) {
880: 					D_ASSERT(Value::DoubleIsValid(doubles[oidx]));
881: 				}
882: 			}
883: 			break;
884: 		}
885: 		default:
886: 			break;
887: 		}
888: 	}
889: 	if (GetType().id() == LogicalTypeId::VARCHAR) {
890: 		// verify that there are no '\0' bytes in string values
891: 		switch (GetVectorType()) {
892: 		case VectorType::FLAT_VECTOR: {
893: 			auto strings = FlatVector::GetData<string_t>(*this);
894: 			for (idx_t i = 0; i < count; i++) {
895: 				auto oidx = sel.get_index(i);
896: 				if (validity.RowIsValid(oidx)) {
897: 					strings[oidx].VerifyNull();
898: 				}
899: 			}
900: 			break;
901: 		}
902: 		default:
903: 			break;
904: 		}
905: 	}
906: 
907: 	if (GetType().InternalType() == PhysicalType::STRUCT || GetType().InternalType() == PhysicalType::MAP) {
908: 		auto &child_types = GetType().child_types();
909: 		D_ASSERT(child_types.size() > 0);
910: 		if (GetVectorType() == VectorType::FLAT_VECTOR || GetVectorType() == VectorType::CONSTANT_VECTOR) {
911: 			auto &children = StructVector::GetEntries(*this);
912: 			D_ASSERT(child_types.size() == children.size());
913: 			for (idx_t child_idx = 0; child_idx < children.size(); child_idx++) {
914: 				D_ASSERT(children[child_idx]->GetType() == child_types[child_idx].second);
915: 				children[child_idx]->Verify(sel, count);
916: 			}
917: 		}
918: 	}
919: 
920: 	if (GetType().InternalType() == PhysicalType::LIST) {
921: 		D_ASSERT(GetType().child_types().size() == 1);
922: 		if (GetVectorType() == VectorType::CONSTANT_VECTOR) {
923: 			if (!ConstantVector::IsNull(*this)) {
924: 				ListVector::GetEntry(*this).Verify(ListVector::GetListSize(*this));
925: 				auto le = ConstantVector::GetData<list_entry_t>(*this);
926: 				D_ASSERT(le->offset + le->length <= ListVector::GetListSize(*this));
927: 			}
928: 		} else if (GetVectorType() == VectorType::FLAT_VECTOR) {
929: 			if (ListVector::HasEntry(*this)) {
930: 				ListVector::GetEntry(*this).Verify(ListVector::GetListSize(*this));
931: 			}
932: 			auto list_data = FlatVector::GetData<list_entry_t>(*this);
933: 			for (idx_t i = 0; i < count; i++) {
934: 				auto idx = sel.get_index(i);
935: 				auto &le = list_data[idx];
936: 				if (validity.RowIsValid(idx) && ListVector::HasEntry(*this)) {
937: 					D_ASSERT(le.offset + le.length <= ListVector::GetListSize(*this));
938: 				}
939: 			}
940: 		}
941: 	}
942: #endif
943: }
944: 
945: void Vector::Verify(idx_t count) {
946: 	if (count > STANDARD_VECTOR_SIZE) {
947: 		SelectionVector selection_vector(count);
948: 		for (size_t i = 0; i < count; i++) {
949: 			selection_vector.set_index(i, i);
950: 		}
951: 		Verify(selection_vector, count);
952: 	} else {
953: 		Verify(FlatVector::INCREMENTAL_SELECTION_VECTOR, count);
954: 	}
955: }
956: 
957: string_t StringVector::AddString(Vector &vector, const char *data, idx_t len) {
958: 	return StringVector::AddString(vector, string_t(data, len));
959: }
960: 
961: string_t StringVector::AddString(Vector &vector, const char *data) {
962: 	return StringVector::AddString(vector, string_t(data, strlen(data)));
963: }
964: 
965: string_t StringVector::AddString(Vector &vector, const string &data) {
966: 	return StringVector::AddString(vector, string_t(data.c_str(), data.size()));
967: }
968: 
969: string_t StringVector::AddString(Vector &vector, string_t data) {
970: 	D_ASSERT(vector.GetType().id() == LogicalTypeId::VARCHAR);
971: 	if (data.IsInlined()) {
972: 		// string will be inlined: no need to store in string heap
973: 		return data;
974: 	}
975: 	if (!vector.auxiliary) {
976: 		vector.auxiliary = make_buffer<VectorStringBuffer>();
977: 	}
978: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::STRING_BUFFER);
979: 	auto &string_buffer = (VectorStringBuffer &)*vector.auxiliary;
980: 	return string_buffer.AddString(data);
981: }
982: 
983: string_t StringVector::AddStringOrBlob(Vector &vector, string_t data) {
984: 	D_ASSERT(vector.GetType().InternalType() == PhysicalType::VARCHAR);
985: 	if (data.IsInlined()) {
986: 		// string will be inlined: no need to store in string heap
987: 		return data;
988: 	}
989: 	if (!vector.auxiliary) {
990: 		vector.auxiliary = make_buffer<VectorStringBuffer>();
991: 	}
992: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::STRING_BUFFER);
993: 	auto &string_buffer = (VectorStringBuffer &)*vector.auxiliary;
994: 	return string_buffer.AddBlob(data);
995: }
996: 
997: string_t StringVector::EmptyString(Vector &vector, idx_t len) {
998: 	D_ASSERT(vector.GetType().InternalType() == PhysicalType::VARCHAR);
999: 	if (len < string_t::INLINE_LENGTH) {
1000: 		return string_t(len);
1001: 	}
1002: 	if (!vector.auxiliary) {
1003: 		vector.auxiliary = make_buffer<VectorStringBuffer>();
1004: 	}
1005: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::STRING_BUFFER);
1006: 	auto &string_buffer = (VectorStringBuffer &)*vector.auxiliary;
1007: 	return string_buffer.EmptyString(len);
1008: }
1009: 
1010: void StringVector::AddHandle(Vector &vector, unique_ptr<BufferHandle> handle) {
1011: 	D_ASSERT(vector.GetType().InternalType() == PhysicalType::VARCHAR);
1012: 	if (!vector.auxiliary) {
1013: 		vector.auxiliary = make_buffer<VectorStringBuffer>();
1014: 	}
1015: 	auto &string_buffer = (VectorStringBuffer &)*vector.auxiliary;
1016: 	string_buffer.AddHeapReference(make_buffer<ManagedVectorBuffer>(move(handle)));
1017: }
1018: 
1019: void StringVector::AddBuffer(Vector &vector, buffer_ptr<VectorBuffer> buffer) {
1020: 	D_ASSERT(vector.GetType().InternalType() == PhysicalType::VARCHAR);
1021: 	if (!vector.auxiliary) {
1022: 		vector.auxiliary = make_buffer<VectorStringBuffer>();
1023: 	}
1024: 	auto &string_buffer = (VectorStringBuffer &)*vector.auxiliary;
1025: 	string_buffer.AddHeapReference(move(buffer));
1026: }
1027: 
1028: void StringVector::AddHeapReference(Vector &vector, Vector &other) {
1029: 	D_ASSERT(vector.GetType().InternalType() == PhysicalType::VARCHAR);
1030: 	D_ASSERT(other.GetType().InternalType() == PhysicalType::VARCHAR);
1031: 
1032: 	if (other.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
1033: 		StringVector::AddHeapReference(vector, DictionaryVector::Child(other));
1034: 		return;
1035: 	}
1036: 	if (!other.auxiliary) {
1037: 		return;
1038: 	}
1039: 	if (!vector.auxiliary) {
1040: 		vector.auxiliary = make_buffer<VectorStringBuffer>();
1041: 	}
1042: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::STRING_BUFFER);
1043: 	D_ASSERT(other.auxiliary->GetBufferType() == VectorBufferType::STRING_BUFFER);
1044: 	auto &string_buffer = (VectorStringBuffer &)*vector.auxiliary;
1045: 	string_buffer.AddHeapReference(other.auxiliary);
1046: }
1047: 
1048: vector<unique_ptr<Vector>> &StructVector::GetEntries(Vector &vector) {
1049: 	D_ASSERT(vector.GetType().id() == LogicalTypeId::STRUCT || vector.GetType().id() == LogicalTypeId::MAP);
1050: 	D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR ||
1051: 	         vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
1052: 	D_ASSERT(vector.auxiliary);
1053: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::STRUCT_BUFFER);
1054: 	return ((VectorStructBuffer *)vector.auxiliary.get())->GetChildren();
1055: }
1056: 
1057: const vector<unique_ptr<Vector>> &StructVector::GetEntries(const Vector &vector) {
1058: 	return GetEntries((Vector &)vector);
1059: }
1060: 
1061: bool ListVector::HasEntry(const Vector &vector) {
1062: 	D_ASSERT(vector.GetType().id() == LogicalTypeId::LIST);
1063: 	if (vector.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
1064: 		auto &child = DictionaryVector::Child(vector);
1065: 		return ListVector::HasEntry(child);
1066: 	}
1067: 	D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR ||
1068: 	         vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
1069: 	return vector.auxiliary != nullptr;
1070: }
1071: 
1072: const Vector &ListVector::GetEntry(const Vector &vector) {
1073: 	D_ASSERT(vector.GetType().id() == LogicalTypeId::LIST);
1074: 	if (vector.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
1075: 		auto &child = DictionaryVector::Child(vector);
1076: 		return ListVector::GetEntry(child);
1077: 	}
1078: 	D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR ||
1079: 	         vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
1080: 	D_ASSERT(vector.auxiliary);
1081: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::LIST_BUFFER);
1082: 	return ((VectorListBuffer *)vector.auxiliary.get())->GetChild();
1083: }
1084: 
1085: Vector &ListVector::GetEntry(Vector &vector) {
1086: 	const Vector &cvector = vector;
1087: 	return const_cast<Vector &>(ListVector::GetEntry(cvector));
1088: }
1089: 
1090: void ListVector::Initialize(Vector &vec) {
1091: 	if (!ListVector::HasEntry(vec)) {
1092: 		auto vec_child = make_unique<Vector>(vec.GetType().child_types()[0].second);
1093: 		ListVector::SetEntry(vec, move(vec_child));
1094: 	}
1095: }
1096: template <class T>
1097: void TemplatedSearchInMap(Vector &list, T key, vector<idx_t> &offsets, bool is_key_null, idx_t offset, idx_t length) {
1098: 	auto &list_vector = ListVector::GetEntry(list);
1099: 	VectorData vector_data;
1100: 	list_vector.Orrify(ListVector::GetListSize(list), vector_data);
1101: 	auto data = (T *)vector_data.data;
1102: 	auto validity_mask = vector_data.validity;
1103: 
1104: 	if (is_key_null) {
1105: 		for (idx_t i = offset; i < offset + length; i++) {
1106: 			if (!validity_mask.RowIsValid(i)) {
1107: 				offsets.push_back(i);
1108: 			}
1109: 		}
1110: 	} else {
1111: 		for (idx_t i = offset; i < offset + length; i++) {
1112: 			if (!validity_mask.RowIsValid(i)) {
1113: 				continue;
1114: 			}
1115: 			if (key == data[i]) {
1116: 				offsets.push_back(i);
1117: 			}
1118: 		}
1119: 	}
1120: }
1121: 
1122: void SearchString(Vector &list, string &key, vector<idx_t> &offsets, bool is_key_null, idx_t offset, idx_t length) {
1123: 	auto &list_vector = ListVector::GetEntry(list);
1124: 	VectorData vector_data;
1125: 	list_vector.Orrify(ListVector::GetListSize(list), vector_data);
1126: 	auto data = (string_t *)vector_data.data;
1127: 	auto validity_mask = vector_data.validity;
1128: 	if (is_key_null) {
1129: 		for (idx_t i = offset; i < offset + length; i++) {
1130: 			if (!validity_mask.RowIsValid(i)) {
1131: 				offsets.push_back(i);
1132: 			}
1133: 		}
1134: 	} else {
1135: 		string_t key_str_t(key);
1136: 		for (idx_t i = offset; i < offset + length; i++) {
1137: 			if (!validity_mask.RowIsValid(i)) {
1138: 				continue;
1139: 			}
1140: 			if (Equals::Operation<string_t>(data[i], key_str_t)) {
1141: 				offsets.push_back(i);
1142: 			}
1143: 		}
1144: 	}
1145: }
1146: 
1147: vector<idx_t> ListVector::Search(Vector &list, Value &key, idx_t row) {
1148: 	vector<idx_t> offsets;
1149: 	if (!ListVector::HasEntry(list)) {
1150: 		return offsets;
1151: 	}
1152: 
1153: 	auto &list_vector = ListVector::GetEntry(list);
1154: 	auto &entry = ((list_entry_t *)list.GetData())[row];
1155: 	switch (list_vector.GetType().id()) {
1156: 
1157: 	case LogicalTypeId::SQLNULL:
1158: 		if (key.is_null) {
1159: 			for (idx_t i = entry.offset; i < entry.offset + entry.length; i++) {
1160: 				offsets.push_back(i);
1161: 			}
1162: 		}
1163: 		break;
1164: 	case LogicalTypeId::UTINYINT:
1165: 		::duckdb::TemplatedSearchInMap<uint8_t>(list, key.value_.utinyint, offsets, key.is_null, entry.offset,
1166: 		                                        entry.length);
1167: 		break;
1168: 	case LogicalTypeId::TINYINT:
1169: 		::duckdb::TemplatedSearchInMap<int8_t>(list, key.value_.tinyint, offsets, key.is_null, entry.offset,
1170: 		                                       entry.length);
1171: 		break;
1172: 	case LogicalTypeId::USMALLINT:
1173: 		::duckdb::TemplatedSearchInMap<uint16_t>(list, key.value_.usmallint, offsets, key.is_null, entry.offset,
1174: 		                                         entry.length);
1175: 		break;
1176: 	case LogicalTypeId::SMALLINT:
1177: 		::duckdb::TemplatedSearchInMap<int16_t>(list, key.value_.smallint, offsets, key.is_null, entry.offset,
1178: 		                                        entry.length);
1179: 		break;
1180: 	case LogicalTypeId::UINTEGER:
1181: 		::duckdb::TemplatedSearchInMap<uint32_t>(list, key.value_.uinteger, offsets, key.is_null, entry.offset,
1182: 		                                         entry.length);
1183: 		break;
1184: 	case LogicalTypeId::INTEGER:
1185: 		::duckdb::TemplatedSearchInMap<int32_t>(list, key.value_.integer, offsets, key.is_null, entry.offset,
1186: 		                                        entry.length);
1187: 		break;
1188: 	case LogicalTypeId::UBIGINT:
1189: 		::duckdb::TemplatedSearchInMap<uint64_t>(list, key.value_.ubigint, offsets, key.is_null, entry.offset,
1190: 		                                         entry.length);
1191: 		break;
1192: 	case LogicalTypeId::BIGINT:
1193: 		::duckdb::TemplatedSearchInMap<int64_t>(list, key.value_.bigint, offsets, key.is_null, entry.offset,
1194: 		                                        entry.length);
1195: 		break;
1196: 	case LogicalTypeId::HUGEINT:
1197: 		::duckdb::TemplatedSearchInMap<hugeint_t>(list, key.value_.hugeint, offsets, key.is_null, entry.offset,
1198: 		                                          entry.length);
1199: 		break;
1200: 	case LogicalTypeId::FLOAT:
1201: 		::duckdb::TemplatedSearchInMap<float>(list, key.value_.float_, offsets, key.is_null, entry.offset,
1202: 		                                      entry.length);
1203: 		break;
1204: 	case LogicalTypeId::DOUBLE:
1205: 		::duckdb::TemplatedSearchInMap<double>(list, key.value_.double_, offsets, key.is_null, entry.offset,
1206: 		                                       entry.length);
1207: 		break;
1208: 	case LogicalTypeId::DATE:
1209: 		::duckdb::TemplatedSearchInMap<date_t>(list, key.value_.date, offsets, key.is_null, entry.offset, entry.length);
1210: 		break;
1211: 	case LogicalTypeId::TIME:
1212: 		::duckdb::TemplatedSearchInMap<dtime_t>(list, key.value_.time, offsets, key.is_null, entry.offset,
1213: 		                                        entry.length);
1214: 		break;
1215: 	case LogicalTypeId::TIMESTAMP:
1216: 		::duckdb::TemplatedSearchInMap<timestamp_t>(list, key.value_.timestamp, offsets, key.is_null, entry.offset,
1217: 		                                            entry.length);
1218: 		break;
1219: 	case LogicalTypeId::BLOB:
1220: 	case LogicalTypeId::VARCHAR:
1221: 		::duckdb::SearchString(list, key.str_value, offsets, key.is_null, entry.offset, entry.length);
1222: 		break;
1223: 	default:
1224: 		throw InvalidTypeException(list.GetType().id(), "Invalid type for List Vector Search");
1225: 	}
1226: 	return offsets;
1227: }
1228: 
1229: Value ListVector::GetValuesFromOffsets(Vector &list, vector<idx_t> &offsets) {
1230: 	Value ret(list.GetType().child_types()[0].second);
1231: 	ret.is_null = false;
1232: 	auto &child_vec = ListVector::GetEntry(list);
1233: 	for (auto &offset : offsets) {
1234: 		ret.list_value.push_back(child_vec.GetValue(offset));
1235: 	}
1236: 	return ret;
1237: }
1238: 
1239: idx_t ListVector::GetListSize(const Vector &vec) {
1240: 	if (vec.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
1241: 		auto &child = DictionaryVector::Child(vec);
1242: 		return ListVector::GetListSize(child);
1243: 	}
1244: 	return ((VectorListBuffer &)*vec.auxiliary).size;
1245: }
1246: 
1247: void ListVector::ReferenceEntry(Vector &vector, Vector &other) {
1248: 	D_ASSERT(vector.GetType().id() == LogicalTypeId::LIST);
1249: 	D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR ||
1250: 	         vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
1251: 	D_ASSERT(other.GetType().id() == LogicalTypeId::LIST);
1252: 	D_ASSERT(other.GetVectorType() == VectorType::FLAT_VECTOR || other.GetVectorType() == VectorType::CONSTANT_VECTOR);
1253: 	vector.auxiliary = other.auxiliary;
1254: }
1255: 
1256: void ListVector::SetListSize(Vector &vec, idx_t size) {
1257: 	ListVector::Initialize(vec);
1258: 	if (vec.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
1259: 		auto &child = DictionaryVector::Child(vec);
1260: 		ListVector::SetListSize(child, size);
1261: 	}
1262: 	((VectorListBuffer &)*vec.auxiliary).size = size;
1263: }
1264: 
1265: void ListVector::SetEntry(Vector &vector, unique_ptr<Vector> cc) {
1266: 	D_ASSERT(vector.GetType().id() == LogicalTypeId::LIST);
1267: 	D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR ||
1268: 	         vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
1269: 	vector.auxiliary = make_buffer<VectorListBuffer>();
1270: 	D_ASSERT(vector.auxiliary);
1271: 	D_ASSERT(vector.auxiliary->GetBufferType() == VectorBufferType::LIST_BUFFER);
1272: 	((VectorListBuffer *)vector.auxiliary.get())->SetChild(move(cc));
1273: }
1274: 
1275: void ListVector::Append(Vector &target, const Vector &source, idx_t source_size, idx_t source_offset) {
1276: 	ListVector::Initialize(target);
1277: 	if (source_size - source_offset == 0) {
1278: 		//! Nothing to add
1279: 		return;
1280: 	}
1281: 	auto &target_buffer = (VectorListBuffer &)*target.auxiliary;
1282: 	target_buffer.Append(source, source_size, source_offset);
1283: }
1284: 
1285: void ListVector::Append(Vector &target, const Vector &source, const SelectionVector &sel, idx_t source_size,
1286:                         idx_t source_offset) {
1287: 	ListVector::Initialize(target);
1288: 	if (source_size - source_offset == 0) {
1289: 		//! Nothing to add
1290: 		return;
1291: 	}
1292: 	auto &target_buffer = (VectorListBuffer &)*target.auxiliary;
1293: 	target_buffer.Append(source, sel, source_size, source_offset);
1294: }
1295: 
1296: void ListVector::PushBack(Vector &target, Value &insert) {
1297: 	ListVector::Initialize(target);
1298: 	auto &target_buffer = (VectorListBuffer &)*target.auxiliary;
1299: 	target_buffer.PushBack(insert);
1300: }
1301: 
1302: } // namespace duckdb
[end of src/common/types/vector.cpp]
[start of src/common/value_operations/comparison_operations.cpp]
1: #include "duckdb/common/exception.hpp"
2: #include "duckdb/common/operator/comparison_operators.hpp"
3: #include "duckdb/common/value_operations/value_operations.hpp"
4: #include "duckdb/planner/expression/bound_comparison_expression.hpp"
5: 
6: namespace duckdb {
7: 
8: //===--------------------------------------------------------------------===//
9: // Comparison Operations
10: //===--------------------------------------------------------------------===//
11: template <class OP>
12: static bool TemplatedBooleanOperation(const Value &left, const Value &right) {
13: 	const auto &left_type = left.type();
14: 	const auto &right_type = right.type();
15: 	if (left_type != right_type) {
16: 		try {
17: 			LogicalType comparison_type = BoundComparisonExpression::BindComparison(left_type, right_type);
18: 			return TemplatedBooleanOperation<OP>(left.CastAs(comparison_type), right.CastAs(comparison_type));
19: 		} catch (...) {
20: 			return false;
21: 		}
22: 	}
23: 	switch (left_type.InternalType()) {
24: 	case PhysicalType::BOOL:
25: 		return OP::Operation(left.value_.boolean, right.value_.boolean);
26: 	case PhysicalType::INT8:
27: 		return OP::Operation(left.value_.tinyint, right.value_.tinyint);
28: 	case PhysicalType::INT16:
29: 		return OP::Operation(left.value_.smallint, right.value_.smallint);
30: 	case PhysicalType::INT32:
31: 		return OP::Operation(left.value_.integer, right.value_.integer);
32: 	case PhysicalType::INT64:
33: 		return OP::Operation(left.value_.bigint, right.value_.bigint);
34: 	case PhysicalType::UINT8:
35: 		return OP::Operation(left.value_.utinyint, right.value_.utinyint);
36: 	case PhysicalType::UINT16:
37: 		return OP::Operation(left.value_.usmallint, right.value_.usmallint);
38: 	case PhysicalType::UINT32:
39: 		return OP::Operation(left.value_.uinteger, right.value_.uinteger);
40: 	case PhysicalType::UINT64:
41: 		return OP::Operation(left.value_.ubigint, right.value_.ubigint);
42: 	case PhysicalType::INT128:
43: 		return OP::Operation(left.value_.hugeint, right.value_.hugeint);
44: 	case PhysicalType::POINTER:
45: 		return OP::Operation(left.value_.pointer, right.value_.pointer);
46: 	case PhysicalType::HASH:
47: 		return OP::Operation(left.value_.hash, right.value_.hash);
48: 	case PhysicalType::FLOAT:
49: 		return OP::Operation(left.value_.float_, right.value_.float_);
50: 	case PhysicalType::DOUBLE:
51: 		return OP::Operation(left.value_.double_, right.value_.double_);
52: 	case PhysicalType::INTERVAL:
53: 		return OP::Operation(left.value_.interval, right.value_.interval);
54: 	case PhysicalType::VARCHAR:
55: 		return OP::Operation(left.str_value, right.str_value);
56: 	case PhysicalType::MAP:
57: 	case PhysicalType::STRUCT: {
58: 		// this should be enforced by the type
59: 		D_ASSERT(left.struct_value.size() == right.struct_value.size());
60: 		for (idx_t i = 0; i < left.struct_value.size(); i++) {
61: 			if (left.struct_value[i] != right.struct_value[i]) {
62: 				return false;
63: 			}
64: 		}
65: 		return true;
66: 	}
67: 	case PhysicalType::LIST: {
68: 		return left.list_value == right.list_value;
69: 	}
70: 	default:
71: 		throw InternalException("Unimplemented type for value comparison");
72: 	}
73: }
74: 
75: bool ValueOperations::Equals(const Value &left, const Value &right) {
76: 	if (left.is_null && right.is_null) {
77: 		return true;
78: 	}
79: 	if (left.is_null != right.is_null) {
80: 		return false;
81: 	}
82: 	return TemplatedBooleanOperation<duckdb::Equals>(left, right);
83: }
84: 
85: bool ValueOperations::NotEquals(const Value &left, const Value &right) {
86: 	return !ValueOperations::Equals(left, right);
87: }
88: 
89: bool ValueOperations::GreaterThan(const Value &left, const Value &right) {
90: 	if (left.is_null && right.is_null) {
91: 		return false;
92: 	} else if (right.is_null) {
93: 		return true;
94: 	} else if (left.is_null) {
95: 		return false;
96: 	}
97: 	return TemplatedBooleanOperation<duckdb::GreaterThan>(left, right);
98: }
99: 
100: bool ValueOperations::GreaterThanEquals(const Value &left, const Value &right) {
101: 	if (left.is_null && right.is_null) {
102: 		return true;
103: 	} else if (right.is_null) {
104: 		return true;
105: 	} else if (left.is_null) {
106: 		return false;
107: 	}
108: 	return TemplatedBooleanOperation<duckdb::GreaterThanEquals>(left, right);
109: }
110: 
111: bool ValueOperations::LessThan(const Value &left, const Value &right) {
112: 	return ValueOperations::GreaterThan(right, left);
113: }
114: 
115: bool ValueOperations::LessThanEquals(const Value &left, const Value &right) {
116: 	return ValueOperations::GreaterThanEquals(right, left);
117: }
118: 
119: } // namespace duckdb
[end of src/common/value_operations/comparison_operations.cpp]
[start of src/common/value_operations/hash.cpp]
1: #include "duckdb/common/types/hash.hpp"
2: 
3: #include "duckdb/common/exception.hpp"
4: #include "duckdb/common/value_operations/value_operations.hpp"
5: 
6: namespace duckdb {
7: 
8: hash_t ValueOperations::Hash(const Value &op) {
9: 	if (op.is_null) {
10: 		return 0;
11: 	}
12: 	switch (op.type().InternalType()) {
13: 	case PhysicalType::BOOL:
14: 		return duckdb::Hash(op.value_.boolean);
15: 	case PhysicalType::INT8:
16: 		return duckdb::Hash(op.value_.tinyint);
17: 	case PhysicalType::INT16:
18: 		return duckdb::Hash(op.value_.smallint);
19: 	case PhysicalType::INT32:
20: 		return duckdb::Hash(op.value_.integer);
21: 	case PhysicalType::INT64:
22: 		return duckdb::Hash(op.value_.bigint);
23: 	case PhysicalType::UINT8:
24: 		return duckdb::Hash(op.value_.utinyint);
25: 	case PhysicalType::UINT16:
26: 		return duckdb::Hash(op.value_.usmallint);
27: 	case PhysicalType::UINT32:
28: 		return duckdb::Hash(op.value_.uinteger);
29: 	case PhysicalType::UINT64:
30: 		return duckdb::Hash(op.value_.ubigint);
31: 	case PhysicalType::INT128:
32: 		return duckdb::Hash(op.value_.hugeint);
33: 	case PhysicalType::FLOAT:
34: 		return duckdb::Hash(op.value_.float_);
35: 	case PhysicalType::DOUBLE:
36: 		return duckdb::Hash(op.value_.double_);
37: 	case PhysicalType::POINTER:
38: 		return duckdb::Hash(op.value_.pointer);
39: 	case PhysicalType::INTERVAL:
40: 		return duckdb::Hash(op.value_.interval);
41: 	case PhysicalType::VARCHAR:
42: 		return duckdb::Hash(op.str_value.c_str());
43: 	case PhysicalType::LIST: {
44: 		hash_t hash = 0;
45: 		for (auto &entry : op.list_value) {
46: 			hash ^= ValueOperations::Hash(entry);
47: 		}
48: 		return hash;
49: 	}
50: 	case PhysicalType::MAP:
51: 	case PhysicalType::STRUCT: {
52: 		hash_t hash = 0;
53: 		for (auto &entry : op.struct_value) {
54: 			hash ^= ValueOperations::Hash(entry);
55: 		}
56: 		return hash;
57: 	}
58: 	default:
59: 		throw InternalException("Unimplemented type for value hash");
60: 	}
61: }
62: 
63: } // namespace duckdb
[end of src/common/value_operations/hash.cpp]
[start of src/common/vector_operations/vector_cast.cpp]
1: #include "duckdb/common/operator/cast_operators.hpp"
2: #include "duckdb/common/types/cast_helpers.hpp"
3: #include "duckdb/common/types/chunk_collection.hpp"
4: #include "duckdb/common/types/decimal.hpp"
5: #include "duckdb/common/vector_operations/unary_executor.hpp"
6: #include "duckdb/common/vector_operations/vector_operations.hpp"
7: 
8: namespace duckdb {
9: 
10: template <class SRC, class OP>
11: static void VectorStringCast(Vector &source, Vector &result, idx_t count) {
12: 	D_ASSERT(result.GetType().InternalType() == PhysicalType::VARCHAR);
13: 	UnaryExecutor::Execute<SRC, string_t>(source, result, count,
14: 	                                      [&](SRC input) { return OP::template Operation<SRC>(input, result); });
15: }
16: 
17: static NotImplementedException UnimplementedCast(const LogicalType &source_type, const LogicalType &target_type) {
18: 	return NotImplementedException("Unimplemented type for cast (%s -> %s)", source_type.ToString(),
19: 	                               target_type.ToString());
20: }
21: 
22: // NULL cast only works if all values in source are NULL, otherwise an unimplemented cast exception is thrown
23: static void VectorNullCast(Vector &source, Vector &result, idx_t count) {
24: 	if (VectorOperations::HasNotNull(source, count)) {
25: 		throw UnimplementedCast(source.GetType(), result.GetType());
26: 	}
27: 	if (source.GetVectorType() == VectorType::CONSTANT_VECTOR) {
28: 		result.SetVectorType(VectorType::CONSTANT_VECTOR);
29: 		ConstantVector::SetNull(result, true);
30: 	} else {
31: 		result.SetVectorType(VectorType::FLAT_VECTOR);
32: 		FlatVector::Validity(result).SetAllInvalid(count);
33: 	}
34: }
35: 
36: template <class T>
37: static void ToDecimalCast(Vector &source, Vector &result, idx_t count) {
38: 	auto &result_type = result.GetType();
39: 	switch (result_type.InternalType()) {
40: 	case PhysicalType::INT16:
41: 		UnaryExecutor::Execute<T, int16_t>(source, result, count, [&](T input) {
42: 			return CastToDecimal::Operation<T, int16_t>(input, result_type.width(), result_type.scale());
43: 		});
44: 		break;
45: 	case PhysicalType::INT32:
46: 		UnaryExecutor::Execute<T, int32_t>(source, result, count, [&](T input) {
47: 			return CastToDecimal::Operation<T, int32_t>(input, result_type.width(), result_type.scale());
48: 		});
49: 		break;
50: 	case PhysicalType::INT64:
51: 		UnaryExecutor::Execute<T, int64_t>(source, result, count, [&](T input) {
52: 			return CastToDecimal::Operation<T, int64_t>(input, result_type.width(), result_type.scale());
53: 		});
54: 		break;
55: 	case PhysicalType::INT128:
56: 		UnaryExecutor::Execute<T, hugeint_t>(source, result, count, [&](T input) {
57: 			return CastToDecimal::Operation<T, hugeint_t>(input, result_type.width(), result_type.scale());
58: 		});
59: 		break;
60: 	default:
61: 		throw NotImplementedException("Unimplemented internal type for decimal");
62: 	}
63: }
64: 
65: template <class T>
66: static void FromDecimalCast(Vector &source, Vector &result, idx_t count) {
67: 	auto &source_type = source.GetType();
68: 	switch (source_type.InternalType()) {
69: 	case PhysicalType::INT16:
70: 		UnaryExecutor::Execute<int16_t, T>(source, result, count, [&](int16_t input) {
71: 			return CastFromDecimal::Operation<int16_t, T>(input, source_type.width(), source_type.scale());
72: 		});
73: 		break;
74: 	case PhysicalType::INT32:
75: 		UnaryExecutor::Execute<int32_t, T>(source, result, count, [&](int32_t input) {
76: 			return CastFromDecimal::Operation<int32_t, T>(input, source_type.width(), source_type.scale());
77: 		});
78: 		break;
79: 	case PhysicalType::INT64:
80: 		UnaryExecutor::Execute<int64_t, T>(source, result, count, [&](int64_t input) {
81: 			return CastFromDecimal::Operation<int64_t, T>(input, source_type.width(), source_type.scale());
82: 		});
83: 		break;
84: 	case PhysicalType::INT128:
85: 		UnaryExecutor::Execute<hugeint_t, T>(source, result, count, [&](hugeint_t input) {
86: 			return CastFromDecimal::Operation<hugeint_t, T>(input, source_type.width(), source_type.scale());
87: 		});
88: 		break;
89: 	default:
90: 		throw NotImplementedException("Unimplemented internal type for decimal");
91: 	}
92: }
93: 
94: template <class SOURCE, class DEST, class POWERS_SOURCE, class POWERS_DEST>
95: void TemplatedDecimalScaleUp(Vector &source, Vector &result, idx_t count) {
96: 	D_ASSERT(result.GetType().scale() >= source.GetType().scale());
97: 	idx_t scale_difference = result.GetType().scale() - source.GetType().scale();
98: 	auto multiply_factor = POWERS_DEST::POWERS_OF_TEN[scale_difference];
99: 	idx_t target_width = result.GetType().width() - scale_difference;
100: 	if (source.GetType().width() < target_width) {
101: 		// type will always fit: no need to check limit
102: 		UnaryExecutor::Execute<SOURCE, DEST>(source, result, count, [&](SOURCE input) {
103: 			return Cast::Operation<SOURCE, DEST>(input) * multiply_factor;
104: 		});
105: 	} else {
106: 		// type might not fit: check limit
107: 		auto limit = POWERS_SOURCE::POWERS_OF_TEN[target_width];
108: 		UnaryExecutor::Execute<SOURCE, DEST>(source, result, count, [&](SOURCE input) {
109: 			if (input >= limit || input <= -limit) {
110: 				throw OutOfRangeException("Casting value \"%s\" to type %s failed: value is out of range!",
111: 				                          Decimal::ToString(input, source.GetType().scale()),
112: 				                          result.GetType().ToString());
113: 			}
114: 			return Cast::Operation<SOURCE, DEST>(input) * multiply_factor;
115: 		});
116: 	}
117: }
118: 
119: template <class SOURCE, class DEST, class POWERS_SOURCE>
120: void TemplatedDecimalScaleDown(Vector &source, Vector &result, idx_t count) {
121: 	D_ASSERT(result.GetType().scale() < source.GetType().scale());
122: 	idx_t scale_difference = source.GetType().scale() - result.GetType().scale();
123: 	idx_t target_width = result.GetType().width() + scale_difference;
124: 	auto divide_factor = POWERS_SOURCE::POWERS_OF_TEN[scale_difference];
125: 	if (source.GetType().width() < target_width) {
126: 		// type will always fit: no need to check limit
127: 		UnaryExecutor::Execute<SOURCE, DEST>(
128: 		    source, result, count, [&](SOURCE input) { return Cast::Operation<SOURCE, DEST>(input / divide_factor); });
129: 	} else {
130: 		// type might not fit: check limit
131: 		auto limit = POWERS_SOURCE::POWERS_OF_TEN[target_width];
132: 		UnaryExecutor::Execute<SOURCE, DEST>(source, result, count, [&](SOURCE input) {
133: 			if (input >= limit || input <= -limit) {
134: 				throw OutOfRangeException("Casting value \"%s\" to type %s failed: value is out of range!",
135: 				                          Decimal::ToString(input, source.GetType().scale()),
136: 				                          result.GetType().ToString());
137: 			}
138: 			return Cast::Operation<SOURCE, DEST>(input / divide_factor);
139: 		});
140: 	}
141: }
142: 
143: template <class SOURCE, class POWERS_SOURCE>
144: static void DecimalDecimalCastSwitch(Vector &source, Vector &result, idx_t count) {
145: 	source.GetType().Verify();
146: 	result.GetType().Verify();
147: 
148: 	// we need to either multiply or divide by the difference in scales
149: 	if (result.GetType().scale() >= source.GetType().scale()) {
150: 		// multiply
151: 		switch (result.GetType().InternalType()) {
152: 		case PhysicalType::INT16:
153: 			TemplatedDecimalScaleUp<SOURCE, int16_t, POWERS_SOURCE, NumericHelper>(source, result, count);
154: 			break;
155: 		case PhysicalType::INT32:
156: 			TemplatedDecimalScaleUp<SOURCE, int32_t, POWERS_SOURCE, NumericHelper>(source, result, count);
157: 			break;
158: 		case PhysicalType::INT64:
159: 			TemplatedDecimalScaleUp<SOURCE, int64_t, POWERS_SOURCE, NumericHelper>(source, result, count);
160: 			break;
161: 		case PhysicalType::INT128:
162: 			TemplatedDecimalScaleUp<SOURCE, hugeint_t, POWERS_SOURCE, Hugeint>(source, result, count);
163: 			break;
164: 		default:
165: 			throw NotImplementedException("Unimplemented internal type for decimal");
166: 		}
167: 	} else {
168: 		// divide
169: 		switch (result.GetType().InternalType()) {
170: 		case PhysicalType::INT16:
171: 			TemplatedDecimalScaleDown<SOURCE, int16_t, POWERS_SOURCE>(source, result, count);
172: 			break;
173: 		case PhysicalType::INT32:
174: 			TemplatedDecimalScaleDown<SOURCE, int32_t, POWERS_SOURCE>(source, result, count);
175: 			break;
176: 		case PhysicalType::INT64:
177: 			TemplatedDecimalScaleDown<SOURCE, int64_t, POWERS_SOURCE>(source, result, count);
178: 			break;
179: 		case PhysicalType::INT128:
180: 			TemplatedDecimalScaleDown<SOURCE, hugeint_t, POWERS_SOURCE>(source, result, count);
181: 			break;
182: 		default:
183: 			throw NotImplementedException("Unimplemented internal type for decimal");
184: 		}
185: 	}
186: }
187: 
188: static void DecimalCastSwitch(Vector &source, Vector &result, idx_t count) {
189: 	// now switch on the result type
190: 	switch (result.GetType().id()) {
191: 	case LogicalTypeId::BOOLEAN:
192: 		FromDecimalCast<bool>(source, result, count);
193: 		break;
194: 	case LogicalTypeId::TINYINT:
195: 		FromDecimalCast<int8_t>(source, result, count);
196: 		break;
197: 	case LogicalTypeId::SMALLINT:
198: 		FromDecimalCast<int16_t>(source, result, count);
199: 		break;
200: 	case LogicalTypeId::INTEGER:
201: 		FromDecimalCast<int32_t>(source, result, count);
202: 		break;
203: 	case LogicalTypeId::BIGINT:
204: 		FromDecimalCast<int64_t>(source, result, count);
205: 		break;
206: 	case LogicalTypeId::UTINYINT:
207: 		FromDecimalCast<uint8_t>(source, result, count);
208: 		break;
209: 	case LogicalTypeId::USMALLINT:
210: 		FromDecimalCast<uint16_t>(source, result, count);
211: 		break;
212: 	case LogicalTypeId::UINTEGER:
213: 		FromDecimalCast<uint32_t>(source, result, count);
214: 		break;
215: 	case LogicalTypeId::UBIGINT:
216: 		FromDecimalCast<uint64_t>(source, result, count);
217: 		break;
218: 	case LogicalTypeId::HUGEINT:
219: 		FromDecimalCast<hugeint_t>(source, result, count);
220: 		break;
221: 	case LogicalTypeId::DECIMAL: {
222: 		// decimal to decimal cast
223: 		// first we need to figure out the source and target internal types
224: 		switch (source.GetType().InternalType()) {
225: 		case PhysicalType::INT16:
226: 			DecimalDecimalCastSwitch<int16_t, NumericHelper>(source, result, count);
227: 			break;
228: 		case PhysicalType::INT32:
229: 			DecimalDecimalCastSwitch<int32_t, NumericHelper>(source, result, count);
230: 			break;
231: 		case PhysicalType::INT64:
232: 			DecimalDecimalCastSwitch<int64_t, NumericHelper>(source, result, count);
233: 			break;
234: 		case PhysicalType::INT128:
235: 			DecimalDecimalCastSwitch<hugeint_t, Hugeint>(source, result, count);
236: 			break;
237: 		default:
238: 			throw NotImplementedException("Unimplemented internal type for decimal in decimal_decimal cast");
239: 		}
240: 		break;
241: 	}
242: 	case LogicalTypeId::FLOAT:
243: 		FromDecimalCast<float>(source, result, count);
244: 		break;
245: 	case LogicalTypeId::DOUBLE:
246: 		FromDecimalCast<double>(source, result, count);
247: 		break;
248: 	case LogicalTypeId::VARCHAR: {
249: 		auto &source_type = source.GetType();
250: 		switch (source_type.InternalType()) {
251: 		case PhysicalType::INT16:
252: 			UnaryExecutor::Execute<int16_t, string_t>(source, result, count, [&](int16_t input) {
253: 				return StringCastFromDecimal::Operation<int16_t>(input, source_type.width(), source_type.scale(),
254: 				                                                 result);
255: 			});
256: 			break;
257: 		case PhysicalType::INT32:
258: 			UnaryExecutor::Execute<int32_t, string_t>(source, result, count, [&](int32_t input) {
259: 				return StringCastFromDecimal::Operation<int32_t>(input, source_type.width(), source_type.scale(),
260: 				                                                 result);
261: 			});
262: 			break;
263: 		case PhysicalType::INT64:
264: 			UnaryExecutor::Execute<int64_t, string_t>(source, result, count, [&](int64_t input) {
265: 				return StringCastFromDecimal::Operation<int64_t>(input, source_type.width(), source_type.scale(),
266: 				                                                 result);
267: 			});
268: 			break;
269: 		case PhysicalType::INT128:
270: 			UnaryExecutor::Execute<hugeint_t, string_t>(source, result, count, [&](hugeint_t input) {
271: 				return StringCastFromDecimal::Operation<hugeint_t>(input, source_type.width(), source_type.scale(),
272: 				                                                   result);
273: 			});
274: 			break;
275: 		default:
276: 			throw NotImplementedException("Unimplemented internal decimal type");
277: 		}
278: 		break;
279: 	}
280: 	default:
281: 		VectorNullCast(source, result, count);
282: 		break;
283: 	}
284: }
285: 
286: template <class SRC>
287: static void NumericCastSwitch(Vector &source, Vector &result, idx_t count) {
288: 	// now switch on the result type
289: 	switch (result.GetType().id()) {
290: 	case LogicalTypeId::BOOLEAN:
291: 		UnaryExecutor::Execute<SRC, bool, duckdb::Cast>(source, result, count);
292: 		break;
293: 	case LogicalTypeId::TINYINT:
294: 		UnaryExecutor::Execute<SRC, int8_t, duckdb::Cast>(source, result, count);
295: 		break;
296: 	case LogicalTypeId::SMALLINT:
297: 		UnaryExecutor::Execute<SRC, int16_t, duckdb::Cast>(source, result, count);
298: 		break;
299: 	case LogicalTypeId::INTEGER:
300: 		UnaryExecutor::Execute<SRC, int32_t, duckdb::Cast>(source, result, count);
301: 		break;
302: 	case LogicalTypeId::BIGINT:
303: 		UnaryExecutor::Execute<SRC, int64_t, duckdb::Cast>(source, result, count);
304: 		break;
305: 	case LogicalTypeId::UTINYINT:
306: 		UnaryExecutor::Execute<SRC, uint8_t, duckdb::Cast>(source, result, count);
307: 		break;
308: 	case LogicalTypeId::USMALLINT:
309: 		UnaryExecutor::Execute<SRC, uint16_t, duckdb::Cast>(source, result, count);
310: 		break;
311: 	case LogicalTypeId::UINTEGER:
312: 		UnaryExecutor::Execute<SRC, uint32_t, duckdb::Cast>(source, result, count);
313: 		break;
314: 	case LogicalTypeId::UBIGINT:
315: 		UnaryExecutor::Execute<SRC, uint64_t, duckdb::Cast>(source, result, count);
316: 		break;
317: 	case LogicalTypeId::HUGEINT:
318: 		UnaryExecutor::Execute<SRC, hugeint_t, duckdb::Cast>(source, result, count);
319: 		break;
320: 	case LogicalTypeId::FLOAT:
321: 		UnaryExecutor::Execute<SRC, float, duckdb::Cast>(source, result, count);
322: 		break;
323: 	case LogicalTypeId::DOUBLE:
324: 		UnaryExecutor::Execute<SRC, double, duckdb::Cast>(source, result, count);
325: 		break;
326: 	case LogicalTypeId::DECIMAL:
327: 		ToDecimalCast<SRC>(source, result, count);
328: 		break;
329: 	case LogicalTypeId::VARCHAR: {
330: 		VectorStringCast<SRC, duckdb::StringCast>(source, result, count);
331: 		break;
332: 	}
333: 	case LogicalTypeId::LIST: {
334: 		auto list_child = make_unique<Vector>();
335: 		ListVector::SetEntry(result, move(list_child));
336: 		VectorNullCast(source, result, count);
337: 		break;
338: 	}
339: 	default:
340: 		VectorNullCast(source, result, count);
341: 		break;
342: 	}
343: }
344: 
345: template <class OP>
346: static void VectorStringCastNumericSwitch(Vector &source, Vector &result, idx_t count) {
347: 	// now switch on the result type
348: 	switch (result.GetType().id()) {
349: 	case LogicalTypeId::BOOLEAN:
350: 		UnaryExecutor::Execute<string_t, bool, OP>(source, result, count);
351: 		break;
352: 	case LogicalTypeId::TINYINT:
353: 		UnaryExecutor::Execute<string_t, int8_t, OP>(source, result, count);
354: 		break;
355: 	case LogicalTypeId::SMALLINT:
356: 		UnaryExecutor::Execute<string_t, int16_t, OP>(source, result, count);
357: 		break;
358: 	case LogicalTypeId::INTEGER:
359: 		UnaryExecutor::Execute<string_t, int32_t, OP>(source, result, count);
360: 		break;
361: 	case LogicalTypeId::BIGINT:
362: 		UnaryExecutor::Execute<string_t, int64_t, OP>(source, result, count);
363: 		break;
364: 	case LogicalTypeId::UTINYINT:
365: 		UnaryExecutor::Execute<string_t, uint8_t, OP>(source, result, count);
366: 		break;
367: 	case LogicalTypeId::USMALLINT:
368: 		UnaryExecutor::Execute<string_t, uint16_t, OP>(source, result, count);
369: 		break;
370: 	case LogicalTypeId::UINTEGER:
371: 		UnaryExecutor::Execute<string_t, uint32_t, OP>(source, result, count);
372: 		break;
373: 	case LogicalTypeId::UBIGINT:
374: 		UnaryExecutor::Execute<string_t, uint64_t, OP>(source, result, count);
375: 		break;
376: 	case LogicalTypeId::HUGEINT:
377: 		UnaryExecutor::Execute<string_t, hugeint_t, OP>(source, result, count);
378: 		break;
379: 	case LogicalTypeId::FLOAT:
380: 		UnaryExecutor::Execute<string_t, float, OP>(source, result, count);
381: 		break;
382: 	case LogicalTypeId::DOUBLE:
383: 		UnaryExecutor::Execute<string_t, double, OP>(source, result, count);
384: 		break;
385: 	case LogicalTypeId::INTERVAL:
386: 		UnaryExecutor::Execute<string_t, interval_t, OP>(source, result, count);
387: 		break;
388: 	case LogicalTypeId::DECIMAL:
389: 		ToDecimalCast<string_t>(source, result, count);
390: 		break;
391: 	default:
392: 		VectorNullCast(source, result, count);
393: 		break;
394: 	}
395: }
396: 
397: static void StringCastSwitch(Vector &source, Vector &result, idx_t count, bool strict = false) {
398: 	// now switch on the result type
399: 	switch (result.GetType().id()) {
400: 	case LogicalTypeId::DATE:
401: 		if (strict) {
402: 			UnaryExecutor::Execute<string_t, date_t, duckdb::StrictCastToDate>(source, result, count);
403: 		} else {
404: 			UnaryExecutor::Execute<string_t, date_t, duckdb::CastToDate>(source, result, count);
405: 		}
406: 		break;
407: 	case LogicalTypeId::TIME:
408: 		if (strict) {
409: 			UnaryExecutor::Execute<string_t, dtime_t, duckdb::StrictCastToTime>(source, result, count);
410: 		} else {
411: 			UnaryExecutor::Execute<string_t, dtime_t, duckdb::CastToTime>(source, result, count);
412: 		}
413: 		break;
414: 	case LogicalTypeId::TIMESTAMP:
415: 		UnaryExecutor::Execute<string_t, timestamp_t, duckdb::CastToTimestamp>(source, result, count);
416: 		break;
417: 	case LogicalTypeId::TIMESTAMP_NS:
418: 		UnaryExecutor::Execute<string_t, timestamp_t, duckdb::CastToTimestampNS>(source, result, count);
419: 		break;
420: 	case LogicalTypeId::TIMESTAMP_SEC:
421: 		UnaryExecutor::Execute<string_t, timestamp_t, duckdb::CastToTimestampSec>(source, result, count);
422: 		break;
423: 	case LogicalTypeId::TIMESTAMP_MS:
424: 		UnaryExecutor::Execute<string_t, timestamp_t, duckdb::CastToTimestampMS>(source, result, count);
425: 		break;
426: 	case LogicalTypeId::BLOB:
427: 		VectorStringCast<string_t, duckdb::CastToBlob>(source, result, count);
428: 		break;
429: 	default:
430: 		if (strict) {
431: 			VectorStringCastNumericSwitch<duckdb::StrictCast>(source, result, count);
432: 		} else {
433: 			VectorStringCastNumericSwitch<duckdb::Cast>(source, result, count);
434: 		}
435: 		break;
436: 	}
437: }
438: 
439: static void DateCastSwitch(Vector &source, Vector &result, idx_t count) {
440: 	// now switch on the result type
441: 	switch (result.GetType().id()) {
442: 	case LogicalTypeId::VARCHAR:
443: 		// date to varchar
444: 		VectorStringCast<date_t, duckdb::CastFromDate>(source, result, count);
445: 		break;
446: 	case LogicalTypeId::TIMESTAMP:
447: 		// date to timestamp
448: 		UnaryExecutor::Execute<date_t, timestamp_t, duckdb::CastDateToTimestamp>(source, result, count);
449: 		break;
450: 	default:
451: 		VectorNullCast(source, result, count);
452: 		break;
453: 	}
454: }
455: 
456: static void TimeCastSwitch(Vector &source, Vector &result, idx_t count) {
457: 	// now switch on the result type
458: 	switch (result.GetType().id()) {
459: 	case LogicalTypeId::VARCHAR:
460: 		// time to varchar
461: 		VectorStringCast<dtime_t, duckdb::CastFromTime>(source, result, count);
462: 		break;
463: 	default:
464: 		VectorNullCast(source, result, count);
465: 		break;
466: 	}
467: }
468: 
469: static void TimestampCastSwitch(Vector &source, Vector &result, idx_t count) {
470: 	// now switch on the result type
471: 	switch (result.GetType().id()) {
472: 	case LogicalTypeId::VARCHAR:
473: 		// timestamp to varchar
474: 		VectorStringCast<timestamp_t, duckdb::CastFromTimestamp>(source, result, count);
475: 		break;
476: 	case LogicalTypeId::DATE:
477: 		// timestamp to date
478: 		UnaryExecutor::Execute<timestamp_t, date_t, duckdb::CastTimestampToDate>(source, result, count);
479: 		break;
480: 	case LogicalTypeId::TIME:
481: 		// timestamp to time
482: 		UnaryExecutor::Execute<timestamp_t, dtime_t, duckdb::CastTimestampToTime>(source, result, count);
483: 		break;
484: 	case LogicalTypeId::TIMESTAMP_NS:
485: 		// timestamp (us) to timestamp (ns)
486: 		UnaryExecutor::Execute<timestamp_t, timestamp_t, duckdb::CastTimestampUsToNs>(source, result, count);
487: 		break;
488: 	case LogicalTypeId::TIMESTAMP_MS:
489: 		// timestamp (us) to timestamp (ms)
490: 		UnaryExecutor::Execute<timestamp_t, timestamp_t, duckdb::CastTimestampUsToMs>(source, result, count);
491: 		break;
492: 	case LogicalTypeId::TIMESTAMP_SEC:
493: 		// timestamp (us) to timestamp (s)
494: 		UnaryExecutor::Execute<timestamp_t, timestamp_t, duckdb::CastTimestampUsToSec>(source, result, count);
495: 		break;
496: 	default:
497: 		VectorNullCast(source, result, count);
498: 		break;
499: 	}
500: }
501: 
502: static void TimestampNsCastSwitch(Vector &source, Vector &result, idx_t count) {
503: 	// now switch on the result type
504: 	switch (result.GetType().id()) {
505: 	case LogicalTypeId::TIMESTAMP:
506: 		// timestamp (ns) to timestamp (us)
507: 		UnaryExecutor::Execute<timestamp_t, timestamp_t, duckdb::CastTimestampNsToUs>(source, result, count);
508: 		break;
509: 	default:
510: 		VectorNullCast(source, result, count);
511: 		break;
512: 	}
513: }
514: 
515: static void TimestampMsCastSwitch(Vector &source, Vector &result, idx_t count) {
516: 	// now switch on the result type
517: 	switch (result.GetType().id()) {
518: 	case LogicalTypeId::TIMESTAMP:
519: 		// timestamp (ms) to timestamp (us)
520: 		UnaryExecutor::Execute<timestamp_t, timestamp_t, duckdb::CastTimestampMsToUs>(source, result, count);
521: 		break;
522: 	default:
523: 		VectorNullCast(source, result, count);
524: 		break;
525: 	}
526: }
527: 
528: static void TimestampSecCastSwitch(Vector &source, Vector &result, idx_t count) {
529: 	// now switch on the result type
530: 	switch (result.GetType().id()) {
531: 	case LogicalTypeId::TIMESTAMP:
532: 		// timestamp (s) to timestamp (us)
533: 		UnaryExecutor::Execute<timestamp_t, timestamp_t, duckdb::CastTimestampSecToUs>(source, result, count);
534: 		break;
535: 	default:
536: 		VectorNullCast(source, result, count);
537: 		break;
538: 	}
539: }
540: 
541: static void IntervalCastSwitch(Vector &source, Vector &result, idx_t count) {
542: 	// now switch on the result type
543: 	switch (result.GetType().id()) {
544: 	case LogicalTypeId::VARCHAR:
545: 		// time to varchar
546: 		VectorStringCast<interval_t, duckdb::StringCast>(source, result, count);
547: 		break;
548: 	default:
549: 		VectorNullCast(source, result, count);
550: 		break;
551: 	}
552: }
553: 
554: static void BlobCastSwitch(Vector &source, Vector &result, idx_t count) {
555: 	// now switch on the result type
556: 	switch (result.GetType().id()) {
557: 	case LogicalTypeId::VARCHAR:
558: 		// blob to varchar
559: 		VectorStringCast<string_t, duckdb::CastFromBlob>(source, result, count);
560: 		break;
561: 	default:
562: 		VectorNullCast(source, result, count);
563: 		break;
564: 	}
565: }
566: 
567: static void ValueStringCastSwitch(Vector &source, Vector &result, idx_t count) {
568: 	switch (result.GetType().id()) {
569: 	case LogicalTypeId::VARCHAR:
570: 		if (source.GetVectorType() == VectorType::CONSTANT_VECTOR) {
571: 			result.SetVectorType(source.GetVectorType());
572: 		} else {
573: 			result.SetVectorType(VectorType::FLAT_VECTOR);
574: 		}
575: 		for (idx_t i = 0; i < count; i++) {
576: 			auto src_val = source.GetValue(i);
577: 			auto str_val = src_val.ToString();
578: 			result.SetValue(i, Value(str_val));
579: 		}
580: 		break;
581: 	default:
582: 		VectorNullCast(source, result, count);
583: 		break;
584: 	}
585: }
586: 
587: static void ListCastSwitch(Vector &source, Vector &result, idx_t count) {
588: 	switch (result.GetType().id()) {
589: 	case LogicalTypeId::LIST: {
590: 		// only handle constant and flat vectors here for now
591: 		if (source.GetVectorType() == VectorType::CONSTANT_VECTOR) {
592: 			result.SetVectorType(source.GetVectorType());
593: 			ConstantVector::SetNull(result, ConstantVector::IsNull(source));
594: 		} else {
595: 			source.Normalify(count);
596: 			result.SetVectorType(VectorType::FLAT_VECTOR);
597: 			FlatVector::SetValidity(result, FlatVector::Validity(source));
598: 		}
599: 		auto list_child = make_unique<Vector>(result.GetType().child_types()[0].second);
600: 		ListVector::SetEntry(result, move(list_child));
601: 		if (ListVector::HasEntry(source)) {
602: 			auto &source_cc = ListVector::GetEntry(source);
603: 			auto source_size = ListVector::GetListSize(source);
604: 			Vector append_vector(result.GetType().child_types()[0].second);
605: 			if (source_size > STANDARD_VECTOR_SIZE) {
606: 				append_vector.Resize(STANDARD_VECTOR_SIZE, source_size);
607: 			}
608: 			if (source_cc.GetData()) {
609: 				VectorOperations::Cast(source_cc, append_vector, source_size);
610: 				ListVector::Append(result, append_vector, source_size);
611: 			}
612: 		}
613: 
614: 		auto ldata = FlatVector::GetData<list_entry_t>(source);
615: 		auto tdata = FlatVector::GetData<list_entry_t>(result);
616: 		for (idx_t i = 0; i < count; i++) {
617: 			tdata[i] = ldata[i];
618: 		}
619: 		break;
620: 	}
621: 	default:
622: 		ValueStringCastSwitch(source, result, count);
623: 		break;
624: 	}
625: }
626: 
627: static void StructCastSwitch(Vector &source, Vector &result, idx_t count) {
628: 	switch (result.GetType().id()) {
629: 	case LogicalTypeId::STRUCT:
630: 	case LogicalTypeId::MAP: {
631: 		if (source.GetType().child_types().size() != result.GetType().child_types().size()) {
632: 			throw TypeMismatchException(source.GetType(), result.GetType(), "Cannot cast STRUCTs of different size");
633: 		}
634: 		auto &source_children = StructVector::GetEntries(source);
635: 		D_ASSERT(source_children.size() == source.GetType().child_types().size());
636: 
637: 		bool is_constant = true;
638: 		auto &result_children = StructVector::GetEntries(result);
639: 		for (idx_t c_idx = 0; c_idx < result.GetType().child_types().size(); c_idx++) {
640: 			auto &result_child_vector = result_children[c_idx];
641: 			auto &source_child_vector = *source_children[c_idx];
642: 			if (source_child_vector.GetVectorType() != VectorType::CONSTANT_VECTOR) {
643: 				is_constant = false;
644: 			}
645: 			if (result_child_vector->GetType() != source_child_vector.GetType()) {
646: 				VectorOperations::Cast(source_child_vector, *result_child_vector, count, false);
647: 			} else {
648: 				result_child_vector->Reference(source_child_vector);
649: 			}
650: 		}
651: 		if (is_constant) {
652: 			result.SetVectorType(VectorType::CONSTANT_VECTOR);
653: 		}
654: 
655: 		break;
656: 	}
657: 	case LogicalTypeId::VARCHAR:
658: 		if (source.GetVectorType() == VectorType::CONSTANT_VECTOR) {
659: 			result.SetVectorType(source.GetVectorType());
660: 		} else {
661: 			result.SetVectorType(VectorType::FLAT_VECTOR);
662: 		}
663: 		for (idx_t i = 0; i < count; i++) {
664: 			auto src_val = source.GetValue(i);
665: 			auto str_val = src_val.ToString();
666: 			result.SetValue(i, Value(str_val));
667: 		}
668: 		break;
669: 
670: 	default:
671: 		VectorNullCast(source, result, count);
672: 		break;
673: 	}
674: }
675: 
676: void VectorOperations::Cast(Vector &source, Vector &result, idx_t count, bool strict) {
677: 	D_ASSERT(source.GetType() != result.GetType());
678: 	// first switch on source type
679: 	switch (source.GetType().id()) {
680: 	case LogicalTypeId::BOOLEAN:
681: 		NumericCastSwitch<bool>(source, result, count);
682: 		break;
683: 	case LogicalTypeId::TINYINT:
684: 		NumericCastSwitch<int8_t>(source, result, count);
685: 		break;
686: 	case LogicalTypeId::SMALLINT:
687: 		NumericCastSwitch<int16_t>(source, result, count);
688: 		break;
689: 	case LogicalTypeId::INTEGER:
690: 		NumericCastSwitch<int32_t>(source, result, count);
691: 		break;
692: 	case LogicalTypeId::BIGINT:
693: 		NumericCastSwitch<int64_t>(source, result, count);
694: 		break;
695: 	case LogicalTypeId::UTINYINT:
696: 		NumericCastSwitch<uint8_t>(source, result, count);
697: 		break;
698: 	case LogicalTypeId::USMALLINT:
699: 		NumericCastSwitch<uint16_t>(source, result, count);
700: 		break;
701: 	case LogicalTypeId::UINTEGER:
702: 		NumericCastSwitch<uint32_t>(source, result, count);
703: 		break;
704: 	case LogicalTypeId::UBIGINT:
705: 		NumericCastSwitch<uint64_t>(source, result, count);
706: 		break;
707: 	case LogicalTypeId::HUGEINT:
708: 		NumericCastSwitch<hugeint_t>(source, result, count);
709: 		break;
710: 	case LogicalTypeId::DECIMAL:
711: 		DecimalCastSwitch(source, result, count);
712: 		break;
713: 	case LogicalTypeId::FLOAT:
714: 		NumericCastSwitch<float>(source, result, count);
715: 		break;
716: 	case LogicalTypeId::DOUBLE:
717: 		NumericCastSwitch<double>(source, result, count);
718: 		break;
719: 	case LogicalTypeId::DATE:
720: 		DateCastSwitch(source, result, count);
721: 		break;
722: 	case LogicalTypeId::TIME:
723: 		TimeCastSwitch(source, result, count);
724: 		break;
725: 	case LogicalTypeId::TIMESTAMP:
726: 		TimestampCastSwitch(source, result, count);
727: 		break;
728: 	case LogicalTypeId::TIMESTAMP_NS:
729: 		TimestampNsCastSwitch(source, result, count);
730: 		break;
731: 	case LogicalTypeId::TIMESTAMP_MS:
732: 		TimestampMsCastSwitch(source, result, count);
733: 		break;
734: 	case LogicalTypeId::TIMESTAMP_SEC:
735: 		TimestampSecCastSwitch(source, result, count);
736: 		break;
737: 	case LogicalTypeId::INTERVAL:
738: 		IntervalCastSwitch(source, result, count);
739: 		break;
740: 	case LogicalTypeId::VARCHAR:
741: 		StringCastSwitch(source, result, count, strict);
742: 		break;
743: 	case LogicalTypeId::BLOB:
744: 		BlobCastSwitch(source, result, count);
745: 		break;
746: 	case LogicalTypeId::SQLNULL: {
747: 		// cast a NULL to another type, just copy the properties and change the type
748: 		result.SetVectorType(VectorType::CONSTANT_VECTOR);
749: 		ConstantVector::SetNull(result, true);
750: 		break;
751: 	}
752: 	case LogicalTypeId::MAP:
753: 	case LogicalTypeId::STRUCT:
754: 		StructCastSwitch(source, result, count);
755: 		break;
756: 	case LogicalTypeId::LIST:
757: 		ListCastSwitch(source, result, count);
758: 		break;
759: 	default:
760: 		throw UnimplementedCast(source.GetType(), result.GetType());
761: 	}
762: }
763: 
764: } // namespace duckdb
[end of src/common/vector_operations/vector_cast.cpp]
[start of src/common/vector_operations/vector_copy.cpp]
1: //===--------------------------------------------------------------------===//
2: // copy.cpp
3: // Description: This file contains the implementation of the different copy
4: // functions
5: //===--------------------------------------------------------------------===//
6: 
7: #include "duckdb/common/exception.hpp"
8: #include "duckdb/common/types/null_value.hpp"
9: #include "duckdb/common/types/chunk_collection.hpp"
10: 
11: #include "duckdb/common/vector_operations/vector_operations.hpp"
12: 
13: namespace duckdb {
14: 
15: template <class T>
16: static void TemplatedCopy(const Vector &source, const SelectionVector &sel, Vector &target, idx_t source_offset,
17:                           idx_t target_offset, idx_t copy_count) {
18: 	auto ldata = FlatVector::GetData<T>(source);
19: 	auto tdata = FlatVector::GetData<T>(target);
20: 	for (idx_t i = 0; i < copy_count; i++) {
21: 		auto source_idx = sel.get_index(source_offset + i);
22: 		tdata[target_offset + i] = ldata[source_idx];
23: 	}
24: }
25: 
26: void VectorOperations::Copy(const Vector &source, Vector &target, const SelectionVector &sel_p, idx_t source_count,
27:                             idx_t source_offset, idx_t target_offset) {
28: 	D_ASSERT(source_offset <= source_count);
29: 	D_ASSERT(target.GetVectorType() == VectorType::FLAT_VECTOR);
30: 	D_ASSERT(source.GetType() == target.GetType());
31: 	const SelectionVector *sel = &sel_p;
32: 	switch (source.GetVectorType()) {
33: 	case VectorType::DICTIONARY_VECTOR: {
34: 		// dictionary vector: merge selection vectors
35: 		auto &child = DictionaryVector::Child(source);
36: 		auto &dict_sel = DictionaryVector::SelVector(source);
37: 		// merge the selection vectors and verify the child
38: 		auto new_buffer = dict_sel.Slice(*sel, source_count);
39: 		SelectionVector merged_sel(new_buffer);
40: 		VectorOperations::Copy(child, target, merged_sel, source_count, source_offset, target_offset);
41: 		return;
42: 	}
43: 	case VectorType::SEQUENCE_VECTOR: {
44: 		int64_t start, increment;
45: 		Vector seq(source.GetType());
46: 		SequenceVector::GetSequence(source, start, increment);
47: 		VectorOperations::GenerateSequence(seq, source_count, *sel, start, increment);
48: 		VectorOperations::Copy(seq, target, *sel, source_count, source_offset, target_offset);
49: 		return;
50: 	}
51: 	case VectorType::CONSTANT_VECTOR:
52: 		sel = &ConstantVector::ZERO_SELECTION_VECTOR;
53: 		break; // carry on with below code
54: 	case VectorType::FLAT_VECTOR:
55: 		break;
56: 	default:
57: 		throw NotImplementedException("FIXME unimplemented vector type for VectorOperations::Copy");
58: 	}
59: 
60: 	idx_t copy_count = source_count - source_offset;
61: 	if (copy_count == 0) {
62: 		return;
63: 	}
64: 
65: 	// first copy the nullmask
66: 	auto &tmask = FlatVector::Validity(target);
67: 	if (source.GetVectorType() == VectorType::CONSTANT_VECTOR) {
68: 		if (ConstantVector::IsNull(source)) {
69: 			for (idx_t i = 0; i < copy_count; i++) {
70: 				tmask.SetInvalid(target_offset + i);
71: 			}
72: 		}
73: 	} else {
74: 		auto &smask = FlatVector::Validity(source);
75: 		if (smask.IsMaskSet()) {
76: 			for (idx_t i = 0; i < copy_count; i++) {
77: 				auto idx = sel->get_index(source_offset + i);
78: 				tmask.Set(target_offset + i, smask.RowIsValid(idx));
79: 			}
80: 		}
81: 	}
82: 
83: 	D_ASSERT(sel);
84: 
85: 	// now copy over the data
86: 	switch (source.GetType().InternalType()) {
87: 	case PhysicalType::BOOL:
88: 	case PhysicalType::INT8:
89: 		TemplatedCopy<int8_t>(source, *sel, target, source_offset, target_offset, copy_count);
90: 		break;
91: 	case PhysicalType::INT16:
92: 		TemplatedCopy<int16_t>(source, *sel, target, source_offset, target_offset, copy_count);
93: 		break;
94: 	case PhysicalType::INT32:
95: 		TemplatedCopy<int32_t>(source, *sel, target, source_offset, target_offset, copy_count);
96: 		break;
97: 	case PhysicalType::HASH:
98: 	case PhysicalType::INT64:
99: 		TemplatedCopy<int64_t>(source, *sel, target, source_offset, target_offset, copy_count);
100: 		break;
101: 	case PhysicalType::UINT8:
102: 		TemplatedCopy<uint8_t>(source, *sel, target, source_offset, target_offset, copy_count);
103: 		break;
104: 	case PhysicalType::UINT16:
105: 		TemplatedCopy<uint16_t>(source, *sel, target, source_offset, target_offset, copy_count);
106: 		break;
107: 	case PhysicalType::UINT32:
108: 		TemplatedCopy<uint32_t>(source, *sel, target, source_offset, target_offset, copy_count);
109: 		break;
110: 	case PhysicalType::UINT64:
111: 		TemplatedCopy<uint64_t>(source, *sel, target, source_offset, target_offset, copy_count);
112: 		break;
113: 	case PhysicalType::INT128:
114: 		TemplatedCopy<hugeint_t>(source, *sel, target, source_offset, target_offset, copy_count);
115: 		break;
116: 	case PhysicalType::POINTER:
117: 		TemplatedCopy<uintptr_t>(source, *sel, target, source_offset, target_offset, copy_count);
118: 		break;
119: 	case PhysicalType::FLOAT:
120: 		TemplatedCopy<float>(source, *sel, target, source_offset, target_offset, copy_count);
121: 		break;
122: 	case PhysicalType::DOUBLE:
123: 		TemplatedCopy<double>(source, *sel, target, source_offset, target_offset, copy_count);
124: 		break;
125: 	case PhysicalType::INTERVAL:
126: 		TemplatedCopy<interval_t>(source, *sel, target, source_offset, target_offset, copy_count);
127: 		break;
128: 	case PhysicalType::VARCHAR: {
129: 		auto ldata = FlatVector::GetData<string_t>(source);
130: 		auto tdata = FlatVector::GetData<string_t>(target);
131: 		for (idx_t i = 0; i < copy_count; i++) {
132: 			auto source_idx = sel->get_index(source_offset + i);
133: 			auto target_idx = target_offset + i;
134: 			if (tmask.RowIsValid(target_idx)) {
135: 				tdata[target_idx] = StringVector::AddStringOrBlob(target, ldata[source_idx]);
136: 			}
137: 		}
138: 		break;
139: 	}
140: 	case PhysicalType::MAP:
141: 	case PhysicalType::STRUCT: {
142: 		auto &source_children = StructVector::GetEntries(source);
143: 		auto &target_children = StructVector::GetEntries(target);
144: 		D_ASSERT(source_children.size() == target_children.size());
145: 		for (idx_t i = 0; i < source_children.size(); i++) {
146: 			VectorOperations::Copy(*source_children[i], *target_children[i], *sel, source_count, source_offset,
147: 			                       target_offset);
148: 		}
149: 		break;
150: 	}
151: 	case PhysicalType::LIST: {
152: 		D_ASSERT(target.GetType().InternalType() == PhysicalType::LIST);
153: 		if (ListVector::HasEntry(source)) {
154: 			//! if the source has list offsets, we need to append them to the target
155: 			if (!ListVector::HasEntry(target)) {
156: 				auto target_child = make_unique<Vector>(target.GetType().child_types()[0].second);
157: 				ListVector::SetEntry(target, move(target_child));
158: 			}
159: 
160: 			//! build a selection vector for the copied child elements
161: 			auto sdata = FlatVector::GetData<list_entry_t>(source);
162: 			vector<sel_t> child_rows;
163: 			for (idx_t i = 0; i < copy_count; ++i) {
164: 				if (tmask.RowIsValid(target_offset + i)) {
165: 					auto source_idx = sel->get_index(source_offset + i);
166: 					auto &source_entry = sdata[source_idx];
167: 					for (idx_t j = 0; j < source_entry.length; ++j) {
168: 						child_rows.emplace_back(source_entry.offset + j);
169: 					}
170: 				}
171: 			}
172: 			idx_t source_child_size = child_rows.size();
173: 			SelectionVector child_sel(child_rows.data());
174: 
175: 			auto &source_child = ListVector::GetEntry(source);
176: 
177: 			idx_t old_target_child_len = ListVector::GetListSize(target);
178: 
179: 			//! append to list itself
180: 			ListVector::Append(target, source_child, child_sel, source_child_size);
181: 
182: 			//! now write the list offsets
183: 			auto tdata = FlatVector::GetData<list_entry_t>(target);
184: 			for (idx_t i = 0; i < copy_count; i++) {
185: 				auto source_idx = sel->get_index(source_offset + i);
186: 				auto &source_entry = sdata[source_idx];
187: 				auto &target_entry = tdata[target_offset + i];
188: 
189: 				target_entry.length = source_entry.length;
190: 				target_entry.offset = old_target_child_len;
191: 				if (tmask.RowIsValid(target_offset + i)) {
192: 					old_target_child_len += target_entry.length;
193: 				}
194: 			}
195: 		}
196: 		break;
197: 	}
198: 	default:
199: 		throw NotImplementedException("Unimplemented type '%s' for copy!",
200: 		                              TypeIdToString(source.GetType().InternalType()));
201: 	}
202: }
203: 
204: void VectorOperations::Copy(const Vector &source, Vector &target, idx_t source_count, idx_t source_offset,
205:                             idx_t target_offset) {
206: 	switch (source.GetVectorType()) {
207: 	case VectorType::DICTIONARY_VECTOR: {
208: 		// dictionary: continue into child with selection vector
209: 		auto &child = DictionaryVector::Child(source);
210: 		auto &dict_sel = DictionaryVector::SelVector(source);
211: 		VectorOperations::Copy(child, target, dict_sel, source_count, source_offset, target_offset);
212: 		break;
213: 	}
214: 	case VectorType::CONSTANT_VECTOR:
215: 		VectorOperations::Copy(source, target, ConstantVector::ZERO_SELECTION_VECTOR, source_count, source_offset,
216: 		                       target_offset);
217: 		break;
218: 	case VectorType::FLAT_VECTOR:
219: 		if (target_offset + source_count - source_offset > STANDARD_VECTOR_SIZE) {
220: 			idx_t sel_vec_size = target_offset + source_count - source_offset;
221: 			SelectionVector selection_vector(sel_vec_size);
222: 			for (size_t i = 0; i < sel_vec_size; i++) {
223: 				selection_vector.set_index(i, i);
224: 			}
225: 			VectorOperations::Copy(source, target, selection_vector, source_count, source_offset, target_offset);
226: 		} else {
227: 			VectorOperations::Copy(source, target, FlatVector::INCREMENTAL_SELECTION_VECTOR, source_count,
228: 			                       source_offset, target_offset);
229: 		}
230: 		break;
231: 	case VectorType::SEQUENCE_VECTOR: {
232: 		int64_t start, increment;
233: 		SequenceVector::GetSequence(source, start, increment);
234: 		Vector flattened(source.GetType());
235: 		VectorOperations::GenerateSequence(flattened, source_count, start, increment);
236: 
237: 		VectorOperations::Copy(flattened, target, FlatVector::INCREMENTAL_SELECTION_VECTOR, source_count, source_offset,
238: 		                       target_offset);
239: 		break;
240: 	}
241: 	default:
242: 		throw NotImplementedException("FIXME: unimplemented vector type for VectorOperations::Copy");
243: 	}
244: }
245: 
246: } // namespace duckdb
[end of src/common/vector_operations/vector_copy.cpp]
[start of src/execution/expression_executor/execute_case.cpp]
1: #include "duckdb/common/vector_operations/vector_operations.hpp"
2: #include "duckdb/execution/expression_executor.hpp"
3: #include "duckdb/planner/expression/bound_case_expression.hpp"
4: #include "duckdb/common/types/chunk_collection.hpp"
5: 
6: namespace duckdb {
7: 
8: void Case(Vector &res_true, Vector &res_false, Vector &result, SelectionVector &tside, idx_t tcount,
9:           SelectionVector &fside, idx_t fcount);
10: 
11: unique_ptr<ExpressionState> ExpressionExecutor::InitializeState(const BoundCaseExpression &expr,
12:                                                                 ExpressionExecutorState &root) {
13: 	auto result = make_unique<ExpressionState>(expr, root);
14: 	result->AddChild(expr.check.get());
15: 	result->AddChild(expr.result_if_true.get());
16: 	result->AddChild(expr.result_if_false.get());
17: 	result->Finalize();
18: 	return result;
19: }
20: 
21: void ExpressionExecutor::Execute(const BoundCaseExpression &expr, ExpressionState *state, const SelectionVector *sel,
22:                                  idx_t count, Vector &result) {
23: 	Vector res_true, res_false;
24: 	res_true.Reference(state->intermediate_chunk.data[1]);
25: 	res_false.Reference(state->intermediate_chunk.data[2]);
26: 
27: 	auto check_state = state->child_states[0].get();
28: 	auto res_true_state = state->child_states[1].get();
29: 	auto res_false_state = state->child_states[2].get();
30: 
31: 	// first execute the check expression
32: 	SelectionVector true_sel(STANDARD_VECTOR_SIZE), false_sel(STANDARD_VECTOR_SIZE);
33: 	idx_t tcount = Select(*expr.check, check_state, sel, count, &true_sel, &false_sel);
34: 	idx_t fcount = count - tcount;
35: 	if (fcount == 0) {
36: 		// everything is true, only execute TRUE side
37: 		Execute(*expr.result_if_true, res_true_state, sel, count, result);
38: 	} else if (tcount == 0) {
39: 		// everything is false, only execute FALSE side
40: 		Execute(*expr.result_if_false, res_false_state, sel, count, result);
41: 	} else {
42: 		// have to execute both and mix and match
43: 		Execute(*expr.result_if_true, res_true_state, &true_sel, tcount, res_true);
44: 		Execute(*expr.result_if_false, res_false_state, &false_sel, fcount, res_false);
45: 
46: 		Case(res_true, res_false, result, true_sel, tcount, false_sel, fcount);
47: 		if (sel) {
48: 			result.Slice(*sel, count);
49: 		}
50: 	}
51: }
52: 
53: template <class T>
54: void TemplatedFillLoop(Vector &vector, Vector &result, SelectionVector &sel, sel_t count) {
55: 	result.SetVectorType(VectorType::FLAT_VECTOR);
56: 	auto res = FlatVector::GetData<T>(result);
57: 	auto &result_mask = FlatVector::Validity(result);
58: 	if (vector.GetVectorType() == VectorType::CONSTANT_VECTOR) {
59: 		auto data = ConstantVector::GetData<T>(vector);
60: 		if (ConstantVector::IsNull(vector)) {
61: 			for (idx_t i = 0; i < count; i++) {
62: 				result_mask.SetInvalid(sel.get_index(i));
63: 			}
64: 		} else {
65: 			for (idx_t i = 0; i < count; i++) {
66: 				res[sel.get_index(i)] = *data;
67: 			}
68: 		}
69: 	} else {
70: 		VectorData vdata;
71: 		vector.Orrify(count, vdata);
72: 		auto data = (T *)vdata.data;
73: 		for (idx_t i = 0; i < count; i++) {
74: 			auto source_idx = vdata.sel->get_index(i);
75: 			auto res_idx = sel.get_index(i);
76: 
77: 			res[res_idx] = data[source_idx];
78: 			result_mask.Set(res_idx, vdata.validity.RowIsValid(source_idx));
79: 		}
80: 	}
81: }
82: 
83: template <class T>
84: void TemplatedCaseLoop(Vector &res_true, Vector &res_false, Vector &result, SelectionVector &tside, idx_t tcount,
85:                        SelectionVector &fside, idx_t fcount) {
86: 	TemplatedFillLoop<T>(res_true, result, tside, tcount);
87: 	TemplatedFillLoop<T>(res_false, result, fside, fcount);
88: }
89: 
90: void Case(Vector &res_true, Vector &res_false, Vector &result, SelectionVector &tside, idx_t tcount,
91:           SelectionVector &fside, idx_t fcount) {
92: 	D_ASSERT(res_true.GetType() == res_false.GetType() && res_true.GetType() == result.GetType());
93: 
94: 	switch (result.GetType().InternalType()) {
95: 	case PhysicalType::BOOL:
96: 	case PhysicalType::INT8:
97: 		TemplatedCaseLoop<int8_t>(res_true, res_false, result, tside, tcount, fside, fcount);
98: 		break;
99: 	case PhysicalType::INT16:
100: 		TemplatedCaseLoop<int16_t>(res_true, res_false, result, tside, tcount, fside, fcount);
101: 		break;
102: 	case PhysicalType::INT32:
103: 		TemplatedCaseLoop<int32_t>(res_true, res_false, result, tside, tcount, fside, fcount);
104: 		break;
105: 	case PhysicalType::INT64:
106: 		TemplatedCaseLoop<int64_t>(res_true, res_false, result, tside, tcount, fside, fcount);
107: 		break;
108: 	case PhysicalType::UINT8:
109: 		TemplatedCaseLoop<uint8_t>(res_true, res_false, result, tside, tcount, fside, fcount);
110: 		break;
111: 	case PhysicalType::UINT16:
112: 		TemplatedCaseLoop<uint16_t>(res_true, res_false, result, tside, tcount, fside, fcount);
113: 		break;
114: 	case PhysicalType::UINT32:
115: 		TemplatedCaseLoop<uint32_t>(res_true, res_false, result, tside, tcount, fside, fcount);
116: 		break;
117: 	case PhysicalType::UINT64:
118: 		TemplatedCaseLoop<uint64_t>(res_true, res_false, result, tside, tcount, fside, fcount);
119: 		break;
120: 	case PhysicalType::INT128:
121: 		TemplatedCaseLoop<hugeint_t>(res_true, res_false, result, tside, tcount, fside, fcount);
122: 		break;
123: 	case PhysicalType::FLOAT:
124: 		TemplatedCaseLoop<float>(res_true, res_false, result, tside, tcount, fside, fcount);
125: 		break;
126: 	case PhysicalType::DOUBLE:
127: 		TemplatedCaseLoop<double>(res_true, res_false, result, tside, tcount, fside, fcount);
128: 		break;
129: 	case PhysicalType::VARCHAR:
130: 		TemplatedCaseLoop<string_t>(res_true, res_false, result, tside, tcount, fside, fcount);
131: 		StringVector::AddHeapReference(result, res_true);
132: 		StringVector::AddHeapReference(result, res_false);
133: 		break;
134: 	case PhysicalType::LIST: {
135: 		auto result_vector = make_unique<Vector>(result.GetType().child_types()[0].second);
136: 		ListVector::SetEntry(result, move(result_vector));
137: 
138: 		idx_t offset = 0;
139: 		if (ListVector::HasEntry(res_true)) {
140: 			auto &true_child = ListVector::GetEntry(res_true);
141: 			offset += ListVector::GetListSize(res_true);
142: 			ListVector::Append(result, true_child, ListVector::GetListSize(res_true));
143: 		}
144: 		if (ListVector::HasEntry(res_false)) {
145: 			auto &false_child = ListVector::GetEntry(res_false);
146: 			ListVector::Append(result, false_child, ListVector::GetListSize(res_false));
147: 		}
148: 
149: 		// all the false offsets need to be incremented by true_child.count
150: 		TemplatedFillLoop<list_entry_t>(res_true, result, tside, tcount);
151: 
152: 		// FIXME the nullmask here is likely borked
153: 		// TODO uuugly
154: 		VectorData fdata;
155: 		res_false.Orrify(fcount, fdata);
156: 
157: 		auto data = (list_entry_t *)fdata.data;
158: 		auto res = FlatVector::GetData<list_entry_t>(result);
159: 		auto &mask = FlatVector::Validity(result);
160: 
161: 		for (idx_t i = 0; i < fcount; i++) {
162: 			auto fidx = fdata.sel->get_index(i);
163: 			auto res_idx = fside.get_index(i);
164: 			auto list_entry = data[fidx];
165: 			list_entry.offset += offset;
166: 			res[res_idx] = list_entry;
167: 			mask.Set(res_idx, fdata.validity.RowIsValid(fidx));
168: 		}
169: 
170: 		result.Verify(tside, tcount);
171: 		result.Verify(fside, fcount);
172: 		break;
173: 	}
174: 	default:
175: 		throw NotImplementedException("Unimplemented type for case expression: %s", result.GetType().ToString());
176: 	}
177: }
178: 
179: } // namespace duckdb
[end of src/execution/expression_executor/execute_case.cpp]
[start of src/function/aggregate/nested/histogram.cpp]
1: #include "duckdb/function/scalar/nested_functions.hpp"
2: #include "duckdb/function/aggregate/nested_functions.hpp"
3: #include "duckdb/planner/expression/bound_aggregate_expression.hpp"
4: #include "duckdb/common/pair.hpp"
5: #include "duckdb/planner/expression/bound_function_expression.hpp"
6: #include "duckdb/common/map.hpp"
7: #include "duckdb/common/types/vector.hpp"
8: 
9: namespace duckdb {
10: template <class T>
11: struct HistogramAggState {
12: 	map<T, idx_t> *hist;
13: };
14: 
15: struct HistogramFunction {
16: 	template <class STATE>
17: 	static void Initialize(STATE *state) {
18: 		state->hist = nullptr;
19: 	}
20: 
21: 	template <class STATE>
22: 	static void Destroy(STATE *state) {
23: 		if (state->hist) {
24: 			delete state->hist;
25: 		}
26: 	}
27: 
28: 	static bool IgnoreNull() {
29: 		return true;
30: 	}
31: };
32: 
33: template <class T>
34: static void HistogramUpdateFunction(Vector inputs[], FunctionData *, idx_t input_count, Vector &state_vector,
35:                                     idx_t count) {
36: 	D_ASSERT(input_count == 1);
37: 
38: 	auto &input = inputs[0];
39: 	VectorData sdata;
40: 	state_vector.Orrify(count, sdata);
41: 	VectorData input_data;
42: 	input.Orrify(count, input_data);
43: 
44: 	auto states = (HistogramAggState<T> **)sdata.data;
45: 	for (idx_t i = 0; i < count; i++) {
46: 		if (input_data.validity.RowIsValid(input_data.sel->get_index(i))) {
47: 			auto state = states[sdata.sel->get_index(i)];
48: 			if (!state->hist) {
49: 				state->hist = new map<T, idx_t>();
50: 			}
51: 			auto value = (T *)input_data.data;
52: 			(*state->hist)[value[input_data.sel->get_index(i)]]++;
53: 		}
54: 	}
55: }
56: 
57: static void HistogramUpdateFunctionString(Vector inputs[], FunctionData *, idx_t input_count, Vector &state_vector,
58:                                           idx_t count) {
59: 	D_ASSERT(input_count == 1);
60: 
61: 	auto &input = inputs[0];
62: 	VectorData sdata;
63: 	state_vector.Orrify(count, sdata);
64: 	VectorData input_data;
65: 	input.Orrify(count, input_data);
66: 
67: 	auto states = (HistogramAggState<string> **)sdata.data;
68: 	for (idx_t i = 0; i < count; i++) {
69: 		if (input_data.validity.RowIsValid(input_data.sel->get_index(i))) {
70: 			auto state = states[sdata.sel->get_index(i)];
71: 			if (!state->hist) {
72: 				state->hist = new map<string, idx_t>();
73: 			}
74: 			auto value = (string_t *)input_data.data;
75: 			(*state->hist)[value[input_data.sel->get_index(i)].GetString()]++;
76: 		}
77: 	}
78: }
79: 
80: template <class T>
81: static void HistogramCombineFunction(Vector &state, Vector &combined, idx_t count) {
82: 	VectorData sdata;
83: 	state.Orrify(count, sdata);
84: 	auto states_ptr = (HistogramAggState<T> **)sdata.data;
85: 
86: 	auto combined_ptr = FlatVector::GetData<HistogramAggState<T> *>(combined);
87: 
88: 	for (idx_t i = 0; i < count; i++) {
89: 		auto state = states_ptr[sdata.sel->get_index(i)];
90: 		if (!state->hist) {
91: 			continue;
92: 		}
93: 		if (!combined_ptr[i]->hist) {
94: 			combined_ptr[i]->hist = new map<T, idx_t>();
95: 		}
96: 		D_ASSERT(combined_ptr[i]->hist);
97: 		D_ASSERT(state->hist);
98: 		for (auto &entry : *state->hist) {
99: 			(*combined_ptr[i]->hist)[entry.first] += entry.second;
100: 		}
101: 	}
102: }
103: 
104: template <class T>
105: static void HistogramFinalize(Vector &state_vector, FunctionData *, Vector &result, idx_t count) {
106: 	VectorData sdata;
107: 	state_vector.Orrify(count, sdata);
108: 	auto states = (HistogramAggState<T> **)sdata.data;
109: 	result.Initialize();
110: 
111: 	idx_t old_len = 0;
112: 
113: 	auto &mask = FlatVector::Validity(result);
114: 
115: 	auto &child_entries = StructVector::GetEntries(result);
116: 	auto &bucket_list = child_entries[0];
117: 	auto &count_list = child_entries[1];
118: 	for (idx_t i = 0; i < count; i++) {
119: 		auto state = states[sdata.sel->get_index(i)];
120: 		if (!state->hist) {
121: 			mask.SetInvalid(i);
122: 			continue;
123: 		}
124: 		for (auto &entry : *state->hist) {
125: 			auto bucket_value = Value::CreateValue(entry.first);
126: 			ListVector::PushBack(*bucket_list, bucket_value);
127: 			auto count_value = Value::CreateValue(entry.second);
128: 			ListVector::PushBack(*count_list, count_value);
129: 		}
130: 		auto list_struct_data = FlatVector::GetData<list_entry_t>(*bucket_list);
131: 		list_struct_data[i].length = ListVector::GetListSize(*bucket_list) - old_len;
132: 		list_struct_data[i].offset = old_len;
133: 
134: 		list_struct_data = FlatVector::GetData<list_entry_t>(*count_list);
135: 		list_struct_data[i].length = ListVector::GetListSize(*count_list) - old_len;
136: 		list_struct_data[i].offset = old_len;
137: 		old_len = list_struct_data[i].length;
138: 	}
139: }
140: 
141: unique_ptr<FunctionData> HistogramBindFunction(ClientContext &context, AggregateFunction &function,
142:                                                vector<unique_ptr<Expression>> &arguments) {
143: 	if (arguments.size() != 1) {
144: 		throw Exception("We need exactly one argument for the histogram");
145: 	}
146: 	D_ASSERT(arguments.size() == 1);
147: 	child_list_t<LogicalType> struct_children, bucket_type, count_type;
148: 
149: 	bucket_type.push_back({"", arguments[0]->return_type});
150: 	count_type.push_back({"", LogicalType::UBIGINT});
151: 
152: 	struct_children.push_back({"bucket", {LogicalTypeId::LIST, bucket_type}});
153: 	struct_children.push_back({"count", {LogicalTypeId::LIST, count_type}});
154: 	auto struct_type = LogicalType(LogicalTypeId::MAP, move(struct_children));
155: 
156: 	function.return_type = struct_type;
157: 	return make_unique<VariableReturnBindData>(function.return_type);
158: }
159: 
160: AggregateFunction GetHistogramFunction(PhysicalType type) {
161: 	switch (type) {
162: 	case PhysicalType::UINT16:
163: 		return AggregateFunction("histogram", {LogicalType::USMALLINT}, LogicalType::MAP,
164: 		                         AggregateFunction::StateSize<HistogramAggState<uint16_t>>,
165: 		                         AggregateFunction::StateInitialize<HistogramAggState<uint16_t>, HistogramFunction>,
166: 		                         HistogramUpdateFunction<uint16_t>, HistogramCombineFunction<uint16_t>,
167: 		                         HistogramFinalize<uint16_t>, nullptr, HistogramBindFunction,
168: 		                         AggregateFunction::StateDestroy<HistogramAggState<uint16_t>, HistogramFunction>);
169: 	case PhysicalType::UINT32:
170: 		return AggregateFunction("histogram", {LogicalType::UINTEGER}, LogicalType::MAP,
171: 		                         AggregateFunction::StateSize<HistogramAggState<uint32_t>>,
172: 		                         AggregateFunction::StateInitialize<HistogramAggState<uint32_t>, HistogramFunction>,
173: 		                         HistogramUpdateFunction<uint32_t>, HistogramCombineFunction<uint32_t>,
174: 		                         HistogramFinalize<uint32_t>, nullptr, HistogramBindFunction,
175: 		                         AggregateFunction::StateDestroy<HistogramAggState<uint32_t>, HistogramFunction>);
176: 	case PhysicalType::UINT64:
177: 		return AggregateFunction("histogram", {LogicalType::UBIGINT}, LogicalType::MAP,
178: 		                         AggregateFunction::StateSize<HistogramAggState<uint64_t>>,
179: 		                         AggregateFunction::StateInitialize<HistogramAggState<uint64_t>, HistogramFunction>,
180: 		                         HistogramUpdateFunction<uint64_t>, HistogramCombineFunction<uint64_t>,
181: 		                         HistogramFinalize<uint64_t>, nullptr, HistogramBindFunction,
182: 		                         AggregateFunction::StateDestroy<HistogramAggState<uint64_t>, HistogramFunction>);
183: 	case PhysicalType::INT16:
184: 		return AggregateFunction("histogram", {LogicalType::SMALLINT}, LogicalType::MAP,
185: 		                         AggregateFunction::StateSize<HistogramAggState<int16_t>>,
186: 		                         AggregateFunction::StateInitialize<HistogramAggState<int16_t>, HistogramFunction>,
187: 		                         HistogramUpdateFunction<int16_t>, HistogramCombineFunction<int16_t>,
188: 		                         HistogramFinalize<int16_t>, nullptr, HistogramBindFunction,
189: 		                         AggregateFunction::StateDestroy<HistogramAggState<int16_t>, HistogramFunction>);
190: 	case PhysicalType::INT32:
191: 		return AggregateFunction("histogram", {LogicalType::INTEGER}, LogicalType::MAP,
192: 		                         AggregateFunction::StateSize<HistogramAggState<int32_t>>,
193: 		                         AggregateFunction::StateInitialize<HistogramAggState<int32_t>, HistogramFunction>,
194: 		                         HistogramUpdateFunction<int32_t>, HistogramCombineFunction<int32_t>,
195: 		                         HistogramFinalize<int32_t>, nullptr, HistogramBindFunction,
196: 		                         AggregateFunction::StateDestroy<HistogramAggState<int32_t>, HistogramFunction>);
197: 	case PhysicalType::INT64:
198: 		return AggregateFunction("histogram", {LogicalType::BIGINT}, LogicalType::MAP,
199: 		                         AggregateFunction::StateSize<HistogramAggState<int64_t>>,
200: 		                         AggregateFunction::StateInitialize<HistogramAggState<int64_t>, HistogramFunction>,
201: 		                         HistogramUpdateFunction<int64_t>, HistogramCombineFunction<int64_t>,
202: 		                         HistogramFinalize<int64_t>, nullptr, HistogramBindFunction,
203: 		                         AggregateFunction::StateDestroy<HistogramAggState<int64_t>, HistogramFunction>);
204: 	case PhysicalType::FLOAT:
205: 		return AggregateFunction(
206: 		    "histogram", {LogicalType::FLOAT}, LogicalType::MAP, AggregateFunction::StateSize<HistogramAggState<float>>,
207: 		    AggregateFunction::StateInitialize<HistogramAggState<float>, HistogramFunction>,
208: 		    HistogramUpdateFunction<float>, HistogramCombineFunction<float>, HistogramFinalize<float>, nullptr,
209: 		    HistogramBindFunction, AggregateFunction::StateDestroy<HistogramAggState<float>, HistogramFunction>);
210: 	case PhysicalType::DOUBLE:
211: 		return AggregateFunction("histogram", {LogicalType::DOUBLE}, LogicalType::MAP,
212: 		                         AggregateFunction::StateSize<HistogramAggState<double>>,
213: 		                         AggregateFunction::StateInitialize<HistogramAggState<double>, HistogramFunction>,
214: 		                         HistogramUpdateFunction<double>, HistogramCombineFunction<double>,
215: 		                         HistogramFinalize<double>, nullptr, HistogramBindFunction,
216: 		                         AggregateFunction::StateDestroy<HistogramAggState<double>, HistogramFunction>);
217: 	case PhysicalType::VARCHAR:
218: 		return AggregateFunction("histogram", {LogicalType::VARCHAR}, LogicalType::MAP,
219: 		                         AggregateFunction::StateSize<HistogramAggState<string>>,
220: 		                         AggregateFunction::StateInitialize<HistogramAggState<string>, HistogramFunction>,
221: 		                         HistogramUpdateFunctionString, HistogramCombineFunction<string>,
222: 		                         HistogramFinalize<string>, nullptr, HistogramBindFunction,
223: 		                         AggregateFunction::StateDestroy<HistogramAggState<string>, HistogramFunction>);
224: 
225: 	default:
226: 		throw NotImplementedException("Unimplemented histogram aggregate");
227: 	}
228: }
229: 
230: void HistogramFun::RegisterFunction(BuiltinFunctions &set) {
231: 	AggregateFunctionSet fun("histogram");
232: 	fun.AddFunction(GetHistogramFunction(PhysicalType::UINT16));
233: 	fun.AddFunction(GetHistogramFunction(PhysicalType::UINT32));
234: 	fun.AddFunction(GetHistogramFunction(PhysicalType::UINT64));
235: 	fun.AddFunction(GetHistogramFunction(PhysicalType::INT16));
236: 	fun.AddFunction(GetHistogramFunction(PhysicalType::INT32));
237: 	fun.AddFunction(GetHistogramFunction(PhysicalType::INT64));
238: 	fun.AddFunction(GetHistogramFunction(PhysicalType::FLOAT));
239: 	fun.AddFunction(GetHistogramFunction(PhysicalType::DOUBLE));
240: 	fun.AddFunction(GetHistogramFunction(PhysicalType::VARCHAR));
241: 	fun.AddFunction(AggregateFunction("histogram", {LogicalType::TIMESTAMP}, LogicalType::MAP,
242: 	                                  AggregateFunction::StateSize<HistogramAggState<int64_t>>,
243: 	                                  AggregateFunction::StateInitialize<HistogramAggState<int64_t>, HistogramFunction>,
244: 	                                  HistogramUpdateFunction<int64_t>, HistogramCombineFunction<int64_t>,
245: 	                                  HistogramFinalize<int64_t>, nullptr, HistogramBindFunction,
246: 	                                  AggregateFunction::StateDestroy<HistogramAggState<int64_t>, HistogramFunction>));
247: 	set.AddFunction(fun);
248: }
249: 
250: } // namespace duckdb
[end of src/function/aggregate/nested/histogram.cpp]
[start of src/function/scalar/nested/map/map.cpp]
1: #include "duckdb/planner/expression/bound_function_expression.hpp"
2: #include "duckdb/common/string_util.hpp"
3: #include "duckdb/parser/expression/bound_expression.hpp"
4: #include "duckdb/function/scalar/nested_functions.hpp"
5: #include "duckdb/common/types/data_chunk.hpp"
6: #include "duckdb/common/pair.hpp"
7: 
8: namespace duckdb {
9: 
10: static void MapFunction(DataChunk &args, ExpressionState &state, Vector &result) {
11: 	D_ASSERT(result.GetType().id() == LogicalTypeId::MAP);
12: 	D_ASSERT(result.GetType().child_types().size() == 2);
13: 
14: 	//! Otherwise if its not a constant vector, this breaks the optimizer
15: 	result.SetVectorType(VectorType::CONSTANT_VECTOR);
16: 	for (idx_t i = 0; i < args.ColumnCount(); i++) {
17: 		if (args.data[i].GetVectorType() != VectorType::CONSTANT_VECTOR) {
18: 			result.SetVectorType(VectorType::FLAT_VECTOR);
19: 		}
20: 	}
21: 
22: 	auto &child_entries = StructVector::GetEntries(result);
23: 	D_ASSERT(child_entries.size() == 2);
24: 	auto &key_vector = child_entries[0];
25: 	auto &value_vector = child_entries[1];
26: 	if (args.data.empty()) {
27: 		// no arguments: construct an empty map
28: 
29: 		auto list_child = make_unique<Vector>(LogicalTypeId::SQLNULL);
30: 		ListVector::SetEntry(*key_vector, move(list_child));
31: 		ListVector::SetListSize(*key_vector, 0);
32: 		auto list_data = FlatVector::GetData<list_entry_t>(*key_vector);
33: 		list_data->offset = 0;
34: 		list_data->length = 0;
35: 
36: 		list_child = make_unique<Vector>(LogicalTypeId::SQLNULL);
37: 		ListVector::SetEntry(*value_vector, move(list_child));
38: 		ListVector::SetListSize(*value_vector, 0);
39: 		list_data = FlatVector::GetData<list_entry_t>(*value_vector);
40: 		list_data->offset = 0;
41: 		list_data->length = 0;
42: 		return;
43: 	}
44: 
45: 	if (ListVector::GetListSize(args.data[0]) != ListVector::GetListSize(args.data[1])) {
46: 		throw Exception("Key list has a different size from Value list");
47: 	}
48: 	key_vector->Reference(args.data[0]);
49: 	value_vector->Reference(args.data[1]);
50: 
51: 	result.Verify(args.size());
52: }
53: 
54: static unique_ptr<FunctionData> MapBind(ClientContext &context, ScalarFunction &bound_function,
55:                                         vector<unique_ptr<Expression>> &arguments) {
56: 	child_list_t<LogicalType> child_types;
57: 
58: 	if (arguments.size() != 2 && !arguments.empty()) {
59: 		throw Exception("We need exactly two lists for a map");
60: 	}
61: 	if (arguments.size() == 2) {
62: 		if (arguments[0]->return_type.id() != LogicalTypeId::LIST) {
63: 			throw Exception("First argument is not a list");
64: 		}
65: 		if (arguments[1]->return_type.id() != LogicalTypeId::LIST) {
66: 			throw Exception("Second argument is not a list");
67: 		}
68: 		child_types.push_back(make_pair("key", arguments[0]->return_type));
69: 		child_types.push_back(make_pair("value", arguments[1]->return_type));
70: 	}
71: 
72: 	if (arguments.empty()) {
73: 		child_list_t<LogicalType> child;
74: 		child.push_back({"", LogicalTypeId::SQLNULL});
75: 		auto empty = LogicalType(LogicalTypeId::LIST, child);
76: 		child_types.push_back(make_pair("key", empty));
77: 		child_types.push_back(make_pair("value", empty));
78: 	}
79: 
80: 	//! this is more for completeness reasons
81: 	bound_function.return_type = LogicalType(LogicalTypeId::MAP, move(child_types));
82: 	return make_unique<VariableReturnBindData>(bound_function.return_type);
83: }
84: 
85: void MapFun::RegisterFunction(BuiltinFunctions &set) {
86: 	//! the arguments and return types are actually set in the binder function
87: 	ScalarFunction fun("map", {}, LogicalType::MAP, MapFunction, false, MapBind);
88: 	fun.varargs = LogicalType::ANY;
89: 	set.AddFunction(fun);
90: }
91: 
92: } // namespace duckdb
[end of src/function/scalar/nested/map/map.cpp]
[start of src/function/scalar/nested/struct_extract.cpp]
1: #include "duckdb/function/scalar/nested_functions.hpp"
2: #include "duckdb/execution/expression_executor.hpp"
3: #include "duckdb/planner/expression/bound_function_expression.hpp"
4: #include "duckdb/common/string_util.hpp"
5: 
6: namespace duckdb {
7: 
8: struct StructExtractBindData : public FunctionData {
9: 	StructExtractBindData(string key, idx_t index, LogicalType type) : key(move(key)), index(index), type(move(type)) {
10: 	}
11: 
12: 	string key;
13: 	idx_t index;
14: 	LogicalType type;
15: 
16: public:
17: 	unique_ptr<FunctionData> Copy() override {
18: 		return make_unique<StructExtractBindData>(key, index, type);
19: 	}
20: 	bool Equals(FunctionData &other_p) override {
21: 		auto &other = (StructExtractBindData &)other_p;
22: 		return key == other.key && index == other.index && type == other.type;
23: 	}
24: };
25: 
26: static void StructExtractFunction(DataChunk &args, ExpressionState &state, Vector &result) {
27: 	auto &func_expr = (BoundFunctionExpression &)state.expr;
28: 	auto &info = (StructExtractBindData &)*func_expr.bind_info;
29: 
30: 	// this should be guaranteed by the binder
31: 	auto &vec = args.data[0];
32: 
33: 	vec.Verify(args.size());
34: 	if (vec.GetVectorType() == VectorType::DICTIONARY_VECTOR) {
35: 		auto &child = DictionaryVector::Child(vec);
36: 		auto &dict_sel = DictionaryVector::SelVector(vec);
37: 		auto &children = StructVector::GetEntries(child);
38: 		D_ASSERT(info.index < children.size());
39: 		auto &struct_child = children[info.index];
40: 		result.Slice(*struct_child, dict_sel, args.size());
41: 	} else {
42: 		auto &children = StructVector::GetEntries(vec);
43: 		D_ASSERT(info.index < children.size());
44: 		auto &struct_child = children[info.index];
45: 		result.Reference(*struct_child);
46: 	}
47: 	result.Verify(args.size());
48: }
49: 
50: static unique_ptr<FunctionData> StructExtractBind(ClientContext &context, ScalarFunction &bound_function,
51:                                                   vector<unique_ptr<Expression>> &arguments) {
52: 	auto &struct_children = arguments[0]->return_type.child_types();
53: 	if (struct_children.empty()) {
54: 		throw Exception("Can't extract something from an empty struct");
55: 	}
56: 
57: 	auto &key_child = arguments[1];
58: 
59: 	if (key_child->return_type.id() != LogicalTypeId::VARCHAR ||
60: 	    key_child->return_type.id() != LogicalTypeId::VARCHAR || !key_child->IsFoldable()) {
61: 		throw Exception("Key name for struct_extract needs to be a constant string");
62: 	}
63: 	Value key_val = ExpressionExecutor::EvaluateScalar(*key_child.get());
64: 	D_ASSERT(key_val.type().id() == LogicalTypeId::VARCHAR);
65: 	if (key_val.is_null || key_val.str_value.length() < 1) {
66: 		throw Exception("Key name for struct_extract needs to be neither NULL nor empty");
67: 	}
68: 	string key = StringUtil::Lower(key_val.str_value);
69: 
70: 	LogicalType return_type;
71: 	idx_t key_index = 0;
72: 	bool found_key = false;
73: 
74: 	for (size_t i = 0; i < struct_children.size(); i++) {
75: 		auto &child = struct_children[i];
76: 		if (child.first == key) {
77: 			found_key = true;
78: 			key_index = i;
79: 			return_type = child.second;
80: 			break;
81: 		}
82: 	}
83: 	if (!found_key) {
84: 		throw Exception("Could not find key in struct");
85: 	}
86: 
87: 	bound_function.return_type = return_type;
88: 	bound_function.arguments[0] = arguments[0]->return_type;
89: 	return make_unique<StructExtractBindData>(key, key_index, return_type);
90: }
91: 
92: ScalarFunction StructExtractFun::GetFunction() {
93: 	return ScalarFunction("struct_extract", {LogicalType::STRUCT, LogicalType::VARCHAR}, LogicalType::ANY,
94: 	                      StructExtractFunction, false, StructExtractBind);
95: }
96: 
97: void StructExtractFun::RegisterFunction(BuiltinFunctions &set) {
98: 	// the arguments and return types are actually set in the binder function
99: 	auto fun = GetFunction();
100: 	set.AddFunction(fun);
101: }
102: 
103: } // namespace duckdb
[end of src/function/scalar/nested/struct_extract.cpp]
[start of src/include/duckdb/common/helper.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/helper.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/constants.hpp"
12: #include <string.h>
13: 
14: #ifdef _MSC_VER
15: #define suint64_t int64_t
16: #endif
17: 
18: namespace duckdb {
19: #if !defined(_MSC_VER) && (__cplusplus < 201402L)
20: template <typename T, typename... Args>
21: unique_ptr<T> make_unique(Args &&... args) {
22: 	return unique_ptr<T>(new T(std::forward<Args>(args)...));
23: }
24: #else // Visual Studio has make_unique
25: using std::make_unique;
26: #endif
27: template <typename S, typename T, typename... Args>
28: unique_ptr<S> make_unique_base(Args &&... args) {
29: 	return unique_ptr<S>(new T(std::forward<Args>(args)...));
30: }
31: 
32: template <typename T, typename S>
33: unique_ptr<S> unique_ptr_cast(unique_ptr<T> src) {
34: 	return unique_ptr<S>(static_cast<S *>(src.release()));
35: }
36: 
37: template <typename T>
38: T MaxValue(T a, T b) {
39: 	return a > b ? a : b;
40: }
41: 
42: template <typename T>
43: T MinValue(T a, T b) {
44: 	return a < b ? a : b;
45: }
46: 
47: template <typename T>
48: const T Load(const_data_ptr_t ptr) {
49: 	T ret;
50: 	memcpy(&ret, ptr, sizeof(ret));
51: 	return ret;
52: }
53: 
54: template <typename T>
55: void Store(const T val, data_ptr_t ptr) {
56: 	memcpy(ptr, (void *)&val, sizeof(val));
57: }
58: 
59: } // namespace duckdb
[end of src/include/duckdb/common/helper.hpp]
[start of src/include/duckdb/common/types/validity_mask.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/types/validity_mask.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/types.hpp"
13: #include "duckdb/common/vector_size.hpp"
14: #include "duckdb/common/to_string.hpp"
15: 
16: namespace duckdb {
17: struct ValidityMask;
18: 
19: template <typename V>
20: struct TemplatedValidityData {
21: 	static constexpr const int BITS_PER_VALUE = sizeof(V) * 8;
22: 	static constexpr const V MAX_ENTRY = ~V(0);
23: 
24: public:
25: 	explicit TemplatedValidityData(idx_t count) {
26: 		auto entry_count = EntryCount(count);
27: 		owned_data = unique_ptr<V[]>(new V[entry_count]);
28: 		for (idx_t entry_idx = 0; entry_idx < entry_count; entry_idx++) {
29: 			owned_data[entry_idx] = MAX_ENTRY;
30: 		}
31: 	}
32: 	TemplatedValidityData(const V *validity_mask, idx_t count) {
33: 		D_ASSERT(validity_mask);
34: 		auto entry_count = EntryCount(count);
35: 		owned_data = unique_ptr<V[]>(new V[entry_count]);
36: 		for (idx_t entry_idx = 0; entry_idx < entry_count; entry_idx++) {
37: 			owned_data[entry_idx] = validity_mask[entry_idx];
38: 		}
39: 	}
40: 
41: 	unique_ptr<V[]> owned_data;
42: 
43: public:
44: 	static inline idx_t EntryCount(idx_t count) {
45: 		return (count + (BITS_PER_VALUE - 1)) / BITS_PER_VALUE;
46: 	}
47: };
48: 
49: using validity_t = uint64_t;
50: 
51: struct ValidityData : TemplatedValidityData<validity_t> {
52: public:
53: 	DUCKDB_API explicit ValidityData(idx_t count);
54: 	DUCKDB_API ValidityData(const ValidityMask &original, idx_t count);
55: };
56: 
57: //! Type used for validity masks
58: template <typename V>
59: struct TemplatedValidityMask {
60: 	using ValidityBuffer = TemplatedValidityData<V>;
61: 
62: public:
63: 	static constexpr const int BITS_PER_VALUE = ValidityBuffer::BITS_PER_VALUE;
64: 	static constexpr const int STANDARD_ENTRY_COUNT = (STANDARD_VECTOR_SIZE + (BITS_PER_VALUE - 1)) / BITS_PER_VALUE;
65: 	static constexpr const int STANDARD_MASK_SIZE = STANDARD_ENTRY_COUNT * sizeof(validity_t);
66: 
67: public:
68: 	TemplatedValidityMask() : validity_mask(nullptr) {
69: 	}
70: 	explicit TemplatedValidityMask(idx_t max_count) {
71: 		Initialize(max_count);
72: 	}
73: 	explicit TemplatedValidityMask(V *ptr) : validity_mask(ptr) {
74: 	}
75: 	TemplatedValidityMask(const TemplatedValidityMask &original, idx_t count) {
76: 		Copy(original, count);
77: 	}
78: 
79: 	static inline idx_t ValidityMaskSize(idx_t count = STANDARD_VECTOR_SIZE) {
80: 		return ValidityBuffer::EntryCount(count) * sizeof(V);
81: 	}
82: 	inline bool AllValid() const {
83: 		return !validity_mask;
84: 	}
85: 	bool CheckAllValid(idx_t count) const {
86: 		if (AllValid()) {
87: 			return true;
88: 		}
89: 		idx_t entry_count = ValidityBuffer::EntryCount(count);
90: 		idx_t valid_count = 0;
91: 		for (idx_t i = 0; i < entry_count; i++) {
92: 			valid_count += validity_mask[i] == ValidityBuffer::MAX_ENTRY;
93: 		}
94: 		return valid_count == entry_count;
95: 	}
96: 	inline V *GetData() const {
97: 		return validity_mask;
98: 	}
99: 	void Reset() {
100: 		validity_mask = nullptr;
101: 		validity_data.reset();
102: 	}
103: 
104: 	static inline idx_t EntryCount(idx_t count) {
105: 		return ValidityBuffer::EntryCount(count);
106: 	}
107: 	inline V GetValidityEntry(idx_t entry_idx) const {
108: 		if (!validity_mask) {
109: 			return ValidityBuffer::MAX_ENTRY;
110: 		}
111: 		return validity_mask[entry_idx];
112: 	}
113: 	static inline bool AllValid(V entry) {
114: 		return entry == ValidityBuffer::MAX_ENTRY;
115: 	}
116: 	static inline bool NoneValid(V entry) {
117: 		return entry == 0;
118: 	}
119: 	static inline bool RowIsValid(V entry, idx_t idx_in_entry) {
120: 		return entry & (V(1) << V(idx_in_entry));
121: 	}
122: 	static inline void GetEntryIndex(idx_t row_idx, idx_t &entry_idx, idx_t &idx_in_entry) {
123: 		entry_idx = row_idx / BITS_PER_VALUE;
124: 		idx_in_entry = row_idx % BITS_PER_VALUE;
125: 	}
126: 
127: 	//! RowIsValidUnsafe should only be used if AllValid() is false: it achieves the same as RowIsValid but skips a
128: 	//! not-null check
129: 	inline bool RowIsValidUnsafe(idx_t row_idx) const {
130: 		D_ASSERT(validity_mask);
131: 		idx_t entry_idx, idx_in_entry;
132: 		GetEntryIndex(row_idx, entry_idx, idx_in_entry);
133: 		auto entry = GetValidityEntry(entry_idx);
134: 		return RowIsValid(entry, idx_in_entry);
135: 	}
136: 
137: 	//! Returns true if a row is valid (i.e. not null), false otherwise
138: 	inline bool RowIsValid(idx_t row_idx) const {
139: 		if (!validity_mask) {
140: 			return true;
141: 		}
142: 		return RowIsValidUnsafe(row_idx);
143: 	}
144: 
145: 	//! Same as SetValid, but skips a null check on validity_mask
146: 	inline void SetValidUnsafe(idx_t row_idx) {
147: 		D_ASSERT(validity_mask);
148: 		idx_t entry_idx, idx_in_entry;
149: 		GetEntryIndex(row_idx, entry_idx, idx_in_entry);
150: 		validity_mask[entry_idx] |= (V(1) << V(idx_in_entry));
151: 	}
152: 
153: 	//! Marks the entry at the specified row index as valid (i.e. not-null)
154: 	inline void SetValid(idx_t row_idx) {
155: 		if (!validity_mask) {
156: 			// if AllValid() we don't need to do anything
157: 			// the row is already valid
158: 			return;
159: 		}
160: 		SetValidUnsafe(row_idx);
161: 	}
162: 
163: 	//! Marks the entry at the specified row index as invalid (i.e. null)
164: 	inline void SetInvalidUnsafe(idx_t row_idx) {
165: 		D_ASSERT(validity_mask);
166: 		idx_t entry_idx, idx_in_entry;
167: 		GetEntryIndex(row_idx, entry_idx, idx_in_entry);
168: 		validity_mask[entry_idx] &= ~(V(1) << V(idx_in_entry));
169: 	}
170: 
171: 	//! Marks the entry at the specified row index as invalid (i.e. null)
172: 	inline void SetInvalid(idx_t row_idx) {
173: 		if (!validity_mask) {
174: 			D_ASSERT(row_idx <= STANDARD_VECTOR_SIZE);
175: 			Initialize(STANDARD_VECTOR_SIZE);
176: 		}
177: 		SetInvalidUnsafe(row_idx);
178: 	}
179: 
180: 	//! Mark the entrry at the specified index as either valid or invalid (non-null or null)
181: 	inline void Set(idx_t row_idx, bool valid) {
182: 		if (valid) {
183: 			SetValid(row_idx);
184: 		} else {
185: 			SetInvalid(row_idx);
186: 		}
187: 	}
188: 
189: 	//! Ensure the validity mask is writable, allocating space if it is not initialized
190: 	inline void EnsureWritable() {
191: 		if (!validity_mask) {
192: 			Initialize();
193: 		}
194: 	}
195: 
196: 	//! Marks "count" entries in the validity mask as invalid (null)
197: 	inline void SetAllInvalid(idx_t count) {
198: 		D_ASSERT(count <= STANDARD_VECTOR_SIZE);
199: 		EnsureWritable();
200: 		for (idx_t i = 0; i < ValidityBuffer::EntryCount(count); i++) {
201: 			validity_mask[i] = 0;
202: 		}
203: 	}
204: 
205: 	//! Marks "count" entries in the validity mask as valid (not null)
206: 	inline void SetAllValid(idx_t count) {
207: 		D_ASSERT(count <= STANDARD_VECTOR_SIZE);
208: 		EnsureWritable();
209: 		for (idx_t i = 0; i < ValidityBuffer::EntryCount(count); i++) {
210: 			validity_mask[i] = ValidityBuffer::MAX_ENTRY;
211: 		}
212: 	}
213: 
214: 	inline bool IsMaskSet() const {
215: 		if (validity_mask) {
216: 			return true;
217: 		}
218: 		return false;
219: 	}
220: 
221: public:
222: 	void Initialize(validity_t *validity) {
223: 		validity_data.reset();
224: 		validity_mask = validity;
225: 	}
226: 	void Initialize(const TemplatedValidityMask &other) {
227: 		validity_mask = other.validity_mask;
228: 		validity_data = other.validity_data;
229: 	}
230: 	void Initialize(idx_t count = STANDARD_VECTOR_SIZE) {
231: 		validity_data = make_buffer<ValidityBuffer>(count);
232: 		validity_mask = validity_data->owned_data.get();
233: 	}
234: 	void Copy(const TemplatedValidityMask &other, idx_t count) {
235: 		if (other.AllValid()) {
236: 			validity_data = nullptr;
237: 			validity_mask = nullptr;
238: 		} else {
239: 			validity_data = make_buffer<ValidityBuffer>(other.validity_mask, count);
240: 			validity_mask = validity_data->owned_data.get();
241: 		}
242: 	}
243: 
244: protected:
245: 	V *validity_mask;
246: 	buffer_ptr<ValidityBuffer> validity_data;
247: };
248: 
249: struct ValidityMask : public TemplatedValidityMask<validity_t> {
250: public:
251: 	ValidityMask() : TemplatedValidityMask(nullptr) {
252: 	}
253: 	explicit ValidityMask(idx_t max_count) : TemplatedValidityMask(max_count) {
254: 	}
255: 	explicit ValidityMask(validity_t *ptr) : TemplatedValidityMask(ptr) {
256: 	}
257: 	ValidityMask(const ValidityMask &original, idx_t count) : TemplatedValidityMask(original, count) {
258: 	}
259: 
260: public:
261: 	void Resize(idx_t old_size, idx_t new_size);
262: 
263: 	void Slice(const ValidityMask &other, idx_t offset);
264: 	void Combine(const ValidityMask &other, idx_t count);
265: 	string ToString(idx_t count) const;
266: };
267: 
268: } // namespace duckdb
[end of src/include/duckdb/common/types/validity_mask.hpp]
[start of src/include/duckdb/common/types/vector.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/types/vector.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/bitset.hpp"
12: #include "duckdb/common/common.hpp"
13: #include "duckdb/common/types/selection_vector.hpp"
14: #include "duckdb/common/types/value.hpp"
15: #include "duckdb/common/enums/vector_type.hpp"
16: #include "duckdb/common/types/vector_buffer.hpp"
17: #include "duckdb/common/vector_size.hpp"
18: #include "duckdb/common/types/validity_mask.hpp"
19: 
20: namespace duckdb {
21: 
22: struct VectorData {
23: 	const SelectionVector *sel;
24: 	data_ptr_t data;
25: 	ValidityMask validity;
26: };
27: 
28: class VectorStructBuffer;
29: class VectorListBuffer;
30: class ChunkCollection;
31: 
32: struct SelCache;
33: 
34: //!  Vector of values of a specified PhysicalType.
35: class Vector {
36: 	friend struct ConstantVector;
37: 	friend struct DictionaryVector;
38: 	friend struct FlatVector;
39: 	friend struct ListVector;
40: 	friend struct StringVector;
41: 	friend struct StructVector;
42: 	friend struct SequenceVector;
43: 
44: 	friend class DataChunk;
45: 
46: public:
47: 	Vector();
48: 	//! Create a vector of size one holding the passed on value
49: 	explicit Vector(const Value &value);
50: 	//! Create an empty standard vector with a type, equivalent to calling Vector(type, true, false)
51: 	explicit Vector(const LogicalType &type);
52: 	//! Create a non-owning vector that references the specified data
53: 	Vector(const LogicalType &type, data_ptr_t dataptr);
54: 	//! Create an owning vector that holds at most STANDARD_VECTOR_SIZE entries.
55: 	/*!
56: 	    Create a new vector
57: 	    If create_data is true, the vector will be an owning empty vector.
58: 	    If zero_data is true, the allocated data will be zero-initialized.
59: 	*/
60: 	Vector(const LogicalType &type, bool create_data, bool zero_data);
61: 	// implicit copying of Vectors is not allowed
62: 	Vector(const Vector &) = delete;
63: 	// but moving of vectors is allowed
64: 	Vector(Vector &&other) noexcept;
65: 
66: public:
67: 	//! Create a vector that references the specified value.
68: 	void Reference(const Value &value);
69: 	//! Causes this vector to reference the data held by the other vector.
70: 	void Reference(Vector &other);
71: 
72: 	//! Creates a reference to a slice of the other vector
73: 	void Slice(Vector &other, idx_t offset);
74: 	//! Creates a reference to a slice of the other vector
75: 	void Slice(Vector &other, const SelectionVector &sel, idx_t count);
76: 	//! Turns the vector into a dictionary vector with the specified dictionary
77: 	void Slice(const SelectionVector &sel, idx_t count);
78: 	//! Slice the vector, keeping the result around in a cache or potentially using the cache instead of slicing
79: 	void Slice(const SelectionVector &sel, idx_t count, SelCache &cache);
80: 
81: 	//! Creates the data of this vector with the specified type. Any data that
82: 	//! is currently in the vector is destroyed.
83: 	void Initialize(const LogicalType &new_type = LogicalType(LogicalTypeId::INVALID), bool zero_data = false);
84: 
85: 	//! Converts this Vector to a printable string representation
86: 	string ToString(idx_t count) const;
87: 	void Print(idx_t count);
88: 
89: 	string ToString() const;
90: 	void Print();
91: 
92: 	//! Flatten the vector, removing any compression and turning it into a FLAT_VECTOR
93: 	DUCKDB_API void Normalify(idx_t count);
94: 	DUCKDB_API void Normalify(const SelectionVector &sel, idx_t count);
95: 	//! Obtains a selection vector and data pointer through which the data of this vector can be accessed
96: 	DUCKDB_API void Orrify(idx_t count, VectorData &data);
97: 
98: 	//! Turn the vector into a sequence vector
99: 	void Sequence(int64_t start, int64_t increment);
100: 
101: 	//! Verify that the Vector is in a consistent, not corrupt state. DEBUG
102: 	//! FUNCTION ONLY!
103: 	void Verify(idx_t count);
104: 	void Verify(const SelectionVector &sel, idx_t count);
105: 	void UTFVerify(idx_t count);
106: 	void UTFVerify(const SelectionVector &sel, idx_t count);
107: 
108: 	//! Returns the [index] element of the Vector as a Value.
109: 	Value GetValue(idx_t index) const;
110: 	//! Sets the [index] element of the Vector to the specified Value.
111: 	void SetValue(idx_t index, const Value &val);
112: 
113: 	void SetAuxiliary(buffer_ptr<VectorBuffer> new_buffer) {
114: 		auxiliary = std::move(new_buffer);
115: 	};
116: 
117: 	//! This functions resizes the vector
118: 	void Resize(idx_t cur_size, idx_t new_size);
119: 
120: 	//! Serializes a Vector to a stand-alone binary blob
121: 	void Serialize(idx_t count, Serializer &serializer);
122: 	//! Deserializes a blob back into a Vector
123: 	void Deserialize(idx_t count, Deserializer &source);
124: 
125: 	// Getters
126: 	inline VectorType GetVectorType() const {
127: 		return buffer->GetVectorType();
128: 	}
129: 	inline const LogicalType &GetType() const {
130: 		return buffer->GetType();
131: 	}
132: 	inline VectorBufferType GetBufferType() const {
133: 		return buffer->GetBufferType();
134: 	}
135: 	inline data_ptr_t GetData() {
136: 		return data;
137: 	}
138: 
139: 	buffer_ptr<VectorBuffer> GetAuxiliary() {
140: 		return auxiliary;
141: 	}
142: 
143: 	buffer_ptr<VectorBuffer> GetBuffer() {
144: 		return buffer;
145: 	}
146: 
147: 	// Setters
148: 	inline void SetVectorType(VectorType vector_type) {
149: 		buffer->SetVectorType(vector_type);
150: 	}
151: 	inline void SetType(const LogicalType &type) {
152: 		buffer->SetType(type);
153: 	}
154: 	inline void SetBufferType(VectorBufferType buffer_type) {
155: 		buffer->SetBufferType(buffer_type);
156: 	}
157: 
158: protected:
159: 	//! A pointer to the data.
160: 	data_ptr_t data;
161: 	//! The validity mask of the vector
162: 	ValidityMask validity;
163: 	//! The main buffer holding the data of the vector
164: 	buffer_ptr<VectorBuffer> buffer;
165: 	//! The buffer holding auxiliary data of the vector
166: 	//! e.g. a string vector uses this to store strings
167: 	buffer_ptr<VectorBuffer> auxiliary;
168: };
169: 
170: //! The DictionaryBuffer holds a selection vector
171: class VectorChildBuffer : public VectorBuffer {
172: public:
173: 	VectorChildBuffer() : VectorBuffer(VectorBufferType::VECTOR_CHILD_BUFFER), data() {
174: 	}
175: 
176: public:
177: 	Vector data;
178: };
179: 
180: struct ConstantVector {
181: 	static inline const_data_ptr_t GetData(const Vector &vector) {
182: 		D_ASSERT(vector.GetVectorType() == VectorType::CONSTANT_VECTOR ||
183: 		         vector.GetVectorType() == VectorType::FLAT_VECTOR);
184: 		return vector.data;
185: 	}
186: 	static inline data_ptr_t GetData(Vector &vector) {
187: 		D_ASSERT(vector.GetVectorType() == VectorType::CONSTANT_VECTOR ||
188: 		         vector.GetVectorType() == VectorType::FLAT_VECTOR);
189: 		return vector.data;
190: 	}
191: 	template <class T>
192: 	static inline const T *GetData(const Vector &vector) {
193: 		return (const T *)ConstantVector::GetData(vector);
194: 	}
195: 	template <class T>
196: 	static inline T *GetData(Vector &vector) {
197: 		return (T *)ConstantVector::GetData(vector);
198: 	}
199: 	static inline bool IsNull(const Vector &vector) {
200: 		D_ASSERT(vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
201: 		return !vector.validity.RowIsValid(0);
202: 	}
203: 	static inline void SetNull(Vector &vector, bool is_null) {
204: 		D_ASSERT(vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
205: 		vector.validity.Set(0, !is_null);
206: 	}
207: 	static inline ValidityMask &Validity(Vector &vector) {
208: 		D_ASSERT(vector.GetVectorType() == VectorType::CONSTANT_VECTOR);
209: 		return vector.validity;
210: 	}
211: 
212: 	static const sel_t ZERO_VECTOR[STANDARD_VECTOR_SIZE];
213: 	static const SelectionVector ZERO_SELECTION_VECTOR;
214: };
215: 
216: struct DictionaryVector {
217: 	static inline const SelectionVector &SelVector(const Vector &vector) {
218: 		D_ASSERT(vector.GetVectorType() == VectorType::DICTIONARY_VECTOR);
219: 		return ((const DictionaryBuffer &)*vector.buffer).GetSelVector();
220: 	}
221: 	static inline SelectionVector &SelVector(Vector &vector) {
222: 		D_ASSERT(vector.GetVectorType() == VectorType::DICTIONARY_VECTOR);
223: 		return ((DictionaryBuffer &)*vector.buffer).GetSelVector();
224: 	}
225: 	static inline const Vector &Child(const Vector &vector) {
226: 		D_ASSERT(vector.GetVectorType() == VectorType::DICTIONARY_VECTOR);
227: 		return ((const VectorChildBuffer &)*vector.auxiliary).data;
228: 	}
229: 	static inline Vector &Child(Vector &vector) {
230: 		D_ASSERT(vector.GetVectorType() == VectorType::DICTIONARY_VECTOR);
231: 		return ((VectorChildBuffer &)*vector.auxiliary).data;
232: 	}
233: };
234: 
235: struct FlatVector {
236: 	static inline data_ptr_t GetData(Vector &vector) {
237: 		return ConstantVector::GetData(vector);
238: 	}
239: 	template <class T>
240: 	static inline const T *GetData(const Vector &vector) {
241: 		return ConstantVector::GetData<T>(vector);
242: 	}
243: 	template <class T>
244: 	static inline T *GetData(Vector &vector) {
245: 		return ConstantVector::GetData<T>(vector);
246: 	}
247: 	static inline void SetData(Vector &vector, data_ptr_t data) {
248: 		D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);
249: 		vector.data = data;
250: 	}
251: 	template <class T>
252: 	static inline T GetValue(Vector &vector, idx_t idx) {
253: 		D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);
254: 		return FlatVector::GetData<T>(vector)[idx];
255: 	}
256: 	static inline const ValidityMask &Validity(const Vector &vector) {
257: 		D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);
258: 		return vector.validity;
259: 	}
260: 	static inline ValidityMask &Validity(Vector &vector) {
261: 		D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);
262: 		return vector.validity;
263: 	}
264: 	static inline void SetValidity(Vector &vector, ValidityMask &new_validity) {
265: 		D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);
266: 		vector.validity.Initialize(new_validity);
267: 	}
268: 	static inline void SetNull(Vector &vector, idx_t idx, bool is_null) {
269: 		D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);
270: 		vector.validity.Set(idx, !is_null);
271: 	}
272: 	static inline bool IsNull(const Vector &vector, idx_t idx) {
273: 		D_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);
274: 		return !vector.validity.RowIsValid(idx);
275: 	}
276: 
277: 	static const sel_t INCREMENTAL_VECTOR[STANDARD_VECTOR_SIZE];
278: 	static const SelectionVector INCREMENTAL_SELECTION_VECTOR;
279: };
280: 
281: struct ListVector {
282: 	static const Vector &GetEntry(const Vector &vector);
283: 	static Vector &GetEntry(Vector &vector);
284: 	static idx_t GetListSize(const Vector &vector);
285: 	static void SetListSize(Vector &vec, idx_t size);
286: 	static bool HasEntry(const Vector &vector);
287: 	static void SetEntry(Vector &vector, unique_ptr<Vector> entry);
288: 	static void Append(Vector &target, const Vector &source, idx_t source_size, idx_t source_offset = 0);
289: 	static void Append(Vector &target, const Vector &source, const SelectionVector &sel, idx_t source_size,
290: 	                   idx_t source_offset = 0);
291: 	static void PushBack(Vector &target, Value &insert);
292: 	static void Initialize(Vector &vec);
293: 	static vector<idx_t> Search(Vector &list, Value &key, idx_t row);
294: 	static Value GetValuesFromOffsets(Vector &list, vector<idx_t> &offsets);
295: 	//! Share the entry of the other list vector
296: 	static void ReferenceEntry(Vector &vector, Vector &other);
297: };
298: 
299: struct StringVector {
300: 	//! Add a string to the string heap of the vector (auxiliary data)
301: 	static string_t AddString(Vector &vector, const char *data, idx_t len);
302: 	//! Add a string to the string heap of the vector (auxiliary data)
303: 	static string_t AddString(Vector &vector, const char *data);
304: 	//! Add a string to the string heap of the vector (auxiliary data)
305: 	static string_t AddString(Vector &vector, string_t data);
306: 	//! Add a string to the string heap of the vector (auxiliary data)
307: 	static string_t AddString(Vector &vector, const string &data);
308: 	//! Add a string or a blob to the string heap of the vector (auxiliary data)
309: 	//! This function is the same as ::AddString, except the added data does not need to be valid UTF8
310: 	static string_t AddStringOrBlob(Vector &vector, string_t data);
311: 	//! Allocates an empty string of the specified size, and returns a writable pointer that can be used to store the
312: 	//! result of an operation
313: 	static string_t EmptyString(Vector &vector, idx_t len);
314: 	//! Adds a reference to a handle that stores strings of this vector
315: 	static void AddHandle(Vector &vector, unique_ptr<BufferHandle> handle);
316: 	//! Adds a reference to an unspecified vector buffer that stores strings of this vector
317: 	static void AddBuffer(Vector &vector, buffer_ptr<VectorBuffer> buffer);
318: 	//! Add a reference from this vector to the string heap of the provided vector
319: 	static void AddHeapReference(Vector &vector, Vector &other);
320: };
321: 
322: struct StructVector {
323: 	static const vector<unique_ptr<Vector>> &GetEntries(const Vector &vector);
324: 	static vector<unique_ptr<Vector>> &GetEntries(Vector &vector);
325: };
326: 
327: struct SequenceVector {
328: 	static void GetSequence(const Vector &vector, int64_t &start, int64_t &increment) {
329: 		D_ASSERT(vector.GetVectorType() == VectorType::SEQUENCE_VECTOR);
330: 		auto data = (int64_t *)vector.buffer->GetData();
331: 		start = data[0];
332: 		increment = data[1];
333: 	}
334: };
335: 
336: } // namespace duckdb
[end of src/include/duckdb/common/types/vector.hpp]
[start of src/include/duckdb/planner/expression_binder.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/planner/expression_binder.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/exception.hpp"
12: #include "duckdb/parser/expression/bound_expression.hpp"
13: #include "duckdb/parser/parsed_expression.hpp"
14: #include "duckdb/parser/tokens.hpp"
15: #include "duckdb/planner/expression.hpp"
16: #include "duckdb/common/unordered_map.hpp"
17: 
18: namespace duckdb {
19: 
20: class Binder;
21: class ClientContext;
22: class QueryNode;
23: 
24: class ScalarFunctionCatalogEntry;
25: class AggregateFunctionCatalogEntry;
26: class MacroCatalogEntry;
27: class CatalogEntry;
28: class SimpleFunction;
29: 
30: struct MacroBinding;
31: 
32: struct BindResult {
33: 	explicit BindResult(string error) : error(error) {
34: 	}
35: 	explicit BindResult(unique_ptr<Expression> expr) : expression(move(expr)) {
36: 	}
37: 
38: 	bool HasError() {
39: 		return !error.empty();
40: 	}
41: 
42: 	unique_ptr<Expression> expression;
43: 	string error;
44: };
45: 
46: class ExpressionBinder {
47: public:
48: 	ExpressionBinder(Binder &binder, ClientContext &context, bool replace_binder = false);
49: 	virtual ~ExpressionBinder();
50: 
51: 	unique_ptr<Expression> Bind(unique_ptr<ParsedExpression> &expr, LogicalType *result_type = nullptr,
52: 	                            bool root_expression = true);
53: 
54: 	//! Returns whether or not any columns have been bound by the expression binder
55: 	bool BoundColumns() {
56: 		return bound_columns;
57: 	}
58: 
59: 	string Bind(unique_ptr<ParsedExpression> *expr, idx_t depth, bool root_expression = false);
60: 
61: 	// Bind table names to ColumnRefExpressions
62: 	static void BindTableNames(Binder &binder, ParsedExpression &expr,
63: 	                           unordered_map<string, idx_t> *alias_map = nullptr);
64: 	static unique_ptr<Expression> PushCollation(ClientContext &context, unique_ptr<Expression> source,
65: 	                                            const string &collation, bool equality_only = false);
66: 	static void TestCollation(ClientContext &context, const string &collation);
67: 
68: 	bool BindCorrelatedColumns(unique_ptr<ParsedExpression> &expr);
69: 
70: 	//! The target type that should result from the binder. If the result is not of this type, a cast to this type will
71: 	//! be added. Defaults to INVALID.
72: 	LogicalType target_type;
73: 
74: 	void BindChild(unique_ptr<ParsedExpression> &expr, idx_t depth, string &error);
75: 	static void ExtractCorrelatedExpressions(Binder &binder, Expression &expr);
76: 
77: protected:
78: 	virtual BindResult BindExpression(unique_ptr<ParsedExpression> *expr_ptr, idx_t depth,
79: 	                                  bool root_expression = false);
80: 
81: 	BindResult BindExpression(CaseExpression &expr, idx_t depth);
82: 	BindResult BindExpression(CollateExpression &expr, idx_t depth);
83: 	BindResult BindExpression(CastExpression &expr, idx_t depth);
84: 	BindResult BindExpression(ColumnRefExpression &expr, idx_t depth);
85: 	BindResult BindExpression(ComparisonExpression &expr, idx_t depth);
86: 	BindResult BindExpression(ConjunctionExpression &expr, idx_t depth);
87: 	BindResult BindExpression(ConstantExpression &expr, idx_t depth);
88: 	BindResult BindExpression(FunctionExpression &expr, idx_t depth, unique_ptr<ParsedExpression> *expr_ptr);
89: 	BindResult BindExpression(LambdaExpression &expr, idx_t depth);
90: 	BindResult BindExpression(OperatorExpression &expr, idx_t depth);
91: 	BindResult BindExpression(ParameterExpression &expr, idx_t depth);
92: 	BindResult BindExpression(PositionalReferenceExpression &ref, idx_t depth);
93: 	BindResult BindExpression(StarExpression &expr, idx_t depth);
94: 	BindResult BindExpression(SubqueryExpression &expr, idx_t depth);
95: 
96: protected:
97: 	virtual BindResult BindFunction(FunctionExpression &expr, ScalarFunctionCatalogEntry *function, idx_t depth);
98: 	virtual BindResult BindAggregate(FunctionExpression &expr, AggregateFunctionCatalogEntry *function, idx_t depth);
99: 	virtual BindResult BindUnnest(FunctionExpression &expr, idx_t depth);
100: 	virtual BindResult BindMacro(FunctionExpression &expr, MacroCatalogEntry *macro, idx_t depth,
101: 	                             unique_ptr<ParsedExpression> *expr_ptr);
102: 
103: 	virtual void ReplaceMacroParametersRecursive(unique_ptr<ParsedExpression> &expr);
104: 	virtual void ReplaceMacroParametersRecursive(ParsedExpression &expr, QueryNode &node);
105: 	virtual void ReplaceMacroParametersRecursive(ParsedExpression &expr, TableRef &ref);
106: 	virtual void CheckForSideEffects(FunctionExpression &function, idx_t depth, string &error);
107: 
108: 	virtual string UnsupportedAggregateMessage();
109: 	virtual string UnsupportedUnnestMessage();
110: 
111: 	Binder &binder;
112: 	ClientContext &context;
113: 	ExpressionBinder *stored_binder;
114: 	MacroBinding *macro_binding;
115: 	bool bound_columns = false;
116: };
117: 
118: } // namespace duckdb
[end of src/include/duckdb/planner/expression_binder.hpp]
[start of src/include/duckdb/storage/statistics/base_statistics.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/statistics/base_statistics.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/types.hpp"
13: #include "duckdb/common/operator/comparison_operators.hpp"
14: #include "duckdb/common/enums/expression_type.hpp"
15: 
16: namespace duckdb {
17: class Serializer;
18: class Deserializer;
19: class Vector;
20: class ValidityStatistics;
21: 
22: class BaseStatistics {
23: public:
24: 	explicit BaseStatistics(LogicalType type);
25: 	virtual ~BaseStatistics();
26: 
27: 	//! The type of the logical segment
28: 	LogicalType type;
29: 	//! The validity stats of the column (if any)
30: 	unique_ptr<BaseStatistics> validity_stats;
31: 
32: public:
33: 	bool CanHaveNull();
34: 
35: 	static unique_ptr<BaseStatistics> CreateEmpty(LogicalType type);
36: 
37: 	virtual void Merge(const BaseStatistics &other);
38: 	virtual unique_ptr<BaseStatistics> Copy();
39: 	virtual void Serialize(Serializer &serializer);
40: 	static unique_ptr<BaseStatistics> Deserialize(Deserializer &source, LogicalType type);
41: 	//! Verify that a vector does not violate the statistics
42: 	virtual void Verify(Vector &vector, idx_t count);
43: 
44: 	virtual string ToString();
45: };
46: 
47: } // namespace duckdb
[end of src/include/duckdb/storage/statistics/base_statistics.hpp]
[start of src/include/duckdb/storage/statistics/string_statistics.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/statistics/string_statistics.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/storage/statistics/base_statistics.hpp"
12: #include "duckdb/common/enums/filter_propagate_result.hpp"
13: 
14: namespace duckdb {
15: 
16: class StringStatistics : public BaseStatistics {
17: public:
18: 	constexpr static uint32_t MAX_STRING_MINMAX_SIZE = 8;
19: 
20: public:
21: 	explicit StringStatistics(LogicalType type);
22: 
23: 	//! The minimum value of the segment, potentially truncated
24: 	data_t min[MAX_STRING_MINMAX_SIZE];
25: 	//! The maximum value of the segment, potentially truncated
26: 	data_t max[MAX_STRING_MINMAX_SIZE];
27: 	//! Whether or not the column can contain unicode characters
28: 	bool has_unicode;
29: 	//! The maximum string length in bytes
30: 	uint32_t max_string_length;
31: 	//! Whether or not the segment contains any big strings in overflow blocks
32: 	bool has_overflow_strings;
33: 
34: public:
35: 	void Update(const string_t &value);
36: 	void Merge(const BaseStatistics &other) override;
37: 
38: 	unique_ptr<BaseStatistics> Copy() override;
39: 	void Serialize(Serializer &serializer) override;
40: 	static unique_ptr<BaseStatistics> Deserialize(Deserializer &source, LogicalType type);
41: 	void Verify(Vector &vector, idx_t count) override;
42: 
43: 	FilterPropagateResult CheckZonemap(ExpressionType comparison_type, const string &value);
44: 
45: 	string ToString() override;
46: };
47: 
48: } // namespace duckdb
[end of src/include/duckdb/storage/statistics/string_statistics.hpp]
[start of src/include/duckdb/storage/table/column_data.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/table/column_data.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/types/data_chunk.hpp"
12: #include "duckdb/storage/table/append_state.hpp"
13: #include "duckdb/storage/table/scan_state.hpp"
14: #include "duckdb/storage/table/persistent_segment.hpp"
15: #include "duckdb/storage/statistics/base_statistics.hpp"
16: #include "duckdb/storage/data_pointer.hpp"
17: #include "duckdb/storage/table/persistent_table_data.hpp"
18: #include "duckdb/storage/statistics/segment_statistics.hpp"
19: #include "duckdb/storage/table/column_checkpoint_state.hpp"
20: #include "duckdb/common/mutex.hpp"
21: 
22: namespace duckdb {
23: class ColumnData;
24: class DatabaseInstance;
25: class RowGroup;
26: class TableDataWriter;
27: class PersistentSegment;
28: class PersistentColumnData;
29: class Transaction;
30: 
31: struct DataTableInfo;
32: 
33: class ColumnData {
34: public:
35: 	ColumnData(DataTableInfo &info, idx_t column_index, idx_t start_row, LogicalType type, ColumnData *parent);
36: 	virtual ~ColumnData();
37: 
38: 	//! Table info for the column
39: 	DataTableInfo &info;
40: 	//! The column index of the column, either within the parent table or within the parent
41: 	idx_t column_index;
42: 	//! The start row
43: 	idx_t start;
44: 	//! The type of the column
45: 	LogicalType type;
46: 	//! The parent column (if any)
47: 	ColumnData *parent;
48: 
49: public:
50: 	virtual bool CheckZonemap(ColumnScanState &state, TableFilter &filter) = 0;
51: 
52: 	DatabaseInstance &GetDatabase() const;
53: 	DataTableInfo &GetTableInfo() const;
54: 
55: 	//! The root type of the column
56: 	const LogicalType &RootType() const;
57: 
58: 	//! Initialize a scan of the column
59: 	virtual void InitializeScan(ColumnScanState &state) = 0;
60: 	//! Initialize a scan starting at the specified offset
61: 	virtual void InitializeScanWithOffset(ColumnScanState &state, idx_t row_idx) = 0;
62: 	//! Scan the next vector from the column
63: 	virtual void Scan(Transaction &transaction, idx_t vector_index, ColumnScanState &state, Vector &result);
64: 	virtual void ScanCommitted(idx_t vector_index, ColumnScanState &state, Vector &result, bool allow_updates);
65: 	//! Initialize an appending phase for this column
66: 	virtual void InitializeAppend(ColumnAppendState &state);
67: 	//! Append a vector of type [type] to the end of the column
68: 	void Append(BaseStatistics &stats, ColumnAppendState &state, Vector &vector, idx_t count);
69: 	virtual void AppendData(BaseStatistics &stats, ColumnAppendState &state, VectorData &vdata, idx_t count);
70: 	//! Revert a set of appends to the ColumnData
71: 	virtual void RevertAppend(row_t start_row);
72: 
73: 	//! Fetch the vector from the column data that belongs to this specific row
74: 	virtual void Fetch(ColumnScanState &state, row_t row_id, Vector &result);
75: 	//! Fetch a specific row id and append it to the vector
76: 	virtual void FetchRow(Transaction &transaction, ColumnFetchState &state, row_t row_id, Vector &result,
77: 	                      idx_t result_idx);
78: 
79: 	virtual void Update(Transaction &transaction, idx_t column_index, Vector &update_vector, row_t *row_ids,
80: 	                    idx_t update_count);
81: 	virtual void UpdateColumn(Transaction &transaction, const vector<column_t> &column_path, Vector &update_vector,
82: 	                          row_t *row_ids, idx_t update_count, idx_t depth);
83: 	virtual unique_ptr<BaseStatistics> GetUpdateStatistics();
84: 
85: 	virtual void CommitDropColumn();
86: 
87: 	virtual unique_ptr<ColumnCheckpointState> CreateCheckpointState(RowGroup &row_group, TableDataWriter &writer);
88: 	virtual unique_ptr<ColumnCheckpointState> Checkpoint(RowGroup &row_group, TableDataWriter &writer,
89: 	                                                     idx_t column_idx);
90: 
91: 	virtual void Initialize(PersistentColumnData &column_data);
92: 
93: 	static void BaseDeserialize(DatabaseInstance &db, Deserializer &source, const LogicalType &type,
94: 	                            ColumnData &result);
95: 	static shared_ptr<ColumnData> Deserialize(DataTableInfo &info, idx_t column_index, idx_t start_row,
96: 	                                          Deserializer &source, const LogicalType &type);
97: 
98: 	virtual void GetStorageInfo(idx_t row_group_index, vector<idx_t> col_path, vector<vector<Value>> &result);
99: 	virtual void Verify(RowGroup &parent);
100: 
101: protected:
102: 	//! Append a transient segment
103: 	void AppendTransientSegment(idx_t start_row);
104: 
105: 	//! Scans a base vector from the column
106: 	void ScanVector(ColumnScanState &state, Vector &result);
107: 	//! Scans a vector from the column merged with any potential updates
108: 	//! If ALLOW_UPDATES is set to false, the function will instead throw an exception if any updates are found
109: 	template <bool SCAN_COMMITTED, bool ALLOW_UPDATES>
110: 	void ScanVector(Transaction *transaction, idx_t vector_index, ColumnScanState &state, Vector &result);
111: 
112: protected:
113: 	//! The segments holding the data of this column segment
114: 	SegmentTree data;
115: 	//! The lock for the updates
116: 	mutex update_lock;
117: 	//! The updates for this column segment
118: 	unique_ptr<UpdateSegment> updates;
119: };
120: 
121: } // namespace duckdb
[end of src/include/duckdb/storage/table/column_data.hpp]
[start of src/include/duckdb/storage/table/persistent_table_data.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/table/persistent_table_data.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/constants.hpp"
12: #include "duckdb/common/vector.hpp"
13: #include "duckdb/storage/table/segment_tree.hpp"
14: #include "duckdb/storage/data_pointer.hpp"
15: 
16: namespace duckdb {
17: class BaseStatistics;
18: class PersistentSegment;
19: 
20: class PersistentColumnData {
21: public:
22: 	virtual ~PersistentColumnData();
23: 
24: 	vector<unique_ptr<PersistentSegment>> segments;
25: 	unique_ptr<BaseStatistics> stats;
26: 	idx_t total_rows = 0;
27: };
28: 
29: class StandardPersistentColumnData : public PersistentColumnData {
30: public:
31: 	unique_ptr<PersistentColumnData> validity;
32: };
33: 
34: class PersistentTableData {
35: public:
36: 	explicit PersistentTableData(idx_t column_count);
37: 	~PersistentTableData();
38: 
39: 	vector<RowGroupPointer> row_groups;
40: 	vector<unique_ptr<BaseStatistics>> column_stats;
41: };
42: 
43: } // namespace duckdb
[end of src/include/duckdb/storage/table/persistent_table_data.hpp]
[start of src/include/duckdb/storage/table/standard_column_data.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/table/standard_column_data.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/storage/table/column_data.hpp"
12: #include "duckdb/storage/table/validity_column_data.hpp"
13: 
14: namespace duckdb {
15: 
16: //! Standard column data represents a regular flat column (e.g. a column of type INTEGER or STRING)
17: class StandardColumnData : public ColumnData {
18: public:
19: 	StandardColumnData(DataTableInfo &info, idx_t column_index, idx_t start_row, LogicalType type,
20: 	                   ColumnData *parent = nullptr);
21: 
22: 	//! The validity column data
23: 	ValidityColumnData validity;
24: 
25: public:
26: 	bool CheckZonemap(ColumnScanState &state, TableFilter &filter) override;
27: 	void InitializeScan(ColumnScanState &state) override;
28: 	void InitializeScanWithOffset(ColumnScanState &state, idx_t row_idx) override;
29: 	void Scan(Transaction &transaction, idx_t vector_index, ColumnScanState &state, Vector &result) override;
30: 	void ScanCommitted(idx_t vector_index, ColumnScanState &state, Vector &result, bool allow_updates) override;
31: 	void InitializeAppend(ColumnAppendState &state) override;
32: 	void AppendData(BaseStatistics &stats, ColumnAppendState &state, VectorData &vdata, idx_t count) override;
33: 	void RevertAppend(row_t start_row) override;
34: 	void Fetch(ColumnScanState &state, row_t row_id, Vector &result) override;
35: 	void FetchRow(Transaction &transaction, ColumnFetchState &state, row_t row_id, Vector &result,
36: 	              idx_t result_idx) override;
37: 	void Update(Transaction &transaction, idx_t column_index, Vector &update_vector, row_t *row_ids,
38: 	            idx_t update_count) override;
39: 	void UpdateColumn(Transaction &transaction, const vector<column_t> &column_path, Vector &update_vector,
40: 	                  row_t *row_ids, idx_t update_count, idx_t depth) override;
41: 	unique_ptr<BaseStatistics> GetUpdateStatistics() override;
42: 
43: 	void CommitDropColumn() override;
44: 	void Initialize(PersistentColumnData &column_data) override;
45: 
46: 	unique_ptr<ColumnCheckpointState> CreateCheckpointState(RowGroup &row_group, TableDataWriter &writer) override;
47: 	unique_ptr<ColumnCheckpointState> Checkpoint(RowGroup &row_group, TableDataWriter &writer,
48: 	                                             idx_t column_idx) override;
49: 	static shared_ptr<ColumnData> Deserialize(DataTableInfo &info, idx_t column_index, idx_t start_row,
50: 	                                          Deserializer &source, const LogicalType &type);
51: 
52: 	void GetStorageInfo(idx_t row_group_index, vector<idx_t> col_path, vector<vector<Value>> &result) override;
53: 
54: private:
55: 	template <bool SCAN_COMMITTED, bool ALLOW_UPDATES>
56: 	void TemplatedScan(Transaction *transaction, ColumnScanState &state, Vector &result);
57: };
58: 
59: } // namespace duckdb
[end of src/include/duckdb/storage/table/standard_column_data.hpp]
[start of src/planner/expression_binder.cpp]
1: #include "duckdb/planner/expression_binder.hpp"
2: 
3: #include "duckdb/parser/expression/columnref_expression.hpp"
4: #include "duckdb/parser/expression/positional_reference_expression.hpp"
5: #include "duckdb/parser/expression/subquery_expression.hpp"
6: #include "duckdb/parser/parsed_expression_iterator.hpp"
7: #include "duckdb/planner/binder.hpp"
8: #include "duckdb/planner/expression/bound_cast_expression.hpp"
9: #include "duckdb/planner/expression/bound_default_expression.hpp"
10: #include "duckdb/planner/expression/bound_parameter_expression.hpp"
11: #include "duckdb/planner/expression/bound_subquery_expression.hpp"
12: #include "duckdb/planner/expression_iterator.hpp"
13: 
14: namespace duckdb {
15: 
16: ExpressionBinder::ExpressionBinder(Binder &binder, ClientContext &context, bool replace_binder)
17:     : binder(binder), context(context), stored_binder(nullptr) {
18: 	if (replace_binder) {
19: 		stored_binder = binder.GetActiveBinder();
20: 		binder.SetActiveBinder(this);
21: 	} else {
22: 		binder.PushExpressionBinder(this);
23: 	}
24: }
25: 
26: ExpressionBinder::~ExpressionBinder() {
27: 	if (binder.HasActiveBinder()) {
28: 		if (stored_binder) {
29: 			binder.SetActiveBinder(stored_binder);
30: 		} else {
31: 			binder.PopExpressionBinder();
32: 		}
33: 	}
34: }
35: 
36: BindResult ExpressionBinder::BindExpression(unique_ptr<ParsedExpression> *expr, idx_t depth, bool root_expression) {
37: 	auto &expr_ref = **expr;
38: 	switch (expr_ref.expression_class) {
39: 	case ExpressionClass::CASE:
40: 		return BindExpression((CaseExpression &)expr_ref, depth);
41: 	case ExpressionClass::CAST:
42: 		return BindExpression((CastExpression &)expr_ref, depth);
43: 	case ExpressionClass::COLLATE:
44: 		return BindExpression((CollateExpression &)expr_ref, depth);
45: 	case ExpressionClass::COLUMN_REF:
46: 		return BindExpression((ColumnRefExpression &)expr_ref, depth);
47: 	case ExpressionClass::COMPARISON:
48: 		return BindExpression((ComparisonExpression &)expr_ref, depth);
49: 	case ExpressionClass::CONJUNCTION:
50: 		return BindExpression((ConjunctionExpression &)expr_ref, depth);
51: 	case ExpressionClass::CONSTANT:
52: 		return BindExpression((ConstantExpression &)expr_ref, depth);
53: 	case ExpressionClass::FUNCTION:
54: 		// binding function expression has extra parameter needed for macro's
55: 		return BindExpression((FunctionExpression &)expr_ref, depth, expr);
56: 	case ExpressionClass::LAMBDA:
57: 		return BindExpression((LambdaExpression &)expr_ref, depth);
58: 	case ExpressionClass::OPERATOR:
59: 		return BindExpression((OperatorExpression &)expr_ref, depth);
60: 	case ExpressionClass::SUBQUERY:
61: 		return BindExpression((SubqueryExpression &)expr_ref, depth);
62: 	case ExpressionClass::PARAMETER:
63: 		return BindExpression((ParameterExpression &)expr_ref, depth);
64: 	case ExpressionClass::POSITIONAL_REFERENCE:
65: 		return BindExpression((PositionalReferenceExpression &)expr_ref, depth);
66: 	default:
67: 		throw NotImplementedException("Unimplemented expression class");
68: 	}
69: }
70: 
71: bool ExpressionBinder::BindCorrelatedColumns(unique_ptr<ParsedExpression> &expr) {
72: 	// try to bind in one of the outer queries, if the binding error occurred in a subquery
73: 	auto &active_binders = binder.GetActiveBinders();
74: 	// make a copy of the set of binders, so we can restore it later
75: 	auto binders = active_binders;
76: 	active_binders.pop_back();
77: 	idx_t depth = 1;
78: 	bool success = false;
79: 	while (!active_binders.empty()) {
80: 		auto &next_binder = active_binders.back();
81: 		ExpressionBinder::BindTableNames(next_binder->binder, *expr);
82: 		auto bind_result = next_binder->Bind(&expr, depth);
83: 		if (bind_result.empty()) {
84: 			success = true;
85: 			break;
86: 		}
87: 		depth++;
88: 		active_binders.pop_back();
89: 	}
90: 	active_binders = binders;
91: 	return success;
92: }
93: 
94: void ExpressionBinder::BindChild(unique_ptr<ParsedExpression> &expr, idx_t depth, string &error) {
95: 	if (expr) {
96: 		string bind_error = Bind(&expr, depth);
97: 		if (error.empty()) {
98: 			error = bind_error;
99: 		}
100: 	}
101: }
102: 
103: void ExpressionBinder::ExtractCorrelatedExpressions(Binder &binder, Expression &expr) {
104: 	if (expr.type == ExpressionType::BOUND_COLUMN_REF) {
105: 		auto &bound_colref = (BoundColumnRefExpression &)expr;
106: 		if (bound_colref.depth > 0) {
107: 			binder.AddCorrelatedColumn(CorrelatedColumnInfo(bound_colref));
108: 		}
109: 	}
110: 	ExpressionIterator::EnumerateChildren(expr,
111: 	                                      [&](Expression &child) { ExtractCorrelatedExpressions(binder, child); });
112: }
113: 
114: unique_ptr<Expression> ExpressionBinder::Bind(unique_ptr<ParsedExpression> &expr, LogicalType *result_type,
115:                                               bool root_expression) {
116: 	// bind the main expression
117: 	auto error_msg = Bind(&expr, 0, root_expression);
118: 	if (!error_msg.empty()) {
119: 		// failed to bind: try to bind correlated columns in the expression (if any)
120: 		bool success = BindCorrelatedColumns(expr);
121: 		if (!success) {
122: 			throw BinderException(error_msg);
123: 		}
124: 		auto bound_expr = (BoundExpression *)expr.get();
125: 		ExtractCorrelatedExpressions(binder, *bound_expr->expr);
126: 	}
127: 	D_ASSERT(expr->expression_class == ExpressionClass::BOUND_EXPRESSION);
128: 	auto bound_expr = (BoundExpression *)expr.get();
129: 	unique_ptr<Expression> result = move(bound_expr->expr);
130: 	if (target_type.id() != LogicalTypeId::INVALID) {
131: 		// the binder has a specific target type: add a cast to that type
132: 		result = BoundCastExpression::AddCastToType(move(result), target_type);
133: 	} else {
134: 		if (result->return_type.id() == LogicalTypeId::SQLNULL) {
135: 			// SQL NULL type is only used internally in the binder
136: 			// cast to INTEGER if we encounter it outside of the binder
137: 			result = BoundCastExpression::AddCastToType(move(result), LogicalType::INTEGER);
138: 		}
139: 	}
140: 	if (result_type) {
141: 		*result_type = result->return_type;
142: 	}
143: 	return result;
144: }
145: 
146: string ExpressionBinder::Bind(unique_ptr<ParsedExpression> *expr, idx_t depth, bool root_expression) {
147: 	// bind the node, but only if it has not been bound yet
148: 	auto &expression = **expr;
149: 	auto alias = expression.alias;
150: 	if (expression.GetExpressionClass() == ExpressionClass::BOUND_EXPRESSION) {
151: 		// already bound, don't bind it again
152: 		return string();
153: 	}
154: 	// bind the expression
155: 	BindResult result = BindExpression(expr, depth, root_expression);
156: 	if (result.HasError()) {
157: 		return result.error;
158: 	} else {
159: 		// successfully bound: replace the node with a BoundExpression
160: 		*expr = make_unique<BoundExpression>(move(result.expression), move(*expr));
161: 		auto be = (BoundExpression *)expr->get();
162: 		D_ASSERT(be);
163: 		be->alias = alias;
164: 		if (!alias.empty()) {
165: 			be->expr->alias = alias;
166: 		}
167: 		return string();
168: 	}
169: }
170: 
171: void ExpressionBinder::BindTableNames(Binder &binder, ParsedExpression &expr, unordered_map<string, idx_t> *alias_map) {
172: 	if (expr.type == ExpressionType::COLUMN_REF) {
173: 		auto &colref = (ColumnRefExpression &)expr;
174: 		if (colref.table_name.empty()) {
175: 			// no table name: find a binding that contains this
176: 			if (binder.macro_binding != nullptr && binder.macro_binding->HasMatchingBinding(colref.column_name)) {
177: 				// macro parameters get priority
178: 				colref.table_name = binder.macro_binding->alias;
179: 			} else if (alias_map && alias_map->find(colref.column_name) != alias_map->end()) {
180: 				// alias: leave unqualified
181: 			} else {
182: 				colref.table_name = binder.bind_context.GetMatchingBinding(colref.column_name);
183: 			}
184: 		}
185: 		binder.bind_context.BindColumn(colref, 0);
186: 	} else if (expr.type == ExpressionType::POSITIONAL_REFERENCE) {
187: 		auto &ref = (PositionalReferenceExpression &)expr;
188: 		if (ref.alias.empty()) {
189: 			string table_name, column_name;
190: 			auto error = binder.bind_context.BindColumn(ref, table_name, column_name);
191: 			if (error.empty()) {
192: 				ref.alias = column_name;
193: 			}
194: 		}
195: 	}
196: 	ParsedExpressionIterator::EnumerateChildren(
197: 	    expr, [&](const ParsedExpression &child) { BindTableNames(binder, (ParsedExpression &)child, alias_map); });
198: }
199: 
200: } // namespace duckdb
[end of src/planner/expression_binder.cpp]
[start of src/storage/numeric_segment.cpp]
1: #include "duckdb/storage/numeric_segment.hpp"
2: #include "duckdb/storage/buffer_manager.hpp"
3: #include "duckdb/common/types/vector.hpp"
4: #include "duckdb/storage/table/append_state.hpp"
5: #include "duckdb/transaction/update_info.hpp"
6: #include "duckdb/transaction/transaction.hpp"
7: #include "duckdb/common/vector_operations/vector_operations.hpp"
8: #include "duckdb/storage/data_table.hpp"
9: #include "duckdb/common/vector_size.hpp"
10: #include "duckdb/storage/statistics/numeric_statistics.hpp"
11: #include "duckdb/planner/table_filter.hpp"
12: 
13: namespace duckdb {
14: 
15: static NumericSegment::append_function_t GetAppendFunction(PhysicalType type);
16: 
17: NumericSegment::NumericSegment(DatabaseInstance &db, PhysicalType type, idx_t row_start, block_id_t block_id)
18:     : UncompressedSegment(db, type, row_start) {
19: 	// set up the different functions for this type of segment
20: 	this->append_function = GetAppendFunction(type);
21: 
22: 	// figure out how many vectors we want to store in this block
23: 	this->type_size = GetTypeIdSize(type);
24: 	this->max_tuple_count = Storage::BLOCK_SIZE / type_size;
25: 
26: 	auto &buffer_manager = BufferManager::GetBufferManager(db);
27: 	if (block_id == INVALID_BLOCK) {
28: 		// no block id specified: allocate a buffer for the uncompressed segment
29: 		this->block = buffer_manager.RegisterMemory(Storage::BLOCK_ALLOC_SIZE, false);
30: 	} else {
31: 		this->block = buffer_manager.RegisterBlock(block_id);
32: 	}
33: }
34: 
35: //===--------------------------------------------------------------------===//
36: // Scan
37: //===--------------------------------------------------------------------===//
38: void NumericSegment::InitializeScan(ColumnScanState &state) {
39: 	// pin the primary buffer
40: 	auto &buffer_manager = BufferManager::GetBufferManager(db);
41: 	state.primary_handle = buffer_manager.Pin(block);
42: }
43: 
44: //===--------------------------------------------------------------------===//
45: // Scan base data
46: //===--------------------------------------------------------------------===//
47: void NumericSegment::Scan(ColumnScanState &state, idx_t start, idx_t scan_count, Vector &result, idx_t result_offset) {
48: 	D_ASSERT(start <= tuple_count);
49: 	D_ASSERT(start + scan_count <= tuple_count);
50: 
51: 	auto data = state.primary_handle->node->buffer;
52: 	auto source_data = data + start * type_size;
53: 
54: 	// copy the data from the base table
55: 	result.SetVectorType(VectorType::FLAT_VECTOR);
56: 	memcpy(FlatVector::GetData(result) + result_offset * type_size, source_data, scan_count * type_size);
57: }
58: 
59: //===--------------------------------------------------------------------===//
60: // Fetch
61: //===--------------------------------------------------------------------===//
62: void NumericSegment::FetchRow(ColumnFetchState &state, row_t row_id, Vector &result, idx_t result_idx) {
63: 	auto &buffer_manager = BufferManager::GetBufferManager(db);
64: 	auto handle = buffer_manager.Pin(block);
65: 
66: 	// first fetch the data from the base table
67: 	auto data_ptr = handle->node->buffer + row_id * type_size;
68: 
69: 	memcpy(FlatVector::GetData(result) + result_idx * type_size, data_ptr, type_size);
70: }
71: 
72: //===--------------------------------------------------------------------===//
73: // Append
74: //===--------------------------------------------------------------------===//
75: idx_t NumericSegment::Append(SegmentStatistics &stats, VectorData &data, idx_t offset, idx_t count) {
76: 	auto &buffer_manager = BufferManager::GetBufferManager(db);
77: 	auto handle = buffer_manager.Pin(block);
78: 
79: 	auto target_ptr = handle->node->buffer;
80: 	idx_t copy_count = MinValue<idx_t>(count, max_tuple_count - tuple_count);
81: 
82: 	append_function(stats, target_ptr, tuple_count, data, offset, copy_count);
83: 	tuple_count += copy_count;
84: 	return copy_count;
85: }
86: 
87: //===--------------------------------------------------------------------===//
88: // Append
89: //===--------------------------------------------------------------------===//
90: template <class T>
91: static void AppendLoop(SegmentStatistics &stats, data_ptr_t target, idx_t target_offset, VectorData &adata,
92:                        idx_t offset, idx_t count) {
93: 	auto sdata = (T *)adata.data;
94: 	auto tdata = (T *)target;
95: 	if (!adata.validity.AllValid()) {
96: 		for (idx_t i = 0; i < count; i++) {
97: 			auto source_idx = adata.sel->get_index(offset + i);
98: 			auto target_idx = target_offset + i;
99: 			bool is_null = !adata.validity.RowIsValid(source_idx);
100: 			if (!is_null) {
101: 				NumericStatistics::Update<T>(stats, sdata[source_idx]);
102: 				tdata[target_idx] = sdata[source_idx];
103: 			}
104: 		}
105: 	} else {
106: 		for (idx_t i = 0; i < count; i++) {
107: 			auto source_idx = adata.sel->get_index(offset + i);
108: 			auto target_idx = target_offset + i;
109: 			NumericStatistics::Update<T>(stats, sdata[source_idx]);
110: 			tdata[target_idx] = sdata[source_idx];
111: 		}
112: 	}
113: }
114: 
115: static NumericSegment::append_function_t GetAppendFunction(PhysicalType type) {
116: 	switch (type) {
117: 	case PhysicalType::BOOL:
118: 	case PhysicalType::INT8:
119: 		return AppendLoop<int8_t>;
120: 	case PhysicalType::INT16:
121: 		return AppendLoop<int16_t>;
122: 	case PhysicalType::INT32:
123: 		return AppendLoop<int32_t>;
124: 	case PhysicalType::INT64:
125: 		return AppendLoop<int64_t>;
126: 	case PhysicalType::UINT8:
127: 		return AppendLoop<uint8_t>;
128: 	case PhysicalType::UINT16:
129: 		return AppendLoop<uint16_t>;
130: 	case PhysicalType::UINT32:
131: 		return AppendLoop<uint32_t>;
132: 	case PhysicalType::UINT64:
133: 		return AppendLoop<uint64_t>;
134: 	case PhysicalType::INT128:
135: 		return AppendLoop<hugeint_t>;
136: 	case PhysicalType::FLOAT:
137: 		return AppendLoop<float>;
138: 	case PhysicalType::DOUBLE:
139: 		return AppendLoop<double>;
140: 	case PhysicalType::INTERVAL:
141: 		return AppendLoop<interval_t>;
142: 	default:
143: 		throw NotImplementedException("Unimplemented type for uncompressed segment");
144: 	}
145: }
146: 
147: } // namespace duckdb
[end of src/storage/numeric_segment.cpp]
[start of src/storage/statistics/CMakeLists.txt]
1: add_library_unity(
2:   duckdb_storage_statistics
3:   OBJECT
4:   base_statistics.cpp
5:   numeric_statistics.cpp
6:   segment_statistics.cpp
7:   string_statistics.cpp
8:   validity_statistics.cpp)
9: set(ALL_OBJECT_FILES
10:     ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:duckdb_storage_statistics>
11:     PARENT_SCOPE)
[end of src/storage/statistics/CMakeLists.txt]
[start of src/storage/statistics/base_statistics.cpp]
1: #include "duckdb/storage/statistics/numeric_statistics.hpp"
2: #include "duckdb/storage/statistics/string_statistics.hpp"
3: #include "duckdb/common/serializer.hpp"
4: #include "duckdb/common/exception.hpp"
5: #include "duckdb/common/string_util.hpp"
6: #include "duckdb/storage/statistics/validity_statistics.hpp"
7: #include "duckdb/common/types/vector.hpp"
8: 
9: namespace duckdb {
10: 
11: BaseStatistics::BaseStatistics(LogicalType type) : type(move(type)) {
12: }
13: 
14: BaseStatistics::~BaseStatistics() {
15: }
16: 
17: bool BaseStatistics::CanHaveNull() {
18: 	if (!validity_stats) {
19: 		// we don't know
20: 		// solid maybe
21: 		return true;
22: 	}
23: 	return ((ValidityStatistics &)*validity_stats).has_null;
24: }
25: 
26: unique_ptr<BaseStatistics> BaseStatistics::Copy() {
27: 	auto statistics = make_unique<BaseStatistics>(type);
28: 	if (validity_stats) {
29: 		statistics->validity_stats = validity_stats->Copy();
30: 	}
31: 	return statistics;
32: }
33: 
34: void BaseStatistics::Merge(const BaseStatistics &other) {
35: 	D_ASSERT(type == other.type);
36: 	if (other.validity_stats) {
37: 		if (validity_stats) {
38: 			validity_stats->Merge(*other.validity_stats);
39: 		} else {
40: 			validity_stats = other.validity_stats->Copy();
41: 		}
42: 	}
43: }
44: 
45: unique_ptr<BaseStatistics> BaseStatistics::CreateEmpty(LogicalType type) {
46: 	switch (type.InternalType()) {
47: 	case PhysicalType::BIT:
48: 		return make_unique<ValidityStatistics>();
49: 	case PhysicalType::BOOL:
50: 	case PhysicalType::INT8:
51: 	case PhysicalType::INT16:
52: 	case PhysicalType::INT32:
53: 	case PhysicalType::INT64:
54: 	case PhysicalType::UINT8:
55: 	case PhysicalType::UINT16:
56: 	case PhysicalType::UINT32:
57: 	case PhysicalType::UINT64:
58: 	case PhysicalType::INT128:
59: 	case PhysicalType::FLOAT:
60: 	case PhysicalType::DOUBLE:
61: 		return make_unique<NumericStatistics>(move(type));
62: 	case PhysicalType::VARCHAR:
63: 		return make_unique<StringStatistics>(move(type));
64: 	case PhysicalType::INTERVAL:
65: 	default:
66: 		return make_unique<BaseStatistics>(move(type));
67: 	}
68: }
69: 
70: void BaseStatistics::Serialize(Serializer &serializer) {
71: 	serializer.Write<bool>(CanHaveNull());
72: }
73: 
74: unique_ptr<BaseStatistics> BaseStatistics::Deserialize(Deserializer &source, LogicalType type) {
75: 	bool can_have_null = source.Read<bool>();
76: 	unique_ptr<BaseStatistics> result;
77: 	switch (type.InternalType()) {
78: 	case PhysicalType::BIT:
79: 		return ValidityStatistics::Deserialize(source);
80: 	case PhysicalType::BOOL:
81: 	case PhysicalType::INT8:
82: 	case PhysicalType::INT16:
83: 	case PhysicalType::INT32:
84: 	case PhysicalType::INT64:
85: 	case PhysicalType::UINT8:
86: 	case PhysicalType::UINT16:
87: 	case PhysicalType::UINT32:
88: 	case PhysicalType::UINT64:
89: 	case PhysicalType::INT128:
90: 	case PhysicalType::FLOAT:
91: 	case PhysicalType::DOUBLE:
92: 		result = NumericStatistics::Deserialize(source, move(type));
93: 		break;
94: 	case PhysicalType::VARCHAR:
95: 		result = StringStatistics::Deserialize(source, move(type));
96: 		break;
97: 	case PhysicalType::INTERVAL:
98: 		result = make_unique<BaseStatistics>(move(type));
99: 		break;
100: 	default:
101: 		throw InternalException("Unimplemented type for statistics deserialization");
102: 	}
103: 	if (!can_have_null) {
104: 		result->validity_stats = make_unique<ValidityStatistics>(can_have_null);
105: 	}
106: 	return result;
107: }
108: 
109: string BaseStatistics::ToString() {
110: 	return StringUtil::Format("Base Statistics %s", validity_stats ? validity_stats->ToString() : "[]");
111: }
112: 
113: void BaseStatistics::Verify(Vector &vector, idx_t count) {
114: 	D_ASSERT(vector.GetType() == this->type);
115: 	if (!validity_stats) {
116: 		validity_stats->Verify(vector, count);
117: 	}
118: }
119: 
120: } // namespace duckdb
[end of src/storage/statistics/base_statistics.cpp]
[start of src/storage/statistics/numeric_statistics.cpp]
1: #include "duckdb/storage/statistics/numeric_statistics.hpp"
2: #include "duckdb/common/types/vector.hpp"
3: #include "duckdb/common/operator/comparison_operators.hpp"
4: 
5: namespace duckdb {
6: 
7: template <>
8: void NumericStatistics::Update<int8_t>(SegmentStatistics &stats, int8_t new_value) {
9: 	auto &nstats = (NumericStatistics &)*stats.statistics;
10: 	UpdateValue<int8_t>(new_value, nstats.min.value_.tinyint, nstats.max.value_.tinyint);
11: }
12: 
13: template <>
14: void NumericStatistics::Update<int16_t>(SegmentStatistics &stats, int16_t new_value) {
15: 	auto &nstats = (NumericStatistics &)*stats.statistics;
16: 	UpdateValue<int16_t>(new_value, nstats.min.value_.smallint, nstats.max.value_.smallint);
17: }
18: 
19: template <>
20: void NumericStatistics::Update<int32_t>(SegmentStatistics &stats, int32_t new_value) {
21: 	auto &nstats = (NumericStatistics &)*stats.statistics;
22: 	UpdateValue<int32_t>(new_value, nstats.min.value_.integer, nstats.max.value_.integer);
23: }
24: 
25: template <>
26: void NumericStatistics::Update<int64_t>(SegmentStatistics &stats, int64_t new_value) {
27: 	auto &nstats = (NumericStatistics &)*stats.statistics;
28: 	UpdateValue<int64_t>(new_value, nstats.min.value_.bigint, nstats.max.value_.bigint);
29: }
30: 
31: template <>
32: void NumericStatistics::Update<uint8_t>(SegmentStatistics &stats, uint8_t new_value) {
33: 	auto &nstats = (NumericStatistics &)*stats.statistics;
34: 	UpdateValue<uint8_t>(new_value, nstats.min.value_.utinyint, nstats.max.value_.utinyint);
35: }
36: 
37: template <>
38: void NumericStatistics::Update<uint16_t>(SegmentStatistics &stats, uint16_t new_value) {
39: 	auto &nstats = (NumericStatistics &)*stats.statistics;
40: 	UpdateValue<uint16_t>(new_value, nstats.min.value_.usmallint, nstats.max.value_.usmallint);
41: }
42: 
43: template <>
44: void NumericStatistics::Update<uint32_t>(SegmentStatistics &stats, uint32_t new_value) {
45: 	auto &nstats = (NumericStatistics &)*stats.statistics;
46: 	UpdateValue<uint32_t>(new_value, nstats.min.value_.uinteger, nstats.max.value_.uinteger);
47: }
48: 
49: template <>
50: void NumericStatistics::Update<uint64_t>(SegmentStatistics &stats, uint64_t new_value) {
51: 	auto &nstats = (NumericStatistics &)*stats.statistics;
52: 	UpdateValue<uint64_t>(new_value, nstats.min.value_.ubigint, nstats.max.value_.ubigint);
53: }
54: 
55: template <>
56: void NumericStatistics::Update<hugeint_t>(SegmentStatistics &stats, hugeint_t new_value) {
57: 	auto &nstats = (NumericStatistics &)*stats.statistics;
58: 	UpdateValue<hugeint_t>(new_value, nstats.min.value_.hugeint, nstats.max.value_.hugeint);
59: }
60: 
61: template <>
62: void NumericStatistics::Update<float>(SegmentStatistics &stats, float new_value) {
63: 	auto &nstats = (NumericStatistics &)*stats.statistics;
64: 	UpdateValue<float>(new_value, nstats.min.value_.float_, nstats.max.value_.float_);
65: }
66: 
67: template <>
68: void NumericStatistics::Update<double>(SegmentStatistics &stats, double new_value) {
69: 	auto &nstats = (NumericStatistics &)*stats.statistics;
70: 	UpdateValue<double>(new_value, nstats.min.value_.double_, nstats.max.value_.double_);
71: }
72: 
73: template <>
74: void NumericStatistics::Update<interval_t>(SegmentStatistics &stats, interval_t new_value) {
75: }
76: 
77: NumericStatistics::NumericStatistics(LogicalType type_p) : BaseStatistics(move(type_p)) {
78: 	min = Value::MaximumValue(type);
79: 	max = Value::MinimumValue(type);
80: }
81: 
82: NumericStatistics::NumericStatistics(LogicalType type_p, Value min_p, Value max_p)
83:     : BaseStatistics(move(type_p)), min(move(min_p)), max(move(max_p)) {
84: }
85: 
86: void NumericStatistics::Merge(const BaseStatistics &other_p) {
87: 	BaseStatistics::Merge(other_p);
88: 	auto &other = (const NumericStatistics &)other_p;
89: 	if (other.min < min) {
90: 		min = other.min;
91: 	}
92: 	if (other.max > max) {
93: 		max = other.max;
94: 	}
95: }
96: 
97: FilterPropagateResult NumericStatistics::CheckZonemap(ExpressionType comparison_type, const Value &constant) {
98: 	switch (comparison_type) {
99: 	case ExpressionType::COMPARE_EQUAL:
100: 		if (constant == min && constant == max) {
101: 			return FilterPropagateResult::FILTER_ALWAYS_TRUE;
102: 		} else if (constant >= min && constant <= max) {
103: 			return FilterPropagateResult::NO_PRUNING_POSSIBLE;
104: 		} else {
105: 			return FilterPropagateResult::FILTER_ALWAYS_FALSE;
106: 		}
107: 	case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
108: 		// X >= C
109: 		// this can be true only if max(X) >= C
110: 		// if min(X) >= C, then this is always true
111: 		if (min >= constant) {
112: 			return FilterPropagateResult::FILTER_ALWAYS_TRUE;
113: 		} else if (max >= constant) {
114: 			return FilterPropagateResult::NO_PRUNING_POSSIBLE;
115: 		} else {
116: 			return FilterPropagateResult::FILTER_ALWAYS_FALSE;
117: 		}
118: 	case ExpressionType::COMPARE_GREATERTHAN:
119: 		// X > C
120: 		// this can be true only if max(X) > C
121: 		// if min(X) > C, then this is always true
122: 		if (min > constant) {
123: 			return FilterPropagateResult::FILTER_ALWAYS_TRUE;
124: 		} else if (max > constant) {
125: 			return FilterPropagateResult::NO_PRUNING_POSSIBLE;
126: 		} else {
127: 			return FilterPropagateResult::FILTER_ALWAYS_FALSE;
128: 		}
129: 	case ExpressionType::COMPARE_LESSTHANOREQUALTO:
130: 		// X <= C
131: 		// this can be true only if min(X) <= C
132: 		// if max(X) <= C, then this is always true
133: 		if (max <= constant) {
134: 			return FilterPropagateResult::FILTER_ALWAYS_TRUE;
135: 		} else if (min <= constant) {
136: 			return FilterPropagateResult::NO_PRUNING_POSSIBLE;
137: 		} else {
138: 			return FilterPropagateResult::FILTER_ALWAYS_FALSE;
139: 		}
140: 	case ExpressionType::COMPARE_LESSTHAN:
141: 		// X < C
142: 		// this can be true only if min(X) < C
143: 		// if max(X) < C, then this is always true
144: 		if (max < constant) {
145: 			return FilterPropagateResult::FILTER_ALWAYS_TRUE;
146: 		} else if (min < constant) {
147: 			return FilterPropagateResult::NO_PRUNING_POSSIBLE;
148: 		} else {
149: 			return FilterPropagateResult::FILTER_ALWAYS_FALSE;
150: 		}
151: 	default:
152: 		throw InternalException("Expression type in zonemap check not implemented");
153: 	}
154: }
155: 
156: unique_ptr<BaseStatistics> NumericStatistics::Copy() {
157: 	auto stats = make_unique<NumericStatistics>(type, min, max);
158: 	if (validity_stats) {
159: 		stats->validity_stats = validity_stats->Copy();
160: 	}
161: 	return move(stats);
162: }
163: 
164: void NumericStatistics::Serialize(Serializer &serializer) {
165: 	BaseStatistics::Serialize(serializer);
166: 	min.Serialize(serializer);
167: 	max.Serialize(serializer);
168: }
169: 
170: unique_ptr<BaseStatistics> NumericStatistics::Deserialize(Deserializer &source, LogicalType type) {
171: 	auto min = Value::Deserialize(source);
172: 	auto max = Value::Deserialize(source);
173: 	return make_unique_base<BaseStatistics, NumericStatistics>(move(type), min, max);
174: }
175: 
176: string NumericStatistics::ToString() {
177: 	return StringUtil::Format("Numeric Statistics<%s> %s[Min: %s, Max: %s]", type.ToString(),
178: 	                          validity_stats ? validity_stats->ToString() : "", min.ToString(), max.ToString());
179: }
180: 
181: template <class T>
182: void NumericStatistics::TemplatedVerify(Vector &vector, idx_t count) {
183: 	VectorData vdata;
184: 	vector.Orrify(count, vdata);
185: 
186: 	auto data = (T *)vdata.data;
187: 	for (idx_t i = 0; i < count; i++) {
188: 		auto index = vdata.sel->get_index(i);
189: 		if (!vdata.validity.RowIsValid(index)) {
190: 			continue;
191: 		}
192: 		if (!min.is_null && LessThan::Operation(data[index], min.GetValueUnsafe<T>())) {
193: 			throw InternalException("Statistics mismatch: value is smaller than min.\nStatistics: %s\nVector: %s",
194: 			                        ToString(), vector.ToString(count));
195: 		}
196: 		if (!max.is_null && GreaterThan::Operation(data[index], max.GetValueUnsafe<T>())) {
197: 			throw InternalException("Statistics mismatch: value is bigger than max.\nStatistics: %s\nVector: %s",
198: 			                        ToString(), vector.ToString(count));
199: 		}
200: 	}
201: }
202: 
203: void NumericStatistics::Verify(Vector &vector, idx_t count) {
204: 	BaseStatistics::Verify(vector, count);
205: 
206: 	switch (type.InternalType()) {
207: 	case PhysicalType::BOOL:
208: 		break;
209: 	case PhysicalType::INT8:
210: 		TemplatedVerify<int8_t>(vector, count);
211: 		break;
212: 	case PhysicalType::INT16:
213: 		TemplatedVerify<int16_t>(vector, count);
214: 		break;
215: 	case PhysicalType::INT32:
216: 		TemplatedVerify<int32_t>(vector, count);
217: 		break;
218: 	case PhysicalType::INT64:
219: 		TemplatedVerify<int64_t>(vector, count);
220: 		break;
221: 	case PhysicalType::INT128:
222: 		TemplatedVerify<hugeint_t>(vector, count);
223: 		break;
224: 	case PhysicalType::FLOAT:
225: 		TemplatedVerify<float>(vector, count);
226: 		break;
227: 	case PhysicalType::DOUBLE:
228: 		TemplatedVerify<double>(vector, count);
229: 		break;
230: 	default:
231: 		throw InternalException("Unsupported type %s for numeric statistics verify", type.ToString());
232: 	}
233: }
234: 
235: } // namespace duckdb
[end of src/storage/statistics/numeric_statistics.cpp]
[start of src/storage/statistics/string_statistics.cpp]
1: #include "duckdb/storage/statistics/string_statistics.hpp"
2: #include "duckdb/common/serializer.hpp"
3: #include "utf8proc_wrapper.hpp"
4: #include "duckdb/common/string_util.hpp"
5: #include "duckdb/common/types/vector.hpp"
6: 
7: namespace duckdb {
8: 
9: StringStatistics::StringStatistics(LogicalType type_p) : BaseStatistics(move(type_p)) {
10: 	for (idx_t i = 0; i < MAX_STRING_MINMAX_SIZE; i++) {
11: 		min[i] = 0xFF;
12: 		max[i] = 0;
13: 	}
14: 	max_string_length = 0;
15: 	has_unicode = false;
16: 	has_overflow_strings = false;
17: }
18: 
19: unique_ptr<BaseStatistics> StringStatistics::Copy() {
20: 	auto stats = make_unique<StringStatistics>(type);
21: 	memcpy(stats->min, min, MAX_STRING_MINMAX_SIZE);
22: 	memcpy(stats->max, max, MAX_STRING_MINMAX_SIZE);
23: 	stats->has_unicode = has_unicode;
24: 	stats->max_string_length = max_string_length;
25: 	stats->max_string_length = max_string_length;
26: 	if (validity_stats) {
27: 		stats->validity_stats = validity_stats->Copy();
28: 	}
29: 	return move(stats);
30: }
31: 
32: void StringStatistics::Serialize(Serializer &serializer) {
33: 	BaseStatistics::Serialize(serializer);
34: 	serializer.WriteData(min, MAX_STRING_MINMAX_SIZE);
35: 	serializer.WriteData(max, MAX_STRING_MINMAX_SIZE);
36: 	serializer.Write<bool>(has_unicode);
37: 	serializer.Write<uint32_t>(max_string_length);
38: 	serializer.Write<bool>(has_overflow_strings);
39: }
40: 
41: unique_ptr<BaseStatistics> StringStatistics::Deserialize(Deserializer &source, LogicalType type) {
42: 	auto stats = make_unique<StringStatistics>(move(type));
43: 	source.ReadData(stats->min, MAX_STRING_MINMAX_SIZE);
44: 	source.ReadData(stats->max, MAX_STRING_MINMAX_SIZE);
45: 	stats->has_unicode = source.Read<bool>();
46: 	stats->max_string_length = source.Read<uint32_t>();
47: 	stats->has_overflow_strings = source.Read<bool>();
48: 	return move(stats);
49: }
50: 
51: static int StringValueComparison(const_data_ptr_t data, idx_t len, const_data_ptr_t comparison) {
52: 	D_ASSERT(len <= StringStatistics::MAX_STRING_MINMAX_SIZE);
53: 	for (idx_t i = 0; i < len; i++) {
54: 		if (data[i] < comparison[i]) {
55: 			return -1;
56: 		} else if (data[i] > comparison[i]) {
57: 			return 1;
58: 		}
59: 	}
60: 	return 0;
61: }
62: 
63: static void ConstructValue(const_data_ptr_t data, idx_t size, data_t target[]) {
64: 	idx_t value_size =
65: 	    size > StringStatistics::MAX_STRING_MINMAX_SIZE ? StringStatistics::MAX_STRING_MINMAX_SIZE : size;
66: 	memcpy(target, data, value_size);
67: 	for (idx_t i = value_size; i < StringStatistics::MAX_STRING_MINMAX_SIZE; i++) {
68: 		target[i] = '\0';
69: 	}
70: }
71: 
72: void StringStatistics::Update(const string_t &value) {
73: 	auto data = (const_data_ptr_t)value.GetDataUnsafe();
74: 	auto size = value.GetSize();
75: 
76: 	//! we can only fit 8 bytes, so we might need to trim our string
77: 	// construct the value
78: 	data_t target[MAX_STRING_MINMAX_SIZE];
79: 	ConstructValue(data, size, target);
80: 
81: 	// update the min and max
82: 	if (StringValueComparison(target, MAX_STRING_MINMAX_SIZE, min) < 0) {
83: 		memcpy(min, target, MAX_STRING_MINMAX_SIZE);
84: 	}
85: 	if (StringValueComparison(target, MAX_STRING_MINMAX_SIZE, max) > 0) {
86: 		memcpy(max, target, MAX_STRING_MINMAX_SIZE);
87: 	}
88: 	if (size > max_string_length) {
89: 		max_string_length = size;
90: 	}
91: 	if (type.id() == LogicalTypeId::VARCHAR && !has_unicode) {
92: 		auto unicode = Utf8Proc::Analyze((const char *)data, size);
93: 		if (unicode == UnicodeType::UNICODE) {
94: 			has_unicode = true;
95: 		} else if (unicode == UnicodeType::INVALID) {
96: 			throw InternalException("Invalid unicode detected in segment statistics update!");
97: 		}
98: 	}
99: }
100: 
101: void StringStatistics::Merge(const BaseStatistics &other_p) {
102: 	BaseStatistics::Merge(other_p);
103: 	auto &other = (const StringStatistics &)other_p;
104: 	if (StringValueComparison(other.min, MAX_STRING_MINMAX_SIZE, min) < 0) {
105: 		memcpy(min, other.min, MAX_STRING_MINMAX_SIZE);
106: 	}
107: 	if (StringValueComparison(other.max, MAX_STRING_MINMAX_SIZE, max) > 0) {
108: 		memcpy(max, other.max, MAX_STRING_MINMAX_SIZE);
109: 	}
110: 	has_unicode = has_unicode || other.has_unicode;
111: 	max_string_length = MaxValue<uint32_t>(max_string_length, other.max_string_length);
112: 	has_overflow_strings = has_overflow_strings || other.has_overflow_strings;
113: }
114: 
115: FilterPropagateResult StringStatistics::CheckZonemap(ExpressionType comparison_type, const string &constant) {
116: 	auto data = (const_data_ptr_t)constant.c_str();
117: 	auto size = constant.size();
118: 
119: 	idx_t value_size = size > MAX_STRING_MINMAX_SIZE ? MAX_STRING_MINMAX_SIZE : size;
120: 	int min_comp = StringValueComparison(data, value_size, min);
121: 	int max_comp = StringValueComparison(data, value_size, max);
122: 	switch (comparison_type) {
123: 	case ExpressionType::COMPARE_EQUAL:
124: 		if (min_comp >= 0 && max_comp <= 0) {
125: 			return FilterPropagateResult::NO_PRUNING_POSSIBLE;
126: 		} else {
127: 			return FilterPropagateResult::FILTER_ALWAYS_FALSE;
128: 		}
129: 	case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
130: 	case ExpressionType::COMPARE_GREATERTHAN:
131: 		if (max_comp <= 0) {
132: 			return FilterPropagateResult::NO_PRUNING_POSSIBLE;
133: 		} else {
134: 			return FilterPropagateResult::FILTER_ALWAYS_FALSE;
135: 		}
136: 	case ExpressionType::COMPARE_LESSTHAN:
137: 	case ExpressionType::COMPARE_LESSTHANOREQUALTO:
138: 		if (min_comp >= 0) {
139: 			return FilterPropagateResult::NO_PRUNING_POSSIBLE;
140: 		} else {
141: 			return FilterPropagateResult::FILTER_ALWAYS_FALSE;
142: 		}
143: 	default:
144: 		throw InternalException("Expression type not implemented for string statistics zone map");
145: 	}
146: }
147: 
148: static idx_t GetValidMinMaxSubstring(data_ptr_t data) {
149: 	idx_t len = 0;
150: 	for (idx_t i = 0; i < StringStatistics::MAX_STRING_MINMAX_SIZE; i++) {
151: 		if (data[i] == '\0') {
152: 			return i;
153: 		}
154: 		if ((data[i] & 0xC0) != 0x80) {
155: 			len = i;
156: 		}
157: 	}
158: 	return len;
159: }
160: 
161: string StringStatistics::ToString() {
162: 	idx_t min_len = GetValidMinMaxSubstring(min);
163: 	idx_t max_len = GetValidMinMaxSubstring(max);
164: 	return StringUtil::Format("String Statistics %s[Min: %s, Max: %s, Has Unicode: %s, Max String Length: %lld]",
165: 	                          validity_stats ? validity_stats->ToString() : "", string((const char *)min, min_len),
166: 	                          string((const char *)max, max_len), has_unicode ? "true" : "false", max_string_length);
167: }
168: 
169: void StringStatistics::Verify(Vector &vector, idx_t count) {
170: 	BaseStatistics::Verify(vector, count);
171: 
172: 	string_t min_string((const char *)min, MAX_STRING_MINMAX_SIZE);
173: 	string_t max_string((const char *)max, MAX_STRING_MINMAX_SIZE);
174: 
175: 	VectorData vdata;
176: 	vector.Orrify(count, vdata);
177: 	auto data = (string_t *)vdata.data;
178: 	for (idx_t i = 0; i < count; i++) {
179: 		auto index = vdata.sel->get_index(i);
180: 		if (!vdata.validity.RowIsValid(index)) {
181: 			continue;
182: 		}
183: 		auto value = data[index];
184: 		auto data = value.GetDataUnsafe();
185: 		auto len = value.GetSize();
186: 		if (len > max_string_length) {
187: 			throw InternalException(
188: 			    "Statistics mismatch: string value exceeds maximum string length.\nStatistics: %s\nVector: %s",
189: 			    ToString(), vector.ToString(count));
190: 		}
191: 		if (type.id() == LogicalTypeId::VARCHAR && !has_unicode) {
192: 			auto unicode = Utf8Proc::Analyze(data, len);
193: 			if (unicode == UnicodeType::UNICODE) {
194: 				throw InternalException("Statistics mismatch: string value contains unicode, but statistics says it "
195: 				                        "shouldn't.\nStatistics: %s\nVector: %s",
196: 				                        ToString(), vector.ToString(count));
197: 			} else if (unicode == UnicodeType::INVALID) {
198: 				throw InternalException("Invalid unicode detected in vector: %s", vector.ToString(count));
199: 			}
200: 		}
201: 		if (StringValueComparison((const_data_ptr_t)data, MinValue<idx_t>(len, MAX_STRING_MINMAX_SIZE), min) < 0) {
202: 			throw InternalException("Statistics mismatch: value is smaller than min.\nStatistics: %s\nVector: %s",
203: 			                        ToString(), vector.ToString(count));
204: 		}
205: 		if (StringValueComparison((const_data_ptr_t)data, MinValue<idx_t>(len, MAX_STRING_MINMAX_SIZE), max) > 0) {
206: 			throw InternalException("Statistics mismatch: value is bigger than max.\nStatistics: %s\nVector: %s",
207: 			                        ToString(), vector.ToString(count));
208: 		}
209: 	}
210: }
211: 
212: } // namespace duckdb
[end of src/storage/statistics/string_statistics.cpp]
[start of src/storage/table/CMakeLists.txt]
1: add_library_unity(
2:   duckdb_storage_table
3:   OBJECT
4:   chunk_info.cpp
5:   column_data.cpp
6:   column_segment.cpp
7:   update_segment.cpp
8:   persistent_table_data.cpp
9:   segment_tree.cpp
10:   persistent_segment.cpp
11:   row_group.cpp
12:   standard_column_data.cpp
13:   transient_segment.cpp
14:   validity_column_data.cpp
15:   validity_segment.cpp)
16: set(ALL_OBJECT_FILES
17:     ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:duckdb_storage_table>
18:     PARENT_SCOPE)
[end of src/storage/table/CMakeLists.txt]
[start of src/storage/table/column_data.cpp]
1: #include "duckdb/storage/table/column_data.hpp"
2: #include "duckdb/storage/table/persistent_segment.hpp"
3: #include "duckdb/storage/table/transient_segment.hpp"
4: #include "duckdb/storage/data_table.hpp"
5: #include "duckdb/storage/storage_manager.hpp"
6: #include "duckdb/storage/data_pointer.hpp"
7: #include "duckdb/storage/checkpoint/table_data_writer.hpp"
8: #include "duckdb/storage/table/update_segment.hpp"
9: #include "duckdb/planner/table_filter.hpp"
10: #include "duckdb/common/vector_operations/vector_operations.hpp"
11: #include "duckdb/storage/table/validity_segment.hpp"
12: 
13: #include "duckdb/storage/numeric_segment.hpp"
14: #include "duckdb/storage/string_segment.hpp"
15: #include "duckdb/storage/table/validity_segment.hpp"
16: #include "duckdb/storage/checkpoint/write_overflow_strings_to_disk.hpp"
17: #include "duckdb/storage/table/validity_column_data.hpp"
18: #include "duckdb/storage/table/standard_column_data.hpp"
19: #include "duckdb/transaction/transaction.hpp"
20: #include "duckdb/storage/table/row_group.hpp"
21: 
22: namespace duckdb {
23: 
24: ColumnData::ColumnData(DataTableInfo &info, idx_t column_index, idx_t start_row, LogicalType type, ColumnData *parent)
25:     : info(info), column_index(column_index), start(start_row), type(move(type)), parent(parent) {
26: }
27: 
28: ColumnData::~ColumnData() {
29: }
30: 
31: DatabaseInstance &ColumnData::GetDatabase() const {
32: 	return info.db;
33: }
34: 
35: DataTableInfo &ColumnData::GetTableInfo() const {
36: 	return info;
37: }
38: 
39: const LogicalType &ColumnData::RootType() const {
40: 	if (parent) {
41: 		return parent->RootType();
42: 	}
43: 	return type;
44: }
45: 
46: void ColumnData::ScanVector(ColumnScanState &state, Vector &result) {
47: 	if (!state.initialized) {
48: 		D_ASSERT(state.current);
49: 		state.current->InitializeScan(state);
50: 		state.initialized = true;
51: 	}
52: 	idx_t row_index = state.row_index;
53: 	idx_t remaining = STANDARD_VECTOR_SIZE;
54: 	while (remaining > 0) {
55: 		D_ASSERT(row_index >= state.current->start && row_index <= state.current->start + state.current->count);
56: 		idx_t scan_count = MinValue<idx_t>(remaining, state.current->start + state.current->count - row_index);
57: 		idx_t start = row_index - state.current->start;
58: 		idx_t result_offset = STANDARD_VECTOR_SIZE - remaining;
59: 		state.current->Scan(state, start, scan_count, result, result_offset);
60: 
61: 		row_index += scan_count;
62: 		remaining -= scan_count;
63: 		if (remaining > 0) {
64: 			if (!state.current->next) {
65: 				break;
66: 			}
67: 			state.current = (ColumnSegment *)state.current->next.get();
68: 			state.current->InitializeScan(state);
69: 			state.segment_checked = false;
70: 			D_ASSERT(row_index >= state.current->start && row_index <= state.current->start + state.current->count);
71: 		}
72: 	}
73: }
74: 
75: template <bool SCAN_COMMITTED, bool ALLOW_UPDATES>
76: void ColumnData::ScanVector(Transaction *transaction, idx_t vector_index, ColumnScanState &state, Vector &result) {
77: 	ScanVector(state, result);
78: 
79: 	lock_guard<mutex> update_guard(update_lock);
80: 	if (updates) {
81: 		if (!ALLOW_UPDATES && updates->HasUncommittedUpdates(vector_index)) {
82: 			throw TransactionException("Cannot create index with outstanding updates");
83: 		}
84: 		if (SCAN_COMMITTED) {
85: 			updates->FetchCommitted(vector_index, result);
86: 		} else {
87: 			D_ASSERT(transaction);
88: 			updates->FetchUpdates(*transaction, vector_index, result);
89: 		}
90: 	}
91: }
92: 
93: template void ColumnData::ScanVector<false, false>(Transaction *transaction, idx_t vector_index, ColumnScanState &state,
94:                                                    Vector &result);
95: template void ColumnData::ScanVector<true, false>(Transaction *transaction, idx_t vector_index, ColumnScanState &state,
96:                                                   Vector &result);
97: template void ColumnData::ScanVector<false, true>(Transaction *transaction, idx_t vector_index, ColumnScanState &state,
98:                                                   Vector &result);
99: template void ColumnData::ScanVector<true, true>(Transaction *transaction, idx_t vector_index, ColumnScanState &state,
100:                                                  Vector &result);
101: 
102: void ColumnData::Scan(Transaction &transaction, idx_t vector_index, ColumnScanState &state, Vector &result) {
103: 	ScanVector<false, true>(&transaction, vector_index, state, result);
104: }
105: 
106: void ColumnData::ScanCommitted(idx_t vector_index, ColumnScanState &state, Vector &result, bool allow_updates) {
107: 	if (allow_updates) {
108: 		ScanVector<true, true>(nullptr, vector_index, state, result);
109: 	} else {
110: 		ScanVector<true, false>(nullptr, vector_index, state, result);
111: 	}
112: }
113: 
114: void ColumnScanState::Next() {
115: 	//! There is no column segment
116: 	if (!current) {
117: 		return;
118: 	}
119: 	row_index += STANDARD_VECTOR_SIZE;
120: 	while (row_index >= current->start + current->count) {
121: 		current = (ColumnSegment *)current->next.get();
122: 		initialized = false;
123: 		segment_checked = false;
124: 		if (!current) {
125: 			break;
126: 		}
127: 	}
128: 	D_ASSERT(!current || (row_index >= current->start && row_index < current->start + current->count));
129: 	for (auto &child_state : child_states) {
130: 		child_state.Next();
131: 	}
132: }
133: 
134: void TableScanState::NextVector() {
135: 	//! nothing to scan for this vector, skip the entire vector
136: 	throw NotImplementedException("FIXME: next vector");
137: 	// for (idx_t j = 0; j < column_ids.size(); j++) {
138: 	// 	column_scans[j].Next();
139: 	// }
140: }
141: 
142: void ColumnData::Append(BaseStatistics &stats, ColumnAppendState &state, Vector &vector, idx_t count) {
143: 	VectorData vdata;
144: 	vector.Orrify(count, vdata);
145: 	AppendData(stats, state, vdata, count);
146: }
147: 
148: void ColumnData::InitializeAppend(ColumnAppendState &state) {
149: 	lock_guard<mutex> tree_lock(data.node_lock);
150: 	if (data.nodes.empty()) {
151: 		// no segments yet, append an empty segment
152: 		AppendTransientSegment(start);
153: 	}
154: 	auto segment = (ColumnSegment *)data.GetLastSegment();
155: 	if (segment->segment_type == ColumnSegmentType::PERSISTENT) {
156: 		// no transient segments yet
157: 		auto total_rows = segment->start + segment->count;
158: 		AppendTransientSegment(total_rows);
159: 		state.current = (TransientSegment *)data.GetLastSegment();
160: 	} else {
161: 		state.current = (TransientSegment *)segment;
162: 	}
163: 
164: 	D_ASSERT(state.current->segment_type == ColumnSegmentType::TRANSIENT);
165: 	state.current->InitializeAppend(state);
166: }
167: 
168: void ColumnData::AppendData(BaseStatistics &stats, ColumnAppendState &state, VectorData &vdata, idx_t count) {
169: 	idx_t offset = 0;
170: 	while (true) {
171: 		// append the data from the vector
172: 		idx_t copied_elements = state.current->Append(state, vdata, offset, count);
173: 		stats.Merge(*state.current->stats.statistics);
174: 		if (copied_elements == count) {
175: 			// finished copying everything
176: 			break;
177: 		}
178: 
179: 		// we couldn't fit everything we wanted in the current column segment, create a new one
180: 		{
181: 			lock_guard<mutex> tree_lock(data.node_lock);
182: 			AppendTransientSegment(state.current->start + state.current->count);
183: 			state.current = (TransientSegment *)data.GetLastSegment();
184: 			state.current->InitializeAppend(state);
185: 		}
186: 		offset += copied_elements;
187: 		count -= copied_elements;
188: 	}
189: }
190: 
191: void ColumnData::RevertAppend(row_t start_row) {
192: 	lock_guard<mutex> tree_lock(data.node_lock);
193: 	// check if this row is in the segment tree at all
194: 	if (idx_t(start_row) >= data.nodes.back().row_start + data.nodes.back().node->count) {
195: 		// the start row is equal to the final portion of the column data: nothing was ever appended here
196: 		D_ASSERT(idx_t(start_row) == data.nodes.back().row_start + data.nodes.back().node->count);
197: 		return;
198: 	}
199: 	// find the segment index that the current row belongs to
200: 	idx_t segment_index = data.GetSegmentIndex(start_row);
201: 	auto segment = data.nodes[segment_index].node;
202: 	auto &transient = (TransientSegment &)*segment;
203: 	D_ASSERT(transient.segment_type == ColumnSegmentType::TRANSIENT);
204: 
205: 	// remove any segments AFTER this segment: they should be deleted entirely
206: 	if (segment_index < data.nodes.size() - 1) {
207: 		data.nodes.erase(data.nodes.begin() + segment_index + 1, data.nodes.end());
208: 	}
209: 	segment->next = nullptr;
210: 	transient.RevertAppend(start_row);
211: }
212: 
213: void ColumnData::Fetch(ColumnScanState &state, row_t row_id, Vector &result) {
214: 	// perform the fetch within the segment
215: 	state.row_index = row_id / STANDARD_VECTOR_SIZE * STANDARD_VECTOR_SIZE;
216: 	state.current = (ColumnSegment *)data.GetSegment(state.row_index);
217: 	ScanVector(state, result);
218: }
219: 
220: void ColumnData::FetchRow(Transaction &transaction, ColumnFetchState &state, row_t row_id, Vector &result,
221:                           idx_t result_idx) {
222: 	auto segment = (ColumnSegment *)data.GetSegment(row_id);
223: 
224: 	// now perform the fetch within the segment
225: 	segment->FetchRow(state, row_id, result, result_idx);
226: 	// merge any updates made to this row
227: 	lock_guard<mutex> update_guard(update_lock);
228: 	if (updates) {
229: 		updates->FetchRow(transaction, row_id, result, result_idx);
230: 	}
231: }
232: 
233: void ColumnData::Update(Transaction &transaction, idx_t column_index, Vector &update_vector, row_t *row_ids,
234:                         idx_t update_count) {
235: 	lock_guard<mutex> update_guard(update_lock);
236: 	if (!updates) {
237: 		updates = make_unique<UpdateSegment>(*this);
238: 	}
239: 	Vector base_vector(type);
240: 	ColumnScanState state;
241: 	Fetch(state, row_ids[0], base_vector);
242: 	updates->Update(transaction, column_index, update_vector, row_ids, update_count, base_vector);
243: }
244: 
245: void ColumnData::UpdateColumn(Transaction &transaction, const vector<column_t> &column_path, Vector &update_vector,
246:                               row_t *row_ids, idx_t update_count, idx_t depth) {
247: 	// this method should only be called at the end of the path in the base column case
248: 	D_ASSERT(depth >= column_path.size());
249: 	ColumnData::Update(transaction, column_path[0], update_vector, row_ids, update_count);
250: }
251: 
252: unique_ptr<BaseStatistics> ColumnData::GetUpdateStatistics() {
253: 	lock_guard<mutex> update_guard(update_lock);
254: 	return updates ? updates->GetStatistics() : nullptr;
255: }
256: 
257: void ColumnData::AppendTransientSegment(idx_t start_row) {
258: 	auto new_segment = make_unique<TransientSegment>(GetDatabase(), type, start_row);
259: 	data.AppendSegment(move(new_segment));
260: }
261: 
262: void ColumnData::CommitDropColumn() {
263: 	auto &block_manager = BlockManager::GetBlockManager(GetDatabase());
264: 	auto segment = (ColumnSegment *)data.GetRootSegment();
265: 	while (segment) {
266: 		if (segment->segment_type == ColumnSegmentType::PERSISTENT) {
267: 			auto &persistent = (PersistentSegment &)*segment;
268: 			block_manager.MarkBlockAsModified(persistent.block_id);
269: 		}
270: 		segment = (ColumnSegment *)segment->next.get();
271: 	}
272: }
273: 
274: unique_ptr<ColumnCheckpointState> ColumnData::CreateCheckpointState(RowGroup &row_group, TableDataWriter &writer) {
275: 	return make_unique<ColumnCheckpointState>(row_group, *this, writer);
276: }
277: 
278: ColumnCheckpointState::ColumnCheckpointState(RowGroup &row_group, ColumnData &column_data, TableDataWriter &writer)
279:     : row_group(row_group), column_data(column_data), writer(writer) {
280: }
281: 
282: ColumnCheckpointState::~ColumnCheckpointState() {
283: }
284: 
285: void ColumnCheckpointState::CreateEmptySegment() {
286: 	auto type_id = column_data.type.InternalType();
287: 	if (type_id == PhysicalType::VARCHAR) {
288: 		auto string_segment = make_unique<StringSegment>(column_data.GetDatabase(), row_group.start);
289: 		string_segment->overflow_writer = make_unique<WriteOverflowStringsToDisk>(column_data.GetDatabase());
290: 		current_segment = move(string_segment);
291: 	} else if (type_id == PhysicalType::BIT) {
292: 		current_segment = make_unique<ValiditySegment>(column_data.GetDatabase(), row_group.start);
293: 	} else {
294: 		current_segment = make_unique<NumericSegment>(column_data.GetDatabase(), type_id, row_group.start);
295: 	}
296: 	segment_stats = make_unique<SegmentStatistics>(column_data.type);
297: }
298: 
299: void ColumnCheckpointState::AppendData(Vector &data, idx_t count) {
300: 	VectorData vdata;
301: 	data.Orrify(count, vdata);
302: 
303: 	idx_t offset = 0;
304: 	while (count > 0) {
305: 		idx_t appended = current_segment->Append(*segment_stats, vdata, offset, count);
306: 		if (appended == count) {
307: 			// appended everything: finished
308: 			return;
309: 		}
310: 		// the segment is full: flush it to disk
311: 		FlushSegment();
312: 
313: 		// now create a new segment and continue appending
314: 		CreateEmptySegment();
315: 		offset += appended;
316: 		count -= appended;
317: 	}
318: }
319: 
320: void ColumnCheckpointState::FlushSegment() {
321: 	auto tuple_count = current_segment->tuple_count.load();
322: 	if (tuple_count == 0) {
323: 		return;
324: 	}
325: 
326: 	// get the buffer of the segment and pin it
327: 	auto &buffer_manager = BufferManager::GetBufferManager(column_data.GetDatabase());
328: 	auto &block_manager = BlockManager::GetBlockManager(column_data.GetDatabase());
329: 
330: 	auto handle = buffer_manager.Pin(current_segment->block);
331: 
332: 	// get a free block id to write to
333: 	auto block_id = block_manager.GetFreeBlockId();
334: 
335: 	// construct the data pointer
336: 	uint32_t offset_in_block = 0;
337: 
338: 	DataPointer data_pointer;
339: 	data_pointer.block_pointer.block_id = block_id;
340: 	data_pointer.block_pointer.offset = offset_in_block;
341: 	data_pointer.row_start = row_group.start;
342: 	if (!data_pointers.empty()) {
343: 		auto &last_pointer = data_pointers.back();
344: 		data_pointer.row_start = last_pointer.row_start + last_pointer.tuple_count;
345: 	}
346: 	data_pointer.tuple_count = tuple_count;
347: 	data_pointer.statistics = segment_stats->statistics->Copy();
348: 
349: 	// construct a persistent segment that points to this block, and append it to the new segment tree
350: 	auto persistent_segment = make_unique<PersistentSegment>(
351: 	    column_data.GetDatabase(), block_id, offset_in_block, column_data.type, data_pointer.row_start,
352: 	    data_pointer.tuple_count, segment_stats->statistics->Copy());
353: 	new_tree.AppendSegment(move(persistent_segment));
354: 
355: 	data_pointers.push_back(move(data_pointer));
356: 	// write the block to disk
357: 	block_manager.Write(*handle->node, block_id);
358: 
359: 	// merge the segment stats into the global stats
360: 	global_stats->Merge(*segment_stats->statistics);
361: 	handle.reset();
362: 
363: 	current_segment.reset();
364: 	segment_stats.reset();
365: }
366: 
367: void ColumnCheckpointState::FlushToDisk() {
368: 	auto &meta_writer = writer.GetMetaWriter();
369: 
370: 	meta_writer.Write<idx_t>(data_pointers.size());
371: 	// then write the data pointers themselves
372: 	for (idx_t k = 0; k < data_pointers.size(); k++) {
373: 		auto &data_pointer = data_pointers[k];
374: 		meta_writer.Write<idx_t>(data_pointer.row_start);
375: 		meta_writer.Write<idx_t>(data_pointer.tuple_count);
376: 		meta_writer.Write<block_id_t>(data_pointer.block_pointer.block_id);
377: 		meta_writer.Write<uint32_t>(data_pointer.block_pointer.offset);
378: 		data_pointer.statistics->Serialize(meta_writer);
379: 	}
380: }
381: 
382: unique_ptr<ColumnCheckpointState> ColumnData::Checkpoint(RowGroup &row_group, TableDataWriter &writer,
383:                                                          idx_t column_idx) {
384: 	// scan the segments of the column data
385: 	// set up the checkpoint state
386: 	auto checkpoint_state = CreateCheckpointState(row_group, writer);
387: 	checkpoint_state->global_stats = BaseStatistics::CreateEmpty(type);
388: 
389: 	if (!data.root_node) {
390: 		// empty table: flush the empty list
391: 		return checkpoint_state;
392: 	}
393: 	lock_guard<mutex> update_guard(update_lock);
394: 
395: 	auto &block_manager = BlockManager::GetBlockManager(GetDatabase());
396: 	checkpoint_state->CreateEmptySegment();
397: 	Vector intermediate(row_group.columns[column_idx]->type);
398: 	// we create a new segment tree with all the new segments
399: 	// we do this by scanning the current segments of the column and checking for changes
400: 	// if there are any changes (e.g. updates or deletes) we write the new changes
401: 	// otherwise we simply write out the current data pointers
402: 	auto owned_segment = move(data.root_node);
403: 	auto segment = (ColumnSegment *)owned_segment.get();
404: 	while (segment) {
405: 		if (segment->segment_type == ColumnSegmentType::PERSISTENT) {
406: 			auto &persistent = (PersistentSegment &)*segment;
407: 			// persistent segment; check if there were any updates or deletions in this segment
408: 			idx_t start_row_idx = persistent.start - row_group.start;
409: 			idx_t end_row_idx = start_row_idx + persistent.count;
410: 			bool has_changes = false;
411: 			if (updates && updates->HasUpdates(start_row_idx, end_row_idx)) {
412: 				has_changes = true;
413: 			}
414: 			if (has_changes) {
415: 				// persistent segment has updates: mark it as modified and rewrite the block with the merged updates
416: 				block_manager.MarkBlockAsModified(persistent.block_id);
417: 			} else {
418: 				// unchanged persistent segment: no need to write the data
419: 
420: 				// flush any segments preceding this persistent segment
421: 				if (checkpoint_state->current_segment->tuple_count > 0) {
422: 					checkpoint_state->FlushSegment();
423: 					checkpoint_state->CreateEmptySegment();
424: 				}
425: 
426: 				// set up the data pointer directly using the data from the persistent segment
427: 				DataPointer pointer;
428: 				pointer.block_pointer.block_id = persistent.block_id;
429: 				pointer.block_pointer.offset = 0;
430: 				pointer.row_start = segment->start;
431: 				pointer.tuple_count = persistent.count;
432: 				pointer.statistics = persistent.stats.statistics->Copy();
433: 
434: 				// merge the persistent stats into the global column stats
435: 				checkpoint_state->global_stats->Merge(*persistent.stats.statistics);
436: 
437: 				// directly append the current segment to the new tree
438: 				checkpoint_state->new_tree.AppendSegment(move(owned_segment));
439: 
440: 				checkpoint_state->data_pointers.push_back(move(pointer));
441: 
442: 				// move to the next segment in the list
443: 				owned_segment = move(segment->next);
444: 				segment = (ColumnSegment *)owned_segment.get();
445: 				continue;
446: 			}
447: 		}
448: 		// not persisted yet: scan the segment and write it to disk
449: 		ColumnScanState state;
450: 		state.current = segment;
451: 		segment->InitializeScan(state);
452: 
453: 		Vector scan_vector(type, nullptr);
454: 		for (idx_t base_row_index = 0; base_row_index < segment->count; base_row_index += STANDARD_VECTOR_SIZE) {
455: 			scan_vector.Reference(intermediate);
456: 
457: 			idx_t count = MinValue<idx_t>(segment->count - base_row_index, STANDARD_VECTOR_SIZE);
458: 			state.row_index = segment->start + base_row_index;
459: 			segment->Scan(state, base_row_index, count, scan_vector, 0);
460: 			if (updates) {
461: 				updates->FetchCommittedRange(segment->start - row_group.start + base_row_index, count, scan_vector);
462: 			}
463: 
464: 			checkpoint_state->AppendData(scan_vector, count);
465: 		}
466: 		// move to the next segment in the list
467: 		owned_segment = move(segment->next);
468: 		segment = (ColumnSegment *)owned_segment.get();
469: 	}
470: 	// flush the final segment
471: 	checkpoint_state->FlushSegment();
472: 	// replace the old tree with the new one
473: 	data.Replace(checkpoint_state->new_tree);
474: 
475: 	return checkpoint_state;
476: }
477: 
478: void ColumnData::Initialize(PersistentColumnData &column_data) {
479: 	// load persistent segments
480: 	idx_t segment_rows = 0;
481: 	for (auto &segment : column_data.segments) {
482: 		segment_rows += segment->count;
483: 		data.AppendSegment(move(segment));
484: 	}
485: 	if (segment_rows != column_data.total_rows) {
486: 		throw Exception("Segment rows does not match total rows stored in column...");
487: 	}
488: }
489: 
490: void ColumnData::BaseDeserialize(DatabaseInstance &db, Deserializer &source, const LogicalType &type,
491:                                  ColumnData &result) {
492: 	// load the data pointers for the column
493: 	idx_t data_pointer_count = source.Read<idx_t>();
494: 	for (idx_t data_ptr = 0; data_ptr < data_pointer_count; data_ptr++) {
495: 		// read the data pointer
496: 		DataPointer data_pointer;
497: 		data_pointer.row_start = source.Read<idx_t>();
498: 		data_pointer.tuple_count = source.Read<idx_t>();
499: 		data_pointer.block_pointer.block_id = source.Read<block_id_t>();
500: 		data_pointer.block_pointer.offset = source.Read<uint32_t>();
501: 		data_pointer.statistics = BaseStatistics::Deserialize(source, type);
502: 
503: 		// create a persistent segment
504: 		auto segment = make_unique<PersistentSegment>(db, data_pointer.block_pointer.block_id,
505: 		                                              data_pointer.block_pointer.offset, type, data_pointer.row_start,
506: 		                                              data_pointer.tuple_count, move(data_pointer.statistics));
507: 		result.data.AppendSegment(move(segment));
508: 	}
509: }
510: 
511: shared_ptr<ColumnData> ColumnData::Deserialize(DataTableInfo &info, idx_t column_index, idx_t start_row,
512:                                                Deserializer &source, const LogicalType &type) {
513: 	return StandardColumnData::Deserialize(info, column_index, start_row, source, type);
514: }
515: 
516: void ColumnData::GetStorageInfo(idx_t row_group_index, vector<idx_t> col_path, vector<vector<Value>> &result) {
517: 	D_ASSERT(!col_path.empty());
518: 
519: 	// convert the column path to a string
520: 	string col_path_str = "[";
521: 	for (idx_t i = 0; i < col_path.size(); i++) {
522: 		if (i > 0) {
523: 			col_path_str += ", ";
524: 		}
525: 		col_path_str += to_string(col_path[i]);
526: 	}
527: 	col_path_str += "]";
528: 
529: 	// iterate over the segments
530: 	idx_t segment_idx = 0;
531: 	auto segment = (ColumnSegment *)data.GetRootSegment();
532: 	while (segment) {
533: 		vector<Value> column_info;
534: 		// row_group_id
535: 		column_info.push_back(Value::BIGINT(row_group_index));
536: 		// column_id
537: 		column_info.push_back(Value::BIGINT(col_path[0]));
538: 		// column_path
539: 		column_info.emplace_back(col_path_str);
540: 		// segment_id
541: 		column_info.push_back(Value::BIGINT(segment_idx));
542: 		// segment_type
543: 		column_info.emplace_back(type.ToString());
544: 		// start
545: 		column_info.push_back(Value::BIGINT(segment->start));
546: 		// count
547: 		column_info.push_back(Value::BIGINT(segment->count));
548: 		// stats
549: 		column_info.emplace_back(segment->stats.statistics ? segment->stats.statistics->ToString()
550: 		                                                   : string("No Stats"));
551: 		// has_updates
552: 		column_info.push_back(Value::BOOLEAN(updates ? true : false));
553: 		// persistent
554: 		// block_id
555: 		// block_offset
556: 		if (segment->segment_type == ColumnSegmentType::PERSISTENT) {
557: 			auto &persistent = (PersistentSegment &)*segment;
558: 			column_info.push_back(Value::BOOLEAN(true));
559: 			column_info.push_back(Value::BIGINT(persistent.block_id));
560: 			column_info.push_back(Value::BIGINT(persistent.offset));
561: 		} else {
562: 			column_info.push_back(Value::BOOLEAN(false));
563: 			column_info.emplace_back();
564: 			column_info.emplace_back();
565: 		}
566: 
567: 		result.push_back(move(column_info));
568: 
569: 		segment_idx++;
570: 		segment = (ColumnSegment *)segment->next.get();
571: 	}
572: }
573: 
574: void ColumnData::Verify(RowGroup &parent) {
575: #ifdef DEBUG
576: 	D_ASSERT(this->start == parent.start);
577: 	auto root = data.GetRootSegment();
578: 	if (root) {
579: 		D_ASSERT(root != nullptr);
580: 		D_ASSERT(root->start == this->start);
581: 		idx_t prev_end = root->start;
582: 		while (root) {
583: 			D_ASSERT(prev_end == root->start);
584: 			prev_end = root->start + root->count;
585: 			if (!root->next) {
586: 				D_ASSERT(prev_end == parent.start + parent.count);
587: 			}
588: 			root = root->next.get();
589: 		}
590: 	} else {
591: 		D_ASSERT(parent.count == 0);
592: 	}
593: #endif
594: }
595: 
596: } // namespace duckdb
[end of src/storage/table/column_data.cpp]
[start of src/storage/table/row_group.cpp]
1: #include "duckdb/storage/table/row_group.hpp"
2: #include "duckdb/common/types/vector.hpp"
3: #include "duckdb/transaction/transaction.hpp"
4: #include "duckdb/common/exception.hpp"
5: #include "duckdb/storage/table/column_data.hpp"
6: #include "duckdb/storage/table/standard_column_data.hpp"
7: #include "duckdb/storage/table/update_segment.hpp"
8: #include "duckdb/common/chrono.hpp"
9: #include "duckdb/planner/table_filter.hpp"
10: #include "duckdb/execution/expression_executor.hpp"
11: #include "duckdb/storage/checkpoint/table_data_writer.hpp"
12: #include "duckdb/storage/meta_block_reader.hpp"
13: 
14: namespace duckdb {
15: 
16: constexpr const idx_t RowGroup::ROW_GROUP_VECTOR_COUNT;
17: constexpr const idx_t RowGroup::ROW_GROUP_SIZE;
18: 
19: RowGroup::RowGroup(DatabaseInstance &db, DataTableInfo &table_info, idx_t start, idx_t count)
20:     : SegmentBase(start, count), db(db), table_info(table_info) {
21: 
22: 	Verify();
23: }
24: 
25: RowGroup::RowGroup(DatabaseInstance &db, DataTableInfo &table_info, const vector<LogicalType> &types,
26:                    RowGroupPointer &pointer)
27:     : SegmentBase(pointer.row_start, pointer.tuple_count), db(db), table_info(table_info) {
28: 	// deserialize the columns
29: 	if (pointer.data_pointers.size() != types.size()) {
30: 		throw IOException("Row group column count is unaligned with table column count. Corrupt file?");
31: 	}
32: 	for (idx_t i = 0; i < pointer.data_pointers.size(); i++) {
33: 		auto &block_pointer = pointer.data_pointers[i];
34: 		MetaBlockReader column_data_reader(db, block_pointer.block_id);
35: 		column_data_reader.offset = block_pointer.offset;
36: 		this->columns.push_back(ColumnData::Deserialize(table_info, i, start, column_data_reader, types[i]));
37: 	}
38: 
39: 	// set up the statistics
40: 	for (auto &stats : pointer.statistics) {
41: 		this->stats.push_back(make_shared<SegmentStatistics>(stats->type, move(stats)));
42: 	}
43: 	this->version_info = move(pointer.versions);
44: 
45: 	Verify();
46: }
47: 
48: RowGroup::~RowGroup() {
49: }
50: 
51: void RowGroup::InitializeEmpty(const vector<LogicalType> &types) {
52: 	// set up the segment trees for the column segments
53: 	for (idx_t i = 0; i < types.size(); i++) {
54: 		auto column_data = make_shared<StandardColumnData>(GetTableInfo(), i, start, types[i]);
55: 		stats.push_back(make_shared<SegmentStatistics>(types[i]));
56: 		columns.push_back(move(column_data));
57: 	}
58: }
59: 
60: bool RowGroup::InitializeScanWithOffset(RowGroupScanState &state, idx_t vector_offset) {
61: 	auto &column_ids = state.parent.column_ids;
62: 	if (state.parent.table_filters) {
63: 		if (!CheckZonemap(*state.parent.table_filters, column_ids)) {
64: 			return false;
65: 		}
66: 	}
67: 
68: 	state.row_group = this;
69: 	state.vector_index = vector_offset;
70: 	state.max_row =
71: 	    this->start > state.parent.max_row ? 0 : MinValue<idx_t>(this->count, state.parent.max_row - this->start);
72: 	state.column_scans = unique_ptr<ColumnScanState[]>(new ColumnScanState[column_ids.size()]);
73: 	for (idx_t i = 0; i < column_ids.size(); i++) {
74: 		auto column = column_ids[i];
75: 		if (column != COLUMN_IDENTIFIER_ROW_ID) {
76: 			columns[column]->InitializeScanWithOffset(state.column_scans[i],
77: 			                                          start + vector_offset * STANDARD_VECTOR_SIZE);
78: 		} else {
79: 			state.column_scans[i].current = nullptr;
80: 		}
81: 	}
82: 	return true;
83: }
84: 
85: bool RowGroup::InitializeScan(RowGroupScanState &state) {
86: 	auto &column_ids = state.parent.column_ids;
87: 	if (state.parent.table_filters) {
88: 		if (!CheckZonemap(*state.parent.table_filters, column_ids)) {
89: 			return false;
90: 		}
91: 	}
92: 	state.row_group = this;
93: 	state.vector_index = 0;
94: 	state.max_row =
95: 	    this->start > state.parent.max_row ? 0 : MinValue<idx_t>(this->count, state.parent.max_row - this->start);
96: 	state.column_scans = unique_ptr<ColumnScanState[]>(new ColumnScanState[column_ids.size()]);
97: 	for (idx_t i = 0; i < column_ids.size(); i++) {
98: 		auto column = column_ids[i];
99: 		if (column != COLUMN_IDENTIFIER_ROW_ID) {
100: 			columns[column]->InitializeScan(state.column_scans[i]);
101: 		} else {
102: 			state.column_scans[i].current = nullptr;
103: 		}
104: 	}
105: 	return true;
106: }
107: 
108: unique_ptr<RowGroup> RowGroup::AlterType(ClientContext &context, const LogicalType &target_type, idx_t changed_idx,
109:                                          ExpressionExecutor &executor, TableScanState &scan_state,
110:                                          DataChunk &scan_chunk) {
111: 	Verify();
112: 
113: 	// construct a new column data for this type
114: 	auto column_data = make_shared<StandardColumnData>(GetTableInfo(), changed_idx, start, target_type);
115: 
116: 	ColumnAppendState append_state;
117: 	column_data->InitializeAppend(append_state);
118: 
119: 	// scan the original table, and fill the new column with the transformed value
120: 	InitializeScan(scan_state.row_group_scan_state);
121: 
122: 	Vector append_vector(target_type);
123: 	auto altered_col_stats = make_shared<SegmentStatistics>(target_type);
124: 	while (true) {
125: 		// scan the table
126: 		scan_chunk.Reset();
127: 		IndexScan(scan_state.row_group_scan_state, scan_chunk, true);
128: 		if (scan_chunk.size() == 0) {
129: 			break;
130: 		}
131: 		// execute the expression
132: 		executor.ExecuteExpression(scan_chunk, append_vector);
133: 		column_data->Append(*altered_col_stats->statistics, append_state, append_vector, scan_chunk.size());
134: 	}
135: 
136: 	// set up the row_group based on this row_group
137: 	auto row_group = make_unique<RowGroup>(db, table_info, this->start, this->count);
138: 	row_group->version_info = version_info;
139: 	for (idx_t i = 0; i < columns.size(); i++) {
140: 		if (i == changed_idx) {
141: 			// this is the altered column: use the new column
142: 			row_group->columns.push_back(move(column_data));
143: 			row_group->stats.push_back(move(altered_col_stats));
144: 		} else {
145: 			// this column was not altered: use the data directly
146: 			row_group->columns.push_back(columns[i]);
147: 			row_group->stats.push_back(stats[i]);
148: 		}
149: 	}
150: 	row_group->Verify();
151: 	return row_group;
152: }
153: 
154: unique_ptr<RowGroup> RowGroup::AddColumn(ClientContext &context, ColumnDefinition &new_column,
155:                                          ExpressionExecutor &executor, Expression *default_value, Vector &result) {
156: 	Verify();
157: 
158: 	// construct a new column data for the new column
159: 	auto added_column = make_shared<StandardColumnData>(GetTableInfo(), columns.size(), start, new_column.type);
160: 
161: 	auto added_col_stats = make_shared<SegmentStatistics>(new_column.type);
162: 	idx_t rows_to_write = this->count;
163: 	if (rows_to_write > 0) {
164: 		DataChunk dummy_chunk;
165: 
166: 		ColumnAppendState state;
167: 		added_column->InitializeAppend(state);
168: 		for (idx_t i = 0; i < rows_to_write; i += STANDARD_VECTOR_SIZE) {
169: 			idx_t rows_in_this_vector = MinValue<idx_t>(rows_to_write - i, STANDARD_VECTOR_SIZE);
170: 			if (default_value) {
171: 				dummy_chunk.SetCardinality(rows_in_this_vector);
172: 				executor.ExecuteExpression(dummy_chunk, result);
173: 			}
174: 			added_column->Append(*added_col_stats->statistics, state, result, rows_in_this_vector);
175: 		}
176: 	}
177: 
178: 	// set up the row_group based on this row_group
179: 	auto row_group = make_unique<RowGroup>(db, table_info, this->start, this->count);
180: 	row_group->version_info = version_info;
181: 	row_group->columns = columns;
182: 	row_group->stats = stats;
183: 	// now add the new column
184: 	row_group->columns.push_back(move(added_column));
185: 	row_group->stats.push_back(move(added_col_stats));
186: 
187: 	row_group->Verify();
188: 	return row_group;
189: }
190: 
191: unique_ptr<RowGroup> RowGroup::RemoveColumn(idx_t removed_column) {
192: 	Verify();
193: 
194: 	D_ASSERT(removed_column < columns.size());
195: 
196: 	auto row_group = make_unique<RowGroup>(db, table_info, this->start, this->count);
197: 	row_group->version_info = version_info;
198: 	row_group->columns = columns;
199: 	row_group->stats = stats;
200: 	// now remove the column
201: 	row_group->columns.erase(row_group->columns.begin() + removed_column);
202: 	row_group->stats.erase(row_group->stats.begin() + removed_column);
203: 
204: 	row_group->Verify();
205: 	return row_group;
206: }
207: 
208: void RowGroup::CommitDrop() {
209: 	for (idx_t column_idx = 0; column_idx < columns.size(); column_idx++) {
210: 		CommitDropColumn(column_idx);
211: 	}
212: }
213: 
214: void RowGroup::CommitDropColumn(idx_t column_idx) {
215: 	D_ASSERT(column_idx < columns.size());
216: 	columns[column_idx]->CommitDropColumn();
217: }
218: 
219: void RowGroupScanState::NextVector() {
220: 	vector_index++;
221: 	for (idx_t i = 0; i < parent.column_ids.size(); i++) {
222: 		column_scans[i].Next();
223: 	}
224: }
225: 
226: bool RowGroup::CheckZonemap(TableFilterSet &filters, const vector<column_t> &column_ids) {
227: 	for (auto &entry : filters.filters) {
228: 		auto column_index = entry.first;
229: 		auto &filter = entry.second;
230: 		auto base_column_index = column_ids[column_index];
231: 
232: 		auto propagate_result = filter->CheckStatistics(*stats[base_column_index]->statistics);
233: 		if (propagate_result == FilterPropagateResult::FILTER_ALWAYS_FALSE ||
234: 		    propagate_result == FilterPropagateResult::FILTER_FALSE_OR_NULL) {
235: 			return false;
236: 		}
237: 	}
238: 	return true;
239: }
240: 
241: bool RowGroup::CheckZonemapSegments(RowGroupScanState &state) {
242: 	if (!state.parent.table_filters) {
243: 		return true;
244: 	}
245: 	auto &column_ids = state.parent.column_ids;
246: 	for (auto &entry : state.parent.table_filters->filters) {
247: 		D_ASSERT(entry.first < column_ids.size());
248: 		auto column_idx = entry.first;
249: 		auto base_column_idx = column_ids[column_idx];
250: 		bool read_segment = columns[base_column_idx]->CheckZonemap(state.column_scans[column_idx], *entry.second);
251: 		if (!read_segment) {
252: 			idx_t target_row =
253: 			    state.column_scans[column_idx].current->start + state.column_scans[column_idx].current->count;
254: 			D_ASSERT(target_row >= this->start);
255: 			D_ASSERT(target_row <= this->start + this->count);
256: 			idx_t target_vector_index = (target_row - this->start) / STANDARD_VECTOR_SIZE;
257: 			if (state.vector_index == target_vector_index) {
258: 				// we can't skip any full vectors because this segment contains less than a full vector
259: 				// for now we just bail-out
260: 				// FIXME: we could check if we can ALSO skip the next segments, in which case skipping a full vector
261: 				// might be possible
262: 				// we don't care that much though, since a single segment that fits less than a full vector is
263: 				// exceedingly rare
264: 				return true;
265: 			}
266: 			while (state.vector_index < target_vector_index) {
267: 				state.NextVector();
268: 			}
269: 			return false;
270: 		}
271: 	}
272: 
273: 	return true;
274: }
275: 
276: template <bool SCAN_DELETES, bool SCAN_COMMITTED, bool ALLOW_UPDATES>
277: void RowGroup::TemplatedScan(Transaction *transaction, RowGroupScanState &state, DataChunk &result) {
278: 	auto &table_filters = state.parent.table_filters;
279: 	auto &column_ids = state.parent.column_ids;
280: 	auto &adaptive_filter = state.parent.adaptive_filter;
281: 	while (true) {
282: 		if (state.vector_index * STANDARD_VECTOR_SIZE >= state.max_row) {
283: 			// exceeded the amount of rows to scan
284: 			return;
285: 		}
286: 		idx_t current_row = state.vector_index * STANDARD_VECTOR_SIZE;
287: 		auto max_count = MinValue<idx_t>(STANDARD_VECTOR_SIZE, state.max_row - current_row);
288: 		// idx_t vector_offset = (current_row - state.base_row) / STANDARD_VECTOR_SIZE;
289: 		// //! first check the zonemap if we have to scan this partition
290: 		if (!CheckZonemapSegments(state)) {
291: 			continue;
292: 		}
293: 		// // second, scan the version chunk manager to figure out which tuples to load for this transaction
294: 		idx_t count;
295: 		SelectionVector valid_sel(STANDARD_VECTOR_SIZE);
296: 		if (SCAN_DELETES) {
297: 			D_ASSERT(transaction);
298: 			count = state.row_group->GetSelVector(*transaction, state.vector_index, valid_sel, max_count);
299: 			if (count == 0) {
300: 				// nothing to scan for this vector, skip the entire vector
301: 				state.NextVector();
302: 				continue;
303: 			}
304: 		} else {
305: 			count = max_count;
306: 		}
307: 		idx_t approved_tuple_count = count;
308: 
309: 		for (idx_t i = 0; i < column_ids.size(); i++) {
310: 			auto column = column_ids[i];
311: 			if (column == COLUMN_IDENTIFIER_ROW_ID) {
312: 				// scan row id
313: 				D_ASSERT(result.data[i].GetType().InternalType() == ROW_TYPE);
314: 				result.data[i].Sequence(this->start + current_row, 1);
315: 			} else {
316: 				if (SCAN_COMMITTED) {
317: 					columns[column]->ScanCommitted(state.vector_index, state.column_scans[i], result.data[i],
318: 					                               ALLOW_UPDATES);
319: 				} else {
320: 					D_ASSERT(transaction);
321: 					D_ASSERT(ALLOW_UPDATES);
322: 					columns[column]->Scan(*transaction, state.vector_index, state.column_scans[i], result.data[i]);
323: 				}
324: 			}
325: 		}
326: 		if (table_filters) {
327: 			SelectionVector sel;
328: 			if (count != max_count) {
329: 				sel.Initialize(valid_sel);
330: 			} else {
331: 				sel.Initialize(FlatVector::INCREMENTAL_SELECTION_VECTOR);
332: 			}
333: 			//! First, we scan the columns with filters, fetch their data and generate a selection vector.
334: 			//! get runtime statistics
335: 			auto start_time = high_resolution_clock::now();
336: 			for (idx_t i = 0; i < adaptive_filter->permutation.size(); i++) {
337: 				auto tf_idx = adaptive_filter->permutation[i];
338: 				D_ASSERT(table_filters->filters.count(tf_idx) == 1);
339: 				auto &filter = table_filters->filters[tf_idx];
340: 				UncompressedSegment::FilterSelection(sel, result.data[tf_idx], *filter, approved_tuple_count,
341: 				                                     FlatVector::Validity(result.data[tf_idx]));
342: 			}
343: 			auto end_time = high_resolution_clock::now();
344: 			if (adaptive_filter && adaptive_filter->permutation.size() > 1) {
345: 				adaptive_filter->AdaptRuntimeStatistics(duration_cast<duration<double>>(end_time - start_time).count());
346: 			}
347: 
348: 			if (approved_tuple_count == 0) {
349: 				result.Reset();
350: 				state.vector_index++;
351: 				continue;
352: 			}
353: 			if (approved_tuple_count != max_count) {
354: 				result.Slice(sel, approved_tuple_count);
355: 			}
356: 		} else if (count != max_count) {
357: 			result.Slice(valid_sel, count);
358: 		}
359: 		D_ASSERT(approved_tuple_count > 0);
360: 		result.SetCardinality(approved_tuple_count);
361: 		state.vector_index++;
362: 		break;
363: 	}
364: }
365: 
366: void RowGroup::Scan(Transaction &transaction, RowGroupScanState &state, DataChunk &result) {
367: 	TemplatedScan<true, false, true>(&transaction, state, result);
368: }
369: 
370: void RowGroup::IndexScan(RowGroupScanState &state, DataChunk &result, bool allow_pending_updates) {
371: 	if (allow_pending_updates) {
372: 		TemplatedScan<false, true, true>(nullptr, state, result);
373: 	} else {
374: 		TemplatedScan<false, true, false>(nullptr, state, result);
375: 	}
376: }
377: 
378: ChunkInfo *RowGroup::GetChunkInfo(idx_t vector_idx) {
379: 	if (!version_info) {
380: 		return nullptr;
381: 	}
382: 	return version_info->info[vector_idx].get();
383: }
384: 
385: idx_t RowGroup::GetSelVector(Transaction &transaction, idx_t vector_idx, SelectionVector &sel_vector, idx_t max_count) {
386: 	lock_guard<mutex> lock(row_group_lock);
387: 
388: 	auto info = GetChunkInfo(vector_idx);
389: 	if (!info) {
390: 		return max_count;
391: 	}
392: 	return info->GetSelVector(transaction, sel_vector, max_count);
393: }
394: 
395: bool RowGroup::Fetch(Transaction &transaction, idx_t row) {
396: 	D_ASSERT(row < this->count);
397: 	lock_guard<mutex> lock(row_group_lock);
398: 
399: 	idx_t vector_index = row / STANDARD_VECTOR_SIZE;
400: 	auto info = GetChunkInfo(vector_index);
401: 	if (!info) {
402: 		return true;
403: 	}
404: 	return info->Fetch(transaction, row - vector_index * STANDARD_VECTOR_SIZE);
405: }
406: 
407: void RowGroup::FetchRow(Transaction &transaction, ColumnFetchState &state, const vector<column_t> &column_ids,
408:                         row_t row_id, DataChunk &result, idx_t result_idx) {
409: 	for (idx_t col_idx = 0; col_idx < column_ids.size(); col_idx++) {
410: 		auto column = column_ids[col_idx];
411: 		if (column == COLUMN_IDENTIFIER_ROW_ID) {
412: 			// row id column: fill in the row ids
413: 			D_ASSERT(result.data[col_idx].GetType().InternalType() == PhysicalType::INT64);
414: 			result.data[col_idx].SetVectorType(VectorType::FLAT_VECTOR);
415: 			auto data = FlatVector::GetData<row_t>(result.data[col_idx]);
416: 			data[result_idx] = row_id;
417: 		} else {
418: 			// regular column: fetch data from the base column
419: 			columns[column]->FetchRow(transaction, state, row_id, result.data[col_idx], result_idx);
420: 		}
421: 	}
422: }
423: 
424: void RowGroup::AppendVersionInfo(Transaction &transaction, idx_t row_group_start, idx_t count,
425:                                  transaction_t commit_id) {
426: 	idx_t row_group_end = row_group_start + count;
427: 	lock_guard<mutex> lock(row_group_lock);
428: 
429: 	this->count += count;
430: 	D_ASSERT(this->count <= RowGroup::ROW_GROUP_SIZE);
431: 
432: 	// create the version_info if it doesn't exist yet
433: 	if (!version_info) {
434: 		version_info = make_unique<VersionNode>();
435: 	}
436: 	idx_t start_vector_idx = row_group_start / STANDARD_VECTOR_SIZE;
437: 	idx_t end_vector_idx = (row_group_end - 1) / STANDARD_VECTOR_SIZE;
438: 	for (idx_t vector_idx = start_vector_idx; vector_idx <= end_vector_idx; vector_idx++) {
439: 		idx_t start = vector_idx == start_vector_idx ? row_group_start - start_vector_idx * STANDARD_VECTOR_SIZE : 0;
440: 		idx_t end =
441: 		    vector_idx == end_vector_idx ? row_group_end - end_vector_idx * STANDARD_VECTOR_SIZE : STANDARD_VECTOR_SIZE;
442: 		if (start == 0 && end == STANDARD_VECTOR_SIZE) {
443: 			// entire vector is encapsulated by append: append a single constant
444: 			auto constant_info = make_unique<ChunkConstantInfo>(this->start + vector_idx * STANDARD_VECTOR_SIZE);
445: 			constant_info->insert_id = commit_id;
446: 			constant_info->delete_id = NOT_DELETED_ID;
447: 			version_info->info[vector_idx] = move(constant_info);
448: 		} else {
449: 			// part of a vector is encapsulated: append to that part
450: 			ChunkVectorInfo *info;
451: 			if (!version_info->info[vector_idx]) {
452: 				// first time appending to this vector: create new info
453: 				auto insert_info = make_unique<ChunkVectorInfo>(this->start + vector_idx * STANDARD_VECTOR_SIZE);
454: 				info = insert_info.get();
455: 				version_info->info[vector_idx] = move(insert_info);
456: 			} else {
457: 				D_ASSERT(version_info->info[vector_idx]->type == ChunkInfoType::VECTOR_INFO);
458: 				// use existing vector
459: 				info = (ChunkVectorInfo *)version_info->info[vector_idx].get();
460: 			}
461: 			info->Append(start, end, commit_id);
462: 		}
463: 	}
464: }
465: 
466: void RowGroup::CommitAppend(transaction_t commit_id, idx_t row_group_start, idx_t count) {
467: 	D_ASSERT(version_info.get());
468: 	idx_t row_group_end = row_group_start + count;
469: 	lock_guard<mutex> lock(row_group_lock);
470: 
471: 	idx_t start_vector_idx = row_group_start / STANDARD_VECTOR_SIZE;
472: 	idx_t end_vector_idx = (row_group_end - 1) / STANDARD_VECTOR_SIZE;
473: 	for (idx_t vector_idx = start_vector_idx; vector_idx <= end_vector_idx; vector_idx++) {
474: 		idx_t start = vector_idx == start_vector_idx ? row_group_start - start_vector_idx * STANDARD_VECTOR_SIZE : 0;
475: 		idx_t end =
476: 		    vector_idx == end_vector_idx ? row_group_end - end_vector_idx * STANDARD_VECTOR_SIZE : STANDARD_VECTOR_SIZE;
477: 
478: 		auto info = version_info->info[vector_idx].get();
479: 		info->CommitAppend(commit_id, start, end);
480: 	}
481: }
482: 
483: void RowGroup::RevertAppend(idx_t row_group_start) {
484: 	if (!version_info) {
485: 		return;
486: 	}
487: 	idx_t start_row = row_group_start - this->start;
488: 	idx_t start_vector_idx = (start_row + (STANDARD_VECTOR_SIZE - 1)) / STANDARD_VECTOR_SIZE;
489: 	for (idx_t vector_idx = start_vector_idx; vector_idx < RowGroup::ROW_GROUP_VECTOR_COUNT; vector_idx++) {
490: 		version_info->info[vector_idx].reset();
491: 	}
492: 	for (auto &column : columns) {
493: 		column->RevertAppend(row_group_start);
494: 	}
495: 	this->count = MinValue<idx_t>(row_group_start - this->start, this->count);
496: 	Verify();
497: }
498: 
499: void RowGroup::InitializeAppend(Transaction &transaction, RowGroupAppendState &append_state,
500:                                 idx_t remaining_append_count) {
501: 	append_state.row_group = this;
502: 	append_state.offset_in_row_group = this->count;
503: 	// for each column, initialize the append state
504: 	append_state.states = unique_ptr<ColumnAppendState[]>(new ColumnAppendState[columns.size()]);
505: 	for (idx_t i = 0; i < columns.size(); i++) {
506: 		columns[i]->InitializeAppend(append_state.states[i]);
507: 	}
508: 	// append the version info for this row_group
509: 	idx_t append_count = MinValue<idx_t>(remaining_append_count, RowGroup::ROW_GROUP_SIZE - this->count);
510: 	AppendVersionInfo(transaction, this->count, append_count, transaction.transaction_id);
511: }
512: 
513: void RowGroup::Append(RowGroupAppendState &state, DataChunk &chunk, idx_t append_count) {
514: 	// append to the current row_group
515: 	for (idx_t i = 0; i < columns.size(); i++) {
516: 		columns[i]->Append(*stats[i]->statistics, state.states[i], chunk.data[i], append_count);
517: 	}
518: 	state.offset_in_row_group += append_count;
519: }
520: 
521: void RowGroup::Update(Transaction &transaction, DataChunk &update_chunk, Vector &row_ids,
522:                       const vector<column_t> &column_ids) {
523: 	auto ids = FlatVector::GetData<row_t>(row_ids);
524: #ifdef DEBUG
525: 	for (size_t i = 0; i < update_chunk.size(); i++) {
526: 		D_ASSERT(ids[i] >= row_t(this->start) && ids[i] < row_t(this->start + this->count));
527: 	}
528: #endif
529: 	for (idx_t i = 0; i < column_ids.size(); i++) {
530: 		auto column = column_ids[i];
531: 		D_ASSERT(column != COLUMN_IDENTIFIER_ROW_ID);
532: 		D_ASSERT(columns[column]->type.id() == update_chunk.data[i].GetType().id());
533: 		columns[column]->Update(transaction, column, update_chunk.data[i], ids, update_chunk.size());
534: 		MergeStatistics(column, *columns[column]->GetUpdateStatistics());
535: 	}
536: }
537: 
538: void RowGroup::UpdateColumn(Transaction &transaction, DataChunk &updates, Vector &row_ids,
539:                             const vector<column_t> &column_path) {
540: 	D_ASSERT(updates.ColumnCount() == 1);
541: 	auto ids = FlatVector::GetData<row_t>(row_ids);
542: 
543: 	auto primary_column_idx = column_path[0];
544: 	D_ASSERT(primary_column_idx != COLUMN_IDENTIFIER_ROW_ID);
545: 	D_ASSERT(primary_column_idx < columns.size());
546: 	columns[primary_column_idx]->UpdateColumn(transaction, column_path, updates.data[0], ids, updates.size(), 1);
547: 	MergeStatistics(primary_column_idx, *columns[primary_column_idx]->GetUpdateStatistics());
548: }
549: 
550: unique_ptr<BaseStatistics> RowGroup::GetStatistics(idx_t column_idx) {
551: 	D_ASSERT(column_idx < stats.size());
552: 
553: 	lock_guard<mutex> slock(stats_lock);
554: 	return stats[column_idx]->statistics->Copy();
555: }
556: 
557: void RowGroup::MergeStatistics(idx_t column_idx, BaseStatistics &other) {
558: 	D_ASSERT(column_idx < stats.size());
559: 
560: 	lock_guard<mutex> slock(stats_lock);
561: 	stats[column_idx]->statistics->Merge(other);
562: }
563: 
564: RowGroupPointer RowGroup::Checkpoint(TableDataWriter &writer, vector<unique_ptr<BaseStatistics>> &global_stats) {
565: 	vector<unique_ptr<ColumnCheckpointState>> states;
566: 	states.reserve(columns.size());
567: 
568: 	// checkpoint the individual columns of the row group
569: 	for (idx_t column_idx = 0; column_idx < columns.size(); column_idx++) {
570: 		auto &column = columns[column_idx];
571: 		auto checkpoint_state = column->Checkpoint(*this, writer, column_idx);
572: 		D_ASSERT(checkpoint_state);
573: 
574: 		auto stats = checkpoint_state->GetStatistics();
575: 		D_ASSERT(stats);
576: 
577: 		global_stats[column_idx]->Merge(*stats);
578: 		states.push_back(move(checkpoint_state));
579: 	}
580: 
581: 	// construct the row group pointer and write the column meta data to disk
582: 	D_ASSERT(states.size() == columns.size());
583: 	RowGroupPointer row_group_pointer;
584: 	row_group_pointer.row_start = start;
585: 	row_group_pointer.tuple_count = count;
586: 	for (auto &state : states) {
587: 		// get the current position of the meta data writer
588: 		auto &meta_writer = writer.GetMetaWriter();
589: 		auto pointer = meta_writer.GetBlockPointer();
590: 
591: 		// store the stats and the data pointers in the row group pointers
592: 		row_group_pointer.data_pointers.push_back(pointer);
593: 		row_group_pointer.statistics.push_back(move(state->global_stats));
594: 
595: 		// now flush the actual column data to disk
596: 		state->FlushToDisk();
597: 	}
598: 	row_group_pointer.versions = version_info;
599: 	Verify();
600: 	return row_group_pointer;
601: }
602: 
603: void RowGroup::CheckpointDeletes(VersionNode *versions, Serializer &serializer) {
604: 	if (!versions) {
605: 		// no version information: write nothing
606: 		serializer.Write<idx_t>(0);
607: 		return;
608: 	}
609: 	// first count how many ChunkInfo's we need to deserialize
610: 	idx_t chunk_info_count = 0;
611: 	for (idx_t vector_idx = 0; vector_idx < RowGroup::ROW_GROUP_VECTOR_COUNT; vector_idx++) {
612: 		auto chunk_info = versions->info[vector_idx].get();
613: 		if (!chunk_info) {
614: 			continue;
615: 		}
616: 		chunk_info_count++;
617: 	}
618: 	// now serialize the actual version information
619: 	serializer.Write<idx_t>(chunk_info_count);
620: 	for (idx_t vector_idx = 0; vector_idx < RowGroup::ROW_GROUP_VECTOR_COUNT; vector_idx++) {
621: 		auto chunk_info = versions->info[vector_idx].get();
622: 		if (!chunk_info) {
623: 			continue;
624: 		}
625: 		serializer.Write<idx_t>(vector_idx);
626: 		chunk_info->Serialize(serializer);
627: 	}
628: }
629: 
630: shared_ptr<VersionNode> RowGroup::DeserializeDeletes(Deserializer &source) {
631: 	auto chunk_count = source.Read<idx_t>();
632: 	if (chunk_count == 0) {
633: 		// no deletes
634: 		return nullptr;
635: 	}
636: 	auto version_info = make_shared<VersionNode>();
637: 	for (idx_t i = 0; i < chunk_count; i++) {
638: 		idx_t vector_index = source.Read<idx_t>();
639: 		version_info->info[vector_index] = ChunkInfo::Deserialize(source);
640: 	}
641: 	return version_info;
642: }
643: 
644: void RowGroup::Serialize(RowGroupPointer &pointer, Serializer &serializer) {
645: 	serializer.Write<uint64_t>(pointer.row_start);
646: 	serializer.Write<uint64_t>(pointer.tuple_count);
647: 	for (auto &stats : pointer.statistics) {
648: 		stats->Serialize(serializer);
649: 	}
650: 	for (auto &data_pointer : pointer.data_pointers) {
651: 		serializer.Write<block_id_t>(data_pointer.block_id);
652: 		serializer.Write<uint64_t>(data_pointer.offset);
653: 	}
654: 	CheckpointDeletes(pointer.versions.get(), serializer);
655: }
656: 
657: RowGroupPointer RowGroup::Deserialize(Deserializer &source, const vector<ColumnDefinition> &columns) {
658: 	RowGroupPointer result;
659: 	result.row_start = source.Read<uint64_t>();
660: 	result.tuple_count = source.Read<uint64_t>();
661: 
662: 	result.data_pointers.reserve(columns.size());
663: 	result.statistics.reserve(columns.size());
664: 
665: 	for (idx_t i = 0; i < columns.size(); i++) {
666: 		auto stats = BaseStatistics::Deserialize(source, columns[i].type);
667: 		result.statistics.push_back(move(stats));
668: 	}
669: 	for (idx_t i = 0; i < columns.size(); i++) {
670: 		BlockPointer pointer;
671: 		pointer.block_id = source.Read<block_id_t>();
672: 		pointer.offset = source.Read<uint64_t>();
673: 		result.data_pointers.push_back(pointer);
674: 	}
675: 	result.versions = DeserializeDeletes(source);
676: 	return result;
677: }
678: 
679: //===--------------------------------------------------------------------===//
680: // GetStorageInfo
681: //===--------------------------------------------------------------------===//
682: void RowGroup::GetStorageInfo(idx_t row_group_index, vector<vector<Value>> &result) {
683: 	for (idx_t col_idx = 0; col_idx < columns.size(); col_idx++) {
684: 		columns[col_idx]->GetStorageInfo(row_group_index, {col_idx}, result);
685: 	}
686: }
687: 
688: //===--------------------------------------------------------------------===//
689: // Version Delete Information
690: //===--------------------------------------------------------------------===//
691: class VersionDeleteState {
692: public:
693: 	VersionDeleteState(RowGroup &info, Transaction &transaction, DataTable *table, idx_t base_row)
694: 	    : info(info), transaction(transaction), table(table), current_info(nullptr), current_chunk(INVALID_INDEX),
695: 	      count(0), base_row(base_row), delete_count(0) {
696: 	}
697: 
698: 	RowGroup &info;
699: 	Transaction &transaction;
700: 	DataTable *table;
701: 	ChunkVectorInfo *current_info;
702: 	idx_t current_chunk;
703: 	row_t rows[STANDARD_VECTOR_SIZE];
704: 	idx_t count;
705: 	idx_t base_row;
706: 	idx_t chunk_row;
707: 	idx_t delete_count;
708: 
709: public:
710: 	void Delete(row_t row_id);
711: 	void Flush();
712: };
713: 
714: idx_t RowGroup::Delete(Transaction &transaction, DataTable *table, row_t *ids, idx_t count) {
715: 	lock_guard<mutex> lock(row_group_lock);
716: 	VersionDeleteState del_state(*this, transaction, table, this->start);
717: 
718: 	// obtain a write lock
719: 	for (idx_t i = 0; i < count; i++) {
720: 		D_ASSERT(ids[i] >= 0);
721: 		D_ASSERT(idx_t(ids[i]) >= this->start && idx_t(ids[i]) < this->start + this->count);
722: 		del_state.Delete(ids[i] - this->start);
723: 	}
724: 	del_state.Flush();
725: 	return del_state.delete_count;
726: }
727: 
728: void RowGroup::Verify() {
729: #ifdef DEBUG
730: 	for (auto &column : columns) {
731: 		column->Verify(*this);
732: 	}
733: #endif
734: }
735: 
736: void VersionDeleteState::Delete(row_t row_id) {
737: 	D_ASSERT(row_id >= 0);
738: 	idx_t vector_idx = row_id / STANDARD_VECTOR_SIZE;
739: 	idx_t idx_in_vector = row_id - vector_idx * STANDARD_VECTOR_SIZE;
740: 	if (current_chunk != vector_idx) {
741: 		Flush();
742: 
743: 		if (!info.version_info) {
744: 			info.version_info = make_unique<VersionNode>();
745: 		}
746: 
747: 		if (!info.version_info->info[vector_idx]) {
748: 			// no info yet: create it
749: 			info.version_info->info[vector_idx] =
750: 			    make_unique<ChunkVectorInfo>(info.start + vector_idx * STANDARD_VECTOR_SIZE);
751: 		} else if (info.version_info->info[vector_idx]->type == ChunkInfoType::CONSTANT_INFO) {
752: 			auto &constant = (ChunkConstantInfo &)*info.version_info->info[vector_idx];
753: 			// info exists but it's a constant info: convert to a vector info
754: 			auto new_info = make_unique<ChunkVectorInfo>(info.start + vector_idx * STANDARD_VECTOR_SIZE);
755: 			new_info->insert_id = constant.insert_id.load();
756: 			for (idx_t i = 0; i < STANDARD_VECTOR_SIZE; i++) {
757: 				new_info->inserted[i] = constant.insert_id.load();
758: 			}
759: 			info.version_info->info[vector_idx] = move(new_info);
760: 		}
761: 		D_ASSERT(info.version_info->info[vector_idx]->type == ChunkInfoType::VECTOR_INFO);
762: 		current_info = (ChunkVectorInfo *)info.version_info->info[vector_idx].get();
763: 		current_chunk = vector_idx;
764: 		chunk_row = vector_idx * STANDARD_VECTOR_SIZE;
765: 	}
766: 	rows[count++] = idx_in_vector;
767: }
768: 
769: void VersionDeleteState::Flush() {
770: 	if (count == 0) {
771: 		return;
772: 	}
773: 	// delete in the current info
774: 	delete_count += current_info->Delete(transaction, rows, count);
775: 	// now push the delete into the undo buffer
776: 	transaction.PushDelete(table, current_info, rows, count, base_row + chunk_row);
777: 	count = 0;
778: }
779: 
780: } // namespace duckdb
[end of src/storage/table/row_group.cpp]
[start of src/storage/table/standard_column_data.cpp]
1: #include "duckdb/storage/table/standard_column_data.hpp"
2: #include "duckdb/storage/table/scan_state.hpp"
3: #include "duckdb/storage/table/update_segment.hpp"
4: #include "duckdb/storage/table/append_state.hpp"
5: #include "duckdb/storage/table/persistent_segment.hpp"
6: #include "duckdb/storage/table/transient_segment.hpp"
7: #include "duckdb/storage/data_table.hpp"
8: #include "duckdb/planner/table_filter.hpp"
9: 
10: namespace duckdb {
11: 
12: StandardColumnData::StandardColumnData(DataTableInfo &info, idx_t column_index, idx_t start_row, LogicalType type,
13:                                        ColumnData *parent)
14:     : ColumnData(info, column_index, start_row, move(type), parent), validity(info, 0, start_row, this) {
15: }
16: 
17: bool StandardColumnData::CheckZonemap(ColumnScanState &state, TableFilter &filter) {
18: 	if (!state.segment_checked) {
19: 		if (!state.current) {
20: 			return true;
21: 		}
22: 		state.segment_checked = true;
23: 		auto prune_result = filter.CheckStatistics(*state.current->stats.statistics);
24: 		if (prune_result != FilterPropagateResult::FILTER_ALWAYS_FALSE) {
25: 			return true;
26: 		}
27: 		if (updates) {
28: 			auto update_stats = updates->GetStatistics();
29: 			prune_result = filter.CheckStatistics(*update_stats);
30: 			return prune_result != FilterPropagateResult::FILTER_ALWAYS_FALSE;
31: 		} else {
32: 			return false;
33: 		}
34: 	} else {
35: 		return true;
36: 	}
37: }
38: 
39: void StandardColumnData::InitializeScan(ColumnScanState &state) {
40: 	// initialize the current segment
41: 	state.current = (ColumnSegment *)data.GetRootSegment();
42: 	state.row_index = state.current ? state.current->start : 0;
43: 	state.initialized = false;
44: 
45: 	// initialize the validity segment
46: 	ColumnScanState child_state;
47: 	validity.InitializeScan(child_state);
48: 	state.child_states.push_back(move(child_state));
49: }
50: 
51: void StandardColumnData::InitializeScanWithOffset(ColumnScanState &state, idx_t row_idx) {
52: 	state.current = (ColumnSegment *)data.GetSegment(row_idx);
53: 	state.row_index = row_idx;
54: 	state.initialized = false;
55: 
56: 	// initialize the validity segment
57: 	ColumnScanState child_state;
58: 	validity.InitializeScanWithOffset(child_state, row_idx);
59: 	state.child_states.push_back(move(child_state));
60: }
61: 
62: void StandardColumnData::Scan(Transaction &transaction, idx_t vector_index, ColumnScanState &state, Vector &result) {
63: 	D_ASSERT(state.row_index == state.child_states[0].row_index);
64: 	ColumnData::Scan(transaction, vector_index, state, result);
65: 	validity.Scan(transaction, vector_index, state.child_states[0], result);
66: 	state.Next();
67: }
68: 
69: void StandardColumnData::ScanCommitted(idx_t vector_index, ColumnScanState &state, Vector &result, bool allow_updates) {
70: 	D_ASSERT(state.row_index == state.child_states[0].row_index);
71: 	ColumnData::ScanCommitted(vector_index, state, result, allow_updates);
72: 	validity.ScanCommitted(vector_index, state.child_states[0], result, allow_updates);
73: 	state.Next();
74: }
75: 
76: void StandardColumnData::InitializeAppend(ColumnAppendState &state) {
77: 	ColumnData::InitializeAppend(state);
78: 
79: 	ColumnAppendState child_append;
80: 	validity.InitializeAppend(child_append);
81: 	state.child_appends.push_back(move(child_append));
82: }
83: 
84: void StandardColumnData::AppendData(BaseStatistics &stats, ColumnAppendState &state, VectorData &vdata, idx_t count) {
85: 	ColumnData::AppendData(stats, state, vdata, count);
86: 
87: 	validity.AppendData(*stats.validity_stats, state.child_appends[0], vdata, count);
88: }
89: 
90: void StandardColumnData::RevertAppend(row_t start_row) {
91: 	ColumnData::RevertAppend(start_row);
92: 
93: 	validity.RevertAppend(start_row);
94: }
95: 
96: void StandardColumnData::Fetch(ColumnScanState &state, row_t row_id, Vector &result) {
97: 	// fetch validity mask
98: 	if (state.child_states.empty()) {
99: 		ColumnScanState child_state;
100: 		state.child_states.push_back(move(child_state));
101: 	}
102: 	validity.Fetch(state.child_states[0], row_id, result);
103: 	ColumnData::Fetch(state, row_id, result);
104: }
105: 
106: void StandardColumnData::Update(Transaction &transaction, idx_t column_index, Vector &update_vector, row_t *row_ids,
107:                                 idx_t update_count) {
108: 	ColumnData::Update(transaction, column_index, update_vector, row_ids, update_count);
109: 	validity.Update(transaction, column_index, update_vector, row_ids, update_count);
110: }
111: 
112: void StandardColumnData::UpdateColumn(Transaction &transaction, const vector<column_t> &column_path,
113:                                       Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t depth) {
114: 	if (depth >= column_path.size()) {
115: 		// update this column
116: 		ColumnData::Update(transaction, column_path[0], update_vector, row_ids, update_count);
117: 	} else {
118: 		// update the child column (i.e. the validity column)
119: 		validity.UpdateColumn(transaction, column_path, update_vector, row_ids, update_count, depth + 1);
120: 	}
121: }
122: 
123: unique_ptr<BaseStatistics> StandardColumnData::GetUpdateStatistics() {
124: 	auto stats = updates ? updates->GetStatistics() : nullptr;
125: 	auto validity_stats = validity.GetUpdateStatistics();
126: 	if (!stats && !validity_stats) {
127: 		return nullptr;
128: 	}
129: 	if (!stats) {
130: 		stats = BaseStatistics::CreateEmpty(type);
131: 	}
132: 	stats->validity_stats = move(validity_stats);
133: 	return stats;
134: }
135: 
136: void StandardColumnData::FetchRow(Transaction &transaction, ColumnFetchState &state, row_t row_id, Vector &result,
137:                                   idx_t result_idx) {
138: 	// find the segment the row belongs to
139: 	if (state.child_states.empty()) {
140: 		auto child_state = make_unique<ColumnFetchState>();
141: 		state.child_states.push_back(move(child_state));
142: 	}
143: 	validity.FetchRow(transaction, *state.child_states[0], row_id, result, result_idx);
144: 	ColumnData::FetchRow(transaction, state, row_id, result, result_idx);
145: }
146: 
147: void StandardColumnData::CommitDropColumn() {
148: 	ColumnData::CommitDropColumn();
149: 	validity.CommitDropColumn();
150: }
151: 
152: struct StandardColumnCheckpointState : public ColumnCheckpointState {
153: 	StandardColumnCheckpointState(RowGroup &row_group, ColumnData &column_data, TableDataWriter &writer)
154: 	    : ColumnCheckpointState(row_group, column_data, writer) {
155: 	}
156: 
157: 	unique_ptr<BaseStatistics> GetStatistics() override {
158: 		auto stats = global_stats->Copy();
159: 		stats->validity_stats = validity_state->GetStatistics();
160: 		return stats;
161: 	}
162: 	unique_ptr<ColumnCheckpointState> validity_state;
163: 
164: 	void FlushToDisk() override {
165: 		ColumnCheckpointState::FlushToDisk();
166: 		validity_state->FlushToDisk();
167: 	}
168: };
169: 
170: unique_ptr<ColumnCheckpointState> StandardColumnData::CreateCheckpointState(RowGroup &row_group,
171:                                                                             TableDataWriter &writer) {
172: 	return make_unique<StandardColumnCheckpointState>(row_group, *this, writer);
173: }
174: 
175: unique_ptr<ColumnCheckpointState> StandardColumnData::Checkpoint(RowGroup &row_group, TableDataWriter &writer,
176:                                                                  idx_t column_idx) {
177: 	auto base_state = ColumnData::Checkpoint(row_group, writer, column_idx);
178: 	auto &checkpoint_state = (StandardColumnCheckpointState &)*base_state;
179: 	checkpoint_state.validity_state = validity.Checkpoint(row_group, writer, column_idx);
180: 	return base_state;
181: }
182: 
183: void StandardColumnData::Initialize(PersistentColumnData &column_data) {
184: 	auto &persistent = (StandardPersistentColumnData &)column_data;
185: 	ColumnData::Initialize(column_data);
186: 	validity.Initialize(*persistent.validity);
187: }
188: 
189: shared_ptr<ColumnData> StandardColumnData::Deserialize(DataTableInfo &info, idx_t column_index, idx_t start_row,
190:                                                        Deserializer &source, const LogicalType &type) {
191: 	auto result = make_shared<StandardColumnData>(info, column_index, start_row, type, nullptr);
192: 	BaseDeserialize(info.db, source, type, *result);
193: 	ColumnData::BaseDeserialize(info.db, source, LogicalType(LogicalTypeId::VALIDITY), result->validity);
194: 	return move(result);
195: }
196: 
197: void StandardColumnData::GetStorageInfo(idx_t row_group_index, vector<idx_t> col_path, vector<vector<Value>> &result) {
198: 	ColumnData::GetStorageInfo(row_group_index, col_path, result);
199: 	col_path.push_back(0);
200: 	validity.GetStorageInfo(row_group_index, move(col_path), result);
201: }
202: 
203: } // namespace duckdb
[end of src/storage/table/standard_column_data.cpp]
[start of src/storage/table/update_segment.cpp]
1: #include "duckdb/storage/table/update_segment.hpp"
2: #include "duckdb/transaction/update_info.hpp"
3: #include "duckdb/storage/table/column_data.hpp"
4: #include "duckdb/storage/statistics/numeric_statistics.hpp"
5: #include "duckdb/transaction/transaction.hpp"
6: #include "duckdb/storage/statistics/string_statistics.hpp"
7: #include "duckdb/storage/statistics/validity_statistics.hpp"
8: 
9: namespace duckdb {
10: 
11: static UpdateSegment::initialize_update_function_t GetInitializeUpdateFunction(PhysicalType type);
12: static UpdateSegment::fetch_update_function_t GetFetchUpdateFunction(PhysicalType type);
13: static UpdateSegment::fetch_committed_function_t GetFetchCommittedFunction(PhysicalType type);
14: static UpdateSegment::fetch_committed_range_function_t GetFetchCommittedRangeFunction(PhysicalType type);
15: 
16: static UpdateSegment::merge_update_function_t GetMergeUpdateFunction(PhysicalType type);
17: static UpdateSegment::rollback_update_function_t GetRollbackUpdateFunction(PhysicalType type);
18: static UpdateSegment::statistics_update_function_t GetStatisticsUpdateFunction(PhysicalType type);
19: static UpdateSegment::fetch_row_function_t GetFetchRowFunction(PhysicalType type);
20: 
21: UpdateSegment::UpdateSegment(ColumnData &column_data) : column_data(column_data), stats(column_data.type) {
22: 	auto physical_type = column_data.type.InternalType();
23: 
24: 	this->type_size = GetTypeIdSize(physical_type);
25: 
26: 	this->initialize_update_function = GetInitializeUpdateFunction(physical_type);
27: 	this->fetch_update_function = GetFetchUpdateFunction(physical_type);
28: 	this->fetch_committed_function = GetFetchCommittedFunction(physical_type);
29: 	this->fetch_committed_range = GetFetchCommittedRangeFunction(physical_type);
30: 	this->fetch_row_function = GetFetchRowFunction(physical_type);
31: 	this->merge_update_function = GetMergeUpdateFunction(physical_type);
32: 	this->rollback_update_function = GetRollbackUpdateFunction(physical_type);
33: 	this->statistics_update_function = GetStatisticsUpdateFunction(physical_type);
34: }
35: 
36: UpdateSegment::~UpdateSegment() {
37: }
38: 
39: void UpdateSegment::ClearUpdates() {
40: 	stats.Reset();
41: 	root.reset();
42: 	heap.Destroy();
43: }
44: 
45: //===--------------------------------------------------------------------===//
46: // Update Info Helpers
47: //===--------------------------------------------------------------------===//
48: Value UpdateInfo::GetValue(idx_t index) {
49: 	auto &type = segment->column_data.type;
50: 
51: 	switch (type.id()) {
52: 	case LogicalTypeId::VALIDITY:
53: 		return Value::BOOLEAN(((bool *)tuple_data)[index]);
54: 	case LogicalTypeId::INTEGER:
55: 		return Value::INTEGER(((int32_t *)tuple_data)[index]);
56: 	default:
57: 		throw NotImplementedException("Unimplemented type for UpdateInfo::GetValue");
58: 	}
59: }
60: 
61: void UpdateInfo::Print() {
62: 	Printer::Print(ToString());
63: }
64: 
65: string UpdateInfo::ToString() {
66: 	auto &type = segment->column_data.type;
67: 	string result = "Update Info [" + type.ToString() + ", Count: " + to_string(N) +
68: 	                ", Transaction Id: " + to_string(version_number) + "]\n";
69: 	for (idx_t i = 0; i < N; i++) {
70: 		result += to_string(tuples[i]) + ": " + GetValue(i).ToString() + "\n";
71: 	}
72: 	if (next) {
73: 		result += "\nChild Segment: " + next->ToString();
74: 	}
75: 	return result;
76: }
77: 
78: void UpdateInfo::Verify() {
79: #ifdef DEBUG
80: 	for (idx_t i = 1; i < N; i++) {
81: 		D_ASSERT(tuples[i] > tuples[i - 1] && tuples[i] < STANDARD_VECTOR_SIZE);
82: 	}
83: #endif
84: }
85: 
86: //===--------------------------------------------------------------------===//
87: // Update Fetch
88: //===--------------------------------------------------------------------===//
89: static void MergeValidityInfo(UpdateInfo *current, ValidityMask &result_mask) {
90: 	auto info_data = (bool *)current->tuple_data;
91: 	for (idx_t i = 0; i < current->N; i++) {
92: 		result_mask.Set(current->tuples[i], info_data[i]);
93: 	}
94: }
95: 
96: static void UpdateMergeValidity(transaction_t start_time, transaction_t transaction_id, UpdateInfo *info,
97:                                 Vector &result) {
98: 	auto &result_mask = FlatVector::Validity(result);
99: 	UpdateInfo::UpdatesForTransaction(info, start_time, transaction_id,
100: 	                                  [&](UpdateInfo *current) { MergeValidityInfo(current, result_mask); });
101: }
102: 
103: template <class T>
104: static void MergeUpdateInfo(UpdateInfo *current, T *result_data) {
105: 	auto info_data = (T *)current->tuple_data;
106: 	if (current->N == STANDARD_VECTOR_SIZE) {
107: 		// special case: update touches ALL tuples of this vector
108: 		// in this case we can just memcpy the data
109: 		// since the layout of the update info is guaranteed to be [0, 1, 2, 3, ...]
110: 		memcpy(result_data, info_data, sizeof(T) * current->N);
111: 	} else {
112: 		for (idx_t i = 0; i < current->N; i++) {
113: 			result_data[current->tuples[i]] = info_data[i];
114: 		}
115: 	}
116: }
117: 
118: template <class T>
119: static void UpdateMergeFetch(transaction_t start_time, transaction_t transaction_id, UpdateInfo *info, Vector &result) {
120: 	auto result_data = FlatVector::GetData<T>(result);
121: 	UpdateInfo::UpdatesForTransaction(info, start_time, transaction_id,
122: 	                                  [&](UpdateInfo *current) { MergeUpdateInfo<T>(current, result_data); });
123: }
124: 
125: static UpdateSegment::fetch_update_function_t GetFetchUpdateFunction(PhysicalType type) {
126: 	switch (type) {
127: 	case PhysicalType::BIT:
128: 		return UpdateMergeValidity;
129: 	case PhysicalType::BOOL:
130: 	case PhysicalType::INT8:
131: 		return UpdateMergeFetch<int8_t>;
132: 	case PhysicalType::INT16:
133: 		return UpdateMergeFetch<int16_t>;
134: 	case PhysicalType::INT32:
135: 		return UpdateMergeFetch<int32_t>;
136: 	case PhysicalType::INT64:
137: 		return UpdateMergeFetch<int64_t>;
138: 	case PhysicalType::UINT8:
139: 		return UpdateMergeFetch<uint8_t>;
140: 	case PhysicalType::UINT16:
141: 		return UpdateMergeFetch<uint16_t>;
142: 	case PhysicalType::UINT32:
143: 		return UpdateMergeFetch<uint32_t>;
144: 	case PhysicalType::UINT64:
145: 		return UpdateMergeFetch<uint64_t>;
146: 	case PhysicalType::INT128:
147: 		return UpdateMergeFetch<hugeint_t>;
148: 	case PhysicalType::FLOAT:
149: 		return UpdateMergeFetch<float>;
150: 	case PhysicalType::DOUBLE:
151: 		return UpdateMergeFetch<double>;
152: 	case PhysicalType::INTERVAL:
153: 		return UpdateMergeFetch<interval_t>;
154: 	case PhysicalType::VARCHAR:
155: 		return UpdateMergeFetch<string_t>;
156: 	default:
157: 		throw NotImplementedException("Unimplemented type for update segment");
158: 	}
159: }
160: 
161: void UpdateSegment::FetchUpdates(Transaction &transaction, idx_t vector_index, Vector &result) {
162: 	auto lock_handle = lock.GetSharedLock();
163: 	if (!root) {
164: 		return;
165: 	}
166: 	if (!root->info[vector_index]) {
167: 		return;
168: 	}
169: 	// FIXME: normalify if this is not the case... need to pass in count?
170: 	D_ASSERT(result.GetVectorType() == VectorType::FLAT_VECTOR);
171: 
172: 	fetch_update_function(transaction.start_time, transaction.transaction_id, root->info[vector_index]->info.get(),
173: 	                      result);
174: }
175: 
176: //===--------------------------------------------------------------------===//
177: // Fetch Committed
178: //===--------------------------------------------------------------------===//
179: static void FetchCommittedValidity(UpdateInfo *info, Vector &result) {
180: 	auto &result_mask = FlatVector::Validity(result);
181: 	MergeValidityInfo(info, result_mask);
182: }
183: 
184: template <class T>
185: static void TemplatedFetchCommitted(UpdateInfo *info, Vector &result) {
186: 	auto result_data = FlatVector::GetData<T>(result);
187: 	MergeUpdateInfo<T>(info, result_data);
188: }
189: 
190: static UpdateSegment::fetch_committed_function_t GetFetchCommittedFunction(PhysicalType type) {
191: 	switch (type) {
192: 	case PhysicalType::BIT:
193: 		return FetchCommittedValidity;
194: 	case PhysicalType::BOOL:
195: 	case PhysicalType::INT8:
196: 		return TemplatedFetchCommitted<int8_t>;
197: 	case PhysicalType::INT16:
198: 		return TemplatedFetchCommitted<int16_t>;
199: 	case PhysicalType::INT32:
200: 		return TemplatedFetchCommitted<int32_t>;
201: 	case PhysicalType::INT64:
202: 		return TemplatedFetchCommitted<int64_t>;
203: 	case PhysicalType::UINT8:
204: 		return TemplatedFetchCommitted<uint8_t>;
205: 	case PhysicalType::UINT16:
206: 		return TemplatedFetchCommitted<uint16_t>;
207: 	case PhysicalType::UINT32:
208: 		return TemplatedFetchCommitted<uint32_t>;
209: 	case PhysicalType::UINT64:
210: 		return TemplatedFetchCommitted<uint64_t>;
211: 	case PhysicalType::INT128:
212: 		return TemplatedFetchCommitted<hugeint_t>;
213: 	case PhysicalType::FLOAT:
214: 		return TemplatedFetchCommitted<float>;
215: 	case PhysicalType::DOUBLE:
216: 		return TemplatedFetchCommitted<double>;
217: 	case PhysicalType::INTERVAL:
218: 		return TemplatedFetchCommitted<interval_t>;
219: 	case PhysicalType::VARCHAR:
220: 		return TemplatedFetchCommitted<string_t>;
221: 	default:
222: 		throw NotImplementedException("Unimplemented type for update segment");
223: 	}
224: }
225: 
226: void UpdateSegment::FetchCommitted(idx_t vector_index, Vector &result) {
227: 	auto lock_handle = lock.GetSharedLock();
228: 
229: 	if (!root) {
230: 		return;
231: 	}
232: 	if (!root->info[vector_index]) {
233: 		return;
234: 	}
235: 	// FIXME: normalify if this is not the case... need to pass in count?
236: 	D_ASSERT(result.GetVectorType() == VectorType::FLAT_VECTOR);
237: 
238: 	fetch_committed_function(root->info[vector_index]->info.get(), result);
239: }
240: 
241: //===--------------------------------------------------------------------===//
242: // Fetch Range
243: //===--------------------------------------------------------------------===//
244: static void MergeUpdateInfoRangeValidity(UpdateInfo *current, idx_t start, idx_t end, idx_t result_offset,
245:                                          ValidityMask &result_mask) {
246: 	auto info_data = (bool *)current->tuple_data;
247: 	for (idx_t i = 0; i < current->N; i++) {
248: 		auto tuple_idx = current->tuples[i];
249: 		if (tuple_idx < start) {
250: 			continue;
251: 		} else if (tuple_idx >= end) {
252: 			break;
253: 		}
254: 		auto result_idx = result_offset + tuple_idx - start;
255: 		result_mask.Set(result_idx, info_data[i]);
256: 	}
257: }
258: 
259: static void FetchCommittedRangeValidity(UpdateInfo *info, idx_t start, idx_t end, idx_t result_offset, Vector &result) {
260: 	auto &result_mask = FlatVector::Validity(result);
261: 	MergeUpdateInfoRangeValidity(info, start, end, result_offset, result_mask);
262: }
263: 
264: template <class T>
265: static void MergeUpdateInfoRange(UpdateInfo *current, idx_t start, idx_t end, idx_t result_offset, T *result_data) {
266: 	auto info_data = (T *)current->tuple_data;
267: 	for (idx_t i = 0; i < current->N; i++) {
268: 		auto tuple_idx = current->tuples[i];
269: 		if (tuple_idx < start) {
270: 			continue;
271: 		} else if (tuple_idx >= end) {
272: 			break;
273: 		}
274: 		auto result_idx = result_offset + tuple_idx - start;
275: 		result_data[result_idx] = info_data[i];
276: 	}
277: }
278: 
279: template <class T>
280: static void TemplatedFetchCommittedRange(UpdateInfo *info, idx_t start, idx_t end, idx_t result_offset,
281:                                          Vector &result) {
282: 	auto result_data = FlatVector::GetData<T>(result);
283: 	MergeUpdateInfoRange<T>(info, start, end, result_offset, result_data);
284: }
285: 
286: static UpdateSegment::fetch_committed_range_function_t GetFetchCommittedRangeFunction(PhysicalType type) {
287: 	switch (type) {
288: 	case PhysicalType::BIT:
289: 		return FetchCommittedRangeValidity;
290: 	case PhysicalType::BOOL:
291: 	case PhysicalType::INT8:
292: 		return TemplatedFetchCommittedRange<int8_t>;
293: 	case PhysicalType::INT16:
294: 		return TemplatedFetchCommittedRange<int16_t>;
295: 	case PhysicalType::INT32:
296: 		return TemplatedFetchCommittedRange<int32_t>;
297: 	case PhysicalType::INT64:
298: 		return TemplatedFetchCommittedRange<int64_t>;
299: 	case PhysicalType::UINT8:
300: 		return TemplatedFetchCommittedRange<uint8_t>;
301: 	case PhysicalType::UINT16:
302: 		return TemplatedFetchCommittedRange<uint16_t>;
303: 	case PhysicalType::UINT32:
304: 		return TemplatedFetchCommittedRange<uint32_t>;
305: 	case PhysicalType::UINT64:
306: 		return TemplatedFetchCommittedRange<uint64_t>;
307: 	case PhysicalType::INT128:
308: 		return TemplatedFetchCommittedRange<hugeint_t>;
309: 	case PhysicalType::FLOAT:
310: 		return TemplatedFetchCommittedRange<float>;
311: 	case PhysicalType::DOUBLE:
312: 		return TemplatedFetchCommittedRange<double>;
313: 	case PhysicalType::INTERVAL:
314: 		return TemplatedFetchCommittedRange<interval_t>;
315: 	case PhysicalType::VARCHAR:
316: 		return TemplatedFetchCommittedRange<string_t>;
317: 	default:
318: 		throw NotImplementedException("Unimplemented type for update segment");
319: 	}
320: }
321: 
322: void UpdateSegment::FetchCommittedRange(idx_t start_row, idx_t count, Vector &result) {
323: 	D_ASSERT(count > 0);
324: 	if (!root) {
325: 		return;
326: 	}
327: 	idx_t end_row = start_row + count;
328: 	idx_t start_vector = start_row / STANDARD_VECTOR_SIZE;
329: 	idx_t end_vector = (end_row - 1) / STANDARD_VECTOR_SIZE;
330: 	D_ASSERT(start_vector <= end_vector);
331: 	D_ASSERT(end_vector < RowGroup::ROW_GROUP_VECTOR_COUNT);
332: 
333: 	for (idx_t vector_idx = start_vector; vector_idx <= end_vector; vector_idx++) {
334: 		if (!root->info[vector_idx]) {
335: 			continue;
336: 		}
337: 		idx_t start_in_vector = vector_idx == start_vector ? start_row - start_vector * STANDARD_VECTOR_SIZE : 0;
338: 		idx_t end_in_vector =
339: 		    vector_idx == end_vector ? end_row - end_vector * STANDARD_VECTOR_SIZE : STANDARD_VECTOR_SIZE;
340: 		D_ASSERT(start_in_vector < end_in_vector);
341: 		D_ASSERT(end_in_vector > 0 && end_in_vector <= STANDARD_VECTOR_SIZE);
342: 		idx_t result_offset = ((vector_idx * STANDARD_VECTOR_SIZE) + start_in_vector) - start_row;
343: 		fetch_committed_range(root->info[vector_idx]->info.get(), start_in_vector, end_in_vector, result_offset,
344: 		                      result);
345: 	}
346: }
347: 
348: //===--------------------------------------------------------------------===//
349: // Fetch Row
350: //===--------------------------------------------------------------------===//
351: static void FetchRowValidity(transaction_t start_time, transaction_t transaction_id, UpdateInfo *info, idx_t row_idx,
352:                              Vector &result, idx_t result_idx) {
353: 	auto &result_mask = FlatVector::Validity(result);
354: 	UpdateInfo::UpdatesForTransaction(info, start_time, transaction_id, [&](UpdateInfo *current) {
355: 		auto info_data = (bool *)current->tuple_data;
356: 		// FIXME: we could do a binary search in here
357: 		for (idx_t i = 0; i < current->N; i++) {
358: 			if (current->tuples[i] == row_idx) {
359: 				result_mask.Set(result_idx, info_data[i]);
360: 				break;
361: 			} else if (current->tuples[i] > row_idx) {
362: 				break;
363: 			}
364: 		}
365: 	});
366: }
367: 
368: template <class T>
369: static void TemplatedFetchRow(transaction_t start_time, transaction_t transaction_id, UpdateInfo *info, idx_t row_idx,
370:                               Vector &result, idx_t result_idx) {
371: 	auto result_data = FlatVector::GetData<T>(result);
372: 	UpdateInfo::UpdatesForTransaction(info, start_time, transaction_id, [&](UpdateInfo *current) {
373: 		auto info_data = (T *)current->tuple_data;
374: 		// FIXME: we could do a binary search in here
375: 		for (idx_t i = 0; i < current->N; i++) {
376: 			if (current->tuples[i] == row_idx) {
377: 				result_data[result_idx] = info_data[i];
378: 				break;
379: 			} else if (current->tuples[i] > row_idx) {
380: 				break;
381: 			}
382: 		}
383: 	});
384: }
385: 
386: static UpdateSegment::fetch_row_function_t GetFetchRowFunction(PhysicalType type) {
387: 	switch (type) {
388: 	case PhysicalType::BIT:
389: 		return FetchRowValidity;
390: 	case PhysicalType::BOOL:
391: 	case PhysicalType::INT8:
392: 		return TemplatedFetchRow<int8_t>;
393: 	case PhysicalType::INT16:
394: 		return TemplatedFetchRow<int16_t>;
395: 	case PhysicalType::INT32:
396: 		return TemplatedFetchRow<int32_t>;
397: 	case PhysicalType::INT64:
398: 		return TemplatedFetchRow<int64_t>;
399: 	case PhysicalType::UINT8:
400: 		return TemplatedFetchRow<uint8_t>;
401: 	case PhysicalType::UINT16:
402: 		return TemplatedFetchRow<uint16_t>;
403: 	case PhysicalType::UINT32:
404: 		return TemplatedFetchRow<uint32_t>;
405: 	case PhysicalType::UINT64:
406: 		return TemplatedFetchRow<uint64_t>;
407: 	case PhysicalType::INT128:
408: 		return TemplatedFetchRow<hugeint_t>;
409: 	case PhysicalType::FLOAT:
410: 		return TemplatedFetchRow<float>;
411: 	case PhysicalType::DOUBLE:
412: 		return TemplatedFetchRow<double>;
413: 	case PhysicalType::INTERVAL:
414: 		return TemplatedFetchRow<interval_t>;
415: 	case PhysicalType::VARCHAR:
416: 		return TemplatedFetchRow<string_t>;
417: 	default:
418: 		throw NotImplementedException("Unimplemented type for update segment fetch row");
419: 	}
420: }
421: 
422: void UpdateSegment::FetchRow(Transaction &transaction, idx_t row_id, Vector &result, idx_t result_idx) {
423: 	if (!root) {
424: 		return;
425: 	}
426: 	idx_t vector_index = (row_id - column_data.start) / STANDARD_VECTOR_SIZE;
427: 	if (!root->info[vector_index]) {
428: 		return;
429: 	}
430: 	idx_t row_in_vector = row_id - vector_index * STANDARD_VECTOR_SIZE;
431: 	fetch_row_function(transaction.start_time, transaction.transaction_id, root->info[vector_index]->info.get(),
432: 	                   row_in_vector, result, result_idx);
433: }
434: 
435: //===--------------------------------------------------------------------===//
436: // Rollback update
437: //===--------------------------------------------------------------------===//
438: template <class T>
439: static void RollbackUpdate(UpdateInfo *base_info, UpdateInfo *rollback_info) {
440: 	auto base_data = (T *)base_info->tuple_data;
441: 	auto rollback_data = (T *)rollback_info->tuple_data;
442: 	idx_t base_offset = 0;
443: 	for (idx_t i = 0; i < rollback_info->N; i++) {
444: 		auto id = rollback_info->tuples[i];
445: 		while (base_info->tuples[base_offset] < id) {
446: 			base_offset++;
447: 			D_ASSERT(base_offset < base_info->N);
448: 		}
449: 		base_data[base_offset] = rollback_data[i];
450: 	}
451: }
452: 
453: static UpdateSegment::rollback_update_function_t GetRollbackUpdateFunction(PhysicalType type) {
454: 	switch (type) {
455: 	case PhysicalType::BIT:
456: 		return RollbackUpdate<bool>;
457: 	case PhysicalType::BOOL:
458: 	case PhysicalType::INT8:
459: 		return RollbackUpdate<int8_t>;
460: 	case PhysicalType::INT16:
461: 		return RollbackUpdate<int16_t>;
462: 	case PhysicalType::INT32:
463: 		return RollbackUpdate<int32_t>;
464: 	case PhysicalType::INT64:
465: 		return RollbackUpdate<int64_t>;
466: 	case PhysicalType::UINT8:
467: 		return RollbackUpdate<uint8_t>;
468: 	case PhysicalType::UINT16:
469: 		return RollbackUpdate<uint16_t>;
470: 	case PhysicalType::UINT32:
471: 		return RollbackUpdate<uint32_t>;
472: 	case PhysicalType::UINT64:
473: 		return RollbackUpdate<uint64_t>;
474: 	case PhysicalType::INT128:
475: 		return RollbackUpdate<hugeint_t>;
476: 	case PhysicalType::FLOAT:
477: 		return RollbackUpdate<float>;
478: 	case PhysicalType::DOUBLE:
479: 		return RollbackUpdate<double>;
480: 	case PhysicalType::INTERVAL:
481: 		return RollbackUpdate<interval_t>;
482: 	case PhysicalType::VARCHAR:
483: 		return RollbackUpdate<string_t>;
484: 	default:
485: 		throw NotImplementedException("Unimplemented type for uncompressed segment");
486: 	}
487: }
488: 
489: void UpdateSegment::RollbackUpdate(UpdateInfo *info) {
490: 	// obtain an exclusive lock
491: 	auto lock_handle = lock.GetExclusiveLock();
492: 
493: 	// move the data from the UpdateInfo back into the base info
494: 	D_ASSERT(root->info[info->vector_index]);
495: 	rollback_update_function(root->info[info->vector_index]->info.get(), info);
496: 
497: 	// clean up the update chain
498: 	CleanupUpdateInternal(*lock_handle, info);
499: }
500: 
501: //===--------------------------------------------------------------------===//
502: // Cleanup Update
503: //===--------------------------------------------------------------------===//
504: void UpdateSegment::CleanupUpdateInternal(const StorageLockKey &lock, UpdateInfo *info) {
505: 	D_ASSERT(info->prev);
506: 	auto prev = info->prev;
507: 	prev->next = info->next;
508: 	if (prev->next) {
509: 		prev->next->prev = prev;
510: 	}
511: }
512: 
513: void UpdateSegment::CleanupUpdate(UpdateInfo *info) {
514: 	// obtain an exclusive lock
515: 	auto lock_handle = lock.GetExclusiveLock();
516: 	CleanupUpdateInternal(*lock_handle, info);
517: }
518: 
519: //===--------------------------------------------------------------------===//
520: // Check for conflicts in update
521: //===--------------------------------------------------------------------===//
522: static void CheckForConflicts(UpdateInfo *info, Transaction &transaction, row_t *ids, idx_t count, row_t offset,
523:                               UpdateInfo *&node) {
524: 	if (!info) {
525: 		return;
526: 	}
527: 	if (info->version_number == transaction.transaction_id) {
528: 		// this UpdateInfo belongs to the current transaction, set it in the node
529: 		node = info;
530: 	} else if (info->version_number > transaction.start_time) {
531: 		// potential conflict, check that tuple ids do not conflict
532: 		// as both ids and info->tuples are sorted, this is similar to a merge join
533: 		idx_t i = 0, j = 0;
534: 		while (true) {
535: 			auto id = ids[i] - offset;
536: 			if (id == info->tuples[j]) {
537: 				throw TransactionException("Conflict on update!");
538: 			} else if (id < info->tuples[j]) {
539: 				// id < the current tuple in info, move to next id
540: 				i++;
541: 				if (i == count) {
542: 					break;
543: 				}
544: 			} else {
545: 				// id > the current tuple, move to next tuple in info
546: 				j++;
547: 				if (j == info->N) {
548: 					break;
549: 				}
550: 			}
551: 		}
552: 	}
553: 	CheckForConflicts(info->next, transaction, ids, count, offset, node);
554: }
555: 
556: //===--------------------------------------------------------------------===//
557: // Initialize update info
558: //===--------------------------------------------------------------------===//
559: void UpdateSegment::InitializeUpdateInfo(UpdateInfo &info, row_t *ids, const SelectionVector &sel, idx_t count,
560:                                          idx_t vector_index, idx_t vector_offset) {
561: 	info.segment = this;
562: 	info.vector_index = vector_index;
563: 	info.prev = nullptr;
564: 	info.next = nullptr;
565: 
566: 	// set up the tuple ids
567: 	info.N = count;
568: 	for (idx_t i = 0; i < count; i++) {
569: 		auto idx = sel.get_index(i);
570: 		auto id = ids[idx];
571: 		D_ASSERT(idx_t(id) >= vector_offset && idx_t(id) < vector_offset + STANDARD_VECTOR_SIZE);
572: 		info.tuples[i] = id - vector_offset;
573: 	};
574: }
575: 
576: static void InitializeUpdateValidity(UpdateInfo *base_info, Vector &base_data, UpdateInfo *update_info, Vector &update,
577:                                      const SelectionVector &sel) {
578: 	auto &update_mask = FlatVector::Validity(update);
579: 	auto tuple_data = (bool *)update_info->tuple_data;
580: 
581: 	if (!update_mask.AllValid()) {
582: 		for (idx_t i = 0; i < update_info->N; i++) {
583: 			auto idx = sel.get_index(i);
584: 			tuple_data[i] = update_mask.RowIsValidUnsafe(idx);
585: 		}
586: 	} else {
587: 		for (idx_t i = 0; i < update_info->N; i++) {
588: 			tuple_data[i] = true;
589: 		}
590: 	}
591: 
592: 	auto &base_mask = FlatVector::Validity(base_data);
593: 	auto base_tuple_data = (bool *)base_info->tuple_data;
594: 	if (!base_mask.AllValid()) {
595: 		for (idx_t i = 0; i < base_info->N; i++) {
596: 			base_tuple_data[i] = base_mask.RowIsValidUnsafe(base_info->tuples[i]);
597: 		}
598: 	} else {
599: 		for (idx_t i = 0; i < base_info->N; i++) {
600: 			base_tuple_data[i] = true;
601: 		}
602: 	}
603: }
604: 
605: template <class T>
606: static void InitializeUpdateData(UpdateInfo *base_info, Vector &base_data, UpdateInfo *update_info, Vector &update,
607:                                  const SelectionVector &sel) {
608: 	auto update_data = FlatVector::GetData<T>(update);
609: 	auto tuple_data = (T *)update_info->tuple_data;
610: 
611: 	for (idx_t i = 0; i < update_info->N; i++) {
612: 		auto idx = sel.get_index(i);
613: 		tuple_data[i] = update_data[idx];
614: 	}
615: 
616: 	auto base_array_data = FlatVector::GetData<T>(base_data);
617: 	auto base_tuple_data = (T *)base_info->tuple_data;
618: 	for (idx_t i = 0; i < base_info->N; i++) {
619: 		base_tuple_data[i] = base_array_data[base_info->tuples[i]];
620: 	}
621: }
622: 
623: static UpdateSegment::initialize_update_function_t GetInitializeUpdateFunction(PhysicalType type) {
624: 	switch (type) {
625: 	case PhysicalType::BIT:
626: 		return InitializeUpdateValidity;
627: 	case PhysicalType::BOOL:
628: 	case PhysicalType::INT8:
629: 		return InitializeUpdateData<int8_t>;
630: 	case PhysicalType::INT16:
631: 		return InitializeUpdateData<int16_t>;
632: 	case PhysicalType::INT32:
633: 		return InitializeUpdateData<int32_t>;
634: 	case PhysicalType::INT64:
635: 		return InitializeUpdateData<int64_t>;
636: 	case PhysicalType::UINT8:
637: 		return InitializeUpdateData<uint8_t>;
638: 	case PhysicalType::UINT16:
639: 		return InitializeUpdateData<uint16_t>;
640: 	case PhysicalType::UINT32:
641: 		return InitializeUpdateData<uint32_t>;
642: 	case PhysicalType::UINT64:
643: 		return InitializeUpdateData<uint64_t>;
644: 	case PhysicalType::INT128:
645: 		return InitializeUpdateData<hugeint_t>;
646: 	case PhysicalType::FLOAT:
647: 		return InitializeUpdateData<float>;
648: 	case PhysicalType::DOUBLE:
649: 		return InitializeUpdateData<double>;
650: 	case PhysicalType::INTERVAL:
651: 		return InitializeUpdateData<interval_t>;
652: 	case PhysicalType::VARCHAR:
653: 		return InitializeUpdateData<string_t>;
654: 	default:
655: 		throw NotImplementedException("Unimplemented type for update segment");
656: 	}
657: }
658: 
659: //===--------------------------------------------------------------------===//
660: // Merge update info
661: //===--------------------------------------------------------------------===//
662: template <class F1, class F2, class F3>
663: static idx_t MergeLoop(row_t a[], sel_t b[], idx_t acount, idx_t bcount, idx_t aoffset, F1 merge, F2 pick_a, F3 pick_b,
664:                        const SelectionVector &asel) {
665: 	idx_t aidx = 0, bidx = 0;
666: 	idx_t count = 0;
667: 	while (aidx < acount && bidx < bcount) {
668: 		auto a_index = asel.get_index(aidx);
669: 		auto a_id = a[a_index] - aoffset;
670: 		auto b_id = b[bidx];
671: 		if (a_id == b_id) {
672: 			merge(a_id, a_index, bidx, count);
673: 			aidx++;
674: 			bidx++;
675: 			count++;
676: 		} else if (a_id < b_id) {
677: 			pick_a(a_id, a_index, count);
678: 			aidx++;
679: 			count++;
680: 		} else {
681: 			pick_b(b_id, bidx, count);
682: 			bidx++;
683: 			count++;
684: 		}
685: 	}
686: 	for (; aidx < acount; aidx++) {
687: 		auto a_index = asel.get_index(aidx);
688: 		pick_a(a[a_index] - aoffset, a_index, count);
689: 		count++;
690: 	}
691: 	for (; bidx < bcount; bidx++) {
692: 		pick_b(b[bidx], bidx, count);
693: 		count++;
694: 	}
695: 	return count;
696: }
697: 
698: struct ExtractStandardEntry {
699: 	template <class T, class V>
700: 	static T Extract(V *data, idx_t entry) {
701: 		return data[entry];
702: 	}
703: };
704: 
705: struct ExtractValidityEntry {
706: 	template <class T, class V>
707: 	static T Extract(V *data, idx_t entry) {
708: 		return data->RowIsValid(entry);
709: 	}
710: };
711: 
712: template <class T, class V, class OP = ExtractStandardEntry>
713: static void MergeUpdateLoopInternal(UpdateInfo *base_info, V *base_table_data, UpdateInfo *update_info,
714:                                     V *update_vector_data, row_t *ids, idx_t count, const SelectionVector &sel) {
715: 	auto base_id = base_info->segment->column_data.start + base_info->vector_index * STANDARD_VECTOR_SIZE;
716: #ifdef DEBUG
717: 	// all of these should be sorted, otherwise the below algorithm does not work
718: 	for (idx_t i = 1; i < count; i++) {
719: 		auto prev_idx = sel.get_index(i - 1);
720: 		auto idx = sel.get_index(i);
721: 		D_ASSERT(ids[idx] > ids[prev_idx] && ids[idx] >= row_t(base_id) &&
722: 		         ids[idx] < row_t(base_id + STANDARD_VECTOR_SIZE));
723: 	}
724: #endif
725: 
726: 	// we have a new batch of updates (update, ids, count)
727: 	// we already have existing updates (base_info)
728: 	// and potentially, this transaction already has updates present (update_info)
729: 	// we need to merge these all together so that the latest updates get merged into base_info
730: 	// and the "old" values (fetched from EITHER base_info OR from base_data) get placed into update_info
731: 	auto base_info_data = (T *)base_info->tuple_data;
732: 	auto update_info_data = (T *)update_info->tuple_data;
733: 
734: 	// we first do the merging of the old values
735: 	// what we are trying to do here is update the "update_info" of this transaction with all the old data we require
736: 	// this means we need to merge (1) any previously updated values (stored in update_info->tuples)
737: 	// together with (2)
738: 	// to simplify this, we create new arrays here
739: 	// we memcpy these over afterwards
740: 	T result_values[STANDARD_VECTOR_SIZE];
741: 	sel_t result_ids[STANDARD_VECTOR_SIZE];
742: 
743: 	idx_t base_info_offset = 0;
744: 	idx_t update_info_offset = 0;
745: 	idx_t result_offset = 0;
746: 	for (idx_t i = 0; i < count; i++) {
747: 		auto idx = sel.get_index(i);
748: 		// we have to merge the info for "ids[i]"
749: 		auto update_id = ids[idx] - base_id;
750: 
751: 		while (update_info_offset < update_info->N && update_info->tuples[update_info_offset] < update_id) {
752: 			// old id comes before the current id: write it
753: 			result_values[result_offset] = update_info_data[update_info_offset];
754: 			result_ids[result_offset++] = update_info->tuples[update_info_offset];
755: 			update_info_offset++;
756: 		}
757: 		// write the new id
758: 		if (update_info_offset < update_info->N && update_info->tuples[update_info_offset] == update_id) {
759: 			// we have an id that is equivalent in the current update info: write the update info
760: 			result_values[result_offset] = update_info_data[update_info_offset];
761: 			result_ids[result_offset++] = update_info->tuples[update_info_offset];
762: 			update_info_offset++;
763: 			continue;
764: 		}
765: 
766: 		/// now check if we have the current update_id in the base_info, or if we should fetch it from the base data
767: 		while (base_info_offset < base_info->N && base_info->tuples[base_info_offset] < update_id) {
768: 			base_info_offset++;
769: 		}
770: 		if (base_info_offset < base_info->N && base_info->tuples[base_info_offset] == update_id) {
771: 			// it is! we have to move the tuple from base_info->ids[base_info_offset] to update_info
772: 			result_values[result_offset] = base_info_data[base_info_offset];
773: 		} else {
774: 			// it is not! we have to move base_table_data[update_id] to update_info
775: 			result_values[result_offset] = OP::template Extract<T, V>(base_table_data, update_id);
776: 		}
777: 		result_ids[result_offset++] = update_id;
778: 	}
779: 	// write any remaining entries from the old updates
780: 	while (update_info_offset < update_info->N) {
781: 		result_values[result_offset] = update_info_data[update_info_offset];
782: 		result_ids[result_offset++] = update_info->tuples[update_info_offset];
783: 		update_info_offset++;
784: 	}
785: 	// now copy them back
786: 	update_info->N = result_offset;
787: 	memcpy(update_info_data, result_values, result_offset * sizeof(T));
788: 	memcpy(update_info->tuples, result_ids, result_offset * sizeof(sel_t));
789: 
790: 	// now we merge the new values into the base_info
791: 	result_offset = 0;
792: 	auto pick_new = [&](idx_t id, idx_t aidx, idx_t count) {
793: 		result_values[result_offset] = OP::template Extract<T, V>(update_vector_data, aidx);
794: 		result_ids[result_offset] = id;
795: 		result_offset++;
796: 	};
797: 	auto pick_old = [&](idx_t id, idx_t bidx, idx_t count) {
798: 		result_values[result_offset] = base_info_data[bidx];
799: 		result_ids[result_offset] = id;
800: 		result_offset++;
801: 	};
802: 	// now we perform a merge of the new ids with the old ids
803: 	auto merge = [&](idx_t id, idx_t aidx, idx_t bidx, idx_t count) {
804: 		pick_new(id, aidx, count);
805: 	};
806: 	MergeLoop(ids, base_info->tuples, count, base_info->N, base_id, merge, pick_new, pick_old, sel);
807: 
808: 	base_info->N = result_offset;
809: 	memcpy(base_info_data, result_values, result_offset * sizeof(T));
810: 	memcpy(base_info->tuples, result_ids, result_offset * sizeof(sel_t));
811: }
812: 
813: static void MergeValidityLoop(UpdateInfo *base_info, Vector &base_data, UpdateInfo *update_info, Vector &update,
814:                               row_t *ids, idx_t count, const SelectionVector &sel) {
815: 	auto &base_validity = FlatVector::Validity(base_data);
816: 	auto &update_validity = FlatVector::Validity(update);
817: 	MergeUpdateLoopInternal<bool, ValidityMask, ExtractValidityEntry>(base_info, &base_validity, update_info,
818: 	                                                                  &update_validity, ids, count, sel);
819: }
820: 
821: template <class T>
822: static void MergeUpdateLoop(UpdateInfo *base_info, Vector &base_data, UpdateInfo *update_info, Vector &update,
823:                             row_t *ids, idx_t count, const SelectionVector &sel) {
824: 	auto base_table_data = FlatVector::GetData<T>(base_data);
825: 	auto update_vector_data = FlatVector::GetData<T>(update);
826: 	MergeUpdateLoopInternal<T, T>(base_info, base_table_data, update_info, update_vector_data, ids, count, sel);
827: }
828: 
829: static UpdateSegment::merge_update_function_t GetMergeUpdateFunction(PhysicalType type) {
830: 	switch (type) {
831: 	case PhysicalType::BIT:
832: 		return MergeValidityLoop;
833: 	case PhysicalType::BOOL:
834: 	case PhysicalType::INT8:
835: 		return MergeUpdateLoop<int8_t>;
836: 	case PhysicalType::INT16:
837: 		return MergeUpdateLoop<int16_t>;
838: 	case PhysicalType::INT32:
839: 		return MergeUpdateLoop<int32_t>;
840: 	case PhysicalType::INT64:
841: 		return MergeUpdateLoop<int64_t>;
842: 	case PhysicalType::UINT8:
843: 		return MergeUpdateLoop<uint8_t>;
844: 	case PhysicalType::UINT16:
845: 		return MergeUpdateLoop<uint16_t>;
846: 	case PhysicalType::UINT32:
847: 		return MergeUpdateLoop<uint32_t>;
848: 	case PhysicalType::UINT64:
849: 		return MergeUpdateLoop<uint64_t>;
850: 	case PhysicalType::INT128:
851: 		return MergeUpdateLoop<hugeint_t>;
852: 	case PhysicalType::FLOAT:
853: 		return MergeUpdateLoop<float>;
854: 	case PhysicalType::DOUBLE:
855: 		return MergeUpdateLoop<double>;
856: 	case PhysicalType::INTERVAL:
857: 		return MergeUpdateLoop<interval_t>;
858: 	case PhysicalType::VARCHAR:
859: 		return MergeUpdateLoop<string_t>;
860: 	default:
861: 		throw NotImplementedException("Unimplemented type for uncompressed segment");
862: 	}
863: }
864: 
865: //===--------------------------------------------------------------------===//
866: // Update statistics
867: //===--------------------------------------------------------------------===//
868: unique_ptr<BaseStatistics> UpdateSegment::GetStatistics() {
869: 	lock_guard<mutex> stats_guard(stats_lock);
870: 	return stats.statistics->Copy();
871: }
872: 
873: idx_t UpdateValidityStatistics(UpdateSegment *segment, SegmentStatistics &stats, Vector &update, idx_t count,
874:                                SelectionVector &sel) {
875: 	auto &mask = FlatVector::Validity(update);
876: 	auto &validity = (ValidityStatistics &)*stats.statistics;
877: 	if (!mask.AllValid() && !validity.has_null) {
878: 		for (idx_t i = 0; i < count; i++) {
879: 			if (!mask.RowIsValid(i)) {
880: 				validity.has_null = true;
881: 				break;
882: 			}
883: 		}
884: 	}
885: 	sel.Initialize(FlatVector::INCREMENTAL_SELECTION_VECTOR);
886: 	return count;
887: }
888: 
889: template <class T>
890: idx_t TemplatedUpdateNumericStatistics(UpdateSegment *segment, SegmentStatistics &stats, Vector &update, idx_t count,
891:                                        SelectionVector &sel) {
892: 	auto update_data = FlatVector::GetData<T>(update);
893: 	auto &mask = FlatVector::Validity(update);
894: 
895: 	if (mask.AllValid()) {
896: 		for (idx_t i = 0; i < count; i++) {
897: 			NumericStatistics::Update<T>(stats, update_data[i]);
898: 		}
899: 		sel.Initialize(FlatVector::INCREMENTAL_SELECTION_VECTOR);
900: 		return count;
901: 	} else {
902: 		idx_t not_null_count = 0;
903: 		sel.Initialize(STANDARD_VECTOR_SIZE);
904: 		for (idx_t i = 0; i < count; i++) {
905: 			if (mask.RowIsValid(i)) {
906: 				sel.set_index(not_null_count++, i);
907: 				NumericStatistics::Update<T>(stats, update_data[i]);
908: 			}
909: 		}
910: 		return not_null_count;
911: 	}
912: }
913: 
914: idx_t UpdateStringStatistics(UpdateSegment *segment, SegmentStatistics &stats, Vector &update, idx_t count,
915:                              SelectionVector &sel) {
916: 	auto update_data = FlatVector::GetData<string_t>(update);
917: 	auto &mask = FlatVector::Validity(update);
918: 	if (mask.AllValid()) {
919: 		for (idx_t i = 0; i < count; i++) {
920: 			((StringStatistics &)*stats.statistics).Update(update_data[i]);
921: 			if (!update_data[i].IsInlined()) {
922: 				update_data[i] = segment->GetStringHeap().AddString(update_data[i]);
923: 			}
924: 		}
925: 		sel.Initialize(FlatVector::INCREMENTAL_SELECTION_VECTOR);
926: 		return count;
927: 	} else {
928: 		idx_t not_null_count = 0;
929: 		sel.Initialize(STANDARD_VECTOR_SIZE);
930: 		for (idx_t i = 0; i < count; i++) {
931: 			if (mask.RowIsValid(i)) {
932: 				sel.set_index(not_null_count++, i);
933: 				((StringStatistics &)*stats.statistics).Update(update_data[i]);
934: 				if (!update_data[i].IsInlined()) {
935: 					update_data[i] = segment->GetStringHeap().AddString(update_data[i]);
936: 				}
937: 			}
938: 		}
939: 		return not_null_count;
940: 	}
941: }
942: 
943: UpdateSegment::statistics_update_function_t GetStatisticsUpdateFunction(PhysicalType type) {
944: 	switch (type) {
945: 	case PhysicalType::BIT:
946: 		return UpdateValidityStatistics;
947: 	case PhysicalType::BOOL:
948: 	case PhysicalType::INT8:
949: 		return TemplatedUpdateNumericStatistics<int8_t>;
950: 	case PhysicalType::INT16:
951: 		return TemplatedUpdateNumericStatistics<int16_t>;
952: 	case PhysicalType::INT32:
953: 		return TemplatedUpdateNumericStatistics<int32_t>;
954: 	case PhysicalType::INT64:
955: 		return TemplatedUpdateNumericStatistics<int64_t>;
956: 	case PhysicalType::UINT8:
957: 		return TemplatedUpdateNumericStatistics<uint8_t>;
958: 	case PhysicalType::UINT16:
959: 		return TemplatedUpdateNumericStatistics<uint16_t>;
960: 	case PhysicalType::UINT32:
961: 		return TemplatedUpdateNumericStatistics<uint32_t>;
962: 	case PhysicalType::UINT64:
963: 		return TemplatedUpdateNumericStatistics<uint64_t>;
964: 	case PhysicalType::INT128:
965: 		return TemplatedUpdateNumericStatistics<hugeint_t>;
966: 	case PhysicalType::FLOAT:
967: 		return TemplatedUpdateNumericStatistics<float>;
968: 	case PhysicalType::DOUBLE:
969: 		return TemplatedUpdateNumericStatistics<double>;
970: 	case PhysicalType::INTERVAL:
971: 		return TemplatedUpdateNumericStatistics<interval_t>;
972: 	case PhysicalType::VARCHAR:
973: 		return UpdateStringStatistics;
974: 	default:
975: 		throw NotImplementedException("Unimplemented type for uncompressed segment");
976: 	}
977: }
978: 
979: //===--------------------------------------------------------------------===//
980: // Update
981: //===--------------------------------------------------------------------===//
982: void UpdateSegment::Update(Transaction &transaction, idx_t column_index, Vector &update, row_t *ids, idx_t count,
983:                            Vector &base_data) {
984: 	// obtain an exclusive lock
985: 	auto write_lock = lock.GetExclusiveLock();
986: 
987: 	// update statistics
988: 	SelectionVector sel;
989: 	{
990: 		lock_guard<mutex> stats_guard(stats_lock);
991: 		count = statistics_update_function(this, stats, update, count, sel);
992: 	}
993: 	if (count == 0) {
994: 		return;
995: 	}
996: 
997: #ifdef DEBUG
998: 	// verify that the ids are sorted and there are no duplicates
999: 	for (idx_t i = 1; i < count; i++) {
1000: 		D_ASSERT(ids[i] > ids[i - 1]);
1001: 	}
1002: #endif
1003: 
1004: 	// create the versions for this segment, if there are none yet
1005: 	if (!root) {
1006: 		root = make_unique<UpdateNode>();
1007: 	}
1008: 
1009: 	// get the vector index based on the first id
1010: 	// we assert that all updates must be part of the same vector
1011: 	auto first_id = ids[0];
1012: 	idx_t vector_index = (first_id - column_data.start) / STANDARD_VECTOR_SIZE;
1013: 	idx_t vector_offset = column_data.start + vector_index * STANDARD_VECTOR_SIZE;
1014: 
1015: 	D_ASSERT(idx_t(first_id) >= column_data.start);
1016: 	D_ASSERT(vector_index < RowGroup::ROW_GROUP_VECTOR_COUNT);
1017: 
1018: 	// first check the version chain
1019: 	UpdateInfo *node = nullptr;
1020: 
1021: 	if (root->info[vector_index]) {
1022: 		// there is already a version here, check if there are any conflicts and search for the node that belongs to
1023: 		// this transaction in the version chain
1024: 		auto base_info = root->info[vector_index]->info.get();
1025: 		CheckForConflicts(base_info->next, transaction, ids, count, vector_offset, node);
1026: 
1027: 		// there are no conflicts
1028: 		// first, check if this thread has already done any updates
1029: 		auto node = base_info->next;
1030: 		while (node) {
1031: 			if (node->version_number == transaction.transaction_id) {
1032: 				// it has! use this node
1033: 				break;
1034: 			}
1035: 			node = node->next;
1036: 		}
1037: 		if (!node) {
1038: 			// no updates made yet by this transaction: initially the update info to empty
1039: 			node = transaction.CreateUpdateInfo(type_size, count);
1040: 			node->segment = this;
1041: 			node->vector_index = vector_index;
1042: 			node->N = 0;
1043: 			node->column_index = column_index;
1044: 
1045: 			// insert the new node into the chain
1046: 			node->next = base_info->next;
1047: 			if (node->next) {
1048: 				node->next->prev = node;
1049: 			}
1050: 			node->prev = base_info;
1051: 			base_info->next = node;
1052: 		}
1053: 		base_info->Verify();
1054: 		node->Verify();
1055: 
1056: 		// now we are going to perform the merge
1057: 		merge_update_function(base_info, base_data, node, update, ids, count, sel);
1058: 
1059: 		base_info->Verify();
1060: 		node->Verify();
1061: 	} else {
1062: 		// there is no version info yet: create the top level update info and fill it with the updates
1063: 		auto result = make_unique<UpdateNodeData>();
1064: 
1065: 		result->info = make_unique<UpdateInfo>();
1066: 		result->tuples = unique_ptr<sel_t[]>(new sel_t[STANDARD_VECTOR_SIZE]);
1067: 		result->tuple_data = unique_ptr<data_t[]>(new data_t[STANDARD_VECTOR_SIZE * type_size]);
1068: 		result->info->tuples = result->tuples.get();
1069: 		result->info->tuple_data = result->tuple_data.get();
1070: 		result->info->version_number = TRANSACTION_ID_START - 1;
1071: 		result->info->column_index = column_index;
1072: 		InitializeUpdateInfo(*result->info, ids, sel, count, vector_index, vector_offset);
1073: 
1074: 		// now create the transaction level update info in the undo log
1075: 		auto transaction_node = transaction.CreateUpdateInfo(type_size, count);
1076: 		InitializeUpdateInfo(*transaction_node, ids, sel, count, vector_index, vector_offset);
1077: 
1078: 		// we write the updates in the
1079: 		initialize_update_function(transaction_node, base_data, result->info.get(), update, sel);
1080: 
1081: 		result->info->next = transaction_node;
1082: 		result->info->prev = nullptr;
1083: 		transaction_node->next = nullptr;
1084: 		transaction_node->prev = result->info.get();
1085: 		transaction_node->column_index = column_index;
1086: 
1087: 		transaction_node->Verify();
1088: 		result->info->Verify();
1089: 
1090: 		root->info[vector_index] = move(result);
1091: 	}
1092: }
1093: 
1094: bool UpdateSegment::HasUpdates() const {
1095: 	return root.get() != nullptr;
1096: }
1097: 
1098: bool UpdateSegment::HasUpdates(idx_t vector_index) const {
1099: 	if (!HasUpdates()) {
1100: 		return false;
1101: 	}
1102: 	return root->info[vector_index].get();
1103: }
1104: 
1105: bool UpdateSegment::HasUncommittedUpdates(idx_t vector_index) {
1106: 	if (!HasUpdates(vector_index)) {
1107: 		return false;
1108: 	}
1109: 	auto read_lock = lock.GetSharedLock();
1110: 	auto entry = root->info[vector_index].get();
1111: 	if (entry->info->next) {
1112: 		return true;
1113: 	}
1114: 	return false;
1115: }
1116: 
1117: bool UpdateSegment::HasUpdates(idx_t start_row_index, idx_t end_row_index) {
1118: 	if (!HasUpdates()) {
1119: 		return false;
1120: 	}
1121: 	auto read_lock = lock.GetSharedLock();
1122: 	idx_t base_vector_index = start_row_index / STANDARD_VECTOR_SIZE;
1123: 	idx_t end_vector_index = end_row_index / STANDARD_VECTOR_SIZE;
1124: 	for (idx_t i = base_vector_index; i <= end_vector_index; i++) {
1125: 		if (root->info[i]) {
1126: 			return true;
1127: 		}
1128: 	}
1129: 	return false;
1130: }
1131: 
1132: } // namespace duckdb
[end of src/storage/table/update_segment.cpp]
[start of src/storage/table/validity_segment.cpp]
1: #include "duckdb/storage/table/validity_segment.hpp"
2: #include "duckdb/storage/buffer_manager.hpp"
3: #include "duckdb/common/types/vector.hpp"
4: #include "duckdb/storage/statistics/validity_statistics.hpp"
5: 
6: namespace duckdb {
7: 
8: ValiditySegment::ValiditySegment(DatabaseInstance &db, idx_t row_start, block_id_t block_id)
9:     : UncompressedSegment(db, PhysicalType::BIT, row_start) {
10: 	// figure out how many vectors we want to store in this block
11: 
12: 	auto vector_size = ValidityMask::STANDARD_MASK_SIZE;
13: 	this->max_tuples = Storage::BLOCK_SIZE / vector_size * STANDARD_VECTOR_SIZE;
14: 	auto &buffer_manager = BufferManager::GetBufferManager(db);
15: 	if (block_id == INVALID_BLOCK) {
16: 		// no block id specified: allocate a buffer for the uncompressed segment
17: 		this->block = buffer_manager.RegisterMemory(Storage::BLOCK_ALLOC_SIZE, false);
18: 		// pin the block and initialize
19: 		auto handle = buffer_manager.Pin(block);
20: 		memset(handle->node->buffer, 0xFF, Storage::BLOCK_SIZE);
21: 	} else {
22: 		this->block = buffer_manager.RegisterBlock(block_id);
23: 	}
24: }
25: 
26: ValiditySegment::~ValiditySegment() {
27: }
28: 
29: void ValiditySegment::InitializeScan(ColumnScanState &state) {
30: 	auto &buffer_manager = BufferManager::GetBufferManager(db);
31: 	state.primary_handle = buffer_manager.Pin(block);
32: }
33: 
34: void ValiditySegment::FetchRow(ColumnFetchState &state, row_t row_id, Vector &result, idx_t result_idx) {
35: 	D_ASSERT(row_id >= 0 && row_id < row_t(this->tuple_count));
36: 	auto &buffer_manager = BufferManager::GetBufferManager(db);
37: 	auto handle = buffer_manager.Pin(block);
38: 	ValidityMask mask((validity_t *)handle->node->buffer);
39: 	if (!mask.RowIsValidUnsafe(row_id)) {
40: 		FlatVector::SetNull(result, result_idx, true);
41: 	}
42: }
43: 
44: idx_t ValiditySegment::Append(SegmentStatistics &stats, VectorData &data, idx_t offset, idx_t vcount) {
45: 	idx_t append_count = MinValue<idx_t>(vcount, max_tuples - tuple_count);
46: 	if (data.validity.AllValid()) {
47: 		// no null values: skip append
48: 		tuple_count += append_count;
49: 		return append_count;
50: 	}
51: 	auto &buffer_manager = BufferManager::GetBufferManager(db);
52: 	auto handle = buffer_manager.Pin(block);
53: 
54: 	auto &validity_stats = (ValidityStatistics &)*stats.statistics;
55: 	ValidityMask mask((validity_t *)handle->node->buffer);
56: 	for (idx_t i = 0; i < append_count; i++) {
57: 		auto idx = data.sel->get_index(offset + i);
58: 		if (!data.validity.RowIsValidUnsafe(idx)) {
59: 			mask.SetInvalidUnsafe(tuple_count + i);
60: 			validity_stats.has_null = true;
61: 		}
62: 	}
63: 	tuple_count += append_count;
64: 	return append_count;
65: }
66: 
67: // LOWER_MASKS contains masks with all the lower bits set until a specific value
68: // LOWER_MASKS[0] has the 0 lowest bits set, i.e.:
69: // 0b0000000000000000000000000000000000000000000000000000000000000000,
70: // LOWER_MASKS[10] has the 10 lowest bits set, i.e.:
71: // 0b0000000000000000000000000000000000000000000000000000000111111111,
72: // etc...
73: // 0b0000000000000000000000000000000000000001111111111111111111111111,
74: // ...
75: // 0b0000000000000000000001111111111111111111111111111111111111111111,
76: // until LOWER_MASKS[64], which has all bits set:
77: // 0b1111111111111111111111111111111111111111111111111111111111111111
78: // generated with this python snippet:
79: // for i in range(65):
80: //   print(hex(int((64 - i) * '0' + i * '1', 2)) + ",")
81: const validity_t ValiditySegment::LOWER_MASKS[] = {0x0,
82:                                                    0x1,
83:                                                    0x3,
84:                                                    0x7,
85:                                                    0xf,
86:                                                    0x1f,
87:                                                    0x3f,
88:                                                    0x7f,
89:                                                    0xff,
90:                                                    0x1ff,
91:                                                    0x3ff,
92:                                                    0x7ff,
93:                                                    0xfff,
94:                                                    0x1fff,
95:                                                    0x3fff,
96:                                                    0x7fff,
97:                                                    0xffff,
98:                                                    0x1ffff,
99:                                                    0x3ffff,
100:                                                    0x7ffff,
101:                                                    0xfffff,
102:                                                    0x1fffff,
103:                                                    0x3fffff,
104:                                                    0x7fffff,
105:                                                    0xffffff,
106:                                                    0x1ffffff,
107:                                                    0x3ffffff,
108:                                                    0x7ffffff,
109:                                                    0xfffffff,
110:                                                    0x1fffffff,
111:                                                    0x3fffffff,
112:                                                    0x7fffffff,
113:                                                    0xffffffff,
114:                                                    0x1ffffffff,
115:                                                    0x3ffffffff,
116:                                                    0x7ffffffff,
117:                                                    0xfffffffff,
118:                                                    0x1fffffffff,
119:                                                    0x3fffffffff,
120:                                                    0x7fffffffff,
121:                                                    0xffffffffff,
122:                                                    0x1ffffffffff,
123:                                                    0x3ffffffffff,
124:                                                    0x7ffffffffff,
125:                                                    0xfffffffffff,
126:                                                    0x1fffffffffff,
127:                                                    0x3fffffffffff,
128:                                                    0x7fffffffffff,
129:                                                    0xffffffffffff,
130:                                                    0x1ffffffffffff,
131:                                                    0x3ffffffffffff,
132:                                                    0x7ffffffffffff,
133:                                                    0xfffffffffffff,
134:                                                    0x1fffffffffffff,
135:                                                    0x3fffffffffffff,
136:                                                    0x7fffffffffffff,
137:                                                    0xffffffffffffff,
138:                                                    0x1ffffffffffffff,
139:                                                    0x3ffffffffffffff,
140:                                                    0x7ffffffffffffff,
141:                                                    0xfffffffffffffff,
142:                                                    0x1fffffffffffffff,
143:                                                    0x3fffffffffffffff,
144:                                                    0x7fffffffffffffff,
145:                                                    0xffffffffffffffff};
146: 
147: // UPPER_MASKS contains masks with all the highest bits set until a specific value
148: // UPPER_MASKS[0] has the 0 highest bits set, i.e.:
149: // 0b0000000000000000000000000000000000000000000000000000000000000000,
150: // UPPER_MASKS[10] has the 10 highest bits set, i.e.:
151: // 0b1111111111110000000000000000000000000000000000000000000000000000,
152: // etc...
153: // 0b1111111111111111111111110000000000000000000000000000000000000000,
154: // ...
155: // 0b1111111111111111111111111111111111111110000000000000000000000000,
156: // until UPPER_MASKS[64], which has all bits set:
157: // 0b1111111111111111111111111111111111111111111111111111111111111111
158: // generated with this python snippet:
159: // for i in range(65):
160: //   print(hex(int(i * '1' + (64 - i) * '0', 2)) + ",")
161: const validity_t ValiditySegment::UPPER_MASKS[] = {0x0,
162:                                                    0x8000000000000000,
163:                                                    0xc000000000000000,
164:                                                    0xe000000000000000,
165:                                                    0xf000000000000000,
166:                                                    0xf800000000000000,
167:                                                    0xfc00000000000000,
168:                                                    0xfe00000000000000,
169:                                                    0xff00000000000000,
170:                                                    0xff80000000000000,
171:                                                    0xffc0000000000000,
172:                                                    0xffe0000000000000,
173:                                                    0xfff0000000000000,
174:                                                    0xfff8000000000000,
175:                                                    0xfffc000000000000,
176:                                                    0xfffe000000000000,
177:                                                    0xffff000000000000,
178:                                                    0xffff800000000000,
179:                                                    0xffffc00000000000,
180:                                                    0xffffe00000000000,
181:                                                    0xfffff00000000000,
182:                                                    0xfffff80000000000,
183:                                                    0xfffffc0000000000,
184:                                                    0xfffffe0000000000,
185:                                                    0xffffff0000000000,
186:                                                    0xffffff8000000000,
187:                                                    0xffffffc000000000,
188:                                                    0xffffffe000000000,
189:                                                    0xfffffff000000000,
190:                                                    0xfffffff800000000,
191:                                                    0xfffffffc00000000,
192:                                                    0xfffffffe00000000,
193:                                                    0xffffffff00000000,
194:                                                    0xffffffff80000000,
195:                                                    0xffffffffc0000000,
196:                                                    0xffffffffe0000000,
197:                                                    0xfffffffff0000000,
198:                                                    0xfffffffff8000000,
199:                                                    0xfffffffffc000000,
200:                                                    0xfffffffffe000000,
201:                                                    0xffffffffff000000,
202:                                                    0xffffffffff800000,
203:                                                    0xffffffffffc00000,
204:                                                    0xffffffffffe00000,
205:                                                    0xfffffffffff00000,
206:                                                    0xfffffffffff80000,
207:                                                    0xfffffffffffc0000,
208:                                                    0xfffffffffffe0000,
209:                                                    0xffffffffffff0000,
210:                                                    0xffffffffffff8000,
211:                                                    0xffffffffffffc000,
212:                                                    0xffffffffffffe000,
213:                                                    0xfffffffffffff000,
214:                                                    0xfffffffffffff800,
215:                                                    0xfffffffffffffc00,
216:                                                    0xfffffffffffffe00,
217:                                                    0xffffffffffffff00,
218:                                                    0xffffffffffffff80,
219:                                                    0xffffffffffffffc0,
220:                                                    0xffffffffffffffe0,
221:                                                    0xfffffffffffffff0,
222:                                                    0xfffffffffffffff8,
223:                                                    0xfffffffffffffffc,
224:                                                    0xfffffffffffffffe,
225:                                                    0xffffffffffffffff};
226: 
227: void ValiditySegment::Scan(ColumnScanState &state, idx_t start, idx_t scan_count, Vector &result, idx_t result_offset) {
228: 	static_assert(sizeof(validity_t) == sizeof(uint64_t), "validity_t should be 64-bit");
229: 
230: 	auto &result_mask = FlatVector::Validity(result);
231: 	auto input_data = (validity_t *)state.primary_handle->node->buffer;
232: 	auto result_data = (validity_t *)result_mask.GetData();
233: 
234: 	// the code below does this, but using bitwise ops:
235: 	// ValidityMask source_mask(input_data);
236: 	// for (idx_t i = 0; i < scan_count; i++) {
237: 	//     result_mask.Set(result_offset + i, source_mask.RowIsValid(start + i));
238: 	// }
239: 
240: 	// set up the initial positions
241: 	// we need to find the validity_entry to modify, together with the bit-index WITHIN the validity entry
242: 	idx_t result_entry = result_offset / ValidityMask::BITS_PER_VALUE;
243: 	idx_t result_idx = result_offset - result_entry * ValidityMask::BITS_PER_VALUE;
244: 
245: 	// same for the input: find the validity_entry we are pulling from, together with the bit-index WITHIN that entry
246: 	idx_t input_entry = start / ValidityMask::BITS_PER_VALUE;
247: 	idx_t input_idx = start - input_entry * ValidityMask::BITS_PER_VALUE;
248: 
249: 	// now start the bit games
250: 	idx_t pos = 0;
251: 	while (pos < scan_count) {
252: 		// these are the current validity entries we are dealing with
253: 		idx_t current_result_idx = result_entry;
254: 		idx_t offset;
255: 		validity_t input_mask = input_data[input_entry];
256: 
257: 		// construct the mask to AND together with the result
258: 		if (result_idx < input_idx) {
259: 			// we have to shift the input RIGHT if the result_idx is smaller than the input_idx
260: 			auto shift_amount = input_idx - result_idx;
261: 			D_ASSERT(shift_amount > 0 && shift_amount <= ValidityMask::BITS_PER_VALUE);
262: 
263: 			input_mask = input_mask >> shift_amount;
264: 
265: 			// now the upper "shift_amount" bits are set to 0
266: 			// we need them to be set to 1
267: 			// otherwise the subsequent bitwise & will modify values outside of the range of values we want to alter
268: 			input_mask |= UPPER_MASKS[shift_amount];
269: 
270: 			// after this, we move to the next input_entry
271: 			offset = ValidityMask::BITS_PER_VALUE - input_idx;
272: 			input_entry++;
273: 			input_idx = 0;
274: 			result_idx += offset;
275: 		} else if (result_idx > input_idx) {
276: 			// we have to shift the input LEFT if the result_idx is bigger than the input_idx
277: 			auto shift_amount = result_idx - input_idx;
278: 			D_ASSERT(shift_amount > 0 && shift_amount <= ValidityMask::BITS_PER_VALUE);
279: 
280: 			// to avoid overflows, we set the upper "shift_amount" values to 0 first
281: 			input_mask = (input_mask & ~UPPER_MASKS[shift_amount]) << shift_amount;
282: 
283: 			// now the lower "shift_amount" bits are set to 0
284: 			// we need them to be set to 1
285: 			// otherwise the subsequent bitwise & will modify values outside of the range of values we want to alter
286: 			input_mask |= LOWER_MASKS[shift_amount];
287: 
288: 			// after this, we move to the next result_entry
289: 			offset = ValidityMask::BITS_PER_VALUE - result_idx;
290: 			result_entry++;
291: 			result_idx = 0;
292: 			input_idx += offset;
293: 		} else {
294: 			// if the input_idx is equal to result_idx they are already aligned
295: 			// we just move to the next entry for both after this
296: 			offset = ValidityMask::BITS_PER_VALUE - result_idx;
297: 			input_entry++;
298: 			result_entry++;
299: 			result_idx = input_idx = 0;
300: 		}
301: 		// now we need to check if we should include the ENTIRE mask
302: 		// OR if we need to mask from the right side
303: 		pos += offset;
304: 		if (pos > scan_count) {
305: 			// we need to set any bits that are past the scan_count on the right-side to 1
306: 			// this is required so we don't influence any bits that are not part of the scan
307: 			input_mask |= UPPER_MASKS[pos - scan_count];
308: 		}
309: 		// now finally we can merge the input mask with the result mask
310: 		if (input_mask != ValidityMask::ValidityBuffer::MAX_ENTRY) {
311: 			if (!result_data) {
312: 				result_mask.Initialize(STANDARD_VECTOR_SIZE);
313: 				result_data = (validity_t *)result_mask.GetData();
314: 			}
315: 			result_data[current_result_idx] &= input_mask;
316: 		}
317: 	}
318: 
319: #ifdef DEBUG
320: 	// verify that we actually accomplished the bitwise ops equivalent that we wanted to do
321: 	ValidityMask input_mask(input_data);
322: 	for (idx_t i = 0; i < scan_count; i++) {
323: 		D_ASSERT(result_mask.RowIsValid(result_offset + i) == input_mask.RowIsValid(start + i));
324: 	}
325: #endif
326: }
327: 
328: void ValiditySegment::RevertAppend(idx_t start_row) {
329: 	idx_t start_bit = start_row - this->row_start;
330: 	UncompressedSegment::RevertAppend(start_row);
331: 
332: 	auto &buffer_manager = BufferManager::GetBufferManager(db);
333: 	auto handle = buffer_manager.Pin(block);
334: 	idx_t revert_start;
335: 	if (start_bit % 8 != 0) {
336: 		// handle sub-bit stuff (yay)
337: 		idx_t byte_pos = start_bit / 8;
338: 		idx_t bit_start = byte_pos * 8;
339: 		idx_t bit_end = (byte_pos + 1) * 8;
340: 		ValidityMask mask((validity_t *)handle->node->buffer + byte_pos);
341: 		for (idx_t i = start_bit; i < bit_end; i++) {
342: 			mask.SetValid(i - bit_start);
343: 		}
344: 		revert_start = bit_end / 8;
345: 	} else {
346: 		revert_start = start_bit / 8;
347: 	}
348: 	// for the rest, we just memset
349: 	memset(handle->node->buffer + revert_start, 0xFF, Storage::BLOCK_SIZE - revert_start);
350: }
351: 
352: } // namespace duckdb
[end of src/storage/table/validity_segment.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: