{
  "repo": "duckdb/duckdb",
  "pull_number": 9619,
  "instance_id": "duckdb__duckdb-9619",
  "issue_numbers": [
    "9417"
  ],
  "base_commit": "162bad643da1687fcb83fc56544cd744b9953811",
  "patch": "diff --git a/src/parallel/pipeline_executor.cpp b/src/parallel/pipeline_executor.cpp\nindex bd26f8eeb393..8cc47690f20a 100644\n--- a/src/parallel/pipeline_executor.cpp\n+++ b/src/parallel/pipeline_executor.cpp\n@@ -351,7 +351,7 @@ void PipelineExecutor::ExecutePull(DataChunk &result) {\n \t\tD_ASSERT(!pipeline.sink);\n \t\tD_ASSERT(!requires_batch_index);\n \t\tauto &source_chunk = pipeline.operators.empty() ? result : *intermediate_chunks[0];\n-\t\twhile (result.size() == 0 && !exhausted_source) {\n+\t\twhile (result.size() == 0 && (!exhausted_source || !in_process_operators.empty())) {\n \t\t\tif (in_process_operators.empty()) {\n \t\t\t\tsource_chunk.Reset();\n \n@@ -361,6 +361,7 @@ void PipelineExecutor::ExecutePull(DataChunk &result) {\n \n \t\t\t\t// Repeatedly try to fetch from the source until it doesn't block. Note that it may block multiple times\n \t\t\t\twhile (true) {\n+\t\t\t\t\tD_ASSERT(!exhausted_source);\n \t\t\t\t\tsource_result = FetchFromSource(source_chunk);\n \n \t\t\t\t\t// No interrupt happened, all good.\n",
  "test_patch": "diff --git a/data/parquet-testing/issue9417.parquet b/data/parquet-testing/issue9417.parquet\nnew file mode 100644\nindex 000000000000..8907f6d39c20\nBinary files /dev/null and b/data/parquet-testing/issue9417.parquet differ\ndiff --git a/test/api/CMakeLists.txt b/test/api/CMakeLists.txt\nindex ec2242e7d79a..2c917fbb9c3e 100644\n--- a/test/api/CMakeLists.txt\n+++ b/test/api/CMakeLists.txt\n@@ -31,8 +31,10 @@ endif()\n \n if(DUCKDB_EXTENSION_TPCH_SHOULD_LINK)\n   include_directories(../../extension/tpch/include)\n-  set(TEST_API_OBJECTS ${TEST_API_OBJECTS} test_tpch_with_relations.cpp\n-                       serialized_plans/test_plan_serialization_bwc.cpp)\n+  set(TEST_API_OBJECTS\n+      ${TEST_API_OBJECTS} test_tpch_with_relations.cpp\n+      test_tpch_with_streaming.cpp\n+      serialized_plans/test_plan_serialization_bwc.cpp)\n endif()\n \n add_library_unity(test_api OBJECT ${TEST_API_OBJECTS})\ndiff --git a/test/api/test_results.cpp b/test/api/test_results.cpp\nindex 12c22d8cfb0a..43ca6a12a8b7 100644\n--- a/test/api/test_results.cpp\n+++ b/test/api/test_results.cpp\n@@ -202,3 +202,30 @@ TEST_CASE(\"Test ARRAY_AGG with ORDER BY\", \"[api][array_agg]\") {\n \tREQUIRE(!result->HasError());\n \tREQUIRE(result->names[1] == \"array_agg(c ORDER BY b)\");\n }\n+\n+TEST_CASE(\"Issue #9417\", \"[api][.]\") {\n+\tDuckDB db(\"issue_replication.db\");\n+\tConnection con(db);\n+\tauto result = con.SendQuery(\"with max_period as (\"\n+\t                            \"            select max(reporting_date) as max_record\\n\"\n+\t                            \"            from \\\"data/parquet-testing/issue9417.parquet\\\"\\n\"\n+\t                            \"        )\\n\"\n+\t                            \"        select\\n\"\n+\t                            \"            *\\n\"\n+\t                            \"        from \\\"data/parquet-testing/issue9417.parquet\\\" e\\n\"\n+\t                            \"            inner join max_period\\n\"\n+\t                            \"            on e.reporting_date = max_period.max_record\\n\"\n+\t                            \"         where e.record_date between '2012-01-31' and '2023-06-30'\");\n+\tidx_t count = 0;\n+\twhile (true) {\n+\t\tauto chunk = result->Fetch();\n+\t\tif (chunk) {\n+\t\t\tREQUIRE(count + chunk->size() <= 46);\n+\t\t\tcount += chunk->size();\n+\t\t} else {\n+\t\t\tbreak;\n+\t\t}\n+\t}\n+\n+\tREQUIRE(count == 46);\n+}\ndiff --git a/test/api/test_tpch_with_streaming.cpp b/test/api/test_tpch_with_streaming.cpp\nnew file mode 100644\nindex 000000000000..5384abc65a90\n--- /dev/null\n+++ b/test/api/test_tpch_with_streaming.cpp\n@@ -0,0 +1,39 @@\n+#include \"catch.hpp\"\n+#include \"test_helpers.hpp\"\n+#include \"tpch_extension.hpp\"\n+\n+#include <chrono>\n+#include <iostream>\n+#include \"duckdb/common/string_util.hpp\"\n+\n+using namespace duckdb;\n+using namespace std;\n+\n+TEST_CASE(\"Test TPC-H SF0.01 using streaming api\", \"[tpch][.]\") {\n+\tduckdb::unique_ptr<QueryResult> result;\n+\tDuckDB db(nullptr);\n+\tConnection con(db);\n+\tdouble sf = 0.01;\n+\tif (!db.ExtensionIsLoaded(\"tpch\")) {\n+\t\treturn;\n+\t}\n+\n+\tREQUIRE_NO_FAIL(con.Query(\"CALL dbgen(sf=\" + to_string(sf) + \")\"));\n+\n+\tfor (idx_t tpch_num = 1; tpch_num <= 22; tpch_num++) {\n+\t\tresult = con.SendQuery(\"pragma tpch(\" + to_string(tpch_num) + \");\");\n+\n+\t\tduckdb::ColumnDataCollection collection(duckdb::Allocator::DefaultAllocator(), result->types);\n+\n+\t\twhile (true) {\n+\t\t\tauto chunk = result->Fetch();\n+\t\t\tif (chunk) {\n+\t\t\t\tcollection.Append(*chunk);\n+\t\t\t} else {\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t}\n+\n+\t\tCOMPARE_CSV_COLLECTION(collection, TpchExtension::GetAnswer(sf, tpch_num), true);\n+\t}\n+}\ndiff --git a/test/helpers/test_helpers.cpp b/test/helpers/test_helpers.cpp\nindex 310876abe355..eb37727f424d 100644\n--- a/test/helpers/test_helpers.cpp\n+++ b/test/helpers/test_helpers.cpp\n@@ -254,6 +254,14 @@ string compare_csv(duckdb::QueryResult &result, string csv, bool header) {\n \treturn \"\";\n }\n \n+string compare_csv_collection(duckdb::ColumnDataCollection &collection, string csv, bool header) {\n+\tstring error;\n+\tif (!compare_result(csv, collection, collection.Types(), header, error)) {\n+\t\treturn error;\n+\t}\n+\treturn \"\";\n+}\n+\n string show_diff(DataChunk &left, DataChunk &right) {\n \tif (left.ColumnCount() != right.ColumnCount()) {\n \t\treturn StringUtil::Format(\"Different column counts: %d vs %d\", (int)left.ColumnCount(),\ndiff --git a/test/include/compare_result.hpp b/test/include/compare_result.hpp\nindex 0fe48e758c0a..7382e3b9772e 100644\n--- a/test/include/compare_result.hpp\n+++ b/test/include/compare_result.hpp\n@@ -20,6 +20,7 @@ bool CHECK_COLUMN(duckdb::unique_ptr<duckdb::MaterializedQueryResult> &result, s\n                   vector<duckdb::Value> values);\n \n string compare_csv(duckdb::QueryResult &result, string csv, bool header = false);\n+string compare_csv_collection(duckdb::ColumnDataCollection &collection, string csv, bool header = false);\n \n bool parse_datachunk(string csv, DataChunk &result, vector<LogicalType> sql_types, bool has_header);\n \ndiff --git a/test/include/test_helpers.hpp b/test/include/test_helpers.hpp\nindex 3444f01d3216..26e12a08356c 100644\n--- a/test/include/test_helpers.hpp\n+++ b/test/include/test_helpers.hpp\n@@ -63,4 +63,11 @@ bool NO_FAIL(duckdb::unique_ptr<QueryResult> result);\n \t\t\tFAIL(res);                                                                                                 \\\n \t}\n \n+#define COMPARE_CSV_COLLECTION(collection, csv, header)                                                                \\\n+\t{                                                                                                                  \\\n+\t\tauto res = compare_csv_collection(collection, csv, header);                                                    \\\n+\t\tif (!res.empty())                                                                                              \\\n+\t\t\tFAIL(res);                                                                                                 \\\n+\t}\n+\n } // namespace duckdb\n",
  "problem_statement": "Data retrieval in Python drops all rows except one without any error (regression in 0.9.1 compared to 0.8.1)\n### What happens?\r\n\r\nFor the query and dataset which are provided below, I get back only 1 row in Python. However, if I run a `count(*)` or export the data to parquet, all 46 rows which fulfill the where clause show up.\r\n\r\nI can only replicate this for a very specific table so I attached a database file which I created with DuckDB 0.9.1. I was not able to reproduce it by creating the table from scratch using SQL statements. I had to zip it so that I was able to upload it.\r\nThe data comes from a private dataset. I stripped out all the information except for the two columns `reporting_date` and `record_date` which are required to reproduce it.\r\nIf I use the same code to create the table buut with DuckDB 0.8.1, it works as expected.\r\n\r\nLet me know if there is any other way how I can help debug this!\r\n\r\n### To Reproduce\r\n\r\nUnzip [issue_replication.zip](https://github.com/duckdb/duckdb/files/13055920/issue_replication.zip)\r\n\r\n```python\r\nimport duckdb\r\nduckdb.__version__  # 0.9.2-dev83. Same with 0.9.1\r\n\r\ncon = duckdb.connect(\"issue_replication.db\")\r\n\r\ncon.execute(\"\"\"\r\n        with max_period as (\r\n            select max(reporting_date) as max_record\r\n            from mkt.table1\r\n        )\r\n        select\r\n            count(*)\r\n        from mkt.table1 e\r\n            inner join max_period\r\n            on e.reporting_date = max_period.max_record\r\n         where e.record_date between '2012-01-31' and '2023-06-30'\r\n                  \"\"\").fetchall()\r\n# This returns [(46,)] as expected\r\n\r\n# Same query but with `select *` instead of `select count(*)`:\r\ncon.execute(\"\"\"\r\n        with max_period as (\r\n            select max(reporting_date) as max_record\r\n            from mkt.table1\r\n        )\r\n        select\r\n            *\r\n        from mkt.table1 e\r\n            inner join max_period\r\n            on e.reporting_date = max_period.max_record\r\n         where e.record_date between '2012-01-31' and '2023-06-30'\r\n                  \"\"\").fetchall()\r\n# This returns only one row\r\n```\r\nSome things I noticed which hopefully help with debugging:\r\n* If I comment out the `where` clause, I get all 59 rows which should be returned in that case\r\n* If I modify the upper bound of the `where` statement to e.g. `2021-06-30`, I get back a different row but still just one\r\n* If I add a `order by e.record_date` I get 38 rows out of the 46 expected ones <- **Maybe there is one row which causes an issue**? But then why do I get all when I remove the `where` clause?\r\n\r\n\r\n\r\n\r\n### OS:\r\n\r\nBullseye Debian provided by python:3.11-slim-bullseye Docker image\r\n\r\n### DuckDB Version:\r\n\r\n0.9.1\r\n\r\n### DuckDB Client:\r\n\r\nPython\r\n\r\n### Full Name:\r\n\r\nStefan Binder\r\n\r\n### Affiliation:\r\n\r\nNone\r\n\r\n### Have you tried this on the latest `main` branch?\r\n\r\nI have tested with a main build\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] Yes, I have\n",
  "hints_text": "I can replicate the example on a Mac with an M2 chip so probably not dependent on the OS. If I use the 0.9.1 CLI to execute the same query, I get back all 46 rows so maybe it happens somewhere in the code doing the conversion to Python objects?",
  "created_at": "2023-11-09T08:46:22Z"
}