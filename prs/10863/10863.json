{
  "repo": "duckdb/duckdb",
  "pull_number": 10863,
  "instance_id": "duckdb__duckdb-10863",
  "issue_numbers": [
    "10745",
    "10745"
  ],
  "base_commit": "b6d37ebe5c3766b08be9f8092132448673316120",
  "patch": "diff --git a/src/include/duckdb/storage/compression/alp/alp_analyze.hpp b/src/include/duckdb/storage/compression/alp/alp_analyze.hpp\nindex 0288a9bd42c4..e8d58ea07bdd 100644\n--- a/src/include/duckdb/storage/compression/alp/alp_analyze.hpp\n+++ b/src/include/duckdb/storage/compression/alp/alp_analyze.hpp\n@@ -164,6 +164,10 @@ idx_t AlpFinalAnalyze(AnalyzeState &state) {\n \t// Flush last unfinished segment\n \tanalyze_state.FlushSegment();\n \n+\tif (compressed_values == 0) {\n+\t\treturn DConstants::INVALID_INDEX;\n+\t}\n+\n \t// We estimate the size by taking into account the portion of the values we took\n \tconst auto factor_of_sampling = analyze_state.total_values_count / compressed_values;\n \tconst auto final_analyze_size = analyze_state.TotalUsedBytes() * factor_of_sampling;\ndiff --git a/src/include/duckdb/storage/compression/alprd/alprd_analyze.hpp b/src/include/duckdb/storage/compression/alprd/alprd_analyze.hpp\nindex 6a2522ba63b4..1d3360ae8a68 100644\n--- a/src/include/duckdb/storage/compression/alprd/alprd_analyze.hpp\n+++ b/src/include/duckdb/storage/compression/alprd/alprd_analyze.hpp\n@@ -108,6 +108,9 @@ bool AlpRDAnalyze(AnalyzeState &state, Vector &input, idx_t count) {\n template <class T>\n idx_t AlpRDFinalAnalyze(AnalyzeState &state) {\n \tauto &analyze_state = (AlpRDAnalyzeState<T> &)state;\n+\tif (analyze_state.total_values_count == 0) {\n+\t\treturn DConstants::INVALID_INDEX;\n+\t}\n \tdouble factor_of_sampling = 1 / ((double)analyze_state.rowgroup_sample.size() / analyze_state.total_values_count);\n \n \t// Finding which is the best dictionary for the sample\ndiff --git a/src/storage/table/column_data_checkpointer.cpp b/src/storage/table/column_data_checkpointer.cpp\nindex 8c943830812e..d03cefbde52a 100644\n--- a/src/storage/table/column_data_checkpointer.cpp\n+++ b/src/storage/table/column_data_checkpointer.cpp\n@@ -147,6 +147,9 @@ unique_ptr<AnalyzeState> ColumnDataCheckpointer::DetectBestCompressionMethod(idx\n \t\tif (!compression_functions[i]) {\n \t\t\tcontinue;\n \t\t}\n+\t\tif (!analyze_states[i]) {\n+\t\t\tcontinue;\n+\t\t}\n \t\t//! Check if the method type is the forced method (if forced is used)\n \t\tbool forced_method_found = compression_functions[i]->type == forced_method;\n \t\tauto score = compression_functions[i]->final_analyze(*analyze_states[i]);\n",
  "test_patch": "diff --git a/test/sql/storage/types/list/empty_float_arrays.test b/test/sql/storage/types/list/empty_float_arrays.test\nnew file mode 100644\nindex 000000000000..a4bf9dd4120e\n--- /dev/null\n+++ b/test/sql/storage/types/list/empty_float_arrays.test\n@@ -0,0 +1,33 @@\n+# name: test/sql/storage/types/list/empty_float_arrays.test\n+# description: Test storage of empty float arrays\n+# group: [list]\n+\n+load __TEST_DIR__/array_empty_storage.db\n+\n+foreach compression <compression> uncompressed\n+\n+statement ok\n+CREATE TABLE test_table (\n+    id INTEGER,\n+    emb FLOAT[],\n+    emb_arr FLOAT[3]\n+);\n+\n+statement ok\n+INSERT INTO test_table (id) VALUES (42);\n+\n+statement ok\n+CHECKPOINT;\n+\n+query III\n+FROM test_table\n+----\n+42\tNULL\tNULL\n+\n+statement ok\n+DROP TABLE test_table\n+\n+statement ok\n+SET force_compression='${compression}'\n+\n+endloop\n",
  "problem_statement": " Attempted to dereference unique_ptr that is NULL when inserting many rows\n### What happens?\n\nWhen inserting (in my case) over 80000 rows results in \"duckdb.duckdb.FatalException: FATAL Error: Failed to create checkpoint because of error: Attempted to dereference unique_ptr that is NULL!\". I first encountered this when I was updating 2000 rows like this\r\n\r\n```\r\nduckdb.sql(\"CREATE TABLE temp AS SELECT * FROM df\")\r\nduckdb.sql(\"UPDATE target SET col = temp.col FROM temp WHERE target.id = temp.id\")\r\nduckdb.sql(\"DROP TABLE temp\")\r\n```\r\n\r\nAnd I got the same error at `DROP TABLE`. I cannot share that code because it is propetiary, but I'll try to see if I can create something else to reproduce it. Below a code snippet that triggers the crash when inserting.\n\n### To Reproduce\n\n```\r\nimport duckdb\r\nimport random\r\nimport string\r\n\r\n# Step 1: Generate 2000 random strings of max length 10\r\ndef generate_random_strings(n=90000, max_length=10):\r\n    random_strings = []\r\n    for _ in range(n):\r\n        length = random.randint(1, max_length)  # Random length up to 10\r\n        random_str = ''.join(random.choices(string.ascii_letters + string.digits, k=length))\r\n        random_strings.append(random_str)\r\n    return random_strings\r\n\r\nrandom_strings = generate_random_strings()\r\n\r\n# Step 2: Connect to DuckDB. This will create an in-memory database by default.\r\n# To persist data, you can specify a file name e.g., duckdb.connect('mydata.db')\r\ncon = duckdb.connect(database='test.db', read_only=False)\r\n\r\n# Step 3: Create a table with an autoincrement ID\r\ncon.execute(\"\"\"\r\n    CREATE TABLE test_table (\r\n        id INTEGER,\r\n        random_string VARCHAR,\r\n        emb FLOAT[]\r\n    );\r\n\"\"\")\r\n\r\ncon.execute(\"\")\r\n\r\n# Insert the strings into the table\r\ninsert_query = \"INSERT INTO test_table (id, random_string) VALUES (?, ?)\"\r\n\r\n# DuckDB's executemany is used for batch insertion.\r\ncon.executemany(insert_query, [(i, s,) for i, s in enumerate(random_strings)])\r\n\r\n# Verify the insertion\r\nresult = con.execute(\"SELECT COUNT(*) FROM test_table\").fetchall()\r\nprint(f\"Inserted rows: {result[0][0]}\")\r\n\r\n# Optional: Display some inserted rows to verify\r\nsample_result = con.execute(\"SELECT * FROM test_table LIMIT 5;\").fetchall()\r\nprint(\"Sample inserted rows:\")\r\nfor row in sample_result:\r\n    print(row)\r\n\r\n# Close the connection (in case of a persistent database)\r\ncon.close()\r\n```\n\n### OS:\n\nmacOS M3 Pro 14.1\n\n### DuckDB Version:\n\nv0.10.0 20b1486d11\n\n### DuckDB Client:\n\npython 3.12.1\n\n### Full Name:\n\nMika Ristim\u00e4ki\n\n### Affiliation:\n\nPrivate\n\n### Have you tried this on the latest [nightly build](https://duckdb.org/docs/installation/?version=main)?\n\nI have tested with a nightly build\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] Yes, I have\n Attempted to dereference unique_ptr that is NULL when inserting many rows\n### What happens?\n\nWhen inserting (in my case) over 80000 rows results in \"duckdb.duckdb.FatalException: FATAL Error: Failed to create checkpoint because of error: Attempted to dereference unique_ptr that is NULL!\". I first encountered this when I was updating 2000 rows like this\r\n\r\n```\r\nduckdb.sql(\"CREATE TABLE temp AS SELECT * FROM df\")\r\nduckdb.sql(\"UPDATE target SET col = temp.col FROM temp WHERE target.id = temp.id\")\r\nduckdb.sql(\"DROP TABLE temp\")\r\n```\r\n\r\nAnd I got the same error at `DROP TABLE`. I cannot share that code because it is propetiary, but I'll try to see if I can create something else to reproduce it. Below a code snippet that triggers the crash when inserting.\n\n### To Reproduce\n\n```\r\nimport duckdb\r\nimport random\r\nimport string\r\n\r\n# Step 1: Generate 2000 random strings of max length 10\r\ndef generate_random_strings(n=90000, max_length=10):\r\n    random_strings = []\r\n    for _ in range(n):\r\n        length = random.randint(1, max_length)  # Random length up to 10\r\n        random_str = ''.join(random.choices(string.ascii_letters + string.digits, k=length))\r\n        random_strings.append(random_str)\r\n    return random_strings\r\n\r\nrandom_strings = generate_random_strings()\r\n\r\n# Step 2: Connect to DuckDB. This will create an in-memory database by default.\r\n# To persist data, you can specify a file name e.g., duckdb.connect('mydata.db')\r\ncon = duckdb.connect(database='test.db', read_only=False)\r\n\r\n# Step 3: Create a table with an autoincrement ID\r\ncon.execute(\"\"\"\r\n    CREATE TABLE test_table (\r\n        id INTEGER,\r\n        random_string VARCHAR,\r\n        emb FLOAT[]\r\n    );\r\n\"\"\")\r\n\r\ncon.execute(\"\")\r\n\r\n# Insert the strings into the table\r\ninsert_query = \"INSERT INTO test_table (id, random_string) VALUES (?, ?)\"\r\n\r\n# DuckDB's executemany is used for batch insertion.\r\ncon.executemany(insert_query, [(i, s,) for i, s in enumerate(random_strings)])\r\n\r\n# Verify the insertion\r\nresult = con.execute(\"SELECT COUNT(*) FROM test_table\").fetchall()\r\nprint(f\"Inserted rows: {result[0][0]}\")\r\n\r\n# Optional: Display some inserted rows to verify\r\nsample_result = con.execute(\"SELECT * FROM test_table LIMIT 5;\").fetchall()\r\nprint(\"Sample inserted rows:\")\r\nfor row in sample_result:\r\n    print(row)\r\n\r\n# Close the connection (in case of a persistent database)\r\ncon.close()\r\n```\n\n### OS:\n\nmacOS M3 Pro 14.1\n\n### DuckDB Version:\n\nv0.10.0 20b1486d11\n\n### DuckDB Client:\n\npython 3.12.1\n\n### Full Name:\n\nMika Ristim\u00e4ki\n\n### Affiliation:\n\nPrivate\n\n### Have you tried this on the latest [nightly build](https://duckdb.org/docs/installation/?version=main)?\n\nI have tested with a nightly build\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] Yes, I have\n",
  "hints_text": "I tried with Python 3.10.13 and DuckDB 0.9.2, and there it works fine. Seems to only affect the newest version\nI tried with Python 3.10.13 and DuckDB 0.9.2, and there it works fine. Seems to only affect the newest version",
  "created_at": "2024-02-27T11:53:00Z"
}