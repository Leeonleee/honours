{
  "repo": "duckdb/duckdb",
  "pull_number": 4315,
  "instance_id": "duckdb__duckdb-4315",
  "issue_numbers": [
    "4147",
    "3621"
  ],
  "base_commit": "77592d18a82413ac4901d990d127f0e3099ecae2",
  "patch": "diff --git a/benchmark/interpreted_benchmark.cpp b/benchmark/interpreted_benchmark.cpp\nindex a21e1fea51fe..c33862bedf44 100644\n--- a/benchmark/interpreted_benchmark.cpp\n+++ b/benchmark/interpreted_benchmark.cpp\n@@ -379,15 +379,15 @@ string InterpretedBenchmark::Verify(BenchmarkState *state_p) {\n \t\t                          state.result->ToString());\n \t}\n \t// compare row count\n-\tif (state.result->collection.Count() != result_values.size()) {\n+\tif (state.result->RowCount() != result_values.size()) {\n \t\treturn StringUtil::Format(\"Error in result: expected %lld rows but got %lld\\nObtained result: %s\",\n-\t\t                          (int64_t)result_values.size(), (int64_t)state.result->collection.Count(),\n+\t\t                          (int64_t)result_values.size(), (int64_t)state.result->RowCount(),\n \t\t                          state.result->ToString());\n \t}\n \t// compare values\n \tfor (int64_t r = 0; r < (int64_t)result_values.size(); r++) {\n \t\tfor (int64_t c = 0; c < result_column_count; c++) {\n-\t\t\tauto value = state.result->collection.GetValue(c, r);\n+\t\t\tauto value = state.result->GetValue(c, r);\n \t\t\tif (result_values[r][c] == \"NULL\" && value.IsNull()) {\n \t\t\t\tcontinue;\n \t\t\t}\ndiff --git a/extension/parquet/include/parquet_reader.hpp b/extension/parquet/include/parquet_reader.hpp\nindex 4bfed934b1f1..aa226fc165c3 100644\n--- a/extension/parquet/include/parquet_reader.hpp\n+++ b/extension/parquet/include/parquet_reader.hpp\n@@ -36,7 +36,6 @@ class FileMetaData;\n namespace duckdb {\n class Allocator;\n class ClientContext;\n-class ChunkCollection;\n class BaseStatistics;\n class TableFilterSet;\n \ndiff --git a/extension/parquet/include/parquet_writer.hpp b/extension/parquet/include/parquet_writer.hpp\nindex d2d9cbb2002c..398e3be078d8 100644\n--- a/extension/parquet/include/parquet_writer.hpp\n+++ b/extension/parquet/include/parquet_writer.hpp\n@@ -14,7 +14,7 @@\n #include \"duckdb/common/exception.hpp\"\n #include \"duckdb/common/mutex.hpp\"\n #include \"duckdb/common/serializer/buffered_file_writer.hpp\"\n-#include \"duckdb/common/types/chunk_collection.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n #endif\n \n #include \"parquet_types.h\"\n@@ -36,7 +36,7 @@ class ParquetWriter {\n \t              vector<string> names, duckdb_parquet::format::CompressionCodec::type codec);\n \n public:\n-\tvoid Flush(ChunkCollection &buffer);\n+\tvoid Flush(ColumnDataCollection &buffer);\n \tvoid Finalize();\n \n \tstatic duckdb_parquet::format::Type::type DuckDBTypeToParquetType(const LogicalType &duckdb_type);\ndiff --git a/extension/parquet/include/resizable_buffer.hpp b/extension/parquet/include/resizable_buffer.hpp\nindex 29f1dcee9276..c3d9cc85e6a5 100644\n--- a/extension/parquet/include/resizable_buffer.hpp\n+++ b/extension/parquet/include/resizable_buffer.hpp\n@@ -76,12 +76,12 @@ class ResizeableBuffer : public ByteBuffer {\n \t\tif (new_size > alloc_len) {\n \t\t\talloc_len = new_size;\n \t\t\tallocated_data = allocator.Allocate(alloc_len);\n-\t\t\tptr = (char *)allocated_data->get();\n+\t\t\tptr = (char *)allocated_data.get();\n \t\t}\n \t}\n \n private:\n-\tunique_ptr<AllocatedData> allocated_data;\n+\tAllocatedData allocated_data;\n \tidx_t alloc_len = 0;\n };\n \ndiff --git a/extension/parquet/include/thrift_tools.hpp b/extension/parquet/include/thrift_tools.hpp\nindex 10eaed4fcb6f..13e09d9d51b2 100644\n--- a/extension/parquet/include/thrift_tools.hpp\n+++ b/extension/parquet/include/thrift_tools.hpp\n@@ -19,7 +19,7 @@ struct ReadHead {\n \tuint64_t size;\n \n \t// Current info\n-\tunique_ptr<AllocatedData> data;\n+\tAllocatedData data;\n \tbool data_isset = false;\n \n \tidx_t GetEnd() const {\n@@ -114,7 +114,7 @@ struct ReadAheadBuffer {\n \t\t\t\tthrow std::runtime_error(\"Prefetch registered requested for bytes outside file\");\n \t\t\t}\n \n-\t\t\thandle.Read(read_head.data->get(), read_head.size, read_head.location);\n+\t\t\thandle.Read(read_head.data.get(), read_head.size, read_head.location);\n \t\t\tread_head.data_isset = true;\n \t\t}\n \t}\n@@ -136,16 +136,16 @@ class ThriftFileTransport : public duckdb_apache::thrift::transport::TVirtualTra\n \n \t\t\tif (!prefetch_buffer->data_isset) {\n \t\t\t\tprefetch_buffer->Allocate(allocator);\n-\t\t\t\thandle.Read(prefetch_buffer->data->get(), prefetch_buffer->size, prefetch_buffer->location);\n+\t\t\t\thandle.Read(prefetch_buffer->data.get(), prefetch_buffer->size, prefetch_buffer->location);\n \t\t\t\tprefetch_buffer->data_isset = true;\n \t\t\t}\n-\t\t\tmemcpy(buf, prefetch_buffer->data->get() + location - prefetch_buffer->location, len);\n+\t\t\tmemcpy(buf, prefetch_buffer->data.get() + location - prefetch_buffer->location, len);\n \t\t} else {\n \t\t\tif (prefetch_mode && len < PREFETCH_FALLBACK_BUFFERSIZE && len > 0) {\n \t\t\t\tPrefetch(location, MinValue<uint64_t>(PREFETCH_FALLBACK_BUFFERSIZE, handle.GetFileSize() - location));\n \t\t\t\tauto prefetch_buffer_fallback = ra_buffer.GetReadHead(location);\n \t\t\t\tD_ASSERT(location - prefetch_buffer_fallback->location + len <= prefetch_buffer_fallback->size);\n-\t\t\t\tmemcpy(buf, prefetch_buffer_fallback->data->get() + location - prefetch_buffer_fallback->location, len);\n+\t\t\t\tmemcpy(buf, prefetch_buffer_fallback->data.get() + location - prefetch_buffer_fallback->location, len);\n \t\t\t} else {\n \t\t\t\thandle.Read(buf, len, location);\n \t\t\t}\ndiff --git a/extension/parquet/parquet-extension.cpp b/extension/parquet/parquet-extension.cpp\nindex fb992ab42bf5..aca65c03a9b0 100644\n--- a/extension/parquet/parquet-extension.cpp\n+++ b/extension/parquet/parquet-extension.cpp\n@@ -98,11 +98,10 @@ struct ParquetWriteGlobalState : public GlobalFunctionData {\n };\n \n struct ParquetWriteLocalState : public LocalFunctionData {\n-\texplicit ParquetWriteLocalState(Allocator &allocator) {\n-\t\tbuffer = make_unique<ChunkCollection>(allocator);\n+\texplicit ParquetWriteLocalState(ClientContext &context, const vector<LogicalType> &types) : buffer(context, types) {\n \t}\n \n-\tunique_ptr<ChunkCollection> buffer;\n+\tColumnDataCollection buffer;\n };\n \n class ParquetScanFunction {\n@@ -531,12 +530,12 @@ void ParquetWriteSink(ExecutionContext &context, FunctionData &bind_data_p, Glob\n \tauto &local_state = (ParquetWriteLocalState &)lstate;\n \n \t// append data to the local (buffered) chunk collection\n-\tlocal_state.buffer->Append(input);\n-\tif (local_state.buffer->Count() > bind_data.row_group_size) {\n+\tlocal_state.buffer.Append(input);\n+\tif (local_state.buffer.Count() > bind_data.row_group_size) {\n \t\t// if the chunk collection exceeds a certain size we flush it to the parquet file\n-\t\tglobal_state.writer->Flush(*local_state.buffer);\n+\t\tglobal_state.writer->Flush(local_state.buffer);\n \t\t// and reset the buffer\n-\t\tlocal_state.buffer = make_unique<ChunkCollection>(Allocator::Get(context.client));\n+\t\tlocal_state.buffer.Reset();\n \t}\n }\n \n@@ -545,7 +544,7 @@ void ParquetWriteCombine(ExecutionContext &context, FunctionData &bind_data, Glo\n \tauto &global_state = (ParquetWriteGlobalState &)gstate;\n \tauto &local_state = (ParquetWriteLocalState &)lstate;\n \t// flush any data left in the local state to the file\n-\tglobal_state.writer->Flush(*local_state.buffer);\n+\tglobal_state.writer->Flush(local_state.buffer);\n }\n \n void ParquetWriteFinalize(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate) {\n@@ -554,8 +553,9 @@ void ParquetWriteFinalize(ClientContext &context, FunctionData &bind_data, Globa\n \tglobal_state.writer->Finalize();\n }\n \n-unique_ptr<LocalFunctionData> ParquetWriteInitializeLocal(ExecutionContext &context, FunctionData &bind_data) {\n-\treturn make_unique<ParquetWriteLocalState>(Allocator::Get(context.client));\n+unique_ptr<LocalFunctionData> ParquetWriteInitializeLocal(ExecutionContext &context, FunctionData &bind_data_p) {\n+\tauto &bind_data = (ParquetWriteBindData &)bind_data_p;\n+\treturn make_unique<ParquetWriteLocalState>(context.client, bind_data.sql_types);\n }\n \n unique_ptr<TableFunctionRef> ParquetScanReplacement(ClientContext &context, const string &table_name,\ndiff --git a/extension/parquet/parquet_metadata.cpp b/extension/parquet/parquet_metadata.cpp\nindex d13308ae16fa..aa9c55704d09 100644\n--- a/extension/parquet/parquet_metadata.cpp\n+++ b/extension/parquet/parquet_metadata.cpp\n@@ -6,6 +6,7 @@\n #ifndef DUCKDB_AMALGAMATION\n #include \"duckdb/common/types/blob.hpp\"\n #include \"duckdb/main/config.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n #endif\n \n namespace duckdb {\n@@ -22,12 +23,15 @@ struct ParquetMetaDataBindData : public TableFunctionData {\n };\n \n struct ParquetMetaDataOperatorData : public GlobalTableFunctionState {\n-\texplicit ParquetMetaDataOperatorData(Allocator &allocator) : collection(allocator) {\n+\texplicit ParquetMetaDataOperatorData(ClientContext &context, const vector<LogicalType> &types)\n+\t    : collection(context, types) {\n \t}\n \n \tidx_t file_index;\n-\tChunkCollection collection;\n+\tColumnDataCollection collection;\n+\tColumnDataScanState scan_state;\n \n+public:\n \tstatic void BindMetaData(vector<LogicalType> &return_types, vector<string> &names);\n \tstatic void BindSchema(vector<LogicalType> &return_types, vector<string> &names);\n \n@@ -253,6 +257,8 @@ void ParquetMetaDataOperatorData::LoadFileMetaData(ClientContext &context, const\n \t}\n \tcurrent_chunk.SetCardinality(count);\n \tcollection.Append(current_chunk);\n+\n+\tcollection.InitializeScan(scan_state);\n }\n \n void ParquetMetaDataOperatorData::BindSchema(vector<LogicalType> &return_types, vector<string> &names) {\n@@ -390,6 +396,8 @@ void ParquetMetaDataOperatorData::LoadSchemaData(ClientContext &context, const v\n \t}\n \tcurrent_chunk.SetCardinality(count);\n \tcollection.Append(current_chunk);\n+\n+\tcollection.InitializeScan(scan_state);\n }\n \n template <bool SCHEMA>\n@@ -422,7 +430,7 @@ unique_ptr<GlobalTableFunctionState> ParquetMetaDataInit(ClientContext &context,\n \tauto &bind_data = (ParquetMetaDataBindData &)*input.bind_data;\n \tD_ASSERT(!bind_data.files.empty());\n \n-\tauto result = make_unique<ParquetMetaDataOperatorData>(Allocator::Get(context));\n+\tauto result = make_unique<ParquetMetaDataOperatorData>(context, bind_data.return_types);\n \tif (SCHEMA) {\n \t\tresult->LoadSchemaData(context, bind_data.return_types, bind_data.files[0]);\n \t} else {\n@@ -436,9 +444,9 @@ template <bool SCHEMA>\n void ParquetMetaDataImplementation(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n \tauto &data = (ParquetMetaDataOperatorData &)*data_p.global_state;\n \tauto &bind_data = (ParquetMetaDataBindData &)*data_p.bind_data;\n+\n \twhile (true) {\n-\t\tauto chunk = data.collection.Fetch();\n-\t\tif (!chunk) {\n+\t\tif (!data.collection.Scan(data.scan_state, output)) {\n \t\t\tif (data.file_index + 1 < bind_data.files.size()) {\n \t\t\t\t// load the metadata for the next file\n \t\t\t\tdata.file_index++;\n@@ -453,7 +461,6 @@ void ParquetMetaDataImplementation(ClientContext &context, TableFunctionInput &d\n \t\t\t\treturn;\n \t\t\t}\n \t\t}\n-\t\toutput.Move(*chunk);\n \t\tif (output.size() != 0) {\n \t\t\treturn;\n \t\t}\ndiff --git a/extension/parquet/parquet_writer.cpp b/extension/parquet/parquet_writer.cpp\nindex 5f3a5b945264..3f7db7b8b96b 100644\n--- a/extension/parquet/parquet_writer.cpp\n+++ b/extension/parquet/parquet_writer.cpp\n@@ -245,7 +245,7 @@ ParquetWriter::ParquetWriter(FileSystem &fs, string file_name_p, FileOpener *fil\n \t}\n }\n \n-void ParquetWriter::Flush(ChunkCollection &buffer) {\n+void ParquetWriter::Flush(ColumnDataCollection &buffer) {\n \tif (buffer.Count() == 0) {\n \t\treturn;\n \t}\n@@ -258,23 +258,22 @@ void ParquetWriter::Flush(ChunkCollection &buffer) {\n \trow_group.__isset.file_offset = true;\n \n \t// iterate over each of the columns of the chunk collection and write them\n-\tauto &chunks = buffer.Chunks();\n \tD_ASSERT(buffer.ColumnCount() == column_writers.size());\n \tfor (idx_t col_idx = 0; col_idx < buffer.ColumnCount(); col_idx++) {\n \t\tconst unique_ptr<ColumnWriter> &col_writer = column_writers[col_idx];\n \t\tauto write_state = col_writer->InitializeWriteState(row_group);\n \t\tif (col_writer->HasAnalyze()) {\n-\t\t\tfor (idx_t chunk_idx = 0; chunk_idx < chunks.size(); chunk_idx++) {\n-\t\t\t\tcol_writer->Analyze(*write_state, nullptr, chunks[chunk_idx]->data[col_idx], chunks[chunk_idx]->size());\n+\t\t\tfor (auto &chunk : buffer.Chunks()) {\n+\t\t\t\tcol_writer->Analyze(*write_state, nullptr, chunk.data[col_idx], chunk.size());\n \t\t\t}\n \t\t\tcol_writer->FinalizeAnalyze(*write_state);\n \t\t}\n-\t\tfor (idx_t chunk_idx = 0; chunk_idx < chunks.size(); chunk_idx++) {\n-\t\t\tcol_writer->Prepare(*write_state, nullptr, chunks[chunk_idx]->data[col_idx], chunks[chunk_idx]->size());\n+\t\tfor (auto &chunk : buffer.Chunks()) {\n+\t\t\tcol_writer->Prepare(*write_state, nullptr, chunk.data[col_idx], chunk.size());\n \t\t}\n \t\tcol_writer->BeginWrite(*write_state);\n-\t\tfor (idx_t chunk_idx = 0; chunk_idx < chunks.size(); chunk_idx++) {\n-\t\t\tcol_writer->Write(*write_state, chunks[chunk_idx]->data[col_idx], chunks[chunk_idx]->size());\n+\t\tfor (auto &chunk : buffer.Chunks()) {\n+\t\t\tcol_writer->Write(*write_state, chunk.data[col_idx], chunk.size());\n \t\t}\n \t\tcol_writer->FinalizeWrite(*write_state);\n \t}\ndiff --git a/extension/sqlsmith/third_party/sqlsmith/duckdb.cc b/extension/sqlsmith/third_party/sqlsmith/duckdb.cc\nindex 87dbed7c29b0..cf71fbd7f7c8 100644\n--- a/extension/sqlsmith/third_party/sqlsmith/duckdb.cc\n+++ b/extension/sqlsmith/third_party/sqlsmith/duckdb.cc\n@@ -36,9 +36,9 @@ schema_duckdb::schema_duckdb(duckdb::DatabaseInstance &database, bool no_catalog\n \tif (!result->success) {\n \t\tthrow runtime_error(result->error);\n \t}\n-\tfor (size_t i = 0; i < result->collection.Count(); i++) {\n-\t\tauto type = StringValue::Get(result->collection.GetValue(0, i));\n-\t\tauto name = StringValue::Get(result->collection.GetValue(2, i));\n+\tfor (size_t i = 0; i < result->RowCount(); i++) {\n+\t\tauto type = StringValue::Get(result->GetValue(0, i));\n+\t\tauto name = StringValue::Get(result->GetValue(2, i));\n \t\tbool view = type == \"view\";\n \t\ttable tab(name, \"main\", !view, !view);\n \t\ttables.push_back(tab);\n@@ -57,9 +57,9 @@ schema_duckdb::schema_duckdb(duckdb::DatabaseInstance &database, bool no_catalog\n \t\tif (!result->success) {\n \t\t\tthrow runtime_error(result->error);\n \t\t}\n-\t\tfor (size_t i = 0; i < result->collection.Count(); i++) {\n-\t\t\tauto name = StringValue::Get(result->collection.GetValue(1, i));\n-\t\t\tauto type = StringValue::Get(result->collection.GetValue(2, i));\n+\t\tfor (size_t i = 0; i < result->RowCount(); i++) {\n+\t\t\tauto name = StringValue::Get(result->GetValue(1, i));\n+\t\t\tauto type = StringValue::Get(result->GetValue(2, i));\n \t\t\tcolumn c(name, sqltype::get(type));\n \t\t\tt->columns().push_back(c);\n \t\t}\ndiff --git a/scripts/amalgamation.py b/scripts/amalgamation.py\nindex 6060464cea38..58ee92002219 100644\n--- a/scripts/amalgamation.py\n+++ b/scripts/amalgamation.py\n@@ -21,7 +21,7 @@\n main_header_files = [os.path.join(include_dir, 'duckdb.hpp'),\n     os.path.join(include_dir, 'duckdb.h'),\n     os.path.join(include_dir, 'duckdb', 'common', 'types', 'date.hpp'),\n-    os.path.join(include_dir, 'duckdb', 'common', 'arrow.hpp'),\n+    os.path.join(include_dir, 'duckdb', 'common', 'arrow', 'arrow.hpp'),\n     os.path.join(include_dir, 'duckdb', 'common', 'types', 'blob.hpp'),\n     os.path.join(include_dir, 'duckdb', 'common', 'types', 'decimal.hpp'),\n     os.path.join(include_dir, 'duckdb', 'common', 'types', 'hugeint.hpp'),\n@@ -65,7 +65,7 @@ def add_include_dir(dirpath):\n         \"duckdb/common/types/vector_cache.hpp\",\n         \"duckdb/common/string_map_set.hpp\",\n         \"duckdb/planner/filter/null_filter.hpp\",\n-        \"duckdb/common/arrow_wrapper.hpp\",\n+        \"duckdb/common/arrow/arrow_wrapper.hpp\",\n         \"duckdb/common/hive_partitioning.hpp\",\n         \"duckdb/planner/operator/logical_get.hpp\",\n         \"duckdb/common/compressed_file_system.hpp\"]]\ndiff --git a/src/common/CMakeLists.txt b/src/common/CMakeLists.txt\nindex b2ae7ca01095..ddca5a45e65c 100644\n--- a/src/common/CMakeLists.txt\n+++ b/src/common/CMakeLists.txt\n@@ -1,3 +1,4 @@\n+add_subdirectory(arrow)\n add_subdirectory(crypto)\n add_subdirectory(enums)\n add_subdirectory(operator)\n@@ -17,7 +18,6 @@ add_library_unity(\n   duckdb_common\n   OBJECT\n   allocator.cpp\n-  arrow_wrapper.cpp\n   assert.cpp\n   compressed_file_system.cpp\n   constants.cpp\ndiff --git a/src/common/allocator.cpp b/src/common/allocator.cpp\nindex 9ab2bbff3691..6578f7ce105d 100644\n--- a/src/common/allocator.cpp\n+++ b/src/common/allocator.cpp\n@@ -11,18 +11,35 @@\n \n namespace duckdb {\n \n+AllocatedData::AllocatedData() : allocator(nullptr), pointer(nullptr), allocated_size(0) {\n+}\n+\n AllocatedData::AllocatedData(Allocator &allocator, data_ptr_t pointer, idx_t allocated_size)\n-    : allocator(allocator), pointer(pointer), allocated_size(allocated_size) {\n+    : allocator(&allocator), pointer(pointer), allocated_size(allocated_size) {\n }\n AllocatedData::~AllocatedData() {\n \tReset();\n }\n \n+AllocatedData::AllocatedData(AllocatedData &&other) noexcept\n+    : allocator(other.allocator), pointer(nullptr), allocated_size(0) {\n+\tstd::swap(pointer, other.pointer);\n+\tstd::swap(allocated_size, other.allocated_size);\n+}\n+\n+AllocatedData &AllocatedData::operator=(AllocatedData &&other) noexcept {\n+\tstd::swap(allocator, other.allocator);\n+\tstd::swap(pointer, other.pointer);\n+\tstd::swap(allocated_size, other.allocated_size);\n+\treturn *this;\n+}\n+\n void AllocatedData::Reset() {\n \tif (!pointer) {\n \t\treturn;\n \t}\n-\tallocator.FreeData(pointer, allocated_size);\n+\tD_ASSERT(allocator);\n+\tallocator->FreeData(pointer, allocated_size);\n \tpointer = nullptr;\n }\n \ndiff --git a/src/common/arrow/CMakeLists.txt b/src/common/arrow/CMakeLists.txt\nnew file mode 100644\nindex 000000000000..5765fe7b7288\n--- /dev/null\n+++ b/src/common/arrow/CMakeLists.txt\n@@ -0,0 +1,5 @@\n+add_library_unity(duckdb_common_arrow OBJECT arrow_appender.cpp\n+                  arrow_converter.cpp arrow_wrapper.cpp)\n+set(ALL_OBJECT_FILES\n+    ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:duckdb_common_arrow>\n+    PARENT_SCOPE)\ndiff --git a/src/common/arrow/arrow_appender.cpp b/src/common/arrow/arrow_appender.cpp\nnew file mode 100644\nindex 000000000000..dc5f8a14dd64\n--- /dev/null\n+++ b/src/common/arrow/arrow_appender.cpp\n@@ -0,0 +1,734 @@\n+#include \"duckdb/common/arrow/arrow_appender.hpp\"\n+#include \"duckdb/common/arrow/arrow_buffer.hpp\"\n+#include \"duckdb/common/vector.hpp\"\n+#include \"duckdb/common/array.hpp\"\n+#include \"duckdb/common/types/interval.hpp\"\n+#include \"duckdb/common/types/uuid.hpp\"\n+\n+namespace duckdb {\n+\n+//===--------------------------------------------------------------------===//\n+// Arrow append data\n+//===--------------------------------------------------------------------===//\n+typedef void (*initialize_t)(ArrowAppendData &result, const LogicalType &type, idx_t capacity);\n+typedef void (*append_vector_t)(ArrowAppendData &append_data, Vector &input, idx_t size);\n+typedef void (*finalize_t)(ArrowAppendData &append_data, const LogicalType &type, ArrowArray *result);\n+\n+struct ArrowAppendData {\n+\t// the buffers of the arrow vector\n+\tArrowBuffer validity;\n+\tArrowBuffer main_buffer;\n+\tArrowBuffer aux_buffer;\n+\n+\tidx_t row_count = 0;\n+\tidx_t null_count = 0;\n+\n+\t// function pointers for construction\n+\tinitialize_t initialize = nullptr;\n+\tappend_vector_t append_vector = nullptr;\n+\tfinalize_t finalize = nullptr;\n+\n+\t// child data (if any)\n+\tvector<unique_ptr<ArrowAppendData>> child_data;\n+\n+\t//! the arrow array C API data, only set after Finalize\n+\tunique_ptr<ArrowArray> array;\n+\tduckdb::array<const void *, 3> buffers = {{nullptr, nullptr, nullptr}};\n+\tvector<ArrowArray *> child_pointers;\n+};\n+\n+//===--------------------------------------------------------------------===//\n+// ArrowAppender\n+//===--------------------------------------------------------------------===//\n+static unique_ptr<ArrowAppendData> InitializeArrowChild(const LogicalType &type, idx_t capacity);\n+static ArrowArray *FinalizeArrowChild(const LogicalType &type, ArrowAppendData &append_data);\n+\n+ArrowAppender::ArrowAppender(vector<LogicalType> types_p, idx_t initial_capacity) : types(move(types_p)) {\n+\tfor (auto &type : types) {\n+\t\tauto entry = InitializeArrowChild(type, initial_capacity);\n+\t\troot_data.push_back(move(entry));\n+\t}\n+}\n+\n+ArrowAppender::~ArrowAppender() {\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// Append Helper Functions\n+//===--------------------------------------------------------------------===//\n+static void GetBitPosition(idx_t row_idx, idx_t &current_byte, uint8_t &current_bit) {\n+\tcurrent_byte = row_idx / 8;\n+\tcurrent_bit = row_idx % 8;\n+}\n+\n+static void UnsetBit(uint8_t *data, idx_t current_byte, uint8_t current_bit) {\n+\tdata[current_byte] &= ~(1 << current_bit);\n+}\n+\n+static void NextBit(idx_t &current_byte, uint8_t &current_bit) {\n+\tcurrent_bit++;\n+\tif (current_bit == 8) {\n+\t\tcurrent_byte++;\n+\t\tcurrent_bit = 0;\n+\t}\n+}\n+\n+static void ResizeValidity(ArrowBuffer &buffer, idx_t row_count) {\n+\tauto byte_count = (row_count + 7) / 8;\n+\tbuffer.resize(byte_count, 0xFF);\n+}\n+\n+static void SetNull(ArrowAppendData &append_data, uint8_t *validity_data, idx_t current_byte, uint8_t current_bit) {\n+\tUnsetBit(validity_data, current_byte, current_bit);\n+\tappend_data.null_count++;\n+}\n+\n+static void AppendValidity(ArrowAppendData &append_data, UnifiedVectorFormat &format, idx_t size) {\n+\t// resize the buffer, filling the validity buffer with all valid values\n+\tResizeValidity(append_data.validity, append_data.row_count + size);\n+\tif (format.validity.AllValid()) {\n+\t\t// if all values are valid we don't need to do anything else\n+\t\treturn;\n+\t}\n+\n+\t// otherwise we iterate through the validity mask\n+\tauto validity_data = (uint8_t *)append_data.validity.data();\n+\tuint8_t current_bit;\n+\tidx_t current_byte;\n+\tGetBitPosition(append_data.row_count, current_byte, current_bit);\n+\tfor (idx_t i = 0; i < size; i++) {\n+\t\tauto source_idx = format.sel->get_index(i);\n+\t\t// append the validity mask\n+\t\tif (!format.validity.RowIsValid(source_idx)) {\n+\t\t\tSetNull(append_data, validity_data, current_byte, current_bit);\n+\t\t}\n+\t\tNextBit(current_byte, current_bit);\n+\t}\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// Scalar Types\n+//===--------------------------------------------------------------------===//\n+struct ArrowScalarConverter {\n+\ttemplate <class TGT, class SRC>\n+\tstatic TGT Operation(SRC input) {\n+\t\treturn input;\n+\t}\n+\n+\tstatic bool SkipNulls() {\n+\t\treturn false;\n+\t}\n+\n+\ttemplate <class TGT>\n+\tstatic void SetNull(TGT &value) {\n+\t}\n+};\n+\n+struct ArrowIntervalConverter {\n+\ttemplate <class TGT, class SRC>\n+\tstatic TGT Operation(SRC input) {\n+\t\treturn Interval::GetMilli(input);\n+\t}\n+\n+\tstatic bool SkipNulls() {\n+\t\treturn true;\n+\t}\n+\n+\ttemplate <class TGT>\n+\tstatic void SetNull(TGT &value) {\n+\t\tvalue = 0;\n+\t}\n+};\n+\n+template <class TGT, class SRC = TGT, class OP = ArrowScalarConverter>\n+struct ArrowScalarBaseData {\n+\tstatic void Append(ArrowAppendData &append_data, Vector &input, idx_t size) {\n+\t\tUnifiedVectorFormat format;\n+\t\tinput.ToUnifiedFormat(size, format);\n+\n+\t\t// append the validity mask\n+\t\tAppendValidity(append_data, format, size);\n+\n+\t\t// append the main data\n+\t\tappend_data.main_buffer.resize(append_data.main_buffer.size() + sizeof(TGT) * size);\n+\t\tauto data = (SRC *)format.data;\n+\t\tauto result_data = (TGT *)append_data.main_buffer.data();\n+\n+\t\tfor (idx_t i = 0; i < size; i++) {\n+\t\t\tauto source_idx = format.sel->get_index(i);\n+\t\t\tauto result_idx = append_data.row_count + i;\n+\n+\t\t\tif (OP::SkipNulls() && !format.validity.RowIsValid(source_idx)) {\n+\t\t\t\tOP::template SetNull<TGT>(result_data[result_idx]);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tresult_data[result_idx] = OP::template Operation<TGT, SRC>(data[source_idx]);\n+\t\t}\n+\t\tappend_data.row_count += size;\n+\t}\n+};\n+\n+template <class TGT, class SRC = TGT, class OP = ArrowScalarConverter>\n+struct ArrowScalarData : public ArrowScalarBaseData<TGT, SRC, OP> {\n+\tstatic void Initialize(ArrowAppendData &result, const LogicalType &type, idx_t capacity) {\n+\t\tresult.main_buffer.reserve(capacity * sizeof(TGT));\n+\t}\n+\n+\tstatic void Finalize(ArrowAppendData &append_data, const LogicalType &type, ArrowArray *result) {\n+\t\tresult->n_buffers = 2;\n+\t\tresult->buffers[1] = append_data.main_buffer.data();\n+\t}\n+};\n+\n+//===--------------------------------------------------------------------===//\n+// Enums\n+//===--------------------------------------------------------------------===//\n+template <class TGT>\n+struct ArrowEnumData : public ArrowScalarBaseData<TGT> {\n+\tstatic void Initialize(ArrowAppendData &result, const LogicalType &type, idx_t capacity) {\n+\t\tresult.main_buffer.reserve(capacity * sizeof(TGT));\n+\t\t// construct the enum child data\n+\t\tauto enum_data = InitializeArrowChild(LogicalType::VARCHAR, EnumType::GetSize(type));\n+\t\tenum_data->append_vector(*enum_data, EnumType::GetValuesInsertOrder(type), EnumType::GetSize(type));\n+\t\tresult.child_data.push_back(move(enum_data));\n+\t}\n+\n+\tstatic void Finalize(ArrowAppendData &append_data, const LogicalType &type, ArrowArray *result) {\n+\t\tresult->n_buffers = 2;\n+\t\tresult->buffers[1] = append_data.main_buffer.data();\n+\t\t// finalize the enum child data, and assign it to the dictionary\n+\t\tresult->dictionary = FinalizeArrowChild(LogicalType::VARCHAR, *append_data.child_data[0]);\n+\t}\n+};\n+\n+//===--------------------------------------------------------------------===//\n+// Boolean\n+//===--------------------------------------------------------------------===//\n+struct ArrowBoolData {\n+\tstatic void Initialize(ArrowAppendData &result, const LogicalType &type, idx_t capacity) {\n+\t\tauto byte_count = (capacity + 7) / 8;\n+\t\tresult.main_buffer.reserve(byte_count);\n+\t}\n+\n+\tstatic void Append(ArrowAppendData &append_data, Vector &input, idx_t size) {\n+\t\tUnifiedVectorFormat format;\n+\t\tinput.ToUnifiedFormat(size, format);\n+\n+\t\t// we initialize both the validity and the bit set to 1's\n+\t\tResizeValidity(append_data.validity, append_data.row_count + size);\n+\t\tResizeValidity(append_data.main_buffer, append_data.row_count + size);\n+\t\tauto data = (bool *)format.data;\n+\n+\t\tauto result_data = (uint8_t *)append_data.main_buffer.data();\n+\t\tauto validity_data = (uint8_t *)append_data.validity.data();\n+\t\tuint8_t current_bit;\n+\t\tidx_t current_byte;\n+\t\tGetBitPosition(append_data.row_count, current_byte, current_bit);\n+\t\tfor (idx_t i = 0; i < size; i++) {\n+\t\t\tauto source_idx = format.sel->get_index(i);\n+\t\t\t// append the validity mask\n+\t\t\tif (!format.validity.RowIsValid(source_idx)) {\n+\t\t\t\tSetNull(append_data, validity_data, current_byte, current_bit);\n+\t\t\t} else if (!data[source_idx]) {\n+\t\t\t\tUnsetBit(result_data, current_byte, current_bit);\n+\t\t\t}\n+\t\t\tNextBit(current_byte, current_bit);\n+\t\t}\n+\t\tappend_data.row_count += size;\n+\t}\n+\n+\tstatic void Finalize(ArrowAppendData &append_data, const LogicalType &type, ArrowArray *result) {\n+\t\tresult->n_buffers = 2;\n+\t\tresult->buffers[1] = append_data.main_buffer.data();\n+\t}\n+};\n+\n+//===--------------------------------------------------------------------===//\n+// Varchar\n+//===--------------------------------------------------------------------===//\n+struct ArrowVarcharConverter {\n+\ttemplate <class SRC>\n+\tstatic idx_t GetLength(SRC input) {\n+\t\treturn input.GetSize();\n+\t}\n+\n+\ttemplate <class SRC>\n+\tstatic void WriteData(data_ptr_t target, SRC input) {\n+\t\tmemcpy(target, input.GetDataUnsafe(), input.GetSize());\n+\t}\n+};\n+\n+struct ArrowUUIDConverter {\n+\ttemplate <class SRC>\n+\tstatic idx_t GetLength(SRC input) {\n+\t\treturn UUID::STRING_SIZE;\n+\t}\n+\n+\ttemplate <class SRC>\n+\tstatic void WriteData(data_ptr_t target, SRC input) {\n+\t\tUUID::ToString(input, (char *)target);\n+\t}\n+};\n+\n+template <class SRC = string_t, class OP = ArrowVarcharConverter>\n+struct ArrowVarcharData {\n+\tstatic void Initialize(ArrowAppendData &result, const LogicalType &type, idx_t capacity) {\n+\t\tresult.main_buffer.reserve((capacity + 1) * sizeof(uint32_t));\n+\t\tresult.aux_buffer.reserve(capacity);\n+\t}\n+\n+\tstatic void Append(ArrowAppendData &append_data, Vector &input, idx_t size) {\n+\t\tUnifiedVectorFormat format;\n+\t\tinput.ToUnifiedFormat(size, format);\n+\n+\t\t// resize the validity mask and set up the validity buffer for iteration\n+\t\tResizeValidity(append_data.validity, append_data.row_count + size);\n+\t\tauto validity_data = (uint8_t *)append_data.validity.data();\n+\n+\t\t// resize the offset buffer - the offset buffer holds the offsets into the child array\n+\t\tappend_data.main_buffer.resize(append_data.main_buffer.size() + sizeof(uint32_t) * (size + 1));\n+\t\tauto data = (SRC *)format.data;\n+\t\tauto offset_data = (uint32_t *)append_data.main_buffer.data();\n+\t\tif (append_data.row_count == 0) {\n+\t\t\t// first entry\n+\t\t\toffset_data[0] = 0;\n+\t\t}\n+\t\t// now append the string data to the auxiliary buffer\n+\t\t// the auxiliary buffer's length depends on the string lengths, so we resize as required\n+\t\tauto last_offset = offset_data[append_data.row_count];\n+\t\tfor (idx_t i = 0; i < size; i++) {\n+\t\t\tauto source_idx = format.sel->get_index(i);\n+\t\t\tauto offset_idx = append_data.row_count + i + 1;\n+\n+\t\t\tif (!format.validity.RowIsValid(source_idx)) {\n+\t\t\t\tuint8_t current_bit;\n+\t\t\t\tidx_t current_byte;\n+\t\t\t\tGetBitPosition(append_data.row_count + i, current_byte, current_bit);\n+\t\t\t\tSetNull(append_data, validity_data, current_byte, current_bit);\n+\t\t\t\toffset_data[offset_idx] = last_offset;\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tauto string_length = OP::GetLength(data[source_idx]);\n+\n+\t\t\t// append the offset data\n+\t\t\tauto current_offset = last_offset + string_length;\n+\t\t\toffset_data[offset_idx] = current_offset;\n+\n+\t\t\t// resize the string buffer if required, and write the string data\n+\t\t\tappend_data.aux_buffer.resize(current_offset);\n+\t\t\tOP::WriteData(append_data.aux_buffer.data() + last_offset, data[source_idx]);\n+\n+\t\t\tlast_offset = current_offset;\n+\t\t}\n+\t\tappend_data.row_count += size;\n+\t}\n+\n+\tstatic void Finalize(ArrowAppendData &append_data, const LogicalType &type, ArrowArray *result) {\n+\t\tresult->n_buffers = 3;\n+\t\tresult->buffers[1] = append_data.main_buffer.data();\n+\t\tresult->buffers[2] = append_data.aux_buffer.data();\n+\t}\n+};\n+\n+//===--------------------------------------------------------------------===//\n+// Structs\n+//===--------------------------------------------------------------------===//\n+struct ArrowStructData {\n+\tstatic void Initialize(ArrowAppendData &result, const LogicalType &type, idx_t capacity) {\n+\t\tauto &children = StructType::GetChildTypes(type);\n+\t\tfor (auto &child : children) {\n+\t\t\tauto child_buffer = InitializeArrowChild(child.second, capacity);\n+\t\t\tresult.child_data.push_back(move(child_buffer));\n+\t\t}\n+\t}\n+\n+\tstatic void Append(ArrowAppendData &append_data, Vector &input, idx_t size) {\n+\t\tUnifiedVectorFormat format;\n+\t\tinput.ToUnifiedFormat(size, format);\n+\n+\t\tAppendValidity(append_data, format, size);\n+\t\t// append the children of the struct\n+\t\tauto &children = StructVector::GetEntries(input);\n+\t\tfor (idx_t child_idx = 0; child_idx < children.size(); child_idx++) {\n+\t\t\tauto &child = children[child_idx];\n+\t\t\tauto &child_data = *append_data.child_data[child_idx];\n+\t\t\tchild_data.append_vector(child_data, *child, size);\n+\t\t}\n+\t\tappend_data.row_count += size;\n+\t}\n+\n+\tstatic void Finalize(ArrowAppendData &append_data, const LogicalType &type, ArrowArray *result) {\n+\t\tresult->n_buffers = 1;\n+\n+\t\tauto &child_types = StructType::GetChildTypes(type);\n+\t\tappend_data.child_pointers.resize(child_types.size());\n+\t\tresult->children = append_data.child_pointers.data();\n+\t\tresult->n_children = child_types.size();\n+\t\tfor (idx_t i = 0; i < child_types.size(); i++) {\n+\t\t\tauto &child_type = child_types[i].second;\n+\t\t\tappend_data.child_pointers[i] = FinalizeArrowChild(child_type, *append_data.child_data[i]);\n+\t\t}\n+\t}\n+};\n+\n+//===--------------------------------------------------------------------===//\n+// Lists\n+//===--------------------------------------------------------------------===//\n+void AppendListOffsets(ArrowAppendData &append_data, UnifiedVectorFormat &format, idx_t size,\n+                       vector<sel_t> &child_sel) {\n+\t// resize the offset buffer - the offset buffer holds the offsets into the child array\n+\tappend_data.main_buffer.resize(append_data.main_buffer.size() + sizeof(uint32_t) * (size + 1));\n+\tauto data = (list_entry_t *)format.data;\n+\tauto offset_data = (uint32_t *)append_data.main_buffer.data();\n+\tif (append_data.row_count == 0) {\n+\t\t// first entry\n+\t\toffset_data[0] = 0;\n+\t}\n+\t// set up the offsets using the list entries\n+\tauto last_offset = offset_data[append_data.row_count];\n+\tfor (idx_t i = 0; i < size; i++) {\n+\t\tauto source_idx = format.sel->get_index(i);\n+\t\tauto offset_idx = append_data.row_count + i + 1;\n+\n+\t\tif (!format.validity.RowIsValid(source_idx)) {\n+\t\t\toffset_data[offset_idx] = last_offset;\n+\t\t\tcontinue;\n+\t\t}\n+\n+\t\t// append the offset data\n+\t\tauto list_length = data[source_idx].length;\n+\t\tlast_offset += list_length;\n+\t\toffset_data[offset_idx] = last_offset;\n+\n+\t\tfor (idx_t k = 0; k < list_length; k++) {\n+\t\t\tchild_sel.push_back(data[source_idx].offset + k);\n+\t\t}\n+\t}\n+}\n+\n+struct ArrowListData {\n+\tstatic void Initialize(ArrowAppendData &result, const LogicalType &type, idx_t capacity) {\n+\t\tauto &child_type = ListType::GetChildType(type);\n+\t\tresult.main_buffer.reserve((capacity + 1) * sizeof(uint32_t));\n+\t\tauto child_buffer = InitializeArrowChild(child_type, capacity);\n+\t\tresult.child_data.push_back(move(child_buffer));\n+\t}\n+\n+\tstatic void Append(ArrowAppendData &append_data, Vector &input, idx_t size) {\n+\t\tUnifiedVectorFormat format;\n+\t\tinput.ToUnifiedFormat(size, format);\n+\n+\t\tvector<sel_t> child_indices;\n+\t\tAppendValidity(append_data, format, size);\n+\t\tAppendListOffsets(append_data, format, size, child_indices);\n+\n+\t\t// append the child vector of the list\n+\t\tSelectionVector child_sel(child_indices.data());\n+\t\tauto &child = ListVector::GetEntry(input);\n+\t\tauto child_size = child_indices.size();\n+\t\tchild.Slice(child_sel, child_size);\n+\n+\t\tappend_data.child_data[0]->append_vector(*append_data.child_data[0], child, child_size);\n+\t\tappend_data.row_count += size;\n+\t}\n+\n+\tstatic void Finalize(ArrowAppendData &append_data, const LogicalType &type, ArrowArray *result) {\n+\t\tresult->n_buffers = 2;\n+\t\tresult->buffers[1] = append_data.main_buffer.data();\n+\n+\t\tauto &child_type = ListType::GetChildType(type);\n+\t\tappend_data.child_pointers.resize(1);\n+\t\tresult->children = append_data.child_pointers.data();\n+\t\tresult->n_children = 1;\n+\t\tappend_data.child_pointers[0] = FinalizeArrowChild(child_type, *append_data.child_data[0]);\n+\t}\n+};\n+\n+//===--------------------------------------------------------------------===//\n+// Maps\n+//===--------------------------------------------------------------------===//\n+struct ArrowMapData {\n+\tstatic void Initialize(ArrowAppendData &result, const LogicalType &type, idx_t capacity) {\n+\t\t// map types are stored in a (too) clever way\n+\t\t// the main buffer holds the null values and the offsets\n+\t\t// then we have a single child, which is a struct of the map_type, and the key_type\n+\t\tresult.main_buffer.reserve((capacity + 1) * sizeof(uint32_t));\n+\n+\t\tauto &key_type = MapType::KeyType(type);\n+\t\tauto &value_type = MapType::ValueType(type);\n+\t\tauto internal_struct = make_unique<ArrowAppendData>();\n+\t\tinternal_struct->child_data.push_back(InitializeArrowChild(key_type, capacity));\n+\t\tinternal_struct->child_data.push_back(InitializeArrowChild(value_type, capacity));\n+\n+\t\tresult.child_data.push_back(move(internal_struct));\n+\t}\n+\n+\tstatic void Append(ArrowAppendData &append_data, Vector &input, idx_t size) {\n+\t\tUnifiedVectorFormat format;\n+\t\tinput.ToUnifiedFormat(size, format);\n+\n+\t\tAppendValidity(append_data, format, size);\n+\t\t// maps exist as a struct of two lists, e.g. STRUCT(key VARCHAR[], value VARCHAR[])\n+\t\t// since both lists are the same, arrow tries to be smart by storing the offsets only once\n+\t\t// we can append the offsets from any of the two children\n+\t\tauto &children = StructVector::GetEntries(input);\n+\n+\t\tUnifiedVectorFormat child_format;\n+\t\tchildren[0]->ToUnifiedFormat(size, child_format);\n+\t\tvector<sel_t> child_indices;\n+\t\tAppendListOffsets(append_data, child_format, size, child_indices);\n+\n+\t\t// now we can append the children to the lists\n+\t\tauto &struct_entries = StructVector::GetEntries(input);\n+\t\tD_ASSERT(struct_entries.size() == 2);\n+\t\tSelectionVector child_sel(child_indices.data());\n+\t\tauto &key_vector = ListVector::GetEntry(*struct_entries[0]);\n+\t\tauto &value_vector = ListVector::GetEntry(*struct_entries[1]);\n+\t\tauto list_size = child_indices.size();\n+\t\tkey_vector.Slice(child_sel, list_size);\n+\t\tvalue_vector.Slice(child_sel, list_size);\n+\n+\t\t// perform the append\n+\t\tauto &struct_data = *append_data.child_data[0];\n+\t\tauto &key_data = *struct_data.child_data[0];\n+\t\tauto &value_data = *struct_data.child_data[1];\n+\t\tkey_data.append_vector(key_data, key_vector, list_size);\n+\t\tvalue_data.append_vector(value_data, value_vector, list_size);\n+\n+\t\tappend_data.row_count += size;\n+\t\tstruct_data.row_count += size;\n+\t}\n+\n+\tstatic void Finalize(ArrowAppendData &append_data, const LogicalType &type, ArrowArray *result) {\n+\t\t// set up the main map buffer\n+\t\tresult->n_buffers = 2;\n+\t\tresult->buffers[1] = append_data.main_buffer.data();\n+\n+\t\t// the main map buffer has a single child: a struct\n+\t\tappend_data.child_pointers.resize(1);\n+\t\tresult->children = append_data.child_pointers.data();\n+\t\tresult->n_children = 1;\n+\t\tappend_data.child_pointers[0] = FinalizeArrowChild(type, *append_data.child_data[0]);\n+\n+\t\t// now that struct has two children: the key and the value type\n+\t\tauto &struct_data = *append_data.child_data[0];\n+\t\tauto &struct_result = append_data.child_pointers[0];\n+\t\tstruct_data.child_pointers.resize(2);\n+\t\tstruct_result->n_buffers = 1;\n+\t\tstruct_result->n_children = 2;\n+\t\tstruct_result->length = struct_data.child_data[0]->row_count;\n+\t\tstruct_result->children = struct_data.child_pointers.data();\n+\n+\t\tD_ASSERT(struct_data.child_data[0]->row_count == struct_data.child_data[1]->row_count);\n+\n+\t\tauto &key_type = MapType::KeyType(type);\n+\t\tauto &value_type = MapType::ValueType(type);\n+\t\tstruct_data.child_pointers[0] = FinalizeArrowChild(key_type, *struct_data.child_data[0]);\n+\t\tstruct_data.child_pointers[1] = FinalizeArrowChild(value_type, *struct_data.child_data[1]);\n+\n+\t\t// keys cannot have null values\n+\t\tif (struct_data.child_pointers[0]->null_count > 0) {\n+\t\t\tthrow std::runtime_error(\"Arrow doesn't accept NULL keys on Maps\");\n+\t\t}\n+\t}\n+};\n+\n+//! Append a data chunk to the underlying arrow array\n+void ArrowAppender::Append(DataChunk &input) {\n+\tD_ASSERT(types == input.GetTypes());\n+\tfor (idx_t i = 0; i < input.ColumnCount(); i++) {\n+\t\troot_data[i]->append_vector(*root_data[i], input.data[i], input.size());\n+\t}\n+\trow_count += input.size();\n+}\n+//===--------------------------------------------------------------------===//\n+// Initialize Arrow Child\n+//===--------------------------------------------------------------------===//\n+template <class OP>\n+static void InitializeFunctionPointers(ArrowAppendData &append_data) {\n+\tappend_data.initialize = OP::Initialize;\n+\tappend_data.append_vector = OP::Append;\n+\tappend_data.finalize = OP::Finalize;\n+}\n+\n+static void InitializeFunctionPointers(ArrowAppendData &append_data, const LogicalType &type) {\n+\t// handle special logical types\n+\tswitch (type.id()) {\n+\tcase LogicalTypeId::BOOLEAN:\n+\t\tInitializeFunctionPointers<ArrowBoolData>(append_data);\n+\t\tbreak;\n+\tcase LogicalTypeId::TINYINT:\n+\t\tInitializeFunctionPointers<ArrowScalarData<int8_t>>(append_data);\n+\t\tbreak;\n+\tcase LogicalTypeId::SMALLINT:\n+\t\tInitializeFunctionPointers<ArrowScalarData<int16_t>>(append_data);\n+\t\tbreak;\n+\tcase LogicalTypeId::DATE:\n+\tcase LogicalTypeId::INTEGER:\n+\t\tInitializeFunctionPointers<ArrowScalarData<int32_t>>(append_data);\n+\t\tbreak;\n+\tcase LogicalTypeId::TIME:\n+\tcase LogicalTypeId::TIMESTAMP_SEC:\n+\tcase LogicalTypeId::TIMESTAMP_MS:\n+\tcase LogicalTypeId::TIMESTAMP:\n+\tcase LogicalTypeId::TIMESTAMP_NS:\n+\tcase LogicalTypeId::TIMESTAMP_TZ:\n+\tcase LogicalTypeId::TIME_TZ:\n+\tcase LogicalTypeId::BIGINT:\n+\t\tInitializeFunctionPointers<ArrowScalarData<int64_t>>(append_data);\n+\t\tbreak;\n+\tcase LogicalTypeId::HUGEINT:\n+\t\tInitializeFunctionPointers<ArrowScalarData<hugeint_t>>(append_data);\n+\t\tbreak;\n+\tcase LogicalTypeId::UTINYINT:\n+\t\tInitializeFunctionPointers<ArrowScalarData<uint8_t>>(append_data);\n+\t\tbreak;\n+\tcase LogicalTypeId::USMALLINT:\n+\t\tInitializeFunctionPointers<ArrowScalarData<uint16_t>>(append_data);\n+\t\tbreak;\n+\tcase LogicalTypeId::UINTEGER:\n+\t\tInitializeFunctionPointers<ArrowScalarData<uint32_t>>(append_data);\n+\t\tbreak;\n+\tcase LogicalTypeId::UBIGINT:\n+\t\tInitializeFunctionPointers<ArrowScalarData<uint64_t>>(append_data);\n+\t\tbreak;\n+\tcase LogicalTypeId::FLOAT:\n+\t\tInitializeFunctionPointers<ArrowScalarData<float>>(append_data);\n+\t\tbreak;\n+\tcase LogicalTypeId::DOUBLE:\n+\t\tInitializeFunctionPointers<ArrowScalarData<double>>(append_data);\n+\t\tbreak;\n+\tcase LogicalTypeId::DECIMAL:\n+\t\tswitch (type.InternalType()) {\n+\t\tcase PhysicalType::INT16:\n+\t\t\tInitializeFunctionPointers<ArrowScalarData<hugeint_t, int16_t>>(append_data);\n+\t\t\tbreak;\n+\t\tcase PhysicalType::INT32:\n+\t\t\tInitializeFunctionPointers<ArrowScalarData<hugeint_t, int32_t>>(append_data);\n+\t\t\tbreak;\n+\t\tcase PhysicalType::INT64:\n+\t\t\tInitializeFunctionPointers<ArrowScalarData<hugeint_t, int64_t>>(append_data);\n+\t\t\tbreak;\n+\t\tcase PhysicalType::INT128:\n+\t\t\tInitializeFunctionPointers<ArrowScalarData<hugeint_t>>(append_data);\n+\t\t\tbreak;\n+\t\tdefault:\n+\t\t\tthrow InternalException(\"Unsupported internal decimal type\");\n+\t\t}\n+\t\tbreak;\n+\tcase LogicalTypeId::VARCHAR:\n+\tcase LogicalTypeId::BLOB:\n+\tcase LogicalTypeId::JSON:\n+\t\tInitializeFunctionPointers<ArrowVarcharData<string_t>>(append_data);\n+\t\tbreak;\n+\tcase LogicalTypeId::UUID:\n+\t\tInitializeFunctionPointers<ArrowVarcharData<hugeint_t, ArrowUUIDConverter>>(append_data);\n+\t\tbreak;\n+\tcase LogicalTypeId::ENUM:\n+\t\tswitch (type.InternalType()) {\n+\t\tcase PhysicalType::UINT8:\n+\t\t\tInitializeFunctionPointers<ArrowEnumData<uint8_t>>(append_data);\n+\t\t\tbreak;\n+\t\tcase PhysicalType::UINT16:\n+\t\t\tInitializeFunctionPointers<ArrowEnumData<uint16_t>>(append_data);\n+\t\t\tbreak;\n+\t\tcase PhysicalType::UINT32:\n+\t\t\tInitializeFunctionPointers<ArrowEnumData<uint32_t>>(append_data);\n+\t\t\tbreak;\n+\t\tdefault:\n+\t\t\tthrow InternalException(\"Unsupported internal enum type\");\n+\t\t}\n+\t\tbreak;\n+\tcase LogicalTypeId::INTERVAL:\n+\t\tInitializeFunctionPointers<ArrowScalarData<int64_t, interval_t, ArrowIntervalConverter>>(append_data);\n+\t\tbreak;\n+\tcase LogicalTypeId::STRUCT:\n+\t\tInitializeFunctionPointers<ArrowStructData>(append_data);\n+\t\tbreak;\n+\tcase LogicalTypeId::LIST:\n+\t\tInitializeFunctionPointers<ArrowListData>(append_data);\n+\t\tbreak;\n+\tcase LogicalTypeId::MAP:\n+\t\tInitializeFunctionPointers<ArrowMapData>(append_data);\n+\t\tbreak;\n+\tdefault:\n+\t\tthrow InternalException(\"Unsupported type in DuckDB -> Arrow Conversion: %s\\n\", type.ToString());\n+\t}\n+}\n+\n+unique_ptr<ArrowAppendData> InitializeArrowChild(const LogicalType &type, idx_t capacity) {\n+\tauto result = make_unique<ArrowAppendData>();\n+\tInitializeFunctionPointers(*result, type);\n+\n+\tauto byte_count = (capacity + 7) / 8;\n+\tresult->validity.reserve(byte_count);\n+\tresult->initialize(*result, type, capacity);\n+\treturn result;\n+}\n+\n+static void ReleaseDuckDBArrowAppendArray(ArrowArray *array) {\n+\tif (!array || !array->release) {\n+\t\treturn;\n+\t}\n+\tarray->release = nullptr;\n+\tauto holder = static_cast<ArrowAppendData *>(array->private_data);\n+\tdelete holder;\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// Finalize Arrow Child\n+//===--------------------------------------------------------------------===//\n+ArrowArray *FinalizeArrowChild(const LogicalType &type, ArrowAppendData &append_data) {\n+\tauto result = make_unique<ArrowArray>();\n+\n+\tresult->private_data = nullptr;\n+\tresult->release = ReleaseDuckDBArrowAppendArray;\n+\tresult->n_children = 0;\n+\tresult->null_count = 0;\n+\tresult->offset = 0;\n+\tresult->dictionary = nullptr;\n+\tresult->buffers = append_data.buffers.data();\n+\tresult->null_count = append_data.null_count;\n+\tresult->length = append_data.row_count;\n+\tresult->buffers[0] = append_data.validity.data();\n+\n+\tif (append_data.finalize) {\n+\t\tappend_data.finalize(append_data, type, result.get());\n+\t}\n+\n+\tappend_data.array = move(result);\n+\treturn append_data.array.get();\n+}\n+\n+//! Returns the underlying arrow array\n+ArrowArray ArrowAppender::Finalize() {\n+\tD_ASSERT(root_data.size() == types.size());\n+\tauto root_holder = make_unique<ArrowAppendData>();\n+\n+\tArrowArray result;\n+\troot_holder->child_pointers.resize(types.size());\n+\tresult.children = root_holder->child_pointers.data();\n+\tresult.n_children = types.size();\n+\n+\t// Configure root array\n+\tresult.length = row_count;\n+\tresult.n_children = types.size();\n+\tresult.n_buffers = 1;\n+\tresult.buffers = root_holder->buffers.data(); // there is no actual buffer there since we don't have NULLs\n+\tresult.offset = 0;\n+\tresult.null_count = 0; // needs to be 0\n+\tresult.dictionary = nullptr;\n+\troot_holder->child_data = move(root_data);\n+\n+\tfor (idx_t i = 0; i < root_holder->child_data.size(); i++) {\n+\t\troot_holder->child_pointers[i] = FinalizeArrowChild(types[i], *root_holder->child_data[i]);\n+\t}\n+\n+\t// Release ownership to caller\n+\tresult.private_data = root_holder.release();\n+\tresult.release = ReleaseDuckDBArrowAppendArray;\n+\treturn result;\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/common/arrow/arrow_converter.cpp b/src/common/arrow/arrow_converter.cpp\nnew file mode 100644\nindex 000000000000..97aba72dfcee\n--- /dev/null\n+++ b/src/common/arrow/arrow_converter.cpp\n@@ -0,0 +1,293 @@\n+#include \"duckdb/common/types/data_chunk.hpp\"\n+\n+#include \"duckdb/common/arrow/arrow.hpp\"\n+#include \"duckdb/common/arrow/arrow_converter.hpp\"\n+#include \"duckdb/common/exception.hpp\"\n+#include \"duckdb/common/helper.hpp\"\n+#include \"duckdb/common/serializer.hpp\"\n+#include \"duckdb/common/types/interval.hpp\"\n+#include \"duckdb/common/types/sel_cache.hpp\"\n+#include \"duckdb/common/types/vector_cache.hpp\"\n+#include \"duckdb/common/unordered_map.hpp\"\n+#include \"duckdb/common/vector.hpp\"\n+#include <list>\n+#include \"duckdb/common/arrow/arrow_appender.hpp\"\n+\n+namespace duckdb {\n+\n+void ArrowConverter::ToArrowArray(DataChunk &input, ArrowArray *out_array) {\n+\tArrowAppender appender(input.GetTypes(), input.size());\n+\tappender.Append(input);\n+\t*out_array = appender.Finalize();\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// Arrow Schema\n+//===--------------------------------------------------------------------===//\n+struct DuckDBArrowSchemaHolder {\n+\t// unused in children\n+\tvector<ArrowSchema> children;\n+\t// unused in children\n+\tvector<ArrowSchema *> children_ptrs;\n+\t//! used for nested structures\n+\tstd::list<std::vector<ArrowSchema>> nested_children;\n+\tstd::list<std::vector<ArrowSchema *>> nested_children_ptr;\n+\t//! This holds strings created to represent decimal types\n+\tvector<unique_ptr<char[]>> owned_type_names;\n+};\n+\n+static void ReleaseDuckDBArrowSchema(ArrowSchema *schema) {\n+\tif (!schema || !schema->release) {\n+\t\treturn;\n+\t}\n+\tschema->release = nullptr;\n+\tauto holder = static_cast<DuckDBArrowSchemaHolder *>(schema->private_data);\n+\tdelete holder;\n+}\n+\n+void InitializeChild(ArrowSchema &child, const string &name = \"\") {\n+\t//! Child is cleaned up by parent\n+\tchild.private_data = nullptr;\n+\tchild.release = ReleaseDuckDBArrowSchema;\n+\n+\t//! Store the child schema\n+\tchild.flags = ARROW_FLAG_NULLABLE;\n+\tchild.name = name.c_str();\n+\tchild.n_children = 0;\n+\tchild.children = nullptr;\n+\tchild.metadata = nullptr;\n+\tchild.dictionary = nullptr;\n+}\n+void SetArrowFormat(DuckDBArrowSchemaHolder &root_holder, ArrowSchema &child, const LogicalType &type,\n+                    string &config_timezone);\n+\n+void SetArrowMapFormat(DuckDBArrowSchemaHolder &root_holder, ArrowSchema &child, const LogicalType &type,\n+                       string &config_timezone) {\n+\tchild.format = \"+m\";\n+\t//! Map has one child which is a struct\n+\tchild.n_children = 1;\n+\troot_holder.nested_children.emplace_back();\n+\troot_holder.nested_children.back().resize(1);\n+\troot_holder.nested_children_ptr.emplace_back();\n+\troot_holder.nested_children_ptr.back().push_back(&root_holder.nested_children.back()[0]);\n+\tInitializeChild(root_holder.nested_children.back()[0]);\n+\tchild.children = &root_holder.nested_children_ptr.back()[0];\n+\tchild.children[0]->name = \"entries\";\n+\tchild_list_t<LogicalType> struct_child_types;\n+\tstruct_child_types.push_back(std::make_pair(\"key\", ListType::GetChildType(StructType::GetChildType(type, 0))));\n+\tstruct_child_types.push_back(std::make_pair(\"value\", ListType::GetChildType(StructType::GetChildType(type, 1))));\n+\tauto struct_type = LogicalType::STRUCT(move(struct_child_types));\n+\tSetArrowFormat(root_holder, *child.children[0], struct_type, config_timezone);\n+}\n+\n+void SetArrowFormat(DuckDBArrowSchemaHolder &root_holder, ArrowSchema &child, const LogicalType &type,\n+                    string &config_timezone) {\n+\tswitch (type.id()) {\n+\tcase LogicalTypeId::BOOLEAN:\n+\t\tchild.format = \"b\";\n+\t\tbreak;\n+\tcase LogicalTypeId::TINYINT:\n+\t\tchild.format = \"c\";\n+\t\tbreak;\n+\tcase LogicalTypeId::SMALLINT:\n+\t\tchild.format = \"s\";\n+\t\tbreak;\n+\tcase LogicalTypeId::INTEGER:\n+\t\tchild.format = \"i\";\n+\t\tbreak;\n+\tcase LogicalTypeId::BIGINT:\n+\t\tchild.format = \"l\";\n+\t\tbreak;\n+\tcase LogicalTypeId::UTINYINT:\n+\t\tchild.format = \"C\";\n+\t\tbreak;\n+\tcase LogicalTypeId::USMALLINT:\n+\t\tchild.format = \"S\";\n+\t\tbreak;\n+\tcase LogicalTypeId::UINTEGER:\n+\t\tchild.format = \"I\";\n+\t\tbreak;\n+\tcase LogicalTypeId::UBIGINT:\n+\t\tchild.format = \"L\";\n+\t\tbreak;\n+\tcase LogicalTypeId::FLOAT:\n+\t\tchild.format = \"f\";\n+\t\tbreak;\n+\tcase LogicalTypeId::HUGEINT:\n+\t\tchild.format = \"d:38,0\";\n+\t\tbreak;\n+\tcase LogicalTypeId::DOUBLE:\n+\t\tchild.format = \"g\";\n+\t\tbreak;\n+\tcase LogicalTypeId::UUID:\n+\tcase LogicalTypeId::JSON:\n+\tcase LogicalTypeId::VARCHAR:\n+\t\tchild.format = \"u\";\n+\t\tbreak;\n+\tcase LogicalTypeId::DATE:\n+\t\tchild.format = \"tdD\";\n+\t\tbreak;\n+\tcase LogicalTypeId::TIME:\n+\tcase LogicalTypeId::TIME_TZ:\n+\t\tchild.format = \"ttu\";\n+\t\tbreak;\n+\tcase LogicalTypeId::TIMESTAMP:\n+\t\tchild.format = \"tsu:\";\n+\t\tbreak;\n+\tcase LogicalTypeId::TIMESTAMP_TZ: {\n+\t\tstring format = \"tsu:\" + config_timezone;\n+\t\tunique_ptr<char[]> format_ptr = unique_ptr<char[]>(new char[format.size() + 1]);\n+\t\tfor (size_t i = 0; i < format.size(); i++) {\n+\t\t\tformat_ptr[i] = format[i];\n+\t\t}\n+\t\tformat_ptr[format.size()] = '\\0';\n+\t\troot_holder.owned_type_names.push_back(move(format_ptr));\n+\t\tchild.format = root_holder.owned_type_names.back().get();\n+\t\tbreak;\n+\t}\n+\tcase LogicalTypeId::TIMESTAMP_SEC:\n+\t\tchild.format = \"tss:\";\n+\t\tbreak;\n+\tcase LogicalTypeId::TIMESTAMP_NS:\n+\t\tchild.format = \"tsn:\";\n+\t\tbreak;\n+\tcase LogicalTypeId::TIMESTAMP_MS:\n+\t\tchild.format = \"tsm:\";\n+\t\tbreak;\n+\tcase LogicalTypeId::INTERVAL:\n+\t\tchild.format = \"tDm\";\n+\t\tbreak;\n+\tcase LogicalTypeId::DECIMAL: {\n+\t\tuint8_t width, scale;\n+\t\ttype.GetDecimalProperties(width, scale);\n+\t\tstring format = \"d:\" + to_string(width) + \",\" + to_string(scale);\n+\t\tunique_ptr<char[]> format_ptr = unique_ptr<char[]>(new char[format.size() + 1]);\n+\t\tfor (size_t i = 0; i < format.size(); i++) {\n+\t\t\tformat_ptr[i] = format[i];\n+\t\t}\n+\t\tformat_ptr[format.size()] = '\\0';\n+\t\troot_holder.owned_type_names.push_back(move(format_ptr));\n+\t\tchild.format = root_holder.owned_type_names.back().get();\n+\t\tbreak;\n+\t}\n+\tcase LogicalTypeId::SQLNULL: {\n+\t\tchild.format = \"n\";\n+\t\tbreak;\n+\t}\n+\tcase LogicalTypeId::BLOB: {\n+\t\tchild.format = \"z\";\n+\t\tbreak;\n+\t}\n+\tcase LogicalTypeId::LIST: {\n+\t\tchild.format = \"+l\";\n+\t\tchild.n_children = 1;\n+\t\troot_holder.nested_children.emplace_back();\n+\t\troot_holder.nested_children.back().resize(1);\n+\t\troot_holder.nested_children_ptr.emplace_back();\n+\t\troot_holder.nested_children_ptr.back().push_back(&root_holder.nested_children.back()[0]);\n+\t\tInitializeChild(root_holder.nested_children.back()[0]);\n+\t\tchild.children = &root_holder.nested_children_ptr.back()[0];\n+\t\tchild.children[0]->name = \"l\";\n+\t\tSetArrowFormat(root_holder, **child.children, ListType::GetChildType(type), config_timezone);\n+\t\tbreak;\n+\t}\n+\tcase LogicalTypeId::STRUCT: {\n+\t\tchild.format = \"+s\";\n+\t\tauto &child_types = StructType::GetChildTypes(type);\n+\t\tchild.n_children = child_types.size();\n+\t\troot_holder.nested_children.emplace_back();\n+\t\troot_holder.nested_children.back().resize(child_types.size());\n+\t\troot_holder.nested_children_ptr.emplace_back();\n+\t\troot_holder.nested_children_ptr.back().resize(child_types.size());\n+\t\tfor (idx_t type_idx = 0; type_idx < child_types.size(); type_idx++) {\n+\t\t\troot_holder.nested_children_ptr.back()[type_idx] = &root_holder.nested_children.back()[type_idx];\n+\t\t}\n+\t\tchild.children = &root_holder.nested_children_ptr.back()[0];\n+\t\tfor (size_t type_idx = 0; type_idx < child_types.size(); type_idx++) {\n+\n+\t\t\tInitializeChild(*child.children[type_idx]);\n+\n+\t\t\tauto &struct_col_name = child_types[type_idx].first;\n+\t\t\tunique_ptr<char[]> name_ptr = unique_ptr<char[]>(new char[struct_col_name.size() + 1]);\n+\t\t\tfor (size_t i = 0; i < struct_col_name.size(); i++) {\n+\t\t\t\tname_ptr[i] = struct_col_name[i];\n+\t\t\t}\n+\t\t\tname_ptr[struct_col_name.size()] = '\\0';\n+\t\t\troot_holder.owned_type_names.push_back(move(name_ptr));\n+\n+\t\t\tchild.children[type_idx]->name = root_holder.owned_type_names.back().get();\n+\t\t\tSetArrowFormat(root_holder, *child.children[type_idx], child_types[type_idx].second, config_timezone);\n+\t\t}\n+\t\tbreak;\n+\t}\n+\tcase LogicalTypeId::MAP: {\n+\t\tSetArrowMapFormat(root_holder, child, type, config_timezone);\n+\t\tbreak;\n+\t}\n+\tcase LogicalTypeId::ENUM: {\n+\t\t// TODO what do we do with pointer enums here?\n+\t\tswitch (EnumType::GetPhysicalType(type)) {\n+\t\tcase PhysicalType::UINT8:\n+\t\t\tchild.format = \"C\";\n+\t\t\tbreak;\n+\t\tcase PhysicalType::UINT16:\n+\t\t\tchild.format = \"S\";\n+\t\t\tbreak;\n+\t\tcase PhysicalType::UINT32:\n+\t\t\tchild.format = \"I\";\n+\t\t\tbreak;\n+\t\tdefault:\n+\t\t\tthrow InternalException(\"Unsupported Enum Internal Type\");\n+\t\t}\n+\t\troot_holder.nested_children.emplace_back();\n+\t\troot_holder.nested_children.back().resize(1);\n+\t\troot_holder.nested_children_ptr.emplace_back();\n+\t\troot_holder.nested_children_ptr.back().push_back(&root_holder.nested_children.back()[0]);\n+\t\tInitializeChild(root_holder.nested_children.back()[0]);\n+\t\tchild.dictionary = root_holder.nested_children_ptr.back()[0];\n+\t\tchild.dictionary->format = \"u\";\n+\t\tbreak;\n+\t}\n+\tdefault:\n+\t\tthrow InternalException(\"Unsupported Arrow type \" + type.ToString());\n+\t}\n+}\n+\n+void ArrowConverter::ToArrowSchema(ArrowSchema *out_schema, vector<LogicalType> &types, vector<string> &names,\n+                                   string &config_timezone) {\n+\tD_ASSERT(out_schema);\n+\tD_ASSERT(types.size() == names.size());\n+\tidx_t column_count = types.size();\n+\t// Allocate as unique_ptr first to cleanup properly on error\n+\tauto root_holder = make_unique<DuckDBArrowSchemaHolder>();\n+\n+\t// Allocate the children\n+\troot_holder->children.resize(column_count);\n+\troot_holder->children_ptrs.resize(column_count, nullptr);\n+\tfor (size_t i = 0; i < column_count; ++i) {\n+\t\troot_holder->children_ptrs[i] = &root_holder->children[i];\n+\t}\n+\tout_schema->children = root_holder->children_ptrs.data();\n+\tout_schema->n_children = column_count;\n+\n+\t// Store the schema\n+\tout_schema->format = \"+s\"; // struct apparently\n+\tout_schema->flags = 0;\n+\tout_schema->metadata = nullptr;\n+\tout_schema->name = \"duckdb_query_result\";\n+\tout_schema->dictionary = nullptr;\n+\n+\t// Configure all child schemas\n+\tfor (idx_t col_idx = 0; col_idx < column_count; col_idx++) {\n+\n+\t\tauto &child = root_holder->children[col_idx];\n+\t\tInitializeChild(child, names[col_idx]);\n+\t\tSetArrowFormat(*root_holder, child, types[col_idx], config_timezone);\n+\t}\n+\n+\t// Release ownership to caller\n+\tout_schema->private_data = root_holder.release();\n+\tout_schema->release = ReleaseDuckDBArrowSchema;\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/common/arrow_wrapper.cpp b/src/common/arrow/arrow_wrapper.cpp\nsimilarity index 73%\nrename from src/common/arrow_wrapper.cpp\nrename to src/common/arrow/arrow_wrapper.cpp\nindex 170f89d66dbf..4e56d1ac7684 100644\n--- a/src/common/arrow_wrapper.cpp\n+++ b/src/common/arrow/arrow_wrapper.cpp\n@@ -1,13 +1,15 @@\n-#include \"duckdb/common/arrow_wrapper.hpp\"\n+#include \"duckdb/common/arrow/arrow_wrapper.hpp\"\n+#include \"duckdb/common/arrow/arrow_converter.hpp\"\n \n #include \"duckdb/common/assert.hpp\"\n #include \"duckdb/common/exception.hpp\"\n \n #include \"duckdb/main/stream_query_result.hpp\"\n \n-#include \"duckdb/common/result_arrow_wrapper.hpp\"\n-\n+#include \"duckdb/common/arrow/result_arrow_wrapper.hpp\"\n+#include \"duckdb/common/arrow/arrow_appender.hpp\"\n #include \"duckdb/main/query_result.hpp\"\n+\n namespace duckdb {\n \n ArrowSchemaWrapper::~ArrowSchemaWrapper() {\n@@ -77,7 +79,8 @@ int ResultArrowArrayStreamWrapper::MyStreamGetSchema(struct ArrowArrayStream *st\n \t}\n \tauto my_stream = (ResultArrowArrayStreamWrapper *)stream->private_data;\n \tif (!my_stream->column_types.empty()) {\n-\t\tQueryResult::ToArrowSchema(out, my_stream->column_types, my_stream->column_names, my_stream->timezone_config);\n+\t\tArrowConverter::ToArrowSchema(out, my_stream->column_types, my_stream->column_names,\n+\t\t                              my_stream->timezone_config);\n \t\treturn 0;\n \t}\n \n@@ -97,7 +100,7 @@ int ResultArrowArrayStreamWrapper::MyStreamGetSchema(struct ArrowArrayStream *st\n \t\tmy_stream->column_types = result.types;\n \t\tmy_stream->column_names = result.names;\n \t}\n-\tQueryResult::ToArrowSchema(out, my_stream->column_types, my_stream->column_names, my_stream->timezone_config);\n+\tArrowConverter::ToArrowSchema(out, my_stream->column_types, my_stream->column_names, my_stream->timezone_config);\n \treturn 0;\n }\n \n@@ -123,29 +126,17 @@ int ResultArrowArrayStreamWrapper::MyStreamGetNext(struct ArrowArrayStream *stre\n \t\tmy_stream->column_types = result.types;\n \t\tmy_stream->column_names = result.names;\n \t}\n-\tunique_ptr<DataChunk> chunk_result = result.Fetch();\n-\tif (!chunk_result) {\n-\t\tif (!result.success) {\n-\t\t\tmy_stream->last_error = result.GetError();\n-\t\t\treturn -1;\n-\t\t}\n+\tidx_t result_count;\n+\tstring error;\n+\tif (!ArrowUtil::TryFetchChunk(&result, my_stream->batch_size, out, result_count, error)) {\n+\t\tD_ASSERT(!error.empty());\n+\t\tmy_stream->last_error = error;\n+\t\treturn -1;\n+\t}\n+\tif (result_count == 0) {\n \t\t// Nothing to output\n \t\tout->release = nullptr;\n-\t\treturn 0;\n-\t}\n-\tunique_ptr<DataChunk> agg_chunk_result = make_unique<DataChunk>();\n-\tagg_chunk_result->Initialize(Allocator::DefaultAllocator(), chunk_result->GetTypes());\n-\tagg_chunk_result->Append(*chunk_result, true);\n-\n-\twhile (agg_chunk_result->size() < my_stream->batch_size) {\n-\t\tauto new_chunk = result.Fetch();\n-\t\tif (!new_chunk) {\n-\t\t\tbreak;\n-\t\t} else {\n-\t\t\tagg_chunk_result->Append(*new_chunk, true);\n-\t\t}\n \t}\n-\tagg_chunk_result->ToArrowArray(out);\n \treturn 0;\n }\n \n@@ -165,6 +156,7 @@ const char *ResultArrowArrayStreamWrapper::MyStreamGetLastError(struct ArrowArra\n \tauto my_stream = (ResultArrowArrayStreamWrapper *)stream->private_data;\n \treturn my_stream->last_error.c_str();\n }\n+\n ResultArrowArrayStreamWrapper::ResultArrowArrayStreamWrapper(unique_ptr<QueryResult> result_p, idx_t batch_size_p)\n     : result(move(result_p)) {\n \t//! We first initialize the private data of the stream\n@@ -181,27 +173,46 @@ ResultArrowArrayStreamWrapper::ResultArrowArrayStreamWrapper(unique_ptr<QueryRes\n \tstream.get_last_error = ResultArrowArrayStreamWrapper::MyStreamGetLastError;\n }\n \n-unique_ptr<DataChunk> ArrowUtil::FetchNext(QueryResult &result) {\n-\tauto chunk = result.Fetch();\n-\tif (!result.success) {\n-\t\tthrow std::runtime_error(result.error);\n+bool ArrowUtil::TryFetchNext(QueryResult &result, unique_ptr<DataChunk> &chunk, string &error) {\n+\tif (result.type == QueryResultType::STREAM_RESULT) {\n+\t\tauto &stream_result = (StreamQueryResult &)result;\n+\t\tif (!stream_result.IsOpen()) {\n+\t\t\treturn true;\n+\t\t}\n \t}\n-\treturn chunk;\n+\treturn result.TryFetch(chunk, error);\n }\n \n-unique_ptr<DataChunk> ArrowUtil::FetchChunk(QueryResult *result, idx_t chunk_size) {\n-\tauto data_chunk = FetchNext(*result);\n-\tif (!data_chunk) {\n-\t\treturn data_chunk;\n-\t}\n-\twhile (data_chunk->size() < chunk_size) {\n-\t\tauto next_chunk = FetchNext(*result);\n-\t\tif (!next_chunk || next_chunk->size() == 0) {\n+bool ArrowUtil::TryFetchChunk(QueryResult *result, idx_t chunk_size, ArrowArray *out, idx_t &count, string &error) {\n+\tcount = 0;\n+\tArrowAppender appender(result->types, chunk_size);\n+\twhile (count < chunk_size) {\n+\t\tunique_ptr<DataChunk> data_chunk;\n+\t\tif (!TryFetchNext(*result, data_chunk, error)) {\n+\t\t\tif (!result->success) {\n+\t\t\t\terror = result->error;\n+\t\t\t}\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (!data_chunk || data_chunk->size() == 0) {\n \t\t\tbreak;\n \t\t}\n-\t\tdata_chunk->Append(*next_chunk, true);\n+\t\tcount += data_chunk->size();\n+\t\tappender.Append(*data_chunk);\n+\t}\n+\tif (count > 0) {\n+\t\t*out = appender.Finalize();\n+\t}\n+\treturn true;\n+}\n+\n+idx_t ArrowUtil::FetchChunk(QueryResult *result, idx_t chunk_size, ArrowArray *out) {\n+\tstring error;\n+\tidx_t result_count;\n+\tif (!TryFetchChunk(result, chunk_size, out, result_count, error)) {\n+\t\tthrow std::runtime_error(error);\n \t}\n-\treturn data_chunk;\n+\treturn result_count;\n }\n \n } // namespace duckdb\ndiff --git a/src/common/enums/physical_operator_type.cpp b/src/common/enums/physical_operator_type.cpp\nindex d59c4a3fa0c8..99d8fb77a4b9 100644\n--- a/src/common/enums/physical_operator_type.cpp\n+++ b/src/common/enums/physical_operator_type.cpp\n@@ -11,6 +11,8 @@ string PhysicalOperatorToString(PhysicalOperatorType type) {\n \t\treturn \"DUMMY_SCAN\";\n \tcase PhysicalOperatorType::CHUNK_SCAN:\n \t\treturn \"CHUNK_SCAN\";\n+\tcase PhysicalOperatorType::COLUMN_DATA_SCAN:\n+\t\treturn \"COLUMN_DATA_SCAN\";\n \tcase PhysicalOperatorType::DELIM_SCAN:\n \t\treturn \"DELIM_SCAN\";\n \tcase PhysicalOperatorType::ORDER_BY:\ndiff --git a/src/common/local_file_system.cpp b/src/common/local_file_system.cpp\nindex 10704617c82c..cb6ff414e6fc 100644\n--- a/src/common/local_file_system.cpp\n+++ b/src/common/local_file_system.cpp\n@@ -132,6 +132,7 @@ struct UnixFileHandle : public FileHandle {\n \tvoid Close() override {\n \t\tif (fd != -1) {\n \t\t\tclose(fd);\n+\t\t\tfd = -1;\n \t\t}\n \t};\n };\ndiff --git a/src/common/types.cpp b/src/common/types.cpp\nindex d085a6677070..6ae30c757cff 100644\n--- a/src/common/types.cpp\n+++ b/src/common/types.cpp\n@@ -835,11 +835,11 @@ struct ExtraTypeInfo {\n \t}\n };\n \n-void LogicalType::SetAlias(string &alias) {\n+void LogicalType::SetAlias(string alias) {\n \tif (!type_info_) {\n-\t\ttype_info_ = make_shared<ExtraTypeInfo>(ExtraTypeInfoType::GENERIC_TYPE_INFO, alias);\n+\t\ttype_info_ = make_shared<ExtraTypeInfo>(ExtraTypeInfoType::GENERIC_TYPE_INFO, move(alias));\n \t} else {\n-\t\ttype_info_->alias = alias;\n+\t\ttype_info_->alias = move(alias);\n \t}\n }\n \n@@ -851,6 +851,13 @@ string LogicalType::GetAlias() const {\n \t}\n }\n \n+bool LogicalType::HasAlias() const {\n+\tif (!type_info_) {\n+\t\treturn false;\n+\t}\n+\treturn !type_info_->alias.empty();\n+}\n+\n void LogicalType::SetCatalog(LogicalType &type, TypeCatalogEntry *catalog_entry) {\n \tauto info = type.AuxInfo();\n \tD_ASSERT(info);\ndiff --git a/src/common/types/CMakeLists.txt b/src/common/types/CMakeLists.txt\nindex 47ced9412737..34b7acce12fd 100644\n--- a/src/common/types/CMakeLists.txt\n+++ b/src/common/types/CMakeLists.txt\n@@ -6,10 +6,13 @@ endif()\n add_library_unity(\n   duckdb_common_types\n   OBJECT\n-  batched_chunk_collection.cpp\n+  batched_data_collection.cpp\n   blob.cpp\n   cast_helpers.cpp\n   chunk_collection.cpp\n+  column_data_allocator.cpp\n+  column_data_collection.cpp\n+  column_data_collection_segment.cpp\n   data_chunk.cpp\n   date.cpp\n   decimal.cpp\ndiff --git a/src/common/types/batched_chunk_collection.cpp b/src/common/types/batched_chunk_collection.cpp\ndeleted file mode 100644\nindex 8cd2a1eac22a..000000000000\n--- a/src/common/types/batched_chunk_collection.cpp\n+++ /dev/null\n@@ -1,71 +0,0 @@\n-#include \"duckdb/common/types/batched_chunk_collection.hpp\"\n-#include \"duckdb/common/printer.hpp\"\n-\n-namespace duckdb {\n-\n-BatchedChunkCollection::BatchedChunkCollection(Allocator &allocator) : allocator(allocator) {\n-}\n-\n-void BatchedChunkCollection::Append(DataChunk &input, idx_t batch_index) {\n-\tD_ASSERT(batch_index != DConstants::INVALID_INDEX);\n-\tauto entry = data.find(batch_index);\n-\tChunkCollection *collection;\n-\tif (entry == data.end()) {\n-\t\tauto new_collection = make_unique<ChunkCollection>(allocator);\n-\t\tcollection = new_collection.get();\n-\t\tdata.insert(make_pair(batch_index, move(new_collection)));\n-\t} else {\n-\t\tcollection = entry->second.get();\n-\t}\n-\tcollection->Append(input);\n-}\n-\n-void BatchedChunkCollection::Merge(BatchedChunkCollection &other) {\n-\tfor (auto &entry : other.data) {\n-\t\tif (data.find(entry.first) != data.end()) {\n-\t\t\tthrow InternalException(\n-\t\t\t    \"BatchChunkCollection::Merge error - batch index %d is present in both collections. This occurs when \"\n-\t\t\t    \"batch indexes are not uniquely distributed over threads\",\n-\t\t\t    entry.first);\n-\t\t}\n-\t\tdata[entry.first] = move(entry.second);\n-\t}\n-\tother.data.clear();\n-}\n-\n-void BatchedChunkCollection::InitializeScan(BatchedChunkScanState &state) {\n-\tstate.iterator = data.begin();\n-\tstate.chunk_index = 0;\n-}\n-\n-void BatchedChunkCollection::Scan(BatchedChunkScanState &state, DataChunk &output) {\n-\twhile (state.iterator != data.end()) {\n-\t\t// check if there is a chunk remaining in this collection\n-\t\tauto collection = state.iterator->second.get();\n-\t\tif (state.chunk_index < collection->ChunkCount()) {\n-\t\t\t// there is! increment the chunk count\n-\t\t\toutput.Reference(collection->GetChunk(state.chunk_index));\n-\t\t\tstate.chunk_index++;\n-\t\t\treturn;\n-\t\t}\n-\t\t// there isn't! move to the next collection\n-\t\tstate.iterator++;\n-\t\tstate.chunk_index = 0;\n-\t}\n-}\n-\n-string BatchedChunkCollection::ToString() const {\n-\tstring result;\n-\tresult += \"Batched Chunk Collection\\n\";\n-\tfor (auto &entry : data) {\n-\t\tresult += \"Batch Index - \" + to_string(entry.first) + \"\\n\";\n-\t\tresult += entry.second->ToString() + \"\\n\\n\";\n-\t}\n-\treturn result;\n-}\n-\n-void BatchedChunkCollection::Print() const {\n-\tPrinter::Print(ToString());\n-}\n-\n-} // namespace duckdb\ndiff --git a/src/common/types/batched_data_collection.cpp b/src/common/types/batched_data_collection.cpp\nnew file mode 100644\nindex 000000000000..a22660a602e0\n--- /dev/null\n+++ b/src/common/types/batched_data_collection.cpp\n@@ -0,0 +1,103 @@\n+#include \"duckdb/common/types/batched_data_collection.hpp\"\n+#include \"duckdb/common/printer.hpp\"\n+#include \"duckdb/storage/buffer_manager.hpp\"\n+\n+namespace duckdb {\n+\n+BatchedDataCollection::BatchedDataCollection(vector<LogicalType> types_p) : types(move(types_p)) {\n+}\n+\n+void BatchedDataCollection::Append(DataChunk &input, idx_t batch_index) {\n+\tD_ASSERT(batch_index != DConstants::INVALID_INDEX);\n+\tColumnDataCollection *collection;\n+\tif (last_collection.collection && last_collection.batch_index == batch_index) {\n+\t\t// we are inserting into the same collection as before: use it directly\n+\t\tcollection = last_collection.collection;\n+\t} else {\n+\t\t// new collection: check if there is already an entry\n+\t\tD_ASSERT(data.find(batch_index) == data.end());\n+\t\tunique_ptr<ColumnDataCollection> new_collection;\n+\t\tif (last_collection.collection) {\n+\t\t\tnew_collection = make_unique<ColumnDataCollection>(*last_collection.collection);\n+\t\t} else {\n+\t\t\tnew_collection = make_unique<ColumnDataCollection>(Allocator::DefaultAllocator(), types);\n+\t\t}\n+\t\tlast_collection.collection = new_collection.get();\n+\t\tlast_collection.batch_index = batch_index;\n+\t\tnew_collection->InitializeAppend(last_collection.append_state);\n+\t\tcollection = new_collection.get();\n+\t\tdata.insert(make_pair(batch_index, move(new_collection)));\n+\t}\n+\tcollection->Append(last_collection.append_state, input);\n+}\n+\n+void BatchedDataCollection::Merge(BatchedDataCollection &other) {\n+\tfor (auto &entry : other.data) {\n+\t\tif (data.find(entry.first) != data.end()) {\n+\t\t\tthrow InternalException(\n+\t\t\t    \"BatchedDataCollection::Merge error - batch index %d is present in both collections. This occurs when \"\n+\t\t\t    \"batch indexes are not uniquely distributed over threads\",\n+\t\t\t    entry.first);\n+\t\t}\n+\t\tdata[entry.first] = move(entry.second);\n+\t}\n+\tother.data.clear();\n+}\n+\n+void BatchedDataCollection::InitializeScan(BatchedChunkScanState &state) {\n+\tstate.iterator = data.begin();\n+\tif (state.iterator == data.end()) {\n+\t\treturn;\n+\t}\n+\tstate.iterator->second->InitializeScan(state.scan_state);\n+}\n+\n+void BatchedDataCollection::Scan(BatchedChunkScanState &state, DataChunk &output) {\n+\twhile (state.iterator != data.end()) {\n+\t\t// check if there is a chunk remaining in this collection\n+\t\tauto collection = state.iterator->second.get();\n+\t\tcollection->Scan(state.scan_state, output);\n+\t\tif (output.size() > 0) {\n+\t\t\treturn;\n+\t\t}\n+\t\t// there isn't! move to the next collection\n+\t\tstate.iterator++;\n+\t\tif (state.iterator == data.end()) {\n+\t\t\treturn;\n+\t\t}\n+\t\tstate.iterator->second->InitializeScan(state.scan_state);\n+\t}\n+}\n+\n+unique_ptr<ColumnDataCollection> BatchedDataCollection::FetchCollection() {\n+\tunique_ptr<ColumnDataCollection> result;\n+\tfor (auto &entry : data) {\n+\t\tif (!result) {\n+\t\t\tresult = move(entry.second);\n+\t\t} else {\n+\t\t\tresult->Combine(*entry.second);\n+\t\t}\n+\t}\n+\tdata.clear();\n+\tif (!result) {\n+\t\t// empty result\n+\t\treturn make_unique<ColumnDataCollection>(Allocator::DefaultAllocator(), types);\n+\t}\n+\treturn result;\n+}\n+\n+string BatchedDataCollection::ToString() const {\n+\tstring result;\n+\tresult += \"Batched Data Collection\\n\";\n+\tfor (auto &entry : data) {\n+\t\tresult += \"Batch Index - \" + to_string(entry.first) + \"\\n\";\n+\t\tresult += entry.second->ToString() + \"\\n\\n\";\n+\t}\n+\treturn result;\n+}\n+\n+void BatchedDataCollection::Print() const {\n+\tPrinter::Print(ToString());\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/common/types/column_data_allocator.cpp b/src/common/types/column_data_allocator.cpp\nnew file mode 100644\nindex 000000000000..7cb767ce9951\n--- /dev/null\n+++ b/src/common/types/column_data_allocator.cpp\n@@ -0,0 +1,139 @@\n+#include \"duckdb/common/types/column_data_allocator.hpp\"\n+#include \"duckdb/storage/buffer_manager.hpp\"\n+#include \"duckdb/common/types/column_data_collection_segment.hpp\"\n+\n+namespace duckdb {\n+\n+ColumnDataAllocator::ColumnDataAllocator(Allocator &allocator) : type(ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR) {\n+\talloc.allocator = &allocator;\n+}\n+\n+ColumnDataAllocator::ColumnDataAllocator(BufferManager &buffer_manager)\n+    : type(ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR) {\n+\talloc.buffer_manager = &buffer_manager;\n+}\n+\n+ColumnDataAllocator::ColumnDataAllocator(ClientContext &context, ColumnDataAllocatorType allocator_type)\n+    : type(allocator_type) {\n+\tswitch (type) {\n+\tcase ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR:\n+\t\talloc.buffer_manager = &BufferManager::GetBufferManager(context);\n+\t\tbreak;\n+\tcase ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR:\n+\t\talloc.allocator = &Allocator::Get(context);\n+\t\tbreak;\n+\tdefault:\n+\t\tthrow InternalException(\"Unrecognized column data allocator type\");\n+\t}\n+}\n+\n+BufferHandle ColumnDataAllocator::Pin(uint32_t block_id) {\n+\tD_ASSERT(type == ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR);\n+\treturn alloc.buffer_manager->Pin(blocks[block_id].handle);\n+}\n+\n+void ColumnDataAllocator::AllocateBlock() {\n+\tD_ASSERT(type == ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR);\n+\tBlockMetaData data;\n+\tdata.size = 0;\n+\tdata.capacity = Storage::BLOCK_ALLOC_SIZE;\n+\tdata.handle = alloc.buffer_manager->RegisterMemory(Storage::BLOCK_ALLOC_SIZE, false);\n+\tblocks.push_back(move(data));\n+}\n+\n+void ColumnDataAllocator::AllocateData(idx_t size, uint32_t &block_id, uint32_t &offset,\n+                                       ChunkManagementState *chunk_state) {\n+\tif (type == ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR) {\n+\t\t// in-memory allocator\n+\t\tauto allocated = alloc.allocator->Allocate(size);\n+\t\tauto pointer_value = uintptr_t(allocated.get());\n+\t\tif (sizeof(uintptr_t) == sizeof(uint32_t)) {\n+\t\t\tblock_id = uint32_t(pointer_value);\n+\t\t} else if (sizeof(uintptr_t) == sizeof(uint64_t)) {\n+\t\t\tblock_id = uint32_t(pointer_value & 0xFFFFFFFF);\n+\t\t\toffset = uint32_t(pointer_value >> 32);\n+\t\t} else {\n+\t\t\tthrow InternalException(\"ColumnDataCollection: Architecture not supported!?\");\n+\t\t}\n+\t\tallocated_data.push_back(move(allocated));\n+\t\treturn;\n+\t}\n+\tif (blocks.empty() || blocks.back().Capacity() < size) {\n+\t\tAllocateBlock();\n+\t\tif (chunk_state && !blocks.empty()) {\n+\t\t\tauto &last_block = blocks.back();\n+\t\t\tauto new_block_id = blocks.size() - 1;\n+\t\t\tauto pinned_block = alloc.buffer_manager->Pin(last_block.handle);\n+\t\t\tchunk_state->handles[new_block_id] = move(pinned_block);\n+\t\t}\n+\t}\n+\tauto &block = blocks.back();\n+\tD_ASSERT(size <= block.capacity - block.size);\n+\tblock_id = blocks.size() - 1;\n+\toffset = block.size;\n+\tblock.size += size;\n+}\n+\n+void ColumnDataAllocator::Initialize(ColumnDataAllocator &other) {\n+\tD_ASSERT(other.HasBlocks());\n+\tblocks.push_back(other.blocks.back());\n+}\n+\n+data_ptr_t ColumnDataAllocator::GetDataPointer(ChunkManagementState &state, uint32_t block_id, uint32_t offset) {\n+\tif (type == ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR) {\n+\t\t// in-memory allocator: construct pointer from block_id and offset\n+\t\tif (sizeof(uintptr_t) == sizeof(uint32_t)) {\n+\t\t\tuintptr_t pointer_value = uintptr_t(block_id);\n+\t\t\treturn (data_ptr_t)pointer_value;\n+\t\t} else if (sizeof(uintptr_t) == sizeof(uint64_t)) {\n+\t\t\tuintptr_t pointer_value = (uintptr_t(offset) << 32) | uintptr_t(block_id);\n+\t\t\treturn (data_ptr_t)pointer_value;\n+\t\t} else {\n+\t\t\tthrow InternalException(\"ColumnDataCollection: Architecture not supported!?\");\n+\t\t}\n+\t}\n+\tD_ASSERT(state.handles.find(block_id) != state.handles.end());\n+\treturn state.handles[block_id].Ptr() + offset;\n+}\n+\n+Allocator &ColumnDataAllocator::GetAllocator() {\n+\treturn type == ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR ? *alloc.allocator\n+\t                                                            : alloc.buffer_manager->GetBufferAllocator();\n+}\n+\n+void ColumnDataAllocator::InitializeChunkState(ChunkManagementState &state, ChunkMetaData &chunk) {\n+\tif (type != ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR) {\n+\t\t// nothing to pin\n+\t\treturn;\n+\t}\n+\t// release any handles that are no longer required\n+\tbool found_handle;\n+\tdo {\n+\t\tfound_handle = false;\n+\t\tfor (auto it = state.handles.begin(); it != state.handles.end(); it++) {\n+\t\t\tif (chunk.block_ids.find(it->first) != chunk.block_ids.end()) {\n+\t\t\t\t// still required: do not release\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tstate.handles.erase(it);\n+\t\t\tfound_handle = true;\n+\t\t\tbreak;\n+\t\t}\n+\t} while (found_handle);\n+\n+\t// grab any handles that are now required\n+\tfor (auto &block_id : chunk.block_ids) {\n+\t\tif (state.handles.find(block_id) != state.handles.end()) {\n+\t\t\t// already pinned: don't need to do anything\n+\t\t\tcontinue;\n+\t\t}\n+\t\tstate.handles[block_id] = Pin(block_id);\n+\t}\n+}\n+\n+uint32_t BlockMetaData::Capacity() {\n+\tD_ASSERT(size <= capacity);\n+\treturn capacity - size;\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/common/types/column_data_collection.cpp b/src/common/types/column_data_collection.cpp\nnew file mode 100644\nindex 000000000000..184e2f1b9448\n--- /dev/null\n+++ b/src/common/types/column_data_collection.cpp\n@@ -0,0 +1,787 @@\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n+#include \"duckdb/storage/buffer_manager.hpp\"\n+#include \"duckdb/common/printer.hpp\"\n+#include \"duckdb/common/vector_operations/vector_operations.hpp\"\n+#include \"duckdb/common/types/column_data_collection_segment.hpp\"\n+#include \"duckdb/common/string_util.hpp\"\n+\n+namespace duckdb {\n+\n+struct ColumnDataMetaData;\n+\n+typedef void (*column_data_copy_function_t)(ColumnDataMetaData &meta_data, const UnifiedVectorFormat &source_data,\n+                                            Vector &source, idx_t offset, idx_t copy_count);\n+\n+struct ColumnDataCopyFunction {\n+\tcolumn_data_copy_function_t function;\n+\tvector<ColumnDataCopyFunction> child_functions;\n+};\n+\n+struct ColumnDataMetaData {\n+\tColumnDataMetaData(ColumnDataCopyFunction &copy_function, ColumnDataCollectionSegment &segment,\n+\t                   ColumnDataAppendState &state, ChunkMetaData &chunk_data, VectorDataIndex vector_data_index)\n+\t    : copy_function(copy_function), segment(segment), state(state), chunk_data(chunk_data),\n+\t      vector_data_index(vector_data_index) {\n+\t}\n+\tColumnDataMetaData(ColumnDataCopyFunction &copy_function, ColumnDataMetaData &parent,\n+\t                   VectorDataIndex vector_data_index)\n+\t    : copy_function(copy_function), segment(parent.segment), state(parent.state), chunk_data(parent.chunk_data),\n+\t      vector_data_index(vector_data_index) {\n+\t}\n+\n+\tColumnDataCopyFunction &copy_function;\n+\tColumnDataCollectionSegment &segment;\n+\tColumnDataAppendState &state;\n+\tChunkMetaData &chunk_data;\n+\tVectorDataIndex vector_data_index;\n+\tidx_t child_list_size = DConstants::INVALID_INDEX;\n+\n+\tVectorMetaData &GetVectorMetaData() {\n+\t\treturn segment.GetVectorData(vector_data_index);\n+\t}\n+};\n+\n+ColumnDataCollection::ColumnDataCollection(Allocator &allocator_p, vector<LogicalType> types_p) {\n+\tInitialize(move(types_p));\n+\tallocator = make_shared<ColumnDataAllocator>(allocator_p);\n+}\n+\n+ColumnDataCollection::ColumnDataCollection(BufferManager &buffer_manager, vector<LogicalType> types_p) {\n+\tInitialize(move(types_p));\n+\tallocator = make_shared<ColumnDataAllocator>(buffer_manager);\n+}\n+\n+ColumnDataCollection::ColumnDataCollection(shared_ptr<ColumnDataAllocator> allocator_p, vector<LogicalType> types_p) {\n+\tInitialize(move(types_p));\n+\tthis->allocator = move(allocator_p);\n+}\n+\n+ColumnDataCollection::ColumnDataCollection(ClientContext &context, vector<LogicalType> types_p,\n+                                           ColumnDataAllocatorType type)\n+    : ColumnDataCollection(make_shared<ColumnDataAllocator>(context, type), move(types_p)) {\n+\tD_ASSERT(!types.empty());\n+}\n+\n+ColumnDataCollection::ColumnDataCollection(ColumnDataCollection &other)\n+    : ColumnDataCollection(other.allocator, other.types) {\n+\tother.finished_append = true;\n+\tD_ASSERT(!types.empty());\n+}\n+\n+ColumnDataCollection::~ColumnDataCollection() {\n+}\n+\n+void ColumnDataCollection::Initialize(vector<LogicalType> types_p) {\n+\tthis->types = move(types_p);\n+\tthis->count = 0;\n+\tthis->finished_append = false;\n+\tD_ASSERT(!types.empty());\n+\tcopy_functions.reserve(types.size());\n+\tfor (auto &type : types) {\n+\t\tcopy_functions.push_back(GetCopyFunction(type));\n+\t}\n+}\n+\n+void ColumnDataCollection::CreateSegment() {\n+\tsegments.emplace_back(make_unique<ColumnDataCollectionSegment>(allocator, types));\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// ColumnDataRow\n+//===--------------------------------------------------------------------===//\n+ColumnDataRow::ColumnDataRow(DataChunk &chunk_p, idx_t row_index, idx_t base_index)\n+    : chunk(chunk_p), row_index(row_index), base_index(base_index) {\n+}\n+\n+Value ColumnDataRow::GetValue(idx_t column_index) const {\n+\tD_ASSERT(column_index < chunk.ColumnCount());\n+\tD_ASSERT(row_index < chunk.size());\n+\treturn chunk.data[column_index].GetValue(row_index);\n+}\n+\n+idx_t ColumnDataRow::RowIndex() const {\n+\treturn base_index + row_index;\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// ColumnDataRowCollection\n+//===--------------------------------------------------------------------===//\n+ColumnDataRowCollection::ColumnDataRowCollection(const ColumnDataCollection &collection) {\n+\tif (collection.Count() == 0) {\n+\t\treturn;\n+\t}\n+\t// read all the chunks\n+\tColumnDataScanState scan_state;\n+\tcollection.InitializeScan(scan_state);\n+\twhile (true) {\n+\t\tauto chunk = make_unique<DataChunk>();\n+\t\tcollection.InitializeScanChunk(*chunk);\n+\t\tif (!collection.Scan(scan_state, *chunk)) {\n+\t\t\tbreak;\n+\t\t}\n+\t\tchunks.push_back(move(chunk));\n+\t}\n+\t// now create all of the column data rows\n+\trows.reserve(collection.Count());\n+\tidx_t base_row = 0;\n+\tfor (auto &chunk : chunks) {\n+\t\tfor (idx_t row_idx = 0; row_idx < chunk->size(); row_idx++) {\n+\t\t\trows.emplace_back(*chunk, row_idx, base_row);\n+\t\t}\n+\t\tbase_row += chunk->size();\n+\t}\n+}\n+\n+ColumnDataRow &ColumnDataRowCollection::operator[](idx_t i) {\n+\treturn rows[i];\n+}\n+\n+const ColumnDataRow &ColumnDataRowCollection::operator[](idx_t i) const {\n+\treturn rows[i];\n+}\n+\n+Value ColumnDataRowCollection::GetValue(idx_t column, idx_t index) const {\n+\treturn rows[index].GetValue(column);\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// ColumnDataChunkIterator\n+//===--------------------------------------------------------------------===//\n+ColumnDataChunkIterationHelper ColumnDataCollection::Chunks() const {\n+\tvector<column_t> column_ids;\n+\tfor (idx_t i = 0; i < ColumnCount(); i++) {\n+\t\tcolumn_ids.push_back(i);\n+\t}\n+\treturn Chunks(column_ids);\n+}\n+\n+ColumnDataChunkIterationHelper ColumnDataCollection::Chunks(vector<column_t> column_ids) const {\n+\treturn ColumnDataChunkIterationHelper(*this, move(column_ids));\n+}\n+\n+ColumnDataChunkIterationHelper::ColumnDataChunkIterationHelper(const ColumnDataCollection &collection_p,\n+                                                               vector<column_t> column_ids_p)\n+    : collection(collection_p), column_ids(move(column_ids_p)) {\n+}\n+\n+ColumnDataChunkIterationHelper::ColumnDataChunkIterator::ColumnDataChunkIterator(\n+    const ColumnDataCollection *collection_p, vector<column_t> column_ids_p)\n+    : collection(collection_p), scan_chunk(make_shared<DataChunk>()), row_index(0) {\n+\tif (!collection) {\n+\t\treturn;\n+\t}\n+\tcollection->InitializeScan(scan_state, move(column_ids_p));\n+\tcollection->InitializeScanChunk(scan_state, *scan_chunk);\n+\tcollection->Scan(scan_state, *scan_chunk);\n+}\n+\n+void ColumnDataChunkIterationHelper::ColumnDataChunkIterator::Next() {\n+\tif (!collection) {\n+\t\treturn;\n+\t}\n+\tif (!collection->Scan(scan_state, *scan_chunk)) {\n+\t\tcollection = nullptr;\n+\t\trow_index = 0;\n+\t} else {\n+\t\trow_index += scan_chunk->size();\n+\t}\n+}\n+\n+ColumnDataChunkIterationHelper::ColumnDataChunkIterator &\n+ColumnDataChunkIterationHelper::ColumnDataChunkIterator::operator++() {\n+\tNext();\n+\treturn *this;\n+}\n+\n+bool ColumnDataChunkIterationHelper::ColumnDataChunkIterator::operator!=(const ColumnDataChunkIterator &other) const {\n+\treturn collection != other.collection || row_index != other.row_index;\n+}\n+\n+DataChunk &ColumnDataChunkIterationHelper::ColumnDataChunkIterator::operator*() const {\n+\treturn *scan_chunk;\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// ColumnDataRowIterator\n+//===--------------------------------------------------------------------===//\n+ColumnDataRowIterationHelper ColumnDataCollection::Rows() const {\n+\treturn ColumnDataRowIterationHelper(*this);\n+}\n+\n+ColumnDataRowIterationHelper::ColumnDataRowIterationHelper(const ColumnDataCollection &collection_p)\n+    : collection(collection_p) {\n+}\n+\n+ColumnDataRowIterationHelper::ColumnDataRowIterator::ColumnDataRowIterator(const ColumnDataCollection *collection_p)\n+    : collection(collection_p), scan_chunk(make_shared<DataChunk>()), current_row(*scan_chunk, 0, 0) {\n+\tif (!collection) {\n+\t\treturn;\n+\t}\n+\tcollection->InitializeScan(scan_state);\n+\tcollection->InitializeScanChunk(*scan_chunk);\n+\tcollection->Scan(scan_state, *scan_chunk);\n+}\n+\n+void ColumnDataRowIterationHelper::ColumnDataRowIterator::Next() {\n+\tif (!collection) {\n+\t\treturn;\n+\t}\n+\tcurrent_row.row_index++;\n+\tif (current_row.row_index >= scan_chunk->size()) {\n+\t\tcurrent_row.base_index += scan_chunk->size();\n+\t\tcurrent_row.row_index = 0;\n+\t\tif (!collection->Scan(scan_state, *scan_chunk)) {\n+\t\t\t// exhausted collection: move iterator to nop state\n+\t\t\tcurrent_row.base_index = 0;\n+\t\t\tcollection = nullptr;\n+\t\t}\n+\t}\n+}\n+\n+ColumnDataRowIterationHelper::ColumnDataRowIterator &ColumnDataRowIterationHelper::ColumnDataRowIterator::operator++() {\n+\tNext();\n+\treturn *this;\n+}\n+\n+bool ColumnDataRowIterationHelper::ColumnDataRowIterator::operator!=(const ColumnDataRowIterator &other) const {\n+\treturn collection != other.collection || current_row.row_index != other.current_row.row_index ||\n+\t       current_row.base_index != other.current_row.base_index;\n+}\n+\n+const ColumnDataRow &ColumnDataRowIterationHelper::ColumnDataRowIterator::operator*() const {\n+\treturn current_row;\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// Append\n+//===--------------------------------------------------------------------===//\n+void ColumnDataCollection::InitializeAppend(ColumnDataAppendState &state) {\n+\tD_ASSERT(!finished_append);\n+\tstate.vector_data.resize(types.size());\n+\tif (segments.empty()) {\n+\t\tCreateSegment();\n+\t}\n+\tauto &segment = *segments.back();\n+\tif (segment.chunk_data.empty()) {\n+\t\tsegment.AllocateNewChunk();\n+\t}\n+\tsegment.InitializeChunkState(segment.chunk_data.size() - 1, state.current_chunk_state);\n+}\n+\n+void ColumnDataCopyValidity(const UnifiedVectorFormat &source_data, validity_t *target, idx_t source_offset,\n+                            idx_t target_offset, idx_t copy_count) {\n+\tValidityMask validity(target);\n+\tif (target_offset == 0) {\n+\t\t// first time appending to this vector\n+\t\t// all data here is still uninitialized\n+\t\t// initialize the validity mask to set all to valid\n+\t\tvalidity.SetAllValid(STANDARD_VECTOR_SIZE);\n+\t}\n+\t// FIXME: we can do something more optimized here using bitshifts & bitwise ors\n+\tif (!source_data.validity.AllValid()) {\n+\t\tfor (idx_t i = 0; i < copy_count; i++) {\n+\t\t\tauto idx = source_data.sel->get_index(source_offset + i);\n+\t\t\tif (!source_data.validity.RowIsValid(idx)) {\n+\t\t\t\tvalidity.SetInvalid(target_offset + i);\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+template <class T>\n+struct BaseValueCopy {\n+\tstatic idx_t TypeSize() {\n+\t\treturn sizeof(T);\n+\t}\n+\n+\ttemplate <class OP>\n+\tstatic void Assign(ColumnDataMetaData &meta_data, data_ptr_t target, data_ptr_t source, idx_t target_idx,\n+\t                   idx_t source_idx) {\n+\t\tauto result_data = (T *)target;\n+\t\tauto source_data = (T *)source;\n+\t\tresult_data[target_idx] = OP::Operation(meta_data, source_data[source_idx]);\n+\t}\n+};\n+\n+template <class T>\n+struct StandardValueCopy : public BaseValueCopy<T> {\n+\tstatic T Operation(ColumnDataMetaData &, T input) {\n+\t\treturn input;\n+\t}\n+};\n+\n+struct StringValueCopy : public BaseValueCopy<string_t> {\n+\tstatic string_t Operation(ColumnDataMetaData &meta_data, string_t input) {\n+\t\treturn input.IsInlined() ? input : meta_data.segment.heap.AddBlob(input);\n+\t}\n+};\n+\n+struct ListValueCopy : public BaseValueCopy<list_entry_t> {\n+\tusing TYPE = list_entry_t;\n+\n+\tstatic TYPE Operation(ColumnDataMetaData &meta_data, TYPE input) {\n+\t\tinput.offset += meta_data.child_list_size;\n+\t\treturn input;\n+\t}\n+};\n+\n+struct StructValueCopy {\n+\tstatic idx_t TypeSize() {\n+\t\treturn 0;\n+\t}\n+\n+\ttemplate <class OP>\n+\tstatic void Assign(ColumnDataMetaData &meta_data, data_ptr_t target, data_ptr_t source, idx_t target_idx,\n+\t                   idx_t source_idx) {\n+\t}\n+};\n+\n+template <class OP>\n+static void TemplatedColumnDataCopy(ColumnDataMetaData &meta_data, const UnifiedVectorFormat &source_data,\n+                                    Vector &source, idx_t offset, idx_t count) {\n+\tauto &segment = meta_data.segment;\n+\tauto &append_state = meta_data.state;\n+\n+\tauto current_index = meta_data.vector_data_index;\n+\tidx_t remaining = count;\n+\twhile (remaining > 0) {\n+\t\tauto &current_segment = segment.GetVectorData(current_index);\n+\t\tidx_t append_count = MinValue<idx_t>(STANDARD_VECTOR_SIZE - current_segment.count, remaining);\n+\n+\t\tauto base_ptr = meta_data.segment.allocator->GetDataPointer(append_state.current_chunk_state,\n+\t\t                                                            current_segment.block_id, current_segment.offset);\n+\t\tauto validity_data = ColumnDataCollectionSegment::GetValidityPointer(base_ptr, OP::TypeSize());\n+\n+\t\tValidityMask result_validity(validity_data);\n+\t\tif (current_segment.count == 0) {\n+\t\t\t// first time appending to this vector\n+\t\t\t// all data here is still uninitialized\n+\t\t\t// initialize the validity mask to set all to valid\n+\t\t\tresult_validity.SetAllValid(STANDARD_VECTOR_SIZE);\n+\t\t}\n+\t\tfor (idx_t i = 0; i < append_count; i++) {\n+\t\t\tauto source_idx = source_data.sel->get_index(offset + i);\n+\t\t\tif (source_data.validity.RowIsValid(source_idx)) {\n+\t\t\t\tOP::template Assign<OP>(meta_data, base_ptr, source_data.data, current_segment.count + i, source_idx);\n+\t\t\t} else {\n+\t\t\t\tresult_validity.SetInvalid(current_segment.count + i);\n+\t\t\t}\n+\t\t}\n+\t\tcurrent_segment.count += append_count;\n+\t\toffset += append_count;\n+\t\tremaining -= append_count;\n+\t\tif (remaining > 0) {\n+\t\t\t// need to append more, check if we need to allocate a new vector or not\n+\t\t\tif (!current_segment.next_data.IsValid()) {\n+\t\t\t\tsegment.AllocateVector(source.GetType(), meta_data.chunk_data, meta_data.state, current_index);\n+\t\t\t}\n+\t\t\tD_ASSERT(segment.GetVectorData(current_index).next_data.IsValid());\n+\t\t\tcurrent_index = segment.GetVectorData(current_index).next_data;\n+\t\t}\n+\t}\n+}\n+\n+template <class T>\n+static void ColumnDataCopy(ColumnDataMetaData &meta_data, const UnifiedVectorFormat &source_data, Vector &source,\n+                           idx_t offset, idx_t copy_count) {\n+\tTemplatedColumnDataCopy<StandardValueCopy<T>>(meta_data, source_data, source, offset, copy_count);\n+}\n+\n+template <>\n+void ColumnDataCopy<string_t>(ColumnDataMetaData &meta_data, const UnifiedVectorFormat &source_data, Vector &source,\n+                              idx_t offset, idx_t copy_count) {\n+\tTemplatedColumnDataCopy<StringValueCopy>(meta_data, source_data, source, offset, copy_count);\n+}\n+\n+template <>\n+void ColumnDataCopy<list_entry_t>(ColumnDataMetaData &meta_data, const UnifiedVectorFormat &source_data, Vector &source,\n+                                  idx_t offset, idx_t copy_count) {\n+\tauto &segment = meta_data.segment;\n+\n+\t// first append the child entries of the list\n+\tauto &child_vector = ListVector::GetEntry(source);\n+\tidx_t child_list_size = ListVector::GetListSize(source);\n+\tauto &child_type = child_vector.GetType();\n+\n+\tUnifiedVectorFormat child_vector_data;\n+\tchild_vector.ToUnifiedFormat(child_list_size, child_vector_data);\n+\n+\tif (!meta_data.GetVectorMetaData().child_index.IsValid()) {\n+\t\tauto child_index = segment.AllocateVector(child_type, meta_data.chunk_data, meta_data.state);\n+\t\tmeta_data.GetVectorMetaData().child_index = meta_data.segment.AddChildIndex(child_index);\n+\t}\n+\tauto &child_function = meta_data.copy_function.child_functions[0];\n+\tauto child_index = segment.GetChildIndex(meta_data.GetVectorMetaData().child_index);\n+\t// figure out the current list size by traversing the set of child entries\n+\tidx_t current_list_size = 0;\n+\tauto current_child_index = child_index;\n+\twhile (current_child_index.IsValid()) {\n+\t\tauto &child_vdata = segment.GetVectorData(current_child_index);\n+\t\tcurrent_list_size += child_vdata.count;\n+\t\tcurrent_child_index = child_vdata.next_data;\n+\t}\n+\tColumnDataMetaData child_meta_data(child_function, meta_data, child_index);\n+\t// FIXME: appending the entire child list here is not required\n+\t// We can also scan the actual list entries required per the offset/copy_count\n+\tchild_function.function(child_meta_data, child_vector_data, child_vector, 0, child_list_size);\n+\n+\t// now copy the list entries\n+\tmeta_data.child_list_size = current_list_size;\n+\tTemplatedColumnDataCopy<ListValueCopy>(meta_data, source_data, source, offset, copy_count);\n+}\n+\n+void ColumnDataCopyStruct(ColumnDataMetaData &meta_data, const UnifiedVectorFormat &source_data, Vector &source,\n+                          idx_t offset, idx_t copy_count) {\n+\tauto &segment = meta_data.segment;\n+\n+\t// copy the NULL values for the main struct vector\n+\tTemplatedColumnDataCopy<StructValueCopy>(meta_data, source_data, source, offset, copy_count);\n+\n+\tauto &child_types = StructType::GetChildTypes(source.GetType());\n+\t// now copy all the child vectors\n+\tD_ASSERT(meta_data.GetVectorMetaData().child_index.IsValid());\n+\tauto &child_vectors = StructVector::GetEntries(source);\n+\tfor (idx_t child_idx = 0; child_idx < child_types.size(); child_idx++) {\n+\t\tauto &child_function = meta_data.copy_function.child_functions[child_idx];\n+\t\tauto child_index = segment.GetChildIndex(meta_data.GetVectorMetaData().child_index, child_idx);\n+\t\tColumnDataMetaData child_meta_data(child_function, meta_data, child_index);\n+\n+\t\tUnifiedVectorFormat child_data;\n+\t\tchild_vectors[child_idx]->ToUnifiedFormat(copy_count, child_data);\n+\n+\t\tchild_function.function(child_meta_data, child_data, *child_vectors[child_idx], offset, copy_count);\n+\t}\n+}\n+\n+ColumnDataCopyFunction ColumnDataCollection::GetCopyFunction(const LogicalType &type) {\n+\tColumnDataCopyFunction result;\n+\tcolumn_data_copy_function_t function;\n+\tswitch (type.InternalType()) {\n+\tcase PhysicalType::BOOL:\n+\t\tfunction = ColumnDataCopy<bool>;\n+\t\tbreak;\n+\tcase PhysicalType::INT8:\n+\t\tfunction = ColumnDataCopy<int8_t>;\n+\t\tbreak;\n+\tcase PhysicalType::INT16:\n+\t\tfunction = ColumnDataCopy<int16_t>;\n+\t\tbreak;\n+\tcase PhysicalType::INT32:\n+\t\tfunction = ColumnDataCopy<int32_t>;\n+\t\tbreak;\n+\tcase PhysicalType::INT64:\n+\t\tfunction = ColumnDataCopy<int64_t>;\n+\t\tbreak;\n+\tcase PhysicalType::INT128:\n+\t\tfunction = ColumnDataCopy<hugeint_t>;\n+\t\tbreak;\n+\tcase PhysicalType::UINT8:\n+\t\tfunction = ColumnDataCopy<uint8_t>;\n+\t\tbreak;\n+\tcase PhysicalType::UINT16:\n+\t\tfunction = ColumnDataCopy<uint16_t>;\n+\t\tbreak;\n+\tcase PhysicalType::UINT32:\n+\t\tfunction = ColumnDataCopy<uint32_t>;\n+\t\tbreak;\n+\tcase PhysicalType::UINT64:\n+\t\tfunction = ColumnDataCopy<uint64_t>;\n+\t\tbreak;\n+\tcase PhysicalType::FLOAT:\n+\t\tfunction = ColumnDataCopy<float>;\n+\t\tbreak;\n+\tcase PhysicalType::DOUBLE:\n+\t\tfunction = ColumnDataCopy<double>;\n+\t\tbreak;\n+\tcase PhysicalType::INTERVAL:\n+\t\tfunction = ColumnDataCopy<interval_t>;\n+\t\tbreak;\n+\tcase PhysicalType::VARCHAR:\n+\t\tfunction = ColumnDataCopy<string_t>;\n+\t\tbreak;\n+\tcase PhysicalType::STRUCT: {\n+\t\tfunction = ColumnDataCopyStruct;\n+\t\tauto &child_types = StructType::GetChildTypes(type);\n+\t\tfor (auto &kv : child_types) {\n+\t\t\tresult.child_functions.push_back(GetCopyFunction(kv.second));\n+\t\t}\n+\t\tbreak;\n+\t}\n+\tcase PhysicalType::LIST: {\n+\t\tfunction = ColumnDataCopy<list_entry_t>;\n+\t\tauto child_function = GetCopyFunction(ListType::GetChildType(type));\n+\t\tresult.child_functions.push_back(child_function);\n+\t\tbreak;\n+\t}\n+\tdefault:\n+\t\tthrow InternalException(\"Unsupported type for ColumnDataCollection::GetCopyFunction\");\n+\t}\n+\tresult.function = function;\n+\treturn result;\n+}\n+\n+static bool IsComplexType(const LogicalType &type) {\n+\tswitch (type.InternalType()) {\n+\tcase PhysicalType::STRUCT:\n+\tcase PhysicalType::LIST:\n+\t\treturn true;\n+\tdefault:\n+\t\treturn false;\n+\t};\n+}\n+\n+void ColumnDataCollection::Append(ColumnDataAppendState &state, DataChunk &input) {\n+\tD_ASSERT(!finished_append);\n+\tD_ASSERT(types == input.GetTypes());\n+\n+\tauto &segment = *segments.back();\n+\tfor (idx_t vector_idx = 0; vector_idx < types.size(); vector_idx++) {\n+\t\tif (IsComplexType(input.data[vector_idx].GetType())) {\n+\t\t\tinput.data[vector_idx].Flatten(input.size());\n+\t\t}\n+\t\tinput.data[vector_idx].ToUnifiedFormat(input.size(), state.vector_data[vector_idx]);\n+\t}\n+\n+\tidx_t remaining = input.size();\n+\twhile (remaining > 0) {\n+\t\tauto &chunk_data = segment.chunk_data.back();\n+\t\tidx_t append_amount = MinValue<idx_t>(remaining, STANDARD_VECTOR_SIZE - chunk_data.count);\n+\t\tif (append_amount > 0) {\n+\t\t\tidx_t offset = input.size() - remaining;\n+\t\t\tfor (idx_t vector_idx = 0; vector_idx < types.size(); vector_idx++) {\n+\t\t\t\tColumnDataMetaData meta_data(copy_functions[vector_idx], segment, state, chunk_data,\n+\t\t\t\t                             chunk_data.vector_data[vector_idx]);\n+\t\t\t\tcopy_functions[vector_idx].function(meta_data, state.vector_data[vector_idx], input.data[vector_idx],\n+\t\t\t\t                                    offset, append_amount);\n+\t\t\t}\n+\t\t\tchunk_data.count += append_amount;\n+\t\t}\n+\t\tremaining -= append_amount;\n+\t\tif (remaining > 0) {\n+\t\t\t// more to do\n+\t\t\t// allocate a new chunk\n+\t\t\tsegment.AllocateNewChunk();\n+\t\t\tsegment.InitializeChunkState(segment.chunk_data.size() - 1, state.current_chunk_state);\n+\t\t}\n+\t}\n+\tsegment.count += input.size();\n+\tcount += input.size();\n+}\n+\n+void ColumnDataCollection::Append(DataChunk &input) {\n+\tColumnDataAppendState state;\n+\tInitializeAppend(state);\n+\tAppend(state, input);\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// Scan\n+//===--------------------------------------------------------------------===//\n+void ColumnDataCollection::InitializeScan(ColumnDataScanState &state, ColumnDataScanProperties properties) const {\n+\tvector<column_t> column_ids;\n+\tcolumn_ids.reserve(types.size());\n+\tfor (idx_t i = 0; i < types.size(); i++) {\n+\t\tcolumn_ids.push_back(i);\n+\t}\n+\tInitializeScan(state, move(column_ids), properties);\n+}\n+\n+void ColumnDataCollection::InitializeScan(ColumnDataScanState &state, vector<column_t> column_ids,\n+                                          ColumnDataScanProperties properties) const {\n+\tstate.chunk_index = 0;\n+\tstate.segment_index = 0;\n+\tstate.current_row_index = 0;\n+\tstate.next_row_index = 0;\n+\tstate.current_chunk_state.handles.clear();\n+\tstate.properties = properties;\n+\tstate.column_ids = move(column_ids);\n+}\n+\n+void ColumnDataCollection::InitializeScan(ColumnDataParallelScanState &state) const {\n+\tInitializeScan(state.scan_state);\n+}\n+\n+void ColumnDataCollection::InitializeScan(ColumnDataParallelScanState &state, vector<column_t> column_ids) const {\n+\tInitializeScan(state.scan_state, move(column_ids));\n+}\n+\n+bool ColumnDataCollection::Scan(ColumnDataParallelScanState &state, ColumnDataLocalScanState &lstate,\n+                                DataChunk &result) const {\n+\tresult.Reset();\n+\n+\tidx_t chunk_index;\n+\tidx_t segment_index;\n+\tidx_t row_index;\n+\t{\n+\t\tlock_guard<mutex> l(state.lock);\n+\t\tif (!NextScanIndex(state.scan_state, chunk_index, segment_index, row_index)) {\n+\t\t\treturn false;\n+\t\t}\n+\t}\n+\tauto &segment = *segments[segment_index];\n+\tlstate.current_chunk_state.properties = state.scan_state.properties;\n+\tsegment.ReadChunk(chunk_index, lstate.current_chunk_state, result, state.scan_state.column_ids);\n+\tlstate.current_row_index = row_index;\n+\tresult.Verify();\n+\treturn true;\n+}\n+\n+void ColumnDataCollection::InitializeScanChunk(DataChunk &chunk) const {\n+\tchunk.Initialize(allocator->GetAllocator(), types);\n+}\n+\n+void ColumnDataCollection::InitializeScanChunk(ColumnDataScanState &state, DataChunk &chunk) const {\n+\tD_ASSERT(!state.column_ids.empty());\n+\tvector<LogicalType> chunk_types;\n+\tchunk_types.reserve(state.column_ids.size());\n+\tfor (idx_t i = 0; i < state.column_ids.size(); i++) {\n+\t\tauto column_idx = state.column_ids[i];\n+\t\tD_ASSERT(column_idx < types.size());\n+\t\tchunk_types.push_back(types[column_idx]);\n+\t}\n+\tchunk.Initialize(allocator->GetAllocator(), chunk_types);\n+}\n+\n+bool ColumnDataCollection::NextScanIndex(ColumnDataScanState &state, idx_t &chunk_index, idx_t &segment_index,\n+                                         idx_t &row_index) const {\n+\trow_index = state.current_row_index = state.next_row_index;\n+\t// check if we still have collections to scan\n+\tif (state.segment_index >= segments.size()) {\n+\t\t// no more data left in the scan\n+\t\treturn false;\n+\t}\n+\t// check within the current collection if we still have chunks to scan\n+\twhile (state.chunk_index >= segments[state.segment_index]->chunk_data.size()) {\n+\t\t// exhausted all chunks for this internal data structure: move to the next one\n+\t\tstate.chunk_index = 0;\n+\t\tstate.segment_index++;\n+\t\tstate.current_chunk_state.handles.clear();\n+\t\tif (state.segment_index >= segments.size()) {\n+\t\t\treturn false;\n+\t\t}\n+\t}\n+\tstate.next_row_index += segments[state.segment_index]->chunk_data[state.chunk_index].count;\n+\tsegment_index = state.segment_index;\n+\tchunk_index = state.chunk_index++;\n+\treturn true;\n+}\n+\n+bool ColumnDataCollection::Scan(ColumnDataScanState &state, DataChunk &result) const {\n+\tresult.Reset();\n+\n+\tidx_t chunk_index;\n+\tidx_t segment_index;\n+\tidx_t row_index;\n+\tif (!NextScanIndex(state, chunk_index, segment_index, row_index)) {\n+\t\treturn false;\n+\t}\n+\n+\t// found a chunk to scan -> scan it\n+\tauto &segment = *segments[segment_index];\n+\tstate.current_chunk_state.properties = state.properties;\n+\tsegment.ReadChunk(chunk_index, state.current_chunk_state, result, state.column_ids);\n+\tresult.Verify();\n+\treturn true;\n+}\n+\n+ColumnDataRowCollection ColumnDataCollection::GetRows() const {\n+\treturn ColumnDataRowCollection(*this);\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// Combine\n+//===--------------------------------------------------------------------===//\n+void ColumnDataCollection::Combine(ColumnDataCollection &other) {\n+\tif (other.count == 0) {\n+\t\treturn;\n+\t}\n+\tif (types != other.types) {\n+\t\tthrow InternalException(\"Attempting to combine ColumnDataCollections with mismatching types\");\n+\t}\n+\tthis->count += other.count;\n+\tthis->segments.reserve(segments.size() + other.segments.size());\n+\tfor (auto &other_seg : other.segments) {\n+\t\tsegments.push_back(move(other_seg));\n+\t}\n+\tVerify();\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// Fetch\n+//===--------------------------------------------------------------------===//\n+idx_t ColumnDataCollection::ChunkCount() const {\n+\tidx_t chunk_count = 0;\n+\tfor (auto &segment : segments) {\n+\t\tchunk_count += segment->ChunkCount();\n+\t}\n+\treturn chunk_count;\n+}\n+\n+void ColumnDataCollection::FetchChunk(idx_t chunk_idx, DataChunk &result) const {\n+\tD_ASSERT(chunk_idx < ChunkCount());\n+\tfor (auto &segment : segments) {\n+\t\tif (chunk_idx >= segment->ChunkCount()) {\n+\t\t\tchunk_idx -= segment->ChunkCount();\n+\t\t} else {\n+\t\t\tsegment->FetchChunk(chunk_idx, result);\n+\t\t\treturn;\n+\t\t}\n+\t}\n+\tthrow InternalException(\"Failed to find chunk in ColumnDataCollection\");\n+}\n+\n+//===--------------------------------------------------------------------===//\n+// Helpers\n+//===--------------------------------------------------------------------===//\n+void ColumnDataCollection::Verify() {\n+#ifdef DEBUG\n+\t// verify counts\n+\tidx_t total_segment_count = 0;\n+\tfor (auto &segment : segments) {\n+\t\tsegment->Verify();\n+\t\ttotal_segment_count += segment->count;\n+\t}\n+\tD_ASSERT(total_segment_count == this->count);\n+#endif\n+}\n+\n+string ColumnDataCollection::ToString() const {\n+\treturn \"Column Data Collection\";\n+}\n+\n+void ColumnDataCollection::Print() const {\n+\tPrinter::Print(ToString());\n+}\n+\n+void ColumnDataCollection::Reset() {\n+\tcount = 0;\n+\tsegments.clear();\n+}\n+\n+bool ColumnDataCollection::ResultEquals(const ColumnDataCollection &left, const ColumnDataCollection &right,\n+                                        string &error_message) {\n+\tif (left.ColumnCount() != right.ColumnCount()) {\n+\t\terror_message = \"Column count mismatch\";\n+\t\treturn false;\n+\t}\n+\tif (left.Count() != right.Count()) {\n+\t\terror_message = \"Row count mismatch\";\n+\t\treturn false;\n+\t}\n+\tauto left_rows = left.GetRows();\n+\tauto right_rows = right.GetRows();\n+\tfor (idx_t r = 0; r < left.Count(); r++) {\n+\t\tfor (idx_t c = 0; c < left.ColumnCount(); c++) {\n+\t\t\tauto lvalue = left_rows.GetValue(c, r);\n+\t\t\tauto rvalue = left_rows.GetValue(c, r);\n+\t\t\tif (!Value::ValuesAreEqual(lvalue, rvalue)) {\n+\t\t\t\terror_message =\n+\t\t\t\t    StringUtil::Format(\"%s <> %s (row: %lld, col: %lld)\\n\", lvalue.ToString(), rvalue.ToString(), r, c);\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn true;\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/common/types/column_data_collection_segment.cpp b/src/common/types/column_data_collection_segment.cpp\nnew file mode 100644\nindex 000000000000..710daf3fd52e\n--- /dev/null\n+++ b/src/common/types/column_data_collection_segment.cpp\n@@ -0,0 +1,232 @@\n+#include \"duckdb/common/types/column_data_collection_segment.hpp\"\n+\n+namespace duckdb {\n+\n+ColumnDataCollectionSegment::ColumnDataCollectionSegment(shared_ptr<ColumnDataAllocator> allocator_p,\n+                                                         vector<LogicalType> types_p)\n+    : allocator(move(allocator_p)), types(move(types_p)), count(0) {\n+}\n+\n+idx_t ColumnDataCollectionSegment::GetDataSize(idx_t type_size) {\n+\treturn AlignValue(type_size * STANDARD_VECTOR_SIZE);\n+}\n+\n+validity_t *ColumnDataCollectionSegment::GetValidityPointer(data_ptr_t base_ptr, idx_t type_size) {\n+\treturn (validity_t *)(base_ptr + GetDataSize(type_size));\n+}\n+\n+VectorDataIndex ColumnDataCollectionSegment::AllocateVectorInternal(const LogicalType &type, ChunkMetaData &chunk_meta,\n+                                                                    ChunkManagementState *chunk_state) {\n+\tVectorMetaData meta_data;\n+\tmeta_data.count = 0;\n+\n+\tauto internal_type = type.InternalType();\n+\tauto type_size = internal_type == PhysicalType::STRUCT ? 0 : GetTypeIdSize(internal_type);\n+\tallocator->AllocateData(GetDataSize(type_size) + ValidityMask::STANDARD_MASK_SIZE, meta_data.block_id,\n+\t                        meta_data.offset, chunk_state);\n+\tif (allocator->GetType() == ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR) {\n+\t\tchunk_meta.block_ids.insert(meta_data.block_id);\n+\t}\n+\n+\tauto index = vector_data.size();\n+\tvector_data.push_back(meta_data);\n+\treturn VectorDataIndex(index);\n+}\n+\n+VectorDataIndex ColumnDataCollectionSegment::AllocateVector(const LogicalType &type, ChunkMetaData &chunk_meta,\n+                                                            ChunkManagementState *chunk_state,\n+                                                            VectorDataIndex prev_index) {\n+\tauto index = AllocateVectorInternal(type, chunk_meta, chunk_state);\n+\tif (prev_index.IsValid()) {\n+\t\tGetVectorData(prev_index).next_data = index;\n+\t}\n+\tif (type.InternalType() == PhysicalType::STRUCT) {\n+\t\t// initialize the struct children\n+\t\tauto &child_types = StructType::GetChildTypes(type);\n+\t\tauto base_child_index = ReserveChildren(child_types.size());\n+\t\tfor (idx_t child_idx = 0; child_idx < child_types.size(); child_idx++) {\n+\t\t\tVectorDataIndex prev_child_index;\n+\t\t\tif (prev_index.IsValid()) {\n+\t\t\t\tprev_child_index = GetChildIndex(GetVectorData(prev_index).child_index, child_idx);\n+\t\t\t}\n+\t\t\tauto child_index = AllocateVector(child_types[child_idx].second, chunk_meta, chunk_state, prev_child_index);\n+\t\t\tSetChildIndex(base_child_index, child_idx, child_index);\n+\t\t}\n+\t\tGetVectorData(index).child_index = base_child_index;\n+\t}\n+\treturn index;\n+}\n+\n+VectorDataIndex ColumnDataCollectionSegment::AllocateVector(const LogicalType &type, ChunkMetaData &chunk_meta,\n+                                                            ColumnDataAppendState &append_state,\n+                                                            VectorDataIndex prev_index) {\n+\treturn AllocateVector(type, chunk_meta, &append_state.current_chunk_state, prev_index);\n+}\n+\n+void ColumnDataCollectionSegment::AllocateNewChunk() {\n+\tChunkMetaData meta_data;\n+\tmeta_data.count = 0;\n+\tmeta_data.vector_data.reserve(types.size());\n+\tfor (idx_t i = 0; i < types.size(); i++) {\n+\t\tauto vector_idx = AllocateVector(types[i], meta_data);\n+\t\tmeta_data.vector_data.push_back(vector_idx);\n+\t}\n+\tchunk_data.push_back(move(meta_data));\n+}\n+\n+void ColumnDataCollectionSegment::InitializeChunkState(idx_t chunk_index, ChunkManagementState &state) {\n+\tauto &chunk = chunk_data[chunk_index];\n+\tallocator->InitializeChunkState(state, chunk);\n+}\n+\n+VectorDataIndex ColumnDataCollectionSegment::GetChildIndex(VectorChildIndex index, idx_t child_entry) {\n+\tD_ASSERT(index.IsValid());\n+\tD_ASSERT(index.index + child_entry < child_indices.size());\n+\treturn VectorDataIndex(child_indices[index.index + child_entry]);\n+}\n+\n+VectorChildIndex ColumnDataCollectionSegment::AddChildIndex(VectorDataIndex index) {\n+\tauto result = child_indices.size();\n+\tchild_indices.push_back(index);\n+\treturn VectorChildIndex(result);\n+}\n+\n+VectorChildIndex ColumnDataCollectionSegment::ReserveChildren(idx_t child_count) {\n+\tauto result = child_indices.size();\n+\tfor (idx_t i = 0; i < child_count; i++) {\n+\t\tchild_indices.emplace_back();\n+\t}\n+\treturn VectorChildIndex(result);\n+}\n+\n+void ColumnDataCollectionSegment::SetChildIndex(VectorChildIndex base_idx, idx_t child_number, VectorDataIndex index) {\n+\tD_ASSERT(base_idx.IsValid());\n+\tD_ASSERT(index.IsValid());\n+\tD_ASSERT(base_idx.index + child_number < child_indices.size());\n+\tchild_indices[base_idx.index + child_number] = index;\n+}\n+\n+idx_t ColumnDataCollectionSegment::ReadVectorInternal(ChunkManagementState &state, VectorDataIndex vector_index,\n+                                                      Vector &result) {\n+\tauto &vector_type = result.GetType();\n+\tauto internal_type = vector_type.InternalType();\n+\tauto type_size = GetTypeIdSize(internal_type);\n+\tauto &vdata = GetVectorData(vector_index);\n+\n+\tauto base_ptr = allocator->GetDataPointer(state, vdata.block_id, vdata.offset);\n+\tauto validity_data = GetValidityPointer(base_ptr, type_size);\n+\tif (!vdata.next_data.IsValid() && state.properties != ColumnDataScanProperties::DISALLOW_ZERO_COPY) {\n+\t\t// no next data, we can do a zero-copy read of this vector\n+\t\tFlatVector::SetData(result, base_ptr);\n+\t\tFlatVector::Validity(result).Initialize(validity_data);\n+\t\treturn vdata.count;\n+\t}\n+\n+\t// the data for this vector is spread over multiple vector data entries\n+\t// we need to copy over the data for each of the vectors\n+\t// first figure out how many rows we need to copy by looping over all of the child vector indexes\n+\tidx_t vector_count = 0;\n+\tauto next_index = vector_index;\n+\twhile (next_index.IsValid()) {\n+\t\tauto &current_vdata = GetVectorData(next_index);\n+\t\tvector_count += current_vdata.count;\n+\t\tnext_index = current_vdata.next_data;\n+\t}\n+\t// resize the result vector\n+\tresult.Resize(0, vector_count);\n+\tnext_index = vector_index;\n+\t// now perform the copy of each of the vectors\n+\tauto target_data = FlatVector::GetData(result);\n+\tauto &target_validity = FlatVector::Validity(result);\n+\tidx_t current_offset = 0;\n+\twhile (next_index.IsValid()) {\n+\t\tauto &current_vdata = GetVectorData(next_index);\n+\t\tbase_ptr = allocator->GetDataPointer(state, current_vdata.block_id, current_vdata.offset);\n+\t\tvalidity_data = GetValidityPointer(base_ptr, type_size);\n+\t\tif (type_size > 0) {\n+\t\t\tmemcpy(target_data + current_offset * type_size, base_ptr, current_vdata.count * type_size);\n+\t\t}\n+\t\t// FIXME: use bitwise operations here\n+\t\tValidityMask current_validity(validity_data);\n+\t\tfor (idx_t k = 0; k < current_vdata.count; k++) {\n+\t\t\ttarget_validity.Set(current_offset + k, current_validity.RowIsValid(k));\n+\t\t}\n+\t\tcurrent_offset += current_vdata.count;\n+\t\tnext_index = current_vdata.next_data;\n+\t}\n+\treturn vector_count;\n+}\n+\n+idx_t ColumnDataCollectionSegment::ReadVector(ChunkManagementState &state, VectorDataIndex vector_index,\n+                                              Vector &result) {\n+\tauto &vector_type = result.GetType();\n+\tauto internal_type = vector_type.InternalType();\n+\tauto &vdata = GetVectorData(vector_index);\n+\tif (vdata.count == 0) {\n+\t\treturn 0;\n+\t}\n+\tauto count = ReadVectorInternal(state, vector_index, result);\n+\tif (internal_type == PhysicalType::LIST) {\n+\t\t// list: copy child\n+\t\tauto &child_vector = ListVector::GetEntry(result);\n+\t\tauto child_count = ReadVector(state, GetChildIndex(vdata.child_index), child_vector);\n+\t\tListVector::SetListSize(result, child_count);\n+\t} else if (internal_type == PhysicalType::STRUCT) {\n+\t\tauto &child_vectors = StructVector::GetEntries(result);\n+\t\tfor (idx_t child_idx = 0; child_idx < child_vectors.size(); child_idx++) {\n+\t\t\tauto child_count =\n+\t\t\t    ReadVector(state, GetChildIndex(vdata.child_index, child_idx), *child_vectors[child_idx]);\n+\t\t\tif (child_count != count) {\n+\t\t\t\tthrow InternalException(\"Column Data Collection: mismatch in struct child sizes\");\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn count;\n+}\n+\n+void ColumnDataCollectionSegment::ReadChunk(idx_t chunk_index, ChunkManagementState &state, DataChunk &chunk,\n+                                            const vector<column_t> &column_ids) {\n+\tD_ASSERT(chunk.ColumnCount() == column_ids.size());\n+\tD_ASSERT(state.properties != ColumnDataScanProperties::INVALID);\n+\tInitializeChunkState(chunk_index, state);\n+\tauto &chunk_meta = chunk_data[chunk_index];\n+\tfor (idx_t i = 0; i < column_ids.size(); i++) {\n+\t\tauto vector_idx = column_ids[i];\n+\t\tD_ASSERT(vector_idx < chunk_meta.vector_data.size());\n+\t\tReadVector(state, chunk_meta.vector_data[vector_idx], chunk.data[i]);\n+\t}\n+\tchunk.SetCardinality(chunk_meta.count);\n+}\n+\n+idx_t ColumnDataCollectionSegment::ChunkCount() const {\n+\treturn chunk_data.size();\n+}\n+\n+void ColumnDataCollectionSegment::FetchChunk(idx_t chunk_idx, DataChunk &result) {\n+\tvector<column_t> column_ids;\n+\tcolumn_ids.reserve(types.size());\n+\tfor (idx_t i = 0; i < types.size(); i++) {\n+\t\tcolumn_ids.push_back(i);\n+\t}\n+\tFetchChunk(chunk_idx, result, column_ids);\n+}\n+\n+void ColumnDataCollectionSegment::FetchChunk(idx_t chunk_idx, DataChunk &result, const vector<column_t> &column_ids) {\n+\tD_ASSERT(chunk_idx < chunk_data.size());\n+\tChunkManagementState state;\n+\tInitializeChunkState(chunk_idx, state);\n+\tstate.properties = ColumnDataScanProperties::DISALLOW_ZERO_COPY;\n+\tReadChunk(chunk_idx, state, result, column_ids);\n+}\n+\n+void ColumnDataCollectionSegment::Verify() {\n+#ifdef DEBUG\n+\tidx_t total_count = 0;\n+\tfor (idx_t i = 0; i < chunk_data.size(); i++) {\n+\t\ttotal_count += chunk_data[i].count;\n+\t}\n+\tD_ASSERT(total_count == this->count);\n+#endif\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/common/types/data_chunk.cpp b/src/common/types/data_chunk.cpp\nindex 66e22ab4ca0c..3d9428491769 100644\n--- a/src/common/types/data_chunk.cpp\n+++ b/src/common/types/data_chunk.cpp\n@@ -1,7 +1,7 @@\n #include \"duckdb/common/types/data_chunk.hpp\"\n \n #include \"duckdb/common/array.hpp\"\n-#include \"duckdb/common/arrow.hpp\"\n+#include \"duckdb/common/arrow/arrow.hpp\"\n #include \"duckdb/common/exception.hpp\"\n #include \"duckdb/common/helper.hpp\"\n #include \"duckdb/common/printer.hpp\"\n@@ -160,10 +160,11 @@ void DataChunk::Append(const DataChunk &other, bool resize, SelectionVector *sel\n \t}\n \tif (new_size > capacity) {\n \t\tif (resize) {\n+\t\t\tauto new_capacity = NextPowerOfTwo(new_size);\n \t\t\tfor (idx_t i = 0; i < ColumnCount(); i++) {\n-\t\t\t\tdata[i].Resize(size(), new_size);\n+\t\t\t\tdata[i].Resize(size(), new_capacity);\n \t\t\t}\n-\t\t\tcapacity = new_size;\n+\t\t\tcapacity = new_capacity;\n \t\t} else {\n \t\t\tthrow InternalException(\"Can't append chunk to other chunk without resizing\");\n \t\t}\n@@ -285,465 +286,4 @@ void DataChunk::Print() {\n \tPrinter::Print(ToString());\n }\n \n-struct DuckDBArrowArrayChildHolder {\n-\tArrowArray array;\n-\t//! need max three pointers for strings\n-\tduckdb::array<const void *, 3> buffers = {{nullptr, nullptr, nullptr}};\n-\tunique_ptr<Vector> vector;\n-\tunique_ptr<data_t[]> offsets;\n-\tunique_ptr<data_t[]> data;\n-\t//! Children of nested structures\n-\t::duckdb::vector<DuckDBArrowArrayChildHolder> children;\n-\t::duckdb::vector<ArrowArray *> children_ptrs;\n-};\n-\n-struct DuckDBArrowArrayHolder {\n-\tvector<DuckDBArrowArrayChildHolder> children = {};\n-\tvector<ArrowArray *> children_ptrs = {};\n-\tarray<const void *, 1> buffers = {{nullptr}};\n-\tvector<shared_ptr<ArrowArrayWrapper>> arrow_original_array;\n-};\n-\n-static void ReleaseDuckDBArrowArray(ArrowArray *array) {\n-\tif (!array || !array->release) {\n-\t\treturn;\n-\t}\n-\tarray->release = nullptr;\n-\tauto holder = static_cast<DuckDBArrowArrayHolder *>(array->private_data);\n-\tdelete holder;\n-}\n-\n-void InitializeChild(DuckDBArrowArrayChildHolder &child_holder, idx_t size) {\n-\tauto &child = child_holder.array;\n-\tchild.private_data = nullptr;\n-\tchild.release = ReleaseDuckDBArrowArray;\n-\tchild.n_children = 0;\n-\tchild.null_count = 0;\n-\tchild.offset = 0;\n-\tchild.dictionary = nullptr;\n-\tchild.buffers = child_holder.buffers.data();\n-\n-\tchild.length = size;\n-}\n-\n-void SetChildValidityMask(Vector &vector, ArrowArray &child) {\n-\tD_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);\n-\tauto &mask = FlatVector::Validity(vector);\n-\tif (!mask.AllValid()) {\n-\t\t//! any bits are set: might have nulls\n-\t\tchild.null_count = -1;\n-\t} else {\n-\t\t//! no bits are set; we know there are no nulls\n-\t\tchild.null_count = 0;\n-\t}\n-\tchild.buffers[0] = (void *)mask.GetData();\n-}\n-\n-void SetArrowChild(DuckDBArrowArrayChildHolder &child_holder, const LogicalType &type, Vector &data, idx_t size);\n-\n-void SetList(DuckDBArrowArrayChildHolder &child_holder, const LogicalType &type, Vector &data, idx_t size) {\n-\tauto &child = child_holder.array;\n-\tchild_holder.vector = make_unique<Vector>(data);\n-\n-\t//! Lists have two buffers\n-\tchild.n_buffers = 2;\n-\t//! Second Buffer is the list offsets\n-\tchild_holder.offsets = unique_ptr<data_t[]>(new data_t[sizeof(uint32_t) * (size + 1)]);\n-\tchild.buffers[1] = child_holder.offsets.get();\n-\tauto offset_ptr = (uint32_t *)child.buffers[1];\n-\tauto list_data = FlatVector::GetData<list_entry_t>(data);\n-\tauto list_mask = FlatVector::Validity(data);\n-\tidx_t offset = 0;\n-\toffset_ptr[0] = 0;\n-\tfor (idx_t i = 0; i < size; i++) {\n-\t\tauto &le = list_data[i];\n-\n-\t\tif (list_mask.RowIsValid(i)) {\n-\t\t\toffset += le.length;\n-\t\t}\n-\n-\t\toffset_ptr[i + 1] = offset;\n-\t}\n-\tauto list_size = ListVector::GetListSize(data);\n-\tchild_holder.children.resize(1);\n-\tInitializeChild(child_holder.children[0], list_size);\n-\tchild.n_children = 1;\n-\tchild_holder.children_ptrs.push_back(&child_holder.children[0].array);\n-\tchild.children = &child_holder.children_ptrs[0];\n-\tauto &child_vector = ListVector::GetEntry(data);\n-\tauto &child_type = ListType::GetChildType(type);\n-\tSetArrowChild(child_holder.children[0], child_type, child_vector, list_size);\n-\tSetChildValidityMask(child_vector, child_holder.children[0].array);\n-}\n-\n-void SetStruct(DuckDBArrowArrayChildHolder &child_holder, const LogicalType &type, Vector &data, idx_t size) {\n-\tauto &child = child_holder.array;\n-\tchild_holder.vector = make_unique<Vector>(data);\n-\n-\t//! Structs only have validity buffers\n-\tchild.n_buffers = 1;\n-\tauto &children = StructVector::GetEntries(*child_holder.vector);\n-\tchild.n_children = children.size();\n-\tchild_holder.children.resize(child.n_children);\n-\tfor (auto &struct_child : child_holder.children) {\n-\t\tInitializeChild(struct_child, size);\n-\t\tchild_holder.children_ptrs.push_back(&struct_child.array);\n-\t}\n-\tchild.children = &child_holder.children_ptrs[0];\n-\tfor (idx_t child_idx = 0; child_idx < child_holder.children.size(); child_idx++) {\n-\t\tSetArrowChild(child_holder.children[child_idx], StructType::GetChildType(type, child_idx), *children[child_idx],\n-\t\t              size);\n-\t\tSetChildValidityMask(*children[child_idx], child_holder.children[child_idx].array);\n-\t}\n-}\n-\n-void SetStructMap(DuckDBArrowArrayChildHolder &child_holder, const LogicalType &type, Vector &data, idx_t size) {\n-\tauto &child = child_holder.array;\n-\tchild_holder.vector = make_unique<Vector>(data);\n-\n-\t//! Structs only have validity buffers\n-\tchild.n_buffers = 1;\n-\tauto &children = StructVector::GetEntries(*child_holder.vector);\n-\tchild.n_children = children.size();\n-\tchild_holder.children.resize(child.n_children);\n-\tauto list_size = ListVector::GetListSize(*children[0]);\n-\tchild.length = list_size;\n-\tfor (auto &struct_child : child_holder.children) {\n-\t\tInitializeChild(struct_child, list_size);\n-\t\tchild_holder.children_ptrs.push_back(&struct_child.array);\n-\t}\n-\tchild.children = &child_holder.children_ptrs[0];\n-\tauto &child_types = StructType::GetChildTypes(type);\n-\tfor (idx_t child_idx = 0; child_idx < child_holder.children.size(); child_idx++) {\n-\t\tauto &list_vector_child = ListVector::GetEntry(*children[child_idx]);\n-\t\tif (child_idx == 0) {\n-\t\t\tUnifiedVectorFormat list_data;\n-\t\t\tchildren[child_idx]->ToUnifiedFormat(size, list_data);\n-\t\t\tauto list_child_validity = FlatVector::Validity(list_vector_child);\n-\t\t\tif (!list_child_validity.AllValid()) {\n-\t\t\t\t//! Get the offsets to check from the selection vector\n-\t\t\t\tauto list_offsets = FlatVector::GetData<list_entry_t>(*children[child_idx]);\n-\t\t\t\tfor (idx_t list_idx = 0; list_idx < size; list_idx++) {\n-\t\t\t\t\tauto offset = list_offsets[list_data.sel->get_index(list_idx)];\n-\t\t\t\t\tif (!list_child_validity.CheckAllValid(offset.length + offset.offset, offset.offset)) {\n-\t\t\t\t\t\tthrow std::runtime_error(\"Arrow doesnt accept NULL keys on Maps\");\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tSetChildValidityMask(list_vector_child, child_holder.children[child_idx].array);\n-\t\t}\n-\t\tSetArrowChild(child_holder.children[child_idx], ListType::GetChildType(child_types[child_idx].second),\n-\t\t              list_vector_child, list_size);\n-\t}\n-}\n-\n-struct ArrowUUIDConversion {\n-\tusing internal_type_t = hugeint_t;\n-\n-\tstatic unique_ptr<Vector> InitializeVector(Vector &data, idx_t size) {\n-\t\treturn make_unique<Vector>(LogicalType::VARCHAR, size);\n-\t}\n-\n-\tstatic idx_t GetStringLength(hugeint_t value) {\n-\t\treturn UUID::STRING_SIZE;\n-\t}\n-\n-\tstatic string_t ConvertValue(Vector &tgt_vec, string_t *tgt_ptr, internal_type_t *src_ptr, idx_t row) {\n-\t\tauto str_value = UUID::ToString(src_ptr[row]);\n-\t\t// Have to store this string\n-\t\ttgt_ptr[row] = StringVector::AddStringOrBlob(tgt_vec, str_value);\n-\t\treturn tgt_ptr[row];\n-\t}\n-};\n-\n-struct ArrowVarcharConversion {\n-\tusing internal_type_t = string_t;\n-\n-\tstatic unique_ptr<Vector> InitializeVector(Vector &data, idx_t size) {\n-\t\treturn make_unique<Vector>(data);\n-\t}\n-\tstatic idx_t GetStringLength(string_t value) {\n-\t\treturn value.GetSize();\n-\t}\n-\n-\tstatic string_t ConvertValue(Vector &tgt_vec, string_t *tgt_ptr, internal_type_t *src_ptr, idx_t row) {\n-\t\treturn src_ptr[row];\n-\t}\n-};\n-\n-template <class CONVERT, class VECTOR_TYPE>\n-void SetVarchar(DuckDBArrowArrayChildHolder &child_holder, const LogicalType &type, Vector &data, idx_t size) {\n-\tauto &child = child_holder.array;\n-\tchild_holder.vector = CONVERT::InitializeVector(data, size);\n-\tauto target_data_ptr = FlatVector::GetData<string_t>(data);\n-\tchild.n_buffers = 3;\n-\tchild_holder.offsets = unique_ptr<data_t[]>(new data_t[sizeof(uint32_t) * (size + 1)]);\n-\tchild.buffers[1] = child_holder.offsets.get();\n-\tD_ASSERT(child.buffers[1]);\n-\t//! step 1: figure out total string length:\n-\tidx_t total_string_length = 0;\n-\tauto source_ptr = FlatVector::GetData<VECTOR_TYPE>(data);\n-\tauto &mask = FlatVector::Validity(data);\n-\tfor (idx_t row_idx = 0; row_idx < size; row_idx++) {\n-\t\tif (!mask.RowIsValid(row_idx)) {\n-\t\t\tcontinue;\n-\t\t}\n-\t\ttotal_string_length += CONVERT::GetStringLength(source_ptr[row_idx]);\n-\t}\n-\t//! step 2: allocate this much\n-\tchild_holder.data = unique_ptr<data_t[]>(new data_t[total_string_length]);\n-\tchild.buffers[2] = child_holder.data.get();\n-\tD_ASSERT(child.buffers[2]);\n-\t//! step 3: assign buffers\n-\tidx_t current_heap_offset = 0;\n-\tauto target_ptr = (uint32_t *)child.buffers[1];\n-\n-\tfor (idx_t row_idx = 0; row_idx < size; row_idx++) {\n-\t\ttarget_ptr[row_idx] = current_heap_offset;\n-\t\tif (!mask.RowIsValid(row_idx)) {\n-\t\t\tcontinue;\n-\t\t}\n-\t\tstring_t str = CONVERT::ConvertValue(*child_holder.vector, target_data_ptr, source_ptr, row_idx);\n-\t\tmemcpy((void *)((uint8_t *)child.buffers[2] + current_heap_offset), str.GetDataUnsafe(), str.GetSize());\n-\t\tcurrent_heap_offset += str.GetSize();\n-\t}\n-\ttarget_ptr[size] = current_heap_offset; //! need to terminate last string!\n-}\n-\n-void SetArrowChild(DuckDBArrowArrayChildHolder &child_holder, const LogicalType &type, Vector &data, idx_t size) {\n-\tauto &child = child_holder.array;\n-\tswitch (type.id()) {\n-\tcase LogicalTypeId::BOOLEAN: {\n-\t\t//! Gotta bitpack these booleans\n-\t\tchild_holder.vector = make_unique<Vector>(data);\n-\t\tchild.n_buffers = 2;\n-\t\tidx_t num_bytes = (size + 8 - 1) / 8;\n-\t\tchild_holder.data = unique_ptr<data_t[]>(new data_t[sizeof(uint8_t) * num_bytes]);\n-\t\tchild.buffers[1] = child_holder.data.get();\n-\t\tauto source_ptr = FlatVector::GetData<uint8_t>(*child_holder.vector);\n-\t\tauto target_ptr = (uint8_t *)child.buffers[1];\n-\t\tidx_t target_pos = 0;\n-\t\tidx_t cur_bit = 0;\n-\t\tfor (idx_t row_idx = 0; row_idx < size; row_idx++) {\n-\t\t\tif (cur_bit == 8) {\n-\t\t\t\ttarget_pos++;\n-\t\t\t\tcur_bit = 0;\n-\t\t\t}\n-\t\t\tif (source_ptr[row_idx] == 0) {\n-\t\t\t\t//! We set the bit to 0\n-\t\t\t\ttarget_ptr[target_pos] &= ~(1 << cur_bit);\n-\t\t\t} else {\n-\t\t\t\t//! We set the bit to 1\n-\t\t\t\ttarget_ptr[target_pos] |= 1 << cur_bit;\n-\t\t\t}\n-\t\t\tcur_bit++;\n-\t\t}\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::TINYINT:\n-\tcase LogicalTypeId::SMALLINT:\n-\tcase LogicalTypeId::INTEGER:\n-\tcase LogicalTypeId::BIGINT:\n-\tcase LogicalTypeId::UTINYINT:\n-\tcase LogicalTypeId::USMALLINT:\n-\tcase LogicalTypeId::UINTEGER:\n-\tcase LogicalTypeId::UBIGINT:\n-\tcase LogicalTypeId::FLOAT:\n-\tcase LogicalTypeId::DOUBLE:\n-\tcase LogicalTypeId::HUGEINT:\n-\tcase LogicalTypeId::DATE:\n-\tcase LogicalTypeId::TIMESTAMP:\n-\tcase LogicalTypeId::TIMESTAMP_MS:\n-\tcase LogicalTypeId::TIMESTAMP_NS:\n-\tcase LogicalTypeId::TIMESTAMP_SEC:\n-\tcase LogicalTypeId::TIME:\n-\tcase LogicalTypeId::TIMESTAMP_TZ:\n-\tcase LogicalTypeId::TIME_TZ:\n-\t\tchild_holder.vector = make_unique<Vector>(data);\n-\t\tchild.n_buffers = 2;\n-\t\tchild.buffers[1] = (void *)FlatVector::GetData(*child_holder.vector);\n-\t\tbreak;\n-\tcase LogicalTypeId::SQLNULL:\n-\t\tchild.n_buffers = 1;\n-\t\tbreak;\n-\tcase LogicalTypeId::DECIMAL: {\n-\t\tchild.n_buffers = 2;\n-\t\tchild_holder.vector = make_unique<Vector>(data);\n-\n-\t\t//! We have to convert to INT128\n-\t\tswitch (type.InternalType()) {\n-\n-\t\tcase PhysicalType::INT16: {\n-\t\t\tchild_holder.data = unique_ptr<data_t[]>(new data_t[sizeof(hugeint_t) * (size)]);\n-\t\t\tchild.buffers[1] = child_holder.data.get();\n-\t\t\tauto source_ptr = FlatVector::GetData<int16_t>(*child_holder.vector);\n-\t\t\tauto target_ptr = (hugeint_t *)child.buffers[1];\n-\t\t\tfor (idx_t row_idx = 0; row_idx < size; row_idx++) {\n-\t\t\t\ttarget_ptr[row_idx] = source_ptr[row_idx];\n-\t\t\t}\n-\t\t\tbreak;\n-\t\t}\n-\t\tcase PhysicalType::INT32: {\n-\t\t\tchild_holder.data = unique_ptr<data_t[]>(new data_t[sizeof(hugeint_t) * (size)]);\n-\t\t\tchild.buffers[1] = child_holder.data.get();\n-\t\t\tauto source_ptr = FlatVector::GetData<int32_t>(*child_holder.vector);\n-\t\t\tauto target_ptr = (hugeint_t *)child.buffers[1];\n-\t\t\tfor (idx_t row_idx = 0; row_idx < size; row_idx++) {\n-\t\t\t\ttarget_ptr[row_idx] = source_ptr[row_idx];\n-\t\t\t}\n-\t\t\tbreak;\n-\t\t}\n-\t\tcase PhysicalType::INT64: {\n-\t\t\tchild_holder.data = unique_ptr<data_t[]>(new data_t[sizeof(hugeint_t) * (size)]);\n-\t\t\tchild.buffers[1] = child_holder.data.get();\n-\t\t\tauto source_ptr = FlatVector::GetData<int64_t>(*child_holder.vector);\n-\t\t\tauto target_ptr = (hugeint_t *)child.buffers[1];\n-\t\t\tfor (idx_t row_idx = 0; row_idx < size; row_idx++) {\n-\t\t\t\ttarget_ptr[row_idx] = source_ptr[row_idx];\n-\t\t\t}\n-\t\t\tbreak;\n-\t\t}\n-\t\tcase PhysicalType::INT128: {\n-\t\t\tchild.buffers[1] = (void *)FlatVector::GetData(*child_holder.vector);\n-\t\t\tbreak;\n-\t\t}\n-\t\tdefault:\n-\t\t\tthrow std::runtime_error(\"Unsupported physical type for Decimal\" + TypeIdToString(type.InternalType()));\n-\t\t}\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::BLOB:\n-\tcase LogicalTypeId::JSON:\n-\tcase LogicalTypeId::VARCHAR: {\n-\t\tSetVarchar<ArrowVarcharConversion, string_t>(child_holder, type, data, size);\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::UUID: {\n-\t\tSetVarchar<ArrowUUIDConversion, hugeint_t>(child_holder, type, data, size);\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::LIST: {\n-\t\tSetList(child_holder, type, data, size);\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::STRUCT: {\n-\t\tSetStruct(child_holder, type, data, size);\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::MAP: {\n-\t\tchild_holder.vector = make_unique<Vector>(data);\n-\n-\t\tauto &map_mask = FlatVector::Validity(*child_holder.vector);\n-\t\tchild.n_buffers = 2;\n-\t\t//! Maps have one child\n-\t\tchild.n_children = 1;\n-\t\tchild_holder.children.resize(1);\n-\t\tInitializeChild(child_holder.children[0], size);\n-\t\tchild_holder.children_ptrs.push_back(&child_holder.children[0].array);\n-\t\t//! Second Buffer is the offsets\n-\t\tchild_holder.offsets = unique_ptr<data_t[]>(new data_t[sizeof(uint32_t) * (size + 1)]);\n-\t\tchild.buffers[1] = child_holder.offsets.get();\n-\t\tauto &struct_children = StructVector::GetEntries(data);\n-\t\tauto offset_ptr = (uint32_t *)child.buffers[1];\n-\t\tauto list_data = FlatVector::GetData<list_entry_t>(*struct_children[0]);\n-\t\tidx_t offset = 0;\n-\t\toffset_ptr[0] = 0;\n-\t\tfor (idx_t i = 0; i < size; i++) {\n-\t\t\tauto &le = list_data[i];\n-\t\t\tif (map_mask.RowIsValid(i)) {\n-\t\t\t\toffset += le.length;\n-\t\t\t}\n-\t\t\toffset_ptr[i + 1] = offset;\n-\t\t}\n-\t\tchild.children = &child_holder.children_ptrs[0];\n-\t\t//! We need to set up a struct\n-\t\tauto struct_type = LogicalType::STRUCT(StructType::GetChildTypes(type));\n-\n-\t\tSetStructMap(child_holder.children[0], struct_type, *child_holder.vector, size);\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::INTERVAL: {\n-\t\t//! convert interval from month/days/ucs to milliseconds\n-\t\tchild_holder.vector = make_unique<Vector>(data);\n-\t\tchild.n_buffers = 2;\n-\t\tchild_holder.data = unique_ptr<data_t[]>(new data_t[sizeof(int64_t) * (size)]);\n-\t\tchild.buffers[1] = child_holder.data.get();\n-\t\tauto source_ptr = FlatVector::GetData<interval_t>(*child_holder.vector);\n-\t\tauto target_ptr = (int64_t *)child.buffers[1];\n-\t\tfor (idx_t row_idx = 0; row_idx < size; row_idx++) {\n-\t\t\ttarget_ptr[row_idx] = Interval::GetMilli(source_ptr[row_idx]);\n-\t\t}\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::ENUM: {\n-\t\t// We need to initialize our dictionary\n-\t\tchild_holder.children.resize(1);\n-\t\tidx_t dict_size = EnumType::GetSize(type);\n-\t\tInitializeChild(child_holder.children[0], dict_size);\n-\t\tVector dictionary(EnumType::GetValuesInsertOrder(type));\n-\t\tSetArrowChild(child_holder.children[0], dictionary.GetType(), dictionary, dict_size);\n-\t\tchild_holder.children_ptrs.push_back(&child_holder.children[0].array);\n-\n-\t\t// now we set the data\n-\t\tchild.dictionary = child_holder.children_ptrs[0];\n-\t\tchild_holder.vector = make_unique<Vector>(data);\n-\t\tchild.n_buffers = 2;\n-\t\tchild.buffers[1] = (void *)FlatVector::GetData(*child_holder.vector);\n-\n-\t\tbreak;\n-\t}\n-\tdefault:\n-\t\tthrow std::runtime_error(\"Unsupported type \" + type.ToString());\n-\t}\n-}\n-\n-void DataChunk::ToArrowArray(ArrowArray *out_array) {\n-\tFlatten();\n-\tD_ASSERT(out_array);\n-\n-\t// Allocate as unique_ptr first to cleanup properly on error\n-\tauto root_holder = make_unique<DuckDBArrowArrayHolder>();\n-\n-\t// Allocate the children\n-\troot_holder->children.resize(ColumnCount());\n-\troot_holder->children_ptrs.resize(ColumnCount(), nullptr);\n-\tfor (size_t i = 0; i < ColumnCount(); ++i) {\n-\t\troot_holder->children_ptrs[i] = &root_holder->children[i].array;\n-\t}\n-\tout_array->children = root_holder->children_ptrs.data();\n-\tout_array->n_children = ColumnCount();\n-\n-\t// Configure root array\n-\tout_array->length = size();\n-\tout_array->n_children = ColumnCount();\n-\tout_array->n_buffers = 1;\n-\tout_array->buffers = root_holder->buffers.data(); // there is no actual buffer there since we don't have NULLs\n-\tout_array->offset = 0;\n-\tout_array->null_count = 0; // needs to be 0\n-\tout_array->dictionary = nullptr;\n-\n-\t//! Configure child arrays\n-\tfor (idx_t col_idx = 0; col_idx < ColumnCount(); col_idx++) {\n-\t\tauto &child_holder = root_holder->children[col_idx];\n-\t\tInitializeChild(child_holder, size());\n-\t\tauto &vector = child_holder.vector;\n-\t\tauto &child = child_holder.array;\n-\t\tauto vec_buffer = data[col_idx].GetBuffer();\n-\t\tif (vec_buffer->GetAuxiliaryData() &&\n-\t\t    vec_buffer->GetAuxiliaryDataType() == VectorAuxiliaryDataType::ARROW_AUXILIARY) {\n-\t\t\tauto arrow_aux_data = (ArrowAuxiliaryData *)vec_buffer->GetAuxiliaryData();\n-\t\t\troot_holder->arrow_original_array.push_back(arrow_aux_data->arrow_array);\n-\t\t}\n-\t\t//! We could, in theory, output other types of vectors here, currently only FLAT Vectors\n-\t\tSetArrowChild(child_holder, GetTypes()[col_idx], data[col_idx], size());\n-\t\tSetChildValidityMask(*vector, child);\n-\t\tout_array->children[col_idx] = &child;\n-\t}\n-\n-\t// Release ownership to caller\n-\tout_array->private_data = root_holder.release();\n-\tout_array->release = ReleaseDuckDBArrowArray;\n-}\n-\n } // namespace duckdb\ndiff --git a/src/common/types/validity_mask.cpp b/src/common/types/validity_mask.cpp\nindex 63c3e6b7645b..55f5756e1118 100644\n--- a/src/common/types/validity_mask.cpp\n+++ b/src/common/types/validity_mask.cpp\n@@ -49,17 +49,19 @@ string ValidityMask::ToString(idx_t count) const {\n // LCOV_EXCL_STOP\n \n void ValidityMask::Resize(idx_t old_size, idx_t new_size) {\n+\tD_ASSERT(new_size >= old_size);\n \tif (validity_mask) {\n \t\tauto new_size_count = EntryCount(new_size);\n \t\tauto old_size_count = EntryCount(old_size);\n-\t\tauto new_owned_data = unique_ptr<validity_t[]>(new validity_t[new_size_count]);\n+\t\tauto new_validity_data = make_buffer<ValidityBuffer>(new_size);\n+\t\tauto new_owned_data = new_validity_data->owned_data.get();\n \t\tfor (idx_t entry_idx = 0; entry_idx < old_size_count; entry_idx++) {\n \t\t\tnew_owned_data[entry_idx] = validity_mask[entry_idx];\n \t\t}\n \t\tfor (idx_t entry_idx = old_size_count; entry_idx < new_size_count; entry_idx++) {\n \t\t\tnew_owned_data[entry_idx] = ValidityData::MAX_ENTRY;\n \t\t}\n-\t\tvalidity_data->owned_data = move(new_owned_data);\n+\t\tvalidity_data = move(new_validity_data);\n \t\tvalidity_mask = validity_data->owned_data.get();\n \t} else {\n \t\tInitialize(new_size);\ndiff --git a/src/common/types/vector.cpp b/src/common/types/vector.cpp\nindex 87b2328ad5d8..883232d7f365 100644\n--- a/src/common/types/vector.cpp\n+++ b/src/common/types/vector.cpp\n@@ -403,7 +403,7 @@ void Vector::SetValue(idx_t index, const Value &val) {\n \t}\n }\n \n-Value Vector::GetValue(const Vector &v_p, idx_t index_p) {\n+Value Vector::GetValueInternal(const Vector &v_p, idx_t index_p) {\n \tconst Vector *vector = &v_p;\n \tidx_t index = index_p;\n \tbool finished = false;\n@@ -561,6 +561,15 @@ Value Vector::GetValue(const Vector &v_p, idx_t index_p) {\n \t}\n }\n \n+Value Vector::GetValue(const Vector &v_p, idx_t index_p) {\n+\tauto value = GetValueInternal(v_p, index_p);\n+\t// set the alias of the type to the correct value, if there is a type alias\n+\tif (v_p.GetType().HasAlias()) {\n+\t\tvalue.type().SetAlias(v_p.GetType().GetAlias());\n+\t}\n+\treturn value;\n+}\n+\n Value Vector::GetValue(idx_t index) const {\n \treturn GetValue(*this, index);\n }\ndiff --git a/src/common/types/vector_cache.cpp b/src/common/types/vector_cache.cpp\nindex 2cd372a89d85..7537cf225781 100644\n--- a/src/common/types/vector_cache.cpp\n+++ b/src/common/types/vector_cache.cpp\n@@ -43,13 +43,14 @@ class VectorCacheBuffer : public VectorBuffer {\n \t\tresult.validity.Reset();\n \t\tswitch (internal_type) {\n \t\tcase PhysicalType::LIST: {\n-\t\t\tresult.data = owned_data->get();\n+\t\t\tresult.data = owned_data.get();\n \t\t\t// reinitialize the VectorListBuffer\n \t\t\tAssignSharedPointer(result.auxiliary, auxiliary);\n \t\t\t// propagate through child\n \t\t\tauto &list_buffer = (VectorListBuffer &)*result.auxiliary;\n \t\t\tlist_buffer.capacity = STANDARD_VECTOR_SIZE;\n \t\t\tlist_buffer.size = 0;\n+\t\t\tlist_buffer.SetAuxiliaryData(nullptr);\n \n \t\t\tauto &list_child = list_buffer.GetChild();\n \t\t\tauto &child_cache = (VectorCacheBuffer &)*child_caches[0];\n@@ -60,6 +61,7 @@ class VectorCacheBuffer : public VectorBuffer {\n \t\t\t// struct does not have data\n \t\t\tresult.data = nullptr;\n \t\t\t// reinitialize the VectorStructBuffer\n+\t\t\tauxiliary->SetAuxiliaryData(nullptr);\n \t\t\tAssignSharedPointer(result.auxiliary, auxiliary);\n \t\t\t// propagate through children\n \t\t\tauto &children = ((VectorStructBuffer &)*result.auxiliary).GetChildren();\n@@ -71,7 +73,7 @@ class VectorCacheBuffer : public VectorBuffer {\n \t\t}\n \t\tdefault:\n \t\t\t// regular type: no aux data and reset data to cached data\n-\t\t\tresult.data = owned_data->get();\n+\t\t\tresult.data = owned_data.get();\n \t\t\tresult.auxiliary.reset();\n \t\t\tbreak;\n \t\t}\n@@ -85,7 +87,7 @@ class VectorCacheBuffer : public VectorBuffer {\n \t//! The type of the vector cache\n \tLogicalType type;\n \t//! Owned data\n-\tunique_ptr<AllocatedData> owned_data;\n+\tAllocatedData owned_data;\n \t//! Child caches (if any). Used for nested types.\n \tvector<buffer_ptr<VectorBuffer>> child_caches;\n \t//! Aux data for the vector (if any)\ndiff --git a/src/execution/nested_loop_join/nested_loop_join_mark.cpp b/src/execution/nested_loop_join/nested_loop_join_mark.cpp\nindex f4d83ad8659f..2fd61fd6393b 100644\n--- a/src/execution/nested_loop_join/nested_loop_join_mark.cpp\n+++ b/src/execution/nested_loop_join/nested_loop_join_mark.cpp\n@@ -87,14 +87,19 @@ static void MarkJoinComparisonSwitch(Vector &left, Vector &right, idx_t lcount,\n \t}\n }\n \n-void NestedLoopJoinMark::Perform(DataChunk &left, ChunkCollection &right, bool found_match[],\n+void NestedLoopJoinMark::Perform(DataChunk &left, ColumnDataCollection &right, bool found_match[],\n                                  const vector<JoinCondition> &conditions) {\n \t// initialize a new temporary selection vector for the left chunk\n \t// loop over all chunks in the RHS\n-\tfor (idx_t chunk_idx = 0; chunk_idx < right.ChunkCount(); chunk_idx++) {\n-\t\tDataChunk &right_chunk = right.GetChunk(chunk_idx);\n+\tColumnDataScanState scan_state;\n+\tright.InitializeScan(scan_state);\n+\n+\tDataChunk scan_chunk;\n+\tright.InitializeScanChunk(scan_chunk);\n+\n+\twhile (right.Scan(scan_state, scan_chunk)) {\n \t\tfor (idx_t i = 0; i < conditions.size(); i++) {\n-\t\t\tMarkJoinComparisonSwitch(left.data[i], right_chunk.data[i], left.size(), right_chunk.size(), found_match,\n+\t\t\tMarkJoinComparisonSwitch(left.data[i], scan_chunk.data[i], left.size(), scan_chunk.size(), found_match,\n \t\t\t                         conditions[i].comparison);\n \t\t}\n \t}\ndiff --git a/src/execution/operator/aggregate/physical_window.cpp b/src/execution/operator/aggregate/physical_window.cpp\nindex a39fa17bb6be..e21fd356f590 100644\n--- a/src/execution/operator/aggregate/physical_window.cpp\n+++ b/src/execution/operator/aggregate/physical_window.cpp\n@@ -328,6 +328,7 @@ class WindowLocalSinkState : public LocalSinkState {\n \n void WindowLocalSinkState::Over(DataChunk &input_chunk) {\n \tif (over_chunk.ColumnCount() > 0) {\n+\t\tover_chunk.Reset();\n \t\texecutor.Execute(input_chunk, over_chunk);\n \t\tover_chunk.Verify();\n \t}\ndiff --git a/src/execution/operator/helper/physical_batch_collector.cpp b/src/execution/operator/helper/physical_batch_collector.cpp\nindex f0a9daa9c3e0..2a53cc189243 100644\n--- a/src/execution/operator/helper/physical_batch_collector.cpp\n+++ b/src/execution/operator/helper/physical_batch_collector.cpp\n@@ -1,5 +1,5 @@\n #include \"duckdb/execution/operator/helper/physical_batch_collector.hpp\"\n-#include \"duckdb/common/types/batched_chunk_collection.hpp\"\n+#include \"duckdb/common/types/batched_data_collection.hpp\"\n #include \"duckdb/main/materialized_query_result.hpp\"\n #include \"duckdb/main/client_context.hpp\"\n \n@@ -13,20 +13,20 @@ PhysicalBatchCollector::PhysicalBatchCollector(PreparedStatementData &data) : Ph\n //===--------------------------------------------------------------------===//\n class BatchCollectorGlobalState : public GlobalSinkState {\n public:\n-\texplicit BatchCollectorGlobalState(Allocator &allocator) : data(allocator) {\n+\tBatchCollectorGlobalState(ClientContext &context, const PhysicalBatchCollector &op) : data(op.types) {\n \t}\n \n \tmutex glock;\n-\tBatchedChunkCollection data;\n+\tBatchedDataCollection data;\n \tunique_ptr<MaterializedQueryResult> result;\n };\n \n class BatchCollectorLocalState : public LocalSinkState {\n public:\n-\texplicit BatchCollectorLocalState(Allocator &allocator) : data(allocator) {\n+\tBatchCollectorLocalState(ClientContext &context, const PhysicalBatchCollector &op) : data(op.types) {\n \t}\n \n-\tBatchedChunkCollection data;\n+\tBatchedDataCollection data;\n };\n \n SinkResultType PhysicalBatchCollector::Sink(ExecutionContext &context, GlobalSinkState &gstate,\n@@ -48,32 +48,20 @@ void PhysicalBatchCollector::Combine(ExecutionContext &context, GlobalSinkState\n SinkFinalizeType PhysicalBatchCollector::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,\n                                                   GlobalSinkState &gstate_p) const {\n \tauto &gstate = (BatchCollectorGlobalState &)gstate_p;\n-\tauto result =\n-\t    make_unique<MaterializedQueryResult>(statement_type, properties, types, names, context.shared_from_this());\n-\tDataChunk output;\n-\toutput.Initialize(BufferAllocator::Get(context), types);\n-\n-\tBatchedChunkScanState state;\n-\tgstate.data.InitializeScan(state);\n-\twhile (true) {\n-\t\toutput.Reset();\n-\t\tgstate.data.Scan(state, output);\n-\t\tif (output.size() == 0) {\n-\t\t\tbreak;\n-\t\t}\n-\t\tresult->collection.Append(output);\n-\t}\n-\n+\tauto collection = gstate.data.FetchCollection();\n+\tD_ASSERT(collection);\n+\tauto result = make_unique<MaterializedQueryResult>(statement_type, properties, names, move(collection),\n+\t                                                   context.GetClientProperties());\n \tgstate.result = move(result);\n \treturn SinkFinalizeType::READY;\n }\n \n unique_ptr<LocalSinkState> PhysicalBatchCollector::GetLocalSinkState(ExecutionContext &context) const {\n-\treturn make_unique<BatchCollectorLocalState>(Allocator::DefaultAllocator());\n+\treturn make_unique<BatchCollectorLocalState>(context.client, *this);\n }\n \n unique_ptr<GlobalSinkState> PhysicalBatchCollector::GetGlobalSinkState(ClientContext &context) const {\n-\treturn make_unique<BatchCollectorGlobalState>(Allocator::DefaultAllocator());\n+\treturn make_unique<BatchCollectorGlobalState>(context, *this);\n }\n \n unique_ptr<QueryResult> PhysicalBatchCollector::GetResult(GlobalSinkState &state) {\ndiff --git a/src/execution/operator/helper/physical_limit.cpp b/src/execution/operator/helper/physical_limit.cpp\nindex 22aed2ac757f..8c3dc7e67183 100644\n--- a/src/execution/operator/helper/physical_limit.cpp\n+++ b/src/execution/operator/helper/physical_limit.cpp\n@@ -4,7 +4,7 @@\n #include \"duckdb/main/config.hpp\"\n \n #include \"duckdb/execution/expression_executor.hpp\"\n-#include \"duckdb/common/types/batched_chunk_collection.hpp\"\n+#include \"duckdb/common/types/batched_data_collection.hpp\"\n #include \"duckdb/execution/operator/helper/physical_streaming_limit.hpp\"\n \n namespace duckdb {\n@@ -21,7 +21,7 @@ PhysicalLimit::PhysicalLimit(vector<LogicalType> types, idx_t limit, idx_t offse\n //===--------------------------------------------------------------------===//\n class LimitGlobalState : public GlobalSinkState {\n public:\n-\texplicit LimitGlobalState(Allocator &allocator, const PhysicalLimit &op) : data(allocator) {\n+\texplicit LimitGlobalState(ClientContext &context, const PhysicalLimit &op) : data(op.types) {\n \t\tlimit = 0;\n \t\toffset = 0;\n \t}\n@@ -29,12 +29,12 @@ class LimitGlobalState : public GlobalSinkState {\n \tmutex glock;\n \tidx_t limit;\n \tidx_t offset;\n-\tBatchedChunkCollection data;\n+\tBatchedDataCollection data;\n };\n \n class LimitLocalState : public LocalSinkState {\n public:\n-\texplicit LimitLocalState(Allocator &allocator, const PhysicalLimit &op) : current_offset(0), data(allocator) {\n+\texplicit LimitLocalState(ClientContext &context, const PhysicalLimit &op) : current_offset(0), data(op.types) {\n \t\tthis->limit = op.limit_expression ? DConstants::INVALID_INDEX : op.limit_value;\n \t\tthis->offset = op.offset_expression ? DConstants::INVALID_INDEX : op.offset_value;\n \t}\n@@ -42,15 +42,15 @@ class LimitLocalState : public LocalSinkState {\n \tidx_t current_offset;\n \tidx_t limit;\n \tidx_t offset;\n-\tBatchedChunkCollection data;\n+\tBatchedDataCollection data;\n };\n \n unique_ptr<GlobalSinkState> PhysicalLimit::GetGlobalSinkState(ClientContext &context) const {\n-\treturn make_unique<LimitGlobalState>(Allocator::Get(context), *this);\n+\treturn make_unique<LimitGlobalState>(context, *this);\n }\n \n unique_ptr<LocalSinkState> PhysicalLimit::GetLocalSinkState(ExecutionContext &context) const {\n-\treturn make_unique<LimitLocalState>(Allocator::Get(context.client), *this);\n+\treturn make_unique<LimitLocalState>(context.client, *this);\n }\n \n bool PhysicalLimit::ComputeOffset(ExecutionContext &context, DataChunk &input, idx_t &limit, idx_t &offset,\n@@ -104,6 +104,10 @@ SinkResultType PhysicalLimit::Sink(ExecutionContext &context, GlobalSinkState &g\n \t                   offset_expression.get())) {\n \t\treturn SinkResultType::FINISHED;\n \t}\n+\tauto max_cardinality = max_element - state.current_offset;\n+\tif (max_cardinality < input.size()) {\n+\t\tinput.SetCardinality(max_cardinality);\n+\t}\n \tstate.data.Append(input, lstate.batch_index);\n \tstate.current_offset += input.size();\n \treturn SinkResultType::NEED_MORE_INPUT;\ndiff --git a/src/execution/operator/helper/physical_limit_percent.cpp b/src/execution/operator/helper/physical_limit_percent.cpp\nindex ad6df63c92fd..fa72eac6e93e 100644\n--- a/src/execution/operator/helper/physical_limit_percent.cpp\n+++ b/src/execution/operator/helper/physical_limit_percent.cpp\n@@ -4,7 +4,7 @@\n #include \"duckdb/common/algorithm.hpp\"\n \n #include \"duckdb/execution/expression_executor.hpp\"\n-#include \"duckdb/common/types/chunk_collection.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n \n namespace duckdb {\n \n@@ -13,8 +13,8 @@ namespace duckdb {\n //===--------------------------------------------------------------------===//\n class LimitPercentGlobalState : public GlobalSinkState {\n public:\n-\texplicit LimitPercentGlobalState(Allocator &allocator, const PhysicalLimitPercent &op)\n-\t    : current_offset(0), data(allocator) {\n+\texplicit LimitPercentGlobalState(ClientContext &context, const PhysicalLimitPercent &op)\n+\t    : current_offset(0), data(context, op.GetTypes()) {\n \t\tif (!op.limit_expression) {\n \t\t\tthis->limit_percent = op.limit_percent;\n \t\t\tis_limit_percent_delimited = true;\n@@ -33,14 +33,14 @@ class LimitPercentGlobalState : public GlobalSinkState {\n \tidx_t current_offset;\n \tdouble limit_percent;\n \tidx_t offset;\n-\tChunkCollection data;\n+\tColumnDataCollection data;\n \n \tbool is_limit_percent_delimited = false;\n \tbool is_offset_delimited = false;\n };\n \n unique_ptr<GlobalSinkState> PhysicalLimitPercent::GetGlobalSinkState(ClientContext &context) const {\n-\treturn make_unique<LimitPercentGlobalState>(Allocator::Get(context), *this);\n+\treturn make_unique<LimitPercentGlobalState>(context, *this);\n }\n \n SinkResultType PhysicalLimitPercent::Sink(ExecutionContext &context, GlobalSinkState &gstate, LocalSinkState &lstate,\n@@ -85,23 +85,26 @@ SinkResultType PhysicalLimitPercent::Sink(ExecutionContext &context, GlobalSinkS\n //===--------------------------------------------------------------------===//\n class LimitPercentOperatorState : public GlobalSourceState {\n public:\n-\tLimitPercentOperatorState() : chunk_idx(0), limit(DConstants::INVALID_INDEX), current_offset(0) {\n+\texplicit LimitPercentOperatorState(const PhysicalLimitPercent &op)\n+\t    : limit(DConstants::INVALID_INDEX), current_offset(0) {\n+\t\tauto &gstate = (LimitPercentGlobalState &)*op.sink_state;\n+\t\tgstate.data.InitializeScan(scan_state);\n \t}\n \n-\tidx_t chunk_idx;\n+\tColumnDataScanState scan_state;\n \tidx_t limit;\n \tidx_t current_offset;\n };\n \n unique_ptr<GlobalSourceState> PhysicalLimitPercent::GetGlobalSourceState(ClientContext &context) const {\n-\treturn make_unique<LimitPercentOperatorState>();\n+\treturn make_unique<LimitPercentOperatorState>(*this);\n }\n \n void PhysicalLimitPercent::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate_p,\n                                    LocalSourceState &lstate) const {\n \tauto &gstate = (LimitPercentGlobalState &)*sink_state;\n \tauto &state = (LimitPercentOperatorState &)gstate_p;\n-\tauto &limit_percent = gstate.limit_percent;\n+\tauto &percent_limit = gstate.limit_percent;\n \tauto &offset = gstate.offset;\n \tauto &limit = state.limit;\n \tauto &current_offset = state.current_offset;\n@@ -111,10 +114,10 @@ void PhysicalLimitPercent::GetData(ExecutionContext &context, DataChunk &chunk,\n \t\tif (count > 0) {\n \t\t\tcount += offset;\n \t\t}\n-\t\tif (Value::IsNan(limit_percent) || limit_percent < 0 || limit_percent > 100) {\n+\t\tif (Value::IsNan(percent_limit) || percent_limit < 0 || percent_limit > 100) {\n \t\t\tthrow OutOfRangeException(\"Limit percent out of range, should be between 0% and 100%\");\n \t\t}\n-\t\tdouble limit_dbl = limit_percent / 100 * count;\n+\t\tdouble limit_dbl = percent_limit / 100 * count;\n \t\tif (limit_dbl > count) {\n \t\t\tlimit = count;\n \t\t} else {\n@@ -125,15 +128,14 @@ void PhysicalLimitPercent::GetData(ExecutionContext &context, DataChunk &chunk,\n \t\t}\n \t}\n \n-\tif (current_offset >= limit || state.chunk_idx >= gstate.data.ChunkCount()) {\n+\tif (current_offset >= limit) {\n \t\treturn;\n \t}\n-\n-\tDataChunk &input = gstate.data.GetChunk(state.chunk_idx);\n-\tstate.chunk_idx++;\n-\tif (PhysicalLimit::HandleOffset(input, current_offset, 0, limit)) {\n-\t\tchunk.Reference(input);\n+\tif (!gstate.data.Scan(state.scan_state, chunk)) {\n+\t\treturn;\n \t}\n+\n+\tPhysicalLimit::HandleOffset(chunk, current_offset, 0, limit);\n }\n \n } // namespace duckdb\ndiff --git a/src/execution/operator/helper/physical_materialized_collector.cpp b/src/execution/operator/helper/physical_materialized_collector.cpp\nindex e06ea66d0e8a..b180c4cc30c1 100644\n--- a/src/execution/operator/helper/physical_materialized_collector.cpp\n+++ b/src/execution/operator/helper/physical_materialized_collector.cpp\n@@ -15,28 +15,32 @@ PhysicalMaterializedCollector::PhysicalMaterializedCollector(PreparedStatementDa\n class MaterializedCollectorGlobalState : public GlobalSinkState {\n public:\n \tmutex glock;\n-\tunique_ptr<MaterializedQueryResult> result;\n+\tunique_ptr<ColumnDataCollection> collection;\n+\tColumnDataAppendState append_state;\n+\tshared_ptr<ClientContext> context;\n };\n \n SinkResultType PhysicalMaterializedCollector::Sink(ExecutionContext &context, GlobalSinkState &gstate_p,\n                                                    LocalSinkState &lstate, DataChunk &input) const {\n \tauto &gstate = (MaterializedCollectorGlobalState &)gstate_p;\n \tlock_guard<mutex> lock(gstate.glock);\n-\tgstate.result->collection.Append(input);\n+\tgstate.collection->Append(gstate.append_state, input);\n \treturn SinkResultType::NEED_MORE_INPUT;\n }\n \n unique_ptr<GlobalSinkState> PhysicalMaterializedCollector::GetGlobalSinkState(ClientContext &context) const {\n \tauto state = make_unique<MaterializedCollectorGlobalState>();\n-\tstate->result =\n-\t    make_unique<MaterializedQueryResult>(statement_type, properties, types, names, context.shared_from_this());\n+\tstate->collection = make_unique<ColumnDataCollection>(Allocator::DefaultAllocator(), types);\n+\tstate->collection->InitializeAppend(state->append_state);\n+\tstate->context = context.shared_from_this();\n \treturn move(state);\n }\n \n unique_ptr<QueryResult> PhysicalMaterializedCollector::GetResult(GlobalSinkState &state) {\n \tauto &gstate = (MaterializedCollectorGlobalState &)state;\n-\tD_ASSERT(gstate.result);\n-\treturn move(gstate.result);\n+\tauto result = make_unique<MaterializedQueryResult>(statement_type, properties, names, move(gstate.collection),\n+\t                                                   gstate.context->GetClientProperties());\n+\treturn move(result);\n }\n \n bool PhysicalMaterializedCollector::ParallelSink() const {\ndiff --git a/src/execution/operator/join/CMakeLists.txt b/src/execution/operator/join/CMakeLists.txt\nindex 5b2ffb25652e..ee44f86d049d 100644\n--- a/src/execution/operator/join/CMakeLists.txt\n+++ b/src/execution/operator/join/CMakeLists.txt\n@@ -1,6 +1,7 @@\n add_library_unity(\n   duckdb_operator_join\n   OBJECT\n+  outer_join_marker.cpp\n   physical_blockwise_nl_join.cpp\n   physical_comparison_join.cpp\n   physical_cross_product.cpp\ndiff --git a/src/execution/operator/join/outer_join_marker.cpp b/src/execution/operator/join/outer_join_marker.cpp\nnew file mode 100644\nindex 000000000000..25a1dcd50c80\n--- /dev/null\n+++ b/src/execution/operator/join/outer_join_marker.cpp\n@@ -0,0 +1,108 @@\n+#include \"duckdb/execution/operator/join/outer_join_marker.hpp\"\n+\n+namespace duckdb {\n+\n+OuterJoinMarker::OuterJoinMarker(bool enabled_p) : enabled(enabled_p), count(0) {\n+}\n+\n+void OuterJoinMarker::Initialize(idx_t count_p) {\n+\tif (!enabled) {\n+\t\treturn;\n+\t}\n+\tthis->count = count_p;\n+\tfound_match = unique_ptr<bool[]>(new bool[count]);\n+\tReset();\n+}\n+\n+void OuterJoinMarker::Reset() {\n+\tif (!enabled) {\n+\t\treturn;\n+\t}\n+\tmemset(found_match.get(), 0, sizeof(bool) * count);\n+}\n+\n+void OuterJoinMarker::SetMatch(idx_t position) {\n+\tif (!enabled) {\n+\t\treturn;\n+\t}\n+\tD_ASSERT(position < count);\n+\tfound_match[position] = true;\n+}\n+\n+void OuterJoinMarker::SetMatches(const SelectionVector &sel, idx_t count, idx_t base_idx) {\n+\tif (!enabled) {\n+\t\treturn;\n+\t}\n+\tfor (idx_t i = 0; i < count; i++) {\n+\t\tauto idx = sel.get_index(i);\n+\t\tauto pos = base_idx + idx;\n+\t\tD_ASSERT(pos < this->count);\n+\t\tfound_match[pos] = true;\n+\t}\n+}\n+\n+void OuterJoinMarker::ConstructLeftJoinResult(DataChunk &left, DataChunk &result) {\n+\tif (!enabled) {\n+\t\treturn;\n+\t}\n+\tD_ASSERT(count == STANDARD_VECTOR_SIZE);\n+\tSelectionVector remaining_sel(STANDARD_VECTOR_SIZE);\n+\tidx_t remaining_count = 0;\n+\tfor (idx_t i = 0; i < left.size(); i++) {\n+\t\tif (!found_match[i]) {\n+\t\t\tremaining_sel.set_index(remaining_count++, i);\n+\t\t}\n+\t}\n+\tif (remaining_count > 0) {\n+\t\tresult.Slice(left, remaining_sel, remaining_count);\n+\t\tfor (idx_t idx = left.ColumnCount(); idx < result.ColumnCount(); idx++) {\n+\t\t\tresult.data[idx].SetVectorType(VectorType::CONSTANT_VECTOR);\n+\t\t\tConstantVector::SetNull(result.data[idx], true);\n+\t\t}\n+\t}\n+}\n+\n+idx_t OuterJoinMarker::MaxThreads() const {\n+\treturn count / (STANDARD_VECTOR_SIZE * 10);\n+}\n+\n+void OuterJoinMarker::InitializeScan(ColumnDataCollection &data, OuterJoinGlobalScanState &gstate) {\n+\tgstate.data = &data;\n+\tdata.InitializeScan(gstate.global_scan);\n+}\n+\n+void OuterJoinMarker::InitializeScan(OuterJoinGlobalScanState &gstate, OuterJoinLocalScanState &lstate) {\n+\tD_ASSERT(gstate.data);\n+\tlstate.match_sel.Initialize(STANDARD_VECTOR_SIZE);\n+\tgstate.data->InitializeScanChunk(lstate.scan_chunk);\n+}\n+\n+void OuterJoinMarker::Scan(OuterJoinGlobalScanState &gstate, OuterJoinLocalScanState &lstate, DataChunk &result) {\n+\tD_ASSERT(gstate.data);\n+\t// fill in NULL values for the LHS\n+\twhile (gstate.data->Scan(gstate.global_scan, lstate.local_scan, lstate.scan_chunk)) {\n+\t\tidx_t result_count = 0;\n+\t\t// figure out which tuples didn't find a match in the RHS\n+\t\tfor (idx_t i = 0; i < lstate.scan_chunk.size(); i++) {\n+\t\t\tif (!found_match[lstate.local_scan.current_row_index + i]) {\n+\t\t\t\tlstate.match_sel.set_index(result_count++, i);\n+\t\t\t}\n+\t\t}\n+\t\tif (result_count > 0) {\n+\t\t\t// if there were any tuples that didn't find a match, output them\n+\t\t\tidx_t left_column_count = result.ColumnCount() - lstate.scan_chunk.ColumnCount();\n+\t\t\tfor (idx_t i = 0; i < left_column_count; i++) {\n+\t\t\t\tresult.data[i].SetVectorType(VectorType::CONSTANT_VECTOR);\n+\t\t\t\tConstantVector::SetNull(result.data[i], true);\n+\t\t\t}\n+\t\t\tfor (idx_t col_idx = left_column_count; col_idx < result.ColumnCount(); col_idx++) {\n+\t\t\t\tresult.data[col_idx].Slice(lstate.scan_chunk.data[col_idx - left_column_count], lstate.match_sel,\n+\t\t\t\t                           result_count);\n+\t\t\t}\n+\t\t\tresult.SetCardinality(result_count);\n+\t\t\treturn;\n+\t\t}\n+\t}\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/execution/operator/join/physical_blockwise_nl_join.cpp b/src/execution/operator/join/physical_blockwise_nl_join.cpp\nindex d0fa2ff22a80..a8f3ccef9459 100644\n--- a/src/execution/operator/join/physical_blockwise_nl_join.cpp\n+++ b/src/execution/operator/join/physical_blockwise_nl_join.cpp\n@@ -3,6 +3,9 @@\n #include \"duckdb/common/vector_operations/vector_operations.hpp\"\n #include \"duckdb/execution/expression_executor.hpp\"\n #include \"duckdb/execution/operator/join/physical_comparison_join.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n+#include \"duckdb/execution/operator/join/outer_join_marker.hpp\"\n+#include \"duckdb/execution/operator/join/physical_cross_product.hpp\"\n \n namespace duckdb {\n \n@@ -29,17 +32,17 @@ class BlockwiseNLJoinLocalState : public LocalSinkState {\n \n class BlockwiseNLJoinGlobalState : public GlobalSinkState {\n public:\n-\texplicit BlockwiseNLJoinGlobalState(Allocator &allocator) : right_chunks(allocator) {\n+\texplicit BlockwiseNLJoinGlobalState(ClientContext &context, const PhysicalBlockwiseNLJoin &op)\n+\t    : right_chunks(context, op.children[1]->GetTypes()), right_outer(IsRightOuterJoin(op.join_type)) {\n \t}\n \n \tmutex lock;\n-\tChunkCollection right_chunks;\n-\t//! Whether or not a tuple on the RHS has found a match, only used for FULL OUTER joins\n-\tunique_ptr<bool[]> rhs_found_match;\n+\tColumnDataCollection right_chunks;\n+\tOuterJoinMarker right_outer;\n };\n \n unique_ptr<GlobalSinkState> PhysicalBlockwiseNLJoin::GetGlobalSinkState(ClientContext &context) const {\n-\treturn make_unique<BlockwiseNLJoinGlobalState>(Allocator::Get(context));\n+\treturn make_unique<BlockwiseNLJoinGlobalState>(context, *this);\n }\n \n unique_ptr<LocalSinkState> PhysicalBlockwiseNLJoin::GetLocalSinkState(ExecutionContext &context) const {\n@@ -60,10 +63,8 @@ SinkResultType PhysicalBlockwiseNLJoin::Sink(ExecutionContext &context, GlobalSi\n SinkFinalizeType PhysicalBlockwiseNLJoin::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,\n                                                    GlobalSinkState &gstate_p) const {\n \tauto &gstate = (BlockwiseNLJoinGlobalState &)gstate_p;\n-\tif (IsRightOuterJoin(join_type)) {\n-\t\tgstate.rhs_found_match = unique_ptr<bool[]>(new bool[gstate.right_chunks.Count()]);\n-\t\tmemset(gstate.rhs_found_match.get(), 0, sizeof(bool) * gstate.right_chunks.Count());\n-\t}\n+\tgstate.right_outer.Initialize(gstate.right_chunks.Count());\n+\n \tif (gstate.right_chunks.Count() == 0 && EmptyResultIfRHSIsEmpty()) {\n \t\treturn SinkFinalizeType::NO_OUTPUT_POSSIBLE;\n \t}\n@@ -75,23 +76,22 @@ SinkFinalizeType PhysicalBlockwiseNLJoin::Finalize(Pipeline &pipeline, Event &ev\n //===--------------------------------------------------------------------===//\n class BlockwiseNLJoinState : public OperatorState {\n public:\n-\texplicit BlockwiseNLJoinState(ExecutionContext &context, const PhysicalBlockwiseNLJoin &op)\n-\t    : left_position(0), right_position(0), executor(Allocator::Get(context.client), *op.condition) {\n-\t\tif (IsLeftOuterJoin(op.join_type)) {\n-\t\t\tleft_found_match = unique_ptr<bool[]>(new bool[STANDARD_VECTOR_SIZE]);\n-\t\t\tmemset(left_found_match.get(), 0, sizeof(bool) * STANDARD_VECTOR_SIZE);\n-\t\t}\n+\texplicit BlockwiseNLJoinState(ExecutionContext &context, ColumnDataCollection &rhs,\n+\t                              const PhysicalBlockwiseNLJoin &op)\n+\t    : cross_product(rhs), left_outer(IsLeftOuterJoin(op.join_type)), match_sel(STANDARD_VECTOR_SIZE),\n+\t      executor(Allocator::Get(context.client), *op.condition) {\n+\t\tleft_outer.Initialize(STANDARD_VECTOR_SIZE);\n \t}\n \n-\t//! Whether or not a tuple on the LHS has found a match, only used for LEFT OUTER and FULL OUTER joins\n-\tunique_ptr<bool[]> left_found_match;\n-\tidx_t left_position;\n-\tidx_t right_position;\n+\tCrossProductExecutor cross_product;\n+\tOuterJoinMarker left_outer;\n+\tSelectionVector match_sel;\n \tExpressionExecutor executor;\n };\n \n unique_ptr<OperatorState> PhysicalBlockwiseNLJoin::GetOperatorState(ExecutionContext &context) const {\n-\treturn make_unique<BlockwiseNLJoinState>(context, *this);\n+\tauto &gstate = (BlockwiseNLJoinGlobalState &)*sink_state;\n+\treturn make_unique<BlockwiseNLJoinState>(context, gstate.right_chunks, *this);\n }\n \n OperatorResultType PhysicalBlockwiseNLJoin::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,\n@@ -111,69 +111,42 @@ OperatorResultType PhysicalBlockwiseNLJoin::Execute(ExecutionContext &context, D\n \t}\n \n \t// now perform the actual join\n-\t// we construct a combined DataChunk by referencing the LHS and the RHS\n-\t// every step that we do not have output results we shift the vectors of the RHS one up or down\n-\t// this creates a new \"alignment\" between the tuples, exhausting all possible O(n^2) combinations\n-\t// while allowing us to use vectorized execution for every step\n+\t// we perform a cross product, then execute the expression directly on the cross product' result\n \tidx_t result_count = 0;\n \tdo {\n-\t\tif (state.left_position >= input.size()) {\n-\t\t\t// exhausted LHS, have to pull new LHS chunk\n-\t\t\tif (state.left_found_match) {\n+\t\tauto result = state.cross_product.Execute(input, chunk);\n+\t\tif (result == OperatorResultType::NEED_MORE_INPUT) {\n+\t\t\t// exhausted input, have to pull new LHS chunk\n+\t\t\tif (state.left_outer.Enabled()) {\n \t\t\t\t// left join: before we move to the next chunk, see if we need to output any vectors that didn't\n \t\t\t\t// have a match found\n-\t\t\t\tPhysicalJoin::ConstructLeftJoinResult(input, chunk, state.left_found_match.get());\n-\t\t\t\tmemset(state.left_found_match.get(), 0, sizeof(bool) * STANDARD_VECTOR_SIZE);\n+\t\t\t\tstate.left_outer.ConstructLeftJoinResult(input, chunk);\n+\t\t\t\tstate.left_outer.Reset();\n \t\t\t}\n-\t\t\tstate.left_position = 0;\n-\t\t\tstate.right_position = 0;\n \t\t\treturn OperatorResultType::NEED_MORE_INPUT;\n \t\t}\n-\t\tauto &lchunk = input;\n-\t\tauto &rchunk = gstate.right_chunks.GetChunk(state.right_position);\n-\n-\t\t// fill in the current element of the LHS into the chunk\n-\t\tD_ASSERT(chunk.ColumnCount() == lchunk.ColumnCount() + rchunk.ColumnCount());\n-\t\tfor (idx_t i = 0; i < lchunk.ColumnCount(); i++) {\n-\t\t\tConstantVector::Reference(chunk.data[i], lchunk.data[i], state.left_position, lchunk.size());\n-\t\t}\n-\t\t// for the RHS we just reference the entire vector\n-\t\tfor (idx_t i = 0; i < rchunk.ColumnCount(); i++) {\n-\t\t\tchunk.data[lchunk.ColumnCount() + i].Reference(rchunk.data[i]);\n-\t\t}\n-\t\tchunk.SetCardinality(rchunk.size());\n \n \t\t// now perform the computation\n-\t\tSelectionVector match_sel(STANDARD_VECTOR_SIZE);\n-\t\tresult_count = state.executor.SelectExpression(chunk, match_sel);\n+\t\tresult_count = state.executor.SelectExpression(chunk, state.match_sel);\n \t\tif (result_count > 0) {\n \t\t\t// found a match!\n-\t\t\t// set the match flags in the LHS\n-\t\t\tif (state.left_found_match) {\n-\t\t\t\tstate.left_found_match[state.left_position] = true;\n-\t\t\t}\n-\t\t\t// set the match flags in the RHS\n-\t\t\tif (gstate.rhs_found_match) {\n-\t\t\t\tfor (idx_t i = 0; i < result_count; i++) {\n-\t\t\t\t\tauto idx = match_sel.get_index(i);\n-\t\t\t\t\tgstate.rhs_found_match[state.right_position * STANDARD_VECTOR_SIZE + idx] = true;\n-\t\t\t\t}\n+\t\t\t// check if the cross product is scanning the LHS or the RHS in its entirety\n+\t\t\tif (!state.cross_product.ScanLHS()) {\n+\t\t\t\t// set the match flags in the LHS\n+\t\t\t\tstate.left_outer.SetMatches(state.match_sel, result_count);\n+\t\t\t\t// set the match flag in the RHS\n+\t\t\t\tgstate.right_outer.SetMatch(state.cross_product.ScanPosition() + state.cross_product.PositionInChunk());\n+\t\t\t} else {\n+\t\t\t\t// set the match flag in the LHS\n+\t\t\t\tstate.left_outer.SetMatch(state.cross_product.PositionInChunk());\n+\t\t\t\t// set the match flags in the RHS\n+\t\t\t\tgstate.right_outer.SetMatches(state.match_sel, result_count, state.cross_product.ScanPosition());\n \t\t\t}\n-\t\t\tchunk.Slice(match_sel, result_count);\n+\t\t\tchunk.Slice(state.match_sel, result_count);\n \t\t} else {\n \t\t\t// no result: reset the chunk\n \t\t\tchunk.Reset();\n \t\t}\n-\t\t// move to the next tuple on the LHS\n-\t\tstate.left_position++;\n-\t\tif (state.left_position >= input.size()) {\n-\t\t\t// exhausted the current chunk, move to the next RHS chunk\n-\t\t\tstate.right_position++;\n-\t\t\tif (state.right_position < gstate.right_chunks.ChunkCount()) {\n-\t\t\t\t// we still have chunks left! start over on the LHS\n-\t\t\t\tstate.left_position = 0;\n-\t\t\t}\n-\t\t}\n \t} while (result_count == 0);\n \treturn OperatorResultType::HAVE_MORE_OUTPUT;\n }\n@@ -187,39 +160,54 @@ string PhysicalBlockwiseNLJoin::ParamsToString() const {\n //===--------------------------------------------------------------------===//\n // Source\n //===--------------------------------------------------------------------===//\n-class BlockwiseNLJoinScanState : public GlobalSourceState {\n+class BlockwiseNLJoinGlobalScanState : public GlobalSourceState {\n public:\n-\texplicit BlockwiseNLJoinScanState(const PhysicalBlockwiseNLJoin &op) : op(op), right_outer_position(0) {\n+\texplicit BlockwiseNLJoinGlobalScanState(const PhysicalBlockwiseNLJoin &op) : op(op) {\n+\t\tD_ASSERT(op.sink_state);\n+\t\tauto &sink = (BlockwiseNLJoinGlobalState &)*op.sink_state;\n+\t\tsink.right_outer.InitializeScan(sink.right_chunks, scan_state);\n \t}\n \n-\tmutex lock;\n \tconst PhysicalBlockwiseNLJoin &op;\n-\t//! The position in the RHS in the final scan of the FULL OUTER JOIN\n-\tidx_t right_outer_position;\n+\tOuterJoinGlobalScanState scan_state;\n \n public:\n \tidx_t MaxThreads() override {\n \t\tauto &sink = (BlockwiseNLJoinGlobalState &)*op.sink_state;\n-\t\treturn sink.right_chunks.Count() / (STANDARD_VECTOR_SIZE * 10);\n+\t\treturn sink.right_outer.MaxThreads();\n+\t}\n+};\n+\n+class BlockwiseNLJoinLocalScanState : public LocalSourceState {\n+public:\n+\texplicit BlockwiseNLJoinLocalScanState(const PhysicalBlockwiseNLJoin &op, BlockwiseNLJoinGlobalScanState &gstate) {\n+\t\tD_ASSERT(op.sink_state);\n+\t\tauto &sink = (BlockwiseNLJoinGlobalState &)*op.sink_state;\n+\t\tsink.right_outer.InitializeScan(gstate.scan_state, scan_state);\n \t}\n+\n+\tOuterJoinLocalScanState scan_state;\n };\n \n unique_ptr<GlobalSourceState> PhysicalBlockwiseNLJoin::GetGlobalSourceState(ClientContext &context) const {\n-\treturn make_unique<BlockwiseNLJoinScanState>(*this);\n+\treturn make_unique<BlockwiseNLJoinGlobalScanState>(*this);\n+}\n+\n+unique_ptr<LocalSourceState> PhysicalBlockwiseNLJoin::GetLocalSourceState(ExecutionContext &context,\n+                                                                          GlobalSourceState &gstate) const {\n+\treturn make_unique<BlockwiseNLJoinLocalScanState>(*this, (BlockwiseNLJoinGlobalScanState &)gstate);\n }\n \n-void PhysicalBlockwiseNLJoin::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,\n-                                      LocalSourceState &lstate) const {\n+void PhysicalBlockwiseNLJoin::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate_p,\n+                                      LocalSourceState &lstate_p) const {\n \tD_ASSERT(IsRightOuterJoin(join_type));\n \t// check if we need to scan any unmatched tuples from the RHS for the full/right outer join\n \tauto &sink = (BlockwiseNLJoinGlobalState &)*sink_state;\n-\tauto &state = (BlockwiseNLJoinScanState &)gstate;\n+\tauto &gstate = (BlockwiseNLJoinGlobalScanState &)gstate_p;\n+\tauto &lstate = (BlockwiseNLJoinLocalScanState &)lstate_p;\n \n-\t// if the LHS is exhausted in a FULL/RIGHT OUTER JOIN, we scan the found_match for any chunks we\n-\t// still need to output\n-\tlock_guard<mutex> l(state.lock);\n-\tPhysicalComparisonJoin::ConstructFullOuterJoinResult(sink.rhs_found_match.get(), sink.right_chunks, chunk,\n-\t                                                     state.right_outer_position);\n+\t// if the LHS is exhausted in a FULL/RIGHT OUTER JOIN, we scan chunks we still need to output\n+\tsink.right_outer.Scan(gstate.scan_state, lstate.scan_state, chunk);\n }\n \n } // namespace duckdb\ndiff --git a/src/execution/operator/join/physical_comparison_join.cpp b/src/execution/operator/join/physical_comparison_join.cpp\nindex c13a971f0b41..092a5edc0423 100644\n--- a/src/execution/operator/join/physical_comparison_join.cpp\n+++ b/src/execution/operator/join/physical_comparison_join.cpp\n@@ -77,35 +77,4 @@ void PhysicalComparisonJoin::ConstructEmptyJoinResult(JoinType join_type, bool h\n \t\t}\n \t}\n }\n-\n-void PhysicalComparisonJoin::ConstructFullOuterJoinResult(bool *found_match, ChunkCollection &input, DataChunk &result,\n-                                                          idx_t &scan_position) {\n-\t// fill in NULL values for the LHS\n-\tSelectionVector rsel(STANDARD_VECTOR_SIZE);\n-\twhile (scan_position < input.Count()) {\n-\t\tauto &rhs_chunk = input.GetChunk(scan_position / STANDARD_VECTOR_SIZE);\n-\t\tidx_t result_count = 0;\n-\t\t// figure out which tuples didn't find a match in the RHS\n-\t\tfor (idx_t i = 0; i < rhs_chunk.size(); i++) {\n-\t\t\tif (!found_match[scan_position + i]) {\n-\t\t\t\trsel.set_index(result_count++, i);\n-\t\t\t}\n-\t\t}\n-\t\tscan_position += STANDARD_VECTOR_SIZE;\n-\t\tif (result_count > 0) {\n-\t\t\t// if there were any tuples that didn't find a match, output them\n-\t\t\tidx_t left_column_count = result.ColumnCount() - input.ColumnCount();\n-\t\t\tfor (idx_t i = 0; i < left_column_count; i++) {\n-\t\t\t\tresult.data[i].SetVectorType(VectorType::CONSTANT_VECTOR);\n-\t\t\t\tConstantVector::SetNull(result.data[i], true);\n-\t\t\t}\n-\t\t\tfor (idx_t col_idx = 0; col_idx < rhs_chunk.ColumnCount(); col_idx++) {\n-\t\t\t\tresult.data[left_column_count + col_idx].Slice(rhs_chunk.data[col_idx], rsel, result_count);\n-\t\t\t}\n-\t\t\tresult.SetCardinality(result_count);\n-\t\t\treturn;\n-\t\t}\n-\t}\n-}\n-\n } // namespace duckdb\ndiff --git a/src/execution/operator/join/physical_cross_product.cpp b/src/execution/operator/join/physical_cross_product.cpp\nindex fe33481202ec..dd65393eea24 100644\n--- a/src/execution/operator/join/physical_cross_product.cpp\n+++ b/src/execution/operator/join/physical_cross_product.cpp\n@@ -1,5 +1,5 @@\n #include \"duckdb/execution/operator/join/physical_cross_product.hpp\"\n-\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n #include \"duckdb/common/vector_operations/vector_operations.hpp\"\n #include \"duckdb/execution/operator/join/physical_join.hpp\"\n \n@@ -17,78 +17,121 @@ PhysicalCrossProduct::PhysicalCrossProduct(vector<LogicalType> types, unique_ptr\n //===--------------------------------------------------------------------===//\n class CrossProductGlobalState : public GlobalSinkState {\n public:\n-\texplicit CrossProductGlobalState(ClientContext &context) : rhs_materialized(BufferAllocator::Get(context)) {\n+\texplicit CrossProductGlobalState(ClientContext &context, const PhysicalCrossProduct &op)\n+\t    : rhs_materialized(context, op.children[1]->GetTypes()) {\n+\t\trhs_materialized.InitializeAppend(append_state);\n \t}\n \n-\tChunkCollection rhs_materialized;\n+\tColumnDataCollection rhs_materialized;\n+\tColumnDataAppendState append_state;\n \tmutex rhs_lock;\n };\n \n unique_ptr<GlobalSinkState> PhysicalCrossProduct::GetGlobalSinkState(ClientContext &context) const {\n-\treturn make_unique<CrossProductGlobalState>(context);\n+\treturn make_unique<CrossProductGlobalState>(context, *this);\n }\n \n SinkResultType PhysicalCrossProduct::Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate_p,\n                                           DataChunk &input) const {\n \tauto &sink = (CrossProductGlobalState &)state;\n \tlock_guard<mutex> client_guard(sink.rhs_lock);\n-\tsink.rhs_materialized.Append(input);\n+\tsink.rhs_materialized.Append(sink.append_state, input);\n \treturn SinkResultType::NEED_MORE_INPUT;\n }\n \n //===--------------------------------------------------------------------===//\n // Operator\n //===--------------------------------------------------------------------===//\n-class CrossProductOperatorState : public OperatorState {\n-public:\n-\tCrossProductOperatorState() : right_position(0) {\n-\t}\n-\n-\tidx_t right_position;\n-};\n+CrossProductExecutor::CrossProductExecutor(ColumnDataCollection &rhs)\n+    : rhs(rhs), position_in_chunk(0), initialized(false), finished(false) {\n+\trhs.InitializeScanChunk(scan_chunk);\n+}\n \n-unique_ptr<OperatorState> PhysicalCrossProduct::GetOperatorState(ExecutionContext &context) const {\n-\treturn make_unique<CrossProductOperatorState>();\n+void CrossProductExecutor::Reset(DataChunk &input, DataChunk &output) {\n+\tinitialized = true;\n+\tfinished = false;\n+\tscan_input_chunk = false;\n+\trhs.InitializeScan(scan_state);\n+\tposition_in_chunk = 0;\n+\tscan_chunk.Reset();\n }\n \n-OperatorResultType PhysicalCrossProduct::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,\n-                                                 GlobalOperatorState &gstate, OperatorState &state_p) const {\n-\tauto &state = (CrossProductOperatorState &)state_p;\n-\tauto &sink = (CrossProductGlobalState &)*sink_state;\n-\tauto &right_collection = sink.rhs_materialized;\n+bool CrossProductExecutor::NextValue(DataChunk &input, DataChunk &output) {\n+\tif (!initialized) {\n+\t\t// not initialized yet: initialize the scan\n+\t\tReset(input, output);\n+\t}\n+\tposition_in_chunk++;\n+\tidx_t chunk_size = scan_input_chunk ? input.size() : scan_chunk.size();\n+\tif (position_in_chunk < chunk_size) {\n+\t\treturn true;\n+\t}\n+\t// fetch the next chunk\n+\trhs.Scan(scan_state, scan_chunk);\n+\tposition_in_chunk = 0;\n+\tif (scan_chunk.size() == 0) {\n+\t\treturn false;\n+\t}\n+\t// the way the cross product works is that we keep one chunk constantly referenced\n+\t// while iterating over the other chunk one value at a time\n+\t// the second one is the chunk we are \"scanning\"\n+\n+\t// for the engine, it is better if we emit larger chunks\n+\t// hence the chunk that we keep constantly referenced should be the larger of the two\n+\tscan_input_chunk = input.size() < scan_chunk.size();\n+\treturn true;\n+}\n \n-\tif (sink.rhs_materialized.Count() == 0) {\n+OperatorResultType CrossProductExecutor::Execute(DataChunk &input, DataChunk &output) {\n+\tif (rhs.Count() == 0) {\n \t\t// no RHS: empty result\n \t\treturn OperatorResultType::FINISHED;\n \t}\n-\tif (state.right_position >= right_collection.Count()) {\n+\tif (!NextValue(input, output)) {\n \t\t// ran out of entries on the RHS\n \t\t// reset the RHS and move to the next chunk on the LHS\n-\t\tstate.right_position = 0;\n+\t\tinitialized = false;\n \t\treturn OperatorResultType::NEED_MORE_INPUT;\n \t}\n \n-\tauto &left_chunk = input;\n-\t// now match the current vector of the left relation with the current row\n-\t// from the right relation\n-\tchunk.SetCardinality(left_chunk.size());\n-\t// create a reference to the vectors of the left column\n-\tfor (idx_t i = 0; i < left_chunk.ColumnCount(); i++) {\n-\t\tchunk.data[i].Reference(left_chunk.data[i]);\n-\t}\n-\t// duplicate the values on the right side\n-\tauto &right_chunk = right_collection.GetChunkForRow(state.right_position);\n-\tauto row_in_chunk = state.right_position % STANDARD_VECTOR_SIZE;\n-\tfor (idx_t i = 0; i < right_collection.ColumnCount(); i++) {\n-\t\tConstantVector::Reference(chunk.data[left_chunk.ColumnCount() + i], right_chunk.data[i], row_in_chunk,\n-\t\t                          right_chunk.size());\n+\t// set up the constant chunk\n+\tauto &constant_chunk = scan_input_chunk ? scan_chunk : input;\n+\tauto col_count = constant_chunk.ColumnCount();\n+\tauto col_offset = scan_input_chunk ? input.ColumnCount() : 0;\n+\toutput.SetCardinality(constant_chunk.size());\n+\tfor (idx_t i = 0; i < col_count; i++) {\n+\t\toutput.data[col_offset + i].Reference(constant_chunk.data[i]);\n \t}\n \n-\t// for the next iteration, move to the next position on the right side\n-\tstate.right_position++;\n+\t// for the chunk that we are scanning, scan a single value from that chunk\n+\tauto &scan = scan_input_chunk ? input : scan_chunk;\n+\tcol_count = scan.ColumnCount();\n+\tcol_offset = scan_input_chunk ? 0 : input.ColumnCount();\n+\tfor (idx_t i = 0; i < col_count; i++) {\n+\t\tConstantVector::Reference(output.data[col_offset + i], scan.data[i], position_in_chunk, scan.size());\n+\t}\n \treturn OperatorResultType::HAVE_MORE_OUTPUT;\n }\n \n+class CrossProductOperatorState : public OperatorState {\n+public:\n+\texplicit CrossProductOperatorState(ColumnDataCollection &rhs) : executor(rhs) {\n+\t}\n+\n+\tCrossProductExecutor executor;\n+};\n+\n+unique_ptr<OperatorState> PhysicalCrossProduct::GetOperatorState(ExecutionContext &context) const {\n+\tauto &sink = (CrossProductGlobalState &)*sink_state;\n+\treturn make_unique<CrossProductOperatorState>(sink.rhs_materialized);\n+}\n+\n+OperatorResultType PhysicalCrossProduct::Execute(ExecutionContext &context, DataChunk &input, DataChunk &chunk,\n+                                                 GlobalOperatorState &gstate, OperatorState &state_p) const {\n+\tauto &state = (CrossProductOperatorState &)state_p;\n+\treturn state.executor.Execute(input, chunk);\n+}\n+\n //===--------------------------------------------------------------------===//\n // Pipeline Construction\n //===--------------------------------------------------------------------===//\ndiff --git a/src/execution/operator/join/physical_delim_join.cpp b/src/execution/operator/join/physical_delim_join.cpp\nindex 4617f7776f40..345d9c1253bb 100644\n--- a/src/execution/operator/join/physical_delim_join.cpp\n+++ b/src/execution/operator/join/physical_delim_join.cpp\n@@ -1,11 +1,12 @@\n #include \"duckdb/execution/operator/join/physical_delim_join.hpp\"\n \n #include \"duckdb/common/vector_operations/vector_operations.hpp\"\n-#include \"duckdb/execution/operator/scan/physical_chunk_scan.hpp\"\n #include \"duckdb/execution/operator/aggregate/physical_hash_aggregate.hpp\"\n #include \"duckdb/execution/operator/set/physical_recursive_cte.hpp\"\n #include \"duckdb/parallel/thread_context.hpp\"\n #include \"duckdb/parallel/pipeline.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n+#include \"duckdb/execution/operator/scan/physical_column_data_scan.hpp\"\n \n namespace duckdb {\n \n@@ -18,10 +19,10 @@ PhysicalDelimJoin::PhysicalDelimJoin(vector<LogicalType> types, unique_ptr<Physi\n \t// we take its left child, this is the side that we will duplicate eliminate\n \tchildren.push_back(move(join->children[0]));\n \n-\t// we replace it with a PhysicalChunkCollectionScan, that scans the ChunkCollection that we keep cached\n+\t// we replace it with a PhysicalColumnDataScan, that scans the ColumnDataCollection that we keep cached\n \t// the actual chunk collection to scan will be created in the DelimJoinGlobalState\n-\tauto cached_chunk_scan = make_unique<PhysicalChunkScan>(children[0]->GetTypes(), PhysicalOperatorType::CHUNK_SCAN,\n-\t                                                        estimated_cardinality);\n+\tauto cached_chunk_scan = make_unique<PhysicalColumnDataScan>(\n+\t    children[0]->GetTypes(), PhysicalOperatorType::COLUMN_DATA_SCAN, estimated_cardinality);\n \tjoin->children[0] = move(cached_chunk_scan);\n }\n \n@@ -40,29 +41,33 @@ vector<PhysicalOperator *> PhysicalDelimJoin::GetChildren() const {\n //===--------------------------------------------------------------------===//\n class DelimJoinGlobalState : public GlobalSinkState {\n public:\n-\texplicit DelimJoinGlobalState(Allocator &allocator, const PhysicalDelimJoin *delim_join) : lhs_data(allocator) {\n-\t\tD_ASSERT(delim_join->delim_scans.size() > 0);\n+\texplicit DelimJoinGlobalState(ClientContext &context, const PhysicalDelimJoin &delim_join)\n+\t    : lhs_data(context, delim_join.children[0]->GetTypes()) {\n+\t\tD_ASSERT(delim_join.delim_scans.size() > 0);\n \t\t// set up the delim join chunk to scan in the original join\n-\t\tauto &cached_chunk_scan = (PhysicalChunkScan &)*delim_join->join->children[0];\n+\t\tauto &cached_chunk_scan = (PhysicalColumnDataScan &)*delim_join.join->children[0];\n \t\tcached_chunk_scan.collection = &lhs_data;\n \t}\n \n-\tChunkCollection lhs_data;\n+\tColumnDataCollection lhs_data;\n \tmutex lhs_lock;\n \n-\tvoid Merge(ChunkCollection &input) {\n+\tvoid Merge(ColumnDataCollection &input) {\n \t\tlock_guard<mutex> guard(lhs_lock);\n-\t\tlhs_data.Append(input);\n+\t\tlhs_data.Combine(input);\n \t}\n };\n \n class DelimJoinLocalState : public LocalSinkState {\n public:\n-\texplicit DelimJoinLocalState(Allocator &allocator) : lhs_data(allocator) {\n+\texplicit DelimJoinLocalState(ClientContext &context, const PhysicalDelimJoin &delim_join)\n+\t    : lhs_data(context, delim_join.children[0]->GetTypes()) {\n+\t\tlhs_data.InitializeAppend(append_state);\n \t}\n \n \tunique_ptr<LocalSinkState> distinct_state;\n-\tChunkCollection lhs_data;\n+\tColumnDataCollection lhs_data;\n+\tColumnDataAppendState append_state;\n \n \tvoid Append(DataChunk &input) {\n \t\tlhs_data.Append(input);\n@@ -70,7 +75,7 @@ class DelimJoinLocalState : public LocalSinkState {\n };\n \n unique_ptr<GlobalSinkState> PhysicalDelimJoin::GetGlobalSinkState(ClientContext &context) const {\n-\tauto state = make_unique<DelimJoinGlobalState>(BufferAllocator::Get(context), this);\n+\tauto state = make_unique<DelimJoinGlobalState>(context, *this);\n \tdistinct->sink_state = distinct->GetGlobalSinkState(context);\n \tif (delim_scans.size() > 1) {\n \t\tPhysicalHashAggregate::SetMultiScan(*distinct->sink_state);\n@@ -79,7 +84,7 @@ unique_ptr<GlobalSinkState> PhysicalDelimJoin::GetGlobalSinkState(ClientContext\n }\n \n unique_ptr<LocalSinkState> PhysicalDelimJoin::GetLocalSinkState(ExecutionContext &context) const {\n-\tauto state = make_unique<DelimJoinLocalState>(Allocator::Get(context.client));\n+\tauto state = make_unique<DelimJoinLocalState>(context.client, *this);\n \tstate->distinct_state = distinct->GetLocalSinkState(context);\n \treturn move(state);\n }\n@@ -87,7 +92,7 @@ unique_ptr<LocalSinkState> PhysicalDelimJoin::GetLocalSinkState(ExecutionContext\n SinkResultType PhysicalDelimJoin::Sink(ExecutionContext &context, GlobalSinkState &state_p, LocalSinkState &lstate_p,\n                                        DataChunk &input) const {\n \tauto &lstate = (DelimJoinLocalState &)lstate_p;\n-\tlstate.lhs_data.Append(input);\n+\tlstate.lhs_data.Append(lstate.append_state, input);\n \tdistinct->Sink(context, *distinct->sink_state, *lstate.distinct_state, input);\n \treturn SinkResultType::NEED_MORE_INPUT;\n }\ndiff --git a/src/execution/operator/join/physical_nested_loop_join.cpp b/src/execution/operator/join/physical_nested_loop_join.cpp\nindex b00dd57bc513..b906e7e25611 100644\n--- a/src/execution/operator/join/physical_nested_loop_join.cpp\n+++ b/src/execution/operator/join/physical_nested_loop_join.cpp\n@@ -5,6 +5,7 @@\n #include \"duckdb/execution/expression_executor.hpp\"\n #include \"duckdb/execution/nested_loop_join.hpp\"\n #include \"duckdb/main/client_context.hpp\"\n+#include \"duckdb/execution/operator/join/outer_join_marker.hpp\"\n \n namespace duckdb {\n \n@@ -138,21 +139,30 @@ class NestedLoopJoinLocalState : public LocalSinkState {\n \n class NestedLoopJoinGlobalState : public GlobalSinkState {\n public:\n-\texplicit NestedLoopJoinGlobalState(Allocator &allocator)\n-\t    : right_data(allocator), right_chunks(allocator), has_null(false) {\n+\texplicit NestedLoopJoinGlobalState(ClientContext &context, const PhysicalNestedLoopJoin &op)\n+\t    : right_payload_data(context, op.children[1]->types), right_condition_data(context, op.GetJoinTypes()),\n+\t      has_null(false), right_outer(IsRightOuterJoin(op.join_type)) {\n \t}\n \n \tmutex nj_lock;\n \t//! Materialized data of the RHS\n-\tChunkCollection right_data;\n+\tColumnDataCollection right_payload_data;\n \t//! Materialized join condition of the RHS\n-\tChunkCollection right_chunks;\n+\tColumnDataCollection right_condition_data;\n \t//! Whether or not the RHS of the nested loop join has NULL values\n \tbool has_null;\n \t//! A bool indicating for each tuple in the RHS if they found a match (only used in FULL OUTER JOIN)\n-\tunique_ptr<bool[]> right_found_match;\n+\tOuterJoinMarker right_outer;\n };\n \n+vector<LogicalType> PhysicalNestedLoopJoin::GetJoinTypes() const {\n+\tvector<LogicalType> result;\n+\tfor (auto &op : conditions) {\n+\t\tresult.push_back(op.right->return_type);\n+\t}\n+\treturn result;\n+}\n+\n SinkResultType PhysicalNestedLoopJoin::Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate,\n                                             DataChunk &input) const {\n \tauto &gstate = (NestedLoopJoinGlobalState &)state;\n@@ -170,10 +180,10 @@ SinkResultType PhysicalNestedLoopJoin::Sink(ExecutionContext &context, GlobalSin\n \t\t}\n \t}\n \n-\t// append the data and the\n+\t// append the payload data and the conditions\n \tlock_guard<mutex> nj_guard(gstate.nj_lock);\n-\tgstate.right_data.Append(input);\n-\tgstate.right_chunks.Append(nlj_state.right_condition);\n+\tgstate.right_payload_data.Append(input);\n+\tgstate.right_condition_data.Append(nlj_state.right_condition);\n \treturn SinkResultType::NEED_MORE_INPUT;\n }\n \n@@ -188,19 +198,15 @@ void PhysicalNestedLoopJoin::Combine(ExecutionContext &context, GlobalSinkState\n SinkFinalizeType PhysicalNestedLoopJoin::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,\n                                                   GlobalSinkState &gstate_p) const {\n \tauto &gstate = (NestedLoopJoinGlobalState &)gstate_p;\n-\tif (join_type == JoinType::OUTER || join_type == JoinType::RIGHT) {\n-\t\t// for FULL/RIGHT OUTER JOIN, initialize found_match to false for every tuple\n-\t\tgstate.right_found_match = unique_ptr<bool[]>(new bool[gstate.right_data.Count()]);\n-\t\tmemset(gstate.right_found_match.get(), 0, sizeof(bool) * gstate.right_data.Count());\n-\t}\n-\tif (gstate.right_chunks.Count() == 0 && EmptyResultIfRHSIsEmpty()) {\n+\tgstate.right_outer.Initialize(gstate.right_payload_data.Count());\n+\tif (gstate.right_payload_data.Count() == 0 && EmptyResultIfRHSIsEmpty()) {\n \t\treturn SinkFinalizeType::NO_OUTPUT_POSSIBLE;\n \t}\n \treturn SinkFinalizeType::READY;\n }\n \n unique_ptr<GlobalSinkState> PhysicalNestedLoopJoin::GetGlobalSinkState(ClientContext &context) const {\n-\treturn make_unique<NestedLoopJoinGlobalState>(Allocator::Get(context));\n+\treturn make_unique<NestedLoopJoinGlobalState>(context, *this);\n }\n \n unique_ptr<LocalSinkState> PhysicalNestedLoopJoin::GetLocalSinkState(ExecutionContext &context) const {\n@@ -214,31 +220,34 @@ class PhysicalNestedLoopJoinState : public OperatorState {\n public:\n \tPhysicalNestedLoopJoinState(Allocator &allocator, const PhysicalNestedLoopJoin &op,\n \t                            const vector<JoinCondition> &conditions)\n-\t    : fetch_next_left(true), fetch_next_right(false), right_chunk(0), lhs_executor(allocator), left_tuple(0),\n-\t      right_tuple(0) {\n+\t    : fetch_next_left(true), fetch_next_right(false), lhs_executor(allocator), left_tuple(0), right_tuple(0),\n+\t      left_outer(IsLeftOuterJoin(op.join_type)) {\n \t\tvector<LogicalType> condition_types;\n \t\tfor (auto &cond : conditions) {\n \t\t\tlhs_executor.AddExpression(*cond.left);\n \t\t\tcondition_types.push_back(cond.left->return_type);\n \t\t}\n \t\tleft_condition.Initialize(allocator, condition_types);\n-\t\tif (IsLeftOuterJoin(op.join_type)) {\n-\t\t\tleft_found_match = unique_ptr<bool[]>(new bool[STANDARD_VECTOR_SIZE]);\n-\t\t\tmemset(left_found_match.get(), 0, sizeof(bool) * STANDARD_VECTOR_SIZE);\n-\t\t}\n+\t\tright_condition.Initialize(allocator, condition_types);\n+\t\tright_payload.Initialize(allocator, op.children[1]->GetTypes());\n+\t\tleft_outer.Initialize(STANDARD_VECTOR_SIZE);\n \t}\n \n \tbool fetch_next_left;\n \tbool fetch_next_right;\n-\tidx_t right_chunk;\n \tDataChunk left_condition;\n \t//! The executor of the LHS condition\n \tExpressionExecutor lhs_executor;\n \n+\tColumnDataScanState condition_scan_state;\n+\tColumnDataScanState payload_scan_state;\n+\tDataChunk right_condition;\n+\tDataChunk right_payload;\n+\n \tidx_t left_tuple;\n \tidx_t right_tuple;\n \n-\tunique_ptr<bool[]> left_found_match;\n+\tOuterJoinMarker left_outer;\n \n public:\n \tvoid Finalize(PhysicalOperator *op, ExecutionContext &context) override {\n@@ -254,7 +263,7 @@ OperatorResultType PhysicalNestedLoopJoin::Execute(ExecutionContext &context, Da\n                                                    GlobalOperatorState &gstate_p, OperatorState &state_p) const {\n \tauto &gstate = (NestedLoopJoinGlobalState &)*sink_state;\n \n-\tif (gstate.right_chunks.Count() == 0) {\n+\tif (gstate.right_payload_data.Count() == 0) {\n \t\t// empty RHS\n \t\tif (!EmptyResultIfRHSIsEmpty()) {\n \t\t\tConstructEmptyJoinResult(join_type, gstate.has_null, input, chunk);\n@@ -290,7 +299,7 @@ void PhysicalNestedLoopJoin::ResolveSimpleJoin(ExecutionContext &context, DataCh\n \tstate.lhs_executor.Execute(input, state.left_condition);\n \n \tbool found_match[STANDARD_VECTOR_SIZE] = {false};\n-\tNestedLoopJoinMark::Perform(state.left_condition, gstate.right_chunks, found_match, conditions);\n+\tNestedLoopJoinMark::Perform(state.left_condition, gstate.right_condition_data, found_match, conditions);\n \tswitch (join_type) {\n \tcase JoinType::MARK:\n \t\t// now construct the mark join result from the found matches\n@@ -309,23 +318,6 @@ void PhysicalNestedLoopJoin::ResolveSimpleJoin(ExecutionContext &context, DataCh\n \t}\n }\n \n-void PhysicalJoin::ConstructLeftJoinResult(DataChunk &left, DataChunk &result, bool found_match[]) {\n-\tSelectionVector remaining_sel(STANDARD_VECTOR_SIZE);\n-\tidx_t remaining_count = 0;\n-\tfor (idx_t i = 0; i < left.size(); i++) {\n-\t\tif (!found_match[i]) {\n-\t\t\tremaining_sel.set_index(remaining_count++, i);\n-\t\t}\n-\t}\n-\tif (remaining_count > 0) {\n-\t\tresult.Slice(left, remaining_sel, remaining_count);\n-\t\tfor (idx_t idx = left.ColumnCount(); idx < result.ColumnCount(); idx++) {\n-\t\t\tresult.data[idx].SetVectorType(VectorType::CONSTANT_VECTOR);\n-\t\t\tConstantVector::SetNull(result.data[idx], true);\n-\t\t}\n-\t}\n-}\n-\n OperatorResultType PhysicalNestedLoopJoin::ResolveComplexJoin(ExecutionContext &context, DataChunk &input,\n                                                               DataChunk &chunk, OperatorState &state_p) const {\n \tauto &state = (PhysicalNestedLoopJoinState &)state_p;\n@@ -335,19 +327,25 @@ OperatorResultType PhysicalNestedLoopJoin::ResolveComplexJoin(ExecutionContext &\n \tdo {\n \t\tif (state.fetch_next_right) {\n \t\t\t// we exhausted the chunk on the right: move to the next chunk on the right\n-\t\t\tstate.right_chunk++;\n \t\t\tstate.left_tuple = 0;\n \t\t\tstate.right_tuple = 0;\n \t\t\tstate.fetch_next_right = false;\n \t\t\t// check if we exhausted all chunks on the RHS\n-\t\t\tif (state.right_chunk >= gstate.right_chunks.ChunkCount()) {\n-\t\t\t\tstate.fetch_next_left = true;\n+\t\t\tif (gstate.right_condition_data.Scan(state.condition_scan_state, state.right_condition)) {\n+\t\t\t\tif (!gstate.right_payload_data.Scan(state.payload_scan_state, state.right_payload)) {\n+\t\t\t\t\tthrow InternalException(\"Nested loop join: payload and conditions are unaligned!?\");\n+\t\t\t\t}\n+\t\t\t\tif (state.right_condition.size() != state.right_payload.size()) {\n+\t\t\t\t\tthrow InternalException(\"Nested loop join: payload and conditions are unaligned!?\");\n+\t\t\t\t}\n+\t\t\t} else {\n \t\t\t\t// we exhausted all chunks on the right: move to the next chunk on the left\n-\t\t\t\tif (IsLeftOuterJoin(join_type)) {\n+\t\t\t\tstate.fetch_next_left = true;\n+\t\t\t\tif (state.left_outer.Enabled()) {\n \t\t\t\t\t// left join: before we move to the next chunk, see if we need to output any vectors that didn't\n \t\t\t\t\t// have a match found\n-\t\t\t\t\tPhysicalJoin::ConstructLeftJoinResult(input, chunk, state.left_found_match.get());\n-\t\t\t\t\tmemset(state.left_found_match.get(), 0, sizeof(bool) * STANDARD_VECTOR_SIZE);\n+\t\t\t\t\tstate.left_outer.ConstructLeftJoinResult(input, chunk);\n+\t\t\t\t\tstate.left_outer.Reset();\n \t\t\t\t}\n \t\t\t\treturn OperatorResultType::NEED_MORE_INPUT;\n \t\t\t}\n@@ -359,44 +357,41 @@ OperatorResultType PhysicalNestedLoopJoin::ResolveComplexJoin(ExecutionContext &\n \n \t\t\tstate.left_tuple = 0;\n \t\t\tstate.right_tuple = 0;\n-\t\t\tstate.right_chunk = 0;\n+\t\t\tgstate.right_condition_data.InitializeScan(state.condition_scan_state);\n+\t\t\tgstate.right_condition_data.Scan(state.condition_scan_state, state.right_condition);\n+\n+\t\t\tgstate.right_payload_data.InitializeScan(state.payload_scan_state);\n+\t\t\tgstate.right_payload_data.Scan(state.payload_scan_state, state.right_payload);\n \t\t\tstate.fetch_next_left = false;\n \t\t}\n \t\t// now we have a left and a right chunk that we can join together\n \t\t// note that we only get here in the case of a LEFT, INNER or FULL join\n \t\tauto &left_chunk = input;\n-\t\tauto &right_chunk = gstate.right_chunks.GetChunk(state.right_chunk);\n-\t\tauto &right_data = gstate.right_data.GetChunk(state.right_chunk);\n+\t\tauto &right_condition = state.right_condition;\n+\t\tauto &right_payload = state.right_payload;\n \n \t\t// sanity check\n \t\tleft_chunk.Verify();\n-\t\tright_chunk.Verify();\n-\t\tright_data.Verify();\n+\t\tright_condition.Verify();\n+\t\tright_payload.Verify();\n \n \t\t// now perform the join\n \t\tSelectionVector lvector(STANDARD_VECTOR_SIZE), rvector(STANDARD_VECTOR_SIZE);\n \t\tmatch_count = NestedLoopJoinInner::Perform(state.left_tuple, state.right_tuple, state.left_condition,\n-\t\t                                           right_chunk, lvector, rvector, conditions);\n+\t\t                                           right_condition, lvector, rvector, conditions);\n \t\t// we have finished resolving the join conditions\n \t\tif (match_count > 0) {\n \t\t\t// we have matching tuples!\n \t\t\t// construct the result\n-\t\t\tif (state.left_found_match) {\n-\t\t\t\tfor (idx_t i = 0; i < match_count; i++) {\n-\t\t\t\t\tstate.left_found_match[lvector.get_index(i)] = true;\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tif (gstate.right_found_match) {\n-\t\t\t\tfor (idx_t i = 0; i < match_count; i++) {\n-\t\t\t\t\tgstate.right_found_match[state.right_chunk * STANDARD_VECTOR_SIZE + rvector.get_index(i)] = true;\n-\t\t\t\t}\n-\t\t\t}\n+\t\t\tstate.left_outer.SetMatches(lvector, match_count);\n+\t\t\tgstate.right_outer.SetMatches(rvector, match_count, state.condition_scan_state.current_row_index);\n+\n \t\t\tchunk.Slice(input, lvector, match_count);\n-\t\t\tchunk.Slice(right_data, rvector, match_count, input.ColumnCount());\n+\t\t\tchunk.Slice(right_payload, rvector, match_count, input.ColumnCount());\n \t\t}\n \n \t\t// check if we exhausted the RHS, if we did we need to move to the next right chunk in the next iteration\n-\t\tif (state.right_tuple >= right_chunk.size()) {\n+\t\tif (state.right_tuple >= right_condition.size()) {\n \t\t\tstate.fetch_next_right = true;\n \t\t}\n \t} while (match_count == 0);\n@@ -406,37 +401,54 @@ OperatorResultType PhysicalNestedLoopJoin::ResolveComplexJoin(ExecutionContext &\n //===--------------------------------------------------------------------===//\n // Source\n //===--------------------------------------------------------------------===//\n-class NestedLoopJoinScanState : public GlobalSourceState {\n+class NestedLoopJoinGlobalScanState : public GlobalSourceState {\n public:\n-\texplicit NestedLoopJoinScanState(const PhysicalNestedLoopJoin &op) : op(op), right_outer_position(0) {\n+\texplicit NestedLoopJoinGlobalScanState(const PhysicalNestedLoopJoin &op) : op(op) {\n+\t\tD_ASSERT(op.sink_state);\n+\t\tauto &sink = (NestedLoopJoinGlobalState &)*op.sink_state;\n+\t\tsink.right_outer.InitializeScan(sink.right_payload_data, scan_state);\n \t}\n \n-\tmutex lock;\n \tconst PhysicalNestedLoopJoin &op;\n-\tidx_t right_outer_position;\n+\tOuterJoinGlobalScanState scan_state;\n \n public:\n \tidx_t MaxThreads() override {\n \t\tauto &sink = (NestedLoopJoinGlobalState &)*op.sink_state;\n-\t\treturn sink.right_chunks.Count() / (STANDARD_VECTOR_SIZE * 10);\n+\t\treturn sink.right_outer.MaxThreads();\n \t}\n };\n \n+class NestedLoopJoinLocalScanState : public LocalSourceState {\n+public:\n+\texplicit NestedLoopJoinLocalScanState(const PhysicalNestedLoopJoin &op, NestedLoopJoinGlobalScanState &gstate) {\n+\t\tD_ASSERT(op.sink_state);\n+\t\tauto &sink = (NestedLoopJoinGlobalState &)*op.sink_state;\n+\t\tsink.right_outer.InitializeScan(gstate.scan_state, scan_state);\n+\t}\n+\n+\tOuterJoinLocalScanState scan_state;\n+};\n+\n unique_ptr<GlobalSourceState> PhysicalNestedLoopJoin::GetGlobalSourceState(ClientContext &context) const {\n-\treturn make_unique<NestedLoopJoinScanState>(*this);\n+\treturn make_unique<NestedLoopJoinGlobalScanState>(*this);\n+}\n+\n+unique_ptr<LocalSourceState> PhysicalNestedLoopJoin::GetLocalSourceState(ExecutionContext &context,\n+                                                                         GlobalSourceState &gstate) const {\n+\treturn make_unique<NestedLoopJoinLocalScanState>(*this, (NestedLoopJoinGlobalScanState &)gstate);\n }\n \n-void PhysicalNestedLoopJoin::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,\n-                                     LocalSourceState &lstate) const {\n+void PhysicalNestedLoopJoin::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate_p,\n+                                     LocalSourceState &lstate_p) const {\n \tD_ASSERT(IsRightOuterJoin(join_type));\n \t// check if we need to scan any unmatched tuples from the RHS for the full/right outer join\n \tauto &sink = (NestedLoopJoinGlobalState &)*sink_state;\n-\tauto &state = (NestedLoopJoinScanState &)gstate;\n+\tauto &gstate = (NestedLoopJoinGlobalScanState &)gstate_p;\n+\tauto &lstate = (NestedLoopJoinLocalScanState &)lstate_p;\n \n-\t// if the LHS is exhausted in a FULL/RIGHT OUTER JOIN, we scan the found_match for any chunks we\n-\t// still need to output\n-\tlock_guard<mutex> l(state.lock);\n-\tConstructFullOuterJoinResult(sink.right_found_match.get(), sink.right_data, chunk, state.right_outer_position);\n+\t// if the LHS is exhausted in a FULL/RIGHT OUTER JOIN, we scan chunks we still need to output\n+\tsink.right_outer.Scan(gstate.scan_state, lstate.scan_state, chunk);\n }\n \n } // namespace duckdb\ndiff --git a/src/execution/operator/join/physical_piecewise_merge_join.cpp b/src/execution/operator/join/physical_piecewise_merge_join.cpp\nindex ca97a250003f..375ea22d1572 100644\n--- a/src/execution/operator/join/physical_piecewise_merge_join.cpp\n+++ b/src/execution/operator/join/physical_piecewise_merge_join.cpp\n@@ -1,4 +1,5 @@\n #include \"duckdb/execution/operator/join/physical_piecewise_merge_join.hpp\"\n+#include \"duckdb/execution/operator/join/outer_join_marker.hpp\"\n \n #include \"duckdb/common/fast_mem.hpp\"\n #include \"duckdb/common/operator/comparison_operators.hpp\"\n@@ -161,16 +162,13 @@ class PiecewiseMergeJoinState : public OperatorState {\n \texplicit PiecewiseMergeJoinState(Allocator &allocator, const PhysicalPiecewiseMergeJoin &op,\n \t                                 BufferManager &buffer_manager, bool force_external)\n \t    : allocator(allocator), op(op), buffer_manager(buffer_manager), force_external(force_external),\n-\t      left_position(0), first_fetch(true), finished(true), right_position(0), right_chunk_index(0),\n-\t      rhs_executor(allocator) {\n+\t      left_outer(IsLeftOuterJoin(op.join_type)), left_position(0), first_fetch(true), finished(true),\n+\t      right_position(0), right_chunk_index(0), rhs_executor(allocator) {\n \t\tvector<LogicalType> condition_types;\n \t\tfor (auto &order : op.lhs_orders) {\n \t\t\tcondition_types.push_back(order.expression->return_type);\n \t\t}\n-\t\tif (IsLeftOuterJoin(op.join_type)) {\n-\t\t\tlhs_found_match = unique_ptr<bool[]>(new bool[STANDARD_VECTOR_SIZE]);\n-\t\t\tmemset(lhs_found_match.get(), 0, sizeof(bool) * STANDARD_VECTOR_SIZE);\n-\t\t}\n+\t\tleft_outer.Initialize(STANDARD_VECTOR_SIZE);\n \t\tlhs_layout.Initialize(op.children[0]->types);\n \t\tlhs_payload.Initialize(allocator, op.children[0]->types);\n \n@@ -193,7 +191,7 @@ class PiecewiseMergeJoinState : public OperatorState {\n \n \t// Block sorting\n \tDataChunk lhs_payload;\n-\tunique_ptr<bool[]> lhs_found_match;\n+\tOuterJoinMarker left_outer;\n \tvector<BoundOrderByNode> lhs_order;\n \tRowLayout lhs_layout;\n \tunique_ptr<LocalSortedTable> lhs_local_table;\n@@ -525,11 +523,11 @@ OperatorResultType PhysicalPiecewiseMergeJoin::ResolveComplexJoin(ExecutionConte\n \t\t\tstate.finished = false;\n \t\t}\n \t\tif (state.finished) {\n-\t\t\tif (IsLeftOuterJoin(join_type)) {\n+\t\t\tif (state.left_outer.Enabled()) {\n \t\t\t\t// left join: before we move to the next chunk, see if we need to output any vectors that didn't\n \t\t\t\t// have a match found\n-\t\t\t\tPhysicalJoin::ConstructLeftJoinResult(state.lhs_payload, chunk, state.lhs_found_match.get());\n-\t\t\t\tmemset(state.lhs_found_match.get(), 0, sizeof(bool) * STANDARD_VECTOR_SIZE);\n+\t\t\t\tstate.left_outer.ConstructLeftJoinResult(state.lhs_payload, chunk);\n+\t\t\t\tstate.left_outer.Reset();\n \t\t\t}\n \t\t\tstate.first_fetch = true;\n \t\t\tstate.finished = false;\n@@ -601,9 +599,9 @@ OperatorResultType PhysicalPiecewiseMergeJoin::ResolveComplexJoin(ExecutionConte\n \t\t\t}\n \n \t\t\t// found matches: mark the found matches if required\n-\t\t\tif (state.lhs_found_match) {\n+\t\t\tif (state.left_outer.Enabled()) {\n \t\t\t\tfor (idx_t i = 0; i < result_count; i++) {\n-\t\t\t\t\tstate.lhs_found_match[left_info.result[sel->get_index(i)]] = true;\n+\t\t\t\t\tstate.left_outer.SetMatch(left_info.result[sel->get_index(i)]);\n \t\t\t\t}\n \t\t\t}\n \t\t\tif (gstate.table->found_match) {\n@@ -696,8 +694,6 @@ void PhysicalPiecewiseMergeJoin::GetData(ExecutionContext &context, DataChunk &r\n \t// still need to output\n \tconst auto found_match = sink.table->found_match.get();\n \n-\t// ConstructFullOuterJoinResult(sink.table->found_match.get(), sink.right_chunks, chunk,\n-\t// state.right_outer_position);\n \tDataChunk rhs_chunk;\n \trhs_chunk.Initialize(Allocator::Get(context.client), sink.table->global_sort_state.payload_layout.GetTypes());\n \tSelectionVector rsel(STANDARD_VECTOR_SIZE);\ndiff --git a/src/execution/operator/persistent/physical_delete.cpp b/src/execution/operator/persistent/physical_delete.cpp\nindex e35e3c8e1780..dcb35582ed8e 100644\n--- a/src/execution/operator/persistent/physical_delete.cpp\n+++ b/src/execution/operator/persistent/physical_delete.cpp\n@@ -3,6 +3,7 @@\n #include \"duckdb/execution/expression_executor.hpp\"\n #include \"duckdb/storage/data_table.hpp\"\n #include \"duckdb/transaction/transaction.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n \n #include \"duckdb/common/atomic.hpp\"\n \n@@ -13,14 +14,13 @@ namespace duckdb {\n //===--------------------------------------------------------------------===//\n class DeleteGlobalState : public GlobalSinkState {\n public:\n-\texplicit DeleteGlobalState(Allocator &allocator)\n-\t    : deleted_count(0), return_chunk_collection(allocator), returned_chunk_count(0) {\n+\texplicit DeleteGlobalState(ClientContext &context, const vector<LogicalType> &return_types)\n+\t    : deleted_count(0), return_collection(context, return_types) {\n \t}\n \n \tmutex delete_lock;\n \tidx_t deleted_count;\n-\tChunkCollection return_chunk_collection;\n-\tidx_t returned_chunk_count;\n+\tColumnDataCollection return_collection;\n };\n \n class DeleteLocalState : public LocalSinkState {\n@@ -50,7 +50,7 @@ SinkResultType PhysicalDelete::Sink(ExecutionContext &context, GlobalSinkState &\n \tif (return_chunk) {\n \t\trow_identifiers.Flatten(input.size());\n \t\ttable.Fetch(transaction, ustate.delete_chunk, column_ids, row_identifiers, input.size(), cfs);\n-\t\tgstate.return_chunk_collection.Append(ustate.delete_chunk);\n+\t\tgstate.return_collection.Append(ustate.delete_chunk);\n \t}\n \tgstate.deleted_count += table.Delete(tableref, context.client, row_identifiers, input.size());\n \n@@ -58,7 +58,7 @@ SinkResultType PhysicalDelete::Sink(ExecutionContext &context, GlobalSinkState &\n }\n \n unique_ptr<GlobalSinkState> PhysicalDelete::GetGlobalSinkState(ClientContext &context) const {\n-\treturn make_unique<DeleteGlobalState>(Allocator::Get(context));\n+\treturn make_unique<DeleteGlobalState>(context, GetTypes());\n }\n \n unique_ptr<LocalSinkState> PhysicalDelete::GetLocalSinkState(ExecutionContext &context) const {\n@@ -70,14 +70,20 @@ unique_ptr<LocalSinkState> PhysicalDelete::GetLocalSinkState(ExecutionContext &c\n //===--------------------------------------------------------------------===//\n class DeleteSourceState : public GlobalSourceState {\n public:\n-\tDeleteSourceState() : finished(false) {\n+\texplicit DeleteSourceState(const PhysicalDelete &op) : finished(false) {\n+\t\tif (op.return_chunk) {\n+\t\t\tD_ASSERT(op.sink_state);\n+\t\t\tauto &g = (DeleteGlobalState &)*op.sink_state;\n+\t\t\tg.return_collection.InitializeScan(scan_state);\n+\t\t}\n \t}\n \n+\tColumnDataScanState scan_state;\n \tbool finished;\n };\n \n unique_ptr<GlobalSourceState> PhysicalDelete::GetGlobalSourceState(ClientContext &context) const {\n-\treturn make_unique<DeleteSourceState>();\n+\treturn make_unique<DeleteSourceState>(*this);\n }\n \n void PhysicalDelete::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,\n@@ -92,18 +98,10 @@ void PhysicalDelete::GetData(ExecutionContext &context, DataChunk &chunk, Global\n \t\tchunk.SetCardinality(1);\n \t\tchunk.SetValue(0, 0, Value::BIGINT(g.deleted_count));\n \t\tstate.finished = true;\n-\t}\n-\n-\tidx_t chunk_return = g.returned_chunk_count;\n-\tif (chunk_return >= g.return_chunk_collection.Chunks().size()) {\n \t\treturn;\n \t}\n-\tchunk.Reference(g.return_chunk_collection.GetChunk(chunk_return));\n-\tchunk.SetCardinality((g.return_chunk_collection.GetChunk(chunk_return)).size());\n-\tg.returned_chunk_count += 1;\n-\tif (g.returned_chunk_count >= g.return_chunk_collection.Chunks().size()) {\n-\t\tstate.finished = true;\n-\t}\n+\n+\tg.return_collection.Scan(state.scan_state, chunk);\n }\n \n } // namespace duckdb\ndiff --git a/src/execution/operator/persistent/physical_insert.cpp b/src/execution/operator/persistent/physical_insert.cpp\nindex a79db692a937..0610a8dd6848 100644\n--- a/src/execution/operator/persistent/physical_insert.cpp\n+++ b/src/execution/operator/persistent/physical_insert.cpp\n@@ -1,7 +1,7 @@\n #include \"duckdb/execution/operator/persistent/physical_insert.hpp\"\n #include \"duckdb/parallel/thread_context.hpp\"\n #include \"duckdb/catalog/catalog_entry/table_catalog_entry.hpp\"\n-#include \"duckdb/common/types/chunk_collection.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n #include \"duckdb/common/vector_operations/vector_operations.hpp\"\n #include \"duckdb/execution/expression_executor.hpp\"\n #include \"duckdb/storage/data_table.hpp\"\n@@ -14,14 +14,13 @@ namespace duckdb {\n //===--------------------------------------------------------------------===//\n class InsertGlobalState : public GlobalSinkState {\n public:\n-\texplicit InsertGlobalState(Allocator &allocator)\n-\t    : insert_count(0), return_chunk_collection(allocator), returned_chunk_count(0) {\n+\texplicit InsertGlobalState(ClientContext &context, const vector<LogicalType> &return_types)\n+\t    : insert_count(0), return_collection(context, return_types) {\n \t}\n \n \tmutex lock;\n \tidx_t insert_count;\n-\tChunkCollection return_chunk_collection;\n-\tidx_t returned_chunk_count;\n+\tColumnDataCollection return_collection;\n };\n \n class InsertLocalState : public LocalSinkState {\n@@ -85,7 +84,7 @@ SinkResultType PhysicalInsert::Sink(ExecutionContext &context, GlobalSinkState &\n \ttable->storage->Append(*table, context.client, istate.insert_chunk);\n \n \tif (return_chunk) {\n-\t\tgstate.return_chunk_collection.Append(istate.insert_chunk);\n+\t\tgstate.return_collection.Append(istate.insert_chunk);\n \t}\n \n \tgstate.insert_count += chunk.size();\n@@ -93,7 +92,7 @@ SinkResultType PhysicalInsert::Sink(ExecutionContext &context, GlobalSinkState &\n }\n \n unique_ptr<GlobalSinkState> PhysicalInsert::GetGlobalSinkState(ClientContext &context) const {\n-\treturn make_unique<InsertGlobalState>(Allocator::Get(context));\n+\treturn make_unique<InsertGlobalState>(context, GetTypes());\n }\n \n unique_ptr<LocalSinkState> PhysicalInsert::GetLocalSinkState(ExecutionContext &context) const {\n@@ -112,14 +111,20 @@ void PhysicalInsert::Combine(ExecutionContext &context, GlobalSinkState &gstate,\n //===--------------------------------------------------------------------===//\n class InsertSourceState : public GlobalSourceState {\n public:\n-\tInsertSourceState() : finished(false) {\n+\texplicit InsertSourceState(const PhysicalInsert &op) : finished(false) {\n+\t\tif (op.return_chunk) {\n+\t\t\tD_ASSERT(op.sink_state);\n+\t\t\tauto &g = (InsertGlobalState &)*op.sink_state;\n+\t\t\tg.return_collection.InitializeScan(scan_state);\n+\t\t}\n \t}\n \n+\tColumnDataScanState scan_state;\n \tbool finished;\n };\n \n unique_ptr<GlobalSourceState> PhysicalInsert::GetGlobalSourceState(ClientContext &context) const {\n-\treturn make_unique<InsertSourceState>();\n+\treturn make_unique<InsertSourceState>(*this);\n }\n \n void PhysicalInsert::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,\n@@ -133,19 +138,10 @@ void PhysicalInsert::GetData(ExecutionContext &context, DataChunk &chunk, Global\n \t\tchunk.SetCardinality(1);\n \t\tchunk.SetValue(0, 0, Value::BIGINT(insert_gstate.insert_count));\n \t\tstate.finished = true;\n-\t}\n-\n-\tidx_t chunk_return = insert_gstate.returned_chunk_count;\n-\tif (chunk_return >= insert_gstate.return_chunk_collection.Chunks().size()) {\n \t\treturn;\n \t}\n \n-\tchunk.Reference(insert_gstate.return_chunk_collection.GetChunk(chunk_return));\n-\tchunk.SetCardinality((insert_gstate.return_chunk_collection.GetChunk(chunk_return)).size());\n-\tinsert_gstate.returned_chunk_count += 1;\n-\tif (insert_gstate.returned_chunk_count >= insert_gstate.return_chunk_collection.Chunks().size()) {\n-\t\tstate.finished = true;\n-\t}\n+\tinsert_gstate.return_collection.Scan(state.scan_state, chunk);\n }\n \n } // namespace duckdb\ndiff --git a/src/execution/operator/persistent/physical_update.cpp b/src/execution/operator/persistent/physical_update.cpp\nindex 92e7bf850685..cc2d01e1a046 100644\n--- a/src/execution/operator/persistent/physical_update.cpp\n+++ b/src/execution/operator/persistent/physical_update.cpp\n@@ -1,7 +1,7 @@\n #include \"duckdb/execution/operator/persistent/physical_update.hpp\"\n #include \"duckdb/parallel/thread_context.hpp\"\n #include \"duckdb/catalog/catalog_entry/table_catalog_entry.hpp\"\n-#include \"duckdb/common/types/chunk_collection.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n #include \"duckdb/common/vector_operations/vector_operations.hpp\"\n #include \"duckdb/execution/expression_executor.hpp\"\n #include \"duckdb/planner/expression/bound_reference_expression.hpp\"\n@@ -15,15 +15,14 @@ namespace duckdb {\n //===--------------------------------------------------------------------===//\n class UpdateGlobalState : public GlobalSinkState {\n public:\n-\texplicit UpdateGlobalState(Allocator &allocator)\n-\t    : updated_count(0), return_chunk_collection(allocator), returned_chunk_count(0) {\n+\texplicit UpdateGlobalState(ClientContext &context, const vector<LogicalType> &return_types)\n+\t    : updated_count(0), return_collection(context, return_types) {\n \t}\n \n \tmutex lock;\n \tidx_t updated_count;\n \tunordered_set<row_t> updated_columns;\n-\tChunkCollection return_chunk_collection;\n-\tidx_t returned_chunk_count;\n+\tColumnDataCollection return_collection;\n };\n \n class UpdateLocalState : public LocalSinkState {\n@@ -113,7 +112,7 @@ SinkResultType PhysicalUpdate::Sink(ExecutionContext &context, GlobalSinkState &\n \t}\n \n \tif (return_chunk) {\n-\t\tgstate.return_chunk_collection.Append(mock_chunk);\n+\t\tgstate.return_collection.Append(mock_chunk);\n \t}\n \n \tgstate.updated_count += chunk.size();\n@@ -122,7 +121,7 @@ SinkResultType PhysicalUpdate::Sink(ExecutionContext &context, GlobalSinkState &\n }\n \n unique_ptr<GlobalSinkState> PhysicalUpdate::GetGlobalSinkState(ClientContext &context) const {\n-\treturn make_unique<UpdateGlobalState>(Allocator::Get(context));\n+\treturn make_unique<UpdateGlobalState>(context, GetTypes());\n }\n \n unique_ptr<LocalSinkState> PhysicalUpdate::GetLocalSinkState(ExecutionContext &context) const {\n@@ -141,14 +140,20 @@ void PhysicalUpdate::Combine(ExecutionContext &context, GlobalSinkState &gstate,\n //===--------------------------------------------------------------------===//\n class UpdateSourceState : public GlobalSourceState {\n public:\n-\tUpdateSourceState() : finished(false) {\n+\texplicit UpdateSourceState(const PhysicalUpdate &op) : finished(false) {\n+\t\tif (op.return_chunk) {\n+\t\t\tD_ASSERT(op.sink_state);\n+\t\t\tauto &g = (UpdateGlobalState &)*op.sink_state;\n+\t\t\tg.return_collection.InitializeScan(scan_state);\n+\t\t}\n \t}\n \n+\tColumnDataScanState scan_state;\n \tbool finished;\n };\n \n unique_ptr<GlobalSourceState> PhysicalUpdate::GetGlobalSourceState(ClientContext &context) const {\n-\treturn make_unique<UpdateSourceState>();\n+\treturn make_unique<UpdateSourceState>(*this);\n }\n \n void PhysicalUpdate::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,\n@@ -162,18 +167,10 @@ void PhysicalUpdate::GetData(ExecutionContext &context, DataChunk &chunk, Global\n \t\tchunk.SetCardinality(1);\n \t\tchunk.SetValue(0, 0, Value::BIGINT(g.updated_count));\n \t\tstate.finished = true;\n-\t}\n-\n-\tidx_t chunk_return = g.returned_chunk_count;\n-\tif (chunk_return >= g.return_chunk_collection.Chunks().size()) {\n \t\treturn;\n \t}\n-\tchunk.Reference(g.return_chunk_collection.GetChunk(chunk_return));\n-\tchunk.SetCardinality((g.return_chunk_collection.GetChunk(chunk_return)).size());\n-\tg.returned_chunk_count += 1;\n-\tif (g.returned_chunk_count >= g.return_chunk_collection.Chunks().size()) {\n-\t\tstate.finished = true;\n-\t}\n+\n+\tg.return_collection.Scan(state.scan_state, chunk);\n }\n \n } // namespace duckdb\ndiff --git a/src/execution/operator/scan/CMakeLists.txt b/src/execution/operator/scan/CMakeLists.txt\nindex bac1efb41958..788c7ea2e9c9 100644\n--- a/src/execution/operator/scan/CMakeLists.txt\n+++ b/src/execution/operator/scan/CMakeLists.txt\n@@ -1,7 +1,7 @@\n add_library_unity(\n   duckdb_operator_scan\n   OBJECT\n-  physical_chunk_scan.cpp\n+  physical_column_data_scan.cpp\n   physical_dummy_scan.cpp\n   physical_empty_result.cpp\n   physical_expression_scan.cpp\ndiff --git a/src/execution/operator/scan/physical_chunk_scan.cpp b/src/execution/operator/scan/physical_column_data_scan.cpp\nsimilarity index 62%\nrename from src/execution/operator/scan/physical_chunk_scan.cpp\nrename to src/execution/operator/scan/physical_column_data_scan.cpp\nindex 3bf2289b2d0b..46f591fb6d8d 100644\n--- a/src/execution/operator/scan/physical_chunk_scan.cpp\n+++ b/src/execution/operator/scan/physical_column_data_scan.cpp\n@@ -1,42 +1,41 @@\n-#include \"duckdb/execution/operator/scan/physical_chunk_scan.hpp\"\n+#include \"duckdb/execution/operator/scan/physical_column_data_scan.hpp\"\n #include \"duckdb/parallel/pipeline.hpp\"\n #include \"duckdb/execution/operator/join/physical_delim_join.hpp\"\n \n namespace duckdb {\n \n-class PhysicalChunkScanState : public GlobalSourceState {\n+class PhysicalColumnDataScanState : public GlobalSourceState {\n public:\n-\texplicit PhysicalChunkScanState() : chunk_index(0) {\n+\texplicit PhysicalColumnDataScanState() : initialized(false) {\n \t}\n \n \t//! The current position in the scan\n-\tidx_t chunk_index;\n+\tColumnDataScanState scan_state;\n+\tbool initialized;\n };\n \n-unique_ptr<GlobalSourceState> PhysicalChunkScan::GetGlobalSourceState(ClientContext &context) const {\n-\treturn make_unique<PhysicalChunkScanState>();\n+unique_ptr<GlobalSourceState> PhysicalColumnDataScan::GetGlobalSourceState(ClientContext &context) const {\n+\treturn make_unique<PhysicalColumnDataScanState>();\n }\n \n-void PhysicalChunkScan::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,\n-                                LocalSourceState &lstate) const {\n-\tauto &state = (PhysicalChunkScanState &)gstate;\n+void PhysicalColumnDataScan::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,\n+                                     LocalSourceState &lstate) const {\n+\tauto &state = (PhysicalColumnDataScanState &)gstate;\n \tD_ASSERT(collection);\n \tif (collection->Count() == 0) {\n \t\treturn;\n \t}\n-\tD_ASSERT(chunk.GetTypes() == collection->Types());\n-\tif (state.chunk_index >= collection->ChunkCount()) {\n-\t\treturn;\n+\tif (!state.initialized) {\n+\t\tcollection->InitializeScan(state.scan_state);\n+\t\tstate.initialized = true;\n \t}\n-\tauto &collection_chunk = collection->GetChunk(state.chunk_index);\n-\tchunk.Reference(collection_chunk);\n-\tstate.chunk_index++;\n+\tcollection->Scan(state.scan_state, chunk);\n }\n \n //===--------------------------------------------------------------------===//\n // Pipeline Construction\n //===--------------------------------------------------------------------===//\n-void PhysicalChunkScan::BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) {\n+void PhysicalColumnDataScan::BuildPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state) {\n \t// check if there is any additional action we need to do depending on the type\n \tswitch (type) {\n \tcase PhysicalOperatorType::DELIM_SCAN: {\ndiff --git a/src/execution/operator/set/physical_recursive_cte.cpp b/src/execution/operator/set/physical_recursive_cte.cpp\nindex a6faa5204aaf..0c1b0fde7ec3 100644\n--- a/src/execution/operator/set/physical_recursive_cte.cpp\n+++ b/src/execution/operator/set/physical_recursive_cte.cpp\n@@ -2,7 +2,7 @@\n \n #include \"duckdb/common/vector_operations/vector_operations.hpp\"\n \n-#include \"duckdb/common/types/chunk_collection.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n #include \"duckdb/execution/aggregate_hashtable.hpp\"\n #include \"duckdb/parallel/pipeline.hpp\"\n #include \"duckdb/storage/buffer_manager.hpp\"\n@@ -28,7 +28,7 @@ PhysicalRecursiveCTE::~PhysicalRecursiveCTE() {\n class RecursiveCTEState : public GlobalSinkState {\n public:\n \texplicit RecursiveCTEState(ClientContext &context, const PhysicalRecursiveCTE &op)\n-\t    : intermediate_table(Allocator::Get(context)), new_groups(STANDARD_VECTOR_SIZE) {\n+\t    : intermediate_table(context, op.GetTypes()), new_groups(STANDARD_VECTOR_SIZE) {\n \t\tht = make_unique<GroupedAggregateHashTable>(Allocator::Get(context), BufferManager::GetBufferManager(context),\n \t\t                                            op.types, vector<LogicalType>(),\n \t\t                                            vector<BoundAggregateExpression *>());\n@@ -37,8 +37,10 @@ class RecursiveCTEState : public GlobalSinkState {\n \tunique_ptr<GroupedAggregateHashTable> ht;\n \n \tbool intermediate_empty = true;\n-\tChunkCollection intermediate_table;\n-\tidx_t chunk_idx = 0;\n+\tColumnDataCollection intermediate_table;\n+\tColumnDataScanState scan_state;\n+\tbool initialized = false;\n+\tbool finished_scan = false;\n \tSelectionVector new_groups;\n };\n \n@@ -78,29 +80,40 @@ SinkResultType PhysicalRecursiveCTE::Sink(ExecutionContext &context, GlobalSinkS\n void PhysicalRecursiveCTE::GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate_p,\n                                    LocalSourceState &lstate) const {\n \tauto &gstate = (RecursiveCTEState &)*sink_state;\n+\tif (!gstate.initialized) {\n+\t\tgstate.intermediate_table.InitializeScan(gstate.scan_state);\n+\t\tgstate.finished_scan = false;\n+\t\tgstate.initialized = true;\n+\t}\n \twhile (chunk.size() == 0) {\n-\t\tif (gstate.chunk_idx < gstate.intermediate_table.ChunkCount()) {\n+\t\tif (!gstate.finished_scan) {\n \t\t\t// scan any chunks we have collected so far\n-\t\t\tchunk.Reference(gstate.intermediate_table.GetChunk(gstate.chunk_idx));\n-\t\t\tgstate.chunk_idx++;\n-\t\t\tbreak;\n+\t\t\tgstate.intermediate_table.Scan(gstate.scan_state, chunk);\n+\t\t\tif (chunk.size() == 0) {\n+\t\t\t\tgstate.finished_scan = true;\n+\t\t\t} else {\n+\t\t\t\tbreak;\n+\t\t\t}\n \t\t} else {\n \t\t\t// we have run out of chunks\n \t\t\t// now we need to recurse\n \t\t\t// we set up the working table as the data we gathered in this iteration of the recursion\n \t\t\tworking_table->Reset();\n-\t\t\tworking_table->Merge(gstate.intermediate_table);\n+\t\t\tworking_table->Combine(gstate.intermediate_table);\n \t\t\t// and we clear the intermediate table\n+\t\t\tgstate.finished_scan = false;\n \t\t\tgstate.intermediate_table.Reset();\n-\t\t\tgstate.chunk_idx = 0;\n \t\t\t// now we need to re-execute all of the pipelines that depend on the recursion\n \t\t\tExecuteRecursivePipelines(context);\n \n \t\t\t// check if we obtained any results\n \t\t\t// if not, we are done\n \t\t\tif (gstate.intermediate_table.Count() == 0) {\n+\t\t\t\tgstate.finished_scan = true;\n \t\t\t\tbreak;\n \t\t\t}\n+\t\t\t// set up the scan again\n+\t\t\tgstate.intermediate_table.InitializeScan(gstate.scan_state);\n \t\t}\n \t}\n }\ndiff --git a/src/execution/physical_plan/CMakeLists.txt b/src/execution/physical_plan/CMakeLists.txt\nindex 676167f2c51f..32895589a00a 100644\n--- a/src/execution/physical_plan/CMakeLists.txt\n+++ b/src/execution/physical_plan/CMakeLists.txt\n@@ -3,7 +3,7 @@ add_library_unity(\n   OBJECT\n   plan_aggregate.cpp\n   plan_any_join.cpp\n-  plan_chunk_get.cpp\n+  plan_column_data_get.cpp\n   plan_comparison_join.cpp\n   plan_copy_to_file.cpp\n   plan_create.cpp\ndiff --git a/src/execution/physical_plan/plan_chunk_get.cpp b/src/execution/physical_plan/plan_column_data_get.cpp\nsimilarity index 62%\nrename from src/execution/physical_plan/plan_chunk_get.cpp\nrename to src/execution/physical_plan/plan_column_data_get.cpp\nindex 615aae70f35d..ab35a022b22b 100644\n--- a/src/execution/physical_plan/plan_chunk_get.cpp\n+++ b/src/execution/physical_plan/plan_column_data_get.cpp\n@@ -1,16 +1,16 @@\n-#include \"duckdb/execution/operator/scan/physical_chunk_scan.hpp\"\n+#include \"duckdb/execution/operator/scan/physical_column_data_scan.hpp\"\n #include \"duckdb/execution/physical_plan_generator.hpp\"\n-#include \"duckdb/planner/operator/logical_chunk_get.hpp\"\n+#include \"duckdb/planner/operator/logical_column_data_get.hpp\"\n \n namespace duckdb {\n \n-unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalChunkGet &op) {\n+unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalColumnDataGet &op) {\n \tD_ASSERT(op.children.size() == 0);\n \tD_ASSERT(op.collection);\n \n \t// create a PhysicalChunkScan pointing towards the owned collection\n \tauto chunk_scan =\n-\t    make_unique<PhysicalChunkScan>(op.types, PhysicalOperatorType::CHUNK_SCAN, op.estimated_cardinality);\n+\t    make_unique<PhysicalColumnDataScan>(op.types, PhysicalOperatorType::COLUMN_DATA_SCAN, op.estimated_cardinality);\n \tchunk_scan->owned_collection = move(op.collection);\n \tchunk_scan->collection = chunk_scan->owned_collection.get();\n \treturn move(chunk_scan);\ndiff --git a/src/execution/physical_plan/plan_comparison_join.cpp b/src/execution/physical_plan/plan_comparison_join.cpp\nindex 12cb87708e67..fce8d4599f9f 100644\n--- a/src/execution/physical_plan/plan_comparison_join.cpp\n+++ b/src/execution/physical_plan/plan_comparison_join.cpp\n@@ -243,7 +243,7 @@ unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalComparison\n \n \t} else {\n \t\tbool can_merge = has_range > 0;\n-\t\tbool can_iejoin = has_range >= 2 && rec_ctes.empty();\n+\t\tbool can_iejoin = has_range >= 2 && recursive_cte_tables.empty();\n \t\tswitch (op.join_type) {\n \t\tcase JoinType::SEMI:\n \t\tcase JoinType::ANTI:\ndiff --git a/src/execution/physical_plan/plan_delim_get.cpp b/src/execution/physical_plan/plan_delim_get.cpp\nindex 1af6ee48110e..e5207e1842bf 100644\n--- a/src/execution/physical_plan/plan_delim_get.cpp\n+++ b/src/execution/physical_plan/plan_delim_get.cpp\n@@ -1,4 +1,4 @@\n-#include \"duckdb/execution/operator/scan/physical_chunk_scan.hpp\"\n+#include \"duckdb/execution/operator/scan/physical_column_data_scan.hpp\"\n #include \"duckdb/execution/physical_plan_generator.hpp\"\n #include \"duckdb/planner/operator/logical_delim_get.hpp\"\n \n@@ -9,7 +9,7 @@ unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalDelimGet &\n \n \t// create a PhysicalChunkScan without an owned_collection, the collection will be added later\n \tauto chunk_scan =\n-\t    make_unique<PhysicalChunkScan>(op.types, PhysicalOperatorType::DELIM_SCAN, op.estimated_cardinality);\n+\t    make_unique<PhysicalColumnDataScan>(op.types, PhysicalOperatorType::DELIM_SCAN, op.estimated_cardinality);\n \treturn move(chunk_scan);\n }\n \ndiff --git a/src/execution/physical_plan/plan_delim_join.cpp b/src/execution/physical_plan/plan_delim_join.cpp\nindex c49175e80015..28ee948b8253 100644\n--- a/src/execution/physical_plan/plan_delim_join.cpp\n+++ b/src/execution/physical_plan/plan_delim_join.cpp\n@@ -2,7 +2,6 @@\n #include \"duckdb/execution/operator/join/physical_delim_join.hpp\"\n #include \"duckdb/execution/operator/join/physical_hash_join.hpp\"\n #include \"duckdb/execution/operator/projection/physical_projection.hpp\"\n-#include \"duckdb/execution/operator/scan/physical_chunk_scan.hpp\"\n #include \"duckdb/execution/physical_plan_generator.hpp\"\n #include \"duckdb/planner/operator/logical_delim_join.hpp\"\n #include \"duckdb/planner/expression/bound_aggregate_expression.hpp\"\ndiff --git a/src/execution/physical_plan/plan_explain.cpp b/src/execution/physical_plan/plan_explain.cpp\nindex 618638a1ab17..a7f3ba92e4e5 100644\n--- a/src/execution/physical_plan/plan_explain.cpp\n+++ b/src/execution/physical_plan/plan_explain.cpp\n@@ -1,8 +1,9 @@\n-#include \"duckdb/execution/operator/scan/physical_chunk_scan.hpp\"\n #include \"duckdb/execution/physical_plan_generator.hpp\"\n #include \"duckdb/planner/operator/logical_explain.hpp\"\n #include \"duckdb/execution/operator/helper/physical_explain_analyze.hpp\"\n #include \"duckdb/main/client_context.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n+#include \"duckdb/execution/operator/scan/physical_column_data_scan.hpp\"\n \n #include \"duckdb/common/tree_renderer.hpp\"\n \n@@ -35,9 +36,11 @@ unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalExplain &o\n \t\tvalues = {op.logical_plan_unopt, logical_plan_opt, op.physical_plan};\n \t}\n \n-\t// create a ChunkCollection from the output\n+\t// create a ColumnDataCollection from the output\n \tauto &allocator = Allocator::Get(context);\n-\tauto collection = make_unique<ChunkCollection>(allocator);\n+\tvector<LogicalType> plan_types {LogicalType::VARCHAR, LogicalType::VARCHAR};\n+\tauto collection =\n+\t    make_unique<ColumnDataCollection>(context, plan_types, ColumnDataAllocatorType::IN_MEMORY_ALLOCATOR);\n \n \tDataChunk chunk;\n \tchunk.Initialize(allocator, op.types);\n@@ -54,7 +57,7 @@ unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalExplain &o\n \n \t// create a chunk scan to output the result\n \tauto chunk_scan =\n-\t    make_unique<PhysicalChunkScan>(op.types, PhysicalOperatorType::CHUNK_SCAN, op.estimated_cardinality);\n+\t    make_unique<PhysicalColumnDataScan>(op.types, PhysicalOperatorType::COLUMN_DATA_SCAN, op.estimated_cardinality);\n \tchunk_scan->owned_collection = move(collection);\n \tchunk_scan->collection = chunk_scan->owned_collection.get();\n \treturn move(chunk_scan);\ndiff --git a/src/execution/physical_plan/plan_expression_get.cpp b/src/execution/physical_plan/plan_expression_get.cpp\nindex dd212ef3b7dc..271ce1b17b29 100644\n--- a/src/execution/physical_plan/plan_expression_get.cpp\n+++ b/src/execution/physical_plan/plan_expression_get.cpp\n@@ -1,7 +1,8 @@\n #include \"duckdb/execution/operator/scan/physical_expression_scan.hpp\"\n #include \"duckdb/execution/physical_plan_generator.hpp\"\n #include \"duckdb/planner/operator/logical_expression_get.hpp\"\n-#include \"duckdb/execution/operator/scan/physical_chunk_scan.hpp\"\n+#include \"duckdb/execution/operator/scan/physical_column_data_scan.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n \n namespace duckdb {\n \n@@ -17,17 +18,20 @@ unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalExpression\n \tauto &allocator = Allocator::Get(context);\n \t// simple expression scan (i.e. no subqueries to evaluate and no prepared statement parameters)\n \t// we can evaluate all the expressions right now and turn this into a chunk collection scan\n-\tauto chunk_scan =\n-\t    make_unique<PhysicalChunkScan>(op.types, PhysicalOperatorType::CHUNK_SCAN, expr_scan->expressions.size());\n-\tchunk_scan->owned_collection = make_unique<ChunkCollection>(allocator);\n+\tauto chunk_scan = make_unique<PhysicalColumnDataScan>(op.types, PhysicalOperatorType::COLUMN_DATA_SCAN,\n+\t                                                      expr_scan->expressions.size());\n+\tchunk_scan->owned_collection = make_unique<ColumnDataCollection>(context, op.types);\n \tchunk_scan->collection = chunk_scan->owned_collection.get();\n \n \tDataChunk chunk;\n \tchunk.Initialize(allocator, op.types);\n+\n+\tColumnDataAppendState append_state;\n+\tchunk_scan->owned_collection->InitializeAppend(append_state);\n \tfor (idx_t expression_idx = 0; expression_idx < expr_scan->expressions.size(); expression_idx++) {\n \t\tchunk.Reset();\n \t\texpr_scan->EvaluateExpression(allocator, expression_idx, nullptr, chunk);\n-\t\tchunk_scan->owned_collection->Append(chunk);\n+\t\tchunk_scan->owned_collection->Append(append_state, chunk);\n \t}\n \treturn move(chunk_scan);\n }\ndiff --git a/src/execution/physical_plan/plan_recursive_cte.cpp b/src/execution/physical_plan/plan_recursive_cte.cpp\nindex 3245a5140624..7d16f8653d81 100644\n--- a/src/execution/physical_plan/plan_recursive_cte.cpp\n+++ b/src/execution/physical_plan/plan_recursive_cte.cpp\n@@ -1,9 +1,10 @@\n #include \"duckdb/execution/operator/set/physical_recursive_cte.hpp\"\n-#include \"duckdb/execution/operator/scan/physical_chunk_scan.hpp\"\n+#include \"duckdb/execution/operator/scan/physical_column_data_scan.hpp\"\n #include \"duckdb/execution/physical_plan_generator.hpp\"\n #include \"duckdb/planner/expression/bound_reference_expression.hpp\"\n #include \"duckdb/planner/operator/logical_recursive_cte.hpp\"\n #include \"duckdb/planner/operator/logical_cteref.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n \n namespace duckdb {\n \n@@ -11,10 +12,10 @@ unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalRecursiveC\n \tD_ASSERT(op.children.size() == 2);\n \n \t// Create the working_table that the PhysicalRecursiveCTE will use for evaluation.\n-\tauto working_table = std::make_shared<ChunkCollection>(context);\n+\tauto working_table = std::make_shared<ColumnDataCollection>(context, op.types);\n \n-\t// Add the ChunkCollection to the context of this PhysicalPlanGenerator\n-\trec_ctes[op.table_index] = working_table;\n+\t// Add the ColumnDataCollection to the context of this PhysicalPlanGenerator\n+\trecursive_cte_tables[op.table_index] = working_table;\n \n \tauto left = CreatePlan(*op.children[0]);\n \tauto right = CreatePlan(*op.children[1]);\n@@ -29,12 +30,12 @@ unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalRecursiveC\n unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalCTERef &op) {\n \tD_ASSERT(op.children.empty());\n \n-\tauto chunk_scan =\n-\t    make_unique<PhysicalChunkScan>(op.types, PhysicalOperatorType::RECURSIVE_CTE_SCAN, op.estimated_cardinality);\n+\tauto chunk_scan = make_unique<PhysicalColumnDataScan>(op.types, PhysicalOperatorType::RECURSIVE_CTE_SCAN,\n+\t                                                      op.estimated_cardinality);\n \n \t// CreatePlan of a LogicalRecursiveCTE must have happened before.\n-\tauto cte = rec_ctes.find(op.cte_index);\n-\tif (cte == rec_ctes.end()) {\n+\tauto cte = recursive_cte_tables.find(op.cte_index);\n+\tif (cte == recursive_cte_tables.end()) {\n \t\tthrow Exception(\"Referenced recursive CTE does not exist.\");\n \t}\n \tchunk_scan->collection = cte->second.get();\ndiff --git a/src/execution/physical_plan/plan_show_select.cpp b/src/execution/physical_plan/plan_show_select.cpp\nindex 0a446f9b39df..10d33af1f6f6 100644\n--- a/src/execution/physical_plan/plan_show_select.cpp\n+++ b/src/execution/physical_plan/plan_show_select.cpp\n@@ -1,4 +1,4 @@\n-#include \"duckdb/execution/operator/scan/physical_chunk_scan.hpp\"\n+#include \"duckdb/execution/operator/scan/physical_column_data_scan.hpp\"\n #include \"duckdb/execution/physical_plan_generator.hpp\"\n #include \"duckdb/parser/parsed_data/show_select_info.hpp\"\n #include \"duckdb/planner/operator/logical_show.hpp\"\n@@ -9,7 +9,9 @@ unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalShow &op)\n \tDataChunk output;\n \toutput.Initialize(Allocator::Get(context), op.types);\n \n-\tauto collection = make_unique<ChunkCollection>(Allocator::Get(context));\n+\tauto collection = make_unique<ColumnDataCollection>(context, op.types);\n+\tColumnDataAppendState append_state;\n+\tcollection->InitializeAppend(append_state);\n \tfor (idx_t column_idx = 0; column_idx < op.types_select.size(); column_idx++) {\n \t\tauto type = op.types_select[column_idx];\n \t\tauto &name = op.aliases[column_idx];\n@@ -29,16 +31,16 @@ unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalShow &op)\n \n \t\toutput.SetCardinality(output.size() + 1);\n \t\tif (output.size() == STANDARD_VECTOR_SIZE) {\n-\t\t\tcollection->Append(output);\n+\t\t\tcollection->Append(append_state, output);\n \t\t\toutput.Reset();\n \t\t}\n \t}\n \n-\tcollection->Append(output);\n+\tcollection->Append(append_state, output);\n \n \t// create a chunk scan to output the result\n \tauto chunk_scan =\n-\t    make_unique<PhysicalChunkScan>(op.types, PhysicalOperatorType::CHUNK_SCAN, op.estimated_cardinality);\n+\t    make_unique<PhysicalColumnDataScan>(op.types, PhysicalOperatorType::COLUMN_DATA_SCAN, op.estimated_cardinality);\n \tchunk_scan->owned_collection = move(collection);\n \tchunk_scan->collection = chunk_scan->owned_collection.get();\n \treturn move(chunk_scan);\ndiff --git a/src/execution/physical_plan_generator.cpp b/src/execution/physical_plan_generator.cpp\nindex a843e62066a0..62f5e5944f6b 100644\n--- a/src/execution/physical_plan_generator.cpp\n+++ b/src/execution/physical_plan_generator.cpp\n@@ -4,6 +4,7 @@\n #include \"duckdb/execution/column_binding_resolver.hpp\"\n #include \"duckdb/main/client_context.hpp\"\n #include \"duckdb/planner/expression/bound_function_expression.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n \n namespace duckdb {\n \n@@ -25,6 +26,12 @@ class DependencyExtractor : public LogicalOperatorVisitor {\n \tunordered_set<CatalogEntry *> &dependencies;\n };\n \n+PhysicalPlanGenerator::PhysicalPlanGenerator(ClientContext &context) : context(context) {\n+}\n+\n+PhysicalPlanGenerator::~PhysicalPlanGenerator() {\n+}\n+\n unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(unique_ptr<LogicalOperator> op) {\n \tauto &profiler = QueryProfiler::Get(context);\n \n@@ -100,7 +107,7 @@ unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalOperator &\n \tcase LogicalOperatorType::LOGICAL_DELETE:\n \t\treturn CreatePlan((LogicalDelete &)op);\n \tcase LogicalOperatorType::LOGICAL_CHUNK_GET:\n-\t\treturn CreatePlan((LogicalChunkGet &)op);\n+\t\treturn CreatePlan((LogicalColumnDataGet &)op);\n \tcase LogicalOperatorType::LOGICAL_DELIM_GET:\n \t\treturn CreatePlan((LogicalDelimGet &)op);\n \tcase LogicalOperatorType::LOGICAL_EXPRESSION_GET:\ndiff --git a/src/function/scalar/map/map.cpp b/src/function/scalar/map/map.cpp\nindex 5ea2f271cc73..fe9d7c4fa89c 100644\n--- a/src/function/scalar/map/map.cpp\n+++ b/src/function/scalar/map/map.cpp\n@@ -108,8 +108,12 @@ static void MapFunction(DataChunk &args, ExpressionState &state, Vector &result)\n \t\treturn;\n \t}\n \n-\tif (ListVector::GetListSize(args.data[0]) != ListVector::GetListSize(args.data[1])) {\n-\t\tthrow Exception(\"Key list has a different size from Value list\");\n+\tauto key_count = ListVector::GetListSize(args.data[0]);\n+\tauto value_count = ListVector::GetListSize(args.data[1]);\n+\tif (key_count != value_count) {\n+\t\tthrow InvalidInputException(\n+\t\t    \"Error in MAP creation: key list has a different size from value list (%lld keys, %lld values)\", key_count,\n+\t\t    value_count);\n \t}\n \n \tkey_vector.Reference(args.data[0]);\ndiff --git a/src/function/table/arrow.cpp b/src/function/table/arrow.cpp\nindex 224681b1bd4f..96482f4261a9 100644\n--- a/src/function/table/arrow.cpp\n+++ b/src/function/table/arrow.cpp\n@@ -1,7 +1,7 @@\n-#include \"duckdb/common/arrow.hpp\"\n+#include \"duckdb/common/arrow/arrow.hpp\"\n \n #include \"duckdb.hpp\"\n-#include \"duckdb/common/arrow_wrapper.hpp\"\n+#include \"duckdb/common/arrow/arrow_wrapper.hpp\"\n #include \"duckdb/common/limits.hpp\"\n #include \"duckdb/common/types/date.hpp\"\n #include \"duckdb/common/to_string.hpp\"\n@@ -199,7 +199,6 @@ unique_ptr<FunctionData> ArrowTableFunction::ArrowScanBind(ClientContext &contex\n \tauto stream_factory_get_schema = (stream_factory_get_schema_t)input.inputs[2].GetPointer();\n \tauto rows_per_thread = input.inputs[3].GetValue<uint64_t>();\n \n-\tpair<unordered_map<idx_t, string>, vector<string>> project_columns;\n \tauto res = make_unique<ArrowScanFunctionData>(rows_per_thread, stream_factory_produce, stream_factory_ptr);\n \n \tauto &data = *res;\n@@ -230,17 +229,18 @@ unique_ptr<FunctionData> ArrowTableFunction::ArrowScanBind(ClientContext &contex\n unique_ptr<ArrowArrayStreamWrapper> ProduceArrowScan(const ArrowScanFunctionData &function,\n                                                      const vector<column_t> &column_ids, TableFilterSet *filters) {\n \t//! Generate Projection Pushdown Vector\n-\tpair<unordered_map<idx_t, string>, vector<string>> project_columns;\n+\tArrowStreamParameters parameters;\n \tD_ASSERT(!column_ids.empty());\n \tfor (idx_t idx = 0; idx < column_ids.size(); idx++) {\n \t\tauto col_idx = column_ids[idx];\n \t\tif (col_idx != COLUMN_IDENTIFIER_ROW_ID) {\n \t\t\tauto &schema = *function.schema_root.arrow_schema.children[col_idx];\n-\t\t\tproject_columns.first[idx] = schema.name;\n-\t\t\tproject_columns.second.emplace_back(schema.name);\n+\t\t\tparameters.projected_columns.projection_map[idx] = schema.name;\n+\t\t\tparameters.projected_columns.columns.emplace_back(schema.name);\n \t\t}\n \t}\n-\treturn function.scanner_producer(function.stream_factory_ptr, project_columns, filters);\n+\tparameters.filters = filters;\n+\treturn function.scanner_producer(function.stream_factory_ptr, parameters);\n }\n \n idx_t ArrowTableFunction::ArrowScanMaxThreads(ClientContext &context, const FunctionData *bind_data_p) {\ndiff --git a/src/function/table/arrow_conversion.cpp b/src/function/table/arrow_conversion.cpp\nindex 095fa05b2925..6a93e879b255 100644\n--- a/src/function/table/arrow_conversion.cpp\n+++ b/src/function/table/arrow_conversion.cpp\n@@ -18,17 +18,16 @@ void ShiftRight(unsigned char *ar, int size, int shift) {\n \t}\n }\n \n-void SetValidityMask(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n-                     int64_t nested_offset, bool add_null = false) {\n-\tauto &mask = FlatVector::Validity(vector);\n+void GetValidityMask(ValidityMask &mask, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n+                     int64_t nested_offset = -1, bool add_null = false) {\n \tif (array.null_count != 0 && array.buffers[0]) {\n-\t\tD_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);\n \t\tauto bit_offset = scan_state.chunk_offset + array.offset;\n \t\tif (nested_offset != -1) {\n \t\t\tbit_offset = nested_offset;\n \t\t}\n-\t\tauto n_bitmask_bytes = (size + 8 - 1) / 8;\n \t\tmask.EnsureWritable();\n+#if STANDARD_VECTOR_SIZE > 64\n+\t\tauto n_bitmask_bytes = (size + 8 - 1) / 8;\n \t\tif (bit_offset % 8 == 0) {\n \t\t\t//! just memcpy nullmask\n \t\t\tmemcpy((void *)mask.GetData(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes);\n@@ -40,6 +39,19 @@ void SetValidityMask(Vector &vector, ArrowArray &array, ArrowScanLocalState &sca\n \t\t\t           bit_offset % 8); //! why this has to be a right shift is a mystery to me\n \t\t\tmemcpy((void *)mask.GetData(), (data_ptr_t)temp_nullmask.data(), n_bitmask_bytes);\n \t\t}\n+#else\n+\t\tauto byte_offset = bit_offset / 8;\n+\t\tauto source_data = (uint8_t *)array.buffers[0];\n+\t\tbit_offset %= 8;\n+\t\tfor (idx_t i = 0; i < size; i++) {\n+\t\t\tmask.Set(i, source_data[byte_offset] & (1 << bit_offset));\n+\t\t\tbit_offset++;\n+\t\t\tif (bit_offset == 8) {\n+\t\t\t\tbit_offset = 0;\n+\t\t\t\tbyte_offset++;\n+\t\t\t}\n+\t\t}\n+#endif\n \t}\n \tif (add_null) {\n \t\t//! We are setting a validity mask of the data part of dictionary vector\n@@ -50,23 +62,11 @@ void SetValidityMask(Vector &vector, ArrowArray &array, ArrowScanLocalState &sca\n \t}\n }\n \n-void GetValidityMask(ValidityMask &mask, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size) {\n-\tif (array.null_count != 0 && array.buffers[0]) {\n-\t\tauto bit_offset = scan_state.chunk_offset + array.offset;\n-\t\tauto n_bitmask_bytes = (size + 8 - 1) / 8;\n-\t\tmask.EnsureWritable();\n-\t\tif (bit_offset % 8 == 0) {\n-\t\t\t//! just memcpy nullmask\n-\t\t\tmemcpy((void *)mask.GetData(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes);\n-\t\t} else {\n-\t\t\t//! need to re-align nullmask\n-\t\t\tstd::vector<uint8_t> temp_nullmask(n_bitmask_bytes + 1);\n-\t\t\tmemcpy(temp_nullmask.data(), (uint8_t *)array.buffers[0] + bit_offset / 8, n_bitmask_bytes + 1);\n-\t\t\tShiftRight(temp_nullmask.data(), n_bitmask_bytes + 1,\n-\t\t\t           bit_offset % 8); //! why this has to be a right shift is a mystery to me\n-\t\t\tmemcpy((void *)mask.GetData(), (data_ptr_t)temp_nullmask.data(), n_bitmask_bytes);\n-\t\t}\n-\t}\n+void SetValidityMask(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n+                     int64_t nested_offset, bool add_null = false) {\n+\tD_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);\n+\tauto &mask = FlatVector::Validity(vector);\n+\tGetValidityMask(mask, array, scan_state, size, nested_offset, add_null);\n }\n \n void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\ndiff --git a/src/function/table/pragma_detailed_profiling_output.cpp b/src/function/table/pragma_detailed_profiling_output.cpp\nindex 5143ee33f017..84bf00ae7912 100644\n--- a/src/function/table/pragma_detailed_profiling_output.cpp\n+++ b/src/function/table/pragma_detailed_profiling_output.cpp\n@@ -6,20 +6,22 @@\n #include \"duckdb/main/client_context.hpp\"\n #include \"duckdb/main/client_data.hpp\"\n #include \"duckdb/common/limits.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n \n namespace duckdb {\n \n struct PragmaDetailedProfilingOutputOperatorData : public GlobalTableFunctionState {\n-\texplicit PragmaDetailedProfilingOutputOperatorData() : chunk_index(0), initialized(false) {\n+\texplicit PragmaDetailedProfilingOutputOperatorData() : initialized(false) {\n \t}\n-\tidx_t chunk_index;\n+\n+\tColumnDataScanState scan_state;\n \tbool initialized;\n };\n \n struct PragmaDetailedProfilingOutputData : public TableFunctionData {\n \texplicit PragmaDetailedProfilingOutputData(vector<LogicalType> &types) : types(types) {\n \t}\n-\tunique_ptr<ChunkCollection> collection;\n+\tunique_ptr<ColumnDataCollection> collection;\n \tvector<LogicalType> types;\n };\n \n@@ -81,7 +83,7 @@ static void SetValue(DataChunk &output, int index, int op_id, string annotation,\n \toutput.SetValue(8, index, move(extra_info));\n }\n \n-static void ExtractFunctions(ChunkCollection &collection, ExpressionInfo &info, DataChunk &chunk, int op_id,\n+static void ExtractFunctions(ColumnDataCollection &collection, ExpressionInfo &info, DataChunk &chunk, int op_id,\n                              int &fun_id) {\n \tif (info.hasfunction) {\n \t\tD_ASSERT(info.sample_tuples_count != 0);\n@@ -110,8 +112,8 @@ static void PragmaDetailedProfilingOutputFunction(ClientContext &context, TableF\n \tauto &data = (PragmaDetailedProfilingOutputData &)*data_p.bind_data;\n \n \tif (!state.initialized) {\n-\t\t// create a ChunkCollection\n-\t\tauto collection = make_unique<ChunkCollection>(context);\n+\t\t// create a ColumnDataCollection\n+\t\tauto collection = make_unique<ColumnDataCollection>(context, data.types);\n \n \t\t// create a chunk\n \t\tDataChunk chunk;\n@@ -155,14 +157,11 @@ static void PragmaDetailedProfilingOutputFunction(ClientContext &context, TableF\n \t\t}\n \t\tcollection->Append(chunk);\n \t\tdata.collection = move(collection);\n+\t\tdata.collection->InitializeScan(state.scan_state);\n \t\tstate.initialized = true;\n \t}\n \n-\tif (state.chunk_index >= data.collection->ChunkCount()) {\n-\t\toutput.SetCardinality(0);\n-\t\treturn;\n-\t}\n-\toutput.Reference(data.collection->GetChunk(state.chunk_index++));\n+\tdata.collection->Scan(state.scan_state, output);\n }\n \n void PragmaDetailedProfilingOutput::RegisterFunction(BuiltinFunctions &set) {\ndiff --git a/src/function/table/pragma_last_profiling_output.cpp b/src/function/table/pragma_last_profiling_output.cpp\nindex 7ed64671f679..59cf549d4e99 100644\n--- a/src/function/table/pragma_last_profiling_output.cpp\n+++ b/src/function/table/pragma_last_profiling_output.cpp\n@@ -6,20 +6,22 @@\n #include \"duckdb/main/client_data.hpp\"\n #include \"duckdb/planner/constraints/bound_not_null_constraint.hpp\"\n #include \"duckdb/main/query_profiler.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n \n namespace duckdb {\n \n struct PragmaLastProfilingOutputOperatorData : public GlobalTableFunctionState {\n-\tPragmaLastProfilingOutputOperatorData() : chunk_index(0), initialized(false) {\n+\tPragmaLastProfilingOutputOperatorData() : initialized(false) {\n \t}\n-\tidx_t chunk_index;\n+\n+\tColumnDataScanState scan_state;\n \tbool initialized;\n };\n \n struct PragmaLastProfilingOutputData : public TableFunctionData {\n \texplicit PragmaLastProfilingOutputData(vector<LogicalType> &types) : types(types) {\n \t}\n-\tunique_ptr<ChunkCollection> collection;\n+\tunique_ptr<ColumnDataCollection> collection;\n \tvector<LogicalType> types;\n };\n \n@@ -62,8 +64,8 @@ static void PragmaLastProfilingOutputFunction(ClientContext &context, TableFunct\n \tauto &state = (PragmaLastProfilingOutputOperatorData &)*data_p.global_state;\n \tauto &data = (PragmaLastProfilingOutputData &)*data_p.bind_data;\n \tif (!state.initialized) {\n-\t\t// create a ChunkCollection\n-\t\tauto collection = make_unique<ChunkCollection>(context);\n+\t\t// create a ColumnDataCollection\n+\t\tauto collection = make_unique<ColumnDataCollection>(context, data.types);\n \n \t\tDataChunk chunk;\n \t\tchunk.Initialize(context, data.types);\n@@ -82,14 +84,11 @@ static void PragmaLastProfilingOutputFunction(ClientContext &context, TableFunct\n \t\t}\n \t\tcollection->Append(chunk);\n \t\tdata.collection = move(collection);\n+\t\tdata.collection->InitializeScan(state.scan_state);\n \t\tstate.initialized = true;\n \t}\n \n-\tif (state.chunk_index >= data.collection->ChunkCount()) {\n-\t\toutput.SetCardinality(0);\n-\t\treturn;\n-\t}\n-\toutput.Reference(data.collection->GetChunk(state.chunk_index++));\n+\tdata.collection->Scan(state.scan_state, output);\n }\n \n void PragmaLastProfilingOutput::RegisterFunction(BuiltinFunctions &set) {\ndiff --git a/src/include/duckdb/common/allocator.hpp b/src/include/duckdb/common/allocator.hpp\nindex 8c7fb63bd8ba..a43cbd573dcb 100644\n--- a/src/include/duckdb/common/allocator.hpp\n+++ b/src/include/duckdb/common/allocator.hpp\n@@ -33,8 +33,15 @@ typedef data_ptr_t (*reallocate_function_ptr_t)(PrivateAllocatorData *private_da\n \n class AllocatedData {\n public:\n-\tAllocatedData(Allocator &allocator, data_ptr_t pointer, idx_t allocated_size);\n-\t~AllocatedData();\n+\tDUCKDB_API AllocatedData();\n+\tDUCKDB_API AllocatedData(Allocator &allocator, data_ptr_t pointer, idx_t allocated_size);\n+\tDUCKDB_API ~AllocatedData();\n+\t// disable copy constructors\n+\tDUCKDB_API AllocatedData(const AllocatedData &other) = delete;\n+\tDUCKDB_API AllocatedData &operator=(const AllocatedData &) = delete;\n+\t//! enable move constructors\n+\tDUCKDB_API AllocatedData(AllocatedData &&other) noexcept;\n+\tDUCKDB_API AllocatedData &operator=(AllocatedData &&) noexcept;\n \n \tdata_ptr_t get() {\n \t\treturn pointer;\n@@ -48,7 +55,7 @@ class AllocatedData {\n \tvoid Reset();\n \n private:\n-\tAllocator &allocator;\n+\tAllocator *allocator;\n \tdata_ptr_t pointer;\n \tidx_t allocated_size;\n };\n@@ -66,8 +73,8 @@ class Allocator {\n \tvoid FreeData(data_ptr_t pointer, idx_t size);\n \tdata_ptr_t ReallocateData(data_ptr_t pointer, idx_t old_size, idx_t new_size);\n \n-\tunique_ptr<AllocatedData> Allocate(idx_t size) {\n-\t\treturn make_unique<AllocatedData>(*this, AllocateData(size), size);\n+\tAllocatedData Allocate(idx_t size) {\n+\t\treturn AllocatedData(*this, AllocateData(size), size);\n \t}\n \n \tstatic data_ptr_t DefaultAllocate(PrivateAllocatorData *private_data, idx_t size) {\ndiff --git a/src/include/duckdb/common/arrow.hpp b/src/include/duckdb/common/arrow/arrow.hpp\nsimilarity index 98%\nrename from src/include/duckdb/common/arrow.hpp\nrename to src/include/duckdb/common/arrow/arrow.hpp\nindex ac3fefd8b890..b2f613ec84f9 100644\n--- a/src/include/duckdb/common/arrow.hpp\n+++ b/src/include/duckdb/common/arrow/arrow.hpp\n@@ -1,7 +1,7 @@\n //===----------------------------------------------------------------------===//\n //                         DuckDB\n //\n-// duckdb/common/arrow.hpp\n+// duckdb/common/arrow/arrow.hpp\n //\n //\n //===----------------------------------------------------------------------===//\ndiff --git a/src/include/duckdb/common/arrow/arrow_appender.hpp b/src/include/duckdb/common/arrow/arrow_appender.hpp\nnew file mode 100644\nindex 000000000000..23bb763ac33b\n--- /dev/null\n+++ b/src/include/duckdb/common/arrow/arrow_appender.hpp\n@@ -0,0 +1,39 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/common/arrow/arrow_appender.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/common/arrow/arrow_converter.hpp\"\n+\n+struct ArrowSchema;\n+\n+namespace duckdb {\n+\n+struct ArrowAppendData;\n+\n+//! The ArrowAppender class can be used to incrementally construct an arrow array by appending data chunks into it\n+class ArrowAppender {\n+public:\n+\tDUCKDB_API ArrowAppender(vector<LogicalType> types, idx_t initial_capacity);\n+\tDUCKDB_API ~ArrowAppender();\n+\n+\t//! Append a data chunk to the underlying arrow array\n+\tDUCKDB_API void Append(DataChunk &input);\n+\t//! Returns the underlying arrow array\n+\tDUCKDB_API ArrowArray Finalize();\n+\n+private:\n+\t//! The types of the chunks that will be appended in\n+\tvector<LogicalType> types;\n+\t//! The root arrow append data\n+\tvector<unique_ptr<ArrowAppendData>> root_data;\n+\t//! The total row count that has been appended\n+\tidx_t row_count = 0;\n+};\n+\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/common/arrow/arrow_buffer.hpp b/src/include/duckdb/common/arrow/arrow_buffer.hpp\nnew file mode 100644\nindex 000000000000..770f5272a32d\n--- /dev/null\n+++ b/src/include/duckdb/common/arrow/arrow_buffer.hpp\n@@ -0,0 +1,92 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/common/arrow/arrow_buffer.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/common/common.hpp\"\n+\n+struct ArrowSchema;\n+\n+namespace duckdb {\n+\n+struct ArrowBuffer {\n+\tstatic constexpr const idx_t MINIMUM_SHRINK_SIZE = 4096;\n+\n+\tArrowBuffer() : dataptr(nullptr), count(0), capacity(0) {\n+\t}\n+\t~ArrowBuffer() {\n+\t\tif (!dataptr) {\n+\t\t\treturn;\n+\t\t}\n+\t\tfree(dataptr);\n+\t\tdataptr = nullptr;\n+\t\tcount = 0;\n+\t\tcapacity = 0;\n+\t}\n+\t// disable copy constructors\n+\tArrowBuffer(const ArrowBuffer &other) = delete;\n+\tArrowBuffer &operator=(const ArrowBuffer &) = delete;\n+\t//! enable move constructors\n+\tArrowBuffer(ArrowBuffer &&other) noexcept {\n+\t\tstd::swap(dataptr, other.dataptr);\n+\t\tstd::swap(count, other.count);\n+\t\tstd::swap(capacity, other.capacity);\n+\t}\n+\tArrowBuffer &operator=(ArrowBuffer &&other) noexcept {\n+\t\tstd::swap(dataptr, other.dataptr);\n+\t\tstd::swap(count, other.count);\n+\t\tstd::swap(capacity, other.capacity);\n+\t\treturn *this;\n+\t}\n+\n+\tvoid reserve(idx_t bytes) { // NOLINT\n+\t\tauto new_capacity = NextPowerOfTwo(bytes);\n+\t\tif (new_capacity <= capacity) {\n+\t\t\treturn;\n+\t\t}\n+\t\tReserveInternal(new_capacity);\n+\t}\n+\n+\tvoid resize(idx_t bytes) { // NOLINT\n+\t\treserve(bytes);\n+\t\tcount = bytes;\n+\t}\n+\n+\tvoid resize(idx_t bytes, data_t value) { // NOLINT\n+\t\treserve(bytes);\n+\t\tfor (idx_t i = count; i < bytes; i++) {\n+\t\t\tdataptr[i] = value;\n+\t\t}\n+\t\tcount = bytes;\n+\t}\n+\n+\tidx_t size() { // NOLINT\n+\t\treturn count;\n+\t}\n+\n+\tdata_ptr_t data() { // NOLINT\n+\t\treturn dataptr;\n+\t}\n+\n+private:\n+\tvoid ReserveInternal(idx_t bytes) {\n+\t\tif (dataptr) {\n+\t\t\tdataptr = (data_ptr_t)realloc(dataptr, bytes);\n+\t\t} else {\n+\t\t\tdataptr = (data_ptr_t)malloc(bytes);\n+\t\t}\n+\t\tcapacity = bytes;\n+\t}\n+\n+private:\n+\tdata_ptr_t dataptr = nullptr;\n+\tidx_t count = 0;\n+\tidx_t capacity = 0;\n+};\n+\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/common/arrow/arrow_converter.hpp b/src/include/duckdb/common/arrow/arrow_converter.hpp\nnew file mode 100644\nindex 000000000000..390add889352\n--- /dev/null\n+++ b/src/include/duckdb/common/arrow/arrow_converter.hpp\n@@ -0,0 +1,24 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/common/arrow/arrow_converter.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/common/types/data_chunk.hpp\"\n+#include \"duckdb/common/arrow/arrow.hpp\"\n+\n+struct ArrowSchema;\n+\n+namespace duckdb {\n+\n+struct ArrowConverter {\n+\tDUCKDB_API static void ToArrowSchema(ArrowSchema *out_schema, vector<LogicalType> &types, vector<string> &names,\n+\t                                     string &config_timezone);\n+\tDUCKDB_API static void ToArrowArray(DataChunk &input, ArrowArray *out_array);\n+};\n+\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/common/arrow_wrapper.hpp b/src/include/duckdb/common/arrow/arrow_wrapper.hpp\nsimilarity index 73%\nrename from src/include/duckdb/common/arrow_wrapper.hpp\nrename to src/include/duckdb/common/arrow/arrow_wrapper.hpp\nindex 6b57677502fc..44587011dc2f 100644\n--- a/src/include/duckdb/common/arrow_wrapper.hpp\n+++ b/src/include/duckdb/common/arrow/arrow_wrapper.hpp\n@@ -1,13 +1,13 @@\n //===----------------------------------------------------------------------===//\n //                         DuckDB\n //\n-// duckdb/common/arrow_wrapper.hpp\n+// duckdb/common/arrow/arrow_wrapper.hpp\n //\n //\n //===----------------------------------------------------------------------===//\n \n #pragma once\n-#include \"duckdb/common/arrow.hpp\"\n+#include \"duckdb/common/arrow/arrow.hpp\"\n #include \"duckdb/common/helper.hpp\"\n \n //! Here we have the internal duckdb classes that interact with Arrow's Internal Header (i.e., duckdb/commons/arrow.hpp)\n@@ -39,6 +39,8 @@ class ArrowArrayStreamWrapper {\n public:\n \tArrowArrayStream arrow_array_stream;\n \tint64_t number_of_rows;\n+\n+public:\n \tvoid GetSchema(ArrowSchemaWrapper &schema);\n \n \tshared_ptr<ArrowArrayWrapper> GetNextChunk();\n@@ -53,9 +55,11 @@ class ArrowArrayStreamWrapper {\n \n class ArrowUtil {\n public:\n-\tstatic unique_ptr<DataChunk> FetchChunk(QueryResult *result, idx_t chunk_size);\n+\tstatic bool TryFetchChunk(QueryResult *result, idx_t chunk_size, ArrowArray *out, idx_t &result_count,\n+\t                          string &error);\n+\tstatic idx_t FetchChunk(QueryResult *result, idx_t chunk_size, ArrowArray *out);\n \n private:\n-\tstatic unique_ptr<DataChunk> FetchNext(QueryResult &result);\n+\tstatic bool TryFetchNext(QueryResult &result, unique_ptr<DataChunk> &out, string &error);\n };\n } // namespace duckdb\ndiff --git a/src/include/duckdb/common/result_arrow_wrapper.hpp b/src/include/duckdb/common/arrow/result_arrow_wrapper.hpp\nsimilarity index 90%\nrename from src/include/duckdb/common/result_arrow_wrapper.hpp\nrename to src/include/duckdb/common/arrow/result_arrow_wrapper.hpp\nindex 0b478a9b52cb..789ec730ea3b 100644\n--- a/src/include/duckdb/common/result_arrow_wrapper.hpp\n+++ b/src/include/duckdb/common/arrow/result_arrow_wrapper.hpp\n@@ -1,7 +1,7 @@\n //===----------------------------------------------------------------------===//\n //                         DuckDB\n //\n-// duckdb/common/result_arrow_wrapper.hpp\n+// duckdb/common/arrow/result_arrow_wrapper.hpp\n //\n //\n //===----------------------------------------------------------------------===//\n@@ -9,7 +9,7 @@\n #pragma once\n \n #include \"duckdb/main/query_result.hpp\"\n-#include \"duckdb/common/arrow_wrapper.hpp\"\n+#include \"duckdb/common/arrow/arrow_wrapper.hpp\"\n \n namespace duckdb {\n class ResultArrowArrayStreamWrapper {\ndiff --git a/src/include/duckdb/common/enums/physical_operator_type.hpp b/src/include/duckdb/common/enums/physical_operator_type.hpp\nindex 2c4e38b5118e..64dbf6892b05 100644\n--- a/src/include/duckdb/common/enums/physical_operator_type.hpp\n+++ b/src/include/duckdb/common/enums/physical_operator_type.hpp\n@@ -38,6 +38,7 @@ enum class PhysicalOperatorType : uint8_t {\n \t// -----------------------------\n \tTABLE_SCAN,\n \tDUMMY_SCAN,\n+\tCOLUMN_DATA_SCAN,\n \tCHUNK_SCAN,\n \tRECURSIVE_CTE_SCAN,\n \tDELIM_SCAN,\ndiff --git a/src/include/duckdb/common/types.hpp b/src/include/duckdb/common/types.hpp\nindex edc3317ab8da..52d0ec436008 100644\n--- a/src/include/duckdb/common/types.hpp\n+++ b/src/include/duckdb/common/types.hpp\n@@ -190,7 +190,7 @@ struct string_t;\n \n template <class T>\n using child_list_t = std::vector<std::pair<std::string, T>>;\n-// we should be using single_thread_ptr here but cross-thread access to ChunkCollections currently prohibits this.\n+//! FIXME: this should be a single_thread_ptr\n template <class T>\n using buffer_ptr = shared_ptr<T>;\n \n@@ -437,7 +437,8 @@ struct LogicalType {\n \tDUCKDB_API bool IsIntegral() const;\n \tDUCKDB_API bool IsNumeric() const;\n \tDUCKDB_API hash_t Hash() const;\n-\tDUCKDB_API void SetAlias(string &alias);\n+\tDUCKDB_API void SetAlias(string alias);\n+\tDUCKDB_API bool HasAlias() const;\n \tDUCKDB_API string GetAlias() const;\n \n \tDUCKDB_API static LogicalType MaxLogicalType(const LogicalType &left, const LogicalType &right);\ndiff --git a/src/include/duckdb/common/types/arrow_aux_data.hpp b/src/include/duckdb/common/types/arrow_aux_data.hpp\nindex 9de201ae46e5..44edbe2119d1 100644\n--- a/src/include/duckdb/common/types/arrow_aux_data.hpp\n+++ b/src/include/duckdb/common/types/arrow_aux_data.hpp\n@@ -9,7 +9,7 @@\n #pragma once\n \n #include \"duckdb/common/types/vector_buffer.hpp\"\n-#include \"duckdb/common/arrow_wrapper.hpp\"\n+#include \"duckdb/common/arrow/arrow_wrapper.hpp\"\n \n namespace duckdb {\n \ndiff --git a/src/include/duckdb/common/types/batched_chunk_collection.hpp b/src/include/duckdb/common/types/batched_chunk_collection.hpp\ndeleted file mode 100644\nindex 38be9f9a8cd6..000000000000\n--- a/src/include/duckdb/common/types/batched_chunk_collection.hpp\n+++ /dev/null\n@@ -1,47 +0,0 @@\n-//===----------------------------------------------------------------------===//\n-//                         DuckDB\n-//\n-// duckdb/common/types/batched_chunk_collection.hpp\n-//\n-//\n-//===----------------------------------------------------------------------===//\n-\n-#pragma once\n-\n-#include \"duckdb/common/map.hpp\"\n-#include \"duckdb/common/types/chunk_collection.hpp\"\n-\n-namespace duckdb {\n-\n-struct BatchedChunkScanState {\n-\tmap<idx_t, unique_ptr<ChunkCollection>>::iterator iterator;\n-\tidx_t chunk_index;\n-};\n-\n-//!  A BatchedChunkCollection holds a number of data entries that are partitioned by batch index\n-//! Scans over a BatchedChunkCollection are ordered by batch index\n-class BatchedChunkCollection {\n-public:\n-\tDUCKDB_API BatchedChunkCollection(Allocator &allocator);\n-\n-\t//! Appends a datachunk with the given batch index to the batched collection\n-\tDUCKDB_API void Append(DataChunk &input, idx_t batch_index);\n-\n-\t//! Merge the other batched chunk collection into this batched collection\n-\tDUCKDB_API void Merge(BatchedChunkCollection &other);\n-\n-\t//! Initialize a scan over the batched chunk collection\n-\tDUCKDB_API void InitializeScan(BatchedChunkScanState &state);\n-\n-\t//! Scan a chunk from the batched chunk collection, in-order of batch index\n-\tDUCKDB_API void Scan(BatchedChunkScanState &state, DataChunk &output);\n-\n-\tDUCKDB_API string ToString() const;\n-\tDUCKDB_API void Print() const;\n-\n-private:\n-\tAllocator &allocator;\n-\t//! The data of the batched chunk collection - a set of batch_index -> ChunkCollection pointers\n-\tmap<idx_t, unique_ptr<ChunkCollection>> data;\n-};\n-} // namespace duckdb\ndiff --git a/src/include/duckdb/common/types/batched_data_collection.hpp b/src/include/duckdb/common/types/batched_data_collection.hpp\nnew file mode 100644\nindex 000000000000..87107996f568\n--- /dev/null\n+++ b/src/include/duckdb/common/types/batched_data_collection.hpp\n@@ -0,0 +1,60 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/common/types/batched_chunk_collection.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/common/map.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n+\n+namespace duckdb {\n+class BufferManager;\n+class ClientContext;\n+\n+struct BatchedChunkScanState {\n+\tmap<idx_t, unique_ptr<ColumnDataCollection>>::iterator iterator;\n+\tColumnDataScanState scan_state;\n+};\n+\n+//!  A BatchedDataCollection holds a number of data entries that are partitioned by batch index\n+//! Scans over a BatchedDataCollection are ordered by batch index\n+class BatchedDataCollection {\n+public:\n+\tDUCKDB_API BatchedDataCollection(vector<LogicalType> types);\n+\n+\t//! Appends a datachunk with the given batch index to the batched collection\n+\tDUCKDB_API void Append(DataChunk &input, idx_t batch_index);\n+\n+\t//! Merge the other batched chunk collection into this batched collection\n+\tDUCKDB_API void Merge(BatchedDataCollection &other);\n+\n+\t//! Initialize a scan over the batched chunk collection\n+\tDUCKDB_API void InitializeScan(BatchedChunkScanState &state);\n+\n+\t//! Scan a chunk from the batched chunk collection, in-order of batch index\n+\tDUCKDB_API void Scan(BatchedChunkScanState &state, DataChunk &output);\n+\n+\t//! Fetch a column data collection from the batched data collection - this consumes all of the data stored within\n+\tDUCKDB_API unique_ptr<ColumnDataCollection> FetchCollection();\n+\n+\tDUCKDB_API string ToString() const;\n+\tDUCKDB_API void Print() const;\n+\n+private:\n+\tstruct CachedCollection {\n+\t\tidx_t batch_index = DConstants::INVALID_INDEX;\n+\t\tColumnDataCollection *collection = nullptr;\n+\t\tColumnDataAppendState append_state;\n+\t};\n+\n+\tvector<LogicalType> types;\n+\t//! The data of the batched chunk collection - a set of batch_index -> ColumnDataCollection pointers\n+\tmap<idx_t, unique_ptr<ColumnDataCollection>> data;\n+\t//! The last batch collection that was inserted into\n+\tCachedCollection last_collection;\n+};\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/common/types/column_data_allocator.hpp b/src/include/duckdb/common/types/column_data_allocator.hpp\nnew file mode 100644\nindex 000000000000..7ec708c8f6f9\n--- /dev/null\n+++ b/src/include/duckdb/common/types/column_data_allocator.hpp\n@@ -0,0 +1,70 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/common/types/column_data_allocator.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n+\n+namespace duckdb {\n+struct ChunkMetaData;\n+\n+struct BlockMetaData {\n+\t//! The underlying block handle\n+\tshared_ptr<BlockHandle> handle;\n+\t//! How much space is currently used within the block\n+\tuint32_t size;\n+\t//! How much space is available in the block\n+\tuint32_t capacity;\n+\n+\tuint32_t Capacity();\n+};\n+\n+class ColumnDataAllocator {\n+public:\n+\tColumnDataAllocator(Allocator &allocator);\n+\tColumnDataAllocator(BufferManager &buffer_manager);\n+\tColumnDataAllocator(ClientContext &context, ColumnDataAllocatorType allocator_type);\n+\n+\t//! Returns an allocator object to allocate with. This returns the allocator in IN_MEMORY_ALLOCATOR, and a buffer\n+\t//! allocator in case of BUFFER_MANAGER_ALLOCATOR.\n+\tAllocator &GetAllocator();\n+\t//! Returns the allocator type\n+\tColumnDataAllocatorType GetType() {\n+\t\treturn type;\n+\t}\n+\n+public:\n+\tvoid AllocateData(idx_t size, uint32_t &block_id, uint32_t &offset, ChunkManagementState *chunk_state);\n+\n+\tvoid Initialize(ColumnDataAllocator &other);\n+\tvoid InitializeChunkState(ChunkManagementState &state, ChunkMetaData &meta_data);\n+\tdata_ptr_t GetDataPointer(ChunkManagementState &state, uint32_t block_id, uint32_t offset);\n+\n+private:\n+\tvoid AllocateBlock();\n+\tBufferHandle Pin(uint32_t block_id);\n+\n+\tbool HasBlocks() {\n+\t\treturn !blocks.empty();\n+\t}\n+\n+private:\n+\tColumnDataAllocatorType type;\n+\tunion {\n+\t\t//! The allocator object (if this is a IN_MEMORY_ALLOCATOR)\n+\t\tAllocator *allocator;\n+\t\t//! The buffer manager (if this is a BUFFER_MANAGER_ALLOCATOR)\n+\t\tBufferManager *buffer_manager;\n+\t} alloc;\n+\t//! The set of blocks used by the column data collection\n+\tvector<BlockMetaData> blocks;\n+\t//! The set of allocated data\n+\tvector<AllocatedData> allocated_data;\n+};\n+\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/common/types/column_data_collection.hpp b/src/include/duckdb/common/types/column_data_collection.hpp\nnew file mode 100644\nindex 000000000000..9ae9d6b6ab8d\n--- /dev/null\n+++ b/src/include/duckdb/common/types/column_data_collection.hpp\n@@ -0,0 +1,197 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/common/types/column_data_collection.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/common/types/column_data_collection_iterators.hpp\"\n+\n+namespace duckdb {\n+class BufferManager;\n+class BlockHandle;\n+class ClientContext;\n+struct ColumnDataCopyFunction;\n+class ColumnDataAllocator;\n+class ColumnDataCollection;\n+class ColumnDataCollectionSegment;\n+class ColumnDataRowCollection;\n+\n+//! The ColumnDataCollection represents a set of (buffer-managed) data stored in columnar format\n+//! It is efficient to read and scan\n+class ColumnDataCollection {\n+public:\n+\t//! Constructs an in-memory column data collection from an allocator\n+\tColumnDataCollection(Allocator &allocator, vector<LogicalType> types);\n+\t//! Constructs a buffer-managed column data collection\n+\tColumnDataCollection(BufferManager &buffer_manager, vector<LogicalType> types);\n+\t//! Constructs either an in-memory or a buffer-managed column data collection\n+\tColumnDataCollection(ClientContext &context, vector<LogicalType> types,\n+\t                     ColumnDataAllocatorType type = ColumnDataAllocatorType::BUFFER_MANAGER_ALLOCATOR);\n+\t//! Creates a column data collection that inherits the blocks to write to. This allows blocks to be shared\n+\t//! between multiple column data collections and prevents wasting space.\n+\t//! Note that after one CDC inherits blocks from another, the other\n+\t//! cannot be written to anymore (i.e. we take ownership of the half-written blocks).\n+\tColumnDataCollection(ColumnDataCollection &parent);\n+\tColumnDataCollection(shared_ptr<ColumnDataAllocator> allocator, vector<LogicalType> types);\n+\t~ColumnDataCollection();\n+\n+public:\n+\tDUCKDB_API vector<LogicalType> &Types() {\n+\t\treturn types;\n+\t}\n+\tDUCKDB_API const vector<LogicalType> &Types() const {\n+\t\treturn types;\n+\t}\n+\n+\t//! The amount of rows in the ColumnDataCollection\n+\tDUCKDB_API const idx_t &Count() const {\n+\t\treturn count;\n+\t}\n+\n+\t//! The amount of columns in the ColumnDataCollection\n+\tDUCKDB_API idx_t ColumnCount() const {\n+\t\treturn types.size();\n+\t}\n+\n+\t//! Initializes an Append state - useful for optimizing many appends made to the same column data collection\n+\tDUCKDB_API void InitializeAppend(ColumnDataAppendState &state);\n+\t//! Append a DataChunk to this ColumnDataCollection using the specified append state\n+\tDUCKDB_API void Append(ColumnDataAppendState &state, DataChunk &new_chunk);\n+\n+\t//! Initializes a chunk with the correct types that can be used to call Scan\n+\tDUCKDB_API void InitializeScanChunk(DataChunk &chunk) const;\n+\t//! Initializes a chunk with the correct types for a given scan state\n+\tDUCKDB_API void InitializeScanChunk(ColumnDataScanState &state, DataChunk &chunk) const;\n+\t//! Initializes a Scan state for scanning all columns\n+\tDUCKDB_API void\n+\tInitializeScan(ColumnDataScanState &state,\n+\t               ColumnDataScanProperties properties = ColumnDataScanProperties::ALLOW_ZERO_COPY) const;\n+\t//! Initializes a Scan state for scanning a subset of the columns\n+\tDUCKDB_API void\n+\tInitializeScan(ColumnDataScanState &state, vector<column_t> column_ids,\n+\t               ColumnDataScanProperties properties = ColumnDataScanProperties::ALLOW_ZERO_COPY) const;\n+\t//! Initialize a parallel scan over the column data collection over all columns\n+\tDUCKDB_API void InitializeScan(ColumnDataParallelScanState &state) const;\n+\t//! Initialize a parallel scan over the column data collection over a subset of the columns\n+\tDUCKDB_API void InitializeScan(ColumnDataParallelScanState &state, vector<column_t> column_ids) const;\n+\t//! Scans a DataChunk from the ColumnDataCollection\n+\tDUCKDB_API bool Scan(ColumnDataScanState &state, DataChunk &result) const;\n+\t//! Scans a DataChunk from the ColumnDataCollection\n+\tDUCKDB_API bool Scan(ColumnDataParallelScanState &state, ColumnDataLocalScanState &lstate, DataChunk &result) const;\n+\n+\t//! Append a DataChunk directly to this ColumnDataCollection - calls InitializeAppend and Append internally\n+\tDUCKDB_API void Append(DataChunk &new_chunk);\n+\n+\t//! Appends the other ColumnDataCollection to this, destroying the other data collection\n+\tDUCKDB_API void Combine(ColumnDataCollection &other);\n+\n+\tDUCKDB_API void Verify();\n+\n+\tDUCKDB_API string ToString() const;\n+\tDUCKDB_API void Print() const;\n+\n+\tDUCKDB_API void Reset();\n+\n+\t//! Returns the number of data chunks present in the ColumnDataCollection\n+\tDUCKDB_API idx_t ChunkCount() const;\n+\t//! Fetch an individual chunk from the ColumnDataCollection\n+\tDUCKDB_API void FetchChunk(idx_t chunk_idx, DataChunk &result) const;\n+\n+\t//! Constructs a class that can be iterated over to fetch individual chunks\n+\t//! Iterating over this is syntactic sugar over just calling Scan\n+\tDUCKDB_API ColumnDataChunkIterationHelper Chunks() const;\n+\t//! Constructs a class that can be iterated over to fetch individual chunks\n+\t//! Only the column indexes specified in the column_ids list are scanned\n+\tDUCKDB_API ColumnDataChunkIterationHelper Chunks(vector<column_t> column_ids) const;\n+\n+\t//! Constructs a class that can be iterated over to fetch individual rows\n+\t//! Note that row iteration is slow, and the `.Chunks()` method should be used instead\n+\tDUCKDB_API ColumnDataRowIterationHelper Rows() const;\n+\n+\t//! Returns a materialized set of all of the rows in the column data collection\n+\t//! Note that usage of this is slow - avoid using this unless the amount of rows is small, or if you do not care\n+\t//! about performance\n+\tDUCKDB_API ColumnDataRowCollection GetRows() const;\n+\n+\t//! Compare two column data collections to another. If they are equal according to result equality rules,\n+\t//! return true. That means null values are equal, and approx equality is used for floating point values.\n+\t//! If they are not equal, return false and fill in the error message.\n+\tstatic bool ResultEquals(const ColumnDataCollection &left, const ColumnDataCollection &right,\n+\t                         string &error_message);\n+\n+private:\n+\t//! Initialize the column data collection\n+\tvoid Initialize(vector<LogicalType> types);\n+\n+\t//! Creates a new segment within the ColumnDataCollection\n+\tvoid CreateSegment();\n+\n+\tstatic ColumnDataCopyFunction GetCopyFunction(const LogicalType &type);\n+\n+\t//! Obtains the next scan index to scan from\n+\tbool NextScanIndex(ColumnDataScanState &state, idx_t &chunk_index, idx_t &segment_index, idx_t &row_index) const;\n+\n+private:\n+\t//! The Column Data Allocator\n+\tbuffer_ptr<ColumnDataAllocator> allocator;\n+\t//! The types of the stored entries\n+\tvector<LogicalType> types;\n+\t//! The number of entries stored in the column data collection\n+\tidx_t count;\n+\t//! The data segments of the column data collection\n+\tvector<unique_ptr<ColumnDataCollectionSegment>> segments;\n+\t//! The set of copy functions\n+\tvector<ColumnDataCopyFunction> copy_functions;\n+\t//! When the column data collection is marked as finished - new tuples can no longer be appended to it\n+\tbool finished_append;\n+};\n+\n+//! The ColumnDataRowCollection represents a set of materialized rows, as obtained from the ColumnDataCollection\n+class ColumnDataRowCollection {\n+public:\n+\tDUCKDB_API ColumnDataRowCollection(const ColumnDataCollection &collection);\n+\n+public:\n+\tDUCKDB_API Value GetValue(idx_t column, idx_t index) const;\n+\n+public:\n+\t// container API\n+\tDUCKDB_API bool empty() const {\n+\t\treturn rows.empty();\n+\t}\n+\tDUCKDB_API idx_t size() const {\n+\t\treturn rows.size();\n+\t}\n+\n+\tDUCKDB_API ColumnDataRow &operator[](idx_t i);\n+\tDUCKDB_API const ColumnDataRow &operator[](idx_t i) const;\n+\n+\tDUCKDB_API vector<ColumnDataRow>::iterator begin() {\n+\t\treturn rows.begin();\n+\t}\n+\tDUCKDB_API vector<ColumnDataRow>::iterator end() {\n+\t\treturn rows.end();\n+\t}\n+\tDUCKDB_API vector<ColumnDataRow>::const_iterator cbegin() const {\n+\t\treturn rows.cbegin();\n+\t}\n+\tDUCKDB_API vector<ColumnDataRow>::const_iterator cend() const {\n+\t\treturn rows.cend();\n+\t}\n+\tDUCKDB_API vector<ColumnDataRow>::const_iterator begin() const {\n+\t\treturn rows.begin();\n+\t}\n+\tDUCKDB_API vector<ColumnDataRow>::const_iterator end() const {\n+\t\treturn rows.end();\n+\t}\n+\n+private:\n+\tvector<ColumnDataRow> rows;\n+\tvector<unique_ptr<DataChunk>> chunks;\n+};\n+\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/common/types/column_data_collection_iterators.hpp b/src/include/duckdb/common/types/column_data_collection_iterators.hpp\nnew file mode 100644\nindex 000000000000..0c9a06759d76\n--- /dev/null\n+++ b/src/include/duckdb/common/types/column_data_collection_iterators.hpp\n@@ -0,0 +1,90 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/common/types/column_data_collection_iterators.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/common/types/column_data_scan_states.hpp\"\n+\n+namespace duckdb {\n+class ColumnDataCollection;\n+\n+class ColumnDataChunkIterationHelper {\n+public:\n+\tDUCKDB_API ColumnDataChunkIterationHelper(const ColumnDataCollection &collection, vector<column_t> column_ids);\n+\n+private:\n+\tconst ColumnDataCollection &collection;\n+\tvector<column_t> column_ids;\n+\n+private:\n+\tclass ColumnDataChunkIterator;\n+\n+\tclass ColumnDataChunkIterator {\n+\tpublic:\n+\t\tDUCKDB_API explicit ColumnDataChunkIterator(const ColumnDataCollection *collection_p,\n+\t\t                                            vector<column_t> column_ids);\n+\n+\t\tconst ColumnDataCollection *collection;\n+\t\tColumnDataScanState scan_state;\n+\t\tshared_ptr<DataChunk> scan_chunk;\n+\t\tidx_t row_index;\n+\n+\tpublic:\n+\t\tDUCKDB_API void Next();\n+\n+\t\tDUCKDB_API ColumnDataChunkIterator &operator++();\n+\t\tDUCKDB_API bool operator!=(const ColumnDataChunkIterator &other) const;\n+\t\tDUCKDB_API DataChunk &operator*() const;\n+\t};\n+\n+public:\n+\tDUCKDB_API ColumnDataChunkIterator begin() {\n+\t\treturn ColumnDataChunkIterator(&collection, column_ids);\n+\t}\n+\tDUCKDB_API ColumnDataChunkIterator end() {\n+\t\treturn ColumnDataChunkIterator(nullptr, vector<column_t>());\n+\t}\n+};\n+\n+class ColumnDataRowIterationHelper {\n+public:\n+\tDUCKDB_API ColumnDataRowIterationHelper(const ColumnDataCollection &collection);\n+\n+private:\n+\tconst ColumnDataCollection &collection;\n+\n+private:\n+\tclass ColumnDataRowIterator;\n+\n+\tclass ColumnDataRowIterator {\n+\tpublic:\n+\t\tDUCKDB_API explicit ColumnDataRowIterator(const ColumnDataCollection *collection_p);\n+\n+\t\tconst ColumnDataCollection *collection;\n+\t\tColumnDataScanState scan_state;\n+\t\tshared_ptr<DataChunk> scan_chunk;\n+\t\tColumnDataRow current_row;\n+\n+\tpublic:\n+\t\tvoid Next();\n+\n+\t\tDUCKDB_API ColumnDataRowIterator &operator++();\n+\t\tDUCKDB_API bool operator!=(const ColumnDataRowIterator &other) const;\n+\t\tDUCKDB_API const ColumnDataRow &operator*() const;\n+\t};\n+\n+public:\n+\tDUCKDB_API ColumnDataRowIterator begin() {\n+\t\treturn ColumnDataRowIterator(&collection);\n+\t}\n+\tDUCKDB_API ColumnDataRowIterator end() {\n+\t\treturn ColumnDataRowIterator(nullptr);\n+\t}\n+};\n+\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/common/types/column_data_collection_segment.hpp b/src/include/duckdb/common/types/column_data_collection_segment.hpp\nnew file mode 100644\nindex 000000000000..bdbda3a52d40\n--- /dev/null\n+++ b/src/include/duckdb/common/types/column_data_collection_segment.hpp\n@@ -0,0 +1,127 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/common/types/column_data_collection_segment.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n+#include \"duckdb/common/types/column_data_allocator.hpp\"\n+\n+namespace duckdb {\n+\n+struct VectorChildIndex {\n+\texplicit VectorChildIndex(idx_t index = DConstants::INVALID_INDEX) : index(index) {\n+\t}\n+\n+\tidx_t index;\n+\n+\tbool IsValid() {\n+\t\treturn index != DConstants::INVALID_INDEX;\n+\t}\n+};\n+\n+struct VectorDataIndex {\n+\texplicit VectorDataIndex(idx_t index = DConstants::INVALID_INDEX) : index(index) {\n+\t}\n+\n+\tidx_t index;\n+\n+\tbool IsValid() {\n+\t\treturn index != DConstants::INVALID_INDEX;\n+\t}\n+};\n+\n+struct VectorMetaData {\n+\t//! Where the vector data lives\n+\tuint32_t block_id;\n+\tuint32_t offset;\n+\t//! The number of entries present in this vector\n+\tuint16_t count;\n+\n+\t//! Child data of this vector (used only for lists and structs)\n+\t//! Note: child indices are stored with one layer of indirection\n+\t//! The child_index here refers to the `child_indices` array in the ColumnDataCollectionSegment\n+\t//! The entry in the child_indices array then refers to the actual `VectorMetaData` index\n+\t//! In case of structs, the child_index refers to the FIRST child in the `child_indices` array\n+\t//! Subsequent children are stored consecutively, i.e.\n+\t//! first child: segment.child_indices[child_index + 0]\n+\t//! nth child  : segment.child_indices[child_index + (n - 1)]\n+\tVectorChildIndex child_index;\n+\t//! Next vector entry (in case there is more data - used only in case of children of lists)\n+\tVectorDataIndex next_data;\n+};\n+\n+struct ChunkMetaData {\n+\t//! The set of vectors of the chunk\n+\tvector<VectorDataIndex> vector_data;\n+\t//! The block ids referenced by the chunk\n+\tunordered_set<uint32_t> block_ids;\n+\t//! The number of entries in the chunk\n+\tuint16_t count;\n+};\n+\n+class ColumnDataCollectionSegment {\n+public:\n+\tColumnDataCollectionSegment(shared_ptr<ColumnDataAllocator> allocator, vector<LogicalType> types_p);\n+\n+\tshared_ptr<ColumnDataAllocator> allocator;\n+\t//! The types of the chunks\n+\tvector<LogicalType> types;\n+\t//! The number of entries in the internal column data\n+\tidx_t count;\n+\t//! Set of chunk meta data\n+\tvector<ChunkMetaData> chunk_data;\n+\t//! Set of vector meta data\n+\tvector<VectorMetaData> vector_data;\n+\t//! The set of child indices\n+\tvector<VectorDataIndex> child_indices;\n+\t//! The string heap for the column data collection\n+\t// FIXME: we should get rid of the string heap and store strings as LIST<UINT8>\n+\tStringHeap heap;\n+\n+public:\n+\tvoid AllocateNewChunk();\n+\t//! Allocate space for a vector of a specific type in the segment\n+\tVectorDataIndex AllocateVector(const LogicalType &type, ChunkMetaData &chunk_data,\n+\t                               ChunkManagementState *chunk_state = nullptr,\n+\t                               VectorDataIndex prev_index = VectorDataIndex());\n+\t//! Allocate space for a vector during append,\n+\tVectorDataIndex AllocateVector(const LogicalType &type, ChunkMetaData &chunk_data,\n+\t                               ColumnDataAppendState &append_state, VectorDataIndex prev_index = VectorDataIndex());\n+\n+\tvoid InitializeChunkState(idx_t chunk_index, ChunkManagementState &state);\n+\tvoid ReadChunk(idx_t chunk_index, ChunkManagementState &state, DataChunk &chunk,\n+\t               const vector<column_t> &column_ids);\n+\n+\tidx_t ReadVector(ChunkManagementState &state, VectorDataIndex vector_index, Vector &result);\n+\n+\tVectorDataIndex GetChildIndex(VectorChildIndex index, idx_t child_entry = 0);\n+\tVectorChildIndex AddChildIndex(VectorDataIndex index);\n+\tVectorChildIndex ReserveChildren(idx_t child_count);\n+\tvoid SetChildIndex(VectorChildIndex base_idx, idx_t child_number, VectorDataIndex index);\n+\n+\tVectorMetaData &GetVectorData(VectorDataIndex index) {\n+\t\tD_ASSERT(index.index < vector_data.size());\n+\t\treturn vector_data[index.index];\n+\t}\n+\n+\tidx_t ChunkCount() const;\n+\tvoid FetchChunk(idx_t chunk_idx, DataChunk &result);\n+\tvoid FetchChunk(idx_t chunk_idx, DataChunk &result, const vector<column_t> &column_ids);\n+\n+\tvoid Verify();\n+\n+\tstatic idx_t GetDataSize(idx_t type_size);\n+\tstatic validity_t *GetValidityPointer(data_ptr_t base_ptr, idx_t type_size);\n+\n+private:\n+\tidx_t ReadVectorInternal(ChunkManagementState &state, VectorDataIndex vector_index, Vector &result);\n+\tVectorDataIndex AllocateVectorInternal(const LogicalType &type, ChunkMetaData &chunk_meta,\n+\t                                       ChunkManagementState *chunk_state);\n+};\n+\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/common/types/column_data_scan_states.hpp b/src/include/duckdb/common/types/column_data_scan_states.hpp\nnew file mode 100644\nindex 000000000000..733b8132eda1\n--- /dev/null\n+++ b/src/include/duckdb/common/types/column_data_scan_states.hpp\n@@ -0,0 +1,79 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/common/types/column_data_scan_states.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/common/types/data_chunk.hpp\"\n+#include \"duckdb/common/unordered_map.hpp\"\n+#include \"duckdb/common/unordered_set.hpp\"\n+#include \"duckdb/common/mutex.hpp\"\n+\n+namespace duckdb {\n+\n+enum class ColumnDataAllocatorType : uint8_t {\n+\t//! Use a buffer manager to allocate large chunks of memory that vectors then use\n+\tBUFFER_MANAGER_ALLOCATOR,\n+\t//! Use an in-memory allocator, allocating data for every chunk\n+\t//! This causes the column data collection to behave similar to the old chunk collection\n+\tIN_MEMORY_ALLOCATOR\n+};\n+\n+enum class ColumnDataScanProperties : uint8_t {\n+\tINVALID,\n+\t//! Allow zero copy scans - this introduces a dependency on the resulting vector on the scan state of the column\n+\t//! data collection, which means vectors might not be valid anymore after the next chunk is scanned.\n+\tALLOW_ZERO_COPY,\n+\t//! Disallow zero-copy scans, always copying data into the target vector\n+\t//! As a result, data scanned will be valid even after the column data collection is destroyed\n+\tDISALLOW_ZERO_COPY\n+};\n+\n+struct ChunkManagementState {\n+\tunordered_map<idx_t, BufferHandle> handles;\n+\tColumnDataScanProperties properties = ColumnDataScanProperties::INVALID;\n+};\n+\n+struct ColumnDataAppendState {\n+\tChunkManagementState current_chunk_state;\n+\tvector<UnifiedVectorFormat> vector_data;\n+};\n+\n+struct ColumnDataScanState {\n+\tChunkManagementState current_chunk_state;\n+\tidx_t segment_index;\n+\tidx_t chunk_index;\n+\tidx_t current_row_index;\n+\tidx_t next_row_index;\n+\tColumnDataScanProperties properties;\n+\tvector<column_t> column_ids;\n+};\n+\n+struct ColumnDataParallelScanState {\n+\tColumnDataScanState scan_state;\n+\tmutex lock;\n+};\n+\n+struct ColumnDataLocalScanState {\n+\tChunkManagementState current_chunk_state;\n+\tidx_t current_row_index;\n+};\n+\n+class ColumnDataRow {\n+public:\n+\tColumnDataRow(DataChunk &chunk, idx_t row_index, idx_t base_index);\n+\n+\tDataChunk &chunk;\n+\tidx_t row_index;\n+\tidx_t base_index;\n+\n+public:\n+\tValue GetValue(idx_t column_index) const;\n+\tidx_t RowIndex() const;\n+};\n+\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/common/types/data_chunk.hpp b/src/include/duckdb/common/types/data_chunk.hpp\nindex 2b2848d27ac0..9ddae044e310 100644\n--- a/src/include/duckdb/common/types/data_chunk.hpp\n+++ b/src/include/duckdb/common/types/data_chunk.hpp\n@@ -12,7 +12,7 @@\n #include \"duckdb/common/types/vector.hpp\"\n #include \"duckdb/common/winapi.hpp\"\n #include \"duckdb/common/allocator.hpp\"\n-#include \"duckdb/common/arrow_wrapper.hpp\"\n+#include \"duckdb/common/arrow/arrow_wrapper.hpp\"\n \n struct ArrowArray;\n \n@@ -137,9 +137,6 @@ class DataChunk {\n \t//! FUNCTION ONLY!\n \tDUCKDB_API void Verify();\n \n-\t//! export data chunk as a arrow struct array that can be imported as arrow record batch\n-\tDUCKDB_API void ToArrowArray(ArrowArray *out_array);\n-\n private:\n \t//! The amount of tuples stored in the data chunk\n \tidx_t count;\ndiff --git a/src/include/duckdb/common/types/value.hpp b/src/include/duckdb/common/types/value.hpp\nindex 70a65144987d..17ff75f97995 100644\n--- a/src/include/duckdb/common/types/value.hpp\n+++ b/src/include/duckdb/common/types/value.hpp\n@@ -56,6 +56,9 @@ class Value {\n \t// move assignment\n \tDUCKDB_API Value &operator=(Value &&other) noexcept;\n \n+\tinline LogicalType &type() {\n+\t\treturn type_;\n+\t}\n \tinline const LogicalType &type() const {\n \t\treturn type_;\n \t}\ndiff --git a/src/include/duckdb/common/types/vector.hpp b/src/include/duckdb/common/types/vector.hpp\nindex 06f86c7b98e0..d123b9032407 100644\n--- a/src/include/duckdb/common/types/vector.hpp\n+++ b/src/include/duckdb/common/types/vector.hpp\n@@ -29,7 +29,6 @@ struct UnifiedVectorFormat {\n class VectorCache;\n class VectorStructBuffer;\n class VectorListBuffer;\n-class ChunkCollection;\n \n struct SelCache;\n \n@@ -176,7 +175,9 @@ class Vector {\n \n private:\n \t//! Returns the [index] element of the Vector as a Value.\n-\tDUCKDB_API static Value GetValue(const Vector &v, idx_t index);\n+\tstatic Value GetValue(const Vector &v, idx_t index);\n+\t//! Returns the [index] element of the Vector as a Value.\n+\tstatic Value GetValueInternal(const Vector &v, idx_t index);\n \n protected:\n \t//! The vector type specifies how the data of the vector is physically stored (i.e. if it is a single repeated\ndiff --git a/src/include/duckdb/common/types/vector_buffer.hpp b/src/include/duckdb/common/types/vector_buffer.hpp\nindex d1e091d0ecf1..2351b05d4997 100644\n--- a/src/include/duckdb/common/types/vector_buffer.hpp\n+++ b/src/include/duckdb/common/types/vector_buffer.hpp\n@@ -19,7 +19,6 @@ namespace duckdb {\n class BufferHandle;\n class VectorBuffer;\n class Vector;\n-class ChunkCollection;\n \n enum class VectorBufferType : uint8_t {\n \tSTANDARD_BUFFER,     // standard buffer, holds a single array of data\ndiff --git a/src/include/duckdb/execution/nested_loop_join.hpp b/src/include/duckdb/execution/nested_loop_join.hpp\nindex 3b7bce47bac4..83c414a40ad8 100644\n--- a/src/include/duckdb/execution/nested_loop_join.hpp\n+++ b/src/include/duckdb/execution/nested_loop_join.hpp\n@@ -9,11 +9,12 @@\n #pragma once\n \n #include \"duckdb/common/common.hpp\"\n-#include \"duckdb/common/types/chunk_collection.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n #include \"duckdb/common/types/vector.hpp\"\n #include \"duckdb/planner/operator/logical_comparison_join.hpp\"\n \n namespace duckdb {\n+class ColumnDataCollection;\n \n struct NestedLoopJoinInner {\n \tstatic idx_t Perform(idx_t &ltuple, idx_t &rtuple, DataChunk &left_conditions, DataChunk &right_conditions,\n@@ -21,7 +22,7 @@ struct NestedLoopJoinInner {\n };\n \n struct NestedLoopJoinMark {\n-\tstatic void Perform(DataChunk &left, ChunkCollection &right, bool found_match[],\n+\tstatic void Perform(DataChunk &left, ColumnDataCollection &right, bool found_match[],\n \t                    const vector<JoinCondition> &conditions);\n };\n \ndiff --git a/src/include/duckdb/execution/operator/join/outer_join_marker.hpp b/src/include/duckdb/execution/operator/join/outer_join_marker.hpp\nnew file mode 100644\nindex 000000000000..b25fd3c5e181\n--- /dev/null\n+++ b/src/include/duckdb/execution/operator/join/outer_join_marker.hpp\n@@ -0,0 +1,69 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/execution/operator/join/outer_join_marker.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/common/mutex.hpp\"\n+#include \"duckdb/execution/physical_operator.hpp\"\n+#include \"duckdb/execution/operator/join/physical_comparison_join.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n+\n+namespace duckdb {\n+\n+struct OuterJoinGlobalScanState {\n+\tmutex lock;\n+\tColumnDataCollection *data = nullptr;\n+\tColumnDataParallelScanState global_scan;\n+};\n+\n+struct OuterJoinLocalScanState {\n+\tDataChunk scan_chunk;\n+\tSelectionVector match_sel;\n+\tColumnDataLocalScanState local_scan;\n+};\n+\n+class OuterJoinMarker {\n+public:\n+\tOuterJoinMarker(bool enabled);\n+\n+\tbool Enabled() {\n+\t\treturn enabled;\n+\t}\n+\t//! Initializes the outer join counter\n+\tvoid Initialize(idx_t count);\n+\t//! Resets the outer join counter\n+\tvoid Reset();\n+\n+\t//! Sets an indiivdual match\n+\tvoid SetMatch(idx_t position);\n+\n+\t//! Sets multiple matches\n+\tvoid SetMatches(const SelectionVector &sel, idx_t count, idx_t base_idx = 0);\n+\n+\t//! Constructs a left-join result based on which tuples have not found matches\n+\tvoid ConstructLeftJoinResult(DataChunk &left, DataChunk &result);\n+\n+\t//! Returns the maximum number of threads that can be associated with an right-outer join scan\n+\tidx_t MaxThreads() const;\n+\n+\t//! Initialize a scan\n+\tvoid InitializeScan(ColumnDataCollection &data, OuterJoinGlobalScanState &gstate);\n+\n+\t//! Initialize a local scan\n+\tvoid InitializeScan(OuterJoinGlobalScanState &gstate, OuterJoinLocalScanState &lstate);\n+\n+\t//! Perform the scan\n+\tvoid Scan(OuterJoinGlobalScanState &gstate, OuterJoinLocalScanState &lstate, DataChunk &result);\n+\n+private:\n+\tbool enabled;\n+\tunique_ptr<bool[]> found_match;\n+\tidx_t count;\n+};\n+\n+} // namespace duckdb\ndiff --git a/src/include/duckdb/execution/operator/join/physical_blockwise_nl_join.hpp b/src/include/duckdb/execution/operator/join/physical_blockwise_nl_join.hpp\nindex b6b5c4de892f..fcae67aeed99 100644\n--- a/src/include/duckdb/execution/operator/join/physical_blockwise_nl_join.hpp\n+++ b/src/include/duckdb/execution/operator/join/physical_blockwise_nl_join.hpp\n@@ -40,6 +40,8 @@ class PhysicalBlockwiseNLJoin : public PhysicalJoin {\n public:\n \t// Source interface\n \tunique_ptr<GlobalSourceState> GetGlobalSourceState(ClientContext &context) const override;\n+\tunique_ptr<LocalSourceState> GetLocalSourceState(ExecutionContext &context,\n+\t                                                 GlobalSourceState &gstate) const override;\n \tvoid GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,\n \t             LocalSourceState &lstate) const override;\n \n@@ -53,7 +55,6 @@ class PhysicalBlockwiseNLJoin : public PhysicalJoin {\n public:\n \t// Sink interface\n \tunique_ptr<GlobalSinkState> GetGlobalSinkState(ClientContext &context) const override;\n-\n \tunique_ptr<LocalSinkState> GetLocalSinkState(ExecutionContext &context) const override;\n \tSinkResultType Sink(ExecutionContext &context, GlobalSinkState &state, LocalSinkState &lstate,\n \t                    DataChunk &input) const override;\ndiff --git a/src/include/duckdb/execution/operator/join/physical_comparison_join.hpp b/src/include/duckdb/execution/operator/join/physical_comparison_join.hpp\nindex 10a319424d4f..fcd0459460dc 100644\n--- a/src/include/duckdb/execution/operator/join/physical_comparison_join.hpp\n+++ b/src/include/duckdb/execution/operator/join/physical_comparison_join.hpp\n@@ -12,7 +12,8 @@\n #include \"duckdb/execution/expression_executor.hpp\"\n \n namespace duckdb {\n-class ChunkCollection;\n+class ColumnDataCollection;\n+struct ColumnDataScanState;\n \n //! PhysicalJoin represents the base class of the join operators\n class PhysicalComparisonJoin : public PhysicalJoin {\n@@ -28,8 +29,8 @@ class PhysicalComparisonJoin : public PhysicalJoin {\n \t//! Construct the join result of a join with an empty RHS\n \tstatic void ConstructEmptyJoinResult(JoinType type, bool has_null, DataChunk &input, DataChunk &result);\n \t//! Construct the remainder of a Full Outer Join based on which tuples in the RHS found no match\n-\tstatic void ConstructFullOuterJoinResult(bool *found_match, ChunkCollection &input, DataChunk &result,\n-\t                                         idx_t &scan_position);\n+\tstatic void ConstructFullOuterJoinResult(bool *found_match, ColumnDataCollection &input, DataChunk &result,\n+\t                                         ColumnDataScanState &scan_state);\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/execution/operator/join/physical_cross_product.hpp b/src/include/duckdb/execution/operator/join/physical_cross_product.hpp\nindex a290dd1f4004..850cd7ff7854 100644\n--- a/src/include/duckdb/execution/operator/join/physical_cross_product.hpp\n+++ b/src/include/duckdb/execution/operator/join/physical_cross_product.hpp\n@@ -8,10 +8,11 @@\n \n #pragma once\n \n-#include \"duckdb/common/types/chunk_collection.hpp\"\n #include \"duckdb/execution/physical_operator.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n \n namespace duckdb {\n+\n //! PhysicalCrossProduct represents a cross product between two tables\n class PhysicalCrossProduct : public PhysicalOperator {\n public:\n@@ -50,4 +51,36 @@ class PhysicalCrossProduct : public PhysicalOperator {\n \tvector<const PhysicalOperator *> GetSources() const override;\n };\n \n+class CrossProductExecutor {\n+public:\n+\tCrossProductExecutor(ColumnDataCollection &rhs);\n+\n+\tOperatorResultType Execute(DataChunk &input, DataChunk &output);\n+\n+\tbool ScanLHS() {\n+\t\treturn scan_input_chunk;\n+\t}\n+\n+\tidx_t PositionInChunk() {\n+\t\treturn position_in_chunk;\n+\t}\n+\n+\tidx_t ScanPosition() {\n+\t\treturn scan_state.current_row_index;\n+\t}\n+\n+private:\n+\tvoid Reset(DataChunk &input, DataChunk &output);\n+\tbool NextValue(DataChunk &input, DataChunk &output);\n+\n+private:\n+\tColumnDataCollection &rhs;\n+\tColumnDataScanState scan_state;\n+\tDataChunk scan_chunk;\n+\tidx_t position_in_chunk;\n+\tbool initialized;\n+\tbool finished;\n+\tbool scan_input_chunk;\n+};\n+\n } // namespace duckdb\ndiff --git a/src/include/duckdb/execution/operator/join/physical_delim_join.hpp b/src/include/duckdb/execution/operator/join/physical_delim_join.hpp\nindex 73523c29f896..c921fa3685f4 100644\n--- a/src/include/duckdb/execution/operator/join/physical_delim_join.hpp\n+++ b/src/include/duckdb/execution/operator/join/physical_delim_join.hpp\n@@ -15,7 +15,7 @@ namespace duckdb {\n class PhysicalHashAggregate;\n \n //! PhysicalDelimJoin represents a join where the LHS will be duplicate eliminated and pushed into a\n-//! PhysicalChunkCollectionScan in the RHS.\n+//! PhysicalColumnDataScan in the RHS.\n class PhysicalDelimJoin : public PhysicalOperator {\n public:\n \tPhysicalDelimJoin(vector<LogicalType> types, unique_ptr<PhysicalOperator> original_join,\ndiff --git a/src/include/duckdb/execution/operator/join/physical_join.hpp b/src/include/duckdb/execution/operator/join/physical_join.hpp\nindex 520c5698213d..97cfaad9f15b 100644\n--- a/src/include/duckdb/execution/operator/join/physical_join.hpp\n+++ b/src/include/duckdb/execution/operator/join/physical_join.hpp\n@@ -27,7 +27,6 @@ class PhysicalJoin : public PhysicalOperator {\n \tstatic void ConstructAntiJoinResult(DataChunk &left, DataChunk &result, bool found_match[]);\n \tstatic void ConstructMarkJoinResult(DataChunk &join_keys, DataChunk &left, DataChunk &result, bool found_match[],\n \t                                    bool has_null);\n-\tstatic void ConstructLeftJoinResult(DataChunk &left, DataChunk &result, bool found_match[]);\n \n public:\n \tstatic void BuildJoinPipelines(Executor &executor, Pipeline &current, PipelineBuildState &state,\ndiff --git a/src/include/duckdb/execution/operator/join/physical_nested_loop_join.hpp b/src/include/duckdb/execution/operator/join/physical_nested_loop_join.hpp\nindex 97a96406e74a..f2568dd5fe4b 100644\n--- a/src/include/duckdb/execution/operator/join/physical_nested_loop_join.hpp\n+++ b/src/include/duckdb/execution/operator/join/physical_nested_loop_join.hpp\n@@ -40,6 +40,8 @@ class PhysicalNestedLoopJoin : public PhysicalComparisonJoin {\n public:\n \t// Source interface\n \tunique_ptr<GlobalSourceState> GetGlobalSourceState(ClientContext &context) const override;\n+\tunique_ptr<LocalSourceState> GetLocalSourceState(ExecutionContext &context,\n+\t                                                 GlobalSourceState &gstate) const override;\n \tvoid GetData(ExecutionContext &context, DataChunk &chunk, GlobalSourceState &gstate,\n \t             LocalSourceState &lstate) const override;\n \n@@ -69,6 +71,10 @@ class PhysicalNestedLoopJoin : public PhysicalComparisonJoin {\n \n \tstatic bool IsSupported(const vector<JoinCondition> &conditions);\n \n+public:\n+\t//! Returns a list of the types of the join conditions\n+\tvector<LogicalType> GetJoinTypes() const;\n+\n private:\n \t// resolve joins that output max N elements (SEMI, ANTI, MARK)\n \tvoid ResolveSimpleJoin(ExecutionContext &context, DataChunk &input, DataChunk &chunk, OperatorState &state) const;\ndiff --git a/src/include/duckdb/execution/operator/order/physical_top_n.hpp b/src/include/duckdb/execution/operator/order/physical_top_n.hpp\nindex 82325779b32c..47442c595a02 100644\n--- a/src/include/duckdb/execution/operator/order/physical_top_n.hpp\n+++ b/src/include/duckdb/execution/operator/order/physical_top_n.hpp\n@@ -48,9 +48,6 @@ class PhysicalTopN : public PhysicalOperator {\n \t}\n \n \tstring ParamsToString() const override;\n-\n-private:\n-\tunique_ptr<idx_t[]> ComputeTopN(ChunkCollection &big_data, idx_t &heap_size);\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/execution/operator/scan/physical_chunk_scan.hpp b/src/include/duckdb/execution/operator/scan/physical_column_data_scan.hpp\nsimilarity index 59%\nrename from src/include/duckdb/execution/operator/scan/physical_chunk_scan.hpp\nrename to src/include/duckdb/execution/operator/scan/physical_column_data_scan.hpp\nindex 7700e8f7f389..4a0f852184c9 100644\n--- a/src/include/duckdb/execution/operator/scan/physical_chunk_scan.hpp\n+++ b/src/include/duckdb/execution/operator/scan/physical_column_data_scan.hpp\n@@ -1,29 +1,29 @@\n //===----------------------------------------------------------------------===//\n //                         DuckDB\n //\n-// duckdb/execution/operator/scan/physical_chunk_scan.hpp\n+// duckdb/execution/operator/scan/physical_column_data_scan.hpp\n //\n //\n //===----------------------------------------------------------------------===//\n \n #pragma once\n \n-#include \"duckdb/common/types/chunk_collection.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n #include \"duckdb/execution/physical_operator.hpp\"\n \n namespace duckdb {\n \n-//! The PhysicalChunkCollectionScan scans a Chunk Collection\n-class PhysicalChunkScan : public PhysicalOperator {\n+//! The PhysicalColumnDataScan scans a ColumnDataCollection\n+class PhysicalColumnDataScan : public PhysicalOperator {\n public:\n-\tPhysicalChunkScan(vector<LogicalType> types, PhysicalOperatorType op_type, idx_t estimated_cardinality)\n+\tPhysicalColumnDataScan(vector<LogicalType> types, PhysicalOperatorType op_type, idx_t estimated_cardinality)\n \t    : PhysicalOperator(op_type, move(types), estimated_cardinality), collection(nullptr) {\n \t}\n \n-\t// the chunk collection to scan\n-\tChunkCollection *collection;\n-\t//! Owned chunk collection, if any\n-\tunique_ptr<ChunkCollection> owned_collection;\n+\t// the column data collection to scan\n+\tColumnDataCollection *collection;\n+\t//! Owned column data collection, if any\n+\tunique_ptr<ColumnDataCollection> owned_collection;\n \n public:\n \tunique_ptr<GlobalSourceState> GetGlobalSourceState(ClientContext &context) const override;\ndiff --git a/src/include/duckdb/execution/operator/set/physical_recursive_cte.hpp b/src/include/duckdb/execution/operator/set/physical_recursive_cte.hpp\nindex e2f9f05f804c..7d5f966c9c4f 100644\n--- a/src/include/duckdb/execution/operator/set/physical_recursive_cte.hpp\n+++ b/src/include/duckdb/execution/operator/set/physical_recursive_cte.hpp\n@@ -9,6 +9,7 @@\n #pragma once\n \n #include \"duckdb/execution/physical_operator.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n \n namespace duckdb {\n class Pipeline;\n@@ -21,7 +22,7 @@ class PhysicalRecursiveCTE : public PhysicalOperator {\n \t~PhysicalRecursiveCTE() override;\n \n \tbool union_all;\n-\tstd::shared_ptr<ChunkCollection> working_table;\n+\tstd::shared_ptr<ColumnDataCollection> working_table;\n \tvector<shared_ptr<Pipeline>> pipelines;\n \n public:\ndiff --git a/src/include/duckdb/execution/physical_plan_generator.hpp b/src/include/duckdb/execution/physical_plan_generator.hpp\nindex 5d4df0c0db18..43a05f36ee5a 100644\n--- a/src/include/duckdb/execution/physical_plan_generator.hpp\n+++ b/src/include/duckdb/execution/physical_plan_generator.hpp\n@@ -13,24 +13,24 @@\n #include \"duckdb/planner/logical_operator.hpp\"\n #include \"duckdb/planner/logical_tokens.hpp\"\n #include \"duckdb/planner/operator/logical_limit_percent.hpp\"\n-#include \"duckdb/common/types/chunk_collection.hpp\"\n #include \"duckdb/common/unordered_map.hpp\"\n #include \"duckdb/common/unordered_set.hpp\"\n \n namespace duckdb {\n class ClientContext;\n+class ColumnDataCollection;\n \n //! The physical plan generator generates a physical execution plan from a\n //! logical query plan\n class PhysicalPlanGenerator {\n public:\n-\texplicit PhysicalPlanGenerator(ClientContext &context) : context(context) {\n-\t}\n+\texplicit PhysicalPlanGenerator(ClientContext &context);\n+\t~PhysicalPlanGenerator();\n \n \tunordered_set<CatalogEntry *> dependencies;\n \t//! Recursive CTEs require at least one ChunkScan, referencing the working_table.\n \t//! This data structure is used to establish it.\n-\tunordered_map<idx_t, std::shared_ptr<ChunkCollection>> rec_ctes;\n+\tunordered_map<idx_t, std::shared_ptr<ColumnDataCollection>> recursive_cte_tables;\n \n public:\n \t//! Creates a plan from the logical operator. This involves resolving column bindings and generating physical\n@@ -42,7 +42,7 @@ class PhysicalPlanGenerator {\n \n \tunique_ptr<PhysicalOperator> CreatePlan(LogicalAggregate &op);\n \tunique_ptr<PhysicalOperator> CreatePlan(LogicalAnyJoin &op);\n-\tunique_ptr<PhysicalOperator> CreatePlan(LogicalChunkGet &op);\n+\tunique_ptr<PhysicalOperator> CreatePlan(LogicalColumnDataGet &op);\n \tunique_ptr<PhysicalOperator> CreatePlan(LogicalComparisonJoin &op);\n \tunique_ptr<PhysicalOperator> CreatePlan(LogicalCreate &op);\n \tunique_ptr<PhysicalOperator> CreatePlan(LogicalCreateTable &op);\ndiff --git a/src/include/duckdb/function/table/arrow.hpp b/src/include/duckdb/function/table/arrow.hpp\nindex 10d65b877327..1f6f01a4dd42 100644\n--- a/src/include/duckdb/function/table/arrow.hpp\n+++ b/src/include/duckdb/function/table/arrow.hpp\n@@ -9,7 +9,7 @@\n #pragma once\n \n #include \"duckdb/function/table_function.hpp\"\n-#include \"duckdb/common/arrow_wrapper.hpp\"\n+#include \"duckdb/common/arrow/arrow_wrapper.hpp\"\n #include \"duckdb/common/atomic.hpp\"\n #include \"duckdb/common/mutex.hpp\"\n #include \"duckdb/common/pair.hpp\"\n@@ -37,6 +37,7 @@ enum class ArrowDateTimeType : uint8_t {\n struct ArrowConvertData {\n \tArrowConvertData(LogicalType type) : dictionary_type(type) {};\n \tArrowConvertData() {};\n+\n \t//! Hold type of dictionary\n \tLogicalType dictionary_type;\n \t//! If its a variable size type (e.g., strings, blobs, lists) holds which type it is\n@@ -45,9 +46,18 @@ struct ArrowConvertData {\n \tvector<ArrowDateTimeType> date_time_precision;\n };\n \n-typedef unique_ptr<ArrowArrayStreamWrapper> (*stream_factory_produce_t)(\n-    uintptr_t stream_factory_ptr, pair<unordered_map<idx_t, string>, vector<string>> &project_columns,\n-    TableFilterSet *filters);\n+struct ArrowProjectedColumns {\n+\tunordered_map<idx_t, string> projection_map;\n+\tvector<string> columns;\n+};\n+\n+struct ArrowStreamParameters {\n+\tArrowProjectedColumns projected_columns;\n+\tTableFilterSet *filters;\n+};\n+\n+typedef unique_ptr<ArrowArrayStreamWrapper> (*stream_factory_produce_t)(uintptr_t stream_factory_ptr,\n+                                                                        ArrowStreamParameters &parameters);\n typedef void (*stream_factory_get_schema_t)(uintptr_t stream_factory_ptr, ArrowSchemaWrapper &schema);\n \n struct ArrowScanFunctionData : public PyTableFunctionData {\ndiff --git a/src/include/duckdb/main/appender.hpp b/src/include/duckdb/main/appender.hpp\nindex 6f8724b5bd22..f38ee51684ff 100644\n--- a/src/include/duckdb/main/appender.hpp\n+++ b/src/include/duckdb/main/appender.hpp\n@@ -11,10 +11,10 @@\n #include \"duckdb/common/types/data_chunk.hpp\"\n #include \"duckdb/common/winapi.hpp\"\n #include \"duckdb/main/table_description.hpp\"\n-#include \"duckdb/common/types/chunk_collection.hpp\"\n \n namespace duckdb {\n \n+class ColumnDataCollection;\n class ClientContext;\n class DuckDB;\n class TableCatalogEntry;\n@@ -23,16 +23,16 @@ class Connection;\n //! The Appender class can be used to append elements to a table.\n class BaseAppender {\n protected:\n-\t//! The amount of chunks that will be gathered in the chunk collection before flushing\n-\tstatic constexpr const idx_t FLUSH_COUNT = 100;\n+\t//! The amount of tuples that will be gathered in the column data collection before flushing\n+\tstatic constexpr const idx_t FLUSH_COUNT = STANDARD_VECTOR_SIZE * 100;\n \n \tAllocator &allocator;\n \t//! The append types\n \tvector<LogicalType> types;\n \t//! The buffered data for the append\n-\tChunkCollection collection;\n+\tunique_ptr<ColumnDataCollection> collection;\n \t//! Internal chunk used for appends\n-\tunique_ptr<DataChunk> chunk;\n+\tDataChunk chunk;\n \t//! The current column to append to\n \tidx_t column = 0;\n \n@@ -78,7 +78,7 @@ class BaseAppender {\n \n protected:\n \tvoid Destructor();\n-\tvirtual void FlushInternal(ChunkCollection &collection) = 0;\n+\tvirtual void FlushInternal(ColumnDataCollection &collection) = 0;\n \tvoid InitializeChunk();\n \tvoid FlushChunk();\n \n@@ -112,7 +112,7 @@ class Appender : public BaseAppender {\n \tDUCKDB_API ~Appender() override;\n \n protected:\n-\tvoid FlushInternal(ChunkCollection &collection) override;\n+\tvoid FlushInternal(ColumnDataCollection &collection) override;\n };\n \n class InternalAppender : public BaseAppender {\n@@ -126,7 +126,7 @@ class InternalAppender : public BaseAppender {\n \tDUCKDB_API ~InternalAppender() override;\n \n protected:\n-\tvoid FlushInternal(ChunkCollection &collection) override;\n+\tvoid FlushInternal(ColumnDataCollection &collection) override;\n };\n \n template <>\ndiff --git a/src/include/duckdb/main/client_config.hpp b/src/include/duckdb/main/client_config.hpp\nindex 4683cdda68e7..13ea439648e4 100644\n--- a/src/include/duckdb/main/client_config.hpp\n+++ b/src/include/duckdb/main/client_config.hpp\n@@ -78,8 +78,9 @@ struct ClientConfig {\n \n public:\n \tstatic ClientConfig &GetConfig(ClientContext &context);\n+\tstatic const ClientConfig &GetConfig(const ClientContext &context);\n \n-\tstatic string ExtractTimezoneFromConfig(ClientConfig &config);\n+\tstring ExtractTimezone() const;\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/main/client_context.hpp b/src/include/duckdb/main/client_context.hpp\nindex a586a5b256d5..fc76d5a9da48 100644\n--- a/src/include/duckdb/main/client_context.hpp\n+++ b/src/include/duckdb/main/client_context.hpp\n@@ -29,7 +29,7 @@ namespace duckdb {\n class Appender;\n class Catalog;\n class CatalogSearchPath;\n-class ChunkCollection;\n+class ColumnDataCollection;\n class DatabaseInstance;\n class FileOpener;\n class LogicalOperator;\n@@ -107,7 +107,7 @@ class ClientContext : public std::enable_shared_from_this<ClientContext> {\n \t//! Get the table info of a specific table, or nullptr if it cannot be found\n \tDUCKDB_API unique_ptr<TableDescription> TableInfo(const string &schema_name, const string &table_name);\n \t//! Appends a DataChunk to the specified table. Returns whether or not the append was successful.\n-\tDUCKDB_API void Append(TableDescription &description, ChunkCollection &collection);\n+\tDUCKDB_API void Append(TableDescription &description, ColumnDataCollection &collection);\n \t//! Try to bind a relation in the current client context; either throws an exception or fills the result_columns\n \t//! list with the set of returned columns\n \tDUCKDB_API void TryBindRelation(Relation &relation, vector<ColumnDefinition> &result_columns);\n@@ -159,7 +159,7 @@ class ClientContext : public std::enable_shared_from_this<ClientContext> {\n \tDUCKDB_API bool TryGetCurrentSetting(const std::string &key, Value &result);\n \n \t//! Returns the parser options for this client context\n-\tDUCKDB_API ParserOptions GetParserOptions();\n+\tDUCKDB_API ParserOptions GetParserOptions() const;\n \n \tDUCKDB_API unique_ptr<DataChunk> Fetch(ClientContextLock &lock, StreamQueryResult &result);\n \n@@ -175,6 +175,8 @@ class ClientContext : public std::enable_shared_from_this<ClientContext> {\n \t//! Fetch a list of table names that are required for a given query\n \tDUCKDB_API unordered_set<string> GetTableNames(const string &query);\n \n+\tDUCKDB_API ClientProperties GetClientProperties() const;\n+\n private:\n \t//! Parse statements and resolve pragmas from a query\n \tbool ParseStatements(ClientContextLock &lock, const string &query, vector<unique_ptr<SQLStatement>> &result,\ndiff --git a/src/include/duckdb/main/config.hpp b/src/include/duckdb/main/config.hpp\nindex f3ef58df58b6..7b7d02237936 100644\n--- a/src/include/duckdb/main/config.hpp\n+++ b/src/include/duckdb/main/config.hpp\n@@ -147,6 +147,8 @@ struct DBConfig {\n public:\n \tDUCKDB_API static DBConfig &GetConfig(ClientContext &context);\n \tDUCKDB_API static DBConfig &GetConfig(DatabaseInstance &db);\n+\tDUCKDB_API static const DBConfig &GetConfig(const ClientContext &context);\n+\tDUCKDB_API static const DBConfig &GetConfig(const DatabaseInstance &db);\n \tDUCKDB_API static vector<ConfigurationOption> GetOptions();\n \tDUCKDB_API static idx_t GetOptionCount();\n \ndiff --git a/src/include/duckdb/main/connection.hpp b/src/include/duckdb/main/connection.hpp\nindex edbc8ede1595..269936f8860c 100644\n--- a/src/include/duckdb/main/connection.hpp\n+++ b/src/include/duckdb/main/connection.hpp\n@@ -23,7 +23,7 @@\n \n namespace duckdb {\n \n-class ChunkCollection;\n+class ColumnDataCollection;\n class ClientContext;\n \n class DatabaseInstance;\n@@ -106,8 +106,8 @@ class Connection {\n \n \t//! Appends a DataChunk to the specified table\n \tDUCKDB_API void Append(TableDescription &description, DataChunk &chunk);\n-\t//! Appends a ChunkCollection to the specified table\n-\tDUCKDB_API void Append(TableDescription &description, ChunkCollection &collection);\n+\t//! Appends a ColumnDataCollection to the specified table\n+\tDUCKDB_API void Append(TableDescription &description, ColumnDataCollection &collection);\n \n \t//! Returns a relation that produces a table from this connection\n \tDUCKDB_API shared_ptr<Relation> Table(const string &tname);\ndiff --git a/src/include/duckdb/main/materialized_query_result.hpp b/src/include/duckdb/main/materialized_query_result.hpp\nindex 5269b69ca3d7..b6f738f0e8b5 100644\n--- a/src/include/duckdb/main/materialized_query_result.hpp\n+++ b/src/include/duckdb/main/materialized_query_result.hpp\n@@ -8,7 +8,7 @@\n \n #pragma once\n \n-#include \"duckdb/common/types/chunk_collection.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n #include \"duckdb/common/winapi.hpp\"\n #include \"duckdb/main/query_result.hpp\"\n \n@@ -21,25 +21,21 @@ class MaterializedQueryResult : public QueryResult {\n \tfriend class ClientContext;\n \t//! Creates a successful query result with the specified names and types\n \tDUCKDB_API MaterializedQueryResult(StatementType statement_type, StatementProperties properties,\n-\t                                   vector<LogicalType> types, vector<string> names,\n-\t                                   const shared_ptr<ClientContext> &context);\n+\t                                   vector<string> names, unique_ptr<ColumnDataCollection> collection,\n+\t                                   ClientProperties client_properties);\n \t//! Creates an unsuccessful query result with error condition\n \tDUCKDB_API explicit MaterializedQueryResult(string error);\n \n-\tChunkCollection collection;\n-\n-\t//! The client context this MaterializedQueryResult belongs to\n-\tstd::weak_ptr<ClientContext> context;\n-\n public:\n \t//! Fetches a DataChunk from the query result.\n-\t//! This will consume the result (i.e. the chunks are taken directly from the ChunkCollection).\n+\t//! This will consume the result (i.e. the result can only be scanned once with this function)\n \tDUCKDB_API unique_ptr<DataChunk> Fetch() override;\n \tDUCKDB_API unique_ptr<DataChunk> FetchRaw() override;\n \t//! Converts the QueryResult to a string\n \tDUCKDB_API string ToString() override;\n \n-\t//! Gets the (index) value of the (column index) column\n+\t//! Gets the (index) value of the (column index) column.\n+\t//! Note: this is very slow. Scanning over the underlying collection is much faster.\n \tDUCKDB_API Value GetValue(idx_t column, idx_t index);\n \n \ttemplate <class T>\n@@ -47,6 +43,19 @@ class MaterializedQueryResult : public QueryResult {\n \t\tauto value = GetValue(column, index);\n \t\treturn (T)value.GetValue<int64_t>();\n \t}\n+\n+\tDUCKDB_API idx_t RowCount() const;\n+\n+\t//! Returns a reference to the underlying column data collection\n+\tColumnDataCollection &Collection();\n+\n+private:\n+\tunique_ptr<ColumnDataCollection> collection;\n+\t//! Row collection, only created if GetValue is called\n+\tunique_ptr<ColumnDataRowCollection> row_collection;\n+\t//! Scan state for Fetch calls\n+\tColumnDataScanState scan_state;\n+\tbool scan_initialized;\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/main/query_result.hpp b/src/include/duckdb/main/query_result.hpp\nindex 570fa1f9ac96..921047b16530 100644\n--- a/src/include/duckdb/main/query_result.hpp\n+++ b/src/include/duckdb/main/query_result.hpp\n@@ -12,12 +12,15 @@\n #include \"duckdb/common/types/data_chunk.hpp\"\n #include \"duckdb/common/winapi.hpp\"\n \n-struct ArrowSchema;\n-\n namespace duckdb {\n \n enum class QueryResultType : uint8_t { MATERIALIZED_RESULT, STREAM_RESULT, PENDING_RESULT };\n \n+//! A set of properties from the client context that can be used to interpret the query result\n+struct ClientProperties {\n+\tstring timezone;\n+};\n+\n class BaseQueryResult {\n public:\n \t//! Creates a successful query result with the specified names and types\n@@ -55,11 +58,13 @@ class QueryResult : public BaseQueryResult {\n public:\n \t//! Creates a successful query result with the specified names and types\n \tDUCKDB_API QueryResult(QueryResultType type, StatementType statement_type, StatementProperties properties,\n-\t                       vector<LogicalType> types, vector<string> names);\n+\t                       vector<LogicalType> types, vector<string> names, ClientProperties client_properties);\n \t//! Creates an unsuccessful query result with error condition\n \tDUCKDB_API QueryResult(QueryResultType type, string error);\n \tDUCKDB_API virtual ~QueryResult() override;\n \n+\t//! Properties from the client context\n+\tClientProperties client_properties;\n \t//! The next result (if any)\n \tunique_ptr<QueryResult> next;\n \n@@ -91,20 +96,13 @@ class QueryResult : public BaseQueryResult {\n \t\t}\n \t}\n \n-\tDUCKDB_API static void ToArrowSchema(ArrowSchema *out_schema, vector<LogicalType> &types, vector<string> &names,\n-\t                                     string &config_timezone);\n-\n \tstatic string GetConfigTimezone(QueryResult &query_result);\n \n-private:\n-\t//! The current chunk used by the iterator\n-\tunique_ptr<DataChunk> iterator_chunk;\n-\n private:\n \tclass QueryResultIterator;\n \tclass QueryResultRow {\n \tpublic:\n-\t\texplicit QueryResultRow(QueryResultIterator &iterator) : iterator(iterator), row(0) {\n+\t\texplicit QueryResultRow(QueryResultIterator &iterator_p, idx_t row_idx) : iterator(iterator_p), row(0) {\n \t\t}\n \n \t\tQueryResultIterator &iterator;\n@@ -112,32 +110,39 @@ class QueryResult : public BaseQueryResult {\n \n \t\ttemplate <class T>\n \t\tT GetValue(idx_t col_idx) const {\n-\t\t\treturn iterator.result->iterator_chunk->GetValue(col_idx, iterator.row_idx).GetValue<T>();\n+\t\t\treturn iterator.chunk->GetValue(col_idx, row).GetValue<T>();\n \t\t}\n \t};\n \t//! The row-based query result iterator. Invoking the\n \tclass QueryResultIterator {\n \tpublic:\n-\t\texplicit QueryResultIterator(QueryResult *result) : current_row(*this), result(result), row_idx(0) {\n+\t\texplicit QueryResultIterator(QueryResult *result) : current_row(*this, 0), result(result), base_row(0) {\n \t\t\tif (result) {\n-\t\t\t\tresult->iterator_chunk = result->Fetch();\n+\t\t\t\tchunk = shared_ptr<DataChunk>(result->Fetch().release());\n \t\t\t}\n \t\t}\n \n \t\tQueryResultRow current_row;\n+\t\tshared_ptr<DataChunk> chunk;\n \t\tQueryResult *result;\n-\t\tidx_t row_idx;\n+\t\tidx_t base_row;\n \n \tpublic:\n \t\tvoid Next() {\n-\t\t\tif (!result->iterator_chunk) {\n+\t\t\tif (!chunk) {\n \t\t\t\treturn;\n \t\t\t}\n \t\t\tcurrent_row.row++;\n-\t\t\trow_idx++;\n-\t\t\tif (row_idx >= result->iterator_chunk->size()) {\n-\t\t\t\tresult->iterator_chunk = result->Fetch();\n-\t\t\t\trow_idx = 0;\n+\t\t\tif (current_row.row >= chunk->size()) {\n+\t\t\t\tbase_row += chunk->size();\n+\t\t\t\tchunk = result->Fetch();\n+\t\t\t\tcurrent_row.row = 0;\n+\t\t\t\tif (!chunk || chunk->size() == 0) {\n+\t\t\t\t\t// exhausted all rows\n+\t\t\t\t\tbase_row = 0;\n+\t\t\t\t\tresult = nullptr;\n+\t\t\t\t\tchunk.reset();\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \n@@ -146,7 +151,7 @@ class QueryResult : public BaseQueryResult {\n \t\t\treturn *this;\n \t\t}\n \t\tbool operator!=(const QueryResultIterator &other) const {\n-\t\t\treturn result->iterator_chunk && result->iterator_chunk->ColumnCount() > 0;\n+\t\t\treturn result != other.result || base_row != other.base_row || current_row.row != other.current_row.row;\n \t\t}\n \t\tconst QueryResultRow &operator*() const {\n \t\t\treturn current_row;\ndiff --git a/src/include/duckdb/parallel/pipeline.hpp b/src/include/duckdb/parallel/pipeline.hpp\nindex 2df636a1043c..f598713ad14a 100644\n--- a/src/include/duckdb/parallel/pipeline.hpp\n+++ b/src/include/duckdb/parallel/pipeline.hpp\n@@ -95,6 +95,8 @@ class Pipeline : public std::enable_shared_from_this<Pipeline> {\n private:\n \t//! Whether or not the pipeline has been readied\n \tbool ready;\n+\t//! Whether or not the pipeline has been initialized\n+\tatomic<bool> initialized;\n \t//! The source of this pipeline\n \tPhysicalOperator *source;\n \t//! The chain of intermediate operators\ndiff --git a/src/include/duckdb/parser/parser.hpp b/src/include/duckdb/parser/parser.hpp\nindex 8c196e7900a2..04d46b29418c 100644\n--- a/src/include/duckdb/parser/parser.hpp\n+++ b/src/include/duckdb/parser/parser.hpp\n@@ -25,7 +25,7 @@ class ParserExtension;\n struct ParserOptions {\n \tbool preserve_identifier_case = true;\n \tidx_t max_expression_depth = 1000;\n-\tvector<ParserExtension> *extensions = nullptr;\n+\tconst vector<ParserExtension> *extensions = nullptr;\n };\n \n //! The parser is responsible for parsing the query and converting it into a set\ndiff --git a/src/include/duckdb/planner/logical_tokens.hpp b/src/include/duckdb/planner/logical_tokens.hpp\nindex 728a7617a496..ce351c5449dc 100644\n--- a/src/include/duckdb/planner/logical_tokens.hpp\n+++ b/src/include/duckdb/planner/logical_tokens.hpp\n@@ -14,7 +14,7 @@ class LogicalOperator;\n \n class LogicalAggregate;\n class LogicalAnyJoin;\n-class LogicalChunkGet;\n+class LogicalColumnDataGet;\n class LogicalComparisonJoin;\n class LogicalCopyToFile;\n class LogicalCreate;\ndiff --git a/src/include/duckdb/planner/operator/list.hpp b/src/include/duckdb/planner/operator/list.hpp\nindex fe3a94c98f00..04dacff94c47 100644\n--- a/src/include/duckdb/planner/operator/list.hpp\n+++ b/src/include/duckdb/planner/operator/list.hpp\n@@ -1,6 +1,6 @@\n #include \"duckdb/planner/operator/logical_aggregate.hpp\"\n #include \"duckdb/planner/operator/logical_any_join.hpp\"\n-#include \"duckdb/planner/operator/logical_chunk_get.hpp\"\n+#include \"duckdb/planner/operator/logical_column_data_get.hpp\"\n #include \"duckdb/planner/operator/logical_comparison_join.hpp\"\n #include \"duckdb/planner/operator/logical_copy_to_file.hpp\"\n #include \"duckdb/planner/operator/logical_create.hpp\"\ndiff --git a/src/include/duckdb/planner/operator/logical_chunk_get.hpp b/src/include/duckdb/planner/operator/logical_column_data_get.hpp\nsimilarity index 69%\nrename from src/include/duckdb/planner/operator/logical_chunk_get.hpp\nrename to src/include/duckdb/planner/operator/logical_column_data_get.hpp\nindex dd95b60f5ba0..250710d1d5c0 100644\n--- a/src/include/duckdb/planner/operator/logical_chunk_get.hpp\n+++ b/src/include/duckdb/planner/operator/logical_column_data_get.hpp\n@@ -1,22 +1,22 @@\n //===----------------------------------------------------------------------===//\n //                         DuckDB\n //\n-// duckdb/planner/operator/logical_chunk_get.hpp\n+// duckdb/planner/operator/logical_column_data_get.hpp\n //\n //\n //===----------------------------------------------------------------------===//\n \n #pragma once\n \n-#include \"duckdb/common/types/chunk_collection.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n #include \"duckdb/planner/logical_operator.hpp\"\n \n namespace duckdb {\n \n-//! LogicalChunkGet represents a scan operation from a ChunkCollection\n-class LogicalChunkGet : public LogicalOperator {\n+//! LogicalColumnDataGet represents a scan operation from a ColumnDataCollection\n+class LogicalColumnDataGet : public LogicalOperator {\n public:\n-\tLogicalChunkGet(idx_t table_index, vector<LogicalType> types, unique_ptr<ChunkCollection> collection)\n+\tLogicalColumnDataGet(idx_t table_index, vector<LogicalType> types, unique_ptr<ColumnDataCollection> collection)\n \t    : LogicalOperator(LogicalOperatorType::LOGICAL_CHUNK_GET), table_index(table_index),\n \t      collection(move(collection)) {\n \t\tD_ASSERT(types.size() > 0);\n@@ -28,7 +28,7 @@ class LogicalChunkGet : public LogicalOperator {\n \t//! The types of the chunk\n \tvector<LogicalType> chunk_types;\n \t//! The chunk collection to scan\n-\tunique_ptr<ChunkCollection> collection;\n+\tunique_ptr<ColumnDataCollection> collection;\n \n public:\n \tvector<ColumnBinding> GetColumnBindings() override {\ndiff --git a/src/include/duckdb/planner/operator/logical_cteref.hpp b/src/include/duckdb/planner/operator/logical_cteref.hpp\nindex f5d7599e0cd9..a90a20de6c28 100644\n--- a/src/include/duckdb/planner/operator/logical_cteref.hpp\n+++ b/src/include/duckdb/planner/operator/logical_cteref.hpp\n@@ -13,7 +13,7 @@\n \n namespace duckdb {\n \n-//! LogicalChunkGet represents a scan operation from a ChunkCollection\n+//! LogicalCTERef represents a reference to a recursive CTE\n class LogicalCTERef : public LogicalOperator {\n public:\n \tLogicalCTERef(idx_t table_index, idx_t cte_index, vector<LogicalType> types, vector<string> colnames)\ndiff --git a/src/include/duckdb/storage/arena_allocator.hpp b/src/include/duckdb/storage/arena_allocator.hpp\nindex 6a398770d412..200f6fd0b0e5 100644\n--- a/src/include/duckdb/storage/arena_allocator.hpp\n+++ b/src/include/duckdb/storage/arena_allocator.hpp\n@@ -17,7 +17,7 @@ struct ArenaChunk {\n \tArenaChunk(Allocator &allocator, idx_t size);\n \t~ArenaChunk();\n \n-\tunique_ptr<AllocatedData> data;\n+\tAllocatedData data;\n \tidx_t current_position;\n \tidx_t maximum_size;\n \tunique_ptr<ArenaChunk> next;\ndiff --git a/src/main/appender.cpp b/src/main/appender.cpp\nindex 8c1a236204e0..6830cfe38179 100644\n--- a/src/main/appender.cpp\n+++ b/src/main/appender.cpp\n@@ -9,14 +9,16 @@\n #include \"duckdb/common/string_util.hpp\"\n #include \"duckdb/common/operator/cast_operators.hpp\"\n #include \"duckdb/common/operator/string_cast.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n \n namespace duckdb {\n \n-BaseAppender::BaseAppender(Allocator &allocator) : allocator(allocator), collection(allocator), column(0) {\n+BaseAppender::BaseAppender(Allocator &allocator) : allocator(allocator), column(0) {\n }\n \n-BaseAppender::BaseAppender(Allocator &allocator, vector<LogicalType> types_p)\n-    : allocator(allocator), types(move(types_p)), collection(allocator), column(0) {\n+BaseAppender::BaseAppender(Allocator &allocator_p, vector<LogicalType> types_p)\n+    : allocator(allocator_p), types(move(types_p)), collection(make_unique<ColumnDataCollection>(allocator, types)),\n+      column(0) {\n \tInitializeChunk();\n }\n \n@@ -54,6 +56,7 @@ Appender::Appender(Connection &con, const string &schema_name, const string &tab\n \t\ttypes.push_back(column.Type());\n \t}\n \tInitializeChunk();\n+\tcollection = make_unique<ColumnDataCollection>(allocator, types);\n }\n \n Appender::Appender(Connection &con, const string &table_name) : Appender(con, DEFAULT_SCHEMA, table_name) {\n@@ -64,8 +67,7 @@ Appender::~Appender() {\n }\n \n void BaseAppender::InitializeChunk() {\n-\tchunk = make_unique<DataChunk>();\n-\tchunk->Initialize(allocator, types);\n+\tchunk.Initialize(allocator, types);\n }\n \n void BaseAppender::BeginRow() {\n@@ -73,19 +75,19 @@ void BaseAppender::BeginRow() {\n \n void BaseAppender::EndRow() {\n \t// check that all rows have been appended to\n-\tif (column != chunk->ColumnCount()) {\n+\tif (column != chunk.ColumnCount()) {\n \t\tthrow InvalidInputException(\"Call to EndRow before all rows have been appended to!\");\n \t}\n \tcolumn = 0;\n-\tchunk->SetCardinality(chunk->size() + 1);\n-\tif (chunk->size() >= STANDARD_VECTOR_SIZE) {\n+\tchunk.SetCardinality(chunk.size() + 1);\n+\tif (chunk.size() >= STANDARD_VECTOR_SIZE) {\n \t\tFlushChunk();\n \t}\n }\n \n template <class SRC, class DST>\n void BaseAppender::AppendValueInternal(Vector &col, SRC input) {\n-\tFlatVector::GetData<DST>(col)[chunk->size()] = Cast::Operation<SRC, DST>(input);\n+\tFlatVector::GetData<DST>(col)[chunk.size()] = Cast::Operation<SRC, DST>(input);\n }\n \n template <class T>\n@@ -93,7 +95,7 @@ void BaseAppender::AppendValueInternal(T input) {\n \tif (column >= types.size()) {\n \t\tthrow InvalidInputException(\"Too many appends for chunk!\");\n \t}\n-\tauto &col = chunk->data[column];\n+\tauto &col = chunk.data[column];\n \tswitch (col.GetType().id()) {\n \tcase LogicalTypeId::BOOLEAN:\n \t\tAppendValueInternal<T, bool>(col, input);\n@@ -162,7 +164,7 @@ void BaseAppender::AppendValueInternal(T input) {\n \t\tAppendValueInternal<T, interval_t>(col, input);\n \t\tbreak;\n \tcase LogicalTypeId::VARCHAR:\n-\t\tFlatVector::GetData<string_t>(col)[chunk->size()] = StringCast::Operation<T>(input, col);\n+\t\tFlatVector::GetData<string_t>(col)[chunk.size()] = StringCast::Operation<T>(input, col);\n \t\tbreak;\n \tdefault:\n \t\tAppendValue(Value::CreateValue<T>(input));\n@@ -267,7 +269,7 @@ void BaseAppender::Append(interval_t value) {\n \n template <>\n void BaseAppender::Append(Value value) { // NOLINT: template shtuff\n-\tif (column >= chunk->ColumnCount()) {\n+\tif (column >= chunk.ColumnCount()) {\n \t\tthrow InvalidInputException(\"Too many appends for chunk!\");\n \t}\n \tAppendValue(value);\n@@ -275,15 +277,15 @@ void BaseAppender::Append(Value value) { // NOLINT: template shtuff\n \n template <>\n void BaseAppender::Append(std::nullptr_t value) {\n-\tif (column >= chunk->ColumnCount()) {\n+\tif (column >= chunk.ColumnCount()) {\n \t\tthrow InvalidInputException(\"Too many appends for chunk!\");\n \t}\n-\tauto &col = chunk->data[column++];\n-\tFlatVector::SetNull(col, chunk->size(), true);\n+\tauto &col = chunk.data[column++];\n+\tFlatVector::SetNull(col, chunk.size(), true);\n }\n \n void BaseAppender::AppendValue(const Value &value) {\n-\tchunk->SetValue(column, chunk->size(), value);\n+\tchunk.SetValue(column, chunk.size(), value);\n \tcolumn++;\n }\n \n@@ -291,19 +293,19 @@ void BaseAppender::AppendDataChunk(DataChunk &chunk) {\n \tif (chunk.GetTypes() != types) {\n \t\tthrow InvalidInputException(\"Type mismatch in Append DataChunk and the types required for appender\");\n \t}\n-\tcollection.Append(chunk);\n-\tif (collection.ChunkCount() >= FLUSH_COUNT) {\n+\tcollection->Append(chunk);\n+\tif (collection->Count() >= FLUSH_COUNT) {\n \t\tFlush();\n \t}\n }\n \n void BaseAppender::FlushChunk() {\n-\tif (chunk->size() == 0) {\n+\tif (chunk.size() == 0) {\n \t\treturn;\n \t}\n-\tcollection.Append(move(chunk));\n-\tInitializeChunk();\n-\tif (collection.ChunkCount() >= FLUSH_COUNT) {\n+\tcollection->Append(chunk);\n+\tchunk.Reset();\n+\tif (collection->Count() >= FLUSH_COUNT) {\n \t\tFlush();\n \t}\n }\n@@ -315,22 +317,22 @@ void BaseAppender::Flush() {\n \t}\n \n \tFlushChunk();\n-\tif (collection.Count() == 0) {\n+\tif (collection->Count() == 0) {\n \t\treturn;\n \t}\n-\tFlushInternal(collection);\n+\tFlushInternal(*collection);\n \n-\tcollection.Reset();\n+\tcollection->Reset();\n \tcolumn = 0;\n }\n \n-void Appender::FlushInternal(ChunkCollection &collection) {\n+void Appender::FlushInternal(ColumnDataCollection &collection) {\n \tcontext->Append(*description, collection);\n }\n \n-void InternalAppender::FlushInternal(ChunkCollection &collection) {\n+void InternalAppender::FlushInternal(ColumnDataCollection &collection) {\n \tfor (auto &chunk : collection.Chunks()) {\n-\t\ttable.storage->Append(table, context, *chunk);\n+\t\ttable.storage->Append(table, context, chunk);\n \t}\n }\n \ndiff --git a/src/main/capi/arrow-c.cpp b/src/main/capi/arrow-c.cpp\nindex 748c0c455db3..04d252a8351d 100644\n--- a/src/main/capi/arrow-c.cpp\n+++ b/src/main/capi/arrow-c.cpp\n@@ -1,5 +1,7 @@\n #include \"duckdb/main/capi_internal.hpp\"\n+#include \"duckdb/common/arrow/arrow_converter.hpp\"\n \n+using duckdb::ArrowConverter;\n using duckdb::ArrowResultWrapper;\n using duckdb::Connection;\n using duckdb::DataChunk;\n@@ -22,8 +24,8 @@ duckdb_state duckdb_query_arrow_schema(duckdb_arrow result, duckdb_arrow_schema\n \t\treturn DuckDBSuccess;\n \t}\n \tauto wrapper = (ArrowResultWrapper *)result;\n-\tQueryResult::ToArrowSchema((ArrowSchema *)*out_schema, wrapper->result->types, wrapper->result->names,\n-\t                           wrapper->timezone_config);\n+\tArrowConverter::ToArrowSchema((ArrowSchema *)*out_schema, wrapper->result->types, wrapper->result->names,\n+\t                              wrapper->timezone_config);\n \treturn DuckDBSuccess;\n }\n \n@@ -39,29 +41,36 @@ duckdb_state duckdb_query_arrow_array(duckdb_arrow result, duckdb_arrow_array *o\n \tif (!wrapper->current_chunk || wrapper->current_chunk->size() == 0) {\n \t\treturn DuckDBSuccess;\n \t}\n-\twrapper->current_chunk->ToArrowArray((ArrowArray *)*out_array);\n+\tArrowConverter::ToArrowArray(*wrapper->current_chunk, (ArrowArray *)*out_array);\n \treturn DuckDBSuccess;\n }\n \n idx_t duckdb_arrow_row_count(duckdb_arrow result) {\n \tauto wrapper = (ArrowResultWrapper *)result;\n-\treturn wrapper->result->collection.Count();\n+\tif (!wrapper->result->success) {\n+\t\treturn 0;\n+\t}\n+\treturn wrapper->result->RowCount();\n }\n \n idx_t duckdb_arrow_column_count(duckdb_arrow result) {\n \tauto wrapper = (ArrowResultWrapper *)result;\n-\treturn wrapper->result->types.size();\n+\treturn wrapper->result->ColumnCount();\n }\n \n idx_t duckdb_arrow_rows_changed(duckdb_arrow result) {\n \tauto wrapper = (ArrowResultWrapper *)result;\n+\tif (!wrapper->result->success) {\n+\t\treturn 0;\n+\t}\n \tidx_t rows_changed = 0;\n-\tidx_t row_count = wrapper->result->collection.Count();\n+\tauto &collection = wrapper->result->Collection();\n+\tidx_t row_count = collection.Count();\n \tif (row_count > 0 && wrapper->result->properties.return_type == duckdb::StatementReturnType::CHANGED_ROWS) {\n-\t\tauto row_changes = wrapper->result->GetValue(0, 0);\n-\t\tif (!row_changes.IsNull() && row_changes.TryCastAs(LogicalType::BIGINT)) {\n-\t\t\trows_changed = row_changes.GetValue<int64_t>();\n-\t\t}\n+\t\tauto rows = collection.GetRows();\n+\t\tD_ASSERT(row_count == 1);\n+\t\tD_ASSERT(rows.size() == 1);\n+\t\trows_changed = rows[0].GetValue(0).GetValue<int64_t>();\n \t}\n \treturn rows_changed;\n }\ndiff --git a/src/main/capi/result-c.cpp b/src/main/capi/result-c.cpp\nindex 063ddba18757..902eec460839 100644\n--- a/src/main/capi/result-c.cpp\n+++ b/src/main/capi/result-c.cpp\n@@ -1,237 +1,239 @@\n #include \"duckdb/main/capi_internal.hpp\"\n #include \"duckdb/common/types/timestamp.hpp\"\n+#include \"duckdb/common/allocator.hpp\"\n \n namespace duckdb {\n \n+struct CBaseConverter {\n+\ttemplate <class DST>\n+\tstatic void NullConvert(DST &target) {\n+\t}\n+};\n+struct CStandardConverter : public CBaseConverter {\n+\ttemplate <class SRC, class DST>\n+\tstatic DST Convert(SRC input) {\n+\t\treturn input;\n+\t}\n+};\n+\n+struct CStringConverter {\n+\ttemplate <class SRC, class DST>\n+\tstatic DST Convert(SRC input) {\n+\t\tauto result = (char *)duckdb_malloc(input.GetSize() + 1);\n+\t\tassert(result);\n+\t\tmemcpy((void *)result, input.GetDataUnsafe(), input.GetSize());\n+\t\tauto write_arr = (char *)result;\n+\t\twrite_arr[input.GetSize()] = '\\0';\n+\t\treturn result;\n+\t}\n+\n+\ttemplate <class DST>\n+\tstatic void NullConvert(DST &target) {\n+\t\ttarget = nullptr;\n+\t}\n+};\n+\n+struct CBlobConverter {\n+\ttemplate <class SRC, class DST>\n+\tstatic DST Convert(SRC input) {\n+\t\tduckdb_blob result;\n+\t\tresult.data = (char *)duckdb_malloc(input.GetSize());\n+\t\tresult.size = input.GetSize();\n+\t\tassert(result.data);\n+\t\tmemcpy((void *)result.data, input.GetDataUnsafe(), input.GetSize());\n+\t\treturn result;\n+\t}\n+\n+\ttemplate <class DST>\n+\tstatic void NullConvert(DST &target) {\n+\t\ttarget.data = nullptr;\n+\t\ttarget.size = 0;\n+\t}\n+};\n+\n+struct CTimestampMsConverter : public CBaseConverter {\n+\ttemplate <class SRC, class DST>\n+\tstatic DST Convert(SRC input) {\n+\t\treturn Timestamp::FromEpochMs(input.value);\n+\t}\n+};\n+\n+struct CTimestampNsConverter : public CBaseConverter {\n+\ttemplate <class SRC, class DST>\n+\tstatic DST Convert(SRC input) {\n+\t\treturn Timestamp::FromEpochNanoSeconds(input.value);\n+\t}\n+};\n+\n+struct CTimestampSecConverter : public CBaseConverter {\n+\ttemplate <class SRC, class DST>\n+\tstatic DST Convert(SRC input) {\n+\t\treturn Timestamp::FromEpochSeconds(input.value);\n+\t}\n+};\n+\n+struct CHugeintConverter : public CBaseConverter {\n+\ttemplate <class SRC, class DST>\n+\tstatic DST Convert(SRC input) {\n+\t\tduckdb_hugeint result;\n+\t\tresult.lower = input.lower;\n+\t\tresult.upper = input.upper;\n+\t\treturn result;\n+\t}\n+};\n+\n+struct CIntervalConverter : public CBaseConverter {\n+\ttemplate <class SRC, class DST>\n+\tstatic DST Convert(SRC input) {\n+\t\tduckdb_interval result;\n+\t\tresult.days = input.days;\n+\t\tresult.months = input.months;\n+\t\tresult.micros = input.micros;\n+\t\treturn result;\n+\t}\n+};\n+\n template <class T>\n-void WriteData(duckdb_column *column, ChunkCollection &source, idx_t col) {\n+struct CDecimalConverter : public CBaseConverter {\n+\ttemplate <class SRC, class DST>\n+\tstatic DST Convert(SRC input) {\n+\t\tduckdb_hugeint result;\n+\t\tresult.lower = input;\n+\t\tresult.upper = 0;\n+\t\treturn result;\n+\t}\n+};\n+\n+template <class SRC, class DST = SRC, class OP = CStandardConverter>\n+void WriteData(duckdb_column *column, ColumnDataCollection &source, const vector<column_t> &column_ids) {\n \tidx_t row = 0;\n-\tauto target = (T *)column->__deprecated_data;\n-\tfor (auto &chunk : source.Chunks()) {\n-\t\tauto source = FlatVector::GetData<T>(chunk->data[col]);\n-\t\tauto &mask = FlatVector::Validity(chunk->data[col]);\n+\tauto target = (DST *)column->__deprecated_data;\n+\tfor (auto &input : source.Chunks(column_ids)) {\n+\t\tauto source = FlatVector::GetData<SRC>(input.data[0]);\n+\t\tauto &mask = FlatVector::Validity(input.data[0]);\n \n-\t\tfor (idx_t k = 0; k < chunk->size(); k++, row++) {\n+\t\tfor (idx_t k = 0; k < input.size(); k++, row++) {\n \t\t\tif (!mask.RowIsValid(k)) {\n-\t\t\t\tcontinue;\n+\t\t\t\tOP::template NullConvert<DST>(target[row]);\n+\t\t\t} else {\n+\t\t\t\ttarget[row] = OP::template Convert<SRC, DST>(source[k]);\n \t\t\t}\n-\t\t\ttarget[row] = source[k];\n \t\t}\n \t}\n }\n \n duckdb_state deprecated_duckdb_translate_column(MaterializedQueryResult &result, duckdb_column *column, idx_t col) {\n-\tidx_t row_count = result.collection.Count();\n-\tcolumn->__deprecated_nullmask = (bool *)duckdb_malloc(sizeof(bool) * result.collection.Count());\n+\tD_ASSERT(result.success);\n+\tauto &collection = result.Collection();\n+\tidx_t row_count = collection.Count();\n+\tcolumn->__deprecated_nullmask = (bool *)duckdb_malloc(sizeof(bool) * collection.Count());\n \tcolumn->__deprecated_data = duckdb_malloc(GetCTypeSize(column->__deprecated_type) * row_count);\n \tif (!column->__deprecated_nullmask || !column->__deprecated_data) { // LCOV_EXCL_START\n \t\t// malloc failure\n \t\treturn DuckDBError;\n \t} // LCOV_EXCL_STOP\n \n+\tvector<column_t> column_ids {col};\n \t// first convert the nullmask\n-\tidx_t row = 0;\n-\tfor (auto &chunk : result.collection.Chunks()) {\n-\t\tfor (idx_t k = 0; k < chunk->size(); k++) {\n-\t\t\tcolumn->__deprecated_nullmask[row++] = FlatVector::IsNull(chunk->data[col], k);\n+\t{\n+\t\tidx_t row = 0;\n+\t\tfor (auto &input : collection.Chunks(column_ids)) {\n+\t\t\tfor (idx_t k = 0; k < input.size(); k++) {\n+\t\t\t\tcolumn->__deprecated_nullmask[row++] = FlatVector::IsNull(input.data[0], k);\n+\t\t\t}\n \t\t}\n \t}\n \t// then write the data\n \tswitch (result.types[col].id()) {\n \tcase LogicalTypeId::BOOLEAN:\n-\t\tWriteData<bool>(column, result.collection, col);\n+\t\tWriteData<bool>(column, collection, column_ids);\n \t\tbreak;\n \tcase LogicalTypeId::TINYINT:\n-\t\tWriteData<int8_t>(column, result.collection, col);\n+\t\tWriteData<int8_t>(column, collection, column_ids);\n \t\tbreak;\n \tcase LogicalTypeId::SMALLINT:\n-\t\tWriteData<int16_t>(column, result.collection, col);\n+\t\tWriteData<int16_t>(column, collection, column_ids);\n \t\tbreak;\n \tcase LogicalTypeId::INTEGER:\n-\t\tWriteData<int32_t>(column, result.collection, col);\n+\t\tWriteData<int32_t>(column, collection, column_ids);\n \t\tbreak;\n \tcase LogicalTypeId::BIGINT:\n-\t\tWriteData<int64_t>(column, result.collection, col);\n+\t\tWriteData<int64_t>(column, collection, column_ids);\n \t\tbreak;\n \tcase LogicalTypeId::UTINYINT:\n-\t\tWriteData<uint8_t>(column, result.collection, col);\n+\t\tWriteData<uint8_t>(column, collection, column_ids);\n \t\tbreak;\n \tcase LogicalTypeId::USMALLINT:\n-\t\tWriteData<uint16_t>(column, result.collection, col);\n+\t\tWriteData<uint16_t>(column, collection, column_ids);\n \t\tbreak;\n \tcase LogicalTypeId::UINTEGER:\n-\t\tWriteData<uint32_t>(column, result.collection, col);\n+\t\tWriteData<uint32_t>(column, collection, column_ids);\n \t\tbreak;\n \tcase LogicalTypeId::UBIGINT:\n-\t\tWriteData<uint64_t>(column, result.collection, col);\n+\t\tWriteData<uint64_t>(column, collection, column_ids);\n \t\tbreak;\n \tcase LogicalTypeId::FLOAT:\n-\t\tWriteData<float>(column, result.collection, col);\n+\t\tWriteData<float>(column, collection, column_ids);\n \t\tbreak;\n \tcase LogicalTypeId::DOUBLE:\n-\t\tWriteData<double>(column, result.collection, col);\n+\t\tWriteData<double>(column, collection, column_ids);\n \t\tbreak;\n \tcase LogicalTypeId::DATE:\n-\t\tWriteData<date_t>(column, result.collection, col);\n+\t\tWriteData<date_t>(column, collection, column_ids);\n \t\tbreak;\n \tcase LogicalTypeId::TIME:\n \tcase LogicalTypeId::TIME_TZ:\n-\t\tWriteData<dtime_t>(column, result.collection, col);\n+\t\tWriteData<dtime_t>(column, collection, column_ids);\n \t\tbreak;\n \tcase LogicalTypeId::TIMESTAMP:\n \tcase LogicalTypeId::TIMESTAMP_TZ:\n-\t\tWriteData<timestamp_t>(column, result.collection, col);\n+\t\tWriteData<timestamp_t>(column, collection, column_ids);\n \t\tbreak;\n \tcase LogicalTypeId::VARCHAR: {\n-\t\tidx_t row = 0;\n-\t\tauto target = (const char **)column->__deprecated_data;\n-\t\tfor (auto &chunk : result.collection.Chunks()) {\n-\t\t\tauto source = FlatVector::GetData<string_t>(chunk->data[col]);\n-\t\t\tfor (idx_t k = 0; k < chunk->size(); k++) {\n-\t\t\t\tif (!FlatVector::IsNull(chunk->data[col], k)) {\n-\t\t\t\t\ttarget[row] = (char *)duckdb_malloc(source[k].GetSize() + 1);\n-\t\t\t\t\tassert(target[row]);\n-\t\t\t\t\tmemcpy((void *)target[row], source[k].GetDataUnsafe(), source[k].GetSize());\n-\t\t\t\t\tauto write_arr = (char *)target[row];\n-\t\t\t\t\twrite_arr[source[k].GetSize()] = '\\0';\n-\t\t\t\t} else {\n-\t\t\t\t\ttarget[row] = nullptr;\n-\t\t\t\t}\n-\t\t\t\trow++;\n-\t\t\t}\n-\t\t}\n+\t\tWriteData<string_t, const char *, CStringConverter>(column, collection, column_ids);\n \t\tbreak;\n \t}\n \tcase LogicalTypeId::BLOB: {\n-\t\tidx_t row = 0;\n-\t\tauto target = (duckdb_blob *)column->__deprecated_data;\n-\t\tfor (auto &chunk : result.collection.Chunks()) {\n-\t\t\tauto source = FlatVector::GetData<string_t>(chunk->data[col]);\n-\t\t\tfor (idx_t k = 0; k < chunk->size(); k++) {\n-\t\t\t\tif (!FlatVector::IsNull(chunk->data[col], k)) {\n-\t\t\t\t\ttarget[row].data = (char *)duckdb_malloc(source[k].GetSize());\n-\t\t\t\t\ttarget[row].size = source[k].GetSize();\n-\t\t\t\t\tassert(target[row].data);\n-\t\t\t\t\tmemcpy((void *)target[row].data, source[k].GetDataUnsafe(), source[k].GetSize());\n-\t\t\t\t} else {\n-\t\t\t\t\ttarget[row].data = nullptr;\n-\t\t\t\t\ttarget[row].size = 0;\n-\t\t\t\t}\n-\t\t\t\trow++;\n-\t\t\t}\n-\t\t}\n+\t\tWriteData<string_t, duckdb_blob, CBlobConverter>(column, collection, column_ids);\n+\t\tbreak;\n+\t}\n+\tcase LogicalTypeId::TIMESTAMP_NS: {\n+\t\tWriteData<timestamp_t, timestamp_t, CTimestampNsConverter>(column, collection, column_ids);\n+\t\tbreak;\n+\t}\n+\tcase LogicalTypeId::TIMESTAMP_MS: {\n+\t\tWriteData<timestamp_t, timestamp_t, CTimestampMsConverter>(column, collection, column_ids);\n \t\tbreak;\n \t}\n-\tcase LogicalTypeId::TIMESTAMP_NS:\n-\tcase LogicalTypeId::TIMESTAMP_MS:\n \tcase LogicalTypeId::TIMESTAMP_SEC: {\n-\t\tidx_t row = 0;\n-\t\tauto target = (timestamp_t *)column->__deprecated_data;\n-\t\tfor (auto &chunk : result.collection.Chunks()) {\n-\t\t\tauto source = FlatVector::GetData<timestamp_t>(chunk->data[col]);\n-\n-\t\t\tfor (idx_t k = 0; k < chunk->size(); k++) {\n-\t\t\t\tif (!FlatVector::IsNull(chunk->data[col], k)) {\n-\t\t\t\t\tif (result.types[col].id() == LogicalTypeId::TIMESTAMP_NS) {\n-\t\t\t\t\t\ttarget[row] = Timestamp::FromEpochNanoSeconds(source[k].value);\n-\t\t\t\t\t} else if (result.types[col].id() == LogicalTypeId::TIMESTAMP_MS) {\n-\t\t\t\t\t\ttarget[row] = Timestamp::FromEpochMs(source[k].value);\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\tD_ASSERT(result.types[col].id() == LogicalTypeId::TIMESTAMP_SEC);\n-\t\t\t\t\t\ttarget[row] = Timestamp::FromEpochSeconds(source[k].value);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\trow++;\n-\t\t\t}\n-\t\t}\n+\t\tWriteData<timestamp_t, timestamp_t, CTimestampSecConverter>(column, collection, column_ids);\n \t\tbreak;\n \t}\n \tcase LogicalTypeId::HUGEINT: {\n-\t\tidx_t row = 0;\n-\t\tauto target = (duckdb_hugeint *)column->__deprecated_data;\n-\t\tfor (auto &chunk : result.collection.Chunks()) {\n-\t\t\tauto source = FlatVector::GetData<hugeint_t>(chunk->data[col]);\n-\t\t\tfor (idx_t k = 0; k < chunk->size(); k++) {\n-\t\t\t\tif (!FlatVector::IsNull(chunk->data[col], k)) {\n-\t\t\t\t\ttarget[row].lower = source[k].lower;\n-\t\t\t\t\ttarget[row].upper = source[k].upper;\n-\t\t\t\t}\n-\t\t\t\trow++;\n-\t\t\t}\n-\t\t}\n+\t\tWriteData<hugeint_t, duckdb_hugeint, CHugeintConverter>(column, collection, column_ids);\n \t\tbreak;\n \t}\n \tcase LogicalTypeId::INTERVAL: {\n-\t\tidx_t row = 0;\n-\t\tauto target = (duckdb_interval *)column->__deprecated_data;\n-\t\tfor (auto &chunk : result.collection.Chunks()) {\n-\t\t\tauto source = FlatVector::GetData<interval_t>(chunk->data[col]);\n-\t\t\tfor (idx_t k = 0; k < chunk->size(); k++) {\n-\t\t\t\tif (!FlatVector::IsNull(chunk->data[col], k)) {\n-\t\t\t\t\ttarget[row].days = source[k].days;\n-\t\t\t\t\ttarget[row].months = source[k].months;\n-\t\t\t\t\ttarget[row].micros = source[k].micros;\n-\t\t\t\t}\n-\t\t\t\trow++;\n-\t\t\t}\n-\t\t}\n+\t\tWriteData<interval_t, duckdb_interval, CIntervalConverter>(column, collection, column_ids);\n \t\tbreak;\n \t}\n \tcase LogicalTypeId::DECIMAL: {\n \t\t// get data\n-\t\tidx_t row = 0;\n-\t\tauto target = (hugeint_t *)column->__deprecated_data;\n \t\tswitch (result.types[col].InternalType()) {\n \t\tcase PhysicalType::INT16: {\n-\t\t\tfor (auto &chunk : result.collection.Chunks()) {\n-\t\t\t\tauto source = FlatVector::GetData<int16_t>(chunk->data[col]);\n-\t\t\t\tfor (idx_t k = 0; k < chunk->size(); k++) {\n-\t\t\t\t\tif (!FlatVector::IsNull(chunk->data[col], k)) {\n-\t\t\t\t\t\ttarget[row].lower = source[k];\n-\t\t\t\t\t\ttarget[row].upper = 0;\n-\t\t\t\t\t}\n-\t\t\t\t\trow++;\n-\t\t\t\t}\n-\t\t\t}\n+\t\t\tWriteData<int16_t, duckdb_hugeint, CDecimalConverter<int16_t>>(column, collection, column_ids);\n \t\t\tbreak;\n \t\t}\n \t\tcase PhysicalType::INT32: {\n-\t\t\tfor (auto &chunk : result.collection.Chunks()) {\n-\t\t\t\tauto source = FlatVector::GetData<int32_t>(chunk->data[col]);\n-\t\t\t\tfor (idx_t k = 0; k < chunk->size(); k++) {\n-\t\t\t\t\tif (!FlatVector::IsNull(chunk->data[col], k)) {\n-\t\t\t\t\t\ttarget[row].lower = source[k];\n-\t\t\t\t\t\ttarget[row].upper = 0;\n-\t\t\t\t\t}\n-\t\t\t\t\trow++;\n-\t\t\t\t}\n-\t\t\t}\n+\t\t\tWriteData<int32_t, duckdb_hugeint, CDecimalConverter<int32_t>>(column, collection, column_ids);\n \t\t\tbreak;\n \t\t}\n \t\tcase PhysicalType::INT64: {\n-\t\t\tfor (auto &chunk : result.collection.Chunks()) {\n-\t\t\t\tauto source = FlatVector::GetData<int64_t>(chunk->data[col]);\n-\t\t\t\tfor (idx_t k = 0; k < chunk->size(); k++) {\n-\t\t\t\t\tif (!FlatVector::IsNull(chunk->data[col], k)) {\n-\t\t\t\t\t\ttarget[row].lower = source[k];\n-\t\t\t\t\t\ttarget[row].upper = 0;\n-\t\t\t\t\t}\n-\t\t\t\t\trow++;\n-\t\t\t\t}\n-\t\t\t}\n+\t\t\tWriteData<int64_t, duckdb_hugeint, CDecimalConverter<int64_t>>(column, collection, column_ids);\n \t\t\tbreak;\n \t\t}\n \t\tcase PhysicalType::INT128: {\n-\t\t\tfor (auto &chunk : result.collection.Chunks()) {\n-\t\t\t\tauto source = FlatVector::GetData<hugeint_t>(chunk->data[col]);\n-\t\t\t\tfor (idx_t k = 0; k < chunk->size(); k++) {\n-\t\t\t\t\tif (!FlatVector::IsNull(chunk->data[col], k)) {\n-\t\t\t\t\t\ttarget[row].lower = source[k].lower;\n-\t\t\t\t\t\ttarget[row].upper = source[k].upper;\n-\t\t\t\t\t}\n-\t\t\t\t\trow++;\n-\t\t\t\t}\n-\t\t\t}\n+\t\t\tWriteData<hugeint_t, duckdb_hugeint, CHugeintConverter>(column, collection, column_ids);\n \t\t\tbreak;\n \t\t}\n \t\tdefault:\n@@ -313,7 +315,7 @@ bool deprecated_materialize_result(duckdb_result *result) {\n \t\tresult->__deprecated_columns[i].__deprecated_type = ConvertCPPTypeToC(result_data->result->types[i]);\n \t\tresult->__deprecated_columns[i].__deprecated_name = (char *)result_data->result->names[i].c_str();\n \t}\n-\tresult->__deprecated_row_count = materialized.collection.Count();\n+\tresult->__deprecated_row_count = materialized.RowCount();\n \tif (result->__deprecated_row_count > 0 &&\n \t    materialized.properties.return_type == StatementReturnType::CHANGED_ROWS) {\n \t\t// update total changes\n@@ -412,7 +414,7 @@ idx_t duckdb_row_count(duckdb_result *result) {\n \t}\n \tauto &result_data = *((duckdb::DuckDBResultData *)result->internal_data);\n \tauto &materialized = (duckdb::MaterializedQueryResult &)*result_data.result;\n-\treturn materialized.collection.Count();\n+\treturn materialized.RowCount();\n }\n \n idx_t duckdb_rows_changed(duckdb_result *result) {\n@@ -463,7 +465,7 @@ idx_t duckdb_result_chunk_count(duckdb_result result) {\n \t}\n \tD_ASSERT(result_data.result->type == duckdb::QueryResultType::MATERIALIZED_RESULT);\n \tauto &materialized = (duckdb::MaterializedQueryResult &)*result_data.result;\n-\treturn materialized.collection.ChunkCount();\n+\treturn materialized.Collection().ChunkCount();\n }\n \n duckdb_data_chunk duckdb_result_get_chunk(duckdb_result result, idx_t chunk_idx) {\n@@ -476,11 +478,12 @@ duckdb_data_chunk duckdb_result_get_chunk(duckdb_result result, idx_t chunk_idx)\n \t}\n \tresult_data.result_set_type = duckdb::CAPIResultSetType::CAPI_RESULT_TYPE_MATERIALIZED;\n \tauto &materialized = (duckdb::MaterializedQueryResult &)*result_data.result;\n-\tif (chunk_idx >= materialized.collection.ChunkCount()) {\n+\tauto &collection = materialized.Collection();\n+\tif (chunk_idx >= collection.ChunkCount()) {\n \t\treturn nullptr;\n \t}\n \tauto chunk = duckdb::make_unique<duckdb::DataChunk>();\n-\tchunk->InitializeEmpty(materialized.collection.Types());\n-\tchunk->Reference(*materialized.collection.Chunks()[chunk_idx]);\n+\tchunk->Initialize(duckdb::Allocator::DefaultAllocator(), collection.Types());\n+\tcollection.FetchChunk(chunk_idx, *chunk);\n \treturn chunk.release();\n }\ndiff --git a/src/main/client_context.cpp b/src/main/client_context.cpp\nindex c1f703f32337..e8652d591ff4 100644\n--- a/src/main/client_context.cpp\n+++ b/src/main/client_context.cpp\n@@ -39,6 +39,7 @@\n #include \"duckdb/parser/parsed_expression_iterator.hpp\"\n #include \"duckdb/parser/statement/prepare_statement.hpp\"\n #include \"duckdb/parser/statement/execute_statement.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n \n namespace duckdb {\n \n@@ -244,8 +245,15 @@ unique_ptr<QueryResult> ClientContext::FetchResultInternal(ClientContextLock &lo\n \t\tCleanupInternal(lock, result.get(), false);\n \t} else {\n \t\t// no result collector - create a materialized result by continuously fetching\n+\t\tauto result_collection = make_unique<ColumnDataCollection>(Allocator::DefaultAllocator(), pending.types);\n+\t\tD_ASSERT(!result_collection->Types().empty());\n \t\tauto materialized_result = make_unique<MaterializedQueryResult>(\n-\t\t    pending.statement_type, pending.properties, pending.types, pending.names, shared_from_this());\n+\t\t    pending.statement_type, pending.properties, pending.names, move(result_collection), GetClientProperties());\n+\n+\t\tauto &collection = materialized_result->Collection();\n+\t\tD_ASSERT(!collection.Types().empty());\n+\t\tColumnDataAppendState append_state;\n+\t\tcollection.InitializeAppend(append_state);\n \t\twhile (true) {\n \t\t\tauto chunk = FetchInternal(lock, GetExecutor(), *materialized_result);\n \t\t\tif (!chunk || chunk->size() == 0) {\n@@ -258,7 +266,7 @@ unique_ptr<QueryResult> ClientContext::FetchResultInternal(ClientContextLock &lo\n \t\t\t\t}\n \t\t\t}\n #endif\n-\t\t\tmaterialized_result->collection.Append(*chunk);\n+\t\t\tcollection.Append(append_state, *chunk);\n \t\t}\n \t\tresult = move(materialized_result);\n \t}\n@@ -716,8 +724,9 @@ unique_ptr<QueryResult> ClientContext::Query(const string &query, bool allow_str\n \t\tStatementProperties properties;\n \t\tvector<LogicalType> types;\n \t\tvector<string> names;\n-\t\treturn make_unique<MaterializedQueryResult>(StatementType::INVALID_STATEMENT, properties, move(types),\n-\t\t                                            move(names), shared_from_this());\n+\t\tauto collection = make_unique<ColumnDataCollection>(Allocator::DefaultAllocator(), move(types));\n+\t\treturn make_unique<MaterializedQueryResult>(StatementType::INVALID_STATEMENT, properties, move(names),\n+\t\t                                            move(collection), GetClientProperties());\n \t}\n \n \tunique_ptr<QueryResult> result;\n@@ -1041,16 +1050,23 @@ string ClientContext::VerifyQuery(ClientContextLock &lock, const string &query,\n \t}\n \tconfig.enable_optimizer = optimizer_enabled;\n \n-\t// check explain, only if q does not already contain EXPLAIN\n+\t// check explain, only if the query was successful\n \tif (results[0]->success) {\n \t\tauto explain_q = \"EXPLAIN \" + query;\n \t\tauto explain_stmt = make_unique<ExplainStatement>(move(statement_copy_for_explain));\n+\t\tstring error;\n \t\ttry {\n-\t\t\tRunStatementInternal(lock, explain_q, move(explain_stmt), false, false);\n+\t\t\tauto result = RunStatementInternal(lock, explain_q, move(explain_stmt), false, false);\n+\t\t\tif (!result->success) {\n+\t\t\t\terror = result->error;\n+\t\t\t}\n \t\t} catch (std::exception &ex) { // LCOV_EXCL_START\n-\t\t\tinterrupted = false;\n-\t\t\treturn \"EXPLAIN failed but query did not (\" + string(ex.what()) + \")\";\n+\t\t\terror = ex.what();\n \t\t} // LCOV_EXCL_STOP\n+\t\tif (!error.empty()) {\n+\t\t\tinterrupted = false;\n+\t\t\treturn \"Explain result differs from original result!\\nEXPLAIN failed but query did not (\" + error + \")\";\n+\t\t}\n \t}\n \n \tif (profiling_is_enabled) {\n@@ -1067,11 +1083,14 @@ string ClientContext::VerifyQuery(ClientContextLock &lock, const string &query,\n \t\t\tresult += \"Original Result:\\n\" + results[0]->ToString();\n \t\t\tresult += name + \":\\n\" + results[i]->ToString();\n \t\t\treturn result;\n-\t\t}                                                             // LCOV_EXCL_STOP\n-\t\tif (!results[0]->collection.Equals(results[i]->collection)) { // LCOV_EXCL_START\n+\t\t} // LCOV_EXCL_STOP\n+\t\tstring error;\n+\t\tif (!ColumnDataCollection::ResultEquals(results[0]->Collection(), results[i]->Collection(),\n+\t\t                                        error)) { // LCOV_EXCL_START\n \t\t\tstring result = name + \" differs from original result!\\n\";\n \t\t\tresult += \"Original Result:\\n\" + results[0]->ToString();\n \t\t\tresult += name + \":\\n\" + results[i]->ToString();\n+\t\t\tresult += \"\\n\\n---------------------------------\\n\" + error;\n \t\t\treturn result;\n \t\t} // LCOV_EXCL_STOP\n \t}\n@@ -1179,7 +1198,7 @@ unique_ptr<TableDescription> ClientContext::TableInfo(const string &schema_name,\n \treturn result;\n }\n \n-void ClientContext::Append(TableDescription &description, ChunkCollection &collection) {\n+void ClientContext::Append(TableDescription &description, ColumnDataCollection &collection) {\n \tRunFunctionInTransaction([&]() {\n \t\tauto &catalog = Catalog::GetCatalog(*this);\n \t\tauto table_entry = catalog.GetEntry<TableCatalogEntry>(*this, description.schema, description.table);\n@@ -1193,7 +1212,7 @@ void ClientContext::Append(TableDescription &description, ChunkCollection &colle\n \t\t\t}\n \t\t}\n \t\tfor (auto &chunk : collection.Chunks()) {\n-\t\t\ttable_entry->storage->Append(*table_entry, *this, *chunk);\n+\t\t\ttable_entry->storage->Append(*table_entry, *this, chunk);\n \t\t}\n \t});\n }\n@@ -1313,7 +1332,7 @@ bool ClientContext::TryGetCurrentSetting(const std::string &key, Value &result)\n \treturn true;\n }\n \n-ParserOptions ClientContext::GetParserOptions() {\n+ParserOptions ClientContext::GetParserOptions() const {\n \tParserOptions options;\n \toptions.preserve_identifier_case = ClientConfig::GetConfig(*this).preserve_identifier_case;\n \toptions.max_expression_depth = ClientConfig::GetConfig(*this).max_expression_depth;\n@@ -1321,4 +1340,10 @@ ParserOptions ClientContext::GetParserOptions() {\n \treturn options;\n }\n \n+ClientProperties ClientContext::GetClientProperties() const {\n+\tClientProperties properties;\n+\tproperties.timezone = ClientConfig::GetConfig(*this).ExtractTimezone();\n+\treturn properties;\n+}\n+\n } // namespace duckdb\ndiff --git a/src/main/connection.cpp b/src/main/connection.cpp\nindex 1a5c7c3527e3..59eec81c6acf 100644\n--- a/src/main/connection.cpp\n+++ b/src/main/connection.cpp\n@@ -13,6 +13,7 @@\n #include \"duckdb/parser/parser.hpp\"\n #include \"duckdb/main/connection_manager.hpp\"\n #include \"duckdb/planner/logical_operator.hpp\"\n+#include \"duckdb/common/types/column_data_collection.hpp\"\n \n namespace duckdb {\n \n@@ -121,12 +122,15 @@ unique_ptr<LogicalOperator> Connection::ExtractPlan(const string &query) {\n }\n \n void Connection::Append(TableDescription &description, DataChunk &chunk) {\n-\tChunkCollection collection(*context);\n+\tif (chunk.size() == 0) {\n+\t\treturn;\n+\t}\n+\tColumnDataCollection collection(Allocator::Get(*context), chunk.GetTypes());\n \tcollection.Append(chunk);\n \tAppend(description, collection);\n }\n \n-void Connection::Append(TableDescription &description, ChunkCollection &collection) {\n+void Connection::Append(TableDescription &description, ColumnDataCollection &collection) {\n \tcontext->Append(description, collection);\n }\n \ndiff --git a/src/main/database.cpp b/src/main/database.cpp\nindex b05e4ca2bc1b..c2174992e167 100644\n--- a/src/main/database.cpp\n+++ b/src/main/database.cpp\n@@ -83,6 +83,14 @@ ClientConfig &ClientConfig::GetConfig(ClientContext &context) {\n \treturn context.config;\n }\n \n+const DBConfig &DBConfig::GetConfig(const DatabaseInstance &db) {\n+\treturn db.config;\n+}\n+\n+const ClientConfig &ClientConfig::GetConfig(const ClientContext &context) {\n+\treturn context.config;\n+}\n+\n TransactionManager &TransactionManager::Get(ClientContext &context) {\n \treturn TransactionManager::Get(DatabaseInstance::GetDatabase(context));\n }\n@@ -244,6 +252,10 @@ DBConfig &DBConfig::GetConfig(ClientContext &context) {\n \treturn context.db->config;\n }\n \n+const DBConfig &DBConfig::GetConfig(const ClientContext &context) {\n+\treturn context.db->config;\n+}\n+\n idx_t DatabaseInstance::NumberOfThreads() {\n \treturn scheduler->NumberOfThreads();\n }\n@@ -263,11 +275,12 @@ void DuckDB::SetExtensionLoaded(const std::string &name) {\n \tinstance->loaded_extensions.insert(name);\n }\n \n-string ClientConfig::ExtractTimezoneFromConfig(ClientConfig &config) {\n-\tif (config.set_variables.find(\"TimeZone\") == config.set_variables.end()) {\n+string ClientConfig::ExtractTimezone() const {\n+\tauto entry = set_variables.find(\"TimeZone\");\n+\tif (entry == set_variables.end()) {\n \t\treturn \"UTC\";\n \t} else {\n-\t\treturn config.set_variables[\"TimeZone\"].GetValue<std::string>();\n+\t\treturn entry->second.GetValue<std::string>();\n \t}\n }\n \ndiff --git a/src/main/materialized_query_result.cpp b/src/main/materialized_query_result.cpp\nindex 57cf8fbba135..c2c9b50b6f91 100644\n--- a/src/main/materialized_query_result.cpp\n+++ b/src/main/materialized_query_result.cpp\n@@ -1,35 +1,34 @@\n #include \"duckdb/main/materialized_query_result.hpp\"\n #include \"duckdb/common/to_string.hpp\"\n+#include \"duckdb/main/client_context.hpp\"\n \n namespace duckdb {\n \n MaterializedQueryResult::MaterializedQueryResult(StatementType statement_type, StatementProperties properties,\n-                                                 vector<LogicalType> types, vector<string> names,\n-                                                 const shared_ptr<ClientContext> &context_p)\n-    : QueryResult(QueryResultType::MATERIALIZED_RESULT, statement_type, properties, move(types), move(names)),\n-      collection(Allocator::DefaultAllocator()), context(context_p) {\n+                                                 vector<string> names_p, unique_ptr<ColumnDataCollection> collection_p,\n+                                                 ClientProperties client_properties)\n+    : QueryResult(QueryResultType::MATERIALIZED_RESULT, statement_type, properties, collection_p->Types(),\n+                  move(names_p), move(client_properties)),\n+      collection(move(collection_p)), scan_initialized(false) {\n }\n \n MaterializedQueryResult::MaterializedQueryResult(string error)\n-    : QueryResult(QueryResultType::MATERIALIZED_RESULT, move(error)), collection(Allocator::DefaultAllocator()) {\n-}\n-\n-Value MaterializedQueryResult::GetValue(idx_t column, idx_t index) {\n-\tauto &data = collection.GetChunkForRow(index).data[column];\n-\tauto offset_in_chunk = index % STANDARD_VECTOR_SIZE;\n-\treturn data.GetValue(offset_in_chunk);\n+    : QueryResult(QueryResultType::MATERIALIZED_RESULT, move(error)), scan_initialized(false) {\n }\n \n string MaterializedQueryResult::ToString() {\n \tstring result;\n \tif (success) {\n \t\tresult = HeaderToString();\n-\t\tresult += \"[ Rows: \" + to_string(collection.Count()) + \"]\\n\";\n-\t\tfor (idx_t j = 0; j < collection.Count(); j++) {\n-\t\t\tfor (idx_t i = 0; i < collection.ColumnCount(); i++) {\n-\t\t\t\tauto val = collection.GetValue(i, j);\n+\t\tresult += \"[ Rows: \" + to_string(collection->Count()) + \"]\\n\";\n+\t\tauto &coll = Collection();\n+\t\tfor (auto &row : coll.Rows()) {\n+\t\t\tfor (idx_t col_idx = 0; col_idx < coll.ColumnCount(); col_idx++) {\n+\t\t\t\tif (col_idx > 0) {\n+\t\t\t\t\tresult += \"\\t\";\n+\t\t\t\t}\n+\t\t\t\tauto val = row.GetValue(col_idx);\n \t\t\t\tresult += val.IsNull() ? \"NULL\" : val.ToString();\n-\t\t\t\tresult += \"\\t\";\n \t\t\t}\n \t\t\tresult += \"\\n\";\n \t\t}\n@@ -40,6 +39,28 @@ string MaterializedQueryResult::ToString() {\n \treturn result;\n }\n \n+Value MaterializedQueryResult::GetValue(idx_t column, idx_t index) {\n+\tif (!row_collection) {\n+\t\trow_collection = make_unique<ColumnDataRowCollection>(collection->GetRows());\n+\t}\n+\treturn row_collection->GetValue(column, index);\n+}\n+\n+idx_t MaterializedQueryResult::RowCount() const {\n+\treturn collection ? collection->Count() : 0;\n+}\n+\n+ColumnDataCollection &MaterializedQueryResult::Collection() {\n+\tif (!success) {\n+\t\tthrow InvalidInputException(\"Attempting to get collection from an unsuccessful query result\\n: Error %s\",\n+\t\t                            error);\n+\t}\n+\tif (!collection) {\n+\t\tthrow InternalException(\"Missing collection from materialized query result\");\n+\t}\n+\treturn *collection;\n+}\n+\n unique_ptr<DataChunk> MaterializedQueryResult::Fetch() {\n \treturn FetchRaw();\n }\n@@ -48,7 +69,18 @@ unique_ptr<DataChunk> MaterializedQueryResult::FetchRaw() {\n \tif (!success) {\n \t\tthrow InvalidInputException(\"Attempting to fetch from an unsuccessful query result\\nError: %s\", error);\n \t}\n-\treturn collection.Fetch();\n+\tauto result = make_unique<DataChunk>();\n+\tcollection->InitializeScanChunk(*result);\n+\tif (!scan_initialized) {\n+\t\t// we disallow zero copy so the chunk is independently usable even after the result is destroyed\n+\t\tcollection->InitializeScan(scan_state, ColumnDataScanProperties::DISALLOW_ZERO_COPY);\n+\t\tscan_initialized = true;\n+\t}\n+\tcollection->Scan(scan_state, *result);\n+\tif (result->size() == 0) {\n+\t\treturn nullptr;\n+\t}\n+\treturn result;\n }\n \n } // namespace duckdb\ndiff --git a/src/main/query_result.cpp b/src/main/query_result.cpp\nindex 7d8c07051210..8fa1a14a66da 100644\n--- a/src/main/query_result.cpp\n+++ b/src/main/query_result.cpp\n@@ -1,7 +1,5 @@\n-#include <list>\n #include \"duckdb/main/query_result.hpp\"\n #include \"duckdb/common/printer.hpp\"\n-#include \"duckdb/common/arrow.hpp\"\n #include \"duckdb/common/vector.hpp\"\n #include \"duckdb/main/client_context.hpp\"\n \n@@ -31,8 +29,9 @@ idx_t BaseQueryResult::ColumnCount() {\n }\n \n QueryResult::QueryResult(QueryResultType type, StatementType statement_type, StatementProperties properties,\n-                         vector<LogicalType> types_p, vector<string> names_p)\n-    : BaseQueryResult(type, statement_type, properties, move(types_p), move(names_p)) {\n+                         vector<LogicalType> types_p, vector<string> names_p, ClientProperties client_properties_p)\n+    : BaseQueryResult(type, statement_type, properties, move(types_p), move(names_p)),\n+      client_properties(move(client_properties_p)) {\n }\n \n QueryResult::QueryResult(QueryResultType type, string error) : BaseQueryResult(type, move(error)) {\n@@ -91,6 +90,9 @@ bool QueryResult::Equals(QueryResult &other) { // LCOV_EXCL_START\n \t\t\t\tif (lvalue.IsNull() && rvalue.IsNull()) {\n \t\t\t\t\tcontinue;\n \t\t\t\t}\n+\t\t\t\tif (lvalue.IsNull() != rvalue.IsNull()) {\n+\t\t\t\t\treturn false;\n+\t\t\t\t}\n \t\t\t\tif (lvalue != rvalue) {\n \t\t\t\t\treturn false;\n \t\t\t\t}\n@@ -115,288 +117,8 @@ string QueryResult::HeaderToString() {\n \tresult += \"\\n\";\n \treturn result;\n }\n-\n-struct DuckDBArrowSchemaHolder {\n-\t// unused in children\n-\tvector<ArrowSchema> children;\n-\t// unused in children\n-\tvector<ArrowSchema *> children_ptrs;\n-\t//! used for nested structures\n-\tstd::list<std::vector<ArrowSchema>> nested_children;\n-\tstd::list<std::vector<ArrowSchema *>> nested_children_ptr;\n-\t//! This holds strings created to represent decimal types\n-\tvector<unique_ptr<char[]>> owned_type_names;\n-};\n-\n-static void ReleaseDuckDBArrowSchema(ArrowSchema *schema) {\n-\tif (!schema || !schema->release) {\n-\t\treturn;\n-\t}\n-\tschema->release = nullptr;\n-\tauto holder = static_cast<DuckDBArrowSchemaHolder *>(schema->private_data);\n-\tdelete holder;\n-}\n-\n-void InitializeChild(ArrowSchema &child, const string &name = \"\") {\n-\t//! Child is cleaned up by parent\n-\tchild.private_data = nullptr;\n-\tchild.release = ReleaseDuckDBArrowSchema;\n-\n-\t//! Store the child schema\n-\tchild.flags = ARROW_FLAG_NULLABLE;\n-\tchild.name = name.c_str();\n-\tchild.n_children = 0;\n-\tchild.children = nullptr;\n-\tchild.metadata = nullptr;\n-\tchild.dictionary = nullptr;\n-}\n-void SetArrowFormat(DuckDBArrowSchemaHolder &root_holder, ArrowSchema &child, const LogicalType &type,\n-                    string &config_timezone);\n-\n-void SetArrowMapFormat(DuckDBArrowSchemaHolder &root_holder, ArrowSchema &child, const LogicalType &type,\n-                       string &config_timezone) {\n-\tchild.format = \"+m\";\n-\t//! Map has one child which is a struct\n-\tchild.n_children = 1;\n-\troot_holder.nested_children.emplace_back();\n-\troot_holder.nested_children.back().resize(1);\n-\troot_holder.nested_children_ptr.emplace_back();\n-\troot_holder.nested_children_ptr.back().push_back(&root_holder.nested_children.back()[0]);\n-\tInitializeChild(root_holder.nested_children.back()[0]);\n-\tchild.children = &root_holder.nested_children_ptr.back()[0];\n-\tchild.children[0]->name = \"entries\";\n-\tchild_list_t<LogicalType> struct_child_types;\n-\tstruct_child_types.push_back(std::make_pair(\"key\", ListType::GetChildType(StructType::GetChildType(type, 0))));\n-\tstruct_child_types.push_back(std::make_pair(\"value\", ListType::GetChildType(StructType::GetChildType(type, 1))));\n-\tauto struct_type = LogicalType::STRUCT(move(struct_child_types));\n-\tSetArrowFormat(root_holder, *child.children[0], struct_type, config_timezone);\n-}\n-\n-void SetArrowFormat(DuckDBArrowSchemaHolder &root_holder, ArrowSchema &child, const LogicalType &type,\n-                    string &config_timezone) {\n-\tswitch (type.id()) {\n-\tcase LogicalTypeId::BOOLEAN:\n-\t\tchild.format = \"b\";\n-\t\tbreak;\n-\tcase LogicalTypeId::TINYINT:\n-\t\tchild.format = \"c\";\n-\t\tbreak;\n-\tcase LogicalTypeId::SMALLINT:\n-\t\tchild.format = \"s\";\n-\t\tbreak;\n-\tcase LogicalTypeId::INTEGER:\n-\t\tchild.format = \"i\";\n-\t\tbreak;\n-\tcase LogicalTypeId::BIGINT:\n-\t\tchild.format = \"l\";\n-\t\tbreak;\n-\tcase LogicalTypeId::UTINYINT:\n-\t\tchild.format = \"C\";\n-\t\tbreak;\n-\tcase LogicalTypeId::USMALLINT:\n-\t\tchild.format = \"S\";\n-\t\tbreak;\n-\tcase LogicalTypeId::UINTEGER:\n-\t\tchild.format = \"I\";\n-\t\tbreak;\n-\tcase LogicalTypeId::UBIGINT:\n-\t\tchild.format = \"L\";\n-\t\tbreak;\n-\tcase LogicalTypeId::FLOAT:\n-\t\tchild.format = \"f\";\n-\t\tbreak;\n-\tcase LogicalTypeId::HUGEINT:\n-\t\tchild.format = \"d:38,0\";\n-\t\tbreak;\n-\tcase LogicalTypeId::DOUBLE:\n-\t\tchild.format = \"g\";\n-\t\tbreak;\n-\tcase LogicalTypeId::UUID:\n-\tcase LogicalTypeId::JSON:\n-\tcase LogicalTypeId::VARCHAR:\n-\t\tchild.format = \"u\";\n-\t\tbreak;\n-\tcase LogicalTypeId::DATE:\n-\t\tchild.format = \"tdD\";\n-\t\tbreak;\n-\tcase LogicalTypeId::TIME:\n-\tcase LogicalTypeId::TIME_TZ:\n-\t\tchild.format = \"ttu\";\n-\t\tbreak;\n-\tcase LogicalTypeId::TIMESTAMP:\n-\t\tchild.format = \"tsu:\";\n-\t\tbreak;\n-\tcase LogicalTypeId::TIMESTAMP_TZ: {\n-\t\tstring format = \"tsu:\" + config_timezone;\n-\t\tunique_ptr<char[]> format_ptr = unique_ptr<char[]>(new char[format.size() + 1]);\n-\t\tfor (size_t i = 0; i < format.size(); i++) {\n-\t\t\tformat_ptr[i] = format[i];\n-\t\t}\n-\t\tformat_ptr[format.size()] = '\\0';\n-\t\troot_holder.owned_type_names.push_back(move(format_ptr));\n-\t\tchild.format = root_holder.owned_type_names.back().get();\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::TIMESTAMP_SEC:\n-\t\tchild.format = \"tss:\";\n-\t\tbreak;\n-\tcase LogicalTypeId::TIMESTAMP_NS:\n-\t\tchild.format = \"tsn:\";\n-\t\tbreak;\n-\tcase LogicalTypeId::TIMESTAMP_MS:\n-\t\tchild.format = \"tsm:\";\n-\t\tbreak;\n-\tcase LogicalTypeId::INTERVAL:\n-\t\tchild.format = \"tDm\";\n-\t\tbreak;\n-\tcase LogicalTypeId::DECIMAL: {\n-\t\tuint8_t width, scale;\n-\t\ttype.GetDecimalProperties(width, scale);\n-\t\tstring format = \"d:\" + to_string(width) + \",\" + to_string(scale);\n-\t\tunique_ptr<char[]> format_ptr = unique_ptr<char[]>(new char[format.size() + 1]);\n-\t\tfor (size_t i = 0; i < format.size(); i++) {\n-\t\t\tformat_ptr[i] = format[i];\n-\t\t}\n-\t\tformat_ptr[format.size()] = '\\0';\n-\t\troot_holder.owned_type_names.push_back(move(format_ptr));\n-\t\tchild.format = root_holder.owned_type_names.back().get();\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::SQLNULL: {\n-\t\tchild.format = \"n\";\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::BLOB: {\n-\t\tchild.format = \"z\";\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::LIST: {\n-\t\tchild.format = \"+l\";\n-\t\tchild.n_children = 1;\n-\t\troot_holder.nested_children.emplace_back();\n-\t\troot_holder.nested_children.back().resize(1);\n-\t\troot_holder.nested_children_ptr.emplace_back();\n-\t\troot_holder.nested_children_ptr.back().push_back(&root_holder.nested_children.back()[0]);\n-\t\tInitializeChild(root_holder.nested_children.back()[0]);\n-\t\tchild.children = &root_holder.nested_children_ptr.back()[0];\n-\t\tchild.children[0]->name = \"l\";\n-\t\tSetArrowFormat(root_holder, **child.children, ListType::GetChildType(type), config_timezone);\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::STRUCT: {\n-\t\tchild.format = \"+s\";\n-\t\tauto &child_types = StructType::GetChildTypes(type);\n-\t\tchild.n_children = child_types.size();\n-\t\troot_holder.nested_children.emplace_back();\n-\t\troot_holder.nested_children.back().resize(child_types.size());\n-\t\troot_holder.nested_children_ptr.emplace_back();\n-\t\troot_holder.nested_children_ptr.back().resize(child_types.size());\n-\t\tfor (idx_t type_idx = 0; type_idx < child_types.size(); type_idx++) {\n-\t\t\troot_holder.nested_children_ptr.back()[type_idx] = &root_holder.nested_children.back()[type_idx];\n-\t\t}\n-\t\tchild.children = &root_holder.nested_children_ptr.back()[0];\n-\t\tfor (size_t type_idx = 0; type_idx < child_types.size(); type_idx++) {\n-\n-\t\t\tInitializeChild(*child.children[type_idx]);\n-\n-\t\t\tauto &struct_col_name = child_types[type_idx].first;\n-\t\t\tunique_ptr<char[]> name_ptr = unique_ptr<char[]>(new char[struct_col_name.size() + 1]);\n-\t\t\tfor (size_t i = 0; i < struct_col_name.size(); i++) {\n-\t\t\t\tname_ptr[i] = struct_col_name[i];\n-\t\t\t}\n-\t\t\tname_ptr[struct_col_name.size()] = '\\0';\n-\t\t\troot_holder.owned_type_names.push_back(move(name_ptr));\n-\n-\t\t\tchild.children[type_idx]->name = root_holder.owned_type_names.back().get();\n-\t\t\tSetArrowFormat(root_holder, *child.children[type_idx], child_types[type_idx].second, config_timezone);\n-\t\t}\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::MAP: {\n-\t\tSetArrowMapFormat(root_holder, child, type, config_timezone);\n-\t\tbreak;\n-\t}\n-\tcase LogicalTypeId::ENUM: {\n-\t\t// TODO what do we do with pointer enums here?\n-\t\tswitch (EnumType::GetPhysicalType(type)) {\n-\t\tcase PhysicalType::UINT8:\n-\t\t\tchild.format = \"C\";\n-\t\t\tbreak;\n-\t\tcase PhysicalType::UINT16:\n-\t\t\tchild.format = \"S\";\n-\t\t\tbreak;\n-\t\tcase PhysicalType::UINT32:\n-\t\t\tchild.format = \"I\";\n-\t\t\tbreak;\n-\t\tdefault:\n-\t\t\tthrow InternalException(\"Unsupported Enum Internal Type\");\n-\t\t}\n-\t\troot_holder.nested_children.emplace_back();\n-\t\troot_holder.nested_children.back().resize(1);\n-\t\troot_holder.nested_children_ptr.emplace_back();\n-\t\troot_holder.nested_children_ptr.back().push_back(&root_holder.nested_children.back()[0]);\n-\t\tInitializeChild(root_holder.nested_children.back()[0]);\n-\t\tchild.dictionary = root_holder.nested_children_ptr.back()[0];\n-\t\tchild.dictionary->format = \"u\";\n-\t\tbreak;\n-\t}\n-\tdefault:\n-\t\tthrow InternalException(\"Unsupported Arrow type \" + type.ToString());\n-\t}\n-}\n-\n-void QueryResult::ToArrowSchema(ArrowSchema *out_schema, vector<LogicalType> &types, vector<string> &names,\n-                                string &config_timezone) {\n-\tD_ASSERT(out_schema);\n-\tD_ASSERT(types.size() == names.size());\n-\tidx_t column_count = types.size();\n-\t// Allocate as unique_ptr first to cleanup properly on error\n-\tauto root_holder = make_unique<DuckDBArrowSchemaHolder>();\n-\n-\t// Allocate the children\n-\troot_holder->children.resize(column_count);\n-\troot_holder->children_ptrs.resize(column_count, nullptr);\n-\tfor (size_t i = 0; i < column_count; ++i) {\n-\t\troot_holder->children_ptrs[i] = &root_holder->children[i];\n-\t}\n-\tout_schema->children = root_holder->children_ptrs.data();\n-\tout_schema->n_children = column_count;\n-\n-\t// Store the schema\n-\tout_schema->format = \"+s\"; // struct apparently\n-\tout_schema->flags = 0;\n-\tout_schema->metadata = nullptr;\n-\tout_schema->name = \"duckdb_query_result\";\n-\tout_schema->dictionary = nullptr;\n-\n-\t// Configure all child schemas\n-\tfor (idx_t col_idx = 0; col_idx < column_count; col_idx++) {\n-\n-\t\tauto &child = root_holder->children[col_idx];\n-\t\tInitializeChild(child, names[col_idx]);\n-\t\tSetArrowFormat(*root_holder, child, types[col_idx], config_timezone);\n-\t}\n-\n-\t// Release ownership to caller\n-\tout_schema->private_data = root_holder.release();\n-\tout_schema->release = ReleaseDuckDBArrowSchema;\n-}\n-\n string QueryResult::GetConfigTimezone(QueryResult &query_result) {\n-\tswitch (query_result.type) {\n-\tcase QueryResultType::MATERIALIZED_RESULT: {\n-\t\tauto actual_context = ((MaterializedQueryResult &)query_result).context.lock();\n-\t\tif (!actual_context) {\n-\t\t\tthrow std::runtime_error(\"This connection is closed\");\n-\t\t}\n-\t\treturn ClientConfig::ExtractTimezoneFromConfig(actual_context->config);\n-\t}\n-\tcase QueryResultType::STREAM_RESULT: {\n-\t\treturn ClientConfig::ExtractTimezoneFromConfig(((StreamQueryResult &)query_result).context->config);\n-\t}\n-\tdefault:\n-\t\tthrow std::runtime_error(\"Can't extract timezone configuration from query type \");\n-\t}\n+\treturn query_result.client_properties.timezone;\n }\n \n } // namespace duckdb\ndiff --git a/src/main/stream_query_result.cpp b/src/main/stream_query_result.cpp\nindex a92d5d32fd26..e13bcbada119 100644\n--- a/src/main/stream_query_result.cpp\n+++ b/src/main/stream_query_result.cpp\n@@ -6,9 +6,12 @@\n namespace duckdb {\n \n StreamQueryResult::StreamQueryResult(StatementType statement_type, StatementProperties properties,\n-                                     shared_ptr<ClientContext> context, vector<LogicalType> types, vector<string> names)\n-    : QueryResult(QueryResultType::STREAM_RESULT, statement_type, properties, move(types), move(names)),\n-      context(move(context)) {\n+                                     shared_ptr<ClientContext> context_p, vector<LogicalType> types,\n+                                     vector<string> names)\n+    : QueryResult(QueryResultType::STREAM_RESULT, statement_type, properties, move(types), move(names),\n+                  context_p->GetClientProperties()),\n+      context(move(context_p)) {\n+\tD_ASSERT(context);\n }\n \n StreamQueryResult::~StreamQueryResult() {\n@@ -55,17 +58,22 @@ unique_ptr<DataChunk> StreamQueryResult::FetchRaw() {\n }\n \n unique_ptr<MaterializedQueryResult> StreamQueryResult::Materialize() {\n-\tif (!success) {\n+\tif (!success || !context) {\n \t\treturn make_unique<MaterializedQueryResult>(error);\n \t}\n-\tauto result = make_unique<MaterializedQueryResult>(statement_type, properties, types, names, context);\n+\tauto collection = make_unique<ColumnDataCollection>(Allocator::DefaultAllocator(), types);\n+\n+\tColumnDataAppendState append_state;\n+\tcollection->InitializeAppend(append_state);\n \twhile (true) {\n \t\tauto chunk = Fetch();\n \t\tif (!chunk || chunk->size() == 0) {\n \t\t\tbreak;\n \t\t}\n-\t\tresult->collection.Append(*chunk);\n+\t\tcollection->Append(append_state, *chunk);\n \t}\n+\tauto result =\n+\t    make_unique<MaterializedQueryResult>(statement_type, properties, names, move(collection), client_properties);\n \tif (!success) {\n \t\treturn make_unique<MaterializedQueryResult>(error);\n \t}\ndiff --git a/src/optimizer/in_clause_rewriter.cpp b/src/optimizer/in_clause_rewriter.cpp\nindex f1a3e0114087..b505613e0ceb 100644\n--- a/src/optimizer/in_clause_rewriter.cpp\n+++ b/src/optimizer/in_clause_rewriter.cpp\n@@ -4,7 +4,7 @@\n #include \"duckdb/planner/expression/bound_comparison_expression.hpp\"\n #include \"duckdb/planner/expression/bound_conjunction_expression.hpp\"\n #include \"duckdb/planner/expression/bound_operator_expression.hpp\"\n-#include \"duckdb/planner/operator/logical_chunk_get.hpp\"\n+#include \"duckdb/planner/operator/logical_column_data_get.hpp\"\n #include \"duckdb/planner/operator/logical_comparison_join.hpp\"\n #include \"duckdb/execution/expression_executor.hpp\"\n \n@@ -63,9 +63,12 @@ unique_ptr<Expression> InClauseRewriter::VisitReplace(BoundOperatorExpression &e\n \t}\n \t// IN clause with many constant children\n \t// generate a mark join that replaces this IN expression\n-\t// first generate a ChunkCollection from the set of expressions\n+\t// first generate a ColumnDataCollection from the set of expressions\n \tvector<LogicalType> types = {in_type};\n-\tauto collection = make_unique<ChunkCollection>(context);\n+\tauto collection = make_unique<ColumnDataCollection>(context, types);\n+\tColumnDataAppendState append_state;\n+\tcollection->InitializeAppend(append_state);\n+\n \tDataChunk chunk;\n \tchunk.Initialize(context, types);\n \tfor (idx_t i = 1; i < expr.children.size(); i++) {\n@@ -76,13 +79,13 @@ unique_ptr<Expression> InClauseRewriter::VisitReplace(BoundOperatorExpression &e\n \t\tchunk.SetValue(0, index, value);\n \t\tif (chunk.size() == STANDARD_VECTOR_SIZE || i + 1 == expr.children.size()) {\n \t\t\t// chunk full: append to chunk collection\n-\t\t\tcollection->Append(chunk);\n+\t\t\tcollection->Append(append_state, chunk);\n \t\t\tchunk.Reset();\n \t\t}\n \t}\n \t// now generate a ChunkGet that scans this collection\n \tauto chunk_index = optimizer.binder.GenerateTableIndex();\n-\tauto chunk_scan = make_unique<LogicalChunkGet>(chunk_index, types, move(collection));\n+\tauto chunk_scan = make_unique<LogicalColumnDataGet>(chunk_index, types, move(collection));\n \n \t// then we generate the MARK join with the chunk scan on the RHS\n \tauto join = make_unique<LogicalComparisonJoin>(JoinType::MARK);\ndiff --git a/src/parallel/executor.cpp b/src/parallel/executor.cpp\nindex dee29fd294ae..39ba943eeaa8 100644\n--- a/src/parallel/executor.cpp\n+++ b/src/parallel/executor.cpp\n@@ -217,6 +217,7 @@ bool Executor::NextExecutor() {\n \tif (root_pipeline_idx >= root_pipelines.size()) {\n \t\treturn false;\n \t}\n+\troot_pipelines[root_pipeline_idx]->Reset();\n \troot_executor = make_unique<PipelineExecutor>(context, *root_pipelines[root_pipeline_idx]);\n \troot_pipeline_idx++;\n \treturn true;\ndiff --git a/src/parallel/pipeline.cpp b/src/parallel/pipeline.cpp\nindex 803f8e981bb6..a10daf266578 100644\n--- a/src/parallel/pipeline.cpp\n+++ b/src/parallel/pipeline.cpp\n@@ -48,7 +48,8 @@ class PipelineTask : public ExecutorTask {\n \t}\n };\n \n-Pipeline::Pipeline(Executor &executor_p) : executor(executor_p), ready(false), source(nullptr), sink(nullptr) {\n+Pipeline::Pipeline(Executor &executor_p)\n+    : executor(executor_p), ready(false), initialized(false), source(nullptr), sink(nullptr) {\n }\n \n ClientContext &Pipeline::GetClientContext() {\n@@ -57,10 +58,13 @@ ClientContext &Pipeline::GetClientContext() {\n \n bool Pipeline::GetProgress(double &current_percentage, idx_t &source_cardinality) {\n \tD_ASSERT(source);\n-\n+\tsource_cardinality = source->estimated_cardinality;\n+\tif (!initialized) {\n+\t\tcurrent_percentage = 0;\n+\t\treturn true;\n+\t}\n \tauto &client = executor.context;\n \tcurrent_percentage = source->GetProgress(client, *source_state);\n-\tsource_cardinality = source->estimated_cardinality;\n \treturn current_percentage >= 0;\n }\n \n@@ -115,6 +119,7 @@ bool Pipeline::IsOrderDependent() const {\n void Pipeline::Schedule(shared_ptr<Event> &event) {\n \tD_ASSERT(ready);\n \tD_ASSERT(sink);\n+\tReset();\n \tif (!ScheduleParallel(event)) {\n \t\t// could not parallelize this pipeline: push a sequential task instead\n \t\tScheduleSequentialTask(event);\n@@ -152,8 +157,8 @@ void Pipeline::Reset() {\n \t\t\top->op_state = op->GetGlobalOperatorState(GetClientContext());\n \t\t}\n \t}\n-\n \tResetSource();\n+\tinitialized = true;\n }\n \n void Pipeline::ResetSource() {\n@@ -166,7 +171,6 @@ void Pipeline::Ready() {\n \t}\n \tready = true;\n \tstd::reverse(operators.begin(), operators.end());\n-\tReset();\n }\n \n void Pipeline::Finalize(Event &event) {\ndiff --git a/src/storage/arena_allocator.cpp b/src/storage/arena_allocator.cpp\nindex a15a99b4841a..85dc791a8ae2 100644\n--- a/src/storage/arena_allocator.cpp\n+++ b/src/storage/arena_allocator.cpp\n@@ -41,7 +41,7 @@ data_ptr_t ArenaAllocator::Allocate(idx_t len) {\n \t\thead = move(new_chunk);\n \t}\n \tD_ASSERT(head->current_position + len <= head->maximum_size);\n-\tauto result = head->data->get() + head->current_position;\n+\tauto result = head->data.get() + head->current_position;\n \thead->current_position += len;\n \treturn result;\n }\ndiff --git a/src/storage/data_table.cpp b/src/storage/data_table.cpp\nindex 4b749f741905..8bb2209be7fb 100644\n--- a/src/storage/data_table.cpp\n+++ b/src/storage/data_table.cpp\n@@ -1287,6 +1287,7 @@ void DataTable::AddIndex(unique_ptr<Index> index, const vector<unique_ptr<Expres\n \t\tExpressionExecutor executor(allocator, expressions);\n \t\twhile (true) {\n \t\t\tintermediate.Reset();\n+\t\t\tresult.Reset();\n \t\t\t// scan a new chunk from the table to index\n \t\t\tScanCreateIndex(state, intermediate, TableScanType::TABLE_SCAN_COMMITTED_ROWS_OMIT_PERMANENTLY_DELETED);\n \t\t\tif (intermediate.size() == 0) {\ndiff --git a/src/transaction/undo_buffer.cpp b/src/transaction/undo_buffer.cpp\nindex c6d3174dea17..c33a354f2319 100644\n--- a/src/transaction/undo_buffer.cpp\n+++ b/src/transaction/undo_buffer.cpp\n@@ -37,7 +37,7 @@ void UndoBuffer::IterateEntries(UndoBuffer::IteratorState &state, T &&callback)\n \t// iterate in insertion order: start with the tail\n \tstate.current = allocator.GetTail();\n \twhile (state.current) {\n-\t\tstate.start = state.current->data->get();\n+\t\tstate.start = state.current->data.get();\n \t\tstate.end = state.start + state.current->current_position;\n \t\twhile (state.start < state.end) {\n \t\t\tUndoFlags type = Load<UndoFlags>(state.start);\n@@ -57,7 +57,7 @@ void UndoBuffer::IterateEntries(UndoBuffer::IteratorState &state, UndoBuffer::It\n \t// iterate in insertion order: start with the tail\n \tstate.current = allocator.GetTail();\n \twhile (state.current) {\n-\t\tstate.start = state.current->data->get();\n+\t\tstate.start = state.current->data.get();\n \t\tstate.end =\n \t\t    state.current == end_state.current ? end_state.start : state.start + state.current->current_position;\n \t\twhile (state.start < state.end) {\n@@ -81,7 +81,7 @@ void UndoBuffer::ReverseIterateEntries(T &&callback) {\n \t// iterate in reverse insertion order: start with the head\n \tauto current = allocator.GetHead();\n \twhile (current) {\n-\t\tdata_ptr_t start = current->data->get();\n+\t\tdata_ptr_t start = current->data.get();\n \t\tdata_ptr_t end = start + current->current_position;\n \t\t// create a vector with all nodes in this chunk\n \t\tvector<pair<UndoFlags, data_ptr_t>> nodes;\ndiff --git a/third_party/jaro_winkler/details/intrinsics.hpp b/third_party/jaro_winkler/details/intrinsics.hpp\nindex 748cad312cec..174bdccd841b 100644\n--- a/third_party/jaro_winkler/details/intrinsics.hpp\n+++ b/third_party/jaro_winkler/details/intrinsics.hpp\n@@ -95,10 +95,10 @@ static inline int tzcnt(uint64_t x)\n #    endif\n \n #else /*  gcc / clang */\n-static inline int tzcnt(uint32_t x)\n-{\n-    return __builtin_ctz(x);\n-}\n+//static inline int tzcnt(uint32_t x)\n+//{\n+//    return __builtin_ctz(x);\n+//}\n \n static inline int tzcnt(uint64_t x)\n {\ndiff --git a/tools/nodejs/src/statement.cpp b/tools/nodejs/src/statement.cpp\nindex e19709fa7d21..aaf98a37d6a2 100644\n--- a/tools/nodejs/src/statement.cpp\n+++ b/tools/nodejs/src/statement.cpp\n@@ -311,7 +311,7 @@ struct RunPreparedTask : public Task {\n \t\t}\n \t\tcase RunType::ALL: {\n \t\t\tauto materialized_result = (duckdb::MaterializedQueryResult *)result.get();\n-\t\t\tNapi::Array result_arr(Napi::Array::New(env, materialized_result->collection.Count()));\n+\t\t\tNapi::Array result_arr(Napi::Array::New(env, materialized_result->RowCount()));\n \n \t\t\tduckdb::idx_t out_idx = 0;\n \t\t\twhile (true) {\ndiff --git a/tools/pythonpkg/src/arrow_array_stream.cpp b/tools/pythonpkg/src/arrow_array_stream.cpp\nindex 9a29a8678c48..1880248ecc3b 100644\n--- a/tools/pythonpkg/src/arrow_array_stream.cpp\n+++ b/tools/pythonpkg/src/arrow_array_stream.cpp\n@@ -10,30 +10,29 @@\n \n namespace duckdb {\n \n-py::object PythonTableArrowArrayStreamFactory::ProduceScanner(\n-    py::object &arrow_scanner, py::handle &arrow_obj_handle,\n-    std::pair<std::unordered_map<idx_t, string>, std::vector<string>> &project_columns, TableFilterSet *filters,\n-    ClientConfig &config) {\n+py::object PythonTableArrowArrayStreamFactory::ProduceScanner(py::object &arrow_scanner, py::handle &arrow_obj_handle,\n+                                                              ArrowStreamParameters &parameters, ClientConfig &config) {\n+\tauto filters = parameters.filters;\n+\tauto &column_list = parameters.projected_columns.columns;\n \tbool has_filter = filters && !filters->filters.empty();\n-\tpy::list projection_list = py::cast(project_columns.second);\n+\tpy::list projection_list = py::cast(column_list);\n \tif (has_filter) {\n-\t\tauto filter = TransformFilter(*filters, project_columns.first, config);\n-\t\tif (project_columns.second.empty()) {\n+\t\tauto filter = TransformFilter(*filters, parameters.projected_columns.projection_map, config);\n+\t\tif (column_list.empty()) {\n \t\t\treturn arrow_scanner(arrow_obj_handle, py::arg(\"filter\") = filter);\n \t\t} else {\n \t\t\treturn arrow_scanner(arrow_obj_handle, py::arg(\"columns\") = projection_list, py::arg(\"filter\") = filter);\n \t\t}\n \t} else {\n-\t\tif (project_columns.second.empty()) {\n+\t\tif (column_list.empty()) {\n \t\t\treturn arrow_scanner(arrow_obj_handle);\n \t\t} else {\n \t\t\treturn arrow_scanner(arrow_obj_handle, py::arg(\"columns\") = projection_list);\n \t\t}\n \t}\n }\n-unique_ptr<ArrowArrayStreamWrapper> PythonTableArrowArrayStreamFactory::Produce(\n-    uintptr_t factory_ptr, std::pair<std::unordered_map<idx_t, string>, std::vector<string>> &project_columns,\n-    TableFilterSet *filters) {\n+unique_ptr<ArrowArrayStreamWrapper> PythonTableArrowArrayStreamFactory::Produce(uintptr_t factory_ptr,\n+                                                                                ArrowStreamParameters &parameters) {\n \tpy::gil_scoped_acquire acquire;\n \tPythonTableArrowArrayStreamFactory *factory = (PythonTableArrowArrayStreamFactory *)factory_ptr;\n \tD_ASSERT(factory->arrow_object);\n@@ -45,18 +44,18 @@ unique_ptr<ArrowArrayStreamWrapper> PythonTableArrowArrayStreamFactory::Produce(\n \tif (py_object_type == \"Table\") {\n \t\tauto arrow_dataset = py::module_::import(\"pyarrow.dataset\").attr(\"dataset\");\n \t\tauto dataset = arrow_dataset(arrow_obj_handle);\n-\t\tscanner = ProduceScanner(arrow_scanner, dataset, project_columns, filters, factory->config);\n+\t\tscanner = ProduceScanner(arrow_scanner, dataset, parameters, factory->config);\n \t} else if (py_object_type == \"RecordBatchReader\") {\n \t\tpy::object arrow_batch_scanner = py::module_::import(\"pyarrow.dataset\").attr(\"Scanner\").attr(\"from_batches\");\n-\t\tscanner = ProduceScanner(arrow_batch_scanner, arrow_obj_handle, project_columns, filters, factory->config);\n+\t\tscanner = ProduceScanner(arrow_batch_scanner, arrow_obj_handle, parameters, factory->config);\n \t} else if (py_object_type == \"Scanner\") {\n \t\t// If it's a scanner we have to turn it to a record batch reader, and then a scanner again since we can't stack\n \t\t// scanners on arrow Otherwise pushed-down projections and filters will disappear like tears in the rain\n \t\tauto record_batches = arrow_obj_handle.attr(\"to_reader\")();\n \t\tpy::object arrow_batch_scanner = py::module_::import(\"pyarrow.dataset\").attr(\"Scanner\").attr(\"from_batches\");\n-\t\tscanner = ProduceScanner(arrow_batch_scanner, record_batches, project_columns, filters, factory->config);\n+\t\tscanner = ProduceScanner(arrow_batch_scanner, record_batches, parameters, factory->config);\n \t} else {\n-\t\tscanner = ProduceScanner(arrow_scanner, arrow_obj_handle, project_columns, filters, factory->config);\n+\t\tscanner = ProduceScanner(arrow_scanner, arrow_obj_handle, parameters, factory->config);\n \t}\n \n \tauto record_batches = scanner.attr(\"to_reader\")();\n@@ -241,7 +240,7 @@ py::object PythonTableArrowArrayStreamFactory::TransformFilter(TableFilterSet &f\n \tauto filters_map = &filter_collection.filters;\n \tauto it = filters_map->begin();\n \tD_ASSERT(columns.find(it->first) != columns.end());\n-\tstring timezone_config = ClientConfig::ExtractTimezoneFromConfig(config);\n+\tstring timezone_config = config.ExtractTimezone();\n \tpy::object expression = TransformFilterRecursive(it->second.get(), columns[it->first], timezone_config);\n \twhile (it != filters_map->end()) {\n \t\tpy::object child_expression = TransformFilterRecursive(it->second.get(), columns[it->first], timezone_config);\ndiff --git a/tools/pythonpkg/src/include/duckdb_python/arrow_array_stream.hpp b/tools/pythonpkg/src/include/duckdb_python/arrow_array_stream.hpp\nindex 69643e3fdf78..18667c148662 100644\n--- a/tools/pythonpkg/src/include/duckdb_python/arrow_array_stream.hpp\n+++ b/tools/pythonpkg/src/include/duckdb_python/arrow_array_stream.hpp\n@@ -8,10 +8,11 @@\n \n #pragma once\n \n-#include \"duckdb/common/arrow_wrapper.hpp\"\n+#include \"duckdb/common/arrow/arrow_wrapper.hpp\"\n #include \"duckdb/common/atomic.hpp\"\n #include \"duckdb/common/constants.hpp\"\n #include \"duckdb/function/table_function.hpp\"\n+#include \"duckdb/function/table/arrow.hpp\"\n #include \"duckdb/main/client_config.hpp\"\n #include \"duckdb/main/config.hpp\"\n #include \"pybind_wrapper.hpp\"\n@@ -26,9 +27,7 @@ class PythonTableArrowArrayStreamFactory {\n \t    : arrow_object(arrow_table), config(config) {};\n \n \t//! Produces an Arrow Scanner, should be only called once when initializing Scan States\n-\tstatic unique_ptr<ArrowArrayStreamWrapper>\n-\tProduce(uintptr_t factory, std::pair<std::unordered_map<idx_t, string>, std::vector<string>> &project_columns,\n-\t        TableFilterSet *filters = nullptr);\n+\tstatic unique_ptr<ArrowArrayStreamWrapper> Produce(uintptr_t factory, ArrowStreamParameters &parameters);\n \n \t//! Get the schema of the arrow object\n \tstatic void GetSchema(uintptr_t factory_ptr, ArrowSchemaWrapper &schema);\n@@ -44,7 +43,6 @@ class PythonTableArrowArrayStreamFactory {\n \t                                  ClientConfig &config);\n \n \tstatic py::object ProduceScanner(py::object &arrow_scanner, py::handle &arrow_obj_handle,\n-\t                                 std::pair<std::unordered_map<idx_t, string>, std::vector<string>> &project_columns,\n-\t                                 TableFilterSet *filters, ClientConfig &config);\n+\t                                 ArrowStreamParameters &parameters, ClientConfig &config);\n };\n } // namespace duckdb\ndiff --git a/tools/pythonpkg/src/pyconnection.cpp b/tools/pythonpkg/src/pyconnection.cpp\nindex 6ba3cd899075..469b9fa38f37 100644\n--- a/tools/pythonpkg/src/pyconnection.cpp\n+++ b/tools/pythonpkg/src/pyconnection.cpp\n@@ -5,7 +5,7 @@\n #include \"duckdb_python/pandas_scan.hpp\"\n #include \"duckdb_python/map.hpp\"\n \n-#include \"duckdb/common/arrow.hpp\"\n+#include \"duckdb/common/arrow/arrow.hpp\"\n #include \"duckdb_python/arrow_array_stream.hpp\"\n #include \"duckdb/parser/parsed_data/create_table_function_info.hpp\"\n #include \"duckdb/main/client_context.hpp\"\ndiff --git a/tools/pythonpkg/src/pyrelation.cpp b/tools/pythonpkg/src/pyrelation.cpp\nindex 65d48ffaa1b2..53e9a1f01128 100644\n--- a/tools/pythonpkg/src/pyrelation.cpp\n+++ b/tools/pythonpkg/src/pyrelation.cpp\n@@ -207,7 +207,7 @@ unique_ptr<DuckDBPyRelation> DuckDBPyRelation::FromArrow(py::object &arrow_objec\n unique_ptr<DuckDBPyRelation> DuckDBPyRelation::Project(const string &expr) {\n \tauto projected_relation = make_unique<DuckDBPyRelation>(rel->Project(expr));\n \tprojected_relation->rel->extra_dependencies = this->rel->extra_dependencies;\n-\treturn move(projected_relation);\n+\treturn projected_relation;\n }\n \n unique_ptr<DuckDBPyRelation> DuckDBPyRelation::ProjectDf(const data_frame &df, const string &expr,\ndiff --git a/tools/pythonpkg/src/pyresult.cpp b/tools/pythonpkg/src/pyresult.cpp\nindex 795cd135f0e6..da2092884421 100644\n--- a/tools/pythonpkg/src/pyresult.cpp\n+++ b/tools/pythonpkg/src/pyresult.cpp\n@@ -3,9 +3,10 @@\n #include \"duckdb_python/pyresult.hpp\"\n \n #include \"datetime.h\" // from Python\n-#include \"duckdb/common/arrow.hpp\"\n-#include \"duckdb/common/arrow_wrapper.hpp\"\n-#include \"duckdb/common/result_arrow_wrapper.hpp\"\n+#include \"duckdb/common/arrow/arrow.hpp\"\n+#include \"duckdb/common/arrow/arrow_converter.hpp\"\n+#include \"duckdb/common/arrow/arrow_wrapper.hpp\"\n+#include \"duckdb/common/arrow/result_arrow_wrapper.hpp\"\n #include \"duckdb/common/types/date.hpp\"\n #include \"duckdb/common/types/hugeint.hpp\"\n #include \"duckdb/common/types/time.hpp\"\n@@ -256,17 +257,17 @@ py::dict DuckDBPyResult::FetchNumpyInternal(bool stream, idx_t vectors_per_chunk\n \tif (result->type == QueryResultType::MATERIALIZED_RESULT) {\n \t\t// materialized query result: we know exactly how much space we need\n \t\tauto &materialized = (MaterializedQueryResult &)*result;\n-\t\tinitial_capacity = materialized.collection.Count();\n+\t\tinitial_capacity = materialized.RowCount();\n \t}\n \n \tNumpyResultConversion conversion(result->types, initial_capacity);\n \tif (result->type == QueryResultType::MATERIALIZED_RESULT) {\n \t\tauto &materialized = (MaterializedQueryResult &)*result;\n-\t\tfor (auto &chunk : materialized.collection.Chunks()) {\n-\t\t\tconversion.Append(*chunk);\n+\t\tfor (auto &chunk : materialized.Collection().Chunks()) {\n+\t\t\tconversion.Append(chunk);\n \t\t}\n \t\tInsertCategory(materialized, categories);\n-\t\tmaterialized.collection.Reset();\n+\t\tmaterialized.Collection().Reset();\n \t} else {\n \t\tD_ASSERT(result->type == QueryResultType::STREAM_RESULT);\n \t\tif (!stream) {\n@@ -340,23 +341,22 @@ data_frame DuckDBPyResult::FetchDFChunk(idx_t num_of_vectors) {\n \treturn FrameFromNumpy(FetchNumpyInternal(true, num_of_vectors));\n }\n \n-void TransformDuckToArrowChunk(ArrowSchema &arrow_schema, DataChunk &duck_chunk, py::list &batches) {\n+void TransformDuckToArrowChunk(ArrowSchema &arrow_schema, ArrowArray &data, py::list &batches) {\n \tauto pyarrow_lib_module = py::module::import(\"pyarrow\").attr(\"lib\");\n \tauto batch_import_func = pyarrow_lib_module.attr(\"RecordBatch\").attr(\"_import_from_c\");\n-\tArrowArray data;\n-\tduck_chunk.ToArrowArray(&data);\n \tbatches.append(batch_import_func((uint64_t)&data, (uint64_t)&arrow_schema));\n }\n \n bool DuckDBPyResult::FetchArrowChunk(QueryResult *result, py::list &batches, idx_t chunk_size) {\n-\tauto data_chunk = ArrowUtil::FetchChunk(result, chunk_size);\n-\tif (!data_chunk || data_chunk->size() == 0) {\n+\tArrowArray data;\n+\tauto count = ArrowUtil::FetchChunk(result, chunk_size, &data);\n+\tif (count == 0) {\n \t\treturn false;\n \t}\n \tArrowSchema arrow_schema;\n \ttimezone_config = QueryResult::GetConfigTimezone(*result);\n-\tQueryResult::ToArrowSchema(&arrow_schema, result->types, result->names, timezone_config);\n-\tTransformDuckToArrowChunk(arrow_schema, *data_chunk, batches);\n+\tArrowConverter::ToArrowSchema(&arrow_schema, result->types, result->names, timezone_config);\n+\tTransformDuckToArrowChunk(arrow_schema, data, batches);\n \treturn true;\n }\n \n@@ -368,9 +368,6 @@ py::object DuckDBPyResult::FetchAllArrowChunks(idx_t chunk_size) {\n \n \tpy::list batches;\n \n-\tif (result->type == QueryResultType::STREAM_RESULT) {\n-\t\tresult = ((StreamQueryResult *)result.get())->Materialize();\n-\t}\n \twhile (FetchArrowChunk(result.get(), batches, chunk_size)) {\n \t}\n \treturn std::move(batches);\n@@ -389,7 +386,8 @@ py::object DuckDBPyResult::FetchArrowTable(idx_t chunk_size) {\n \tArrowSchema schema;\n \n \ttimezone_config = QueryResult::GetConfigTimezone(*result);\n-\tQueryResult::ToArrowSchema(&schema, result->types, result->names, timezone_config);\n+\tArrowConverter::ToArrowSchema(&schema, result->types, result->names, timezone_config);\n+\n \tauto schema_obj = schema_import_func((uint64_t)&schema);\n \n \tpy::list batches = FetchAllArrowChunks(chunk_size);\ndiff --git a/tools/rpkg/R/dbDisconnect__duckdb_connection.R b/tools/rpkg/R/dbDisconnect__duckdb_connection.R\nindex 1d2c74ff7121..61678d5864a5 100644\n--- a/tools/rpkg/R/dbDisconnect__duckdb_connection.R\n+++ b/tools/rpkg/R/dbDisconnect__duckdb_connection.R\n@@ -9,6 +9,7 @@\n dbDisconnect__duckdb_connection <- function(conn, ..., shutdown = FALSE) {\n   if (!dbIsValid(conn)) {\n     warning(\"Connection already closed.\", call. = FALSE)\n+    invisible(FALSE)\n   }\n   rapi_disconnect(conn@conn_ref)\n   if (shutdown) {\ndiff --git a/tools/rpkg/R/register.R b/tools/rpkg/R/register.R\nindex bad61d8b3ec9..b6bdd0513e29 100644\n--- a/tools/rpkg/R/register.R\n+++ b/tools/rpkg/R/register.R\n@@ -50,7 +50,6 @@ duckdb_register <- function(conn, name, df) {\n #' @rdname duckdb_register\n #' @export\n duckdb_unregister <- function(conn, name) {\n-  stopifnot(dbIsValid(conn))\n   rapi_unregister_df(conn@conn_ref, enc2utf8(as.character(name)))\n   invisible(TRUE)\n }\ndiff --git a/tools/rpkg/src/altrep.cpp b/tools/rpkg/src/altrep.cpp\nindex a79fbfc01ef7..5c12b4ceb308 100644\n--- a/tools/rpkg/src/altrep.cpp\n+++ b/tools/rpkg/src/altrep.cpp\n@@ -45,23 +45,17 @@ void *AltrepString::Dataptr(SEXP x, Rboolean writeable) {\n \tauto *wrapper = duckdb_altrep_wrapper(x);\n \tif (R_altrep_data2(x) == R_NilValue) {\n \t\tR_set_altrep_data2(x, NEW_STRING(wrapper->length));\n-\t\tidx_t dest_offset = 0;\n-\t\tfor (auto &vec : wrapper->vectors) {\n-\t\t\tauto src_ptr = FlatVector::GetData<string_t>(vec);\n-\t\t\tauto &mask = FlatVector::Validity(vec);\n-\t\t\tfor (size_t row_idx = 0; row_idx < MinValue<idx_t>(STANDARD_VECTOR_SIZE, wrapper->length - dest_offset);\n-\t\t\t     row_idx++) {\n-\t\t\t\tif (!mask.RowIsValid(row_idx)) {\n-\t\t\t\t\tSET_STRING_ELT(R_altrep_data2(x), dest_offset + row_idx, NA_STRING);\n-\t\t\t\t} else {\n-\t\t\t\t\tSET_STRING_ELT(\n-\t\t\t\t\t    R_altrep_data2(x), dest_offset + row_idx,\n-\t\t\t\t\t    Rf_mkCharLenCE(src_ptr[row_idx].GetDataUnsafe(), src_ptr[row_idx].GetSize(), CE_UTF8));\n-\t\t\t\t}\n+\t\tfor (idx_t row_idx = 0; row_idx < wrapper->length; row_idx++) {\n+\t\t\tif (!wrapper->mask_data[row_idx]) {\n+\t\t\t\tSET_STRING_ELT(R_altrep_data2(x), row_idx, NA_STRING);\n+\t\t\t} else {\n+\t\t\t\tSET_STRING_ELT(R_altrep_data2(x), row_idx,\n+\t\t\t\t               Rf_mkCharLenCE(wrapper->string_data[row_idx].GetDataUnsafe(),\n+\t\t\t\t                              wrapper->string_data[row_idx].GetSize(), CE_UTF8));\n \t\t\t}\n-\t\t\tdest_offset += STANDARD_VECTOR_SIZE;\n \t\t}\n-\t\twrapper->vectors.clear();\n+\t\twrapper->string_data.reset();\n+\t\twrapper->mask_data.reset();\n \t}\n \treturn CHARACTER_POINTER(R_altrep_data2(x));\n }\n@@ -75,14 +69,10 @@ SEXP AltrepString::Elt(SEXP x, R_xlen_t i) {\n \tif (R_altrep_data2(x) != R_NilValue) {\n \t\treturn STRING_ELT(R_altrep_data2(x), i);\n \t}\n-\tauto &vec = wrapper->vectors[i / STANDARD_VECTOR_SIZE];\n-\tauto src_ptr = FlatVector::GetData<string_t>(vec);\n-\tauto &mask = FlatVector::Validity(vec);\n-\tauto vec_idx = i % STANDARD_VECTOR_SIZE;\n-\tif (!mask.RowIsValid(vec_idx)) {\n+\tif (!wrapper->mask_data[i]) {\n \t\treturn NA_STRING;\n \t}\n-\treturn Rf_mkCharLenCE(src_ptr[vec_idx].GetDataUnsafe(), src_ptr[vec_idx].GetSize(), CE_UTF8);\n+\treturn Rf_mkCharLenCE(wrapper->string_data[i].GetDataUnsafe(), wrapper->string_data[i].GetSize(), CE_UTF8);\n }\n \n void AltrepString::SetElt(SEXP x, R_xlen_t i, SEXP val) {\ndiff --git a/tools/rpkg/src/connection.cpp b/tools/rpkg/src/connection.cpp\nindex f373256e8cfb..95150ddf8b0c 100644\n--- a/tools/rpkg/src/connection.cpp\n+++ b/tools/rpkg/src/connection.cpp\n@@ -8,7 +8,7 @@ void duckdb::ConnDeleter(ConnWrapper *conn) {\n }\n \n [[cpp11::register]] duckdb::conn_eptr_t rapi_connect(duckdb::db_eptr_t db) {\n-\tif (!db || !db->db) {\n+\tif (!db || !db.get() || !db->db) {\n \t\tcpp11::stop(\"rapi_connect: Invalid database reference\");\n \t}\n \ndiff --git a/tools/rpkg/src/include/typesr.hpp b/tools/rpkg/src/include/typesr.hpp\nindex b7252afd4011..340af7d0f8e9 100644\n--- a/tools/rpkg/src/include/typesr.hpp\n+++ b/tools/rpkg/src/include/typesr.hpp\n@@ -3,7 +3,9 @@\n namespace duckdb {\n \n struct DuckDBAltrepStringWrapper {\n-\tvector<Vector> vectors;\n+\tstd::unique_ptr<string_t[]> string_data;\n+\tstd::unique_ptr<bool[]> mask_data;\n+\tStringHeap heap;\n \tidx_t length;\n };\n \ndiff --git a/tools/rpkg/src/register.cpp b/tools/rpkg/src/register.cpp\nindex e8c7b9e4a456..9a7644011aac 100644\n--- a/tools/rpkg/src/register.cpp\n+++ b/tools/rpkg/src/register.cpp\n@@ -1,18 +1,19 @@\n #include \"rapi.hpp\"\n #include \"typesr.hpp\"\n \n-#include \"duckdb/common/arrow_wrapper.hpp\"\n+#include \"duckdb/common/arrow/arrow_wrapper.hpp\"\n #include \"duckdb/planner/table_filter.hpp\"\n #include \"duckdb/planner/filter/constant_filter.hpp\"\n #include \"duckdb/planner/filter/conjunction_filter.hpp\"\n #include \"duckdb/parser/expression/constant_expression.hpp\"\n #include \"duckdb/parser/expression/function_expression.hpp\"\n+#include \"duckdb/function/table/arrow.hpp\"\n \n using namespace duckdb;\n \n [[cpp11::register]] void rapi_register_df(duckdb::conn_eptr_t conn, std::string name, cpp11::data_frame value,\n                                           bool integer64) {\n-\tif (!conn || !conn->conn) {\n+\tif (!conn || !conn.get() || !conn->conn) {\n \t\tcpp11::stop(\"rapi_register_df: Invalid connection\");\n \t}\n \tif (name.empty()) {\n@@ -33,8 +34,8 @@ using namespace duckdb;\n }\n \n [[cpp11::register]] void rapi_unregister_df(duckdb::conn_eptr_t conn, std::string name) {\n-\tif (!conn || !conn->conn) {\n-\t\tcpp11::stop(\"rapi_unregister_df: Invalid connection\");\n+\tif (!conn || !conn.get() || !conn->conn) {\n+\t\treturn;\n \t}\n \tstatic_cast<cpp11::sexp>(conn).attr(\"_registered_df_\" + name) = R_NilValue;\n \tauto res = conn->conn->Query(\"DROP VIEW IF EXISTS \\\"\" + name + \"\\\"\");\n@@ -48,10 +49,7 @@ class RArrowTabularStreamFactory {\n \tRArrowTabularStreamFactory(SEXP export_fun_p, SEXP arrow_scannable_p, ClientConfig &config)\n \t    : arrow_scannable(arrow_scannable_p), export_fun(export_fun_p), config(config) {};\n \n-\tstatic unique_ptr<ArrowArrayStreamWrapper>\n-\tProduce(uintptr_t factory_p, std::pair<std::unordered_map<idx_t, string>, std::vector<string>> &project_columns,\n-\t        TableFilterSet *filters) {\n-\n+\tstatic unique_ptr<ArrowArrayStreamWrapper> Produce(uintptr_t factory_p, ArrowStreamParameters &parameters) {\n \t\tRProtector r;\n \t\tauto res = make_unique<ArrowArrayStreamWrapper>();\n \t\tauto factory = (RArrowTabularStreamFactory *)factory_p;\n@@ -60,15 +58,18 @@ class RArrowTabularStreamFactory {\n \n \t\tcpp11::function export_fun = VECTOR_ELT(factory->export_fun, 0);\n \n-\t\tif (project_columns.second.empty()) {\n+\t\tauto &column_list = parameters.projected_columns.columns;\n+\t\tauto filters = parameters.filters;\n+\t\tauto &projection_map = parameters.projected_columns.projection_map;\n+\t\tif (column_list.empty()) {\n \t\t\texport_fun(factory->arrow_scannable, stream_ptr_sexp);\n \t\t} else {\n-\t\t\tauto projection_sexp = r.Protect(StringsToSexp(project_columns.second));\n+\t\t\tauto projection_sexp = r.Protect(StringsToSexp(column_list));\n \t\t\tSEXP filters_sexp = r.Protect(Rf_ScalarLogical(true));\n \t\t\tif (filters && !filters->filters.empty()) {\n-\t\t\t\tauto timezone_config = ClientConfig::ExtractTimezoneFromConfig(factory->config);\n+\t\t\t\tauto timezone_config = factory->config.ExtractTimezone();\n \t\t\t\tfilters_sexp =\n-\t\t\t\t    r.Protect(TransformFilter(*filters, project_columns.first, factory->export_fun, timezone_config));\n+\t\t\t\t    r.Protect(TransformFilter(*filters, projection_map, factory->export_fun, timezone_config));\n \t\t\t}\n \t\t\texport_fun(factory->arrow_scannable, stream_ptr_sexp, projection_sexp, filters_sexp);\n \t\t}\n@@ -230,7 +231,7 @@ unique_ptr<TableFunctionRef> duckdb::ArrowScanReplacement(ClientContext &context\n \n [[cpp11::register]] void rapi_register_arrow(duckdb::conn_eptr_t conn, std::string name, cpp11::list export_funs,\n                                              cpp11::sexp valuesexp) {\n-\tif (!conn || !conn->conn) {\n+\tif (!conn || !conn.get() || !conn->conn) {\n \t\tcpp11::stop(\"rapi_register_arrow: Invalid connection\");\n \t}\n \tif (name.empty()) {\n@@ -252,10 +253,9 @@ unique_ptr<TableFunctionRef> duckdb::ArrowScanReplacement(ClientContext &context\n }\n \n [[cpp11::register]] void rapi_unregister_arrow(duckdb::conn_eptr_t conn, std::string name) {\n-\tif (!conn || !conn->conn) {\n-\t\tcpp11::stop(\"rapi_unregister_arrow: Invalid connection\");\n+\tif (!conn || !conn.get() || !conn->conn) {\n+\t\treturn; // if the connection is already dead there is probably no point in cleaning this\n \t}\n-\n \t{\n \t\tlock_guard<mutex> arrow_scans_lock(conn->db_eptr->lock);\n \t\tauto &arrow_scans = conn->db_eptr->arrow_scans;\ndiff --git a/tools/rpkg/src/relational.cpp b/tools/rpkg/src/relational.cpp\nindex ddf4e2ed7f24..b5c6b6adf93c 100644\n--- a/tools/rpkg/src/relational.cpp\n+++ b/tools/rpkg/src/relational.cpp\n@@ -76,7 +76,7 @@ external_pointer<T> make_external(const string &rclass, Args &&...args) {\n // DuckDB Relations\n \n [[cpp11::register]] SEXP rapi_rel_from_df(duckdb::conn_eptr_t con, data_frame df) {\n-\tif (!con->conn) {\n+\tif (!con || !con.get() || !con->conn) {\n \t\tstop(\"rel_from_df: Invalid connection\");\n \t}\n \tif (df.size() == 0) {\n@@ -207,7 +207,7 @@ static SEXP result_to_df(unique_ptr<QueryResult> res) {\n \n \twritable::integers row_names;\n \trow_names.push_back(NA_INTEGER);\n-\trow_names.push_back(-mat_res->collection.Count());\n+\trow_names.push_back(-mat_res->RowCount());\n \n \t// TODO this thing we can probably statically cache\n \twritable::strings classes;\ndiff --git a/tools/rpkg/src/scan.cpp b/tools/rpkg/src/scan.cpp\nindex b9fa51420b50..33b5b6266a9f 100644\n--- a/tools/rpkg/src/scan.cpp\n+++ b/tools/rpkg/src/scan.cpp\n@@ -240,7 +240,7 @@ static void DataFrameScanFunc(ClientContext &context, TableFunctionInput &data,\n \tauto sexp_offset = operator_data.offset + operator_data.position;\n \tD_ASSERT(sexp_offset + this_count <= bind_data.row_count);\n \n-\tfor (R_xlen_t out_col_idx = 0; out_col_idx < output.ColumnCount(); out_col_idx++) {\n+\tfor (R_xlen_t out_col_idx = 0; out_col_idx < R_xlen_t(output.ColumnCount()); out_col_idx++) {\n \t\tauto &v = output.data[out_col_idx];\n \t\tauto src_df_col_idx = operator_data.column_ids[out_col_idx];\n \ndiff --git a/tools/rpkg/src/statement.cpp b/tools/rpkg/src/statement.cpp\nindex b20b23ab8bb7..7af80bef46c2 100644\n--- a/tools/rpkg/src/statement.cpp\n+++ b/tools/rpkg/src/statement.cpp\n@@ -4,10 +4,11 @@\n \n #include <R_ext/Utils.h>\n \n-#include \"duckdb/common/arrow.hpp\"\n+#include \"duckdb/common/arrow/arrow.hpp\"\n+#include \"duckdb/common/arrow/arrow_converter.hpp\"\n #include \"duckdb/common/types/timestamp.hpp\"\n-#include \"duckdb/common/arrow_wrapper.hpp\"\n-#include \"duckdb/common/result_arrow_wrapper.hpp\"\n+#include \"duckdb/common/arrow/arrow_wrapper.hpp\"\n+#include \"duckdb/common/arrow/result_arrow_wrapper.hpp\"\n #include \"duckdb/main/stream_query_result.hpp\"\n \n #include \"duckdb/parser/statement/relation_statement.hpp\"\n@@ -34,8 +35,7 @@ static void VectorToR(Vector &src_vec, size_t count, void *dest, uint64_t dest_o\n }\n \n [[cpp11::register]] SEXP rapi_get_substrait(duckdb::conn_eptr_t conn, std::string query) {\n-\n-\tif (!conn || !conn->conn) {\n+\tif (!conn || !conn.get() || !conn->conn) {\n \t\tcpp11::stop(\"rapi_get_substrait: Invalid connection\");\n \t}\n \n@@ -54,8 +54,7 @@ static void VectorToR(Vector &src_vec, size_t count, void *dest, uint64_t dest_o\n }\n \n [[cpp11::register]] SEXP rapi_get_substrait_json(duckdb::conn_eptr_t conn, std::string query) {\n-\n-\tif (!conn || !conn->conn) {\n+\tif (!conn || !conn.get() || !conn->conn) {\n \t\tcpp11::stop(\"rapi_get_substrait_json: Invalid connection\");\n \t}\n \n@@ -68,7 +67,6 @@ static void VectorToR(Vector &src_vec, size_t count, void *dest, uint64_t dest_o\n }\n \n static cpp11::list construct_retlist(unique_ptr<PreparedStatement> stmt, const string &query, idx_t n_param) {\n-\n \tcpp11::writable::list retlist;\n \tretlist.reserve(6);\n \tretlist.push_back({\"str\"_nm = query});\n@@ -97,7 +95,7 @@ static cpp11::list construct_retlist(unique_ptr<PreparedStatement> stmt, const s\n }\n \n [[cpp11::register]] cpp11::list rapi_prepare_substrait(duckdb::conn_eptr_t conn, cpp11::sexp query) {\n-\tif (!conn || !conn->conn) {\n+\tif (!conn || !conn.get() || !conn->conn) {\n \t\tcpp11::stop(\"rapi_prepare_substrait: Invalid connection\");\n \t}\n \n@@ -118,7 +116,7 @@ static cpp11::list construct_retlist(unique_ptr<PreparedStatement> stmt, const s\n }\n \n [[cpp11::register]] cpp11::list rapi_prepare(duckdb::conn_eptr_t conn, std::string query) {\n-\tif (!conn || !conn->conn) {\n+\tif (!conn || !conn.get() || !conn->conn) {\n \t\tcpp11::stop(\"rapi_prepare: Invalid connection\");\n \t}\n \n@@ -144,7 +142,7 @@ static cpp11::list construct_retlist(unique_ptr<PreparedStatement> stmt, const s\n }\n \n [[cpp11::register]] cpp11::list rapi_bind(duckdb::stmt_eptr_t stmt, cpp11::list params, bool arrow, bool integer64) {\n-\tif (!stmt || !stmt->stmt) {\n+\tif (!stmt || !stmt.get() || !stmt->stmt) {\n \t\tcpp11::stop(\"rapi_bind: Invalid statement\");\n \t}\n \n@@ -155,7 +153,7 @@ static cpp11::list construct_retlist(unique_ptr<PreparedStatement> stmt, const s\n \t\tcpp11::stop(\"rapi_bind: dbBind called but query takes no parameters\");\n \t}\n \n-\tif (params.size() != stmt->stmt->n_param) {\n+\tif (params.size() != R_xlen_t(stmt->stmt->n_param)) {\n \t\tcpp11::stop(\"rapi_bind: Bind parameters need to be a list of length %i\", stmt->stmt->n_param);\n \t}\n \n@@ -243,6 +241,8 @@ static SEXP allocate(const LogicalType &type, RProtector &r_varvalue, idx_t nrow\n \tcase LogicalTypeId::VARCHAR: {\n \t\tauto wrapper = new DuckDBAltrepStringWrapper();\n \t\twrapper->length = nrows;\n+\t\twrapper->string_data = std::unique_ptr<string_t[]>(new string_t[nrows]);\n+\t\twrapper->mask_data = std::unique_ptr<bool[]>(new bool[nrows]);\n \n \t\tcpp11::external_pointer<DuckDBAltrepStringWrapper> ptr(wrapper);\n \t\tvarvalue = r_varvalue.Protect(R_new_altrep(AltrepString::rclass, ptr, R_NilValue));\n@@ -449,8 +449,17 @@ static void transform(Vector &src_vec, SEXP &dest, idx_t dest_offset, idx_t n, b\n \t\tbreak;\n \tcase LogicalTypeId::VARCHAR: {\n \t\tauto wrapper = (DuckDBAltrepStringWrapper *)R_ExternalPtrAddr(R_altrep_data1(dest));\n-\t\twrapper->vectors.emplace_back(LogicalType::VARCHAR, nullptr);\n-\t\twrapper->vectors.back().Reference(src_vec);\n+\t\tauto src_data = FlatVector::GetData<string_t>(src_vec);\n+\t\tauto &mask = FlatVector::Validity(src_vec);\n+\t\tfor (size_t row_idx = 0; row_idx < n; row_idx++) {\n+\t\t\tauto valid = mask.RowIsValid(row_idx);\n+\t\t\tauto dest_idx = dest_offset + row_idx;\n+\t\t\twrapper->mask_data[dest_idx] = valid;\n+\t\t\tif (valid) {\n+\t\t\t\twrapper->string_data[dest_idx] =\n+\t\t\t\t    src_data[row_idx].IsInlined() ? src_data[row_idx] : wrapper->heap.AddString(src_data[row_idx]);\n+\t\t\t}\n+\t\t}\n \t\tbreak;\n \t}\n \tcase LogicalTypeId::LIST: {\n@@ -574,7 +583,7 @@ SEXP duckdb::duckdb_execute_R_impl(MaterializedQueryResult *result, bool integer\n \t\treturn Rf_ScalarReal(0); // no need for protection because no allocation can happen afterwards\n \t}\n \n-\tuint64_t nrows = result->collection.Count();\n+\tuint64_t nrows = result->RowCount();\n \n \t// Note we cannot use cpp11's data frame here as it tries to calculate the number of rows itself,\n \t// but gives the wrong answer if the first column is another data frame. So we set the necessary\n@@ -595,21 +604,14 @@ SEXP duckdb::duckdb_execute_R_impl(MaterializedQueryResult *result, bool integer\n \n \t// step 3: set values from chunks\n \tuint64_t dest_offset = 0;\n-\tidx_t chunk_idx = 0;\n-\twhile (true) {\n-\t\tauto chunk = result->Fetch();\n-\t\tif (!chunk || chunk->size() == 0) {\n-\t\t\tbreak;\n-\t\t}\n-\n-\t\tD_ASSERT(chunk->ColumnCount() == ncols);\n-\t\tD_ASSERT(chunk->ColumnCount() == (idx_t)Rf_length(data_frame));\n-\t\tfor (size_t col_idx = 0; col_idx < chunk->ColumnCount(); col_idx++) {\n+\tfor (auto &chunk : result->Collection().Chunks()) {\n+\t\tD_ASSERT(chunk.ColumnCount() == ncols);\n+\t\tD_ASSERT(chunk.ColumnCount() == (idx_t)Rf_length(data_frame));\n+\t\tfor (size_t col_idx = 0; col_idx < chunk.ColumnCount(); col_idx++) {\n \t\t\tSEXP dest = VECTOR_ELT(data_frame, col_idx);\n-\t\t\ttransform(chunk->data[col_idx], dest, dest_offset, chunk->size(), integer64);\n+\t\t\ttransform(chunk.data[col_idx], dest, dest_offset, chunk.size(), integer64);\n \t\t}\n-\t\tdest_offset += chunk->size();\n-\t\tchunk_idx++;\n+\t\tdest_offset += chunk.size();\n \t}\n \n \tD_ASSERT(dest_offset == nrows);\n@@ -645,14 +647,12 @@ struct AppendableRList {\n \n bool FetchArrowChunk(QueryResult *result, AppendableRList &batches_list, ArrowArray &arrow_data,\n                      ArrowSchema &arrow_schema, SEXP batch_import_from_c, SEXP arrow_namespace, idx_t chunk_size) {\n-\n-\tauto data_chunk = ArrowUtil::FetchChunk(result, chunk_size);\n-\tif (!data_chunk || data_chunk->size() == 0) {\n+\tauto count = ArrowUtil::FetchChunk(result, chunk_size, &arrow_data);\n+\tif (count == 0) {\n \t\treturn false;\n \t}\n \tstring timezone_config = QueryResult::GetConfigTimezone(*result);\n-\tQueryResult::ToArrowSchema(&arrow_schema, result->types, result->names, timezone_config);\n-\tdata_chunk->ToArrowArray(&arrow_data);\n+\tArrowConverter::ToArrowSchema(&arrow_schema, result->types, result->names, timezone_config);\n \tbatches_list.PrepAppend();\n \tbatches_list.Append(cpp11::safe[Rf_eval](batch_import_from_c, arrow_namespace));\n \treturn true;\n@@ -660,9 +660,6 @@ bool FetchArrowChunk(QueryResult *result, AppendableRList &batches_list, ArrowAr\n \n // Turn a DuckDB result set into an Arrow Table\n [[cpp11::register]] SEXP rapi_execute_arrow(duckdb::rqry_eptr_t qry_res, int chunk_size) {\n-\tif (qry_res->result->type == QueryResultType::STREAM_RESULT) {\n-\t\tqry_res->result = ((StreamQueryResult *)qry_res->result.get())->Materialize();\n-\t}\n \tauto result = qry_res->result.get();\n \t// somewhat dark magic below\n \tcpp11::function getNamespace = RStrings::get().getNamespace_sym;\n@@ -686,7 +683,7 @@ bool FetchArrowChunk(QueryResult *result, AppendableRList &batches_list, ArrowAr\n \n \tSET_LENGTH(batches_list.the_list, batches_list.size);\n \tstring timezone_config = QueryResult::GetConfigTimezone(*result);\n-\tQueryResult::ToArrowSchema(&arrow_schema, result->types, result->names, timezone_config);\n+\tArrowConverter::ToArrowSchema(&arrow_schema, result->types, result->names, timezone_config);\n \tcpp11::sexp schema_arrow_obj(cpp11::safe[Rf_eval](schema_import_from_c, arrow_namespace));\n \n \t// create arrow::Table\n@@ -709,7 +706,7 @@ bool FetchArrowChunk(QueryResult *result, AppendableRList &batches_list, ArrowAr\n }\n \n [[cpp11::register]] SEXP rapi_execute(duckdb::stmt_eptr_t stmt, bool arrow, bool integer64) {\n-\tif (!stmt || !stmt->stmt) {\n+\tif (!stmt || !stmt.get() || !stmt->stmt) {\n \t\tcpp11::stop(\"rapi_execute: Invalid statement\");\n \t}\n \n",
  "test_patch": "diff --git a/scripts/regression_test_python.py b/scripts/regression_test_python.py\nindex 0089e9681f2c..bc4301d7eb95 100644\n--- a/scripts/regression_test_python.py\n+++ b/scripts/regression_test_python.py\n@@ -50,6 +50,16 @@ def open_connection():\n         con.execute(f\"SET threads={threads}\")\n     return con\n \n+\n+def write_result(benchmark_name, nrun, t):\n+    bench_result = f\"{benchmark_name}\\t{nrun}\\t{t}\"\n+\n+    if out_file is not None:\n+        f.write(bench_result)\n+        f.write('\\n')\n+    else:\n+        print(bench_result)\n+\n def benchmark_queries(benchmark_name, con, queries):\n     if verbose:\n         print(benchmark_name)\n@@ -67,14 +77,28 @@ def benchmark_queries(benchmark_name, con, queries):\n             if verbose:\n                 padding = \" \" * len(str(nruns))\n                 print(f\"T{padding}: {t}s\")\n+        write_result(benchmark_name, nrun, t)\n \n-        bench_result = f\"{benchmark_name}\\t{nrun}\\t{t}\"\n-\n-        if out_file is not None:\n-            f.write(bench_result)\n-            f.write('\\n')\n-        else:\n-            print(bench_result)\n+def run_dataload(con, type):\n+    benchmark_name = type + \"_load_lineitem\"\n+    if verbose:\n+        print(benchmark_name)\n+        print(type)\n+    q = 'SELECT * FROM lineitem'\n+    for nrun in range(nruns):\n+        t = 0.0\n+        start = time.time()\n+        if type == 'pandas':\n+            res = con.execute(q).df()\n+        elif type == 'arrow':\n+            res = con.execute(q).arrow()\n+        end = time.time()\n+        t = float(end - start)\n+        del res\n+        if verbose:\n+            padding = \" \" * len(str(nruns))\n+            print(f\"T{padding}: {t}s\")\n+        write_result(benchmark_name, nrun, t)\n \n def run_tpch(con, prefix):\n     benchmark_name = f\"{prefix}tpch\"\n@@ -96,6 +120,7 @@ def run_tpch(con, prefix):\n for table in tables:\n     df_con.register(table, data_frames[table])\n \n+run_dataload(main_con, \"pandas\")\n run_tpch(df_con, \"pandas_\")\n \n # arrow scans\n@@ -107,4 +132,5 @@ def run_tpch(con, prefix):\n for table in tables:\n     arrow_con.register(table, arrow_tables[table])\n \n+run_dataload(main_con, \"arrow\")\n run_tpch(arrow_con, \"arrow_\")\ndiff --git a/test/CMakeLists.txt b/test/CMakeLists.txt\nindex 09000ed163af..ad8f5c96f751 100644\n--- a/test/CMakeLists.txt\n+++ b/test/CMakeLists.txt\n@@ -5,6 +5,7 @@ include_directories(include)\n \n add_subdirectory(api)\n add_subdirectory(appender)\n+add_subdirectory(arrow_roundtrip)\n add_subdirectory(common)\n add_subdirectory(extension)\n add_subdirectory(helpers)\ndiff --git a/test/api/test_api.cpp b/test/api/test_api.cpp\nindex fc6970f42c9b..20769b024eb7 100644\n--- a/test/api/test_api.cpp\n+++ b/test/api/test_api.cpp\n@@ -211,7 +211,6 @@ TEST_CASE(\"Test streaming API errors\", \"[api]\") {\n \tunique_ptr<QueryResult> result, result2;\n \tDuckDB db(nullptr);\n \tConnection con(db);\n-\tcon.EnableQueryVerification();\n \n \t// multiple streaming result\n \tresult = con.SendQuery(\"SELECT 42;\");\n@@ -251,6 +250,7 @@ TEST_CASE(\"Test streaming API errors\", \"[api]\") {\n \tresult = con.SendQuery(\n \t    \"SELECT x::INT FROM (SELECT x::VARCHAR x FROM range(10) tbl(x) UNION ALL SELECT 'hello' x) tbl(x);\");\n \tREQUIRE(!result->ToString().empty());\n+\tREQUIRE(result->type == QueryResultType::STREAM_RESULT);\n \tresult = ((StreamQueryResult &)*result).Materialize();\n \tREQUIRE_FAIL(result);\n \n@@ -264,6 +264,7 @@ TEST_CASE(\"Test streaming API errors\", \"[api]\") {\n \t\t}\n \t}\n \tREQUIRE(!result->ToString().empty());\n+\tREQUIRE(result->type == QueryResultType::STREAM_RESULT);\n \tresult = ((StreamQueryResult &)*result).Materialize();\n \tREQUIRE_FAIL(result);\n }\ndiff --git a/test/arrow/abi_test.cpp b/test/arrow/abi_test.cpp\nindex 98ab29990490..3ab9522c97f7 100644\n--- a/test/arrow/abi_test.cpp\n+++ b/test/arrow/abi_test.cpp\n@@ -12,6 +12,7 @@\n #include \"duckdb/common/vector.hpp\"\n #include \"duckdb/main/connection.hpp\"\n #include \"duckdb/main/database.hpp\"\n+#include \"duckdb/function/table/arrow.hpp\"\n #include <parquet/arrow/reader.h>\n #include \"arrow/io/file.h\"\n #include <arrow/type_traits.h>\ndiff --git a/test/arrow/arrow_test_factory.hpp b/test/arrow/arrow_test_factory.hpp\nindex 520d9c100b98..5ad113b4aeb0 100644\n--- a/test/arrow/arrow_test_factory.hpp\n+++ b/test/arrow/arrow_test_factory.hpp\n@@ -1,6 +1,7 @@\n #pragma once\n #include \"arrow/record_batch.h\"\n-#include \"duckdb/common/arrow_wrapper.hpp\"\n+#include \"duckdb/function/table/arrow.hpp\"\n+#include \"duckdb/common/arrow/arrow_wrapper.hpp\"\n #include \"arrow/array.h\"\n #include \"catch.hpp\"\n \n@@ -18,7 +19,8 @@ struct SimpleFactory {\n \t    : batches(std::move(batches)), schema(std::move(schema)) {\n \t}\n \n-\tstatic std::unique_ptr<duckdb::ArrowArrayStreamWrapper> CreateStream(uintptr_t this_ptr) {\n+\tstatic std::unique_ptr<duckdb::ArrowArrayStreamWrapper> CreateStream(uintptr_t this_ptr,\n+\t                                                                     duckdb::ArrowStreamParameters &parameters) {\n \t\t//! Create a new batch reader\n \t\tauto &factory = *reinterpret_cast<SimpleFactory *>(this_ptr); //! NOLINT\n \t\tREQUIRE_RESULT(auto reader, arrow::RecordBatchReader::Make(factory.batches, factory.schema));\ndiff --git a/test/arrow/fetch_big_chunks_test.cpp b/test/arrow/fetch_big_chunks_test.cpp\nindex bf9915eafc8c..f14ff01da923 100644\n--- a/test/arrow/fetch_big_chunks_test.cpp\n+++ b/test/arrow/fetch_big_chunks_test.cpp\n@@ -23,7 +23,7 @@\n #include \"duckdb/main/query_result.hpp\"\n #include \"duckdb.h\"\n #include <arrow/c/bridge.h>\n-#include \"duckdb/common/result_arrow_wrapper.hpp\"\n+#include \"duckdb/common/arrow/result_arrow_wrapper.hpp\"\n \n TEST_CASE(\"Test Fetch Bigger Than Vector Chunks\", \"[arrow]\") {\n \ndiff --git a/test/arrow/parquet_test.cpp b/test/arrow/parquet_test.cpp\nindex 4ba178228242..da912c0b53ba 100644\n--- a/test/arrow/parquet_test.cpp\n+++ b/test/arrow/parquet_test.cpp\n@@ -23,6 +23,7 @@\n #include \"parquet/exception.h\"\n #include \"duckdb/common/file_system.hpp\"\n #include \"duckdb/main/query_result.hpp\"\n+#include \"duckdb/common/arrow/arrow_converter.hpp\"\n #include \"test_helpers.hpp\"\n \n std::shared_ptr<arrow::Table> ReadParquetFile(const duckdb::string &path) {\n@@ -85,7 +86,7 @@ bool RoundTrip(std::string &path, std::vector<std::string> &skip, duckdb::Connec\n \tArrowSchema abi_arrow_schema;\n \tstd::vector<std::shared_ptr<arrow::RecordBatch>> batches_result;\n \tauto timezone_config = duckdb::QueryResult::GetConfigTimezone(*result);\n-\tduckdb::QueryResult::ToArrowSchema(&abi_arrow_schema, result->types, result->names, timezone_config);\n+\tduckdb::ArrowConverter::ToArrowSchema(&abi_arrow_schema, result->types, result->names, timezone_config);\n \tauto result_schema = arrow::ImportSchema(&abi_arrow_schema);\n \n \twhile (true) {\n@@ -93,8 +94,9 @@ bool RoundTrip(std::string &path, std::vector<std::string> &skip, duckdb::Connec\n \t\tif (!data_chunk || data_chunk->size() == 0) {\n \t\t\tbreak;\n \t\t}\n+\t\tdata_chunk->Verify();\n \t\tArrowArray arrow_array;\n-\t\tdata_chunk->ToArrowArray(&arrow_array);\n+\t\tduckdb::ArrowConverter::ToArrowArray(*data_chunk, &arrow_array);\n \t\tauto batch = arrow::ImportRecordBatch(&arrow_array, result_schema.ValueUnsafe());\n \t\tbatches_result.push_back(batch.MoveValueUnsafe());\n \t}\n@@ -109,7 +111,7 @@ bool RoundTrip(std::string &path, std::vector<std::string> &skip, duckdb::Connec\n \t\t\tbreak;\n \t\t}\n \t\tArrowArray arrow_array;\n-\t\tdata_chunk->ToArrowArray(&arrow_array);\n+\t\tduckdb::ArrowConverter::ToArrowArray(*data_chunk, &arrow_array);\n \t\tauto batch = arrow::ImportRecordBatch(&arrow_array, result_schema.ValueUnsafe());\n \t\tbatches_result.push_back(batch.MoveValueUnsafe());\n \t}\ndiff --git a/test/arrow_roundtrip/CMakeLists.txt b/test/arrow_roundtrip/CMakeLists.txt\nnew file mode 100644\nindex 000000000000..48bf8302c7ae\n--- /dev/null\n+++ b/test/arrow_roundtrip/CMakeLists.txt\n@@ -0,0 +1,4 @@\n+add_library_unity(test_arrow_roundtrip OBJECT arrow_roundtrip.cpp)\n+set(ALL_OBJECT_FILES\n+    ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:test_arrow_roundtrip>\n+    PARENT_SCOPE)\ndiff --git a/test/arrow_roundtrip/arrow_roundtrip.cpp b/test/arrow_roundtrip/arrow_roundtrip.cpp\nnew file mode 100644\nindex 000000000000..610922e26299\n--- /dev/null\n+++ b/test/arrow_roundtrip/arrow_roundtrip.cpp\n@@ -0,0 +1,289 @@\n+#include \"catch.hpp\"\n+#include \"test_helpers.hpp\"\n+#include \"duckdb/common/helper.hpp\"\n+#include \"duckdb/common/types/value.hpp\"\n+#include \"duckdb/common/vector.hpp\"\n+#include \"duckdb/main/connection.hpp\"\n+#include \"duckdb/main/client_config.hpp\"\n+#include \"duckdb/main/database.hpp\"\n+#include \"duckdb/function/table/arrow.hpp\"\n+#include \"duckdb/common/arrow/arrow_appender.hpp\"\n+#include \"duckdb/common/arrow/arrow_converter.hpp\"\n+#include \"duckdb/common/arrow/arrow_wrapper.hpp\"\n+#include \"duckdb/main/extension_helper.hpp\"\n+\n+using namespace duckdb;\n+\n+struct ArrowRoundtripFactory {\n+\tArrowRoundtripFactory(vector<LogicalType> types_p, vector<string> names_p, string tz_p,\n+\t                      unique_ptr<QueryResult> result_p, bool big_result)\n+\t    : types(move(types_p)), names(move(names_p)), tz(move(tz_p)), result(move(result_p)), big_result(big_result) {\n+\t}\n+\n+\tvector<LogicalType> types;\n+\tvector<string> names;\n+\tstring tz;\n+\tunique_ptr<QueryResult> result;\n+\tbool big_result;\n+\n+public:\n+\tstruct ArrowArrayStreamData {\n+\t\tArrowArrayStreamData(ArrowRoundtripFactory &factory) : factory(factory) {\n+\t\t}\n+\n+\t\tArrowRoundtripFactory &factory;\n+\t};\n+\n+\tstatic int ArrowArrayStreamGetSchema(struct ArrowArrayStream *stream, struct ArrowSchema *out) {\n+\t\tif (!stream->private_data) {\n+\t\t\tthrow InternalException(\"No private data!?\");\n+\t\t}\n+\t\tauto &data = *((ArrowArrayStreamData *)stream->private_data);\n+\t\tdata.factory.ToArrowSchema(out);\n+\t\treturn 0;\n+\t}\n+\n+\tstatic int ArrowArrayStreamGetNext(struct ArrowArrayStream *stream, struct ArrowArray *out) {\n+\t\tif (!stream->private_data) {\n+\t\t\tthrow InternalException(\"No private data!?\");\n+\t\t}\n+\t\tauto &data = *((ArrowArrayStreamData *)stream->private_data);\n+\t\tif (!data.factory.big_result) {\n+\t\t\tauto chunk = data.factory.result->Fetch();\n+\t\t\tif (!chunk || chunk->size() == 0) {\n+\t\t\t\treturn 0;\n+\t\t\t}\n+\t\t\tArrowConverter::ToArrowArray(*chunk, out);\n+\t\t} else {\n+\t\t\tArrowAppender appender(data.factory.result->types, STANDARD_VECTOR_SIZE);\n+\t\t\tidx_t count = 0;\n+\t\t\twhile (true) {\n+\t\t\t\tauto chunk = data.factory.result->Fetch();\n+\t\t\t\tif (!chunk || chunk->size() == 0) {\n+\t\t\t\t\tbreak;\n+\t\t\t\t}\n+\t\t\t\tcount += chunk->size();\n+\t\t\t\tappender.Append(*chunk);\n+\t\t\t}\n+\t\t\tif (count > 0) {\n+\t\t\t\t*out = appender.Finalize();\n+\t\t\t}\n+\t\t}\n+\t\treturn 0;\n+\t}\n+\n+\tstatic const char *ArrowArrayStreamGetLastError(struct ArrowArrayStream *stream) {\n+\t\tthrow InternalException(\"Error!?!!\");\n+\t}\n+\n+\tstatic void ArrowArrayStreamRelease(struct ArrowArrayStream *stream) {\n+\t\tif (!stream->private_data) {\n+\t\t\treturn;\n+\t\t}\n+\t\tauto data = (ArrowArrayStreamData *)stream->private_data;\n+\t\tdelete data;\n+\t\tstream->private_data = nullptr;\n+\t}\n+\n+\tstatic std::unique_ptr<duckdb::ArrowArrayStreamWrapper> CreateStream(uintptr_t this_ptr,\n+\t                                                                     ArrowStreamParameters &parameters) {\n+\t\t//! Create a new batch reader\n+\t\tauto &factory = *reinterpret_cast<ArrowRoundtripFactory *>(this_ptr); //! NOLINT\n+\t\tif (!factory.result) {\n+\t\t\tthrow InternalException(\"Stream already consumed!\");\n+\t\t}\n+\n+\t\tauto stream_wrapper = make_unique<ArrowArrayStreamWrapper>();\n+\t\tstream_wrapper->number_of_rows = -1;\n+\t\tauto private_data = make_unique<ArrowArrayStreamData>(factory);\n+\t\tstream_wrapper->arrow_array_stream.get_schema = ArrowArrayStreamGetSchema;\n+\t\tstream_wrapper->arrow_array_stream.get_next = ArrowArrayStreamGetNext;\n+\t\tstream_wrapper->arrow_array_stream.get_last_error = ArrowArrayStreamGetLastError;\n+\t\tstream_wrapper->arrow_array_stream.release = ArrowArrayStreamRelease;\n+\t\tstream_wrapper->arrow_array_stream.private_data = private_data.release();\n+\n+\t\treturn stream_wrapper;\n+\t}\n+\n+\tstatic void GetSchema(uintptr_t factory_ptr, duckdb::ArrowSchemaWrapper &schema) {\n+\t\t//! Create a new batch reader\n+\t\tauto &factory = *reinterpret_cast<ArrowRoundtripFactory *>(factory_ptr); //! NOLINT\n+\t\tfactory.ToArrowSchema(&schema.arrow_schema);\n+\t}\n+\n+\tvoid ToArrowSchema(struct ArrowSchema *out) {\n+\t\tArrowConverter::ToArrowSchema(out, types, names, tz);\n+\t}\n+};\n+\n+void RunArrowComparison(Connection &con, const string &query, bool big_result = false) {\n+\t// run the query\n+\tauto initial_result = con.Query(query);\n+\tif (!initial_result->success) {\n+\t\tinitial_result->Print();\n+\t\tFAIL();\n+\t}\n+\t// create the roundtrip factory\n+\tauto tz = ClientConfig::GetConfig(*con.context).ExtractTimezone();\n+\tauto types = initial_result->types;\n+\tauto names = initial_result->names;\n+\tArrowRoundtripFactory factory(move(types), move(names), tz, move(initial_result), big_result);\n+\n+\t// construct the arrow scan\n+\tvector<Value> params;\n+\tparams.push_back(Value::POINTER((uintptr_t)&factory));\n+\tparams.push_back(Value::POINTER((uintptr_t)&ArrowRoundtripFactory::CreateStream));\n+\tparams.push_back(Value::POINTER((uintptr_t)&ArrowRoundtripFactory::GetSchema));\n+\tparams.push_back(Value::UBIGINT(1000000));\n+\n+\t// run the arrow scan over the result\n+\tauto arrow_result = con.TableFunction(\"arrow_scan\", params)->Execute();\n+\tREQUIRE(arrow_result->type == QueryResultType::MATERIALIZED_RESULT);\n+\tif (!arrow_result->success) {\n+\t\tprintf(\"-------------------------------------\\n\");\n+\t\tprintf(\"Arrow round-trip query error: %s\\n\", arrow_result->error.c_str());\n+\t\tprintf(\"-------------------------------------\\n\");\n+\t\tprintf(\"Query: %s\\n\", query.c_str());\n+\t\tprintf(\"-------------------------------------\\n\");\n+\t\tFAIL();\n+\t}\n+\tauto &materialized_arrow = (MaterializedQueryResult &)*arrow_result;\n+\n+\tauto result = con.Query(query);\n+\n+\t// compare the results\n+\tstring error;\n+\tif (!ColumnDataCollection::ResultEquals(result->Collection(), materialized_arrow.Collection(), error)) {\n+\t\tprintf(\"-------------------------------------\\n\");\n+\t\tprintf(\"Arrow round-trip failed: %s\\n\", error.c_str());\n+\t\tprintf(\"-------------------------------------\\n\");\n+\t\tprintf(\"Query: %s\\n\", query.c_str());\n+\t\tprintf(\"-----------------DuckDB-------------------\\n\");\n+\t\tresult->Print();\n+\t\tprintf(\"-----------------Arrow--------------------\\n\");\n+\t\tmaterialized_arrow.Print();\n+\t\tprintf(\"-------------------------------------\\n\");\n+\t\tFAIL();\n+\t}\n+\tREQUIRE(1);\n+}\n+\n+static void TestArrowRoundtrip(const string &query) {\n+\tDuckDB db;\n+\tConnection con(db);\n+\n+\tRunArrowComparison(con, query, true);\n+\tRunArrowComparison(con, query);\n+}\n+\n+static void TestParquetRoundtrip(const string &path) {\n+\tDuckDB db;\n+\tConnection con(db);\n+\n+\tif (ExtensionHelper::LoadExtension(db, \"parquet\") == ExtensionLoadResult::NOT_LOADED) {\n+\t\tFAIL();\n+\t\treturn;\n+\t}\n+\n+\t// run the query\n+\tauto query = \"SELECT * FROM parquet_scan('\" + path + \"')\";\n+\tRunArrowComparison(con, query, true);\n+\tRunArrowComparison(con, query);\n+}\n+\n+TEST_CASE(\"Test arrow roundtrip\", \"[arrow]\") {\n+\tTestArrowRoundtrip(\"SELECT * FROM range(10000) tbl(i) UNION ALL SELECT NULL\");\n+\tTestArrowRoundtrip(\"SELECT m from (select MAP(list_value(1), list_value(2)) from range(5) tbl(i)) tbl(m)\");\n+\tTestArrowRoundtrip(\"SELECT * FROM range(10) tbl(i)\");\n+\tTestArrowRoundtrip(\"SELECT case when i%2=0 then null else i end i FROM range(10) tbl(i)\");\n+\tTestArrowRoundtrip(\"SELECT case when i%2=0 then true else false end b FROM range(10) tbl(i)\");\n+\tTestArrowRoundtrip(\"SELECT case when i%2=0 then i%4=0 else null end b FROM range(10) tbl(i)\");\n+\tTestArrowRoundtrip(\"SELECT 'thisisalongstring'||i::varchar str FROM range(10) tbl(i)\");\n+\tTestArrowRoundtrip(\n+\t    \"SELECT case when i%2=0 then null else 'thisisalongstring'||i::varchar end str FROM range(10) tbl(i)\");\n+\tTestArrowRoundtrip(\"SELECT {'i': i, 'b': 10-i} str FROM range(10) tbl(i)\");\n+\tTestArrowRoundtrip(\"SELECT case when i%2=0 then {'i': case when i%4=0 then null else i end, 'b': 10-i} else null \"\n+\t                   \"end str FROM range(10) tbl(i)\");\n+\tTestArrowRoundtrip(\"SELECT [i, i+1, i+2] FROM range(10) tbl(i)\");\n+\tTestArrowRoundtrip(\n+\t    \"SELECT MAP(LIST_VALUE({'i':1,'j':2},{'i':3,'j':4}),LIST_VALUE({'i':1,'j':2},{'i':3,'j':4})) as a\");\n+\tTestArrowRoundtrip(\n+\t    \"SELECT MAP(LIST_VALUE({'i':i,'j':i+2},{'i':3,'j':NULL}),LIST_VALUE({'i':i+10,'j':2},{'i':i+4,'j':4})) as a \"\n+\t    \"FROM range(10) tbl(i)\");\n+\tTestArrowRoundtrip(\"SELECT MAP(['hello', 'world'||i::VARCHAR],[i + 1, NULL]) as a FROM range(10) tbl(i)\");\n+\tTestArrowRoundtrip(\"SELECT (1.5 + i)::DECIMAL(4,2) dec4, (1.5 + i)::DECIMAL(9,3) dec9, (1.5 + i)::DECIMAL(18,3) \"\n+\t                   \"dec18, (1.5 + i)::DECIMAL(38,3) dec38 FROM range(10) tbl(i)\");\n+\tTestArrowRoundtrip(\n+\t    \"SELECT case when i%2=0 then null else INTERVAL (i) seconds end AS interval FROM range(10) tbl(i)\");\n+#if STANDARD_VECTOR_SIZE < 64\n+\t// FIXME: there seems to be a bug in the enum arrow reader in this test when run with vsize=2\n+\treturn;\n+#endif\n+\tTestArrowRoundtrip(\"SELECT * REPLACE \"\n+\t                   \"(interval (1) seconds AS interval) FROM test_all_types()\");\n+}\n+\n+TEST_CASE(\"Test Parquet Files round-trip\", \"[arrow][.]\") {\n+\tstd::vector<std::string> data;\n+\t// data.emplace_back(\"data/parquet-testing/7-set.snappy.arrow2.parquet\");\n+\t//\tdata.emplace_back(\"data/parquet-testing/adam_genotypes.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/apkwan.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/aws1.snappy.parquet\");\n+\t// not supported by arrow\n+\t// data.emplace_back(\"data/parquet-testing/aws2.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/binary_string.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/blob.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/boolean_stats.parquet\");\n+\t// arrow can't read this\n+\t// data.emplace_back(\"data/parquet-testing/broken-arrow.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/bug1554.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/bug1588.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/bug1589.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/bug1618_struct_strings.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/bug2267.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/bug2557.parquet\");\n+\t// slow\n+\t// data.emplace_back(\"data/parquet-testing/bug687_nulls.parquet\");\n+\t// data.emplace_back(\"data/parquet-testing/complex.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/data-types.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/date.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/date_stats.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/decimal_stats.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/decimals.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/enum.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/filter_bug1391.parquet\");\n+\t//\tdata.emplace_back(\"data/parquet-testing/fixed.parquet\");\n+\t// slow\n+\t// data.emplace_back(\"data/parquet-testing/leftdate3_192_loop_1.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/lineitem-top10000.gzip.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/manyrowgroups.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/manyrowgroups2.parquet\");\n+\t//\tdata.emplace_back(\"data/parquet-testing/map.parquet\");\n+\t// Can't roundtrip NaNs\n+\tdata.emplace_back(\"data/parquet-testing/nan-float.parquet\");\n+\t// null byte in file\n+\t// data.emplace_back(\"data/parquet-testing/nullbyte.parquet\");\n+\t// data.emplace_back(\"data/parquet-testing/nullbyte_multiple.parquet\");\n+\t// borked\n+\t// data.emplace_back(\"data/parquet-testing/p2.parquet\");\n+\t// data.emplace_back(\"data/parquet-testing/p2strings.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/pandas-date.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/signed_stats.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/silly-names.parquet\");\n+\t// borked\n+\t// data.emplace_back(\"data/parquet-testing/simple.parquet\");\n+\t// data.emplace_back(\"data/parquet-testing/sorted.zstd_18_131072_small.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/struct.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/struct_skip_test.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/timestamp-ms.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/timestamp.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/unsigned.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/unsigned_stats.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/userdata1.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/varchar_stats.parquet\");\n+\tdata.emplace_back(\"data/parquet-testing/zstd.parquet\");\n+\n+\tfor (auto &parquet_path : data) {\n+\t\tTestParquetRoundtrip(parquet_path);\n+\t}\n+}\ndiff --git a/test/helpers/test_helpers.cpp b/test/helpers/test_helpers.cpp\nindex 1d3b2c8e7d76..2485841c518b 100644\n--- a/test/helpers/test_helpers.cpp\n+++ b/test/helpers/test_helpers.cpp\n@@ -120,20 +120,20 @@ bool CHECK_COLUMN(QueryResult &result_, size_t column_number, vector<duckdb::Val\n \t\tfprintf(stderr, \"Query failed with message: %s\\n\", result.error.c_str());\n \t\treturn false;\n \t}\n-\tif (!(result.names.size() == result.types.size())) {\n+\tif (result.names.size() != result.types.size()) {\n \t\t// column names do not match\n \t\tresult.Print();\n \t\treturn false;\n \t}\n \tif (values.size() == 0) {\n-\t\tif (result.collection.Count() != 0) {\n+\t\tif (result.RowCount() != 0) {\n \t\t\tresult.Print();\n \t\t\treturn false;\n \t\t} else {\n \t\t\treturn true;\n \t\t}\n \t}\n-\tif (result.collection.Count() == 0) {\n+\tif (result.RowCount() == 0) {\n \t\tresult.Print();\n \t\treturn false;\n \t}\n@@ -141,37 +141,20 @@ bool CHECK_COLUMN(QueryResult &result_, size_t column_number, vector<duckdb::Val\n \t\tresult.Print();\n \t\treturn false;\n \t}\n-\tsize_t chunk_index = 0;\n-\tfor (size_t i = 0; i < values.size();) {\n-\t\tif (chunk_index >= result.collection.ChunkCount()) {\n-\t\t\t// ran out of chunks\n-\t\t\tresult.Print();\n-\t\t\treturn false;\n+\tfor (idx_t row_idx = 0; row_idx < values.size(); row_idx++) {\n+\t\tauto value = result.GetValue(column_number, row_idx);\n+\t\t// NULL <> NULL, hence special handling\n+\t\tif (value.IsNull() && values[row_idx].IsNull()) {\n+\t\t\tcontinue;\n \t\t}\n-\t\t// check this vector\n-\t\tauto &chunk = result.collection.GetChunk(chunk_index);\n-\t\tauto &vector = chunk.data[column_number];\n-\t\tif (i + chunk.size() > values.size()) {\n-\t\t\t// too many values in this vector\n+\n+\t\tif (!Value::ValuesAreEqual(value, values[row_idx])) {\n+\t\t\t// FAIL(\"Incorrect result! Got \" + vector.GetValue(j).ToString()\n+\t\t\t// +\n+\t\t\t//      \" but expected \" + values[i + j].ToString());\n \t\t\tresult.Print();\n \t\t\treturn false;\n \t\t}\n-\t\tfor (size_t j = 0; j < chunk.size(); j++) {\n-\t\t\t// NULL <> NULL, hence special handling\n-\t\t\tif (vector.GetValue(j).IsNull() && values[i + j].IsNull()) {\n-\t\t\t\tcontinue;\n-\t\t\t}\n-\n-\t\t\tif (!Value::ValuesAreEqual(vector.GetValue(j), values[i + j])) {\n-\t\t\t\t// FAIL(\"Incorrect result! Got \" + vector.GetValue(j).ToString()\n-\t\t\t\t// +\n-\t\t\t\t//      \" but expected \" + values[i + j].ToString());\n-\t\t\t\tresult.Print();\n-\t\t\t\treturn false;\n-\t\t\t}\n-\t\t}\n-\t\tchunk_index++;\n-\t\ti += chunk.size();\n \t}\n \treturn true;\n }\n@@ -197,7 +180,7 @@ string compare_csv(duckdb::QueryResult &result, string csv, bool header) {\n \t\treturn materialized.error;\n \t}\n \tstring error;\n-\tif (!compare_result(csv, materialized.collection, materialized.types, header, error)) {\n+\tif (!compare_result(csv, materialized.Collection(), materialized.types, header, error)) {\n \t\treturn error;\n \t}\n \treturn \"\";\n@@ -245,32 +228,9 @@ string show_diff(DataChunk &left, DataChunk &right) {\n \treturn difference;\n }\n \n-bool compare_chunk(DataChunk &left, DataChunk &right) {\n-\tif (left.ColumnCount() != right.ColumnCount()) {\n-\t\treturn false;\n-\t}\n-\tif (left.size() != right.size()) {\n-\t\treturn false;\n-\t}\n-\tfor (size_t i = 0; i < left.ColumnCount(); i++) {\n-\t\tauto &left_vector = left.data[i];\n-\t\tauto &right_vector = right.data[i];\n-\t\tif (left_vector.GetType() == right_vector.GetType()) {\n-\t\t\tfor (size_t j = 0; j < left.size(); j++) {\n-\t\t\t\tauto left_value = left_vector.GetValue(j);\n-\t\t\t\tauto right_value = right_vector.GetValue(j);\n-\t\t\t\tif (!Value::ValuesAreEqual(left_value, right_value)) {\n-\t\t\t\t\treturn false;\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn true;\n-}\n-\n //! Compares the result of a pipe-delimited CSV with the given DataChunk\n //! Returns true if they are equal, and stores an error_message otherwise\n-bool compare_result(string csv, ChunkCollection &collection, vector<LogicalType> sql_types, bool has_header,\n+bool compare_result(string csv, ColumnDataCollection &collection, vector<LogicalType> sql_types, bool has_header,\n                     string &error_message) {\n \tD_ASSERT(collection.Count() == 0 || collection.Types().size() == sql_types.size());\n \n@@ -296,8 +256,8 @@ bool compare_result(string csv, ChunkCollection &collection, vector<LogicalType>\n \tDuckDB db;\n \tConnection con(db);\n \tBufferedCSVReader reader(*con.context, move(options), sql_types);\n-\tidx_t collection_index = 0;\n-\tidx_t tuple_count = 0;\n+\n+\tColumnDataCollection csv_data_collection(*con.context, sql_types);\n \twhile (true) {\n \t\t// parse a chunk from the CSV file\n \t\ttry {\n@@ -308,34 +268,14 @@ bool compare_result(string csv, ChunkCollection &collection, vector<LogicalType>\n \t\t\treturn false;\n \t\t}\n \t\tif (parsed_result.size() == 0) {\n-\t\t\t// out of tuples in CSV file\n-\t\t\tif (collection_index < collection.ChunkCount()) {\n-\t\t\t\terror_message = StringUtil::Format(\"Too many tuples in result! Found %llu tuples, but expected %llu\",\n-\t\t\t\t                                   collection.Count(), tuple_count);\n-\t\t\t\treturn false;\n-\t\t\t}\n-\t\t\treturn true;\n+\t\t\tbreak;\n \t\t}\n-\t\tif (collection_index >= collection.ChunkCount()) {\n-\t\t\t// ran out of chunks in the collection, but there are still tuples in the result\n-\t\t\t// keep parsing the csv file to get the total expected count\n-\t\t\twhile (parsed_result.size() > 0) {\n-\t\t\t\ttuple_count += parsed_result.size();\n-\t\t\t\tparsed_result.Reset();\n-\t\t\t\treader.ParseCSV(parsed_result);\n-\t\t\t}\n-\t\t\terror_message = StringUtil::Format(\"Too few tuples in result! Found %llu tuples, but expected %llu\",\n-\t\t\t                                   collection.Count(), tuple_count);\n-\t\t\treturn false;\n-\t\t}\n-\t\t// same counts, compare tuples in chunks\n-\t\tif (!compare_chunk(collection.GetChunk(collection_index), parsed_result)) {\n-\t\t\terror_message = show_diff(collection.GetChunk(collection_index), parsed_result);\n-\t\t\treturn false;\n-\t\t}\n-\n-\t\tcollection_index++;\n-\t\ttuple_count += parsed_result.size();\n+\t\tcsv_data_collection.Append(parsed_result);\n \t}\n+\tstring error;\n+\tif (!ColumnDataCollection::ResultEquals(collection, csv_data_collection, error_message)) {\n+\t\treturn false;\n+\t}\n+\treturn true;\n }\n } // namespace duckdb\ndiff --git a/test/include/capi_tester.hpp b/test/include/capi_tester.hpp\nindex cd404ee4bc10..7d17964aa166 100644\n--- a/test/include/capi_tester.hpp\n+++ b/test/include/capi_tester.hpp\n@@ -3,7 +3,7 @@\n #include \"catch.hpp\"\n #include \"duckdb.h\"\n #include \"test_helpers.hpp\"\n-#include \"duckdb/common/arrow.hpp\"\n+#include \"duckdb/common/arrow/arrow.hpp\"\n #include \"duckdb/common/exception.hpp\"\n \n namespace duckdb {\ndiff --git a/test/include/compare_result.hpp b/test/include/compare_result.hpp\nindex 08dd7a26d4ff..451623257460 100644\n--- a/test/include/compare_result.hpp\n+++ b/test/include/compare_result.hpp\n@@ -16,7 +16,7 @@ bool parse_datachunk(string csv, DataChunk &result, vector<LogicalType> sql_type\n \n //! Compares the result of a pipe-delimited CSV with the given DataChunk\n //! Returns true if they are equal, and stores an error_message otherwise\n-bool compare_result(string csv, ChunkCollection &collection, vector<LogicalType> sql_types, bool has_header,\n+bool compare_result(string csv, ColumnDataCollection &collection, vector<LogicalType> sql_types, bool has_header,\n                     string &error_message);\n \n } // namespace duckdb\ndiff --git a/test/sql/copy/parquet/parquet_metadata.test b/test/sql/copy/parquet/parquet_metadata.test\nindex 04b459ee5c25..144c2be72931 100644\n--- a/test/sql/copy/parquet/parquet_metadata.test\n+++ b/test/sql/copy/parquet/parquet_metadata.test\n@@ -10,6 +10,16 @@ SELECT * FROM parquet_metadata('data/parquet-testing/lineitem-top10000.gzip.parq\n statement ok\n SELECT * FROM parquet_schema('data/parquet-testing/lineitem-top10000.gzip.parquet');\n \n+query I\n+SELECT COUNT(*) > 0 FROM parquet_metadata('data/parquet-testing/lineitem-top10000.gzip.parquet');\n+----\n+true\n+\n+query I\n+SELECT COUNT(*) > 0 FROM parquet_schema('data/parquet-testing/lineitem-top10000.gzip.parquet');\n+----\n+true\n+\n statement ok\n select * from parquet_schema('data/parquet-testing/decimal/decimal_dc.parquet');\n \n@@ -22,9 +32,3 @@ select * from parquet_metadata('data/parquet-testing/glob/*.parquet');\n \n statement ok\n select * from parquet_schema('data/parquet-testing/glob/*.parquet');\n-\n-#statement ok\n-#SELECT * FROM parquet_metadata('data/parquet-testing/*.parquet');\n-\n-#statement ok\n-#SELECT * FROM parquet_schema('data/parquet-testing/*.parquet');\ndiff --git a/test/sql/limit/test_parallel_limit.test_slow b/test/sql/limit/test_parallel_limit.test_slow\nindex db525fb57882..4d43b2035467 100644\n--- a/test/sql/limit/test_parallel_limit.test_slow\n+++ b/test/sql/limit/test_parallel_limit.test_slow\n@@ -52,6 +52,25 @@ SELECT * FROM integers WHERE i>4978321 LIMIT 5\n 4978325\n 4978326\n \n+# large offset\n+query I\n+SELECT * FROM integers WHERE i>4978321 LIMIT 5 OFFSET 100000;\n+----\n+5078322\n+5078323\n+5078324\n+5078325\n+5078326\n+\n+query I\n+SELECT * FROM integers WHERE i>4978321 LIMIT 5 OFFSET 1000000;\n+----\n+5978322\n+5978323\n+5978324\n+5978325\n+5978326\n+\n # IN-clause (semi join)\n query I\n SELECT * FROM integers WHERE i IN (SELECT * FROM other_table)\ndiff --git a/test/sql/limit/test_parallel_limit_nested.test_slow b/test/sql/limit/test_parallel_limit_nested.test_slow\nnew file mode 100644\nindex 000000000000..f533a72a957d\n--- /dev/null\n+++ b/test/sql/limit/test_parallel_limit_nested.test_slow\n@@ -0,0 +1,95 @@\n+# name: test/sql/limit/test_parallel_limit_nested.test_slow\n+# description: Test parallel limit execution with basic nested types\n+# group: [limit]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+PRAGMA threads=8\n+\n+statement ok\n+CREATE TABLE other_table AS SELECT 337 i UNION ALL SELECT 94824 UNION ALL SELECT 177979 UNION ALL SELECT 477979;\n+\n+foreach table_type TABLE VIEW\n+\n+statement ok\n+CREATE ${table_type} integers AS SELECT i, [i, i + 1, i + 2] AS k, {'i': i} AS l FROM range(1000000) tbl(i);\n+\n+query II\n+SELECT k, l FROM integers LIMIT 5\n+----\n+[0, 1, 2]\t{'i': 0}\n+[1, 2, 3]\t{'i': 1}\n+[2, 3, 4]\t{'i': 2}\n+[3, 4, 5]\t{'i': 3}\n+[4, 5, 6]\t{'i': 4}\n+\n+query II\n+SELECT k, l FROM integers WHERE i>197832 OR i=334 LIMIT 5\n+----\n+[334, 335, 336]\t{'i': 334}\n+[197833, 197834, 197835]\t{'i': 197833}\n+[197834, 197835, 197836]\t{'i': 197834}\n+[197835, 197836, 197837]\t{'i': 197835}\n+[197836, 197837, 197838]\t{'i': 197836}\n+\n+query II\n+SELECT k, l FROM integers WHERE i>197832 LIMIT 5\n+----\n+[197833, 197834, 197835]\t{'i': 197833}\n+[197834, 197835, 197836]\t{'i': 197834}\n+[197835, 197836, 197837]\t{'i': 197835}\n+[197836, 197837, 197838]\t{'i': 197836}\n+[197837, 197838, 197839]\t{'i': 197837}\n+\n+query II\n+SELECT k, l FROM integers WHERE i>497832 LIMIT 5\n+----\n+[497833, 497834, 497835]\t{'i': 497833}\n+[497834, 497835, 497836]\t{'i': 497834}\n+[497835, 497836, 497837]\t{'i': 497835}\n+[497836, 497837, 497838]\t{'i': 497836}\n+[497837, 497838, 497839]\t{'i': 497837}\n+\n+# large offset\n+query II\n+SELECT k, l FROM integers WHERE i>497832 LIMIT 5 OFFSET 100000;\n+----\n+[597833, 597834, 597835]\t{'i': 597833}\n+[597834, 597835, 597836]\t{'i': 597834}\n+[597835, 597836, 597837]\t{'i': 597835}\n+[597836, 597837, 597838]\t{'i': 597836}\n+[597837, 597838, 597839]\t{'i': 597837}\n+\n+query II\n+SELECT k, l FROM integers WHERE i>497832 LIMIT 5 OFFSET 1000000;\n+----\n+\n+# IN-clause (semi join)\n+query II\n+SELECT k, l FROM integers WHERE i IN (SELECT * FROM other_table)\n+----\n+[337, 338, 339]\t{'i': 337}\n+[94824, 94825, 94826]\t{'i': 94824}\n+[177979, 177980, 177981]\t{'i': 177979}\n+[477979, 477980, 477981]\t{'i': 477979}\n+\n+query II\n+(SELECT k, l FROM integers WHERE i>197832 LIMIT 5) UNION ALL (SELECT k, l FROM integers WHERE i>497832 LIMIT 5)\n+----\n+[197833, 197834, 197835]\t{'i': 197833}\n+[197834, 197835, 197836]\t{'i': 197834}\n+[197835, 197836, 197837]\t{'i': 197835}\n+[197836, 197837, 197838]\t{'i': 197836}\n+[197837, 197838, 197839]\t{'i': 197837}\n+[497833, 497834, 497835]\t{'i': 497833}\n+[497834, 497835, 497836]\t{'i': 497834}\n+[497835, 497836, 497837]\t{'i': 497835}\n+[497836, 497837, 497838]\t{'i': 497836}\n+[497837, 497838, 497839]\t{'i': 497837}\n+\n+statement ok\n+DROP ${table_type} integers\n+\n+endloop\ndiff --git a/test/sql/limit/test_parallel_limit_nested_null.test_slow b/test/sql/limit/test_parallel_limit_nested_null.test_slow\nnew file mode 100644\nindex 000000000000..7b7f06103e6e\n--- /dev/null\n+++ b/test/sql/limit/test_parallel_limit_nested_null.test_slow\n@@ -0,0 +1,95 @@\n+# name: test/sql/limit/test_parallel_limit_nested_null.test_slow\n+# description: Test parallel limit execution with nested types and nulls\n+# group: [limit]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+PRAGMA threads=8\n+\n+statement ok\n+CREATE TABLE other_table AS SELECT 337 i UNION ALL SELECT 94824 UNION ALL SELECT 177979 UNION ALL SELECT 477979;\n+\n+foreach table_type TABLE VIEW\n+\n+statement ok\n+CREATE ${table_type} integers AS SELECT i, case when i%2=0 then [i, NULL, i + 2] else [NULL, i + 1, NULL] end AS k, {'i': case when i%2=0 then null else i end} AS l FROM range(1000000) tbl(i);\n+\n+query II\n+SELECT k, l FROM integers LIMIT 5\n+----\n+[0, NULL, 2]\t{'i': NULL}\n+[NULL, 2, NULL]\t{'i': 1}\n+[2, NULL, 4]\t{'i': NULL}\n+[NULL, 4, NULL]\t{'i': 3}\n+[4, NULL, 6]\t{'i': NULL}\n+\n+query II\n+SELECT k, l FROM integers WHERE i>197832 OR i=334 LIMIT 5\n+----\n+[334, NULL, 336]\t{'i': NULL}\n+[NULL, 197834, NULL]\t{'i': 197833}\n+[197834, NULL, 197836]\t{'i': NULL}\n+[NULL, 197836, NULL]\t{'i': 197835}\n+[197836, NULL, 197838]\t{'i': NULL}\n+\n+query II\n+SELECT k, l FROM integers WHERE i>197832 LIMIT 5\n+----\n+[NULL, 197834, NULL]\t{'i': 197833}\n+[197834, NULL, 197836]\t{'i': NULL}\n+[NULL, 197836, NULL]\t{'i': 197835}\n+[197836, NULL, 197838]\t{'i': NULL}\n+[NULL, 197838, NULL]\t{'i': 197837}\n+\n+query II\n+SELECT k, l FROM integers WHERE i>497832 LIMIT 5\n+----\n+[NULL, 497834, NULL]\t{'i': 497833}\n+[497834, NULL, 497836]\t{'i': NULL}\n+[NULL, 497836, NULL]\t{'i': 497835}\n+[497836, NULL, 497838]\t{'i': NULL}\n+[NULL, 497838, NULL]\t{'i': 497837}\n+\n+# large offset\n+query II\n+SELECT k, l FROM integers WHERE i>497832 LIMIT 5 OFFSET 100000;\n+----\n+[NULL, 597834, NULL]\t{'i': 597833}\n+[597834, NULL, 597836]\t{'i': NULL}\n+[NULL, 597836, NULL]\t{'i': 597835}\n+[597836, NULL, 597838]\t{'i': NULL}\n+[NULL, 597838, NULL]\t{'i': 597837}\n+\n+query II\n+SELECT k, l FROM integers WHERE i>497832 LIMIT 5 OFFSET 1000000;\n+----\n+\n+# IN-clause (semi join)\n+query II\n+SELECT k, l FROM integers WHERE i IN (SELECT * FROM other_table)\n+----\n+[NULL, 338, NULL]\t{'i': 337}\n+[94824, NULL, 94826]\t{'i': NULL}\n+[NULL, 177980, NULL]\t{'i': 177979}\n+[NULL, 477980, NULL]\t{'i': 477979}\n+\n+query II\n+(SELECT k, l FROM integers WHERE i>197832 LIMIT 5) UNION ALL (SELECT k, l FROM integers WHERE i>497832 LIMIT 5)\n+----\n+[NULL, 197834, NULL]\t{'i': 197833}\n+[197834, NULL, 197836]\t{'i': NULL}\n+[NULL, 197836, NULL]\t{'i': 197835}\n+[197836, NULL, 197838]\t{'i': NULL}\n+[NULL, 197838, NULL]\t{'i': 197837}\n+[NULL, 497834, NULL]\t{'i': 497833}\n+[497834, NULL, 497836]\t{'i': NULL}\n+[NULL, 497836, NULL]\t{'i': 497835}\n+[497836, NULL, 497838]\t{'i': NULL}\n+[NULL, 497838, NULL]\t{'i': 497837}\n+\n+statement ok\n+DROP ${table_type} integers\n+\n+endloop\ndiff --git a/test/sql/limit/test_parallel_limit_nulls.test_slow b/test/sql/limit/test_parallel_limit_nulls.test_slow\nnew file mode 100644\nindex 000000000000..29ace9952678\n--- /dev/null\n+++ b/test/sql/limit/test_parallel_limit_nulls.test_slow\n@@ -0,0 +1,100 @@\n+# name: test/sql/limit/test_parallel_limit_nulls.test_slow\n+# description: Test parallel limit execution with NULLs\n+# group: [limit]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+PRAGMA threads=8\n+\n+statement ok\n+CREATE TABLE other_table AS SELECT 337 i UNION ALL SELECT 948247 UNION ALL SELECT 1779793 UNION ALL SELECT 4779793;\n+\n+foreach table_type TABLE VIEW\n+\n+statement ok\n+CREATE ${table_type} integers AS SELECT i, CASE WHEN i%2=0 THEN NULL ELSE i END AS k FROM range(10000000) tbl(i);\n+\n+query I\n+SELECT k FROM integers LIMIT 5\n+----\n+NULL\n+1\n+NULL\n+3\n+NULL\n+\n+query I\n+SELECT k FROM integers WHERE i>1978321 OR i=334 LIMIT 5\n+----\n+NULL\n+NULL\n+1978323\n+NULL\n+1978325\n+\n+query I\n+SELECT k FROM integers WHERE i>1978321 LIMIT 5\n+----\n+NULL\n+1978323\n+NULL\n+1978325\n+NULL\n+\n+query I\n+SELECT k FROM integers WHERE i>4978321 LIMIT 5\n+----\n+NULL\n+4978323\n+NULL\n+4978325\n+NULL\n+\n+# large offset\n+query I\n+SELECT k FROM integers WHERE i>4978321 LIMIT 5 OFFSET 100000;\n+----\n+NULL\n+5078323\n+NULL\n+5078325\n+NULL\n+\n+query I\n+SELECT k FROM integers WHERE i>4978321 LIMIT 5 OFFSET 1000000;\n+----\n+NULL\n+5978323\n+NULL\n+5978325\n+NULL\n+\n+# IN-clause (semi join)\n+query I\n+SELECT k FROM integers WHERE i IN (SELECT * FROM other_table)\n+----\n+337\n+948247\n+1779793\n+4779793\n+\n+query I\n+(SELECT k FROM integers WHERE i>1978321 LIMIT 5) UNION ALL (SELECT k FROM integers WHERE i>4978321 LIMIT 5)\n+----\n+NULL\n+1978323\n+NULL\n+1978325\n+NULL\n+NULL\n+4978323\n+NULL\n+4978325\n+NULL\n+\n+statement ok\n+DROP ${table_type} integers\n+\n+endloop\ndiff --git a/test/sql/limit/test_parallel_limit_strings.test_slow b/test/sql/limit/test_parallel_limit_strings.test_slow\nnew file mode 100644\nindex 000000000000..ab1b54906474\n--- /dev/null\n+++ b/test/sql/limit/test_parallel_limit_strings.test_slow\n@@ -0,0 +1,100 @@\n+# name: test/sql/limit/test_parallel_limit_strings.test_slow\n+# description: Test parallel limit execution with strings\n+# group: [limit]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+PRAGMA threads=8\n+\n+statement ok\n+CREATE TABLE other_table AS SELECT 337 i UNION ALL SELECT 948247 UNION ALL SELECT 1779793 UNION ALL SELECT 4779793;\n+\n+foreach table_type TABLE VIEW\n+\n+statement ok\n+CREATE ${table_type} integers AS SELECT i, CONCAT('quackquack', i) AS k FROM range(10000000) tbl(i);\n+\n+query I\n+SELECT k FROM integers LIMIT 5\n+----\n+quackquack0\n+quackquack1\n+quackquack2\n+quackquack3\n+quackquack4\n+\n+query I\n+SELECT k FROM integers WHERE i>1978321 OR i=334 LIMIT 5\n+----\n+quackquack334\n+quackquack1978322\n+quackquack1978323\n+quackquack1978324\n+quackquack1978325\n+\n+query I\n+SELECT k FROM integers WHERE i>1978321 LIMIT 5\n+----\n+quackquack1978322\n+quackquack1978323\n+quackquack1978324\n+quackquack1978325\n+quackquack1978326\n+\n+query I\n+SELECT k FROM integers WHERE i>4978321 LIMIT 5\n+----\n+quackquack4978322\n+quackquack4978323\n+quackquack4978324\n+quackquack4978325\n+quackquack4978326\n+\n+# large offset\n+query I\n+SELECT k FROM integers WHERE i>4978321 LIMIT 5 OFFSET 100000;\n+----\n+quackquack5078322\n+quackquack5078323\n+quackquack5078324\n+quackquack5078325\n+quackquack5078326\n+\n+query I\n+SELECT k FROM integers WHERE i>4978321 LIMIT 5 OFFSET 1000000;\n+----\n+quackquack5978322\n+quackquack5978323\n+quackquack5978324\n+quackquack5978325\n+quackquack5978326\n+\n+# IN-clause (semi join)\n+query I\n+SELECT k FROM integers WHERE i IN (SELECT * FROM other_table)\n+----\n+quackquack337\n+quackquack948247\n+quackquack1779793\n+quackquack4779793\n+\n+query I\n+(SELECT k FROM integers WHERE i>1978321 LIMIT 5) UNION ALL (SELECT k FROM integers WHERE i>4978321 LIMIT 5)\n+----\n+quackquack1978322\n+quackquack1978323\n+quackquack1978324\n+quackquack1978325\n+quackquack1978326\n+quackquack4978322\n+quackquack4978323\n+quackquack4978324\n+quackquack4978325\n+quackquack4978326\n+\n+statement ok\n+DROP ${table_type} integers\n+\n+endloop\ndiff --git a/test/sql/parallelism/interquery/concurrent_checkpoint.cpp b/test/sql/parallelism/interquery/concurrent_checkpoint.cpp\nindex 888100c4831a..999c888b9c39 100644\n--- a/test/sql/parallelism/interquery/concurrent_checkpoint.cpp\n+++ b/test/sql/parallelism/interquery/concurrent_checkpoint.cpp\n@@ -152,9 +152,9 @@ TEST_CASE(\"Concurrent checkpoint with single updater\", \"[interquery][.]\") {\n \n \t\tREQUIRE_NO_FAIL(con.Query(\"BEGIN TRANSACTION\"));\n \t\tresult = con.Query(\"SELECT money FROM accounts WHERE id=\" + to_string(from));\n-\t\tValue money_from = result->collection.GetValue(0, 0);\n+\t\tValue money_from = result->GetValue(0, 0);\n \t\tresult = con.Query(\"SELECT money FROM accounts WHERE id=\" + to_string(to));\n-\t\tValue money_to = result->collection.GetValue(0, 0);\n+\t\tValue money_to = result->GetValue(0, 0);\n \n \t\tREQUIRE_NO_FAIL(\n \t\t    con.Query(\"UPDATE accounts SET money = money - \" + to_string(amount) + \" WHERE id = \" + to_string(from)));\n@@ -162,9 +162,9 @@ TEST_CASE(\"Concurrent checkpoint with single updater\", \"[interquery][.]\") {\n \t\t    con.Query(\"UPDATE accounts SET money = money + \" + to_string(amount) + \" WHERE id = \" + to_string(to)));\n \n \t\tresult = con.Query(\"SELECT money FROM accounts WHERE id=\" + to_string(from));\n-\t\tValue new_money_from = result->collection.GetValue(0, 0);\n+\t\tValue new_money_from = result->GetValue(0, 0);\n \t\tresult = con.Query(\"SELECT money FROM accounts WHERE id=\" + to_string(to));\n-\t\tValue new_money_to = result->collection.GetValue(0, 0);\n+\t\tValue new_money_to = result->GetValue(0, 0);\n \n \t\tValue expected_money_from, expected_money_to;\n \ndiff --git a/test/sql/parallelism/interquery/test_concurrentappend.cpp b/test/sql/parallelism/interquery/test_concurrentappend.cpp\nindex e74b694cd33b..9064382a8583 100644\n--- a/test/sql/parallelism/interquery/test_concurrentappend.cpp\n+++ b/test/sql/parallelism/interquery/test_concurrentappend.cpp\n@@ -33,13 +33,13 @@ TEST_CASE(\"Sequential append\", \"[interquery][.]\") {\n \n \tfor (size_t i = 0; i < CONCURRENT_APPEND_THREAD_COUNT; i++) {\n \t\tresult = connections[i]->Query(\"SELECT COUNT(*) FROM integers\");\n-\t\tD_ASSERT(result->collection.Count() > 0);\n-\t\tValue count = result->collection.GetValue(0, 0);\n+\t\tD_ASSERT(result->RowCount() > 0);\n+\t\tValue count = result->GetValue(0, 0);\n \t\tREQUIRE(count == 0);\n \t\tfor (size_t j = 0; j < CONCURRENT_APPEND_INSERT_ELEMENTS; j++) {\n \t\t\tconnections[i]->Query(\"INSERT INTO integers VALUES (3)\");\n \t\t\tresult = connections[i]->Query(\"SELECT COUNT(*) FROM integers\");\n-\t\t\tValue new_count = result->collection.GetValue(0, 0);\n+\t\t\tValue new_count = result->GetValue(0, 0);\n \t\t\tREQUIRE(new_count == j + 1);\n \t\t\tcount = new_count;\n \t\t}\n@@ -49,7 +49,7 @@ TEST_CASE(\"Sequential append\", \"[interquery][.]\") {\n \t\tconnections[i]->Query(\"COMMIT;\");\n \t}\n \tresult = con.Query(\"SELECT COUNT(*) FROM integers\");\n-\tValue count = result->collection.GetValue(0, 0);\n+\tValue count = result->GetValue(0, 0);\n \tREQUIRE(count == CONCURRENT_APPEND_THREAD_COUNT * CONCURRENT_APPEND_INSERT_ELEMENTS);\n }\n \n@@ -61,13 +61,13 @@ static void insert_random_elements(DuckDB *db, bool *correct, int threadnr) {\n \t// initial count\n \tcon.Query(\"BEGIN TRANSACTION;\");\n \tauto result = con.Query(\"SELECT COUNT(*) FROM integers\");\n-\tValue count = result->collection.GetValue(0, 0);\n+\tValue count = result->GetValue(0, 0);\n \tauto start_count = count.GetValue<int64_t>();\n \tfor (size_t i = 0; i < CONCURRENT_APPEND_INSERT_ELEMENTS; i++) {\n \t\t// count should increase by one for every append we do\n \t\tcon.Query(\"INSERT INTO integers VALUES (3)\");\n \t\tresult = con.Query(\"SELECT COUNT(*) FROM integers\");\n-\t\tValue new_count = result->collection.GetValue(0, 0);\n+\t\tValue new_count = result->GetValue(0, 0);\n \t\tif (new_count != start_count + i + 1) {\n \t\t\tcorrect[threadnr] = false;\n \t\t}\ndiff --git a/test/sql/parallelism/interquery/test_concurrentdelete.cpp b/test/sql/parallelism/interquery/test_concurrentdelete.cpp\nindex 75638851bfc9..4f627d967c6d 100644\n--- a/test/sql/parallelism/interquery/test_concurrentdelete.cpp\n+++ b/test/sql/parallelism/interquery/test_concurrentdelete.cpp\n@@ -80,20 +80,20 @@ TEST_CASE(\"Sequential delete\", \"[interquery][.]\") {\n \t\t// check the current count\n \t\tresult = connections[i]->Query(\"SELECT SUM(i) FROM integers\");\n \t\tREQUIRE_NO_FAIL(*result);\n-\t\tcount = result->collection.GetValue(0, 0);\n+\t\tcount = result->GetValue(0, 0);\n \t\tREQUIRE(count == sum);\n \t\t// delete the elements for this thread\n \t\tREQUIRE_NO_FAIL(connections[i]->Query(\"DELETE FROM integers WHERE i=\" + to_string(i + 1)));\n \t\t// check the updated count\n \t\tresult = connections[i]->Query(\"SELECT SUM(i) FROM integers\");\n \t\tREQUIRE_NO_FAIL(*result);\n-\t\tcount = result->collection.GetValue(0, 0);\n+\t\tcount = result->GetValue(0, 0);\n \t\tREQUIRE(count == sum - (i + 1) * CONCURRENT_DELETE_INSERT_ELEMENTS);\n \t}\n \t// check the count on the original connection\n \tresult = con.Query(\"SELECT SUM(i) FROM integers\");\n \tREQUIRE_NO_FAIL(*result);\n-\tcount = result->collection.GetValue(0, 0);\n+\tcount = result->GetValue(0, 0);\n \tREQUIRE(count == sum);\n \n \t// commit everything\n@@ -104,7 +104,7 @@ TEST_CASE(\"Sequential delete\", \"[interquery][.]\") {\n \t// check that the count is 0 now\n \tresult = con.Query(\"SELECT COUNT(i) FROM integers\");\n \tREQUIRE_NO_FAIL(*result);\n-\tcount = result->collection.GetValue(0, 0);\n+\tcount = result->GetValue(0, 0);\n \tREQUIRE(count == 0);\n }\n \n@@ -161,7 +161,7 @@ static void delete_elements(DuckDB *db, bool *correct, size_t threadnr) {\n \t// initial count\n \tcon.Query(\"BEGIN TRANSACTION;\");\n \tauto result = con.Query(\"SELECT COUNT(*) FROM integers\");\n-\tValue count = result->collection.GetValue(0, 0);\n+\tValue count = result->GetValue(0, 0);\n \tauto start_count = count.GetValue<int64_t>();\n \n \tfor (size_t i = 0; i < CONCURRENT_DELETE_INSERT_ELEMENTS; i++) {\n@@ -174,7 +174,7 @@ static void delete_elements(DuckDB *db, bool *correct, size_t threadnr) {\n \t\tif (!result->success) {\n \t\t\tcorrect[threadnr] = false;\n \t\t} else {\n-\t\t\tValue new_count = result->collection.GetValue(0, 0);\n+\t\t\tValue new_count = result->GetValue(0, 0);\n \t\t\tif (new_count != start_count - (i + 1)) {\n \t\t\t\tcorrect[threadnr] = false;\n \t\t\t}\n@@ -223,6 +223,6 @@ TEST_CASE(\"Concurrent delete\", \"[interquery][.]\") {\n \t// check that the count is 0 now\n \tresult = con.Query(\"SELECT COUNT(i) FROM integers\");\n \tREQUIRE_NO_FAIL(*result);\n-\tauto count = result->collection.GetValue(0, 0);\n+\tauto count = result->GetValue(0, 0);\n \tREQUIRE(count == 0);\n }\ndiff --git a/test/sql/parallelism/interquery/test_concurrentupdate.cpp b/test/sql/parallelism/interquery/test_concurrentupdate.cpp\nindex c55e6063e41b..971ab6da9494 100644\n--- a/test/sql/parallelism/interquery/test_concurrentupdate.cpp\n+++ b/test/sql/parallelism/interquery/test_concurrentupdate.cpp\n@@ -105,9 +105,9 @@ TEST_CASE(\"Concurrent update\", \"[interquery][.]\") {\n \n \t\tREQUIRE_NO_FAIL(con.Query(\"BEGIN TRANSACTION\"));\n \t\tresult = con.Query(\"SELECT money FROM accounts WHERE id=\" + to_string(from));\n-\t\tValue money_from = result->collection.GetValue(0, 0);\n+\t\tValue money_from = result->GetValue(0, 0);\n \t\tresult = con.Query(\"SELECT money FROM accounts WHERE id=\" + to_string(to));\n-\t\tValue money_to = result->collection.GetValue(0, 0);\n+\t\tValue money_to = result->GetValue(0, 0);\n \n \t\tREQUIRE_NO_FAIL(\n \t\t    con.Query(\"UPDATE accounts SET money = money - \" + to_string(amount) + \" WHERE id = \" + to_string(from)));\n@@ -115,9 +115,9 @@ TEST_CASE(\"Concurrent update\", \"[interquery][.]\") {\n \t\t    con.Query(\"UPDATE accounts SET money = money + \" + to_string(amount) + \" WHERE id = \" + to_string(to)));\n \n \t\tresult = con.Query(\"SELECT money FROM accounts WHERE id=\" + to_string(from));\n-\t\tValue new_money_from = result->collection.GetValue(0, 0);\n+\t\tValue new_money_from = result->GetValue(0, 0);\n \t\tresult = con.Query(\"SELECT money FROM accounts WHERE id=\" + to_string(to));\n-\t\tValue new_money_to = result->collection.GetValue(0, 0);\n+\t\tValue new_money_to = result->GetValue(0, 0);\n \n \t\tValue expected_money_from, expected_money_to;\n \ndiff --git a/test/sql/setops/test_setops.test b/test/sql/setops/test_setops.test\nindex c70a2282bae2..944cadc0edf2 100644\n--- a/test/sql/setops/test_setops.test\n+++ b/test/sql/setops/test_setops.test\n@@ -2,6 +2,9 @@\n # description: Test UNION/EXCEPT/INTERSECT\n # group: [setops]\n \n+statement ok\n+PRAGMA enable_verification\n+\n query I\n SELECT 1 UNION ALL SELECT 2\n ----\ndiff --git a/test/sqlite/result_helper.cpp b/test/sqlite/result_helper.cpp\nindex b9be2fda04a5..a97a6e6642f7 100644\n--- a/test/sqlite/result_helper.cpp\n+++ b/test/sqlite/result_helper.cpp\n@@ -49,7 +49,7 @@ void TestResultHelper::CheckQueryResult(unique_ptr<MaterializedQueryResult> owne\n \t\t}\n \t\tFAIL_LINE(file_name, query_line, 0);\n \t}\n-\tidx_t row_count = result.collection.Count();\n+\tidx_t row_count = result.RowCount();\n \tidx_t column_count = result.ColumnCount();\n \tidx_t total_value_count = row_count * column_count;\n \tbool compare_hash =\n@@ -81,7 +81,7 @@ void TestResultHelper::CheckQueryResult(unique_ptr<MaterializedQueryResult> owne\n \t\t}\n \t\tstd::cerr << std::endl;\n \t\tPrintLineSep();\n-\t\tfor (idx_t r = 0; r < result.collection.Count(); r++) {\n+\t\tfor (idx_t r = 0; r < result.RowCount(); r++) {\n \t\t\tfor (idx_t c = 0; c < result.ColumnCount(); c++) {\n \t\t\t\tif (c != 0) {\n \t\t\t\t\tstd::cerr << \"\\t\";\n@@ -168,7 +168,7 @@ void TestResultHelper::CheckQueryResult(unique_ptr<MaterializedQueryResult> owne\n \t\t}\n \t\tidx_t expected_rows = comparison_values.size() / expected_column_count;\n \t\t// we first check the counts: if the values are equal to the amount of rows we expect the results to be row-wise\n-\t\tbool row_wise = expected_column_count > 1 && comparison_values.size() == result.collection.Count();\n+\t\tbool row_wise = expected_column_count > 1 && comparison_values.size() == result.RowCount();\n \t\tif (!row_wise) {\n \t\t\t// the counts do not match up for it to be row-wise\n \t\t\t// however, this can also be because the query returned an incorrect # of rows\n@@ -197,13 +197,13 @@ void TestResultHelper::CheckQueryResult(unique_ptr<MaterializedQueryResult> owne\n \t\t\tfprintf(stderr, \"This is not cleanly divisible (i.e. the last row does not have enough values)\\n\");\n \t\t\tFAIL_LINE(file_name, query_line, 0);\n \t\t}\n-\t\tif (expected_rows != result.collection.Count()) {\n+\t\tif (expected_rows != result.RowCount()) {\n \t\t\tif (column_count_mismatch) {\n \t\t\t\tColumnCountMismatch(original_expected_columns, row_wise);\n \t\t\t}\n \t\t\tPrintErrorHeader(\"Wrong row count in query!\");\n \t\t\tstd::cerr << \"Expected \" << termcolor::bold << expected_rows << termcolor::reset << \" rows, but got \"\n-\t\t\t          << termcolor::bold << result.collection.Count() << termcolor::reset << \" rows\" << std::endl;\n+\t\t\t          << termcolor::bold << result.RowCount() << termcolor::reset << \" rows\" << std::endl;\n \t\t\tPrintLineSep();\n \t\t\tPrintSQL(sql_query);\n \t\t\tPrintLineSep();\n@@ -458,7 +458,7 @@ string TestResultHelper::SQLLogicTestConvertValue(Value value, LogicalType sql_t\n void TestResultHelper::DuckDBConvertResult(MaterializedQueryResult &result, bool original_sqlite_test,\n                                            vector<string> &out_result) {\n \tsize_t r, c;\n-\tidx_t row_count = result.collection.Count();\n+\tidx_t row_count = result.RowCount();\n \tidx_t column_count = result.ColumnCount();\n \n \tout_result.resize(row_count * column_count);\ndiff --git a/test/tpce/test_tpce.cpp b/test/tpce/test_tpce.cpp\nindex 679f9e76fb1b..95fcd5e6ae9a 100644\n--- a/test/tpce/test_tpce.cpp\n+++ b/test/tpce/test_tpce.cpp\n@@ -19,8 +19,8 @@ TEST_CASE(\"Test TPC-E\", \"[tpce][.]\") {\n \n \tauto result = con.Query(\"SELECT * FROM sqlite_master\");\n \n-\tfor (size_t i = 0; i < result->collection.Count(); i++) {\n-\t\tauto table_name = result->collection.GetValue(1, i);\n+\tfor (size_t i = 0; i < result->RowCount(); i++) {\n+\t\tauto table_name = result->GetValue(1, i);\n \n \t\tREQUIRE_NO_FAIL(con.Query(\"SELECT COUNT(*) FROM \" + StringValue::Get(table_name)));\n \ndiff --git a/tools/pythonpkg/tests/fast/arrow/test_nested_arrow.py b/tools/pythonpkg/tests/fast/arrow/test_nested_arrow.py\nindex 9d763c712f02..d6c189d7614b 100644\n--- a/tools/pythonpkg/tests/fast/arrow/test_nested_arrow.py\n+++ b/tools/pythonpkg/tests/fast/arrow/test_nested_arrow.py\n@@ -21,19 +21,19 @@ class TestArrowNested(object):\n     def test_lists_basic(self,duckdb_cursor):\n         if not can_run:\n             return\n-        \n+\n         #Test Constant List\n-        query = duckdb.query(\"SELECT a from (select list_value(3,5,10) as a) as t\").arrow()['a'].to_numpy() \n+        query = duckdb.query(\"SELECT a from (select list_value(3,5,10) as a) as t\").arrow()['a'].to_numpy()\n         assert query[0][0] == 3\n         assert query[0][1] == 5\n         assert query[0][2] == 10\n \n         # Empty List\n-        query = duckdb.query(\"SELECT a from (select list_value() as a) as t\").arrow()['a'].to_numpy() \n+        query = duckdb.query(\"SELECT a from (select list_value() as a) as t\").arrow()['a'].to_numpy()\n         assert len(query[0]) == 0\n \n         #Test Constant List With Null\n-        query = duckdb.query(\"SELECT a from (select list_value(3,NULL) as a) as t\").arrow()['a'].to_numpy() \n+        query = duckdb.query(\"SELECT a from (select list_value(3,NULL) as a) as t\").arrow()['a'].to_numpy()\n         assert query[0][0] == 3\n         assert np.isnan(query[0][1])\n \n@@ -115,9 +115,9 @@ def test_map_roundtrip(self,duckdb_cursor):\n         if not can_run:\n             return\n         compare_results(\"SELECT a from (select MAP(LIST_VALUE(1, 2, 3, 4),LIST_VALUE(10, 9, 8, 7)) as a) as t\")\n-        \n+\n         compare_results(\"SELECT a from (select MAP(LIST_VALUE(1, 2, 3, 4),LIST_VALUE(10, 9, 8, 7)) as a) as t\")\n-        \n+\n         compare_results(\"SELECT a from (select MAP(LIST_VALUE(),LIST_VALUE()) as a) as t\")\n         compare_results(\"SELECT a from (select MAP(LIST_VALUE('Jon Lajoie', 'Backstreet Boys', 'Tenacious D'),LIST_VALUE(10,9,10)) as a) as t\")\n         compare_results(\"SELECT a from (select MAP(LIST_VALUE('Jon Lajoie','Tenacious D'),LIST_VALUE(10,10)) as a) as t\")\n@@ -142,7 +142,7 @@ def test_map_arrow_to_duckdb(self, duckdb_cursor):\n         )\n         with pytest.raises(Exception, match=\"Invalid Input Error: Arrow map contains duplicate key, which isn't supported by DuckDB map type\"):\n             rel = duckdb.from_arrow(arrow_table).fetchall()\n-    \n+\n     def test_map_arrow_to_pandas(self,duckdb_cursor):\n         if not can_run:\n             return\n@@ -161,11 +161,11 @@ def test_frankstein_nested(self,duckdb_cursor):\n \n         # Lists of structs with lists\n         compare_results(\"SELECT [{'i':1,'j':[2,3]},NULL]\")\n-                \n+\n         # Maps embedded in a struct\n         compare_results(\"SELECT {'i':mp,'j':mp2} FROM (SELECT MAP(LIST_VALUE(1, 2, 3, 4),LIST_VALUE(10, 9, 8, 7)) as mp, MAP(LIST_VALUE(1, 2, 3, 5),LIST_VALUE(10, 9, 8, 7)) as mp2) as t\")\n \n-        # List of maps \n+        # List of maps\n         compare_results(\"SELECT [mp,mp2] FROM (SELECT MAP(LIST_VALUE(1, 2, 3, 4),LIST_VALUE(10, 9, 8, 7)) as mp, MAP(LIST_VALUE(1, 2, 3, 5),LIST_VALUE(10, 9, 8, 7)) as mp2) as t\")\n \n         # Map with list as key and/or value\ndiff --git a/tools/rpkg/tests/testthat/test_arrow.R b/tools/rpkg/tests/testthat/test_arrow.R\nindex caa36044ce62..2a2e372c229e 100644\n--- a/tools/rpkg/tests/testthat/test_arrow.R\n+++ b/tools/rpkg/tests/testthat/test_arrow.R\n@@ -26,7 +26,6 @@ skip_if_not(arrow::arrow_with_parquet(), message = \"The installed Arrow is not f\n library(arrow, warn.conflicts = FALSE)\n library(dplyr, warn.conflicts = FALSE)\n library(duckdb)\n-library(\"testthat\")\n library(\"DBI\")\n \n example_data <- dplyr::tibble(\n@@ -201,7 +200,7 @@ test_that(\"to_arrow roundtrip, with dataset\", {\n # we need to create a connection separate from the ephemeral one that is made\n # with arrow_duck_connection()\n con <- dbConnect(duckdb::duckdb())\n-dbExecute(con, \"PRAGMA threads=2\")\n+dbExecute(con, \"PRAGMA threads=1\")\n on.exit(dbDisconnect(con, shutdown = TRUE), add = TRUE)\n \n test_that(\"Joining, auto-cleanup enabled\", {\n@@ -226,7 +225,7 @@ test_that(\"Joining, auto-cleanup enabled\", {\n   expect_true(all(c(table_one_name, table_two_name) %in% duckdb::duckdb_list_arrow(con)))\n   rm(table_one, table_two)\n   gc()\n-  expect_false(any(c(table_one_name, table_two_name) %in% duckdb::duckdb_list_arrow(con)))\n+  expect_false(any(c(table_one_name,     table_two_name) %in% duckdb::duckdb_list_arrow(con)))\n })\n \n test_that(\"Joining, auto-cleanup disabled\", {\ndiff --git a/tools/rpkg/tests/testthat/test_register.R b/tools/rpkg/tests/testthat/test_register.R\nindex cde63ef22ef1..f5963a4d8d19 100644\n--- a/tools/rpkg/tests/testthat/test_register.R\n+++ b/tools/rpkg/tests/testthat/test_register.R\n@@ -47,8 +47,8 @@ test_that(\"various error cases for duckdb_register()\", {\n   expect_error(duckdb::duckdb_unregister(1, \"my_df1\"))\n   expect_error(duckdb::duckdb_unregister(con, \"\"))\n   dbDisconnect(con, shutdown = TRUE)\n-\n-  expect_error(duckdb::duckdb_unregister(con, \"my_df1\"))\n+  # this is fine\n+  duckdb::duckdb_unregister(con, \"my_df1\")\n })\n \n \n",
  "problem_statement": "Adding a where clause slows down a query by 10x.\nTest file can be found here  https://github.com/lloydtabb/malloy_examples/blob/main/names/data/usa_names.parquet\r\n\r\nThe query below runs in about 20 seconds (on my machine).  Comment out the where clause and the query runs in 2 seconds.  I'm wondering if this can be optimized or if you have suggestions for rewriting the expression.  This is a pretty common Malloy use case.\r\n\r\n```\r\n  SELECT\r\n    group_set,\r\n    names_table.\"state\" as \"state__0\",\r\n    CASE WHEN group_set=0 THEN\r\n      SUM(names_table.\"number\")\r\n      END as \"population__0\",\r\n    CASE WHEN group_set=1 THEN\r\n      names_table.\"name\"\r\n      END as \"name__1\",\r\n    CASE WHEN group_set=1 THEN\r\n      SUM(names_table.\"number\")\r\n      END as \"population__1\"\r\n  FROM 'usa_names.parquet' as names_table\r\n  CROSS JOIN (SELECT UNNEST(GENERATE_SERIES(0,1,1)) as group_set  ) as group_set\r\n  WHERE (group_set NOT IN (1) OR (group_set IN (1) AND names_table.\"gender\"='M'))\r\n  GROUP BY 1,2,4\r\n  LIMIT 5\r\n  ```\r\n\r\nvs\r\n\r\n```\r\n  SELECT\r\n    group_set,\r\n    names_table.\"state\" as \"state__0\",\r\n    CASE WHEN group_set=0 THEN\r\n      SUM(names_table.\"number\")\r\n      END as \"population__0\",\r\n    CASE WHEN group_set=1 THEN\r\n      names_table.\"name\"\r\n      END as \"name__1\",\r\n    CASE WHEN group_set=1 THEN\r\n      SUM(names_table.\"number\")\r\n      END as \"population__1\"\r\n  FROM 'usa_names.parquet' as names_table\r\n  CROSS JOIN (SELECT UNNEST(GENERATE_SERIES(0,1,1)) as group_set  ) as group_set\r\n  -- WHERE (group_set NOT IN (1) OR (group_set IN (1) AND names_table.\"gender\"='M'))\r\n  GROUP BY 1,2,4\r\n  LIMIT 5\r\n  ```\nDuckDB 0.3.4 fetch_arrow_table() is over 20x slower vs 0.3.2\n#### What happens?\r\n\r\nfetch_arrow_table() appears to be significantly slower in 0.3.4 vs 0.3.2 (124 seconds vs 5.55 seconds)\r\n\r\n#### To Reproduce\r\nSteps to reproduce the behavior. Bonus points if those are only SQL queries.\r\n\r\n!aws s3 cp s3://ursa-labs-taxi-data/2015/06/data.parquet /tmp\r\n\r\n# using pyarrow==7 and duckdb 0.3.2\r\n\r\n[ins] In [1]: import duckdb\r\n[ins] In [2]: c = duckdb.connect()\r\n[ins] In [5]: duckdb.__version__\r\nOut[5]: '0.3.2'\r\n\r\n[ins] In [6]: %time r = c.execute(\"select * from parquet_scan('/tmp/data.parquet')\")\r\nCPU times: user 17.7 ms, sys: 4.39 ms, total: 22.1 ms\r\nWall time: 26.4 ms\r\n\r\n[ins] In [7]: %time t = r.fetch_arrow_table()\r\nCPU times: user 5.55 s\r\n\r\n------\r\n# using pyarrow==7 and duckdb 0.3.4\r\n# pyarrow==8 is also slow\r\n\r\n[ins] In [1]: import duckdb\r\n[nav] In [2]: c = duckdb.connect()\r\n\r\n[ins] In [3]: duckdb.__version__\r\nOut[3]: '0.3.4'\r\n\r\n[ins] In [4]: %time r = c.execute(\"select * from parquet_scan('/tmp/data.parquet')\")\r\nCPU times: user 5.36 ms, sys: 7.14 ms, total: 12.5 ms\r\nWall time: 12 ms\r\n\r\n[ins] In [5]: %time t = r.fetch_arrow_table()\r\nCPU times: user 1min 46s, sys: 20 s, total: 2min 6s\r\nWall time: 2min 4s\r\n\r\n#### Environment (please complete the following information):\r\n - OS: Ubuntu 20.04\r\n - DuckDB Version: 0.3.4\r\n - DuckDB Client: Python\r\n\r\n#### Before Submitting\r\n\r\n- [x] **Have you tried this on the latest `master` branch?**\r\n* **Python**: `pip install duckdb --upgrade --pre`\r\n'0.3.5-dev359'\r\n\r\n[ins] In [5]: %time t = r.fetch_arrow_table()\r\nCPU times: user 1min 29s, sys: 12.5 s, total: 1min 41s\r\nWall time: 1min 40s\r\n\r\n- [x] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**\r\n\r\nYes.  Created separate conda and docker environments. \n",
  "hints_text": "\nHello!\r\n\r\nCould you try a few different values for the new chunk size parameter? That might be the difference...\r\nhttps://github.com/duckdb/duckdb/pull/3173\nI think is probably fair (and a good idea) to add regression tests on the Python API, especially on the Pandas and Arrow integrations.\r\n\r\nBut as Alex pointed out, this issue can be related to chunk sizes. The previous code would always gerenate an arrow table with one chunk, so if you set the chunk_size parameter to whatever is the table size, that should generate something similar to the prior duckdb version.\n> Hello!\r\n> \r\n> Could you try a few different values for the new chunk size parameter? That might be the difference... #3173\r\n\r\nOk.  Definitely works faster with chunks being small like 1K.   I do not understand why larger chunk size is slower.  Apologies for bad copy and paste.  Can\u2019t seem to copy well from iPhone.  (fixed after editing on desktop)\r\n```\r\n[ins] In [9]: %time t = r.fetch_arrow_table(100000)\r\nCPU times: user 8.8 s, sys: 1.39 s, total: 10.2 s\r\nWall time: 10.2 s                                 \r\n\r\n[ins] In [10]: %time r = c.execute(\"select * from parquet_scan('/tmp/data.parquet')\")\r\nCPU times: user 8.71 ms, sys: 181 \u00b5s, total: 8.89 ms\r\nWall time: 8.51 ms                  \r\n                                                                                                                                \r\n\r\n[ins] In [11]: %time t = r.fetch_arrow_table(10000)\r\nCPU times: user 2.89 s, sys: 609 ms, total: 3.5 s\r\nWall time: 3.49 s                          \r\n                                                                                                                                                                                                                                                                                     [ins] In [12]: t.num_rows\r\nOut[12]: 12324935\r\n\r\n[ins] In [13]: %time r = c.execute(\"select * from parquet_scan('/tmp/data.parquet')\")\r\nCPU times: user 10.9 ms, sys: 0 ns, total: 10.9 ms\r\nWall time: 10.6 ms                     \r\n                                                                                                                                   \r\n\r\n[ins] In [14]: %time t = r.fetch_arrow_table(1000)\r\nCPU times: user 2.53 s, sys: 141 ms, total: 2.67 s\r\nWall time: 2.67 s                 \r\n[ins] In [15]: t.num_rows\r\nOut[15]: 12324935\r\n```",
  "created_at": "2022-08-07T19:42:49Z"
}