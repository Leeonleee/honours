You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Support pandas nullable boolean type
#### What happens?
Pandas dataframes containing any columns of the (experimental) [nullable boolean type ](https://pandas.pydata.org/pandas-docs/stable/user_guide/boolean.html) will fail to register with duckdb.

#### To Reproduce

```
In [3]: df1 = pandas.DataFrame({"foo": [True, None, False]})
   ...: df2 = pandas.DataFrame({"foo": [True, None, False]}, dtype="boolean")

In [4]: df1.dtypes
Out[4]:
foo    object
dtype: object

In [5]: df2.dtypes
Out[5]:
foo    boolean
dtype: object

In [6]: con = duckdb.connect(database=":memory:", read_only=False)

In [7]: con.register("df1", df1)
Out[7]: <duckdb.DuckDBPyConnection at 0x127c35530>

In [8]: con.register("df2", df2)
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-8-e94dc73d8e5d> in <module>
----> 1 con.register("df2", df2)

RuntimeError: unsupported python type boolean
```

#### Environment (please complete the following information):
 - OS: [e.g. iOS]: Mac OS
 - DuckDB Version: [e.g. 22]: 0.3.1
 - DuckDB Client: [e.g. Python]: python



</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
44: 
45: 
[end of README.md]
[start of tools/pythonpkg/src/include/duckdb_python/vector_conversion.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb_python/array_wrapper.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb_python/pybind_wrapper.hpp"
12: 
13: #include "duckdb.hpp"
14: #include "duckdb_python/python_object_container.hpp"
15: namespace duckdb {
16: 
17: enum class PandasType : uint8_t {
18: 	BOOLEAN,
19: 	TINYINT,
20: 	SMALLINT,
21: 	INTEGER,
22: 	BIGINT,
23: 	UTINYINT,
24: 	USMALLINT,
25: 	UINTEGER,
26: 	UBIGINT,
27: 	FLOAT,
28: 	DOUBLE,
29: 	TIMESTAMP,
30: 	INTERVAL,
31: 	VARCHAR,
32: 	OBJECT,
33: 	CATEGORY
34: };
35: 
36: struct NumPyArrayWrapper {
37: 	explicit NumPyArrayWrapper(py::array numpy_array) : numpy_array(move(numpy_array)) {
38: 	}
39: 
40: 	py::array numpy_array;
41: };
42: 
43: struct PandasColumnBindData {
44: 	PandasType pandas_type;
45: 	py::array numpy_col;
46: 	idx_t numpy_stride;
47: 	unique_ptr<NumPyArrayWrapper> mask;
48: 	// Only for categorical types
49: 	string internal_categorical_type;
50: 	// When object types are cast we must hold their data somewhere
51: 	PythonObjectContainer<py::str> object_str_val;
52: };
53: 
54: class VectorConversion {
55: public:
56: 	static void NumpyToDuckDB(PandasColumnBindData &bind_data, py::array &numpy_col, idx_t count, idx_t offset,
57: 	                          Vector &out);
58: 
59: 	static void BindPandas(py::handle df, vector<PandasColumnBindData> &out, vector<LogicalType> &return_types,
60: 	                       vector<string> &names);
61: };
62: 
63: } // namespace duckdb
[end of tools/pythonpkg/src/include/duckdb_python/vector_conversion.hpp]
[start of tools/pythonpkg/src/vector_conversion.cpp]
1: #include "duckdb_python/vector_conversion.hpp"
2: #include "duckdb/common/string_util.hpp"
3: #include "duckdb/common/types/timestamp.hpp"
4: #include "utf8proc_wrapper.hpp"
5: 
6: namespace duckdb {
7: 
8: template <class T>
9: void ScanPandasColumn(py::array &numpy_col, idx_t stride, idx_t offset, Vector &out, idx_t count) {
10: 	auto src_ptr = (T *)numpy_col.data();
11: 	if (stride == sizeof(T)) {
12: 		FlatVector::SetData(out, (data_ptr_t)(src_ptr + offset));
13: 	} else {
14: 		auto tgt_ptr = (T *)FlatVector::GetData(out);
15: 		for (idx_t i = 0; i < count; i++) {
16: 			tgt_ptr[i] = src_ptr[stride / sizeof(T) * (i + offset)];
17: 		}
18: 	}
19: }
20: 
21: template <class T, class V>
22: void ScanPandasCategoryTemplated(py::array &column, idx_t offset, Vector &out, idx_t count) {
23: 	auto src_ptr = (T *)column.data();
24: 	auto tgt_ptr = (V *)FlatVector::GetData(out);
25: 	auto &tgt_mask = FlatVector::Validity(out);
26: 	for (idx_t i = 0; i < count; i++) {
27: 		if (src_ptr[i + offset] == -1) {
28: 			// Null value
29: 			tgt_mask.SetInvalid(i);
30: 		} else {
31: 			tgt_ptr[i] = src_ptr[i + offset];
32: 		}
33: 	}
34: }
35: 
36: template <class T>
37: void ScanPandasCategory(py::array &column, idx_t count, idx_t offset, Vector &out, string &src_type) {
38: 	if (src_type == "int8") {
39: 		ScanPandasCategoryTemplated<int8_t, T>(column, offset, out, count);
40: 	} else if (src_type == "int16") {
41: 		ScanPandasCategoryTemplated<int16_t, T>(column, offset, out, count);
42: 	} else if (src_type == "int32") {
43: 		ScanPandasCategoryTemplated<int32_t, T>(column, offset, out, count);
44: 	} else {
45: 		throw NotImplementedException("The Pandas type " + src_type + " for categorical types is not implemented yet");
46: 	}
47: }
48: 
49: template <class T>
50: void ScanPandasNumeric(PandasColumnBindData &bind_data, idx_t count, idx_t offset, Vector &out) {
51: 	ScanPandasColumn<T>(bind_data.numpy_col, bind_data.numpy_stride, offset, out, count);
52: 	auto &result_mask = FlatVector::Validity(out);
53: 	if (bind_data.mask) {
54: 		auto mask = (bool *)bind_data.mask->numpy_array.data();
55: 		for (idx_t i = 0; i < count; i++) {
56: 			auto is_null = mask[offset + i];
57: 			if (is_null) {
58: 				result_mask.SetInvalid(i);
59: 			}
60: 		}
61: 	}
62: }
63: 
64: template <class T>
65: bool ValueIsNull(T value) {
66: 	throw std::runtime_error("unsupported type for ValueIsNull");
67: }
68: 
69: template <>
70: bool ValueIsNull(float value) {
71: 	return !Value::FloatIsFinite(value);
72: }
73: 
74: template <>
75: bool ValueIsNull(double value) {
76: 	return !Value::DoubleIsFinite(value);
77: }
78: 
79: template <class T>
80: void ScanPandasFpColumn(T *src_ptr, idx_t count, idx_t offset, Vector &out) {
81: 	FlatVector::SetData(out, (data_ptr_t)(src_ptr + offset));
82: 	auto tgt_ptr = FlatVector::GetData<T>(out);
83: 	auto &mask = FlatVector::Validity(out);
84: 	for (idx_t i = 0; i < count; i++) {
85: 		if (ValueIsNull(tgt_ptr[i])) {
86: 			mask.SetInvalid(i);
87: 		}
88: 	}
89: }
90: 
91: template <class T>
92: static string_t DecodePythonUnicode(T *codepoints, idx_t codepoint_count, Vector &out) {
93: 	// first figure out how many bytes to allocate
94: 	idx_t utf8_length = 0;
95: 	for (idx_t i = 0; i < codepoint_count; i++) {
96: 		int len = Utf8Proc::CodepointLength(int(codepoints[i]));
97: 		D_ASSERT(len >= 1);
98: 		utf8_length += len;
99: 	}
100: 	int sz;
101: 	auto result = StringVector::EmptyString(out, utf8_length);
102: 	auto target = result.GetDataWriteable();
103: 	for (idx_t i = 0; i < codepoint_count; i++) {
104: 		Utf8Proc::CodepointToUtf8(int(codepoints[i]), sz, target);
105: 		D_ASSERT(sz >= 1);
106: 		target += sz;
107: 	}
108: 	result.Finalize();
109: 	return result;
110: }
111: 
112: void VectorConversion::NumpyToDuckDB(PandasColumnBindData &bind_data, py::array &numpy_col, idx_t count, idx_t offset,
113:                                      Vector &out) {
114: 	switch (bind_data.pandas_type) {
115: 	case PandasType::BOOLEAN:
116: 		ScanPandasColumn<bool>(numpy_col, bind_data.numpy_stride, offset, out, count);
117: 		break;
118: 	case PandasType::UTINYINT:
119: 		ScanPandasNumeric<uint8_t>(bind_data, count, offset, out);
120: 		break;
121: 	case PandasType::USMALLINT:
122: 		ScanPandasNumeric<uint16_t>(bind_data, count, offset, out);
123: 		break;
124: 	case PandasType::UINTEGER:
125: 		ScanPandasNumeric<uint32_t>(bind_data, count, offset, out);
126: 		break;
127: 	case PandasType::UBIGINT:
128: 		ScanPandasNumeric<uint64_t>(bind_data, count, offset, out);
129: 		break;
130: 	case PandasType::TINYINT:
131: 		ScanPandasNumeric<int8_t>(bind_data, count, offset, out);
132: 		break;
133: 	case PandasType::SMALLINT:
134: 		ScanPandasNumeric<int16_t>(bind_data, count, offset, out);
135: 		break;
136: 	case PandasType::INTEGER:
137: 		ScanPandasNumeric<int32_t>(bind_data, count, offset, out);
138: 		break;
139: 	case PandasType::BIGINT:
140: 		ScanPandasNumeric<int64_t>(bind_data, count, offset, out);
141: 		break;
142: 	case PandasType::FLOAT:
143: 		ScanPandasFpColumn<float>((float *)numpy_col.data(), count, offset, out);
144: 		break;
145: 	case PandasType::DOUBLE:
146: 		ScanPandasFpColumn<double>((double *)numpy_col.data(), count, offset, out);
147: 		break;
148: 	case PandasType::TIMESTAMP: {
149: 		auto src_ptr = (int64_t *)numpy_col.data();
150: 		auto tgt_ptr = FlatVector::GetData<timestamp_t>(out);
151: 		auto &mask = FlatVector::Validity(out);
152: 
153: 		for (idx_t row = 0; row < count; row++) {
154: 			auto source_idx = offset + row;
155: 			if (src_ptr[source_idx] <= NumericLimits<int64_t>::Minimum()) {
156: 				// pandas Not a Time (NaT)
157: 				mask.SetInvalid(row);
158: 				continue;
159: 			}
160: 			tgt_ptr[row] = Timestamp::FromEpochNanoSeconds(src_ptr[source_idx]);
161: 		}
162: 		break;
163: 	}
164: 	case PandasType::INTERVAL: {
165: 		auto src_ptr = (int64_t *)numpy_col.data();
166: 		auto tgt_ptr = FlatVector::GetData<interval_t>(out);
167: 		auto &mask = FlatVector::Validity(out);
168: 
169: 		for (idx_t row = 0; row < count; row++) {
170: 			auto source_idx = offset + row;
171: 			if (src_ptr[source_idx] <= NumericLimits<int64_t>::Minimum()) {
172: 				// pandas Not a Time (NaT)
173: 				mask.SetInvalid(row);
174: 				continue;
175: 			}
176: 			int64_t micro = src_ptr[source_idx] / 1000;
177: 			int64_t days = micro / Interval::MICROS_PER_DAY;
178: 			micro = micro % Interval::MICROS_PER_DAY;
179: 			int64_t months = days / Interval::DAYS_PER_MONTH;
180: 			days = days % Interval::DAYS_PER_MONTH;
181: 			interval_t interval;
182: 			interval.months = months;
183: 			interval.days = days;
184: 			interval.micros = micro;
185: 			tgt_ptr[row] = interval;
186: 		}
187: 		break;
188: 	}
189: 	case PandasType::VARCHAR:
190: 	case PandasType::OBJECT: {
191: 		auto src_ptr = (PyObject **)numpy_col.data();
192: 		auto tgt_ptr = FlatVector::GetData<string_t>(out);
193: 		auto &out_mask = FlatVector::Validity(out);
194: 		unique_ptr<PythonGILWrapper> gil;
195: 		for (idx_t row = 0; row < count; row++) {
196: 			auto source_idx = offset + row;
197: 			PyObject *val = src_ptr[source_idx];
198: 			if (bind_data.pandas_type == PandasType::OBJECT && !PyUnicode_CheckExact(val)) {
199: 				if (val == Py_None) {
200: 					out_mask.SetInvalid(row);
201: 					continue;
202: 				}
203: 				if (py::isinstance<py::float_>(val) && std::isnan(PyFloat_AsDouble(val))) {
204: 					out_mask.SetInvalid(row);
205: 					continue;
206: 				}
207: 				if (!py::isinstance<py::str>(val)) {
208: 					if (!gil) {
209: 						gil = bind_data.object_str_val.GetLock();
210: 					}
211: 					bind_data.object_str_val.AssignInternal<PyObject>(
212: 					    [](py::str &obj, PyObject &new_val) {
213: 						    py::handle object_handle = &new_val;
214: 						    obj = py::str(object_handle);
215: 					    },
216: 					    *val, *gil);
217: 					val = (PyObject *)bind_data.object_str_val.GetPointerTop()->ptr();
218: 				}
219: 			}
220: 			// Python 3 string representation:
221: 			// https://github.com/python/cpython/blob/3a8fdb28794b2f19f6c8464378fb8b46bce1f5f4/Include/cpython/unicodeobject.h#L79
222: 			if (!PyUnicode_CheckExact(val)) {
223: 				out_mask.SetInvalid(row);
224: 				continue;
225: 			}
226: 			if (PyUnicode_IS_COMPACT_ASCII(val)) {
227: 				// ascii string: we can zero copy
228: 				tgt_ptr[row] = string_t((const char *)PyUnicode_DATA(val), PyUnicode_GET_LENGTH(val));
229: 			} else {
230: 				// unicode gunk
231: 				auto ascii_obj = (PyASCIIObject *)val;
232: 				auto unicode_obj = (PyCompactUnicodeObject *)val;
233: 				// compact unicode string: is there utf8 data available?
234: 				if (unicode_obj->utf8) {
235: 					// there is! zero copy
236: 					tgt_ptr[row] = string_t((const char *)unicode_obj->utf8, unicode_obj->utf8_length);
237: 				} else if (PyUnicode_IS_COMPACT(unicode_obj) && !PyUnicode_IS_ASCII(unicode_obj)) {
238: 					auto kind = PyUnicode_KIND(val);
239: 					switch (kind) {
240: 					case PyUnicode_1BYTE_KIND:
241: 						tgt_ptr[row] =
242: 						    DecodePythonUnicode<Py_UCS1>(PyUnicode_1BYTE_DATA(val), PyUnicode_GET_LENGTH(val), out);
243: 						break;
244: 					case PyUnicode_2BYTE_KIND:
245: 						tgt_ptr[row] =
246: 						    DecodePythonUnicode<Py_UCS2>(PyUnicode_2BYTE_DATA(val), PyUnicode_GET_LENGTH(val), out);
247: 						break;
248: 					case PyUnicode_4BYTE_KIND:
249: 						tgt_ptr[row] =
250: 						    DecodePythonUnicode<Py_UCS4>(PyUnicode_4BYTE_DATA(val), PyUnicode_GET_LENGTH(val), out);
251: 						break;
252: 					default:
253: 						throw std::runtime_error("Unsupported typekind for Python Unicode Compact decode");
254: 					}
255: 				} else if (ascii_obj->state.kind == PyUnicode_WCHAR_KIND) {
256: 					throw std::runtime_error("Unsupported: decode not ready legacy string");
257: 				} else if (!PyUnicode_IS_COMPACT(unicode_obj) && ascii_obj->state.kind != PyUnicode_WCHAR_KIND) {
258: 					throw std::runtime_error("Unsupported: decode ready legacy string");
259: 				} else {
260: 					throw std::runtime_error("Unsupported string type: no clue what this string is");
261: 				}
262: 			}
263: 		}
264: 		break;
265: 	}
266: 	case PandasType::CATEGORY: {
267: 		switch (out.GetType().InternalType()) {
268: 		case PhysicalType::UINT8:
269: 			ScanPandasCategory<uint8_t>(numpy_col, count, offset, out, bind_data.internal_categorical_type);
270: 			break;
271: 		case PhysicalType::UINT16:
272: 			ScanPandasCategory<uint16_t>(numpy_col, count, offset, out, bind_data.internal_categorical_type);
273: 			break;
274: 		case PhysicalType::UINT32:
275: 			ScanPandasCategory<uint32_t>(numpy_col, count, offset, out, bind_data.internal_categorical_type);
276: 			break;
277: 		default:
278: 			throw InternalException("Invalid Physical Type for ENUMs");
279: 		}
280: 		break;
281: 	}
282: 
283: 	default:
284: 		throw std::runtime_error("Unsupported type " + out.GetType().ToString());
285: 	}
286: }
287: 
288: static void ConvertPandasType(const string &col_type, LogicalType &duckdb_col_type, PandasType &pandas_type) {
289: 	if (col_type == "bool") {
290: 		duckdb_col_type = LogicalType::BOOLEAN;
291: 		pandas_type = PandasType::BOOLEAN;
292: 	} else if (col_type == "uint8" || col_type == "Uint8") {
293: 		duckdb_col_type = LogicalType::UTINYINT;
294: 		pandas_type = PandasType::UTINYINT;
295: 	} else if (col_type == "uint16" || col_type == "Uint16") {
296: 		duckdb_col_type = LogicalType::USMALLINT;
297: 		pandas_type = PandasType::USMALLINT;
298: 	} else if (col_type == "uint32" || col_type == "Uint32") {
299: 		duckdb_col_type = LogicalType::UINTEGER;
300: 		pandas_type = PandasType::UINTEGER;
301: 	} else if (col_type == "uint64" || col_type == "Uint64") {
302: 		duckdb_col_type = LogicalType::UBIGINT;
303: 		pandas_type = PandasType::UBIGINT;
304: 	} else if (col_type == "int8" || col_type == "Int8") {
305: 		duckdb_col_type = LogicalType::TINYINT;
306: 		pandas_type = PandasType::TINYINT;
307: 	} else if (col_type == "int16" || col_type == "Int16") {
308: 		duckdb_col_type = LogicalType::SMALLINT;
309: 		pandas_type = PandasType::SMALLINT;
310: 	} else if (col_type == "int32" || col_type == "Int32") {
311: 		duckdb_col_type = LogicalType::INTEGER;
312: 		pandas_type = PandasType::INTEGER;
313: 	} else if (col_type == "int64" || col_type == "Int64") {
314: 		duckdb_col_type = LogicalType::BIGINT;
315: 		pandas_type = PandasType::BIGINT;
316: 	} else if (col_type == "float32") {
317: 		duckdb_col_type = LogicalType::FLOAT;
318: 		pandas_type = PandasType::FLOAT;
319: 	} else if (col_type == "float64") {
320: 		duckdb_col_type = LogicalType::DOUBLE;
321: 		pandas_type = PandasType::DOUBLE;
322: 	} else if (col_type == "object") {
323: 		//! this better be castable to strings
324: 		duckdb_col_type = LogicalType::VARCHAR;
325: 		pandas_type = PandasType::OBJECT;
326: 	} else if (col_type == "string") {
327: 		duckdb_col_type = LogicalType::VARCHAR;
328: 		pandas_type = PandasType::VARCHAR;
329: 	} else if (col_type == "timedelta64[ns]") {
330: 		duckdb_col_type = LogicalType::INTERVAL;
331: 		pandas_type = PandasType::INTERVAL;
332: 	} else {
333: 		throw std::runtime_error("unsupported python type " + col_type);
334: 	}
335: }
336: 
337: void VectorConversion::BindPandas(py::handle original_df, vector<PandasColumnBindData> &bind_columns,
338:                                   vector<LogicalType> &return_types, vector<string> &names) {
339: 	// This performs a shallow copy that allows us to rename the dataframe
340: 	auto df = original_df.attr("copy")(false);
341: 	auto df_columns = py::list(df.attr("columns"));
342: 	auto df_types = py::list(df.attr("dtypes"));
343: 	auto get_fun = df.attr("__getitem__");
344: 	// TODO support masked arrays as well
345: 	// TODO support dicts of numpy arrays as well
346: 	if (py::len(df_columns) == 0 || py::len(df_types) == 0 || py::len(df_columns) != py::len(df_types)) {
347: 		throw std::runtime_error("Need a DataFrame with at least one column");
348: 	}
349: 
350: 	// check if names in pandas dataframe are unique
351: 	unordered_map<string, idx_t> pandas_column_names_map;
352: 	py::array column_attributes = df.attr("columns").attr("values");
353: 	for (idx_t col_idx = 0; col_idx < py::len(df_columns); col_idx++) {
354: 		auto column_name_py = py::str(df_columns[col_idx]);
355: 		pandas_column_names_map[column_name_py]++;
356: 		if (pandas_column_names_map[column_name_py] > 1) {
357: 			// If the column name is repeated we start adding _x where x is the repetition number
358: 			string column_name = column_name_py;
359: 			column_name += "_" + to_string(pandas_column_names_map[column_name_py] - 1);
360: 			auto new_column_name_py = py::str(column_name);
361: 			names.emplace_back(new_column_name_py);
362: 			column_attributes[py::cast(col_idx)] = new_column_name_py;
363: 			pandas_column_names_map[new_column_name_py]++;
364: 		} else {
365: 			names.emplace_back(column_name_py);
366: 		}
367: 	}
368: 
369: 	for (idx_t col_idx = 0; col_idx < py::len(df_columns); col_idx++) {
370: 		LogicalType duckdb_col_type;
371: 		PandasColumnBindData bind_data;
372: 		auto col_type = string(py::str(df_types[col_idx]));
373: 		if (col_type == "Int8" || col_type == "Int16" || col_type == "Int32" || col_type == "Int64") {
374: 			// numeric object
375: 			// fetch the internal data and mask array
376: 			bind_data.numpy_col = get_fun(df_columns[col_idx]).attr("array").attr("_data");
377: 			bind_data.mask = make_unique<NumPyArrayWrapper>(get_fun(df_columns[col_idx]).attr("array").attr("_mask"));
378: 			ConvertPandasType(col_type, duckdb_col_type, bind_data.pandas_type);
379: 		} else if (StringUtil::StartsWith(col_type, "datetime64[ns") || col_type == "<M8[ns]") {
380: 			// timestamp type
381: 			bind_data.numpy_col = get_fun(df_columns[col_idx]).attr("array").attr("_data");
382: 			bind_data.mask = nullptr;
383: 			duckdb_col_type = LogicalType::TIMESTAMP;
384: 			bind_data.pandas_type = PandasType::TIMESTAMP;
385: 		} else {
386: 			// regular type
387: 			auto column = get_fun(df_columns[col_idx]);
388: 			if (col_type == "category") {
389: 				// for category types, we create an ENUM type for string or use the converted numpy type for the rest
390: 				D_ASSERT(py::hasattr(column, "cat"));
391: 				D_ASSERT(py::hasattr(column.attr("cat"), "categories"));
392: 				auto categories = py::array(column.attr("cat").attr("categories"));
393: 				auto category_type = string(py::str(categories.attr("dtype")));
394: 				if (category_type == "object") {
395: 					// Let's hope the object type is a string.
396: 					bind_data.pandas_type = PandasType::CATEGORY;
397: 					auto enum_name = string(py::str(df_columns[col_idx]));
398: 					vector<string> enum_entries = py::cast<vector<string>>(categories);
399: 					idx_t size = enum_entries.size();
400: 					Vector enum_entries_vec(LogicalType::VARCHAR, size);
401: 					auto enum_entries_ptr = FlatVector::GetData<string_t>(enum_entries_vec);
402: 					for (idx_t i = 0; i < size; i++) {
403: 						enum_entries_ptr[i] = StringVector::AddStringOrBlob(enum_entries_vec, enum_entries[i]);
404: 					}
405: 					D_ASSERT(py::hasattr(column.attr("cat"), "codes"));
406: 					duckdb_col_type = LogicalType::ENUM(enum_name, enum_entries_vec, size);
407: 					bind_data.numpy_col = py::array(column.attr("cat").attr("codes"));
408: 					bind_data.mask = nullptr;
409: 					D_ASSERT(py::hasattr(bind_data.numpy_col, "dtype"));
410: 					bind_data.internal_categorical_type = string(py::str(bind_data.numpy_col.attr("dtype")));
411: 				} else {
412: 					bind_data.numpy_col = py::array(column.attr("to_numpy")());
413: 					bind_data.mask = nullptr;
414: 					auto numpy_type = bind_data.numpy_col.attr("dtype");
415: 					// for category types (non-strings), we use the converted numpy type
416: 					category_type = string(py::str(numpy_type));
417: 					ConvertPandasType(category_type, duckdb_col_type, bind_data.pandas_type);
418: 				}
419: 			} else {
420: 				bind_data.numpy_col = py::array(column.attr("to_numpy")());
421: 				bind_data.mask = nullptr;
422: 				ConvertPandasType(col_type, duckdb_col_type, bind_data.pandas_type);
423: 			}
424: 		}
425: 		D_ASSERT(py::hasattr(bind_data.numpy_col, "strides"));
426: 		bind_data.numpy_stride = bind_data.numpy_col.attr("strides").attr("__getitem__")(0).cast<idx_t>();
427: 		return_types.push_back(duckdb_col_type);
428: 		bind_columns.push_back(move(bind_data));
429: 	}
430: }
431: } // namespace duckdb
[end of tools/pythonpkg/src/vector_conversion.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: