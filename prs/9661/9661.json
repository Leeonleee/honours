{
  "repo": "duckdb/duckdb",
  "pull_number": 9661,
  "instance_id": "duckdb__duckdb-9661",
  "issue_numbers": [
    "5547"
  ],
  "base_commit": "c92602b864a09717b56843ae0811d85914509a9a",
  "patch": "diff --git a/src/function/table/arrow_conversion.cpp b/src/function/table/arrow_conversion.cpp\nindex 204078357e43..7aaede6f5725 100644\n--- a/src/function/table/arrow_conversion.cpp\n+++ b/src/function/table/arrow_conversion.cpp\n@@ -18,13 +18,26 @@ static void ShiftRight(unsigned char *ar, int size, int shift) {\n \t}\n }\n \n+idx_t GetEffectiveOffset(ArrowArray &array, int64_t parent_offset, ArrowScanLocalState &state,\n+                         int64_t nested_offset = -1) {\n+\tif (nested_offset != -1) {\n+\t\t// The parent of this array is a list\n+\t\t// We just ignore the parent offset, it's already applied to the list\n+\t\treturn array.offset + nested_offset;\n+\t}\n+\t// Parent offset is set in the case of a struct, it applies to all child arrays\n+\t// 'chunk_offset' is how much of the chunk we've already scanned, in case the chunk size exceeds\n+\t// STANDARD_VECTOR_SIZE\n+\treturn array.offset + parent_offset + state.chunk_offset;\n+}\n+\n template <class T>\n T *ArrowBufferData(ArrowArray &array, idx_t buffer_idx) {\n \treturn (T *)array.buffers[buffer_idx]; // NOLINT\n }\n \n static void GetValidityMask(ValidityMask &mask, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n-                            int64_t nested_offset = -1, bool add_null = false) {\n+                            int64_t parent_offset, int64_t nested_offset = -1, bool add_null = false) {\n \t// In certains we don't need to or cannot copy arrow's validity mask to duckdb.\n \t//\n \t// The conditions where we do want to copy arrow's mask to duckdb are:\n@@ -32,10 +45,7 @@ static void GetValidityMask(ValidityMask &mask, ArrowArray &array, ArrowScanLoca\n \t// 2. n_buffers > 0, meaning the array's arrow type is not `null`\n \t// 3. the validity buffer (the first buffer) is not a nullptr\n \tif (array.null_count != 0 && array.n_buffers > 0 && array.buffers[0]) {\n-\t\tauto bit_offset = scan_state.chunk_offset + array.offset;\n-\t\tif (nested_offset != -1) {\n-\t\t\tbit_offset = nested_offset;\n-\t\t}\n+\t\tauto bit_offset = GetEffectiveOffset(array, parent_offset, scan_state, nested_offset);\n \t\tmask.EnsureWritable();\n #if STANDARD_VECTOR_SIZE > 64\n \t\tauto n_bitmask_bytes = (size + 8 - 1) / 8;\n@@ -74,10 +84,10 @@ static void GetValidityMask(ValidityMask &mask, ArrowArray &array, ArrowScanLoca\n }\n \n static void SetValidityMask(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n-                            int64_t nested_offset, bool add_null = false) {\n+                            int64_t parent_offset, int64_t nested_offset, bool add_null = false) {\n \tD_ASSERT(vector.GetVectorType() == VectorType::FLAT_VECTOR);\n \tauto &mask = FlatVector::Validity(vector);\n-\tGetValidityMask(mask, array, scan_state, size, nested_offset, add_null);\n+\tGetValidityMask(mask, array, scan_state, size, parent_offset, nested_offset, add_null);\n }\n \n static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowArrayScanState &array_state, idx_t size,\n@@ -89,21 +99,19 @@ static void ColumnArrowToDuckDBDictionary(Vector &vector, ArrowArray &array, Arr\n                                           ValidityMask *parent_mask = nullptr, uint64_t parent_offset = 0);\n \n static void ArrowToDuckDBList(Vector &vector, ArrowArray &array, ArrowArrayScanState &array_state, idx_t size,\n-                              const ArrowType &arrow_type, int64_t nested_offset, ValidityMask *parent_mask) {\n+                              const ArrowType &arrow_type, int64_t nested_offset, ValidityMask *parent_mask,\n+                              int64_t parent_offset) {\n \tauto size_type = arrow_type.GetSizeType();\n \tidx_t list_size = 0;\n \tauto &scan_state = array_state.state;\n \n-\tSetValidityMask(vector, array, scan_state, size, nested_offset);\n+\tSetValidityMask(vector, array, scan_state, size, parent_offset, nested_offset);\n \tidx_t start_offset = 0;\n \tidx_t cur_offset = 0;\n \tif (size_type == ArrowVariableSizeType::FIXED_SIZE) {\n \t\tauto fixed_size = arrow_type.FixedSize();\n \t\t//! Have to check validity mask before setting this up\n-\t\tidx_t offset = (scan_state.chunk_offset + array.offset) * fixed_size;\n-\t\tif (nested_offset != -1) {\n-\t\t\toffset = fixed_size * nested_offset;\n-\t\t}\n+\t\tidx_t offset = GetEffectiveOffset(array, parent_offset, scan_state, nested_offset) * fixed_size;\n \t\tstart_offset = offset;\n \t\tauto list_data = FlatVector::GetData<list_entry_t>(vector);\n \t\tfor (idx_t i = 0; i < size; i++) {\n@@ -114,10 +122,8 @@ static void ArrowToDuckDBList(Vector &vector, ArrowArray &array, ArrowArrayScanS\n \t\t}\n \t\tlist_size = start_offset + cur_offset;\n \t} else if (size_type == ArrowVariableSizeType::NORMAL) {\n-\t\tauto offsets = ArrowBufferData<uint32_t>(array, 1) + array.offset + scan_state.chunk_offset;\n-\t\tif (nested_offset != -1) {\n-\t\t\toffsets = ArrowBufferData<uint32_t>(array, 1) + nested_offset;\n-\t\t}\n+\t\tauto offsets =\n+\t\t    ArrowBufferData<uint32_t>(array, 1) + GetEffectiveOffset(array, parent_offset, scan_state, nested_offset);\n \t\tstart_offset = offsets[0];\n \t\tauto list_data = FlatVector::GetData<list_entry_t>(vector);\n \t\tfor (idx_t i = 0; i < size; i++) {\n@@ -128,10 +134,8 @@ static void ArrowToDuckDBList(Vector &vector, ArrowArray &array, ArrowArrayScanS\n \t\t}\n \t\tlist_size = offsets[size];\n \t} else {\n-\t\tauto offsets = ArrowBufferData<uint64_t>(array, 1) + array.offset + scan_state.chunk_offset;\n-\t\tif (nested_offset != -1) {\n-\t\t\toffsets = ArrowBufferData<uint64_t>(array, 1) + nested_offset;\n-\t\t}\n+\t\tauto offsets =\n+\t\t    ArrowBufferData<uint64_t>(array, 1) + GetEffectiveOffset(array, parent_offset, scan_state, nested_offset);\n \t\tstart_offset = offsets[0];\n \t\tauto list_data = FlatVector::GetData<list_entry_t>(vector);\n \t\tfor (idx_t i = 0; i < size; i++) {\n@@ -146,7 +150,7 @@ static void ArrowToDuckDBList(Vector &vector, ArrowArray &array, ArrowArrayScanS\n \tListVector::Reserve(vector, list_size);\n \tListVector::SetListSize(vector, list_size);\n \tauto &child_vector = ListVector::GetEntry(vector);\n-\tSetValidityMask(child_vector, *array.children[0], scan_state, list_size, start_offset);\n+\tSetValidityMask(child_vector, *array.children[0], scan_state, list_size, array.offset, start_offset);\n \tauto &list_mask = FlatVector::Validity(vector);\n \tif (parent_mask) {\n \t\t//! Since this List is owned by a struct we must guarantee their validity map matches on Null\n@@ -175,16 +179,13 @@ static void ArrowToDuckDBList(Vector &vector, ArrowArray &array, ArrowArrayScanS\n }\n \n static void ArrowToDuckDBBlob(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, idx_t size,\n-                              const ArrowType &arrow_type, int64_t nested_offset) {\n+                              const ArrowType &arrow_type, int64_t nested_offset, int64_t parent_offset) {\n \tauto size_type = arrow_type.GetSizeType();\n-\tSetValidityMask(vector, array, scan_state, size, nested_offset);\n+\tSetValidityMask(vector, array, scan_state, size, parent_offset, nested_offset);\n \tif (size_type == ArrowVariableSizeType::FIXED_SIZE) {\n \t\tauto fixed_size = arrow_type.FixedSize();\n \t\t//! Have to check validity mask before setting this up\n-\t\tidx_t offset = (scan_state.chunk_offset + array.offset) * fixed_size;\n-\t\tif (nested_offset != -1) {\n-\t\t\toffset = fixed_size * nested_offset;\n-\t\t}\n+\t\tidx_t offset = GetEffectiveOffset(array, parent_offset, scan_state, nested_offset) * fixed_size;\n \t\tauto cdata = ArrowBufferData<char>(array, 1);\n \t\tfor (idx_t row_idx = 0; row_idx < size; row_idx++) {\n \t\t\tif (FlatVector::IsNull(vector, row_idx)) {\n@@ -196,10 +197,8 @@ static void ArrowToDuckDBBlob(Vector &vector, ArrowArray &array, ArrowScanLocalS\n \t\t\toffset += blob_len;\n \t\t}\n \t} else if (size_type == ArrowVariableSizeType::NORMAL) {\n-\t\tauto offsets = ArrowBufferData<uint32_t>(array, 1) + array.offset + scan_state.chunk_offset;\n-\t\tif (nested_offset != -1) {\n-\t\t\toffsets = ArrowBufferData<uint32_t>(array, 1) + array.offset + nested_offset;\n-\t\t}\n+\t\tauto offsets =\n+\t\t    ArrowBufferData<uint32_t>(array, 1) + GetEffectiveOffset(array, parent_offset, scan_state, nested_offset);\n \t\tauto cdata = ArrowBufferData<char>(array, 2);\n \t\tfor (idx_t row_idx = 0; row_idx < size; row_idx++) {\n \t\t\tif (FlatVector::IsNull(vector, row_idx)) {\n@@ -214,10 +213,8 @@ static void ArrowToDuckDBBlob(Vector &vector, ArrowArray &array, ArrowScanLocalS\n \t\tif (ArrowBufferData<uint64_t>(array, 1)[array.length] > NumericLimits<uint32_t>::Maximum()) { // LCOV_EXCL_START\n \t\t\tthrow ConversionException(\"DuckDB does not support Blobs over 4GB\");\n \t\t} // LCOV_EXCL_STOP\n-\t\tauto offsets = ArrowBufferData<uint64_t>(array, 1) + array.offset + scan_state.chunk_offset;\n-\t\tif (nested_offset != -1) {\n-\t\t\toffsets = ArrowBufferData<uint64_t>(array, 1) + array.offset + nested_offset;\n-\t\t}\n+\t\tauto offsets =\n+\t\t    ArrowBufferData<uint64_t>(array, 1) + GetEffectiveOffset(array, parent_offset, scan_state, nested_offset);\n \t\tauto cdata = ArrowBufferData<char>(array, 2);\n \t\tfor (idx_t row_idx = 0; row_idx < size; row_idx++) {\n \t\t\tif (FlatVector::IsNull(vector, row_idx)) {\n@@ -269,23 +266,17 @@ static void SetVectorString(Vector &vector, idx_t size, char *cdata, T *offsets)\n static void DirectConversion(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, int64_t nested_offset,\n                              uint64_t parent_offset) {\n \tauto internal_type = GetTypeIdSize(vector.GetType().InternalType());\n-\tauto data_ptr =\n-\t    ArrowBufferData<data_t>(array, 1) + internal_type * (scan_state.chunk_offset + array.offset + parent_offset);\n-\tif (nested_offset != -1) {\n-\t\tdata_ptr = ArrowBufferData<data_t>(array, 1) + internal_type * (array.offset + nested_offset + parent_offset);\n-\t}\n+\tauto data_ptr = ArrowBufferData<data_t>(array, 1) +\n+\t                internal_type * GetEffectiveOffset(array, parent_offset, scan_state, nested_offset);\n \tFlatVector::SetData(vector, data_ptr);\n }\n \n template <class T>\n static void TimeConversion(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state, int64_t nested_offset,\n-                           idx_t size, int64_t conversion) {\n+                           int64_t parent_offset, idx_t size, int64_t conversion) {\n \tauto tgt_ptr = FlatVector::GetData<dtime_t>(vector);\n \tauto &validity_mask = FlatVector::Validity(vector);\n-\tauto src_ptr = (T *)array.buffers[1] + scan_state.chunk_offset + array.offset;\n-\tif (nested_offset != -1) {\n-\t\tsrc_ptr = (T *)array.buffers[1] + nested_offset + array.offset;\n-\t}\n+\tauto src_ptr = (T *)array.buffers[1] + GetEffectiveOffset(array, parent_offset, scan_state, nested_offset);\n \tfor (idx_t row = 0; row < size; row++) {\n \t\tif (!validity_mask.RowIsValid(row)) {\n \t\t\tcontinue;\n@@ -297,13 +288,11 @@ static void TimeConversion(Vector &vector, ArrowArray &array, ArrowScanLocalStat\n }\n \n static void TimestampTZConversion(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state,\n-                                  int64_t nested_offset, idx_t size, int64_t conversion) {\n+                                  int64_t nested_offset, int64_t parent_offset, idx_t size, int64_t conversion) {\n \tauto tgt_ptr = FlatVector::GetData<timestamp_t>(vector);\n \tauto &validity_mask = FlatVector::Validity(vector);\n-\tauto src_ptr = ArrowBufferData<int64_t>(array, 1) + scan_state.chunk_offset + array.offset;\n-\tif (nested_offset != -1) {\n-\t\tsrc_ptr = ArrowBufferData<int64_t>(array, 1) + nested_offset + array.offset;\n-\t}\n+\tauto src_ptr =\n+\t    ArrowBufferData<int64_t>(array, 1) + GetEffectiveOffset(array, parent_offset, scan_state, nested_offset);\n \tfor (idx_t row = 0; row < size; row++) {\n \t\tif (!validity_mask.RowIsValid(row)) {\n \t\t\tcontinue;\n@@ -315,12 +304,10 @@ static void TimestampTZConversion(Vector &vector, ArrowArray &array, ArrowScanLo\n }\n \n static void IntervalConversionUs(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state,\n-                                 int64_t nested_offset, idx_t size, int64_t conversion) {\n+                                 int64_t nested_offset, int64_t parent_offset, idx_t size, int64_t conversion) {\n \tauto tgt_ptr = FlatVector::GetData<interval_t>(vector);\n-\tauto src_ptr = ArrowBufferData<int64_t>(array, 1) + scan_state.chunk_offset + array.offset;\n-\tif (nested_offset != -1) {\n-\t\tsrc_ptr = ArrowBufferData<int64_t>(array, 1) + nested_offset + array.offset;\n-\t}\n+\tauto src_ptr =\n+\t    ArrowBufferData<int64_t>(array, 1) + GetEffectiveOffset(array, parent_offset, scan_state, nested_offset);\n \tfor (idx_t row = 0; row < size; row++) {\n \t\ttgt_ptr[row].days = 0;\n \t\ttgt_ptr[row].months = 0;\n@@ -331,12 +318,10 @@ static void IntervalConversionUs(Vector &vector, ArrowArray &array, ArrowScanLoc\n }\n \n static void IntervalConversionMonths(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state,\n-                                     int64_t nested_offset, idx_t size) {\n+                                     int64_t nested_offset, int64_t parent_offset, idx_t size) {\n \tauto tgt_ptr = FlatVector::GetData<interval_t>(vector);\n-\tauto src_ptr = ArrowBufferData<int32_t>(array, 1) + scan_state.chunk_offset + array.offset;\n-\tif (nested_offset != -1) {\n-\t\tsrc_ptr = ArrowBufferData<int32_t>(array, 1) + nested_offset + array.offset;\n-\t}\n+\tauto src_ptr =\n+\t    ArrowBufferData<int32_t>(array, 1) + GetEffectiveOffset(array, parent_offset, scan_state, nested_offset);\n \tfor (idx_t row = 0; row < size; row++) {\n \t\ttgt_ptr[row].days = 0;\n \t\ttgt_ptr[row].micros = 0;\n@@ -345,12 +330,10 @@ static void IntervalConversionMonths(Vector &vector, ArrowArray &array, ArrowSca\n }\n \n static void IntervalConversionMonthDayNanos(Vector &vector, ArrowArray &array, ArrowScanLocalState &scan_state,\n-                                            int64_t nested_offset, idx_t size) {\n+                                            int64_t nested_offset, int64_t parent_offset, idx_t size) {\n \tauto tgt_ptr = FlatVector::GetData<interval_t>(vector);\n-\tauto src_ptr = ArrowBufferData<ArrowInterval>(array, 1) + scan_state.chunk_offset + array.offset;\n-\tif (nested_offset != -1) {\n-\t\tsrc_ptr = ArrowBufferData<ArrowInterval>(array, 1) + nested_offset + array.offset;\n-\t}\n+\tauto src_ptr =\n+\t    ArrowBufferData<ArrowInterval>(array, 1) + GetEffectiveOffset(array, parent_offset, scan_state, nested_offset);\n \tfor (idx_t row = 0; row < size; row++) {\n \t\ttgt_ptr[row].days = src_ptr[row].days;\n \t\ttgt_ptr[row].micros = src_ptr[row].nanoseconds / Interval::NANOS_PER_MICRO;\n@@ -361,6 +344,9 @@ static void IntervalConversionMonthDayNanos(Vector &vector, ArrowArray &array, A\n static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowArrayScanState &array_state, idx_t size,\n                                 const ArrowType &arrow_type, int64_t nested_offset, ValidityMask *parent_mask,\n                                 uint64_t parent_offset) {\n+\tif (parent_offset != 0) {\n+\t\t(void)array_state;\n+\t}\n \tauto &scan_state = array_state.state;\n \tD_ASSERT(!array.dictionary);\n \tswitch (vector.GetType().id()) {\n@@ -370,11 +356,8 @@ static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowArraySca\n \tcase LogicalTypeId::BOOLEAN: {\n \t\t//! Arrow bit-packs boolean values\n \t\t//! Lets first figure out where we are in the source array\n-\t\tauto src_ptr = ArrowBufferData<uint8_t>(array, 1) + (scan_state.chunk_offset + array.offset) / 8;\n-\n-\t\tif (nested_offset != -1) {\n-\t\t\tsrc_ptr = ArrowBufferData<uint8_t>(array, 1) + (nested_offset + array.offset) / 8;\n-\t\t}\n+\t\tauto src_ptr = ArrowBufferData<uint8_t>(array, 1) +\n+\t\t               GetEffectiveOffset(array, parent_offset, scan_state, nested_offset) / 8;\n \t\tauto tgt_ptr = (uint8_t *)FlatVector::GetData(vector);\n \t\tint src_pos = 0;\n \t\tidx_t cur_bit = scan_state.chunk_offset % 8;\n@@ -417,16 +400,12 @@ static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowArraySca\n \t\tauto size_type = arrow_type.GetSizeType();\n \t\tauto cdata = ArrowBufferData<char>(array, 2);\n \t\tif (size_type == ArrowVariableSizeType::SUPER_SIZE) {\n-\t\t\tauto offsets = ArrowBufferData<uint64_t>(array, 1) + array.offset + scan_state.chunk_offset;\n-\t\t\tif (nested_offset != -1) {\n-\t\t\t\toffsets = ArrowBufferData<uint64_t>(array, 1) + array.offset + nested_offset;\n-\t\t\t}\n+\t\t\tauto offsets = ArrowBufferData<uint64_t>(array, 1) +\n+\t\t\t               GetEffectiveOffset(array, parent_offset, scan_state, nested_offset);\n \t\t\tSetVectorString(vector, size, cdata, offsets);\n \t\t} else {\n-\t\t\tauto offsets = ArrowBufferData<uint32_t>(array, 1) + array.offset + scan_state.chunk_offset;\n-\t\t\tif (nested_offset != -1) {\n-\t\t\t\toffsets = ArrowBufferData<uint32_t>(array, 1) + array.offset + nested_offset;\n-\t\t\t}\n+\t\t\tauto offsets = ArrowBufferData<uint32_t>(array, 1) +\n+\t\t\t               GetEffectiveOffset(array, parent_offset, scan_state, nested_offset);\n \t\t\tSetVectorString(vector, size, cdata, offsets);\n \t\t}\n \t\tbreak;\n@@ -441,10 +420,8 @@ static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowArraySca\n \t\t}\n \t\tcase ArrowDateTimeType::MILLISECONDS: {\n \t\t\t//! convert date from nanoseconds to days\n-\t\t\tauto src_ptr = ArrowBufferData<uint64_t>(array, 1) + scan_state.chunk_offset + array.offset;\n-\t\t\tif (nested_offset != -1) {\n-\t\t\t\tsrc_ptr = ArrowBufferData<uint64_t>(array, 1) + nested_offset + array.offset;\n-\t\t\t}\n+\t\t\tauto src_ptr = ArrowBufferData<uint64_t>(array, 1) +\n+\t\t\t               GetEffectiveOffset(array, parent_offset, scan_state, nested_offset);\n \t\t\tauto tgt_ptr = FlatVector::GetData<date_t>(vector);\n \t\t\tfor (idx_t row = 0; row < size; row++) {\n \t\t\t\ttgt_ptr[row] = date_t(int64_t(src_ptr[row]) / static_cast<int64_t>(1000 * 60 * 60 * 24));\n@@ -460,23 +437,21 @@ static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowArraySca\n \t\tauto precision = arrow_type.GetDateTimeType();\n \t\tswitch (precision) {\n \t\tcase ArrowDateTimeType::SECONDS: {\n-\t\t\tTimeConversion<int32_t>(vector, array, scan_state, nested_offset, size, 1000000);\n+\t\t\tTimeConversion<int32_t>(vector, array, scan_state, nested_offset, parent_offset, size, 1000000);\n \t\t\tbreak;\n \t\t}\n \t\tcase ArrowDateTimeType::MILLISECONDS: {\n-\t\t\tTimeConversion<int32_t>(vector, array, scan_state, nested_offset, size, 1000);\n+\t\t\tTimeConversion<int32_t>(vector, array, scan_state, nested_offset, parent_offset, size, 1000);\n \t\t\tbreak;\n \t\t}\n \t\tcase ArrowDateTimeType::MICROSECONDS: {\n-\t\t\tTimeConversion<int64_t>(vector, array, scan_state, nested_offset, size, 1);\n+\t\t\tTimeConversion<int64_t>(vector, array, scan_state, nested_offset, parent_offset, size, 1);\n \t\t\tbreak;\n \t\t}\n \t\tcase ArrowDateTimeType::NANOSECONDS: {\n \t\t\tauto tgt_ptr = FlatVector::GetData<dtime_t>(vector);\n-\t\t\tauto src_ptr = ArrowBufferData<int64_t>(array, 1) + scan_state.chunk_offset + array.offset;\n-\t\t\tif (nested_offset != -1) {\n-\t\t\t\tsrc_ptr = ArrowBufferData<int64_t>(array, 1) + nested_offset + array.offset;\n-\t\t\t}\n+\t\t\tauto src_ptr = ArrowBufferData<int64_t>(array, 1) +\n+\t\t\t               GetEffectiveOffset(array, parent_offset, scan_state, nested_offset);\n \t\t\tfor (idx_t row = 0; row < size; row++) {\n \t\t\t\ttgt_ptr[row].micros = src_ptr[row] / 1000;\n \t\t\t}\n@@ -491,11 +466,11 @@ static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowArraySca\n \t\tauto precision = arrow_type.GetDateTimeType();\n \t\tswitch (precision) {\n \t\tcase ArrowDateTimeType::SECONDS: {\n-\t\t\tTimestampTZConversion(vector, array, scan_state, nested_offset, size, 1000000);\n+\t\t\tTimestampTZConversion(vector, array, scan_state, nested_offset, parent_offset, size, 1000000);\n \t\t\tbreak;\n \t\t}\n \t\tcase ArrowDateTimeType::MILLISECONDS: {\n-\t\t\tTimestampTZConversion(vector, array, scan_state, nested_offset, size, 1000);\n+\t\t\tTimestampTZConversion(vector, array, scan_state, nested_offset, parent_offset, size, 1000);\n \t\t\tbreak;\n \t\t}\n \t\tcase ArrowDateTimeType::MICROSECONDS: {\n@@ -504,10 +479,8 @@ static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowArraySca\n \t\t}\n \t\tcase ArrowDateTimeType::NANOSECONDS: {\n \t\t\tauto tgt_ptr = FlatVector::GetData<timestamp_t>(vector);\n-\t\t\tauto src_ptr = ArrowBufferData<int64_t>(array, 1) + scan_state.chunk_offset + array.offset;\n-\t\t\tif (nested_offset != -1) {\n-\t\t\t\tsrc_ptr = ArrowBufferData<int64_t>(array, 1) + nested_offset + array.offset;\n-\t\t\t}\n+\t\t\tauto src_ptr = ArrowBufferData<int64_t>(array, 1) +\n+\t\t\t               GetEffectiveOffset(array, parent_offset, scan_state, nested_offset);\n \t\t\tfor (idx_t row = 0; row < size; row++) {\n \t\t\t\ttgt_ptr[row].value = src_ptr[row] / 1000;\n \t\t\t}\n@@ -522,24 +495,22 @@ static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowArraySca\n \t\tauto precision = arrow_type.GetDateTimeType();\n \t\tswitch (precision) {\n \t\tcase ArrowDateTimeType::SECONDS: {\n-\t\t\tIntervalConversionUs(vector, array, scan_state, nested_offset, size, 1000000);\n+\t\t\tIntervalConversionUs(vector, array, scan_state, nested_offset, parent_offset, size, 1000000);\n \t\t\tbreak;\n \t\t}\n \t\tcase ArrowDateTimeType::DAYS:\n \t\tcase ArrowDateTimeType::MILLISECONDS: {\n-\t\t\tIntervalConversionUs(vector, array, scan_state, nested_offset, size, 1000);\n+\t\t\tIntervalConversionUs(vector, array, scan_state, nested_offset, parent_offset, size, 1000);\n \t\t\tbreak;\n \t\t}\n \t\tcase ArrowDateTimeType::MICROSECONDS: {\n-\t\t\tIntervalConversionUs(vector, array, scan_state, nested_offset, size, 1);\n+\t\t\tIntervalConversionUs(vector, array, scan_state, nested_offset, parent_offset, size, 1);\n \t\t\tbreak;\n \t\t}\n \t\tcase ArrowDateTimeType::NANOSECONDS: {\n \t\t\tauto tgt_ptr = FlatVector::GetData<interval_t>(vector);\n-\t\t\tauto src_ptr = ArrowBufferData<int64_t>(array, 1) + scan_state.chunk_offset + array.offset;\n-\t\t\tif (nested_offset != -1) {\n-\t\t\t\tsrc_ptr = ArrowBufferData<int64_t>(array, 1) + nested_offset + array.offset;\n-\t\t\t}\n+\t\t\tauto src_ptr = ArrowBufferData<int64_t>(array, 1) +\n+\t\t\t               GetEffectiveOffset(array, parent_offset, scan_state, nested_offset);\n \t\t\tfor (idx_t row = 0; row < size; row++) {\n \t\t\t\ttgt_ptr[row].micros = src_ptr[row] / 1000;\n \t\t\t\ttgt_ptr[row].days = 0;\n@@ -548,11 +519,11 @@ static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowArraySca\n \t\t\tbreak;\n \t\t}\n \t\tcase ArrowDateTimeType::MONTHS: {\n-\t\t\tIntervalConversionMonths(vector, array, scan_state, nested_offset, size);\n+\t\t\tIntervalConversionMonths(vector, array, scan_state, nested_offset, parent_offset, size);\n \t\t\tbreak;\n \t\t}\n \t\tcase ArrowDateTimeType::MONTH_DAY_NANO: {\n-\t\t\tIntervalConversionMonthDayNanos(vector, array, scan_state, nested_offset, size);\n+\t\t\tIntervalConversionMonthDayNanos(vector, array, scan_state, nested_offset, parent_offset, size);\n \t\t\tbreak;\n \t\t}\n \t\tdefault:\n@@ -563,10 +534,8 @@ static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowArraySca\n \tcase LogicalTypeId::DECIMAL: {\n \t\tauto val_mask = FlatVector::Validity(vector);\n \t\t//! We have to convert from INT128\n-\t\tauto src_ptr = ArrowBufferData<hugeint_t>(array, 1) + scan_state.chunk_offset + array.offset;\n-\t\tif (nested_offset != -1) {\n-\t\t\tsrc_ptr = ArrowBufferData<hugeint_t>(array, 1) + nested_offset + array.offset;\n-\t\t}\n+\t\tauto src_ptr =\n+\t\t    ArrowBufferData<hugeint_t>(array, 1) + GetEffectiveOffset(array, parent_offset, scan_state, nested_offset);\n \t\tswitch (vector.GetType().InternalType()) {\n \t\tcase PhysicalType::INT16: {\n \t\t\tauto tgt_ptr = FlatVector::GetData<int16_t>(vector);\n@@ -602,9 +571,9 @@ static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowArraySca\n \t\t\tbreak;\n \t\t}\n \t\tcase PhysicalType::INT128: {\n-\t\t\tFlatVector::SetData(vector,\n-\t\t\t                    ArrowBufferData<data_t>(array, 1) + GetTypeIdSize(vector.GetType().InternalType()) *\n-\t\t\t                                                            (scan_state.chunk_offset + array.offset));\n+\t\t\tFlatVector::SetData(vector, ArrowBufferData<data_t>(array, 1) +\n+\t\t\t                                GetTypeIdSize(vector.GetType().InternalType()) *\n+\t\t\t                                    GetEffectiveOffset(array, parent_offset, scan_state, nested_offset));\n \t\t\tbreak;\n \t\t}\n \t\tdefault:\n@@ -614,15 +583,15 @@ static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowArraySca\n \t\tbreak;\n \t}\n \tcase LogicalTypeId::BLOB: {\n-\t\tArrowToDuckDBBlob(vector, array, scan_state, size, arrow_type, nested_offset);\n+\t\tArrowToDuckDBBlob(vector, array, scan_state, size, arrow_type, nested_offset, parent_offset);\n \t\tbreak;\n \t}\n \tcase LogicalTypeId::LIST: {\n-\t\tArrowToDuckDBList(vector, array, array_state, size, arrow_type, nested_offset, parent_mask);\n+\t\tArrowToDuckDBList(vector, array, array_state, size, arrow_type, nested_offset, parent_mask, parent_offset);\n \t\tbreak;\n \t}\n \tcase LogicalTypeId::MAP: {\n-\t\tArrowToDuckDBList(vector, array, array_state, size, arrow_type, nested_offset, parent_mask);\n+\t\tArrowToDuckDBList(vector, array, array_state, size, arrow_type, nested_offset, parent_mask, parent_offset);\n \t\tArrowToDuckDBMapVerify(vector, size);\n \t\tbreak;\n \t}\n@@ -636,7 +605,7 @@ static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowArraySca\n \t\t\tauto &child_type = arrow_type[child_idx];\n \t\t\tauto &child_state = array_state.GetChild(child_idx);\n \n-\t\t\tSetValidityMask(child_entry, child_array, scan_state, size, nested_offset);\n+\t\t\tSetValidityMask(child_entry, child_array, scan_state, size, array.offset, nested_offset);\n \t\t\tif (!struct_validity_mask.AllValid()) {\n \t\t\t\tauto &child_validity_mark = FlatVector::Validity(child_entry);\n \t\t\t\tfor (idx_t i = 0; i < size; i++) {\n@@ -646,7 +615,6 @@ static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowArraySca\n \t\t\t\t}\n \t\t\t}\n \t\t\tif (child_array.dictionary) {\n-\t\t\t\t// TODO: add support for offsets\n \t\t\t\tColumnArrowToDuckDBDictionary(child_entry, child_array, child_state, size, child_type, nested_offset,\n \t\t\t\t                              &struct_validity_mask, array.offset);\n \t\t\t} else {\n@@ -670,7 +638,7 @@ static void ColumnArrowToDuckDB(Vector &vector, ArrowArray &array, ArrowArraySca\n \t\t\tauto &child_state = array_state.GetChild(child_idx);\n \t\t\tauto &child_type = arrow_type[child_idx];\n \n-\t\t\tSetValidityMask(child, child_array, scan_state, size, nested_offset);\n+\t\t\tSetValidityMask(child, child_array, scan_state, size, parent_offset, nested_offset);\n \n \t\t\tif (child_array.dictionary) {\n \t\t\t\tColumnArrowToDuckDBDictionary(child, child_array, child_state, size, child_type);\n@@ -823,26 +791,48 @@ static void SetSelectionVector(SelectionVector &sel, data_ptr_t indices_p, Logic\n \t}\n }\n \n+static bool CanContainNull(ArrowArray &array, ValidityMask *parent_mask) {\n+\tif (array.null_count > 0) {\n+\t\treturn true;\n+\t}\n+\tif (!parent_mask) {\n+\t\treturn false;\n+\t}\n+\treturn !parent_mask->AllValid();\n+}\n+\n static void ColumnArrowToDuckDBDictionary(Vector &vector, ArrowArray &array, ArrowArrayScanState &array_state,\n                                           idx_t size, const ArrowType &arrow_type, int64_t nested_offset,\n                                           ValidityMask *parent_mask, uint64_t parent_offset) {\n-\tSelectionVector sel;\n \tauto &scan_state = array_state.state;\n+\n+\tconst bool has_nulls = CanContainNull(array, parent_mask);\n \tif (!array_state.HasDictionary()) {\n \t\t//! We need to set the dictionary data for this column\n \t\tauto base_vector = make_uniq<Vector>(vector.GetType(), array.dictionary->length);\n-\t\tSetValidityMask(*base_vector, *array.dictionary, scan_state, array.dictionary->length, 0, array.null_count > 0);\n+\t\tSetValidityMask(*base_vector, *array.dictionary, scan_state, array.dictionary->length, 0, 0, has_nulls);\n \t\tColumnArrowToDuckDB(*base_vector, *array.dictionary, array_state, array.dictionary->length,\n \t\t                    arrow_type.GetDictionary());\n \t\tarray_state.AddDictionary(std::move(base_vector));\n \t}\n \tauto offset_type = arrow_type.GetDuckType();\n \t//! Get Pointer to Indices of Dictionary\n-\tauto indices = ArrowBufferData<data_t>(array, 1) +\n-\t               GetTypeIdSize(offset_type.InternalType()) * (scan_state.chunk_offset + array.offset);\n-\tif (array.null_count > 0) {\n+\tauto indices =\n+\t    ArrowBufferData<data_t>(array, 1) +\n+\t    GetTypeIdSize(offset_type.InternalType()) * GetEffectiveOffset(array, parent_offset, scan_state, nested_offset);\n+\n+\tSelectionVector sel;\n+\tif (has_nulls) {\n \t\tValidityMask indices_validity;\n-\t\tGetValidityMask(indices_validity, array, scan_state, size);\n+\t\tGetValidityMask(indices_validity, array, scan_state, size, parent_offset);\n+\t\tif (parent_mask && !parent_mask->AllValid()) {\n+\t\t\tauto &struct_validity_mask = *parent_mask;\n+\t\t\tfor (idx_t i = 0; i < size; i++) {\n+\t\t\t\tif (!struct_validity_mask.RowIsValid(i)) {\n+\t\t\t\t\tindices_validity.SetInvalid(i);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n \t\tSetSelectionVector(sel, indices, offset_type, size, &indices_validity, array.dictionary->length);\n \t} else {\n \t\tSetSelectionVector(sel, indices, offset_type, size);\n@@ -864,6 +854,7 @@ void ArrowTableFunction::ArrowToDuckDB(ArrowScanLocalState &scan_state, const ar\n \t\t\tcontinue;\n \t\t}\n \n+\t\tauto &parent_array = scan_state.chunk->arrow_array;\n \t\tauto &array = *scan_state.chunk->arrow_array.children[arrow_array_idx];\n \t\tif (!array.release) {\n \t\t\tthrow InvalidInputException(\"arrow_scan: released array passed\");\n@@ -888,7 +879,7 @@ void ArrowTableFunction::ArrowToDuckDB(ArrowScanLocalState &scan_state, const ar\n \t\tif (array.dictionary) {\n \t\t\tColumnArrowToDuckDBDictionary(output.data[idx], array, array_state, output.size(), arrow_type);\n \t\t} else {\n-\t\t\tSetValidityMask(output.data[idx], array, scan_state, output.size(), -1);\n+\t\t\tSetValidityMask(output.data[idx], array, scan_state, output.size(), parent_array.offset, -1);\n \t\t\tColumnArrowToDuckDB(output.data[idx], array, array_state, output.size(), arrow_type);\n \t\t}\n \t}\n",
  "test_patch": "diff --git a/test/arrow/arrow_move_children.cpp b/test/arrow/arrow_move_children.cpp\nindex 147493a0ca02..2952f764688e 100644\n--- a/test/arrow/arrow_move_children.cpp\n+++ b/test/arrow/arrow_move_children.cpp\n@@ -27,6 +27,7 @@ void AssertExpectedResult(ArrowSchema *schema, ArrowArrayWrapper &array, T expec\n \tstruct_array.children[0] = &array.arrow_array;\n \tstruct_array.length = array.arrow_array.length;\n \tstruct_array.release = EmptyRelease;\n+\tstruct_array.offset = 0;\n \n \tduckdb_adbc::AdbcError unused;\n \t(void)BatchToArrayStream(&struct_array, schema, &stream, &unused);\ndiff --git a/tools/pythonpkg/tests/fast/arrow/test_arrow_offsets.py b/tools/pythonpkg/tests/fast/arrow/test_arrow_offsets.py\nnew file mode 100644\nindex 000000000000..e83c8c3794a1\n--- /dev/null\n+++ b/tools/pythonpkg/tests/fast/arrow/test_arrow_offsets.py\n@@ -0,0 +1,673 @@\n+import duckdb\n+import pytest\n+from pytest import mark\n+import datetime\n+import decimal\n+import pytz\n+\n+pa = pytest.importorskip(\"pyarrow\")\n+\n+# This causes Arrow to output an array at the very end with an offset\n+# the parent struct will have an offset that needs to be used when scanning the child array\n+MAGIC_ARRAY_SIZE = 2**17 + 1\n+\n+\n+def pa_time32():\n+    return pa.time32\n+\n+\n+def pa_time64():\n+    return pa.time64\n+\n+\n+def pa_date32():\n+    return pa.date32\n+\n+\n+def pa_date64():\n+    return pa.date64\n+\n+\n+def pa_timestamp():\n+    return pa.timestamp\n+\n+\n+def pa_duration():\n+    return pa.duration\n+\n+\n+def pa_month_day_nano_interval():\n+    return pa.month_day_nano_interval\n+\n+\n+def month_interval(months):\n+    return (months, 0, 0)\n+\n+\n+def day_interval(days):\n+    return (0, days, 0)\n+\n+\n+def nano_interval(nanos):\n+    return (0, 0, nanos)\n+\n+\n+def increment_or_null(val: str, increment):\n+    if not val:\n+        return val\n+    return str(int(val) + increment)\n+\n+\n+def decimal_value(value, precision, scale):\n+    val = str(value)\n+    actual_width = precision - scale\n+    if len(val) > actual_width:\n+        return decimal.Decimal('9' * actual_width)\n+    return decimal.Decimal(val)\n+\n+\n+def expected_result(col1_null, col2_null, expected):\n+    col1 = None if col1_null else expected\n+    if col1_null or col2_null:\n+        col2 = None\n+    else:\n+        col2 = expected\n+    return [(col1, col2)]\n+\n+\n+test_nulls = lambda: mark.parametrize(\n+    ['col1_null', 'col2_null'], [(False, True), (True, False), (True, True), (False, False)]\n+)\n+\n+\n+class TestArrowOffsets(object):\n+    @test_nulls()\n+    def test_struct_of_strings(self, duckdb_cursor, col1_null, col2_null):\n+        col1 = [str(i) for i in range(0, MAGIC_ARRAY_SIZE)]\n+        if col1_null:\n+            col1[-1] = None\n+        # \"a\" in the struct matches the value for col1\n+        col2 = [{\"a\": i} for i in col1]\n+        if col2_null:\n+            col2[-1] = None\n+        arrow_table = pa.Table.from_pydict(\n+            {\"col1\": col1, \"col2\": col2},\n+            schema=pa.schema([(\"col1\", pa.string()), (\"col2\", pa.struct({\"a\": pa.string()}))]),\n+        )\n+\n+        res = duckdb_cursor.sql(\n+            f\"\"\"\n+            SELECT\n+                col1,\n+                col2.a\n+            FROM arrow_table offset {MAGIC_ARRAY_SIZE-1}\n+        \"\"\"\n+        ).fetchall()\n+        assert res == expected_result(col1_null, col2_null, '131072')\n+\n+    @test_nulls()\n+    def test_struct_of_bools(self, duckdb_cursor, col1_null, col2_null):\n+        tuples = [False for i in range(0, MAGIC_ARRAY_SIZE)]\n+        tuples[-1] = True\n+\n+        col1 = tuples\n+        if col1_null:\n+            col1[-1] = None\n+        col2 = [{\"a\": i} for i in col1]\n+        if col2_null:\n+            col2[-1] = None\n+        arrow_table = pa.Table.from_pydict(\n+            {\"col1\": col1, \"col2\": col2},\n+            schema=pa.schema([(\"col1\", pa.bool_()), (\"col2\", pa.struct({\"a\": pa.bool_()}))]),\n+        )\n+\n+        res = duckdb_cursor.sql(\n+            f\"\"\"\n+            SELECT\n+                col1,\n+                col2.a\n+            FROM arrow_table offset {MAGIC_ARRAY_SIZE-1}\n+        \"\"\"\n+        ).fetchall()\n+        assert res == expected_result(col1_null, col2_null, True)\n+\n+    @pytest.mark.parametrize(\n+        [\"constructor\", \"expected\"],\n+        [\n+            (pa_date32(), datetime.date(2328, 11, 12)),\n+            (pa_date64(), datetime.date(1970, 1, 1)),\n+        ],\n+    )\n+    @test_nulls()\n+    def test_struct_of_dates(self, duckdb_cursor, constructor, expected, col1_null, col2_null):\n+        tuples = [i for i in range(0, MAGIC_ARRAY_SIZE)]\n+\n+        col1 = tuples\n+        if col1_null:\n+            col1[-1] = None\n+        col2 = [{\"a\": i} for i in col1]\n+        if col2_null:\n+            col2[-1] = None\n+        arrow_table = pa.Table.from_pydict(\n+            {\"col1\": col1, \"col2\": col2},\n+            schema=pa.schema([(\"col1\", constructor()), (\"col2\", pa.struct({\"a\": constructor()}))]),\n+        )\n+\n+        res = duckdb_cursor.sql(\n+            f\"\"\"\n+            SELECT\n+                col1,\n+                col2.a\n+            FROM arrow_table offset {MAGIC_ARRAY_SIZE-1}\n+        \"\"\"\n+        ).fetchall()\n+        assert res == expected_result(col1_null, col2_null, expected)\n+\n+    @test_nulls()\n+    def test_struct_of_enum(self, duckdb_cursor, col1_null, col2_null):\n+        enum_type = pa.dictionary(pa.int64(), pa.utf8())\n+\n+        tuples = ['red' for i in range(MAGIC_ARRAY_SIZE)]\n+        tuples[-1] = 'green'\n+        if col1_null:\n+            tuples[-1] = None\n+\n+        struct_tuples = [{\"a\": x} for x in tuples]\n+        if col2_null:\n+            struct_tuples[-1] = None\n+\n+        arrow_table = pa.Table.from_pydict(\n+            {'col1': pa.array(tuples, enum_type), 'col2': pa.array(struct_tuples, pa.struct({\"a\": enum_type}))},\n+            schema=pa.schema([(\"col1\", enum_type), (\"col2\", pa.struct({\"a\": enum_type}))]),\n+        )\n+        res = duckdb_cursor.sql(\n+            f\"\"\"\n+            SELECT\n+                col1,\n+                col2.a\n+            FROM arrow_table offset {MAGIC_ARRAY_SIZE-1}\n+        \"\"\"\n+        ).fetchall()\n+        assert res == expected_result(col1_null, col2_null, 'green')\n+\n+    @test_nulls()\n+    def test_struct_of_blobs(self, duckdb_cursor, col1_null, col2_null):\n+        col1 = [str(i) for i in range(0, MAGIC_ARRAY_SIZE)]\n+        if col1_null:\n+            col1[-1] = None\n+        # \"a\" in the struct matches the value for col1\n+        col2 = [{\"a\": i} for i in col1]\n+        if col2_null:\n+            col2[-1] = None\n+        arrow_table = pa.Table.from_pydict(\n+            {\"col1\": col1, \"col2\": col2},\n+            schema=pa.schema([(\"col1\", pa.binary()), (\"col2\", pa.struct({\"a\": pa.binary()}))]),\n+        )\n+\n+        res = duckdb_cursor.sql(\n+            f\"\"\"\n+            SELECT\n+                col1,\n+                col2.a\n+            FROM arrow_table offset {MAGIC_ARRAY_SIZE-1}\n+        \"\"\"\n+        ).fetchall()\n+        assert res == expected_result(col1_null, col2_null, b'131072')\n+\n+    @test_nulls()\n+    @pytest.mark.parametrize(\n+        [\"constructor\", \"unit\", \"expected\"],\n+        [\n+            (pa_time32(), 'ms', datetime.time(0, 2, 11, 72000)),\n+            (pa_time32(), 's', datetime.time(23, 59, 59)),\n+            (pa_time64(), 'ns', datetime.time(0, 0, 0, 131)),\n+            (pa_time64(), 'us', datetime.time(0, 0, 0, 131072)),\n+        ],\n+    )\n+    def test_struct_of_time(self, duckdb_cursor, constructor, unit, expected, col1_null, col2_null):\n+        size = MAGIC_ARRAY_SIZE\n+        if unit == 's':\n+            # FIXME: We limit the size because we don't support time values > 24 hours\n+            size = 86400  # The amount of seconds in a day\n+\n+        col1 = [i for i in range(0, size)]\n+        if col1_null:\n+            col1[-1] = None\n+        # \"a\" in the struct matches the value for col1\n+        col2 = [{\"a\": i} for i in col1]\n+        if col2_null:\n+            col2[-1] = None\n+        arrow_table = pa.Table.from_pydict(\n+            {\"col1\": col1, \"col2\": col2},\n+            schema=pa.schema([(\"col1\", constructor(unit)), (\"col2\", pa.struct({\"a\": constructor(unit)}))]),\n+        )\n+\n+        res = duckdb_cursor.sql(\n+            f\"\"\"\n+            SELECT\n+                col1,\n+                col2.a\n+            FROM arrow_table offset {size-1}\n+        \"\"\"\n+        ).fetchall()\n+        assert res == expected_result(col1_null, col2_null, expected)\n+\n+    @test_nulls()\n+    # NOTE: there is sadly no way to create a 'interval[months]' (tiM) type from pyarrow\n+    @pytest.mark.parametrize(\n+        [\"constructor\", \"expected\", \"converter\"],\n+        [\n+            (pa_month_day_nano_interval(), datetime.timedelta(days=3932160), month_interval),\n+            (pa_month_day_nano_interval(), datetime.timedelta(days=131072), day_interval),\n+            (pa_month_day_nano_interval(), datetime.timedelta(microseconds=131), nano_interval),\n+        ],\n+    )\n+    def test_struct_of_interval(self, duckdb_cursor, constructor, expected, converter, col1_null, col2_null):\n+        size = MAGIC_ARRAY_SIZE\n+\n+        col1 = [converter(i) for i in range(0, size)]\n+        if col1_null:\n+            col1[-1] = None\n+        # \"a\" in the struct matches the value for col1\n+        col2 = [{\"a\": i} for i in col1]\n+        if col2_null:\n+            col2[-1] = None\n+        arrow_table = pa.Table.from_pydict(\n+            {\"col1\": col1, \"col2\": col2},\n+            schema=pa.schema([(\"col1\", constructor()), (\"col2\", pa.struct({\"a\": constructor()}))]),\n+        )\n+\n+        res = duckdb_cursor.sql(\n+            f\"\"\"\n+            SELECT\n+                col1,\n+                col2.a\n+            FROM arrow_table offset {size-1}\n+        \"\"\"\n+        ).fetchall()\n+        assert res == expected_result(col1_null, col2_null, expected)\n+\n+    @test_nulls()\n+    @pytest.mark.parametrize(\n+        [\"constructor\", \"unit\", \"expected\"],\n+        [\n+            (pa_duration(), 'ms', datetime.timedelta(seconds=131, microseconds=72000)),\n+            (pa_duration(), 's', datetime.timedelta(days=1, seconds=44672)),\n+            (pa_duration(), 'ns', datetime.timedelta(microseconds=131)),\n+            (pa_duration(), 'us', datetime.timedelta(microseconds=131072)),\n+        ],\n+    )\n+    def test_struct_of_duration(self, duckdb_cursor, constructor, unit, expected, col1_null, col2_null):\n+        size = MAGIC_ARRAY_SIZE\n+\n+        col1 = [i for i in range(0, size)]\n+        if col1_null:\n+            col1[-1] = None\n+        # \"a\" in the struct matches the value for col1\n+        col2 = [{\"a\": i} for i in col1]\n+        if col2_null:\n+            col2[-1] = None\n+        arrow_table = pa.Table.from_pydict(\n+            {\"col1\": col1, \"col2\": col2},\n+            schema=pa.schema([(\"col1\", constructor(unit)), (\"col2\", pa.struct({\"a\": constructor(unit)}))]),\n+        )\n+\n+        res = duckdb_cursor.sql(\n+            f\"\"\"\n+            SELECT\n+                col1,\n+                col2.a\n+            FROM arrow_table offset {size-1}\n+        \"\"\"\n+        ).fetchall()\n+        assert res == expected_result(col1_null, col2_null, expected)\n+\n+    @test_nulls()\n+    @pytest.mark.parametrize(\n+        [\"constructor\", \"unit\", \"expected\"],\n+        [\n+            (pa_timestamp(), 'ms', datetime.datetime(1970, 1, 1, 0, 2, 11, 72000, tzinfo=pytz.utc)),\n+            (pa_timestamp(), 's', datetime.datetime(1970, 1, 2, 12, 24, 32, 0, tzinfo=pytz.utc)),\n+            (pa_timestamp(), 'ns', datetime.datetime(1970, 1, 1, 0, 0, 0, 131, tzinfo=pytz.utc)),\n+            (pa_timestamp(), 'us', datetime.datetime(1970, 1, 1, 0, 0, 0, 131072, tzinfo=pytz.utc)),\n+        ],\n+    )\n+    def test_struct_of_timestamp_tz(self, duckdb_cursor, constructor, unit, expected, col1_null, col2_null):\n+        size = MAGIC_ARRAY_SIZE\n+\n+        duckdb_cursor.execute(\"set timezone='UTC'\")\n+        col1 = [i for i in range(0, size)]\n+        if col1_null:\n+            col1[-1] = None\n+        # \"a\" in the struct matches the value for col1\n+        col2 = [{\"a\": i} for i in col1]\n+        if col2_null:\n+            col2[-1] = None\n+        arrow_table = pa.Table.from_pydict(\n+            {\"col1\": col1, \"col2\": col2},\n+            schema=pa.schema(\n+                [(\"col1\", constructor(unit, 'UTC')), (\"col2\", pa.struct({\"a\": constructor(unit, 'UTC')}))]\n+            ),\n+        )\n+\n+        res = duckdb_cursor.sql(\n+            f\"\"\"\n+            SELECT\n+                col1,\n+                col2.a\n+            FROM arrow_table offset {size-1}\n+        \"\"\"\n+        ).fetchall()\n+        assert res == expected_result(col1_null, col2_null, expected)\n+\n+    @test_nulls()\n+    def test_struct_of_large_blobs(self, duckdb_cursor, col1_null, col2_null):\n+        col1 = [str(i) for i in range(0, MAGIC_ARRAY_SIZE)]\n+        if col1_null:\n+            col1[-1] = None\n+        # \"a\" in the struct matches the value for col1\n+        col2 = [{\"a\": i} for i in col1]\n+        if col2_null:\n+            col2[-1] = None\n+        arrow_table = pa.Table.from_pydict(\n+            {\"col1\": col1, \"col2\": col2},\n+            schema=pa.schema([(\"col1\", pa.large_binary()), (\"col2\", pa.struct({\"a\": pa.large_binary()}))]),\n+        )\n+\n+        res = duckdb_cursor.sql(\n+            f\"\"\"\n+            SELECT\n+                col1,\n+                col2.a\n+            FROM arrow_table offset {MAGIC_ARRAY_SIZE-1}\n+        \"\"\"\n+        ).fetchall()\n+        assert res == expected_result(col1_null, col2_null, b'131072')\n+\n+    @test_nulls()\n+    @pytest.mark.parametrize(\n+        [\"precision_scale\", \"expected\"],\n+        [\n+            ((38, 37), decimal.Decimal('9.0000000000000000000000000000000000000')),\n+            ((38, 24), decimal.Decimal('131072.000000000000000000000000')),\n+            ((18, 14), decimal.Decimal('9999.00000000000000')),\n+            ((18, 5), decimal.Decimal('131072.00000')),\n+            ((9, 7), decimal.Decimal('99.0000000')),\n+            ((9, 3), decimal.Decimal('131072.000')),\n+            ((4, 2), decimal.Decimal('99.00')),\n+            ((4, 0), decimal.Decimal('9999')),\n+        ],\n+    )\n+    def test_struct_of_decimal(self, duckdb_cursor, precision_scale, expected, col1_null, col2_null):\n+        precision, scale = precision_scale\n+        col1 = [decimal_value(i, precision, scale) for i in range(0, MAGIC_ARRAY_SIZE)]\n+        if col1_null:\n+            col1[-1] = None\n+        # \"a\" in the struct matches the value for col1\n+        col2 = [{\"a\": i} for i in col1]\n+        if col2_null:\n+            col2[-1] = None\n+\n+        arrow_table = pa.Table.from_pydict(\n+            {\"col1\": col1, \"col2\": col2},\n+            schema=pa.schema(\n+                [(\"col1\", pa.decimal128(precision, scale)), (\"col2\", pa.struct({\"a\": pa.decimal128(precision, scale)}))]\n+            ),\n+        )\n+\n+        res = duckdb_cursor.sql(\n+            f\"\"\"\n+            SELECT\n+                col1,\n+                col2.a\n+            FROM arrow_table offset {MAGIC_ARRAY_SIZE-1}\n+        \"\"\"\n+        ).fetchall()\n+        assert res == expected_result(col1_null, col2_null, expected)\n+\n+    @test_nulls()\n+    def test_struct_of_small_list(self, duckdb_cursor, col1_null, col2_null):\n+        col1 = [str(i) for i in range(0, MAGIC_ARRAY_SIZE)]\n+        if col1_null:\n+            col1[-1] = None\n+        # \"a\" in the struct matches the value for col1\n+        col2 = [{\"a\": [i, i, i]} for i in col1]\n+        if col2_null:\n+            col2[-1] = None\n+        arrow_table = pa.Table.from_pydict(\n+            {\"col1\": col1, \"col2\": col2},\n+            schema=pa.schema([(\"col1\", pa.string()), (\"col2\", pa.struct({\"a\": pa.list_(pa.string())}))]),\n+        )\n+        res = duckdb_cursor.sql(\n+            f\"\"\"\n+            SELECT\n+                col1,\n+                col2.a\n+            FROM arrow_table offset {MAGIC_ARRAY_SIZE-1}\n+        \"\"\"\n+        ).fetchall()\n+        res1 = None if col1_null else '131072'\n+        if col2_null:\n+            res2 = None\n+        elif col1_null:\n+            res2 = [None, None, None]\n+        else:\n+            res2 = ['131072', '131072', '131072']\n+        assert res == [(res1, res2)]\n+\n+    @test_nulls()\n+    def test_struct_of_fixed_size_list(self, duckdb_cursor, col1_null, col2_null):\n+        col1 = [str(i) for i in range(0, MAGIC_ARRAY_SIZE)]\n+        if col1_null:\n+            col1[-1] = None\n+        # \"a\" in the struct matches the value for col1\n+        col2 = [{\"a\": [i, i, i]} for i in col1]\n+        if col2_null:\n+            col2[-1] = None\n+        arrow_table = pa.Table.from_pydict(\n+            {\"col1\": col1, \"col2\": col2},\n+            schema=pa.schema([(\"col1\", pa.string()), (\"col2\", pa.struct({\"a\": pa.list_(pa.string(), 3)}))]),\n+        )\n+        res = duckdb_cursor.sql(\n+            f\"\"\"\n+            SELECT\n+                col1,\n+                col2.a\n+            FROM arrow_table offset {MAGIC_ARRAY_SIZE-1}\n+        \"\"\"\n+        ).fetchall()\n+        res1 = None if col1_null else '131072'\n+        if col2_null:\n+            res2 = None\n+        elif col1_null:\n+            res2 = [None, None, None]\n+        else:\n+            res2 = ['131072', '131072', '131072']\n+        assert res == [(res1, res2)]\n+\n+    @test_nulls()\n+    def test_struct_of_fixed_size_blob(self, duckdb_cursor, col1_null, col2_null):\n+        col1 = [str(i) for i in range(0, MAGIC_ARRAY_SIZE)]\n+        if col1_null:\n+            col1[-1] = None\n+        # \"a\" in the struct matches the value for col1\n+        col2 = [{\"a\": [i, increment_or_null(i, 1), increment_or_null(i, 2)]} for i in col1]\n+        if col2_null:\n+            col2[-1] = None\n+        arrow_table = pa.Table.from_pydict(\n+            {\"col1\": col1, \"col2\": col2},\n+            schema=pa.schema([(\"col1\", pa.binary()), (\"col2\", pa.struct({\"a\": pa.list_(pa.binary(), 3)}))]),\n+        )\n+\n+        res = duckdb_cursor.sql(\n+            f\"\"\"\n+            SELECT\n+                col1,\n+                col2.a\n+            FROM arrow_table offset {MAGIC_ARRAY_SIZE-1}\n+        \"\"\"\n+        ).fetchall()\n+        res1 = None if col1_null else b'131072'\n+        if col2_null:\n+            res2 = None\n+        elif col1_null:\n+            res2 = [None, None, None]\n+        else:\n+            res2 = [b'131072', b'131073', b'131074']\n+        assert res == [(res1, res2)]\n+\n+    @test_nulls()\n+    def test_struct_of_list_of_blobs(self, duckdb_cursor, col1_null, col2_null):\n+        col1 = [str(i) for i in range(0, MAGIC_ARRAY_SIZE)]\n+        if col1_null:\n+            col1[-1] = None\n+        # \"a\" in the struct matches the value for col1\n+        col2 = [{\"a\": [i, increment_or_null(i, 1), increment_or_null(i, 2)]} for i in col1]\n+        if col2_null:\n+            col2[-1] = None\n+        arrow_table = pa.Table.from_pydict(\n+            {\"col1\": col1, \"col2\": col2},\n+            schema=pa.schema([(\"col1\", pa.binary()), (\"col2\", pa.struct({\"a\": pa.list_(pa.binary())}))]),\n+        )\n+\n+        res = duckdb_cursor.sql(\n+            f\"\"\"\n+            SELECT\n+                col1,\n+                col2.a\n+            FROM arrow_table offset {MAGIC_ARRAY_SIZE-1}\n+        \"\"\"\n+        ).fetchall()\n+        res1 = None if col1_null else b'131072'\n+        if col2_null:\n+            res2 = None\n+        elif col1_null:\n+            res2 = [None, None, None]\n+        else:\n+            res2 = [b'131072', b'131073', b'131074']\n+        assert res == [(res1, res2)]\n+\n+    @test_nulls()\n+    def test_struct_of_list_of_list(self, duckdb_cursor, col1_null, col2_null):\n+        col1 = [i for i in range(0, MAGIC_ARRAY_SIZE)]\n+        if col1_null:\n+            col1[-1] = None\n+        # \"a\" in the struct matches the value for col1\n+        col2 = [{\"a\": [[i, i, i], [], None, [i]]} for i in col1]\n+        if col2_null:\n+            col2[-1] = None\n+        arrow_table = pa.Table.from_pydict(\n+            {\"col1\": col1, \"col2\": col2},\n+            # thanks formatter\n+            schema=pa.schema([(\"col1\", pa.int32()), (\"col2\", pa.struct({\"a\": pa.list_(pa.list_(pa.int32()))}))]),\n+        )\n+        res = duckdb_cursor.sql(\n+            f\"\"\"\n+            SELECT\n+                col1,\n+                col2.a\n+            FROM arrow_table offset {MAGIC_ARRAY_SIZE-1}\n+        \"\"\"\n+        ).fetchall()\n+        res1 = None if col1_null else 131072\n+        if col2_null:\n+            res2 = None\n+        elif col1_null:\n+            res2 = [[None, None, None], [], None, [None]]\n+        else:\n+            res2 = [[131072, 131072, 131072], [], None, [131072]]\n+        assert res == [(res1, res2)]\n+\n+    @pytest.mark.parametrize('col1_null', [True, False])\n+    def test_list_of_struct(self, duckdb_cursor, col1_null):\n+        # One single tuple containing a very big list\n+        tuples = [{\"a\": i} for i in range(0, MAGIC_ARRAY_SIZE)]\n+        if col1_null:\n+            tuples[-1] = None\n+        tuples = [tuples]\n+        arrow_table = pa.Table.from_pydict(\n+            {\"col1\": tuples},\n+            schema=pa.schema([(\"col1\", pa.list_(pa.struct({\"a\": pa.int32()})))]),\n+        )\n+        res = duckdb_cursor.sql(\n+            f\"\"\"\n+            SELECT\n+                col1\n+            FROM arrow_table\n+        \"\"\"\n+        ).fetchall()\n+        res = res[0][0]\n+        for i, x in enumerate(res[:-1]):\n+            assert x.__class__ == dict\n+            assert x['a'] == i\n+        if col1_null:\n+            assert res[-1] == None\n+        else:\n+            assert res[-1]['a'] == len(res) - 1\n+\n+    @pytest.mark.parametrize(['outer_null', 'inner_null'], [(True, False), (False, True)])\n+    def test_list_of_list_of_struct(self, duckdb_cursor, outer_null, inner_null):\n+        tuples = [[[{\"a\": str(i), \"b\": None, \"c\": [i]}]] for i in range(MAGIC_ARRAY_SIZE)]\n+        if outer_null:\n+            tuples[-1] = None\n+        else:\n+            inner = [[{\"a\": 'aaaaaaaaaaaaaaa', \"b\": 'test', \"c\": [1, 2, 3]}] for _ in range(MAGIC_ARRAY_SIZE)]\n+            if inner_null:\n+                inner[-1] = None\n+            tuples[-1] = inner\n+\n+        # MAGIC_ARRAY_SIZE tuples, all containing a single child list\n+        # except the last tuple, the child list of which is also MAGIC_ARRAY_SIZE in length\n+        arrow_table = pa.Table.from_pydict(\n+            {\"col1\": tuples},\n+            schema=pa.schema(\n+                [\n+                    (\n+                        \"col1\",\n+                        pa.list_(\n+                            pa.list_(pa.struct([(\"a\", pa.string()), (\"b\", pa.string()), (\"c\", pa.list_(pa.int32()))]))\n+                        ),\n+                    )\n+                ]\n+            ),\n+        )\n+        res = duckdb_cursor.sql(\n+            f\"\"\"\n+            SELECT\n+                col1\n+            FROM arrow_table OFFSET {MAGIC_ARRAY_SIZE-1}\n+        \"\"\"\n+        ).fetchall()\n+        if outer_null:\n+            assert res == [(None,)]\n+        else:\n+            if inner_null:\n+                assert res[-1][-1][-1] == None\n+            else:\n+                assert res[-1][-1][-1] == 131072\n+\n+    @pytest.mark.parametrize('col1_null', [True, False])\n+    def test_struct_of_list(self, duckdb_cursor, col1_null):\n+        # All elements are of size 1\n+        tuples = [{\"a\": [str(i)]} for i in range(MAGIC_ARRAY_SIZE)]\n+        if col1_null:\n+            tuples[-1] = None\n+        else:\n+            tuples[-1] = {\"a\": [str(x) for x in range(MAGIC_ARRAY_SIZE)]}\n+\n+        # Except the very last element, which is big\n+\n+        arrow_table = pa.Table.from_pydict(\n+            {\"col1\": tuples}, schema=pa.schema([(\"col1\", pa.struct({\"a\": pa.list_(pa.string())}))])\n+        )\n+        res = duckdb_cursor.sql(\n+            f\"\"\"\n+            SELECT\n+                col1\n+            FROM arrow_table offset {MAGIC_ARRAY_SIZE-1}\n+        \"\"\"\n+        ).fetchone()\n+        if col1_null:\n+            assert res[0] == None\n+        else:\n+            assert res[0]['a'][-1] == '131072'\n",
  "problem_statement": "Incorrect nested data converted in Arrow <-> DuckDB conversion when read through Arrow DataSet (Python)\n### What happens?\n\nI have a parquet file with a nested struct column. When querying this file and extracting the struct column, it appears to return the wrong data. It has done this when using a Arrow Dataset / Table as inputs.\r\n\r\nI have uploaded the specific file in question down below. The nested aspect might be a coincidence - I have no idea what's going wrong.\n\n### To Reproduce\n\nDataset (zipped parquet file - could not upload parquet alone):\r\n[obfuscated.zip](https://github.com/duckdb/duckdb/files/10118562/obfuscated.zip)\r\n\r\n## Environment\r\n```\r\nPackage         Version\r\n--------------- ------------\r\nduckdb          0.6.1.dev191\r\nnumpy           1.24.0rc1\r\npandas          1.5.2\r\npip             22.3.1\r\npyarrow         10.0.1\r\npython-dateutil 2.8.2\r\npytz            2022.6\r\nsetuptools      65.5.1\r\nsix             1.16.0\r\nwheel           0.37.1\r\n```\r\n\r\n\r\n## Python code\r\n```python\r\nimport pyarrow.dataset as ds\r\nimport duckdb\r\n\r\ndata = ds.dataset(\"~/obfuscated.parquet\")\r\n\r\ncon = duckdb.connect()\r\n\r\ndisplay(\"Select then filter\")\r\ndisplay(con.execute(\"\"\"\r\nSELECT\r\n    col1,\r\n    col2,\r\n    col3,\r\n    col4,\r\n    col5,\r\n    nested_col.*,\r\nFROM data\r\nORDER BY ALL\r\n\"\"\").df().loc[lambda df: (df.col1 == 749) & (df.col2 == 747) & (df.col3 == 5) & (df.col4 == 1)])\r\n\r\ndisplay(\"Filter then select\")\r\ndisplay(con.execute(\"\"\"\r\nSELECT\r\n    col1,\r\n    col2,\r\n    col3,\r\n    col4,\r\n    col5,\r\n    nested_col.*,\r\nFROM data\r\nWHERE col1 = 749\r\n    AND col2 = 747\r\n    AND col3 = 5\r\n    AND col4 = 1\r\nORDER BY ALL\r\n\"\"\").df())\r\n\r\ncon.close()\r\n```\r\n\r\n## Output\r\n![image](https://user-images.githubusercontent.com/91922857/204685867-c90a5e4e-22f8-4b23-ab14-264399dfe278.png)\r\n\r\nThe expected output is the second one above. This issue only shows with Arrow Dataset/Table inputs. The issue doesn't show when using a pandas DataFrame as inputs.\n\n### OS:\n\nLinux x64\n\n### DuckDB Version:\n\nduckdb-0.6.1.dev191\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nEdward Davis\n\n### Affiliation:\n\nVeitch Lister Consulting\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "Thanks for the report!\r\n\r\nThis looks like a bug in the arrow conversion code. @pdet can you have a look?\nHi, I have found a minimal example: see below. I noticed some weird things happening when the length of the pyarrow table exceeds 2^17. A similar issue came about when using polars (see pola-rs/polars#5796).\r\n\r\nMy understanding isn't great but looks like it's an upstream issue where arrow returns duplicate chunks through the FFI. \r\n\r\n```python\r\nimport duckdb\r\nimport pyarrow as pa\r\nimport pandas as pd\r\n\r\nnum_rows = 2**17 + 1\r\n\r\ntbl = pa.Table.from_pandas(pd.DataFrame.from_records([\r\n        dict(\r\n            id=i, \r\n            nested=dict(\r\n                a=i,\r\n            )\r\n        )\r\n        for i in range(num_rows)\r\n]))\r\n\r\ncon = duckdb.connect()\r\n\r\nprint(con.execute(\"\"\"\r\nSELECT *\r\nFROM tbl\r\n\"\"\").df())\r\n\r\n# Last row shows id=131072,  nested={'a': 0}  instead of id=131072,  nested={'a': 131072} \r\n# Does not occur for num_rows <= 2^17\r\n\r\ncon.close()\r\n```\r\n\nI'm not sure whether this issue has the same root cause, but I am also getting incorrect results on boolean fields when I read a Parquet file in. I originally thought it was an issue with Arrow's write, but pyarrow and fastparquet both read it correctly.\r\n\r\nThe original Arrow ticket is here and has all of the data/details needed to reproduce: https://github.com/apache/arrow/issues/33605\n I'm also getting an issue with random boolean fields being flipped. \r\n\r\nIn my case, the boolean doesn't have to be nested (although I noticed the nested booleans first) \r\n\r\nI write my dataset twice via spark (same data, but partitioned differently). \r\nWhen I query both datasets via duckdb, 2887 out of 481917 rows have a column mismatch! \r\n\r\nWhen I read both datasets via pandas, or via spark, or via pyarrow directly, both datasets match perfectly. \r\n___\r\nThis is extremely unnerving and a huge blow to our confidence in duckdb producing accurate results.\r\n\r\nI recommend categorizing this as a critical level issue/bug\n> I'm not sure whether this issue has the same root cause, but I am also getting incorrect results on boolean fields when I read a Parquet file in. I originally thought it was an issue with Arrow's write, but pyarrow and fastparquet both read it correctly.\r\n> \r\n> The original Arrow ticket is here and has all of the data/details needed to reproduce: [apache/arrow#33605](https://github.com/apache/arrow/issues/33605)\r\n\r\nThanks for the detailed report - I have pushed a fix in #5926. In the future it would be helpful if you could file it as an actual issue rather than as a comment in an unrelated issue. Although I can see the confusion given the original title this issue is about a bug in the Arrow <> DuckDB conversion code that originated on Arrows' side. I have renamed the issue to clear up future confusion.\r\n\r\n\n@alex-shchetkov\r\n> This is extremely unnerving and a huge blow to our confidence in duckdb producing accurate results.\r\n> I recommend categorizing this as a critical level issue/bug\r\n\r\nPlease note that DuckDB is provided as-is at no charge under the MIT license which states (in caps) that 'THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, [...] FITNESS FOR A PARTICULAR PURPOSE [...]'. \r\n\r\nDespite this, we are doing our best to support our users here and elsewhere. But best-effort does not a guarantee that we will address a particular issue at all or in any particular time frame. Those kind of guarantees are usually part of a commercial support agreement. \r\n\r\nI would also like to ask you to re-think the tone that you are using in your comment. It comes over somewhat aggressive. Please refer to our code of conduct for some additional guidance.\r\n\r\nIt would be more productive if you would provide reproducible steps so we can work together in addressing the issue.\r\n\nHi there, I've also encountered the issue that @edavisau has pointed to, and came up with a similar minimal example (before seeing this issue!):\r\n```python\r\nimport duckdb\r\nimport pyarrow as pa\r\n\r\n\r\ndef test_duckdb_struct():\r\n    # threshold is important 2^17, so go just over\r\n    col1 = [i for i in range(0, 131075)]\r\n    # \"a\" in the struct matches the value for col1\r\n    col2 = [{\"a\": i} for i in col1]\r\n    table = pa.Table.from_pydict(\r\n        {\"col1\": col1, \"col2\": col2},\r\n        schema=pa.schema(\r\n            [(\"col1\", pa.float64()), (\"col2\", pa.struct({\"a\": pa.float64()}))]\r\n        ),\r\n    )\r\n\r\n    conn = duckdb.connect(\":memory:\")\r\n    conn.register(\"my_table\", table)\r\n    # If working correctly I would expect an empty list to be printed here\r\n    print(\r\n        conn.execute(\"SELECT * FROM my_table WHERE col1 <> col2.a LIMIT 10\")\r\n        .fetch_arrow_table()\r\n        .to_pylist()\r\n    )\r\n```\r\n\r\nWhat's worth noting is that the struct column `col2` seems to have rotated back to the start, since we see `col2.a` values 0, 1, 2.\r\n\r\nI'm not sure that this test helps, but reifying the table seems to show correct data:\r\n```python\r\nprint([o for o in table.to_pylist() if o[\"col1\"] != o[\"col2\"][\"a\"]]) # prints empty list\r\n```\r\n\r\nAll run on duckdb 0.8.2.dev2068\nIn case it's of interest a workaround exists using the same approach as in the test suite of subclassing `pyarrow.dataset.Dataset` https://github.com/duckdb/duckdb/blob/09e2d957342607904124ebab892be70b0ecf9a10/tools/pythonpkg/tests/fast/arrow/test_dataset.py#L100\r\n\r\nSetting the batch size in the scanner to a large value such that we only have a single batch side steps the issue.  Standard disclaimer that this isn't ideal because it prevents batchwise processing and caps your max dataset row count:\r\n```python\r\ndef test_duckdb_struct_workaround():\r\n    # threshold is important 2^17, so go just over\r\n    col1 = [i for i in range(0, 131075)]\r\n    # \"a\" in the struct matches the value for col1\r\n    col2 = [{\"a\": i} for i in col1]\r\n    table = pa.Table.from_pydict(\r\n        {\"col1\": col1, \"col2\": col2},\r\n        schema=pa.schema(\r\n            [(\"col1\", pa.float64()), (\"col2\", pa.struct({\"a\": pa.float64()}))]\r\n        ),\r\n    )\r\n\r\n    class CustomDataset(ds.Dataset):\r\n        def __init__(self, dataset):\r\n            self.original = dataset\r\n\r\n        def scanner(self, **kwargs):\r\n            return self.original.scanner(**kwargs, batch_size=1_000_000)\r\n\r\n        @property\r\n        def schema(self):\r\n            return self.original.schema\r\n\r\n    dataset = CustomDataset(ds.dataset(table))\r\n\r\n    conn = duckdb.connect(\":memory:\")\r\n    conn.register(\"my_table\", dataset)\r\n    # If working correctly I would expect an empty list to be printed here\r\n    print(\r\n        conn.execute(\"SELECT * FROM my_table WHERE col1 <> col2.a LIMIT 10\")\r\n        .fetch_arrow_table()\r\n        .to_pylist()\r\n    )\r\n\r\n    # Check _something_ happens\r\n    print(\r\n        conn.execute(\"SELECT COUNT(*), SUM(col1), SUM(col2.a) FROM my_table\")\r\n        .fetch_arrow_table()\r\n        .to_pylist()\r\n    )\r\n\r\n    # See this value matches SUM(col1) in the above SQL execution\r\n    print(sum(col1))\r\n```\n@Tishj perhaps you can have a look when you have time?\nIn case helpful, I _think_ @pdet has a branch which has addressed this:\r\nhttps://github.com/duckdb/duckdb/compare/master...pdet:duckdb:bug_5547\r\n\r\nWithout understanding in detail, the `parent_offset` looks like a fix\nAwesome - thanks all for the fast turnaround\nHi, the issue reported here seems to still be occurring in some cases. Specifically, when I modify @tomsimpkins 's [example](https://github.com/duckdb/duckdb/issues/5547#issuecomment-1650592715) to use `pa.string()` instead of `pa.float64()`, this issue occurs. I'm not sure whether it's specific to `string` or something else, this is just the minimal reproduction I found that was closest to the issue we encountered in our real data. We found a workaround to using arrow at all, but figured I should report this. Thanks!\r\n\r\nUsing:\r\n* `duckdb==0.9.1`\r\n* `pyarrow=0.14.1`\r\n\r\n```\r\ndef test_duckdb_struct(n: int):\r\n    # threshold is important 2^17, so go just over\r\n    col1 = [str(i) for i in range(0, n)]\r\n    # \"a\" in the struct matches the value for col1\r\n    col2 = [{\"a\": str(i)} for i in col1]\r\n    table = pa.Table.from_pydict(\r\n        {\"col1\": col1, \"col2\": col2},\r\n        schema=pa.schema(\r\n            [(\"col1\", pa.string()), (\"col2\", pa.struct({\"a\": pa.string()}))]\r\n        ),\r\n    )\r\n\r\n    conn = duckdb.connect(\":memory:\")\r\n    conn.register(\"my_table\", table)\r\n    # If working correctly I would expect an empty list to be printed here\r\n    print(\r\n        conn.execute(\"SELECT col1, col2.a FROM my_table WHERE col1 != col2.a LIMIT 10\")\r\n        .fetch_arrow_table()\r\n        .to_pylist()\r\n    )\r\n\r\ntest_duckdb_struct(2**17)\r\n# prints: []\r\n\r\ntest_duckdb_struct(2**17+1)\r\n# prints: [{'col1': '131072', 'a': '0'}]\r\n# should be [{'col1': '131072', 'a': '131072'}]\r\n\r\ntest_duckdb_struct(2**17+2)\r\n# prints: [{'col1': '131072', 'a': '0'}, {'col1': '131073', 'a': '1'}]\r\n```",
  "created_at": "2023-11-13T16:23:23Z"
}