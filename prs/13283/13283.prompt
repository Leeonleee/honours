You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
INTERNAL Error: Incorrect stats size for type TIMETZ
### What happens?

Loading parquet file into DuckDB table with read_parquet leads to internal error. But, the R arrow package can successfully load the parquet file with no issues leading me to think that the parquet is fine and the duckdb parquet reader may have an issue.

### To Reproduce

Reading these time columns leads to an internal error.
```
library(duckdb)
con <- dbConnect(duckdb())
dbSendQuery(con, "CREATE TABLE mytable AS FROM read_parquet('times.parquet');")
```
Gives error:
```
Error: rapi_prepare: Failed to prepare query CREATE TABLE mytable AS FROM read_parquet('pk/1.pk');
Error: INTERNAL Error: Incorrect stats size for type TIMETZ
This error signals an assertion failure within DuckDB. This usually occurs due to unexpected conditions or errors in the program's logic.
For more information, see https://duckdb.org/docs/dev/internal_errors
```
But using arrow package I get : 
```
arrow::read_parquet('times.parquet')
# A tibble: 1 × 2
  col1   col2  
  <time> <time>
1 00'00" 00'00"
```
I've reduced the original file down to the minimum required to reproduce the internal error, 1 line, 2 columns. I didn't check if only one column is the issue or if its both.
[times.zip](https://github.com/user-attachments/files/16467664/times.zip)


### OS:

Windows 10 Professionnel x64

### DuckDB Version:

R package version 1.0.0-2

### DuckDB Client:

R version 4.3.2

### Full Name:

Eli Daniels

### Affiliation:

ArData

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a source build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have
INTERNAL Error: Incorrect stats size for type TIMETZ
### What happens?

Loading parquet file into DuckDB table with read_parquet leads to internal error. But, the R arrow package can successfully load the parquet file with no issues leading me to think that the parquet is fine and the duckdb parquet reader may have an issue.

### To Reproduce

Reading these time columns leads to an internal error.
```
library(duckdb)
con <- dbConnect(duckdb())
dbSendQuery(con, "CREATE TABLE mytable AS FROM read_parquet('times.parquet');")
```
Gives error:
```
Error: rapi_prepare: Failed to prepare query CREATE TABLE mytable AS FROM read_parquet('pk/1.pk');
Error: INTERNAL Error: Incorrect stats size for type TIMETZ
This error signals an assertion failure within DuckDB. This usually occurs due to unexpected conditions or errors in the program's logic.
For more information, see https://duckdb.org/docs/dev/internal_errors
```
But using arrow package I get : 
```
arrow::read_parquet('times.parquet')
# A tibble: 1 × 2
  col1   col2  
  <time> <time>
1 00'00" 00'00"
```
I've reduced the original file down to the minimum required to reproduce the internal error, 1 line, 2 columns. I didn't check if only one column is the issue or if its both.
[times.zip](https://github.com/user-attachments/files/16467664/times.zip)


### OS:

Windows 10 Professionnel x64

### DuckDB Version:

R package version 1.0.0-2

### DuckDB Client:

R version 4.3.2

### Full Name:

Eli Daniels

### Affiliation:

ArData

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a source build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://www.duckdb.org/docs/installation) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of extension/parquet/parquet_statistics.cpp]
1: #include "parquet_statistics.hpp"
2: 
3: #include "duckdb.hpp"
4: #include "parquet_decimal_utils.hpp"
5: #include "parquet_timestamp.hpp"
6: #include "string_column_reader.hpp"
7: #include "struct_column_reader.hpp"
8: #ifndef DUCKDB_AMALGAMATION
9: #include "duckdb/common/types/blob.hpp"
10: #include "duckdb/common/types/time.hpp"
11: #include "duckdb/common/types/value.hpp"
12: #include "duckdb/storage/statistics/struct_stats.hpp"
13: #endif
14: 
15: namespace duckdb {
16: 
17: using duckdb_parquet::format::ConvertedType;
18: using duckdb_parquet::format::Type;
19: 
20: static unique_ptr<BaseStatistics> CreateNumericStats(const LogicalType &type,
21:                                                      const duckdb_parquet::format::SchemaElement &schema_ele,
22:                                                      const duckdb_parquet::format::Statistics &parquet_stats) {
23: 	auto stats = NumericStats::CreateUnknown(type);
24: 
25: 	// for reasons unknown to science, Parquet defines *both* `min` and `min_value` as well as `max` and
26: 	// `max_value`. All are optional. such elegance.
27: 	Value min;
28: 	Value max;
29: 	if (parquet_stats.__isset.min_value) {
30: 		min = ParquetStatisticsUtils::ConvertValue(type, schema_ele, parquet_stats.min_value).DefaultCastAs(type);
31: 	} else if (parquet_stats.__isset.min) {
32: 		min = ParquetStatisticsUtils::ConvertValue(type, schema_ele, parquet_stats.min).DefaultCastAs(type);
33: 	} else {
34: 		min = Value(type);
35: 	}
36: 	if (parquet_stats.__isset.max_value) {
37: 		max = ParquetStatisticsUtils::ConvertValue(type, schema_ele, parquet_stats.max_value).DefaultCastAs(type);
38: 	} else if (parquet_stats.__isset.max) {
39: 		max = ParquetStatisticsUtils::ConvertValue(type, schema_ele, parquet_stats.max).DefaultCastAs(type);
40: 	} else {
41: 		max = Value(type);
42: 	}
43: 	NumericStats::SetMin(stats, min);
44: 	NumericStats::SetMax(stats, max);
45: 	return stats.ToUnique();
46: }
47: 
48: Value ParquetStatisticsUtils::ConvertValue(const LogicalType &type,
49:                                            const duckdb_parquet::format::SchemaElement &schema_ele,
50:                                            const std::string &stats) {
51: 	auto stats_data = const_data_ptr_cast(stats.c_str());
52: 	switch (type.id()) {
53: 	case LogicalTypeId::BOOLEAN: {
54: 		if (stats.size() != sizeof(bool)) {
55: 			throw InternalException("Incorrect stats size for type BOOLEAN");
56: 		}
57: 		return Value::BOOLEAN(Load<bool>(stats_data));
58: 	}
59: 	case LogicalTypeId::UTINYINT:
60: 	case LogicalTypeId::USMALLINT:
61: 	case LogicalTypeId::UINTEGER:
62: 		if (stats.size() != sizeof(uint32_t)) {
63: 			throw InternalException("Incorrect stats size for type UINTEGER");
64: 		}
65: 		return Value::UINTEGER(Load<uint32_t>(stats_data));
66: 	case LogicalTypeId::UBIGINT:
67: 		if (stats.size() != sizeof(uint64_t)) {
68: 			throw InternalException("Incorrect stats size for type UBIGINT");
69: 		}
70: 		return Value::UBIGINT(Load<uint64_t>(stats_data));
71: 	case LogicalTypeId::TINYINT:
72: 	case LogicalTypeId::SMALLINT:
73: 	case LogicalTypeId::INTEGER:
74: 		if (stats.size() != sizeof(int32_t)) {
75: 			throw InternalException("Incorrect stats size for type INTEGER");
76: 		}
77: 		return Value::INTEGER(Load<int32_t>(stats_data));
78: 	case LogicalTypeId::BIGINT:
79: 		if (stats.size() != sizeof(int64_t)) {
80: 			throw InternalException("Incorrect stats size for type BIGINT");
81: 		}
82: 		return Value::BIGINT(Load<int64_t>(stats_data));
83: 	case LogicalTypeId::FLOAT: {
84: 		if (stats.size() != sizeof(float)) {
85: 			throw InternalException("Incorrect stats size for type FLOAT");
86: 		}
87: 		auto val = Load<float>(stats_data);
88: 		if (!Value::FloatIsFinite(val)) {
89: 			return Value();
90: 		}
91: 		return Value::FLOAT(val);
92: 	}
93: 	case LogicalTypeId::DOUBLE: {
94: 		switch (schema_ele.type) {
95: 		case Type::FIXED_LEN_BYTE_ARRAY:
96: 		case Type::BYTE_ARRAY:
97: 			// decimals cast to double
98: 			return Value::DOUBLE(ParquetDecimalUtils::ReadDecimalValue<double>(stats_data, stats.size(), schema_ele));
99: 		default:
100: 			break;
101: 		}
102: 		if (stats.size() != sizeof(double)) {
103: 			throw InternalException("Incorrect stats size for type DOUBLE");
104: 		}
105: 		auto val = Load<double>(stats_data);
106: 		if (!Value::DoubleIsFinite(val)) {
107: 			return Value();
108: 		}
109: 		return Value::DOUBLE(val);
110: 	}
111: 	case LogicalTypeId::DECIMAL: {
112: 		auto width = DecimalType::GetWidth(type);
113: 		auto scale = DecimalType::GetScale(type);
114: 		switch (schema_ele.type) {
115: 		case Type::INT32: {
116: 			if (stats.size() != sizeof(int32_t)) {
117: 				throw InternalException("Incorrect stats size for type %s", type.ToString());
118: 			}
119: 			return Value::DECIMAL(Load<int32_t>(stats_data), width, scale);
120: 		}
121: 		case Type::INT64: {
122: 			if (stats.size() != sizeof(int64_t)) {
123: 				throw InternalException("Incorrect stats size for type %s", type.ToString());
124: 			}
125: 			return Value::DECIMAL(Load<int64_t>(stats_data), width, scale);
126: 		}
127: 		case Type::BYTE_ARRAY:
128: 		case Type::FIXED_LEN_BYTE_ARRAY:
129: 			switch (type.InternalType()) {
130: 			case PhysicalType::INT16:
131: 				return Value::DECIMAL(
132: 				    ParquetDecimalUtils::ReadDecimalValue<int16_t>(stats_data, stats.size(), schema_ele), width, scale);
133: 			case PhysicalType::INT32:
134: 				return Value::DECIMAL(
135: 				    ParquetDecimalUtils::ReadDecimalValue<int32_t>(stats_data, stats.size(), schema_ele), width, scale);
136: 			case PhysicalType::INT64:
137: 				return Value::DECIMAL(
138: 				    ParquetDecimalUtils::ReadDecimalValue<int64_t>(stats_data, stats.size(), schema_ele), width, scale);
139: 			case PhysicalType::INT128:
140: 				return Value::DECIMAL(
141: 				    ParquetDecimalUtils::ReadDecimalValue<hugeint_t>(stats_data, stats.size(), schema_ele), width,
142: 				    scale);
143: 			default:
144: 				throw InternalException("Unsupported internal type for decimal");
145: 			}
146: 		default:
147: 			throw InternalException("Unsupported internal type for decimal?..");
148: 		}
149: 	}
150: 	case LogicalType::VARCHAR:
151: 	case LogicalType::BLOB:
152: 		if (Value::StringIsValid(stats)) {
153: 			return Value(stats);
154: 		} else {
155: 			return Value(Blob::ToString(string_t(stats)));
156: 		}
157: 	case LogicalTypeId::DATE:
158: 		if (stats.size() != sizeof(int32_t)) {
159: 			throw InternalException("Incorrect stats size for type DATE");
160: 		}
161: 		return Value::DATE(date_t(Load<int32_t>(stats_data)));
162: 	case LogicalTypeId::TIME: {
163: 		int64_t val;
164: 		if (stats.size() == sizeof(int32_t)) {
165: 			val = Load<int32_t>(stats_data);
166: 		} else if (stats.size() == sizeof(int64_t)) {
167: 			val = Load<int64_t>(stats_data);
168: 		} else {
169: 			throw InternalException("Incorrect stats size for type TIME");
170: 		}
171: 		if (schema_ele.__isset.logicalType && schema_ele.logicalType.__isset.TIME) {
172: 			// logical type
173: 			if (schema_ele.logicalType.TIME.unit.__isset.MILLIS) {
174: 				return Value::TIME(Time::FromTimeMs(val));
175: 			} else if (schema_ele.logicalType.TIME.unit.__isset.NANOS) {
176: 				return Value::TIME(Time::FromTimeNs(val));
177: 			} else if (schema_ele.logicalType.TIME.unit.__isset.MICROS) {
178: 				return Value::TIME(dtime_t(val));
179: 			} else {
180: 				throw InternalException("Time logicalType is set but unit is not defined");
181: 			}
182: 		}
183: 		if (schema_ele.converted_type == duckdb_parquet::format::ConvertedType::TIME_MILLIS) {
184: 			return Value::TIME(Time::FromTimeMs(val));
185: 		} else {
186: 			return Value::TIME(dtime_t(val));
187: 		}
188: 	}
189: 	case LogicalTypeId::TIME_TZ: {
190: 		int64_t val;
191: 		if (stats.size() == sizeof(int64_t)) {
192: 			val = Load<int64_t>(stats_data);
193: 		} else {
194: 			throw InternalException("Incorrect stats size for type TIMETZ");
195: 		}
196: 		if (schema_ele.__isset.logicalType && schema_ele.logicalType.__isset.TIME) {
197: 			// logical type
198: 			if (schema_ele.logicalType.TIME.unit.__isset.MILLIS) {
199: 				return Value::TIMETZ(ParquetIntToTimeMsTZ(NumericCast<int32_t>(val)));
200: 			} else if (schema_ele.logicalType.TIME.unit.__isset.MICROS) {
201: 				return Value::TIMETZ(ParquetIntToTimeTZ(val));
202: 			} else if (schema_ele.logicalType.TIME.unit.__isset.NANOS) {
203: 				return Value::TIMETZ(ParquetIntToTimeNsTZ(val));
204: 			} else {
205: 				throw InternalException("Time With Time Zone logicalType is set but unit is not defined");
206: 			}
207: 		}
208: 		return Value::TIMETZ(ParquetIntToTimeTZ(val));
209: 	}
210: 	case LogicalTypeId::TIMESTAMP:
211: 	case LogicalTypeId::TIMESTAMP_TZ: {
212: 		timestamp_t timestamp_value;
213: 		if (schema_ele.type == Type::INT96) {
214: 			if (stats.size() != sizeof(Int96)) {
215: 				throw InternalException("Incorrect stats size for type TIMESTAMP");
216: 			}
217: 			timestamp_value = ImpalaTimestampToTimestamp(Load<Int96>(stats_data));
218: 		} else {
219: 			D_ASSERT(schema_ele.type == Type::INT64);
220: 			if (stats.size() != sizeof(int64_t)) {
221: 				throw InternalException("Incorrect stats size for type TIMESTAMP");
222: 			}
223: 			auto val = Load<int64_t>(stats_data);
224: 			if (schema_ele.__isset.logicalType && schema_ele.logicalType.__isset.TIMESTAMP) {
225: 				// logical type
226: 				if (schema_ele.logicalType.TIMESTAMP.unit.__isset.MILLIS) {
227: 					timestamp_value = Timestamp::FromEpochMs(val);
228: 				} else if (schema_ele.logicalType.TIMESTAMP.unit.__isset.NANOS) {
229: 					timestamp_value = Timestamp::FromEpochNanoSeconds(val);
230: 				} else if (schema_ele.logicalType.TIMESTAMP.unit.__isset.MICROS) {
231: 					timestamp_value = timestamp_t(val);
232: 				} else {
233: 					throw InternalException("Timestamp logicalType is set but unit is not defined");
234: 				}
235: 			} else if (schema_ele.converted_type == duckdb_parquet::format::ConvertedType::TIMESTAMP_MILLIS) {
236: 				timestamp_value = Timestamp::FromEpochMs(val);
237: 			} else {
238: 				timestamp_value = timestamp_t(val);
239: 			}
240: 		}
241: 		if (type.id() == LogicalTypeId::TIMESTAMP_TZ) {
242: 			return Value::TIMESTAMPTZ(timestamp_value);
243: 		} else {
244: 			return Value::TIMESTAMP(timestamp_value);
245: 		}
246: 	}
247: 	case LogicalTypeId::TIMESTAMP_NS: {
248: 		timestamp_ns_t timestamp_value;
249: 		if (schema_ele.type == Type::INT96) {
250: 			if (stats.size() != sizeof(Int96)) {
251: 				throw InternalException("Incorrect stats size for type TIMESTAMP_NS");
252: 			}
253: 			timestamp_value = ImpalaTimestampToTimestampNS(Load<Int96>(stats_data));
254: 		} else {
255: 			D_ASSERT(schema_ele.type == Type::INT64);
256: 			if (stats.size() != sizeof(int64_t)) {
257: 				throw InternalException("Incorrect stats size for type TIMESTAMP_NS");
258: 			}
259: 			auto val = Load<int64_t>(stats_data);
260: 			if (schema_ele.__isset.logicalType && schema_ele.logicalType.__isset.TIMESTAMP) {
261: 				// logical type
262: 				if (schema_ele.logicalType.TIMESTAMP.unit.__isset.MILLIS) {
263: 					timestamp_value = ParquetTimestampMsToTimestampNs(val);
264: 				} else if (schema_ele.logicalType.TIMESTAMP.unit.__isset.NANOS) {
265: 					timestamp_value = ParquetTimestampNsToTimestampNs(val);
266: 				} else if (schema_ele.logicalType.TIMESTAMP.unit.__isset.MICROS) {
267: 					timestamp_value = ParquetTimestampUsToTimestampNs(val);
268: 				} else {
269: 					throw InternalException("Timestamp (NS) logicalType is set but unit is unknown");
270: 				}
271: 			} else if (schema_ele.converted_type == duckdb_parquet::format::ConvertedType::TIMESTAMP_MILLIS) {
272: 				timestamp_value = ParquetTimestampMsToTimestampNs(val);
273: 			} else {
274: 				timestamp_value = ParquetTimestampUsToTimestampNs(val);
275: 			}
276: 		}
277: 		return Value::TIMESTAMPNS(timestamp_value);
278: 	}
279: 	default:
280: 		throw InternalException("Unsupported type for stats %s", type.ToString());
281: 	}
282: }
283: 
284: unique_ptr<BaseStatistics> ParquetStatisticsUtils::TransformColumnStatistics(const ColumnReader &reader,
285:                                                                              const vector<ColumnChunk> &columns) {
286: 
287: 	// Not supported types
288: 	if (reader.Type().id() == LogicalTypeId::ARRAY || reader.Type().id() == LogicalTypeId::MAP ||
289: 	    reader.Type().id() == LogicalTypeId::LIST) {
290: 		return nullptr;
291: 	}
292: 
293: 	unique_ptr<BaseStatistics> row_group_stats;
294: 
295: 	// Structs are handled differently (they dont have stats)
296: 	if (reader.Type().id() == LogicalTypeId::STRUCT) {
297: 		auto struct_stats = StructStats::CreateUnknown(reader.Type());
298: 		auto &struct_reader = reader.Cast<StructColumnReader>();
299: 		// Recurse into child readers
300: 		for (idx_t i = 0; i < struct_reader.child_readers.size(); i++) {
301: 			auto &child_reader = *struct_reader.child_readers[i];
302: 			auto child_stats = ParquetStatisticsUtils::TransformColumnStatistics(child_reader, columns);
303: 			StructStats::SetChildStats(struct_stats, i, std::move(child_stats));
304: 		}
305: 		row_group_stats = struct_stats.ToUnique();
306: 
307: 		// null count is generic
308: 		if (row_group_stats) {
309: 			row_group_stats->Set(StatsInfo::CAN_HAVE_NULL_AND_VALID_VALUES);
310: 		}
311: 		return row_group_stats;
312: 	}
313: 
314: 	// Otherwise, its a standard column with stats
315: 
316: 	auto &column_chunk = columns[reader.FileIdx()];
317: 	if (!column_chunk.__isset.meta_data || !column_chunk.meta_data.__isset.statistics) {
318: 		// no stats present for row group
319: 		return nullptr;
320: 	}
321: 	auto &parquet_stats = column_chunk.meta_data.statistics;
322: 
323: 	auto &type = reader.Type();
324: 	auto &s_ele = reader.Schema();
325: 
326: 	switch (type.id()) {
327: 	case LogicalTypeId::UTINYINT:
328: 	case LogicalTypeId::USMALLINT:
329: 	case LogicalTypeId::UINTEGER:
330: 	case LogicalTypeId::UBIGINT:
331: 	case LogicalTypeId::TINYINT:
332: 	case LogicalTypeId::SMALLINT:
333: 	case LogicalTypeId::INTEGER:
334: 	case LogicalTypeId::BIGINT:
335: 	case LogicalTypeId::FLOAT:
336: 	case LogicalTypeId::DOUBLE:
337: 	case LogicalTypeId::DATE:
338: 	case LogicalTypeId::TIME:
339: 	case LogicalTypeId::TIME_TZ:
340: 	case LogicalTypeId::TIMESTAMP:
341: 	case LogicalTypeId::TIMESTAMP_TZ:
342: 	case LogicalTypeId::TIMESTAMP_SEC:
343: 	case LogicalTypeId::TIMESTAMP_MS:
344: 	case LogicalTypeId::TIMESTAMP_NS:
345: 	case LogicalTypeId::DECIMAL:
346: 		row_group_stats = CreateNumericStats(type, s_ele, parquet_stats);
347: 		break;
348: 	case LogicalTypeId::VARCHAR: {
349: 		auto string_stats = StringStats::CreateEmpty(type);
350: 		if (parquet_stats.__isset.min_value) {
351: 			StringColumnReader::VerifyString(parquet_stats.min_value.c_str(), parquet_stats.min_value.size(), true);
352: 			StringStats::Update(string_stats, parquet_stats.min_value);
353: 		} else if (parquet_stats.__isset.min) {
354: 			StringColumnReader::VerifyString(parquet_stats.min.c_str(), parquet_stats.min.size(), true);
355: 			StringStats::Update(string_stats, parquet_stats.min);
356: 		} else {
357: 			return nullptr;
358: 		}
359: 		if (parquet_stats.__isset.max_value) {
360: 			StringColumnReader::VerifyString(parquet_stats.max_value.c_str(), parquet_stats.max_value.size(), true);
361: 			StringStats::Update(string_stats, parquet_stats.max_value);
362: 		} else if (parquet_stats.__isset.max) {
363: 			StringColumnReader::VerifyString(parquet_stats.max.c_str(), parquet_stats.max.size(), true);
364: 			StringStats::Update(string_stats, parquet_stats.max);
365: 		} else {
366: 			return nullptr;
367: 		}
368: 		StringStats::SetContainsUnicode(string_stats);
369: 		StringStats::ResetMaxStringLength(string_stats);
370: 		row_group_stats = string_stats.ToUnique();
371: 		break;
372: 	}
373: 	default:
374: 		// no stats for you
375: 		break;
376: 	} // end of type switch
377: 
378: 	// null count is generic
379: 	if (row_group_stats) {
380: 		row_group_stats->Set(StatsInfo::CAN_HAVE_NULL_AND_VALID_VALUES);
381: 		if (parquet_stats.__isset.null_count && parquet_stats.null_count == 0) {
382: 			row_group_stats->Set(StatsInfo::CANNOT_HAVE_NULL_VALUES);
383: 		}
384: 	}
385: 	return row_group_stats;
386: }
387: 
388: } // namespace duckdb
[end of extension/parquet/parquet_statistics.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: