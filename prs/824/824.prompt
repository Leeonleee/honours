You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Error when insert timestamps containing T delimiter
I'm trying to import a table that contains a series of timestamps that contain a 'T' delimiter (e.g. 2009-11-11T15:21:19) as per the ISO 8601 standard but I'm getting an error from duckdb. As a short example (using the python interface):
```
con = duckdb.connect(database=':memory:', read_only=False)
con.execute(f"CREATE TABLE tmp(testdate TIMESTAMP);")
con.execute(f"INSERT INTO tmp VALUES ('2009-11-11 15:21:19'), ('2009-11-11T15:21:19');")
```
results in the following error
```
RuntimeError: Conversion: time field value out of range: "T15:21:19", expected format is ([YYY-MM-DD ]HH:MM:SS[.MS])
```
It seemed odd to me that only the second part of the data format was returned as an error. I did some digging and FromString(string str) in 'src/common/types/timestamp.cpp' seems to indicate that the 'T' should be accepted, at least based on the comments and my quick read of the code. 
```
// string format is YYYY-MM-DDThh:mm:ssZ
// T may be a space
```

Am I doing something incorrect in my insertion in the above example or is there some sort of quotes/escaping I can use to make this work? 







</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="30">
2: 
3: [![Travis](https://api.travis-ci.org/cwida/duckdb.svg?branch=master)](https://travis-ci.org/cwida/duckdb)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
6: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3901452.svg)](https://zenodo.org/record/3901452)
7: 
8: 
9: ## Installation
10: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
11: 
12: ## Development
13: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
14: 
15: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
16: 
17: 
[end of README.md]
[start of benchmark/interpreted_benchmark.cpp]
1: #include "interpreted_benchmark.hpp"
2: #include "duckdb.hpp"
3: 
4: #include <fstream>
5: #include <sstream>
6: 
7: #include "duckdb/common/string_util.hpp"
8: #include "duckdb/main/client_context.hpp"
9: 
10: namespace duckdb {
11: 
12: static string ParseGroupFromPath(string file) {
13: 	string extension = "";
14: 	// move backwards to the last slash
15: 	int group_begin = -1, group_end = -1;
16: 	for(size_t i = file.size(); i > 0; i--) {
17: 		if (file[i - 1] == '/' || file[i - 1] == '\\') {
18: 			if (group_end == -1) {
19: 				group_end = i - 1;
20: 			} else {
21: 				group_begin = i;
22: 				return "[" + file.substr(group_begin, group_end - group_begin) + "]" + extension;
23: 			}
24: 		}
25: 	}
26: 	if (group_end == -1) {
27: 		return "[" + file + "]" + extension;
28: 	}
29: 	return "[" + file.substr(0, group_end) + "]" + extension;
30: }
31: 
32: struct InterpretedBenchmarkState : public BenchmarkState {
33: 	DuckDB db;
34: 	Connection con;
35: 	unique_ptr<MaterializedQueryResult> result;
36: 
37: 	InterpretedBenchmarkState() : db(nullptr), con(db) {
38: 		con.EnableProfiling();
39: 	}
40: };
41: 
42: InterpretedBenchmark::InterpretedBenchmark(string full_path) :
43: 	Benchmark(true, full_path, ParseGroupFromPath(full_path)), benchmark_path(full_path) {
44: }
45: 
46: void InterpretedBenchmark::LoadBenchmark() {
47: 	std::ifstream infile(benchmark_path);
48: 	std::string line;
49: 	int linenr = 0;
50: 	while (std::getline(infile, line)) {
51: 		linenr++;
52: 		// skip comments
53: 		if (line[0] == '#') {
54: 			continue;
55: 		}
56: 		StringUtil::Trim(line);
57: 		// skip blank lines
58: 		if (line.empty()) {
59: 			continue;
60: 		}
61: 		// look for a command in this line
62: 		auto splits = StringUtil::Split(StringUtil::Lower(line), ' ');
63: 		if (splits[0] == "load") {
64: 			if (!init_query.empty()) {
65: 				throw std::runtime_error("Multiple calls to LOAD in the same benchmark file");
66: 			}
67: 			// load command: keep reading until we find a blank line or EOF
68: 			while (std::getline(infile, line)) {
69: 				linenr++;
70: 				StringUtil::Trim(line);
71: 				if (line.empty()) {
72: 					break;
73: 				} else {
74: 					init_query += line;
75: 				}
76: 			}
77: 		} else if (splits[0] == "run") {
78: 			if (!run_query.empty()) {
79: 				throw std::runtime_error("Multiple calls to RUN in the same benchmark file");
80: 			}
81: 			// load command: keep reading until we find a blank line or EOF
82: 			while (std::getline(infile, line)) {
83: 				linenr++;
84: 				StringUtil::Trim(line);
85: 				if (line.empty()) {
86: 					break;
87: 				} else {
88: 					run_query += line;
89: 				}
90: 			}
91: 		} else if (splits[0] == "result") {
92: 			if (result_column_count > 0) {
93: 				throw std::runtime_error("multiple results found!");
94: 			}
95: 			// count the amount of columns
96: 			if (splits.size() <= 1 || splits[1].size() == 0) {
97: 				throw std::runtime_error("result must be followed by a column count (e.g. result III)");
98: 			}
99: 			for(int i = 0; i < splits[1].size(); i++) {
100: 				if (splits[1][i] != 'i') {
101: 					throw std::runtime_error("result must be followed by a column count (e.g. result III)");
102: 				}
103: 			}
104: 			result_column_count = splits[1].size();
105: 			// keep reading results until eof
106: 			while (std::getline(infile, line)) {
107: 				linenr++;
108: 				auto result_splits = StringUtil::Split(line, "\t");
109: 				if (result_splits.size() != result_column_count) {
110: 					throw std::runtime_error("error on line " + to_string(linenr) + ", expected " + to_string(result_column_count) + " values but got " + to_string(result_splits.size()));
111: 				}
112: 				result_values.push_back(move(result_splits));
113: 			}
114: 		}
115: 	}
116: 
117: }
118: 
119: unique_ptr<BenchmarkState> InterpretedBenchmark::Initialize() {
120: 	LoadBenchmark();
121: 
122: 	auto state = make_unique<InterpretedBenchmarkState>();
123: 	state->con.Query(init_query);
124: 	return state;
125: }
126: 
127: void InterpretedBenchmark::Run(BenchmarkState *state_) {
128: 	auto &state = (InterpretedBenchmarkState &) *state_;
129: 	state.result = state.con.Query(run_query);
130: }
131: 
132: void InterpretedBenchmark::Cleanup(BenchmarkState *state) {
133: 
134: }
135: 
136: string InterpretedBenchmark::Verify(BenchmarkState *state_) {
137: 	auto &state = (InterpretedBenchmarkState &) *state_;
138: 	if (!state.result->success) {
139: 		return state.result->error;
140: 	}
141: 	if (result_column_count == 0) {
142: 		// no result specified
143: 		return string();
144: 	}
145: 	// compare the column count
146: 	if (state.result->column_count() != result_column_count) {
147: 		return StringUtil::Format("Error in result: expected %lld columns but got %lld", (int64_t) result_column_count, (int64_t) state.result->column_count());
148: 	}
149: 	// compare row count
150: 	if (state.result->collection.count != result_values.size()) {
151: 		return StringUtil::Format("Error in result: expected %lld rows but got %lld", (int64_t) state.result->collection.count, (int64_t) result_values.size());
152: 	}
153: 	// compare values
154: 	for(int64_t r = 0; r < (int64_t) result_values.size(); r++) {
155: 		for(int64_t c = 0; c < result_column_count; c++) {
156: 			auto value = state.result->collection.GetValue(c, r);
157: 			Value verify_val(result_values[r][c]);
158: 			verify_val = verify_val.CastAs(SQLType::VARCHAR, state.result->sql_types[c]);
159: 			if (!Value::ValuesAreEqual(value, verify_val)) {
160: 				return StringUtil::Format("Error in result on row %lld column %lld: expected value \"%s\" but got value \"%s\"", r, c, verify_val.ToString().c_str(), value.ToString().c_str());
161: 			}
162: 		}
163: 	}
164: 	return string();
165: }
166: 
167: 
168: void InterpretedBenchmark::Interrupt(BenchmarkState *state_) {
169: 	auto &state = (InterpretedBenchmarkState &) *state_;
170: 	state.con.Interrupt();
171: }
172: 
173: string InterpretedBenchmark::BenchmarkInfo() {
174: 	return name + " - " + run_query;
175: }
176: 
177: string InterpretedBenchmark::GetLogOutput(BenchmarkState *state_) {
178: 	auto &state = (InterpretedBenchmarkState &) *state_;
179: 	return state.con.context->profiler.ToJSON();
180: }
181: 
182: }
[end of benchmark/interpreted_benchmark.cpp]
[start of extension/parquet/parquet-extension.cpp]
1: #include <string>
2: #include <vector>
3: #include <bitset>
4: #include <fstream>
5: #include <cstring>
6: #include <iostream>
7: #include <sstream>
8: 
9: #include "parquet-extension.hpp"
10: 
11: #ifndef DUCKDB_AMALGAMATION
12: #include "duckdb/function/table_function.hpp"
13: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
14: #include "duckdb/parser/parsed_data/create_copy_function_info.hpp"
15: #include "duckdb/main/client_context.hpp"
16: #include "duckdb/main/connection.hpp"
17: #include "duckdb/common/types/date.hpp"
18: #include "duckdb/common/types/time.hpp"
19: #include "duckdb/common/types/timestamp.hpp"
20: #include "duckdb/common/serializer/buffered_file_writer.hpp"
21: #include "duckdb/common/serializer/buffered_serializer.hpp"
22: 
23: #include "utf8proc_wrapper.hpp"
24: 
25: #include "thrift/protocol/TCompactProtocol.h"
26: #include "thrift/transport/TBufferTransports.h"
27: #include "parquet_types.h"
28: #include "snappy.h"
29: #include "miniz.hpp"
30: #endif
31: 
32: using namespace duckdb;
33: using namespace std;
34: 
35: using namespace parquet;
36: using namespace parquet::format;
37: using namespace apache::thrift;
38: using namespace apache::thrift::protocol;
39: using namespace apache::thrift::transport;
40: 
41: struct Int96 {
42: 	uint32_t value[3];
43: };
44: 
45: class ByteBuffer { // on to the 10 thousandth impl
46: public:
47: 	char *ptr = nullptr;
48: 	uint64_t len = 0;
49: 
50: 	ByteBuffer(){};
51: 	ByteBuffer(char *ptr, uint64_t len) : ptr(ptr), len(len){};
52: 
53: 	void inc(uint64_t increment) {
54: 		available(increment);
55: 		len -= increment;
56: 		ptr += increment;
57: 	}
58: 
59: 	template <class T> T read() {
60: 		available(sizeof(T));
61: 		T val = *(T *)ptr;
62: 		inc(sizeof(T));
63: 		return val;
64: 	}
65: 
66: 	void copy_to(char *dest, uint64_t len) {
67: 		available(len);
68: 		memcpy(dest, ptr, len);
69: 	}
70: 
71: 	void available(uint64_t req_len) {
72: 		if (req_len > len) {
73: 			throw runtime_error("Out of buffer");
74: 		}
75: 	}
76: };
77: 
78: class ResizeableBuffer : public ByteBuffer {
79: public:
80: 	void resize(uint64_t new_size) {
81: 		if (new_size > len) {
82: 			auto new_holder = std::unique_ptr<char[]>(new char[new_size]);
83: 			holder = move(new_holder);
84: 		}
85: 		len = new_size;
86: 		ptr = holder.get();
87: 	}
88: 
89: private:
90: 	std::unique_ptr<char[]> holder = nullptr;
91: };
92: 
93: static TCompactProtocolFactoryT<TMemoryBuffer> tproto_factory;
94: 
95: template <class T> static void thrift_unpack(const uint8_t *buf, uint32_t *len, T *deserialized_msg) {
96: 	shared_ptr<TMemoryBuffer> tmem_transport(new TMemoryBuffer(const_cast<uint8_t *>(buf), *len));
97: 	shared_ptr<TProtocol> tproto = tproto_factory.getProtocol(tmem_transport);
98: 	try {
99: 		deserialized_msg->read(tproto.get());
100: 	} catch (std::exception &e) {
101: 		std::stringstream ss;
102: 		ss << "Couldn't deserialize thrift: " << e.what() << "\n";
103: 		throw std::runtime_error(ss.str());
104: 	}
105: 	uint32_t bytes_left = tmem_transport->available_read();
106: 	*len = *len - bytes_left;
107: }
108: 
109: // adapted from arrow parquet reader
110: class RleBpDecoder {
111: 
112: public:
113: 	/// Create a decoder object. buffer/buffer_len is the decoded data.
114: 	/// bit_width is the width of each value (before encoding).
115: 	RleBpDecoder(const uint8_t *buffer, uint32_t buffer_len, uint32_t bit_width)
116: 	    : buffer(buffer), bit_width_(bit_width), current_value_(0), repeat_count_(0), literal_count_(0) {
117: 
118: 		if (bit_width >= 64) {
119: 			throw runtime_error("Decode bit width too large");
120: 		}
121: 		byte_encoded_len = ((bit_width_ + 7) / 8);
122: 		max_val = (1 << bit_width_) - 1;
123: 	}
124: 
125: 	/// Gets a batch of values.  Returns the number of decoded elements.
126: 	template <typename T> void GetBatch(char *values_target_ptr, uint32_t batch_size) {
127: 		auto values = (T *)values_target_ptr;
128: 		uint32_t values_read = 0;
129: 
130: 		while (values_read < batch_size) {
131: 			if (repeat_count_ > 0) {
132: 				int repeat_batch = std::min(batch_size - values_read, static_cast<uint32_t>(repeat_count_));
133: 				std::fill(values + values_read, values + values_read + repeat_batch, static_cast<T>(current_value_));
134: 				repeat_count_ -= repeat_batch;
135: 				values_read += repeat_batch;
136: 			} else if (literal_count_ > 0) {
137: 				uint32_t literal_batch = std::min(batch_size - values_read, static_cast<uint32_t>(literal_count_));
138: 				uint32_t actual_read = BitUnpack<T>(values + values_read, literal_batch);
139: 				if (literal_batch != actual_read) {
140: 					throw runtime_error("Did not find enough values");
141: 				}
142: 				literal_count_ -= literal_batch;
143: 				values_read += literal_batch;
144: 			} else {
145: 				if (!NextCounts<T>()) {
146: 					if (values_read != batch_size) {
147: 						throw runtime_error("RLE decode did not find enough values");
148: 					}
149: 					return;
150: 				}
151: 			}
152: 		}
153: 		if (values_read != batch_size) {
154: 			throw runtime_error("RLE decode did not find enough values");
155: 		}
156: 	}
157: 
158: private:
159: 	const uint8_t *buffer;
160: 
161: 	/// Number of bits needed to encode the value. Must be between 0 and 64.
162: 	int bit_width_;
163: 	uint64_t current_value_;
164: 	uint32_t repeat_count_;
165: 	uint32_t literal_count_;
166: 	uint8_t byte_encoded_len;
167: 	uint32_t max_val;
168: 
169: 	int8_t bitpack_pos = 0;
170: 
171: 	// this is slow but whatever, calls are rare
172: 	static uint8_t VarintDecode(const uint8_t *source, uint32_t *result_out) {
173: 		uint32_t result = 0;
174: 		uint8_t shift = 0;
175: 		uint8_t len = 0;
176: 		while (true) {
177: 			auto byte = *source++;
178: 			len++;
179: 			result |= (byte & 127) << shift;
180: 			if ((byte & 128) == 0)
181: 				break;
182: 			shift += 7;
183: 			if (shift > 32) {
184: 				throw runtime_error("Varint-decoding found too large number");
185: 			}
186: 		}
187: 		*result_out = result;
188: 		return len;
189: 	}
190: 
191: 	/// Fills literal_count_ and repeat_count_ with next values. Returns false if there
192: 	/// are no more.
193: 	template <typename T> bool NextCounts() {
194: 		// Read the next run's indicator int, it could be a literal or repeated run.
195: 		// The int is encoded as a vlq-encoded value.
196: 		uint32_t indicator_value;
197: 		if (bitpack_pos != 0) {
198: 			buffer++;
199: 			bitpack_pos = 0;
200: 		}
201: 		buffer += VarintDecode(buffer, &indicator_value);
202: 
203: 		// lsb indicates if it is a literal run or repeated run
204: 		bool is_literal = indicator_value & 1;
205: 		if (is_literal) {
206: 			literal_count_ = (indicator_value >> 1) * 8;
207: 		} else {
208: 			repeat_count_ = indicator_value >> 1;
209: 			// (ARROW-4018) this is not big-endian compatible, lol
210: 			current_value_ = 0;
211: 			for (auto i = 0; i < byte_encoded_len; i++) {
212: 				current_value_ |= ((uint8_t)*buffer++) << (i * 8);
213: 			}
214: 			// sanity check
215: 			if (repeat_count_ > 0 && current_value_ > max_val) {
216: 				throw runtime_error("Payload value bigger than allowed. Corrupted file?");
217: 			}
218: 		}
219: 		// TODO complain if we run out of buffer
220: 		return true;
221: 	}
222: 
223: 	// somewhat optimized implementation that avoids non-alignment
224: 
225: 	static const uint32_t BITPACK_MASKS[];
226: 	static const uint8_t BITPACK_DLEN;
227: 
228: 	template <typename T> uint32_t BitUnpack(T *dest, uint32_t count) {
229: 		assert(bit_width_ < 32);
230: 
231: 		// auto source = buffer;
232: 		auto mask = BITPACK_MASKS[bit_width_];
233: 
234: 		for (uint32_t i = 0; i < count; i++) {
235: 			T val = (*buffer >> bitpack_pos) & mask;
236: 			bitpack_pos += bit_width_;
237: 			while (bitpack_pos > BITPACK_DLEN) {
238: 				val |= (*++buffer << (BITPACK_DLEN - (bitpack_pos - bit_width_))) & mask;
239: 				bitpack_pos -= BITPACK_DLEN;
240: 			}
241: 			dest[i] = val;
242: 		}
243: 		return count;
244: 	}
245: };
246: 
247: const uint32_t RleBpDecoder::BITPACK_MASKS[] = {
248:     0,       1,       3,        7,        15,       31,        63,        127,       255,        511,       1023,
249:     2047,    4095,    8191,     16383,    32767,    65535,     131071,    262143,    524287,     1048575,   2097151,
250:     4194303, 8388607, 16777215, 33554431, 67108863, 134217727, 268435455, 536870911, 1073741823, 2147483647};
251: 
252: const uint8_t RleBpDecoder::BITPACK_DLEN = 8;
253: 
254: // surely they are joking
255: static constexpr int64_t kJulianToUnixEpochDays = 2440588LL;
256: static constexpr int64_t kMillisecondsInADay = 86400000LL;
257: static constexpr int64_t kNanosecondsInADay = kMillisecondsInADay * 1000LL * 1000LL;
258: 
259: static int64_t impala_timestamp_to_nanoseconds(const Int96 &impala_timestamp) {
260: 	int64_t days_since_epoch = impala_timestamp.value[2] - kJulianToUnixEpochDays;
261: 	int64_t nanoseconds = *(reinterpret_cast<const int64_t *>(&(impala_timestamp.value)));
262: 	return days_since_epoch * kNanosecondsInADay + nanoseconds;
263: }
264: 
265: static timestamp_t impala_timestamp_to_timestamp_t(const Int96 &raw_ts) {
266: 	auto impala_ns = impala_timestamp_to_nanoseconds(raw_ts);
267: 	auto ms = impala_ns / 1000000; // nanoseconds
268: 	auto ms_per_day = (int64_t)60 * 60 * 24 * 1000;
269: 	date_t date = Date::EpochToDate(ms / 1000);
270: 	dtime_t time = (dtime_t)(ms % ms_per_day);
271: 	return Timestamp::FromDatetime(date, time);
272: }
273: 
274: static Int96 timestamp_t_to_impala_timestamp(timestamp_t &ts) {
275: 	int32_t hour, min, sec, msec;
276: 	Time::Convert(Timestamp::GetTime(ts), hour, min, sec, msec);
277: 	uint64_t ms_since_midnight = hour * 60 * 60 * 1000 + min * 60 * 1000 + sec * 1000 + msec;
278: 	auto days_since_epoch = Date::Epoch(Timestamp::GetDate(ts)) / (24 * 60 * 60);
279: 	// first two uint32 in Int96 are nanoseconds since midnights
280: 	// last uint32 is number of days since year 4713 BC ("Julian date")
281: 	Int96 impala_ts;
282: 	*((uint64_t *)impala_ts.value) = ms_since_midnight * 1000000;
283: 	impala_ts.value[2] = days_since_epoch + kJulianToUnixEpochDays;
284: 	return impala_ts;
285: }
286: 
287: struct ParquetScanColumnData {
288: 	idx_t chunk_offset;
289: 
290: 	idx_t page_offset;
291: 	idx_t page_value_count = 0;
292: 
293: 	idx_t dict_size;
294: 
295: 	ResizeableBuffer buf;
296: 	ResizeableBuffer decompressed_buf; // only used for compressed files
297: 	ResizeableBuffer dict;
298: 	ResizeableBuffer offset_buf;
299: 	ResizeableBuffer defined_buf;
300: 
301: 	ByteBuffer payload;
302: 
303: 	Encoding::type page_encoding;
304: 	// these point into buf or decompressed_buf
305: 	unique_ptr<RleBpDecoder> defined_decoder;
306: 	unique_ptr<RleBpDecoder> dict_decoder;
307: 
308: 	unique_ptr<ChunkCollection> string_collection;
309: };
310: 
311: struct ParquetScanFunctionData : public TableFunctionData {
312: 	int64_t current_group;
313: 	int64_t group_offset;
314: 
315: 	ifstream pfile;
316: 
317: 	FileMetaData file_meta_data;
318: 	vector<SQLType> sql_types;
319: 	vector<ParquetScanColumnData> column_data;
320: 	bool finished;
321: };
322: 
323: class ParquetScanFunction : public TableFunction {
324: public:
325: 	ParquetScanFunction()
326: 	    : TableFunction("parquet_scan", {SQLType::VARCHAR}, parquet_scan_bind, parquet_scan_function, nullptr) {
327: 		supports_projection = true;
328: 	}
329: 
330: private:
331: 	static unique_ptr<FunctionData> parquet_scan_bind(ClientContext &context, vector<Value> inputs,
332: 	                                                  vector<SQLType> &return_types, vector<string> &names) {
333: 
334: 		auto file_name = inputs[0].GetValue<string>();
335: 		auto res = make_unique<ParquetScanFunctionData>();
336: 
337: 		auto &pfile = res->pfile;
338: 		auto &file_meta_data = res->file_meta_data;
339: 
340: 		pfile.open(file_name, std::ios::binary);
341: 
342: 		ResizeableBuffer buf;
343: 		buf.resize(4);
344: 		memset(buf.ptr, '\0', 4);
345: 		// check for magic bytes at start of file
346: 		pfile.read(buf.ptr, 4);
347: 		if (strncmp(buf.ptr, "PAR1", 4) != 0) {
348: 			throw runtime_error("File not found or missing magic bytes");
349: 		}
350: 
351: 		// check for magic bytes at end of file
352: 		pfile.seekg(-4, ios_base::end);
353: 		pfile.read(buf.ptr, 4);
354: 		if (strncmp(buf.ptr, "PAR1", 4) != 0) {
355: 			throw runtime_error("No magic bytes found at end of file");
356: 		}
357: 
358: 		// read four-byte footer length from just before the end magic bytes
359: 		pfile.seekg(-8, ios_base::end);
360: 		pfile.read(buf.ptr, 4);
361: 		int32_t footer_len = *(uint32_t *)buf.ptr;
362: 		if (footer_len == 0) {
363: 			throw runtime_error("Footer length can't be 0");
364: 		}
365: 
366: 		// read footer into buffer and de-thrift
367: 		buf.resize(footer_len);
368: 		pfile.seekg(-(footer_len + 8), ios_base::end);
369: 		pfile.read(buf.ptr, footer_len);
370: 		if (!pfile) {
371: 			throw runtime_error("Could not read footer");
372: 		}
373: 
374: 		thrift_unpack((const uint8_t *)buf.ptr, (uint32_t *)&footer_len, &file_meta_data);
375: 
376: 		if (file_meta_data.__isset.encryption_algorithm) {
377: 			throw runtime_error("Encrypted Parquet files are not supported");
378: 		}
379: 		// check if we like this schema
380: 		if (file_meta_data.schema.size() < 2) {
381: 			throw runtime_error("Need at least one column in the file");
382: 		}
383: 		if (file_meta_data.schema[0].num_children != (int32_t)(file_meta_data.schema.size() - 1)) {
384: 			throw runtime_error("Only flat tables are supported (no nesting)");
385: 		}
386: 
387: 		// skip the first column its the root and otherwise useless
388: 		for (uint64_t col_idx = 1; col_idx < file_meta_data.schema.size(); col_idx++) {
389: 			auto &s_ele = file_meta_data.schema[col_idx];
390: 			if (!s_ele.__isset.type || s_ele.num_children > 0) {
391: 				throw runtime_error("Only flat tables are supported (no nesting)");
392: 			}
393: 			// if this is REQUIRED, there are no defined levels in file, seems unused
394: 			// if field is REPEATED, no bueno
395: 			if (s_ele.repetition_type != FieldRepetitionType::OPTIONAL) {
396: 				throw runtime_error("Only OPTIONAL fields support");
397: 			}
398: 
399: 			names.push_back(s_ele.name);
400: 			SQLType type;
401: 			switch (s_ele.type) {
402: 			case Type::BOOLEAN:
403: 				type = SQLType::BOOLEAN;
404: 				break;
405: 			case Type::INT32:
406: 				type = SQLType::INTEGER;
407: 				break;
408: 			case Type::INT64:
409: 				type = SQLType::BIGINT;
410: 				break;
411: 			case Type::INT96: // always a timestamp?
412: 				type = SQLType::TIMESTAMP;
413: 				break;
414: 			case Type::FLOAT:
415: 				type = SQLType::FLOAT;
416: 				break;
417: 			case Type::DOUBLE:
418: 				type = SQLType::DOUBLE;
419: 				break;
420: 				//			case parquet::format::Type::FIXED_LEN_BYTE_ARRAY: {
421: 				// TODO some decimals yuck
422: 			case Type::BYTE_ARRAY:
423: 				type = SQLType::VARCHAR;
424: 				break;
425: 
426: 			default:
427: 				throw NotImplementedException("Invalid type");
428: 				break;
429: 			}
430: 
431: 			return_types.push_back(type);
432: 			res->sql_types.push_back(type);
433: 		}
434: 		res->group_offset = 0;
435: 		res->current_group = -1;
436: 		res->column_data.resize(return_types.size());
437: 		res->finished = false;
438: 		return move(res);
439: 	}
440: 
441: 	template <class T>
442: 	static void _fill_from_dict(ParquetScanColumnData &col_data, idx_t count, Vector &target, idx_t target_offset) {
443: 		for (idx_t i = 0; i < count; i++) {
444: 			if (col_data.defined_buf.ptr[i]) {
445: 				auto offset = col_data.offset_buf.read<uint32_t>();
446: 				if (offset > col_data.dict_size) {
447: 					throw runtime_error("Offset " + to_string(offset) + " greater than dictionary size " +
448: 					                    to_string(col_data.dict_size) + " at " + to_string(i + target_offset) +
449: 					                    ". Corrupt file?");
450: 				}
451: 				((T *)FlatVector::GetData(target))[i + target_offset] = ((const T *)col_data.dict.ptr)[offset];
452: 			} else {
453: 				FlatVector::SetNull(target, i + target_offset, true);
454: 			}
455: 		}
456: 	}
457: 
458: 	template <class T>
459: 	static void _fill_from_plain(ParquetScanColumnData &col_data, idx_t count, Vector &target, idx_t target_offset) {
460: 		for (idx_t i = 0; i < count; i++) {
461: 			if (col_data.defined_buf.ptr[i]) {
462: 				((T *)FlatVector::GetData(target))[i + target_offset] = col_data.payload.read<T>();
463: 			} else {
464: 				FlatVector::SetNull(target, i + target_offset, true);
465: 			}
466: 		}
467: 	}
468: 
469: 	static const uint8_t GZIP_HEADER_MINSIZE = 10;
470: 	static const uint8_t GZIP_COMPRESSION_DEFLATE = 0x08;
471: 	static const unsigned char GZIP_FLAG_UNSUPPORTED = 0x1 | 0x2 | 0x4 | 0x10 | 0x20;
472: 
473: 	static bool _prepare_page_buffers(ParquetScanFunctionData &data, idx_t col_idx) {
474: 		auto &col_data = data.column_data[col_idx];
475: 		auto &chunk = data.file_meta_data.row_groups[data.current_group].columns[col_idx];
476: 
477: 		// clean up a bit to avoid nasty surprises
478: 		col_data.payload.ptr = nullptr;
479: 		col_data.payload.len = 0;
480: 		col_data.dict_decoder = nullptr;
481: 		col_data.defined_decoder = nullptr;
482: 
483: 		auto page_header_len = col_data.buf.len;
484: 		if (page_header_len < 1) {
485: 			throw runtime_error("Ran out of bytes to read header from. File corrupt?");
486: 		}
487: 		PageHeader page_hdr;
488: 		thrift_unpack((const uint8_t *)col_data.buf.ptr + col_data.chunk_offset, (uint32_t *)&page_header_len,
489: 		              &page_hdr);
490: 
491: 		// the payload starts behind the header, obvsl.
492: 		col_data.buf.inc(page_header_len);
493: 
494: 		col_data.payload.len = page_hdr.uncompressed_page_size;
495: 
496: 		// handle compression, in the end we expect a pointer to uncompressed parquet data in payload_ptr
497: 		switch (chunk.meta_data.codec) {
498: 		case CompressionCodec::UNCOMPRESSED:
499: 			col_data.payload.ptr = col_data.buf.ptr;
500: 			break;
501: 
502: 		case CompressionCodec::SNAPPY: {
503: 			col_data.decompressed_buf.resize(page_hdr.uncompressed_page_size);
504: 			auto res =
505: 			    snappy::RawUncompress(col_data.buf.ptr, page_hdr.compressed_page_size, col_data.decompressed_buf.ptr);
506: 			if (!res) {
507: 				throw runtime_error("Decompression failure");
508: 			}
509: 			col_data.payload.ptr = col_data.decompressed_buf.ptr;
510: 			break;
511: 		}
512: 		case CompressionCodec::GZIP: {
513: 			struct MiniZStream {
514: 				~MiniZStream() {
515: 					if (init) {
516: 						mz_inflateEnd(&stream);
517: 					}
518: 				}
519: 
520: 				mz_stream stream;
521: 				bool init = false;
522: 			} s;
523: 			auto &stream = s.stream;
524: 			memset(&stream, 0, sizeof(mz_stream));
525: 
526: 			auto mz_ret = mz_inflateInit2(&stream, -MZ_DEFAULT_WINDOW_BITS);
527: 			if (mz_ret != MZ_OK) {
528: 				throw Exception("Failed to initialize miniz");
529: 			}
530: 			s.init = true;
531: 
532: 			col_data.buf.available(GZIP_HEADER_MINSIZE);
533: 			auto gzip_hdr = (const unsigned char *)col_data.buf.ptr;
534: 
535: 			if (gzip_hdr[0] != 0x1F || gzip_hdr[1] != 0x8B || gzip_hdr[2] != GZIP_COMPRESSION_DEFLATE ||
536: 			    gzip_hdr[3] & GZIP_FLAG_UNSUPPORTED) {
537: 				throw Exception("Input is invalid/unsupported GZIP stream");
538: 			}
539: 
540: 			col_data.decompressed_buf.resize(page_hdr.uncompressed_page_size);
541: 
542: 			stream.next_in = (const unsigned char *)col_data.buf.ptr + GZIP_HEADER_MINSIZE;
543: 			stream.avail_in = page_hdr.compressed_page_size - GZIP_HEADER_MINSIZE;
544: 			stream.next_out = (unsigned char *)col_data.decompressed_buf.ptr;
545: 			stream.avail_out = page_hdr.uncompressed_page_size;
546: 
547: 			mz_ret = mz_inflate(&stream, MZ_FINISH);
548: 			if (mz_ret != MZ_OK && mz_ret != MZ_STREAM_END) {
549: 				throw runtime_error("Decompression failure: " + string(mz_error(mz_ret)));
550: 			}
551: 
552: 			col_data.payload.ptr = col_data.decompressed_buf.ptr;
553: 			break;
554: 		}
555: 		default:
556: 			throw runtime_error("Unsupported compression codec. Try uncompressed, gzip or snappy");
557: 		}
558: 		col_data.buf.inc(page_hdr.compressed_page_size);
559: 
560: 		// handle page contents
561: 		switch (page_hdr.type) {
562: 		case PageType::DICTIONARY_PAGE: {
563: 			// fill the dictionary vector
564: 
565: 			if (page_hdr.__isset.data_page_header || !page_hdr.__isset.dictionary_page_header) {
566: 				throw runtime_error("Dictionary page header mismatch");
567: 			}
568: 
569: 			// make sure we like the encoding
570: 			switch (page_hdr.dictionary_page_header.encoding) {
571: 			case Encoding::PLAIN:
572: 			case Encoding::PLAIN_DICTIONARY: // deprecated
573: 				break;
574: 
575: 			default:
576: 				throw runtime_error("Dictionary page has unsupported/invalid encoding");
577: 			}
578: 
579: 			col_data.dict_size = page_hdr.dictionary_page_header.num_values;
580: 			auto dict_byte_size = col_data.dict_size * GetTypeIdSize(GetInternalType(data.sql_types[col_idx]));
581: 
582: 			col_data.dict.resize(dict_byte_size);
583: 
584: 			switch (data.sql_types[col_idx].id) {
585: 			case SQLTypeId::BOOLEAN:
586: 			case SQLTypeId::INTEGER:
587: 			case SQLTypeId::BIGINT:
588: 			case SQLTypeId::FLOAT:
589: 			case SQLTypeId::DOUBLE:
590: 				col_data.payload.available(dict_byte_size);
591: 				// TODO this copy could be avoided if we use different buffers for dicts
592: 				col_data.payload.copy_to(col_data.dict.ptr, dict_byte_size);
593: 				break;
594: 			case SQLTypeId::TIMESTAMP:
595: 				col_data.payload.available(dict_byte_size);
596: 				// immediately convert timestamps to duckdb format, potentially fewer conversions
597: 				for (idx_t dict_index = 0; dict_index < col_data.dict_size; dict_index++) {
598: 					((timestamp_t *)col_data.dict.ptr)[dict_index] =
599: 					    impala_timestamp_to_timestamp_t(((Int96 *)col_data.payload.ptr)[dict_index]);
600: 				}
601: 
602: 				break;
603: 			case SQLTypeId::VARCHAR: {
604: 				// strings we directly fill a string heap that we can use for the vectors later
605: 				col_data.string_collection = make_unique<ChunkCollection>();
606: 
607: 				// we hand-roll a chunk collection to avoid copying strings
608: 				auto append_chunk = make_unique<DataChunk>();
609: 				vector<TypeId> types = {TypeId::VARCHAR};
610: 				col_data.string_collection->types = types;
611: 				append_chunk->Initialize(types);
612: 
613: 				for (idx_t dict_index = 0; dict_index < col_data.dict_size; dict_index++) {
614: 					uint32_t str_len = col_data.payload.read<uint32_t>();
615: 					col_data.payload.available(str_len);
616: 
617: 					if (append_chunk->size() == STANDARD_VECTOR_SIZE) {
618: 						col_data.string_collection->count += append_chunk->size();
619: 						col_data.string_collection->chunks.push_back(move(append_chunk));
620: 						append_chunk = make_unique<DataChunk>();
621: 						append_chunk->Initialize(types);
622: 					}
623: 
624: 					auto utf_type = Utf8Proc::Analyze(col_data.payload.ptr, str_len);
625: 					switch (utf_type) {
626: 					case UnicodeType::ASCII:
627: 						FlatVector::GetData<string_t>(append_chunk->data[0])[append_chunk->size()] =
628: 						    StringVector::AddString(append_chunk->data[0], col_data.payload.ptr, str_len);
629: 						break;
630: 					case UnicodeType::UNICODE:
631: 						// this regrettably copies to normalize
632: 						FlatVector::GetData<string_t>(append_chunk->data[0])[append_chunk->size()] =
633: 						    StringVector::AddString(append_chunk->data[0],
634: 						                            Utf8Proc::Normalize(string(col_data.payload.ptr, str_len)));
635: 
636: 						break;
637: 					case UnicodeType::INVALID:
638: 						throw runtime_error("invalid string encoding");
639: 					}
640: 
641: 					append_chunk->SetCardinality(append_chunk->size() + 1);
642: 					col_data.payload.inc(str_len);
643: 				}
644: 				// FLUSH last chunk!
645: 				if (append_chunk->size() > 0) {
646: 					col_data.string_collection->count += append_chunk->size();
647: 					col_data.string_collection->chunks.push_back(move(append_chunk));
648: 				}
649: 				col_data.string_collection->Verify();
650: 			} break;
651: 			default:
652: 				throw runtime_error(SQLTypeToString(data.sql_types[col_idx]));
653: 			}
654: 			// important, move to next page which should be a data page
655: 			return false;
656: 		}
657: 		case PageType::DATA_PAGE: {
658: 			if (!page_hdr.__isset.data_page_header || page_hdr.__isset.dictionary_page_header) {
659: 				throw runtime_error("Data page header mismatch");
660: 			}
661: 
662: 			if (page_hdr.__isset.data_page_header_v2) {
663: 				throw runtime_error("v2 data page format is not supported");
664: 			}
665: 
666: 			col_data.page_value_count = page_hdr.data_page_header.num_values;
667: 			col_data.page_encoding = page_hdr.data_page_header.encoding;
668: 
669: 			// we have to first decode the define levels
670: 			switch (page_hdr.data_page_header.definition_level_encoding) {
671: 			case Encoding::RLE: {
672: 				// read length of define payload, always
673: 				uint32_t def_length = col_data.payload.read<uint32_t>();
674: 				col_data.payload.available(def_length);
675: 				col_data.defined_decoder =
676: 				    make_unique<RleBpDecoder>((const uint8_t *)col_data.payload.ptr, def_length, 1);
677: 				col_data.payload.inc(def_length);
678: 			} break;
679: 			default:
680: 				throw runtime_error("Definition levels have unsupported/invalid encoding");
681: 			}
682: 
683: 			switch (page_hdr.data_page_header.encoding) {
684: 			case Encoding::RLE_DICTIONARY:
685: 			case Encoding::PLAIN_DICTIONARY: {
686: 				auto enc_length = col_data.payload.read<uint8_t>();
687: 				col_data.dict_decoder =
688: 				    make_unique<RleBpDecoder>((const uint8_t *)col_data.payload.ptr, col_data.payload.len, enc_length);
689: 				break;
690: 			}
691: 			case Encoding::PLAIN:
692: 				// nothing here, see below
693: 				break;
694: 
695: 			default:
696: 				throw runtime_error("Data page has unsupported/invalid encoding");
697: 			}
698: 
699: 			break;
700: 		}
701: 		case PageType::DATA_PAGE_V2:
702: 			throw runtime_error("v2 data page format is not supported");
703: 
704: 		default:
705: 			break; // ignore INDEX page type and any other custom extensions
706: 		}
707: 		return true;
708: 	}
709: 
710: 	static void _prepare_chunk_buffer(ParquetScanFunctionData &data, idx_t col_idx) {
711: 		auto &chunk = data.file_meta_data.row_groups[data.current_group].columns[col_idx];
712: 		if (chunk.__isset.file_path) {
713: 			throw runtime_error("Only inlined data files are supported (no references)");
714: 		}
715: 
716: 		if (chunk.meta_data.path_in_schema.size() != 1) {
717: 			throw runtime_error("Only flat tables are supported (no nesting)");
718: 		}
719: 
720: 		// ugh. sometimes there is an extra offset for the dict. sometimes it's wrong.
721: 		auto chunk_start = chunk.meta_data.data_page_offset;
722: 		if (chunk.meta_data.__isset.dictionary_page_offset && chunk.meta_data.dictionary_page_offset >= 4) {
723: 			// this assumes the data pages follow the dict pages directly.
724: 			chunk_start = chunk.meta_data.dictionary_page_offset;
725: 		}
726: 		auto chunk_len = chunk.meta_data.total_compressed_size;
727: 
728: 		// read entire chunk into RAM
729: 		data.pfile.seekg(chunk_start);
730: 		data.column_data[col_idx].buf.resize(chunk_len);
731: 		data.pfile.read(data.column_data[col_idx].buf.ptr, chunk_len);
732: 		if (!data.pfile) {
733: 			throw runtime_error("Could not read chunk. File corrupt?");
734: 		}
735: 	}
736: 
737: 	static void parquet_scan_function(ClientContext &context, vector<Value> &input, DataChunk &output,
738: 	                                  FunctionData *dataptr) {
739: 		auto &data = *((ParquetScanFunctionData *)dataptr);
740: 
741: 		if (data.finished) {
742: 			return;
743: 		}
744: 
745: 		// see if we have to switch to the next row group in the parquet file
746: 		if (data.current_group < 0 ||
747: 		    data.group_offset >= data.file_meta_data.row_groups[data.current_group].num_rows) {
748: 
749: 			data.current_group++;
750: 			data.group_offset = 0;
751: 
752: 			if ((idx_t)data.current_group == data.file_meta_data.row_groups.size()) {
753: 				data.finished = true;
754: 				return;
755: 			}
756: 
757: 			for (idx_t out_col_idx = 0; out_col_idx < output.column_count(); out_col_idx++) {
758: 				auto file_col_idx = data.column_ids[out_col_idx];
759: 
760: 				// this is a special case where we are not interested in the actual contents of the file
761: 				if (file_col_idx == COLUMN_IDENTIFIER_ROW_ID) {
762: 					continue;
763: 				}
764: 
765: 				_prepare_chunk_buffer(data, file_col_idx);
766: 				// trigger the reading of a new page below
767: 				data.column_data[file_col_idx].page_value_count = 0;
768: 			}
769: 		}
770: 
771: 		auto current_group = data.file_meta_data.row_groups[data.current_group];
772: 		output.SetCardinality(std::min((int64_t)STANDARD_VECTOR_SIZE, current_group.num_rows - data.group_offset));
773: 
774: 		if (output.size() == 0) {
775: 			return;
776: 		}
777: 
778: 		for (idx_t out_col_idx = 0; out_col_idx < output.column_count(); out_col_idx++) {
779: 			auto file_col_idx = data.column_ids[out_col_idx];
780: 			if (file_col_idx == COLUMN_IDENTIFIER_ROW_ID) {
781: 				Value constant_42 = Value::BIGINT(42);
782: 				output.data[out_col_idx].Reference(constant_42);
783: 				continue;
784: 			}
785: 
786: 			auto &col_data = data.column_data[file_col_idx];
787: 
788: 			// we might need to read multiple pages to fill the data chunk
789: 			idx_t output_offset = 0;
790: 			while (output_offset < output.size()) {
791: 				// do this unpack business only if we run out of stuff from the current page
792: 				if (col_data.page_offset >= col_data.page_value_count) {
793: 
794: 					// read dictionaries and data page headers so that we are ready to go for scan
795: 					if (!_prepare_page_buffers(data, file_col_idx)) {
796: 						continue;
797: 					}
798: 					col_data.page_offset = 0;
799: 				}
800: 
801: 				auto current_batch_size =
802: 				    std::min(col_data.page_value_count - col_data.page_offset, output.size() - output_offset);
803: 
804: 				assert(current_batch_size > 0);
805: 
806: 				col_data.defined_buf.resize(current_batch_size);
807: 				col_data.defined_decoder->GetBatch<uint8_t>(col_data.defined_buf.ptr, current_batch_size);
808: 
809: 				switch (col_data.page_encoding) {
810: 				case Encoding::RLE_DICTIONARY:
811: 				case Encoding::PLAIN_DICTIONARY: {
812: 
813: 					idx_t null_count = 0;
814: 					for (idx_t i = 0; i < current_batch_size; i++) {
815: 						if (!col_data.defined_buf.ptr[i]) {
816: 							null_count++;
817: 						}
818: 					}
819: 
820: 					col_data.offset_buf.resize(current_batch_size * sizeof(uint32_t));
821: 					col_data.dict_decoder->GetBatch<uint32_t>(col_data.offset_buf.ptr, current_batch_size - null_count);
822: 
823: 					// TODO ensure we had seen a dict page IN THIS CHUNK before getting here
824: 
825: 					switch (data.sql_types[file_col_idx].id) {
826: 					case SQLTypeId::BOOLEAN:
827: 						_fill_from_dict<bool>(col_data, current_batch_size, output.data[out_col_idx], output_offset);
828: 						break;
829: 					case SQLTypeId::INTEGER:
830: 						_fill_from_dict<int32_t>(col_data, current_batch_size, output.data[out_col_idx], output_offset);
831: 						break;
832: 					case SQLTypeId::BIGINT:
833: 						_fill_from_dict<int64_t>(col_data, current_batch_size, output.data[out_col_idx], output_offset);
834: 						break;
835: 					case SQLTypeId::FLOAT:
836: 						_fill_from_dict<float>(col_data, current_batch_size, output.data[out_col_idx], output_offset);
837: 						break;
838: 					case SQLTypeId::DOUBLE:
839: 						_fill_from_dict<double>(col_data, current_batch_size, output.data[out_col_idx], output_offset);
840: 						break;
841: 					case SQLTypeId::TIMESTAMP:
842: 						_fill_from_dict<timestamp_t>(col_data, current_batch_size, output.data[out_col_idx],
843: 						                             output_offset);
844: 						break;
845: 					case SQLTypeId::VARCHAR: {
846: 						if (!col_data.string_collection) {
847: 							throw runtime_error("Did not see a dictionary for strings. Corrupt file?");
848: 						}
849: 
850: 						// the strings can be anywhere in the collection so just reference it all
851: 						for (auto &chunk : col_data.string_collection->chunks) {
852: 							StringVector::AddHeapReference(output.data[out_col_idx], chunk->data[0]);
853: 						}
854: 
855: 						auto out_data_ptr = FlatVector::GetData<string_t>(output.data[out_col_idx]);
856: 						for (idx_t i = 0; i < current_batch_size; i++) {
857: 							if (col_data.defined_buf.ptr[i]) {
858: 								auto offset = col_data.offset_buf.read<uint32_t>();
859: 								if (offset >= col_data.string_collection->count) {
860: 									throw runtime_error("string dictionary offset out of bounds");
861: 								}
862: 								auto &chunk = col_data.string_collection->chunks[offset / STANDARD_VECTOR_SIZE];
863: 								auto &vec = chunk->data[0];
864: 
865: 								out_data_ptr[i + output_offset] =
866: 								    FlatVector::GetData<string_t>(vec)[offset % STANDARD_VECTOR_SIZE];
867: 							} else {
868: 								FlatVector::SetNull(output.data[out_col_idx], i + output_offset, true);
869: 							}
870: 						}
871: 					} break;
872: 					default:
873: 						throw runtime_error(SQLTypeToString(data.sql_types[file_col_idx]));
874: 					}
875: 
876: 					break;
877: 				}
878: 				case Encoding::PLAIN:
879: 					assert(col_data.payload.ptr);
880: 					switch (data.sql_types[file_col_idx].id) {
881: 					case SQLTypeId::BOOLEAN: {
882: 						// bit packed this
883: 						auto target_ptr = FlatVector::GetData<bool>(output.data[out_col_idx]);
884: 						int byte_pos = 0;
885: 						for (idx_t i = 0; i < current_batch_size; i++) {
886: 							if (!col_data.defined_buf.ptr[i]) {
887: 								FlatVector::SetNull(output.data[out_col_idx], i + output_offset, true);
888: 								continue;
889: 							}
890: 							col_data.payload.available(1);
891: 							target_ptr[i + output_offset] = (*col_data.payload.ptr >> byte_pos) & 1;
892: 							byte_pos++;
893: 							if (byte_pos == 8) {
894: 								byte_pos = 0;
895: 								col_data.payload.inc(1);
896: 							}
897: 						}
898: 						break;
899: 					}
900: 					case SQLTypeId::INTEGER:
901: 						_fill_from_plain<int32_t>(col_data, current_batch_size, output.data[out_col_idx],
902: 						                          output_offset);
903: 						break;
904: 					case SQLTypeId::BIGINT:
905: 						_fill_from_plain<int64_t>(col_data, current_batch_size, output.data[out_col_idx],
906: 						                          output_offset);
907: 						break;
908: 					case SQLTypeId::FLOAT:
909: 						_fill_from_plain<float>(col_data, current_batch_size, output.data[out_col_idx], output_offset);
910: 						break;
911: 					case SQLTypeId::DOUBLE:
912: 						_fill_from_plain<double>(col_data, current_batch_size, output.data[out_col_idx], output_offset);
913: 						break;
914: 					case SQLTypeId::TIMESTAMP: {
915: 						for (idx_t i = 0; i < current_batch_size; i++) {
916: 							if (col_data.defined_buf.ptr[i]) {
917: 								((timestamp_t *)FlatVector::GetData(output.data[out_col_idx]))[i + output_offset] =
918: 								    impala_timestamp_to_timestamp_t(col_data.payload.read<Int96>());
919: 							} else {
920: 								FlatVector::SetNull(output.data[out_col_idx], i + output_offset, true);
921: 							}
922: 						}
923: 
924: 						break;
925: 					}
926: 					case SQLTypeId::VARCHAR: {
927: 						for (idx_t i = 0; i < current_batch_size; i++) {
928: 							if (col_data.defined_buf.ptr[i]) {
929: 								uint32_t str_len = col_data.payload.read<uint32_t>();
930: 								col_data.payload.available(str_len);
931: 								FlatVector::GetData<string_t>(output.data[out_col_idx])[i + output_offset] =
932: 								    StringVector::AddString(output.data[out_col_idx], col_data.payload.ptr, str_len);
933: 								col_data.payload.inc(str_len);
934: 							} else {
935: 								FlatVector::SetNull(output.data[out_col_idx], i + output_offset, true);
936: 							}
937: 						}
938: 						break;
939: 					}
940: 					default:
941: 						throw runtime_error(SQLTypeToString(data.sql_types[file_col_idx]));
942: 					}
943: 
944: 					break;
945: 
946: 				default:
947: 					throw runtime_error("Data page has unsupported/invalid encoding");
948: 				}
949: 
950: 				output_offset += current_batch_size;
951: 				col_data.page_offset += current_batch_size;
952: 			}
953: 		}
954: 		data.group_offset += output.size();
955: 	}
956: };
957: 
958: class MyTransport : public TTransport {
959: public:
960: 	MyTransport(Serializer &serializer) : serializer(serializer) {
961: 	}
962: 
963: 	bool isOpen() const override {
964: 		return true;
965: 	}
966: 
967: 	void open() override {
968: 	}
969: 
970: 	void close() override {
971: 	}
972: 
973: 	void write_virt(const uint8_t *buf, uint32_t len) override {
974: 		serializer.WriteData((const_data_ptr_t)buf, len);
975: 	}
976: 
977: private:
978: 	Serializer &serializer;
979: };
980: 
981: static Type::type duckdb_type_to_parquet_type(SQLType duckdb_type) {
982: 	switch (duckdb_type.id) {
983: 	case SQLTypeId::BOOLEAN:
984: 		return Type::BOOLEAN;
985: 	case SQLTypeId::TINYINT:
986: 	case SQLTypeId::SMALLINT:
987: 	case SQLTypeId::INTEGER:
988: 		return Type::INT32;
989: 	case SQLTypeId::BIGINT:
990: 		return Type::INT64;
991: 	case SQLTypeId::FLOAT:
992: 		return Type::FLOAT;
993: 	case SQLTypeId::DOUBLE:
994: 		return Type::DOUBLE;
995: 	case SQLTypeId::VARCHAR:
996: 	case SQLTypeId::BLOB:
997: 		return Type::BYTE_ARRAY;
998: 	case SQLTypeId::DATE:
999: 	case SQLTypeId::TIMESTAMP:
1000: 		return Type::INT96;
1001: 	default:
1002: 		throw NotImplementedException(SQLTypeToString(duckdb_type));
1003: 	}
1004: }
1005: 
1006: static void VarintEncode(uint32_t val, Serializer &ser) {
1007: 	do {
1008: 		uint8_t byte = val & 127;
1009: 		val >>= 7;
1010: 		if (val != 0) {
1011: 			byte |= 128;
1012: 		}
1013: 		ser.Write<uint8_t>(byte);
1014: 	} while (val != 0);
1015: }
1016: 
1017: static uint8_t GetVarintSize(uint32_t val) {
1018: 	uint8_t res = 0;
1019: 	do {
1020: 		uint8_t byte = val & 127;
1021: 		val >>= 7;
1022: 		if (val != 0) {
1023: 			byte |= 128;
1024: 		}
1025: 		res++;
1026: 	} while (val != 0);
1027: 	return res;
1028: }
1029: 
1030: template <class SRC, class TGT>
1031: static void _write_plain(Vector &col, idx_t length, nullmask_t &nullmask, Serializer &ser) {
1032: 	auto *ptr = FlatVector::GetData<SRC>(col);
1033: 	for (idx_t r = 0; r < length; r++) {
1034: 		if (!nullmask[r]) {
1035: 			ser.Write<TGT>((TGT)ptr[r]);
1036: 		}
1037: 	}
1038: }
1039: 
1040: struct ParquetWriteBindData : public FunctionData {
1041: 	vector<SQLType> sql_types;
1042: 	string file_name;
1043: 	vector<string> column_names;
1044: 	// TODO compression flag to test the param passing stuff
1045: };
1046: 
1047: struct ParquetWriteGlobalState : public GlobalFunctionData {
1048: public:
1049: 	void Flush(ChunkCollection &buffer) {
1050: 		if (buffer.count == 0) {
1051: 			return;
1052: 		}
1053: 		std::lock_guard<std::mutex> glock(lock);
1054: 
1055: 		// set up a new row group for this chunk collection
1056: 		RowGroup row_group;
1057: 		row_group.num_rows = 0;
1058: 		row_group.file_offset = writer->GetTotalWritten();
1059: 		row_group.__isset.file_offset = true;
1060: 		row_group.columns.resize(buffer.column_count());
1061: 
1062: 		// iterate over each of the columns of the chunk collection and write them
1063: 		for (idx_t i = 0; i < buffer.column_count(); i++) {
1064: 			// we start off by writing everything into a temporary buffer
1065: 			// this is necessary to (1) know the total written size, and (2) to compress it with snappy afterwards
1066: 			BufferedSerializer temp_writer;
1067: 
1068: 			// set up some metadata
1069: 			PageHeader hdr;
1070: 			hdr.compressed_page_size = 0;
1071: 			hdr.uncompressed_page_size = 0;
1072: 			hdr.type = PageType::DATA_PAGE;
1073: 			hdr.__isset.data_page_header = true;
1074: 
1075: 			hdr.data_page_header.num_values = buffer.count;
1076: 			hdr.data_page_header.encoding = Encoding::PLAIN;
1077: 			hdr.data_page_header.definition_level_encoding = Encoding::RLE;
1078: 			hdr.data_page_header.repetition_level_encoding = Encoding::BIT_PACKED;
1079: 
1080: 			// record the current offset of the writer into the file
1081: 			// this is the starting position of the current page
1082: 			auto start_offset = writer->GetTotalWritten();
1083: 
1084: 			// write the definition levels (i.e. the inverse of the nullmask)
1085: 			// we always bit pack everything
1086: 
1087: 			// first figure out how many bytes we need (1 byte per 8 rows, rounded up)
1088: 			auto define_byte_count = (buffer.count + 7) / 8;
1089: 			// we need to set up the count as a varint, plus an added marker for the RLE scheme
1090: 			// for this marker we shift the count left 1 and set low bit to 1 to indicate bit packed literals
1091: 			uint32_t define_header = (define_byte_count << 1) | 1;
1092: 			uint32_t define_size = GetVarintSize(define_header) + define_byte_count;
1093: 
1094: 			// we write the actual definitions into the temp_writer for now
1095: 			temp_writer.Write<uint32_t>(define_size);
1096: 			VarintEncode(define_header, temp_writer);
1097: 
1098: 			for (auto &chunk : buffer.chunks) {
1099: 				auto defined = FlatVector::Nullmask(chunk->data[i]);
1100: 				// flip the nullmask to go from nulls -> defines
1101: 				defined.flip();
1102: 				// write the bits of the nullmask
1103: 				auto chunk_define_byte_count = (chunk->size() + 7) / 8;
1104: 				temp_writer.WriteData((const_data_ptr_t)&defined, chunk_define_byte_count);
1105: 			}
1106: 
1107: 			// now write the actual payload: we write this as PLAIN values (for now? possibly for ever?)
1108: 			for (auto &chunk : buffer.chunks) {
1109: 				auto &input = *chunk;
1110: 				auto &input_column = input.data[i];
1111: 				auto &nullmask = FlatVector::Nullmask(input_column);
1112: 
1113: 				// write actual payload data
1114: 				switch (sql_types[i].id) {
1115: 				case SQLTypeId::BOOLEAN: {
1116: 					auto *ptr = FlatVector::GetData<bool>(input_column);
1117: 					uint8_t byte = 0;
1118: 					uint8_t byte_pos = 0;
1119: 					for (idx_t r = 0; r < input.size(); r++) {
1120: 						if (!nullmask[r]) { // only encode if non-null
1121: 							byte |= (ptr[r] & 1) << byte_pos;
1122: 							byte_pos++;
1123: 
1124: 							temp_writer.Write<uint8_t>(byte);
1125: 							if (byte_pos == 8) {
1126: 								temp_writer.Write<uint8_t>(byte);
1127: 								byte = 0;
1128: 								byte_pos = 0;
1129: 							}
1130: 						}
1131: 					}
1132: 					// flush last byte if req
1133: 					if (byte_pos > 0) {
1134: 						temp_writer.Write<uint8_t>(byte);
1135: 					}
1136: 					break;
1137: 				}
1138: 				case SQLTypeId::TINYINT:
1139: 					_write_plain<int8_t, int32_t>(input_column, input.size(), nullmask, temp_writer);
1140: 					break;
1141: 				case SQLTypeId::SMALLINT:
1142: 					_write_plain<int16_t, int32_t>(input_column, input.size(), nullmask, temp_writer);
1143: 					break;
1144: 				case SQLTypeId::INTEGER:
1145: 					_write_plain<int32_t, int32_t>(input_column, input.size(), nullmask, temp_writer);
1146: 					break;
1147: 				case SQLTypeId::BIGINT:
1148: 					_write_plain<int64_t, int64_t>(input_column, input.size(), nullmask, temp_writer);
1149: 					break;
1150: 				case SQLTypeId::FLOAT:
1151: 					_write_plain<float, float>(input_column, input.size(), nullmask, temp_writer);
1152: 					break;
1153: 				case SQLTypeId::DOUBLE:
1154: 					_write_plain<double, double>(input_column, input.size(), nullmask, temp_writer);
1155: 					break;
1156: 				case SQLTypeId::TIMESTAMP: {
1157: 					auto *ptr = FlatVector::GetData<timestamp_t>(input_column);
1158: 					for (idx_t r = 0; r < input.size(); r++) {
1159: 						if (!nullmask[r]) {
1160: 							temp_writer.Write<Int96>(timestamp_t_to_impala_timestamp(ptr[r]));
1161: 						}
1162: 					}
1163: 					break;
1164: 				}
1165: 				case SQLTypeId::VARCHAR: {
1166: 					auto *ptr = FlatVector::GetData<string_t>(input_column);
1167: 					for (idx_t r = 0; r < input.size(); r++) {
1168: 						if (!nullmask[r]) {
1169: 							temp_writer.Write<uint32_t>(ptr[r].GetSize());
1170: 							temp_writer.WriteData((const_data_ptr_t)ptr[r].GetData(), ptr[r].GetSize());
1171: 						}
1172: 					}
1173: 					break;
1174: 				}
1175: 					// TODO date blob etc.
1176: 				default:
1177: 					throw NotImplementedException(SQLTypeToString((sql_types[i])));
1178: 				}
1179: 			}
1180: 
1181: 			// now that we have finished writing the data we know the uncompressed size
1182: 			hdr.uncompressed_page_size = temp_writer.blob.size;
1183: 
1184: 			// we perform snappy compression (FIXME: this should be a flag, possibly also include gzip?)
1185: 			size_t compressed_size = snappy::MaxCompressedLength(temp_writer.blob.size);
1186: 			auto compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
1187: 			snappy::RawCompress((const char *)temp_writer.blob.data.get(), temp_writer.blob.size,
1188: 			                    (char *)compressed_buf.get(), &compressed_size);
1189: 
1190: 			hdr.compressed_page_size = compressed_size;
1191: 
1192: 			// now finally write the data to the actual file
1193: 			hdr.write(protocol.get());
1194: 			writer->WriteData(compressed_buf.get(), compressed_size);
1195: 
1196: 			auto &column_chunk = row_group.columns[i];
1197: 			column_chunk.__isset.meta_data = true;
1198: 			column_chunk.meta_data.data_page_offset = start_offset;
1199: 			column_chunk.meta_data.total_compressed_size = writer->GetTotalWritten() - start_offset;
1200: 			column_chunk.meta_data.codec = CompressionCodec::SNAPPY;
1201: 			column_chunk.meta_data.path_in_schema.push_back(file_meta_data.schema[i + 1].name);
1202: 			column_chunk.meta_data.num_values = buffer.count;
1203: 			column_chunk.meta_data.type = file_meta_data.schema[i + 1].type;
1204: 		}
1205: 		row_group.num_rows += buffer.count;
1206: 
1207: 		// append the row group to the file meta data
1208: 		file_meta_data.row_groups.push_back(row_group);
1209: 		file_meta_data.num_rows += buffer.count;
1210: 	}
1211: 
1212: 	void Finalize() {
1213: 		auto start_offset = writer->GetTotalWritten();
1214: 		file_meta_data.write(protocol.get());
1215: 
1216: 		writer->Write<uint32_t>(writer->GetTotalWritten() - start_offset);
1217: 
1218: 		// parquet files also end with the string "PAR1"
1219: 		writer->WriteData((const_data_ptr_t) "PAR1", 4);
1220: 
1221: 		// flush to disk
1222: 		writer->Sync();
1223: 		writer.reset();
1224: 	}
1225: 
1226: public:
1227: 	unique_ptr<BufferedFileWriter> writer;
1228: 	shared_ptr<TProtocol> protocol;
1229: 	FileMetaData file_meta_data;
1230: 	vector<SQLType> sql_types;
1231: 	std::mutex lock;
1232: };
1233: 
1234: struct ParquetWriteLocalState : public LocalFunctionData {
1235: 	ParquetWriteLocalState() {
1236: 		buffer = make_unique<ChunkCollection>();
1237: 	}
1238: 
1239: 	unique_ptr<ChunkCollection> buffer;
1240: };
1241: 
1242: unique_ptr<FunctionData> parquet_write_bind(ClientContext &context, CopyInfo &info, vector<string> &names,
1243:                                             vector<SQLType> &sql_types) {
1244: 	auto bind_data = make_unique<ParquetWriteBindData>();
1245: 	bind_data->sql_types = sql_types;
1246: 	bind_data->column_names = names;
1247: 	bind_data->file_name = info.file_path;
1248: 	return move(bind_data);
1249: }
1250: 
1251: unique_ptr<GlobalFunctionData> parquet_write_initialize_global(ClientContext &context, FunctionData &bind_data) {
1252: 	auto global_state = make_unique<ParquetWriteGlobalState>();
1253: 	auto &parquet_bind = (ParquetWriteBindData &)bind_data;
1254: 
1255: 	// initialize the file writer
1256: 	global_state->writer = make_unique<BufferedFileWriter>(context.db.GetFileSystem(), parquet_bind.file_name.c_str(),
1257: 	                                                       FileFlags::WRITE | FileFlags::FILE_CREATE_NEW);
1258: 	// parquet files start with the string "PAR1"
1259: 	global_state->writer->WriteData((const_data_ptr_t) "PAR1", 4);
1260: 	TCompactProtocolFactoryT<MyTransport> tproto_factory;
1261: 	global_state->protocol = tproto_factory.getProtocol(make_shared<MyTransport>(*global_state->writer));
1262: 	global_state->file_meta_data.num_rows = 0;
1263: 	global_state->file_meta_data.schema.resize(parquet_bind.sql_types.size() + 1);
1264: 
1265: 	global_state->file_meta_data.schema[0].num_children = parquet_bind.sql_types.size();
1266: 	global_state->file_meta_data.schema[0].__isset.num_children = true;
1267: 	global_state->file_meta_data.version = 1;
1268: 
1269: 	for (idx_t i = 0; i < parquet_bind.sql_types.size(); i++) {
1270: 		auto &schema_element = global_state->file_meta_data.schema[i + 1];
1271: 
1272: 		schema_element.type = duckdb_type_to_parquet_type(parquet_bind.sql_types[i]);
1273: 		schema_element.repetition_type = FieldRepetitionType::OPTIONAL;
1274: 		schema_element.num_children = 0;
1275: 		schema_element.__isset.num_children = true;
1276: 		schema_element.__isset.type = true;
1277: 		schema_element.__isset.repetition_type = true;
1278: 		schema_element.name = parquet_bind.column_names[i];
1279: 	}
1280: 	global_state->sql_types = parquet_bind.sql_types;
1281: 	return move(global_state);
1282: }
1283: 
1284: void parquet_write_sink(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
1285:                         LocalFunctionData &lstate, DataChunk &input) {
1286: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
1287: 	auto &local_state = (ParquetWriteLocalState &)lstate;
1288: 
1289: 	// append data to the local (buffered) chunk collection
1290: 	local_state.buffer->Append(input);
1291: 	if (local_state.buffer->count > 100000) {
1292: 		// if the chunk collection exceeds a certain size we flush it to the parquet file
1293: 		global_state.Flush(*local_state.buffer);
1294: 		// and reset the buffer
1295: 		local_state.buffer = make_unique<ChunkCollection>();
1296: 	}
1297: }
1298: 
1299: void parquet_write_combine(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
1300:                            LocalFunctionData &lstate) {
1301: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
1302: 	auto &local_state = (ParquetWriteLocalState &)lstate;
1303: 	// flush any data left in the local state to the file
1304: 	global_state.Flush(*local_state.buffer);
1305: }
1306: 
1307: void parquet_write_finalize(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate) {
1308: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
1309: 	// finalize: write any additional metadata to the file here
1310: 	global_state.Finalize();
1311: }
1312: 
1313: unique_ptr<LocalFunctionData> parquet_write_initialize_local(ClientContext &context, FunctionData &bind_data) {
1314: 	return make_unique<ParquetWriteLocalState>();
1315: }
1316: 
1317: void ParquetExtension::Load(DuckDB &db) {
1318: 	ParquetScanFunction scan_fun;
1319: 	CreateTableFunctionInfo cinfo(scan_fun);
1320: 	cinfo.name = "read_parquet";
1321: 	CreateTableFunctionInfo pq_scan = cinfo;
1322: 	pq_scan.name = "parquet_scan";
1323: 
1324: 	CopyFunction function("parquet");
1325: 	function.copy_to_bind = parquet_write_bind;
1326: 	function.copy_to_initialize_global = parquet_write_initialize_global;
1327: 	function.copy_to_initialize_local = parquet_write_initialize_local;
1328: 	function.copy_to_sink = parquet_write_sink;
1329: 	function.copy_to_combine = parquet_write_combine;
1330: 	function.copy_to_finalize = parquet_write_finalize;
1331: 	CreateCopyFunctionInfo info(function);
1332: 
1333: 	Connection conn(db);
1334: 	conn.context->transaction.BeginTransaction();
1335: 	db.catalog->CreateCopyFunction(*conn.context, &info);
1336: 	db.catalog->CreateTableFunction(*conn.context, &cinfo);
1337: 	db.catalog->CreateTableFunction(*conn.context, &pq_scan);
1338: 
1339: 	conn.context->transaction.Commit();
1340: }
[end of extension/parquet/parquet-extension.cpp]
[start of grammar/delete.js]
1: 
2: function GenerateValues(options) {
3: 	return [
4: 		Keyword("VALUES"),
5: 		OneOrMore(
6: 			Sequence([
7: 				Keyword("("),
8: 				OneOrMore(Expression(), ","),
9: 				Keyword(")")
10: 			]), Keyword(","))
11: 	]
12: }
13: 
14: function GenerateDistinctClause(options) {
15: 	return [
16: 		Choice(0, [
17: 			new Skip(),
18: 			Sequence([
19: 				Keyword("DISTINCT"),
20: 				Optional(Sequence([
21: 					Keyword("("),
22: 					OneOrMore(Expression("distinct-term"), ","),
23: 					Keyword(")"),
24: 				]) , "skip")
25: 			]),
26: 			Keyword("ALL")
27: 		])
28: 	]
29: }
30: 
31: function GenerateSelectNode(options) {
32: 	return [Stack([
33: 		Sequence([
34: 			Keyword("SELECT"),
35: 			Expandable("distinct-clause", options, "distinct-clause", GenerateDistinctClause),
36: 			OneOrMore(Expression(), ",")
37: 		]),
38: 		Sequence([
39: 			Optional(Sequence([
40: 				Keyword("FROM"),
41: 				Choice(0, [
42: 					OneOrMore(Expression("table-or-subquery"), ","),
43: 					Sequence([Expression("join-clause")])
44: 				])
45: 			])),
46: 			Optional(
47: 				Sequence([
48: 					Keyword("WHERE"),
49: 					Expression()
50: 				])
51: 			)
52: 		]),
53: 		Sequence([
54: 			Optional(Sequence([
55: 				Keyword("GROUP"),
56: 				Keyword("BY"),
57: 				OneOrMore(Expression(), ","),
58: 			])),
59: 			Optional(Sequence([
60: 				Keyword("HAVING"),
61: 				Expression()
62: 			]))
63: 		]),
64: 		Optional(
65: 			Sequence([Sequence([
66: 					Keyword("WINDOW"),
67: 					Expression("window-name"),
68: 					Keyword("AS"),
69: 					Expression("window-definition")
70: 				])
71: 			]), "skip"),
72: 		Sequence([
73: 			Optional(Sequence([
74: 				Keyword("ORDER"),
75: 				Keyword("BY"),
76: 				OneOrMore(Expression("ordering-term"), ",")
77: 			]))
78: 		]),
79: 		Optional(Sequence([
80: 			Keyword("LIMIT"),
81: 			Expression(),
82: 			Optional(Sequence([
83: 				Keyword("OFFSET"),
84: 				Expression()
85: 			]), "skip")
86: 		]))
87: 	])]
88: }
89: 
90: function GenerateDelete(options = {}) {
91: 	return Diagram([
92: 		Stack([
93: 			Optional(
94: 				Sequence([
95: 					Keyword("WITH"),
96: 					Optional(Keyword("RECURSIVE"), "skip"),
97: 					OneOrMore(Expression("common-table-expr"), ",")
98: 				]), "skip"),
99: 			Choice(0, [
100: 				Expandable("select-node", options, "select-node", GenerateSelectNode),
101: 				Expandable("values-list", options, "values", GenerateValues)
102: 			])
103: 		])
104: 	])
105: }
106: 
107: function Initialize(options = {}) {
108: 	document.getElementById("rrdiagram").innerHTML = GenerateDelete(options).toString();
109: }
110: 
111: function Refresh(node_name, set_node) {
112: 	options[node_name] = set_node;
113: 	Initialize(options);
114: }
115: 
116: options = {}
117: Initialize()
118: 
119: 
[end of grammar/delete.js]
[start of grammar/railroad.css]
1: svg {
2: 	background-color: hsl(30,20%,95%);
3: }
4: path {
5: 	stroke-width: 3;
6: 	stroke: black;
7: 	fill: rgba(0,0,0,0);
8: }
9: text {
10: 	font: bold 14px monospace;
11: 	text-anchor: middle;
12: 	white-space: pre;
13: }
14: text.diagram-text {
15: 	font-size: 12px;
16: }
17: text.diagram-arrow {
18: 	font-size: 16px;
19: }
20: text.label {
21: 	text-anchor: start;
22: }
23: text.comment {
24: 	font: italic 12px monospace;
25: }
26: g.non-terminal text {
27: 	/*font-style: italic;*/
28: }
29: rect {
30: 	stroke-width: 3;
31: 	stroke: black;
32: 	fill: hsl(120,100%,90%);
33: }
34: .expandable, .expanded {
35: 	cursor:pointer;
36: 	-webkit-user-select: none; /* Safari */
37: 	-moz-user-select: none; /* Firefox */
38: 	-ms-user-select: none; /* IE10+/Edge */
39: 	user-select: none; /* Standard */
40: }
41: rect.expandable, rect.expanded, rect.expression {
42: 	fill:#f6d573;
43: }
44: text.expandable, text.expanded, text.expression {
45: 	font:14px monospace;
46: 	font-style:italic;
47: }
48: .non-terminal:hover rect.expandable, .non-terminal:hover rect.expanded {
49: 	fill:#f8d9aa;
50: }
51: path.diagram-text {
52: 	stroke-width: 3;
53: 	stroke: black;
54: 	fill: white;
55: 	cursor: help;
56: }
57: g.diagram-text:hover path.diagram-text {
58: 	fill: #eee;
59: }
[end of grammar/railroad.css]
[start of grammar/railroad.js]
1: "use strict";
2: /*
3: Railroad DiagramNodes
4: by Tab Atkins Jr. (and others)
5: http://xanthir.com
6: http://twitter.com/tabatkins
7: http://github.com/tabatkins/railroad-diagrams
8: 
9: This document and all associated files in the github project are licensed under CC0: http://creativecommons.org/publicdomain/zero/1.0/
10: This means you can reuse, remix, or otherwise appropriate this project for your own use WITHOUT RESTRICTION.
11: (The actual legal meaning can be found at the above link.)
12: Don't ask me for permission to use any part of this project, JUST USE IT.
13: I would appreciate attribution, but that is not required by the license.
14: */
15: 
16: /*
17: This file uses a module pattern to avoid leaking names into the global scope.
18: Should be compatible with AMD, CommonJS, and plain ol' browser JS.
19: 
20: As well, several configuration constants are passed into the module function at the bottom of this file.
21: At runtime, these constants can be found on the DiagramNode class,
22: and can be changed before creating a DiagramNode.
23: */
24: 
25: (function(options) {
26: 	var funcs = {};
27: 
28: 	function subclassOf(baseClass, superClass) {
29: 		baseClass.prototype = Object.create(superClass.prototype);
30: 		baseClass.prototype.$super = superClass.prototype;
31: 	}
32: 
33: 	function unnull(/* children */) {
34: 		return [].slice.call(arguments).reduce(function(sofar, x) { return sofar !== undefined ? sofar : x; });
35: 	}
36: 
37: 	function determineGaps(outer, inner) {
38: 		var diff = outer - inner;
39: 		switch(DiagramNode.INTERNAL_ALIGNMENT) {
40: 			case 'left': return [0, diff]; break;
41: 			case 'right': return [diff, 0]; break;
42: 			case 'center':
43: 			default: return [diff/2, diff/2]; break;
44: 		}
45: 	}
46: 
47: 	function wrapString(value) {
48: 		return value instanceof FakeSVG ? value : new Terminal(""+value);
49: 	}
50: 
51: 	function sum(iter, func) {
52: 		if(!func) func = function(x) { return x; };
53: 		return iter.map(func).reduce(function(a,b){return a+b}, 0);
54: 	}
55: 
56: 	function max(iter, func) {
57: 		if(!func) func = function(x) { return x; };
58: 		return Math.max.apply(null, iter.map(func));
59: 	}
60: 
61: 	function* enumerate(iter) {
62: 		var count = 0;
63: 		for(const x of iter) {
64: 			yield [count, x];
65: 			count++;
66: 		}
67: 	}
68: 
69: 	var SVG = funcs.SVG = function SVG(name, attrs, text) {
70: 		attrs = attrs || {};
71: 		text = text || '';
72: 		var el = document.createElementNS("http://www.w3.org/2000/svg",name);
73: 		for(var attr in attrs) {
74: 			if(attr === 'xlink:href')
75: 				el.setAttributeNS("http://www.w3.org/1999/xlink", 'href', attrs[attr]);
76: 			else
77: 				el.setAttribute(attr, attrs[attr]);
78: 		}
79: 		el.textContent = text;
80: 		return el;
81: 	}
82: 
83: 	var FakeSVG = funcs.FakeSVG = function FakeSVG(tagName, attrs, text){
84: 		if(!(this instanceof FakeSVG)) return new FakeSVG(tagName, attrs, text);
85: 		if(text) this.children = text;
86: 		else this.children = [];
87: 		this.tagName = tagName;
88: 		this.attrs = unnull(attrs, {});
89: 		return this;
90: 	};
91: 	FakeSVG.prototype.format = function(x, y, width) {
92: 		// Virtual
93: 	};
94: 	FakeSVG.prototype.addTo = function(parent) {
95: 		if(parent instanceof FakeSVG) {
96: 			parent.children.push(this);
97: 			return this;
98: 		} else {
99: 			var svg = this.toSVG();
100: 			parent.appendChild(svg);
101: 			return svg;
102: 		}
103: 	};
104: 	FakeSVG.prototype.escapeString = function(string) {
105: 		// Escape markdown and HTML special characters
106: 		return string.replace(/[*_\`\[\]<&]/g, function(charString) {
107: 			return '&#' + charString.charCodeAt(0) + ';';
108: 		});
109: 	};
110: 	FakeSVG.prototype.toSVG = function() {
111: 		var el = SVG(this.tagName, this.attrs);
112: 		if(typeof this.children == 'string') {
113: 			el.textContent = this.children;
114: 		} else {
115: 			this.children.forEach(function(e) {
116: 				el.appendChild(e.toSVG());
117: 			});
118: 		}
119: 		return el;
120: 	};
121: 	FakeSVG.prototype.toString = function() {
122: 		var str = '<' + this.tagName;
123: 		var group = this.tagName == "g" || this.tagName == "svg";
124: 		for(var attr in this.attrs) {
125: 			str += ' ' + attr + '="' + (this.attrs[attr]+'').replace(/&/g, '&amp;').replace(/"/g, '&quot;') + '"';
126: 		}
127: 		str += '>';
128: 		if(group) str += "\n";
129: 		if(typeof this.children == 'string') {
130: 			str += FakeSVG.prototype.escapeString(this.children);
131: 		} else {
132: 			this.children.forEach(function(e) {
133: 				str += e;
134: 			});
135: 		}
136: 		str += '</' + this.tagName + '>\n';
137: 		return str;
138: 	}
139: 	FakeSVG.prototype.walk = function(cb) {
140: 		cb(this);
141: 	}
142: 
143: 	var Path = funcs.Path = function Path(x,y) {
144: 		if(!(this instanceof Path)) return new Path(x,y);
145: 		FakeSVG.call(this, 'path');
146: 		this.attrs.d = "M"+x+' '+y;
147: 	}
148: 	subclassOf(Path, FakeSVG);
149: 	Path.prototype.m = function(x,y) {
150: 		this.attrs.d += 'm'+x+' '+y;
151: 		return this;
152: 	}
153: 	Path.prototype.h = function(val) {
154: 		this.attrs.d += 'h'+val;
155: 		return this;
156: 	}
157: 	Path.prototype.right = function(val) { return this.h(Math.max(0, val)); }
158: 	Path.prototype.left = function(val) { return this.h(-Math.max(0, val)); }
159: 	Path.prototype.v = function(val) {
160: 		this.attrs.d += 'v'+val;
161: 		return this;
162: 	}
163: 	Path.prototype.down = function(val) { return this.v(Math.max(0, val)); }
164: 	Path.prototype.up = function(val) { return this.v(-Math.max(0, val)); }
165: 	Path.prototype.arc = function(sweep){
166: 		// 1/4 of a circle
167: 		var x = DiagramNode.ARC_RADIUS;
168: 		var y = DiagramNode.ARC_RADIUS;
169: 		if(sweep[0] == 'e' || sweep[1] == 'w') {
170: 			x *= -1;
171: 		}
172: 		if(sweep[0] == 's' || sweep[1] == 'n') {
173: 			y *= -1;
174: 		}
175: 		if(sweep == 'ne' || sweep == 'es' || sweep == 'sw' || sweep == 'wn') {
176: 			var cw = 1;
177: 		} else {
178: 			var cw = 0;
179: 		}
180: 		this.attrs.d += "a"+DiagramNode.ARC_RADIUS+" "+DiagramNode.ARC_RADIUS+" 0 0 "+cw+' '+x+' '+y;
181: 		return this;
182: 	}
183: 	Path.prototype.arc_8 = function(start, dir) {
184: 		// 1/8 of a circle
185: 		const arc = DiagramNode.ARC_RADIUS;
186: 		const s2 = 1/Math.sqrt(2) * arc;
187: 		const s2inv = (arc - s2);
188: 		let path = "a " + arc + " " + arc + " 0 0 " + (dir=='cw' ? "1" : "0") + " ";
189: 		const sd = start+dir;
190: 		const offset =
191: 			sd == 'ncw'   ? [s2, s2inv] :
192: 			sd == 'necw'  ? [s2inv, s2] :
193: 			sd == 'ecw'   ? [-s2inv, s2] :
194: 			sd == 'secw'  ? [-s2, s2inv] :
195: 			sd == 'scw'   ? [-s2, -s2inv] :
196: 			sd == 'swcw'  ? [-s2inv, -s2] :
197: 			sd == 'wcw'   ? [s2inv, -s2] :
198: 			sd == 'nwcw'  ? [s2, -s2inv] :
199: 			sd == 'nccw'  ? [-s2, s2inv] :
200: 			sd == 'nwccw' ? [-s2inv, s2] :
201: 			sd == 'wccw'  ? [s2inv, s2] :
202: 			sd == 'swccw' ? [s2, s2inv] :
203: 			sd == 'sccw'  ? [s2, -s2inv] :
204: 			sd == 'seccw' ? [s2inv, -s2] :
205: 			sd == 'eccw'  ? [-s2inv, -s2] :
206: 			sd == 'neccw' ? [-s2, -s2inv] : null
207: 		;
208: 		path += offset.join(" ");
209: 		this.attrs.d += path;
210: 		return this;
211: 	}
212: 	Path.prototype.l = function(x, y) {
213: 		this.attrs.d += 'l'+x+' '+y;
214: 		return this;
215: 	}
216: 	Path.prototype.format = function() {
217: 		// All paths in this library start/end horizontally.
218: 		// The extra .5 ensures a minor overlap, so there's no seams in bad rasterizers.
219: 		this.attrs.d += 'h.5';
220: 		return this;
221: 	}
222: 
223: 
224: 	var DiagramNodeMultiContainer = funcs.DiagramNodeMultiContainer = function DiagramNodeMultiContainer(tagName, items, attrs, text) {
225: 		FakeSVG.call(this, tagName, attrs, text);
226: 		this.items = items.map(wrapString);
227: 	}
228: 	subclassOf(DiagramNodeMultiContainer, FakeSVG);
229: 	DiagramNodeMultiContainer.prototype.walk = function(cb) {
230: 		cb(this);
231: 		this.items.forEach(x=>w.walk(cb));
232: 	}
233: 
234: 
235: 	var DiagramNode = funcs.DiagramNode = function DiagramNode(items) {
236: 		if(!(this instanceof DiagramNode)) return new DiagramNode([].slice.call(arguments));
237: 		DiagramNodeMultiContainer.call(this, 'svg', items, {class: DiagramNode.DIAGRAM_CLASS});
238: 		if(!(this.items[0] instanceof Start)) {
239: 			this.items.unshift(new Start());
240: 		}
241: 		if(!(this.items[this.items.length-1] instanceof End)) {
242: 			this.items.push(new End());
243: 		}
244: 		this.up = this.down = this.height = this.width = 0;
245: 		for(var i = 0; i < this.items.length; i++) {
246: 			var item = this.items[i];
247: 			this.width += item.width + (item.needsSpace?20:0);
248: 			this.up = Math.max(this.up, item.up - this.height);
249: 			this.height += item.height;
250: 			this.down = Math.max(this.down - item.height, item.down);
251: 		}
252: 		this.formatted = false;
253: 	}
254: 	subclassOf(DiagramNode, DiagramNodeMultiContainer);
255: 	for(var option in options) {
256: 		DiagramNode[option] = options[option];
257: 	}
258: 	DiagramNode.prototype.format = function(paddingt, paddingr, paddingb, paddingl) {
259: 		paddingt = unnull(paddingt, 20);
260: 		paddingr = unnull(paddingr, paddingt, 20);
261: 		paddingb = unnull(paddingb, paddingt, 20);
262: 		paddingl = unnull(paddingl, paddingr, 20);
263: 		var x = paddingl;
264: 		var y = paddingt;
265: 		y += this.up;
266: 		var g = FakeSVG('g', DiagramNode.STROKE_ODD_PIXEL_LENGTH ? {transform:'translate(.5 .5)'} : {});
267: 		for(var i = 0; i < this.items.length; i++) {
268: 			var item = this.items[i];
269: 			if(item.needsSpace) {
270: 				Path(x,y).h(10).addTo(g);
271: 				x += 10;
272: 			}
273: 			item.format(x, y, item.width).addTo(g);
274: 			x += item.width;
275: 			y += item.height;
276: 			if(item.needsSpace) {
277: 				Path(x,y).h(10).addTo(g);
278: 				x += 10;
279: 			}
280: 		}
281: 		this.attrs.width = this.width + paddingl + paddingr;
282: 		this.attrs.height = this.up + this.height + this.down + paddingt + paddingb;
283: 		this.attrs.viewBox = "0 0 " + this.attrs.width + " " + this.attrs.height;
284: 		g.addTo(this);
285: 		this.formatted = true;
286: 		return this;
287: 	}
288: 	DiagramNode.prototype.addTo = function(parent) {
289: 		if(!parent) {
290: 			var scriptTag = document.getElementsByTagName('script');
291: 			scriptTag = scriptTag[scriptTag.length - 1];
292: 			parent = scriptTag.parentNode;
293: 		}
294: 		return this.$super.addTo.call(this, parent);
295: 	}
296: 	DiagramNode.prototype.toSVG = function() {
297: 		if (!this.formatted) {
298: 			this.format();
299: 		}
300: 		return this.$super.toSVG.call(this);
301: 	}
302: 	DiagramNode.prototype.toString = function() {
303: 		if (!this.formatted) {
304: 			this.format();
305: 		}
306: 		return this.$super.toString.call(this);
307: 	}
308: 	DiagramNode.DEBUG = false;
309: 
310: 	var ComplexDiagramNode = funcs.ComplexDiagramNode = function ComplexDiagramNode() {
311: 		var diagram = new DiagramNode([].slice.call(arguments));
312: 		var items = diagram.items;
313: 		items.shift();
314: 		items.pop();
315: 		items.unshift(new Start({type:"complex"}));
316: 		items.push(new End({type:"complex"}));
317: 		diagram.items = items;
318: 		return diagram;
319: 	}
320: 
321: 	var SequenceNode = funcs.SequenceNode = function SequenceNode(items) {
322: 		if(!(this instanceof SequenceNode)) return new SequenceNode([].slice.call(arguments));
323: 		DiagramNodeMultiContainer.call(this, 'g', items);
324: 		var numberOfItems = this.items.length;
325: 		this.needsSpace = true;
326: 		this.up = this.down = this.height = this.width = 0;
327: 		for(var i = 0; i < this.items.length; i++) {
328: 			var item = this.items[i];
329: 			this.width += item.width + (item.needsSpace?20:0);
330: 			this.up = Math.max(this.up, item.up - this.height);
331: 			this.height += item.height;
332: 			this.down = Math.max(this.down - item.height, item.down);
333: 		}
334: 		if(this.items[0].needsSpace) this.width -= 10;
335: 		if(this.items[this.items.length-1].needsSpace) this.width -= 10;
336: 		if(DiagramNode.DEBUG) {
337: 			this.attrs['data-updown'] = this.up + " " + this.height + " " + this.down
338: 			this.attrs['data-type'] = "sequence"
339: 		}
340: 	}
341: 	subclassOf(SequenceNode, DiagramNodeMultiContainer);
342: 	SequenceNode.prototype.format = function(x,y,width) {
343: 		// Hook up the two sides if this is narrower than its stated width.
344: 		var gaps = determineGaps(width, this.width);
345: 		Path(x,y).h(gaps[0]).addTo(this);
346: 		Path(x+gaps[0]+this.width,y+this.height).h(gaps[1]).addTo(this);
347: 		x += gaps[0];
348: 
349: 		for(var i = 0; i < this.items.length; i++) {
350: 			var item = this.items[i];
351: 			if(item.needsSpace && i > 0) {
352: 				Path(x,y).h(10).addTo(this);
353: 				x += 10;
354: 			}
355: 			item.format(x, y, item.width).addTo(this);
356: 			x += item.width;
357: 			y += item.height;
358: 			if(item.needsSpace && i < this.items.length-1) {
359: 				Path(x,y).h(10).addTo(this);
360: 				x += 10;
361: 			}
362: 		}
363: 		return this;
364: 	}
365: 
366: 	var StackNode = funcs.StackNode = function StackNode(items) {
367: 		if(!(this instanceof StackNode)) return new StackNode([].slice.call(arguments));
368: 		DiagramNodeMultiContainer.call(this, 'g', items);
369: 		if( items.length === 0 ) {
370: 			throw new RangeError("StackNode() must have at least one child.");
371: 		}
372: 		this.width = Math.max.apply(null, this.items.map(function(e) { return e.width + (e.needsSpace?20:0); }));
373: 		//if(this.items[0].needsSpace) this.width -= 10;
374: 		//if(this.items[this.items.length-1].needsSpace) this.width -= 10;
375: 		if(this.items.length > 1){
376: 			this.width += DiagramNode.ARC_RADIUS*2;
377: 		}
378: 		this.needsSpace = true;
379: 		this.up = this.items[0].up;
380: 		this.down = this.items[this.items.length-1].down;
381: 
382: 		this.height = 0;
383: 		var last = this.items.length - 1;
384: 		for(var i = 0; i < this.items.length; i++) {
385: 			var item = this.items[i];
386: 			this.height += item.height;
387: 			if(i > 0) {
388: 				this.height += Math.max(DiagramNode.ARC_RADIUS*2, item.up + DiagramNode.VERTICAL_SEPARATION);
389: 			}
390: 			if(i < last) {
391: 				this.height += Math.max(DiagramNode.ARC_RADIUS*2, item.down + DiagramNode.VERTICAL_SEPARATION);
392: 			}
393: 		}
394: 		if(DiagramNode.DEBUG) {
395: 			this.attrs['data-updown'] = this.up + " " + this.height + " " + this.down
396: 			this.attrs['data-type'] = "stack"
397: 		}
398: 	}
399: 	subclassOf(StackNode, DiagramNodeMultiContainer);
400: 	StackNode.prototype.format = function(x,y,width) {
401: 		var gaps = determineGaps(width, this.width);
402: 		Path(x,y).h(gaps[0]).addTo(this);
403: 		x += gaps[0];
404: 		var xInitial = x;
405: 		if(this.items.length > 1) {
406: 			Path(x, y).h(DiagramNode.ARC_RADIUS).addTo(this);
407: 			x += DiagramNode.ARC_RADIUS;
408: 		}
409: 
410: 		for(var i = 0; i < this.items.length; i++) {
411: 			var item = this.items[i];
412: 			var innerWidth = this.width - (this.items.length>1 ? DiagramNode.ARC_RADIUS*2 : 0);
413: 			item.format(x, y, innerWidth).addTo(this);
414: 			x += innerWidth;
415: 			y += item.height;
416: 
417: 			if(i !== this.items.length-1) {
418: 				Path(x, y)
419: 					.arc('ne').down(Math.max(0, item.down + DiagramNode.VERTICAL_SEPARATION - DiagramNode.ARC_RADIUS*2))
420: 					.arc('es').left(innerWidth)
421: 					.arc('nw').down(Math.max(0, this.items[i+1].up + DiagramNode.VERTICAL_SEPARATION - DiagramNode.ARC_RADIUS*2))
422: 					.arc('ws').addTo(this);
423: 				y += Math.max(item.down + DiagramNode.VERTICAL_SEPARATION, DiagramNode.ARC_RADIUS*2) + Math.max(this.items[i+1].up + DiagramNode.VERTICAL_SEPARATION, DiagramNode.ARC_RADIUS*2);
424: 				//y += Math.max(DiagramNode.ARC_RADIUS*4, item.down + DiagramNode.VERTICAL_SEPARATION*2 + this.items[i+1].up)
425: 				x = xInitial+DiagramNode.ARC_RADIUS;
426: 			}
427: 
428: 		}
429: 
430: 		if(this.items.length > 1) {
431: 			Path(x,y).h(DiagramNode.ARC_RADIUS).addTo(this);
432: 			x += DiagramNode.ARC_RADIUS;
433: 		}
434: 		Path(x,y).h(gaps[1]).addTo(this);
435: 
436: 		return this;
437: 	}
438: 
439: 	var OptionalSequenceNode = funcs.OptionalSequenceNode = function OptionalSequenceNode(items) {
440: 		if(!(this instanceof OptionalSequenceNode)) return new OptionalSequenceNode([].slice.call(arguments));
441: 		DiagramNodeMultiContainer.call(this, 'g', items);
442: 		if( items.length === 0 ) {
443: 			throw new RangeError("OptionalSequenceNode() must have at least one child.");
444: 		}
445: 		if( items.length === 1 ) {
446: 			return new SequenceNode(items);
447: 		}
448: 		var arc = DiagramNode.ARC_RADIUS;
449: 		this.needsSpace = false;
450: 		this.width = 0;
451: 		this.up = 0;
452: 		this.height = sum(this.items, function(x){return x.height});
453: 		this.down = this.items[0].down;
454: 		var heightSoFar = 0;
455: 		for(var i = 0; i < this.items.length; i++) {
456: 			var item = this.items[i];
457: 			this.up = Math.max(this.up, Math.max(arc*2, item.up + DiagramNode.VERTICAL_SEPARATION) - heightSoFar);
458: 			heightSoFar += item.height;
459: 			if(i > 0) {
460: 				this.down = Math.max(this.height + this.down, heightSoFar + Math.max(arc*2, item.down + DiagramNode.VERTICAL_SEPARATION)) - this.height;
461: 			}
462: 			var itemWidth = (item.needsSpace?10:0) + item.width;
463: 			if(i == 0) {
464: 				this.width += arc + Math.max(itemWidth, arc);
465: 			} else {
466: 				this.width += arc*2 + Math.max(itemWidth, arc) + arc;
467: 			}
468: 		}
469: 		if(DiagramNode.DEBUG) {
470: 			this.attrs['data-updown'] = this.up + " " + this.height + " " + this.down
471: 			this.attrs['data-type'] = "optseq"
472: 		}
473: 	}
474: 	subclassOf(OptionalSequenceNode, DiagramNodeMultiContainer);
475: 	OptionalSequenceNode.prototype.format = function(x, y, width) {
476: 		var arc = DiagramNode.ARC_RADIUS;
477: 		var gaps = determineGaps(width, this.width);
478: 		Path(x, y).right(gaps[0]).addTo(this);
479: 		Path(x + gaps[0] + this.width, y + this.height).right(gaps[1]).addTo(this);
480: 		x += gaps[0]
481: 		var upperLineY = y - this.up;
482: 		var last = this.items.length - 1;
483: 		for(var i = 0; i < this.items.length; i++) {
484: 			var item = this.items[i];
485: 			var itemSpace = (item.needsSpace?10:0);
486: 			var itemWidth = item.width + itemSpace;
487: 			if(i == 0) {
488: 				// Upper skip
489: 				Path(x,y)
490: 					.arc('se')
491: 					.up(y - upperLineY - arc*2)
492: 					.arc('wn')
493: 					.right(itemWidth - arc)
494: 					.arc('ne')
495: 					.down(y + item.height - upperLineY - arc*2)
496: 					.arc('ws')
497: 					.addTo(this);
498: 				// Straight line
499: 				Path(x, y)
500: 					.right(itemSpace + arc)
501: 					.addTo(this);
502: 				item.format(x + itemSpace + arc, y, item.width).addTo(this);
503: 				x += itemWidth + arc;
504: 				y += item.height;
505: 				// x ends on the far side of the first element,
506: 				// where the next element's skip needs to begin
507: 			} else if(i < last) {
508: 				// Upper skip
509: 				Path(x, upperLineY)
510: 					.right(arc*2 + Math.max(itemWidth, arc) + arc)
511: 					.arc('ne')
512: 					.down(y - upperLineY + item.height - arc*2)
513: 					.arc('ws')
514: 					.addTo(this);
515: 				// Straight line
516: 				Path(x,y)
517: 					.right(arc*2)
518: 					.addTo(this);
519: 				item.format(x + arc*2, y, item.width).addTo(this);
520: 				Path(x + item.width + arc*2, y + item.height)
521: 					.right(itemSpace + arc)
522: 					.addTo(this);
523: 				// Lower skip
524: 				Path(x,y)
525: 					.arc('ne')
526: 					.down(item.height + Math.max(item.down + DiagramNode.VERTICAL_SEPARATION, arc*2) - arc*2)
527: 					.arc('ws')
528: 					.right(itemWidth - arc)
529: 					.arc('se')
530: 					.up(item.down + DiagramNode.VERTICAL_SEPARATION - arc*2)
531: 					.arc('wn')
532: 					.addTo(this);
533: 				x += arc*2 + Math.max(itemWidth, arc) + arc;
534: 				y += item.height;
535: 			} else {
536: 				// Straight line
537: 				Path(x, y)
538: 					.right(arc*2)
539: 					.addTo(this);
540: 				item.format(x + arc*2, y, item.width).addTo(this);
541: 				Path(x + arc*2 + item.width, y + item.height)
542: 					.right(itemSpace + arc)
543: 					.addTo(this);
544: 				// Lower skip
545: 				Path(x,y)
546: 					.arc('ne')
547: 					.down(item.height + Math.max(item.down + DiagramNode.VERTICAL_SEPARATION, arc*2) - arc*2)
548: 					.arc('ws')
549: 					.right(itemWidth - arc)
550: 					.arc('se')
551: 					.up(item.down + DiagramNode.VERTICAL_SEPARATION - arc*2)
552: 					.arc('wn')
553: 					.addTo(this);
554: 			}
555: 		}
556: 		return this;
557: 	}
558: 
559: 	var AlternatingSequenceNode = funcs.AlternatingSequenceNode = function AlternatingSequenceNode(items) {
560: 		if(!(this instanceof AlternatingSequenceNode)) return new AlternatingSequenceNode([].slice.call(arguments));
561: 		DiagramNodeMultiContainer.call(this, 'g', items);
562: 		if( items.length === 1 ) {
563: 			return new SequenceNode(items);
564: 		}
565: 		if( items.length !== 2 ) {
566: 			throw new RangeError("AlternatingSequenceNode() must have one or two children.");
567: 		}
568: 		this.needsSpace = false;
569: 
570: 		const arc = DiagramNode.ARC_RADIUS;
571: 		const vert = DiagramNode.VERTICAL_SEPARATION;
572: 		const max = Math.max;
573: 		const first = this.items[0];
574: 		const second = this.items[1];
575: 
576: 		const arcX = 1 / Math.sqrt(2) * arc * 2;
577: 		const arcY = (1 - 1 / Math.sqrt(2)) * arc * 2;
578: 		const crossY = Math.max(arc, DiagramNode.VERTICAL_SEPARATION);
579: 		const crossX = (crossY - arcY) + arcX;
580: 
581: 		const firstOut = max(arc + arc, crossY/2 + arc + arc, crossY/2 + vert + first.down);
582: 		this.up = firstOut + first.height + first.up;
583: 
584: 		const secondIn = max(arc + arc, crossY/2 + arc + arc, crossY/2 + vert + second.up);
585: 		this.down = secondIn + second.height + second.down;
586: 
587: 		this.height = 0;
588: 
589: 		const firstWidth = 2*(first.needsSpace?10:0) + first.width;
590: 		const secondWidth = 2*(second.needsSpace?10:0) + second.width;
591: 		this.width = 2*arc + max(firstWidth, crossX, secondWidth) + 2*arc;
592: 
593: 		if(DiagramNode.DEBUG) {
594: 			this.attrs['data-updown'] = this.up + " " + this.height + " " + this.down
595: 			this.attrs['data-type'] = "altseq"
596: 		}
597: 	}
598: 	subclassOf(AlternatingSequenceNode, DiagramNodeMultiContainer);
599: 	AlternatingSequenceNode.prototype.format = function(x, y, width) {
600: 		const arc = DiagramNode.ARC_RADIUS;
601: 		const gaps = determineGaps(width, this.width);
602: 		Path(x,y).right(gaps[0]).addTo(this);
603: 		console.log(gaps);
604: 		x += gaps[0];
605: 		Path(x+this.width, y).right(gaps[1]).addTo(this);
606: 		// bounding box
607: 		//Path(x+gaps[0], y).up(this.up).right(this.width).down(this.up+this.down).left(this.width).up(this.down).addTo(this);
608: 		const first = this.items[0];
609: 		const second = this.items[1];
610: 
611: 		// top
612: 		const firstIn = this.up - first.up;
613: 		const firstOut = this.up - first.up - first.height;
614: 		Path(x,y).arc('se').up(firstIn-2*arc).arc('wn').addTo(this);
615: 		first.format(x + 2*arc, y - firstIn, this.width - 4*arc).addTo(this);
616: 		Path(x + this.width - 2*arc, y - firstOut).arc('ne').down(firstOut - 2*arc).arc('ws').addTo(this);
617: 
618: 		// bottom
619: 		const secondIn = this.down - second.down - second.height;
620: 		const secondOut = this.down - second.down;
621: 		Path(x,y).arc('ne').down(secondIn - 2*arc).arc('ws').addTo(this);
622: 		second.format(x + 2*arc, y + secondIn, this.width - 4*arc).addTo(this);
623: 		Path(x + this.width - 2*arc, y + secondOut).arc('se').up(secondOut - 2*arc).arc('wn').addTo(this);
624: 
625: 		// crossover
626: 		const arcX = 1 / Math.sqrt(2) * arc * 2;
627: 		const arcY = (1 - 1 / Math.sqrt(2)) * arc * 2;
628: 		const crossY = Math.max(arc, DiagramNode.VERTICAL_SEPARATION);
629: 		const crossX = (crossY - arcY) + arcX;
630: 		const crossBar = (this.width - 4*arc - crossX)/2;
631: 		Path(x+arc, y - crossY/2 - arc).arc('ws').right(crossBar)
632: 			.arc_8('n', 'cw').l(crossX - arcX, crossY - arcY).arc_8('sw', 'ccw')
633: 			.right(crossBar).arc('ne').addTo(this);
634: 		Path(x+arc, y + crossY/2 + arc).arc('wn').right(crossBar)
635: 			.arc_8('s', 'ccw').l(crossX - arcX, -(crossY - arcY)).arc_8('nw', 'cw')
636: 			.right(crossBar).arc('se').addTo(this);
637: 
638: 		return this;
639: 	}
640: 
641: 	var ChoiceNode = funcs.ChoiceNode = function ChoiceNode(normal, items) {
642: 		if(!(this instanceof ChoiceNode)) return new ChoiceNode(normal, [].slice.call(arguments,1));
643: 		DiagramNodeMultiContainer.call(this, 'g', items);
644: 		if( typeof normal !== "number" || normal !== Math.floor(normal) ) {
645: 			throw new TypeError("The first argument of ChoiceNode() must be an integer.");
646: 		} else if(normal < 0 || normal >= items.length) {
647: 			throw new RangeError("The first argument of ChoiceNode() must be an index for one of the items.");
648: 		} else {
649: 			this.normal = normal;
650: 		}
651: 		var first = 0;
652: 		var last = items.length - 1;
653: 		this.width = Math.max.apply(null, this.items.map(function(el){return el.width})) + DiagramNode.ARC_RADIUS*4;
654: 		this.height = this.items[normal].height;
655: 		this.up = this.items[first].up;
656: 		for(var i = first; i < normal; i++) {
657: 			if(i == normal-1) var arcs = DiagramNode.ARC_RADIUS*2;
658: 			else var arcs = DiagramNode.ARC_RADIUS;
659: 			this.up += Math.max(arcs, this.items[i].height + this.items[i].down + DiagramNode.VERTICAL_SEPARATION + this.items[i+1].up);
660: 		}
661: 		this.down = this.items[last].down;
662: 		for(var i = normal+1; i <= last; i++) {
663: 			if(i == normal+1) var arcs = DiagramNode.ARC_RADIUS*2;
664: 			else var arcs = DiagramNode.ARC_RADIUS;
665: 			this.down += Math.max(arcs, this.items[i-1].height + this.items[i-1].down + DiagramNode.VERTICAL_SEPARATION + this.items[i].up);
666: 		}
667: 		this.down -= this.items[normal].height; // already counted in ChoiceNode.height
668: 		if(DiagramNode.DEBUG) {
669: 			this.attrs['data-updown'] = this.up + " " + this.height + " " + this.down
670: 			this.attrs['data-type'] = "choice"
671: 		}
672: 	}
673: 	subclassOf(ChoiceNode, DiagramNodeMultiContainer);
674: 	ChoiceNode.prototype.format = function(x,y,width) {
675: 		// Hook up the two sides if this is narrower than its stated width.
676: 		var gaps = determineGaps(width, this.width);
677: 		Path(x,y).h(gaps[0]).addTo(this);
678: 		Path(x+gaps[0]+this.width,y+this.height).h(gaps[1]).addTo(this);
679: 		x += gaps[0];
680: 
681: 		var last = this.items.length -1;
682: 		var innerWidth = this.width - DiagramNode.ARC_RADIUS*4;
683: 
684: 		// Do the elements that curve above
685: 		for(var i = this.normal - 1; i >= 0; i--) {
686: 			var item = this.items[i];
687: 			if( i == this.normal - 1 ) {
688: 				var distanceFromY = Math.max(DiagramNode.ARC_RADIUS*2, this.items[this.normal].up + DiagramNode.VERTICAL_SEPARATION + item.down + item.height);
689: 			}
690: 			Path(x,y)
691: 				.arc('se')
692: 				.up(distanceFromY - DiagramNode.ARC_RADIUS*2)
693: 				.arc('wn').addTo(this);
694: 			item.format(x+DiagramNode.ARC_RADIUS*2,y - distanceFromY,innerWidth).addTo(this);
695: 			Path(x+DiagramNode.ARC_RADIUS*2+innerWidth, y-distanceFromY+item.height)
696: 				.arc('ne')
697: 				.down(distanceFromY - item.height + this.height - DiagramNode.ARC_RADIUS*2)
698: 				.arc('ws').addTo(this);
699: 			distanceFromY += Math.max(DiagramNode.ARC_RADIUS, item.up + DiagramNode.VERTICAL_SEPARATION + (i == 0 ? 0 : this.items[i-1].down+this.items[i-1].height));
700: 		}
701: 
702: 		// Do the straight-line path.
703: 		Path(x,y).right(DiagramNode.ARC_RADIUS*2).addTo(this);
704: 		this.items[this.normal].format(x+DiagramNode.ARC_RADIUS*2, y, innerWidth).addTo(this);
705: 		Path(x+DiagramNode.ARC_RADIUS*2+innerWidth, y+this.height).right(DiagramNode.ARC_RADIUS*2).addTo(this);
706: 
707: 		// Do the elements that curve below
708: 		for(var i = this.normal+1; i <= last; i++) {
709: 			var item = this.items[i];
710: 			if( i == this.normal + 1 ) {
711: 				var distanceFromY = Math.max(DiagramNode.ARC_RADIUS*2, this.height + this.items[this.normal].down + DiagramNode.VERTICAL_SEPARATION + item.up);
712: 			}
713: 			Path(x,y)
714: 				.arc('ne')
715: 				.down(distanceFromY - DiagramNode.ARC_RADIUS*2)
716: 				.arc('ws').addTo(this);
717: 			item.format(x+DiagramNode.ARC_RADIUS*2, y+distanceFromY, innerWidth).addTo(this);
718: 			Path(x+DiagramNode.ARC_RADIUS*2+innerWidth, y+distanceFromY+item.height)
719: 				.arc('se')
720: 				.up(distanceFromY - DiagramNode.ARC_RADIUS*2 + item.height - this.height)
721: 				.arc('wn').addTo(this);
722: 			distanceFromY += Math.max(DiagramNode.ARC_RADIUS, item.height + item.down + DiagramNode.VERTICAL_SEPARATION + (i == last ? 0 : this.items[i+1].up));
723: 		}
724: 
725: 		return this;
726: 	}
727: 
728: 
729: 	var HorizontalChoiceNode = funcs.HorizontalChoiceNode = function HorizontalChoiceNode(items) {
730: 		if(!(this instanceof HorizontalChoiceNode)) return new HorizontalChoiceNode([].slice.call(arguments));
731: 		if( items.length === 0 ) {
732: 			throw new RangeError("HorizontalChoiceNode() must have at least one child.");
733: 		}
734: 		if( items.length === 1) {
735: 			return new SequenceNode(items);
736: 		}
737: 		DiagramNodeMultiContainer.call(this, 'g', items);
738: 
739: 		const allButLast = this.items.slice(0, -1);
740: 		const middles = this.items.slice(1, -1);
741: 		const first = this.items[0];
742: 		const last = this.items[this.items.length - 1];
743: 		this.needsSpace = false;
744: 
745: 		this.width = DiagramNode.ARC_RADIUS; // starting track
746: 		this.width += DiagramNode.ARC_RADIUS*2 * (this.items.length-1); // inbetween tracks
747: 		this.width += sum(this.items, x=>x.width + (x.needsSpace?20:0)); // items
748: 		this.width += (last.height > 0 ? DiagramNode.ARC_RADIUS : 0); // needs space to curve up
749: 		this.width += DiagramNode.ARC_RADIUS; //ending track
750: 
751: 		// Always exits at entrance height
752: 		this.height = 0;
753: 
754: 		// All but the last have a track running above them
755: 		this._upperTrack = Math.max(
756: 			DiagramNode.ARC_RADIUS*2,
757: 			DiagramNode.VERTICAL_SEPARATION,
758: 			max(allButLast, x=>x.up) + DiagramNode.VERTICAL_SEPARATION
759: 		);
760: 		this.up = Math.max(this._upperTrack, last.up);
761: 
762: 		// All but the first have a track running below them
763: 		// Last either straight-lines or curves up, so has different calculation
764: 		this._lowerTrack = Math.max(
765: 			DiagramNode.VERTICAL_SEPARATION,
766: 			max(middles, x=>x.height+Math.max(x.down+DiagramNode.VERTICAL_SEPARATION, DiagramNode.ARC_RADIUS*2)),
767: 			last.height + last.down + DiagramNode.VERTICAL_SEPARATION
768: 		);
769: 		if(first.height < this._lowerTrack) {
770: 			// Make sure there's at least 2*AR room between first exit and lower track
771: 			this._lowerTrack = Math.max(this._lowerTrack, first.height + DiagramNode.ARC_RADIUS*2);
772: 		}
773: 		this.down = Math.max(this._lowerTrack, first.height + first.down);
774: 
775: 		if(DiagramNode.DEBUG) {
776: 			this.attrs['data-updown'] = this.up + " " + this.height + " " + this.down
777: 			this.attrs['data-type'] = "horizontalchoice"
778: 		}
779: 	}
780: 	subclassOf(HorizontalChoiceNode, DiagramNodeMultiContainer);
781: 	HorizontalChoiceNode.prototype.format = function(x,y,width) {
782: 		// Hook up the two sides if this is narrower than its stated width.
783: 		var gaps = determineGaps(width, this.width);
784: 		new Path(x,y).h(gaps[0]).addTo(this);
785: 		new Path(x+gaps[0]+this.width,y+this.height).h(gaps[1]).addTo(this);
786: 		x += gaps[0];
787: 
788: 		const first = this.items[0];
789: 		const last = this.items[this.items.length-1];
790: 		const allButFirst = this.items.slice(1);
791: 		const allButLast = this.items.slice(0, -1);
792: 
793: 		// upper track
794: 		var upperSpan = (sum(allButLast, x=>x.width+(x.needsSpace?20:0))
795: 			+ (this.items.length - 2) * DiagramNode.ARC_RADIUS*2
796: 			- DiagramNode.ARC_RADIUS
797: 		);
798: 		new Path(x,y)
799: 			.arc('se')
800: 			.v(-(this._upperTrack - DiagramNode.ARC_RADIUS*2))
801: 			.arc('wn')
802: 			.h(upperSpan)
803: 			.addTo(this);
804: 
805: 		// lower track
806: 		var lowerSpan = (sum(allButFirst, x=>x.width+(x.needsSpace?20:0))
807: 			+ (this.items.length - 2) * DiagramNode.ARC_RADIUS*2
808: 			+ (last.height > 0 ? DiagramNode.ARC_RADIUS : 0)
809: 			- DiagramNode.ARC_RADIUS
810: 		);
811: 		var lowerStart = x + DiagramNode.ARC_RADIUS + first.width+(first.needsSpace?20:0) + DiagramNode.ARC_RADIUS*2;
812: 		new Path(lowerStart, y+this._lowerTrack)
813: 			.h(lowerSpan)
814: 			.arc('se')
815: 			.v(-(this._lowerTrack - DiagramNode.ARC_RADIUS*2))
816: 			.arc('wn')
817: 			.addTo(this);
818: 
819: 		// Items
820: 		for(const [i, item] of enumerate(this.items)) {
821: 			// input track
822: 			if(i === 0) {
823: 				new Path(x,y)
824: 					.h(DiagramNode.ARC_RADIUS)
825: 					.addTo(this);
826: 				x += DiagramNode.ARC_RADIUS;
827: 			} else {
828: 				new Path(x, y - this._upperTrack)
829: 					.arc('ne')
830: 					.v(this._upperTrack - DiagramNode.ARC_RADIUS*2)
831: 					.arc('ws')
832: 					.addTo(this);
833: 				x += DiagramNode.ARC_RADIUS*2;
834: 			}
835: 
836: 			// item
837: 			var itemWidth = item.width + (item.needsSpace?20:0);
838: 			item.format(x, y, itemWidth).addTo(this);
839: 			x += itemWidth;
840: 
841: 			// output track
842: 			if(i === this.items.length-1) {
843: 				if(item.height === 0) {
844: 					new Path(x,y)
845: 						.h(DiagramNode.ARC_RADIUS)
846: 						.addTo(this);
847: 				} else {
848: 					new Path(x,y+item.height)
849: 					.arc('se')
850: 					.addTo(this);
851: 				}
852: 			} else if(i === 0 && item.height > this._lowerTrack) {
853: 				// Needs to arc up to meet the lower track, not down.
854: 				if(item.height - this._lowerTrack >= DiagramNode.ARC_RADIUS*2) {
855: 					new Path(x, y+item.height)
856: 						.arc('se')
857: 						.v(this._lowerTrack - item.height + DiagramNode.ARC_RADIUS*2)
858: 						.arc('wn')
859: 						.addTo(this);
860: 				} else {
861: 					// Not enough space to fit two arcs
862: 					// so just bail and draw a straight line for now.
863: 					new Path(x, y+item.height)
864: 						.l(DiagramNode.ARC_RADIUS*2, this._lowerTrack - item.height)
865: 						.addTo(this);
866: 				}
867: 			} else {
868: 				new Path(x, y+item.height)
869: 					.arc('ne')
870: 					.v(this._lowerTrack - item.height - DiagramNode.ARC_RADIUS*2)
871: 					.arc('ws')
872: 					.addTo(this);
873: 			}
874: 		}
875: 		return this;
876: 	}
877: 
878: 
879: 	var MultipleChoiceNode = funcs.MultipleChoiceNode = function MultipleChoiceNode(normal, type, items) {
880: 		if(!(this instanceof MultipleChoiceNode)) return new MultipleChoiceNode(normal, type, [].slice.call(arguments,2));
881: 		DiagramNodeMultiContainer.call(this, 'g', items);
882: 		if( typeof normal !== "number" || normal !== Math.floor(normal) ) {
883: 			throw new TypeError("The first argument of MultipleChoiceNode() must be an integer.");
884: 		} else if(normal < 0 || normal >= items.length) {
885: 			throw new RangeError("The first argument of MultipleChoiceNode() must be an index for one of the items.");
886: 		} else {
887: 			this.normal = normal;
888: 		}
889: 		if( type != "any" && type != "all" ) {
890: 			throw new SyntaxError("The second argument of MultipleChoiceNode must be 'any' or 'all'.");
891: 		} else {
892: 			this.type = type;
893: 		}
894: 		this.needsSpace = true;
895: 		this.innerWidth = max(this.items, function(x){return x.width});
896: 		this.width = 30 + DiagramNode.ARC_RADIUS + this.innerWidth + DiagramNode.ARC_RADIUS + 20;
897: 		this.up = this.items[0].up;
898: 		this.down = this.items[this.items.length-1].down;
899: 		this.height = this.items[normal].height;
900: 		for(var i = 0; i < this.items.length; i++) {
901: 			var item = this.items[i];
902: 			if(i == normal - 1 || i == normal + 1) var minimum = 10 + DiagramNode.ARC_RADIUS;
903: 			else var minimum = DiagramNode.ARC_RADIUS;
904: 			if(i < normal) {
905: 				this.up += Math.max(minimum, item.height + item.down + DiagramNode.VERTICAL_SEPARATION + this.items[i+1].up);
906: 			} else if(i > normal) {
907: 				this.down += Math.max(minimum, item.up + DiagramNode.VERTICAL_SEPARATION + this.items[i-1].down + this.items[i-1].height);
908: 			}
909: 		}
910: 		this.down -= this.items[normal].height; // already counted in this.height
911: 		if(DiagramNode.DEBUG) {
912: 			this.attrs['data-updown'] = this.up + " " + this.height + " " + this.down
913: 			this.attrs['data-type'] = "multiplechoice"
914: 		}
915: 	}
916: 	subclassOf(MultipleChoiceNode, DiagramNodeMultiContainer);
917: 	MultipleChoiceNode.prototype.format = function(x, y, width) {
918: 		var gaps = determineGaps(width, this.width);
919: 		Path(x, y).right(gaps[0]).addTo(this);
920: 		Path(x + gaps[0] + this.width, y + this.height).right(gaps[1]).addTo(this);
921: 		x += gaps[0];
922: 
923: 		var normal = this.items[this.normal];
924: 
925: 		// Do the elements that curve above
926: 		for(var i = this.normal - 1; i >= 0; i--) {
927: 			var item = this.items[i];
928: 			if( i == this.normal - 1 ) {
929: 				var distanceFromY = Math.max(10 + DiagramNode.ARC_RADIUS, normal.up + DiagramNode.VERTICAL_SEPARATION + item.down + item.height);
930: 			}
931: 			Path(x + 30,y)
932: 				.up(distanceFromY - DiagramNode.ARC_RADIUS)
933: 				.arc('wn').addTo(this);
934: 			item.format(x + 30 + DiagramNode.ARC_RADIUS, y - distanceFromY, this.innerWidth).addTo(this);
935: 			Path(x + 30 + DiagramNode.ARC_RADIUS + this.innerWidth, y - distanceFromY + item.height)
936: 				.arc('ne')
937: 				.down(distanceFromY - item.height + this.height - DiagramNode.ARC_RADIUS - 10)
938: 				.addTo(this);
939: 			if(i != 0) {
940: 				distanceFromY += Math.max(DiagramNode.ARC_RADIUS, item.up + DiagramNode.VERTICAL_SEPARATION + this.items[i-1].down + this.items[i-1].height);
941: 			}
942: 		}
943: 
944: 		Path(x + 30, y).right(DiagramNode.ARC_RADIUS).addTo(this);
945: 		normal.format(x + 30 + DiagramNode.ARC_RADIUS, y, this.innerWidth).addTo(this);
946: 		Path(x + 30 + DiagramNode.ARC_RADIUS + this.innerWidth, y + this.height).right(DiagramNode.ARC_RADIUS).addTo(this);
947: 
948: 		for(var i = this.normal+1; i < this.items.length; i++) {
949: 			var item = this.items[i];
950: 			if(i == this.normal + 1) {
951: 				var distanceFromY = Math.max(10+DiagramNode.ARC_RADIUS, normal.height + normal.down + DiagramNode.VERTICAL_SEPARATION + item.up);
952: 			}
953: 			Path(x + 30, y)
954: 				.down(distanceFromY - DiagramNode.ARC_RADIUS)
955: 				.arc('ws')
956: 				.addTo(this);
957: 			item.format(x + 30 + DiagramNode.ARC_RADIUS, y + distanceFromY, this.innerWidth).addTo(this)
958: 			Path(x + 30 + DiagramNode.ARC_RADIUS + this.innerWidth, y + distanceFromY + item.height)
959: 				.arc('se')
960: 				.up(distanceFromY - DiagramNode.ARC_RADIUS + item.height - normal.height)
961: 				.addTo(this);
962: 			if(i != this.items.length - 1) {
963: 				distanceFromY += Math.max(DiagramNode.ARC_RADIUS, item.height + item.down + DiagramNode.VERTICAL_SEPARATION + this.items[i+1].up);
964: 			}
965: 		}
966: 		var text = FakeSVG('g', {"class": "diagram-text"}).addTo(this)
967: 		FakeSVG('title', {}, (this.type=="any"?"take one or more branches, once each, in any order":"take all branches, once each, in any order")).addTo(text)
968: 		FakeSVG('path', {
969: 			"d": "M "+(x+30)+" "+(y-10)+" h -26 a 4 4 0 0 0 -4 4 v 12 a 4 4 0 0 0 4 4 h 26 z",
970: 			"class": "diagram-text"
971: 			}).addTo(text)
972: 		FakeSVG('text', {
973: 			"x": x + 15,
974: 			"y": y + 4,
975: 			"class": "diagram-text"
976: 			}, (this.type=="any"?"1+":"all")).addTo(text)
977: 		FakeSVG('path', {
978: 			"d": "M "+(x+this.width-20)+" "+(y-10)+" h 16 a 4 4 0 0 1 4 4 v 12 a 4 4 0 0 1 -4 4 h -16 z",
979: 			"class": "diagram-text"
980: 			}).addTo(text)
981: 		FakeSVG('path', {
982: 			"d": "M "+(x+this.width-13)+" "+(y-2)+" a 4 4 0 1 0 6 -1 m 2.75 -1 h -4 v 4 m 0 -3 h 2",
983: 			"style": "stroke-width: 1.75"
984: 		}).addTo(text)
985: 		return this;
986: 	};
987: 
988: 	var OptionalNode = funcs.OptionalNode = function OptionalNode(item, skip) {
989: 		if( skip === undefined )
990: 			return ChoiceNode(1, Skip(), item);
991: 		else if ( skip === "skip" )
992: 			return ChoiceNode(0, Skip(), item);
993: 		else
994: 			throw "Unknown value for OptionalNode()'s 'skip' argument.";
995: 	}
996: 
997: 	var OneOrMoreNode = funcs.OneOrMoreNode = function OneOrMoreNode(item, rep) {
998: 		if(!(this instanceof OneOrMoreNode)) return new OneOrMoreNode(item, rep);
999: 		FakeSVG.call(this, 'g');
1000: 		rep = rep || (new Skip);
1001: 		this.item = wrapString(item);
1002: 		this.rep = wrapString(rep);
1003: 		this.width = Math.max(this.item.width, this.rep.width) + DiagramNode.ARC_RADIUS*2;
1004: 		this.height = this.item.height;
1005: 		this.up = this.item.up;
1006: 		this.down = Math.max(DiagramNode.ARC_RADIUS*2, this.item.down + DiagramNode.VERTICAL_SEPARATION + this.rep.up + this.rep.height + this.rep.down);
1007: 		if(DiagramNode.DEBUG) {
1008: 			this.attrs['data-updown'] = this.up + " " + this.height + " " + this.down
1009: 			this.attrs['data-type'] = "oneormore"
1010: 		}
1011: 	}
1012: 	subclassOf(OneOrMoreNode, FakeSVG);
1013: 	OneOrMoreNode.prototype.needsSpace = true;
1014: 	OneOrMoreNode.prototype.format = function(x,y,width) {
1015: 		// Hook up the two sides if this is narrower than its stated width.
1016: 		var gaps = determineGaps(width, this.width);
1017: 		Path(x,y).h(gaps[0]).addTo(this);
1018: 		Path(x+gaps[0]+this.width,y+this.height).h(gaps[1]).addTo(this);
1019: 		x += gaps[0];
1020: 
1021: 		// Draw item
1022: 		Path(x,y).right(DiagramNode.ARC_RADIUS).addTo(this);
1023: 		this.item.format(x+DiagramNode.ARC_RADIUS,y,this.width-DiagramNode.ARC_RADIUS*2).addTo(this);
1024: 		Path(x+this.width-DiagramNode.ARC_RADIUS,y+this.height).right(DiagramNode.ARC_RADIUS).addTo(this);
1025: 
1026: 		// Draw repeat arc
1027: 		var distanceFromY = Math.max(DiagramNode.ARC_RADIUS*2, this.item.height+this.item.down+DiagramNode.VERTICAL_SEPARATION+this.rep.up);
1028: 		Path(x+DiagramNode.ARC_RADIUS,y).arc('nw').down(distanceFromY-DiagramNode.ARC_RADIUS*2).arc('ws').addTo(this);
1029: 		this.rep.format(x+DiagramNode.ARC_RADIUS, y+distanceFromY, this.width - DiagramNode.ARC_RADIUS*2).addTo(this);
1030: 		Path(x+this.width-DiagramNode.ARC_RADIUS, y+distanceFromY+this.rep.height).arc('se').up(distanceFromY-DiagramNode.ARC_RADIUS*2+this.rep.height-this.item.height).arc('en').addTo(this);
1031: 
1032: 		return this;
1033: 	}
1034: 	OneOrMoreNode.prototype.walk = function(cb) {
1035: 		cb(this);
1036: 		this.item.walk(cb);
1037: 		this.rep.walk(cb);
1038: 	}
1039: 
1040: 	var ZeroOrMore = funcs.ZeroOrMore = function ZeroOrMore(item, rep, skip) {
1041: 		return OptionalNode(OneOrMoreNode(item, rep), skip);
1042: 	}
1043: 
1044: 	var Start = funcs.Start = function Start({type="simple", label}={}) {
1045: 		if(!(this instanceof Start)) return new Start({type, label});
1046: 		FakeSVG.call(this, 'g');
1047: 		this.width = 20;
1048: 		this.height = 0;
1049: 		this.up = 10;
1050: 		this.down = 10;
1051: 		this.type = type;
1052: 		if(label != undefined) {
1053: 			this.label = ""+label;
1054: 			this.width = Math.max(20, this.label.length * DiagramNode.CHAR_WIDTH + 10);
1055: 		}
1056: 		if(DiagramNode.DEBUG) {
1057: 			this.attrs['data-updown'] = this.up + " " + this.height + " " + this.down
1058: 			this.attrs['data-type'] = "start"
1059: 		}
1060: 	}
1061: 	subclassOf(Start, FakeSVG);
1062: 	Start.prototype.format = function(x,y) {
1063: 		let path = new Path(x, y-10);
1064: 		if (this.type === "complex") {
1065: 			path.down(20)
1066: 				.m(0, -10)
1067: 				.right(this.width)
1068: 				.addTo(this);
1069: 		} else {
1070: 			path.down(20)
1071: 				.m(10, -20)
1072: 				.down(20)
1073: 				.m(-10, -10)
1074: 				.right(this.width)
1075: 				.addTo(this);
1076: 		}
1077: 		if(this.label) {
1078: 			new FakeSVG('text', {x:x, y:y-15, style:"text-anchor:start"}, this.label).addTo(this);
1079: 		}
1080: 		return this;
1081: 	}
1082: 
1083: 	var End = funcs.End = function End({type="simple"}={}) {
1084: 		if(!(this instanceof End)) return new End({type});
1085: 		FakeSVG.call(this, 'path');
1086: 		this.width = 20;
1087: 		this.height = 0;
1088: 		this.up = 10;
1089: 		this.down = 10;
1090: 		this.type = type;
1091: 		if(DiagramNode.DEBUG) {
1092: 			this.attrs['data-updown'] = this.up + " " + this.height + " " + this.down
1093: 			this.attrs['data-type'] = "end"
1094: 		}
1095: 	}
1096: 	subclassOf(End, FakeSVG);
1097: 	End.prototype.format = function(x,y) {
1098: 		if (this.type === "complex") {
1099: 			this.attrs.d = 'M '+x+' '+y+' h 20 m 0 -10 v 20';
1100: 		} else {
1101: 			this.attrs.d = 'M '+x+' '+y+' h 20 m -10 -10 v 20 m 10 -20 v 20';
1102: 		}
1103: 		return this;
1104: 	}
1105: 
1106: 	var Terminal = funcs.Terminal = function Terminal(text, {href, title}={}) {
1107: 		if(!(this instanceof Terminal)) return new Terminal(text, {href, title});
1108: 		FakeSVG.call(this, 'g', {'class': 'terminal'});
1109: 		this.text = ""+text;
1110: 		this.href = href;
1111: 		this.title = title;
1112: 		this.width = this.text.length * DiagramNode.CHAR_WIDTH + 20;
1113: 		this.height = 0;
1114: 		this.up = 11;
1115: 		this.down = 11;
1116: 		if(DiagramNode.DEBUG) {
1117: 			this.attrs['data-updown'] = this.up + " " + this.height + " " + this.down
1118: 			this.attrs['data-type'] = "terminal"
1119: 		}
1120: 	}
1121: 	subclassOf(Terminal, FakeSVG);
1122: 	Terminal.prototype.needsSpace = true;
1123: 	Terminal.prototype.format = function(x, y, width) {
1124: 		// Hook up the two sides if this is narrower than its stated width.
1125: 		var gaps = determineGaps(width, this.width);
1126: 		Path(x,y).h(gaps[0]).addTo(this);
1127: 		Path(x+gaps[0]+this.width,y).h(gaps[1]).addTo(this);
1128: 		x += gaps[0];
1129: 
1130: 		FakeSVG('rect', {x:x, y:y-11, width:this.width, height:this.up+this.down, rx:10, ry:10}).addTo(this);
1131: 		var text = FakeSVG('text', {x:x+this.width/2, y:y+4}, this.text);
1132: 		if(this.href)
1133: 			FakeSVG('a', {'xlink:href': this.href}, [text]).addTo(this);
1134: 		else
1135: 			text.addTo(this);
1136: 		if(this.title)
1137: 			new FakeSVG('title', {}, this.title).addTo(this);
1138: 		return this;
1139: 	}
1140: 
1141: 	var NonTerminal = funcs.NonTerminal = function NonTerminal(text, element_class, on_click, {href, title}={}) {
1142: 		if(!(this instanceof NonTerminal)) return new NonTerminal(text, {href, title});
1143: 		FakeSVG.call(this, 'g', {'class': 'non-terminal'});
1144: 		this.text = ""+text;
1145: 		this.href = href;
1146: 		this.title = title;
1147: 		this.width = this.text.length * DiagramNode.CHAR_WIDTH + 20;
1148: 		this.height = 0;
1149: 		this.element_class = element_class;
1150: 		this.on_click = on_click;
1151: 		this.up = 11;
1152: 		this.down = 11;
1153: 		if(DiagramNode.DEBUG) {
1154: 			this.attrs['data-updown'] = this.up + " " + this.height + " " + this.down
1155: 			this.attrs['data-type'] = "nonterminal"
1156: 		}
1157: 	}
1158: 	subclassOf(NonTerminal, FakeSVG);
1159: 	NonTerminal.prototype.needsSpace = true;
1160: 	NonTerminal.prototype.format = function(x, y, width) {
1161: 		// Hook up the two sides if this is narrower than its stated width.
1162: 		var gaps = determineGaps(width, this.width);
1163: 		Path(x,y).h(gaps[0]).addTo(this);
1164: 		Path(x+gaps[0]+this.width,y).h(gaps[1]).addTo(this);
1165: 		x += gaps[0];
1166: 
1167: 		var rect = FakeSVG('rect', {x:x, y:y-11, width:this.width, height:this.up+this.down})
1168: 		rect.attrs['class'] = this.element_class
1169: 		rect.addTo(this);
1170: 		if (this.on_click !== undefined) {
1171: 			rect.attrs['onclick'] = this.on_click;
1172: 		}
1173: 
1174: 		var text = FakeSVG('text', {x:x+this.width/2, y:y+4}, this.text);
1175: 		text.attrs['class'] = this.element_class
1176: 		if (this.on_click !== undefined) {
1177: 			console.log(this.on_click);
1178: 			text.attrs['onclick'] = this.on_click;
1179: 		}
1180: 		if(this.href)
1181: 			FakeSVG('a', {'xlink:href': this.href}, [text]).addTo(this);
1182: 		else
1183: 			text.addTo(this);
1184: 		if(this.title)
1185: 			new FakeSVG('title', {}, this.title).addTo(this);
1186: 		return this;
1187: 	}
1188: 
1189: 	var Comment = funcs.Comment = function Comment(text, {href, title}={}) {
1190: 		if(!(this instanceof Comment)) return new Comment(text, {href, title});
1191: 		FakeSVG.call(this, 'g');
1192: 		this.text = ""+text;
1193: 		this.href = href;
1194: 		this.title = title;
1195: 		this.width = this.text.length * DiagramNode.COMMENT_CHAR_WIDTH + 10;
1196: 		this.height = 0;
1197: 		this.up = 11;
1198: 		this.down = 11;
1199: 		if(DiagramNode.DEBUG) {
1200: 			this.attrs['data-updown'] = this.up + " " + this.height + " " + this.down
1201: 			this.attrs['data-type'] = "comment"
1202: 		}
1203: 	}
1204: 	subclassOf(Comment, FakeSVG);
1205: 	Comment.prototype.needsSpace = true;
1206: 	Comment.prototype.format = function(x, y, width) {
1207: 		// Hook up the two sides if this is narrower than its stated width.
1208: 		var gaps = determineGaps(width, this.width);
1209: 		Path(x,y).h(gaps[0]).addTo(this);
1210: 		Path(x+gaps[0]+this.width,y+this.height).h(gaps[1]).addTo(this);
1211: 		x += gaps[0];
1212: 
1213: 		var text = FakeSVG('text', {x:x+this.width/2, y:y+5, class:'comment'}, this.text);
1214: 		if(this.href)
1215: 			FakeSVG('a', {'xlink:href': this.href}, [text]).addTo(this);
1216: 		else
1217: 			text.addTo(this);
1218: 		if(this.title)
1219: 			new FakeSVG('title', {}, this.title).addTo(this);
1220: 		return this;
1221: 	}
1222: 
1223: 	var Skip = funcs.Skip = function Skip() {
1224: 		if(!(this instanceof Skip)) return new Skip();
1225: 		FakeSVG.call(this, 'g');
1226: 		this.width = 0;
1227: 		this.height = 0;
1228: 		this.up = 0;
1229: 		this.down = 0;
1230: 		if(DiagramNode.DEBUG) {
1231: 			this.attrs['data-updown'] = this.up + " " + this.height + " " + this.down
1232: 			this.attrs['data-type'] = "skip"
1233: 		}
1234: 	}
1235: 	subclassOf(Skip, FakeSVG);
1236: 	Skip.prototype.format = function(x, y, width) {
1237: 		Path(x,y).right(width).addTo(this);
1238: 		return this;
1239: 	}
1240: 
1241: 
1242: 	var Block = funcs.Block = function Block({width=50, up=15, height=25, down=15, needsSpace=true}={}) {
1243: 		if(!(this instanceof Block)) return new Block({width, up, height, down, needsSpace});
1244: 		FakeSVG.call(this, 'g');
1245: 		this.width = width;
1246: 		this.height = height;
1247: 		this.up = up;
1248: 		this.down = down;
1249: 		this.needsSpace = true;
1250: 		if(DiagramNode.DEBUG) {
1251: 			this.attrs['data-updown'] = this.up + " " + this.height + " " + this.down;
1252: 			this.attrs['data-type'] = "block"
1253: 		}
1254: 	}
1255: 	subclassOf(Block, FakeSVG);
1256: 	Block.prototype.format = function(x, y, width) {
1257: 		// Hook up the two sides if this is narrower than its stated width.
1258: 		var gaps = determineGaps(width, this.width);
1259: 		new Path(x,y).h(gaps[0]).addTo(this);
1260: 		new Path(x+gaps[0]+this.width,y).h(gaps[1]).addTo(this);
1261: 		x += gaps[0];
1262: 
1263: 		new FakeSVG('rect', {x:x, y:y-this.up, width:this.width, height:this.up+this.height+this.down}).addTo(this);
1264: 		return this;
1265: 	}
1266: 
1267: 	var root;
1268: 	if (typeof define === 'function' && define.amd) {
1269: 		// AMD. Register as an anonymous module.
1270: 		root = {};
1271: 		define([], function() {
1272: 			return root;
1273: 		});
1274: 	} else if (typeof exports === 'object') {
1275: 		// CommonJS for node
1276: 		root = exports;
1277: 	} else {
1278: 		// Browser globals (root is window)
1279: 		root = this;
1280: 	}
1281: 
1282: 	for(var name in funcs) {
1283: 		root[name] = funcs[name];
1284: 	}
1285: }).call(this,
1286: 	{
1287: 	VERTICAL_SEPARATION: 8,
1288: 	ARC_RADIUS: 10,
1289: 	DIAGRAM_CLASS: 'railroad-diagram',
1290: 	STROKE_ODD_PIXEL_LENGTH: true,
1291: 	INTERNAL_ALIGNMENT: 'left',
1292: 	CHAR_WIDTH: 8.5, // width of each monospace character. play until you find the right value for your font
1293: 	COMMENT_CHAR_WIDTH: 7, // comments are in smaller text by default
1294: 	}
1295: );
1296: 
1297: function Diagram(items) {
1298: 	return new DiagramNode(items);
1299: }
1300: 
1301: function Sequence(items) {
1302: 	return new SequenceNode(items);
1303: }
1304: 
1305: function Stack(items) {
1306: 	return new StackNode(items);
1307: }
1308: 
1309: function Optional(items, skip) {
1310: 	return new OptionalNode(items, skip);
1311: }
1312: 
1313: function OptionalSequence(items) {
1314: 	return new OptionalSequenceNode(items);
1315: }
1316: 
1317: function OneOrMore(items, rep) {
1318: 	return new OneOrMoreNode(items, rep);
1319: }
1320: 
1321: function Choice(normal, items) {
1322: 	return new ChoiceNode(normal, items);
1323: }
1324: 
1325: function Expression(text="expr", cl="expression", on_click=undefined) {
1326: 	return Sequence([new NonTerminal(text, cl, on_click)]);
1327: }
1328: 
1329: function Keyword(text) {
1330: 	return new Terminal(text);
1331: 	// return new Terminal(text)
1332: }
1333: 
1334: function Expandable(text, options, node_name, expand_func) {
1335: 	if (options[node_name]) {
1336: 		var sequence = expand_func(options);
1337: 		sequence.unshift(Expression("-", "expanded", "Refresh(\"" + node_name + "\", false)"));
1338: 		return Sequence(sequence)
1339: 	} else {
1340: 		return Expression(text, "expandable", "Refresh(\"" + node_name + "\", true)");
1341: 	}
1342: }
1343: 
1344: 
1345: // # see https://github.com/tabatkins/railroad-diagrams
1346: 
1347: // literal = Choice(0,
1348: // 	Expression("numeric-literal"),
1349: // 	Expression("string-literal"),
1350: // 	Keyword("NULL"),
1351: // 	Keyword("TRUE"),
1352: // 	Keyword("FALSE"),
1353: // 	Keyword("CURRENT_TIME"),
1354: // 	Keyword("CURRENT_DATE"),
1355: // 	Keyword("CURRENT_TIMESTAMP")
1356: // )
1357: 
1358: // expr = Diagram(
1359: // 	Choice(0,
1360: // 		Sequence(Expression("literal")),
1361: // 		Sequence(Expression("bind-parameter")),
1362: // 		Sequence(
1363: // 			Optional(Sequence(
1364: // 				Expression("table-name"),
1365: // 				Keyword(".")
1366: // 			),	skip=True),
1367: // 			Expression("column-name")
1368: // 		),
1369: // 		Sequence(
1370: // 			Expression("function-name"),
1371: // 			Keyword("("),
1372: // 			Optional(
1373: // 				Sequence(
1374: // 					Optional(Keyword("DISTINCT"), skip=True),
1375: // 					OneOrMore(
1376: // 						Expression(), repeat=","
1377: // 					)
1378: // 				), skip=False
1379: // 			),
1380: // 			Keyword(")"),
1381: // 			Optional(Expression("filter-clause"), skip=True),
1382: // 			Optional(Expression("over-clause"), skip=True)
1383: // 		),
1384: // 		Sequence(
1385: // 			Keyword("("),
1386: // 			Expression("select-node"),
1387: // 			Keyword(")")
1388: // 		),
1389: // 		Sequence(
1390: // 			Keyword("CAST"),
1391: // 			Keyword("("),
1392: // 			Expression(),
1393: // 			Keyword("AS"),
1394: // 			Expression("type-name"),
1395: // 			Keyword(")")
1396: // 		),
1397: // 		Sequence(
1398: // 			Expression(),
1399: // 			Keyword("::"),
1400: // 			Expression("type-name")
1401: // 		),
1402: // 		Sequence(
1403: // 			Expression(),
1404: // 			Keyword("IS"),
1405: // 			Optional(Keyword("NOT"), skip=True),
1406: // 			Keyword("NULL")
1407: // 		),
1408: // 		Sequence(
1409: // 			Expression(),
1410: // 			Optional(Keyword("NOT"), skip=True),
1411: // 			Choice(0,
1412: // 				Keyword("LIKE"),
1413: // 				Sequence(
1414: // 					Keyword("SIMILAR"),
1415: // 					Keyword("TO")
1416: // 				)),
1417: // 			Expression(),
1418: // 			Optional(Sequence(Keyword("ESCAPE"), Expression()), skip=True)
1419: // 		),
1420: // 		Sequence(
1421: // 			Expression(),
1422: // 			Optional(Keyword("NOT"), skip=True),
1423: // 			Keyword("BETWEEN"),
1424: // 			Expression(),
1425: // 			Keyword("AND"),
1426: // 			Expression()
1427: // 		),
1428: // 		Sequence(
1429: // 			Expression(),
1430: // 			Optional(Keyword("NOT"), skip=True),
1431: // 			Keyword("IN"),
1432: // 			Choice(0,
1433: // 				OneOrMore(
1434: // 					Expression(), ","
1435: // 				),
1436: // 				Expression("select-node")
1437: // 			)
1438: // 		),
1439: // 		Sequence(
1440: // 			Optional(Keyword("NOT"), skip=True),
1441: // 			Keyword("EXISTS"),
1442: // 			Expression("select-node")
1443: // 		),
1444: // 		Sequence(
1445: // 			Keyword("CASE"),
1446: // 			Expression(),
1447: // 			OneOrMore(
1448: // 				Sequence(
1449: // 					Keyword("WHEN"),
1450: // 					Expression(),
1451: // 					Keyword("THEN"),
1452: // 					Expression()
1453: // 				)
1454: // 			),
1455: // 			Keyword("ELSE"),
1456: // 			Expression(),
1457: // 			Keyword("END")
1458: // 		)
1459: // 	)
1460: // )
[end of grammar/railroad.js]
[start of src/common/file_system.cpp]
1: #include "duckdb/common/file_system.hpp"
2: 
3: #include "duckdb/common/exception.hpp"
4: #include "duckdb/common/helper.hpp"
5: #include "duckdb/common/string_util.hpp"
6: #include "duckdb/common/checksum.hpp"
7: #include "duckdb/main/client_context.hpp"
8: #include "duckdb/main/database.hpp"
9: 
10: using namespace duckdb;
11: using namespace std;
12: 
13: #include <cstdio>
14: 
15: FileSystem &FileSystem::GetFileSystem(ClientContext &context) {
16: 	return *context.db.config.file_system;
17: }
18: 
19: static void AssertValidFileFlags(uint8_t flags) {
20: 	// cannot combine Read and Write flags
21: 	assert(!(flags & FileFlags::READ && flags & FileFlags::WRITE));
22: 	// cannot combine Read and CREATE/Append flags
23: 	assert(!(flags & FileFlags::READ && flags & FileFlags::APPEND));
24: 	assert(!(flags & FileFlags::READ && flags & FileFlags::FILE_CREATE));
25: 	assert(!(flags & FileFlags::READ && flags & FileFlags::FILE_CREATE_NEW));
26: 	// cannot combine CREATE and CREATE_NEW flags
27: 	assert(!(flags & FileFlags::FILE_CREATE && flags & FileFlags::FILE_CREATE_NEW));
28: }
29: 
30: #ifndef _WIN32
31: #include <dirent.h>
32: #include <fcntl.h>
33: #include <string.h>
34: #include <sys/stat.h>
35: #include <sys/types.h>
36: #include <unistd.h>
37: 
38: // somehow sometimes this is missing
39: #ifndef O_CLOEXEC
40: #define O_CLOEXEC 0
41: #endif
42: 
43: // Solaris
44: #ifndef O_DIRECT
45: #define O_DIRECT 0
46: #endif
47: 
48: struct UnixFileHandle : public FileHandle {
49: public:
50: 	UnixFileHandle(FileSystem &file_system, string path, int fd) : FileHandle(file_system, path), fd(fd) {
51: 	}
52: 	virtual ~UnixFileHandle() {
53: 		Close();
54: 	}
55: 
56: protected:
57: 	void Close() override {
58: 		if (fd != -1) {
59: 			close(fd);
60: 		}
61: 	};
62: 
63: public:
64: 	int fd;
65: };
66: 
67: unique_ptr<FileHandle> FileSystem::OpenFile(const char *path, uint8_t flags, FileLockType lock_type) {
68: 	AssertValidFileFlags(flags);
69: 
70: 	int open_flags = 0;
71: 	int rc;
72: 	if (flags & FileFlags::READ) {
73: 		open_flags = O_RDONLY;
74: 	} else {
75: 		// need Read or Write
76: 		assert(flags & FileFlags::WRITE);
77: 		open_flags = O_RDWR | O_CLOEXEC;
78: 		if (flags & FileFlags::FILE_CREATE) {
79: 			open_flags |= O_CREAT;
80: 		} else if (flags & FileFlags::FILE_CREATE_NEW) {
81: 			open_flags |= O_CREAT | O_TRUNC;
82: 		}
83: 		if (flags & FileFlags::APPEND) {
84: 			open_flags |= O_APPEND;
85: 		}
86: 	}
87: 	if (flags & FileFlags::DIRECT_IO) {
88: #if defined(__sun) && defined(__SVR4)
89: 		throw Exception("DIRECT_IO not supported on Solaris");
90: #endif
91: #if defined(__DARWIN__) || defined(__APPLE__) || defined(__OpenBSD__)
92: 		// OSX does not have O_DIRECT, instead we need to use fcntl afterwards to support direct IO
93: 		open_flags |= O_SYNC;
94: #else
95: 		open_flags |= O_DIRECT | O_SYNC;
96: #endif
97: 	}
98: 	int fd = open(path, open_flags, 0666);
99: 	if (fd == -1) {
100: 		throw IOException("Cannot open file \"%s\": %s", path, strerror(errno));
101: 	}
102: 	// #if defined(__DARWIN__) || defined(__APPLE__)
103: 	// 	if (flags & FileFlags::DIRECT_IO) {
104: 	// 		// OSX requires fcntl for Direct IO
105: 	// 		rc = fcntl(fd, F_NOCACHE, 1);
106: 	// 		if (fd == -1) {
107: 	// 			throw IOException("Could not enable direct IO for file \"%s\": %s", path, strerror(errno));
108: 	// 		}
109: 	// 	}
110: 	// #endif
111: 	if (lock_type != FileLockType::NO_LOCK) {
112: 		// set lock on file
113: 		struct flock fl;
114: 		memset(&fl, 0, sizeof fl);
115: 		fl.l_type = lock_type == FileLockType::READ_LOCK ? F_RDLCK : F_WRLCK;
116: 		fl.l_whence = SEEK_SET;
117: 		fl.l_start = 0;
118: 		fl.l_len = 0;
119: 		rc = fcntl(fd, F_SETLK, &fl);
120: 		if (rc == -1) {
121: 			throw IOException("Could not set lock on file \"%s\": %s", path, strerror(errno));
122: 		}
123: 	}
124: 	return make_unique<UnixFileHandle>(*this, path, fd);
125: }
126: 
127: void FileSystem::SetFilePointer(FileHandle &handle, idx_t location) {
128: 	int fd = ((UnixFileHandle &)handle).fd;
129: 	off_t offset = lseek(fd, location, SEEK_SET);
130: 	if (offset == (off_t)-1) {
131: 		throw IOException("Could not seek to location %lld for file \"%s\": %s", location, handle.path.c_str(),
132: 		                  strerror(errno));
133: 	}
134: }
135: 
136: int64_t FileSystem::Read(FileHandle &handle, void *buffer, int64_t nr_bytes) {
137: 	int fd = ((UnixFileHandle &)handle).fd;
138: 	int64_t bytes_read = read(fd, buffer, nr_bytes);
139: 	if (bytes_read == -1) {
140: 		throw IOException("Could not read from file \"%s\": %s", handle.path.c_str(), strerror(errno));
141: 	}
142: 	return bytes_read;
143: }
144: 
145: int64_t FileSystem::Write(FileHandle &handle, void *buffer, int64_t nr_bytes) {
146: 	int fd = ((UnixFileHandle &)handle).fd;
147: 	int64_t bytes_written = write(fd, buffer, nr_bytes);
148: 	if (bytes_written == -1) {
149: 		throw IOException("Could not write file \"%s\": %s", handle.path.c_str(), strerror(errno));
150: 	}
151: 	return bytes_written;
152: }
153: 
154: int64_t FileSystem::GetFileSize(FileHandle &handle) {
155: 	int fd = ((UnixFileHandle &)handle).fd;
156: 	struct stat s;
157: 	if (fstat(fd, &s) == -1) {
158: 		return -1;
159: 	}
160: 	return s.st_size;
161: }
162: 
163: void FileSystem::Truncate(FileHandle &handle, int64_t new_size) {
164: 	int fd = ((UnixFileHandle &)handle).fd;
165: 	if (ftruncate(fd, new_size) != 0) {
166: 		throw IOException("Could not truncate file \"%s\": %s", handle.path.c_str(), strerror(errno));
167: 	}
168: }
169: 
170: bool FileSystem::DirectoryExists(const string &directory) {
171: 	if (!directory.empty()) {
172: 		if (access(directory.c_str(), 0) == 0) {
173: 			struct stat status;
174: 			stat(directory.c_str(), &status);
175: 			if (status.st_mode & S_IFDIR)
176: 				return true;
177: 		}
178: 	}
179: 	// if any condition fails
180: 	return false;
181: }
182: 
183: bool FileSystem::FileExists(const string &filename) {
184: 	if (!filename.empty()) {
185: 		if (access(filename.c_str(), 0) == 0) {
186: 			struct stat status;
187: 			stat(filename.c_str(), &status);
188: 			if (!(status.st_mode & S_IFDIR))
189: 				return true;
190: 		}
191: 	}
192: 	// if any condition fails
193: 	return false;
194: }
195: 
196: void FileSystem::CreateDirectory(const string &directory) {
197: 	struct stat st;
198: 
199: 	if (stat(directory.c_str(), &st) != 0) {
200: 		/* Directory does not exist. EEXIST for race condition */
201: 		if (mkdir(directory.c_str(), 0755) != 0 && errno != EEXIST) {
202: 			throw IOException("Failed to create directory \"%s\"!", directory.c_str());
203: 		}
204: 	} else if (!S_ISDIR(st.st_mode)) {
205: 		throw IOException("Failed to create directory \"%s\": path exists but is not a directory!", directory.c_str());
206: 	}
207: }
208: 
209: int remove_directory_recursively(const char *path) {
210: 	DIR *d = opendir(path);
211: 	idx_t path_len = (idx_t)strlen(path);
212: 	int r = -1;
213: 
214: 	if (d) {
215: 		struct dirent *p;
216: 		r = 0;
217: 		while (!r && (p = readdir(d))) {
218: 			int r2 = -1;
219: 			char *buf;
220: 			idx_t len;
221: 			/* Skip the names "." and ".." as we don't want to recurse on them. */
222: 			if (!strcmp(p->d_name, ".") || !strcmp(p->d_name, "..")) {
223: 				continue;
224: 			}
225: 			len = path_len + (idx_t)strlen(p->d_name) + 2;
226: 			buf = new char[len];
227: 			if (buf) {
228: 				struct stat statbuf;
229: 				snprintf(buf, len, "%s/%s", path, p->d_name);
230: 				if (!stat(buf, &statbuf)) {
231: 					if (S_ISDIR(statbuf.st_mode)) {
232: 						r2 = remove_directory_recursively(buf);
233: 					} else {
234: 						r2 = unlink(buf);
235: 					}
236: 				}
237: 				delete[] buf;
238: 			}
239: 			r = r2;
240: 		}
241: 		closedir(d);
242: 	}
243: 	if (!r) {
244: 		r = rmdir(path);
245: 	}
246: 	return r;
247: }
248: 
249: void FileSystem::RemoveDirectory(const string &directory) {
250: 	remove_directory_recursively(directory.c_str());
251: }
252: 
253: void FileSystem::RemoveFile(const string &filename) {
254: 	if (std::remove(filename.c_str()) != 0) {
255: 		throw IOException("Could not remove file \"%s\": %s", filename.c_str(), strerror(errno));
256: 	}
257: }
258: 
259: bool FileSystem::ListFiles(const string &directory, function<void(string, bool)> callback) {
260: 	if (!DirectoryExists(directory)) {
261: 		return false;
262: 	}
263: 	DIR *dir = opendir(directory.c_str());
264: 	if (!dir) {
265: 		return false;
266: 	}
267: 	struct dirent *ent;
268: 	// loop over all files in the directory
269: 	while ((ent = readdir(dir)) != NULL) {
270: 		string name = string(ent->d_name);
271: 		// skip . .. and empty files
272: 		if (name.empty() || name == "." || name == "..") {
273: 			continue;
274: 		}
275: 		// now stat the file to figure out if it is a regular file or directory
276: 		string full_path = JoinPath(directory, name);
277: 		if (access(full_path.c_str(), 0) != 0) {
278: 			continue;
279: 		}
280: 		struct stat status;
281: 		stat(full_path.c_str(), &status);
282: 		if (!(status.st_mode & S_IFREG) && !(status.st_mode & S_IFDIR)) {
283: 			// not a file or directory: skip
284: 			continue;
285: 		}
286: 		// invoke callback
287: 		callback(name, status.st_mode & S_IFDIR);
288: 	}
289: 	closedir(dir);
290: 	return true;
291: }
292: 
293: string FileSystem::PathSeparator() {
294: 	return "/";
295: }
296: 
297: void FileSystem::FileSync(FileHandle &handle) {
298: 	int fd = ((UnixFileHandle &)handle).fd;
299: 	if (fsync(fd) != 0) {
300: 		throw FatalException("fsync failed!");
301: 	}
302: }
303: 
304: void FileSystem::MoveFile(const string &source, const string &target) {
305: 	//! FIXME: rename does not guarantee atomicity or overwriting target file if it exists
306: 	if (rename(source.c_str(), target.c_str()) != 0) {
307: 		throw IOException("Could not rename file!");
308: 	}
309: }
310: 
311: #else
312: 
313: #include <string>
314: #ifndef NOMINMAX
315: #define NOMINMAX
316: #endif
317: #include <windows.h>
318: 
319: #undef CreateDirectory
320: #undef MoveFile
321: #undef RemoveDirectory
322: #undef FILE_CREATE // woo mingw
323: 
324: // Returns the last Win32 error, in string format. Returns an empty string if there is no error.
325: std::string GetLastErrorAsString() {
326: 	// Get the error message, if any.
327: 	DWORD errorMessageID = ::GetLastError();
328: 	if (errorMessageID == 0)
329: 		return std::string(); // No error message has been recorded
330: 
331: 	LPSTR messageBuffer = nullptr;
332: 	idx_t size =
333: 	    FormatMessageA(FORMAT_MESSAGE_ALLOCATE_BUFFER | FORMAT_MESSAGE_FROM_SYSTEM | FORMAT_MESSAGE_IGNORE_INSERTS,
334: 	                   NULL, errorMessageID, MAKELANGID(LANG_NEUTRAL, SUBLANG_DEFAULT), (LPSTR)&messageBuffer, 0, NULL);
335: 
336: 	std::string message(messageBuffer, size);
337: 
338: 	// Free the buffer.
339: 	LocalFree(messageBuffer);
340: 
341: 	return message;
342: }
343: 
344: struct WindowsFileHandle : public FileHandle {
345: public:
346: 	WindowsFileHandle(FileSystem &file_system, string path, HANDLE fd) : FileHandle(file_system, path), fd(fd) {
347: 	}
348: 	virtual ~WindowsFileHandle() {
349: 		Close();
350: 	}
351: 
352: protected:
353: 	void Close() override {
354: 		CloseHandle(fd);
355: 	};
356: 
357: public:
358: 	HANDLE fd;
359: };
360: 
361: unique_ptr<FileHandle> FileSystem::OpenFile(const char *path, uint8_t flags, FileLockType lock_type) {
362: 	AssertValidFileFlags(flags);
363: 
364: 	DWORD desired_access;
365: 	DWORD share_mode;
366: 	DWORD creation_disposition = OPEN_EXISTING;
367: 	DWORD flags_and_attributes = FILE_ATTRIBUTE_NORMAL;
368: 	if (flags & FileFlags::READ) {
369: 		desired_access = GENERIC_READ;
370: 		share_mode = FILE_SHARE_READ;
371: 	} else {
372: 		// need Read or Write
373: 		assert(flags & FileFlags::WRITE);
374: 		desired_access = GENERIC_READ | GENERIC_WRITE;
375: 		share_mode = 0;
376: 		if (flags & FileFlags::FILE_CREATE) {
377: 			creation_disposition = OPEN_ALWAYS;
378: 		} else if (flags & FileFlags::FILE_CREATE_NEW) {
379: 			creation_disposition = CREATE_ALWAYS;
380: 		}
381: 		if (flags & FileFlags::DIRECT_IO) {
382: 			flags_and_attributes |= FILE_FLAG_WRITE_THROUGH;
383: 		}
384: 	}
385: 	if (flags & FileFlags::DIRECT_IO) {
386: 		flags_and_attributes |= FILE_FLAG_NO_BUFFERING;
387: 	}
388: 	HANDLE hFile =
389: 	    CreateFileA(path, desired_access, share_mode, NULL, creation_disposition, flags_and_attributes, NULL);
390: 	if (hFile == INVALID_HANDLE_VALUE) {
391: 		auto error = GetLastErrorAsString();
392: 		throw IOException("Cannot open file \"%s\": %s", path, error.c_str());
393: 	}
394: 	auto handle = make_unique<WindowsFileHandle>(*this, path, hFile);
395: 	if (flags & FileFlags::APPEND) {
396: 		SetFilePointer(*handle, GetFileSize(*handle));
397: 	}
398: 	return move(handle);
399: }
400: 
401: void FileSystem::SetFilePointer(FileHandle &handle, idx_t location) {
402: 	HANDLE hFile = ((WindowsFileHandle &)handle).fd;
403: 	LARGE_INTEGER loc;
404: 	loc.QuadPart = location;
405: 	auto rc = SetFilePointerEx(hFile, loc, NULL, FILE_BEGIN);
406: 	if (rc == 0) {
407: 		auto error = GetLastErrorAsString();
408: 		throw IOException("Could not seek to location %lld for file \"%s\": %s", location, handle.path.c_str(),
409: 		                  error.c_str());
410: 	}
411: }
412: 
413: int64_t FileSystem::Read(FileHandle &handle, void *buffer, int64_t nr_bytes) {
414: 	HANDLE hFile = ((WindowsFileHandle &)handle).fd;
415: 	DWORD bytes_read;
416: 	auto rc = ReadFile(hFile, buffer, (DWORD)nr_bytes, &bytes_read, NULL);
417: 	if (rc == 0) {
418: 		auto error = GetLastErrorAsString();
419: 		throw IOException("Could not write file \"%s\": %s", handle.path.c_str(), error.c_str());
420: 	}
421: 	return bytes_read;
422: }
423: 
424: int64_t FileSystem::Write(FileHandle &handle, void *buffer, int64_t nr_bytes) {
425: 	HANDLE hFile = ((WindowsFileHandle &)handle).fd;
426: 	DWORD bytes_read;
427: 	auto rc = WriteFile(hFile, buffer, (DWORD)nr_bytes, &bytes_read, NULL);
428: 	if (rc == 0) {
429: 		auto error = GetLastErrorAsString();
430: 		throw IOException("Could not write file \"%s\": %s", handle.path.c_str(), error.c_str());
431: 	}
432: 	return bytes_read;
433: }
434: 
435: int64_t FileSystem::GetFileSize(FileHandle &handle) {
436: 	HANDLE hFile = ((WindowsFileHandle &)handle).fd;
437: 	LARGE_INTEGER result;
438: 	if (!GetFileSizeEx(hFile, &result)) {
439: 		return -1;
440: 	}
441: 	return result.QuadPart;
442: }
443: 
444: void FileSystem::Truncate(FileHandle &handle, int64_t new_size) {
445: 	HANDLE hFile = ((WindowsFileHandle &)handle).fd;
446: 	// seek to the location
447: 	SetFilePointer(handle, new_size);
448: 	// now set the end of file position
449: 	if (!SetEndOfFile(hFile)) {
450: 		auto error = GetLastErrorAsString();
451: 		throw IOException("Failure in SetEndOfFile call on file \"%s\": %s", handle.path.c_str(), error.c_str());
452: 	}
453: }
454: 
455: bool FileSystem::DirectoryExists(const string &directory) {
456: 	DWORD attrs = GetFileAttributesA(directory.c_str());
457: 	return (attrs != INVALID_FILE_ATTRIBUTES && (attrs & FILE_ATTRIBUTE_DIRECTORY));
458: }
459: 
460: bool FileSystem::FileExists(const string &filename) {
461: 	DWORD attrs = GetFileAttributesA(filename.c_str());
462: 	return (attrs != INVALID_FILE_ATTRIBUTES && !(attrs & FILE_ATTRIBUTE_DIRECTORY));
463: }
464: 
465: void FileSystem::CreateDirectory(const string &directory) {
466: 	if (DirectoryExists(directory)) {
467: 		return;
468: 	}
469: 	if (directory.empty() || !CreateDirectoryA(directory.c_str(), NULL) || !DirectoryExists(directory)) {
470: 		throw IOException("Could not create directory!");
471: 	}
472: }
473: 
474: static void delete_dir_special_snowflake_windows(string directory) {
475: 	if (directory.size() + 3 > MAX_PATH) {
476: 		throw IOException("Pathname too long");
477: 	}
478: 	// create search pattern
479: 	TCHAR szDir[MAX_PATH];
480: 	snprintf(szDir, MAX_PATH, "%s\\*", directory.c_str());
481: 
482: 	WIN32_FIND_DATA ffd;
483: 	HANDLE hFind = FindFirstFile(szDir, &ffd);
484: 	if (hFind == INVALID_HANDLE_VALUE) {
485: 		return;
486: 	}
487: 
488: 	do {
489: 		if (string(ffd.cFileName) == "." || string(ffd.cFileName) == "..") {
490: 			continue;
491: 		}
492: 		if (ffd.dwFileAttributes & FILE_ATTRIBUTE_DIRECTORY) {
493: 			// recurse to zap directory contents
494: 			FileSystem fs;
495: 			delete_dir_special_snowflake_windows(fs.JoinPath(directory, ffd.cFileName));
496: 		} else {
497: 			if (strlen(ffd.cFileName) + directory.size() + 1 > MAX_PATH) {
498: 				throw IOException("Pathname too long");
499: 			}
500: 			// create search pattern
501: 			TCHAR del_path[MAX_PATH];
502: 			snprintf(del_path, MAX_PATH, "%s\\%s", directory.c_str(), ffd.cFileName);
503: 			if (!DeleteFileA(del_path)) {
504: 				throw IOException("Failed to delete directory entry");
505: 			}
506: 		}
507: 	} while (FindNextFile(hFind, &ffd) != 0);
508: 
509: 	DWORD dwError = GetLastError();
510: 	if (dwError != ERROR_NO_MORE_FILES) {
511: 		throw IOException("Something went wrong");
512: 	}
513: 	FindClose(hFind);
514: 
515: 	if (!RemoveDirectoryA(directory.c_str())) {
516: 		throw IOException("Failed to delete directory");
517: 	}
518: }
519: 
520: void FileSystem::RemoveDirectory(const string &directory) {
521: 	delete_dir_special_snowflake_windows(directory.c_str());
522: }
523: 
524: void FileSystem::RemoveFile(const string &filename) {
525: 	DeleteFileA(filename.c_str());
526: }
527: 
528: bool FileSystem::ListFiles(const string &directory, function<void(string, bool)> callback) {
529: 	string search_dir = JoinPath(directory, "*");
530: 
531: 	WIN32_FIND_DATA ffd;
532: 	HANDLE hFind = FindFirstFile(search_dir.c_str(), &ffd);
533: 	if (hFind == INVALID_HANDLE_VALUE) {
534: 		return false;
535: 	}
536: 	do {
537: 		string cFileName = string(ffd.cFileName);
538: 		if (cFileName == "." || cFileName == "..") {
539: 			continue;
540: 		}
541: 		callback(cFileName, ffd.dwFileAttributes & FILE_ATTRIBUTE_DIRECTORY);
542: 	} while (FindNextFile(hFind, &ffd) != 0);
543: 
544: 	DWORD dwError = GetLastError();
545: 	if (dwError != ERROR_NO_MORE_FILES) {
546: 		FindClose(hFind);
547: 		return false;
548: 	}
549: 
550: 	FindClose(hFind);
551: 	return true;
552: }
553: 
554: string FileSystem::PathSeparator() {
555: 	return "\\";
556: }
557: 
558: void FileSystem::FileSync(FileHandle &handle) {
559: 	HANDLE hFile = ((WindowsFileHandle &)handle).fd;
560: 	if (FlushFileBuffers(hFile) == 0) {
561: 		throw IOException("Could not flush file handle to disk!");
562: 	}
563: }
564: 
565: void FileSystem::MoveFile(const string &source, const string &target) {
566: 	if (!MoveFileA(source.c_str(), target.c_str())) {
567: 		throw IOException("Could not move file");
568: 	}
569: }
570: #endif
571: 
572: void FileSystem::Read(FileHandle &handle, void *buffer, int64_t nr_bytes, idx_t location) {
573: 	// seek to the location
574: 	SetFilePointer(handle, location);
575: 	// now read from the location
576: 	int64_t bytes_read = Read(handle, buffer, nr_bytes);
577: 	if (bytes_read != nr_bytes) {
578: 		throw IOException("Could not read sufficient bytes from file \"%s\"", handle.path.c_str());
579: 	}
580: }
581: 
582: void FileSystem::Write(FileHandle &handle, void *buffer, int64_t nr_bytes, idx_t location) {
583: 	// seek to the location
584: 	SetFilePointer(handle, location);
585: 	// now write to the location
586: 	int64_t bytes_written = Write(handle, buffer, nr_bytes);
587: 	if (bytes_written != nr_bytes) {
588: 		throw IOException("Could not write sufficient bytes from file \"%s\"", handle.path.c_str());
589: 	}
590: }
591: 
592: string FileSystem::JoinPath(const string &a, const string &b) {
593: 	// FIXME: sanitize paths
594: 	return a + PathSeparator() + b;
595: }
596: 
597: void FileHandle::Read(void *buffer, idx_t nr_bytes, idx_t location) {
598: 	file_system.Read(*this, buffer, nr_bytes, location);
599: }
600: 
601: void FileHandle::Write(void *buffer, idx_t nr_bytes, idx_t location) {
602: 	file_system.Write(*this, buffer, nr_bytes, location);
603: }
604: 
605: void FileHandle::Sync() {
606: 	file_system.FileSync(*this);
607: }
608: 
609: void FileHandle::Truncate(int64_t new_size) {
610: 	file_system.Truncate(*this, new_size);
611: }
[end of src/common/file_system.cpp]
[start of src/common/operator/cast_operators.cpp]
1: #include "duckdb/common/operator/cast_operators.hpp"
2: 
3: #include "duckdb/common/exception.hpp"
4: #include "duckdb/common/limits.hpp"
5: #include "duckdb/common/types/date.hpp"
6: #include "duckdb/common/types/hugeint.hpp"
7: #include "duckdb/common/types/interval.hpp"
8: #include "duckdb/common/types/time.hpp"
9: #include "duckdb/common/types/timestamp.hpp"
10: #include "duckdb/common/types/vector.hpp"
11: #include "fmt/format.h"
12: 
13: #include <cctype>
14: #include <cmath>
15: #include <cstdlib>
16: 
17: using namespace std;
18: 
19: namespace duckdb {
20: 
21: template <class SRC, class DST> static bool try_cast_with_overflow_check(SRC value, DST &result) {
22: 	if (value < NumericLimits<DST>::Minimum() || value > NumericLimits<DST>::Maximum()) {
23: 		return false;
24: 	}
25: 	result = (DST)value;
26: 	return true;
27: }
28: 
29: template <class SRC, class DST> static DST cast_with_overflow_check(SRC value) {
30: 	DST result;
31: 	if (!try_cast_with_overflow_check<SRC, DST>(value, result)) {
32: 		throw ValueOutOfRangeException((int64_t)value, GetTypeId<SRC>(), GetTypeId<DST>());
33: 	}
34: 	return result;
35: }
36: 
37: //===--------------------------------------------------------------------===//
38: // Numeric -> int8_t casts
39: //===--------------------------------------------------------------------===//
40: template <> bool TryCast::Operation(int16_t input, int8_t &result, bool strict) {
41: 	return try_cast_with_overflow_check(input, result);
42: }
43: template <> bool TryCast::Operation(int32_t input, int8_t &result, bool strict) {
44: 	return try_cast_with_overflow_check(input, result);
45: }
46: template <> bool TryCast::Operation(int64_t input, int8_t &result, bool strict) {
47: 	return try_cast_with_overflow_check(input, result);
48: }
49: template <> bool TryCast::Operation(float input, int8_t &result, bool strict) {
50: 	return try_cast_with_overflow_check(input, result);
51: }
52: template <> bool TryCast::Operation(double input, int8_t &result, bool strict) {
53: 	return try_cast_with_overflow_check(input, result);
54: }
55: 
56: template <> int8_t Cast::Operation(int16_t input) {
57: 	return cast_with_overflow_check<int16_t, int8_t>(input);
58: }
59: template <> int8_t Cast::Operation(int32_t input) {
60: 	return cast_with_overflow_check<int32_t, int8_t>(input);
61: }
62: template <> int8_t Cast::Operation(int64_t input) {
63: 	return cast_with_overflow_check<int64_t, int8_t>(input);
64: }
65: template <> int8_t Cast::Operation(float input) {
66: 	return cast_with_overflow_check<float, int8_t>(input);
67: }
68: template <> int8_t Cast::Operation(double input) {
69: 	return cast_with_overflow_check<double, int8_t>(input);
70: }
71: //===--------------------------------------------------------------------===//
72: // Numeric -> int16_t casts
73: //===--------------------------------------------------------------------===//
74: template <> bool TryCast::Operation(int32_t input, int16_t &result, bool strict) {
75: 	return try_cast_with_overflow_check(input, result);
76: }
77: template <> bool TryCast::Operation(int64_t input, int16_t &result, bool strict) {
78: 	return try_cast_with_overflow_check(input, result);
79: }
80: template <> bool TryCast::Operation(float input, int16_t &result, bool strict) {
81: 	return try_cast_with_overflow_check(input, result);
82: }
83: template <> bool TryCast::Operation(double input, int16_t &result, bool strict) {
84: 	return try_cast_with_overflow_check(input, result);
85: }
86: 
87: template <> int16_t Cast::Operation(int32_t input) {
88: 	return cast_with_overflow_check<int32_t, int16_t>(input);
89: }
90: template <> int16_t Cast::Operation(int64_t input) {
91: 	return cast_with_overflow_check<int64_t, int16_t>(input);
92: }
93: template <> int16_t Cast::Operation(float input) {
94: 	return cast_with_overflow_check<float, int16_t>(input);
95: }
96: template <> int16_t Cast::Operation(double input) {
97: 	return cast_with_overflow_check<double, int16_t>(input);
98: }
99: //===--------------------------------------------------------------------===//
100: // Numeric -> int32_t casts
101: //===--------------------------------------------------------------------===//
102: template <> bool TryCast::Operation(int64_t input, int32_t &result, bool strict) {
103: 	return try_cast_with_overflow_check(input, result);
104: }
105: template <> bool TryCast::Operation(float input, int32_t &result, bool strict) {
106: 	return try_cast_with_overflow_check(input, result);
107: }
108: template <> bool TryCast::Operation(double input, int32_t &result, bool strict) {
109: 	return try_cast_with_overflow_check(input, result);
110: }
111: 
112: template <> int32_t Cast::Operation(int64_t input) {
113: 	return cast_with_overflow_check<int64_t, int32_t>(input);
114: }
115: template <> int32_t Cast::Operation(float input) {
116: 	return cast_with_overflow_check<float, int32_t>(input);
117: }
118: template <> int32_t Cast::Operation(double input) {
119: 	return cast_with_overflow_check<double, int32_t>(input);
120: }
121: //===--------------------------------------------------------------------===//
122: // Numeric -> int64_t casts
123: //===--------------------------------------------------------------------===//
124: template <> bool TryCast::Operation(float input, int64_t &result, bool strict) {
125: 	return try_cast_with_overflow_check(input, result);
126: }
127: template <> bool TryCast::Operation(double input, int64_t &result, bool strict) {
128: 	return try_cast_with_overflow_check(input, result);
129: }
130: 
131: template <> int64_t Cast::Operation(float input) {
132: 	return cast_with_overflow_check<float, int64_t>(input);
133: }
134: template <> int64_t Cast::Operation(double input) {
135: 	return cast_with_overflow_check<double, int64_t>(input);
136: }
137: 
138: //===--------------------------------------------------------------------===//
139: // Double -> float casts
140: //===--------------------------------------------------------------------===//
141: template <> bool TryCast::Operation(double input, float &result, bool strict) {
142: 	auto res = (float)input;
143: 	if (std::isnan(res) || std::isinf(res)) {
144: 		return false;
145: 	}
146: 	result = res;
147: 	return true;
148: }
149: 
150: template <> float Cast::Operation(double input) {
151: 	float result;
152: 	bool strict = false;
153: 	if (!TryCast::Operation(input, result, strict)) {
154: 		throw ValueOutOfRangeException(input, GetTypeId<double>(), GetTypeId<float>());
155: 	}
156: 	return result;
157: }
158: 
159: //===--------------------------------------------------------------------===//
160: // Cast String -> Numeric
161: //===--------------------------------------------------------------------===//
162: template <class T> static T try_cast_string(string_t input) {
163: 	T result;
164: 	if (!TryCast::Operation<string_t, T>(input, result)) {
165: 		throw ConversionException("Could not convert string '%s' to %s", input.GetData(), TypeIdToString(GetTypeId<T>()).c_str());
166: 	}
167: 	return result;
168: }
169: 
170: 
171: 
172: template <class T> static T try_strict_cast_string(string_t input) {
173: 	T result;
174: 	if (!TryCast::Operation<string_t, T>(input, result, true)) {
175: 		throw ConversionException("Could not convert string '%s' to %s", input.GetData(), TypeIdToString(GetTypeId<T>()).c_str());
176: 	}
177: 	return result;
178: }
179: 
180: struct IntegerCastOperation {
181: 	template<class T, bool NEGATIVE>
182: 	static bool HandleDigit(T &result, uint8_t digit) {
183: 		if (NEGATIVE) {
184: 			if (result < (NumericLimits<T>::Minimum() + digit) / 10) {
185: 				return false;
186: 			}
187: 			result = result * 10 - digit;
188: 		} else {
189: 			if (result > (NumericLimits<T>::Maximum() - digit) / 10) {
190: 				return false;
191: 			}
192: 			result = result * 10 + digit;
193: 		}
194: 		return true;
195: 	}
196: 
197: 	template<class T>
198: 	static bool HandleExponent(T &result, int64_t exponent) {
199: 		double dbl_res = result * pow(10, exponent);
200: 		if (dbl_res < NumericLimits<T>::Minimum() || dbl_res > NumericLimits<T>::Maximum()) {
201: 			return false;
202: 		}
203: 		result = (T)dbl_res;
204: 		return true;
205: 	}
206: 
207: 	template<class T>
208: 	static bool Finalize(T &result) {
209: 		return true;
210: 	}
211: };
212: 
213: template <class T, bool NEGATIVE, bool ALLOW_EXPONENT, class OP=IntegerCastOperation>
214: static bool IntegerCastLoop(const char *buf, idx_t len, T &result, bool strict) {
215: 	idx_t start_pos = NEGATIVE || *buf == '+' ? 1 : 0;
216: 	idx_t pos = start_pos;
217: 	while(pos < len) {
218: 		if (!std::isdigit((unsigned char)buf[pos])) {
219: 			// not a digit!
220: 			if (buf[pos] == '.') {
221: 				if (strict) {
222: 					return false;
223: 				}
224: 				bool number_before_period = pos > start_pos;
225: 				if (!OP::template Finalize<T>(result)) {
226: 					return false;
227: 				}
228: 				// decimal point: we accept decimal values for integers as well
229: 				// we just truncate them
230: 				// make sure everything after the period is a number
231: 				pos++;
232: 				idx_t start_digit = pos;
233: 				while(pos < len) {
234: 					if (!std::isdigit((unsigned char)buf[pos++])) {
235: 						return false;
236: 					}
237: 				}
238: 				// make sure there is either (1) one number after the period, or (2) one number before the period
239: 				// i.e. we accept "1." and ".1" as valid numbers, but not "."
240: 				return number_before_period || pos > start_digit;
241: 			}
242: 			if (std::isspace((unsigned char)buf[pos])) {
243: 				// skip any trailing spaces
244: 				while(++pos < len) {
245: 					if (!std::isspace((unsigned char)buf[pos])) {
246: 						return false;
247: 					}
248: 				}
249: 				break;
250: 			}
251: 			if (ALLOW_EXPONENT) {
252: 				if (buf[pos] == 'e' || buf[pos] == 'E') {
253: 					pos++;
254: 					int64_t exponent = 0;
255: 					int negative = buf[pos] == '-';
256: 					if (negative) {
257: 						if (!IntegerCastLoop<int64_t, true, false>(buf + pos, len - pos, exponent, strict)) {
258: 							return false;
259: 						}
260: 					} else {
261: 						if (!IntegerCastLoop<int64_t, false, false>(buf + pos, len - pos, exponent, strict)) {
262: 							return false;
263: 						}
264: 					}
265: 					return OP::template HandleExponent<T>(result, exponent);
266: 				}
267: 			}
268: 			return false;
269: 		}
270: 		uint8_t digit = buf[pos++] - '0';
271: 		if (!OP::template HandleDigit<T, NEGATIVE>(result, digit)) {
272: 			return false;
273: 		}
274: 	}
275: 	if (!OP::template Finalize<T>(result)) {
276: 		return false;
277: 	}
278: 	return pos > start_pos;
279: }
280: 
281: template <class T, bool ALLOW_EXPONENT = true, class OP=IntegerCastOperation> static bool TryIntegerCast(const char *buf, idx_t len, T &result, bool strict) {
282: 	// skip any spaces at the start
283: 	while(len > 0 && std::isspace(*buf)) {
284: 		buf++;
285: 		len--;
286: 	}
287: 	if (len == 0) {
288: 		return false;
289: 	}
290: 	int negative = *buf == '-';
291: 
292: 	memset(&result, 0, sizeof(T));
293: 	if (!negative) {
294: 		return IntegerCastLoop<T, false, ALLOW_EXPONENT, OP>(buf, len, result, strict);
295: 	} else {
296: 		return IntegerCastLoop<T, true, ALLOW_EXPONENT, OP>(buf, len, result, strict);
297: 	}
298: }
299: 
300: template <> bool TryCast::Operation(string_t input, bool &result, bool strict) {
301: 	auto input_data = input.GetData();
302: 	auto input_size = input.GetSize();
303: 
304: 	switch(input_size) {
305: 	case 1: {
306: 		char c = std::tolower(*input_data);
307: 		if (c == 't' || (!strict && c == '1')) {
308: 			result = true;
309: 			return true;
310: 		} else if (c == 'f' || (!strict && c == '0')) {
311: 			result = false;
312: 			return true;
313: 		}
314: 		return false;
315: 	}
316: 	case 4: {
317: 		char t = std::tolower(input_data[0]);
318: 		char r = std::tolower(input_data[1]);
319: 		char u = std::tolower(input_data[2]);
320: 		char e = std::tolower(input_data[3]);
321: 		if (t == 't' && r == 'r' && u == 'u' && e == 'e') {
322: 			result = true;
323: 			return true;
324: 		}
325: 		return false;
326: 	}
327: 	case 5: {
328: 		char f = std::tolower(input_data[0]);
329: 		char a = std::tolower(input_data[1]);
330: 		char l = std::tolower(input_data[2]);
331: 		char s = std::tolower(input_data[3]);
332: 		char e = std::tolower(input_data[4]);
333: 		if (f == 'f' && a == 'a' && l == 'l' && s == 's' && e == 'e') {
334: 			result = false;
335: 			return true;
336: 		}
337: 		return false;
338: 	}
339: 	default:
340: 		return false;
341: 	}
342: }
343: template <> bool TryCast::Operation(string_t input, int8_t &result, bool strict) {
344: 	return TryIntegerCast<int8_t>(input.GetData(), input.GetSize(), result, strict);
345: }
346: template <> bool TryCast::Operation(string_t input, int16_t &result, bool strict) {
347: 	return TryIntegerCast<int16_t>(input.GetData(), input.GetSize(), result, strict);
348: }
349: template <> bool TryCast::Operation(string_t input, int32_t &result, bool strict) {
350: 	return TryIntegerCast<int32_t>(input.GetData(), input.GetSize(), result, strict);
351: }
352: template <> bool TryCast::Operation(string_t input, int64_t &result, bool strict) {
353: 	return TryIntegerCast<int64_t>(input.GetData(), input.GetSize(), result, strict);
354: }
355: 
356: template <class T, bool NEGATIVE> static void ComputeDoubleResult(T &result, idx_t decimal, idx_t decimal_factor) {
357: 	if (decimal_factor > 1) {
358: 		if (NEGATIVE) {
359: 			result -= (T)decimal / (T)decimal_factor;
360: 		} else {
361: 			result += (T)decimal / (T)decimal_factor;
362: 		}
363: 	}
364: }
365: 
366: template <class T, bool NEGATIVE> static bool DoubleCastLoop(const char *buf, idx_t len, T &result, bool strict) {
367: 	idx_t start_pos = NEGATIVE || *buf == '+' ? 1 : 0;
368: 	idx_t pos = start_pos;
369: 	idx_t decimal = 0;
370: 	idx_t decimal_factor = 0;
371: 	while (pos < len) {
372: 		if (!std::isdigit((unsigned char)buf[pos])) {
373: 			// not a digit!
374: 			if (buf[pos] == '.') {
375: 				// decimal point
376: 				if (decimal_factor != 0) {
377: 					// nested periods
378: 					return false;
379: 				}
380: 				decimal_factor = 1;
381: 				pos++;
382: 				continue;
383: 			} else if (std::isspace((unsigned char)buf[pos])) {
384: 				// skip any trailing spaces
385: 				while (++pos < len) {
386: 					if (!std::isspace((unsigned char)buf[pos])) {
387: 						return false;
388: 					}
389: 				}
390: 				ComputeDoubleResult<T, NEGATIVE>(result, decimal, decimal_factor);
391: 				return true;
392: 			} else if (buf[pos] == 'e' || buf[pos] == 'E') {
393: 				// E power
394: 				// parse an integer, this time not allowing another exponent
395: 				pos++;
396: 				int64_t exponent;
397: 				if (!TryIntegerCast<int64_t, false>(buf + pos, len - pos, exponent, strict)) {
398: 					return false;
399: 				}
400: 				ComputeDoubleResult<T, NEGATIVE>(result, decimal, decimal_factor);
401: 				result = result * pow(10, exponent);
402: 				return true;
403: 			} else {
404: 				return false;
405: 			}
406: 		}
407: 		T digit = buf[pos++] - '0';
408: 		if (decimal_factor == 0) {
409: 			result = result * 10 + (NEGATIVE ? -digit : digit);
410: 		} else {
411: 			if (decimal_factor >= 1000000000000000000) {
412: 				// decimal value will overflow if we parse more, ignore any subsequent numbers
413: 				continue;
414: 			}
415: 			decimal = decimal * 10 + digit;
416: 			decimal_factor *= 10;
417: 		}
418: 	}
419: 	ComputeDoubleResult<T, NEGATIVE>(result, decimal, decimal_factor);
420: 	return pos > start_pos;
421: }
422: 
423: template <class T> bool CheckDoubleValidity(T value);
424: 
425: template <> bool CheckDoubleValidity(float value) {
426: 	return Value::FloatIsValid(value);
427: }
428: 
429: template <> bool CheckDoubleValidity(double value) {
430: 	return Value::DoubleIsValid(value);
431: }
432: 
433: template <class T> static bool TryDoubleCast(const char *buf, idx_t len, T &result, bool strict) {
434: 	// skip any spaces at the start
435: 	while(len > 0 && std::isspace(*buf)) {
436: 		buf++;
437: 		len--;
438: 	}
439: 	if (len == 0) {
440: 		return false;
441: 	}
442: 	int negative = *buf == '-';
443: 
444: 	result = 0;
445: 	if (!negative) {
446: 		if (!DoubleCastLoop<T, false>(buf, len, result, strict)) {
447: 			return false;
448: 		}
449: 	} else {
450: 		if (!DoubleCastLoop<T, true>(buf, len, result, strict)) {
451: 			return false;
452: 		}
453: 	}
454: 	if (!CheckDoubleValidity<T>(result)) {
455: 		return false;
456: 	}
457: 	return true;
458: }
459: 
460: template <> bool TryCast::Operation(string_t input, float &result, bool strict) {
461: 	return TryDoubleCast<float>(input.GetData(), input.GetSize(), result, strict);
462: }
463: template <> bool TryCast::Operation(string_t input, double &result, bool strict) {
464: 	return TryDoubleCast<double>(input.GetData(), input.GetSize(), result, strict);
465: }
466: 
467: template <> bool Cast::Operation(string_t input) {
468: 	return try_cast_string<bool>(input);
469: }
470: template <> int8_t Cast::Operation(string_t input) {
471: 	return try_cast_string<int8_t>(input);
472: }
473: template <> int16_t Cast::Operation(string_t input) {
474: 	return try_cast_string<int16_t>(input);
475: }
476: template <> int32_t Cast::Operation(string_t input) {
477: 	return try_cast_string<int32_t>(input);
478: }
479: template <> int64_t Cast::Operation(string_t input) {
480: 	return try_cast_string<int64_t>(input);
481: }
482: template <> float Cast::Operation(string_t input) {
483: 	return try_cast_string<float>(input);
484: }
485: template <> double Cast::Operation(string_t input) {
486: 	return try_cast_string<double>(input);
487: }
488: 
489: template <> bool StrictCast::Operation(string_t input) {
490: 	return try_strict_cast_string<bool>(input);
491: }
492: template <> int8_t StrictCast::Operation(string_t input) {
493: 	return try_strict_cast_string<int8_t>(input);
494: }
495: template <> int16_t StrictCast::Operation(string_t input) {
496: 	return try_strict_cast_string<int16_t>(input);
497: }
498: template <> int32_t StrictCast::Operation(string_t input) {
499: 	return try_strict_cast_string<int32_t>(input);
500: }
501: template <> int64_t StrictCast::Operation(string_t input) {
502: 	return try_strict_cast_string<int64_t>(input);
503: }
504: template <> float StrictCast::Operation(string_t input) {
505: 	return try_strict_cast_string<float>(input);
506: }
507: template <> double StrictCast::Operation(string_t input) {
508: 	return try_strict_cast_string<double>(input);
509: }
510: 
511: //===--------------------------------------------------------------------===//
512: // Cast Numeric -> String
513: //===--------------------------------------------------------------------===//
514: template <class T> string CastToStandardString(T input) {
515: 	Vector v(TypeId::VARCHAR);
516: 	return StringCast::Operation(input, v).GetString();
517: }
518: 
519: template <> string Cast::Operation(bool input) {
520: 	return CastToStandardString(input);
521: }
522: template <> string Cast::Operation(int8_t input) {
523: 	return CastToStandardString(input);
524: }
525: template <> string Cast::Operation(int16_t input) {
526: 	return CastToStandardString(input);
527: }
528: template <> string Cast::Operation(int32_t input) {
529: 	return CastToStandardString(input);
530: }
531: template <> string Cast::Operation(int64_t input) {
532: 	return CastToStandardString(input);
533: }
534: template <> string Cast::Operation(hugeint_t input) {
535: 	return Hugeint::ToString(input);
536: }
537: template <> string Cast::Operation(float input) {
538: 	return CastToStandardString(input);
539: }
540: template <> string Cast::Operation(double input) {
541: 	return CastToStandardString(input);
542: }
543: template <> string Cast::Operation(string_t input) {
544: 	return input.GetString();
545: }
546: 
547: template <> string_t StringCast::Operation(bool input, Vector &vector) {
548: 	if (input) {
549: 		return StringVector::AddString(vector, "true", 4);
550: 	} else {
551: 		return StringVector::AddString(vector, "false", 5);
552: 	}
553: }
554: 
555: struct StringToIntegerCast {
556: 	template <class T> static int UnsignedLength(T value);
557: 
558: 	// Formats value in reverse and returns a pointer to the beginning.
559: 	template <class T> static char *FormatUnsigned(T value, char *ptr) {
560: 		while (value >= 100) {
561: 			// Integer division is slow so do it for a group of two digits instead
562: 			// of for every digit. The idea comes from the talk by Alexandrescu
563: 			// "Three Optimization Tips for C++". See speed-test for a comparison.
564: 			auto index = static_cast<unsigned>((value % 100) * 2);
565: 			value /= 100;
566: 			*--ptr = duckdb_fmt::internal::data::digits[index + 1];
567: 			*--ptr = duckdb_fmt::internal::data::digits[index];
568: 		}
569: 		if (value < 10) {
570: 			*--ptr = static_cast<char>('0' + value);
571: 			return ptr;
572: 		}
573: 		auto index = static_cast<unsigned>(value * 2);
574: 		*--ptr = duckdb_fmt::internal::data::digits[index + 1];
575: 		*--ptr = duckdb_fmt::internal::data::digits[index];
576: 		return ptr;
577: 	}
578: 
579: 	template <class SIGNED, class UNSIGNED> static string_t FormatSigned(SIGNED value, Vector &vector) {
580: 		int sign = -(value < 0);
581: 		UNSIGNED unsigned_value = (value ^ sign) - sign;
582: 		int length = UnsignedLength<UNSIGNED>(unsigned_value) - sign;
583: 		string_t result = StringVector::EmptyString(vector, length);
584: 		auto dataptr = result.GetData();
585: 		auto endptr = dataptr + length;
586: 		endptr = FormatUnsigned(unsigned_value, endptr);
587: 		if (sign) {
588: 			*--endptr = '-';
589: 		}
590: 		result.Finalize();
591: 		return result;
592: 	}
593: };
594: 
595: template <> int StringToIntegerCast::UnsignedLength(uint8_t value) {
596: 	int length = 1;
597: 	length += value >= 10;
598: 	length += value >= 100;
599: 	return length;
600: }
601: 
602: template <> int StringToIntegerCast::UnsignedLength(uint16_t value) {
603: 	int length = 1;
604: 	length += value >= 10;
605: 	length += value >= 100;
606: 	length += value >= 1000;
607: 	length += value >= 10000;
608: 	return length;
609: }
610: 
611: template <> int StringToIntegerCast::UnsignedLength(uint32_t value) {
612: 	if (value >= 10000) {
613: 		int length = 5;
614: 		length += value >= 100000;
615: 		length += value >= 1000000;
616: 		length += value >= 10000000;
617: 		length += value >= 100000000;
618: 		length += value >= 1000000000;
619: 		return length;
620: 	} else {
621: 		int length = 1;
622: 		length += value >= 10;
623: 		length += value >= 100;
624: 		length += value >= 1000;
625: 		return length;
626: 	}
627: }
628: 
629: template <> int StringToIntegerCast::UnsignedLength(uint64_t value) {
630: 	if (value >= 10000000000ULL) {
631: 		if (value >= 1000000000000000ULL) {
632: 			int length = 16;
633: 			length += value >= 10000000000000000ULL;
634: 			length += value >= 100000000000000000ULL;
635: 			length += value >= 1000000000000000000ULL;
636: 			length += value >= 10000000000000000000ULL;
637: 			return length;
638: 		} else {
639: 			int length = 11;
640: 			length += value >= 100000000000ULL;
641: 			length += value >= 1000000000000ULL;
642: 			length += value >= 10000000000000ULL;
643: 			length += value >= 100000000000000ULL;
644: 			return length;
645: 		}
646: 	} else {
647: 		if (value >= 100000ULL) {
648: 			int length = 6;
649: 			length += value >= 1000000ULL;
650: 			length += value >= 10000000ULL;
651: 			length += value >= 100000000ULL;
652: 			length += value >= 1000000000ULL;
653: 			return length;
654: 		} else {
655: 			int length = 1;
656: 			length += value >= 10ULL;
657: 			length += value >= 100ULL;
658: 			length += value >= 1000ULL;
659: 			length += value >= 10000ULL;
660: 			return length;
661: 		}
662: 	}
663: }
664: 
665: template <> string_t StringCast::Operation(int8_t input, Vector &vector) {
666: 	return StringToIntegerCast::FormatSigned<int8_t, uint8_t>(input, vector);
667: }
668: 
669: template <> string_t StringCast::Operation(int16_t input, Vector &vector) {
670: 	return StringToIntegerCast::FormatSigned<int16_t, uint16_t>(input, vector);
671: }
672: template <> string_t StringCast::Operation(int32_t input, Vector &vector) {
673: 	return StringToIntegerCast::FormatSigned<int32_t, uint32_t>(input, vector);
674: }
675: 
676: template <> string_t StringCast::Operation(int64_t input, Vector &vector) {
677: 	return StringToIntegerCast::FormatSigned<int64_t, uint64_t>(input, vector);
678: }
679: 
680: template <> string_t StringCast::Operation(float input, Vector &vector) {
681: 	std::string s = duckdb_fmt::format("{}", input);
682: 	return StringVector::AddString(vector, s);
683: }
684: 
685: template <> string_t StringCast::Operation(double input, Vector &vector) {
686: 	std::string s = duckdb_fmt::format("{}", input);
687: 	return StringVector::AddString(vector, s);
688: }
689: 
690: template <> string_t StringCast::Operation(interval_t input, Vector &vector) {
691: 	std::string s = Interval::ToString(input);
692: 	return StringVector::AddString(vector, s);
693: }
694: 
695: struct HugeintToStringCast {
696: 	static int UnsignedLength(hugeint_t value) {
697: 		assert(value.upper >= 0);
698: 		if (value.upper == 0) {
699: 			return StringToIntegerCast::UnsignedLength<uint64_t>(value.lower);
700: 		}
701: 		// search the length using the PowersOfTen array
702: 		// the length has to be between [17] and [38], because the hugeint is bigger than 2^63
703: 		// we use the same approach as above, but split a bit more because comparisons for hugeints are more expensive
704: 		if (value >= Hugeint::PowersOfTen[27]) {
705: 			// [27..38]
706: 			if (value >= Hugeint::PowersOfTen[32]) {
707: 				if (value >= Hugeint::PowersOfTen[36]) {
708: 					int length = 37;
709: 					length += value >= Hugeint::PowersOfTen[37];
710: 					length += value >= Hugeint::PowersOfTen[38];
711: 					return length;
712: 				} else {
713: 					int length = 33;
714: 					length += value >= Hugeint::PowersOfTen[33];
715: 					length += value >= Hugeint::PowersOfTen[34];
716: 					length += value >= Hugeint::PowersOfTen[35];
717: 					return length;
718: 				}
719: 			} else {
720: 				if (value >= Hugeint::PowersOfTen[30]) {
721: 					int length = 31;
722: 					length += value >= Hugeint::PowersOfTen[31];
723: 					length += value >= Hugeint::PowersOfTen[32];
724: 					return length;
725: 				} else {
726: 					int length = 28;
727: 					length += value >= Hugeint::PowersOfTen[28];
728: 					length += value >= Hugeint::PowersOfTen[29];
729: 					return length;
730: 				}
731: 			}
732: 		} else {
733: 			// [17..27]
734: 			if (value >= Hugeint::PowersOfTen[22]) {
735: 				// [22..27]
736: 				if (value >= Hugeint::PowersOfTen[25]) {
737: 					int length = 26;
738: 					length += value >= Hugeint::PowersOfTen[26];
739: 					return length;
740: 				} else {
741: 					int length = 23;
742: 					length += value >= Hugeint::PowersOfTen[23];
743: 					length += value >= Hugeint::PowersOfTen[24];
744: 					return length;
745: 				}
746: 			} else {
747: 				// [17..22]
748: 				if (value >= Hugeint::PowersOfTen[20]) {
749: 					int length = 21;
750: 					length += value >= Hugeint::PowersOfTen[21];
751: 					return length;
752: 				} else {
753: 					int length = 18;
754: 					length += value >= Hugeint::PowersOfTen[18];
755: 					length += value >= Hugeint::PowersOfTen[19];
756: 					return length;
757: 				}
758: 			}
759: 		}
760: 	}
761: 
762: 	// Formats value in reverse and returns a pointer to the beginning.
763: 	static char *FormatUnsigned(hugeint_t value, char *ptr) {
764: 		while (value.upper > 0) {
765: 			// while integer division is slow, hugeint division is MEGA slow
766: 			// we want to avoid doing as many divisions as possible
767: 			// for that reason we start off doing a division by a large power of ten that uint64_t can hold
768: 			// (100000000000000000) - this is the third largest
769: 			// the reason we don't use the largest is because that can result in an overflow inside the division function
770: 			uint64_t remainder;
771: 			value = Hugeint::DivModPositive(value, 100000000000000000ULL, remainder);
772: 
773: 			auto startptr = ptr;
774: 			// now we format the remainder: note that we need to pad with zero's in case
775: 			// the remainder is small (i.e. less than 10000000000000000)
776: 			ptr = StringToIntegerCast::FormatUnsigned<uint64_t>(remainder, ptr);
777: 
778: 			int format_length = startptr - ptr;
779: 			// pad with zero
780: 			for(int i = format_length; i < 17; i++) {
781: 				*--ptr = '0';
782: 			}
783: 		}
784: 		// once the value falls in the range of a uint64_t, fallback to formatting as uint64_t to avoid hugeint division
785: 		return StringToIntegerCast::FormatUnsigned<uint64_t>(value.lower, ptr);
786: 	}
787: 
788: 	static string_t FormatSigned(hugeint_t value, Vector &vector) {
789: 		int negative = value.upper < 0;
790: 		if (negative) {
791: 			Hugeint::NegateInPlace(value);
792: 		}
793: 		int length = UnsignedLength(value) + negative;
794: 		string_t result = StringVector::EmptyString(vector, length);
795: 		auto dataptr = result.GetData();
796: 		auto endptr = dataptr + length;
797: 		if (value.upper == 0) {
798: 			// small value: format as uint64_t
799: 			endptr = StringToIntegerCast::FormatUnsigned<uint64_t>(value.lower, endptr);
800: 		} else {
801: 			endptr = FormatUnsigned(value, endptr);
802: 		}
803: 		if (negative) {
804: 			*--endptr = '-';
805: 		}
806: 		assert(endptr == dataptr);
807: 		result.Finalize();
808: 		return result;
809: 	}
810: };
811: 
812: template <> duckdb::string_t StringCast::Operation(hugeint_t input, Vector &vector) {
813: 	return HugeintToStringCast::FormatSigned(move(input), vector);
814: }
815: 
816: //===--------------------------------------------------------------------===//
817: // Cast From Date
818: //===--------------------------------------------------------------------===//
819: struct DateToStringCast {
820: 	static idx_t Length(int32_t date[], idx_t &year_length, bool &add_bc) {
821: 		// format is YYYY-MM-DD with optional (BC) at the end
822: 		// regular length is 10
823: 		idx_t length = 6;
824: 		year_length = 4;
825: 		add_bc = false;
826: 		if (date[0] <= 0) {
827: 			// add (BC) suffix
828: 			length += 5;
829: 			date[0] = -date[0];
830: 			add_bc = true;
831: 		}
832: 
833: 		// potentially add extra characters depending on length of year
834: 		year_length += date[0] >= 10000;
835: 		year_length += date[0] >= 100000;
836: 		year_length += date[0] >= 1000000;
837: 		year_length += date[0] >= 10000000;
838: 		length += year_length;
839: 		return length;
840: 	}
841: 
842: 	static void Format(char *data, int32_t date[], idx_t year_length, bool add_bc) {
843: 		// now we write the string, first write the year
844: 		auto endptr = data + year_length;
845: 		endptr = StringToIntegerCast::FormatUnsigned(date[0], endptr);
846: 		// add optional leading zeros
847: 		while (endptr > data) {
848: 			*--endptr = '0';
849: 		}
850: 		// now write the month and day
851: 		auto ptr = data + year_length;
852: 		for (int i = 1; i <= 2; i++) {
853: 			ptr[0] = '-';
854: 			if (date[i] < 10) {
855: 				ptr[1] = '0';
856: 				ptr[2] = '0' + date[i];
857: 			} else {
858: 				auto index = static_cast<unsigned>(date[i] * 2);
859: 				ptr[1] = duckdb_fmt::internal::data::digits[index];
860: 				ptr[2] = duckdb_fmt::internal::data::digits[index + 1];
861: 			}
862: 			ptr += 3;
863: 		}
864: 		// optionally add BC to the end of the date
865: 		if (add_bc) {
866: 			memcpy(ptr, " (BC)", 5);
867: 		}
868: 	}
869: };
870: 
871: template <> string_t CastFromDate::Operation(date_t input, Vector &vector) {
872: 	int32_t date[3];
873: 	Date::Convert(input, date[0], date[1], date[2]);
874: 
875: 	idx_t year_length;
876: 	bool add_bc;
877: 	idx_t length = DateToStringCast::Length(date, year_length, add_bc);
878: 
879: 	string_t result = StringVector::EmptyString(vector, length);
880: 	auto data = result.GetData();
881: 
882: 	DateToStringCast::Format(data, date, year_length, add_bc);
883: 
884: 	result.Finalize();
885: 	return result;
886: }
887: 
888: //===--------------------------------------------------------------------===//
889: // Cast To Date
890: //===--------------------------------------------------------------------===//
891: template <> date_t CastToDate::Operation(string_t input) {
892: 	return Date::FromCString(input.GetData());
893: }
894: 
895: template <> date_t StrictCastToDate::Operation(string_t input) {
896: 	return Date::FromCString(input.GetData(), true);
897: }
898: 
899: //===--------------------------------------------------------------------===//
900: // Cast From Time
901: //===--------------------------------------------------------------------===//
902: struct TimeToStringCast {
903: 	static idx_t Length(int32_t time[]) {
904: 		// format is HH:MM:DD
905: 		// regular length is 8
906: 		idx_t length = 8;
907: 		if (time[3] > 0) {
908: 			// if there are msecs, we add the miliseconds after the time with a period separator
909: 			// i.e. the format becomes HH:MM:DD.msec
910: 			length += 4;
911: 		}
912: 		return length;
913: 	}
914: 
915: 	static void Format(char *data, idx_t length, int32_t time[]) {
916: 		// first write hour, month and day
917: 		auto ptr = data;
918: 		for (int i = 0; i <= 2; i++) {
919: 			if (time[i] < 10) {
920: 				ptr[0] = '0';
921: 				ptr[1] = '0' + time[i];
922: 			} else {
923: 				auto index = static_cast<unsigned>(time[i] * 2);
924: 				ptr[0] = duckdb_fmt::internal::data::digits[index];
925: 				ptr[1] = duckdb_fmt::internal::data::digits[index + 1];
926: 			}
927: 			ptr[2] = ':';
928: 			ptr += 3;
929: 		}
930: 		// now optionally write ms at the end
931: 		if (time[3] > 0) {
932: 			auto start = ptr;
933: 			ptr = StringToIntegerCast::FormatUnsigned(time[3], data + length);
934: 			while (ptr > start) {
935: 				*--ptr = '0';
936: 			}
937: 			*--ptr = '.';
938: 		}
939: 	}
940: };
941: 
942: template <> string_t CastFromTime::Operation(dtime_t input, Vector &vector) {
943: 	int32_t time[4];
944: 	Time::Convert(input, time[0], time[1], time[2], time[3]);
945: 
946: 	idx_t length = TimeToStringCast::Length(time);
947: 
948: 	string_t result = StringVector::EmptyString(vector, length);
949: 	auto data = result.GetData();
950: 
951: 	TimeToStringCast::Format(data, length, time);
952: 
953: 	result.Finalize();
954: 	return result;
955: }
956: 
957: //===--------------------------------------------------------------------===//
958: // Cast To Time
959: //===--------------------------------------------------------------------===//
960: template <> dtime_t CastToTime::Operation(string_t input) {
961: 	return Time::FromCString(input.GetData());
962: }
963: 
964: template <> dtime_t StrictCastToTime::Operation(string_t input) {
965: 	return Time::FromCString(input.GetData(), true);
966: }
967: 
968: template <> timestamp_t CastDateToTimestamp::Operation(date_t input) {
969: 	return Timestamp::FromDatetime(input, Time::FromTime(0, 0, 0, 0));
970: }
971: 
972: //===--------------------------------------------------------------------===//
973: // Cast From Timestamps
974: //===--------------------------------------------------------------------===//
975: template <> string_t CastFromTimestamp::Operation(timestamp_t input, Vector &vector) {
976: 	date_t date_entry;
977: 	dtime_t time_entry;
978: 	Timestamp::Convert(input, date_entry, time_entry);
979: 
980: 	int32_t date[3], time[4];
981: 	Date::Convert(date_entry, date[0], date[1], date[2]);
982: 	Time::Convert(time_entry, time[0], time[1], time[2], time[3]);
983: 
984: 	// format for timestamp is DATE TIME (separated by space)
985: 	idx_t year_length;
986: 	bool add_bc;
987: 	idx_t date_length = DateToStringCast::Length(date, year_length, add_bc);
988: 	idx_t time_length = TimeToStringCast::Length(time);
989: 	idx_t length = date_length + time_length + 1;
990: 
991: 	string_t result = StringVector::EmptyString(vector, length);
992: 	auto data = result.GetData();
993: 
994: 	DateToStringCast::Format(data, date, year_length, add_bc);
995: 	data[date_length] = ' ';
996: 	TimeToStringCast::Format(data + date_length + 1, time_length, time);
997: 
998: 	result.Finalize();
999: 	return result;
1000: }
1001: 
1002: template <> date_t CastTimestampToDate::Operation(timestamp_t input) {
1003: 	return Timestamp::GetDate(input);
1004: }
1005: 
1006: template <> dtime_t CastTimestampToTime::Operation(timestamp_t input) {
1007: 	return Timestamp::GetTime(input);
1008: }
1009: 
1010: //===--------------------------------------------------------------------===//
1011: // Cast To Timestamp
1012: //===--------------------------------------------------------------------===//
1013: template <> timestamp_t CastToTimestamp::Operation(string_t input) {
1014: 	return Timestamp::FromString(input.GetData());
1015: }
1016: 
1017: //===--------------------------------------------------------------------===//
1018: // Cast From Blob
1019: //===--------------------------------------------------------------------===//
1020: template <> string_t CastFromBlob::Operation(string_t input, Vector &vector) {
1021: 	idx_t input_size = input.GetSize();
1022: 	// double chars for hex string plus two because of hex identifier ('\x')
1023: 	string_t result = StringVector::EmptyString(vector, input_size * 2 + 2);
1024: 	CastFromBlob::ToHexString(input, result);
1025: 	return result;
1026: }
1027: 
1028: void CastFromBlob::ToHexString(string_t input, string_t &output) {
1029: 	const char hexa_table[] = {'0','1','2','3','4','5','6','7','8','9','A','B','C','D','E','F'};
1030: 	idx_t input_size = input.GetSize();
1031: 	assert(output.GetSize() == (input_size * 2 + 2));
1032: 	auto input_data = input.GetData();
1033: 	auto hexa_data  = output.GetData();
1034: 	// hex identifier
1035: 	hexa_data[0] = '\\'; hexa_data[1] = 'x';
1036: 	hexa_data += 2;
1037: 	for(idx_t idx = 0; idx < input_size; ++idx) {
1038: 		hexa_data[idx * 2]     = hexa_table[(input_data[idx] >> 4) & 0x0F];
1039: 		hexa_data[idx * 2 + 1] = hexa_table[input_data[idx] & 0x0F];
1040: 	}
1041: 	output.Finalize();
1042: }
1043: 
1044: void CastFromBlob::FromHexToBytes(string_t input, string_t &output) {
1045: 	idx_t in_size = input.GetSize();
1046: 	// amount of hex chars must be even
1047: 	if((in_size % 2) != 0) {
1048: 		throw OutOfRangeException("Hex string must have an even number of bytes.");
1049: 	}
1050: 
1051: 	auto in_data = input.GetData();
1052: 	// removing '\x'
1053: 	in_data += 2;
1054: 	in_size -= 2;
1055: 
1056: 	auto out_data = output.GetData();
1057: 	idx_t out_size = output.GetSize();
1058: 	assert(out_size == (in_size / 2));
1059: 	idx_t out_idx=0;
1060: 
1061: 	idx_t num_hex_per_byte = 2;
1062: 	uint8_t hex[2];
1063: 
1064: 	for(idx_t in_idx = 0; in_idx < in_size; in_idx+=2, ++out_idx) {
1065: 		for(idx_t hex_idx = 0; hex_idx < num_hex_per_byte; ++hex_idx) {
1066: 			uint8_t int_ch = in_data[in_idx + hex_idx];
1067: 			if(int_ch >= (uint8_t)'0' && int_ch <= (uint8_t)'9') {
1068: 				// numeric ascii chars: '0' to '9'
1069: 				hex[hex_idx] = int_ch & 0X0F;
1070: 			}
1071: 			else if((int_ch >= (uint8_t)'A' && int_ch <= (uint8_t)'F') ||
1072: 					(int_ch >= (uint8_t)'a' && int_ch <= (uint8_t)'f')) {
1073: 					// hex chars: ['A':'F'] or ['a':'f']
1074: 				// transforming char into an integer in the range of 10 to 15
1075: 				hex[hex_idx] = ((int_ch & 0X0F) - 1) + 10;
1076: 			} else {
1077: 				throw OutOfRangeException("\"%c\" is not a valid hexadecimal char.", in_data[in_idx + hex_idx]);
1078: 			}
1079: 		}
1080: 		// adding two hex into the same byte
1081: 		out_data[out_idx] = hex[0];
1082: 		out_data[out_idx] = (out_data[out_idx] << 4) | hex[1];
1083: 	}
1084: 	out_data[out_idx] = '\0';
1085: }
1086: 
1087: //===--------------------------------------------------------------------===//
1088: // Cast To Blob
1089: //===--------------------------------------------------------------------===//
1090: template <> string_t CastToBlob::Operation(string_t input, Vector &vector) {
1091: 	idx_t input_size = input.GetSize();
1092: 	auto input_data = input.GetData();
1093: 	string_t result;
1094: 	// Check by a hex string
1095: 	if(input_size >= 2 && input_data[0] == '\\' && input_data[1] == 'x') {
1096: 		auto output = StringVector::EmptyString(vector, (input_size - 2) / 2);
1097: 		CastFromBlob::FromHexToBytes(input, output);
1098: 		result = output;
1099: 	} else {
1100: 		// raw string
1101: 		result = StringVector::AddBlob(vector, input);
1102: 	}
1103: 	return result;
1104: }
1105: 
1106: //===--------------------------------------------------------------------===//
1107: // Cast From Interval
1108: //===--------------------------------------------------------------------===//
1109: template <> bool TryCast::Operation(string_t input, interval_t &result, bool strict) {
1110: 	return Interval::FromCString(input.GetData(), input.GetSize(), result);
1111: }
1112: 
1113: template <> interval_t StrictCast::Operation(string_t input) {
1114: 	return try_strict_cast_string<interval_t>(input);
1115: }
1116: 
1117: template <> interval_t Cast::Operation(string_t input) {
1118: 	return try_cast_string<interval_t>(input);
1119: }
1120: 
1121: //===--------------------------------------------------------------------===//
1122: // Cast From Hugeint
1123: //===--------------------------------------------------------------------===//
1124: // parsing hugeint from string is done a bit differently for performance reasons
1125: // for other integer types we keep track of a single value
1126: // and multiply that value by 10 for every digit we read
1127: // however, for hugeints, multiplication is very expensive (>20X as expensive as for int64)
1128: // for that reason, we parse numbers first into an int64 value
1129: // when that value is full, we perform a HUGEINT multiplication to flush it into the hugeint
1130: // this takes the number of HUGEINT multiplications down from [0-38] to [0-2]
1131: struct HugeIntCastData {
1132: 	hugeint_t hugeint;
1133: 	int64_t intermediate;
1134: 	uint8_t digits;
1135: 
1136: 	bool Flush() {
1137: 		if (digits == 0 && intermediate == 0) {
1138: 			return true;
1139: 		}
1140: 		if (hugeint.lower != 0 || hugeint.upper != 0) {
1141: 			if (digits > 38) {
1142: 				return false;
1143: 			}
1144: 			if (!Hugeint::TryMultiply(hugeint, Hugeint::PowersOfTen[digits], hugeint)) {
1145: 				return false;
1146: 			}
1147: 		}
1148: 		if (!Hugeint::AddInPlace(hugeint, hugeint_t(intermediate))) {
1149: 			return false;
1150: 		}
1151: 		digits = 0;
1152: 		intermediate = 0;
1153: 		return true;
1154: 	}
1155: };
1156: 
1157: struct HugeIntegerCastOperation {
1158: 	template<class T, bool NEGATIVE>
1159: 	static bool HandleDigit(T &result, uint8_t digit) {
1160: 		if (NEGATIVE) {
1161: 			if (result.intermediate < (NumericLimits<int64_t>::Minimum() + digit) / 10) {
1162: 				// intermediate is full: need to flush it
1163: 				if (!result.Flush()) {
1164: 					return false;
1165: 				}
1166: 			}
1167: 			result.intermediate = result.intermediate * 10 - digit;
1168: 		} else {
1169: 			if (result.intermediate > (NumericLimits<int64_t>::Maximum() - digit) / 10) {
1170: 				if (!result.Flush()) {
1171: 					return false;
1172: 				}
1173: 			}
1174: 			result.intermediate = result.intermediate * 10 + digit;
1175: 		}
1176: 		result.digits++;
1177: 		return true;
1178: 	}
1179: 
1180: 	template<class T>
1181: 	static bool HandleExponent(T &result, int64_t exponent) {
1182: 		result.Flush();
1183: 		if (exponent < -38 || exponent > 38) {
1184: 			// out of range for exact exponent: use double and convert
1185: 			double dbl_res = Hugeint::Cast<double>(result.hugeint) * pow(10, exponent);
1186: 			if (dbl_res < Hugeint::Cast<double>(NumericLimits<hugeint_t>::Minimum()) || dbl_res > Hugeint::Cast<double>(NumericLimits<hugeint_t>::Maximum())) {
1187: 				return false;
1188: 			}
1189: 			result.hugeint = Hugeint::Convert(dbl_res);
1190: 			return true;
1191: 		}
1192: 		if (exponent < 0) {
1193: 			// negative exponent: divide by power of 10
1194: 			result.hugeint = Hugeint::Divide(result.hugeint, Hugeint::PowersOfTen[-exponent]);
1195: 			return true;
1196: 		} else {
1197: 			// positive exponent: multiply by power of 10
1198: 			return Hugeint::TryMultiply(result.hugeint, Hugeint::PowersOfTen[exponent], result.hugeint);
1199: 		}
1200: 	}
1201: 
1202: 	template<class T>
1203: 	static bool Finalize(T &result) {
1204: 		return result.Flush();
1205: 	}
1206: };
1207: 
1208: template <> bool TryCast::Operation(string_t input, hugeint_t &result, bool strict) {
1209: 	HugeIntCastData data;
1210: 	if (!TryIntegerCast<HugeIntCastData, true, HugeIntegerCastOperation>(input.GetData(), input.GetSize(), data, strict)) {
1211: 		return false;
1212: 	}
1213: 	result = data.hugeint;
1214: 	return true;
1215: }
1216: 
1217: template <> hugeint_t Cast::Operation(string_t input) {
1218: 	return try_cast_string<hugeint_t>(input);
1219: }
1220: 
1221: template <> hugeint_t StrictCast::Operation(string_t input) {
1222: 	return try_strict_cast_string<hugeint_t>(input);
1223: }
1224: 
1225: //===--------------------------------------------------------------------===//
1226: // Numeric -> Hugeint
1227: //===--------------------------------------------------------------------===//
1228: template <> bool TryCast::Operation(bool input, hugeint_t &result, bool strict) {
1229: 	result = Cast::Operation<bool, hugeint_t>(input);
1230: 	return true;
1231: }
1232: 
1233: template <> bool TryCast::Operation(int8_t input, hugeint_t &result, bool strict) {
1234: 	result = Cast::Operation<int8_t, hugeint_t>(input);
1235: 	return true;
1236: }
1237: 
1238: template <> bool TryCast::Operation(int16_t input, hugeint_t &result, bool strict) {
1239: 	result = Cast::Operation<int16_t, hugeint_t>(input);
1240: 	return true;
1241: }
1242: 
1243: template <> bool TryCast::Operation(int32_t input, hugeint_t &result, bool strict) {
1244: 	result = Cast::Operation<int32_t, hugeint_t>(input);
1245: 	return true;
1246: }
1247: 
1248: template <> bool TryCast::Operation(int64_t input, hugeint_t &result, bool strict) {
1249: 	result = Cast::Operation<int64_t, hugeint_t>(input);
1250: 	return true;
1251: }
1252: 
1253: template <> bool TryCast::Operation(float input, hugeint_t &result, bool strict) {
1254: 	result = Cast::Operation<float, hugeint_t>(input);
1255: 	return true;
1256: }
1257: 
1258: template <> bool TryCast::Operation(double input, hugeint_t &result, bool strict) {
1259: 	result = Cast::Operation<double, hugeint_t>(input);
1260: 	return true;
1261: }
1262: 
1263: template <> hugeint_t Cast::Operation(bool input) {
1264: 	hugeint_t result;
1265: 	result.upper = 0;
1266: 	result.lower = input ? 1 : 0;
1267: 	return result;
1268: }
1269: template <> hugeint_t Cast::Operation(int8_t input) {
1270: 	return Hugeint::Convert<int8_t>(input);
1271: }
1272: template <> hugeint_t Cast::Operation(int16_t input) {
1273: 	return Hugeint::Convert<int16_t>(input);
1274: }
1275: template <> hugeint_t Cast::Operation(int32_t input) {
1276: 	return Hugeint::Convert<int32_t>(input);
1277: }
1278: template <> hugeint_t Cast::Operation(int64_t input) {
1279: 	return Hugeint::Convert<int64_t>(input);
1280: }
1281: template <> hugeint_t Cast::Operation(float input) {
1282: 	return Hugeint::Convert<float>(input);
1283: }
1284: template <> hugeint_t Cast::Operation(double input) {
1285: 	return Hugeint::Convert<double>(input);
1286: }
1287: 
1288: //===--------------------------------------------------------------------===//
1289: // Hugeint -> Numeric
1290: //===--------------------------------------------------------------------===//
1291: template <> bool TryCast::Operation(hugeint_t input, bool &result, bool strict) {
1292: 	// any positive number converts to true
1293: 	result = input.upper > 0 || (input.upper == 0 && input.lower > 0);
1294: 	return true;
1295: }
1296: 
1297: template <> bool TryCast::Operation(hugeint_t input, int8_t &result, bool strict) {
1298: 	return Hugeint::TryCast<int8_t>(input, result);
1299: }
1300: 
1301: template <> bool TryCast::Operation(hugeint_t input, int16_t &result, bool strict) {
1302: 	return Hugeint::TryCast<int16_t>(input, result);
1303: }
1304: 
1305: template <> bool TryCast::Operation(hugeint_t input, int32_t &result, bool strict) {
1306: 	return Hugeint::TryCast<int32_t>(input, result);
1307: }
1308: 
1309: template <> bool TryCast::Operation(hugeint_t input, int64_t &result, bool strict) {
1310: 	return Hugeint::TryCast<int64_t>(input, result);
1311: }
1312: 
1313: template <> bool TryCast::Operation(hugeint_t input, float &result, bool strict) {
1314: 	return Hugeint::TryCast<float>(input, result);
1315: }
1316: 
1317: template <> bool TryCast::Operation(hugeint_t input, double &result, bool strict) {
1318: 	return Hugeint::TryCast<double>(input, result);
1319: }
1320: 
1321: template <> bool Cast::Operation(hugeint_t input) {
1322: 	bool result;
1323: 	TryCast::Operation(input, result);
1324: 	return result;
1325: }
1326: 
1327: template<class T>
1328: static T hugeint_cast_to_numeric(hugeint_t input) {
1329: 	T result;
1330: 	if (!TryCast::Operation<hugeint_t, T>(input, result)) {
1331: 		throw OutOfRangeException("Failed to cast from hugeint: value is out of range");
1332: 	}
1333: 	return result;
1334: }
1335: 
1336: template <> int8_t Cast::Operation(hugeint_t input) {
1337: 	return hugeint_cast_to_numeric<int8_t>(input);
1338: }
1339: 
1340: template <> int16_t Cast::Operation(hugeint_t input) {
1341: 	return hugeint_cast_to_numeric<int16_t>(input);
1342: }
1343: 
1344: template <> int32_t Cast::Operation(hugeint_t input) {
1345: 	return hugeint_cast_to_numeric<int32_t>(input);
1346: }
1347: 
1348: template <> int64_t Cast::Operation(hugeint_t input) {
1349: 	return hugeint_cast_to_numeric<int64_t>(input);
1350: }
1351: 
1352: template <> float Cast::Operation(hugeint_t input) {
1353: 	return hugeint_cast_to_numeric<float>(input);
1354: }
1355: 
1356: template <> double Cast::Operation(hugeint_t input) {
1357: 	return hugeint_cast_to_numeric<double>(input);
1358: }
1359: 
1360: template <> bool TryCast::Operation(hugeint_t input, hugeint_t &result, bool strict) {
1361: 	result = input;
1362: 	return true;
1363: }
1364: 
1365: template <> hugeint_t Cast::Operation(hugeint_t input) {
1366: 	return input;
1367: }
1368: 
1369: } // namespace duckdb
[end of src/common/operator/cast_operators.cpp]
[start of src/common/serializer/buffered_file_reader.cpp]
1: #include "duckdb/common/serializer/buffered_file_reader.hpp"
2: #include "duckdb/common/serializer/buffered_file_writer.hpp"
3: #include "duckdb/common/exception.hpp"
4: 
5: #include <cstring>
6: #include <algorithm>
7: 
8: using namespace duckdb;
9: using namespace std;
10: 
11: BufferedFileReader::BufferedFileReader(FileSystem &fs, const char *path)
12:     : fs(fs), data(unique_ptr<data_t[]>(new data_t[FILE_BUFFER_SIZE])), offset(0), read_data(0), total_read(0) {
13: 	handle = fs.OpenFile(path, FileFlags::READ, FileLockType::READ_LOCK);
14: 	file_size = fs.GetFileSize(*handle);
15: }
16: 
17: void BufferedFileReader::ReadData(data_ptr_t target_buffer, uint64_t read_size) {
18: 	// first copy anything we can from the buffer
19: 	data_ptr_t end_ptr = target_buffer + read_size;
20: 	while (true) {
21: 		idx_t to_read = std::min((idx_t)(end_ptr - target_buffer), read_data - offset);
22: 		if (to_read > 0) {
23: 			memcpy(target_buffer, data.get() + offset, to_read);
24: 			offset += to_read;
25: 			target_buffer += to_read;
26: 		}
27: 		if (target_buffer < end_ptr) {
28: 			assert(offset == read_data);
29: 			total_read += read_data;
30: 			// did not finish reading yet but exhausted buffer
31: 			// read data into buffer
32: 			offset = 0;
33: 			read_data = fs.Read(*handle, data.get(), FILE_BUFFER_SIZE);
34: 			if (read_data == 0) {
35: 				throw SerializationException("not enough data in file to deserialize result");
36: 			}
37: 		} else {
38: 			return;
39: 		}
40: 	}
41: }
42: 
43: bool BufferedFileReader::Finished() {
44: 	return total_read + offset == file_size;
45: }
[end of src/common/serializer/buffered_file_reader.cpp]
[start of src/common/types/CMakeLists.txt]
1: add_library_unity(duckdb_common_types
2:                   OBJECT
3:                   chunk_collection.cpp
4:                   data_chunk.cpp
5:                   date.cpp
6:                   hash.cpp
7:                   hugeint.cpp
8:                   hyperloglog.cpp
9:                   interval.cpp
10:                   null_value.cpp
11:                   selection_vector.cpp
12:                   string_heap.cpp
13:                   string_type.cpp
14:                   timestamp.cpp
15:                   time.cpp
16:                   value.cpp
17:                   vector_buffer.cpp
18:                   vector.cpp
19:                   vector_constants.cpp)
20: set(ALL_OBJECT_FILES
21:     ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:duckdb_common_types>
22:     PARENT_SCOPE)
[end of src/common/types/CMakeLists.txt]
[start of src/common/types/date.cpp]
1: #include "duckdb/common/types/date.hpp"
2: 
3: #include "duckdb/common/exception.hpp"
4: #include "duckdb/common/string_util.hpp"
5: #include "duckdb/common/assert.hpp"
6: #include "duckdb/common/limits.hpp"
7: 
8: #include <cstring>
9: #include <cctype>
10: #include <algorithm>
11: 
12: using namespace duckdb;
13: using namespace std;
14: 
15: // Taken from MonetDB mtime.c
16: 
17: static int NORMALDAYS[13] = {0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31};
18: static int LEAPDAYS[13] = {0, 31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31};
19: static int CUMDAYS[13] = {0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334, 365};
20: static int CUMLEAPDAYS[13] = {0, 31, 60, 91, 121, 152, 182, 213, 244, 274, 305, 335, 366};
21: 
22: #define YEAR_MAX 5867411
23: #define YEAR_MIN (-YEAR_MAX)
24: #define MONTHDAYS(m, y) ((m) != 2 ? LEAPDAYS[m] : leapyear(y) ? 29 : 28)
25: #define YEARDAYS(y) (leapyear(y) ? 366 : 365)
26: #define DD_DATE(d, m, y)                                                                                               \
27: 	((m) > 0 && (m) <= 12 && (d) > 0 && (y) != 0 && (y) >= YEAR_MIN && (y) <= YEAR_MAX && (d) <= MONTHDAYS(m, y))
28: #define LOWER(c) ((c) >= 'A' && (c) <= 'Z' ? (c) + 'a' - 'A' : (c))
29: // 1970-01-01 in date_t format
30: #define EPOCH_DATE 719528
31: // 1970-01-01 was a Thursday
32: #define EPOCH_DAY_OF_THE_WEEK 4
33: #define SECONDS_PER_DAY (60 * 60 * 24)
34: 
35: #define leapyear(y) ((y) % 4 == 0 && ((y) % 100 != 0 || (y) % 400 == 0))
36: 
37: static inline int leapyears(int year) {
38: 	/* count the 4-fold years that passed since jan-1-0 */
39: 	int y4 = year / 4;
40: 
41: 	/* count the 100-fold years */
42: 	int y100 = year / 100;
43: 
44: 	/* count the 400-fold years */
45: 	int y400 = year / 400;
46: 
47: 	return y4 + y400 - y100 + (year >= 0); /* may be negative */
48: }
49: 
50: static inline void number_to_date(int32_t n, int32_t &year, int32_t &month, int32_t &day) {
51: 	year = n / 365;
52: 	day = (n - year * 365) - leapyears(year >= 0 ? year - 1 : year);
53: 	if (n < 0) {
54: 		year--;
55: 		while (day >= 0) {
56: 			year++;
57: 			day -= YEARDAYS(year);
58: 		}
59: 		day = YEARDAYS(year) + day;
60: 	} else {
61: 		while (day < 0) {
62: 			year--;
63: 			day += YEARDAYS(year);
64: 		}
65: 	}
66: 
67: 	day++;
68: 	if (leapyear(year)) {
69: 		for (month = day / 31 == 0 ? 1 : day / 31; month <= 12; month++)
70: 			if (day > CUMLEAPDAYS[month - 1] && day <= CUMLEAPDAYS[month]) {
71: 				break;
72: 			}
73: 		day -= CUMLEAPDAYS[month - 1];
74: 	} else {
75: 		for (month = day / 31 == 0 ? 1 : day / 31; month <= 12; month++)
76: 			if (day > CUMDAYS[month - 1] && day <= CUMDAYS[month]) {
77: 				break;
78: 			}
79: 		day -= CUMDAYS[month - 1];
80: 	}
81: 	year = (year <= 0) ? year - 1 : year;
82: }
83: 
84: static inline int32_t date_to_number(int32_t year, int32_t month, int32_t day) {
85: 	int32_t n = 0;
86: 	if (!(DD_DATE(day, month, year))) {
87: 		throw ConversionException("Date out of range: %d-%d-%d", year, month, day);
88: 	}
89: 
90: 	if (year < 0)
91: 		year++;
92: 	n = (int32_t)(day - 1);
93: 	if (month > 2 && leapyear(year)) {
94: 		n++;
95: 	}
96: 	n += CUMDAYS[month - 1];
97: 	/* current year does not count as leapyear */
98: 	n += 365 * year + leapyears(year >= 0 ? year - 1 : year);
99: 
100: 	return n;
101: }
102: 
103: static bool ParseDoubleDigit(const char *buf, idx_t &pos, int32_t &result) {
104: 	if (std::isdigit((unsigned char)buf[pos])) {
105: 		result = buf[pos++] - '0';
106: 		if (std::isdigit((unsigned char)buf[pos])) {
107: 			result = (buf[pos++] - '0') + result * 10;
108: 		}
109: 		return true;
110: 	}
111: 	return false;
112: }
113: 
114: static bool TryConvertDate(const char *buf, date_t &result, bool strict = false) {
115: 	int32_t day = 0, month = -1;
116: 	int32_t year = 0, yearneg = (buf[0] == '-');
117: 	idx_t pos = 0;
118: 	int sep;
119: 
120: 	// skip leading spaces
121: 	while (std::isspace((unsigned char)buf[pos])) {
122: 		pos++;
123: 	}
124: 
125: 	if (yearneg == 0 && !std::isdigit((unsigned char)buf[pos])) {
126: 		return false;
127: 	}
128: 
129: 	// first parse the year
130: 	for (pos = pos + yearneg; std::isdigit((unsigned char)buf[pos]); pos++) {
131: 		year = (buf[pos] - '0') + year * 10;
132: 		if (year > YEAR_MAX) {
133: 			break;
134: 		}
135: 	}
136: 
137: 	// fetch the separator
138: 	sep = buf[pos++];
139: 	if (sep != ' ' && sep != '-' && sep != '/' && sep != '\\') {
140: 		// invalid separator
141: 		return false;
142: 	}
143: 
144: 	// parse the month
145: 	if (!ParseDoubleDigit(buf, pos, month)) {
146: 		return false;
147: 	}
148: 
149: 	if (buf[pos++] != sep) {
150: 		return false;
151: 	}
152: 
153: 	// now parse the day
154: 	if (!ParseDoubleDigit(buf, pos, day)) {
155: 		return false;
156: 	}
157: 
158: 	// check for an optional trailing " (BC)""
159: 	if (std::isspace(buf[pos]) && buf[pos + 1] == '(' &&
160: 	                              buf[pos + 2] == 'B' &&
161: 								  buf[pos + 3] == 'C' &&
162: 								  buf[pos + 4] == ')') {
163: 		year = -year;
164: 		pos += 5;
165: 	}
166: 
167: 	// in strict mode, check remaining string for non-space characters
168: 	if (strict) {
169: 		// skip trailing spaces
170: 		while (std::isspace((unsigned char)buf[pos])) {
171: 			pos++;
172: 		}
173: 		// check position. if end was not reached, non-space chars remaining
174: 		if (pos < strlen(buf)) {
175: 			return false;
176: 		}
177: 	} else {
178: 		// in non-strict mode, check for any direct trailing digits
179: 		if (std::isdigit((unsigned char)buf[pos])) {
180: 			return false;
181: 		}
182: 	}
183: 
184: 	result = Date::FromDate(yearneg ? -year : year, month, day);
185: 	return true;
186: }
187: 
188: date_t Date::FromCString(const char *buf, bool strict) {
189: 	date_t result;
190: 	if (!TryConvertDate(buf, result, strict)) {
191: 		throw ConversionException("date/time field value out of range: \"%s\", "
192: 		                          "expected format is (YYYY-MM-DD)",
193: 		                          buf);
194: 	}
195: 	return result;
196: }
197: 
198: date_t Date::FromString(string str, bool strict) {
199: 	return Date::FromCString(str.c_str(), strict);
200: }
201: 
202: string Date::ToString(int32_t date) {
203: 	int32_t year, month, day;
204: 	number_to_date(date, year, month, day);
205: 	if (year < 0) {
206: 		return StringUtil::Format("%04d-%02d-%02d (BC)", -year, month, day);
207: 	} else {
208: 		return StringUtil::Format("%04d-%02d-%02d", year, month, day);
209: 	}
210: }
211: 
212: string Date::Format(int32_t year, int32_t month, int32_t day) {
213: 	return ToString(Date::FromDate(year, month, day));
214: }
215: 
216: void Date::Convert(int32_t date, int32_t &out_year, int32_t &out_month, int32_t &out_day) {
217: 	number_to_date(date, out_year, out_month, out_day);
218: }
219: 
220: int32_t Date::FromDate(int32_t year, int32_t month, int32_t day) {
221: 	return date_to_number(year, month, day);
222: }
223: 
224: bool Date::IsLeapYear(int32_t year) {
225: 	return year % 4 == 0 && (year % 100 != 0 || year % 400 == 0);
226: }
227: 
228: bool Date::IsValidDay(int32_t year, int32_t month, int32_t day) {
229: 	if (month < 1 || month > 12)
230: 		return false;
231: 	if (day < 1)
232: 		return false;
233: 	if (year < YEAR_MIN || year > YEAR_MAX)
234: 		return false;
235: 
236: 	return IsLeapYear(year) ? day <= LEAPDAYS[month] : day <= NORMALDAYS[month];
237: }
238: 
239: date_t Date::EpochToDate(int64_t epoch) {
240: 	assert((epoch / SECONDS_PER_DAY) + EPOCH_DATE <= NumericLimits<int32_t>::Maximum());
241: 	return (date_t)((epoch / SECONDS_PER_DAY) + EPOCH_DATE);
242: }
243: 
244: int64_t Date::Epoch(date_t date) {
245: 	return ((int64_t)date - EPOCH_DATE) * SECONDS_PER_DAY;
246: }
247: int32_t Date::ExtractYear(date_t date) {
248: 	int32_t out_year, out_month, out_day;
249: 	Date::Convert(date, out_year, out_month, out_day);
250: 	return out_year;
251: }
252: 
253: int32_t Date::ExtractMonth(date_t date) {
254: 	int32_t out_year, out_month, out_day;
255: 	Date::Convert(date, out_year, out_month, out_day);
256: 	return out_month;
257: }
258: 
259: int32_t Date::ExtractDay(date_t date) {
260: 	int32_t out_year, out_month, out_day;
261: 	Date::Convert(date, out_year, out_month, out_day);
262: 	return out_day;
263: }
264: 
265: int32_t Date::ExtractDayOfTheYear(date_t date) {
266: 	int32_t out_year, out_month, out_day;
267: 	Date::Convert(date, out_year, out_month, out_day);
268: 	if (out_month >= 1) {
269: 		out_day += Date::IsLeapYear(out_year) ? CUMLEAPDAYS[out_month - 1] : CUMDAYS[out_month - 1];
270: 	}
271: 	return out_day;
272: }
273: 
274: int32_t Date::ExtractISODayOfTheWeek(date_t date) {
275: 	// -1 = 5
276: 	// 0 = 6
277: 	// 1 = 7
278: 	// 2 = 1
279: 	// 3 = 2
280: 	// 4 = 3
281: 	// 5 = 4
282: 	// 6 = 5
283: 	// 0 = 6
284: 	// 1 = 7
285: 	if (date < 2) {
286: 		return ((date - 1) % 7) + 7;
287: 	} else {
288: 		return ((date - 2) % 7) + 1;
289: 	}
290: }
291: 
292: static int32_t GetWeek(int32_t year, int32_t month, int32_t day) {
293: 	auto day_of_the_year = (leapyear(year) ? CUMLEAPDAYS[month] : CUMDAYS[month]) + day;
294: 	// get the first day of the first week of the year
295: 	// the first week is the week that has the 4th of January in it
296: 	auto day_of_the_fourth = Date::ExtractISODayOfTheWeek(Date::FromDate(year, 1, 4));
297: 	// if fourth is monday, then fourth is the first day
298: 	// if fourth is tuesday, third is the first day
299: 	// if fourth is wednesday, second is the first day
300: 	// if fourth is thursday - sunday, first is the first day
301: 	auto first_day_of_the_first_week = day_of_the_fourth >= 4 ? 0 : 5 - day_of_the_fourth;
302: 	if (day_of_the_year < first_day_of_the_first_week) {
303: 		// day is part of last year
304: 		return GetWeek(year - 1, 12, day);
305: 	} else {
306: 		return ((day_of_the_year - first_day_of_the_first_week) / 7) + 1;
307: 	}
308: }
309: 
310: int32_t Date::ExtractWeekNumber(date_t date) {
311: 	int32_t year, month, day;
312: 	Date::Convert(date, year, month, day);
313: 	return GetWeek(year, month - 1, day - 1);
314: }
315: 
316: // Returns the date of the monday of the current week.
317: date_t Date::GetMondayOfCurrentWeek(date_t date) {
318: 	int32_t dotw = Date::ExtractISODayOfTheWeek(date);
319: 
320: 	int32_t days = date_to_number(Date::ExtractYear(date), Date::ExtractMonth(date), Date::ExtractDay(date));
321: 
322: 	days -= dotw - 1;
323: 
324: 	int32_t year, month, day;
325: 	number_to_date(days, year, month, day);
326: 
327: 	return (Date::FromDate(year, month, day));
328: }
[end of src/common/types/date.cpp]
[start of src/common/types/time.cpp]
1: #include "duckdb/common/types/time.hpp"
2: #include "duckdb/common/types/timestamp.hpp"
3: 
4: #include "duckdb/common/string_util.hpp"
5: #include "duckdb/common/exception.hpp"
6: 
7: #include <iomanip>
8: #include <cstring>
9: #include <iostream>
10: #include <sstream>
11: #include <cctype>
12: 
13: using namespace duckdb;
14: using namespace std;
15: 
16: // string format is hh:mm:ssZ
17: // Z is optional
18: // ISO 8601
19: 
20: // Taken from MonetDB mtime.c
21: #define DD_TIME(h, m, s, x)                                                                                            \
22: 	((h) >= 0 && (h) < 24 && (m) >= 0 && (m) < 60 && (s) >= 0 && (s) <= 60 && (x) >= 0 && (x) < 1000)
23: 
24: static dtime_t time_to_number(int hour, int min, int sec, int msec) {
25: 	if (!DD_TIME(hour, min, sec, msec)) {
26: 		throw Exception("Invalid time");
27: 	}
28: 	return (dtime_t)(((((hour * 60) + min) * 60) + sec) * 1000 + msec);
29: }
30: 
31: static void number_to_time(dtime_t n, int32_t &hour, int32_t &min, int32_t &sec, int32_t &msec) {
32: 	int h, m, s, ms;
33: 
34: 	h = n / 3600000;
35: 	n -= h * 3600000;
36: 	m = n / 60000;
37: 	n -= m * 60000;
38: 	s = n / 1000;
39: 	n -= s * 1000;
40: 	ms = n;
41: 
42: 	hour = h;
43: 	min = m;
44: 	sec = s;
45: 	msec = ms;
46: }
47: 
48: // TODO this is duplicated in date.cpp
49: static bool ParseDoubleDigit2(const char *buf, idx_t &pos, int32_t &result) {
50: 	if (std::isdigit((unsigned char)buf[pos])) {
51: 		result = buf[pos++] - '0';
52: 		if (std::isdigit((unsigned char)buf[pos])) {
53: 			result = (buf[pos++] - '0') + result * 10;
54: 		}
55: 		return true;
56: 	}
57: 	return false;
58: }
59: 
60: static bool TryConvertTime(const char *buf, dtime_t &result, bool strict = false) {
61: 	int32_t hour = -1, min = -1, sec = -1, msec = -1;
62: 	idx_t pos = 0;
63: 	int sep;
64: 
65: 	// skip leading spaces
66: 	while (std::isspace((unsigned char)buf[pos])) {
67: 		pos++;
68: 	}
69: 
70: 	if (!std::isdigit((unsigned char)buf[pos])) {
71: 		return false;
72: 	}
73: 
74: 	if (!ParseDoubleDigit2(buf, pos, hour)) {
75: 		return false;
76: 	}
77: 	if (hour < 0 || hour > 24) {
78: 		return false;
79: 	}
80: 
81: 	// fetch the separator
82: 	sep = buf[pos++];
83: 	if (sep != ':') {
84: 		// invalid separator
85: 		return false;
86: 	}
87: 
88: 	if (!ParseDoubleDigit2(buf, pos, min)) {
89: 		return false;
90: 	}
91: 	if (min < 0 || min > 60) {
92: 		return false;
93: 	}
94: 
95: 	if (buf[pos++] != sep) {
96: 		return false;
97: 	}
98: 
99: 	if (!ParseDoubleDigit2(buf, pos, sec)) {
100: 		return false;
101: 	}
102: 	if (sec < 0 || sec > 60) {
103: 		return false;
104: 	}
105: 
106: 	msec = 0;
107: 	sep = buf[pos++];
108: 	if (sep == '.') { // we expect some milliseconds
109: 		uint8_t mult = 100;
110: 		for (; std::isdigit((unsigned char)buf[pos]) && mult > 0; pos++, mult /= 10) {
111: 			msec += (buf[pos] - '0') * mult;
112: 		}
113: 	}
114: 
115: 	// in strict mode, check remaining string for non-space characters
116: 	if (strict) {
117: 		// skip trailing spaces
118: 		while (std::isspace((unsigned char)buf[pos])) {
119: 			pos++;
120: 		}
121: 		// check position. if end was not reached, non-space chars remaining
122: 		if (pos < strlen(buf)) {
123: 			return false;
124: 		}
125: 	}
126: 
127: 	result = Time::FromTime(hour, min, sec, msec);
128: 	return true;
129: }
130: 
131: dtime_t Time::FromCString(const char *buf, bool strict) {
132: 	dtime_t result;
133: 	if (!TryConvertTime(buf, result, strict)) {
134: 		// last chance, check if we can parse as timestamp
135: 		if (strlen(buf) > 10 && !strict) {
136: 			return Timestamp::GetTime(Timestamp::FromString(buf));
137: 		}
138: 		throw ConversionException("time field value out of range: \"%s\", "
139: 		                          "expected format is ([YYY-MM-DD ]HH:MM:SS[.MS])",
140: 		                          buf);
141: 	}
142: 	return result;
143: }
144: 
145: dtime_t Time::FromString(string str, bool strict) {
146: 	return Time::FromCString(str.c_str(), strict);
147: }
148: 
149: string Time::ToString(dtime_t time) {
150: 	int32_t hour, min, sec, msec;
151: 	number_to_time(time, hour, min, sec, msec);
152: 
153: 	if (msec > 0) {
154: 		return StringUtil::Format("%02d:%02d:%02d.%03d", hour, min, sec, msec);
155: 	} else {
156: 		return StringUtil::Format("%02d:%02d:%02d", hour, min, sec);
157: 	}
158: }
159: 
160: string Time::Format(int32_t hour, int32_t minute, int32_t second, int32_t milisecond) {
161: 	return ToString(Time::FromTime(hour, minute, second, milisecond));
162: }
163: 
164: dtime_t Time::FromTime(int32_t hour, int32_t minute, int32_t second, int32_t milisecond) {
165: 	return time_to_number(hour, minute, second, milisecond);
166: }
167: 
168: bool Time::IsValidTime(int32_t hour, int32_t minute, int32_t second, int32_t milisecond) {
169: 	return DD_TIME(hour, minute, second, milisecond);
170: }
171: 
172: void Time::Convert(dtime_t time, int32_t &out_hour, int32_t &out_min, int32_t &out_sec, int32_t &out_msec) {
173: 	number_to_time(time, out_hour, out_min, out_sec, out_msec);
174: }
[end of src/common/types/time.cpp]
[start of src/common/types/timestamp.cpp]
1: #include "duckdb/common/types/timestamp.hpp"
2: 
3: #include "duckdb/common/exception.hpp"
4: #include "duckdb/common/types/date.hpp"
5: #include "duckdb/common/types/time.hpp"
6: 
7: #include <chrono>  // chrono::system_clock
8: #include <string>  // string
9: #include <ctime>
10: 
11: using namespace std;
12: 
13: namespace duckdb {
14: 
15: constexpr const int32_t STD_TIMESTAMP_LENGTH = 19;
16: constexpr const int32_t TM_START_YEAR = 1900;
17: 
18: // timestamp/datetime uses 64 bits, high 32 bits for date and low 32 bits for time
19: // string format is YYYY-MM-DDThh:mm:ssZ
20: // T may be a space
21: // Z is optional
22: // ISO 8601
23: 
24: timestamp_t Timestamp::FromString(string str) {
25: 	assert(sizeof(timestamp_t) == 8);
26: 	assert(sizeof(date_t) == 4);
27: 	assert(sizeof(dtime_t) == 4);
28: 
29: 	// In case we have only date we add a default time
30: 	if (str.size() == 10) {
31: 		str += " 00:00:00";
32: 	}
33: 	// Character length	19 positions minimum to 23 maximum
34: 	if (str.size() < STD_TIMESTAMP_LENGTH) {
35: 		throw ConversionException("timestamp field value out of range: \"%s\", "
36: 		                          "expected format is (YYYY-MM-DD HH:MM:SS[.MS])",
37: 		                          str.c_str());
38: 	}
39: 
40: 	date_t date = Date::FromString(str.substr(0, 10));
41: 	dtime_t time = Time::FromString(str.substr(10));
42: 
43: 	return ((int64_t)date << 32 | (int32_t)time);
44: }
45: 
46: string Timestamp::ToString(timestamp_t timestamp) {
47: 	assert(sizeof(timestamp_t) == 8);
48: 	assert(sizeof(date_t) == 4);
49: 	assert(sizeof(dtime_t) == 4);
50: 
51: 	return Date::ToString(GetDate(timestamp)) + " " + Time::ToString(GetTime(timestamp));
52: }
53: 
54: date_t Timestamp::GetDate(timestamp_t timestamp) {
55: 	return (date_t)(((int64_t)timestamp) >> 32);
56: }
57: 
58: dtime_t Timestamp::GetTime(timestamp_t timestamp) {
59: 	return (dtime_t)(timestamp & 0xFFFFFFFF);
60: }
61: 
62: timestamp_t Timestamp::FromDatetime(date_t date, dtime_t time) {
63: 	return ((int64_t)date << 32 | (int64_t)time);
64: }
65: 
66: void Timestamp::Convert(timestamp_t date, date_t &out_date, dtime_t &out_time) {
67: 	out_date = GetDate(date);
68: 	out_time = GetTime(date);
69: }
70: 
71: timestamp_t Timestamp::GetCurrentTimestamp() {
72: 	auto in_time_t = std::time(nullptr);
73: 	auto utc = std::gmtime(&in_time_t);
74: 
75: 	// tm_year[0...] considers the amount of years since 1900 and tm_mon considers the amount of months since january
76: 	// tm_mon[0-11]
77: 	auto date = Date::FromDate(utc->tm_year + TM_START_YEAR, utc->tm_mon + 1, utc->tm_mday);
78: 	auto time = Time::FromTime(utc->tm_hour, utc->tm_min, utc->tm_sec);
79: 
80: 	return Timestamp::FromDatetime(date, time);
81: }
82: 
83: int64_t Timestamp::GetEpoch(timestamp_t timestamp) {
84: 	return Date::Epoch(Timestamp::GetDate(timestamp)) + (int64_t)(Timestamp::GetTime(timestamp) / 1000);
85: }
86: 
87: int64_t Timestamp::GetMilliseconds(timestamp_t timestamp) {
88: 	int n = Timestamp::GetTime(timestamp);
89: 	int m = n / 60000;
90: 	return n - m * 60000;
91: }
92: 
93: int64_t Timestamp::GetSeconds(timestamp_t timestamp) {
94: 	int n = Timestamp::GetTime(timestamp);
95: 	int m = n / 60000;
96: 	return (n - m * 60000) / 1000;
97: }
98: 
99: int64_t Timestamp::GetMinutes(timestamp_t timestamp) {
100: 	int n = Timestamp::GetTime(timestamp);
101: 	int h = n / 3600000;
102: 	return (n - h * 3600000) / 60000;
103: }
104: 
105: int64_t Timestamp::GetHours(timestamp_t timestamp) {
106: 	return Timestamp::GetTime(timestamp) / 3600000;
107: }
108: 
109: }
[end of src/common/types/timestamp.cpp]
[start of src/execution/operator/persistent/buffered_csv_reader.cpp]
1: #include "duckdb/execution/operator/persistent/buffered_csv_reader.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/common/file_system.hpp"
5: #include "duckdb/common/gzip_stream.hpp"
6: #include "duckdb/common/string_util.hpp"
7: #include "duckdb/common/vector_operations/vector_operations.hpp"
8: #include "duckdb/execution/operator/persistent/physical_copy_from_file.hpp"
9: #include "duckdb/main/database.hpp"
10: #include "duckdb/parser/column_definition.hpp"
11: #include "duckdb/storage/data_table.hpp"
12: #include "utf8proc_wrapper.hpp"
13: 
14: #include <algorithm>
15: #include <cstring>
16: #include <fstream>
17: #include <queue>
18: 
19: using namespace duckdb;
20: using namespace std;
21: 
22: static char is_newline(char c) {
23: 	return c == '\n' || c == '\r';
24: }
25: 
26: // Helper function to generate column names
27: static string GenerateColumnName(const idx_t total_cols, const idx_t col_number, const string prefix = "column") {
28: 	uint8_t max_digits = total_cols > 10 ? (int)log10((double)total_cols - 1) + 1 : 1;
29: 	uint8_t digits = col_number >= 10 ? (int)log10((double)col_number) + 1 : 1;
30: 	string leading_zeros = string("0", max_digits - digits);
31: 	string value = std::to_string(col_number);
32: 	return string(prefix + leading_zeros + value);
33: }
34: 
35: static string GetLineNumberStr(idx_t linenr, bool linenr_estimated) {
36: 	string estimated = (linenr_estimated ? string(" (estimated)") : string(""));
37: 	return std::to_string(linenr) + estimated;
38: }
39: 
40: TextSearchShiftArray::TextSearchShiftArray() {
41: }
42: 
43: TextSearchShiftArray::TextSearchShiftArray(string search_term) : length(search_term.size()) {
44: 	if (length > 255) {
45: 		throw Exception("Size of delimiter/quote/escape in CSV reader is limited to 255 bytes");
46: 	}
47: 	// initialize the shifts array
48: 	shifts = unique_ptr<uint8_t[]>(new uint8_t[length * 255]);
49: 	memset(shifts.get(), 0, length * 255 * sizeof(uint8_t));
50: 	// iterate over each of the characters in the array
51: 	for (idx_t main_idx = 0; main_idx < length; main_idx++) {
52: 		uint8_t current_char = (uint8_t)search_term[main_idx];
53: 		// now move over all the remaining positions
54: 		for (idx_t i = main_idx; i < length; i++) {
55: 			bool is_match = true;
56: 			// check if the prefix matches at this position
57: 			// if it does, we move to this position after encountering the current character
58: 			for (idx_t j = 0; j < main_idx; j++) {
59: 				if (search_term[i - main_idx + j] != search_term[j]) {
60: 					is_match = false;
61: 				}
62: 			}
63: 			if (!is_match) {
64: 				continue;
65: 			}
66: 			shifts[i * 255 + current_char] = main_idx + 1;
67: 		}
68: 	}
69: }
70: 
71: BufferedCSVReader::BufferedCSVReader(ClientContext &context, BufferedCSVReaderOptions options, vector<SQLType> requested_types)
72: 	: options(options), buffer_size(0), position(0), start(0) {
73: 	source = OpenCSV(context, options);
74: 	Initialize(requested_types);
75: }
76: 
77: BufferedCSVReader::BufferedCSVReader(BufferedCSVReaderOptions options, vector<SQLType> requested_types, unique_ptr<istream> ssource)
78: 	: options(options), source(move(ssource)), buffer_size(0), position(0), start(0) {
79: 	Initialize(requested_types);
80: }
81: 
82: void BufferedCSVReader::Initialize(vector<SQLType> requested_types) {
83: 	if (options.auto_detect) {
84: 		sql_types = SniffCSV(requested_types);
85: 	} else {
86: 		sql_types = requested_types;
87: 	}
88: 
89: 	PrepareComplexParser();
90: 	InitParseChunk(sql_types.size());
91: 	SkipHeader();
92: }
93: 
94: void BufferedCSVReader::PrepareComplexParser() {
95: 	delimiter_search = TextSearchShiftArray(options.delimiter);
96: 	escape_search = TextSearchShiftArray(options.escape);
97: 	quote_search = TextSearchShiftArray(options.quote);
98: }
99: 
100: unique_ptr<istream> BufferedCSVReader::OpenCSV(ClientContext &context, BufferedCSVReaderOptions options) {
101: 	if (!FileSystem::GetFileSystem(context).FileExists(options.file_path)) {
102: 		throw IOException("File \"%s\" not found", options.file_path.c_str());
103: 	}
104: 	unique_ptr<istream> result;
105: 	// decide based on the extension which stream to use
106: 	if (StringUtil::EndsWith(StringUtil::Lower(options.file_path), ".gz")) {
107: 		result = make_unique<GzipStream>(options.file_path);
108: 		plain_file_source = false;
109: 	} else {
110: 		auto csv_local = make_unique<ifstream>();
111: 		csv_local->open(options.file_path);
112: 		result = move(csv_local);
113: 
114: 		// determine filesize
115: 		plain_file_source = true;
116: 		result->seekg(0, result->end);
117: 		file_size = (idx_t)result->tellg();
118: 		result->clear();
119: 		result->seekg(0, result->beg);
120: 	}
121: 	return result;
122: }
123: 
124: void BufferedCSVReader::SkipHeader() {
125: 	for (idx_t i = 0; i < options.skip_rows; i++) {
126: 		// ignore skip rows
127: 		string read_line;
128: 		getline(*source, read_line);
129: 		linenr++;
130: 	}
131: 
132: 	if (options.header) {
133: 		// ignore the first line as a header line
134: 		string read_line;
135: 		getline(*source, read_line);
136: 		linenr++;
137: 	}
138: }
139: 
140: void BufferedCSVReader::ResetBuffer() {
141: 	buffer.reset();
142: 	buffer_size = 0;
143: 	position = 0;
144: 	start = 0;
145: 	cached_buffers.clear();
146: }
147: 
148: void BufferedCSVReader::ResetStream() {
149: 	if (!plain_file_source && StringUtil::EndsWith(StringUtil::Lower(options.file_path), ".gz")) {
150: 		// seeking to the beginning appears to not be supported in all compiler/os-scenarios,
151: 		// so we have to create a new stream source here for now
152: 		source = make_unique<GzipStream>(options.file_path);
153: 	} else {
154: 		source->clear();
155: 		source->seekg(0, source->beg);
156: 	}
157: 	linenr = 0;
158: 	linenr_estimated = false;
159: 	bytes_per_line_avg = 0;
160: 	sample_chunk_idx = 0;
161: 	jumping_samples = false;
162: }
163: 
164: void BufferedCSVReader::ResetParseChunk() {
165: 	bytes_in_chunk = 0;
166: 	parse_chunk.Reset();
167: }
168: 
169: void BufferedCSVReader::InitParseChunk(idx_t num_cols) {
170: 	// adapt not null info
171: 	if (options.force_not_null.size() != num_cols) {
172: 		options.force_not_null.resize(num_cols, false);
173: 	}
174: 
175: 	// destroy previous chunk
176: 	parse_chunk.Destroy();
177: 
178: 	// initialize the parse_chunk with a set of VARCHAR types
179: 	vector<TypeId> varchar_types(num_cols, TypeId::VARCHAR);
180: 	parse_chunk.Initialize(varchar_types);
181: }
182: 
183: void BufferedCSVReader::JumpToBeginning() {
184: 	ResetBuffer();
185: 	ResetStream();
186: 	ResetParseChunk();
187: 	SkipHeader();
188: }
189: 
190: bool BufferedCSVReader::JumpToNextSample() {
191: 	if (source->eof() || sample_chunk_idx >= MAX_SAMPLE_CHUNKS) {
192: 		return false;
193: 	}
194: 
195: 	// update average bytes per line
196: 	double bytes_per_line = bytes_in_chunk / (double)SAMPLE_CHUNK_SIZE;
197: 	bytes_per_line_avg = ((bytes_per_line_avg * sample_chunk_idx) + bytes_per_line) / (sample_chunk_idx + 1);
198: 
199: 	// assess if it makes sense to jump, based on size of the first chunk relative to size of the entire file
200: 	if (sample_chunk_idx == 0) {
201: 		idx_t bytes_first_chunk = bytes_in_chunk;
202: 		double chunks_fit = (file_size / (double)bytes_first_chunk);
203: 		jumping_samples = chunks_fit >= (MAX_SAMPLE_CHUNKS - 1);
204: 	}
205: 
206: 	// if we deal with any other sources than plaintext files, jumping_samples can be tricky. In that case
207: 	// we just read x continuous chunks from the stream TODO: make jumps possible for zipfiles.
208: 	if (!plain_file_source || !jumping_samples) {
209: 		sample_chunk_idx++;
210: 		ResetParseChunk();
211: 		return true;
212: 	}
213: 
214: 	// adjust the value of bytes_in_chunk, based on current state of the buffer
215: 	idx_t remaining_bytes_in_buffer = buffer_size - start;
216: 	bytes_in_chunk -= remaining_bytes_in_buffer;
217: 
218: 	// if none of the previous conditions were met, we can jump
219: 	idx_t partition_size = (idx_t)round(file_size / (double)MAX_SAMPLE_CHUNKS);
220: 
221: 	// calculate offset to end of the current partition
222: 	int64_t offset = partition_size - bytes_in_chunk - remaining_bytes_in_buffer;
223: 	idx_t current_pos = (idx_t)source->tellg();
224: 
225: 	if (current_pos + offset < file_size) {
226: 		// set position in stream and clear failure bits
227: 		source->clear();
228: 		source->seekg(offset, source->cur);
229: 
230: 		// estimate linenr
231: 		linenr += (idx_t)round((offset + remaining_bytes_in_buffer) / bytes_per_line_avg);
232: 		linenr_estimated = true;
233: 	} else {
234: 		// seek backwards from the end in last chunk and hope to catch the end of the file
235: 		// TODO: actually it would be good to make sure that the end of file is being reached, because
236: 		// messy end-lines are quite common. For this case, however, we first need a skip_end detection anyways.
237: 		source->seekg(-bytes_in_chunk, source->end);
238: 
239: 		// estimate linenr
240: 		linenr = (idx_t)round((file_size - bytes_in_chunk) / bytes_per_line_avg);
241: 		linenr_estimated = true;
242: 	}
243: 
244: 	// reset buffers and internal positions
245: 	ResetBuffer();
246: 	ResetParseChunk();
247: 
248: 	// seek beginning of next line
249: 	// FIXME: if this jump ends up in a quoted linebreak, we will have a problem
250: 	string read_line;
251: 	getline(*source, read_line);
252: 	linenr++;
253: 
254: 	sample_chunk_idx++;
255: 
256: 	return true;
257: }
258: 
259: vector<SQLType> BufferedCSVReader::SniffCSV(vector<SQLType> requested_types) {
260: 	// TODO: sniff for uncommon (UTF-8) delimiter variants in first lines and add them to the list
261: 	const vector<string> delim_candidates = {",", "|", ";", "\t"};
262: 	const vector<QuoteRule> quoterule_candidates = {QuoteRule::QUOTES_RFC, QuoteRule::QUOTES_OTHER,
263: 													QuoteRule::NO_QUOTES};
264: 	// quote candiates depend on quote rule
265: 	const vector<vector<string>> quote_candidates_map = {{"\""}, {"\"", "'"}, {""}};
266: 	// escape candiates also depend on quote rule.
267: 	// Note: RFC-conform escapes are handled automatically, and without quotes no escape char is required
268: 	const vector<vector<string>> escape_candidates_map = {{""}, {"\\"}, {""}};
269: 
270: 	vector<BufferedCSVReaderOptions> info_candidates;
271: 	idx_t best_consistent_rows = 0;
272: 	idx_t best_num_cols = 0;
273: 
274: 	// if requested_types were provided, use them already in dialect detection
275: 	// TODO: currently they only serve to solve the edge case of trailing empty delimiters,
276: 	// however, they could be used to solve additional ambigious scenarios.
277: 	sql_types = requested_types;
278: 	// TODO: add a flag to indicate that no option actually worked and default will be used (RFC-4180)
279: 	for (QuoteRule quoterule : quoterule_candidates) {
280: 		vector<string> quote_candidates = quote_candidates_map[static_cast<uint8_t>(quoterule)];
281: 		for (const auto &quote : quote_candidates) {
282: 			for (const auto &delim : delim_candidates) {
283: 				vector<string> escape_candidates = escape_candidates_map[static_cast<uint8_t>(quoterule)];
284: 				for (const auto &escape : escape_candidates) {
285: 					BufferedCSVReaderOptions sniff_info = options;
286: 					sniff_info.delimiter = delim;
287: 					sniff_info.quote = quote;
288: 					sniff_info.escape = escape;
289: 
290: 					options = sniff_info;
291: 					PrepareComplexParser();
292: 
293: 					ResetBuffer();
294: 					ResetStream();
295: 					sniffed_column_counts.clear();
296: 					try {
297: 						ParseCSV(ParserMode::SNIFFING_DIALECT);
298: 					} catch (const ParserException &e) {
299: 						continue;
300: 					}
301: 
302: 					idx_t start_row = 0;
303: 					idx_t consistent_rows = 0;
304: 					idx_t num_cols = 0;
305: 
306: 					for (idx_t row = 0; row < sniffed_column_counts.size(); row++) {
307: 						if (sniffed_column_counts[row] == num_cols) {
308: 							consistent_rows++;
309: 						} else {
310: 							num_cols = sniffed_column_counts[row];
311: 							start_row = row;
312: 							consistent_rows = 1;
313: 						}
314: 					}
315: 
316: 					// some logic
317: 					bool more_values = (consistent_rows > best_consistent_rows && num_cols >= best_num_cols);
318: 					bool single_column_before = best_num_cols < 2 && num_cols > best_num_cols;
319: 					bool rows_consistent = start_row + consistent_rows == sniffed_column_counts.size();
320: 					bool more_than_one_row = (consistent_rows > 1);
321: 					bool more_than_one_column = (num_cols > 1);
322: 					bool start_good = info_candidates.size() > 0 && (start_row <= info_candidates.front().skip_rows);
323: 
324: 					if ((more_values || single_column_before) && rows_consistent) {
325: 						sniff_info.skip_rows = start_row;
326: 						sniff_info.num_cols = num_cols;
327: 						best_consistent_rows = consistent_rows;
328: 						best_num_cols = num_cols;
329: 
330: 						info_candidates.clear();
331: 						info_candidates.push_back(sniff_info);
332: 					} else if (more_than_one_row && more_than_one_column && start_good && rows_consistent) {
333: 						bool same_quote_is_candidate = false;
334: 						for (auto &info_candidate : info_candidates) {
335: 							if (quote.compare(info_candidate.quote) == 0) {
336: 								same_quote_is_candidate = true;
337: 							}
338: 						}
339: 						if (!same_quote_is_candidate) {
340: 							sniff_info.skip_rows = start_row;
341: 							sniff_info.num_cols = num_cols;
342: 							info_candidates.push_back(sniff_info);
343: 						}
344: 					}
345: 				}
346: 			}
347: 		}
348: 	}
349: 
350: 	// then, file was most likely empty and we can do no more
351: 	if (info_candidates.size() < 1) {
352: 		if (requested_types.size() == 0) {
353: 			// no types requested and no types/names could be deduced: default to a single varchar column
354: 			col_names.push_back("col0");
355: 			requested_types.push_back(SQLType::VARCHAR);
356: 		}
357: 		return requested_types;
358: 	}
359: 
360: 	// type candidates, ordered by descending specificity (~ from high to low)
361: 	vector<SQLType> type_candidates = {SQLType::VARCHAR, SQLType::TIMESTAMP, SQLType::DATE,
362: 									   SQLType::TIME,    SQLType::DOUBLE,    /*SQLType::FLOAT,*/ SQLType::BIGINT,
363: 									   SQLType::INTEGER, /* SQLType::SMALLINT, */  /*SQLType::TINYINT,*/ SQLType::BOOLEAN,
364: 									   SQLType::SQLNULL};
365: 
366: 	// check which info candiate leads to minimum amount of non-varchar columns...
367: 	BufferedCSVReaderOptions best_options;
368: 	idx_t min_varchar_cols = best_num_cols + 1;
369: 	vector<vector<SQLType>> best_sql_types_candidates;
370: 	for (auto &info_candidate : info_candidates) {
371: 		options = info_candidate;
372: 		vector<vector<SQLType>> info_sql_types_candidates(options.num_cols, type_candidates);
373: 
374: 		// set all sql_types to VARCHAR so we can do datatype detection based on VARCHAR values
375: 		sql_types.clear();
376: 		sql_types.assign(options.num_cols, SQLType::VARCHAR);
377: 		InitParseChunk(sql_types.size());
378: 
379: 		// detect types in first chunk
380: 		JumpToBeginning();
381: 		ParseCSV(ParserMode::SNIFFING_DATATYPES);
382: 		for (idx_t row = 0; row < parse_chunk.size(); row++) {
383: 			for (idx_t col = 0; col < parse_chunk.column_count(); col++) {
384: 				vector<SQLType> &col_type_candidates = info_sql_types_candidates[col];
385: 				while (col_type_candidates.size() > 1) {
386: 					const auto &sql_type = col_type_candidates.back();
387: 					// try cast from string to sql_type
388: 					auto dummy_val = parse_chunk.GetValue(col, row);
389: 					try {
390: 						dummy_val.CastAs(SQLType::VARCHAR, sql_type, true);
391: 						break;
392: 					} catch (const Exception &e) {
393: 						col_type_candidates.pop_back();
394: 					}
395: 				}
396: 			}
397: 			// reset type detection for second row, because first row could be header,
398: 			// but only do it if csv has more than one line
399: 			if (parse_chunk.size() > 1 && row == 0) {
400: 				info_sql_types_candidates = vector<vector<SQLType>>(options.num_cols, type_candidates);
401: 			}
402: 		}
403: 
404: 		// check number of varchar columns
405: 		idx_t varchar_cols = 0;
406: 		for (idx_t col = 0; col < parse_chunk.column_count(); col++) {
407: 			const auto &col_type = info_sql_types_candidates[col].back();
408: 			if (col_type == SQLType::VARCHAR) {
409: 				varchar_cols++;
410: 			}
411: 		}
412: 
413: 		// it's good if the dialect creates more non-varchar columns, but only if we sacrifice < 40% of best_num_cols.
414: 		if (varchar_cols < min_varchar_cols && parse_chunk.column_count() > (best_num_cols * 0.7)) {
415: 			// we have a new best_info candidate
416: 			best_options = info_candidate;
417: 			min_varchar_cols = varchar_cols;
418: 			best_sql_types_candidates = info_sql_types_candidates;
419: 		}
420: 	}
421: 
422: 	options = best_options;
423: 
424: 	// if data types were provided, exit here if number of columns does not match
425: 	// TODO: we could think about postponing this to see if the csv happens to contain a superset of requested columns
426: 	if (requested_types.size() > 0 && requested_types.size() != options.num_cols) {
427: 		throw ParserException("Error while determining column types: found %lld columns but expected %d", options.num_cols,
428: 							  requested_types.size());
429: 	}
430: 
431: 	// sql_types and parse_chunk have to be in line with new info
432: 	sql_types.clear();
433: 	sql_types.assign(options.num_cols, SQLType::VARCHAR);
434: 	InitParseChunk(sql_types.size());
435: 
436: 	// jump through the rest of the file and continue to refine the sql type guess
437: 	while (JumpToNextSample()) {
438: 		// if jump ends up a bad line, we just skip this chunk
439: 		try {
440: 			ParseCSV(ParserMode::SNIFFING_DATATYPES);
441: 		} catch (const ParserException &e) {
442: 			continue;
443: 		}
444: 		for (idx_t col = 0; col < parse_chunk.column_count(); col++) {
445: 			vector<SQLType> &col_type_candidates = best_sql_types_candidates[col];
446: 			while (col_type_candidates.size() > 1) {
447: 				try {
448: 					const auto &sql_type = col_type_candidates.back();
449: 					// try vector-cast from string to sql_type
450: 					parse_chunk.data[col];
451: 					Vector dummy_result(GetInternalType(sql_type));
452: 					VectorOperations::Cast(parse_chunk.data[col], dummy_result, SQLType::VARCHAR, sql_type,
453: 										   parse_chunk.size(), true);
454: 					break;
455: 				} catch (const Exception &e) {
456: 					col_type_candidates.pop_back();
457: 				}
458: 			}
459: 		}
460: 	}
461: 
462: 	// information for header detection
463: 	bool first_row_consistent = true;
464: 	bool first_row_nulls = true;
465: 
466: 	// parse first row again with knowledge from the rest of the file to check
467: 	// whether first row is consistent with the others or not.
468: 	JumpToBeginning();
469: 	ParseCSV(ParserMode::SNIFFING_DATATYPES);
470: 	if (parse_chunk.size() > 0) {
471: 		for (idx_t col = 0; col < parse_chunk.column_count(); col++) {
472: 			auto dummy_val = parse_chunk.GetValue(col, 0);
473: 			// try cast as SQLNULL
474: 			try {
475: 				dummy_val.CastAs(SQLType::VARCHAR, SQLType::SQLNULL, true);
476: 			} catch (const Exception &e) {
477: 				first_row_nulls = false;
478: 			}
479: 			// try cast to sql_type of column
480: 			vector<SQLType> &col_type_candidates = best_sql_types_candidates[col];
481: 			const auto &sql_type = col_type_candidates.back();
482: 
483: 			try {
484: 				dummy_val.CastAs(SQLType::VARCHAR, sql_type, true);
485: 			} catch (const Exception &e) {
486: 				first_row_consistent = false;
487: 				break;
488: 			}
489: 		}
490: 	}
491: 
492: 	// if all rows are of type string, we will currently make the assumption there is no header.
493: 	// TODO: Do some kind of string-distance based constistency metic between first row and others
494: 	/*bool all_types_string = true;
495: 	for (idx_t col = 0; col < parse_chunk.column_count(); col++) {
496: 		const auto &col_type = best_sql_types_candidates[col].back();
497: 		all_types_string &= (col_type == SQLType::VARCHAR);
498: 	}*/
499: 
500: 	// update parser info, and read, generate & set col_names based on previous findings
501: 	if (!first_row_consistent || first_row_nulls) {
502: 		options.header = true;
503: 		vector<string> t_col_names;
504: 		for (idx_t col = 0; col < parse_chunk.column_count(); col++) {
505: 			const auto &val = parse_chunk.GetValue(col, 0);
506: 			string col_name = val.ToString();
507: 			if (col_name.empty() || val.is_null) {
508: 				col_name = GenerateColumnName(parse_chunk.column_count(), col);
509: 			}
510: 			// We'll keep column names as they appear in the file, no canonicalization
511: 			// col_name = StringUtil::Lower(col_name);
512: 			t_col_names.push_back(col_name);
513: 		}
514: 		for (idx_t col = 0; col < t_col_names.size(); col++) {
515: 			string col_name = t_col_names[col];
516: 			idx_t exists_n_times = std::count(t_col_names.begin(), t_col_names.end(), col_name);
517: 			idx_t exists_n_times_before = std::count(t_col_names.begin(), t_col_names.begin() + col, col_name);
518: 			if (exists_n_times > 1) {
519: 				col_name = GenerateColumnName(exists_n_times, exists_n_times_before, col_name + "_");
520: 			}
521: 			col_names.push_back(col_name);
522: 		}
523: 	} else {
524: 		options.header = false;
525: 		idx_t total_columns = parse_chunk.column_count();
526: 		for (idx_t col = 0; col < total_columns; col++) {
527: 			string column_name = GenerateColumnName(total_columns, col);
528: 			col_names.push_back(column_name);
529: 		}
530: 	}
531: 
532: 	// set sql types
533: 	vector<SQLType> detected_types;
534: 	for (idx_t col = 0; col < best_sql_types_candidates.size(); col++) {
535: 		SQLType d_type = best_sql_types_candidates[col].back();
536: 
537: 		if (requested_types.size() > 0) {
538: 			SQLType r_type = requested_types[col];
539: 
540: 			// check if the detected types are in line with the provided types
541: 			if (r_type != d_type) {
542: 				if (r_type.IsMoreGenericThan(d_type)) {
543: 					d_type = r_type;
544: 				} else {
545: 					throw ParserException(
546: 						"Error while sniffing data type for column '%s': Requested column type %s, detected type %s",
547: 						col_names[col].c_str(), SQLTypeToString(r_type).c_str(), SQLTypeToString(d_type).c_str());
548: 				}
549: 			}
550: 		}
551: 
552: 		detected_types.push_back(d_type);
553: 	}
554: 
555: 	// back to normal
556: 	ResetBuffer();
557: 	ResetStream();
558: 	ResetParseChunk();
559: 	sniffed_column_counts.clear();
560: 
561: 	return detected_types;
562: }
563: 
564: void BufferedCSVReader::ParseComplexCSV(DataChunk &insert_chunk) {
565: 	// used for parsing algorithm
566: 	bool finished_chunk = false;
567: 	idx_t column = 0;
568: 	vector<idx_t> escape_positions;
569: 	uint8_t delimiter_pos = 0, escape_pos = 0, quote_pos = 0;
570: 	idx_t offset = 0;
571: 
572: 	// read values into the buffer (if any)
573: 	if (position >= buffer_size) {
574: 		if (!ReadBuffer(start)) {
575: 			return;
576: 		}
577: 	}
578: 	// start parsing the first value
579: 	start = position;
580: 	goto value_start;
581: value_start:
582: 	/* state: value_start */
583: 	// this state parses the first characters of a value
584: 	offset = 0;
585: 	delimiter_pos = 0;
586: 	quote_pos = 0;
587: 	do {
588: 		idx_t count = 0;
589: 		for (; position < buffer_size; position++) {
590: 			quote_search.Match(quote_pos, buffer[position]);
591: 			delimiter_search.Match(delimiter_pos, buffer[position]);
592: 			count++;
593: 			if (delimiter_pos == options.delimiter.size()) {
594: 				// found a delimiter, add the value
595: 				offset = options.delimiter.size() - 1;
596: 				goto add_value;
597: 			} else if (is_newline(buffer[position])) {
598: 				// found a newline, add the row
599: 				goto add_row;
600: 			}
601: 			if (count > quote_pos) {
602: 				// did not find a quote directly at the start of the value, stop looking for the quote now
603: 				goto normal;
604: 			}
605: 			if (quote_pos == options.quote.size()) {
606: 				// found a quote, go to quoted loop and skip the initial quote
607: 				start += options.quote.size();
608: 				goto in_quotes;
609: 			}
610: 		}
611: 	} while (ReadBuffer(start));
612: 	// file ends while scanning for quote/delimiter, go to final state
613: 	goto final_state;
614: normal:
615: 	/* state: normal parsing state */
616: 	// this state parses the remainder of a non-quoted value until we reach a delimiter or newline
617: 	position++;
618: 	do {
619: 		for (; position < buffer_size; position++) {
620: 			delimiter_search.Match(delimiter_pos, buffer[position]);
621: 			if (delimiter_pos == options.delimiter.size()) {
622: 				offset = options.delimiter.size() - 1;
623: 				goto add_value;
624: 			} else if (is_newline(buffer[position])) {
625: 				goto add_row;
626: 			}
627: 		}
628: 	} while (ReadBuffer(start));
629: 	goto final_state;
630: add_value:
631: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
632: 	// increase position by 1 and move start to the new position
633: 	offset = 0;
634: 	start = ++position;
635: 	if (position >= buffer_size && !ReadBuffer(start)) {
636: 		// file ends right after delimiter, go to final state
637: 		goto final_state;
638: 	}
639: 	goto value_start;
640: add_row : {
641: 	// check type of newline (\r or \n)
642: 	bool carriage_return = buffer[position] == '\r';
643: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
644: 	finished_chunk = AddRow(insert_chunk, column);
645: 	// increase position by 1 and move start to the new position
646: 	offset = 0;
647: 	start = ++position;
648: 	if (position >= buffer_size && !ReadBuffer(start)) {
649: 		// file ends right after newline, go to final state
650: 		goto final_state;
651: 	}
652: 	if (carriage_return) {
653: 		// \r newline, go to special state that parses an optional \n afterwards
654: 		goto carriage_return;
655: 	} else {
656: 		// \n newline, move to value start
657: 		if (finished_chunk) {
658: 			return;
659: 		}
660: 		goto value_start;
661: 	}
662: }
663: in_quotes:
664: 	/* state: in_quotes */
665: 	// this state parses the remainder of a quoted value
666: 	quote_pos = 0;
667: 	escape_pos = 0;
668: 	position++;
669: 	do {
670: 		for (; position < buffer_size; position++) {
671: 			quote_search.Match(quote_pos, buffer[position]);
672: 			escape_search.Match(escape_pos, buffer[position]);
673: 			if (quote_pos == options.quote.size()) {
674: 				goto unquote;
675: 			} else if (escape_pos == options.escape.size()) {
676: 				escape_positions.push_back(position - start - (options.escape.size() - 1));
677: 				goto handle_escape;
678: 			}
679: 		}
680: 	} while (ReadBuffer(start));
681: 	// still in quoted state at the end of the file, error:
682: 	throw ParserException("Error on line %s: unterminated quotes", GetLineNumberStr(linenr, linenr_estimated).c_str());
683: unquote:
684: 	/* state: unquote */
685: 	// this state handles the state directly after we unquote
686: 	// in this state we expect either another quote (entering the quoted state again, and escaping the quote)
687: 	// or a delimiter/newline, ending the current value and moving on to the next value
688: 	delimiter_pos = 0;
689: 	quote_pos = 0;
690: 	position++;
691: 	if (position >= buffer_size && !ReadBuffer(start)) {
692: 		// file ends right after unquote, go to final state
693: 		offset = options.quote.size();
694: 		goto final_state;
695: 	}
696: 	if (is_newline(buffer[position])) {
697: 		// quote followed by newline, add row
698: 		offset = options.quote.size();
699: 		goto add_row;
700: 	}
701: 	do {
702: 		idx_t count = 0;
703: 		for (; position < buffer_size; position++) {
704: 			quote_search.Match(quote_pos, buffer[position]);
705: 			delimiter_search.Match(delimiter_pos, buffer[position]);
706: 			count++;
707: 			if (count > delimiter_pos && count > quote_pos) {
708: 				throw ParserException(
709: 					"Error on line %s: quote should be followed by end of value, end of row or another quote",
710: 					GetLineNumberStr(linenr, linenr_estimated).c_str());
711: 			}
712: 			if (delimiter_pos == options.delimiter.size()) {
713: 				// quote followed by delimiter, add value
714: 				offset = options.quote.size() + options.delimiter.size() - 1;
715: 				goto add_value;
716: 			} else if (quote_pos == options.quote.size() && (options.escape.size() == 0 || options.escape == options.quote)) {
717: 				// quote followed by quote, go back to quoted state and add to escape
718: 				escape_positions.push_back(position - start - (options.quote.size() - 1));
719: 				goto in_quotes;
720: 			}
721: 		}
722: 	} while (ReadBuffer(start));
723: 	throw ParserException("Error on line %s: quote should be followed by end of value, end of row or another quote",
724: 						  GetLineNumberStr(linenr, linenr_estimated).c_str());
725: handle_escape:
726: 	escape_pos = 0;
727: 	quote_pos = 0;
728: 	position++;
729: 	do {
730: 		idx_t count = 0;
731: 		for (; position < buffer_size; position++) {
732: 			quote_search.Match(quote_pos, buffer[position]);
733: 			escape_search.Match(escape_pos, buffer[position]);
734: 			count++;
735: 			if (count > escape_pos && count > quote_pos) {
736: 				throw ParserException("Error on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE",
737: 									  GetLineNumberStr(linenr, linenr_estimated).c_str());
738: 			}
739: 			if (quote_pos == options.quote.size() || escape_pos == options.escape.size()) {
740: 				// found quote or escape: move back to quoted state
741: 				goto in_quotes;
742: 			}
743: 		}
744: 	} while (ReadBuffer(start));
745: 	throw ParserException("Error on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE",
746: 						  GetLineNumberStr(linenr, linenr_estimated).c_str());
747: carriage_return:
748: 	/* state: carriage_return */
749: 	// this stage optionally skips a newline (\n) character, which allows \r\n to be interpreted as a single line
750: 	if (buffer[position] == '\n') {
751: 		// newline after carriage return: skip
752: 		start = ++position;
753: 		if (position >= buffer_size && !ReadBuffer(start)) {
754: 			// file ends right after newline, go to final state
755: 			goto final_state;
756: 		}
757: 	}
758: 	if (finished_chunk) {
759: 		return;
760: 	}
761: 	goto value_start;
762: final_state:
763: 	if (finished_chunk) {
764: 		return;
765: 	}
766: 	if (column > 0 || position > start) {
767: 		// remaining values to be added to the chunk
768: 		AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
769: 		finished_chunk = AddRow(insert_chunk, column);
770: 	}
771: 	// final stage, only reached after parsing the file is finished
772: 	// flush the parsed chunk and finalize parsing
773: 	if (mode == ParserMode::PARSING) {
774: 		Flush(insert_chunk);
775: 	}
776: }
777: 
778: void BufferedCSVReader::ParseSimpleCSV(DataChunk &insert_chunk) {
779: 	// used for parsing algorithm
780: 	bool finished_chunk = false;
781: 	idx_t column = 0;
782: 	idx_t offset = 0;
783: 	vector<idx_t> escape_positions;
784: 
785: 	// read values into the buffer (if any)
786: 	if (position >= buffer_size) {
787: 		if (!ReadBuffer(start)) {
788: 			return;
789: 		}
790: 	}
791: 	// start parsing the first value
792: 	goto value_start;
793: value_start:
794: 	offset = 0;
795: 	/* state: value_start */
796: 	// this state parses the first character of a value
797: 	if (buffer[position] == options.quote[0]) {
798: 		// quote: actual value starts in the next position
799: 		// move to in_quotes state
800: 		start = position + 1;
801: 		goto in_quotes;
802: 	} else {
803: 		// no quote, move to normal parsing state
804: 		start = position;
805: 		goto normal;
806: 	}
807: normal:
808: 	/* state: normal parsing state */
809: 	// this state parses the remainder of a non-quoted value until we reach a delimiter or newline
810: 	do {
811: 		for (; position < buffer_size; position++) {
812: 			if (buffer[position] == options.delimiter[0]) {
813: 				// delimiter: end the value and add it to the chunk
814: 				goto add_value;
815: 			} else if (is_newline(buffer[position])) {
816: 				// newline: add row
817: 				goto add_row;
818: 			}
819: 		}
820: 	} while (ReadBuffer(start));
821: 	// file ends during normal scan: go to end state
822: 	goto final_state;
823: add_value:
824: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
825: 	// increase position by 1 and move start to the new position
826: 	offset = 0;
827: 	start = ++position;
828: 	if (position >= buffer_size && !ReadBuffer(start)) {
829: 		// file ends right after delimiter, go to final state
830: 		goto final_state;
831: 	}
832: 	goto value_start;
833: add_row : {
834: 	// check type of newline (\r or \n)
835: 	bool carriage_return = buffer[position] == '\r';
836: 	AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
837: 	finished_chunk = AddRow(insert_chunk, column);
838: 	// increase position by 1 and move start to the new position
839: 	offset = 0;
840: 	start = ++position;
841: 	if (position >= buffer_size && !ReadBuffer(start)) {
842: 		// file ends right after delimiter, go to final state
843: 		goto final_state;
844: 	}
845: 	if (carriage_return) {
846: 		// \r newline, go to special state that parses an optional \n afterwards
847: 		goto carriage_return;
848: 	} else {
849: 		// \n newline, move to value start
850: 		if (finished_chunk) {
851: 			return;
852: 		}
853: 		goto value_start;
854: 	}
855: }
856: in_quotes:
857: 	/* state: in_quotes */
858: 	// this state parses the remainder of a quoted value
859: 	position++;
860: 	do {
861: 		for (; position < buffer_size; position++) {
862: 			if (buffer[position] == options.quote[0]) {
863: 				// quote: move to unquoted state
864: 				goto unquote;
865: 			} else if (buffer[position] == options.escape[0]) {
866: 				// escape: store the escaped position and move to handle_escape state
867: 				escape_positions.push_back(position - start);
868: 				goto handle_escape;
869: 			}
870: 		}
871: 	} while (ReadBuffer(start));
872: 	// still in quoted state at the end of the file, error:
873: 	throw ParserException("Error on line %s: unterminated quotes", GetLineNumberStr(linenr, linenr_estimated).c_str());
874: unquote:
875: 	/* state: unquote */
876: 	// this state handles the state directly after we unquote
877: 	// in this state we expect either another quote (entering the quoted state again, and escaping the quote)
878: 	// or a delimiter/newline, ending the current value and moving on to the next value
879: 	position++;
880: 	if (position >= buffer_size && !ReadBuffer(start)) {
881: 		// file ends right after unquote, go to final state
882: 		offset = 1;
883: 		goto final_state;
884: 	}
885: 	if (buffer[position] == options.quote[0] && (options.escape.size() == 0 || options.escape[0] == options.quote[0])) {
886: 		// escaped quote, return to quoted state and store escape position
887: 		escape_positions.push_back(position - start);
888: 		goto in_quotes;
889: 	} else if (buffer[position] == options.delimiter[0]) {
890: 		// delimiter, add value
891: 		offset = 1;
892: 		goto add_value;
893: 	} else if (is_newline(buffer[position])) {
894: 		offset = 1;
895: 		goto add_row;
896: 	} else {
897: 		throw ParserException("Error on line %s: quote should be followed by end of value, end of row or another quote",
898: 							  GetLineNumberStr(linenr, linenr_estimated).c_str());
899: 	}
900: handle_escape:
901: 	/* state: handle_escape */
902: 	// escape should be followed by a quote or another escape character
903: 	position++;
904: 	if (position >= buffer_size && !ReadBuffer(start)) {
905: 		throw ParserException("Error on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE",
906: 							  GetLineNumberStr(linenr, linenr_estimated).c_str());
907: 	}
908: 	if (buffer[position] != options.quote[0] && buffer[position] != options.escape[0]) {
909: 		throw ParserException("Error on line %s: neither QUOTE nor ESCAPE is proceeded by ESCAPE",
910: 							  GetLineNumberStr(linenr, linenr_estimated).c_str());
911: 	}
912: 	// escape was followed by quote or escape, go back to quoted state
913: 	goto in_quotes;
914: carriage_return:
915: 	/* state: carriage_return */
916: 	// this stage optionally skips a newline (\n) character, which allows \r\n to be interpreted as a single line
917: 	if (buffer[position] == '\n') {
918: 		// newline after carriage return: skip
919: 		// increase position by 1 and move start to the new position
920: 		start = ++position;
921: 		if (position >= buffer_size && !ReadBuffer(start)) {
922: 			// file ends right after delimiter, go to final state
923: 			goto final_state;
924: 		}
925: 	}
926: 	if (finished_chunk) {
927: 		return;
928: 	}
929: 	goto value_start;
930: final_state:
931: 	if (finished_chunk) {
932: 		return;
933: 	}
934: 
935: 	if (column > 0 || position > start) {
936: 		// remaining values to be added to the chunk
937: 		AddValue(buffer.get() + start, position - start - offset, column, escape_positions);
938: 		finished_chunk = AddRow(insert_chunk, column);
939: 	}
940: 
941: 	// final stage, only reached after parsing the file is finished
942: 	// flush the parsed chunk and finalize parsing
943: 	if (mode == ParserMode::PARSING) {
944: 		Flush(insert_chunk);
945: 	}
946: }
947: 
948: bool BufferedCSVReader::ReadBuffer(idx_t &start) {
949: 	auto old_buffer = move(buffer);
950: 
951: 	// the remaining part of the last buffer
952: 	idx_t remaining = buffer_size - start;
953: 	idx_t buffer_read_size = INITIAL_BUFFER_SIZE;
954: 	while (remaining > buffer_read_size) {
955: 		buffer_read_size *= 2;
956: 	}
957: 	if (remaining + buffer_read_size > MAXIMUM_CSV_LINE_SIZE) {
958: 		throw ParserException("Maximum line size of %llu bytes exceeded!", MAXIMUM_CSV_LINE_SIZE);
959: 	}
960: 	buffer = unique_ptr<char[]>(new char[buffer_read_size + remaining + 1]);
961: 	buffer_size = remaining + buffer_read_size;
962: 	if (remaining > 0) {
963: 		// remaining from last buffer: copy it here
964: 		memcpy(buffer.get(), old_buffer.get() + start, remaining);
965: 	}
966: 	source->read(buffer.get() + remaining, buffer_read_size);
967: 
968: 	idx_t read_count = source->eof() ? source->gcount() : buffer_read_size;
969: 	bytes_in_chunk += read_count;
970: 	buffer_size = remaining + read_count;
971: 	buffer[buffer_size] = '\0';
972: 	if (old_buffer) {
973: 		cached_buffers.push_back(move(old_buffer));
974: 	}
975: 	start = 0;
976: 	position = remaining;
977: 
978: 	return read_count > 0;
979: }
980: 
981: void BufferedCSVReader::ParseCSV(DataChunk &insert_chunk) {
982: 	cached_buffers.clear();
983: 
984: 	ParseCSV(ParserMode::PARSING, insert_chunk);
985: }
986: 
987: void BufferedCSVReader::ParseCSV(ParserMode parser_mode, DataChunk &insert_chunk) {
988: 	mode = parser_mode;
989: 
990: 	if (options.quote.size() <= 1 && options.escape.size() <= 1 && options.delimiter.size() == 1) {
991: 		ParseSimpleCSV(insert_chunk);
992: 	} else {
993: 		ParseComplexCSV(insert_chunk);
994: 	}
995: }
996: 
997: void BufferedCSVReader::AddValue(char *str_val, idx_t length, idx_t &column, vector<idx_t> &escape_positions) {
998: 	if (sql_types.size() > 0 && column == sql_types.size() && length == 0) {
999: 		// skip a single trailing delimiter in last column
1000: 		return;
1001: 	}
1002: 	if (mode == ParserMode::SNIFFING_DIALECT) {
1003: 		column++;
1004: 		return;
1005: 	}
1006: 	if (column >= sql_types.size()) {
1007: 		throw ParserException("Error on line %s: expected %lld values but got %d",
1008: 							  GetLineNumberStr(linenr, linenr_estimated).c_str(), sql_types.size(), column + 1);
1009: 	}
1010: 
1011: 	// insert the line number into the chunk
1012: 	idx_t row_entry = parse_chunk.size();
1013: 
1014: 	str_val[length] = '\0';
1015: 
1016: 	// test against null string
1017: 	if (!options.force_not_null[column] && strcmp(options.null_str.c_str(), str_val) == 0) {
1018: 		FlatVector::SetNull(parse_chunk.data[column], row_entry, true);
1019: 	} else {
1020: 		auto &v = parse_chunk.data[column];
1021: 		auto parse_data = FlatVector::GetData<string_t>(v);
1022: 		if (escape_positions.size() > 0) {
1023: 			// remove escape characters (if any)
1024: 			string old_val = str_val;
1025: 			string new_val = "";
1026: 			idx_t prev_pos = 0;
1027: 			for (idx_t i = 0; i < escape_positions.size(); i++) {
1028: 				idx_t next_pos = escape_positions[i];
1029: 				new_val += old_val.substr(prev_pos, next_pos - prev_pos);
1030: 
1031: 				if (options.escape.size() == 0 || options.escape == options.quote) {
1032: 					prev_pos = next_pos + options.quote.size();
1033: 				} else {
1034: 					prev_pos = next_pos + options.escape.size();
1035: 				}
1036: 			}
1037: 			new_val += old_val.substr(prev_pos, old_val.size() - prev_pos);
1038: 			escape_positions.clear();
1039: 			parse_data[row_entry] = StringVector::AddBlob(v, string_t(new_val));
1040: 		} else {
1041: 			parse_data[row_entry] = string_t(str_val, length);
1042: 		}
1043: 	}
1044: 
1045: 	// move to the next column
1046: 	column++;
1047: }
1048: 
1049: bool BufferedCSVReader::AddRow(DataChunk &insert_chunk, idx_t &column) {
1050: 	if (column < sql_types.size() && mode != ParserMode::SNIFFING_DIALECT) {
1051: 		throw ParserException("Error on line %s: expected %lld values but got %d",
1052: 							  GetLineNumberStr(linenr, linenr_estimated).c_str(), sql_types.size(), column);
1053: 	}
1054: 
1055: 	if (mode == ParserMode::SNIFFING_DIALECT) {
1056: 		sniffed_column_counts.push_back(column);
1057: 
1058: 		if (sniffed_column_counts.size() == SAMPLE_CHUNK_SIZE) {
1059: 			return true;
1060: 		}
1061: 	} else {
1062: 		parse_chunk.SetCardinality(parse_chunk.size() + 1);
1063: 	}
1064: 
1065: 	if (mode == ParserMode::SNIFFING_DATATYPES && parse_chunk.size() == SAMPLE_CHUNK_SIZE) {
1066: 		return true;
1067: 	}
1068: 
1069: 	if (mode == ParserMode::PARSING && parse_chunk.size() == STANDARD_VECTOR_SIZE) {
1070: 		Flush(insert_chunk);
1071: 		return true;
1072: 	}
1073: 
1074: 	column = 0;
1075: 	linenr++;
1076: 	return false;
1077: }
1078: 
1079: void BufferedCSVReader::Flush(DataChunk &insert_chunk) {
1080: 	if (parse_chunk.size() == 0) {
1081: 		return;
1082: 	}
1083: 	// convert the columns in the parsed chunk to the types of the table
1084: 	insert_chunk.SetCardinality(parse_chunk);
1085: 	for (idx_t col_idx = 0; col_idx < sql_types.size(); col_idx++) {
1086: 		if (sql_types[col_idx].id == SQLTypeId::VARCHAR) {
1087: 
1088: 			// target type is varchar: no need to convert
1089: 			// just test that all strings are valid utf-8 strings
1090: 			auto parse_data = FlatVector::GetData<string_t>(parse_chunk.data[col_idx]);
1091: 			for (idx_t i = 0; i < parse_chunk.size(); i++) {
1092: 				if (!FlatVector::IsNull(parse_chunk.data[col_idx], i)) {
1093: 					auto s = parse_data[i];
1094: 					auto utf_type = Utf8Proc::Analyze(s.GetData(), s.GetSize());
1095: 					switch (utf_type) {
1096: 					case UnicodeType::INVALID:
1097: 						throw ParserException("Error on line %s: file is not valid UTF8",
1098: 											  GetLineNumberStr(linenr, linenr_estimated).c_str());
1099: 					case UnicodeType::ASCII:
1100: 						break;
1101: 					case UnicodeType::UNICODE: {
1102: 						auto normie = Utf8Proc::Normalize(s.GetData());
1103: 						parse_data[i] = StringVector::AddString(parse_chunk.data[col_idx], normie);
1104: 						free(normie);
1105: 						break;
1106: 					}
1107: 					}
1108: 				}
1109: 			}
1110: 
1111: 			insert_chunk.data[col_idx].Reference(parse_chunk.data[col_idx]);
1112: 		} else {
1113: 			// target type is not varchar: perform a cast
1114: 			VectorOperations::Cast(parse_chunk.data[col_idx], insert_chunk.data[col_idx], SQLType::VARCHAR,
1115: 								   sql_types[col_idx], parse_chunk.size());
1116: 		}
1117: 	}
1118: 	parse_chunk.Reset();
1119: }
[end of src/execution/operator/persistent/buffered_csv_reader.cpp]
[start of src/function/cast_rules.cpp]
1: #include "duckdb/function/cast_rules.hpp"
2: #include "duckdb/common/exception.hpp"
3: 
4: using namespace duckdb;
5: using namespace std;
6: 
7: //! The target type determines the preferred implicit casts
8: static int64_t TargetTypeCost(SQLType type) {
9: 	switch (type.id) {
10: 	case SQLTypeId::INTEGER:
11: 		return 103;
12: 	case SQLTypeId::BIGINT:
13: 		return 101;
14: 	case SQLTypeId::DOUBLE:
15: 		return 102;
16: 	case SQLTypeId::HUGEINT:
17: 		return 120;
18: 	case SQLTypeId::VARCHAR:
19: 		return 199;
20: 	default:
21: 		return 110;
22: 	}
23: }
24: 
25: static int64_t ImplicitCastTinyint(SQLType to) {
26: 	switch (to.id) {
27: 	case SQLTypeId::SMALLINT:
28: 	case SQLTypeId::INTEGER:
29: 	case SQLTypeId::BIGINT:
30: 	case SQLTypeId::FLOAT:
31: 	case SQLTypeId::DOUBLE:
32: 	case SQLTypeId::DECIMAL:
33: 	case SQLTypeId::HUGEINT:
34: 		return TargetTypeCost(to);
35: 	default:
36: 		return -1;
37: 	}
38: }
39: 
40: static int64_t ImplicitCastSmallint(SQLType to) {
41: 	switch (to.id) {
42: 	case SQLTypeId::INTEGER:
43: 	case SQLTypeId::BIGINT:
44: 	case SQLTypeId::FLOAT:
45: 	case SQLTypeId::DOUBLE:
46: 	case SQLTypeId::DECIMAL:
47: 	case SQLTypeId::HUGEINT:
48: 		return TargetTypeCost(to);
49: 	default:
50: 		return -1;
51: 	}
52: }
53: 
54: static int64_t ImplicitCastInteger(SQLType to) {
55: 	switch (to.id) {
56: 	case SQLTypeId::BIGINT:
57: 	case SQLTypeId::FLOAT:
58: 	case SQLTypeId::DOUBLE:
59: 	case SQLTypeId::DECIMAL:
60: 	case SQLTypeId::HUGEINT:
61: 		return TargetTypeCost(to);
62: 	default:
63: 		return -1;
64: 	}
65: }
66: 
67: static int64_t ImplicitCastBigint(SQLType to) {
68: 	switch (to.id) {
69: 	case SQLTypeId::FLOAT:
70: 	case SQLTypeId::DOUBLE:
71: 	case SQLTypeId::DECIMAL:
72: 	case SQLTypeId::HUGEINT:
73: 		return TargetTypeCost(to);
74: 	default:
75: 		return -1;
76: 	}
77: }
78: 
79: static int64_t ImplicitCastFloat(SQLType to) {
80: 	switch (to.id) {
81: 	case SQLTypeId::DOUBLE:
82: 	case SQLTypeId::DECIMAL:
83: 		return TargetTypeCost(to);
84: 	default:
85: 		return -1;
86: 	}
87: }
88: 
89: static int64_t ImplicitCastDouble(SQLType to) {
90: 	switch (to.id) {
91: 	case SQLTypeId::DECIMAL:
92: 		return TargetTypeCost(to);
93: 	default:
94: 		return -1;
95: 	}
96: }
97: 
98: static int64_t ImplicitCastHugeint(SQLType to) {
99: 	switch (to.id) {
100: 	case SQLTypeId::FLOAT:
101: 	case SQLTypeId::DOUBLE:
102: 		return TargetTypeCost(to);
103: 	default:
104: 		return -1;
105: 	}
106: }
107: 
108: int64_t CastRules::ImplicitCast(SQLType from, SQLType to) {
109: 	if (to.id == SQLTypeId::ANY) {
110: 		// anything can be cast to ANY type for no cost
111: 		return 0;
112: 	}
113: 	if (from.id == SQLTypeId::SQLNULL || from.id == SQLTypeId::UNKNOWN) {
114: 		// NULL expression or parameter expression can be cast to anything
115: 		return TargetTypeCost(to);
116: 	}
117: 	if (from.id == SQLTypeId::BLOB && to.id == SQLTypeId::VARCHAR) {
118: 		//Implicit cast not allowed from BLOB to VARCHAR
119: 		return -1;
120: 	}
121: 	if (to.id == SQLTypeId::VARCHAR) {
122: 		// everything can be cast to VARCHAR, but this cast has a high cost
123: 		return TargetTypeCost(to);
124: 	}
125: 	switch (from.id) {
126: 	case SQLTypeId::TINYINT:
127: 		return ImplicitCastTinyint(to);
128: 	case SQLTypeId::SMALLINT:
129: 		return ImplicitCastSmallint(to);
130: 	case SQLTypeId::INTEGER:
131: 		return ImplicitCastInteger(to);
132: 	case SQLTypeId::BIGINT:
133: 		return ImplicitCastBigint(to);
134: 	case SQLTypeId::HUGEINT:
135: 		return ImplicitCastHugeint(to);
136: 	case SQLTypeId::FLOAT:
137: 		return ImplicitCastFloat(to);
138: 	case SQLTypeId::DOUBLE:
139: 		return ImplicitCastDouble(to);
140: 	default:
141: 		return -1;
142: 	}
143: }
[end of src/function/cast_rules.cpp]
[start of src/function/scalar/date/CMakeLists.txt]
1: add_library_unity(
2:   duckdb_func_date
3:   OBJECT
4:   age.cpp
5:   current.cpp
6:   epoch.cpp
7:   date_trunc.cpp
8:   date_part.cpp)
9: set(ALL_OBJECT_FILES
10:     ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:duckdb_func_date>
11:     PARENT_SCOPE)
[end of src/function/scalar/date/CMakeLists.txt]
[start of src/function/scalar/date/age.cpp]
1: #include "duckdb/function/scalar/date_functions.hpp"
2: #include "duckdb/common/types/interval.hpp"
3: #include "duckdb/common/types/time.hpp"
4: #include "duckdb/common/types/timestamp.hpp"
5: #include "duckdb/common/vector_operations/vector_operations.hpp"
6: #include "duckdb/common/vector_operations/unary_executor.hpp"
7: #include "duckdb/common/vector_operations/binary_executor.hpp"
8: 
9: using namespace std;
10: 
11: namespace duckdb {
12: 
13: static void age_function_standard(DataChunk &input, ExpressionState &state, Vector &result) {
14: 	assert(input.column_count() == 1);
15: 	auto current_timestamp = Timestamp::GetCurrentTimestamp();
16: 
17: 	UnaryExecutor::Execute<timestamp_t, interval_t, true>(input.data[0], result, input.size(), [&](timestamp_t input) {
18: 		return Interval::GetDifference(input, current_timestamp);
19: 	});
20: }
21: 
22: static void age_function(DataChunk &input, ExpressionState &state, Vector &result) {
23: 	assert(input.column_count() == 2);
24: 
25: 	BinaryExecutor::Execute<timestamp_t, timestamp_t, interval_t, true>(
26: 	    input.data[0], input.data[1], result, input.size(), [&](timestamp_t input1, timestamp_t input2) {
27: 		    return Interval::GetDifference(input1, input2);
28: 	    });
29: }
30: 
31: void AgeFun::RegisterFunction(BuiltinFunctions &set) {
32: 	ScalarFunctionSet age("age");
33: 	age.AddFunction(ScalarFunction({SQLType::TIMESTAMP}, SQLType::INTERVAL, age_function_standard));
34: 	age.AddFunction(ScalarFunction({SQLType::TIMESTAMP, SQLType::TIMESTAMP}, SQLType::INTERVAL, age_function));
35: 	set.AddFunction(age);
36: }
37: 
38: } // namespace duckdb
[end of src/function/scalar/date/age.cpp]
[start of src/function/scalar/date/date_part.cpp]
1: #include "duckdb/function/scalar/date_functions.hpp"
2: #include "duckdb/common/enums/date_part_specifier.hpp"
3: #include "duckdb/common/exception.hpp"
4: #include "duckdb/common/types/date.hpp"
5: #include "duckdb/common/types/timestamp.hpp"
6: #include "duckdb/common/vector_operations/vector_operations.hpp"
7: #include "duckdb/common/string_util.hpp"
8: using namespace std;
9: 
10: namespace duckdb {
11: 
12: DatePartSpecifier GetDatePartSpecifier(string specifier) {
13: 	specifier = StringUtil::Lower(specifier);
14: 	if (specifier == "year" || specifier == "y" || specifier == "years") {
15: 		return DatePartSpecifier::YEAR;
16: 	} else if (specifier == "month" || specifier == "mon" || specifier == "months" || specifier == "mons") {
17: 		return DatePartSpecifier::MONTH;
18: 	} else if (specifier == "day" || specifier == "days" || specifier == "d") {
19: 		return DatePartSpecifier::DAY;
20: 	} else if (specifier == "decade" || specifier == "decades") {
21: 		return DatePartSpecifier::DECADE;
22: 	} else if (specifier == "century" || specifier == "centuries") {
23: 		return DatePartSpecifier::CENTURY;
24: 	} else if (specifier == "millennium" || specifier == "millenia") {
25: 		return DatePartSpecifier::MILLENNIUM;
26: 	} else if (specifier == "microseconds" || specifier == "microsecond") {
27: 		return DatePartSpecifier::MICROSECONDS;
28: 	} else if (specifier == "milliseconds" || specifier == "millisecond" || specifier == "ms" || specifier == "msec" || specifier == "msecs") {
29: 		return DatePartSpecifier::MILLISECONDS;
30: 	} else if (specifier == "second" || specifier == "seconds" || specifier == "s") {
31: 		return DatePartSpecifier::SECOND;
32: 	} else if (specifier == "minute" || specifier == "minutes" || specifier == "m") {
33: 		return DatePartSpecifier::MINUTE;
34: 	} else if (specifier == "hour" || specifier == "hours" || specifier == "h") {
35: 		return DatePartSpecifier::HOUR;
36: 	} else if (specifier == "epoch") {
37: 		// seconds since 1970-01-01
38: 		return DatePartSpecifier::EPOCH;
39: 	} else if (specifier == "dow") {
40: 		// day of the week (Sunday = 0, Saturday = 6)
41: 		return DatePartSpecifier::DOW;
42: 	} else if (specifier == "isodow") {
43: 		// isodow (Monday = 1, Sunday = 7)
44: 		return DatePartSpecifier::ISODOW;
45: 	} else if (specifier == "week" || specifier == "weeks" || specifier == "w") {
46: 		// week number
47: 		return DatePartSpecifier::WEEK;
48: 	} else if (specifier == "doy") {
49: 		// day of the year (1-365/366)
50: 		return DatePartSpecifier::DOY;
51: 	} else if (specifier == "quarter") {
52: 		// quarter of the year (1-4)
53: 		return DatePartSpecifier::QUARTER;
54: 	} else {
55: 		throw ConversionException("extract specifier \"%s\" not recognized", specifier.c_str());
56: 	}
57: }
58: 
59: struct YearOperator {
60: 	template <class TA, class TR> static inline TR Operation(TA input) {
61: 		return Date::ExtractYear(input);
62: 	}
63: };
64: 
65: template <> int64_t YearOperator::Operation(timestamp_t input) {
66: 	return YearOperator::Operation<date_t, int64_t>(Timestamp::GetDate(input));
67: }
68: 
69: struct MonthOperator {
70: 	template <class TA, class TR> static inline TR Operation(TA input) {
71: 		return Date::ExtractMonth(input);
72: 	}
73: };
74: 
75: template <> int64_t MonthOperator::Operation(timestamp_t input) {
76: 	return MonthOperator::Operation<date_t, int64_t>(Timestamp::GetDate(input));
77: }
78: 
79: struct DayOperator {
80: 	template <class TA, class TR> static inline TR Operation(TA input) {
81: 		return Date::ExtractDay(input);
82: 	}
83: };
84: 
85: template <> int64_t DayOperator::Operation(timestamp_t input) {
86: 	return DayOperator::Operation<date_t, int64_t>(Timestamp::GetDate(input));
87: }
88: 
89: struct DecadeOperator {
90: 	template <class TA, class TR> static inline TR Operation(TA input) {
91: 		return Date::ExtractYear(input) / 10;
92: 	}
93: };
94: 
95: template <> int64_t DecadeOperator::Operation(timestamp_t input) {
96: 	return DecadeOperator::Operation<date_t, int64_t>(Timestamp::GetDate(input));
97: }
98: 
99: struct CenturyOperator {
100: 	template <class TA, class TR> static inline TR Operation(TA input) {
101: 		return ((Date::ExtractYear(input) - 1) / 100) + 1;
102: 	}
103: };
104: 
105: template <> int64_t CenturyOperator::Operation(timestamp_t input) {
106: 	return CenturyOperator::Operation<date_t, int64_t>(Timestamp::GetDate(input));
107: }
108: 
109: struct MilleniumOperator {
110: 	template <class TA, class TR> static inline TR Operation(TA input) {
111: 		return ((Date::ExtractYear(input) - 1) / 1000) + 1;
112: 	}
113: };
114: 
115: template <> int64_t MilleniumOperator::Operation(timestamp_t input) {
116: 	return MilleniumOperator::Operation<date_t, int64_t>(Timestamp::GetDate(input));
117: }
118: 
119: struct QuarterOperator {
120: 	template <class TA, class TR> static inline TR Operation(TA input) {
121: 		return Date::ExtractMonth(input) / 4;
122: 	}
123: };
124: 
125: template <> int64_t QuarterOperator::Operation(timestamp_t input) {
126: 	return QuarterOperator::Operation<date_t, int64_t>(Timestamp::GetDate(input));
127: }
128: 
129: struct DayOfWeekOperator {
130: 	template <class TA, class TR> static inline TR Operation(TA input) {
131: 		// day of the week (Sunday = 0, Saturday = 6)
132: 		// turn sunday into 0 by doing mod 7
133: 		return Date::ExtractISODayOfTheWeek(input) % 7;
134: 	}
135: };
136: 
137: template <> int64_t DayOfWeekOperator::Operation(timestamp_t input) {
138: 	return DayOfWeekOperator::Operation<date_t, int64_t>(Timestamp::GetDate(input));
139: }
140: 
141: struct ISODayOfWeekOperator {
142: 	template <class TA, class TR> static inline TR Operation(TA input) {
143: 		// isodow (Monday = 1, Sunday = 7)
144: 		return Date::ExtractISODayOfTheWeek(input);
145: 	}
146: };
147: 
148: template <> int64_t ISODayOfWeekOperator::Operation(timestamp_t input) {
149: 	return ISODayOfWeekOperator::Operation<date_t, int64_t>(Timestamp::GetDate(input));
150: }
151: 
152: struct DayOfYearOperator {
153: 	template <class TA, class TR> static inline TR Operation(TA input) {
154: 		return Date::ExtractDayOfTheYear(input);
155: 	}
156: };
157: 
158: template <> int64_t DayOfYearOperator::Operation(timestamp_t input) {
159: 	return DayOfYearOperator::Operation<date_t, int64_t>(Timestamp::GetDate(input));
160: }
161: 
162: struct WeekOperator {
163: 	template <class TA, class TR> static inline TR Operation(TA input) {
164: 		return Date::ExtractWeekNumber(input);
165: 	}
166: };
167: 
168: template <> int64_t WeekOperator::Operation(timestamp_t input) {
169: 	return WeekOperator::Operation<date_t, int64_t>(Timestamp::GetDate(input));
170: }
171: 
172: struct YearWeekOperator {
173: 	template <class TA, class TR> static inline TR Operation(TA input) {
174: 		return YearOperator::Operation<TA, TR>(input) * 100 + WeekOperator::Operation<TA, TR>(input);
175: 	}
176: };
177: 
178: struct EpochOperator {
179: 	template <class TA, class TR> static inline TR Operation(TA input) {
180: 		return Date::Epoch(input);
181: 	}
182: };
183: 
184: template <> int64_t EpochOperator::Operation(timestamp_t input) {
185: 	return Timestamp::GetEpoch(input);
186: }
187: 
188: struct MicrosecondsOperator {
189: 	template <class TA, class TR> static inline TR Operation(TA input) {
190: 		return 0;
191: 	}
192: };
193: 
194: template <> int64_t MicrosecondsOperator::Operation(timestamp_t input) {
195: 	return Timestamp::GetMilliseconds(input) * 1000;
196: }
197: 
198: struct MillisecondsOperator {
199: 	template <class TA, class TR> static inline TR Operation(TA input) {
200: 		return 0;
201: 	}
202: };
203: 
204: template <> int64_t MillisecondsOperator::Operation(timestamp_t input) {
205: 	return Timestamp::GetMilliseconds(input);
206: }
207: 
208: struct SecondsOperator {
209: 	template <class TA, class TR> static inline TR Operation(TA input) {
210: 		return 0;
211: 	}
212: };
213: 
214: template <> int64_t SecondsOperator::Operation(timestamp_t input) {
215: 	return Timestamp::GetSeconds(input);
216: }
217: 
218: struct MinutesOperator {
219: 	template <class TA, class TR> static inline TR Operation(TA input) {
220: 		return 0;
221: 	}
222: };
223: 
224: template <> int64_t MinutesOperator::Operation(timestamp_t input) {
225: 	return Timestamp::GetMinutes(input);
226: }
227: 
228: struct HoursOperator {
229: 	template <class TA, class TR> static inline TR Operation(TA input) {
230: 		return 0;
231: 	}
232: };
233: 
234: template <> int64_t HoursOperator::Operation(timestamp_t input) {
235: 	return Timestamp::GetHours(input);
236: }
237: 
238: template <class T> static int64_t extract_element(DatePartSpecifier type, T element) {
239: 	switch (type) {
240: 	case DatePartSpecifier::YEAR:
241: 		return YearOperator::Operation<T, int64_t>(element);
242: 	case DatePartSpecifier::MONTH:
243: 		return MonthOperator::Operation<T, int64_t>(element);
244: 	case DatePartSpecifier::DAY:
245: 		return DayOperator::Operation<T, int64_t>(element);
246: 	case DatePartSpecifier::DECADE:
247: 		return DecadeOperator::Operation<T, int64_t>(element);
248: 	case DatePartSpecifier::CENTURY:
249: 		return CenturyOperator::Operation<T, int64_t>(element);
250: 	case DatePartSpecifier::MILLENNIUM:
251: 		return MilleniumOperator::Operation<T, int64_t>(element);
252: 	case DatePartSpecifier::QUARTER:
253: 		return QuarterOperator::Operation<T, int64_t>(element);
254: 	case DatePartSpecifier::DOW:
255: 		return DayOfWeekOperator::Operation<T, int64_t>(element);
256: 	case DatePartSpecifier::ISODOW:
257: 		return ISODayOfWeekOperator::Operation<T, int64_t>(element);
258: 	case DatePartSpecifier::DOY:
259: 		return DayOfYearOperator::Operation<T, int64_t>(element);
260: 	case DatePartSpecifier::WEEK:
261: 		return WeekOperator::Operation<T, int64_t>(element);
262: 	case DatePartSpecifier::EPOCH:
263: 		return EpochOperator::Operation<T, int64_t>(element);
264: 	case DatePartSpecifier::MICROSECONDS:
265: 		return MicrosecondsOperator::Operation<T, int64_t>(element);
266: 	case DatePartSpecifier::MILLISECONDS:
267: 		return MillisecondsOperator::Operation<T, int64_t>(element);
268: 	case DatePartSpecifier::SECOND:
269: 		return SecondsOperator::Operation<T, int64_t>(element);
270: 	case DatePartSpecifier::MINUTE:
271: 		return MinutesOperator::Operation<T, int64_t>(element);
272: 	case DatePartSpecifier::HOUR:
273: 		return HoursOperator::Operation<T, int64_t>(element);
274: 	default:
275: 		throw NotImplementedException("Specifier type not implemented");
276: 	}
277: }
278: 
279: struct DatePartOperator {
280: 	template <class TA, class TB, class TR> static inline TR Operation(TA specifier, TB date) {
281: 		return extract_element<TB>(GetDatePartSpecifier(specifier.GetString()), date);
282: 	}
283: };
284: 
285: template <class OP> static void AddDatePartOperator(BuiltinFunctions &set, string name) {
286: 	ScalarFunctionSet operator_set(name);
287: 	operator_set.AddFunction(
288: 	    ScalarFunction({SQLType::DATE}, SQLType::BIGINT, ScalarFunction::UnaryFunction<date_t, int64_t, OP>));
289: 	operator_set.AddFunction(
290: 	    ScalarFunction({SQLType::TIMESTAMP}, SQLType::BIGINT, ScalarFunction::UnaryFunction<timestamp_t, int64_t, OP>));
291: 	set.AddFunction(operator_set);
292: }
293: 
294: struct LastDayOperator {
295: 	template <class TA, class TR> static inline TR Operation(TA input) {
296: 		int32_t yyyy, mm, dd;
297: 		Date::Convert(input, yyyy, mm, dd);
298: 		yyyy += (mm / 12);
299: 		mm %= 12;
300: 		++mm;
301: 		return Date::FromDate(yyyy, mm, 1) - 1;
302: 	}
303: };
304: 
305: template <> date_t LastDayOperator::Operation(timestamp_t input) {
306: 	return LastDayOperator::Operation<date_t, date_t>(Timestamp::GetDate(input));
307: }
308: 
309: static string_t s_monthNames[] = {"January", "February", "March",     "April",   "May",      "June",
310:                                   "July",    "August",   "September", "October", "November", "December"};
311: 
312: struct MonthNameOperator {
313: 	template <class TA, class TR> static inline TR Operation(TA input) {
314: 		return s_monthNames[MonthOperator::Operation<TA, int64_t>(input) - 1];
315: 	}
316: };
317: 
318: static string_t s_dayNames[] = {"Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"};
319: 
320: struct DayNameOperator {
321: 	template <class TA, class TR> static inline TR Operation(TA input) {
322: 		return s_dayNames[DayOfWeekOperator::Operation<TA, int64_t>(input)];
323: 	}
324: };
325: 
326: void DatePartFun::RegisterFunction(BuiltinFunctions &set) {
327: 	// register the individual operators
328: 	AddDatePartOperator<YearOperator>(set, "year");
329: 	AddDatePartOperator<MonthOperator>(set, "month");
330: 	AddDatePartOperator<DayOperator>(set, "day");
331: 	AddDatePartOperator<DecadeOperator>(set, "decade");
332: 	AddDatePartOperator<CenturyOperator>(set, "century");
333: 	AddDatePartOperator<MilleniumOperator>(set, "millenium");
334: 	AddDatePartOperator<QuarterOperator>(set, "quarter");
335: 	AddDatePartOperator<DayOfWeekOperator>(set, "dayofweek");
336: 	AddDatePartOperator<ISODayOfWeekOperator>(set, "isodow");
337: 	AddDatePartOperator<DayOfYearOperator>(set, "dayofyear");
338: 	AddDatePartOperator<WeekOperator>(set, "week");
339: 	AddDatePartOperator<EpochOperator>(set, "epoch");
340: 	AddDatePartOperator<MicrosecondsOperator>(set, "microsecond");
341: 	AddDatePartOperator<MillisecondsOperator>(set, "millisecond");
342: 	AddDatePartOperator<SecondsOperator>(set, "second");
343: 	AddDatePartOperator<MinutesOperator>(set, "minute");
344: 	AddDatePartOperator<HoursOperator>(set, "hour");
345: 
346: 	//  register combinations
347: 	AddDatePartOperator<YearWeekOperator>(set, "yearweek");
348: 
349: 	//  register various aliases
350: 	AddDatePartOperator<DayOperator>(set, "dayofmonth");
351: 	AddDatePartOperator<DayOfWeekOperator>(set, "weekday");
352: 	AddDatePartOperator<WeekOperator>(set, "weekofyear"); //  Note that WeekOperator is ISO-8601, not US
353: 
354: 	//  register the last_day function
355: 	ScalarFunctionSet last_day("last_day");
356: 	last_day.AddFunction(ScalarFunction({SQLType::DATE}, SQLType::DATE,
357: 	                                    ScalarFunction::UnaryFunction<date_t, date_t, LastDayOperator, true>));
358: 	last_day.AddFunction(ScalarFunction({SQLType::TIMESTAMP}, SQLType::DATE,
359: 	                                    ScalarFunction::UnaryFunction<timestamp_t, date_t, LastDayOperator, true>));
360: 	set.AddFunction(last_day);
361: 
362: 	//  register the monthname function
363: 	ScalarFunctionSet monthname("monthname");
364: 	monthname.AddFunction(ScalarFunction({SQLType::DATE}, SQLType::VARCHAR,
365: 	                                     ScalarFunction::UnaryFunction<date_t, string_t, MonthNameOperator, true>));
366: 	monthname.AddFunction(
367: 	    ScalarFunction({SQLType::TIMESTAMP}, SQLType::VARCHAR,
368: 	                   ScalarFunction::UnaryFunction<timestamp_t, string_t, MonthNameOperator, true>));
369: 	set.AddFunction(monthname);
370: 
371: 	//  register the dayname function
372: 	ScalarFunctionSet dayname("dayname");
373: 	dayname.AddFunction(ScalarFunction({SQLType::DATE}, SQLType::VARCHAR,
374: 	                                   ScalarFunction::UnaryFunction<date_t, string_t, DayNameOperator, true>));
375: 	dayname.AddFunction(ScalarFunction({SQLType::TIMESTAMP}, SQLType::VARCHAR,
376: 	                                   ScalarFunction::UnaryFunction<timestamp_t, string_t, DayNameOperator, true>));
377: 	set.AddFunction(dayname);
378: 
379: 	// finally the actual date_part function
380: 	ScalarFunctionSet date_part("date_part");
381: 	date_part.AddFunction(
382: 	    ScalarFunction({SQLType::VARCHAR, SQLType::DATE}, SQLType::BIGINT,
383: 	                   ScalarFunction::BinaryFunction<string_t, date_t, int64_t, DatePartOperator, true>));
384: 	date_part.AddFunction(
385: 	    ScalarFunction({SQLType::VARCHAR, SQLType::TIMESTAMP}, SQLType::BIGINT,
386: 	                   ScalarFunction::BinaryFunction<string_t, timestamp_t, int64_t, DatePartOperator, true>));
387: 	set.AddFunction(date_part);
388: 	date_part.name = "datepart";
389: 	set.AddFunction(date_part);
390: }
391: 
392: } // namespace duckdb
[end of src/function/scalar/date/date_part.cpp]
[start of src/function/scalar/date_functions.cpp]
1: #include "duckdb/function/scalar/date_functions.hpp"
2: 
3: using namespace duckdb;
4: using namespace std;
5: 
6: void BuiltinFunctions::RegisterDateFunctions() {
7: 	Register<AgeFun>();
8: 	Register<DatePartFun>();
9: 	Register<DateTruncFun>();
10: 	Register<CurrentTimeFun>();
11: 	Register<CurrentDateFun>();
12: 	Register<CurrentTimestampFun>();
13: 	Register<EpochFun>();
14: }
[end of src/function/scalar/date_functions.cpp]
[start of src/function/table/copy_csv.cpp]
1: #include "duckdb/function/table/read_csv.hpp"
2: #include "duckdb/execution/operator/persistent/buffered_csv_reader.hpp"
3: #include "duckdb/common/serializer/buffered_serializer.hpp"
4: #include "duckdb/function/copy_function.hpp"
5: #include "duckdb/parser/parsed_data/copy_info.hpp"
6: #include "duckdb/common/string_util.hpp"
7: #include "duckdb/common/file_system.hpp"
8: #include "duckdb/common/types/string_type.hpp"
9: #include "duckdb/common/vector_operations/vector_operations.hpp"
10: 
11: using namespace std;
12: 
13: namespace duckdb {
14: 
15: struct BaseCSVData : public FunctionData {
16: 	BaseCSVData(string file_path) :
17: 		file_path(move(file_path)) {}
18: 
19: 	//! The file path of the CSV file to read or write
20: 	string file_path;
21: 	//! Whether or not to write a header in the file
22: 	bool header = false;
23: 	//! Delimiter to separate columns within each line
24: 	string delimiter = ",";
25: 	//! Quote used for columns that contain reserved characters, e.g., delimiter
26: 	string quote = "\"";
27: 	//! Escape character to escape quote character
28: 	string escape;
29: 	//! Specifies the string that represents a null value
30: 	string null_str;
31: 	//! Whether or not the options are specified; if not we default to auto detect
32: 	bool is_auto_detect = true;
33: 
34: 	void Finalize();
35: };
36: 
37: struct WriteCSVData : public BaseCSVData {
38: 	WriteCSVData(string file_path, vector<SQLType> sql_types, vector<string> names) :
39: 		BaseCSVData(move(file_path)), sql_types(move(sql_types)), names(move(names)) {}
40: 
41: 	//! The SQL types to write
42: 	vector<SQLType> sql_types;
43: 	//! The column names of the columns to write
44: 	vector<string> names;
45: 	//! True, if column with that index must be quoted
46: 	vector<bool> force_quote;
47: 	//! The newline string to write
48: 	string newline = "\n";
49: 	//! Whether or not we are writing a simple CSV (delimiter, quote and escape are all 1 byte in length)
50: 	bool is_simple;
51: 	//! The size of the CSV file (in bytes) that we buffer before we flush it to disk
52: 	idx_t flush_size = 4096 * 8;
53: };
54: 
55: struct ReadCSVData : public BaseCSVData {
56: 	ReadCSVData(string file_path, vector<SQLType> sql_types) :
57: 		BaseCSVData(move(file_path)), sql_types(move(sql_types)) {}
58: 
59: 	//! The expected SQL types to read
60: 	vector<SQLType> sql_types;
61: 	//! True, if column with that index must be quoted
62: 	vector<bool> force_not_null;
63: };
64: 
65: void SubstringDetection(string &str_1, string &str_2, string name_str_1, string name_str_2) {
66: 	if (str_1.find(str_2) != string::npos || str_2.find(str_1) != std::string::npos) {
67: 		throw BinderException("COPY " + name_str_1 + " must not appear in the " + name_str_2 +
68: 		                " specification and vice versa");
69: 	}
70: }
71: 
72: static bool ParseBoolean(vector<Value> &set) {
73: 	if (set.size() == 0) {
74: 		// no option specified: default to true
75: 		return true;
76: 	}
77: 	if (set.size() > 1) {
78: 		throw BinderException("Expected a single argument as a boolean value (e.g. TRUE or 1)");
79: 	}
80: 	if (set[0].type == TypeId::FLOAT || set[0].type == TypeId::DOUBLE) {
81: 		throw BinderException("Expected a boolean value (e.g. TRUE or 1)");
82: 	}
83: 	return set[0].CastAs(TypeId::BOOL).value_.boolean;
84: }
85: 
86: static string ParseString(vector<Value> &set) {
87: 	if (set.size() != 1) {
88: 		// no option specified or multiple options specified
89: 		throw BinderException("Expected a single argument as a string value");
90: 	}
91: 	if (set[0].type != TypeId::VARCHAR) {
92: 		throw BinderException("Expected a string argument!");
93: 	}
94: 	return set[0].str_value;
95: }
96: 
97: //===--------------------------------------------------------------------===//
98: // Bind
99: //===--------------------------------------------------------------------===//
100: static bool ParseBaseOption(BaseCSVData &bind_data, string &loption, vector<Value> &set) {
101: 	if (StringUtil::StartsWith(loption, "delim") || StringUtil::StartsWith(loption, "sep")) {
102: 		bind_data.delimiter = ParseString(set);
103: 		bind_data.is_auto_detect = false;
104: 		if (bind_data.delimiter.length() == 0) {
105: 			throw BinderException("QUOTE must not be empty");
106: 		}
107: 	} else if (loption == "quote") {
108: 		bind_data.quote = ParseString(set);
109: 		bind_data.is_auto_detect = false;
110: 		if (bind_data.quote.length() == 0) {
111: 			throw BinderException("QUOTE must not be empty");
112: 		}
113: 	} else if (loption == "escape") {
114: 		bind_data.escape = ParseString(set);
115: 		bind_data.is_auto_detect = false;
116: 		if (bind_data.escape.length() == 0) {
117: 			throw BinderException("ESCAPE must not be empty");
118: 		}
119: 	} else if (loption == "header") {
120: 		bind_data.header = ParseBoolean(set);
121: 		bind_data.is_auto_detect = false;
122: 	} else if (loption == "null") {
123: 		bind_data.null_str = ParseString(set);
124: 		bind_data.is_auto_detect = false;
125: 	} else if (loption == "encoding") {
126: 		auto encoding = StringUtil::Lower(ParseString(set));
127: 		if (encoding != "utf8" && encoding != "utf-8") {
128: 			throw BinderException("Copy is only supported for UTF-8 encoded files, ENCODING 'UTF-8'");
129: 		}
130: 	} else {
131: 		// unrecognized option in base CSV
132: 		return false;
133: 	}
134: 	return true;
135: }
136: 
137: void BaseCSVData::Finalize() {
138: 	// verify that the options are correct in the final pass
139: 	if (escape.empty()) {
140: 		escape = quote;
141: 	}
142: 	// escape and delimiter must not be substrings of each other
143: 	SubstringDetection(delimiter, escape, "DELIMITER", "ESCAPE");
144: 	// delimiter and quote must not be substrings of each other
145: 	SubstringDetection(quote, delimiter, "DELIMITER", "QUOTE");
146: 	// escape and quote must not be substrings of each other (but can be the same)
147: 	if (quote != escape) {
148: 		SubstringDetection(quote, escape, "QUOTE", "ESCAPE");
149: 	}
150: 	if (null_str != "") {
151: 		// null string and delimiter must not be substrings of each other
152: 		SubstringDetection(delimiter, null_str, "DELIMITER", "NULL");
153: 		// quote/escape and nullstr must not be substrings of each other
154: 		SubstringDetection(quote, null_str, "QUOTE", "NULL");
155: 		SubstringDetection(escape, null_str, "ESCAPE", "NULL");
156: 	}
157: }
158: 
159: static vector<bool> ParseColumnList(vector<Value> &set, vector<string> &names) {
160: 	vector<bool> result;
161: 	if (set.size() == 0) {
162: 		throw BinderException("Expected a column list or * as parameter");
163: 	}
164: 	if (set.size() == 1 && set[0].type == TypeId::VARCHAR && set[0].str_value == "*") {
165: 		// *, force_not_null on all columns
166: 		result.resize(names.size(), true);
167: 	} else {
168: 		// list of options: parse the list
169: 		unordered_map<string, bool> option_map;
170: 		for(idx_t i = 0; i < set.size(); i++) {
171: 			option_map[set[i].ToString()] = false;
172: 		}
173: 		result.resize(names.size(), false);
174: 		for(idx_t i = 0; i < names.size(); i++) {
175: 			auto entry = option_map.find(names[i]);
176: 			if (entry != option_map.end()) {
177: 				result[i] = true;
178: 				entry->second = true;
179: 			}
180: 		}
181: 		for(auto entry : option_map) {
182: 			if (!entry.second) {
183: 				throw BinderException("Column %s not found in table", entry.first.c_str());
184: 			}
185: 		}
186: 	}
187: 	return result;
188: }
189: 
190: static unique_ptr<FunctionData> write_csv_bind(ClientContext &context, CopyInfo &info, vector<string> &names,
191:                                            vector<SQLType> &sql_types) {
192: 	auto bind_data = make_unique<WriteCSVData>(info.file_path, sql_types, names);
193: 
194: 	// check all the options in the copy info
195: 	for(auto &option : info.options) {
196: 		auto loption = StringUtil::Lower(option.first);
197: 		auto &set = option.second;
198: 		if (ParseBaseOption(*bind_data, loption, set)) {
199: 			// parsed option in base CSV options: continue
200: 			continue;
201: 		} else if (loption == "force_quote") {
202: 			bind_data->force_quote = ParseColumnList(set, names);
203: 		} else {
204: 			throw NotImplementedException("Unrecognized option for CSV: %s", option.first.c_str());
205: 		}
206: 	}
207: 	// verify the parsed options
208: 	if (bind_data->force_quote.size() == 0) {
209: 		// no FORCE_QUOTE specified: initialize to false
210: 		bind_data->force_quote.resize(names.size(), false);
211: 	}
212: 	bind_data->Finalize();
213: 	bind_data->is_simple = bind_data->delimiter.size() == 1 && bind_data->escape.size() == 1 && bind_data->quote.size() == 1;
214: 	return move(bind_data);
215: }
216: 
217: static unique_ptr<FunctionData> read_csv_bind(ClientContext &context, CopyInfo &info, vector<string> &expected_names, vector<SQLType> &expected_types) {
218: 	auto bind_data = make_unique<ReadCSVData>(info.file_path, expected_types);
219: 
220: 	// check all the options in the copy info
221: 	for(auto &option : info.options) {
222: 		auto loption = StringUtil::Lower(option.first);
223: 		auto &set = option.second;
224: 		if (ParseBaseOption(*bind_data, loption, set)) {
225: 			// parsed option in base CSV options: continue
226: 			continue;
227: 		} else if (loption == "force_not_null") {
228: 			bind_data->force_not_null = ParseColumnList(set, expected_names);
229: 		} else {
230: 			throw NotImplementedException("Unrecognized option for CSV: %s", option.first.c_str());
231: 		}
232: 	}
233: 	// verify the parsed options
234: 	if (bind_data->force_not_null.size() == 0) {
235: 		// no FORCE_QUOTE specified: initialize to false
236: 		bind_data->force_not_null.resize(expected_types.size(), false);
237: 	}
238: 	bind_data->Finalize();
239: 	return move(bind_data);
240: }
241: 
242: static unique_ptr<FunctionData> read_csv_auto_bind(ClientContext &context, CopyInfo &info, vector<string> &expected_names, vector<SQLType> &expected_types) {
243: 	auto bind_data = make_unique<ReadCSVData>(info.file_path, expected_types);
244: 
245: 	for(auto &option : info.options) {
246: 		auto loption = StringUtil::Lower(option.first);
247: 		// auto &set = option.second;
248: 		// CSV auto accepts no options!
249: 		throw NotImplementedException("Unrecognized option for CSV_AUTO: %s", option.first.c_str());
250: 	}
251: 
252: 	bind_data->Finalize();
253: 	return move(bind_data);
254: }
255: 
256: //===--------------------------------------------------------------------===//
257: // Helper writing functions
258: //===--------------------------------------------------------------------===//
259: static string AddEscapes(string &to_be_escaped, string escape, string val) {
260: 	idx_t i = 0;
261: 	string new_val = "";
262: 	idx_t found = val.find(to_be_escaped);
263: 
264: 	while (found != string::npos) {
265: 		while (i < found) {
266: 			new_val += val[i];
267: 			i++;
268: 		}
269: 		new_val += escape;
270: 		found = val.find(to_be_escaped, found + escape.length());
271: 	}
272: 	while (i < val.length()) {
273: 		new_val += val[i];
274: 		i++;
275: 	}
276: 	return new_val;
277: }
278: 
279: static bool RequiresQuotes(WriteCSVData &options, const char *str, idx_t len) {
280: 	// check if the string is equal to the null string
281: 	if (len == options.null_str.size() && memcmp(str, options.null_str.c_str(), len) == 0) {
282: 		return true;
283: 	}
284: 	if (options.is_simple) {
285: 		// simple CSV: check for newlines, quotes and delimiter all at once
286: 		for (idx_t i = 0; i < len; i++) {
287: 			if (str[i] == '\n' || str[i] == '\r' || str[i] == options.quote[0] || str[i] == options.delimiter[0]) {
288: 				// newline, write a quoted string
289: 				return true;
290: 			}
291: 		}
292: 		// no newline, quote or delimiter in the string
293: 		// no quoting or escaping necessary
294: 		return false;
295: 	} else {
296: 		// CSV with complex quotes/delimiter (multiple bytes)
297: 
298: 		// first check for \n, \r, \n\r in string
299: 		for (idx_t i = 0; i < len; i++) {
300: 			if (str[i] == '\n' || str[i] == '\r') {
301: 				// newline, write a quoted string
302: 				return true;
303: 			}
304: 		}
305: 
306: 		// check for delimiter
307: 		if (strstr(str, options.delimiter.c_str())) {
308: 			return true;
309: 		}
310: 		// check for quote
311: 		if (strstr(str, options.quote.c_str())) {
312: 			return true;
313: 		}
314: 		return false;
315: 	}
316: }
317: 
318: static void WriteQuotedString(Serializer &serializer, WriteCSVData &options, const char *str, idx_t len, bool force_quote) {
319: 	if (!force_quote) {
320: 		// force quote is disabled: check if we need to add quotes anyway
321: 		force_quote = RequiresQuotes(options, str, len);
322: 	}
323: 	if (force_quote) {
324: 		// quoting is enabled: we might need to escape things in the string
325: 		bool requires_escape = false;
326: 		if (options.is_simple) {
327: 			// simple CSV
328: 			// do a single loop to check for a quote or escape value
329: 			for (idx_t i = 0; i < len; i++) {
330: 				if (str[i] == options.quote[0] || str[i] == options.escape[0]) {
331: 					requires_escape = true;
332: 					break;
333: 				}
334: 			}
335: 		} else {
336: 			// complex CSV
337: 			// check for quote or escape separately
338: 			if (strstr(str, options.quote.c_str())) {
339: 				requires_escape = true;
340: 			} else if (strstr(str, options.escape.c_str())) {
341: 				requires_escape = true;
342: 			}
343: 		}
344: 		if (!requires_escape) {
345: 			// fast path: no need to escape anything
346: 			serializer.WriteBufferData(options.quote);
347: 			serializer.WriteData((const_data_ptr_t) str, len);
348: 			serializer.WriteBufferData(options.quote);
349: 			return;
350: 		}
351: 
352: 		// slow path: need to add escapes
353: 		string new_val(str, len);
354: 		new_val = AddEscapes(options.escape, options.escape, new_val);
355: 		if (options.escape != options.quote) {
356: 			// need to escape quotes separately
357: 			new_val = AddEscapes(options.quote, options.escape, new_val);
358: 		}
359: 		serializer.WriteBufferData(options.quote);
360: 		serializer.WriteBufferData(new_val);
361: 		serializer.WriteBufferData(options.quote);
362: 	} else {
363: 		serializer.WriteData((const_data_ptr_t) str, len);
364: 	}
365: }
366: 
367: //===--------------------------------------------------------------------===//
368: // Sink
369: //===--------------------------------------------------------------------===//
370: struct LocalReadCSVData : public LocalFunctionData {
371: 	//! The thread-local buffer to write data into
372: 	BufferedSerializer serializer;
373: 	//! A chunk with VARCHAR columns to cast intermediates into
374: 	DataChunk cast_chunk;
375: };
376: 
377: struct GlobalWriteCSVData : public GlobalFunctionData {
378: 	GlobalWriteCSVData(FileSystem &fs, string file_path) : fs(fs) {
379: 		handle = fs.OpenFile(file_path, FileFlags::WRITE | FileFlags::FILE_CREATE_NEW, FileLockType::WRITE_LOCK);
380: 	}
381: 
382: 	void WriteData(const_data_ptr_t data, idx_t size) {
383: 		lock_guard<mutex> flock(lock);
384: 		fs.Write(*handle, (void*) data, size);
385: 	}
386: 
387: 	FileSystem &fs;
388: 	//! The mutex for writing to the physical file
389: 	mutex lock;
390: 	//! The file handle to write to
391: 	unique_ptr<FileHandle> handle;
392: };
393: 
394: static unique_ptr<LocalFunctionData> write_csv_initialize_local(ClientContext &context, FunctionData &bind_data) {
395: 	auto &csv_data = (WriteCSVData &) bind_data;
396: 	auto local_data = make_unique<LocalReadCSVData>();
397: 
398: 	// create the chunk with VARCHAR types
399: 	vector<TypeId> types;
400: 	types.resize(csv_data.names.size(), TypeId::VARCHAR);
401: 
402: 	local_data->cast_chunk.Initialize(types);
403: 	return move(local_data);
404: }
405: 
406: static unique_ptr<GlobalFunctionData> write_csv_initialize_global(ClientContext &context, FunctionData &bind_data) {
407: 	auto &csv_data = (WriteCSVData &) bind_data;
408: 	auto global_data =  make_unique<GlobalWriteCSVData>(FileSystem::GetFileSystem(context), csv_data.file_path);
409: 
410: 	if (csv_data.header) {
411: 		BufferedSerializer serializer;
412: 		// write the header line to the file
413: 		for (idx_t i = 0; i < csv_data.names.size(); i++) {
414: 			if (i != 0) {
415: 				serializer.WriteBufferData(csv_data.delimiter);
416: 			}
417: 			WriteQuotedString(serializer, csv_data, csv_data.names[i].c_str(), csv_data.names[i].size(), false);
418: 		}
419: 		serializer.WriteBufferData(csv_data.newline);
420: 
421: 		global_data->WriteData(serializer.blob.data.get(), serializer.blob.size);
422: 	}
423: 	return move(global_data);
424: }
425: 
426: 
427: static void write_csv_sink(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate, LocalFunctionData &lstate, DataChunk &input) {
428: 	auto &csv_data = (WriteCSVData &) bind_data;
429: 	auto &local_data = (LocalReadCSVData &) lstate;
430: 	auto &global_state = (GlobalWriteCSVData &) gstate;
431: 
432: 	// write data into the local buffer
433: 
434: 	// first cast the columns of the chunk to varchar
435: 	auto &cast_chunk = local_data.cast_chunk;
436: 	cast_chunk.SetCardinality(input);
437: 	for (idx_t col_idx = 0; col_idx < input.column_count(); col_idx++) {
438: 		if (csv_data.sql_types[col_idx].id == SQLTypeId::VARCHAR || csv_data.sql_types[col_idx].id == SQLTypeId::BLOB) {
439: 			// VARCHAR, just create a reference
440: 			cast_chunk.data[col_idx].Reference(input.data[col_idx]);
441: 		} else {
442: 			// non varchar column, perform the cast
443: 			VectorOperations::Cast(input.data[col_idx], cast_chunk.data[col_idx], csv_data.sql_types[col_idx],
444: 									SQLType::VARCHAR, input.size());
445: 		}
446: 	}
447: 
448: 	cast_chunk.Normalify();
449: 	auto &writer = local_data.serializer;
450: 	// now loop over the vectors and output the values
451: 	for (idx_t row_idx = 0; row_idx < cast_chunk.size(); row_idx++) {
452: 		// write values
453: 		for (idx_t col_idx = 0; col_idx < cast_chunk.column_count(); col_idx++) {
454: 			if (col_idx != 0) {
455: 				writer.WriteBufferData(csv_data.delimiter);
456: 			}
457: 			if (FlatVector::IsNull(cast_chunk.data[col_idx], row_idx)) {
458: 				// write null value
459: 				writer.WriteBufferData(csv_data.null_str);
460: 				continue;
461: 			}
462: 
463: 			// non-null value, fetch the string value from the cast chunk
464: 			auto str_data = FlatVector::GetData<string_t>(cast_chunk.data[col_idx]);
465: 			auto str_value = str_data[row_idx];
466: 			// FIXME: we could gain some performance here by checking for certain types if they ever require quotes (e.g. integers only require quotes if the delimiter is a number, decimals only require quotes if the delimiter is a number or "." character)
467: 			WriteQuotedString(writer, csv_data, str_value.GetData(), str_value.GetSize(), csv_data.force_quote[col_idx]);
468: 		}
469: 		writer.WriteBufferData(csv_data.newline);
470: 	}
471: 	// check if we should flush what we have currently written
472: 	if (writer.blob.size >= csv_data.flush_size) {
473: 		global_state.WriteData(writer.blob.data.get(), writer.blob.size);
474: 		writer.Reset();
475: 	}
476: }
477: 
478: //===--------------------------------------------------------------------===//
479: // Combine
480: //===--------------------------------------------------------------------===//
481: static void write_csv_combine(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
482:                           LocalFunctionData &lstate) {
483: 	auto &local_data = (LocalReadCSVData &) lstate;
484: 	auto &global_state = (GlobalWriteCSVData &) gstate;
485: 	auto &writer = local_data.serializer;
486: 	// flush the local writer
487: 	if (writer.blob.size > 0) {
488: 		global_state.WriteData(writer.blob.data.get(), writer.blob.size);
489: 		writer.Reset();
490: 	}
491: }
492: 
493: //===--------------------------------------------------------------------===//
494: // Read CSV
495: //===--------------------------------------------------------------------===//
496: struct GlobalReadCSVData : public GlobalFunctionData {
497: 	unique_ptr<BufferedCSVReader> csv_reader;
498: };
499: 
500: unique_ptr<GlobalFunctionData> read_csv_initialize(ClientContext &context, FunctionData &fdata) {
501: 	auto global_data = make_unique<GlobalReadCSVData>();
502: 	auto &bind_data = (ReadCSVData&) fdata;
503: 
504: 	// set up the CSV reader with the parsed options
505:     BufferedCSVReaderOptions options;
506: 	options.file_path = bind_data.file_path;
507: 	options.auto_detect = bind_data.is_auto_detect;
508: 	options.delimiter = bind_data.delimiter;
509: 	options.quote = bind_data.quote;
510: 	options.escape = bind_data.escape;
511: 	options.header = bind_data.header;
512: 	options.null_str = bind_data.null_str;
513: 	options.skip_rows = 0;
514: 	options.num_cols = bind_data.sql_types.size();
515: 	options.force_not_null = bind_data.force_not_null;
516: 
517: 	global_data->csv_reader = make_unique<BufferedCSVReader>(context, move(options), bind_data.sql_types);
518: 	return move(global_data);
519: }
520: 
521: void read_csv_get_chunk(ExecutionContext &context, GlobalFunctionData &gstate, FunctionData &bind_data, DataChunk &chunk) {
522: 	// read a chunk from the CSV reader
523: 	auto &gdata = (GlobalReadCSVData &) gstate;
524: 	gdata.csv_reader->ParseCSV(chunk);
525: }
526: 
527: void CSVCopyFunction::RegisterFunction(BuiltinFunctions &set) {
528: 	CopyFunction info("csv");
529: 	info.copy_to_bind = write_csv_bind;
530: 	info.copy_to_initialize_local = write_csv_initialize_local;
531: 	info.copy_to_initialize_global = write_csv_initialize_global;
532: 	info.copy_to_sink = write_csv_sink;
533: 	info.copy_to_combine = write_csv_combine;
534: 
535: 	info.copy_from_bind = read_csv_bind;
536: 	info.copy_from_initialize = read_csv_initialize;
537: 	info.copy_from_get_chunk = read_csv_get_chunk;
538: 
539: 	// CSV_AUTO can only be used in COPY FROM
540: 	CopyFunction auto_info("csv_auto");
541: 	auto_info.copy_from_bind = read_csv_auto_bind;
542: 	auto_info.copy_from_initialize = read_csv_initialize;
543: 	auto_info.copy_from_get_chunk = read_csv_get_chunk;
544: 
545: 	set.AddFunction(info);
546: 	set.AddFunction(auto_info);
547: }
548: 
549: } // namespace duckdb
[end of src/function/table/copy_csv.cpp]
[start of src/function/table/range.cpp]
1: #include "duckdb/function/table/range.hpp"
2: #include "duckdb/function/table_function.hpp"
3: #include "duckdb/function/function_set.hpp"
4: #include "duckdb/common/algorithm.hpp"
5: 
6: using namespace std;
7: 
8: namespace duckdb {
9: 
10: struct RangeFunctionData : public TableFunctionData {
11: 	Value start;
12: 	Value end;
13: 	Value increment;
14: 	idx_t current_idx;
15: };
16: 
17: static unique_ptr<FunctionData> range_function_bind(ClientContext &context, vector<Value> inputs,
18:                                               vector<SQLType> &return_types, vector<string> &names) {
19: 	auto result = make_unique<RangeFunctionData>();
20: 	if (inputs.size() < 2) {
21: 		// single argument: only the end is specified
22: 		result->start = Value::BIGINT(0);
23: 		result->end = inputs[0].CastAs(TypeId::INT64);
24: 	} else {
25: 		// two arguments: first two arguments are start and end
26: 		result->start = inputs[0].CastAs(TypeId::INT64);
27: 		result->end = inputs[1].CastAs(TypeId::INT64);
28: 	}
29: 	if (inputs.size() < 3) {
30: 		result->increment = Value::BIGINT(1);
31: 	} else {
32: 		result->increment = inputs[2].CastAs(TypeId::INT64);
33: 	}
34: 	if (result->increment == 0) {
35: 		throw BinderException("interval cannot be 0!");
36: 	}
37: 	if (result->start > result->end && result->increment > 0) {
38: 		throw BinderException("start is bigger than end, but increment is positive: cannot generate infinite series");
39: 	} else if (result->start < result->end && result->increment < 0) {
40: 		throw BinderException("start is smaller than end, but increment is negative: cannot generate infinite series");
41: 	}
42: 	result->current_idx = 0;
43: 	return_types.push_back(SQLType::BIGINT);
44: 	names.push_back("range");
45: 	return move(result);
46: }
47: 
48: static void range_function(ClientContext &context, vector<Value> &input, DataChunk &output, FunctionData *dataptr) {
49: 	auto &data = ((RangeFunctionData &)*dataptr);
50: 	auto increment = data.increment.value_.bigint;
51: 	auto end = data.end.value_.bigint;
52: 	int64_t current_value = data.start.value_.bigint + (int64_t) increment * data.current_idx;
53: 	// set the result vector as a sequence vector
54: 	output.data[0].Sequence(current_value, increment);
55: 	idx_t remaining = min<int64_t>((end - current_value) / increment, STANDARD_VECTOR_SIZE);
56: 	// increment the index pointer by the remaining count
57: 	data.current_idx += remaining;
58: 	output.SetCardinality(remaining);
59: }
60: 
61: void RangeTableFunction::RegisterFunction(BuiltinFunctions &set) {
62: 	TableFunctionSet range("range");
63: 
64: 	// single argument range: (end) - implicit start = 0 and increment = 1
65: 	range.AddFunction(TableFunction({SQLType::BIGINT}, range_function_bind, range_function));
66: 	// two arguments range: (start, end) - implicit increment = 1
67: 	range.AddFunction(TableFunction({SQLType::BIGINT, SQLType::BIGINT}, range_function_bind, range_function));
68: 	// three arguments range: (start, end, increment)
69: 	range.AddFunction(TableFunction({SQLType::BIGINT, SQLType::BIGINT, SQLType::BIGINT}, range_function_bind, range_function));
70: 	set.AddFunction(range);
71: }
72: 
73: void BuiltinFunctions::RegisterTableFunctions() {
74: 	RangeTableFunction::RegisterFunction(*this);
75: 	RepeatTableFunction::RegisterFunction(*this);
76: }
77: 
78: } // namespace duckdb
[end of src/function/table/range.cpp]
[start of src/function/table/read_csv.cpp]
1: #include "duckdb/function/table/read_csv.hpp"
2: #include "duckdb/execution/operator/persistent/buffered_csv_reader.hpp"
3: #include "duckdb/function/function_set.hpp"
4: #include "duckdb/main/client_context.hpp"
5: #include "duckdb/main/database.hpp"
6: 
7: using namespace std;
8: 
9: namespace duckdb {
10: 
11: struct ReadCSVFunctionData : public TableFunctionData {
12: 	ReadCSVFunctionData() {
13: 	}
14: 
15: 	//! The CSV reader
16: 	unique_ptr<BufferedCSVReader> csv_reader;
17: };
18: 
19: static unique_ptr<FunctionData> read_csv_bind(ClientContext &context, vector<Value> inputs,
20:                                               vector<SQLType> &return_types, vector<string> &names) {
21: 
22: 	if (!context.db.config.enable_copy) {
23: 		throw Exception("read_csv is disabled by configuration");
24: 	}
25: 	for (auto &val : inputs[2].struct_value) {
26: 		names.push_back(val.first);
27: 		if (val.second.type != TypeId::VARCHAR) {
28: 			throw BinderException("read_csv requires a type specification as string");
29: 		}
30: 		return_types.push_back(TransformStringToSQLType(val.second.str_value.c_str()));
31: 	}
32: 	if (names.size() == 0) {
33: 		throw BinderException("read_csv requires at least a single column as input!");
34: 	}
35: 	auto result = make_unique<ReadCSVFunctionData>();
36: 
37: 	BufferedCSVReaderOptions options;
38: 	options.auto_detect = false;
39: 	options.file_path = inputs[0].str_value;
40: 	options.header = false;
41: 	options.delimiter = inputs[1].str_value;
42: 
43: 	result->csv_reader = make_unique<BufferedCSVReader>(context, move(options), return_types);
44: 	return move(result);
45: }
46: 
47: static unique_ptr<FunctionData> read_csv_auto_bind(ClientContext &context, vector<Value> inputs,
48:                                                    vector<SQLType> &return_types, vector<string> &names) {
49: 
50: 	if (!context.db.config.enable_copy) {
51: 		throw Exception("read_csv_auto is disabled by configuration");
52: 	}
53: 	auto result = make_unique<ReadCSVFunctionData>();
54: 	BufferedCSVReaderOptions options;
55: 	options.auto_detect = true;
56: 	options.file_path = inputs[0].str_value;
57: 
58: 	result->csv_reader = make_unique<BufferedCSVReader>(context, move(options));
59: 
60: 	// TODO: print detected dialect from result->csv_reader->info
61: 	return_types.assign(result->csv_reader->sql_types.begin(), result->csv_reader->sql_types.end());
62: 	names.assign(result->csv_reader->col_names.begin(), result->csv_reader->col_names.end());
63: 
64: 	return move(result);
65: }
66: 
67: static void read_csv_info(ClientContext &context, vector<Value> &input, DataChunk &output, FunctionData *dataptr) {
68: 	auto &data = ((ReadCSVFunctionData &)*dataptr);
69: 	data.csv_reader->ParseCSV(output);
70: }
71: 
72: void ReadCSVTableFunction::RegisterFunction(BuiltinFunctions &set) {
73: 	TableFunctionSet read_csv("read_csv");
74: 	read_csv.AddFunction(
75: 	    TableFunction({SQLType::VARCHAR, SQLType::VARCHAR, SQLType::STRUCT}, read_csv_bind, read_csv_info, nullptr));
76: 	read_csv.AddFunction(TableFunction({SQLType::VARCHAR}, read_csv_auto_bind, read_csv_info, nullptr));
77: 
78: 	set.AddFunction(read_csv);
79: 	set.AddFunction(TableFunction("read_csv_auto", {SQLType::VARCHAR}, read_csv_auto_bind, read_csv_info, nullptr));
80: }
81: 
82: void BuiltinFunctions::RegisterReadFunctions() {
83: 	CSVCopyFunction::RegisterFunction(*this);
84: 	ReadCSVTableFunction::RegisterFunction(*this);
85: }
86: 
87: } // namespace duckdb
[end of src/function/table/read_csv.cpp]
[start of src/function/table/repeat.cpp]
1: #include "duckdb/function/table/range.hpp"
2: #include "duckdb/common/algorithm.hpp"
3: 
4: using namespace std;
5: 
6: namespace duckdb {
7: 
8: struct RepeatFunctionData : public TableFunctionData {
9: 	RepeatFunctionData(idx_t target_count) : current_count(0), target_count(target_count) { }
10: 
11: 	idx_t current_count;
12: 	idx_t target_count;
13: };
14: 
15: static unique_ptr<FunctionData> repeat_bind(ClientContext &context, vector<Value> inputs,
16:                                               vector<SQLType> &return_types, vector<string> &names) {
17: 	// the repeat function returns the type of the first argument
18: 	return_types.push_back(inputs[0].GetSQLType());
19: 	names.push_back(inputs[0].ToString());
20: 	return make_unique<RepeatFunctionData>(inputs[1].GetValue<int64_t>());
21: }
22: 
23: static void repeat_function(ClientContext &context, vector<Value> &input, DataChunk &output, FunctionData *dataptr) {
24: 	auto &repeat = (RepeatFunctionData &) *dataptr;
25: 	idx_t remaining = min<idx_t>(repeat.target_count - repeat.current_count, STANDARD_VECTOR_SIZE);
26: 	output.data[0].Reference(input[0]);
27: 	output.SetCardinality(remaining);
28: 	repeat.current_count += remaining;
29: }
30: 
31: void RepeatTableFunction::RegisterFunction(BuiltinFunctions &set) {
32: 	TableFunction repeat("repeat", {SQLType::ANY, SQLType::BIGINT}, repeat_bind, repeat_function, nullptr);
33: 	set.AddFunction(repeat);
34: }
35: 
36: } // namespace duckdb
[end of src/function/table/repeat.cpp]
[start of src/function/table/sqlite/pragma_collations.cpp]
1: #include "duckdb/function/table/sqlite_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/collate_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
6: #include "duckdb/common/exception.hpp"
7: #include "duckdb/transaction/transaction.hpp"
8: 
9: using namespace std;
10: 
11: namespace duckdb {
12: 
13: struct PragmaCollateData : public TableFunctionData {
14: 	PragmaCollateData() : initialized(false), offset(0) {
15: 	}
16: 
17: 	bool initialized;
18: 	vector<CatalogEntry *> entries;
19: 	idx_t offset;
20: };
21: 
22: static unique_ptr<FunctionData> pragma_collate_bind(ClientContext &context, vector<Value> inputs,
23:                                                     vector<SQLType> &return_types, vector<string> &names) {
24: 	names.push_back("collname");
25: 	return_types.push_back(SQLType::VARCHAR);
26: 
27: 	return make_unique<PragmaCollateData>();
28: }
29: 
30: static void pragma_collate_info(ClientContext &context, vector<Value> &input, DataChunk &output,
31:                                 FunctionData *dataptr) {
32: 	auto &data = *((PragmaCollateData *)dataptr);
33: 	assert(input.size() == 0);
34: 	if (!data.initialized) {
35: 		// scan all the schemas
36: 		auto &transaction = Transaction::GetTransaction(context);
37: 		Catalog::GetCatalog(context).schemas->Scan(transaction, [&](CatalogEntry *entry) {
38: 			auto schema = (SchemaCatalogEntry *)entry;
39: 			schema->collations.Scan(transaction, [&](CatalogEntry *entry) { data.entries.push_back(entry); });
40: 		});
41: 		data.initialized = true;
42: 	}
43: 
44: 	if (data.offset >= data.entries.size()) {
45: 		// finished returning values
46: 		return;
47: 	}
48: 	idx_t next = min(data.offset + STANDARD_VECTOR_SIZE, (idx_t)data.entries.size());
49: 	output.SetCardinality(next - data.offset);
50: 	for (idx_t i = data.offset; i < next; i++) {
51: 		auto index = i - data.offset;
52: 		auto entry = (CollateCatalogEntry *)data.entries[i];
53: 
54: 		output.SetValue(0, index, Value(entry->name));
55: 	}
56: 
57: 	data.offset = next;
58: }
59: 
60: void PragmaCollations::RegisterFunction(BuiltinFunctions &set) {
61: 	set.AddFunction(TableFunction("pragma_collations", {}, pragma_collate_bind, pragma_collate_info, nullptr));
62: }
63: 
64: } // namespace duckdb
[end of src/function/table/sqlite/pragma_collations.cpp]
[start of src/function/table/sqlite/pragma_database_list.cpp]
1: #include "duckdb/function/table/sqlite_functions.hpp"
2: 
3: #include "duckdb/storage/storage_manager.hpp"
4: 
5: using namespace std;
6: 
7: namespace duckdb {
8: 
9: struct PragmaDatabaseListData : public TableFunctionData {
10: 	PragmaDatabaseListData() : finished(false) {
11: 	}
12: 
13: 	bool finished;
14: };
15: 
16: static unique_ptr<FunctionData> pragma_database_list_bind(ClientContext &context, vector<Value> inputs,
17:                                                    vector<SQLType> &return_types, vector<string> &names) {
18: 	names.push_back("seq");
19: 	return_types.push_back(SQLType::INTEGER);
20: 
21: 	names.push_back("name");
22: 	return_types.push_back(SQLType::VARCHAR);
23: 
24: 	names.push_back("file");
25: 	return_types.push_back(SQLType::VARCHAR);
26: 
27: 	// initialize the function data structure
28: 	return make_unique<PragmaDatabaseListData>();
29: }
30: 
31: void pragma_database_list(ClientContext &context, vector<Value> &input, DataChunk &output, FunctionData *dataptr) {
32: 	auto &data = *((PragmaDatabaseListData *)dataptr);
33: 	if (data.finished) {
34: 		return;
35: 	}
36: 
37: 	output.SetCardinality(1);
38: 	output.data[0].SetValue(0, Value::INTEGER(0));
39: 	output.data[1].SetValue(0, Value("main"));
40: 	output.data[2].SetValue(0, Value(StorageManager::GetStorageManager(context).GetDBPath()));
41: 
42: 	data.finished = true;
43: }
44: 
45: void PragmaDatabaseList::RegisterFunction(BuiltinFunctions &set) {
46: 	set.AddFunction(TableFunction("pragma_database_list", {}, pragma_database_list_bind, pragma_database_list, nullptr));
47: }
48: 
49: } // namespace duckdb
[end of src/function/table/sqlite/pragma_database_list.cpp]
[start of src/function/table/sqlite/pragma_table_info.cpp]
1: #include "duckdb/function/table/sqlite_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
6: #include "duckdb/common/exception.hpp"
7: 
8: #include <algorithm>
9: 
10: using namespace std;
11: 
12: namespace duckdb {
13: 
14: struct PragmaTableFunctionData : public TableFunctionData {
15: 	PragmaTableFunctionData() : entry(nullptr), offset(0) {
16: 	}
17: 
18: 	CatalogEntry *entry;
19: 	idx_t offset;
20: };
21: 
22: static unique_ptr<FunctionData> pragma_table_info_bind(ClientContext &context, vector<Value> inputs,
23:                                                        vector<SQLType> &return_types, vector<string> &names) {
24: 	names.push_back("cid");
25: 	return_types.push_back(SQLType::INTEGER);
26: 
27: 	names.push_back("name");
28: 	return_types.push_back(SQLType::VARCHAR);
29: 
30: 	names.push_back("type");
31: 	return_types.push_back(SQLType::VARCHAR);
32: 
33: 	names.push_back("notnull");
34: 	return_types.push_back(SQLType::BOOLEAN);
35: 
36: 	names.push_back("dflt_value");
37: 	return_types.push_back(SQLType::VARCHAR);
38: 
39: 	names.push_back("pk");
40: 	return_types.push_back(SQLType::BOOLEAN);
41: 
42: 	return make_unique<PragmaTableFunctionData>();
43: }
44: 
45: static void pragma_table_info_table(PragmaTableFunctionData &data, TableCatalogEntry *table, DataChunk &output) {
46: 	if (data.offset >= table->columns.size()) {
47: 		// finished returning values
48: 		return;
49: 	}
50: 	// start returning values
51: 	// either fill up the chunk or return all the remaining columns
52: 	idx_t next = min(data.offset + STANDARD_VECTOR_SIZE, (idx_t)table->columns.size());
53: 	output.SetCardinality(next - data.offset);
54: 
55: 	for (idx_t i = data.offset; i < next; i++) {
56: 		auto index = i - data.offset;
57: 		auto &column = table->columns[i];
58: 		// return values:
59: 		// "cid", TypeId::INT32
60: 		assert(column.oid < (idx_t)NumericLimits<int32_t>::Maximum());
61: 
62: 		output.SetValue(0, index, Value::INTEGER((int32_t)column.oid));
63: 		// "name", TypeId::VARCHAR
64: 		output.SetValue(1, index, Value(column.name));
65: 		// "type", TypeId::VARCHAR
66: 		output.SetValue(2, index, Value(SQLTypeToString(column.type)));
67: 		// "notnull", TypeId::BOOL
68: 		// FIXME: look at constraints
69: 		output.SetValue(3, index, Value::BOOLEAN(false));
70: 		// "dflt_value", TypeId::VARCHAR
71: 		Value def_value = column.default_value ? Value(column.default_value->ToString()) : Value();
72: 		output.SetValue(4, index, def_value);
73: 		// "pk", TypeId::BOOL
74: 		// FIXME: look at constraints
75: 		output.SetValue(5, index, Value::BOOLEAN(false));
76: 	}
77: 	data.offset = next;
78: }
79: 
80: static void pragma_table_info_view(PragmaTableFunctionData &data, ViewCatalogEntry *view, DataChunk &output) {
81: 	if (data.offset >= view->types.size()) {
82: 		// finished returning values
83: 		return;
84: 	}
85: 	// start returning values
86: 	// either fill up the chunk or return all the remaining columns
87: 	idx_t next = min(data.offset + STANDARD_VECTOR_SIZE, (idx_t)view->types.size());
88: 	output.SetCardinality(next - data.offset);
89: 
90: 	for (idx_t i = data.offset; i < next; i++) {
91: 		auto index = i - data.offset;
92: 		auto type = view->types[index];
93: 		auto &name = view->aliases[index];
94: 		// return values:
95: 		// "cid", TypeId::INT32
96: 
97: 		output.SetValue(0, index, Value::INTEGER((int32_t)index));
98: 		// "name", TypeId::VARCHAR
99: 		output.SetValue(1, index, Value(name));
100: 		// "type", TypeId::VARCHAR
101: 		output.SetValue(2, index, Value(SQLTypeToString(type)));
102: 		// "notnull", TypeId::BOOL
103: 		output.SetValue(3, index, Value::BOOLEAN(false));
104: 		// "dflt_value", TypeId::VARCHAR
105: 		output.SetValue(4, index, Value());
106: 		// "pk", TypeId::BOOL
107: 		output.SetValue(5, index, Value::BOOLEAN(false));
108: 	}
109: 	data.offset = next;
110: }
111: 
112: static void pragma_table_info(ClientContext &context, vector<Value> &input, DataChunk &output, FunctionData *dataptr) {
113: 	auto &data = *((PragmaTableFunctionData *)dataptr);
114: 	if (!data.entry) {
115: 		// first call: load the entry from the catalog
116: 		assert(input.size() == 1);
117: 
118: 		string schema, table_name;
119: 		auto range_var = input[0].GetValue<string>();
120: 		Catalog::ParseRangeVar(range_var, schema, table_name);
121: 
122: 		// look up the table name in the catalog
123: 		auto &catalog = Catalog::GetCatalog(context);
124: 		data.entry = catalog.GetEntry(context, CatalogType::TABLE, schema, table_name);
125: 	}
126: 	switch (data.entry->type) {
127: 	case CatalogType::TABLE:
128: 		pragma_table_info_table(data, (TableCatalogEntry *)data.entry, output);
129: 		break;
130: 	case CatalogType::VIEW:
131: 		pragma_table_info_view(data, (ViewCatalogEntry *)data.entry, output);
132: 		break;
133: 	default:
134: 		throw NotImplementedException("Unimplemented catalog type for pragma_table_info");
135: 	}
136: }
137: 
138: void PragmaTableInfo::RegisterFunction(BuiltinFunctions &set) {
139: 	set.AddFunction(
140: 	    TableFunction("pragma_table_info", {SQLType::VARCHAR}, pragma_table_info_bind, pragma_table_info, nullptr));
141: }
142: 
143: } // namespace duckdb
[end of src/function/table/sqlite/pragma_table_info.cpp]
[start of src/function/table/sqlite/sqlite_master.cpp]
1: #include "duckdb/function/table/sqlite_functions.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
6: #include "duckdb/common/exception.hpp"
7: #include "duckdb/transaction/transaction.hpp"
8: 
9: #include <algorithm>
10: #include <sstream>
11: 
12: using namespace std;
13: 
14: namespace duckdb {
15: 
16: struct SQLiteMasterData : public TableFunctionData {
17: 	SQLiteMasterData() : initialized(false), offset(0) {
18: 	}
19: 
20: 	bool initialized;
21: 	vector<CatalogEntry *> entries;
22: 	idx_t offset;
23: };
24: 
25: string GenerateQuery(CatalogEntry *entry) {
26: 	// generate a query from a catalog entry
27: 	if (entry->type == CatalogType::TABLE) {
28: 		// FIXME: constraints
29: 		stringstream ss;
30: 		auto table = (TableCatalogEntry *)entry;
31: 		ss << "CREATE TABLE " << table->name << "(";
32: 
33: 		for (idx_t i = 0; i < table->columns.size(); i++) {
34: 			auto &column = table->columns[i];
35: 			ss << column.name << " " << SQLTypeToString(column.type);
36: 			if (i + 1 < table->columns.size()) {
37: 				ss << ", ";
38: 			}
39: 		}
40: 
41: 		ss << ");";
42: 		return ss.str();
43: 	} else {
44: 		return "[Unknown]";
45: 	}
46: }
47: 
48: static unique_ptr<FunctionData> sqlite_master_bind(ClientContext &context, vector<Value> inputs,
49:                                                    vector<SQLType> &return_types, vector<string> &names) {
50: 	names.push_back("type");
51: 	return_types.push_back(SQLType::VARCHAR);
52: 
53: 	names.push_back("name");
54: 	return_types.push_back(SQLType::VARCHAR);
55: 
56: 	names.push_back("tbl_name");
57: 	return_types.push_back(SQLType::VARCHAR);
58: 
59: 	names.push_back("rootpage");
60: 	return_types.push_back(SQLType::INTEGER);
61: 
62: 	names.push_back("sql");
63: 	return_types.push_back(SQLType::VARCHAR);
64: 
65: 	// initialize the function data structure
66: 	return make_unique<SQLiteMasterData>();
67: }
68: 
69: void sqlite_master(ClientContext &context, vector<Value> &input, DataChunk &output, FunctionData *dataptr) {
70: 	auto &data = *((SQLiteMasterData *)dataptr);
71: 	assert(input.size() == 0);
72: 	if (!data.initialized) {
73: 		// scan all the schemas
74: 		auto &transaction = Transaction::GetTransaction(context);
75: 		Catalog::GetCatalog(context).schemas->Scan(transaction, [&](CatalogEntry *entry) {
76: 			auto schema = (SchemaCatalogEntry *)entry;
77: 			schema->tables.Scan(transaction, [&](CatalogEntry *entry) { data.entries.push_back(entry); });
78: 		});
79: 		data.initialized = true;
80: 	}
81: 
82: 	if (data.offset >= data.entries.size()) {
83: 		// finished returning values
84: 		return;
85: 	}
86: 	idx_t next = min(data.offset + STANDARD_VECTOR_SIZE, (idx_t)data.entries.size());
87: 	output.SetCardinality(next - data.offset);
88: 
89: 	// start returning values
90: 	// either fill up the chunk or return all the remaining columns
91: 	for (idx_t i = data.offset; i < next; i++) {
92: 		auto index = i - data.offset;
93: 		auto &entry = data.entries[i];
94: 
95: 		// return values:
96: 		// "type", TypeId::VARCHAR
97: 		const char *type_str;
98: 		switch (entry->type) {
99: 		case CatalogType::TABLE:
100: 			type_str = "table";
101: 			break;
102: 		case CatalogType::SCHEMA:
103: 			type_str = "schema";
104: 			break;
105: 		case CatalogType::TABLE_FUNCTION:
106: 			type_str = "function";
107: 			break;
108: 		case CatalogType::VIEW:
109: 			type_str = "view";
110: 			break;
111: 		default:
112: 			type_str = "unknown";
113: 		}
114: 		output.SetValue(0, index, Value(type_str));
115: 		// "name", TypeId::VARCHAR
116: 		output.SetValue(1, index, Value(entry->name));
117: 		// "tbl_name", TypeId::VARCHAR
118: 		output.SetValue(2, index, Value(entry->name));
119: 		// "rootpage", TypeId::INT32
120: 		output.SetValue(3, index, Value::INTEGER(0));
121: 		// "sql", TypeId::VARCHAR
122: 		output.SetValue(4, index, Value(GenerateQuery(entry)));
123: 	}
124: 	data.offset = next;
125: }
126: 
127: void SQLiteMaster::RegisterFunction(BuiltinFunctions &set) {
128: 	set.AddFunction(TableFunction("sqlite_master", {}, sqlite_master_bind, sqlite_master, nullptr));
129: }
130: 
131: } // namespace duckdb
[end of src/function/table/sqlite/sqlite_master.cpp]
[start of src/function/table/version/pragma_version.cpp]
1: #include "duckdb/function/table/sqlite_functions.hpp"
2: #include "duckdb/main/database.hpp"
3: 
4: namespace duckdb {
5: 
6: struct PragmaVersionData : public TableFunctionData {
7: 	PragmaVersionData() : done(false) {
8: 	}
9: 	bool done;
10: };
11: 
12: static unique_ptr<FunctionData> pragma_version_bind(ClientContext &context, vector<Value> inputs,
13:                                                     vector<SQLType> &return_types, vector<string> &names) {
14: 	names.push_back("library_version");
15: 	return_types.push_back(SQLType::VARCHAR);
16: 	names.push_back("source_id");
17: 	return_types.push_back(SQLType::VARCHAR);
18: 
19: 	return make_unique<PragmaVersionData>();
20: }
21: 
22: static void pragma_version_info(ClientContext &context, vector<Value> &input, DataChunk &output,
23:                                 FunctionData *dataptr) {
24: 	auto &data = *((PragmaVersionData *)dataptr);
25: 	assert(input.size() == 0);
26: 	if (data.done) {
27: 		// finished returning values
28: 		return;
29: 	}
30: 	output.SetCardinality(1);
31: 	output.SetValue(0, 0, DuckDB::LibraryVersion());
32: 	output.SetValue(1, 0, DuckDB::SourceID());
33: 	data.done = true;
34: }
35: 
36: void PragmaVersion::RegisterFunction(BuiltinFunctions &set) {
37: 	set.AddFunction(TableFunction("pragma_version", {}, pragma_version_bind, pragma_version_info, nullptr));
38: }
39: 
40: const char *DuckDB::SourceID() {
41: 	return DUCKDB_SOURCE_ID;
42: }
43: 
44: const char *DuckDB::LibraryVersion() {
45: 	return "DuckDB";
46: }
47: 
48: } // namespace duckdb
[end of src/function/table/version/pragma_version.cpp]
[start of src/include/duckdb/common/file_system.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/file_system.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/constants.hpp"
12: #include "duckdb/common/file_buffer.hpp"
13: 
14: #include <functional>
15: 
16: #undef CreateDirectory
17: #undef MoveFile
18: #undef RemoveDirectory
19: 
20: namespace duckdb {
21: class ClientContext;
22: class FileSystem;
23: 
24: struct FileHandle {
25: public:
26: 	FileHandle(FileSystem &file_system, string path) : file_system(file_system), path(path) {
27: 	}
28: 	FileHandle(const FileHandle &) = delete;
29: 	virtual ~FileHandle() {
30: 	}
31: 
32: 	void Read(void *buffer, idx_t nr_bytes, idx_t location);
33: 	void Write(void *buffer, idx_t nr_bytes, idx_t location);
34: 	void Sync();
35: 	void Truncate(int64_t new_size);
36: 
37: protected:
38: 	virtual void Close() = 0;
39: 
40: public:
41: 	FileSystem &file_system;
42: 	string path;
43: };
44: 
45: enum class FileLockType : uint8_t { NO_LOCK = 0, READ_LOCK = 1, WRITE_LOCK = 2 };
46: 
47: class FileFlags {
48: public:
49: 	//! Open file with read access
50: 	static constexpr uint8_t READ = 1 << 0;
51: 	//! Open file with read/write access
52: 	static constexpr uint8_t WRITE = 1 << 1;
53: 	//! Use direct IO when reading/writing to the file
54: 	static constexpr uint8_t DIRECT_IO = 1 << 2;
55: 	//! Create file if not exists, can only be used together with WRITE
56: 	static constexpr uint8_t FILE_CREATE = 1 << 3;
57: 	//! Always create a new file. If a file exists, the file is truncated. Cannot be used together with CREATE.
58: 	static constexpr uint8_t FILE_CREATE_NEW = 1 << 4;
59: 	//! Open file in append mode
60: 	static constexpr uint8_t APPEND = 1 << 5;
61: };
62: 
63: class FileSystem {
64: public:
65: 	virtual ~FileSystem() {
66: 	}
67: 
68: public:
69: 	static FileSystem &GetFileSystem(ClientContext &context);
70: 
71: 	virtual unique_ptr<FileHandle> OpenFile(const char *path, uint8_t flags, FileLockType lock = FileLockType::NO_LOCK);
72: 	unique_ptr<FileHandle> OpenFile(string &path, uint8_t flags, FileLockType lock = FileLockType::NO_LOCK) {
73: 		return OpenFile(path.c_str(), flags, lock);
74: 	}
75: 	//! Read exactly nr_bytes from the specified location in the file. Fails if nr_bytes could not be read. This is
76: 	//! equivalent to calling SetFilePointer(location) followed by calling Read().
77: 	virtual void Read(FileHandle &handle, void *buffer, int64_t nr_bytes, idx_t location);
78: 	//! Write exactly nr_bytes to the specified location in the file. Fails if nr_bytes could not be read. This is
79: 	//! equivalent to calling SetFilePointer(location) followed by calling Write().
80: 	virtual void Write(FileHandle &handle, void *buffer, int64_t nr_bytes, idx_t location);
81: 	//! Read nr_bytes from the specified file into the buffer, moving the file pointer forward by nr_bytes. Returns the
82: 	//! amount of bytes read.
83: 	virtual int64_t Read(FileHandle &handle, void *buffer, int64_t nr_bytes);
84: 	//! Write nr_bytes from the buffer into the file, moving the file pointer forward by nr_bytes.
85: 	virtual int64_t Write(FileHandle &handle, void *buffer, int64_t nr_bytes);
86: 
87: 	//! Returns the file size of a file handle, returns -1 on error
88: 	virtual int64_t GetFileSize(FileHandle &handle);
89: 	//! Truncate a file to a maximum size of new_size, new_size should be smaller than or equal to the current size of
90: 	//! the file
91: 	virtual void Truncate(FileHandle &handle, int64_t new_size);
92: 
93: 	//! Check if a directory exists
94: 	virtual bool DirectoryExists(const string &directory);
95: 	//! Create a directory if it does not exist
96: 	virtual void CreateDirectory(const string &directory);
97: 	//! Recursively remove a directory and all files in it
98: 	virtual void RemoveDirectory(const string &directory);
99: 	//! List files in a directory, invoking the callback method for each one with (filename, is_dir)
100: 	virtual bool ListFiles(const string &directory, std::function<void(string, bool)> callback);
101: 	//! Move a file from source path to the target, StorageManager relies on this being an atomic action for ACID
102: 	//! properties
103: 	virtual void MoveFile(const string &source, const string &target);
104: 	//! Check if a file exists
105: 	virtual bool FileExists(const string &filename);
106: 	//! Remove a file from disk
107: 	virtual void RemoveFile(const string &filename);
108: 	//! Path separator for the current file system
109: 	virtual string PathSeparator();
110: 	//! Join two paths together
111: 	virtual string JoinPath(const string &a, const string &path);
112: 	//! Sync a file handle to disk
113: 	virtual void FileSync(FileHandle &handle);
114: 
115: private:
116: 	//! Set the file pointer of a file handle to a specified location. Reads and writes will happen from this location
117: 	void SetFilePointer(FileHandle &handle, idx_t location);
118: };
119: 
120: } // namespace duckdb
[end of src/include/duckdb/common/file_system.hpp]
[start of src/include/duckdb/common/serializer/buffered_file_writer.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/serializer/buffered_file_writer.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/serializer.hpp"
12: #include "duckdb/common/file_system.hpp"
13: 
14: namespace duckdb {
15: 
16: #define FILE_BUFFER_SIZE 4096
17: 
18: class BufferedFileWriter : public Serializer {
19: public:
20: 	//! Serializes to a buffer allocated by the serializer, will expand when
21: 	//! writing past the initial threshold
22: 	BufferedFileWriter(FileSystem &fs, string path, uint8_t open_flags = FileFlags::WRITE | FileFlags::FILE_CREATE);
23: 
24: 	FileSystem &fs;
25: 	unique_ptr<data_t[]> data;
26: 	idx_t offset;
27: 	idx_t total_written;
28: 	unique_ptr<FileHandle> handle;
29: 
30: public:
31: 	void WriteData(const_data_ptr_t buffer, uint64_t write_size) override;
32: 	//! Flush the buffer to disk and sync the file to ensure writing is completed
33: 	void Sync();
34: 	//! Flush the buffer to the file (without sync)
35: 	void Flush();
36: 	//! Returns the current size of the file
37: 	int64_t GetFileSize();
38: 	//! Truncate the size to a previous size (given that size <= GetFileSize())
39: 	void Truncate(int64_t size);
40: 
41: 	idx_t GetTotalWritten();
42: };
43: 
44: } // namespace duckdb
[end of src/include/duckdb/common/serializer/buffered_file_writer.hpp]
[start of src/include/duckdb/common/types/date.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/types/date.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: 
13: namespace duckdb {
14: 
15: //! The Date class is a static class that holds helper functions for the Date
16: //! type.
17: class Date {
18: public:
19: 	//! Convert a string in the format "YYYY-MM-DD" to a date object
20: 	static date_t FromString(string str, bool strict = false);
21: 	//! Convert a string in the format "YYYY-MM-DD" to a date object
22: 	static date_t FromCString(const char *str, bool strict = false);
23: 	//! Convert a date object to a string in the format "YYYY-MM-DD"
24: 	static string ToString(date_t date);
25: 
26: 	//! Create a string "YYYY-MM-DD" from a specified (year, month, day)
27: 	//! combination
28: 	static string Format(int32_t year, int32_t month, int32_t day);
29: 
30: 	//! Extract the year, month and day from a given date object
31: 	static void Convert(date_t date, int32_t &out_year, int32_t &out_month, int32_t &out_day);
32: 	//! Create a Date object from a specified (year, month, day) combination
33: 	static date_t FromDate(int32_t year, int32_t month, int32_t day);
34: 
35: 	//! Returns true if (year) is a leap year, and false otherwise
36: 	static bool IsLeapYear(int32_t year);
37: 
38: 	//! Returns true if the specified (year, month, day) combination is a valid
39: 	//! date
40: 	static bool IsValidDay(int32_t year, int32_t month, int32_t day);
41: 
42: 	//! Extract the epoch from the date (seconds since 1970-01-01)
43: 	static int64_t Epoch(date_t date);
44: 	//! Convert the epoch (seconds since 1970-01-01) to a date_t
45: 	static date_t EpochToDate(int64_t epoch);
46: 	//! Extract year of a date entry
47: 	static int32_t ExtractYear(date_t date);
48: 	//! Extract month of a date entry
49: 	static int32_t ExtractMonth(date_t date);
50: 	//! Extract day of a date entry
51: 	static int32_t ExtractDay(date_t date);
52: 	//! Extract the day of the week (1-7)
53: 	static int32_t ExtractISODayOfTheWeek(date_t date);
54: 	//! Extract the day of the year
55: 	static int32_t ExtractDayOfTheYear(date_t date);
56: 	//! Extract the week number
57: 	static int32_t ExtractWeekNumber(date_t date);
58: 	//! Returns the date of the monday of the current week.
59: 	static date_t GetMondayOfCurrentWeek(date_t date);
60: };
61: } // namespace duckdb
[end of src/include/duckdb/common/types/date.hpp]
[start of src/include/duckdb/common/types/time.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/types/time.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: 
13: namespace duckdb {
14: 
15: //! The Date class is a static class that holds helper functions for the Time
16: //! type.
17: class Time {
18: public:
19: 	//! Convert a string in the format "hh:mm:ss" to a time object
20: 	static dtime_t FromString(string str, bool strict = false);
21: 	static dtime_t FromCString(const char *buf, bool strict = false);
22: 
23: 	//! Convert a time object to a string in the format "hh:mm:ss"
24: 	static string ToString(dtime_t time);
25: 
26: 	static string Format(int32_t hour, int32_t minute, int32_t second, int32_t milisecond = 0);
27: 
28: 	static dtime_t FromTime(int32_t hour, int32_t minute, int32_t second, int32_t milisecond = 0);
29: 
30: 	static bool IsValidTime(int32_t hour, int32_t minute, int32_t second, int32_t milisecond = 0);
31: 	//! Extract the time from a given timestamp object
32: 	static void Convert(dtime_t time, int32_t &out_hour, int32_t &out_min, int32_t &out_sec, int32_t &out_msec);
33: };
34: } // namespace duckdb
[end of src/include/duckdb/common/types/time.hpp]
[start of src/include/duckdb/common/types/timestamp.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/types/timestamp.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: 
13: namespace duckdb {
14: 
15: struct timestamp_struct {
16: 	int32_t year;
17: 	int8_t month;
18: 	int8_t day;
19: 	int8_t hour;
20: 	int8_t min;
21: 	int8_t sec;
22: 	int16_t msec;
23: };
24: //! The Timestamp class is a static class that holds helper functions for the Timestamp
25: //! type.
26: class Timestamp {
27: public:
28: 	//! Convert a string in the format "YYYY-MM-DD hh:mm:ss" to a timestamp object
29: 	static timestamp_t FromString(string str);
30: 	//! Convert a date object to a string in the format "YYYY-MM-DDThh:mm:ssZ"
31: 	static string ToString(timestamp_t timestamp);
32: 
33: 	static date_t GetDate(timestamp_t timestamp);
34: 
35: 	static dtime_t GetTime(timestamp_t timestamp);
36: 	//! Create a Timestamp object from a specified (date, time) combination
37: 	static timestamp_t FromDatetime(date_t date, dtime_t time);
38: 	//! Extract the date and time from a given timestamp object
39: 	static void Convert(timestamp_t date, date_t &out_date, dtime_t &out_time);
40: 	//! Returns current timestamp
41: 	static timestamp_t GetCurrentTimestamp();
42: 
43: 	// Unix epoch: milliseconds since 1970
44: 	static int64_t GetEpoch(timestamp_t timestamp);
45: 	// Seconds including fractional part multiplied by 1000
46: 	static int64_t GetMilliseconds(timestamp_t timestamp);
47: 	static int64_t GetSeconds(timestamp_t timestamp);
48: 	static int64_t GetMinutes(timestamp_t timestamp);
49: 	static int64_t GetHours(timestamp_t timestamp);
50: };
51: } // namespace duckdb
[end of src/include/duckdb/common/types/timestamp.hpp]
[start of src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/execution/operator/persistent/buffered_csv_reader.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/execution/physical_operator.hpp"
12: #include "duckdb/parser/parsed_data/copy_info.hpp"
13: #include <sstream>
14: 
15: #define SAMPLE_CHUNK_SIZE 100
16: #if STANDARD_VECTOR_SIZE < SAMPLE_CHUNK_SIZE
17: #undef SAMPLE_CHUNK_SIZE
18: #define SAMPLE_CHUNK_SIZE STANDARD_VECTOR_SIZE
19: #endif
20: 
21: namespace duckdb {
22: struct CopyInfo;
23: 
24: //! The shifts array allows for linear searching of multi-byte values. For each position, it determines the next
25: //! position given that we encounter a byte with the given value.
26: /*! For example, if we have a string "ABAC", the shifts array will have the following values:
27:  *  [0] --> ['A'] = 1, all others = 0
28:  *  [1] --> ['B'] = 2, ['A'] = 1, all others = 0
29:  *  [2] --> ['A'] = 3, all others = 0
30:  *  [3] --> ['C'] = 4 (match), 'B' = 2, 'A' = 1, all others = 0
31:  * Suppose we then search in the following string "ABABAC", our progression will be as follows:
32:  * 'A' -> [1], 'B' -> [2], 'A' -> [3], 'B' -> [2], 'A' -> [3], 'C' -> [4] (match!)
33:  */
34: struct TextSearchShiftArray {
35: 	TextSearchShiftArray();
36: 	TextSearchShiftArray(string search_term);
37: 
38: 	inline bool Match(uint8_t &position, uint8_t byte_value) {
39: 		position = shifts[position * 255 + byte_value];
40: 		return position == length;
41: 	}
42: 
43: 	idx_t length;
44: 	unique_ptr<uint8_t[]> shifts;
45: };
46: 
47: struct BufferedCSVReaderOptions  {
48: 	//! The file path of the CSV file to read
49: 	string file_path;
50:     //! Whether or not to automatically detect dialect and datatypes
51:     bool auto_detect;
52:     //! Delimiter to separate columns within each line
53:     string delimiter;
54:     //! Quote used for columns that contain reserved characters, e.g., delimiter
55:     string quote;
56:     //! Escape character to escape quote character
57:     string escape;
58:     //! Whether or not the file has a header line
59:     bool header = false;
60:     //! How many leading rows to skip
61:     idx_t skip_rows = 0;
62:     //! Expected number of columns
63:     idx_t num_cols = 0;
64:     //! Specifies the string that represents a null value
65:     string null_str;
66:     //! True, if column with that index must skip null check
67:     vector<bool> force_not_null;
68: };
69: 
70: enum class QuoteRule : uint8_t { QUOTES_RFC = 0, QUOTES_OTHER = 1, NO_QUOTES = 2 };
71: 
72: enum class ParserMode : uint8_t { PARSING = 0, SNIFFING_DIALECT = 1, SNIFFING_DATATYPES = 2 };
73: 
74: static DataChunk DUMMY_CHUNK;
75: 
76: //! Buffered CSV reader is a class that reads values from a stream and parses them as a CSV file
77: class BufferedCSVReader {
78: 	//! Initial buffer read size; can be extended for long lines
79: 	static constexpr idx_t INITIAL_BUFFER_SIZE = 16384;
80: 	//! Maximum CSV line size: specified because if we reach this amount, we likely have the wrong delimiters
81: 	static constexpr idx_t MAXIMUM_CSV_LINE_SIZE = 1048576;
82: 	static constexpr uint8_t MAX_SAMPLE_CHUNKS = 10;
83: 	ParserMode mode;
84: 
85: public:
86: 	BufferedCSVReader(ClientContext &context, BufferedCSVReaderOptions options, vector<SQLType> requested_types = vector<SQLType>());
87: 	BufferedCSVReader(BufferedCSVReaderOptions options, vector<SQLType> requested_types, unique_ptr<std::istream> source);
88: 
89:     BufferedCSVReaderOptions options;
90: 	vector<SQLType> sql_types;
91: 	vector<string> col_names;
92: 	unique_ptr<std::istream> source;
93: 	bool plain_file_source = false;
94: 	idx_t file_size = 0;
95: 
96: 	unique_ptr<char[]> buffer;
97: 	idx_t buffer_size;
98: 	idx_t position;
99: 	idx_t start = 0;
100: 
101: 	idx_t linenr = 0;
102: 	bool linenr_estimated = false;
103: 
104: 	vector<idx_t> sniffed_column_counts;
105: 	uint8_t sample_chunk_idx = 0;
106: 	bool jumping_samples = false;
107: 
108: 	idx_t bytes_in_chunk = 0;
109: 	double bytes_per_line_avg = 0;
110: 
111: 	vector<unique_ptr<char[]>> cached_buffers;
112: 
113: 	TextSearchShiftArray delimiter_search, escape_search, quote_search;
114: 
115: 	DataChunk parse_chunk;
116: 
117: public:
118: 	//! Extract a single DataChunk from the CSV file and stores it in insert_chunk
119: 	void ParseCSV(DataChunk &insert_chunk);
120: 
121: private:
122: 	//! Initialize Parser
123: 	void Initialize(vector<SQLType> requested_types);
124: 	//! Initializes the parse_chunk with varchar columns and aligns info with new number of cols
125: 	void InitParseChunk(idx_t num_cols);
126: 	//! Initializes the TextSearchShiftArrays for complex parser
127: 	void PrepareComplexParser();
128: 	//! Extract a single DataChunk from the CSV file and stores it in insert_chunk
129: 	void ParseCSV(ParserMode mode, DataChunk &insert_chunk = DUMMY_CHUNK);
130: 	//! Sniffs CSV dialect and determines skip rows, header row, column types and column names
131: 	vector<SQLType> SniffCSV(vector<SQLType> requested_types);
132: 	//! Skips header rows and skip_rows in the input stream
133: 	void SkipHeader();
134: 	//! Jumps back to the beginning of input stream and resets necessary internal states
135: 	void JumpToBeginning();
136: 	//! Jumps back to the beginning of input stream and resets necessary internal states
137: 	bool JumpToNextSample();
138: 	//! Resets the buffer
139: 	void ResetBuffer();
140: 	//! Resets the steam
141: 	void ResetStream();
142: 	//! Resets the parse_chunk and related internal states, keep_types keeps the parse_chunk initialized
143: 	void ResetParseChunk();
144: 
145: 	//! Parses a CSV file with a one-byte delimiter, escape and quote character
146: 	void ParseSimpleCSV(DataChunk &insert_chunk);
147: 	//! Parses more complex CSV files with multi-byte delimiters, escapes or quotes
148: 	void ParseComplexCSV(DataChunk &insert_chunk);
149: 
150: 	//! Adds a value to the current row
151: 	void AddValue(char *str_val, idx_t length, idx_t &column, vector<idx_t> &escape_positions);
152: 	//! Adds a row to the insert_chunk, returns true if the chunk is filled as a result of this row being added
153: 	bool AddRow(DataChunk &insert_chunk, idx_t &column);
154: 	//! Finalizes a chunk, parsing all values that have been added so far and adding them to the insert_chunk
155: 	void Flush(DataChunk &insert_chunk);
156: 	//! Reads a new buffer from the CSV file if the current one has been exhausted
157: 	bool ReadBuffer(idx_t &start);
158: 
159: 	unique_ptr<std::istream> OpenCSV(ClientContext &context, BufferedCSVReaderOptions options);
160: };
161: 
162: } // namespace duckdb
[end of src/include/duckdb/execution/operator/persistent/buffered_csv_reader.hpp]
[start of src/include/duckdb/function/scalar/date_functions.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/function/scalar/date_functions.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/function/scalar_function.hpp"
12: #include "duckdb/function/function_set.hpp"
13: 
14: namespace duckdb {
15: 
16: struct AgeFun {
17: 	static void RegisterFunction(BuiltinFunctions &set);
18: };
19: 
20: struct DatePartFun {
21: 	static void RegisterFunction(BuiltinFunctions &set);
22: };
23: 
24: struct DateTruncFun {
25: 	static void RegisterFunction(BuiltinFunctions &set);
26: };
27: 
28: struct CurrentTimeFun {
29: 	static void RegisterFunction(BuiltinFunctions &set);
30: };
31: 
32: struct CurrentDateFun {
33: 	static void RegisterFunction(BuiltinFunctions &set);
34: };
35: 
36: struct CurrentTimestampFun {
37: 	static void RegisterFunction(BuiltinFunctions &set);
38: };
39: 
40: struct EpochFun {
41: 	static void RegisterFunction(BuiltinFunctions &set);
42: };
43: 
44: } // namespace duckdb
[end of src/include/duckdb/function/scalar/date_functions.hpp]
[start of src/include/duckdb/function/table_function.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/function/table_function.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/function/function.hpp"
12: 
13: namespace duckdb {
14: 
15: //! Function used for determining the return type of a table producing function
16: typedef unique_ptr<FunctionData> (*table_function_bind_t)(ClientContext &context, vector<Value> inputs,
17:                                                           vector<SQLType> &return_types, vector<string> &names);
18: //! Type used for table-returning function
19: typedef void (*table_function_t)(ClientContext &context, vector<Value> &input, DataChunk &output,
20:                                  FunctionData *dataptr);
21: //! Type used for final (cleanup) function
22: typedef void (*table_function_final_t)(ClientContext &context, FunctionData *dataptr);
23: 
24: class TableFunction : public SimpleFunction {
25: public:
26: 	TableFunction(string name, vector<SQLType> arguments, table_function_bind_t bind, table_function_t function,
27: 	              table_function_final_t final = nullptr, bool supports_projection = false)
28: 	    : SimpleFunction(name, move(arguments)), bind(bind), function(function), final(final), supports_projection(supports_projection) {
29: 	}
30: 	TableFunction(vector<SQLType> arguments, table_function_bind_t bind, table_function_t function,
31: 	              table_function_final_t final = nullptr, bool supports_projection = false)
32: 	    : TableFunction(string(), move(arguments), bind, function, final, supports_projection) {
33: 	}
34: 
35: 
36: 	//! The bind function
37: 	table_function_bind_t bind;
38: 	//! The function pointer
39: 	table_function_t function;
40: 	//! Final function pointer
41: 	table_function_final_t final;
42: 	//! Whether or not the table function supports projection
43: 	bool supports_projection;
44: 
45: 	string ToString() {
46: 		return Function::CallToString(name, arguments);
47: 	}
48: };
49: 
50: } // namespace duckdb
[end of src/include/duckdb/function/table_function.hpp]
[start of src/main/relation/read_csv_relation.cpp]
1: #include "duckdb/main/relation/read_csv_relation.hpp"
2: #include "duckdb/parser/tableref/table_function_ref.hpp"
3: #include "duckdb/parser/tableref/basetableref.hpp"
4: #include "duckdb/parser/query_node/select_node.hpp"
5: #include "duckdb/parser/expression/star_expression.hpp"
6: #include "duckdb/parser/expression/constant_expression.hpp"
7: #include "duckdb/parser/expression/function_expression.hpp"
8: #include "duckdb/common/string_util.hpp"
9: 
10: namespace duckdb {
11: 
12: ReadCSVRelation::ReadCSVRelation(ClientContext &context, string csv_file_p, vector<ColumnDefinition> columns_p,
13:                                  string alias_p)
14:     : Relation(context, RelationType::READ_CSV_RELATION), csv_file(move(csv_file_p)), alias(move(alias_p)),
15:       columns(move(columns_p)) {
16: 	if (alias.empty()) {
17: 		alias = StringUtil::Split(csv_file, ".")[0];
18: 	}
19: }
20: 
21: unique_ptr<QueryNode> ReadCSVRelation::GetQueryNode() {
22: 	auto result = make_unique<SelectNode>();
23: 	result->select_list.push_back(make_unique<StarExpression>());
24: 	result->from_table = GetTableRef();
25: 	return move(result);
26: }
27: 
28: unique_ptr<TableRef> ReadCSVRelation::GetTableRef() {
29: 	auto table_ref = make_unique<TableFunctionRef>();
30: 	table_ref->alias = alias;
31: 	vector<unique_ptr<ParsedExpression>> children;
32: 	// CSV file
33: 	children.push_back(make_unique<ConstantExpression>(SQLType::VARCHAR, Value(csv_file)));
34: 	children.push_back(make_unique<ConstantExpression>(SQLType::VARCHAR, Value(",")));
35: 	// parameters
36: 	child_list_t<Value> column_names;
37: 	for (idx_t i = 0; i < columns.size(); i++) {
38: 		column_names.push_back(make_pair(columns[i].name, Value(SQLTypeToString(columns[i].type))));
39: 	}
40: 	children.push_back(make_unique<ConstantExpression>(SQLType::STRUCT, Value::STRUCT(move(column_names))));
41: 	table_ref->function = make_unique<FunctionExpression>("read_csv", children);
42: 	return move(table_ref);
43: }
44: 
45: string ReadCSVRelation::GetAlias() {
46: 	return alias;
47: }
48: 
49: const vector<ColumnDefinition> &ReadCSVRelation::Columns() {
50: 	return columns;
51: }
52: 
53: string ReadCSVRelation::ToString(idx_t depth) {
54: 	return RenderWhitespace(depth) + "Read CSV [" + csv_file + "]";
55: }
56: 
57: } // namespace duckdb
[end of src/main/relation/read_csv_relation.cpp]
[start of src/planner/binder/tableref/bind_table_function.cpp]
1: #include "duckdb/catalog/catalog.hpp"
2: #include "duckdb/parser/expression/function_expression.hpp"
3: #include "duckdb/parser/tableref/table_function_ref.hpp"
4: #include "duckdb/planner/binder.hpp"
5: #include "duckdb/planner/expression_binder/constant_binder.hpp"
6: #include "duckdb/planner/tableref/bound_table_function.hpp"
7: #include "duckdb/execution/expression_executor.hpp"
8: #include "duckdb/common/algorithm.hpp"
9: 
10: using namespace duckdb;
11: using namespace std;
12: 
13: unique_ptr<BoundTableRef> Binder::Bind(TableFunctionRef &ref) {
14: 	auto bind_index = GenerateTableIndex();
15: 
16: 	assert(ref.function->type == ExpressionType::FUNCTION);
17: 	auto fexpr = (FunctionExpression *)ref.function.get();
18: 
19: 	// evalate the input parameters to the function
20: 	vector<SQLType> arguments;
21: 	vector<Value> parameters;
22: 	for (auto &child : fexpr->children) {
23: 		ConstantBinder binder(*this, context, "TABLE FUNCTION parameter");
24: 		SQLType sql_type;
25: 		auto expr = binder.Bind(child, &sql_type);
26: 		auto constant = ExpressionExecutor::EvaluateScalar(*expr);
27: 		constant.SetSQLType(sql_type);
28: 
29: 		arguments.push_back(sql_type);
30: 		parameters.push_back(move(constant));
31: 	}
32: 	// fetch the function from the catalog
33: 	auto function =
34: 	    Catalog::GetCatalog(context).GetEntry<TableFunctionCatalogEntry>(context, fexpr->schema, fexpr->function_name);
35: 
36: 	// select the function based on the input parameters
37: 	idx_t best_function_idx = Function::BindFunction(function->name, function->functions, arguments);
38: 	auto &table_function = function->functions[best_function_idx];
39: 
40: 	// cast the parameters to the type of the function
41: 	auto result = make_unique<BoundTableFunction>(table_function, bind_index);
42: 	for(idx_t i = 0; i < arguments.size(); i++) {
43: 		if (table_function.arguments[i] == SQLType::ANY) {
44: 			result->parameters.push_back(move(parameters[i]));
45: 		} else {
46: 			result->parameters.push_back(parameters[i].CastAs(arguments[i], table_function.arguments[i]));
47: 		}
48: 	}
49: 
50: 	// perform the binding
51: 	result->bind_data = table_function.bind(context, result->parameters, result->return_types, result->names);
52: 	assert(result->return_types.size() == result->names.size());
53: 	assert(result->return_types.size() > 0);
54: 	vector<string> names;
55: 	// first push any column name aliases
56: 	for(idx_t i = 0; i < min<idx_t>(ref.column_name_alias.size(), result->names.size()); i++) {
57: 		names.push_back(ref.column_name_alias[i]);
58: 	}
59: 	// then fill up the remainder with the given result names
60: 	for(idx_t i = names.size(); i < result->names.size(); i++) {
61: 		names.push_back(result->names[i]);
62: 	}
63: 	// now add the table function to the bind context so its columns can be bound
64: 	bind_context.AddGenericBinding(bind_index, ref.alias.empty() ? fexpr->function_name : ref.alias, names,
65: 	                               result->return_types);
66: 
67: 	return move(result);
68: }
[end of src/planner/binder/tableref/bind_table_function.cpp]
[start of src/storage/buffer_manager.cpp]
1: #include "duckdb/storage/buffer_manager.hpp"
2: 
3: #include "duckdb/common/exception.hpp"
4: 
5: using namespace duckdb;
6: using namespace std;
7: 
8: BufferManager::BufferManager(FileSystem &fs, BlockManager &manager, string tmp, idx_t maximum_memory)
9:     : fs(fs), manager(manager), current_memory(0), maximum_memory(maximum_memory), temp_directory(move(tmp)),
10:       temporary_id(MAXIMUM_BLOCK) {
11: 	if (!temp_directory.empty()) {
12: 		fs.CreateDirectory(temp_directory);
13: 	}
14: }
15: 
16: BufferManager::~BufferManager() {
17: 	if (!temp_directory.empty()) {
18: 		fs.RemoveDirectory(temp_directory);
19: 	}
20: }
21: 
22: unique_ptr<BufferHandle> BufferManager::Pin(block_id_t block_id, bool can_destroy) {
23: 	// first obtain a lock on the set of blocks
24: 	lock_guard<mutex> lock(block_lock);
25: 	if (block_id < MAXIMUM_BLOCK) {
26: 		return PinBlock(block_id);
27: 	} else {
28: 		return PinBuffer(block_id, can_destroy);
29: 	}
30: }
31: 
32: unique_ptr<BufferHandle> BufferManager::PinBlock(block_id_t block_id) {
33: 	// this method should only be used to pin blocks that exist in the file
34: 	assert(block_id < MAXIMUM_BLOCK);
35: 
36: 	// check if the block is already loaded
37: 	Block *result_block;
38: 	auto entry = blocks.find(block_id);
39: 	if (entry == blocks.end()) {
40: 		// block is not loaded, load the block
41: 		current_memory += Storage::BLOCK_ALLOC_SIZE;
42: 		unique_ptr<Block> block;
43: 		if (current_memory > maximum_memory) {
44: 			// not enough memory to hold the block: have to evict a block first
45: 			block = EvictBlock();
46: 			if (!block) {
47: 				// evicted a managed buffer: no block returned
48: 				// create a new block
49: 				block = make_unique<Block>(block_id);
50: 			} else {
51: 				// take over the evicted block and use it to hold this block
52: 				block->id = block_id;
53: 			}
54: 		} else {
55: 			// enough memory to create a new block: allocate it
56: 			block = make_unique<Block>(block_id);
57: 		}
58: 		manager.Read(*block);
59: 		result_block = block.get();
60: 		// create a new buffer entry for this block and insert it into the block list
61: 		auto buffer_entry = make_unique<BufferEntry>(move(block));
62: 		blocks.insert(make_pair(block_id, buffer_entry.get()));
63: 		used_list.Append(move(buffer_entry));
64: 	} else {
65: 		auto buffer = entry->second->buffer.get();
66: 		assert(buffer->type == FileBufferType::BLOCK);
67: 		result_block = (Block *)buffer;
68: 		// add one to the reference count
69: 		AddReference(entry->second);
70: 	}
71: 	return make_unique<BufferHandle>(*this, block_id, result_block);
72: }
73: 
74: void BufferManager::AddReference(BufferEntry *entry) {
75: 	entry->ref_count++;
76: 	if (entry->ref_count == 1) {
77: 		// ref count is 1, that means it used to be 0 (unused)
78: 		// move from lru to used_list
79: 		auto current_entry = lru.Erase(entry);
80: 		used_list.Append(move(current_entry));
81: 	}
82: }
83: 
84: void BufferManager::Unpin(block_id_t block_id) {
85: 	lock_guard<mutex> lock(block_lock);
86: 	// first find the block in the set of blocks
87: 	auto entry = blocks.find(block_id);
88: 	assert(entry != blocks.end());
89: 
90: 	auto buffer_entry = entry->second;
91: 	// then decerase the ref count
92: 	assert(buffer_entry->ref_count > 0);
93: 	buffer_entry->ref_count--;
94: 	if (buffer_entry->ref_count == 0) {
95: 		if (buffer_entry->buffer->type == FileBufferType::MANAGED_BUFFER) {
96: 			auto managed = (ManagedBuffer *)buffer_entry->buffer.get();
97: 			if (managed->can_destroy) {
98: 				// this is a managed buffer that we can destroy
99: 				// instead of adding it to the LRU list, just deallocate the managed buffer immediately
100: 				current_memory -= managed->size;
101: 				return;
102: 			}
103: 		}
104: 		// no references left: move block out of used list and into lru list
105: 		auto entry = used_list.Erase(buffer_entry);
106: 		lru.Append(move(entry));
107: 	}
108: }
109: 
110: unique_ptr<Block> BufferManager::EvictBlock() {
111: 	if (temp_directory.empty()) {
112: 		throw Exception("Out-of-memory: cannot evict buffer because no temporary directory is specified!\nTo enable "
113: 		                "temporary buffer eviction set a temporary directory in the configuration");
114: 	}
115: 	// pop the first entry from the lru list
116: 	auto entry = lru.Pop();
117: 	if (!entry) {
118: 		throw Exception("Not enough memory to complete operation!");
119: 	}
120: 	assert(entry->ref_count == 0);
121: 	// erase this identifier from the set of blocks
122: 	auto buffer = entry->buffer.get();
123: 	if (buffer->type == FileBufferType::BLOCK) {
124: 		// block buffer: remove the block and reuse it
125: 		auto block = (Block *)buffer;
126: 		blocks.erase(block->id);
127: 		// free up the memory
128: 		current_memory -= Storage::BLOCK_ALLOC_SIZE;
129: 		// finally return the block obtained from the current entry
130: 		return unique_ptr_cast<FileBuffer, Block>(move(entry->buffer));
131: 	} else {
132: 		// managed buffer: cannot return a block here
133: 		auto managed = (ManagedBuffer *)buffer;
134: 		assert(!managed->can_destroy);
135: 
136: 		// cannot destroy this buffer: write it to disk first so it can be reloaded later
137: 		WriteTemporaryBuffer(*managed);
138: 
139: 		blocks.erase(managed->id);
140: 		// free up the memory
141: 		current_memory -= managed->size;
142: 		return nullptr;
143: 	}
144: }
145: 
146: unique_ptr<BufferHandle> BufferManager::Allocate(idx_t alloc_size, bool can_destroy) {
147: 	assert(alloc_size >= Storage::BLOCK_ALLOC_SIZE);
148: 
149: 	lock_guard<mutex> lock(block_lock);
150: 	// first evict blocks until we have enough memory to store this buffer
151: 	while (current_memory + alloc_size > maximum_memory) {
152: 		EvictBlock();
153: 	}
154: 	// now allocate the buffer with a new temporary id
155: 	auto temp_id = ++temporary_id;
156: 	auto buffer = make_unique<ManagedBuffer>(*this, alloc_size, can_destroy, temp_id);
157: 	auto managed_buffer = buffer.get();
158: 	current_memory += buffer->AllocSize();
159: 	// create a new entry and append it to the used list
160: 	auto buffer_entry = make_unique<BufferEntry>(move(buffer));
161: 	blocks.insert(make_pair(temp_id, buffer_entry.get()));
162: 	used_list.Append(move(buffer_entry));
163: 	// now return a handle to the entry
164: 	return make_unique<BufferHandle>(*this, temp_id, managed_buffer);
165: }
166: 
167: void BufferManager::DestroyBuffer(block_id_t buffer_id, bool can_destroy) {
168: 	lock_guard<mutex> lock(block_lock);
169: 
170: 	assert(buffer_id >= MAXIMUM_BLOCK);
171: 	// this is like unpin, except we just destroy the entry entirely instead of adding it to the LRU list
172: 	// first find the block in the set of blocks
173: 	auto entry = blocks.find(buffer_id);
174: 	if (entry == blocks.end()) {
175: 		// buffer is not currently loaded into memory
176: 		// check if it was offloaded to disk instead
177: 		if (!can_destroy) {
178: 			// buffer was offloaded to disk: remove the file instead
179: 			DeleteTemporaryFile(buffer_id);
180: 		}
181: 		return;
182: 	}
183: 
184: 	auto handle = entry->second;
185: 	assert(handle->ref_count == 0);
186: 
187: 	current_memory -= handle->buffer->AllocSize();
188: 	blocks.erase(buffer_id);
189: 	lru.Erase(handle);
190: }
191: 
192: void BufferManager::SetLimit(idx_t limit) {
193: 	lock_guard<mutex> lock(block_lock);
194: 
195: 	while (current_memory > limit) {
196: 		EvictBlock();
197: 	}
198: 	maximum_memory = limit;
199: }
200: 
201: unique_ptr<BufferHandle> BufferManager::PinBuffer(block_id_t buffer_id, bool can_destroy) {
202: 	assert(buffer_id >= MAXIMUM_BLOCK);
203: 	// check if we have this buffer here
204: 	auto entry = blocks.find(buffer_id);
205: 	if (entry == blocks.end()) {
206: 		if (can_destroy) {
207: 			// buffer was destroyed: return nullptr
208: 			return nullptr;
209: 		} else {
210: 			// buffer was unloaded but not destroyed: read from disk
211: 			return ReadTemporaryBuffer(buffer_id);
212: 		}
213: 	}
214: 	// we still have the buffer, add a reference to it
215: 	auto buffer = entry->second->buffer.get();
216: 	AddReference(entry->second);
217: 	// now return it
218: 	assert(buffer->type == FileBufferType::MANAGED_BUFFER);
219: 	auto managed = (ManagedBuffer *)buffer;
220: 	assert(managed->id == buffer_id);
221: 	return make_unique<BufferHandle>(*this, buffer_id, managed);
222: }
223: 
224: string BufferManager::GetTemporaryPath(block_id_t id) {
225: 	return fs.JoinPath(temp_directory, to_string(id) + ".block");
226: }
227: 
228: void BufferManager::WriteTemporaryBuffer(ManagedBuffer &buffer) {
229: 	assert(buffer.size + Storage::BLOCK_HEADER_SIZE >= Storage::BLOCK_ALLOC_SIZE);
230: 	// get the path to write to
231: 	auto path = GetTemporaryPath(buffer.id);
232: 	// create the file and write the size followed by the buffer contents
233: 	auto handle = fs.OpenFile(path, FileFlags::WRITE | FileFlags::FILE_CREATE);
234: 	handle->Write(&buffer.size, sizeof(idx_t), 0);
235: 	buffer.Write(*handle, sizeof(idx_t));
236: }
237: 
238: unique_ptr<BufferHandle> BufferManager::ReadTemporaryBuffer(block_id_t id) {
239: 	if (temp_directory.empty()) {
240: 		throw Exception("Out-of-memory: cannot read buffer because no temporary directory is specified!\nTo enable "
241: 		                "temporary buffer eviction set a temporary directory in the configuration");
242: 	}
243: 	idx_t alloc_size;
244: 	// open the temporary file and read the size
245: 	auto path = GetTemporaryPath(id);
246: 	auto handle = fs.OpenFile(path, FileFlags::READ);
247: 	handle->Read(&alloc_size, sizeof(idx_t), 0);
248: 	// first evict blocks until we can handle the size
249: 	while (current_memory + alloc_size > maximum_memory) {
250: 		EvictBlock();
251: 	}
252: 	// now allocate a buffer of this size and read the data into that buffer
253: 	auto buffer = make_unique<ManagedBuffer>(*this, alloc_size + Storage::BLOCK_HEADER_SIZE, false, id);
254: 	buffer->Read(*handle, sizeof(idx_t));
255: 
256: 	auto managed_buffer = buffer.get();
257: 	current_memory += buffer->AllocSize();
258: 	// create a new entry and append it to the used list
259: 	auto buffer_entry = make_unique<BufferEntry>(move(buffer));
260: 	blocks.insert(make_pair(id, buffer_entry.get()));
261: 	used_list.Append(move(buffer_entry));
262: 	// now return a handle to the entry
263: 	return make_unique<BufferHandle>(*this, id, managed_buffer);
264: }
265: 
266: void BufferManager::DeleteTemporaryFile(block_id_t id) {
267: 	auto path = GetTemporaryPath(id);
268: 	if (fs.FileExists(path)) {
269: 		fs.RemoveFile(path);
270: 	}
271: }
[end of src/storage/buffer_manager.cpp]
[start of src/storage/single_file_block_manager.cpp]
1: #include "duckdb/storage/single_file_block_manager.hpp"
2: #include "duckdb/storage/meta_block_writer.hpp"
3: #include "duckdb/storage/meta_block_reader.hpp"
4: #include "duckdb/common/exception.hpp"
5: 
6: #include <algorithm>
7: 
8: using namespace duckdb;
9: using namespace std;
10: 
11: SingleFileBlockManager::SingleFileBlockManager(FileSystem &fs, string path, bool read_only, bool create_new,
12:                                                bool use_direct_io)
13:     : path(path), header_buffer(FileBufferType::MANAGED_BUFFER, Storage::FILE_HEADER_SIZE), read_only(read_only),
14:       use_direct_io(use_direct_io) {
15: 
16: 	uint8_t flags;
17: 	FileLockType lock;
18: 	if (read_only) {
19: 		assert(!create_new);
20: 		flags = FileFlags::READ;
21: 		lock = FileLockType::READ_LOCK;
22: 	} else {
23: 		flags = FileFlags::WRITE;
24: 		lock = FileLockType::WRITE_LOCK;
25: 		if (create_new) {
26: 			flags |= FileFlags::FILE_CREATE;
27: 		}
28: 	}
29: 	if (use_direct_io) {
30: 		flags |= FileFlags::DIRECT_IO;
31: 	}
32: 	// open the RDBMS handle
33: 	handle = fs.OpenFile(path, flags, lock);
34: 	if (create_new) {
35: 		// if we create a new file, we fill the metadata of the file
36: 		// first fill in the new header
37: 		header_buffer.Clear();
38: 		MainHeader *main_header = (MainHeader *)header_buffer.buffer;
39: 		main_header->version_number = VERSION_NUMBER;
40: 		// now write the header to the file
41: 		header_buffer.Write(*handle, 0);
42: 		header_buffer.Clear();
43: 
44: 		// write the database headers
45: 		// initialize meta_block and free_list to INVALID_BLOCK because the database file does not contain any actual
46: 		// content yet
47: 		DatabaseHeader *header = (DatabaseHeader *)header_buffer.buffer;
48: 		// header 1
49: 		header->iteration = 0;
50: 		header->meta_block = INVALID_BLOCK;
51: 		header->free_list = INVALID_BLOCK;
52: 		header->block_count = 0;
53: 		header_buffer.Write(*handle, Storage::FILE_HEADER_SIZE);
54: 		// header 2
55: 		header->iteration = 1;
56: 		header_buffer.Write(*handle, Storage::FILE_HEADER_SIZE * 2);
57: 		// ensure that writing to disk is completed before returning
58: 		handle->Sync();
59: 		// we start with h2 as active_header, this way our initial write will be in h1
60: 		active_header = 1;
61: 		max_block = 0;
62: 	} else {
63: 		MainHeader header;
64: 		// otherwise, we check the metadata of the file
65: 		header_buffer.Read(*handle, 0);
66: 		header = *((MainHeader *)header_buffer.buffer);
67: 		// check the version number
68: 		if (header.version_number != VERSION_NUMBER) {
69: 			throw IOException(
70: 			    "Trying to read a database file with version number %lld, but we can only read version %lld",
71: 			    header.version_number, VERSION_NUMBER);
72: 		}
73: 		// read the database headers from disk
74: 		DatabaseHeader h1, h2;
75: 		header_buffer.Read(*handle, Storage::FILE_HEADER_SIZE);
76: 		h1 = *((DatabaseHeader *)header_buffer.buffer);
77: 		header_buffer.Read(*handle, Storage::FILE_HEADER_SIZE * 2);
78: 		h2 = *((DatabaseHeader *)header_buffer.buffer);
79: 		// check the header with the highest iteration count
80: 		if (h1.iteration > h2.iteration) {
81: 			// h1 is active header
82: 			active_header = 0;
83: 			Initialize(h1);
84: 		} else {
85: 			// h2 is active header
86: 			active_header = 1;
87: 			Initialize(h2);
88: 		}
89: 	}
90: }
91: 
92: void SingleFileBlockManager::Initialize(DatabaseHeader &header) {
93: 	free_list_id = header.free_list;
94: 	meta_block = header.meta_block;
95: 	iteration_count = header.iteration;
96: 	max_block = header.block_count;
97: }
98: 
99: void SingleFileBlockManager::LoadFreeList(BufferManager &manager) {
100: 	if (read_only) {
101: 		// no need to load free list for read only db
102: 		return;
103: 	}
104: 	if (free_list_id == INVALID_BLOCK) {
105: 		// no free list
106: 		return;
107: 	}
108: 	MetaBlockReader reader(manager, free_list_id);
109: 	auto free_list_count = reader.Read<uint64_t>();
110: 	free_list.clear();
111: 	free_list.reserve(free_list_count);
112: 	for (idx_t i = 0; i < free_list_count; i++) {
113: 		free_list.push_back(reader.Read<block_id_t>());
114: 	}
115: }
116: 
117: void SingleFileBlockManager::StartCheckpoint() {
118: 	used_blocks.clear();
119: }
120: 
121: block_id_t SingleFileBlockManager::GetFreeBlockId() {
122: 	block_id_t block;
123: 	if (free_list.size() > 0) {
124: 		// free list is non empty
125: 		// take an entry from the free list
126: 		block = free_list.back();
127: 		// erase the entry from the free list again
128: 		free_list.pop_back();
129: 	} else {
130: 		block = max_block++;
131: 	}
132: 	used_blocks.insert(block);
133: 	return block;
134: }
135: 
136: block_id_t SingleFileBlockManager::GetMetaBlock() {
137: 	return meta_block;
138: }
139: 
140: unique_ptr<Block> SingleFileBlockManager::CreateBlock() {
141: 	return make_unique<Block>(GetFreeBlockId());
142: }
143: 
144: void SingleFileBlockManager::Read(Block &block) {
145: 	assert(block.id >= 0);
146: 	assert(std::find(free_list.begin(), free_list.end(), block.id) == free_list.end());
147: 	block.Read(*handle, BLOCK_START + block.id * Storage::BLOCK_ALLOC_SIZE);
148: }
149: 
150: void SingleFileBlockManager::Write(FileBuffer &buffer, block_id_t block_id) {
151: 	assert(block_id >= 0);
152: 	buffer.Write(*handle, BLOCK_START + block_id * Storage::BLOCK_ALLOC_SIZE);
153: }
154: 
155: void SingleFileBlockManager::WriteHeader(DatabaseHeader header) {
156: 	// set the iteration count
157: 	header.iteration = ++iteration_count;
158: 	header.block_count = max_block;
159: 	// now handle the free list
160: 	free_list.clear();
161: 	for (block_id_t i = 0; i < max_block; i++) {
162: 		if (used_blocks.find(i) == used_blocks.end()) {
163: 			free_list.push_back(i);
164: 		}
165: 	}
166: 	if (free_list.size() > 0) {
167: 		// there are blocks in the free list
168: 		// write them to the file
169: 		MetaBlockWriter writer(*this);
170: 		auto entry = std::find(free_list.begin(), free_list.end(), writer.block->id);
171: 		if (entry != free_list.end()) {
172: 			free_list.erase(entry);
173: 		}
174: 		header.free_list = writer.block->id;
175: 
176: 		writer.Write<uint64_t>(free_list.size());
177: 		for (auto &block_id : free_list) {
178: 			writer.Write<block_id_t>(block_id);
179: 		}
180: 		writer.Flush();
181: 	} else {
182: 		// no blocks in the free list
183: 		header.free_list = INVALID_BLOCK;
184: 	}
185: 	if (!use_direct_io) {
186: 		// if we are not using Direct IO we need to fsync BEFORE we write the header to ensure that all the previous
187: 		// blocks are written as well
188: 		handle->Sync();
189: 	}
190: 	// set the header inside the buffer
191: 	header_buffer.Clear();
192: 	*((DatabaseHeader *)header_buffer.buffer) = header;
193: 	// now write the header to the file, active_header determines whether we write to h1 or h2
194: 	// note that if active_header is h1 we write to h2, and vice versa
195: 	header_buffer.Write(*handle, active_header == 1 ? Storage::FILE_HEADER_SIZE : Storage::FILE_HEADER_SIZE * 2);
196: 	// switch active header to the other header
197: 	active_header = 1 - active_header;
198: 	//! Ensure the header write ends up on disk
199: 	handle->Sync();
200: 
201: 	// the free list is now equal to the blocks that were used by the previous iteration
202: 	free_list.clear();
203: 	for (auto &block_id : used_blocks) {
204: 		free_list.push_back(block_id);
205: 	}
206: 	used_blocks.clear();
207: }
[end of src/storage/single_file_block_manager.cpp]
[start of src/storage/write_ahead_log.cpp]
1: #include "duckdb/storage/write_ahead_log.hpp"
2: #include "duckdb/main/database.hpp"
3: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
4: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
5: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
6: #include "duckdb/parser/parsed_data/alter_table_info.hpp"
7: #include <cstring>
8: 
9: using namespace duckdb;
10: using namespace std;
11: 
12: WriteAheadLog::WriteAheadLog(DuckDB &database) : initialized(false), database(database) {
13: }
14: 
15: void WriteAheadLog::Initialize(string &path) {
16: 	writer = make_unique<BufferedFileWriter>(database.GetFileSystem(), path.c_str(), FileFlags::WRITE | FileFlags::FILE_CREATE | FileFlags::APPEND);
17: 	initialized = true;
18: }
19: 
20: int64_t WriteAheadLog::GetWALSize() {
21: 	return writer->GetFileSize();
22: }
23: 
24: void WriteAheadLog::Truncate(int64_t size) {
25: 	writer->Truncate(size);
26: }
27: //===--------------------------------------------------------------------===//
28: // Write Entries
29: //===--------------------------------------------------------------------===//
30: //===--------------------------------------------------------------------===//
31: // CREATE TABLE
32: //===--------------------------------------------------------------------===//
33: void WriteAheadLog::WriteCreateTable(TableCatalogEntry *entry) {
34: 	writer->Write<WALType>(WALType::CREATE_TABLE);
35: 	entry->Serialize(*writer);
36: }
37: 
38: //===--------------------------------------------------------------------===//
39: // DROP TABLE
40: //===--------------------------------------------------------------------===//
41: void WriteAheadLog::WriteDropTable(TableCatalogEntry *entry) {
42: 	writer->Write<WALType>(WALType::DROP_TABLE);
43: 	writer->WriteString(entry->schema->name);
44: 	writer->WriteString(entry->name);
45: }
46: 
47: //===--------------------------------------------------------------------===//
48: // CREATE SCHEMA
49: //===--------------------------------------------------------------------===//
50: void WriteAheadLog::WriteCreateSchema(SchemaCatalogEntry *entry) {
51: 	writer->Write<WALType>(WALType::CREATE_SCHEMA);
52: 	writer->WriteString(entry->name);
53: }
54: 
55: //===--------------------------------------------------------------------===//
56: // SEQUENCES
57: //===--------------------------------------------------------------------===//
58: void WriteAheadLog::WriteCreateSequence(SequenceCatalogEntry *entry) {
59: 	writer->Write<WALType>(WALType::CREATE_SEQUENCE);
60: 	entry->Serialize(*writer);
61: }
62: 
63: void WriteAheadLog::WriteDropSequence(SequenceCatalogEntry *entry) {
64: 	writer->Write<WALType>(WALType::DROP_SEQUENCE);
65: 	writer->WriteString(entry->schema->name);
66: 	writer->WriteString(entry->name);
67: }
68: 
69: void WriteAheadLog::WriteSequenceValue(SequenceCatalogEntry *entry, SequenceValue val) {
70: 	writer->Write<WALType>(WALType::SEQUENCE_VALUE);
71: 	writer->WriteString(entry->schema->name);
72: 	writer->WriteString(entry->name);
73: 	writer->Write<uint64_t>(val.usage_count);
74: 	writer->Write<int64_t>(val.counter);
75: }
76: 
77: //===--------------------------------------------------------------------===//
78: // VIEWS
79: //===--------------------------------------------------------------------===//
80: void WriteAheadLog::WriteCreateView(ViewCatalogEntry *entry) {
81: 	writer->Write<WALType>(WALType::CREATE_VIEW);
82: 	entry->Serialize(*writer);
83: }
84: 
85: void WriteAheadLog::WriteDropView(ViewCatalogEntry *entry) {
86: 	writer->Write<WALType>(WALType::DROP_VIEW);
87: 	writer->WriteString(entry->schema->name);
88: 	writer->WriteString(entry->name);
89: }
90: 
91: //===--------------------------------------------------------------------===//
92: // DROP SCHEMA
93: //===--------------------------------------------------------------------===//
94: void WriteAheadLog::WriteDropSchema(SchemaCatalogEntry *entry) {
95: 	writer->Write<WALType>(WALType::DROP_SCHEMA);
96: 	writer->WriteString(entry->name);
97: }
98: 
99: //===--------------------------------------------------------------------===//
100: // DATA
101: //===--------------------------------------------------------------------===//
102: void WriteAheadLog::WriteSetTable(string &schema, string &table) {
103: 	writer->Write<WALType>(WALType::USE_TABLE);
104: 	writer->WriteString(schema);
105: 	writer->WriteString(table);
106: }
107: 
108: void WriteAheadLog::WriteInsert(DataChunk &chunk) {
109: 	assert(chunk.size() > 0);
110: 	chunk.Verify();
111: 
112: 	writer->Write<WALType>(WALType::INSERT_TUPLE);
113: 	chunk.Serialize(*writer);
114: }
115: 
116: void WriteAheadLog::WriteDelete(DataChunk &chunk) {
117: 	assert(chunk.size() > 0);
118: 	assert(chunk.column_count() == 1 && chunk.data[0].type == ROW_TYPE);
119: 	chunk.Verify();
120: 
121: 	writer->Write<WALType>(WALType::DELETE_TUPLE);
122: 	chunk.Serialize(*writer);
123: }
124: 
125: void WriteAheadLog::WriteUpdate(DataChunk &chunk, column_t col_idx) {
126: 	assert(chunk.size() > 0);
127: 	chunk.Verify();
128: 
129: 	writer->Write<WALType>(WALType::UPDATE_TUPLE);
130: 	writer->Write<column_t>(col_idx);
131: 	chunk.Serialize(*writer);
132: }
133: 
134: //===--------------------------------------------------------------------===//
135: // Write ALTER Statement
136: //===--------------------------------------------------------------------===//
137: void WriteAheadLog::WriteAlter(AlterInfo &info) {
138: 	writer->Write<WALType>(WALType::ALTER_INFO);
139: 	info.Serialize(*writer);
140: }
141: 
142: //===--------------------------------------------------------------------===//
143: // FLUSH
144: //===--------------------------------------------------------------------===//
145: void WriteAheadLog::Flush() {
146: 	// write an empty entry
147: 	writer->Write<WALType>(WALType::WAL_FLUSH);
148: 	// flushes all changes made to the WAL to disk
149: 	writer->Sync();
150: }
[end of src/storage/write_ahead_log.cpp]
[start of tools/pythonpkg/duckdb_python.cpp]
1: #include <pybind11/pybind11.h>
2: #include <pybind11/numpy.h>
3: 
4: #include <unordered_map>
5: #include <vector>
6: 
7: #include "datetime.h" // from Python
8: 
9: #include "duckdb.hpp"
10: #include "parquet-extension.hpp"
11: 
12: namespace py = pybind11;
13: 
14: using namespace duckdb;
15: using namespace std;
16: 
17: namespace duckdb_py_convert {
18: 
19: struct RegularConvert {
20: 	template <class DUCKDB_T, class NUMPY_T> static NUMPY_T convert_value(DUCKDB_T val) {
21: 		return (NUMPY_T)val;
22: 	}
23: };
24: 
25: struct TimestampConvert {
26: 	template <class DUCKDB_T, class NUMPY_T> static int64_t convert_value(timestamp_t val) {
27: 		return Date::Epoch(Timestamp::GetDate(val)) * 1000 + (int64_t)(Timestamp::GetTime(val));
28: 	}
29: };
30: 
31: struct DateConvert {
32: 	template <class DUCKDB_T, class NUMPY_T> static int64_t convert_value(date_t val) {
33: 		return Date::Epoch(val);
34: 	}
35: };
36: 
37: struct TimeConvert {
38: 	template <class DUCKDB_T, class NUMPY_T> static py::str convert_value(time_t val) {
39: 		return py::str(duckdb::Time::ToString(val).c_str());
40: 	}
41: };
42: 
43: struct StringConvert {
44: 	template <class DUCKDB_T, class NUMPY_T> static py::str convert_value(string_t val) {
45: 		return py::str(val.GetData());
46: 	}
47: };
48: 
49: struct HugeIntConvert {
50: 	template <class DUCKDB_T, class NUMPY_T> static double convert_value(hugeint_t val) {
51: 		double result;
52: 		Hugeint::TryCast(val, result);
53: 		return result;
54: 	}
55: };
56: 
57: template <class DUCKDB_T, class NUMPY_T, class CONVERT>
58: static py::array fetch_column(string numpy_type, ChunkCollection &collection, idx_t column) {
59: 	auto out = py::array(py::dtype(numpy_type), collection.count);
60: 	auto out_ptr = (NUMPY_T *)out.mutable_data();
61: 
62: 	idx_t out_offset = 0;
63: 	for (auto &data_chunk : collection.chunks) {
64: 		auto &src = data_chunk->data[column];
65: 		auto src_ptr = FlatVector::GetData<DUCKDB_T>(src);
66: 		auto &nullmask = FlatVector::Nullmask(src);
67: 		for (idx_t i = 0; i < data_chunk->size(); i++) {
68: 			if (nullmask[i]) {
69: 				continue;
70: 			}
71: 			out_ptr[i + out_offset] = CONVERT::template convert_value<DUCKDB_T, NUMPY_T>(src_ptr[i]);
72: 		}
73: 		out_offset += data_chunk->size();
74: 	}
75: 	return out;
76: }
77: 
78: template <class T> static py::array fetch_column_regular(string numpy_type, ChunkCollection &collection, idx_t column) {
79: 	return fetch_column<T, T, RegularConvert>(numpy_type, collection, column);
80: }
81: 
82: }; // namespace duckdb_py_convert
83: // namespace duckdb_py_convert
84: 
85: namespace random_string {
86: static std::random_device rd;
87: static std::mt19937 gen(rd());
88: static std::uniform_int_distribution<> dis(0, 15);
89: 
90: std::string generate() {
91: 	std::stringstream ss;
92: 	int i;
93: 	ss << std::hex;
94: 	for (i = 0; i < 16; i++) {
95: 		ss << dis(gen);
96: 	}
97: 	return ss.str();
98: }
99: } // namespace random_string
100: 
101: struct PandasScanFunctionData : public TableFunctionData {
102: 	PandasScanFunctionData(py::handle df, idx_t row_count, vector<SQLType> sql_types)
103: 	    : df(df), row_count(row_count), sql_types(sql_types), position(0) {
104: 	}
105: 	py::handle df;
106: 	idx_t row_count;
107: 	vector<SQLType> sql_types;
108: 	idx_t position;
109: };
110: 
111: struct PandasScanFunction : public TableFunction {
112: 	PandasScanFunction()
113: 	    : TableFunction("pandas_scan", {SQLType::VARCHAR}, pandas_scan_bind, pandas_scan_function, nullptr){};
114: 
115: 	static unique_ptr<FunctionData> pandas_scan_bind(ClientContext &context, vector<Value> inputs,
116: 	                                                 vector<SQLType> &return_types, vector<string> &names) {
117: 		// Hey, it works (TM)
118: 		py::handle df((PyObject *)std::stoull(inputs[0].GetValue<string>(), nullptr, 16));
119: 
120: 		/* TODO this fails on Python2 for some reason
121: 		auto pandas_mod = py::module::import("pandas.core.frame");
122: 		auto df_class = pandas_mod.attr("DataFrame");
123: 
124: 		if (!df.get_type().is(df_class)) {
125: 		    throw Exception("parameter is not a DataFrame");
126: 		} */
127: 
128: 		auto df_names = py::list(df.attr("columns"));
129: 		auto df_types = py::list(df.attr("dtypes"));
130: 		// TODO support masked arrays as well
131: 		// TODO support dicts of numpy arrays as well
132: 		if (py::len(df_names) == 0 || py::len(df_types) == 0 || py::len(df_names) != py::len(df_types)) {
133: 			throw runtime_error("Need a DataFrame with at least one column");
134: 		}
135: 		for (idx_t col_idx = 0; col_idx < py::len(df_names); col_idx++) {
136: 			auto col_type = string(py::str(df_types[col_idx]));
137: 			names.push_back(string(py::str(df_names[col_idx])));
138: 			SQLType duckdb_col_type;
139: 			if (col_type == "bool") {
140: 				duckdb_col_type = SQLType::BOOLEAN;
141: 			} else if (col_type == "int8") {
142: 				duckdb_col_type = SQLType::TINYINT;
143: 			} else if (col_type == "int16") {
144: 				duckdb_col_type = SQLType::SMALLINT;
145: 			} else if (col_type == "int32") {
146: 				duckdb_col_type = SQLType::INTEGER;
147: 			} else if (col_type == "int64") {
148: 				duckdb_col_type = SQLType::BIGINT;
149: 			} else if (col_type == "float32") {
150: 				duckdb_col_type = SQLType::FLOAT;
151: 			} else if (col_type == "float64") {
152: 				duckdb_col_type = SQLType::DOUBLE;
153: 			} else if (col_type == "datetime64[ns]") {
154: 				duckdb_col_type = SQLType::TIMESTAMP;
155: 			} else if (col_type == "object") {
156: 				// this better be strings
157: 				duckdb_col_type = SQLType::VARCHAR;
158: 			} else {
159: 				throw runtime_error("unsupported python type " + col_type);
160: 			}
161: 			return_types.push_back(duckdb_col_type);
162: 		}
163: 		idx_t row_count = py::len(df.attr("__getitem__")(df_names[0]));
164: 		return make_unique<PandasScanFunctionData>(df, row_count, return_types);
165: 	}
166: 
167: 	template <class T> static void scan_pandas_column(py::array numpy_col, idx_t count, idx_t offset, Vector &out) {
168: 		auto src_ptr = (T *)numpy_col.data();
169: 		FlatVector::SetData(out, (data_ptr_t) (src_ptr + offset));
170: 	}
171: 
172: 	template<class T>
173: 	static bool ValueIsNull(T value) {
174: 		throw runtime_error("unsupported type for ValueIsNull");
175: 	}
176: 
177: 	template <class T> static void scan_pandas_fp_column(T *src_ptr, idx_t count, idx_t offset, Vector &out) {
178: 		FlatVector::SetData(out, (data_ptr_t) (src_ptr + offset));
179: 		auto tgt_ptr = (T *)FlatVector::GetData(out);
180: 		auto &nullmask = FlatVector::Nullmask(out);
181: 		for(idx_t i = 0; i < count; i++) {
182: 			if (ValueIsNull(tgt_ptr[i])) {
183: 				nullmask[i] = true;
184: 			}
185: 		}
186: 	}
187: 
188: 	static void pandas_scan_function(ClientContext &context, vector<Value> &input, DataChunk &output,
189: 	                                 FunctionData *dataptr) {
190: 		auto &data = *((PandasScanFunctionData *)dataptr);
191: 
192: 		if (data.position >= data.row_count) {
193: 			return;
194: 		}
195: 		idx_t this_count = std::min((idx_t)STANDARD_VECTOR_SIZE, data.row_count - data.position);
196: 
197: 		auto df_names = py::list(data.df.attr("columns"));
198: 		auto get_fun = data.df.attr("__getitem__");
199: 
200: 		output.SetCardinality(this_count);
201: 		for (idx_t col_idx = 0; col_idx < output.column_count(); col_idx++) {
202: 			auto numpy_col = py::array(get_fun(df_names[col_idx]).attr("to_numpy")());
203: 
204: 			switch (data.sql_types[col_idx].id) {
205: 			case SQLTypeId::BOOLEAN:
206: 				scan_pandas_column<bool>(numpy_col, this_count, data.position, output.data[col_idx]);
207: 				break;
208: 			case SQLTypeId::TINYINT:
209: 				scan_pandas_column<int8_t>(numpy_col, this_count, data.position, output.data[col_idx]);
210: 				break;
211: 			case SQLTypeId::SMALLINT:
212: 				scan_pandas_column<int16_t>(numpy_col, this_count, data.position, output.data[col_idx]);
213: 				break;
214: 			case SQLTypeId::INTEGER:
215: 				scan_pandas_column<int32_t>(numpy_col, this_count, data.position, output.data[col_idx]);
216: 				break;
217: 			case SQLTypeId::BIGINT:
218: 				scan_pandas_column<int64_t>(numpy_col, this_count, data.position, output.data[col_idx]);
219: 				break;
220: 			case SQLTypeId::FLOAT:
221: 				scan_pandas_fp_column<float>((float*) numpy_col.data(), this_count, data.position, output.data[col_idx]);
222: 				break;
223: 			case SQLTypeId::DOUBLE:
224: 				scan_pandas_fp_column<double>((double*) numpy_col.data(), this_count, data.position, output.data[col_idx]);
225: 				break;
226: 			case SQLTypeId::TIMESTAMP: {
227: 				auto src_ptr = (int64_t *)numpy_col.data();
228: 				auto tgt_ptr = (timestamp_t *)FlatVector::GetData(output.data[col_idx]);
229: 				auto &nullmask = FlatVector::Nullmask(output.data[col_idx]);
230: 
231: 				for (idx_t row = 0; row < this_count; row++) {
232: 					auto source_idx = data.position + row;
233: 					if (src_ptr[source_idx] <= NumericLimits<int64_t>::Minimum()) {
234: 						// pandas Not a Time (NaT)
235: 						nullmask[row] = true;
236: 						continue;
237: 					}
238: 					auto ms = src_ptr[source_idx] / 1000000; // nanoseconds
239: 					auto ms_per_day = (int64_t)60 * 60 * 24 * 1000;
240: 					date_t date = Date::EpochToDate(ms / 1000);
241: 					dtime_t time = (dtime_t)(ms % ms_per_day);
242: 					tgt_ptr[row] = Timestamp::FromDatetime(date, time);
243: 				}
244: 				break;
245: 			} break;
246: 			case SQLTypeId::VARCHAR: {
247: 				auto src_ptr = (PyObject **)numpy_col.data();
248: 				auto tgt_ptr = (string_t *)FlatVector::GetData(output.data[col_idx]);
249: 
250: 				for (idx_t row = 0; row < this_count; row++) {
251: 					auto source_idx = data.position + row;
252: 					auto val = src_ptr[source_idx];
253: 
254: #if PY_MAJOR_VERSION >= 3
255: 					if (!PyUnicode_Check(val)) {
256: 						FlatVector::SetNull(output.data[col_idx], row, true);
257: 						continue;
258: 					}
259: 					if (PyUnicode_READY(val) != 0) {
260: 						throw runtime_error("failure in PyUnicode_READY");
261: 					}
262: 					tgt_ptr[row] = StringVector::AddString(output.data[col_idx], ((py::object*) &val)->cast<string>());
263: #else
264: 					if (!py::isinstance<py::str>(*((py::object*) &val))) {
265: 						FlatVector::SetNull(output.data[col_idx], row, true);
266: 						continue;
267: 					}
268: 
269: 					tgt_ptr[row] = StringVector::AddString(output.data[col_idx], ((py::object*) &val)->cast<string>());
270: #endif
271: 				}
272: 				break;
273: 			}
274: 			default:
275: 				throw runtime_error("Unsupported type " + SQLTypeToString(data.sql_types[col_idx]));
276: 			}
277: 		}
278: 		data.position += this_count;
279: 	}
280: };
281: 
282: template<> bool PandasScanFunction::ValueIsNull(float value);
283: template<> bool PandasScanFunction::ValueIsNull(double value);
284: 
285: template<>
286: bool PandasScanFunction::ValueIsNull(float value) {
287: 	return !Value::FloatIsValid(value);
288: }
289: 
290: template<>
291: bool PandasScanFunction::ValueIsNull(double value) {
292: 	return !Value::DoubleIsValid(value);
293: }
294: 
295: struct DuckDBPyResult {
296: 
297: 	template <class SRC> static SRC fetch_scalar(Vector &src_vec, idx_t offset) {
298: 		auto src_ptr = FlatVector::GetData<SRC>(src_vec);
299: 		return src_ptr[offset];
300: 	}
301: 
302: 	py::object fetchone() {
303: 		if (!result) {
304: 			throw runtime_error("result closed");
305: 		}
306: 		if (!current_chunk || chunk_offset >= current_chunk->size()) {
307: 			current_chunk = result->Fetch();
308: 			chunk_offset = 0;
309: 		}
310: 		if (current_chunk->size() == 0) {
311: 			return py::none();
312: 		}
313: 		py::tuple res(result->types.size());
314: 
315: 		for (idx_t col_idx = 0; col_idx < result->types.size(); col_idx++) {
316: 			auto &nullmask = FlatVector::Nullmask(current_chunk->data[col_idx]);
317: 			if (nullmask[chunk_offset]) {
318: 				res[col_idx] = py::none();
319: 				continue;
320: 			}
321: 			auto val = current_chunk->data[col_idx].GetValue(chunk_offset);
322: 			switch (result->sql_types[col_idx].id) {
323: 			case SQLTypeId::BOOLEAN:
324: 				res[col_idx] = val.GetValue<bool>();
325: 				break;
326: 			case SQLTypeId::TINYINT:
327: 				res[col_idx] = val.GetValue<int8_t>();
328: 				break;
329: 			case SQLTypeId::SMALLINT:
330: 				res[col_idx] = val.GetValue<int16_t>();
331: 				break;
332: 			case SQLTypeId::INTEGER:
333: 				res[col_idx] = val.GetValue<int32_t>();
334: 				break;
335: 			case SQLTypeId::BIGINT:
336: 				res[col_idx] = val.GetValue<int64_t>();
337: 				break;
338: 			case SQLTypeId::HUGEINT: {
339: 				auto hugeint_str = val.GetValue<string>();
340: 				res[col_idx] = PyLong_FromString((char*) hugeint_str.c_str(), nullptr, 10);
341: 				break;
342: 			}
343: 			case SQLTypeId::FLOAT:
344: 				res[col_idx] = val.GetValue<float>();
345: 				break;
346: 			case SQLTypeId::DOUBLE:
347: 				res[col_idx] = val.GetValue<double>();
348: 				break;
349: 			case SQLTypeId::VARCHAR:
350: 				res[col_idx] = val.GetValue<string>();
351: 				break;
352: 
353: 			case SQLTypeId::TIMESTAMP: {
354: 				if (result->types[col_idx] != TypeId::INT64) {
355: 					throw runtime_error("expected int64 for timestamp");
356: 				}
357: 				auto timestamp = val.GetValue<int64_t>();
358: 				auto date = Timestamp::GetDate(timestamp);
359: 				res[col_idx] = PyDateTime_FromDateAndTime(
360: 				    Date::ExtractYear(date), Date::ExtractMonth(date), Date::ExtractDay(date),
361: 				    Timestamp::GetHours(timestamp), Timestamp::GetMinutes(timestamp), Timestamp::GetSeconds(timestamp),
362: 				    Timestamp::GetMilliseconds(timestamp) * 1000 - Timestamp::GetSeconds(timestamp) * 1000000);
363: 
364: 				break;
365: 			}
366: 			case SQLTypeId::TIME: {
367: 				if (result->types[col_idx] != TypeId::INT32) {
368: 					throw runtime_error("expected int32 for time");
369: 				}
370: 				int32_t hour, min, sec, msec;
371: 				auto time = val.GetValue<int32_t>();
372: 				duckdb::Time::Convert(time, hour, min, sec, msec);
373: 				res[col_idx] = PyTime_FromTime(hour, min, sec, msec * 1000);
374: 				break;
375: 			}
376: 			case SQLTypeId::DATE: {
377: 				if (result->types[col_idx] != TypeId::INT32) {
378: 					throw runtime_error("expected int32 for date");
379: 				}
380: 				auto date = val.GetValue<int32_t>();
381: 				res[col_idx] = PyDate_FromDate(duckdb::Date::ExtractYear(date), duckdb::Date::ExtractMonth(date),
382: 				                               duckdb::Date::ExtractDay(date));
383: 				break;
384: 			}
385: 
386: 			default:
387: 				throw runtime_error("unsupported type: " + SQLTypeToString(result->sql_types[col_idx]));
388: 			}
389: 		}
390: 		chunk_offset++;
391: 		return move(res);
392: 	}
393: 
394: 	py::list fetchall() {
395: 		py::list res;
396: 		while (true) {
397: 			auto fres = fetchone();
398: 			if (fres.is_none()) {
399: 				break;
400: 			}
401: 			res.append(fres);
402: 		}
403: 		return res;
404: 	}
405: 
406: 	py::dict fetchnumpy() {
407: 		if (!result) {
408: 			throw runtime_error("result closed");
409: 		}
410: 		// need to materialize the result if it was streamed because we need the count :/
411: 		MaterializedQueryResult *mres = nullptr;
412: 		unique_ptr<QueryResult> mat_res_holder;
413: 		if (result->type == QueryResultType::STREAM_RESULT) {
414: 			mat_res_holder = ((StreamQueryResult *)result.get())->Materialize();
415: 			mres = (MaterializedQueryResult *)mat_res_holder.get();
416: 		} else {
417: 			mres = (MaterializedQueryResult *)result.get();
418: 		}
419: 		assert(mres);
420: 
421: 		py::dict res;
422: 		for (idx_t col_idx = 0; col_idx < mres->types.size(); col_idx++) {
423: 			// convert the actual payload
424: 			py::array col_res;
425: 			switch (mres->sql_types[col_idx].id) {
426: 			case SQLTypeId::BOOLEAN:
427: 				col_res = duckdb_py_convert::fetch_column_regular<bool>("bool", mres->collection, col_idx);
428: 				break;
429: 			case SQLTypeId::TINYINT:
430: 				col_res = duckdb_py_convert::fetch_column_regular<int8_t>("int8", mres->collection, col_idx);
431: 				break;
432: 			case SQLTypeId::SMALLINT:
433: 				col_res = duckdb_py_convert::fetch_column_regular<int16_t>("int16", mres->collection, col_idx);
434: 				break;
435: 			case SQLTypeId::INTEGER:
436: 				col_res = duckdb_py_convert::fetch_column_regular<int32_t>("int32", mres->collection, col_idx);
437: 				break;
438: 			case SQLTypeId::BIGINT:
439: 				col_res = duckdb_py_convert::fetch_column_regular<int64_t>("int64", mres->collection, col_idx);
440: 				break;
441: 			case SQLTypeId::HUGEINT:
442: 				col_res = duckdb_py_convert::fetch_column<hugeint_t, double, duckdb_py_convert::HugeIntConvert>("float64", mres->collection, col_idx);
443: 				break;
444: 			case SQLTypeId::FLOAT:
445: 				col_res = duckdb_py_convert::fetch_column_regular<float>("float32", mres->collection, col_idx);
446: 				break;
447: 			case SQLTypeId::DOUBLE:
448: 				col_res = duckdb_py_convert::fetch_column_regular<double>("float64", mres->collection, col_idx);
449: 				break;
450: 			case SQLTypeId::TIMESTAMP:
451: 				col_res = duckdb_py_convert::fetch_column<timestamp_t, int64_t, duckdb_py_convert::TimestampConvert>(
452: 				    "datetime64[ms]", mres->collection, col_idx);
453: 				break;
454: 			case SQLTypeId::DATE:
455: 				col_res = duckdb_py_convert::fetch_column<date_t, int64_t, duckdb_py_convert::DateConvert>(
456: 				    "datetime64[s]", mres->collection, col_idx);
457: 				break;
458: 			case SQLTypeId::TIME:
459: 				col_res = duckdb_py_convert::fetch_column<time_t, py::str, duckdb_py_convert::TimeConvert>(
460: 				    "object", mres->collection, col_idx);
461: 				break;
462: 			case SQLTypeId::VARCHAR:
463: 				col_res = duckdb_py_convert::fetch_column<string_t, py::str, duckdb_py_convert::StringConvert>(
464: 				    "object", mres->collection, col_idx);
465: 				break;
466: 			default:
467: 				throw runtime_error("unsupported type " + SQLTypeToString(mres->sql_types[col_idx]));
468: 			}
469: 
470: 			// convert the nullmask
471: 			auto nullmask = py::array(py::dtype("bool"), mres->collection.count);
472: 			auto nullmask_ptr = (bool*) nullmask.mutable_data();
473: 			idx_t out_offset = 0;
474: 			for (auto &data_chunk : mres->collection.chunks) {
475: 				auto &src_nm = FlatVector::Nullmask(data_chunk->data[col_idx]);
476: 				for (idx_t i = 0; i < data_chunk->size(); i++) {
477: 					nullmask_ptr[i + out_offset] = src_nm[i];
478: 				}
479: 				out_offset += data_chunk->size();
480: 			}
481: 
482: 			// create masked array and assign to output
483: 			auto masked_array = py::module::import("numpy.ma").attr("masked_array")(col_res, nullmask);
484: 			res[mres->names[col_idx].c_str()] = masked_array;
485: 		}
486: 		return res;
487: 	}
488: 
489: 	py::object fetchdf() {
490: 		return py::module::import("pandas").attr("DataFrame").attr("from_dict")(fetchnumpy());
491: 	}
492: 
493: 	py::list description() {
494: 		py::list desc(result->names.size());
495: 		for (idx_t col_idx = 0; col_idx < result->names.size(); col_idx++) {
496: 			py::tuple col_desc(7);
497: 			col_desc[0] = py::str(result->names[col_idx]);
498: 			col_desc[1] = py::none();
499: 			col_desc[2] = py::none();
500: 			col_desc[3] = py::none();
501: 			col_desc[4] = py::none();
502: 			col_desc[5] = py::none();
503: 			col_desc[6] = py::none();
504: 			desc[col_idx] = col_desc;
505: 		}
506: 		return desc;
507: 	}
508: 
509: 	void close() {
510: 		result = nullptr;
511: 	}
512: 	idx_t chunk_offset = 0;
513: 
514: 	unique_ptr<QueryResult> result;
515: 	unique_ptr<DataChunk> current_chunk;
516: };
517: 
518: struct DuckDBPyRelation;
519: 
520: struct DuckDBPyConnection {
521: 
522: 	DuckDBPyConnection *executemany(string query, py::object params = py::list()) {
523: 		execute(query, params, true);
524: 		return this;
525: 	}
526: 
527: 	~DuckDBPyConnection() {
528: 		for (auto &element : registered_dfs) {
529: 			unregister_df(element.first);
530: 		}
531: 	}
532: 
533: 	DuckDBPyConnection *execute(string query, py::object params = py::list(), bool many = false) {
534: 		if (!connection) {
535: 			throw runtime_error("connection closed");
536: 		}
537: 		result = nullptr;
538: 
539: 		auto prep = connection->Prepare(query);
540: 		if (!prep->success) {
541: 			throw runtime_error(prep->error);
542: 		}
543: 
544: 		// this is a list of a list of parameters in executemany
545: 		py::list params_set;
546: 		if (!many) {
547: 			params_set = py::list(1);
548: 			params_set[0] = params;
549: 		} else {
550: 			params_set = params;
551: 		}
552: 
553: 		for (const auto &single_query_params : params_set) {
554: 			if (prep->n_param != py::len(single_query_params)) {
555: 				throw runtime_error("Prepared statments needs " + to_string(prep->n_param) + " parameters, " +
556: 				                    to_string(py::len(single_query_params)) + " given");
557: 			}
558: 			auto args = DuckDBPyConnection::transform_python_param_list(single_query_params);
559: 			auto res = make_unique<DuckDBPyResult>();
560: 			res->result = prep->Execute(args);
561: 			if (!res->result->success) {
562: 				throw runtime_error(res->result->error);
563: 			}
564: 			if (!many) {
565: 				result = move(res);
566: 			}
567: 		}
568: 		return this;
569: 	}
570: 
571: 	DuckDBPyConnection *append(string name, py::object value) {
572: 		register_df("__append_df", value);
573: 		return execute("INSERT INTO \"" + name + "\" SELECT * FROM __append_df");
574: 	}
575: 
576: 	static string ptr_to_string(void const *ptr) {
577: 		std::ostringstream address;
578: 		address << ptr;
579: 		return address.str();
580: 	}
581: 
582: 	DuckDBPyConnection *register_df(string name, py::object value) {
583: 		// hack alert: put the pointer address into the function call as a string
584: 		execute("CREATE OR REPLACE VIEW \"" + name + "\" AS SELECT * FROM pandas_scan('" + ptr_to_string(value.ptr()) +
585: 		        "')");
586: 
587: 		// try to bind
588: 		execute("SELECT * FROM \"" + name + "\" WHERE FALSE");
589: 
590: 		// keep a reference
591: 		registered_dfs[name] = value;
592: 		return this;
593: 	}
594: 
595: 	unique_ptr<DuckDBPyRelation> table(string tname) {
596: 		if (!connection) {
597: 			throw runtime_error("connection closed");
598: 		}
599: 		return make_unique<DuckDBPyRelation>(connection->Table(tname));
600: 	}
601: 
602: 	unique_ptr<DuckDBPyRelation> values(py::object params = py::list()) {
603: 		if (!connection) {
604: 			throw runtime_error("connection closed");
605: 		}
606: 		vector<vector<Value>> values {DuckDBPyConnection::transform_python_param_list(params)};
607: 		return make_unique<DuckDBPyRelation>(connection->Values(values));
608: 	}
609: 
610: 	unique_ptr<DuckDBPyRelation> view(string vname) {
611: 		if (!connection) {
612: 			throw runtime_error("connection closed");
613: 		}
614: 		return make_unique<DuckDBPyRelation>(connection->View(vname));
615: 	}
616: 
617: 	unique_ptr<DuckDBPyRelation> table_function(string fname, py::object params = py::list()) {
618: 		if (!connection) {
619: 			throw runtime_error("connection closed");
620: 		}
621: 
622: 		return make_unique<DuckDBPyRelation>(
623: 		    connection->TableFunction(fname, DuckDBPyConnection::transform_python_param_list(params)));
624: 	}
625: 
626: 	unique_ptr<DuckDBPyRelation> from_df(py::object value) {
627: 		if (!connection) {
628: 			throw runtime_error("connection closed");
629: 		};
630: 		string name = "df_" + random_string::generate();
631: 		registered_dfs[name] = value;
632: 		vector<Value> params;
633: 		params.push_back(Value(ptr_to_string(value.ptr())));
634: 		return make_unique<DuckDBPyRelation>(connection->TableFunction("pandas_scan", params)->Alias(name));
635: 	}
636: 
637: 	unique_ptr<DuckDBPyRelation> from_csv_auto(string filename) {
638: 		if (!connection) {
639: 			throw runtime_error("connection closed");
640: 		};
641: 		vector<Value> params;
642: 		params.push_back(Value(filename));
643: 		return make_unique<DuckDBPyRelation>(connection->TableFunction("read_csv_auto", params)->Alias(filename));
644: 	}
645: 
646: 
647: 	unique_ptr<DuckDBPyRelation> from_parquet(string filename) {
648: 		if (!connection) {
649: 			throw runtime_error("connection closed");
650: 		};
651: 		vector<Value> params;
652: 		params.push_back(Value(filename));
653: 		return make_unique<DuckDBPyRelation>(connection->TableFunction("parquet_scan", params)->Alias(filename));
654: 	}
655: 
656: 
657: 	DuckDBPyConnection *unregister_df(string name) {
658: 		registered_dfs[name] = py::none();
659: 		return this;
660: 	}
661: 
662: 	DuckDBPyConnection *begin() {
663: 		execute("BEGIN TRANSACTION");
664: 		return this;
665: 	}
666: 
667: 	DuckDBPyConnection *commit() {
668: 		if (connection->context->transaction.IsAutoCommit()) {
669: 			return this;
670: 		}
671: 		execute("COMMIT");
672: 		return this;
673: 	}
674: 
675: 	DuckDBPyConnection *rollback() {
676: 		execute("ROLLBACK");
677: 		return this;
678: 	}
679: 
680: 	py::object getattr(py::str key) {
681: 		if (key.cast<string>() == "description") {
682: 			if (!result) {
683: 				throw runtime_error("no open result set");
684: 			}
685: 			return result->description();
686: 		}
687: 		return py::none();
688: 	}
689: 
690: 	void close() {
691: 		connection = nullptr;
692: 		database = nullptr;
693: 	}
694: 
695: 	// cursor() is stupid
696: 	unique_ptr<DuckDBPyConnection> cursor() {
697: 		auto res = make_unique<DuckDBPyConnection>();
698: 		res->database = database;
699: 		res->connection = make_unique<Connection>(*res->database);
700: 		return res;
701: 	}
702: 
703: 	// these should be functions on the result but well
704: 	py::tuple fetchone() {
705: 		if (!result) {
706: 			throw runtime_error("no open result set");
707: 		}
708: 		return result->fetchone();
709: 	}
710: 
711: 	py::list fetchall() {
712: 		if (!result) {
713: 			throw runtime_error("no open result set");
714: 		}
715: 		return result->fetchall();
716: 	}
717: 
718: 	py::dict fetchnumpy() {
719: 		if (!result) {
720: 			throw runtime_error("no open result set");
721: 		}
722: 		return result->fetchnumpy();
723: 	}
724: 	py::object fetchdf() {
725: 		if (!result) {
726: 			throw runtime_error("no open result set");
727: 		}
728: 		return result->fetchdf();
729: 	}
730: 
731: 	static unique_ptr<DuckDBPyConnection> connect(string database, bool read_only) {
732: 		auto res = make_unique<DuckDBPyConnection>();
733: 		DBConfig config;
734: 		if (read_only)
735: 			config.access_mode = AccessMode::READ_ONLY;
736: 		res->database = make_unique<DuckDB>(database, &config);
737: 		res->database->LoadExtension<ParquetExtension>();
738: 		res->connection = make_unique<Connection>(*res->database);
739: 
740: 		PandasScanFunction scan_fun;
741: 		CreateTableFunctionInfo info(scan_fun);
742: 
743: 		auto &context = *res->connection->context;
744: 		context.transaction.BeginTransaction();
745: 		context.catalog.CreateTableFunction(context, &info);
746: 		context.transaction.Commit();
747: 
748: 		if (!read_only) {
749: 			res->connection->Query("CREATE OR REPLACE VIEW sqlite_master AS SELECT * FROM sqlite_master()");
750: 		}
751: 
752: 		return res;
753: 	}
754: 
755: 	shared_ptr<DuckDB> database;
756: 	unique_ptr<Connection> connection;
757: 	unordered_map<string, py::object> registered_dfs;
758: 	unique_ptr<DuckDBPyResult> result;
759: 
760: 	static vector<Value> transform_python_param_list(py::handle params) {
761: 		vector<Value> args;
762: 
763: 		auto datetime_mod = py::module::import("datetime");
764: 		auto datetime_date = datetime_mod.attr("datetime");
765: 		auto datetime_datetime = datetime_mod.attr("date");
766: 
767: 		for (auto &ele : params) {
768: 			if (ele.is_none()) {
769: 				args.push_back(Value());
770: 			} else if (py::isinstance<py::bool_>(ele)) {
771: 				args.push_back(Value::BOOLEAN(ele.cast<bool>()));
772: 			} else if (py::isinstance<py::int_>(ele)) {
773: 				args.push_back(Value::BIGINT(ele.cast<int64_t>()));
774: 			} else if (py::isinstance<py::float_>(ele)) {
775: 				args.push_back(Value::DOUBLE(ele.cast<double>()));
776: 			} else if (py::isinstance<py::str>(ele)) {
777: 				args.push_back(Value(ele.cast<string>()));
778: 			} else if (ele.get_type().is(datetime_date)) {
779: 				throw runtime_error("date parameters not supported yet :/");
780: 				// args.push_back(Value::DATE(1984, 4, 24));
781: 			} else if (ele.get_type().is(datetime_datetime)) {
782: 				throw runtime_error("datetime parameters not supported yet :/");
783: 				// args.push_back(Value::TIMESTAMP(1984, 4, 24, 14, 42, 0, 0));
784: 			} else {
785: 				throw runtime_error("unknown param type " + py::str(ele.get_type()).cast<string>());
786: 			}
787: 		}
788: 		return args;
789: 	}
790: };
791: 
792: static unique_ptr<DuckDBPyConnection> default_connection_ = nullptr;
793: 
794: static DuckDBPyConnection *default_connection() {
795: 	if (!default_connection_) {
796: 		default_connection_ = DuckDBPyConnection::connect(":memory:", false);
797: 	}
798: 	return default_connection_.get();
799: }
800: 
801: struct DuckDBPyRelation {
802: 
803: 	DuckDBPyRelation(shared_ptr<Relation> rel) : rel(rel) {
804: 	}
805: 
806: 	static unique_ptr<DuckDBPyRelation> from_df(py::object df) {
807: 		return default_connection()->from_df(df);
808: 	}
809: 
810: 	static unique_ptr<DuckDBPyRelation> from_csv_auto(string filename) {
811: 		return default_connection()->from_csv_auto(filename);
812: 	}
813: 
814: 	static unique_ptr<DuckDBPyRelation> from_parquet(string filename) {
815: 		return default_connection()->from_parquet(filename);
816: 	}
817: 
818: 	unique_ptr<DuckDBPyRelation> project(string expr) {
819: 		return make_unique<DuckDBPyRelation>(rel->Project(expr));
820: 	}
821: 
822: 	static unique_ptr<DuckDBPyRelation> project_df(py::object df, string expr) {
823: 		return default_connection()->from_df(df)->project(expr);
824: 	}
825: 
826: 	unique_ptr<DuckDBPyRelation> alias(string expr) {
827: 		return make_unique<DuckDBPyRelation>(rel->Alias(expr));
828: 	}
829: 
830: 	static unique_ptr<DuckDBPyRelation> alias_df(py::object df, string expr) {
831: 		return default_connection()->from_df(df)->alias(expr);
832: 	}
833: 
834: 	unique_ptr<DuckDBPyRelation> filter(string expr) {
835: 		return make_unique<DuckDBPyRelation>(rel->Filter(expr));
836: 	}
837: 
838: 	static unique_ptr<DuckDBPyRelation> filter_df(py::object df, string expr) {
839: 		return default_connection()->from_df(df)->filter(expr);
840: 	}
841: 
842: 	unique_ptr<DuckDBPyRelation> limit(int64_t n) {
843: 		return make_unique<DuckDBPyRelation>(rel->Limit(n));
844: 	}
845: 
846: 	static unique_ptr<DuckDBPyRelation> limit_df(py::object df, int64_t n) {
847: 		return default_connection()->from_df(df)->limit(n);
848: 	}
849: 
850: 	unique_ptr<DuckDBPyRelation> order(string expr) {
851: 		return make_unique<DuckDBPyRelation>(rel->Order(expr));
852: 	}
853: 
854: 	static unique_ptr<DuckDBPyRelation> order_df(py::object df, string expr) {
855: 		return default_connection()->from_df(df)->order(expr);
856: 	}
857: 
858: 	unique_ptr<DuckDBPyRelation> aggregate(string expr, string groups = "") {
859: 		if (groups.size() > 0) {
860: 			return make_unique<DuckDBPyRelation>(rel->Aggregate(expr, groups));
861: 		}
862: 		return make_unique<DuckDBPyRelation>(rel->Aggregate(expr));
863: 	}
864: 
865: 	static unique_ptr<DuckDBPyRelation> aggregate_df(py::object df, string expr, string groups = "") {
866: 		return default_connection()->from_df(df)->aggregate(expr, groups);
867: 	}
868: 
869: 	unique_ptr<DuckDBPyRelation> distinct() {
870: 		return make_unique<DuckDBPyRelation>(rel->Distinct());
871: 	}
872: 
873: 	static unique_ptr<DuckDBPyRelation> distinct_df(py::object df) {
874: 		return default_connection()->from_df(df)->distinct();
875: 	}
876: 
877: 	py::object to_df() {
878: 		auto res = make_unique<DuckDBPyResult>();
879: 		res->result = rel->Execute();
880: 		if (!res->result->success) {
881: 			throw runtime_error(res->result->error);
882: 		}
883: 		return res->fetchdf();
884: 	}
885: 
886: 	unique_ptr<DuckDBPyRelation> union_(DuckDBPyRelation *other) {
887: 		return make_unique<DuckDBPyRelation>(rel->Union(other->rel));
888: 	}
889: 
890: 	unique_ptr<DuckDBPyRelation> except(DuckDBPyRelation *other) {
891: 		return make_unique<DuckDBPyRelation>(rel->Except(other->rel));
892: 	}
893: 
894: 	unique_ptr<DuckDBPyRelation> intersect(DuckDBPyRelation *other) {
895: 		return make_unique<DuckDBPyRelation>(rel->Intersect(other->rel));
896: 	}
897: 
898: 	unique_ptr<DuckDBPyRelation> join(DuckDBPyRelation *other, string condition) {
899: 		return make_unique<DuckDBPyRelation>(rel->Join(other->rel, condition));
900: 	}
901: 
902: 	void write_csv(string file) {
903: 		rel->WriteCSV(file);
904: 	}
905: 
906: 	static void write_csv_df(py::object df, string file) {
907: 		return default_connection()->from_df(df)->write_csv(file);
908: 	}
909: 
910: 	// should this return a rel with the new view?
911: 	unique_ptr<DuckDBPyRelation> create_view(string view_name, bool replace = true) {
912: 		rel->CreateView(view_name, replace);
913: 		return make_unique<DuckDBPyRelation>(rel);
914: 	}
915: 
916: 	static unique_ptr<DuckDBPyRelation> create_view_df(py::object df, string view_name, bool replace = true) {
917: 		return default_connection()->from_df(df)->create_view(view_name, replace);
918: 	}
919: 
920: 	unique_ptr<DuckDBPyResult> query(string view_name, string sql_query) {
921: 		auto res = make_unique<DuckDBPyResult>();
922: 		res->result = rel->Query(view_name, sql_query);
923: 		if (!res->result->success) {
924: 			throw runtime_error(res->result->error);
925: 		}
926: 		return res;
927: 	}
928: 
929: 	unique_ptr<DuckDBPyResult> execute() {
930: 		auto res = make_unique<DuckDBPyResult>();
931: 		res->result = rel->Execute();
932: 		if (!res->result->success) {
933: 			throw runtime_error(res->result->error);
934: 		}
935: 		return res;
936: 	}
937: 
938: 	static unique_ptr<DuckDBPyResult> query_df(py::object df, string view_name, string sql_query) {
939: 		return default_connection()->from_df(df)->query(view_name, sql_query);
940: 	}
941: 
942: 	void insert_into(string table) {
943: 		rel->Insert(table);
944: 	}
945: 
946: 	void insert(py::object params = py::list()) {
947: 		vector<vector<Value>> values {DuckDBPyConnection::transform_python_param_list(params)};
948: 		rel->Insert(values);
949: 	}
950: 
951: 	void create(string table) {
952: 		rel->Create(table);
953: 	}
954: 
955: 	string print() {
956: 		return rel->ToString() + "\n---------------------\n-- Result Preview  --\n---------------------\n" +  rel->Limit(10)->Execute()->ToString() + "\n";
957: 	}
958: 
959: 	py::object getattr(py::str key) {
960: 		auto key_s = key.cast<string>();
961: 		if (key_s == "alias") {
962: 			return py::str(string(rel->GetAlias()));
963: 		} else if (key_s == "type") {
964: 			return py::str(RelationTypeToString(rel->type));
965: 		} else if (key_s == "columns") {
966: 			py::list res;
967: 			for (auto &col : rel->Columns()) {
968: 				res.append(col.name);
969: 			}
970: 			return move(res);
971: 		} else if (key_s == "types" || key_s == "dtypes") {
972: 			py::list res;
973: 			for (auto &col : rel->Columns()) {
974: 				res.append(SQLTypeToString(col.type));
975: 			}
976: 			return move(res);
977: 		}
978: 		return py::none();
979: 	}
980: 
981: 	shared_ptr<Relation> rel;
982: };
983: 
984: PYBIND11_MODULE(duckdb, m) {
985: 	m.def("connect", &DuckDBPyConnection::connect, "Create a DuckDB database instance. Can take a database file name to read/write persistent data and a read_only flag if no changes are desired",
986: 	      py::arg("database") = ":memory:", py::arg("read_only") = false);
987: 
988: 	auto conn_class =
989: 	    py::class_<DuckDBPyConnection>(m, "DuckDBPyConnection")
990: 	        .def("cursor", &DuckDBPyConnection::cursor, "Create a duplicate of the current connection")
991: 	        .def("duplicate", &DuckDBPyConnection::cursor, "Create a duplicate of the current connection")
992: 	        .def("execute", &DuckDBPyConnection::execute, "Execute the given SQL query, optionally using prepared statements with parameters set", py::arg("query"),
993: 	             py::arg("parameters") = py::list(), py::arg("multiple_parameter_sets") = false)
994: 	        .def("executemany", &DuckDBPyConnection::executemany, "Execute the given prepared statement multiple times using the list of parameter sets in parameters", py::arg("query"),
995: 	             py::arg("parameters") = py::list())
996: 	        .def("close", &DuckDBPyConnection::close, "Close the connection")
997: 	        .def("fetchone", &DuckDBPyConnection::fetchone, "Fetch a single row from a result following execute")
998: 	        .def("fetchall", &DuckDBPyConnection::fetchall, "Fetch all rows from a result following execute")
999: 	        .def("fetchnumpy", &DuckDBPyConnection::fetchnumpy, "Fetch a result as list of NumPy arrays following execute")
1000: 	        .def("fetchdf", &DuckDBPyConnection::fetchdf, "Fetch a result as Data.Frame following execute()")
1001: 	        .def("begin", &DuckDBPyConnection::begin, "Start a new transaction")
1002: 	        .def("commit", &DuckDBPyConnection::commit, "Commit changes performed within a transaction")
1003: 	        .def("rollback", &DuckDBPyConnection::rollback, "Roll back changes performed within a transaction")
1004: 	        .def("append", &DuckDBPyConnection::append, "Append the passed Data.Frame to the named table", py::arg("table_name"), py::arg("df"))
1005: 			.def("register", &DuckDBPyConnection::register_df, "Register the passed Data.Frame value for querying with a view", py::arg("view_name"), py::arg("df"))
1006: 	        .def("unregister", &DuckDBPyConnection::unregister_df, "Unregister the view name",  py::arg("view_name"))
1007: 			.def("table", &DuckDBPyConnection::table, "Create a relation object for the name'd table", py::arg("table_name"))
1008: 	        .def("view", &DuckDBPyConnection::view, "Create a relation object for the name'd view", py::arg("view_name"))
1009: 	        .def("values", &DuckDBPyConnection::values, "Create a relation object from the passed values", py::arg("values"))
1010: 	        .def("table_function", &DuckDBPyConnection::table_function, "Create a relation object from the name'd table function with given parameters",
1011: 	             py::arg("name"), py::arg("parameters") = py::list())
1012: 	        .def("from_df", &DuckDBPyConnection::from_df, "Create a relation object from the Data.Frame in df", py::arg("df"))
1013: 	        .def("df", &DuckDBPyConnection::from_df, "Create a relation object from the Data.Frame in df (alias of from_df)", py::arg("df"))
1014: 			.def("from_csv_auto", &DuckDBPyConnection::from_csv_auto, "Create a relation object from the CSV file in file_name",
1015: 	             py::arg("file_name"))
1016: 	       	.def("from_parquet", &DuckDBPyConnection::from_parquet, "Create a relation object from the Parquet file in file_name",
1017: 	             py::arg("file_name"))
1018: 		    .def("__getattr__", &DuckDBPyConnection::getattr, "Get result set attributes, mainly column names");
1019: 
1020: 	py::class_<DuckDBPyResult>(m, "DuckDBPyResult")
1021: 	    .def("close", &DuckDBPyResult::close)
1022: 	    .def("fetchone", &DuckDBPyResult::fetchone)
1023: 	    .def("fetchall", &DuckDBPyResult::fetchall)
1024: 	    .def("fetchnumpy", &DuckDBPyResult::fetchnumpy)
1025: 	    .def("fetchdf", &DuckDBPyResult::fetchdf)
1026: 	    .def("fetch_df", &DuckDBPyResult::fetchdf)
1027: 	    .def("df", &DuckDBPyResult::fetchdf);
1028: 
1029: 	py::class_<DuckDBPyRelation>(m, "DuckDBPyRelation")
1030: 	    .def("filter", &DuckDBPyRelation::filter, "Filter the relation object by the filter in filter_expr", py::arg("filter_expr"))
1031: 	    .def("project", &DuckDBPyRelation::project, "Project the relation object by the projection in project_expr", py::arg("project_expr"))
1032: 	    .def("set_alias", &DuckDBPyRelation::alias, "Rename the relation object to new alias", py::arg("alias"))
1033: 	    .def("order", &DuckDBPyRelation::order, "Reorder the relation object by order_expr", py::arg("order_expr"))
1034: 	    .def("aggregate", &DuckDBPyRelation::aggregate, "Compute the aggregate aggr_expr by the optional groups group_expr on the relation", py::arg("aggr_expr"),
1035: 	         py::arg("group_expr") = "")
1036: 	    .def("union", &DuckDBPyRelation::union_, "Create the set union of this relation object with another relation object in other_rel")
1037: 	    .def("except_", &DuckDBPyRelation::except, "Create the set except of this relation object with another relation object in other_rel", py::arg("other_rel"))
1038: 	    .def("intersect", &DuckDBPyRelation::intersect, "Create the set intersection of this relation object with another relation object in other_rel", py::arg("other_rel"))
1039: 	    .def("join", &DuckDBPyRelation::join, "Join the relation object with another relation object in other_rel using the join condition expression in join_condition", py::arg("other_rel"),
1040: 	         py::arg("join_condition"))
1041: 	    .def("distinct", &DuckDBPyRelation::distinct, "Retrieve distinct rows from this relation object")
1042: 	    .def("limit", &DuckDBPyRelation::limit, "Only retrieve the first n rows from this relation object", py::arg("n"))
1043: 	    .def("query", &DuckDBPyRelation::query, "Run the given SQL query in sql_query on the view named virtual_table_name that refers to the relation object", py::arg("virtual_table_name"),
1044: 	         py::arg("sql_query"))
1045: 	    .def("execute", &DuckDBPyRelation::execute, "Transform the relation into a result set")
1046: 	    .def("write_csv", &DuckDBPyRelation::write_csv, "Write the relation object to a CSV file in file_name", py::arg("file_name"))
1047: 	    .def("insert_into", &DuckDBPyRelation::insert_into, "Inserts the relation object into an existing table named table_name", py::arg("table_name"))
1048: 	    .def("insert", &DuckDBPyRelation::insert, "Inserts the given values into the relation", py::arg("values"))
1049: 	    .def("create", &DuckDBPyRelation::create, "Creates a new table named table_name with the contents of the relation object", py::arg("table_name"))
1050: 	    .def("create_view", &DuckDBPyRelation::create_view, "Creates a view named view_name that refers to the relation object", py::arg("view_name"),
1051: 	         py::arg("replace") = true)
1052: 	    .def("to_df", &DuckDBPyRelation::to_df, "Transforms the relation object into a Data.Frame")
1053: 	    .def("df", &DuckDBPyRelation::to_df, "Transforms the relation object into a Data.Frame")
1054: 	    .def("__str__", &DuckDBPyRelation::print)
1055: 	    .def("__repr__", &DuckDBPyRelation::print)
1056: 	    .def("__getattr__", &DuckDBPyRelation::getattr);
1057: 
1058: 	m.def("from_csv_auto", &DuckDBPyRelation::from_csv_auto, "Creates a relation object from the CSV file in file_name", py::arg("file_name"));
1059: 	m.def("from_parquet", &DuckDBPyRelation::from_parquet, "Creates a relation object from the Parquet file in file_name", py::arg("file_name"));
1060: 	m.def("df", &DuckDBPyRelation::from_df, "Create a relation object from the Data.Frame df", py::arg("df"));
1061: 	m.def("from_df", &DuckDBPyRelation::from_df, "Create a relation object from the Data.Frame df", py::arg("df"));
1062: 	m.def("filter", &DuckDBPyRelation::filter_df, "Filter the Data.Frame df by the filter in filter_expr", py::arg("df"), py::arg("filter_expr"));
1063: 	m.def("project", &DuckDBPyRelation::project_df, "Project the Data.Frame df by the projection in project_expr", py::arg("df"),
1064: 	      py::arg("project_expr"));
1065: 	m.def("alias", &DuckDBPyRelation::alias_df, "Create a relation from Data.Frame df with the passed alias", py::arg("df"), py::arg("alias"));
1066: 	m.def("order", &DuckDBPyRelation::order_df, "Reorder the Data.Frame df by order_expr", py::arg("df"), py::arg("order_expr"));
1067: 	m.def("aggregate", &DuckDBPyRelation::aggregate_df, "Compute the aggregate aggr_expr by the optional groups group_expr on Data.frame df", py::arg("df"),
1068: 	      py::arg("aggr_expr"), py::arg("group_expr") = "");
1069: 	m.def("distinct", &DuckDBPyRelation::distinct_df, "Compute the distinct rows from Data.Frame df ", py::arg("df"));
1070: 	m.def("limit", &DuckDBPyRelation::limit_df, "Retrieve the first n rows from the Data.Frame df", py::arg("df"), py::arg("n"));
1071: 	m.def("query", &DuckDBPyRelation::query_df, "Run the given SQL query in sql_query on the view named virtual_table_name that contains the content of Data.Frame df", py::arg("df"),
1072: 	      py::arg("virtual_table_name"), py::arg("sql_query"));
1073: 	m.def("write_csv", &DuckDBPyRelation::write_csv_df, "Write the Data.Frame df to a CSV file in file_name", py::arg("df"),
1074: 	      py::arg("file_name"));
1075: 
1076: 	// we need this because otherwise we try to remove registered_dfs on shutdown when python is already dead
1077: 	auto clean_default_connection = []() { default_connection_ = nullptr; };
1078: 	m.add_object("_clean_default_connection", py::capsule(clean_default_connection));
1079: 	PyDateTime_IMPORT;
1080: }
[end of tools/pythonpkg/duckdb_python.cpp]
[start of tools/rpkg/src/duckdbr.cpp]
1: #include "duckdb.h"
2: #include <sstream>
3: #include "parquet-extension.h"
4: 
5: #include <Rdefines.h>
6: #include <algorithm>
7: 
8: // motherfucker
9: #undef error
10: 
11: using namespace duckdb;
12: using namespace std;
13: 
14: struct RStatement {
15: 	unique_ptr<PreparedStatement> stmt;
16: 	vector<Value> parameters;
17: };
18: 
19: // converter for primitive types
20: template <class SRC, class DEST>
21: static void vector_to_r(Vector &src_vec, size_t count, void *dest, uint64_t dest_offset, DEST na_val) {
22: 	auto src_ptr = FlatVector::GetData<SRC>(src_vec);
23: 	auto &nullmask = FlatVector::Nullmask(src_vec);
24: 	auto dest_ptr = ((DEST *)dest) + dest_offset;
25: 	for (size_t row_idx = 0; row_idx < count; row_idx++) {
26: 		dest_ptr[row_idx] = nullmask[row_idx] ? na_val : src_ptr[row_idx];
27: 	}
28: }
29: 
30: struct RDoubleType {
31: 	static bool IsNull(double val) {
32: 		return ISNA(val);
33: 	}
34: 
35: 	static double Convert(double val) {
36: 		return val;
37: 	}
38: };
39: 
40: struct RDateType {
41: 	static bool IsNull(double val) {
42: 		return RDoubleType::IsNull(val);
43: 	}
44: 
45: 	static double Convert(double val) {
46: 		return (date_t)val + 719528; // MAGIC!
47: 	}
48: };
49: 
50: struct RTimestampType {
51: 	static bool IsNull(double val) {
52: 		return RDoubleType::IsNull(val);
53: 	}
54: 
55: 	static timestamp_t Convert(double val) {
56: 		date_t date = Date::EpochToDate((int64_t)val);
57: 		dtime_t time = (dtime_t)(((int64_t)val % (60 * 60 * 24)) * 1000);
58: 		return Timestamp::FromDatetime(date, time);
59: 	}
60: };
61: 
62: struct RIntegerType {
63: 	static bool IsNull(int val) {
64: 		return val == NA_INTEGER;
65: 	}
66: 
67: 	static int Convert(int val) {
68: 		return val;
69: 	}
70: };
71: 
72: struct RBooleanType {
73: 	static bool IsNull(int val) {
74: 		return RIntegerType::IsNull(val);
75: 	}
76: 
77: 	static bool Convert(int val) {
78: 		return val;
79: 	}
80: };
81: 
82: template <class SRC, class DST, class RTYPE>
83: static void AppendColumnSegment(SRC *source_data, Vector &result, idx_t count) {
84: 	auto result_data = FlatVector::GetData<DST>(result);
85: 	auto &result_mask = FlatVector::Nullmask(result);
86: 	for (idx_t i = 0; i < count; i++) {
87: 		auto val = source_data[i];
88: 		if (RTYPE::IsNull(val)) {
89: 			result_mask[i] = true;
90: 		} else {
91: 			result_data[i] = RTYPE::Convert(val);
92: 		}
93: 	}
94: }
95: 
96: static void AppendStringSegment(SEXP coldata, Vector &result, idx_t row_idx, idx_t count) {
97: 	auto result_data = FlatVector::GetData<string_t>(result);
98: 	auto &result_mask = FlatVector::Nullmask(result);
99: 	for (idx_t i = 0; i < count; i++) {
100: 		SEXP val = STRING_ELT(coldata, row_idx + i);
101: 		if (val == NA_STRING) {
102: 			result_mask[i] = true;
103: 		} else {
104: 			result_data[i] = string_t((char *)CHAR(val));
105: 		}
106: 	}
107: }
108: 
109: static void AppendFactor(SEXP coldata, Vector &result, idx_t row_idx, idx_t count) {
110: 	auto source_data = INTEGER_POINTER(coldata) + row_idx;
111: 	auto result_data = FlatVector::GetData<string_t>(result);
112: 	auto &result_mask = FlatVector::Nullmask(result);
113: 	SEXP factor_levels = GET_LEVELS(coldata);
114: 	for (idx_t i = 0; i < count; i++) {
115: 		int val = source_data[i];
116: 		if (RIntegerType::IsNull(val)) {
117: 			result_mask[i] = true;
118: 		} else {
119: 			result_data[i] = string_t(CHAR(STRING_ELT(factor_levels, val - 1)));
120: 		}
121: 	}
122: }
123: 
124: static SEXP cstr_to_charsexp(const char *s) {
125: 	SEXP retsexp = PROTECT(mkCharCE(s, CE_UTF8));
126: 	if (!retsexp) {
127: 		Rf_error("cpp_str_to_charsexp: Memory allocation failed");
128: 		UNPROTECT(1);
129: 	}
130: 	return retsexp;
131: }
132: 
133: static SEXP cpp_str_to_charsexp(string s) {
134: 	return cstr_to_charsexp(s.c_str());
135: }
136: 
137: static SEXP cpp_str_to_strsexp(vector<string> s) {
138: 	SEXP retsexp = PROTECT(NEW_STRING(s.size()));
139: 	if (!retsexp) {
140: 		Rf_error("cpp_str_to_strsexp: Memory allocation failed");
141: 		UNPROTECT(1);
142: 	}
143: 	for (idx_t i = 0; i < s.size(); i++) {
144: 		SET_STRING_ELT(retsexp, i, cpp_str_to_charsexp(s[i]));
145: 		UNPROTECT(1);
146: 	}
147: 	return retsexp;
148: }
149: 
150: enum class RType { UNKNOWN, LOGICAL, INTEGER, NUMERIC, STRING, FACTOR, DATE, TIMESTAMP };
151: 
152: static RType detect_rtype(SEXP v) {
153: 	if (TYPEOF(v) == REALSXP && TYPEOF(GET_CLASS(v)) == STRSXP &&
154: 	    strcmp("POSIXct", CHAR(STRING_ELT(GET_CLASS(v), 0))) == 0) {
155: 		return RType::TIMESTAMP;
156: 	} else if (TYPEOF(v) == REALSXP && TYPEOF(GET_CLASS(v)) == STRSXP &&
157: 	           strcmp("Date", CHAR(STRING_ELT(GET_CLASS(v), 0))) == 0) {
158: 		return RType::DATE;
159: 	} else if (isFactor(v) && TYPEOF(v) == INTSXP) {
160: 		return RType::FACTOR;
161: 	} else if (TYPEOF(v) == LGLSXP) {
162: 		return RType::LOGICAL;
163: 	} else if (TYPEOF(v) == INTSXP) {
164: 		return RType::INTEGER;
165: 	} else if (TYPEOF(v) == REALSXP) {
166: 		return RType::NUMERIC;
167: 	} else if (TYPEOF(v) == STRSXP) {
168: 		return RType::STRING;
169: 	}
170: 	return RType::UNKNOWN;
171: }
172: 
173: extern "C" {
174: 
175: SEXP duckdb_release_R(SEXP stmtsexp) {
176: 	if (TYPEOF(stmtsexp) != EXTPTRSXP) {
177: 		Rf_error("duckdb_release_R: Need external pointer parameter");
178: 	}
179: 	RStatement *stmtholder = (RStatement *)R_ExternalPtrAddr(stmtsexp);
180: 	if (stmtsexp) {
181: 		R_ClearExternalPtr(stmtsexp);
182: 		delete stmtholder;
183: 	}
184: 	return R_NilValue;
185: }
186: 
187: SEXP duckdb_finalize_statement_R(SEXP stmtsexp) {
188: 	return duckdb_release_R(stmtsexp);
189: }
190: 
191: SEXP duckdb_prepare_R(SEXP connsexp, SEXP querysexp) {
192: 	if (TYPEOF(querysexp) != STRSXP || LENGTH(querysexp) != 1) {
193: 		Rf_error("duckdb_prepare_R: Need single string parameter for query");
194: 	}
195: 	if (TYPEOF(connsexp) != EXTPTRSXP) {
196: 		Rf_error("duckdb_prepare_R: Need external pointer parameter for connections");
197: 	}
198: 
199: 	char *query = (char *)CHAR(STRING_ELT(querysexp, 0));
200: 	if (!query) {
201: 		Rf_error("duckdb_prepare_R: No query");
202: 	}
203: 
204: 	Connection *conn = (Connection *)R_ExternalPtrAddr(connsexp);
205: 	if (!conn) {
206: 		Rf_error("duckdb_prepare_R: Invalid connection");
207: 	}
208: 
209: 	auto stmt = conn->Prepare(query);
210: 	if (!stmt->success) {
211: 		Rf_error("duckdb_prepare_R: Failed to prepare query %s\nError: %s", query, stmt->error.c_str());
212: 	}
213: 
214: 	auto stmtholder = new RStatement();
215: 	stmtholder->stmt = move(stmt);
216: 
217: 	SEXP stmtsexp = PROTECT(R_MakeExternalPtr(stmtholder, R_NilValue, R_NilValue));
218: 	R_RegisterCFinalizer(stmtsexp, (void (*)(SEXP))duckdb_finalize_statement_R);
219: 
220: 	SEXP retlist = PROTECT(NEW_LIST(6));
221: 	if (!retlist) {
222: 		UNPROTECT(2); // retlist, stmtsexp
223: 		Rf_error("duckdb_prepare_R: Memory allocation failed");
224: 	}
225: 	SEXP ret_names = cpp_str_to_strsexp({"str", "ref", "type", "names", "rtypes", "n_param"});
226: 	SET_NAMES(retlist, ret_names);
227: 	UNPROTECT(1); // ret_names
228: 
229: 	SET_VECTOR_ELT(retlist, 0, querysexp);
230: 	SET_VECTOR_ELT(retlist, 1, stmtsexp);
231: 	UNPROTECT(1); // stmtsxp
232: 
233: 	SEXP stmt_type = cpp_str_to_strsexp({StatementTypeToString(stmtholder->stmt->type)});
234: 	SET_VECTOR_ELT(retlist, 2, stmt_type);
235: 	UNPROTECT(1); // stmt_type
236: 
237: 	SEXP col_names = cpp_str_to_strsexp(stmtholder->stmt->names);
238: 	SET_VECTOR_ELT(retlist, 3, col_names);
239: 	UNPROTECT(1); // col_names
240: 
241: 	vector<string> rtypes;
242: 
243: 	for (auto &stype : stmtholder->stmt->types) {
244: 		string rtype = "";
245: 		switch (stype.id) {
246: 		case SQLTypeId::BOOLEAN:
247: 			rtype = "logical";
248: 			break;
249: 		case SQLTypeId::TINYINT:
250: 		case SQLTypeId::SMALLINT:
251: 		case SQLTypeId::INTEGER:
252: 			rtype = "integer";
253: 			break;
254: 		case SQLTypeId::TIMESTAMP:
255: 			rtype = "POSIXct";
256: 			break;
257: 		case SQLTypeId::DATE:
258: 			rtype = "Date";
259: 			break;
260: 		case SQLTypeId::TIME:
261: 			rtype = "difftime";
262: 			break;
263: 		case SQLTypeId::BIGINT:
264: 		case SQLTypeId::HUGEINT:
265: 		case SQLTypeId::FLOAT:
266: 		case SQLTypeId::DOUBLE:
267: 			rtype = "numeric";
268: 			break;
269: 		case SQLTypeId::VARCHAR: {
270: 			rtype = "character";
271: 			break;
272: 		}
273: 		default:
274: 			UNPROTECT(1); // retlist
275: 			Rf_error("duckdb_prepare_R: Unknown column type for prepare: %s", SQLTypeToString(stype).c_str());
276: 			break;
277: 		}
278: 		rtypes.push_back(rtype);
279: 	}
280: 
281: 	SEXP rtypessexp = cpp_str_to_strsexp(rtypes);
282: 	SET_VECTOR_ELT(retlist, 4, rtypessexp);
283: 	UNPROTECT(1); // rtypessexp
284: 
285: 	SET_VECTOR_ELT(retlist, 5, ScalarInteger(stmtholder->stmt->n_param));
286: 
287: 	UNPROTECT(1); // retlist
288: 	return retlist;
289: }
290: 
291: SEXP duckdb_bind_R(SEXP stmtsexp, SEXP paramsexp) {
292: 	if (TYPEOF(stmtsexp) != EXTPTRSXP) {
293: 		Rf_error("duckdb_bind_R: Need external pointer parameter");
294: 	}
295: 	RStatement *stmtholder = (RStatement *)R_ExternalPtrAddr(stmtsexp);
296: 	if (!stmtholder || !stmtholder->stmt) {
297: 		Rf_error("duckdb_bind_R: Invalid statement");
298: 	}
299: 
300: 	stmtholder->parameters.clear();
301: 	stmtholder->parameters.resize(stmtholder->stmt->n_param);
302: 
303: 	if (stmtholder->stmt->n_param == 0) {
304: 		Rf_error("duckdb_bind_R: dbBind called but query takes no parameters");
305: 		return R_NilValue;
306: 	}
307: 
308: 	if (TYPEOF(paramsexp) != VECSXP || LENGTH(paramsexp) != stmtholder->stmt->n_param) {
309: 		Rf_error("duckdb_bind_R: bind parameters need to be a list of length %i", stmtholder->stmt->n_param);
310: 	}
311: 
312: 	for (idx_t param_idx = 0; param_idx < LENGTH(paramsexp); param_idx++) {
313: 		Value val;
314: 		SEXP valsexp = VECTOR_ELT(paramsexp, param_idx);
315: 		if (LENGTH(valsexp) != 1) {
316: 			Rf_error("duckdb_bind_R: bind parameter values need to have length 1");
317: 		}
318: 		auto rtype = detect_rtype(valsexp);
319: 		switch (rtype) {
320: 		case RType::LOGICAL: {
321: 			auto lgl_val = INTEGER_POINTER(valsexp)[0];
322: 			val = Value::BOOLEAN(lgl_val);
323: 			val.is_null = RBooleanType::IsNull(lgl_val);
324: 			break;
325: 		}
326: 		case RType::INTEGER: {
327: 			auto int_val = INTEGER_POINTER(valsexp)[0];
328: 			val = Value::INTEGER(int_val);
329: 			val.is_null = RIntegerType::IsNull(int_val);
330: 			break;
331: 		}
332: 		case RType::NUMERIC: {
333: 			auto dbl_val = NUMERIC_POINTER(valsexp)[0];
334: 			val = Value::DOUBLE(dbl_val);
335: 			val.is_null = RDoubleType::IsNull(dbl_val);
336: 			break;
337: 		}
338: 		case RType::STRING: {
339: 			auto str_val = STRING_ELT(valsexp, 0);
340: 			val = Value(CHAR(str_val));
341: 			val.is_null = str_val == NA_STRING;
342: 			break;
343: 		}
344: 		case RType::FACTOR: {
345: 			auto int_val = INTEGER_POINTER(valsexp)[0];
346: 			auto levels = GET_LEVELS(valsexp);
347: 			val.type = TypeId::VARCHAR;
348: 			val.is_null = RIntegerType::IsNull(int_val);
349: 			if (!val.is_null) {
350: 				auto str_val = STRING_ELT(levels, int_val - 1);
351: 				val.str_value = string(CHAR(str_val));
352: 			}
353: 			break;
354: 		}
355: 		case RType::TIMESTAMP: {
356: 			auto ts_val = NUMERIC_POINTER(valsexp)[0];
357: 			val = Value::TIMESTAMP(RTimestampType::Convert(ts_val));
358: 			val.is_null = RTimestampType::IsNull(ts_val);
359: 			break;
360: 		}
361: 		case RType::DATE: {
362: 			auto d_val = NUMERIC_POINTER(valsexp)[0];
363: 			val = Value::DATE(RDateType::Convert(d_val));
364: 			val.is_null = RDateType::IsNull(d_val);
365: 			break;
366: 		}
367: 		default:
368: 			Rf_error("duckdb_bind_R: Unsupported parameter type");
369: 		}
370: 		stmtholder->parameters[param_idx] = val;
371: 	}
372: 	return R_NilValue;
373: }
374: 
375: SEXP duckdb_execute_R(SEXP stmtsexp) {
376: 	if (TYPEOF(stmtsexp) != EXTPTRSXP) {
377: 		Rf_error("duckdb_execute_R: Need external pointer parameter");
378: 	}
379: 	RStatement *stmtholder = (RStatement *)R_ExternalPtrAddr(stmtsexp);
380: 	if (!stmtholder || !stmtholder->stmt) {
381: 		Rf_error("duckdb_execute_R: Invalid statement");
382: 	}
383: 
384: 	auto generic_result = stmtholder->stmt->Execute(stmtholder->parameters, false);
385: 
386: 	if (!generic_result->success) {
387: 		Rf_error("duckdb_execute_R: Failed to run query\nError: %s", generic_result->error.c_str());
388: 	}
389: 	assert(generic_result->type == QueryResultType::MATERIALIZED_RESULT);
390: 	MaterializedQueryResult *result = (MaterializedQueryResult *)generic_result.get();
391: 
392: 	// step 2: create result data frame and allocate columns
393: 	uint32_t ncols = result->types.size();
394: 	uint64_t nrows = result->collection.count;
395: 
396: 	if (ncols > 0) {
397: 		SEXP retlist = PROTECT(NEW_LIST(ncols));
398: 		if (!retlist) {
399: 			UNPROTECT(1); // retlist
400: 			Rf_error("duckdb_execute_R: Memory allocation failed");
401: 		}
402: 		SET_NAMES(retlist, cpp_str_to_strsexp(result->names));
403: 		UNPROTECT(1); // names
404: 
405: 		for (size_t col_idx = 0; col_idx < ncols; col_idx++) {
406: 			SEXP varvalue = NULL;
407: 			switch (result->sql_types[col_idx].id) {
408: 			case SQLTypeId::BOOLEAN:
409: 				varvalue = PROTECT(NEW_LOGICAL(nrows));
410: 				break;
411: 			case SQLTypeId::TINYINT:
412: 			case SQLTypeId::SMALLINT:
413: 			case SQLTypeId::INTEGER:
414: 				varvalue = PROTECT(NEW_INTEGER(nrows));
415: 				break;
416: 			case SQLTypeId::BIGINT:
417: 			case SQLTypeId::HUGEINT:
418: 			case SQLTypeId::FLOAT:
419: 			case SQLTypeId::DOUBLE:
420: 			case SQLTypeId::DECIMAL:
421: 			case SQLTypeId::TIMESTAMP:
422: 			case SQLTypeId::DATE:
423: 			case SQLTypeId::TIME:
424: 				varvalue = PROTECT(NEW_NUMERIC(nrows));
425: 				break;
426: 			case SQLTypeId::VARCHAR:
427: 				varvalue = PROTECT(NEW_STRING(nrows));
428: 				break;
429: 			default:
430: 				UNPROTECT(1); // retlist
431: 				Rf_error("duckdb_execute_R: Unknown column type for execute: %s/%s",
432: 				         SQLTypeToString(result->sql_types[col_idx]).c_str(),
433: 				         TypeIdToString(result->types[col_idx]).c_str());
434: 			}
435: 			if (!varvalue) {
436: 				UNPROTECT(2); // varvalue, retlist
437: 				Rf_error("duckdb_execute_R: Memory allocation failed");
438: 			}
439: 			SET_VECTOR_ELT(retlist, col_idx, varvalue);
440: 			UNPROTECT(1); /* varvalue */
441: 		}
442: 
443: 		// at this point retlist is fully allocated and the only protected SEXP
444: 
445: 		// step 3: set values from chunks
446: 		uint64_t dest_offset = 0;
447: 		while (true) {
448: 			auto chunk = result->Fetch();
449: 			if (chunk->size() == 0) {
450: 				break;
451: 			}
452: 			assert(chunk->column_count() == ncols);
453: 			assert(chunk->column_count() == LENGTH(retlist));
454: 			for (size_t col_idx = 0; col_idx < chunk->column_count(); col_idx++) {
455: 				SEXP dest = VECTOR_ELT(retlist, col_idx);
456: 				switch (result->sql_types[col_idx].id) {
457: 				case SQLTypeId::BOOLEAN:
458: 					vector_to_r<int8_t, uint32_t>(chunk->data[col_idx], chunk->size(), LOGICAL_POINTER(dest),
459: 					                              dest_offset, NA_LOGICAL);
460: 					break;
461: 				case SQLTypeId::TINYINT:
462: 					vector_to_r<int8_t, uint32_t>(chunk->data[col_idx], chunk->size(), INTEGER_POINTER(dest),
463: 					                              dest_offset, NA_INTEGER);
464: 					break;
465: 				case SQLTypeId::SMALLINT:
466: 					vector_to_r<int16_t, uint32_t>(chunk->data[col_idx], chunk->size(), INTEGER_POINTER(dest),
467: 					                               dest_offset, NA_INTEGER);
468: 					break;
469: 				case SQLTypeId::INTEGER:
470: 					vector_to_r<int32_t, uint32_t>(chunk->data[col_idx], chunk->size(), INTEGER_POINTER(dest),
471: 					                               dest_offset, NA_INTEGER);
472: 					break;
473: 				case SQLTypeId::TIMESTAMP: {
474: 					auto &src_vec = chunk->data[col_idx];
475: 					auto src_data = FlatVector::GetData<int64_t>(src_vec);
476: 					auto &nullmask = FlatVector::Nullmask(src_vec);
477: 					double *dest_ptr = ((double *)NUMERIC_POINTER(dest)) + dest_offset;
478: 					for (size_t row_idx = 0; row_idx < chunk->size(); row_idx++) {
479: 						dest_ptr[row_idx] =
480: 						    nullmask[row_idx] ? NA_REAL : (double)Timestamp::GetEpoch(src_data[row_idx]);
481: 					}
482: 
483: 					// some dresssup for R
484: 					SEXP cl = PROTECT(NEW_STRING(2));
485: 					SET_STRING_ELT(cl, 0, PROTECT(mkChar("POSIXct")));
486: 					SET_STRING_ELT(cl, 1, PROTECT(mkChar("POSIXt")));
487: 					SET_CLASS(dest, cl);
488: 					setAttrib(dest, install("tzone"), PROTECT(mkString("UTC")));
489: 					UNPROTECT(4);
490: 					break;
491: 				}
492: 				case SQLTypeId::DATE: {
493: 					auto &src_vec = chunk->data[col_idx];
494: 					auto src_data = FlatVector::GetData<int32_t>(src_vec);
495: 					auto &nullmask = FlatVector::Nullmask(src_vec);
496: 					double *dest_ptr = ((double *)NUMERIC_POINTER(dest)) + dest_offset;
497: 					for (size_t row_idx = 0; row_idx < chunk->size(); row_idx++) {
498: 						dest_ptr[row_idx] = nullmask[row_idx] ? NA_REAL : (double)(src_data[row_idx]) - 719528;
499: 					}
500: 
501: 					// some dresssup for R
502: 					SET_CLASS(dest, PROTECT(mkString("Date")));
503: 					UNPROTECT(1);
504: 					break;
505: 				}
506: 				case SQLTypeId::TIME: {
507: 					auto &src_vec = chunk->data[col_idx];
508: 					auto src_data = FlatVector::GetData<int32_t>(src_vec);
509: 					auto &nullmask = FlatVector::Nullmask(src_vec);
510: 					double *dest_ptr = ((double *)NUMERIC_POINTER(dest)) + dest_offset;
511: 					for (size_t row_idx = 0; row_idx < chunk->size(); row_idx++) {
512: 
513: 						if (nullmask[row_idx]) {
514: 							dest_ptr[row_idx] = NA_REAL;
515: 						} else {
516: 							time_t n = src_data[row_idx];
517: 							int h;
518: 							double frac;
519: 							h = n / 3600000;
520: 							n -= h * 3600000;
521: 							frac = (n / 60000.0) / 60.0;
522: 							dest_ptr[row_idx] = h + frac;
523: 						}
524: 					}
525: 
526: 					// some dresssup for R
527: 					SET_CLASS(dest, PROTECT(mkString("difftime")));
528: 					setAttrib(dest, install("units"), PROTECT(mkString("hours")));
529: 					UNPROTECT(2);
530: 					break;
531: 				}
532: 				case SQLTypeId::BIGINT:
533: 					vector_to_r<int64_t, double>(chunk->data[col_idx], chunk->size(), NUMERIC_POINTER(dest),
534: 					                             dest_offset, NA_REAL);
535: 					break;
536: 				case SQLTypeId::HUGEINT: {
537: 					auto &src_vec = chunk->data[col_idx];
538: 					auto src_data = FlatVector::GetData<hugeint_t>(src_vec);
539: 					auto &nullmask = FlatVector::Nullmask(src_vec);
540: 					double *dest_ptr = ((double *)NUMERIC_POINTER(dest)) + dest_offset;
541: 					for (size_t row_idx = 0; row_idx < chunk->size(); row_idx++) {
542: 						if (nullmask[row_idx]) {
543: 							dest_ptr[row_idx] = NA_REAL;
544: 						} else {
545: 							Hugeint::TryCast(src_data[row_idx], dest_ptr[row_idx]);
546: 						}
547: 					}
548: 					break;
549: 				}
550: 				case SQLTypeId::FLOAT:
551: 					vector_to_r<float, double>(chunk->data[col_idx], chunk->size(), NUMERIC_POINTER(dest), dest_offset,
552: 					                           NA_REAL);
553: 					break;
554: 
555: 				case SQLTypeId::DOUBLE:
556: 					vector_to_r<double, double>(chunk->data[col_idx], chunk->size(), NUMERIC_POINTER(dest), dest_offset,
557: 					                            NA_REAL);
558: 					break;
559: 				case SQLTypeId::VARCHAR: {
560: 					auto src_ptr = FlatVector::GetData<string_t>(chunk->data[col_idx]);
561: 					auto &nullmask = FlatVector::Nullmask(chunk->data[col_idx]);
562: 					for (size_t row_idx = 0; row_idx < chunk->size(); row_idx++) {
563: 						if (nullmask[row_idx]) {
564: 							SET_STRING_ELT(dest, dest_offset + row_idx, NA_STRING);
565: 						} else {
566: 							SET_STRING_ELT(dest, dest_offset + row_idx, mkCharCE(src_ptr[row_idx].GetData(), CE_UTF8));
567: 						}
568: 					}
569: 					break;
570: 				}
571: 				default:
572: 					Rf_error("duckdb_execute_R: Unknown column type for convert: %s",
573: 					         TypeIdToString(chunk->GetTypes()[col_idx]).c_str());
574: 					break;
575: 				}
576: 			}
577: 			dest_offset += chunk->size();
578: 		}
579: 
580: 		assert(dest_offset == nrows);
581: 		UNPROTECT(1); /* retlist */
582: 		return retlist;
583: 	}
584: 	return ScalarReal(0); // no need for protection because no allocation can happen afterwards
585: }
586: 
587: static SEXP duckdb_finalize_database_R(SEXP dbsexp) {
588: 	if (TYPEOF(dbsexp) != EXTPTRSXP) {
589: 		Rf_error("duckdb_finalize_connection_R: Need external pointer parameter");
590: 	}
591: 	DuckDB *dbaddr = (DuckDB *)R_ExternalPtrAddr(dbsexp);
592: 	if (dbaddr) {
593: 		warning("duckdb_finalize_database_R: Database is garbage-collected, use dbDisconnect(con, shutdown=TRUE) or "
594: 		        "duckdb::duckdb_shutdown(drv) to avoid this.");
595: 		R_ClearExternalPtr(dbsexp);
596: 		delete dbaddr;
597: 	}
598: 	return R_NilValue;
599: }
600: 
601: struct DataFrameScanFunctionData : public TableFunctionData {
602: 	DataFrameScanFunctionData(SEXP df, idx_t row_count, vector<RType> rtypes)
603: 	    : df(df), row_count(row_count), rtypes(rtypes), position(0) {
604: 	}
605: 	SEXP df;
606: 	idx_t row_count;
607: 	vector<RType> rtypes;
608: 	idx_t position;
609: };
610: 
611: struct DataFrameScanFunction : public TableFunction {
612: 	DataFrameScanFunction()
613: 	    : TableFunction("dataframe_scan", {SQLType::VARCHAR}, dataframe_scan_bind, dataframe_scan_function, nullptr){};
614: 
615: 	static unique_ptr<FunctionData> dataframe_scan_bind(ClientContext &context, vector<Value> inputs,
616: 	                                                    vector<SQLType> &return_types, vector<string> &names) {
617: 		// TODO have a better way to pass this pointer
618: 		SEXP df((SEXP)std::stoull(inputs[0].GetValue<string>(), nullptr, 16));
619: 
620: 		auto df_names = GET_NAMES(df);
621: 		vector<RType> rtypes;
622: 
623: 		for (idx_t col_idx = 0; col_idx < LENGTH(df); col_idx++) {
624: 			names.push_back(string(CHAR(STRING_ELT(df_names, col_idx))));
625: 			SEXP coldata = VECTOR_ELT(df, col_idx);
626: 			rtypes.push_back(detect_rtype(coldata));
627: 			SQLType duckdb_col_type;
628: 			switch (rtypes[col_idx]) {
629: 			case RType::LOGICAL:
630: 				duckdb_col_type = SQLType::BOOLEAN;
631: 				break;
632: 			case RType::INTEGER:
633: 				duckdb_col_type = SQLType::INTEGER;
634: 				break;
635: 			case RType::NUMERIC:
636: 				duckdb_col_type = SQLType::DOUBLE;
637: 				break;
638: 			case RType::FACTOR:
639: 			case RType::STRING:
640: 				duckdb_col_type = SQLType::VARCHAR;
641: 				break;
642: 			case RType::TIMESTAMP:
643: 				duckdb_col_type = SQLType::TIMESTAMP;
644: 				break;
645: 			case RType::DATE:
646: 				duckdb_col_type = SQLType::DATE;
647: 				break;
648: 			default:
649: 				Rf_error("Unsupported column type for scan");
650: 			}
651: 			return_types.push_back(duckdb_col_type);
652: 		}
653: 
654: 		auto row_count = LENGTH(VECTOR_ELT(df, 0));
655: 		return make_unique<DataFrameScanFunctionData>(df, row_count, rtypes);
656: 	}
657: 
658: 	static void dataframe_scan_function(ClientContext &context, vector<Value> &input, DataChunk &output,
659: 	                                    FunctionData *dataptr) {
660: 		auto &data = *((DataFrameScanFunctionData *)dataptr);
661: 
662: 		if (data.position >= data.row_count) {
663: 			return;
664: 		}
665: 		idx_t this_count = std::min((idx_t)STANDARD_VECTOR_SIZE, data.row_count - data.position);
666: 
667: 		output.SetCardinality(this_count);
668: 
669: 		// TODO this is quite similar to append, unify!
670: 		for (idx_t col_idx = 0; col_idx < output.column_count(); col_idx++) {
671: 			auto &v = output.data[col_idx];
672: 			SEXP coldata = VECTOR_ELT(data.df, col_idx);
673: 
674: 			switch (data.rtypes[col_idx]) {
675: 			case RType::LOGICAL: {
676: 				auto data_ptr = INTEGER_POINTER(coldata) + data.position;
677: 				AppendColumnSegment<int, bool, RBooleanType>(data_ptr, v, this_count);
678: 				break;
679: 			}
680: 			case RType::INTEGER: {
681: 				auto data_ptr = INTEGER_POINTER(coldata) + data.position;
682: 				AppendColumnSegment<int, int, RIntegerType>(data_ptr, v, this_count);
683: 				break;
684: 			}
685: 			case RType::NUMERIC: {
686: 				auto data_ptr = NUMERIC_POINTER(coldata) + data.position;
687: 				AppendColumnSegment<double, double, RDoubleType>(data_ptr, v, this_count);
688: 				break;
689: 			}
690: 			case RType::STRING:
691: 				AppendStringSegment(coldata, v, data.position, this_count);
692: 				break;
693: 			case RType::FACTOR:
694: 				AppendFactor(coldata, v, data.position, this_count);
695: 				break;
696: 			case RType::TIMESTAMP: {
697: 				auto data_ptr = NUMERIC_POINTER(coldata) + data.position;
698: 				AppendColumnSegment<double, timestamp_t, RTimestampType>(data_ptr, v, this_count);
699: 				break;
700: 			}
701: 			case RType::DATE: {
702: 				auto data_ptr = NUMERIC_POINTER(coldata) + data.position;
703: 				AppendColumnSegment<double, date_t, RDateType>(data_ptr, v, this_count);
704: 				break;
705: 			}
706: 			default:
707: 				throw;
708: 			}
709: 		}
710: 
711: 		data.position += this_count;
712: 	}
713: };
714: 
715: SEXP duckdb_startup_R(SEXP dbdirsexp, SEXP readonlysexp) {
716: 	if (TYPEOF(dbdirsexp) != STRSXP || LENGTH(dbdirsexp) != 1) {
717: 		Rf_error("duckdb_startup_R: Need string parameter for dbdir");
718: 	}
719: 	char *dbdir = (char *)CHAR(STRING_ELT(dbdirsexp, 0));
720: 
721: 	if (TYPEOF(readonlysexp) != LGLSXP || LENGTH(readonlysexp) != 1) {
722: 		Rf_error("duckdb_startup_R: Need string parameter for read_only");
723: 	}
724: 	bool read_only = (bool)LOGICAL_ELT(readonlysexp, 0);
725: 
726: 	if (strlen(dbdir) == 0 || strcmp(dbdir, ":memory:") == 0) {
727: 		dbdir = NULL;
728: 	}
729: 
730: 	DBConfig config;
731: 	config.access_mode = AccessMode::READ_WRITE;
732: 	if (read_only) {
733: 		config.access_mode = AccessMode::READ_ONLY;
734: 	}
735: 	DuckDB *dbaddr;
736: 	try {
737: 		dbaddr = new DuckDB(dbdir, &config);
738: 	} catch (...) {
739: 		Rf_error("duckdb_startup_R: Failed to open database");
740: 	}
741: 	dbaddr->LoadExtension<ParquetExtension>();
742: 
743: 	DataFrameScanFunction scan_fun;
744: 	CreateTableFunctionInfo info(scan_fun);
745: 	Connection conn(*dbaddr);
746: 	auto &context = *conn.context;
747: 	context.transaction.BeginTransaction();
748: 	context.catalog.CreateTableFunction(context, &info);
749: 	context.transaction.Commit();
750: 
751: 	SEXP dbsexp = PROTECT(R_MakeExternalPtr(dbaddr, R_NilValue, R_NilValue));
752: 	R_RegisterCFinalizer(dbsexp, (void (*)(SEXP))duckdb_finalize_database_R);
753: 	UNPROTECT(1);
754: 	return dbsexp;
755: }
756: 
757: SEXP duckdb_shutdown_R(SEXP dbsexp) {
758: 	if (TYPEOF(dbsexp) != EXTPTRSXP) {
759: 		Rf_error("duckdb_finalize_connection_R: Need external pointer parameter");
760: 	}
761: 	DuckDB *dbaddr = (DuckDB *)R_ExternalPtrAddr(dbsexp);
762: 	if (dbaddr) {
763: 		R_ClearExternalPtr(dbsexp);
764: 		delete dbaddr;
765: 	}
766: 
767: 	return R_NilValue;
768: }
769: 
770: static SEXP duckdb_finalize_connection_R(SEXP connsexp) {
771: 	if (TYPEOF(connsexp) != EXTPTRSXP) {
772: 		Rf_error("duckdb_finalize_connection_R: Need external pointer parameter");
773: 	}
774: 	Connection *connaddr = (Connection *)R_ExternalPtrAddr(connsexp);
775: 	if (connaddr) {
776: 		warning("duckdb_finalize_connection_R: Connection is garbage-collected, use dbDisconnect() to avoid this.");
777: 		R_ClearExternalPtr(connsexp);
778: 		delete connaddr;
779: 	}
780: 	return R_NilValue;
781: }
782: 
783: SEXP duckdb_register_R(SEXP connsexp, SEXP namesexp, SEXP valuesexp) {
784: 
785: 	if (TYPEOF(connsexp) != EXTPTRSXP) {
786: 		Rf_error("duckdb_append_R: Need external pointer parameter for connection");
787: 	}
788: 
789: 	Connection *conn = (Connection *)R_ExternalPtrAddr(connsexp);
790: 	if (!conn) {
791: 		Rf_error("duckdb_append_R: Invalid connection");
792: 	}
793: 
794: 	if (TYPEOF(namesexp) != STRSXP || LENGTH(namesexp) != 1) {
795: 		Rf_error("duckdb_append_R: Need single string parameter for name");
796: 	}
797: 	auto name = string(CHAR(STRING_ELT(namesexp, 0)));
798: 
799: 	if (TYPEOF(valuesexp) != VECSXP || LENGTH(valuesexp) < 1 ||
800: 	    strcmp("data.frame", CHAR(STRING_ELT(GET_CLASS(valuesexp), 0))) != 0) {
801: 		Rf_error("duckdb_append_R: Need at least one-column data frame parameter for value");
802: 	}
803: 
804: 	auto key = install(("_registered_df_" + name).c_str());
805: 	setAttrib(connsexp, key, valuesexp);
806: 
807: 	// TODO put it into a conn attr that contains a named list to keep from gc!
808: 	std::ostringstream address;
809: 	address << (void const *)valuesexp;
810: 	string pointer_str = address.str();
811: 
812: 	// hack alert: put the pointer address into the function call as a string
813: 	auto res = conn->Query("CREATE OR REPLACE TEMPORARY VIEW \"" + name + "\" AS SELECT * FROM dataframe_scan('" +
814: 	                       pointer_str + "')");
815: 	if (!res->success) {
816: 		Rf_error(res->error.c_str());
817: 	}
818: 	return R_NilValue;
819: }
820: 
821: SEXP duckdb_unregister_R(SEXP connsexp, SEXP namesexp) {
822: 
823: 	if (TYPEOF(connsexp) != EXTPTRSXP) {
824: 		Rf_error("duckdb_append_R: Need external pointer parameter for connection");
825: 	}
826: 
827: 	Connection *conn = (Connection *)R_ExternalPtrAddr(connsexp);
828: 	if (!conn) {
829: 		Rf_error("duckdb_append_R: Invalid connection");
830: 	}
831: 
832: 	if (TYPEOF(namesexp) != STRSXP || LENGTH(namesexp) != 1) {
833: 		Rf_error("duckdb_append_R: Need single string parameter for name");
834: 	}
835: 	auto name = string(CHAR(STRING_ELT(namesexp, 0)));
836: 
837: 	auto key = install(("_registered_df_" + name).c_str());
838: 	setAttrib(connsexp, key, R_NilValue);
839: 
840: 	auto res = conn->Query("DROP VIEW IF EXISTS \"" + name + "\"");
841: 	if (!res->success) {
842: 		Rf_error(res->error.c_str());
843: 	}
844: 
845: 	// TODO
846: 	return R_NilValue;
847: }
848: 
849: SEXP duckdb_connect_R(SEXP dbsexp) {
850: 	if (TYPEOF(dbsexp) != EXTPTRSXP) {
851: 		Rf_error("duckdb_connect_R: Need external pointer parameter");
852: 	}
853: 	DuckDB *dbaddr = (DuckDB *)R_ExternalPtrAddr(dbsexp);
854: 	if (!dbaddr) {
855: 		Rf_error("duckdb_connect_R: Invalid database reference");
856: 	}
857: 
858: 	SEXP connsexp = PROTECT(R_MakeExternalPtr(new Connection(*dbaddr), R_NilValue, R_NilValue));
859: 	R_RegisterCFinalizer(connsexp, (void (*)(SEXP))duckdb_finalize_connection_R);
860: 	UNPROTECT(1);
861: 
862: 	return connsexp;
863: }
864: 
865: SEXP duckdb_disconnect_R(SEXP connsexp) {
866: 	if (TYPEOF(connsexp) != EXTPTRSXP) {
867: 		Rf_error("duckdb_disconnect_R: Need external pointer parameter");
868: 	}
869: 	Connection *connaddr = (Connection *)R_ExternalPtrAddr(connsexp);
870: 	if (connaddr) {
871: 		R_ClearExternalPtr(connsexp);
872: 		delete connaddr;
873: 	}
874: 	return R_NilValue;
875: }
876: 
877: SEXP duckdb_ptr_to_str(SEXP extptr) {
878: 	if (TYPEOF(extptr) != EXTPTRSXP) {
879: 		Rf_error("duckdb_ptr_to_str: Need external pointer parameter");
880: 	}
881: 	SEXP ret = PROTECT(NEW_STRING(1));
882: 	SET_STRING_ELT(ret, 0, NA_STRING);
883: 	void *ptr = R_ExternalPtrAddr(extptr);
884: 	if (ptr != NULL) {
885: 		char buf[100];
886: 		snprintf(buf, 100, "%p", ptr);
887: 		SET_STRING_ELT(ret, 0, mkChar(buf));
888: 	}
889: 	UNPROTECT(1);
890: 	return ret;
891: }
892: 
893: // R native routine registration
894: #define CALLDEF(name, n)                                                                                               \
895: 	{ #name, (DL_FUNC)&name, n }
896: static const R_CallMethodDef R_CallDef[] = {CALLDEF(duckdb_startup_R, 2),
897:                                             CALLDEF(duckdb_connect_R, 1),
898:                                             CALLDEF(duckdb_prepare_R, 2),
899:                                             CALLDEF(duckdb_bind_R, 2),
900:                                             CALLDEF(duckdb_execute_R, 1),
901:                                             CALLDEF(duckdb_release_R, 1),
902:                                             CALLDEF(duckdb_register_R, 3),
903:                                             CALLDEF(duckdb_unregister_R, 2),
904:                                             CALLDEF(duckdb_disconnect_R, 1),
905:                                             CALLDEF(duckdb_shutdown_R, 1),
906:                                             CALLDEF(duckdb_ptr_to_str, 1),
907: 
908:                                             {NULL, NULL, 0}};
909: 
910: void R_init_duckdb(DllInfo *dll) {
911: 	R_registerRoutines(dll, NULL, R_CallDef, NULL, NULL);
912: 	R_useDynamicSymbols(dll, FALSE);
913: }
914: }
[end of tools/rpkg/src/duckdbr.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: