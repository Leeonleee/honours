You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Wrong behavior when including a filter field in the select over a parquet file view
I have a parquet file with the following layout:

ORGUNITID: int64
VALIDFROM: string
VALIDTO: string
ORGUNITNAME: string
**NAMEVALIDFROM: string
NAMEVALIDTO: string**
PARENTID: int64
PARENTNAME: string
PARENTNAMEVALIDFROM: string
PARENTNAMEVALIDTO: string
LEVELID: int64
LEVELNAME: string
PARENTLEVELID: int64
PARENTLEVELNAME: string
ORGSTRUCTVALIDFROM: string
ORGSTRUCTVALIDTO: string
PARENTORGSTRUCTVALIDFROM: string
PARENTORGSTRUCTVALIDTO: string
CUSTOMERCODE: string

The fields VALIDFROM and VALIDTO range from '1900-01-01' to '9999-31-12'. 

In python I create a view of the parquet file:

c = duckdb.connect()
c.execute(f"CREATE VIEW tbl AS SELECT * FROM parquet_scan('table.parquet');")

If I select all fields everything works fine, 

sql = '''
    SELECT * from tbl
    where Namevalidfrom <=  '2017-03-01'
    AND Namevalidto >=  '2017-03-01'
    AND Parentnamevalidfrom <=  '2017-03-01'
    AND Parentnamevalidto >=  '2017-03-01'
    AND Orgunitid IN  (.... numbers ...)
    AND Parentid in (... numbers ...)
    AND CustomerCode = 'code'        
'''
this returns an array of tuples which satisfy criteria

If I select any of these 2 fields alone or in combination with other fields, the response is empty array
NAMEVALIDFROM: string
NAMEVALIDTO: string

sql = '''
    SELECT **Namevalidfrom** from tbl
    where Namevalidfrom <=  '2017-03-01'
    AND Namevalidto >=  '2017-03-01'
    AND Parentnamevalidfrom <=  '2017-03-01'
    AND Parentnamevalidto >=  '2017-03-01'
    AND Orgunitid IN  (.... numbers ...)
    AND Parentid in (... numbers ...)
    AND CustomerCode = 'code'   
'''

^^^ this returns an empty array

Or even if select through a subquery:
sql = '''
    SELECT **Namevalidfrom**  from (
        select * from OrgUnitBridge 
    where Namevalidfrom <=  '2017-03-01'
    AND Namevalidto >=  '2017-03-01'
    AND Parentnamevalidfrom <=  '2017-03-01'
    AND Parentnamevalidto >=  '2017-03-01'
    AND Orgunitid IN  (.... numbers ...)
    AND Parentid in (... numbers ...)
    AND CustomerCode = 'code' ) t
'''
^^^ this returns an empty array

I have changed the data type in the parquet file for these 2 fields to both STRING and TIMESTAMP and I get the same behavior.

Using v.0.2.4 through Python in Ubuntu 20.
 
Here's a sample of the data:
[(10318, '1900-01-01 00:00:00.000', '9999-12-31 00:00:00.000', 'XYZ Admin', '1900-01-01 00:00:00.000', '9999-12-31 00:00:00.000', 10144, 'XYZ', '1900-01-01 00:00:00.000', '9999-12-31 00:00:00.000', 10005, 'unit', 10004, 'department', '2020-04-27 00:00:00.000', '9999-12-31 00:00:00.000', '1900-01-01 00:00:00.000', '2020-04-23 00:00:00.000', 'code')]

I have enabled logs PRAGMA but the file is empty.


</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
2: 
3: ![.github/workflows/main.yml](https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
6: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3901452.svg)](https://zenodo.org/record/3901452)
7: 
8: 
9: ## Installation
10: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
11: 
12: ## Development
13: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
14: 
15: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
16: 
17: 
[end of README.md]
[start of extension/parquet/include/parquet_reader.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // parquet_reader.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/common.hpp"
12: #include "duckdb/common/exception.hpp"
13: #include "duckdb/common/string_util.hpp"
14: #include "duckdb/common/types/data_chunk.hpp"
15: #include "resizable_buffer.hpp"
16: #include "column_reader.hpp"
17: 
18: #include "parquet_file_metadata_cache.hpp"
19: #include "parquet_types.h"
20: #include "parquet_rle_bp_decoder.hpp"
21: 
22: #include <exception>
23: 
24: namespace parquet {
25: namespace format {
26: class FileMetaData;
27: }
28: } // namespace parquet
29: 
30: namespace duckdb {
31: class ClientContext;
32: class ChunkCollection;
33: class BaseStatistics;
34: struct TableFilterSet;
35: 
36: struct ParquetReaderScanState {
37: 	vector<idx_t> group_idx_list;
38: 	int64_t current_group;
39: 	vector<column_t> column_ids;
40: 	idx_t group_offset;
41: 	unique_ptr<ColumnReader> root_reader;
42: 	unique_ptr<apache::thrift::protocol::TProtocol> thrift_file_proto;
43: 
44: 	bool finished;
45: 	TableFilterSet *filters;
46: 	SelectionVector sel;
47: 
48: 	ResizeableBuffer define_buf;
49: 	ResizeableBuffer repeat_buf;
50: };
51: 
52: class ParquetReader {
53: public:
54: 	ParquetReader(ClientContext &context, string file_name, vector<LogicalType> expected_types,
55: 	              const string &initial_filename = string());
56: 	ParquetReader(ClientContext &context, string file_name) : ParquetReader(context, file_name, vector<LogicalType>()) {
57: 	}
58: 	~ParquetReader();
59: 
60: 	string file_name;
61: 	vector<LogicalType> return_types;
62: 	vector<string> names;
63: 	shared_ptr<ParquetFileMetadataCache> metadata;
64: 
65: public:
66: 	void Initialize(ParquetReaderScanState &state, vector<column_t> column_ids, vector<idx_t> groups_to_read,
67: 	                TableFilterSet *table_filters);
68: 	void Scan(ParquetReaderScanState &state, DataChunk &output);
69: 
70: 	idx_t NumRows();
71: 	idx_t NumRowGroups();
72: 
73: 	const parquet::format::FileMetaData *GetFileMetadata();
74: 
75: 	static unique_ptr<BaseStatistics> ReadStatistics(LogicalType &type, column_t column_index,
76: 	                                                 const parquet::format::FileMetaData *file_meta_data);
77: 
78: private:
79: 	bool ScanInternal(ParquetReaderScanState &state, DataChunk &output);
80: 
81: 	const parquet::format::RowGroup &GetGroup(ParquetReaderScanState &state);
82: 	void PrepareRowGroupBuffer(ParquetReaderScanState &state, idx_t col_idx);
83: 
84: 	template <typename... Args>
85: 	std::runtime_error FormatException(const string fmt_str, Args... params) {
86: 		return std::runtime_error("Failed to read Parquet file \"" + file_name +
87: 		                          "\": " + StringUtil::Format(fmt_str, params...));
88: 	}
89: 
90: private:
91: 	ClientContext &context;
92: };
93: 
94: } // namespace duckdb
[end of extension/parquet/include/parquet_reader.hpp]
[start of extension/parquet/parquet_reader.cpp]
1: #include "parquet_reader.hpp"
2: #include "parquet_timestamp.hpp"
3: #include "parquet_statistics.hpp"
4: #include "column_reader.hpp"
5: 
6: #include "thrift_tools.hpp"
7: 
8: #include "parquet_file_metadata_cache.hpp"
9: 
10: #include "duckdb/planner/table_filter.hpp"
11: 
12: #include "duckdb/common/file_system.hpp"
13: #include "duckdb/common/string_util.hpp"
14: #include "duckdb/common/types/date.hpp"
15: #include "duckdb/common/pair.hpp"
16: 
17: #include "duckdb/storage/object_cache.hpp"
18: 
19: #include <sstream>
20: #include <cassert>
21: #include <chrono>
22: #include <cstring>
23: #include <iostream>
24: 
25: namespace duckdb {
26: 
27: using parquet::format::ColumnChunk;
28: using parquet::format::ConvertedType;
29: using parquet::format::FieldRepetitionType;
30: using parquet::format::FileMetaData;
31: using parquet::format::RowGroup;
32: using parquet::format::SchemaElement;
33: using parquet::format::Statistics;
34: using parquet::format::Type;
35: 
36: static shared_ptr<ParquetFileMetadataCache> LoadMetaData(apache::thrift::protocol::TProtocol &proto, idx_t read_pos) {
37: 	auto current_time = std::chrono::system_clock::to_time_t(std::chrono::system_clock::now());
38: 	auto metadata = make_unique<FileMetaData>();
39: 	((ThriftFileTransport *)proto.getTransport().get())->SetLocation(read_pos);
40: 	metadata->read(&proto);
41: 	return make_shared<ParquetFileMetadataCache>(move(metadata), current_time);
42: }
43: 
44: static LogicalType DeriveLogicalType(const SchemaElement &s_ele) {
45: 	// inner node
46: 	D_ASSERT(s_ele.__isset.type && s_ele.num_children == 0);
47: 	switch (s_ele.type) {
48: 	case Type::BOOLEAN:
49: 		return LogicalType::BOOLEAN;
50: 	case Type::INT32:
51: 		if (s_ele.__isset.converted_type) {
52: 			switch (s_ele.converted_type) {
53: 			case ConvertedType::DATE:
54: 				return LogicalType::DATE;
55: 			case ConvertedType::UINT_8:
56: 				return LogicalType::UTINYINT;
57: 			case ConvertedType::UINT_16:
58: 				return LogicalType::USMALLINT;
59: 			default:
60: 				return LogicalType::INTEGER;
61: 			}
62: 		}
63: 		return LogicalType::INTEGER;
64: 	case Type::INT64:
65: 		if (s_ele.__isset.converted_type) {
66: 			switch (s_ele.converted_type) {
67: 			case ConvertedType::TIMESTAMP_MICROS:
68: 			case ConvertedType::TIMESTAMP_MILLIS:
69: 				return LogicalType::TIMESTAMP;
70: 			case ConvertedType::UINT_32:
71: 				return LogicalType::UINTEGER;
72: 			case ConvertedType::UINT_64:
73: 				return LogicalType::UBIGINT;
74: 			default:
75: 				return LogicalType::BIGINT;
76: 			}
77: 		}
78: 		return LogicalType::BIGINT;
79: 
80: 	case Type::INT96: // always a timestamp it would seem
81: 		return LogicalType::TIMESTAMP;
82: 	case Type::FLOAT:
83: 		return LogicalType::FLOAT;
84: 	case Type::DOUBLE:
85: 		return LogicalType::DOUBLE;
86: 		//			case parquet::format::Type::FIXED_LEN_BYTE_ARRAY: {
87: 		// TODO some decimals yuck
88: 	case Type::BYTE_ARRAY:
89: 		if (s_ele.__isset.converted_type) {
90: 			switch (s_ele.converted_type) {
91: 			case ConvertedType::UTF8:
92: 				return LogicalType::VARCHAR;
93: 			default:
94: 				return LogicalType::BLOB;
95: 			}
96: 		}
97: 		return LogicalType::BLOB;
98: 	case Type::FIXED_LEN_BYTE_ARRAY:
99: 		if (s_ele.__isset.converted_type && s_ele.converted_type == ConvertedType::DECIMAL && s_ele.__isset.scale &&
100: 		    s_ele.__isset.scale && s_ele.__isset.type_length) {
101: 			// habemus decimal
102: 			return LogicalType(LogicalTypeId::DECIMAL, s_ele.precision, s_ele.scale);
103: 		}
104: 	default:
105: 		return LogicalType::INVALID;
106: 	}
107: }
108: 
109: static unique_ptr<ColumnReader> CreateReaderRecursive(const FileMetaData *file_meta_data, idx_t depth, idx_t max_define,
110:                                                       idx_t max_repeat, idx_t &next_schema_idx, idx_t &next_file_idx) {
111: 	D_ASSERT(file_meta_data);
112: 	D_ASSERT(next_schema_idx < file_meta_data->schema.size());
113: 	auto &s_ele = file_meta_data->schema[next_schema_idx];
114: 	auto this_idx = next_schema_idx;
115: 
116: 	if (s_ele.__isset.repetition_type) {
117: 		if (s_ele.repetition_type != FieldRepetitionType::REQUIRED) {
118: 			max_define = depth;
119: 		}
120: 		if (s_ele.repetition_type == FieldRepetitionType::REPEATED) {
121: 			max_repeat++;
122: 		}
123: 	}
124: 
125: 	if (!s_ele.__isset.type) { // inner node
126: 		if (s_ele.num_children == 0) {
127: 			throw std::runtime_error("Node has no children but should");
128: 		}
129: 		child_list_t<LogicalType> child_types;
130: 		vector<unique_ptr<ColumnReader>> child_readers;
131: 
132: 		idx_t c_idx = 0;
133: 		while (c_idx < (idx_t)s_ele.num_children) {
134: 			next_schema_idx++;
135: 
136: 			auto &child_ele = file_meta_data->schema[next_schema_idx];
137: 
138: 			auto child_reader = CreateReaderRecursive(file_meta_data, depth + 1, max_define, max_repeat,
139: 			                                          next_schema_idx, next_file_idx);
140: 			child_types.push_back(make_pair(child_ele.name, child_reader->Type()));
141: 			child_readers.push_back(move(child_reader));
142: 
143: 			c_idx++;
144: 		}
145: 		D_ASSERT(!child_types.empty());
146: 		unique_ptr<ColumnReader> result;
147: 		LogicalType result_type;
148: 		// if we only have a single child no reason to create a struct ay
149: 		if (child_types.size() > 1 || depth == 0) {
150: 			result_type = LogicalType(LogicalTypeId::STRUCT, child_types);
151: 			result = make_unique<StructColumnReader>(result_type, s_ele, this_idx, max_define, max_repeat,
152: 			                                         move(child_readers));
153: 		} else {
154: 			// if we have a struct with only a single type, pull up
155: 			result_type = child_types[0].second;
156: 			result = move(child_readers[0]);
157: 		}
158: 		if (s_ele.repetition_type == FieldRepetitionType::REPEATED) {
159: 			result_type = LogicalType(LogicalTypeId::LIST, {make_pair("", result_type)});
160: 			return make_unique<ListColumnReader>(result_type, s_ele, this_idx, max_define, max_repeat, move(result));
161: 		}
162: 		return result;
163: 	} else { // leaf node
164: 		// TODO check return value of derive type or should we only do this on read()
165: 		return ColumnReader::CreateReader(DeriveLogicalType(s_ele), s_ele, next_file_idx++, max_define, max_repeat);
166: 	}
167: }
168: 
169: static unique_ptr<ColumnReader> CreateReader(const FileMetaData *file_meta_data) {
170: 	idx_t next_schema_idx = 0;
171: 	idx_t next_file_idx = 0;
172: 
173: 	auto ret = CreateReaderRecursive(file_meta_data, 0, 0, 0, next_schema_idx, next_file_idx);
174: 	D_ASSERT(next_schema_idx == file_meta_data->schema.size() - 1);
175: 	D_ASSERT(file_meta_data->row_groups.empty() || next_file_idx == file_meta_data->row_groups[0].columns.size());
176: 	return ret;
177: }
178: 
179: ParquetReader::ParquetReader(ClientContext &context, string file_name_p, vector<LogicalType> expected_types,
180:                              const string &initial_filename)
181:     : file_name(move(file_name_p)), context(context) {
182: 	auto &fs = FileSystem::GetFileSystem(context);
183: 
184: 	// todo move this gunk to separate function so this is cleaned up a bit
185: 	auto handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ);
186: 
187: 	ResizeableBuffer buf;
188: 	buf.resize(8);
189: 	memset(buf.ptr, '\0', 8);
190: 	// this check at the beginning is a pretty unneccessary iop
191: 	/*
192: 	#ifdef DEBUG // this is a pretty unnecessary io op
193: 	    // check for magic bytes at start of file
194: 	    fs.Read(*handle, buf.ptr, 4);
195: 	    if (strncmp(buf.ptr, "PAR1", 4) != 0) {
196: 	        throw FormatException("Missing magic bytes in front of Parquet file");
197: 	    }
198: 	#endif
199: 	*/
200: 	// check for magic bytes at end of file
201: 	auto file_size_signed = fs.GetFileSize(*handle);
202: 	if (file_size_signed < 12) {
203: 		throw FormatException("File too small to be a Parquet file");
204: 	}
205: 	auto file_size = (uint64_t)file_size_signed;
206: 	fs.Read(*handle, buf.ptr, 8, file_size - 8);
207: 	if (strncmp(buf.ptr + 4, "PAR1", 4) != 0) {
208: 		throw FormatException("No magic bytes found at end of file");
209: 	}
210: 	// read four-byte footer length from just before the end magic bytes
211: 	auto footer_len = *(uint32_t *)buf.ptr;
212: 	if (footer_len <= 0) {
213: 		throw FormatException("Footer length can't be 0");
214: 	}
215: 	if (file_size < 12 + footer_len) {
216: 		throw FormatException("Footer length %d is too big for the file of size %d", footer_len, file_size);
217: 	}
218: 
219: 	auto last_modify_time = fs.GetLastModifiedTime(*handle);
220: 	// centrally create thrift transport/protocol to reduce allocation
221: 	// TODO this is duplicated in Initialize()
222: 	shared_ptr<ThriftFileTransport> trans(new ThriftFileTransport(move(handle)));
223: 	auto thrift_file_proto = make_unique<apache::thrift::protocol::TCompactProtocolT<ThriftFileTransport>>(trans);
224: 
225: 	// If object cached is disabled
226: 	// or if this file has cached metadata
227: 	// or if the cached version already expired
228: 	auto metadata_pos = file_size - (footer_len + 8);
229: 	if (!ObjectCache::ObjectCacheEnabled(context)) {
230: 		metadata = LoadMetaData(*thrift_file_proto, metadata_pos);
231: 	} else {
232: 		metadata =
233: 		    std::dynamic_pointer_cast<ParquetFileMetadataCache>(ObjectCache::GetObjectCache(context).Get(file_name));
234: 		if (!metadata || (last_modify_time + 10 >= metadata->read_time)) {
235: 			metadata = LoadMetaData(*thrift_file_proto, metadata_pos);
236: 			ObjectCache::GetObjectCache(context).Put(file_name, metadata);
237: 		}
238: 	}
239: 
240: 	auto file_meta_data = GetFileMetadata();
241: 
242: 	if (file_meta_data->__isset.encryption_algorithm) {
243: 		throw FormatException("Encrypted Parquet files are not supported");
244: 	}
245: 	// check if we like this schema
246: 	if (file_meta_data->schema.size() < 2) {
247: 		throw FormatException("Need at least one non-root column in the file");
248: 	}
249: 
250: 	bool has_expected_types = !expected_types.empty();
251: 	auto root_reader = CreateReader(file_meta_data);
252: 
253: 	auto root_type = root_reader->Type();
254: 	D_ASSERT(root_type.id() == LogicalTypeId::STRUCT);
255: 	idx_t col_idx = 0;
256: 	for (auto &type_pair : root_type.child_types()) {
257: 		if (has_expected_types && expected_types[col_idx] != type_pair.second) {
258: 			if (initial_filename.empty()) {
259: 				throw FormatException("column \"%d\" in parquet file is of type %s, could not auto cast to "
260: 				                      "expected type %s for this column",
261: 				                      col_idx, type_pair.second, expected_types[col_idx].ToString());
262: 			} else {
263: 				throw FormatException("schema mismatch in Parquet glob: column \"%d\" in parquet file is of type "
264: 				                      "%s, but in the original file \"%s\" this column is of type \"%s\"",
265: 				                      col_idx, type_pair.second, initial_filename, expected_types[col_idx].ToString());
266: 			}
267: 		} else {
268: 			names.push_back(type_pair.first);
269: 			return_types.push_back(type_pair.second);
270: 		}
271: 		col_idx++;
272: 	}
273: }
274: 
275: ParquetReader::~ParquetReader() {
276: }
277: 
278: const FileMetaData *ParquetReader::GetFileMetadata() {
279: 	D_ASSERT(metadata);
280: 	D_ASSERT(metadata->metadata);
281: 	return metadata->metadata.get();
282: }
283: 
284: // TODO also somewhat ugly, perhaps this can be moved to the column reader too
285: unique_ptr<BaseStatistics> ParquetReader::ReadStatistics(LogicalType &type, column_t file_col_idx,
286:                                                          const FileMetaData *file_meta_data) {
287: 	unique_ptr<BaseStatistics> column_stats;
288: 	auto root_reader = CreateReader(file_meta_data);
289: 	auto column_reader = ((StructColumnReader *)root_reader.get())->GetChildReader(file_col_idx);
290: 
291: 	for (auto &row_group : file_meta_data->row_groups) {
292: 		auto chunk_stats = column_reader->Stats(row_group.columns);
293: 		if (!column_stats) {
294: 			column_stats = move(chunk_stats);
295: 		} else {
296: 			column_stats->Merge(*chunk_stats);
297: 		}
298: 	}
299: 	return column_stats;
300: }
301: 
302: const RowGroup &ParquetReader::GetGroup(ParquetReaderScanState &state) {
303: 	auto file_meta_data = GetFileMetadata();
304: 	D_ASSERT(state.current_group >= 0 && (idx_t)state.current_group < state.group_idx_list.size());
305: 	D_ASSERT(state.group_idx_list[state.current_group] >= 0 &&
306: 	         state.group_idx_list[state.current_group] < file_meta_data->row_groups.size());
307: 	return file_meta_data->row_groups[state.group_idx_list[state.current_group]];
308: }
309: 
310: void ParquetReader::PrepareRowGroupBuffer(ParquetReaderScanState &state, idx_t file_col_idx) {
311: 	auto &group = GetGroup(state);
312: 
313: 	auto column_reader = ((StructColumnReader *)state.root_reader.get())->GetChildReader(file_col_idx);
314: 
315: 	// TODO move this to columnreader too
316: 	if (state.filters) {
317: 		auto stats = column_reader->Stats(group.columns);
318: 		auto filter_entry = state.filters->filters.find(file_col_idx);
319: 		if (stats && filter_entry != state.filters->filters.end()) {
320: 			bool skip_chunk = false;
321: 			switch (column_reader->Type().id()) {
322: 			case LogicalTypeId::UTINYINT:
323: 			case LogicalTypeId::USMALLINT:
324: 			case LogicalTypeId::UINTEGER:
325: 			case LogicalTypeId::UBIGINT:
326: 			case LogicalTypeId::INTEGER:
327: 			case LogicalTypeId::BIGINT:
328: 			case LogicalTypeId::FLOAT:
329: 			case LogicalTypeId::TIMESTAMP:
330: 			case LogicalTypeId::DOUBLE: {
331: 				auto num_stats = (NumericStatistics &)*stats;
332: 				for (auto &filter : filter_entry->second) {
333: 					skip_chunk = !num_stats.CheckZonemap(filter.comparison_type, filter.constant);
334: 					if (skip_chunk) {
335: 						break;
336: 					}
337: 				}
338: 				break;
339: 			}
340: 			case LogicalTypeId::BLOB:
341: 			case LogicalTypeId::VARCHAR: {
342: 				auto str_stats = (StringStatistics &)*stats;
343: 				for (auto &filter : filter_entry->second) {
344: 					skip_chunk = !str_stats.CheckZonemap(filter.comparison_type, filter.constant.str_value);
345: 					if (skip_chunk) {
346: 						break;
347: 					}
348: 				}
349: 				break;
350: 			}
351: 			default:
352: 				break;
353: 			}
354: 			if (skip_chunk) {
355: 				state.group_offset = group.num_rows;
356: 				return;
357: 				// this effectively will skip this chunk
358: 			}
359: 		}
360: 	}
361: 
362: 	state.root_reader->IntializeRead(group.columns, *state.thrift_file_proto);
363: }
364: 
365: idx_t ParquetReader::NumRows() {
366: 	return GetFileMetadata()->num_rows;
367: }
368: 
369: idx_t ParquetReader::NumRowGroups() {
370: 	return GetFileMetadata()->row_groups.size();
371: }
372: 
373: void ParquetReader::Initialize(ParquetReaderScanState &state, vector<column_t> column_ids, vector<idx_t> groups_to_read,
374:                                TableFilterSet *filters) {
375: 	state.current_group = -1;
376: 	state.finished = false;
377: 	state.column_ids = move(column_ids);
378: 	state.group_offset = 0;
379: 	state.group_idx_list = move(groups_to_read);
380: 	state.filters = filters;
381: 	state.sel.Initialize(STANDARD_VECTOR_SIZE);
382: 
383: 	auto handle = FileSystem::GetFileSystem(context).OpenFile(file_name, FileFlags::FILE_FLAGS_READ);
384: 	shared_ptr<ThriftFileTransport> trans(new ThriftFileTransport(move(handle)));
385: 	state.thrift_file_proto = make_unique<apache::thrift::protocol::TCompactProtocolT<ThriftFileTransport>>(trans);
386: 	state.root_reader = CreateReader(GetFileMetadata());
387: 
388: 	state.define_buf.resize(STANDARD_VECTOR_SIZE);
389: 	state.repeat_buf.resize(STANDARD_VECTOR_SIZE);
390: }
391: 
392: template <class T, class OP>
393: void TemplatedFilterOperation(Vector &v, T constant, parquet_filter_t &filter_mask, idx_t count) {
394: 	D_ASSERT(v.GetVectorType() == VectorType::FLAT_VECTOR); // we just created the damn thing it better be
395: 
396: 	auto v_ptr = FlatVector::GetData<T>(v);
397: 	auto &mask = FlatVector::Validity(v);
398: 
399: 	if (!mask.AllValid()) {
400: 		for (idx_t i = 0; i < count; i++) {
401: 			filter_mask[i] = filter_mask[i] && mask.RowIsValid(i) && OP::Operation(v_ptr[i], constant);
402: 		}
403: 	} else {
404: 		for (idx_t i = 0; i < count; i++) {
405: 			filter_mask[i] = filter_mask[i] && OP::Operation(v_ptr[i], constant);
406: 		}
407: 	}
408: }
409: 
410: template <class OP>
411: static void FilterOperationSwitch(Vector &v, Value &constant, parquet_filter_t &filter_mask, idx_t count) {
412: 	if (filter_mask.none() || count == 0) {
413: 		return;
414: 	}
415: 	switch (v.GetType().id()) {
416: 	case LogicalTypeId::BOOLEAN:
417: 		TemplatedFilterOperation<bool, OP>(v, constant.value_.boolean, filter_mask, count);
418: 		break;
419: 
420: 	case LogicalTypeId::UTINYINT:
421: 		TemplatedFilterOperation<uint8_t, OP>(v, constant.value_.utinyint, filter_mask, count);
422: 		break;
423: 
424: 	case LogicalTypeId::USMALLINT:
425: 		TemplatedFilterOperation<uint16_t, OP>(v, constant.value_.usmallint, filter_mask, count);
426: 		break;
427: 
428: 	case LogicalTypeId::UINTEGER:
429: 		TemplatedFilterOperation<uint32_t, OP>(v, constant.value_.uinteger, filter_mask, count);
430: 		break;
431: 
432: 	case LogicalTypeId::UBIGINT:
433: 		TemplatedFilterOperation<uint64_t, OP>(v, constant.value_.ubigint, filter_mask, count);
434: 		break;
435: 
436: 	case LogicalTypeId::INTEGER:
437: 		TemplatedFilterOperation<int32_t, OP>(v, constant.value_.integer, filter_mask, count);
438: 		break;
439: 
440: 	case LogicalTypeId::BIGINT:
441: 		TemplatedFilterOperation<int64_t, OP>(v, constant.value_.bigint, filter_mask, count);
442: 		break;
443: 
444: 	case LogicalTypeId::FLOAT:
445: 		TemplatedFilterOperation<float, OP>(v, constant.value_.float_, filter_mask, count);
446: 		break;
447: 
448: 	case LogicalTypeId::DOUBLE:
449: 		TemplatedFilterOperation<double, OP>(v, constant.value_.double_, filter_mask, count);
450: 		break;
451: 
452: 	case LogicalTypeId::TIMESTAMP:
453: 		TemplatedFilterOperation<timestamp_t, OP>(v, constant.value_.bigint, filter_mask, count);
454: 		break;
455: 
456: 	case LogicalTypeId::BLOB:
457: 	case LogicalTypeId::VARCHAR:
458: 		TemplatedFilterOperation<string_t, OP>(v, string_t(constant.str_value), filter_mask, count);
459: 		break;
460: 
461: 	default:
462: 		throw NotImplementedException("Unsupported type for filter %s", v.ToString());
463: 	}
464: }
465: 
466: void ParquetReader::Scan(ParquetReaderScanState &state, DataChunk &result) {
467: 	while (ScanInternal(state, result)) {
468: 		if (result.size() > 0) {
469: 			break;
470: 		}
471: 		result.Reset();
472: 	}
473: }
474: 
475: bool ParquetReader::ScanInternal(ParquetReaderScanState &state, DataChunk &result) {
476: 	if (state.finished) {
477: 		return false;
478: 	}
479: 
480: 	// see if we have to switch to the next row group in the parquet file
481: 	if (state.current_group < 0 || (int64_t)state.group_offset >= GetGroup(state).num_rows) {
482: 		state.current_group++;
483: 		state.group_offset = 0;
484: 
485: 		if ((idx_t)state.current_group == state.group_idx_list.size()) {
486: 			state.finished = true;
487: 			return false;
488: 		}
489: 
490: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
491: 			auto file_col_idx = state.column_ids[out_col_idx];
492: 
493: 			// this is a special case where we are not interested in the actual contents of the file
494: 			if (file_col_idx == COLUMN_IDENTIFIER_ROW_ID) {
495: 				continue;
496: 			}
497: 
498: 			PrepareRowGroupBuffer(state, file_col_idx);
499: 		}
500: 		return true;
501: 	}
502: 
503: 	auto this_output_chunk_rows = MinValue<idx_t>(STANDARD_VECTOR_SIZE, GetGroup(state).num_rows - state.group_offset);
504: 	result.SetCardinality(this_output_chunk_rows);
505: 
506: 	if (this_output_chunk_rows == 0) {
507: 		state.finished = true;
508: 		return false; // end of last group, we are done
509: 	}
510: 
511: 	// we evaluate simple table filters directly in this scan so we can skip decoding column data that's never going to
512: 	// be relevant
513: 	parquet_filter_t filter_mask;
514: 	filter_mask.set();
515: 
516: 	state.define_buf.zero();
517: 	state.repeat_buf.zero();
518: 
519: 	auto define_ptr = (uint8_t *)state.define_buf.ptr;
520: 	auto repeat_ptr = (uint8_t *)state.repeat_buf.ptr;
521: 
522: 	auto root_reader = ((StructColumnReader *)state.root_reader.get());
523: 
524: 	if (state.filters) {
525: 		vector<bool> need_to_read(result.ColumnCount(), true);
526: 
527: 		// first load the columns that are used in filters
528: 		for (auto &filter_col : state.filters->filters) {
529: 			auto file_col_idx = state.column_ids[filter_col.first];
530: 
531: 			if (filter_mask.none()) { // if no rows are left we can stop checking filters
532: 				break;
533: 			}
534: 
535: 			root_reader->GetChildReader(file_col_idx)
536: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[filter_col.first]);
537: 
538: 			need_to_read[filter_col.first] = false;
539: 
540: 			for (auto &filter : filter_col.second) {
541: 				switch (filter.comparison_type) {
542: 				case ExpressionType::COMPARE_EQUAL:
543: 					FilterOperationSwitch<Equals>(result.data[filter_col.first], filter.constant, filter_mask,
544: 					                              this_output_chunk_rows);
545: 					break;
546: 				case ExpressionType::COMPARE_LESSTHAN:
547: 					FilterOperationSwitch<LessThan>(result.data[filter_col.first], filter.constant, filter_mask,
548: 					                                this_output_chunk_rows);
549: 					break;
550: 				case ExpressionType::COMPARE_LESSTHANOREQUALTO:
551: 					FilterOperationSwitch<LessThanEquals>(result.data[filter_col.first], filter.constant, filter_mask,
552: 					                                      this_output_chunk_rows);
553: 					break;
554: 				case ExpressionType::COMPARE_GREATERTHAN:
555: 					FilterOperationSwitch<GreaterThan>(result.data[filter_col.first], filter.constant, filter_mask,
556: 					                                   this_output_chunk_rows);
557: 					break;
558: 				case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
559: 					FilterOperationSwitch<GreaterThanEquals>(result.data[filter_col.first], filter.constant,
560: 					                                         filter_mask, this_output_chunk_rows);
561: 					break;
562: 				default:
563: 					D_ASSERT(0);
564: 				}
565: 			}
566: 		}
567: 
568: 		// we still may have to read some cols
569: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
570: 			if (!need_to_read[out_col_idx]) {
571: 				continue;
572: 			}
573: 			auto file_col_idx = state.column_ids[out_col_idx];
574: 
575: 			if (filter_mask.none()) {
576: 				root_reader->GetChildReader(file_col_idx)->Skip(result.size());
577: 				continue;
578: 			}
579: 			// TODO handle ROWID here, too
580: 			root_reader->GetChildReader(file_col_idx)
581: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[out_col_idx]);
582: 		}
583: 
584: 		idx_t sel_size = 0;
585: 		for (idx_t i = 0; i < this_output_chunk_rows; i++) {
586: 			if (filter_mask[i]) {
587: 				state.sel.set_index(sel_size++, i);
588: 			}
589: 		}
590: 
591: 		result.Slice(state.sel, sel_size);
592: 		result.Verify();
593: 
594: 	} else { // #nofilter, just fricking load the data
595: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
596: 			auto file_col_idx = state.column_ids[out_col_idx];
597: 
598: 			if (file_col_idx == COLUMN_IDENTIFIER_ROW_ID) {
599: 				Value constant_42 = Value::BIGINT(42);
600: 				result.data[out_col_idx].Reference(constant_42);
601: 				continue;
602: 			}
603: 
604: 			root_reader->GetChildReader(file_col_idx)
605: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[out_col_idx]);
606: 		}
607: 	}
608: 
609: 	state.group_offset += this_output_chunk_rows;
610: 	return true;
611: }
612: 
613: } // namespace duckdb
[end of extension/parquet/parquet_reader.cpp]
[start of extension/parquet/parquet_statistics.cpp]
1: #include "parquet_statistics.hpp"
2: #include "parquet_timestamp.hpp"
3: 
4: #include "duckdb/common/types/value.hpp"
5: #include "duckdb/storage/statistics/string_statistics.hpp"
6: #include "duckdb/storage/statistics/numeric_statistics.hpp"
7: 
8: namespace duckdb {
9: 
10: using parquet::format::ConvertedType;
11: using parquet::format::Type;
12: 
13: template <Value (*FUNC)(const_data_ptr_t input)>
14: static unique_ptr<BaseStatistics> TemplatedGetNumericStats(const LogicalType &type,
15:                                                            const parquet::format::Statistics &parquet_stats) {
16: 	auto stats = make_unique<NumericStatistics>(type);
17: 
18: 	// for reasons unknown to science, Parquet defines *both* `min` and `min_value` as well as `max` and
19: 	// `max_value`. All are optional. such elegance.
20: 	if (parquet_stats.__isset.min) {
21: 		stats->min = FUNC((const_data_ptr_t)parquet_stats.min.data());
22: 	} else if (parquet_stats.__isset.min_value) {
23: 		stats->min = FUNC((const_data_ptr_t)parquet_stats.min_value.data());
24: 	} else {
25: 		stats->min.is_null = true;
26: 	}
27: 	if (parquet_stats.__isset.max) {
28: 		stats->max = FUNC((const_data_ptr_t)parquet_stats.max.data());
29: 	} else if (parquet_stats.__isset.max_value) {
30: 		stats->max = FUNC((const_data_ptr_t)parquet_stats.max_value.data());
31: 	} else {
32: 		stats->max.is_null = true;
33: 	}
34: 	// GCC 4.x insists on a move() here
35: 	return move(stats);
36: }
37: 
38: template <class T>
39: static Value TransformStatisticsPlain(const_data_ptr_t input) {
40: 	return Value::CreateValue<T>(Load<T>(input));
41: }
42: 
43: static Value TransformStatisticsFloat(const_data_ptr_t input) {
44: 	auto val = Load<float>(input);
45: 	if (!Value::FloatIsValid(val)) {
46: 		return Value(LogicalType::FLOAT);
47: 	}
48: 	return Value::CreateValue<float>(val);
49: }
50: 
51: static Value TransformStatisticsDouble(const_data_ptr_t input) {
52: 	auto val = Load<double>(input);
53: 	if (!Value::DoubleIsValid(val)) {
54: 		return Value(LogicalType::DOUBLE);
55: 	}
56: 	return Value::CreateValue<double>(val);
57: }
58: 
59: static Value TransformStatisticsTimestampMs(const_data_ptr_t input) {
60: 	return Value::TIMESTAMP(ParquetTimestampMsToTimestamp(Load<int64_t>(input)));
61: }
62: 
63: static Value TransformStatisticsTimestampMicros(const_data_ptr_t input) {
64: 	return Value::TIMESTAMP(ParquetTimestampMicrosToTimestamp(Load<int64_t>(input)));
65: }
66: 
67: static Value TransformStatisticsTimestampImpala(const_data_ptr_t input) {
68: 	return Value::TIMESTAMP(ImpalaTimestampToTimestamp(Load<Int96>(input)));
69: }
70: 
71: unique_ptr<BaseStatistics> ParquetTransformColumnStatistics(const SchemaElement &s_ele, const LogicalType &type,
72:                                                             const ColumnChunk &column_chunk) {
73: 	if (!column_chunk.__isset.meta_data || !column_chunk.meta_data.__isset.statistics) {
74: 		// no stats present for row group
75: 		return nullptr;
76: 	}
77: 	auto &parquet_stats = column_chunk.meta_data.statistics;
78: 	unique_ptr<BaseStatistics> row_group_stats;
79: 
80: 	switch (type.id()) {
81: 
82: 	case LogicalTypeId::UTINYINT:
83: 		row_group_stats = TemplatedGetNumericStats<TransformStatisticsPlain<uint8_t>>(type, parquet_stats);
84: 		break;
85: 
86: 	case LogicalTypeId::USMALLINT:
87: 		row_group_stats = TemplatedGetNumericStats<TransformStatisticsPlain<uint16_t>>(type, parquet_stats);
88: 		break;
89: 
90: 	case LogicalTypeId::UINTEGER:
91: 		row_group_stats = TemplatedGetNumericStats<TransformStatisticsPlain<uint32_t>>(type, parquet_stats);
92: 		break;
93: 
94: 	case LogicalTypeId::UBIGINT:
95: 		row_group_stats = TemplatedGetNumericStats<TransformStatisticsPlain<uint64_t>>(type, parquet_stats);
96: 		break;
97: 	case LogicalTypeId::INTEGER:
98: 		row_group_stats = TemplatedGetNumericStats<TransformStatisticsPlain<int32_t>>(type, parquet_stats);
99: 		break;
100: 
101: 	case LogicalTypeId::BIGINT:
102: 		row_group_stats = TemplatedGetNumericStats<TransformStatisticsPlain<int64_t>>(type, parquet_stats);
103: 		break;
104: 
105: 	case LogicalTypeId::FLOAT:
106: 		row_group_stats = TemplatedGetNumericStats<TransformStatisticsFloat>(type, parquet_stats);
107: 		break;
108: 
109: 	case LogicalTypeId::DOUBLE:
110: 		row_group_stats = TemplatedGetNumericStats<TransformStatisticsDouble>(type, parquet_stats);
111: 		break;
112: 
113: 		// here we go, our favorite type
114: 	case LogicalTypeId::TIMESTAMP: {
115: 		switch (s_ele.type) {
116: 		case Type::INT64:
117: 			// arrow timestamp
118: 			switch (s_ele.converted_type) {
119: 			case ConvertedType::TIMESTAMP_MICROS:
120: 				row_group_stats = TemplatedGetNumericStats<TransformStatisticsTimestampMicros>(type, parquet_stats);
121: 				break;
122: 			case ConvertedType::TIMESTAMP_MILLIS:
123: 				row_group_stats = TemplatedGetNumericStats<TransformStatisticsTimestampMs>(type, parquet_stats);
124: 				break;
125: 			default:
126: 				return nullptr;
127: 			}
128: 			break;
129: 		case Type::INT96:
130: 			// impala timestamp
131: 			row_group_stats = TemplatedGetNumericStats<TransformStatisticsTimestampImpala>(type, parquet_stats);
132: 			break;
133: 		default:
134: 			return nullptr;
135: 		}
136: 		break;
137: 	}
138: 	case LogicalTypeId::VARCHAR: {
139: 		auto string_stats = make_unique<StringStatistics>(type);
140: 		if (parquet_stats.__isset.min) {
141: 			memcpy(string_stats->min, (data_ptr_t)parquet_stats.min.data(),
142: 			       MinValue<idx_t>(parquet_stats.min.size(), StringStatistics::MAX_STRING_MINMAX_SIZE));
143: 		} else if (parquet_stats.__isset.min_value) {
144: 			memcpy(string_stats->min, (data_ptr_t)parquet_stats.min_value.data(),
145: 			       MinValue<idx_t>(parquet_stats.min_value.size(), StringStatistics::MAX_STRING_MINMAX_SIZE));
146: 		} else {
147: 			return nullptr;
148: 		}
149: 		if (parquet_stats.__isset.max) {
150: 			memcpy(string_stats->max, (data_ptr_t)parquet_stats.max.data(),
151: 			       MinValue<idx_t>(parquet_stats.max.size(), StringStatistics::MAX_STRING_MINMAX_SIZE));
152: 		} else if (parquet_stats.__isset.max_value) {
153: 			memcpy(string_stats->max, (data_ptr_t)parquet_stats.max_value.data(),
154: 			       MinValue<idx_t>(parquet_stats.max_value.size(), StringStatistics::MAX_STRING_MINMAX_SIZE));
155: 		} else {
156: 			return nullptr;
157: 		}
158: 
159: 		string_stats->has_unicode = true; // we dont know better
160: 		row_group_stats = move(string_stats);
161: 		break;
162: 	}
163: 	default:
164: 		// no stats for you
165: 		break;
166: 	} // end of type switch
167: 
168: 	// null count is generic
169: 	if (row_group_stats) {
170: 		if (parquet_stats.__isset.null_count) {
171: 			row_group_stats->has_null = parquet_stats.null_count != 0;
172: 		} else {
173: 			row_group_stats->has_null = true;
174: 		}
175: 	} else {
176: 		// if stats are missing from any row group we know squat
177: 		return nullptr;
178: 	}
179: 
180: 	return row_group_stats;
181: }
182: 
183: } // namespace duckdb
[end of extension/parquet/parquet_statistics.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: