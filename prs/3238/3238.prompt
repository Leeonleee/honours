You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Use column names for schema validation
#### What happens?
When querying parquet files on the filesystem directly, the schema validation fails if columns are out of order. Would it be possible for the expected types for each column to operate on the names of the columns rather than their integer index?

#### To Reproduce
```
import pandas as pd
import duckdb
df = pd.DataFrame({"A": [1,2,3], "B": [1.0, 2.0, 3.0]})
df.to_parquet("1.parquet")
df[reversed(df.columns)].to_parquet("2.parquet")
duckdb.query("select * from '*.parquet'")
```
```
---------------------
-- Expression Tree --
---------------------
Subquery

---------------------
-- Result Columns  --
---------------------
- A (BIGINT)
- B (DOUBLE)

---------------------
-- Result Preview  --
---------------------
Failed to read Parquet file "2.parquet": column "0" in parquet file is of type DOUBLE, could not auto cast to expected type BIGINT for this column
```

#### Environment (please complete the following information):
 - OS: Red Hat Enterprise Linux Server release 7.6 (Maipo)
 - DuckDB Version: 0.3.2
 - DuckDB Client: Python



</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
44: 
45: 
[end of README.md]
[start of extension/parquet/column_reader.cpp]
1: #include "column_reader.hpp"
2: #include "parquet_timestamp.hpp"
3: #include "utf8proc_wrapper.hpp"
4: #include "parquet_reader.hpp"
5: 
6: #include "boolean_column_reader.hpp"
7: #include "callback_column_reader.hpp"
8: #include "parquet_decimal_utils.hpp"
9: #include "list_column_reader.hpp"
10: #include "string_column_reader.hpp"
11: #include "struct_column_reader.hpp"
12: #include "templated_column_reader.hpp"
13: 
14: #include "snappy.h"
15: #include "miniz_wrapper.hpp"
16: #include "zstd.h"
17: #include <iostream>
18: 
19: #include "duckdb.hpp"
20: #ifndef DUCKDB_AMALGAMATION
21: #include "duckdb/common/types/blob.hpp"
22: #include "duckdb/common/types/chunk_collection.hpp"
23: #endif
24: 
25: namespace duckdb {
26: 
27: using duckdb_parquet::format::CompressionCodec;
28: using duckdb_parquet::format::ConvertedType;
29: using duckdb_parquet::format::Encoding;
30: using duckdb_parquet::format::PageType;
31: using duckdb_parquet::format::Type;
32: 
33: const uint32_t ParquetDecodeUtils::BITPACK_MASKS[] = {
34:     0,       1,       3,        7,        15,       31,        63,        127,       255,        511,       1023,
35:     2047,    4095,    8191,     16383,    32767,    65535,     131071,    262143,    524287,     1048575,   2097151,
36:     4194303, 8388607, 16777215, 33554431, 67108863, 134217727, 268435455, 536870911, 1073741823, 2147483647};
37: 
38: const uint8_t ParquetDecodeUtils::BITPACK_DLEN = 8;
39: 
40: ColumnReader::ColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,
41:                            idx_t max_define_p, idx_t max_repeat_p)
42:     : schema(schema_p), file_idx(file_idx_p), max_define(max_define_p), max_repeat(max_repeat_p), reader(reader),
43:       type(move(type_p)), page_rows_available(0) {
44: 
45: 	// dummies for Skip()
46: 	none_filter.none();
47: 	dummy_define.resize(reader.allocator, STANDARD_VECTOR_SIZE);
48: 	dummy_repeat.resize(reader.allocator, STANDARD_VECTOR_SIZE);
49: }
50: 
51: ColumnReader::~ColumnReader() {
52: }
53: 
54: const LogicalType &ColumnReader::Type() {
55: 	return type;
56: }
57: 
58: const SchemaElement &ColumnReader::Schema() {
59: 	return schema;
60: }
61: 
62: idx_t ColumnReader::GroupRowsAvailable() {
63: 	return group_rows_available;
64: }
65: 
66: unique_ptr<BaseStatistics> ColumnReader::Stats(const std::vector<ColumnChunk> &columns) {
67: 	if (Type().id() == LogicalTypeId::LIST || Type().id() == LogicalTypeId::STRUCT ||
68: 	    Type().id() == LogicalTypeId::MAP) {
69: 		return nullptr;
70: 	}
71: 	return ParquetStatisticsUtils::TransformColumnStatistics(Schema(), Type(), columns[file_idx]);
72: }
73: 
74: void ColumnReader::Plain(shared_ptr<ByteBuffer> plain_data, uint8_t *defines, idx_t num_values, // NOLINT
75:                          parquet_filter_t &filter, idx_t result_offset, Vector &result) {
76: 	throw NotImplementedException("Plain");
77: }
78: 
79: void ColumnReader::Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) { // NOLINT
80: 	throw NotImplementedException("Dictionary");
81: }
82: 
83: void ColumnReader::Offsets(uint32_t *offsets, uint8_t *defines, idx_t num_values, parquet_filter_t &filter,
84:                            idx_t result_offset, Vector &result) {
85: 	throw NotImplementedException("Offsets");
86: }
87: 
88: void ColumnReader::DictReference(Vector &result) {
89: }
90: void ColumnReader::PlainReference(shared_ptr<ByteBuffer>, Vector &result) { // NOLINT
91: }
92: 
93: void ColumnReader::InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) {
94: 	D_ASSERT(file_idx < columns.size());
95: 	chunk = &columns[file_idx];
96: 	protocol = &protocol_p;
97: 	D_ASSERT(chunk);
98: 	D_ASSERT(chunk->__isset.meta_data);
99: 
100: 	if (chunk->__isset.file_path) {
101: 		throw std::runtime_error("Only inlined data files are supported (no references)");
102: 	}
103: 
104: 	// ugh. sometimes there is an extra offset for the dict. sometimes it's wrong.
105: 	chunk_read_offset = chunk->meta_data.data_page_offset;
106: 	if (chunk->meta_data.__isset.dictionary_page_offset && chunk->meta_data.dictionary_page_offset >= 4) {
107: 		// this assumes the data pages follow the dict pages directly.
108: 		chunk_read_offset = chunk->meta_data.dictionary_page_offset;
109: 	}
110: 	group_rows_available = chunk->meta_data.num_values;
111: }
112: 
113: void ColumnReader::PrepareRead(parquet_filter_t &filter) {
114: 	dict_decoder.reset();
115: 	defined_decoder.reset();
116: 	block.reset();
117: 
118: 	PageHeader page_hdr;
119: 	page_hdr.read(protocol);
120: 
121: 	//		page_hdr.printTo(std::cout);
122: 	//		std::cout << '\n';
123: 
124: 	switch (page_hdr.type) {
125: 	case PageType::DATA_PAGE_V2:
126: 		PreparePageV2(page_hdr);
127: 		PrepareDataPage(page_hdr);
128: 		break;
129: 	case PageType::DATA_PAGE:
130: 		PreparePage(page_hdr.compressed_page_size, page_hdr.uncompressed_page_size);
131: 		PrepareDataPage(page_hdr);
132: 		break;
133: 	case PageType::DICTIONARY_PAGE:
134: 		PreparePage(page_hdr.compressed_page_size, page_hdr.uncompressed_page_size);
135: 		Dictionary(move(block), page_hdr.dictionary_page_header.num_values);
136: 		break;
137: 	default:
138: 		break; // ignore INDEX page type and any other custom extensions
139: 	}
140: }
141: 
142: void ColumnReader::PreparePageV2(PageHeader &page_hdr) {
143: 	// FIXME this is copied from the other prepare, merge the decomp part
144: 
145: 	D_ASSERT(page_hdr.type == PageType::DATA_PAGE_V2);
146: 
147: 	auto &trans = (ThriftFileTransport &)*protocol->getTransport();
148: 
149: 	block = make_shared<ResizeableBuffer>(reader.allocator, page_hdr.uncompressed_page_size + 1);
150: 	// copy repeats & defines as-is because FOR SOME REASON they are uncompressed
151: 	auto uncompressed_bytes = page_hdr.data_page_header_v2.repetition_levels_byte_length +
152: 	                          page_hdr.data_page_header_v2.definition_levels_byte_length;
153: 	auto possibly_compressed_bytes = page_hdr.compressed_page_size - uncompressed_bytes;
154: 	trans.read((uint8_t *)block->ptr, uncompressed_bytes);
155: 
156: 	switch (chunk->meta_data.codec) {
157: 	case CompressionCodec::UNCOMPRESSED:
158: 		trans.read(((uint8_t *)block->ptr) + uncompressed_bytes, possibly_compressed_bytes);
159: 		break;
160: 
161: 	case CompressionCodec::SNAPPY: {
162: 		// TODO move allocation outta here
163: 		ResizeableBuffer compressed_bytes_buffer(reader.allocator, possibly_compressed_bytes);
164: 		trans.read((uint8_t *)compressed_bytes_buffer.ptr, possibly_compressed_bytes);
165: 
166: 		auto res = duckdb_snappy::RawUncompress((const char *)compressed_bytes_buffer.ptr, possibly_compressed_bytes,
167: 		                                        ((char *)block->ptr) + uncompressed_bytes);
168: 		if (!res) {
169: 			throw std::runtime_error("Decompression failure");
170: 		}
171: 		break;
172: 	}
173: 
174: 	default: {
175: 		std::stringstream codec_name;
176: 		codec_name << chunk->meta_data.codec;
177: 		throw std::runtime_error("Unsupported compression codec \"" + codec_name.str() +
178: 		                         "\". Supported options are uncompressed, gzip or snappy");
179: 		break;
180: 	}
181: 	}
182: }
183: 
184: void ColumnReader::PreparePage(idx_t compressed_page_size, idx_t uncompressed_page_size) {
185: 	auto &trans = (ThriftFileTransport &)*protocol->getTransport();
186: 
187: 	block = make_shared<ResizeableBuffer>(reader.allocator, compressed_page_size + 1);
188: 	trans.read((uint8_t *)block->ptr, compressed_page_size);
189: 
190: 	// TODO this allocation should probably be avoided
191: 	shared_ptr<ResizeableBuffer> unpacked_block;
192: 	if (chunk->meta_data.codec != CompressionCodec::UNCOMPRESSED) {
193: 		unpacked_block = make_shared<ResizeableBuffer>(reader.allocator, uncompressed_page_size + 1);
194: 	}
195: 
196: 	switch (chunk->meta_data.codec) {
197: 	case CompressionCodec::UNCOMPRESSED:
198: 		break;
199: 	case CompressionCodec::GZIP: {
200: 		MiniZStream s;
201: 
202: 		s.Decompress((const char *)block->ptr, compressed_page_size, (char *)unpacked_block->ptr,
203: 		             uncompressed_page_size);
204: 		block = move(unpacked_block);
205: 
206: 		break;
207: 	}
208: 	case CompressionCodec::SNAPPY: {
209: 		auto res =
210: 		    duckdb_snappy::RawUncompress((const char *)block->ptr, compressed_page_size, (char *)unpacked_block->ptr);
211: 		if (!res) {
212: 			throw std::runtime_error("Decompression failure");
213: 		}
214: 		block = move(unpacked_block);
215: 		break;
216: 	}
217: 	case CompressionCodec::ZSTD: {
218: 		auto res = duckdb_zstd::ZSTD_decompress((char *)unpacked_block->ptr, uncompressed_page_size,
219: 		                                        (const char *)block->ptr, compressed_page_size);
220: 		if (duckdb_zstd::ZSTD_isError(res) || res != (size_t)uncompressed_page_size) {
221: 			throw std::runtime_error("ZSTD Decompression failure");
222: 		}
223: 		block = move(unpacked_block);
224: 		break;
225: 	}
226: 
227: 	default: {
228: 		std::stringstream codec_name;
229: 		codec_name << chunk->meta_data.codec;
230: 		throw std::runtime_error("Unsupported compression codec \"" + codec_name.str() +
231: 		                         "\". Supported options are uncompressed, gzip or snappy");
232: 		break;
233: 	}
234: 	}
235: }
236: 
237: void ColumnReader::PrepareDataPage(PageHeader &page_hdr) {
238: 	if (page_hdr.type == PageType::DATA_PAGE && !page_hdr.__isset.data_page_header) {
239: 		throw std::runtime_error("Missing data page header from data page");
240: 	}
241: 	if (page_hdr.type == PageType::DATA_PAGE_V2 && !page_hdr.__isset.data_page_header_v2) {
242: 		throw std::runtime_error("Missing data page header from data page v2");
243: 	}
244: 
245: 	page_rows_available = page_hdr.type == PageType::DATA_PAGE ? page_hdr.data_page_header.num_values
246: 	                                                           : page_hdr.data_page_header_v2.num_values;
247: 	auto page_encoding = page_hdr.type == PageType::DATA_PAGE ? page_hdr.data_page_header.encoding
248: 	                                                          : page_hdr.data_page_header_v2.encoding;
249: 
250: 	if (HasRepeats()) {
251: 		uint32_t rep_length = page_hdr.type == PageType::DATA_PAGE
252: 		                          ? block->read<uint32_t>()
253: 		                          : page_hdr.data_page_header_v2.repetition_levels_byte_length;
254: 		block->available(rep_length);
255: 		repeated_decoder = make_unique<RleBpDecoder>((const uint8_t *)block->ptr, rep_length,
256: 		                                             RleBpDecoder::ComputeBitWidth(max_repeat));
257: 		block->inc(rep_length);
258: 	}
259: 
260: 	if (HasDefines()) {
261: 		uint32_t def_length = page_hdr.type == PageType::DATA_PAGE
262: 		                          ? block->read<uint32_t>()
263: 		                          : page_hdr.data_page_header_v2.definition_levels_byte_length;
264: 		block->available(def_length);
265: 		defined_decoder = make_unique<RleBpDecoder>((const uint8_t *)block->ptr, def_length,
266: 		                                            RleBpDecoder::ComputeBitWidth(max_define));
267: 		block->inc(def_length);
268: 	}
269: 
270: 	switch (page_encoding) {
271: 	case Encoding::RLE_DICTIONARY:
272: 	case Encoding::PLAIN_DICTIONARY: {
273: 		// where is it otherwise??
274: 		auto dict_width = block->read<uint8_t>();
275: 		// TODO somehow dict_width can be 0 ?
276: 		dict_decoder = make_unique<RleBpDecoder>((const uint8_t *)block->ptr, block->len, dict_width);
277: 		block->inc(block->len);
278: 		break;
279: 	}
280: 	case Encoding::DELTA_BINARY_PACKED: {
281: 		dbp_decoder = make_unique<DbpDecoder>((const uint8_t *)block->ptr, block->len);
282: 		block->inc(block->len);
283: 		break;
284: 	}
285: 		/*
286: 	case Encoding::DELTA_BYTE_ARRAY: {
287: 		dbp_decoder = make_unique<DbpDecoder>((const uint8_t *)block->ptr, block->len);
288: 		auto prefix_buffer = make_shared<ResizeableBuffer>();
289: 		prefix_buffer->resize(reader.allocator, sizeof(uint32_t) * page_hdr.data_page_header_v2.num_rows);
290: 
291: 		auto suffix_buffer = make_shared<ResizeableBuffer>();
292: 		suffix_buffer->resize(reader.allocator, sizeof(uint32_t) * page_hdr.data_page_header_v2.num_rows);
293: 
294: 		dbp_decoder->GetBatch<uint32_t>(prefix_buffer->ptr, page_hdr.data_page_header_v2.num_rows);
295: 		auto buffer_after_prefixes = dbp_decoder->BufferPtr();
296: 
297: 		dbp_decoder = make_unique<DbpDecoder>((const uint8_t *)buffer_after_prefixes.ptr, buffer_after_prefixes.len);
298: 		dbp_decoder->GetBatch<uint32_t>(suffix_buffer->ptr, page_hdr.data_page_header_v2.num_rows);
299: 
300: 		auto string_buffer = dbp_decoder->BufferPtr();
301: 
302: 		for (idx_t i = 0 ; i < page_hdr.data_page_header_v2.num_rows; i++) {
303: 		    auto suffix_length = (uint32_t*) suffix_buffer->ptr;
304: 		    string str( suffix_length[i] + 1, '\0');
305: 		    string_buffer.copy_to((char*) str.data(), suffix_length[i]);
306: 		    printf("%s\n", str.c_str());
307: 		}
308: 		throw std::runtime_error("eek");
309: 
310: 
311: 		// This is also known as incremental encoding or front compression: for each element in a sequence of strings,
312: 		// store the prefix length of the previous entry plus the suffix. This is stored as a sequence of delta-encoded
313: 		// prefix lengths (DELTA_BINARY_PACKED), followed by the suffixes encoded as delta length byte arrays
314: 		// (DELTA_LENGTH_BYTE_ARRAY). DELTA_LENGTH_BYTE_ARRAY: The encoded data would be DeltaEncoding(5, 5, 6, 6)
315: 		// "HelloWorldFoobarABCDEF"
316: 
317: 		// TODO actually do something here
318: 		break;
319: 	}
320: 		 */
321: 	case Encoding::PLAIN:
322: 		// nothing to do here, will be read directly below
323: 		break;
324: 
325: 	default:
326: 		throw std::runtime_error("Unsupported page encoding");
327: 	}
328: }
329: 
330: idx_t ColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
331:                          Vector &result) {
332: 	// we need to reset the location because multiple column readers share the same protocol
333: 	auto &trans = (ThriftFileTransport &)*protocol->getTransport();
334: 	trans.SetLocation(chunk_read_offset);
335: 
336: 	idx_t result_offset = 0;
337: 	auto to_read = num_values;
338: 
339: 	while (to_read > 0) {
340: 		while (page_rows_available == 0) {
341: 			PrepareRead(filter);
342: 		}
343: 
344: 		D_ASSERT(block);
345: 		auto read_now = MinValue<idx_t>(to_read, page_rows_available);
346: 
347: 		D_ASSERT(read_now <= STANDARD_VECTOR_SIZE);
348: 
349: 		if (HasRepeats()) {
350: 			D_ASSERT(repeated_decoder);
351: 			repeated_decoder->GetBatch<uint8_t>((char *)repeat_out + result_offset, read_now);
352: 		}
353: 
354: 		if (HasDefines()) {
355: 			D_ASSERT(defined_decoder);
356: 			defined_decoder->GetBatch<uint8_t>((char *)define_out + result_offset, read_now);
357: 		}
358: 
359: 		idx_t null_count = 0;
360: 
361: 		if ((dict_decoder || dbp_decoder) && HasDefines()) {
362: 			// we need the null count because the dictionary offsets have no entries for nulls
363: 			for (idx_t i = 0; i < read_now; i++) {
364: 				if (define_out[i + result_offset] != max_define) {
365: 					null_count++;
366: 				}
367: 			}
368: 		}
369: 
370: 		if (dict_decoder) {
371: 			offset_buffer.resize(reader.allocator, sizeof(uint32_t) * (read_now - null_count));
372: 			dict_decoder->GetBatch<uint32_t>(offset_buffer.ptr, read_now - null_count);
373: 			DictReference(result);
374: 			Offsets((uint32_t *)offset_buffer.ptr, define_out, read_now, filter, result_offset, result);
375: 		} else if (dbp_decoder) {
376: 			// TODO keep this in the state
377: 			auto read_buf = make_shared<ResizeableBuffer>();
378: 
379: 			switch (type.id()) {
380: 			case LogicalTypeId::INTEGER:
381: 				read_buf->resize(reader.allocator, sizeof(int32_t) * (read_now - null_count));
382: 				dbp_decoder->GetBatch<int32_t>(read_buf->ptr, read_now - null_count);
383: 
384: 				break;
385: 			case LogicalTypeId::BIGINT:
386: 				read_buf->resize(reader.allocator, sizeof(int64_t) * (read_now - null_count));
387: 				dbp_decoder->GetBatch<int64_t>(read_buf->ptr, read_now - null_count);
388: 				break;
389: 
390: 			default:
391: 				throw std::runtime_error("DELTA_BINARY_PACKED should only be INT32 or INT64");
392: 			}
393: 			// Plain() will put NULLs in the right place
394: 			Plain(read_buf, define_out, read_now, filter, result_offset, result);
395: 		} else {
396: 			PlainReference(block, result);
397: 			Plain(block, define_out, read_now, filter, result_offset, result);
398: 		}
399: 
400: 		result_offset += read_now;
401: 		page_rows_available -= read_now;
402: 		to_read -= read_now;
403: 	}
404: 	group_rows_available -= num_values;
405: 	chunk_read_offset = trans.GetLocation();
406: 
407: 	return num_values;
408: }
409: 
410: void ColumnReader::Skip(idx_t num_values) {
411: 	dummy_define.zero();
412: 	dummy_repeat.zero();
413: 
414: 	// TODO this can be optimized, for example we dont actually have to bitunpack offsets
415: 	Vector dummy_result(type, nullptr);
416: 	auto values_read =
417: 	    Read(num_values, none_filter, (uint8_t *)dummy_define.ptr, (uint8_t *)dummy_repeat.ptr, dummy_result);
418: 	if (values_read != num_values) {
419: 		throw std::runtime_error("Row count mismatch when skipping rows");
420: 	}
421: }
422: 
423: //===--------------------------------------------------------------------===//
424: // String Column Reader
425: //===--------------------------------------------------------------------===//
426: StringColumnReader::StringColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,
427:                                        idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p)
428:     : TemplatedColumnReader<string_t, StringParquetValueConversion>(reader, move(type_p), schema_p, schema_idx_p,
429:                                                                     max_define_p, max_repeat_p) {
430: 	fixed_width_string_length = 0;
431: 	if (schema_p.type == Type::FIXED_LEN_BYTE_ARRAY) {
432: 		D_ASSERT(schema_p.__isset.type_length);
433: 		fixed_width_string_length = schema_p.type_length;
434: 	}
435: }
436: 
437: uint32_t StringColumnReader::VerifyString(const char *str_data, uint32_t str_len) {
438: 	if (Type() != LogicalTypeId::VARCHAR) {
439: 		return str_len;
440: 	}
441: 	// verify if a string is actually UTF8, and if there are no null bytes in the middle of the string
442: 	// technically Parquet should guarantee this, but reality is often disappointing
443: 	UnicodeInvalidReason reason;
444: 	size_t pos;
445: 	auto utf_type = Utf8Proc::Analyze(str_data, str_len, &reason, &pos);
446: 	if (utf_type == UnicodeType::INVALID) {
447: 		if (reason == UnicodeInvalidReason::NULL_BYTE) {
448: 			// for null bytes we just truncate the string
449: 			return pos;
450: 		}
451: 		throw InvalidInputException("Invalid string encoding found in Parquet file: value \"" +
452: 		                            Blob::ToString(string_t(str_data, str_len)) + "\" is not valid UTF8!");
453: 	}
454: 	return str_len;
455: }
456: 
457: void StringColumnReader::Dictionary(shared_ptr<ByteBuffer> data, idx_t num_entries) {
458: 	dict = move(data);
459: 	dict_strings = unique_ptr<string_t[]>(new string_t[num_entries]);
460: 	for (idx_t dict_idx = 0; dict_idx < num_entries; dict_idx++) {
461: 		uint32_t str_len = dict->read<uint32_t>();
462: 		dict->available(str_len);
463: 
464: 		auto actual_str_len = VerifyString(dict->ptr, str_len);
465: 		dict_strings[dict_idx] = string_t(dict->ptr, actual_str_len);
466: 		dict->inc(str_len);
467: 	}
468: }
469: 
470: class ParquetStringVectorBuffer : public VectorBuffer {
471: public:
472: 	explicit ParquetStringVectorBuffer(shared_ptr<ByteBuffer> buffer_p)
473: 	    : VectorBuffer(VectorBufferType::OPAQUE_BUFFER), buffer(move(buffer_p)) {
474: 	}
475: 
476: private:
477: 	shared_ptr<ByteBuffer> buffer;
478: };
479: 
480: void StringColumnReader::DictReference(Vector &result) {
481: 	StringVector::AddBuffer(result, make_buffer<ParquetStringVectorBuffer>(dict));
482: }
483: void StringColumnReader::PlainReference(shared_ptr<ByteBuffer> plain_data, Vector &result) {
484: 	StringVector::AddBuffer(result, make_buffer<ParquetStringVectorBuffer>(move(plain_data)));
485: }
486: 
487: string_t StringParquetValueConversion::DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
488: 	auto &dict_strings = ((StringColumnReader &)reader).dict_strings;
489: 	return dict_strings[offset];
490: }
491: 
492: string_t StringParquetValueConversion::PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
493: 	auto &scr = ((StringColumnReader &)reader);
494: 	uint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;
495: 	plain_data.available(str_len);
496: 	auto actual_str_len = ((StringColumnReader &)reader).VerifyString(plain_data.ptr, str_len);
497: 	auto ret_str = string_t(plain_data.ptr, actual_str_len);
498: 	plain_data.inc(str_len);
499: 	return ret_str;
500: }
501: 
502: void StringParquetValueConversion::PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
503: 	auto &scr = ((StringColumnReader &)reader);
504: 	uint32_t str_len = scr.fixed_width_string_length == 0 ? plain_data.read<uint32_t>() : scr.fixed_width_string_length;
505: 	plain_data.inc(str_len);
506: }
507: 
508: //===--------------------------------------------------------------------===//
509: // List Column Reader
510: //===--------------------------------------------------------------------===//
511: idx_t ListColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
512:                              Vector &result_out) {
513: 	idx_t result_offset = 0;
514: 	auto result_ptr = FlatVector::GetData<list_entry_t>(result_out);
515: 	auto &result_mask = FlatVector::Validity(result_out);
516: 
517: 	D_ASSERT(ListVector::GetListSize(result_out) == 0);
518: 	// if an individual list is longer than STANDARD_VECTOR_SIZE we actually have to loop the child read to fill it
519: 	bool finished = false;
520: 	while (!finished) {
521: 		idx_t child_actual_num_values = 0;
522: 
523: 		// check if we have any overflow from a previous read
524: 		if (overflow_child_count == 0) {
525: 			// we don't: read elements from the child reader
526: 			child_defines.zero();
527: 			child_repeats.zero();
528: 			// we don't know in advance how many values to read because of the beautiful repetition/definition setup
529: 			// we just read (up to) a vector from the child column, and see if we have read enough
530: 			// if we have not read enough, we read another vector
531: 			// if we have read enough, we leave any unhandled elements in the overflow vector for a subsequent read
532: 			auto child_req_num_values =
533: 			    MinValue<idx_t>(STANDARD_VECTOR_SIZE, child_column_reader->GroupRowsAvailable());
534: 			read_vector.ResetFromCache(read_cache);
535: 			child_actual_num_values = child_column_reader->Read(child_req_num_values, child_filter, child_defines_ptr,
536: 			                                                    child_repeats_ptr, read_vector);
537: 		} else {
538: 			// we do: use the overflow values
539: 			child_actual_num_values = overflow_child_count;
540: 			overflow_child_count = 0;
541: 		}
542: 
543: 		if (child_actual_num_values == 0) {
544: 			// no more elements available: we are done
545: 			break;
546: 		}
547: 		read_vector.Verify(child_actual_num_values);
548: 		idx_t current_chunk_offset = ListVector::GetListSize(result_out);
549: 
550: 		// hard-won piece of code this, modify at your own risk
551: 		// the intuition is that we have to only collapse values into lists that are repeated *on this level*
552: 		// the rest is pretty much handed up as-is as a single-valued list or NULL
553: 		idx_t child_idx;
554: 		for (child_idx = 0; child_idx < child_actual_num_values; child_idx++) {
555: 			if (child_repeats_ptr[child_idx] == max_repeat) {
556: 				// value repeats on this level, append
557: 				D_ASSERT(result_offset > 0);
558: 				result_ptr[result_offset - 1].length++;
559: 				continue;
560: 			}
561: 
562: 			if (result_offset >= num_values) {
563: 				// we ran out of output space
564: 				finished = true;
565: 				break;
566: 			}
567: 			if (child_defines_ptr[child_idx] >= max_define) {
568: 				// value has been defined down the stack, hence its NOT NULL
569: 				result_ptr[result_offset].offset = child_idx + current_chunk_offset;
570: 				result_ptr[result_offset].length = 1;
571: 			} else if (child_defines_ptr[child_idx] == max_define - 1) {
572: 				// empty list
573: 				result_ptr[result_offset].offset = child_idx + current_chunk_offset;
574: 				result_ptr[result_offset].length = 0;
575: 			} else {
576: 				// value is NULL somewhere up the stack
577: 				result_mask.SetInvalid(result_offset);
578: 				result_ptr[result_offset].offset = 0;
579: 				result_ptr[result_offset].length = 0;
580: 			}
581: 
582: 			repeat_out[result_offset] = child_repeats_ptr[child_idx];
583: 			define_out[result_offset] = child_defines_ptr[child_idx];
584: 
585: 			result_offset++;
586: 		}
587: 		// actually append the required elements to the child list
588: 		ListVector::Append(result_out, read_vector, child_idx);
589: 
590: 		// we have read more values from the child reader than we can fit into the result for this read
591: 		// we have to pass everything from child_idx to child_actual_num_values into the next call
592: 		if (child_idx < child_actual_num_values && result_offset == num_values) {
593: 			read_vector.Slice(read_vector, child_idx);
594: 			overflow_child_count = child_actual_num_values - child_idx;
595: 			read_vector.Verify(overflow_child_count);
596: 
597: 			// move values in the child repeats and defines *backward* by child_idx
598: 			for (idx_t repdef_idx = 0; repdef_idx < overflow_child_count; repdef_idx++) {
599: 				child_defines_ptr[repdef_idx] = child_defines_ptr[child_idx + repdef_idx];
600: 				child_repeats_ptr[repdef_idx] = child_repeats_ptr[child_idx + repdef_idx];
601: 			}
602: 		}
603: 	}
604: 	result_out.Verify(result_offset);
605: 	return result_offset;
606: }
607: 
608: ListColumnReader::ListColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,
609:                                    idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p,
610:                                    unique_ptr<ColumnReader> child_column_reader_p)
611:     : ColumnReader(reader, move(type_p), schema_p, schema_idx_p, max_define_p, max_repeat_p),
612:       child_column_reader(move(child_column_reader_p)), read_cache(ListType::GetChildType(Type())),
613:       read_vector(read_cache), overflow_child_count(0) {
614: 
615: 	child_defines.resize(reader.allocator, STANDARD_VECTOR_SIZE);
616: 	child_repeats.resize(reader.allocator, STANDARD_VECTOR_SIZE);
617: 	child_defines_ptr = (uint8_t *)child_defines.ptr;
618: 	child_repeats_ptr = (uint8_t *)child_repeats.ptr;
619: 
620: 	child_filter.set();
621: }
622: 
623: //===--------------------------------------------------------------------===//
624: // Struct Column Reader
625: //===--------------------------------------------------------------------===//
626: StructColumnReader::StructColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p,
627:                                        idx_t schema_idx_p, idx_t max_define_p, idx_t max_repeat_p,
628:                                        vector<unique_ptr<ColumnReader>> child_readers_p)
629:     : ColumnReader(reader, move(type_p), schema_p, schema_idx_p, max_define_p, max_repeat_p),
630:       child_readers(move(child_readers_p)) {
631: 	D_ASSERT(type.InternalType() == PhysicalType::STRUCT);
632: }
633: 
634: ColumnReader *StructColumnReader::GetChildReader(idx_t child_idx) {
635: 	return child_readers[child_idx].get();
636: }
637: 
638: void StructColumnReader::InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) {
639: 	for (auto &child : child_readers) {
640: 		child->InitializeRead(columns, protocol_p);
641: 	}
642: }
643: 
644: idx_t StructColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
645:                                Vector &result) {
646: 	auto &struct_entries = StructVector::GetEntries(result);
647: 	D_ASSERT(StructType::GetChildTypes(Type()).size() == struct_entries.size());
648: 
649: 	idx_t read_count = num_values;
650: 	for (idx_t i = 0; i < struct_entries.size(); i++) {
651: 		auto child_num_values = child_readers[i]->Read(num_values, filter, define_out, repeat_out, *struct_entries[i]);
652: 		if (i == 0) {
653: 			read_count = child_num_values;
654: 		} else if (read_count != child_num_values) {
655: 			throw std::runtime_error("Struct child row count mismatch");
656: 		}
657: 	}
658: 	// set the validity mask for this level
659: 	auto &validity = FlatVector::Validity(result);
660: 	for (idx_t i = 0; i < read_count; i++) {
661: 		if (define_out[i] < max_define) {
662: 			validity.SetInvalid(i);
663: 		}
664: 	}
665: 
666: 	return read_count;
667: }
668: 
669: void StructColumnReader::Skip(idx_t num_values) {
670: 	throw InternalException("Skip not implemented for StructColumnReader");
671: }
672: 
673: idx_t StructColumnReader::GroupRowsAvailable() {
674: 	for (idx_t i = 0; i < child_readers.size(); i++) {
675: 		if (child_readers[i]->Type().id() != LogicalTypeId::LIST) {
676: 			return child_readers[i]->GroupRowsAvailable();
677: 		}
678: 	}
679: 	return child_readers[0]->GroupRowsAvailable();
680: }
681: 
682: //===--------------------------------------------------------------------===//
683: // Decimal Column Reader
684: //===--------------------------------------------------------------------===//
685: template <class DUCKDB_PHYSICAL_TYPE, bool FIXED_LENGTH>
686: struct DecimalParquetValueConversion {
687: 	static DUCKDB_PHYSICAL_TYPE DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
688: 		auto dict_ptr = (DUCKDB_PHYSICAL_TYPE *)dict.ptr;
689: 		return dict_ptr[offset];
690: 	}
691: 
692: 	static DUCKDB_PHYSICAL_TYPE PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
693: 		idx_t byte_len;
694: 		if (FIXED_LENGTH) {
695: 			byte_len = (idx_t)reader.Schema().type_length; /* sure, type length needs to be a signed int */
696: 		} else {
697: 			byte_len = plain_data.read<uint32_t>();
698: 		}
699: 		plain_data.available(byte_len);
700: 		auto res =
701: 		    ParquetDecimalUtils::ReadDecimalValue<DUCKDB_PHYSICAL_TYPE>((const_data_ptr_t)plain_data.ptr, byte_len);
702: 
703: 		plain_data.inc(byte_len);
704: 		return res;
705: 	}
706: 
707: 	static void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
708: 		uint32_t decimal_len = FIXED_LENGTH ? reader.Schema().type_length : plain_data.read<uint32_t>();
709: 		plain_data.inc(decimal_len);
710: 	}
711: };
712: 
713: template <class DUCKDB_PHYSICAL_TYPE, bool FIXED_LENGTH>
714: class DecimalColumnReader
715:     : public TemplatedColumnReader<DUCKDB_PHYSICAL_TYPE,
716:                                    DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE, FIXED_LENGTH>> {
717: 
718: public:
719: 	DecimalColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, // NOLINT
720: 	                    idx_t file_idx_p, idx_t max_define_p, idx_t max_repeat_p)
721: 	    : TemplatedColumnReader<DUCKDB_PHYSICAL_TYPE,
722: 	                            DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE, FIXED_LENGTH>>(
723: 	          reader, move(type_p), schema_p, file_idx_p, max_define_p, max_repeat_p) {};
724: 
725: protected:
726: 	void Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) { // NOLINT
727: 		this->dict = make_shared<ResizeableBuffer>(this->reader.allocator, num_entries * sizeof(DUCKDB_PHYSICAL_TYPE));
728: 		auto dict_ptr = (DUCKDB_PHYSICAL_TYPE *)this->dict->ptr;
729: 		for (idx_t i = 0; i < num_entries; i++) {
730: 			dict_ptr[i] =
731: 			    DecimalParquetValueConversion<DUCKDB_PHYSICAL_TYPE, FIXED_LENGTH>::PlainRead(*dictionary_data, *this);
732: 		}
733: 	}
734: };
735: 
736: template <bool FIXED_LENGTH>
737: static unique_ptr<ColumnReader> CreateDecimalReaderInternal(ParquetReader &reader, const LogicalType &type_p,
738:                                                             const SchemaElement &schema_p, idx_t file_idx_p,
739:                                                             idx_t max_define, idx_t max_repeat) {
740: 	switch (type_p.InternalType()) {
741: 	case PhysicalType::INT16:
742: 		return make_unique<DecimalColumnReader<int16_t, FIXED_LENGTH>>(reader, type_p, schema_p, file_idx_p, max_define,
743: 		                                                               max_repeat);
744: 	case PhysicalType::INT32:
745: 		return make_unique<DecimalColumnReader<int32_t, FIXED_LENGTH>>(reader, type_p, schema_p, file_idx_p, max_define,
746: 		                                                               max_repeat);
747: 	case PhysicalType::INT64:
748: 		return make_unique<DecimalColumnReader<int64_t, FIXED_LENGTH>>(reader, type_p, schema_p, file_idx_p, max_define,
749: 		                                                               max_repeat);
750: 	case PhysicalType::INT128:
751: 		return make_unique<DecimalColumnReader<hugeint_t, FIXED_LENGTH>>(reader, type_p, schema_p, file_idx_p,
752: 		                                                                 max_define, max_repeat);
753: 	default:
754: 		throw InternalException("Unrecognized type for Decimal");
755: 	}
756: }
757: 
758: unique_ptr<ColumnReader> ParquetDecimalUtils::CreateReader(ParquetReader &reader, const LogicalType &type_p,
759:                                                            const SchemaElement &schema_p, idx_t file_idx_p,
760:                                                            idx_t max_define, idx_t max_repeat) {
761: 	if (schema_p.__isset.type_length) {
762: 		return CreateDecimalReaderInternal<true>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
763: 	} else {
764: 		return CreateDecimalReaderInternal<false>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
765: 	}
766: }
767: 
768: //===--------------------------------------------------------------------===//
769: // UUID Column Reader
770: //===--------------------------------------------------------------------===//
771: struct UUIDValueConversion {
772: 	static hugeint_t DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
773: 		auto dict_ptr = (hugeint_t *)dict.ptr;
774: 		return dict_ptr[offset];
775: 	}
776: 
777: 	static hugeint_t ReadParquetUUID(const_data_ptr_t input) {
778: 		hugeint_t result;
779: 		result.lower = 0;
780: 		uint64_t unsigned_upper = 0;
781: 		for (idx_t i = 0; i < sizeof(uint64_t); i++) {
782: 			unsigned_upper <<= 8;
783: 			unsigned_upper += input[i];
784: 		}
785: 		for (idx_t i = sizeof(uint64_t); i < sizeof(hugeint_t); i++) {
786: 			result.lower <<= 8;
787: 			result.lower += input[i];
788: 		}
789: 		result.upper = unsigned_upper;
790: 		result.upper ^= (int64_t(1) << 63);
791: 		return result;
792: 	}
793: 
794: 	static hugeint_t PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
795: 		idx_t byte_len = sizeof(hugeint_t);
796: 		plain_data.available(byte_len);
797: 		auto res = ReadParquetUUID((const_data_ptr_t)plain_data.ptr);
798: 
799: 		plain_data.inc(byte_len);
800: 		return res;
801: 	}
802: 
803: 	static void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
804: 		plain_data.inc(sizeof(hugeint_t));
805: 	}
806: };
807: 
808: class UUIDColumnReader : public TemplatedColumnReader<hugeint_t, UUIDValueConversion> {
809: 
810: public:
811: 	UUIDColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,
812: 	                 idx_t max_define_p, idx_t max_repeat_p)
813: 	    : TemplatedColumnReader<hugeint_t, UUIDValueConversion>(reader, move(type_p), schema_p, file_idx_p,
814: 	                                                            max_define_p, max_repeat_p) {};
815: 
816: protected:
817: 	void Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) { // NOLINT
818: 		this->dict = make_shared<ResizeableBuffer>(this->reader.allocator, num_entries * sizeof(hugeint_t));
819: 		auto dict_ptr = (hugeint_t *)this->dict->ptr;
820: 		for (idx_t i = 0; i < num_entries; i++) {
821: 			dict_ptr[i] = UUIDValueConversion::PlainRead(*dictionary_data, *this);
822: 		}
823: 	}
824: };
825: 
826: //===--------------------------------------------------------------------===//
827: // Interval Column Reader
828: //===--------------------------------------------------------------------===//
829: struct IntervalValueConversion {
830: 	static constexpr const idx_t PARQUET_INTERVAL_SIZE = 12;
831: 
832: 	static interval_t DictRead(ByteBuffer &dict, uint32_t &offset, ColumnReader &reader) {
833: 		auto dict_ptr = (interval_t *)dict.ptr;
834: 		return dict_ptr[offset];
835: 	}
836: 
837: 	static interval_t ReadParquetInterval(const_data_ptr_t input) {
838: 		interval_t result;
839: 		result.months = Load<uint32_t>(input);
840: 		result.days = Load<uint32_t>(input + sizeof(uint32_t));
841: 		result.micros = int64_t(Load<uint32_t>(input + sizeof(uint32_t) * 2)) * 1000;
842: 		return result;
843: 	}
844: 
845: 	static interval_t PlainRead(ByteBuffer &plain_data, ColumnReader &reader) {
846: 		idx_t byte_len = PARQUET_INTERVAL_SIZE;
847: 		plain_data.available(byte_len);
848: 		auto res = ReadParquetInterval((const_data_ptr_t)plain_data.ptr);
849: 
850: 		plain_data.inc(byte_len);
851: 		return res;
852: 	}
853: 
854: 	static void PlainSkip(ByteBuffer &plain_data, ColumnReader &reader) {
855: 		plain_data.inc(PARQUET_INTERVAL_SIZE);
856: 	}
857: };
858: 
859: class IntervalColumnReader : public TemplatedColumnReader<interval_t, IntervalValueConversion> {
860: 
861: public:
862: 	IntervalColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,
863: 	                     idx_t max_define_p, idx_t max_repeat_p)
864: 	    : TemplatedColumnReader<interval_t, IntervalValueConversion>(reader, move(type_p), schema_p, file_idx_p,
865: 	                                                                 max_define_p, max_repeat_p) {};
866: 
867: protected:
868: 	void Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries) override { // NOLINT
869: 		this->dict = make_shared<ResizeableBuffer>(this->reader.allocator, num_entries * sizeof(interval_t));
870: 		auto dict_ptr = (interval_t *)this->dict->ptr;
871: 		for (idx_t i = 0; i < num_entries; i++) {
872: 			dict_ptr[i] = IntervalValueConversion::PlainRead(*dictionary_data, *this);
873: 		}
874: 	}
875: };
876: 
877: //===--------------------------------------------------------------------===//
878: // Create Column Reader
879: //===--------------------------------------------------------------------===//
880: template <class T>
881: unique_ptr<ColumnReader> CreateDecimalReader(ParquetReader &reader, const LogicalType &type_p,
882:                                              const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,
883:                                              idx_t max_repeat) {
884: 	switch (type_p.InternalType()) {
885: 	case PhysicalType::INT16:
886: 		return make_unique<TemplatedColumnReader<int16_t, TemplatedParquetValueConversion<T>>>(
887: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
888: 	case PhysicalType::INT32:
889: 		return make_unique<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<T>>>(
890: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
891: 	case PhysicalType::INT64:
892: 		return make_unique<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<T>>>(
893: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
894: 	default:
895: 		throw NotImplementedException("Unimplemented internal type for CreateDecimalReader");
896: 	}
897: }
898: 
899: unique_ptr<ColumnReader> ColumnReader::CreateReader(ParquetReader &reader, const LogicalType &type_p,
900:                                                     const SchemaElement &schema_p, idx_t file_idx_p, idx_t max_define,
901:                                                     idx_t max_repeat) {
902: 	switch (type_p.id()) {
903: 	case LogicalTypeId::BOOLEAN:
904: 		return make_unique<BooleanColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
905: 	case LogicalTypeId::UTINYINT:
906: 		return make_unique<TemplatedColumnReader<uint8_t, TemplatedParquetValueConversion<uint32_t>>>(
907: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
908: 	case LogicalTypeId::USMALLINT:
909: 		return make_unique<TemplatedColumnReader<uint16_t, TemplatedParquetValueConversion<uint32_t>>>(
910: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
911: 	case LogicalTypeId::UINTEGER:
912: 		return make_unique<TemplatedColumnReader<uint32_t, TemplatedParquetValueConversion<uint32_t>>>(
913: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
914: 	case LogicalTypeId::UBIGINT:
915: 		return make_unique<TemplatedColumnReader<uint64_t, TemplatedParquetValueConversion<uint64_t>>>(
916: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
917: 	case LogicalTypeId::TINYINT:
918: 		return make_unique<TemplatedColumnReader<int8_t, TemplatedParquetValueConversion<int32_t>>>(
919: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
920: 	case LogicalTypeId::SMALLINT:
921: 		return make_unique<TemplatedColumnReader<int16_t, TemplatedParquetValueConversion<int32_t>>>(
922: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
923: 	case LogicalTypeId::INTEGER:
924: 		return make_unique<TemplatedColumnReader<int32_t, TemplatedParquetValueConversion<int32_t>>>(
925: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
926: 	case LogicalTypeId::BIGINT:
927: 		return make_unique<TemplatedColumnReader<int64_t, TemplatedParquetValueConversion<int64_t>>>(
928: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
929: 	case LogicalTypeId::FLOAT:
930: 		return make_unique<TemplatedColumnReader<float, TemplatedParquetValueConversion<float>>>(
931: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
932: 	case LogicalTypeId::DOUBLE:
933: 		return make_unique<TemplatedColumnReader<double, TemplatedParquetValueConversion<double>>>(
934: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
935: 	case LogicalTypeId::TIMESTAMP:
936: 		switch (schema_p.type) {
937: 		case Type::INT96:
938: 			return make_unique<CallbackColumnReader<Int96, timestamp_t, ImpalaTimestampToTimestamp>>(
939: 			    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
940: 		case Type::INT64:
941: 			switch (schema_p.converted_type) {
942: 			case ConvertedType::TIMESTAMP_MICROS:
943: 				return make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMicrosToTimestamp>>(
944: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
945: 			case ConvertedType::TIMESTAMP_MILLIS:
946: 				return make_unique<CallbackColumnReader<int64_t, timestamp_t, ParquetTimestampMsToTimestamp>>(
947: 				    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
948: 			default:
949: 				break;
950: 			}
951: 		default:
952: 			break;
953: 		}
954: 		break;
955: 	case LogicalTypeId::DATE:
956: 		return make_unique<CallbackColumnReader<int32_t, date_t, ParquetIntToDate>>(reader, type_p, schema_p,
957: 		                                                                            file_idx_p, max_define, max_repeat);
958: 	case LogicalTypeId::TIME:
959: 		return make_unique<CallbackColumnReader<int64_t, dtime_t, ParquetIntToTime>>(
960: 		    reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
961: 	case LogicalTypeId::BLOB:
962: 	case LogicalTypeId::VARCHAR:
963: 		return make_unique<StringColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
964: 	case LogicalTypeId::DECIMAL:
965: 		// we have to figure out what kind of int we need
966: 		switch (schema_p.type) {
967: 		case Type::INT32:
968: 			return CreateDecimalReader<int32_t>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
969: 		case Type::INT64:
970: 			return CreateDecimalReader<int64_t>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
971: 		case Type::BYTE_ARRAY:
972: 		case Type::FIXED_LEN_BYTE_ARRAY:
973: 			return ParquetDecimalUtils::CreateReader(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
974: 		default:
975: 			throw NotImplementedException("Unrecognized Parquet type for Decimal");
976: 		}
977: 		break;
978: 	case LogicalTypeId::UUID:
979: 		return make_unique<UUIDColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
980: 	case LogicalTypeId::INTERVAL:
981: 		return make_unique<IntervalColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);
982: 	default:
983: 		break;
984: 	}
985: 	throw NotImplementedException(type_p.ToString());
986: }
987: 
988: } // namespace duckdb
[end of extension/parquet/column_reader.cpp]
[start of extension/parquet/include/column_reader.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // column_reader.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "parquet_types.h"
12: #include "thrift_tools.hpp"
13: #include "resizable_buffer.hpp"
14: 
15: #include "parquet_rle_bp_decoder.hpp"
16: #include "parquet_dbp_decoder.hpp"
17: #include "parquet_statistics.hpp"
18: 
19: #include "duckdb.hpp"
20: #ifndef DUCKDB_AMALGAMATION
21: #include "duckdb/storage/statistics/string_statistics.hpp"
22: #include "duckdb/storage/statistics/numeric_statistics.hpp"
23: #include "duckdb/common/types/vector.hpp"
24: #include "duckdb/common/types/string_type.hpp"
25: #include "duckdb/common/types/chunk_collection.hpp"
26: #include "duckdb/common/operator/cast_operators.hpp"
27: #include "duckdb/common/types/vector_cache.hpp"
28: #endif
29: 
30: namespace duckdb {
31: class ParquetReader;
32: 
33: using duckdb_apache::thrift::protocol::TProtocol;
34: 
35: using duckdb_parquet::format::ColumnChunk;
36: using duckdb_parquet::format::FieldRepetitionType;
37: using duckdb_parquet::format::PageHeader;
38: using duckdb_parquet::format::SchemaElement;
39: using duckdb_parquet::format::Type;
40: 
41: typedef std::bitset<STANDARD_VECTOR_SIZE> parquet_filter_t;
42: 
43: class ColumnReader {
44: public:
45: 	ColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t file_idx_p,
46: 	             idx_t max_define_p, idx_t max_repeat_p);
47: 	virtual ~ColumnReader();
48: 
49: public:
50: 	static unique_ptr<ColumnReader> CreateReader(ParquetReader &reader, const LogicalType &type_p,
51: 	                                             const SchemaElement &schema_p, idx_t schema_idx_p, idx_t max_define,
52: 	                                             idx_t max_repeat);
53: 	virtual void InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p);
54: 	virtual idx_t Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,
55: 	                   Vector &result_out);
56: 
57: 	virtual void Skip(idx_t num_values);
58: 
59: 	const LogicalType &Type();
60: 	const SchemaElement &Schema();
61: 
62: 	virtual idx_t GroupRowsAvailable();
63: 
64: 	unique_ptr<BaseStatistics> Stats(const std::vector<ColumnChunk> &columns);
65: 
66: protected:
67: 	// readers that use the default Read() need to implement those
68: 	virtual void Plain(shared_ptr<ByteBuffer> plain_data, uint8_t *defines, idx_t num_values, parquet_filter_t &filter,
69: 	                   idx_t result_offset, Vector &result);
70: 	virtual void Dictionary(shared_ptr<ByteBuffer> dictionary_data, idx_t num_entries);
71: 	virtual void Offsets(uint32_t *offsets, uint8_t *defines, idx_t num_values, parquet_filter_t &filter,
72: 	                     idx_t result_offset, Vector &result);
73: 
74: 	// these are nops for most types, but not for strings
75: 	virtual void DictReference(Vector &result);
76: 	virtual void PlainReference(shared_ptr<ByteBuffer>, Vector &result);
77: 
78: 	bool HasDefines() {
79: 		return max_define > 0;
80: 	}
81: 
82: 	bool HasRepeats() {
83: 		return max_repeat > 0;
84: 	}
85: 
86: protected:
87: 	const SchemaElement &schema;
88: 
89: 	idx_t file_idx;
90: 	idx_t max_define;
91: 	idx_t max_repeat;
92: 
93: 	ParquetReader &reader;
94: 	LogicalType type;
95: 
96: private:
97: 	void PrepareRead(parquet_filter_t &filter);
98: 	void PreparePage(idx_t compressed_page_size, idx_t uncompressed_page_size);
99: 	void PrepareDataPage(PageHeader &page_hdr);
100: 	void PreparePageV2(PageHeader &page_hdr);
101: 
102: 	const duckdb_parquet::format::ColumnChunk *chunk;
103: 
104: 	duckdb_apache::thrift::protocol::TProtocol *protocol;
105: 	idx_t page_rows_available;
106: 	idx_t group_rows_available;
107: 	idx_t chunk_read_offset;
108: 
109: 	shared_ptr<ResizeableBuffer> block;
110: 
111: 	ResizeableBuffer offset_buffer;
112: 
113: 	unique_ptr<RleBpDecoder> dict_decoder;
114: 	unique_ptr<RleBpDecoder> defined_decoder;
115: 	unique_ptr<RleBpDecoder> repeated_decoder;
116: 	unique_ptr<DbpDecoder> dbp_decoder;
117: 
118: 	// dummies for Skip()
119: 	parquet_filter_t none_filter;
120: 	ResizeableBuffer dummy_define;
121: 	ResizeableBuffer dummy_repeat;
122: };
123: 
124: } // namespace duckdb
[end of extension/parquet/include/column_reader.hpp]
[start of extension/parquet/include/parquet_reader.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // parquet_reader.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb.hpp"
12: #ifndef DUCKDB_AMALGAMATION
13: #include "duckdb/common/common.hpp"
14: #include "duckdb/common/exception.hpp"
15: #include "duckdb/common/string_util.hpp"
16: #include "duckdb/common/types/data_chunk.hpp"
17: #endif
18: #include "column_reader.hpp"
19: #include "parquet_file_metadata_cache.hpp"
20: #include "parquet_rle_bp_decoder.hpp"
21: #include "parquet_types.h"
22: #include "resizable_buffer.hpp"
23: 
24: #include <exception>
25: 
26: namespace duckdb_parquet {
27: namespace format {
28: class FileMetaData;
29: }
30: } // namespace duckdb_parquet
31: 
32: namespace duckdb {
33: class Allocator;
34: class ClientContext;
35: class ChunkCollection;
36: class BaseStatistics;
37: class TableFilterSet;
38: 
39: struct ParquetReaderScanState {
40: 	vector<idx_t> group_idx_list;
41: 	int64_t current_group;
42: 	vector<column_t> column_ids;
43: 	idx_t group_offset;
44: 	unique_ptr<FileHandle> file_handle;
45: 	unique_ptr<ColumnReader> root_reader;
46: 	unique_ptr<duckdb_apache::thrift::protocol::TProtocol> thrift_file_proto;
47: 
48: 	bool finished;
49: 	TableFilterSet *filters;
50: 	SelectionVector sel;
51: 
52: 	ResizeableBuffer define_buf;
53: 	ResizeableBuffer repeat_buf;
54: };
55: 
56: struct ParquetOptions {
57: 	explicit ParquetOptions() {
58: 	}
59: 	explicit ParquetOptions(ClientContext &context);
60: 
61: 	bool binary_as_string = false;
62: };
63: 
64: class ParquetReader {
65: public:
66: 	ParquetReader(Allocator &allocator, unique_ptr<FileHandle> file_handle_p,
67: 	              const vector<LogicalType> &expected_types_p, const string &initial_filename_p = string());
68: 	ParquetReader(Allocator &allocator, unique_ptr<FileHandle> file_handle_p)
69: 	    : ParquetReader(allocator, move(file_handle_p), vector<LogicalType>(), string()) {
70: 	}
71: 
72: 	ParquetReader(ClientContext &context, string file_name, const vector<LogicalType> &expected_types_p,
73: 	              ParquetOptions parquet_options, const string &initial_filename = string());
74: 	ParquetReader(ClientContext &context, string file_name, ParquetOptions parquet_options)
75: 	    : ParquetReader(context, move(file_name), vector<LogicalType>(), parquet_options, string()) {
76: 	}
77: 	~ParquetReader();
78: 
79: 	Allocator &allocator;
80: 	string file_name;
81: 	FileOpener *file_opener;
82: 	vector<LogicalType> return_types;
83: 	vector<string> names;
84: 	shared_ptr<ParquetFileMetadataCache> metadata;
85: 	ParquetOptions parquet_options;
86: 
87: public:
88: 	void InitializeScan(ParquetReaderScanState &state, vector<column_t> column_ids, vector<idx_t> groups_to_read,
89: 	                    TableFilterSet *table_filters);
90: 	void Scan(ParquetReaderScanState &state, DataChunk &output);
91: 
92: 	idx_t NumRows();
93: 	idx_t NumRowGroups();
94: 
95: 	const duckdb_parquet::format::FileMetaData *GetFileMetadata();
96: 
97: 	static unique_ptr<BaseStatistics> ReadStatistics(ParquetReader &reader, LogicalType &type, column_t column_index,
98: 	                                                 const duckdb_parquet::format::FileMetaData *file_meta_data);
99: 	static LogicalType DeriveLogicalType(const SchemaElement &s_ele, bool binary_as_string);
100: 
101: private:
102: 	void InitializeSchema(const vector<LogicalType> &expected_types_p, const string &initial_filename_p);
103: 	bool ScanInternal(ParquetReaderScanState &state, DataChunk &output);
104: 	unique_ptr<ColumnReader> CreateReader(const duckdb_parquet::format::FileMetaData *file_meta_data);
105: 
106: 	unique_ptr<ColumnReader> CreateReaderRecursive(const duckdb_parquet::format::FileMetaData *file_meta_data,
107: 	                                               idx_t depth, idx_t max_define, idx_t max_repeat,
108: 	                                               idx_t &next_schema_idx, idx_t &next_file_idx);
109: 	const duckdb_parquet::format::RowGroup &GetGroup(ParquetReaderScanState &state);
110: 	void PrepareRowGroupBuffer(ParquetReaderScanState &state, idx_t out_col_idx);
111: 	LogicalType DeriveLogicalType(const SchemaElement &s_ele);
112: 
113: 	template <typename... Args>
114: 	std::runtime_error FormatException(const string fmt_str, Args... params) {
115: 		return std::runtime_error("Failed to read Parquet file \"" + file_name +
116: 		                          "\": " + StringUtil::Format(fmt_str, params...));
117: 	}
118: 
119: private:
120: 	unique_ptr<FileHandle> file_handle;
121: };
122: 
123: } // namespace duckdb
[end of extension/parquet/include/parquet_reader.hpp]
[start of extension/parquet/parquet-extension.cpp]
1: #define DUCKDB_EXTENSION_MAIN
2: 
3: #include <string>
4: #include <vector>
5: #include <fstream>
6: #include <iostream>
7: 
8: #include "parquet-extension.hpp"
9: #include "parquet_reader.hpp"
10: #include "parquet_writer.hpp"
11: #include "parquet_metadata.hpp"
12: #include "zstd_file_system.hpp"
13: 
14: #include "duckdb.hpp"
15: #ifndef DUCKDB_AMALGAMATION
16: #include "duckdb/common/file_system.hpp"
17: #include "duckdb/common/types/chunk_collection.hpp"
18: #include "duckdb/function/copy_function.hpp"
19: #include "duckdb/function/table_function.hpp"
20: #include "duckdb/common/file_system.hpp"
21: #include "duckdb/parallel/parallel_state.hpp"
22: #include "duckdb/parser/parsed_data/create_copy_function_info.hpp"
23: #include "duckdb/parser/parsed_data/create_table_function_info.hpp"
24: 
25: #include "duckdb/common/enums/file_compression_type.hpp"
26: #include "duckdb/main/config.hpp"
27: #include "duckdb/parser/expression/constant_expression.hpp"
28: #include "duckdb/parser/expression/function_expression.hpp"
29: #include "duckdb/parser/tableref/table_function_ref.hpp"
30: 
31: #include "duckdb/storage/statistics/base_statistics.hpp"
32: 
33: #include "duckdb/main/client_context.hpp"
34: #include "duckdb/catalog/catalog.hpp"
35: #endif
36: 
37: namespace duckdb {
38: 
39: struct ParquetReadBindData : public FunctionData {
40: 	shared_ptr<ParquetReader> initial_reader;
41: 	vector<string> files;
42: 	vector<column_t> column_ids;
43: 	atomic<idx_t> chunk_count;
44: 	atomic<idx_t> cur_file;
45: };
46: 
47: struct ParquetReadOperatorData : public FunctionOperatorData {
48: 	shared_ptr<ParquetReader> reader;
49: 	ParquetReaderScanState scan_state;
50: 	bool is_parallel;
51: 	idx_t file_index;
52: 	vector<column_t> column_ids;
53: 	TableFilterSet *table_filters;
54: };
55: 
56: struct ParquetReadParallelState : public ParallelState {
57: 	mutex lock;
58: 	shared_ptr<ParquetReader> current_reader;
59: 	idx_t file_index;
60: 	idx_t row_group_index;
61: };
62: 
63: class ParquetScanFunction {
64: public:
65: 	static TableFunctionSet GetFunctionSet() {
66: 		TableFunctionSet set("parquet_scan");
67: 		auto table_function =
68: 		    TableFunction({LogicalType::VARCHAR}, ParquetScanImplementation, ParquetScanBind, ParquetScanInit,
69: 		                  /* statistics */ ParquetScanStats, /* cleanup */ nullptr,
70: 		                  /* dependency */ nullptr, ParquetCardinality,
71: 		                  /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr, ParquetScanMaxThreads,
72: 		                  ParquetInitParallelState, ParquetScanFuncParallel, ParquetScanParallelInit,
73: 		                  ParquetParallelStateNext, true, true, ParquetProgress);
74: 		table_function.named_parameters["binary_as_string"] = LogicalType::BOOLEAN;
75: 		set.AddFunction(table_function);
76: 		table_function = TableFunction({LogicalType::LIST(LogicalType::VARCHAR)}, ParquetScanImplementation,
77: 		                               ParquetScanBindList, ParquetScanInit, /* statistics */ ParquetScanStats,
78: 		                               /* cleanup */ nullptr,
79: 		                               /* dependency */ nullptr, ParquetCardinality,
80: 		                               /* pushdown_complex_filter */ nullptr, /* to_string */ nullptr,
81: 		                               ParquetScanMaxThreads, ParquetInitParallelState, ParquetScanFuncParallel,
82: 		                               ParquetScanParallelInit, ParquetParallelStateNext, true, true, ParquetProgress);
83: 		table_function.named_parameters["binary_as_string"] = LogicalType::BOOLEAN;
84: 		set.AddFunction(table_function);
85: 		return set;
86: 	}
87: 
88: 	static unique_ptr<FunctionData> ParquetReadBind(ClientContext &context, CopyInfo &info,
89: 	                                                vector<string> &expected_names,
90: 	                                                vector<LogicalType> &expected_types) {
91: 		for (auto &option : info.options) {
92: 			auto loption = StringUtil::Lower(option.first);
93: 			if (loption == "compression" || loption == "codec") {
94: 				// CODEC option has no effect on parquet read: we determine codec from the file
95: 				continue;
96: 			} else {
97: 				throw NotImplementedException("Unsupported option for COPY FROM parquet: %s", option.first);
98: 			}
99: 		}
100: 		auto result = make_unique<ParquetReadBindData>();
101: 
102: 		FileSystem &fs = FileSystem::GetFileSystem(context);
103: 		result->files = fs.Glob(info.file_path);
104: 		if (result->files.empty()) {
105: 			throw IOException("No files found that match the pattern \"%s\"", info.file_path);
106: 		}
107: 		ParquetOptions parquet_options(context);
108: 		result->initial_reader = make_shared<ParquetReader>(context, result->files[0], expected_types, parquet_options);
109: 		return move(result);
110: 	}
111: 
112: 	static unique_ptr<BaseStatistics> ParquetScanStats(ClientContext &context, const FunctionData *bind_data_p,
113: 	                                                   column_t column_index) {
114: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
115: 
116: 		if (column_index == COLUMN_IDENTIFIER_ROW_ID) {
117: 			return nullptr;
118: 		}
119: 
120: 		// we do not want to parse the Parquet metadata for the sole purpose of getting column statistics
121: 
122: 		// We already parsed the metadata for the first file in a glob because we need some type info.
123: 		auto overall_stats = ParquetReader::ReadStatistics(
124: 		    *bind_data.initial_reader, bind_data.initial_reader->return_types[column_index], column_index,
125: 		    bind_data.initial_reader->metadata->metadata.get());
126: 
127: 		if (!overall_stats) {
128: 			return nullptr;
129: 		}
130: 
131: 		// if there is only one file in the glob (quite common case), we are done
132: 		auto &config = DBConfig::GetConfig(context);
133: 		if (bind_data.files.size() < 2) {
134: 			return overall_stats;
135: 		} else if (config.object_cache_enable) {
136: 			auto &cache = ObjectCache::GetObjectCache(context);
137: 			// for more than one file, we could be lucky and metadata for *every* file is in the object cache (if
138: 			// enabled at all)
139: 			FileSystem &fs = FileSystem::GetFileSystem(context);
140: 			for (idx_t file_idx = 1; file_idx < bind_data.files.size(); file_idx++) {
141: 				auto &file_name = bind_data.files[file_idx];
142: 				auto metadata = cache.Get<ParquetFileMetadataCache>(file_name);
143: 				if (!metadata) {
144: 					// missing metadata entry in cache, no usable stats
145: 					return nullptr;
146: 				}
147: 				auto handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK,
148: 				                          FileSystem::DEFAULT_COMPRESSION, FileSystem::GetFileOpener(context));
149: 				// we need to check if the metadata cache entries are current
150: 				if (fs.GetLastModifiedTime(*handle) >= metadata->read_time) {
151: 					// missing or invalid metadata entry in cache, no usable stats overall
152: 					return nullptr;
153: 				}
154: 				// get and merge stats for file
155: 				auto file_stats = ParquetReader::ReadStatistics(*bind_data.initial_reader,
156: 				                                                bind_data.initial_reader->return_types[column_index],
157: 				                                                column_index, metadata->metadata.get());
158: 				if (!file_stats) {
159: 					return nullptr;
160: 				}
161: 				overall_stats->Merge(*file_stats);
162: 			}
163: 			// success!
164: 			return overall_stats;
165: 		}
166: 		// we have more than one file and no object cache so no statistics overall
167: 		return nullptr;
168: 	}
169: 
170: 	static void ParquetScanFuncParallel(ClientContext &context, const FunctionData *bind_data,
171: 	                                    FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output,
172: 	                                    ParallelState *parallel_state_p) {
173: 		//! FIXME: Have specialized parallel function from pandas scan here
174: 		ParquetScanImplementation(context, bind_data, operator_state, input, output);
175: 	}
176: 
177: 	static unique_ptr<FunctionData> ParquetScanBindInternal(ClientContext &context, vector<string> files,
178: 	                                                        vector<LogicalType> &return_types, vector<string> &names,
179: 	                                                        ParquetOptions parquet_options) {
180: 		auto result = make_unique<ParquetReadBindData>();
181: 		result->files = move(files);
182: 
183: 		result->initial_reader = make_shared<ParquetReader>(context, result->files[0], parquet_options);
184: 		return_types = result->initial_reader->return_types;
185: 
186: 		names = result->initial_reader->names;
187: 		return move(result);
188: 	}
189: 
190: 	static vector<string> ParquetGlob(FileSystem &fs, const string &glob) {
191: 		auto files = fs.Glob(glob);
192: 		if (files.empty()) {
193: 			throw IOException("No files found that match the pattern \"%s\"", glob);
194: 		}
195: 		return files;
196: 	}
197: 
198: 	static unique_ptr<FunctionData> ParquetScanBind(ClientContext &context, vector<Value> &inputs,
199: 	                                                named_parameter_map_t &named_parameters,
200: 	                                                vector<LogicalType> &input_table_types,
201: 	                                                vector<string> &input_table_names,
202: 	                                                vector<LogicalType> &return_types, vector<string> &names) {
203: 		auto &config = DBConfig::GetConfig(context);
204: 		if (!config.enable_external_access) {
205: 			throw PermissionException("Scanning Parquet files is disabled through configuration");
206: 		}
207: 		auto file_name = inputs[0].GetValue<string>();
208: 		ParquetOptions parquet_options(context);
209: 		for (auto &kv : named_parameters) {
210: 			if (kv.first == "binary_as_string") {
211: 				parquet_options.binary_as_string = BooleanValue::Get(kv.second);
212: 			}
213: 		}
214: 		FileSystem &fs = FileSystem::GetFileSystem(context);
215: 		auto files = ParquetGlob(fs, file_name);
216: 		return ParquetScanBindInternal(context, move(files), return_types, names, parquet_options);
217: 	}
218: 
219: 	static unique_ptr<FunctionData> ParquetScanBindList(ClientContext &context, vector<Value> &inputs,
220: 	                                                    named_parameter_map_t &named_parameters,
221: 	                                                    vector<LogicalType> &input_table_types,
222: 	                                                    vector<string> &input_table_names,
223: 	                                                    vector<LogicalType> &return_types, vector<string> &names) {
224: 		auto &config = DBConfig::GetConfig(context);
225: 		if (!config.enable_external_access) {
226: 			throw PermissionException("Scanning Parquet files is disabled through configuration");
227: 		}
228: 		FileSystem &fs = FileSystem::GetFileSystem(context);
229: 		vector<string> files;
230: 		for (auto &val : ListValue::GetChildren(inputs[0])) {
231: 			auto glob_files = ParquetGlob(fs, val.ToString());
232: 			files.insert(files.end(), glob_files.begin(), glob_files.end());
233: 		}
234: 		if (files.empty()) {
235: 			throw IOException("Parquet reader needs at least one file to read");
236: 		}
237: 		ParquetOptions parquet_options(context);
238: 		for (auto &kv : named_parameters) {
239: 			if (kv.first == "binary_as_string") {
240: 				parquet_options.binary_as_string = BooleanValue::Get(kv.second);
241: 			}
242: 		}
243: 		return ParquetScanBindInternal(context, move(files), return_types, names, parquet_options);
244: 	}
245: 
246: 	static unique_ptr<FunctionOperatorData> ParquetScanInit(ClientContext &context, const FunctionData *bind_data_p,
247: 	                                                        const vector<column_t> &column_ids,
248: 	                                                        TableFilterCollection *filters) {
249: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
250: 		bind_data.chunk_count = 0;
251: 		bind_data.cur_file = 0;
252: 		auto result = make_unique<ParquetReadOperatorData>();
253: 		result->column_ids = column_ids;
254: 
255: 		result->is_parallel = false;
256: 		result->file_index = 0;
257: 		result->table_filters = filters->table_filters;
258: 		// single-threaded: one thread has to read all groups
259: 		vector<idx_t> group_ids;
260: 		for (idx_t i = 0; i < bind_data.initial_reader->NumRowGroups(); i++) {
261: 			group_ids.push_back(i);
262: 		}
263: 		result->reader = bind_data.initial_reader;
264: 		result->reader->InitializeScan(result->scan_state, column_ids, move(group_ids), filters->table_filters);
265: 		return move(result);
266: 	}
267: 
268: 	static double ParquetProgress(ClientContext &context, const FunctionData *bind_data_p) {
269: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
270: 		if (bind_data.initial_reader->NumRows() == 0) {
271: 			return (100.0 * (bind_data.cur_file + 1)) / bind_data.files.size();
272: 		}
273: 		auto percentage = (bind_data.chunk_count * STANDARD_VECTOR_SIZE * 100.0 / bind_data.initial_reader->NumRows()) /
274: 		                  bind_data.files.size();
275: 		percentage += 100.0 * bind_data.cur_file / bind_data.files.size();
276: 		return percentage;
277: 	}
278: 
279: 	static unique_ptr<FunctionOperatorData>
280: 	ParquetScanParallelInit(ClientContext &context, const FunctionData *bind_data_p, ParallelState *parallel_state_p,
281: 	                        const vector<column_t> &column_ids, TableFilterCollection *filters) {
282: 		auto result = make_unique<ParquetReadOperatorData>();
283: 		result->column_ids = column_ids;
284: 		result->is_parallel = true;
285: 		result->table_filters = filters->table_filters;
286: 		if (!ParquetParallelStateNext(context, bind_data_p, result.get(), parallel_state_p)) {
287: 			return nullptr;
288: 		}
289: 		return move(result);
290: 	}
291: 
292: 	static void ParquetScanImplementation(ClientContext &context, const FunctionData *bind_data_p,
293: 	                                      FunctionOperatorData *operator_state, DataChunk *input, DataChunk &output) {
294: 		if (!operator_state) {
295: 			return;
296: 		}
297: 		auto &data = (ParquetReadOperatorData &)*operator_state;
298: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
299: 
300: 		do {
301: 			data.reader->Scan(data.scan_state, output);
302: 			bind_data.chunk_count++;
303: 			if (output.size() == 0 && !data.is_parallel) {
304: 				auto &bind_data = (ParquetReadBindData &)*bind_data_p;
305: 				// check if there is another file
306: 				if (data.file_index + 1 < bind_data.files.size()) {
307: 					data.file_index++;
308: 					bind_data.cur_file++;
309: 					bind_data.chunk_count = 0;
310: 					string file = bind_data.files[data.file_index];
311: 					// move to the next file
312: 					data.reader = make_shared<ParquetReader>(context, file, data.reader->return_types,
313: 					                                         data.reader->parquet_options, bind_data.files[0]);
314: 					vector<idx_t> group_ids;
315: 					for (idx_t i = 0; i < data.reader->NumRowGroups(); i++) {
316: 						group_ids.push_back(i);
317: 					}
318: 					data.reader->InitializeScan(data.scan_state, data.column_ids, move(group_ids), data.table_filters);
319: 				} else {
320: 					// exhausted all the files: done
321: 					break;
322: 				}
323: 			} else {
324: 				break;
325: 			}
326: 		} while (true);
327: 	}
328: 
329: 	static unique_ptr<NodeStatistics> ParquetCardinality(ClientContext &context, const FunctionData *bind_data) {
330: 		auto &data = (ParquetReadBindData &)*bind_data;
331: 		return make_unique<NodeStatistics>(data.initial_reader->NumRows() * data.files.size());
332: 	}
333: 
334: 	static idx_t ParquetScanMaxThreads(ClientContext &context, const FunctionData *bind_data) {
335: 		auto &data = (ParquetReadBindData &)*bind_data;
336: 		return data.initial_reader->NumRowGroups() * data.files.size();
337: 	}
338: 
339: 	static unique_ptr<ParallelState> ParquetInitParallelState(ClientContext &context, const FunctionData *bind_data_p,
340: 	                                                          const vector<column_t> &column_ids,
341: 	                                                          TableFilterCollection *filters) {
342: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
343: 		auto result = make_unique<ParquetReadParallelState>();
344: 		result->current_reader = bind_data.initial_reader;
345: 		result->row_group_index = 0;
346: 		result->file_index = 0;
347: 		return move(result);
348: 	}
349: 
350: 	static bool ParquetParallelStateNext(ClientContext &context, const FunctionData *bind_data_p,
351: 	                                     FunctionOperatorData *state_p, ParallelState *parallel_state_p) {
352: 		if (!state_p) {
353: 			return false;
354: 		}
355: 		auto &bind_data = (ParquetReadBindData &)*bind_data_p;
356: 		auto &parallel_state = (ParquetReadParallelState &)*parallel_state_p;
357: 		auto &scan_data = (ParquetReadOperatorData &)*state_p;
358: 
359: 		lock_guard<mutex> parallel_lock(parallel_state.lock);
360: 		if (parallel_state.row_group_index < parallel_state.current_reader->NumRowGroups()) {
361: 			// groups remain in the current parquet file: read the next group
362: 			scan_data.reader = parallel_state.current_reader;
363: 			vector<idx_t> group_indexes {parallel_state.row_group_index};
364: 			scan_data.reader->InitializeScan(scan_data.scan_state, scan_data.column_ids, group_indexes,
365: 			                                 scan_data.table_filters);
366: 			parallel_state.row_group_index++;
367: 			return true;
368: 		} else {
369: 			// no groups remain in the current parquet file: check if there are more files to read
370: 			while (parallel_state.file_index + 1 < bind_data.files.size()) {
371: 				// read the next file
372: 				string file = bind_data.files[++parallel_state.file_index];
373: 				parallel_state.current_reader =
374: 				    make_shared<ParquetReader>(context, file, parallel_state.current_reader->return_types,
375: 				                               parallel_state.current_reader->parquet_options);
376: 				if (parallel_state.current_reader->NumRowGroups() == 0) {
377: 					// empty parquet file, move to next file
378: 					continue;
379: 				}
380: 				// set up the scan state to read the first group
381: 				scan_data.reader = parallel_state.current_reader;
382: 				vector<idx_t> group_indexes {0};
383: 				scan_data.reader->InitializeScan(scan_data.scan_state, scan_data.column_ids, group_indexes,
384: 				                                 scan_data.table_filters);
385: 				parallel_state.row_group_index = 1;
386: 				return true;
387: 			}
388: 		}
389: 		return false;
390: 	}
391: };
392: 
393: struct ParquetWriteBindData : public FunctionData {
394: 	vector<LogicalType> sql_types;
395: 	string file_name;
396: 	vector<string> column_names;
397: 	duckdb_parquet::format::CompressionCodec::type codec = duckdb_parquet::format::CompressionCodec::SNAPPY;
398: 	idx_t row_group_size = 100000;
399: };
400: 
401: struct ParquetWriteGlobalState : public GlobalFunctionData {
402: 	unique_ptr<ParquetWriter> writer;
403: };
404: 
405: struct ParquetWriteLocalState : public LocalFunctionData {
406: 	ParquetWriteLocalState() {
407: 		buffer = make_unique<ChunkCollection>();
408: 	}
409: 
410: 	unique_ptr<ChunkCollection> buffer;
411: };
412: 
413: unique_ptr<FunctionData> ParquetWriteBind(ClientContext &context, CopyInfo &info, vector<string> &names,
414:                                           vector<LogicalType> &sql_types) {
415: 	auto bind_data = make_unique<ParquetWriteBindData>();
416: 	for (auto &option : info.options) {
417: 		auto loption = StringUtil::Lower(option.first);
418: 		if (loption == "row_group_size" || loption == "chunk_size") {
419: 			bind_data->row_group_size = option.second[0].GetValue<uint64_t>();
420: 		} else if (loption == "compression" || loption == "codec") {
421: 			if (!option.second.empty()) {
422: 				auto roption = StringUtil::Lower(option.second[0].ToString());
423: 				if (roption == "uncompressed") {
424: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::UNCOMPRESSED;
425: 					continue;
426: 				} else if (roption == "snappy") {
427: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::SNAPPY;
428: 					continue;
429: 				} else if (roption == "gzip") {
430: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::GZIP;
431: 					continue;
432: 				} else if (roption == "zstd") {
433: 					bind_data->codec = duckdb_parquet::format::CompressionCodec::ZSTD;
434: 					continue;
435: 				}
436: 			}
437: 			throw ParserException("Expected %s argument to be either [uncompressed, snappy, gzip or zstd]", loption);
438: 		} else {
439: 			throw NotImplementedException("Unrecognized option for PARQUET: %s", option.first.c_str());
440: 		}
441: 	}
442: 	bind_data->sql_types = sql_types;
443: 	bind_data->column_names = names;
444: 	bind_data->file_name = info.file_path;
445: 	return move(bind_data);
446: }
447: 
448: unique_ptr<GlobalFunctionData> ParquetWriteInitializeGlobal(ClientContext &context, FunctionData &bind_data,
449:                                                             const string &file_path) {
450: 	auto global_state = make_unique<ParquetWriteGlobalState>();
451: 	auto &parquet_bind = (ParquetWriteBindData &)bind_data;
452: 
453: 	auto &fs = FileSystem::GetFileSystem(context);
454: 	global_state->writer =
455: 	    make_unique<ParquetWriter>(fs, file_path, FileSystem::GetFileOpener(context), parquet_bind.sql_types,
456: 	                               parquet_bind.column_names, parquet_bind.codec);
457: 	return move(global_state);
458: }
459: 
460: void ParquetWriteSink(ClientContext &context, FunctionData &bind_data_p, GlobalFunctionData &gstate,
461:                       LocalFunctionData &lstate, DataChunk &input) {
462: 	auto &bind_data = (ParquetWriteBindData &)bind_data_p;
463: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
464: 	auto &local_state = (ParquetWriteLocalState &)lstate;
465: 
466: 	// append data to the local (buffered) chunk collection
467: 	local_state.buffer->Append(input);
468: 	if (local_state.buffer->Count() > bind_data.row_group_size) {
469: 		// if the chunk collection exceeds a certain size we flush it to the parquet file
470: 		global_state.writer->Flush(*local_state.buffer);
471: 		// and reset the buffer
472: 		local_state.buffer = make_unique<ChunkCollection>();
473: 	}
474: }
475: 
476: void ParquetWriteCombine(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate,
477:                          LocalFunctionData &lstate) {
478: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
479: 	auto &local_state = (ParquetWriteLocalState &)lstate;
480: 	// flush any data left in the local state to the file
481: 	global_state.writer->Flush(*local_state.buffer);
482: }
483: 
484: void ParquetWriteFinalize(ClientContext &context, FunctionData &bind_data, GlobalFunctionData &gstate) {
485: 	auto &global_state = (ParquetWriteGlobalState &)gstate;
486: 	// finalize: write any additional metadata to the file here
487: 	global_state.writer->Finalize();
488: }
489: 
490: unique_ptr<LocalFunctionData> ParquetWriteInitializeLocal(ClientContext &context, FunctionData &bind_data) {
491: 	return make_unique<ParquetWriteLocalState>();
492: }
493: 
494: unique_ptr<TableFunctionRef> ParquetScanReplacement(const string &table_name, void *data) {
495: 	if (!StringUtil::EndsWith(StringUtil::Lower(table_name), ".parquet")) {
496: 		return nullptr;
497: 	}
498: 	auto table_function = make_unique<TableFunctionRef>();
499: 	vector<unique_ptr<ParsedExpression>> children;
500: 	children.push_back(make_unique<ConstantExpression>(Value(table_name)));
501: 	table_function->function = make_unique<FunctionExpression>("parquet_scan", move(children));
502: 	return table_function;
503: }
504: 
505: void ParquetExtension::Load(DuckDB &db) {
506: 	auto &fs = db.GetFileSystem();
507: 	fs.RegisterSubSystem(FileCompressionType::ZSTD, make_unique<ZStdFileSystem>());
508: 
509: 	auto scan_fun = ParquetScanFunction::GetFunctionSet();
510: 	CreateTableFunctionInfo cinfo(scan_fun);
511: 	cinfo.name = "read_parquet";
512: 	CreateTableFunctionInfo pq_scan = cinfo;
513: 	pq_scan.name = "parquet_scan";
514: 
515: 	ParquetMetaDataFunction meta_fun;
516: 	CreateTableFunctionInfo meta_cinfo(meta_fun);
517: 
518: 	ParquetSchemaFunction schema_fun;
519: 	CreateTableFunctionInfo schema_cinfo(schema_fun);
520: 
521: 	CopyFunction function("parquet");
522: 	function.copy_to_bind = ParquetWriteBind;
523: 	function.copy_to_initialize_global = ParquetWriteInitializeGlobal;
524: 	function.copy_to_initialize_local = ParquetWriteInitializeLocal;
525: 	function.copy_to_sink = ParquetWriteSink;
526: 	function.copy_to_combine = ParquetWriteCombine;
527: 	function.copy_to_finalize = ParquetWriteFinalize;
528: 	function.copy_from_bind = ParquetScanFunction::ParquetReadBind;
529: 	function.copy_from_function = scan_fun.functions[0];
530: 
531: 	function.extension = "parquet";
532: 	CreateCopyFunctionInfo info(function);
533: 
534: 	Connection con(db);
535: 	con.BeginTransaction();
536: 	auto &context = *con.context;
537: 	auto &catalog = Catalog::GetCatalog(context);
538: 	catalog.CreateCopyFunction(context, &info);
539: 	catalog.CreateTableFunction(context, &cinfo);
540: 	catalog.CreateTableFunction(context, &pq_scan);
541: 	catalog.CreateTableFunction(context, &meta_cinfo);
542: 	catalog.CreateTableFunction(context, &schema_cinfo);
543: 	con.Commit();
544: 
545: 	auto &config = DBConfig::GetConfig(*db.instance);
546: 	config.replacement_scans.emplace_back(ParquetScanReplacement);
547: 	config.AddExtensionOption("binary_as_string", "In Parquet files, interpret binary data as a string.",
548: 	                          LogicalType::BOOLEAN);
549: }
550: 
551: std::string ParquetExtension::Name() {
552: 	return "parquet";
553: }
554: 
555: } // namespace duckdb
556: 
557: #ifndef DUCKDB_EXTENSION_MAIN
558: #error DUCKDB_EXTENSION_MAIN not defined
559: #endif
[end of extension/parquet/parquet-extension.cpp]
[start of extension/parquet/parquet_reader.cpp]
1: #include "parquet_reader.hpp"
2: #include "parquet_timestamp.hpp"
3: #include "parquet_statistics.hpp"
4: #include "column_reader.hpp"
5: 
6: #include "boolean_column_reader.hpp"
7: #include "callback_column_reader.hpp"
8: #include "list_column_reader.hpp"
9: #include "string_column_reader.hpp"
10: #include "struct_column_reader.hpp"
11: #include "templated_column_reader.hpp"
12: 
13: #include "thrift_tools.hpp"
14: 
15: #include "parquet_file_metadata_cache.hpp"
16: 
17: #include "duckdb.hpp"
18: #ifndef DUCKDB_AMALGAMATION
19: #include "duckdb/planner/table_filter.hpp"
20: #include "duckdb/planner/filter/constant_filter.hpp"
21: #include "duckdb/planner/filter/null_filter.hpp"
22: #include "duckdb/planner/filter/conjunction_filter.hpp"
23: #include "duckdb/common/file_system.hpp"
24: #include "duckdb/common/string_util.hpp"
25: #include "duckdb/common/types/date.hpp"
26: #include "duckdb/common/pair.hpp"
27: 
28: #include "duckdb/storage/object_cache.hpp"
29: #endif
30: 
31: #include <sstream>
32: #include <cassert>
33: #include <chrono>
34: #include <cstring>
35: #include <iostream>
36: 
37: namespace duckdb {
38: 
39: using duckdb_parquet::format::ColumnChunk;
40: using duckdb_parquet::format::ConvertedType;
41: using duckdb_parquet::format::FieldRepetitionType;
42: using duckdb_parquet::format::FileMetaData;
43: using ParquetRowGroup = duckdb_parquet::format::RowGroup;
44: using duckdb_parquet::format::SchemaElement;
45: using duckdb_parquet::format::Statistics;
46: using duckdb_parquet::format::Type;
47: 
48: static unique_ptr<duckdb_apache::thrift::protocol::TProtocol> CreateThriftProtocol(Allocator &allocator,
49:                                                                                    FileHandle &file_handle) {
50: 	auto transport = make_shared<ThriftFileTransport>(allocator, file_handle);
51: 	return make_unique<duckdb_apache::thrift::protocol::TCompactProtocolT<ThriftFileTransport>>(move(transport));
52: }
53: 
54: static shared_ptr<ParquetFileMetadataCache> LoadMetadata(Allocator &allocator, FileHandle &file_handle) {
55: 	auto current_time = std::chrono::system_clock::to_time_t(std::chrono::system_clock::now());
56: 
57: 	auto proto = CreateThriftProtocol(allocator, file_handle);
58: 	auto &transport = ((ThriftFileTransport &)*proto->getTransport());
59: 	auto file_size = transport.GetSize();
60: 	if (file_size < 12) {
61: 		throw InvalidInputException("File '%s' too small to be a Parquet file", file_handle.path);
62: 	}
63: 
64: 	ResizeableBuffer buf;
65: 	buf.resize(allocator, 8);
66: 	buf.zero();
67: 
68: 	transport.SetLocation(file_size - 8);
69: 	transport.read((uint8_t *)buf.ptr, 8);
70: 
71: 	if (strncmp(buf.ptr + 4, "PAR1", 4) != 0) {
72: 		throw InvalidInputException("No magic bytes found at end of file '%s'", file_handle.path);
73: 	}
74: 	// read four-byte footer length from just before the end magic bytes
75: 	auto footer_len = *(uint32_t *)buf.ptr;
76: 	if (footer_len <= 0 || file_size < 12 + footer_len) {
77: 		throw InvalidInputException("Footer length error in file '%s'", file_handle.path);
78: 	}
79: 	auto metadata_pos = file_size - (footer_len + 8);
80: 	transport.SetLocation(metadata_pos);
81: 	transport.Prefetch(metadata_pos, footer_len);
82: 
83: 	auto metadata = make_unique<FileMetaData>();
84: 	metadata->read(proto.get());
85: 	return make_shared<ParquetFileMetadataCache>(move(metadata), current_time);
86: }
87: 
88: LogicalType ParquetReader::DeriveLogicalType(const SchemaElement &s_ele, bool binary_as_string) {
89: 	// inner node
90: 	D_ASSERT(s_ele.__isset.type && s_ele.num_children == 0);
91: 	if (s_ele.type == Type::FIXED_LEN_BYTE_ARRAY && !s_ele.__isset.type_length) {
92: 		throw IOException("FIXED_LEN_BYTE_ARRAY requires length to be set");
93: 	}
94: 	if (s_ele.type == Type::FIXED_LEN_BYTE_ARRAY && s_ele.__isset.logicalType && s_ele.logicalType.__isset.UUID) {
95: 		return LogicalType::UUID;
96: 	}
97: 	if (s_ele.__isset.converted_type) {
98: 		switch (s_ele.converted_type) {
99: 		case ConvertedType::INT_8:
100: 			if (s_ele.type == Type::INT32) {
101: 				return LogicalType::TINYINT;
102: 			} else {
103: 				throw IOException("INT8 converted type can only be set for value of Type::INT32");
104: 			}
105: 		case ConvertedType::INT_16:
106: 			if (s_ele.type == Type::INT32) {
107: 				return LogicalType::SMALLINT;
108: 			} else {
109: 				throw IOException("INT16 converted type can only be set for value of Type::INT32");
110: 			}
111: 		case ConvertedType::INT_32:
112: 			if (s_ele.type == Type::INT32) {
113: 				return LogicalType::INTEGER;
114: 			} else {
115: 				throw IOException("INT32 converted type can only be set for value of Type::INT32");
116: 			}
117: 		case ConvertedType::INT_64:
118: 			if (s_ele.type == Type::INT64) {
119: 				return LogicalType::BIGINT;
120: 			} else {
121: 				throw IOException("INT64 converted type can only be set for value of Type::INT32");
122: 			}
123: 		case ConvertedType::UINT_8:
124: 			if (s_ele.type == Type::INT32) {
125: 				return LogicalType::UTINYINT;
126: 			} else {
127: 				throw IOException("UINT8 converted type can only be set for value of Type::INT32");
128: 			}
129: 		case ConvertedType::UINT_16:
130: 			if (s_ele.type == Type::INT32) {
131: 				return LogicalType::USMALLINT;
132: 			} else {
133: 				throw IOException("UINT16 converted type can only be set for value of Type::INT32");
134: 			}
135: 		case ConvertedType::UINT_32:
136: 			if (s_ele.type == Type::INT32) {
137: 				return LogicalType::UINTEGER;
138: 			} else {
139: 				throw IOException("UINT32 converted type can only be set for value of Type::INT32");
140: 			}
141: 		case ConvertedType::UINT_64:
142: 			if (s_ele.type == Type::INT64) {
143: 				return LogicalType::UBIGINT;
144: 			} else {
145: 				throw IOException("UINT64 converted type can only be set for value of Type::INT64");
146: 			}
147: 		case ConvertedType::DATE:
148: 			if (s_ele.type == Type::INT32) {
149: 				return LogicalType::DATE;
150: 			} else {
151: 				throw IOException("DATE converted type can only be set for value of Type::INT32");
152: 			}
153: 		case ConvertedType::TIMESTAMP_MICROS:
154: 		case ConvertedType::TIMESTAMP_MILLIS:
155: 			if (s_ele.type == Type::INT64) {
156: 				return LogicalType::TIMESTAMP;
157: 			} else {
158: 				throw IOException("TIMESTAMP converted type can only be set for value of Type::INT64");
159: 			}
160: 		case ConvertedType::DECIMAL:
161: 			if (!s_ele.__isset.precision || !s_ele.__isset.scale) {
162: 				throw IOException("DECIMAL requires a length and scale specifier!");
163: 			}
164: 			switch (s_ele.type) {
165: 			case Type::BYTE_ARRAY:
166: 			case Type::FIXED_LEN_BYTE_ARRAY:
167: 			case Type::INT32:
168: 			case Type::INT64:
169: 				return LogicalType::DECIMAL(s_ele.precision, s_ele.scale);
170: 			default:
171: 				throw IOException(
172: 				    "DECIMAL converted type can only be set for value of Type::(FIXED_LEN_)BYTE_ARRAY/INT32/INT64");
173: 			}
174: 		case ConvertedType::UTF8:
175: 			switch (s_ele.type) {
176: 			case Type::BYTE_ARRAY:
177: 			case Type::FIXED_LEN_BYTE_ARRAY:
178: 				return LogicalType::VARCHAR;
179: 			default:
180: 				throw IOException("UTF8 converted type can only be set for Type::(FIXED_LEN_)BYTE_ARRAY");
181: 			}
182: 		case ConvertedType::TIME_MILLIS:
183: 		case ConvertedType::TIME_MICROS:
184: 			if (s_ele.type == Type::INT64) {
185: 				return LogicalType::TIME;
186: 			} else {
187: 				throw IOException("TIME_MICROS converted type can only be set for value of Type::INT64");
188: 			}
189: 		case ConvertedType::INTERVAL:
190: 			return LogicalType::INTERVAL;
191: 		case ConvertedType::MAP:
192: 		case ConvertedType::MAP_KEY_VALUE:
193: 		case ConvertedType::LIST:
194: 		case ConvertedType::ENUM:
195: 		case ConvertedType::JSON:
196: 		case ConvertedType::BSON:
197: 		default:
198: 			throw IOException("Unsupported converted type");
199: 		}
200: 	} else {
201: 		// no converted type set
202: 		// use default type for each physical type
203: 		switch (s_ele.type) {
204: 		case Type::BOOLEAN:
205: 			return LogicalType::BOOLEAN;
206: 		case Type::INT32:
207: 			return LogicalType::INTEGER;
208: 		case Type::INT64:
209: 			return LogicalType::BIGINT;
210: 		case Type::INT96: // always a timestamp it would seem
211: 			return LogicalType::TIMESTAMP;
212: 		case Type::FLOAT:
213: 			return LogicalType::FLOAT;
214: 		case Type::DOUBLE:
215: 			return LogicalType::DOUBLE;
216: 		case Type::BYTE_ARRAY:
217: 		case Type::FIXED_LEN_BYTE_ARRAY:
218: 			if (binary_as_string) {
219: 				return LogicalType::VARCHAR;
220: 			}
221: 			return LogicalType::BLOB;
222: 		default:
223: 			return LogicalType::INVALID;
224: 		}
225: 	}
226: }
227: 
228: LogicalType ParquetReader::DeriveLogicalType(const SchemaElement &s_ele) {
229: 	return DeriveLogicalType(s_ele, parquet_options.binary_as_string);
230: }
231: 
232: unique_ptr<ColumnReader> ParquetReader::CreateReaderRecursive(const FileMetaData *file_meta_data, idx_t depth,
233:                                                               idx_t max_define, idx_t max_repeat,
234:                                                               idx_t &next_schema_idx, idx_t &next_file_idx) {
235: 	D_ASSERT(file_meta_data);
236: 	D_ASSERT(next_schema_idx < file_meta_data->schema.size());
237: 	auto &s_ele = file_meta_data->schema[next_schema_idx];
238: 	auto this_idx = next_schema_idx;
239: 
240: 	if (s_ele.__isset.repetition_type) {
241: 		if (s_ele.repetition_type != FieldRepetitionType::REQUIRED) {
242: 			max_define++;
243: 		}
244: 		if (s_ele.repetition_type == FieldRepetitionType::REPEATED) {
245: 			max_repeat++;
246: 		}
247: 	}
248: 
249: 	if (!s_ele.__isset.type) { // inner node
250: 		if (s_ele.num_children == 0) {
251: 			throw std::runtime_error("Node has no children but should");
252: 		}
253: 		child_list_t<LogicalType> child_types;
254: 		vector<unique_ptr<ColumnReader>> child_readers;
255: 
256: 		idx_t c_idx = 0;
257: 		while (c_idx < (idx_t)s_ele.num_children) {
258: 			next_schema_idx++;
259: 
260: 			auto &child_ele = file_meta_data->schema[next_schema_idx];
261: 
262: 			auto child_reader = CreateReaderRecursive(file_meta_data, depth + 1, max_define, max_repeat,
263: 			                                          next_schema_idx, next_file_idx);
264: 			child_types.push_back(make_pair(child_ele.name, child_reader->Type()));
265: 			child_readers.push_back(move(child_reader));
266: 
267: 			c_idx++;
268: 		}
269: 		D_ASSERT(!child_types.empty());
270: 		unique_ptr<ColumnReader> result;
271: 		LogicalType result_type;
272: 
273: 		bool is_repeated = s_ele.repetition_type == FieldRepetitionType::REPEATED;
274: 		bool is_list = s_ele.__isset.converted_type && s_ele.converted_type == ConvertedType::LIST;
275: 		bool is_map = s_ele.__isset.converted_type && s_ele.converted_type == ConvertedType::MAP;
276: 		bool is_map_kv = s_ele.__isset.converted_type && s_ele.converted_type == ConvertedType::MAP_KEY_VALUE;
277: 		if (!is_map_kv && this_idx > 0) {
278: 			// check if the parent node of this is a map
279: 			auto &p_ele = file_meta_data->schema[this_idx - 1];
280: 			bool parent_is_map = p_ele.__isset.converted_type && p_ele.converted_type == ConvertedType::MAP;
281: 			bool parent_has_children = p_ele.__isset.num_children && p_ele.num_children == 1;
282: 			is_map_kv = parent_is_map && parent_has_children;
283: 		}
284: 
285: 		if (is_map_kv) {
286: 			if (child_types.size() != 2) {
287: 				throw IOException("MAP_KEY_VALUE requires two children");
288: 			}
289: 			if (!is_repeated) {
290: 				throw IOException("MAP_KEY_VALUE needs to be repeated");
291: 			}
292: 			result_type = LogicalType::MAP(move(child_types[0].second), move(child_types[1].second));
293: 			for (auto &child_reader : child_readers) {
294: 				auto child_type = LogicalType::LIST(child_reader->Type());
295: 				child_reader = make_unique<ListColumnReader>(*this, move(child_type), s_ele, this_idx, max_define,
296: 				                                             max_repeat, move(child_reader));
297: 			}
298: 			result = make_unique<StructColumnReader>(*this, result_type, s_ele, this_idx, max_define - 1,
299: 			                                         max_repeat - 1, move(child_readers));
300: 			return result;
301: 		}
302: 		if (child_types.size() > 1 || (!is_list && !is_map && !is_repeated)) {
303: 			result_type = LogicalType::STRUCT(move(child_types));
304: 			result = make_unique<StructColumnReader>(*this, result_type, s_ele, this_idx, max_define, max_repeat,
305: 			                                         move(child_readers));
306: 		} else {
307: 			// if we have a struct with only a single type, pull up
308: 			result_type = child_types[0].second;
309: 			result = move(child_readers[0]);
310: 		}
311: 		if (is_repeated) {
312: 			result_type = LogicalType::LIST(result_type);
313: 			return make_unique<ListColumnReader>(*this, result_type, s_ele, this_idx, max_define, max_repeat,
314: 			                                     move(result));
315: 		}
316: 		return result;
317: 	} else { // leaf node
318: 		if (s_ele.repetition_type == FieldRepetitionType::REPEATED) {
319: 			const auto derived_type = DeriveLogicalType(s_ele);
320: 			auto list_type = LogicalType::LIST(derived_type);
321: 
322: 			auto element_reader =
323: 			    ColumnReader::CreateReader(*this, derived_type, s_ele, next_file_idx++, max_define, max_repeat);
324: 
325: 			return make_unique<ListColumnReader>(*this, list_type, s_ele, this_idx, max_define, max_repeat,
326: 			                                     move(element_reader));
327: 		}
328: 
329: 		// TODO check return value of derive type or should we only do this on read()
330: 		return ColumnReader::CreateReader(*this, DeriveLogicalType(s_ele), s_ele, next_file_idx++, max_define,
331: 		                                  max_repeat);
332: 	}
333: }
334: 
335: // TODO we don't need readers for columns we are not going to read ay
336: unique_ptr<ColumnReader> ParquetReader::CreateReader(const duckdb_parquet::format::FileMetaData *file_meta_data) {
337: 	idx_t next_schema_idx = 0;
338: 	idx_t next_file_idx = 0;
339: 
340: 	auto ret = CreateReaderRecursive(file_meta_data, 0, 0, 0, next_schema_idx, next_file_idx);
341: 	D_ASSERT(next_schema_idx == file_meta_data->schema.size() - 1);
342: 	D_ASSERT(file_meta_data->row_groups.empty() || next_file_idx == file_meta_data->row_groups[0].columns.size());
343: 	return ret;
344: }
345: 
346: void ParquetReader::InitializeSchema(const vector<LogicalType> &expected_types_p, const string &initial_filename_p) {
347: 	auto file_meta_data = GetFileMetadata();
348: 
349: 	if (file_meta_data->__isset.encryption_algorithm) {
350: 		throw FormatException("Encrypted Parquet files are not supported");
351: 	}
352: 	// check if we like this schema
353: 	if (file_meta_data->schema.size() < 2) {
354: 		throw FormatException("Need at least one non-root column in the file");
355: 	}
356: 
357: 	bool has_expected_types = !expected_types_p.empty();
358: 	auto root_reader = CreateReader(file_meta_data);
359: 
360: 	auto &root_type = root_reader->Type();
361: 	auto &child_types = StructType::GetChildTypes(root_type);
362: 	D_ASSERT(root_type.id() == LogicalTypeId::STRUCT);
363: 	if (has_expected_types && child_types.size() != expected_types_p.size()) {
364: 		throw FormatException("column count mismatch");
365: 	}
366: 	idx_t col_idx = 0;
367: 	for (auto &type_pair : child_types) {
368: 		if (has_expected_types && expected_types_p[col_idx] != type_pair.second) {
369: 			if (initial_filename_p.empty()) {
370: 				throw FormatException("column \"%d\" in parquet file is of type %s, could not auto cast to "
371: 				                      "expected type %s for this column",
372: 				                      col_idx, type_pair.second, expected_types_p[col_idx].ToString());
373: 			} else {
374: 				throw FormatException("schema mismatch in Parquet glob: column \"%d\" in parquet file is of type "
375: 				                      "%s, but in the original file \"%s\" this column is of type \"%s\"",
376: 				                      col_idx, type_pair.second, initial_filename_p,
377: 				                      expected_types_p[col_idx].ToString());
378: 			}
379: 		} else {
380: 			names.push_back(type_pair.first);
381: 			return_types.push_back(type_pair.second);
382: 		}
383: 		col_idx++;
384: 	}
385: 	D_ASSERT(!names.empty());
386: 	D_ASSERT(!return_types.empty());
387: }
388: 
389: ParquetOptions::ParquetOptions(ClientContext &context) {
390: 	Value binary_as_string_val;
391: 	if (context.TryGetCurrentSetting("binary_as_string", binary_as_string_val)) {
392: 		binary_as_string = binary_as_string_val.GetValue<bool>();
393: 	}
394: }
395: 
396: ParquetReader::ParquetReader(Allocator &allocator_p, unique_ptr<FileHandle> file_handle_p,
397:                              const vector<LogicalType> &expected_types_p, const string &initial_filename_p)
398:     : allocator(allocator_p) {
399: 	file_name = file_handle_p->path;
400: 	file_handle = move(file_handle_p);
401: 	metadata = LoadMetadata(allocator, *file_handle);
402: 	InitializeSchema(expected_types_p, initial_filename_p);
403: }
404: 
405: ParquetReader::ParquetReader(ClientContext &context_p, string file_name_p, const vector<LogicalType> &expected_types_p,
406:                              ParquetOptions parquet_options_p, const string &initial_filename_p)
407:     : allocator(Allocator::Get(context_p)), file_opener(FileSystem::GetFileOpener(context_p)),
408:       parquet_options(parquet_options_p) {
409: 	auto &fs = FileSystem::GetFileSystem(context_p);
410: 	file_name = move(file_name_p);
411: 	file_handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK,
412: 	                          FileSystem::DEFAULT_COMPRESSION, file_opener);
413: 	if (!file_handle->CanSeek()) {
414: 		throw NotImplementedException(
415: 		    "Reading parquet files from a FIFO stream is not supported and cannot be efficiently supported since "
416: 		    "metadata is located at the end of the file. Write the stream to disk first and read from there instead.");
417: 	}
418: 	// If object cached is disabled
419: 	// or if this file has cached metadata
420: 	// or if the cached version already expired
421: 	auto last_modify_time = fs.GetLastModifiedTime(*file_handle);
422: 	if (!ObjectCache::ObjectCacheEnabled(context_p)) {
423: 		metadata = LoadMetadata(allocator, *file_handle);
424: 	} else {
425: 		metadata = ObjectCache::GetObjectCache(context_p).Get<ParquetFileMetadataCache>(file_name);
426: 		if (!metadata || (last_modify_time + 10 >= metadata->read_time)) {
427: 			metadata = LoadMetadata(allocator, *file_handle);
428: 			ObjectCache::GetObjectCache(context_p).Put(file_name, metadata);
429: 		}
430: 	}
431: 	InitializeSchema(expected_types_p, initial_filename_p);
432: }
433: 
434: ParquetReader::~ParquetReader() {
435: }
436: 
437: const FileMetaData *ParquetReader::GetFileMetadata() {
438: 	D_ASSERT(metadata);
439: 	D_ASSERT(metadata->metadata);
440: 	return metadata->metadata.get();
441: }
442: 
443: // TODO also somewhat ugly, perhaps this can be moved to the column reader too
444: unique_ptr<BaseStatistics> ParquetReader::ReadStatistics(ParquetReader &reader, LogicalType &type,
445:                                                          column_t file_col_idx, const FileMetaData *file_meta_data) {
446: 	unique_ptr<BaseStatistics> column_stats;
447: 	auto root_reader = reader.CreateReader(file_meta_data);
448: 	auto column_reader = ((StructColumnReader *)root_reader.get())->GetChildReader(file_col_idx);
449: 
450: 	for (auto &row_group : file_meta_data->row_groups) {
451: 		auto chunk_stats = column_reader->Stats(row_group.columns);
452: 		if (!chunk_stats) {
453: 			return nullptr;
454: 		}
455: 		if (!column_stats) {
456: 			column_stats = move(chunk_stats);
457: 		} else {
458: 			column_stats->Merge(*chunk_stats);
459: 		}
460: 	}
461: 	return column_stats;
462: }
463: 
464: const ParquetRowGroup &ParquetReader::GetGroup(ParquetReaderScanState &state) {
465: 	auto file_meta_data = GetFileMetadata();
466: 	D_ASSERT(state.current_group >= 0 && (idx_t)state.current_group < state.group_idx_list.size());
467: 	D_ASSERT(state.group_idx_list[state.current_group] >= 0 &&
468: 	         state.group_idx_list[state.current_group] < file_meta_data->row_groups.size());
469: 	return file_meta_data->row_groups[state.group_idx_list[state.current_group]];
470: }
471: 
472: void ParquetReader::PrepareRowGroupBuffer(ParquetReaderScanState &state, idx_t out_col_idx) {
473: 	auto &group = GetGroup(state);
474: 
475: 	auto column_reader = ((StructColumnReader *)state.root_reader.get())->GetChildReader(state.column_ids[out_col_idx]);
476: 
477: 	// TODO move this to columnreader too
478: 	if (state.filters) {
479: 		auto stats = column_reader->Stats(group.columns);
480: 		// filters contain output chunk index, not file col idx!
481: 		auto filter_entry = state.filters->filters.find(out_col_idx);
482: 		if (stats && filter_entry != state.filters->filters.end()) {
483: 			bool skip_chunk = false;
484: 			auto &filter = *filter_entry->second;
485: 			auto prune_result = filter.CheckStatistics(*stats);
486: 			if (prune_result == FilterPropagateResult::FILTER_ALWAYS_FALSE) {
487: 				skip_chunk = true;
488: 			}
489: 			if (skip_chunk) {
490: 				state.group_offset = group.num_rows;
491: 				return;
492: 				// this effectively will skip this chunk
493: 			}
494: 		}
495: 	}
496: 
497: 	state.root_reader->InitializeRead(group.columns, *state.thrift_file_proto);
498: }
499: 
500: idx_t ParquetReader::NumRows() {
501: 	return GetFileMetadata()->num_rows;
502: }
503: 
504: idx_t ParquetReader::NumRowGroups() {
505: 	return GetFileMetadata()->row_groups.size();
506: }
507: 
508: void ParquetReader::InitializeScan(ParquetReaderScanState &state, vector<column_t> column_ids,
509:                                    vector<idx_t> groups_to_read, TableFilterSet *filters) {
510: 	state.current_group = -1;
511: 	state.finished = false;
512: 	state.column_ids = move(column_ids);
513: 	state.group_offset = 0;
514: 	state.group_idx_list = move(groups_to_read);
515: 	state.filters = filters;
516: 	state.sel.Initialize(STANDARD_VECTOR_SIZE);
517: 
518: 	state.file_handle =
519: 	    file_handle->file_system.OpenFile(file_handle->path, FileFlags::FILE_FLAGS_READ, FileSystem::DEFAULT_LOCK,
520: 	                                      FileSystem::DEFAULT_COMPRESSION, file_opener);
521: 
522: 	state.thrift_file_proto = CreateThriftProtocol(allocator, *state.file_handle);
523: 	state.root_reader = CreateReader(GetFileMetadata());
524: 
525: 	state.define_buf.resize(allocator, STANDARD_VECTOR_SIZE);
526: 	state.repeat_buf.resize(allocator, STANDARD_VECTOR_SIZE);
527: }
528: 
529: void FilterIsNull(Vector &v, parquet_filter_t &filter_mask, idx_t count) {
530: 	auto &mask = FlatVector::Validity(v);
531: 	if (mask.AllValid()) {
532: 		filter_mask.reset();
533: 	} else {
534: 		for (idx_t i = 0; i < count; i++) {
535: 			filter_mask[i] = filter_mask[i] && !mask.RowIsValid(i);
536: 		}
537: 	}
538: }
539: 
540: void FilterIsNotNull(Vector &v, parquet_filter_t &filter_mask, idx_t count) {
541: 	auto &mask = FlatVector::Validity(v);
542: 	if (!mask.AllValid()) {
543: 		for (idx_t i = 0; i < count; i++) {
544: 			filter_mask[i] = filter_mask[i] && mask.RowIsValid(i);
545: 		}
546: 	}
547: }
548: 
549: template <class T, class OP>
550: void TemplatedFilterOperation(Vector &v, T constant, parquet_filter_t &filter_mask, idx_t count) {
551: 	D_ASSERT(v.GetVectorType() == VectorType::FLAT_VECTOR); // we just created the damn thing it better be
552: 
553: 	auto v_ptr = FlatVector::GetData<T>(v);
554: 	auto &mask = FlatVector::Validity(v);
555: 
556: 	if (!mask.AllValid()) {
557: 		for (idx_t i = 0; i < count; i++) {
558: 			if (mask.RowIsValid(i)) {
559: 				filter_mask[i] = filter_mask[i] && OP::Operation(v_ptr[i], constant);
560: 			}
561: 		}
562: 	} else {
563: 		for (idx_t i = 0; i < count; i++) {
564: 			filter_mask[i] = filter_mask[i] && OP::Operation(v_ptr[i], constant);
565: 		}
566: 	}
567: }
568: 
569: template <class T, class OP>
570: void TemplatedFilterOperation(Vector &v, const Value &constant, parquet_filter_t &filter_mask, idx_t count) {
571: 	TemplatedFilterOperation<T, OP>(v, constant.template GetValueUnsafe<T>(), filter_mask, count);
572: }
573: 
574: template <class OP>
575: static void FilterOperationSwitch(Vector &v, Value &constant, parquet_filter_t &filter_mask, idx_t count) {
576: 	if (filter_mask.none() || count == 0) {
577: 		return;
578: 	}
579: 	switch (v.GetType().InternalType()) {
580: 	case PhysicalType::BOOL:
581: 		TemplatedFilterOperation<bool, OP>(v, constant, filter_mask, count);
582: 		break;
583: 	case PhysicalType::UINT8:
584: 		TemplatedFilterOperation<uint8_t, OP>(v, constant, filter_mask, count);
585: 		break;
586: 	case PhysicalType::UINT16:
587: 		TemplatedFilterOperation<uint16_t, OP>(v, constant, filter_mask, count);
588: 		break;
589: 	case PhysicalType::UINT32:
590: 		TemplatedFilterOperation<uint32_t, OP>(v, constant, filter_mask, count);
591: 		break;
592: 	case PhysicalType::UINT64:
593: 		TemplatedFilterOperation<uint64_t, OP>(v, constant, filter_mask, count);
594: 		break;
595: 	case PhysicalType::INT8:
596: 		TemplatedFilterOperation<int8_t, OP>(v, constant, filter_mask, count);
597: 		break;
598: 	case PhysicalType::INT16:
599: 		TemplatedFilterOperation<int16_t, OP>(v, constant, filter_mask, count);
600: 		break;
601: 	case PhysicalType::INT32:
602: 		TemplatedFilterOperation<int32_t, OP>(v, constant, filter_mask, count);
603: 		break;
604: 	case PhysicalType::INT64:
605: 		TemplatedFilterOperation<int64_t, OP>(v, constant, filter_mask, count);
606: 		break;
607: 	case PhysicalType::INT128:
608: 		TemplatedFilterOperation<hugeint_t, OP>(v, constant, filter_mask, count);
609: 		break;
610: 	case PhysicalType::FLOAT:
611: 		TemplatedFilterOperation<float, OP>(v, constant, filter_mask, count);
612: 		break;
613: 	case PhysicalType::DOUBLE:
614: 		TemplatedFilterOperation<double, OP>(v, constant, filter_mask, count);
615: 		break;
616: 	case PhysicalType::VARCHAR:
617: 		TemplatedFilterOperation<string_t, OP>(v, constant, filter_mask, count);
618: 		break;
619: 	default:
620: 		throw NotImplementedException("Unsupported type for filter %s", v.ToString());
621: 	}
622: }
623: 
624: static void ApplyFilter(Vector &v, TableFilter &filter, parquet_filter_t &filter_mask, idx_t count) {
625: 	switch (filter.filter_type) {
626: 	case TableFilterType::CONJUNCTION_AND: {
627: 		auto &conjunction = (ConjunctionAndFilter &)filter;
628: 		for (auto &child_filter : conjunction.child_filters) {
629: 			ApplyFilter(v, *child_filter, filter_mask, count);
630: 		}
631: 		break;
632: 	}
633: 	case TableFilterType::CONJUNCTION_OR: {
634: 		auto &conjunction = (ConjunctionOrFilter &)filter;
635: 		parquet_filter_t or_mask;
636: 		for (auto &child_filter : conjunction.child_filters) {
637: 			parquet_filter_t child_mask = filter_mask;
638: 			ApplyFilter(v, *child_filter, child_mask, count);
639: 			or_mask |= child_mask;
640: 		}
641: 		filter_mask &= or_mask;
642: 		break;
643: 	}
644: 	case TableFilterType::CONSTANT_COMPARISON: {
645: 		auto &constant_filter = (ConstantFilter &)filter;
646: 		switch (constant_filter.comparison_type) {
647: 		case ExpressionType::COMPARE_EQUAL:
648: 			FilterOperationSwitch<Equals>(v, constant_filter.constant, filter_mask, count);
649: 			break;
650: 		case ExpressionType::COMPARE_LESSTHAN:
651: 			FilterOperationSwitch<LessThan>(v, constant_filter.constant, filter_mask, count);
652: 			break;
653: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO:
654: 			FilterOperationSwitch<LessThanEquals>(v, constant_filter.constant, filter_mask, count);
655: 			break;
656: 		case ExpressionType::COMPARE_GREATERTHAN:
657: 			FilterOperationSwitch<GreaterThan>(v, constant_filter.constant, filter_mask, count);
658: 			break;
659: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
660: 			FilterOperationSwitch<GreaterThanEquals>(v, constant_filter.constant, filter_mask, count);
661: 			break;
662: 		default:
663: 			D_ASSERT(0);
664: 		}
665: 		break;
666: 	}
667: 	case TableFilterType::IS_NOT_NULL:
668: 		FilterIsNotNull(v, filter_mask, count);
669: 		break;
670: 	case TableFilterType::IS_NULL:
671: 		FilterIsNull(v, filter_mask, count);
672: 		break;
673: 	default:
674: 		D_ASSERT(0);
675: 		break;
676: 	}
677: }
678: 
679: void ParquetReader::Scan(ParquetReaderScanState &state, DataChunk &result) {
680: 	while (ScanInternal(state, result)) {
681: 		if (result.size() > 0) {
682: 			break;
683: 		}
684: 		result.Reset();
685: 	}
686: }
687: 
688: bool ParquetReader::ScanInternal(ParquetReaderScanState &state, DataChunk &result) {
689: 	if (state.finished) {
690: 		return false;
691: 	}
692: 
693: 	// see if we have to switch to the next row group in the parquet file
694: 	if (state.current_group < 0 || (int64_t)state.group_offset >= GetGroup(state).num_rows) {
695: 		state.current_group++;
696: 		state.group_offset = 0;
697: 
698: 		if ((idx_t)state.current_group == state.group_idx_list.size()) {
699: 			state.finished = true;
700: 			return false;
701: 		}
702: 
703: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
704: 			// this is a special case where we are not interested in the actual contents of the file
705: 			if (state.column_ids[out_col_idx] == COLUMN_IDENTIFIER_ROW_ID) {
706: 				continue;
707: 			}
708: 
709: 			PrepareRowGroupBuffer(state, out_col_idx);
710: 		}
711: 		return true;
712: 	}
713: 
714: 	auto this_output_chunk_rows = MinValue<idx_t>(STANDARD_VECTOR_SIZE, GetGroup(state).num_rows - state.group_offset);
715: 	result.SetCardinality(this_output_chunk_rows);
716: 
717: 	if (this_output_chunk_rows == 0) {
718: 		state.finished = true;
719: 		return false; // end of last group, we are done
720: 	}
721: 
722: 	// we evaluate simple table filters directly in this scan so we can skip decoding column data that's never going to
723: 	// be relevant
724: 	parquet_filter_t filter_mask;
725: 	filter_mask.set();
726: 
727: 	state.define_buf.zero();
728: 	state.repeat_buf.zero();
729: 
730: 	auto define_ptr = (uint8_t *)state.define_buf.ptr;
731: 	auto repeat_ptr = (uint8_t *)state.repeat_buf.ptr;
732: 
733: 	auto root_reader = ((StructColumnReader *)state.root_reader.get());
734: 
735: 	if (state.filters) {
736: 		vector<bool> need_to_read(result.ColumnCount(), true);
737: 
738: 		// first load the columns that are used in filters
739: 		for (auto &filter_col : state.filters->filters) {
740: 			auto file_col_idx = state.column_ids[filter_col.first];
741: 
742: 			if (filter_mask.none()) { // if no rows are left we can stop checking filters
743: 				break;
744: 			}
745: 
746: 			root_reader->GetChildReader(file_col_idx)
747: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[filter_col.first]);
748: 
749: 			need_to_read[filter_col.first] = false;
750: 
751: 			ApplyFilter(result.data[filter_col.first], *filter_col.second, filter_mask, this_output_chunk_rows);
752: 		}
753: 
754: 		// we still may have to read some cols
755: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
756: 			if (!need_to_read[out_col_idx]) {
757: 				continue;
758: 			}
759: 			auto file_col_idx = state.column_ids[out_col_idx];
760: 
761: 			if (filter_mask.none()) {
762: 				root_reader->GetChildReader(file_col_idx)->Skip(result.size());
763: 				continue;
764: 			}
765: 			// TODO handle ROWID here, too
766: 			root_reader->GetChildReader(file_col_idx)
767: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[out_col_idx]);
768: 		}
769: 
770: 		idx_t sel_size = 0;
771: 		for (idx_t i = 0; i < this_output_chunk_rows; i++) {
772: 			if (filter_mask[i]) {
773: 				state.sel.set_index(sel_size++, i);
774: 			}
775: 		}
776: 
777: 		result.Slice(state.sel, sel_size);
778: 		result.Verify();
779: 
780: 	} else { // #nofilter, just fricking load the data
781: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
782: 			auto file_col_idx = state.column_ids[out_col_idx];
783: 
784: 			if (file_col_idx == COLUMN_IDENTIFIER_ROW_ID) {
785: 				Value constant_42 = Value::BIGINT(42);
786: 				result.data[out_col_idx].Reference(constant_42);
787: 				continue;
788: 			}
789: 
790: 			root_reader->GetChildReader(file_col_idx)
791: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[out_col_idx]);
792: 		}
793: 	}
794: 
795: 	state.group_offset += this_output_chunk_rows;
796: 	return true;
797: }
798: 
799: } // namespace duckdb
[end of extension/parquet/parquet_reader.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: