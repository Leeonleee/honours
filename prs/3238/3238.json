{
  "repo": "duckdb/duckdb",
  "pull_number": 3238,
  "instance_id": "duckdb__duckdb-3238",
  "issue_numbers": [
    "3089"
  ],
  "base_commit": "96666e411d91d1f15f42e0f092d58015959f193f",
  "patch": "diff --git a/extension/parquet/column_reader.cpp b/extension/parquet/column_reader.cpp\nindex fe51325b83df..e87819c248e6 100644\n--- a/extension/parquet/column_reader.cpp\n+++ b/extension/parquet/column_reader.cpp\n@@ -4,6 +4,7 @@\n #include \"parquet_reader.hpp\"\n \n #include \"boolean_column_reader.hpp\"\n+#include \"cast_column_reader.hpp\"\n #include \"callback_column_reader.hpp\"\n #include \"parquet_decimal_utils.hpp\"\n #include \"list_column_reader.hpp\"\n@@ -51,14 +52,30 @@ ColumnReader::ColumnReader(ParquetReader &reader, LogicalType type_p, const Sche\n ColumnReader::~ColumnReader() {\n }\n \n-const LogicalType &ColumnReader::Type() {\n+ParquetReader &ColumnReader::Reader() {\n+\treturn reader;\n+}\n+\n+const LogicalType &ColumnReader::Type() const {\n \treturn type;\n }\n \n-const SchemaElement &ColumnReader::Schema() {\n+const SchemaElement &ColumnReader::Schema() const {\n \treturn schema;\n }\n \n+idx_t ColumnReader::FileIdx() const {\n+\treturn file_idx;\n+}\n+\n+idx_t ColumnReader::MaxDefine() const {\n+\treturn max_define;\n+}\n+\n+idx_t ColumnReader::MaxRepeat() const {\n+\treturn max_repeat;\n+}\n+\n idx_t ColumnReader::GroupRowsAvailable() {\n \treturn group_rows_available;\n }\n@@ -118,9 +135,6 @@ void ColumnReader::PrepareRead(parquet_filter_t &filter) {\n \tPageHeader page_hdr;\n \tpage_hdr.read(protocol);\n \n-\t//\t\tpage_hdr.printTo(std::cout);\n-\t//\t\tstd::cout << '\\n';\n-\n \tswitch (page_hdr.type) {\n \tcase PageType::DATA_PAGE_V2:\n \t\tPreparePageV2(page_hdr);\n@@ -620,6 +634,55 @@ ListColumnReader::ListColumnReader(ParquetReader &reader, LogicalType type_p, co\n \tchild_filter.set();\n }\n \n+//===--------------------------------------------------------------------===//\n+// Cast Column Reader\n+//===--------------------------------------------------------------------===//\n+CastColumnReader::CastColumnReader(unique_ptr<ColumnReader> child_reader_p, LogicalType target_type_p)\n+    : ColumnReader(child_reader_p->Reader(), move(target_type_p), child_reader_p->Schema(), child_reader_p->FileIdx(),\n+                   child_reader_p->MaxDefine(), child_reader_p->MaxRepeat()),\n+      child_reader(move(child_reader_p)) {\n+\tvector<LogicalType> intermediate_types {child_reader->Type()};\n+\tintermediate_chunk.Initialize(intermediate_types);\n+}\n+\n+unique_ptr<BaseStatistics> CastColumnReader::Stats(const std::vector<ColumnChunk> &columns) {\n+\t// casting stats is not supported (yet)\n+\treturn nullptr;\n+}\n+\n+void CastColumnReader::InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) {\n+\tchild_reader->InitializeRead(columns, protocol_p);\n+}\n+\n+idx_t CastColumnReader::Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,\n+                             Vector &result) {\n+\tintermediate_chunk.Reset();\n+\tauto &intermediate_vector = intermediate_chunk.data[0];\n+\n+\tauto amount = child_reader->Read(num_values, filter, define_out, repeat_out, intermediate_vector);\n+\tif (!filter.all()) {\n+\t\t// work-around for filters: set all values that are filtered to NULL to prevent the cast from failing on\n+\t\t// uninitialized data\n+\t\tintermediate_vector.Normalify(amount);\n+\t\tauto &validity = FlatVector::Validity(intermediate_vector);\n+\t\tfor (idx_t i = 0; i < amount; i++) {\n+\t\t\tif (!filter[i]) {\n+\t\t\t\tvalidity.SetInvalid(i);\n+\t\t\t}\n+\t\t}\n+\t}\n+\tVectorOperations::Cast(intermediate_vector, result, amount);\n+\treturn amount;\n+}\n+\n+void CastColumnReader::Skip(idx_t num_values) {\n+\tchild_reader->Skip(num_values);\n+}\n+\n+idx_t CastColumnReader::GroupRowsAvailable() {\n+\treturn child_reader->GroupRowsAvailable();\n+}\n+\n //===--------------------------------------------------------------------===//\n // Struct Column Reader\n //===--------------------------------------------------------------------===//\ndiff --git a/extension/parquet/include/cast_column_reader.hpp b/extension/parquet/include/cast_column_reader.hpp\nnew file mode 100644\nindex 000000000000..0cd54cb93da9\n--- /dev/null\n+++ b/extension/parquet/include/cast_column_reader.hpp\n@@ -0,0 +1,35 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// cast_column_reader.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"column_reader.hpp\"\n+#include \"templated_column_reader.hpp\"\n+\n+namespace duckdb {\n+\n+//! A column reader that represents a cast over a child reader\n+class CastColumnReader : public ColumnReader {\n+public:\n+\tCastColumnReader(unique_ptr<ColumnReader> child_reader, LogicalType target_type);\n+\n+\tunique_ptr<ColumnReader> child_reader;\n+\tDataChunk intermediate_chunk;\n+\n+public:\n+\tunique_ptr<BaseStatistics> Stats(const std::vector<ColumnChunk> &columns) override;\n+\tvoid InitializeRead(const std::vector<ColumnChunk> &columns, TProtocol &protocol_p) override;\n+\n+\tidx_t Read(uint64_t num_values, parquet_filter_t &filter, uint8_t *define_out, uint8_t *repeat_out,\n+\t           Vector &result) override;\n+\n+\tvoid Skip(idx_t num_values) override;\n+\tidx_t GroupRowsAvailable() override;\n+};\n+\n+} // namespace duckdb\ndiff --git a/extension/parquet/include/column_reader.hpp b/extension/parquet/include/column_reader.hpp\nindex c16ce52c2bf5..4a21bebacdee 100644\n--- a/extension/parquet/include/column_reader.hpp\n+++ b/extension/parquet/include/column_reader.hpp\n@@ -56,12 +56,16 @@ class ColumnReader {\n \n \tvirtual void Skip(idx_t num_values);\n \n-\tconst LogicalType &Type();\n-\tconst SchemaElement &Schema();\n+\tParquetReader &Reader();\n+\tconst LogicalType &Type() const;\n+\tconst SchemaElement &Schema() const;\n+\tidx_t FileIdx() const;\n+\tidx_t MaxDefine() const;\n+\tidx_t MaxRepeat() const;\n \n \tvirtual idx_t GroupRowsAvailable();\n \n-\tunique_ptr<BaseStatistics> Stats(const std::vector<ColumnChunk> &columns);\n+\tvirtual unique_ptr<BaseStatistics> Stats(const std::vector<ColumnChunk> &columns);\n \n protected:\n \t// readers that use the default Read() need to implement those\ndiff --git a/extension/parquet/include/parquet_reader.hpp b/extension/parquet/include/parquet_reader.hpp\nindex 6223aa51d8b1..9bb5136e5214 100644\n--- a/extension/parquet/include/parquet_reader.hpp\n+++ b/extension/parquet/include/parquet_reader.hpp\n@@ -69,10 +69,17 @@ class ParquetReader {\n \t    : ParquetReader(allocator, move(file_handle_p), vector<LogicalType>(), string()) {\n \t}\n \n-\tParquetReader(ClientContext &context, string file_name, const vector<LogicalType> &expected_types_p,\n+\tParquetReader(ClientContext &context, string file_name, const vector<string> &names,\n+\t              const vector<LogicalType> &expected_types_p, const vector<column_t> &column_ids,\n \t              ParquetOptions parquet_options, const string &initial_filename = string());\n \tParquetReader(ClientContext &context, string file_name, ParquetOptions parquet_options)\n-\t    : ParquetReader(context, move(file_name), vector<LogicalType>(), parquet_options, string()) {\n+\t    : ParquetReader(context, move(file_name), vector<string>(), vector<LogicalType>(), vector<column_t>(),\n+\t                    parquet_options, string()) {\n+\t}\n+\tParquetReader(ClientContext &context, string file_name, const vector<LogicalType> &expected_types_p,\n+\t              ParquetOptions parquet_options)\n+\t    : ParquetReader(context, move(file_name), vector<string>(), expected_types_p, vector<column_t>(),\n+\t                    parquet_options, string()) {\n \t}\n \t~ParquetReader();\n \n@@ -99,7 +106,8 @@ class ParquetReader {\n \tstatic LogicalType DeriveLogicalType(const SchemaElement &s_ele, bool binary_as_string);\n \n private:\n-\tvoid InitializeSchema(const vector<LogicalType> &expected_types_p, const string &initial_filename_p);\n+\tvoid InitializeSchema(const vector<string> &names, const vector<LogicalType> &expected_types_p,\n+\t                      const vector<column_t> &column_ids, const string &initial_filename_p);\n \tbool ScanInternal(ParquetReaderScanState &state, DataChunk &output);\n \tunique_ptr<ColumnReader> CreateReader(const duckdb_parquet::format::FileMetaData *file_meta_data);\n \n@@ -118,6 +126,12 @@ class ParquetReader {\n \n private:\n \tunique_ptr<FileHandle> file_handle;\n+\t//! column-id map, used when reading multiple parquet files since separate parquet files might have columns at\n+\t//! different positions e.g. the first file might have column \"a\" at position 0, the second at position 1, etc\n+\tvector<column_t> column_id_map;\n+\t//! Map of column_id -> cast, used when reading multiple parquet files when parquet files have diverging types\n+\t//! for the same column\n+\tunordered_map<column_t, LogicalType> cast_map;\n };\n \n } // namespace duckdb\ndiff --git a/extension/parquet/parquet-extension.cpp b/extension/parquet/parquet-extension.cpp\nindex 6abe835974d1..c7941bdd5ec9 100644\n--- a/extension/parquet/parquet-extension.cpp\n+++ b/extension/parquet/parquet-extension.cpp\n@@ -42,6 +42,8 @@ struct ParquetReadBindData : public FunctionData {\n \tvector<column_t> column_ids;\n \tatomic<idx_t> chunk_count;\n \tatomic<idx_t> cur_file;\n+\tvector<string> names;\n+\tvector<LogicalType> types;\n };\n \n struct ParquetReadOperatorData : public FunctionOperatorData {\n@@ -88,6 +90,7 @@ class ParquetScanFunction {\n \tstatic unique_ptr<FunctionData> ParquetReadBind(ClientContext &context, CopyInfo &info,\n \t                                                vector<string> &expected_names,\n \t                                                vector<LogicalType> &expected_types) {\n+\t\tD_ASSERT(expected_names.size() == expected_types.size());\n \t\tfor (auto &option : info.options) {\n \t\t\tauto loption = StringUtil::Lower(option.first);\n \t\t\tif (loption == \"compression\" || loption == \"codec\") {\n@@ -106,6 +109,8 @@ class ParquetScanFunction {\n \t\t}\n \t\tParquetOptions parquet_options(context);\n \t\tresult->initial_reader = make_shared<ParquetReader>(context, result->files[0], expected_types, parquet_options);\n+\t\tresult->names = result->initial_reader->names;\n+\t\tresult->types = result->initial_reader->return_types;\n \t\treturn move(result);\n \t}\n \n@@ -181,9 +186,8 @@ class ParquetScanFunction {\n \t\tresult->files = move(files);\n \n \t\tresult->initial_reader = make_shared<ParquetReader>(context, result->files[0], parquet_options);\n-\t\treturn_types = result->initial_reader->return_types;\n-\n-\t\tnames = result->initial_reader->names;\n+\t\treturn_types = result->types = result->initial_reader->return_types;\n+\t\tnames = result->names = result->initial_reader->names;\n \t\treturn move(result);\n \t}\n \n@@ -309,8 +313,9 @@ class ParquetScanFunction {\n \t\t\t\t\tbind_data.chunk_count = 0;\n \t\t\t\t\tstring file = bind_data.files[data.file_index];\n \t\t\t\t\t// move to the next file\n-\t\t\t\t\tdata.reader = make_shared<ParquetReader>(context, file, data.reader->return_types,\n-\t\t\t\t\t                                         data.reader->parquet_options, bind_data.files[0]);\n+\t\t\t\t\tdata.reader =\n+\t\t\t\t\t    make_shared<ParquetReader>(context, file, bind_data.names, bind_data.types, data.column_ids,\n+\t\t\t\t\t                               data.reader->parquet_options, bind_data.files[0]);\n \t\t\t\t\tvector<idx_t> group_ids;\n \t\t\t\t\tfor (idx_t i = 0; i < data.reader->NumRowGroups(); i++) {\n \t\t\t\t\t\tgroup_ids.push_back(i);\n@@ -371,8 +376,8 @@ class ParquetScanFunction {\n \t\t\t\t// read the next file\n \t\t\t\tstring file = bind_data.files[++parallel_state.file_index];\n \t\t\t\tparallel_state.current_reader =\n-\t\t\t\t    make_shared<ParquetReader>(context, file, parallel_state.current_reader->return_types,\n-\t\t\t\t                               parallel_state.current_reader->parquet_options);\n+\t\t\t\t    make_shared<ParquetReader>(context, file, bind_data.names, bind_data.types, scan_data.column_ids,\n+\t\t\t\t                               parallel_state.current_reader->parquet_options, bind_data.files[0]);\n \t\t\t\tif (parallel_state.current_reader->NumRowGroups() == 0) {\n \t\t\t\t\t// empty parquet file, move to next file\n \t\t\t\t\tcontinue;\ndiff --git a/extension/parquet/parquet_reader.cpp b/extension/parquet/parquet_reader.cpp\nindex 22207c649b19..46001103416b 100644\n--- a/extension/parquet/parquet_reader.cpp\n+++ b/extension/parquet/parquet_reader.cpp\n@@ -4,6 +4,7 @@\n #include \"column_reader.hpp\"\n \n #include \"boolean_column_reader.hpp\"\n+#include \"cast_column_reader.hpp\"\n #include \"callback_column_reader.hpp\"\n #include \"list_column_reader.hpp\"\n #include \"string_column_reader.hpp\"\n@@ -24,6 +25,7 @@\n #include \"duckdb/common/string_util.hpp\"\n #include \"duckdb/common/types/date.hpp\"\n #include \"duckdb/common/pair.hpp\"\n+#include \"duckdb/common/vector_operations/vector_operations.hpp\"\n \n #include \"duckdb/storage/object_cache.hpp\"\n #endif\n@@ -340,10 +342,23 @@ unique_ptr<ColumnReader> ParquetReader::CreateReader(const duckdb_parquet::forma\n \tauto ret = CreateReaderRecursive(file_meta_data, 0, 0, 0, next_schema_idx, next_file_idx);\n \tD_ASSERT(next_schema_idx == file_meta_data->schema.size() - 1);\n \tD_ASSERT(file_meta_data->row_groups.empty() || next_file_idx == file_meta_data->row_groups[0].columns.size());\n+\n+\t// add casts if required\n+\tfor (auto &entry : cast_map) {\n+\t\tauto column_idx = entry.first;\n+\t\tauto &expected_type = entry.second;\n+\n+\t\tauto &root_struct_reader = (StructColumnReader &)*ret;\n+\t\tauto child_reader = move(root_struct_reader.child_readers[column_idx]);\n+\t\tauto cast_reader = make_unique<CastColumnReader>(move(child_reader), expected_type);\n+\t\troot_struct_reader.child_readers[column_idx] = move(cast_reader);\n+\t}\n \treturn ret;\n }\n \n-void ParquetReader::InitializeSchema(const vector<LogicalType> &expected_types_p, const string &initial_filename_p) {\n+void ParquetReader::InitializeSchema(const vector<string> &expected_names, const vector<LogicalType> &expected_types,\n+                                     const vector<column_t> &column_ids, const string &initial_filename_p) {\n+\tD_ASSERT(expected_names.size() == expected_types.size() || (expected_names.empty() && column_ids.empty()));\n \tauto file_meta_data = GetFileMetadata();\n \n \tif (file_meta_data->__isset.encryption_algorithm) {\n@@ -354,36 +369,65 @@ void ParquetReader::InitializeSchema(const vector<LogicalType> &expected_types_p\n \t\tthrow FormatException(\"Need at least one non-root column in the file\");\n \t}\n \n-\tbool has_expected_types = !expected_types_p.empty();\n+\tbool has_expected_names = !expected_names.empty();\n+\tbool has_expected_types = !expected_types.empty();\n \tauto root_reader = CreateReader(file_meta_data);\n \n \tauto &root_type = root_reader->Type();\n \tauto &child_types = StructType::GetChildTypes(root_type);\n \tD_ASSERT(root_type.id() == LogicalTypeId::STRUCT);\n-\tif (has_expected_types && child_types.size() != expected_types_p.size()) {\n-\t\tthrow FormatException(\"column count mismatch\");\n-\t}\n-\tidx_t col_idx = 0;\n \tfor (auto &type_pair : child_types) {\n-\t\tif (has_expected_types && expected_types_p[col_idx] != type_pair.second) {\n-\t\t\tif (initial_filename_p.empty()) {\n-\t\t\t\tthrow FormatException(\"column \\\"%d\\\" in parquet file is of type %s, could not auto cast to \"\n-\t\t\t\t                      \"expected type %s for this column\",\n-\t\t\t\t                      col_idx, type_pair.second, expected_types_p[col_idx].ToString());\n-\t\t\t} else {\n-\t\t\t\tthrow FormatException(\"schema mismatch in Parquet glob: column \\\"%d\\\" in parquet file is of type \"\n-\t\t\t\t                      \"%s, but in the original file \\\"%s\\\" this column is of type \\\"%s\\\"\",\n-\t\t\t\t                      col_idx, type_pair.second, initial_filename_p,\n-\t\t\t\t                      expected_types_p[col_idx].ToString());\n-\t\t\t}\n-\t\t} else {\n-\t\t\tnames.push_back(type_pair.first);\n-\t\t\treturn_types.push_back(type_pair.second);\n-\t\t}\n-\t\tcol_idx++;\n+\t\tnames.push_back(type_pair.first);\n+\t\treturn_types.push_back(type_pair.second);\n \t}\n \tD_ASSERT(!names.empty());\n \tD_ASSERT(!return_types.empty());\n+\tif (!has_expected_types) {\n+\t\treturn;\n+\t}\n+\tif (!has_expected_names && has_expected_types) {\n+\t\t// we ONLY have expected types, but no expected names\n+\t\t// in this case we need all types to match in-order\n+\t\tif (return_types.size() != expected_types.size()) {\n+\t\t\tthrow FormatException(\"column count mismatch: expected %d columns but found %d\", expected_types.size(),\n+\t\t\t                      return_types.size());\n+\t\t}\n+\t\tfor (idx_t col_idx = 0; col_idx < return_types.size(); col_idx++) {\n+\t\t\tif (return_types[col_idx] == expected_types[col_idx]) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t// type mismatch: have to add a cast\n+\t\t\tcast_map[col_idx] = expected_types[col_idx];\n+\t\t}\n+\t\treturn;\n+\t}\n+\tD_ASSERT(column_ids.size() > 0);\n+\t// we have expected types: create a map of name -> column index\n+\tunordered_map<string, idx_t> name_map;\n+\tfor (idx_t col_idx = 0; col_idx < names.size(); col_idx++) {\n+\t\tname_map[names[col_idx]] = col_idx;\n+\t}\n+\t// now for each of the expected names, look it up in the name map and fill the column_id_map\n+\tD_ASSERT(column_id_map.empty());\n+\tfor (idx_t i = 0; i < column_ids.size(); i++) {\n+\t\tif (column_ids[i] == COLUMN_IDENTIFIER_ROW_ID) {\n+\t\t\tcontinue;\n+\t\t}\n+\t\tauto &expected_name = expected_names[column_ids[i]];\n+\t\tauto &expected_type = expected_types[column_ids[i]];\n+\t\tauto entry = name_map.find(expected_name);\n+\t\tif (entry == name_map.end()) {\n+\t\t\tthrow FormatException(\"schema mismatch in Parquet glob: column \\\"%s\\\" was read from the original file \"\n+\t\t\t                      \"\\\"%s\\\", but could not be found in file \\\"%s\\\"\",\n+\t\t\t                      expected_name, initial_filename_p, file_name);\n+\t\t}\n+\t\tauto column_idx = entry->second;\n+\t\tcolumn_id_map.push_back(column_idx);\n+\t\tif (expected_type != return_types[column_idx]) {\n+\t\t\t// type mismatch: have to add a cast\n+\t\t\tcast_map[column_idx] = expected_type;\n+\t\t}\n+\t}\n }\n \n ParquetOptions::ParquetOptions(ClientContext &context) {\n@@ -399,10 +443,13 @@ ParquetReader::ParquetReader(Allocator &allocator_p, unique_ptr<FileHandle> file\n \tfile_name = file_handle_p->path;\n \tfile_handle = move(file_handle_p);\n \tmetadata = LoadMetadata(allocator, *file_handle);\n-\tInitializeSchema(expected_types_p, initial_filename_p);\n+\tvector<string> expected_names;\n+\tvector<column_t> expected_column_ids;\n+\tInitializeSchema(expected_names, expected_types_p, expected_column_ids, initial_filename_p);\n }\n \n-ParquetReader::ParquetReader(ClientContext &context_p, string file_name_p, const vector<LogicalType> &expected_types_p,\n+ParquetReader::ParquetReader(ClientContext &context_p, string file_name_p, const vector<string> &expected_names,\n+                             const vector<LogicalType> &expected_types_p, const vector<column_t> &column_ids,\n                              ParquetOptions parquet_options_p, const string &initial_filename_p)\n     : allocator(Allocator::Get(context_p)), file_opener(FileSystem::GetFileOpener(context_p)),\n       parquet_options(parquet_options_p) {\n@@ -428,7 +475,7 @@ ParquetReader::ParquetReader(ClientContext &context_p, string file_name_p, const\n \t\t\tObjectCache::GetObjectCache(context_p).Put(file_name, metadata);\n \t\t}\n \t}\n-\tInitializeSchema(expected_types_p, initial_filename_p);\n+\tInitializeSchema(expected_names, expected_types_p, column_ids, initial_filename_p);\n }\n \n ParquetReader::~ParquetReader() {\n@@ -487,9 +534,9 @@ void ParquetReader::PrepareRowGroupBuffer(ParquetReaderScanState &state, idx_t o\n \t\t\t\tskip_chunk = true;\n \t\t\t}\n \t\t\tif (skip_chunk) {\n+\t\t\t\t// this effectively will skip this chunk\n \t\t\t\tstate.group_offset = group.num_rows;\n \t\t\t\treturn;\n-\t\t\t\t// this effectively will skip this chunk\n \t\t\t}\n \t\t}\n \t}\n@@ -509,7 +556,7 @@ void ParquetReader::InitializeScan(ParquetReaderScanState &state, vector<column_\n                                    vector<idx_t> groups_to_read, TableFilterSet *filters) {\n \tstate.current_group = -1;\n \tstate.finished = false;\n-\tstate.column_ids = move(column_ids);\n+\tstate.column_ids = column_id_map.empty() ? move(column_ids) : column_id_map;\n \tstate.group_offset = 0;\n \tstate.group_idx_list = move(groups_to_read);\n \tstate.filters = filters;\n",
  "test_patch": "diff --git a/test/sql/copy/parquet/parquet_schema_evolution.test b/test/sql/copy/parquet/parquet_schema_evolution.test\nnew file mode 100644\nindex 000000000000..95cd4b5e22b0\n--- /dev/null\n+++ b/test/sql/copy/parquet/parquet_schema_evolution.test\n@@ -0,0 +1,140 @@\n+# name: test/sql/copy/parquet/parquet_schema_evolution.test\n+# description: Test parquet schema evolution\n+# group: [parquet]\n+\n+require parquet\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+# we run this twice, once with multi threading and once without\n+loop i 0 2\n+\n+# test column names in different orders\n+statement ok\n+COPY (SELECT 42::INT a, 43::INT c) TO '__TEST_DIR__/evolution_1.parquet' (FORMAT PARQUET);\n+\n+statement ok\n+COPY (SELECT 88::INT b, 84::INT a) TO '__TEST_DIR__/evolution_2.parquet' (FORMAT PARQUET);\n+\n+query I\n+SELECT a FROM '__TEST_DIR__/evolution_*.parquet' ORDER BY a\n+----\n+42\n+84\n+\n+query I\n+SELECT a FROM parquet_scan(['__TEST_DIR__/evolution_2.parquet', '__TEST_DIR__/evolution_1.parquet', '__TEST_DIR__/evolution_*.parquet']) ORDER BY a\n+----\n+42\n+42\n+84\n+84\n+\n+# reading either b or c results in an error, since the name is not present in both files\n+statement error\n+SELECT b FROM '__TEST_DIR__/evolution_*.parquet' ORDER BY 1\n+\n+statement error\n+SELECT c FROM '__TEST_DIR__/evolution_*.parquet' ORDER BY 1\n+\n+# we can also do this with COPY\n+statement ok\n+CREATE TABLE copy_test(a INT, b INT);\n+\n+# if we copy from both files, we run into a schema mismatch in the files themselves (name differences)\n+statement error\n+COPY copy_test FROM '__TEST_DIR__/evolution_*.parquet'\n+\n+# copying from one file works, however\n+statement ok\n+COPY copy_test FROM '__TEST_DIR__/evolution_1.parquet'\n+\n+statement ok\n+DROP TABLE copy_test\n+\n+# test type promotion\n+statement ok\n+COPY (SELECT 42::INT a, 43::INT b) TO '__TEST_DIR__/evolution_1.parquet' (FORMAT PARQUET);\n+\n+statement ok\n+COPY (SELECT 'hello'::VARCHAR b, 84::TINYINT a) TO '__TEST_DIR__/evolution_2.parquet' (FORMAT PARQUET);\n+\n+query I\n+SELECT a FROM '__TEST_DIR__/evolution_*.parquet' ORDER BY 1\n+----\n+42\n+84\n+\n+# reading either b results in an error, however, since we can't cast from 'hello' to integer\n+statement error\n+SELECT b FROM parquet_scan(['__TEST_DIR__/evolution_1.parquet', '__TEST_DIR__/evolution_2.parquet']) ORDER BY 1\n+\n+# if we flip the order of the reads we can read b\n+query I\n+SELECT b FROM parquet_scan(['__TEST_DIR__/evolution_2.parquet', '__TEST_DIR__/evolution_1.parquet']) ORDER BY 1\n+----\n+43\n+hello\n+\n+# type promotion, but with lists\n+statement ok\n+COPY (SELECT [42::INT] a, [43::INT] b) TO '__TEST_DIR__/evolution_1.parquet' (FORMAT PARQUET);\n+\n+statement ok\n+COPY (SELECT ['hello'::VARCHAR] b, [84::TINYINT] a) TO '__TEST_DIR__/evolution_2.parquet' (FORMAT PARQUET);\n+\n+query I\n+SELECT a FROM '__TEST_DIR__/evolution_*.parquet' ORDER BY 1\n+----\n+[42]\n+[84]\n+\n+# reading either b results in an error, however, since we can't cast from 'hello' to integer\n+statement error\n+SELECT b FROM parquet_scan(['__TEST_DIR__/evolution_1.parquet', '__TEST_DIR__/evolution_2.parquet']) ORDER BY 1\n+\n+# if we flip the order of the reads we can read b\n+query I\n+SELECT b FROM parquet_scan(['__TEST_DIR__/evolution_2.parquet', '__TEST_DIR__/evolution_1.parquet']) ORDER BY 1\n+----\n+[43]\n+[hello]\n+\n+# type promotion & skipping\n+statement ok\n+COPY (SELECT range id, 1::INT a FROM range(10000) UNION ALL SELECT 10000+range id, 2::INT FROM range(10000)) TO '__TEST_DIR__/evolution_1.parquet' (FORMAT PARQUET);\n+\n+statement ok\n+COPY (SELECT (20000+range)::BIGINT id, 'hello'::VARCHAR b, 3::BIGINT a FROM range(10000) UNION ALL SELECT (30000+range)::BIGINT id, 'world'::VARCHAR, 4 FROM range(10000)) TO '__TEST_DIR__/evolution_2.parquet' (FORMAT PARQUET);\n+\n+query I\n+SELECT COUNT(*) FROM '__TEST_DIR__/evolution_*.parquet' WHERE a=2\n+----\n+10000\n+\n+query I\n+SELECT COUNT(*) FROM '__TEST_DIR__/evolution_*.parquet' WHERE a>=2\n+----\n+30000\n+\n+query II\n+SELECT id, a FROM '__TEST_DIR__/evolution_*.parquet' WHERE id=2\n+----\n+2\t1\n+\n+query II\n+SELECT id, a FROM '__TEST_DIR__/evolution_*.parquet' WHERE id=27777\n+----\n+27777\t3\n+\n+query II\n+SELECT id, a FROM '__TEST_DIR__/evolution_*.parquet' WHERE id>=39998 ORDER BY id\n+----\n+39998\t4\n+39999\t4\n+\n+statement ok\n+PRAGMA threads=1\n+\n+endloop\ndiff --git a/test/sql/copy/parquet/tpch_parquet_copy_cast.test_slow b/test/sql/copy/parquet/tpch_parquet_copy_cast.test_slow\nnew file mode 100644\nindex 000000000000..fe327936c0c1\n--- /dev/null\n+++ b/test/sql/copy/parquet/tpch_parquet_copy_cast.test_slow\n@@ -0,0 +1,50 @@\n+# name: test/sql/copy/parquet/tpch_parquet_copy_cast.test_slow\n+# description: Test auto-casting to table types in COPY from parquet file\n+# group: [parquet]\n+\n+require parquet\n+\n+require tpch\n+\n+statement ok\n+CREATE SCHEMA tpch\n+\n+statement ok\n+CALL dbgen(sf=0.01, schema='tpch');\n+\n+statement ok\n+COPY (\n+\tSELECT\n+\t\tl_orderkey::UBIGINT l_orderkey,\n+\t\tl_partkey::BIGINT l_partkey,\n+\t\tl_suppkey::SMALLINT l_suppkey,\n+\t\tl_linenumber::UINTEGER,\n+\t\tl_quantity::SMALLINT l_quantity,\n+\t\tl_extendedprice::DECIMAL(18,4) l_extendedprice,\n+\t\tl_discount::DECIMAL(8,3) l_discount,\n+\t\tl_tax::DECIMAL(38,4) l_tax,\n+\t\tl_returnflag,\n+\t\tl_linestatus,\n+\t\tl_shipdate::TIMESTAMP l_shipdate,\n+\t\tl_commitdate::TIMESTAMP l_commitdate,\n+\t\tl_receiptdate::VARCHAR l_receiptdate,\n+\t\tl_shipinstruct,\n+\t\tl_shipmode,\n+\t\tl_comment FROM tpch.lineitem\n+) TO '__TEST_DIR__/lineitem_different_types.parquet' (FORMAT PARQUET);\n+\n+statement ok\n+CREATE TABLE lineitem AS SELECT * FROM tpch.lineitem LIMIT 0;\n+\n+statement ok\n+COPY lineitem FROM '__TEST_DIR__/lineitem_different_types.parquet'\n+\n+query I\n+PRAGMA tpch(1)\n+----\n+<FILE>:extension/tpch/dbgen/answers/sf0.01/q01.csv\n+\n+query I\n+PRAGMA tpch(6)\n+----\n+<FILE>:extension/tpch/dbgen/answers/sf0.01/q06.csv\ndiff --git a/test/sql/copy/parquet/tpch_parquet_schema_evolution.test_slow b/test/sql/copy/parquet/tpch_parquet_schema_evolution.test_slow\nnew file mode 100644\nindex 000000000000..734ab3eb67f9\n--- /dev/null\n+++ b/test/sql/copy/parquet/tpch_parquet_schema_evolution.test_slow\n@@ -0,0 +1,66 @@\n+# name: test/sql/copy/parquet/tpch_parquet_schema_evolution.test_slow\n+# description: Test more complex type evolution on TPC-H schema\n+# group: [parquet]\n+\n+require parquet\n+\n+require tpch\n+\n+statement ok\n+CREATE SCHEMA tpch\n+\n+statement ok\n+CALL dbgen(sf=0.01, schema='tpch');\n+\n+# original schema, minus some unnecessary columns\n+statement ok\n+COPY (\n+\tSELECT\n+\tl_shipdate,\n+\tl_discount,\n+\tl_linenumber,\n+\tl_tax,\n+\tl_extendedprice,\n+\tl_shipinstruct,\n+\tl_commitdate,\n+\tl_linestatus,\n+\tl_returnflag,\n+\tl_quantity,\n+\tl_orderkey,\n+\tl_shipmode FROM tpch.lineitem LIMIT 30000\n+) TO '__TEST_DIR__/schema_evolution_lineitem1.parquet' (FORMAT PARQUET);\n+\n+# complete schema, with casts\n+statement ok\n+COPY (\n+\tSELECT\n+\tl_comment,\n+\tl_shipmode,\n+\tl_commitdate::TIMESTAMP l_commitdate,\n+\tl_shipinstruct,\n+\tl_quantity::SMALLINT l_quantity,\n+\tl_suppkey::BIGINT l_suppkey,\n+\tl_linenumber,\n+\tl_linestatus,\n+\tl_receiptdate::VARCHAR l_receiptdate,\n+\tl_returnflag,\n+\tl_extendedprice::DECIMAL(18,4) l_extendedprice,\n+\tl_partkey::BIGINT l_partkey,\n+\tl_orderkey::HUGEINT l_orderkey,\n+\tl_discount::DECIMAL(8,3) l_discount,\n+\tl_shipdate::TIMESTAMP l_shipdate,\n+\tl_tax FROM tpch.lineitem OFFSET 30000\n+) TO '__TEST_DIR__/schema_evolution_lineitem2.parquet' (FORMAT PARQUET);\n+\n+statement ok\n+CREATE VIEW lineitem AS SELECT * FROM '__TEST_DIR__/schema_evolution_lineitem*.parquet'\n+\n+query I\n+PRAGMA tpch(1)\n+----\n+<FILE>:extension/tpch/dbgen/answers/sf0.01/q01.csv\n+\n+query I\n+PRAGMA tpch(6)\n+----\n+<FILE>:extension/tpch/dbgen/answers/sf0.01/q06.csv\n",
  "problem_statement": "Use column names for schema validation\n#### What happens?\r\nWhen querying parquet files on the filesystem directly, the schema validation fails if columns are out of order. Would it be possible for the expected types for each column to operate on the names of the columns rather than their integer index?\r\n\r\n#### To Reproduce\r\n```\r\nimport pandas as pd\r\nimport duckdb\r\ndf = pd.DataFrame({\"A\": [1,2,3], \"B\": [1.0, 2.0, 3.0]})\r\ndf.to_parquet(\"1.parquet\")\r\ndf[reversed(df.columns)].to_parquet(\"2.parquet\")\r\nduckdb.query(\"select * from '*.parquet'\")\r\n```\r\n```\r\n---------------------\r\n-- Expression Tree --\r\n---------------------\r\nSubquery\r\n\r\n---------------------\r\n-- Result Columns  --\r\n---------------------\r\n- A (BIGINT)\r\n- B (DOUBLE)\r\n\r\n---------------------\r\n-- Result Preview  --\r\n---------------------\r\nFailed to read Parquet file \"2.parquet\": column \"0\" in parquet file is of type DOUBLE, could not auto cast to expected type BIGINT for this column\r\n```\r\n\r\n#### Environment (please complete the following information):\r\n - OS: Red Hat Enterprise Linux Server release 7.6 (Maipo)\r\n - DuckDB Version: 0.3.2\r\n - DuckDB Client: Python\r\n\r\n\n",
  "hints_text": "",
  "created_at": "2022-03-15T13:12:49Z"
}