You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Parquet reader doesn't appear to handle (some) fields inside nested objects
Given a parquet file with this schema:
```
message test {
  required group inner {
    optional binary str_field (STRING);
    optional double f64_field;
  }
}
```

`parquet_scan` does not appear to read the `str_field` fields correctly. Using the file in `data/struct.parquet` in https://github.com/nelhage/duckdb-parquet-bugs I see:

```
[nelhage@monolithique:~/code/duckdb-parquet]$ parquet-tool cat data/struct.parquet
inner:
.str_field = hello

inner:
.f64_field = 1.23

(GIT: master)
[nelhage@monolithique:~/code/duckdb-parquet]$ duckdb test.duckdb "select * from parquet_scan('data/struct.parquet');"
┌────────────────────────────────────────┐
│                 inner                  │
├────────────────────────────────────────┤
│ <str_field: NULL, f64_field: 1.230000> │
│ <str_field: NULL, f64_field: NULL>     │
└────────────────────────────────────────┘
```

We can see that the `f64_field` is read properly, but the `str_field` is `NULL` even when present in the file.

That test file is generated by https://github.com/nelhage/duckdb-parquet-bugs/blob/master/gen-struct.go in case it is relevant.

</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
2: 
3: ![.github/workflows/main.yml](https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
6: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3901452.svg)](https://zenodo.org/record/3901452)
7: 
8: 
9: ## Installation
10: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
11: 
12: ## Development
13: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
14: 
15: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
16: 
17: 
[end of README.md]
[start of extension/parquet/parquet_reader.cpp]
1: #include "parquet_reader.hpp"
2: #include "parquet_timestamp.hpp"
3: #include "parquet_statistics.hpp"
4: #include "column_reader.hpp"
5: 
6: #include "thrift_tools.hpp"
7: 
8: #include "parquet_file_metadata_cache.hpp"
9: 
10: #include "duckdb.hpp"
11: #ifndef DUCKDB_AMALGAMATION
12: #include "duckdb/planner/table_filter.hpp"
13: #include "duckdb/common/file_system.hpp"
14: #include "duckdb/common/string_util.hpp"
15: #include "duckdb/common/types/date.hpp"
16: #include "duckdb/common/pair.hpp"
17: 
18: #include "duckdb/storage/object_cache.hpp"
19: #endif
20: 
21: #include <sstream>
22: #include <cassert>
23: #include <chrono>
24: #include <cstring>
25: #include <iostream>
26: 
27: namespace duckdb {
28: 
29: using parquet::format::ColumnChunk;
30: using parquet::format::ConvertedType;
31: using parquet::format::FieldRepetitionType;
32: using parquet::format::FileMetaData;
33: using parquet::format::RowGroup;
34: using parquet::format::SchemaElement;
35: using parquet::format::Statistics;
36: using parquet::format::Type;
37: 
38: static unique_ptr<apache::thrift::protocol::TProtocol> CreateThriftProtocol(FileHandle &file_handle) {
39: 	shared_ptr<ThriftFileTransport> trans(new ThriftFileTransport(file_handle));
40: 	return make_unique<apache::thrift::protocol::TCompactProtocolT<ThriftFileTransport>>(trans);
41: }
42: 
43: static shared_ptr<ParquetFileMetadataCache> LoadMetadata(FileHandle &file_handle) {
44: 	auto current_time = std::chrono::system_clock::to_time_t(std::chrono::system_clock::now());
45: 
46: 	auto proto = CreateThriftProtocol(file_handle);
47: 	auto &transport = ((ThriftFileTransport &)*proto->getTransport());
48: 	auto file_size = transport.GetSize();
49: 	if (file_size < 12) {
50: 		throw InvalidInputException("File '%s' too small to be a Parquet file", file_handle.path);
51: 	}
52: 
53: 	ResizeableBuffer buf;
54: 	buf.resize(8);
55: 	buf.zero();
56: 
57: 	transport.SetLocation(file_size - 8);
58: 	transport.read((uint8_t *)buf.ptr, 8);
59: 
60: 	if (strncmp(buf.ptr + 4, "PAR1", 4) != 0) {
61: 		throw InvalidInputException("No magic bytes found at end of file '%s'", file_handle.path);
62: 	}
63: 	// read four-byte footer length from just before the end magic bytes
64: 	auto footer_len = *(uint32_t *)buf.ptr;
65: 	if (footer_len <= 0 || file_size < 12 + footer_len) {
66: 		throw InvalidInputException("Footer length error in file '%s'", file_handle.path);
67: 	}
68: 	auto metadata_pos = file_size - (footer_len + 8);
69: 	transport.SetLocation(metadata_pos);
70: 
71: 	auto metadata = make_unique<FileMetaData>();
72: 	metadata->read(proto.get());
73: 	return make_shared<ParquetFileMetadataCache>(move(metadata), current_time);
74: }
75: 
76: static LogicalType DeriveLogicalType(const SchemaElement &s_ele) {
77: 	// inner node
78: 	D_ASSERT(s_ele.__isset.type && s_ele.num_children == 0);
79: 	switch (s_ele.type) {
80: 	case Type::BOOLEAN:
81: 		return LogicalType::BOOLEAN;
82: 	case Type::INT32:
83: 		if (s_ele.__isset.converted_type) {
84: 			switch (s_ele.converted_type) {
85: 			case ConvertedType::DATE:
86: 				return LogicalType::DATE;
87: 			case ConvertedType::UINT_8:
88: 				return LogicalType::UTINYINT;
89: 			case ConvertedType::UINT_16:
90: 				return LogicalType::USMALLINT;
91: 			default:
92: 				return LogicalType::INTEGER;
93: 			}
94: 		}
95: 		return LogicalType::INTEGER;
96: 	case Type::INT64:
97: 		if (s_ele.__isset.converted_type) {
98: 			switch (s_ele.converted_type) {
99: 			case ConvertedType::TIMESTAMP_MICROS:
100: 			case ConvertedType::TIMESTAMP_MILLIS:
101: 				return LogicalType::TIMESTAMP;
102: 			case ConvertedType::UINT_32:
103: 				return LogicalType::UINTEGER;
104: 			case ConvertedType::UINT_64:
105: 				return LogicalType::UBIGINT;
106: 			default:
107: 				return LogicalType::BIGINT;
108: 			}
109: 		}
110: 		return LogicalType::BIGINT;
111: 
112: 	case Type::INT96: // always a timestamp it would seem
113: 		return LogicalType::TIMESTAMP;
114: 	case Type::FLOAT:
115: 		return LogicalType::FLOAT;
116: 	case Type::DOUBLE:
117: 		return LogicalType::DOUBLE;
118: 		//			case parquet::format::Type::FIXED_LEN_BYTE_ARRAY: {
119: 		// TODO some decimals yuck
120: 	case Type::BYTE_ARRAY:
121: 		if (s_ele.__isset.converted_type) {
122: 			switch (s_ele.converted_type) {
123: 			case ConvertedType::UTF8:
124: 				return LogicalType::VARCHAR;
125: 			default:
126: 				return LogicalType::BLOB;
127: 			}
128: 		}
129: 		return LogicalType::BLOB;
130: 	case Type::FIXED_LEN_BYTE_ARRAY:
131: 		if (s_ele.__isset.converted_type && s_ele.converted_type == ConvertedType::DECIMAL && s_ele.__isset.scale &&
132: 		    s_ele.__isset.scale && s_ele.__isset.type_length) {
133: 			// habemus decimal
134: 			return LogicalType(LogicalTypeId::DECIMAL, s_ele.precision, s_ele.scale);
135: 		}
136: 	default:
137: 		return LogicalType::INVALID;
138: 	}
139: }
140: 
141: static unique_ptr<ColumnReader> CreateReaderRecursive(const FileMetaData *file_meta_data, idx_t depth, idx_t max_define,
142:                                                       idx_t max_repeat, idx_t &next_schema_idx, idx_t &next_file_idx) {
143: 	D_ASSERT(file_meta_data);
144: 	D_ASSERT(next_schema_idx < file_meta_data->schema.size());
145: 	auto &s_ele = file_meta_data->schema[next_schema_idx];
146: 	auto this_idx = next_schema_idx;
147: 
148: 	if (s_ele.__isset.repetition_type) {
149: 		if (s_ele.repetition_type != FieldRepetitionType::REQUIRED) {
150: 			max_define = depth;
151: 		}
152: 		if (s_ele.repetition_type == FieldRepetitionType::REPEATED) {
153: 			max_repeat++;
154: 		}
155: 	}
156: 
157: 	if (!s_ele.__isset.type) { // inner node
158: 		if (s_ele.num_children == 0) {
159: 			throw std::runtime_error("Node has no children but should");
160: 		}
161: 		child_list_t<LogicalType> child_types;
162: 		vector<unique_ptr<ColumnReader>> child_readers;
163: 
164: 		idx_t c_idx = 0;
165: 		while (c_idx < (idx_t)s_ele.num_children) {
166: 			next_schema_idx++;
167: 
168: 			auto &child_ele = file_meta_data->schema[next_schema_idx];
169: 
170: 			auto child_reader = CreateReaderRecursive(file_meta_data, depth + 1, max_define, max_repeat,
171: 			                                          next_schema_idx, next_file_idx);
172: 			child_types.push_back(make_pair(child_ele.name, child_reader->Type()));
173: 			child_readers.push_back(move(child_reader));
174: 
175: 			c_idx++;
176: 		}
177: 		D_ASSERT(!child_types.empty());
178: 		unique_ptr<ColumnReader> result;
179: 		LogicalType result_type;
180: 		// if we only have a single child no reason to create a struct ay
181: 		if (child_types.size() > 1 || depth == 0) {
182: 			result_type = LogicalType(LogicalTypeId::STRUCT, child_types);
183: 			result = make_unique<StructColumnReader>(result_type, s_ele, this_idx, max_define, max_repeat,
184: 			                                         move(child_readers));
185: 		} else {
186: 			// if we have a struct with only a single type, pull up
187: 			result_type = child_types[0].second;
188: 			result = move(child_readers[0]);
189: 		}
190: 		if (s_ele.repetition_type == FieldRepetitionType::REPEATED) {
191: 			result_type = LogicalType(LogicalTypeId::LIST, {make_pair("", result_type)});
192: 			return make_unique<ListColumnReader>(result_type, s_ele, this_idx, max_define, max_repeat, move(result));
193: 		}
194: 		return result;
195: 	} else { // leaf node
196: 		// TODO check return value of derive type or should we only do this on read()
197: 		return ColumnReader::CreateReader(DeriveLogicalType(s_ele), s_ele, next_file_idx++, max_define, max_repeat);
198: 	}
199: }
200: 
201: // TODO we don't need readers for columns we are not going to read ay
202: static unique_ptr<ColumnReader> CreateReader(const FileMetaData *file_meta_data) {
203: 	idx_t next_schema_idx = 0;
204: 	idx_t next_file_idx = 0;
205: 
206: 	auto ret = CreateReaderRecursive(file_meta_data, 0, 0, 0, next_schema_idx, next_file_idx);
207: 	D_ASSERT(next_schema_idx == file_meta_data->schema.size() - 1);
208: 	D_ASSERT(file_meta_data->row_groups.empty() || next_file_idx == file_meta_data->row_groups[0].columns.size());
209: 	return ret;
210: }
211: 
212: void ParquetReader::InitializeSchema(const vector<LogicalType> &expected_types_p, const string &initial_filename_p) {
213: 	auto file_meta_data = GetFileMetadata();
214: 
215: 	if (file_meta_data->__isset.encryption_algorithm) {
216: 		throw FormatException("Encrypted Parquet files are not supported");
217: 	}
218: 	// check if we like this schema
219: 	if (file_meta_data->schema.size() < 2) {
220: 		throw FormatException("Need at least one non-root column in the file");
221: 	}
222: 
223: 	bool has_expected_types = !expected_types_p.empty();
224: 	auto root_reader = CreateReader(file_meta_data);
225: 
226: 	auto root_type = root_reader->Type();
227: 	D_ASSERT(root_type.id() == LogicalTypeId::STRUCT);
228: 	idx_t col_idx = 0;
229: 	for (auto &type_pair : root_type.child_types()) {
230: 		if (has_expected_types && expected_types_p[col_idx] != type_pair.second) {
231: 			if (initial_filename_p.empty()) {
232: 				throw FormatException("column \"%d\" in parquet file is of type %s, could not auto cast to "
233: 				                      "expected type %s for this column",
234: 				                      col_idx, type_pair.second, expected_types_p[col_idx].ToString());
235: 			} else {
236: 				throw FormatException("schema mismatch in Parquet glob: column \"%d\" in parquet file is of type "
237: 				                      "%s, but in the original file \"%s\" this column is of type \"%s\"",
238: 				                      col_idx, type_pair.second, initial_filename_p,
239: 				                      expected_types_p[col_idx].ToString());
240: 			}
241: 		} else {
242: 			names.push_back(type_pair.first);
243: 			return_types.push_back(type_pair.second);
244: 		}
245: 		col_idx++;
246: 	}
247: 	D_ASSERT(!names.empty());
248: 	D_ASSERT(!return_types.empty());
249: }
250: 
251: ParquetReader::ParquetReader(unique_ptr<FileHandle> file_handle_p, const vector<LogicalType> &expected_types_p,
252:                              const string &initial_filename_p) {
253: 	file_name = file_handle_p->path;
254: 	file_handle = move(file_handle_p);
255: 	metadata = LoadMetadata(*file_handle);
256: 	InitializeSchema(expected_types_p, initial_filename_p);
257: }
258: 
259: ParquetReader::ParquetReader(ClientContext &context_p, string file_name_p, const vector<LogicalType> &expected_types_p,
260:                              const string &initial_filename_p) {
261: 	auto &fs = FileSystem::GetFileSystem(context_p);
262: 	file_name = move(file_name_p);
263: 	file_handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ);
264: 	// If object cached is disabled
265: 	// or if this file has cached metadata
266: 	// or if the cached version already expired
267: 
268: 	auto last_modify_time = fs.GetLastModifiedTime(*file_handle);
269: 	if (!ObjectCache::ObjectCacheEnabled(context_p)) {
270: 		metadata = LoadMetadata(*file_handle);
271: 	} else {
272: 		metadata =
273: 		    std::dynamic_pointer_cast<ParquetFileMetadataCache>(ObjectCache::GetObjectCache(context_p).Get(file_name));
274: 		if (!metadata || (last_modify_time + 10 >= metadata->read_time)) {
275: 			metadata = LoadMetadata(*file_handle);
276: 			ObjectCache::GetObjectCache(context_p).Put(file_name, metadata);
277: 		}
278: 	}
279: 
280: 	InitializeSchema(expected_types_p, initial_filename_p);
281: }
282: 
283: ParquetReader::~ParquetReader() {
284: }
285: 
286: const FileMetaData *ParquetReader::GetFileMetadata() {
287: 	D_ASSERT(metadata);
288: 	D_ASSERT(metadata->metadata);
289: 	return metadata->metadata.get();
290: }
291: 
292: // TODO also somewhat ugly, perhaps this can be moved to the column reader too
293: unique_ptr<BaseStatistics> ParquetReader::ReadStatistics(LogicalType &type, column_t file_col_idx,
294:                                                          const FileMetaData *file_meta_data) {
295: 	unique_ptr<BaseStatistics> column_stats;
296: 	auto root_reader = CreateReader(file_meta_data);
297: 	auto column_reader = ((StructColumnReader *)root_reader.get())->GetChildReader(file_col_idx);
298: 
299: 	for (auto &row_group : file_meta_data->row_groups) {
300: 		auto chunk_stats = column_reader->Stats(row_group.columns);
301: 		if (!chunk_stats) {
302: 			return nullptr;
303: 		}
304: 		if (!column_stats) {
305: 			column_stats = move(chunk_stats);
306: 		} else {
307: 			column_stats->Merge(*chunk_stats);
308: 		}
309: 	}
310: 	return column_stats;
311: }
312: 
313: const RowGroup &ParquetReader::GetGroup(ParquetReaderScanState &state) {
314: 	auto file_meta_data = GetFileMetadata();
315: 	D_ASSERT(state.current_group >= 0 && (idx_t)state.current_group < state.group_idx_list.size());
316: 	D_ASSERT(state.group_idx_list[state.current_group] >= 0 &&
317: 	         state.group_idx_list[state.current_group] < file_meta_data->row_groups.size());
318: 	return file_meta_data->row_groups[state.group_idx_list[state.current_group]];
319: }
320: 
321: void ParquetReader::PrepareRowGroupBuffer(ParquetReaderScanState &state, idx_t out_col_idx) {
322: 	auto &group = GetGroup(state);
323: 
324: 	auto column_reader = ((StructColumnReader *)state.root_reader.get())->GetChildReader(state.column_ids[out_col_idx]);
325: 
326: 	// TODO move this to columnreader too
327: 	if (state.filters) {
328: 		auto stats = column_reader->Stats(group.columns);
329: 		// filters contain output chunk index, not file col idx!
330: 		auto filter_entry = state.filters->filters.find(out_col_idx);
331: 		if (stats && filter_entry != state.filters->filters.end()) {
332: 			bool skip_chunk = false;
333: 			switch (column_reader->Type().id()) {
334: 			case LogicalTypeId::UTINYINT:
335: 			case LogicalTypeId::USMALLINT:
336: 			case LogicalTypeId::UINTEGER:
337: 			case LogicalTypeId::UBIGINT:
338: 			case LogicalTypeId::INTEGER:
339: 			case LogicalTypeId::BIGINT:
340: 			case LogicalTypeId::FLOAT:
341: 			case LogicalTypeId::TIMESTAMP:
342: 			case LogicalTypeId::DOUBLE: {
343: 				auto &num_stats = (NumericStatistics &)*stats;
344: 				for (auto &filter : filter_entry->second) {
345: 					skip_chunk = !num_stats.CheckZonemap(filter.comparison_type, filter.constant);
346: 					if (skip_chunk) {
347: 						break;
348: 					}
349: 				}
350: 				break;
351: 			}
352: 			case LogicalTypeId::BLOB:
353: 			case LogicalTypeId::VARCHAR: {
354: 				auto &str_stats = (StringStatistics &)*stats;
355: 				for (auto &filter : filter_entry->second) {
356: 					skip_chunk = !str_stats.CheckZonemap(filter.comparison_type, filter.constant.str_value);
357: 					if (skip_chunk) {
358: 						break;
359: 					}
360: 				}
361: 				break;
362: 			}
363: 			default:
364: 				break;
365: 			}
366: 			if (skip_chunk) {
367: 				state.group_offset = group.num_rows;
368: 				return;
369: 				// this effectively will skip this chunk
370: 			}
371: 		}
372: 	}
373: 
374: 	state.root_reader->IntializeRead(group.columns, *state.thrift_file_proto);
375: }
376: 
377: idx_t ParquetReader::NumRows() {
378: 	return GetFileMetadata()->num_rows;
379: }
380: 
381: idx_t ParquetReader::NumRowGroups() {
382: 	return GetFileMetadata()->row_groups.size();
383: }
384: 
385: void ParquetReader::InitializeScan(ParquetReaderScanState &state, vector<column_t> column_ids,
386:                                    vector<idx_t> groups_to_read, TableFilterSet *filters) {
387: 	state.current_group = -1;
388: 	state.finished = false;
389: 	state.column_ids = move(column_ids);
390: 	state.group_offset = 0;
391: 	state.group_idx_list = move(groups_to_read);
392: 	state.filters = filters;
393: 	state.sel.Initialize(STANDARD_VECTOR_SIZE);
394: 	state.file_handle = file_handle->file_system.OpenFile(file_handle->path, FileFlags::FILE_FLAGS_READ);
395: 	state.thrift_file_proto = CreateThriftProtocol(*state.file_handle);
396: 	state.root_reader = CreateReader(GetFileMetadata());
397: 
398: 	state.define_buf.resize(STANDARD_VECTOR_SIZE);
399: 	state.repeat_buf.resize(STANDARD_VECTOR_SIZE);
400: }
401: 
402: template <class T, class OP>
403: void TemplatedFilterOperation(Vector &v, T constant, parquet_filter_t &filter_mask, idx_t count) {
404: 	D_ASSERT(v.GetVectorType() == VectorType::FLAT_VECTOR); // we just created the damn thing it better be
405: 
406: 	auto v_ptr = FlatVector::GetData<T>(v);
407: 	auto &mask = FlatVector::Validity(v);
408: 
409: 	if (!mask.AllValid()) {
410: 		for (idx_t i = 0; i < count; i++) {
411: 			filter_mask[i] = filter_mask[i] && mask.RowIsValid(i) && OP::Operation(v_ptr[i], constant);
412: 		}
413: 	} else {
414: 		for (idx_t i = 0; i < count; i++) {
415: 			filter_mask[i] = filter_mask[i] && OP::Operation(v_ptr[i], constant);
416: 		}
417: 	}
418: }
419: 
420: template <class OP>
421: static void FilterOperationSwitch(Vector &v, Value &constant, parquet_filter_t &filter_mask, idx_t count) {
422: 	if (filter_mask.none() || count == 0) {
423: 		return;
424: 	}
425: 	switch (v.GetType().id()) {
426: 	case LogicalTypeId::BOOLEAN:
427: 		TemplatedFilterOperation<bool, OP>(v, constant.value_.boolean, filter_mask, count);
428: 		break;
429: 
430: 	case LogicalTypeId::UTINYINT:
431: 		TemplatedFilterOperation<uint8_t, OP>(v, constant.value_.utinyint, filter_mask, count);
432: 		break;
433: 
434: 	case LogicalTypeId::USMALLINT:
435: 		TemplatedFilterOperation<uint16_t, OP>(v, constant.value_.usmallint, filter_mask, count);
436: 		break;
437: 
438: 	case LogicalTypeId::UINTEGER:
439: 		TemplatedFilterOperation<uint32_t, OP>(v, constant.value_.uinteger, filter_mask, count);
440: 		break;
441: 
442: 	case LogicalTypeId::UBIGINT:
443: 		TemplatedFilterOperation<uint64_t, OP>(v, constant.value_.ubigint, filter_mask, count);
444: 		break;
445: 
446: 	case LogicalTypeId::INTEGER:
447: 		TemplatedFilterOperation<int32_t, OP>(v, constant.value_.integer, filter_mask, count);
448: 		break;
449: 
450: 	case LogicalTypeId::BIGINT:
451: 		TemplatedFilterOperation<int64_t, OP>(v, constant.value_.bigint, filter_mask, count);
452: 		break;
453: 
454: 	case LogicalTypeId::FLOAT:
455: 		TemplatedFilterOperation<float, OP>(v, constant.value_.float_, filter_mask, count);
456: 		break;
457: 
458: 	case LogicalTypeId::DOUBLE:
459: 		TemplatedFilterOperation<double, OP>(v, constant.value_.double_, filter_mask, count);
460: 		break;
461: 
462: 	case LogicalTypeId::TIMESTAMP:
463: 		TemplatedFilterOperation<timestamp_t, OP>(v, constant.value_.bigint, filter_mask, count);
464: 		break;
465: 
466: 	case LogicalTypeId::BLOB:
467: 	case LogicalTypeId::VARCHAR:
468: 		TemplatedFilterOperation<string_t, OP>(v, string_t(constant.str_value), filter_mask, count);
469: 		break;
470: 
471: 	default:
472: 		throw NotImplementedException("Unsupported type for filter %s", v.ToString());
473: 	}
474: }
475: 
476: void ParquetReader::Scan(ParquetReaderScanState &state, DataChunk &result) {
477: 	while (ScanInternal(state, result)) {
478: 		if (result.size() > 0) {
479: 			break;
480: 		}
481: 		result.Reset();
482: 	}
483: }
484: 
485: bool ParquetReader::ScanInternal(ParquetReaderScanState &state, DataChunk &result) {
486: 	if (state.finished) {
487: 		return false;
488: 	}
489: 
490: 	// see if we have to switch to the next row group in the parquet file
491: 	if (state.current_group < 0 || (int64_t)state.group_offset >= GetGroup(state).num_rows) {
492: 		state.current_group++;
493: 		state.group_offset = 0;
494: 
495: 		if ((idx_t)state.current_group == state.group_idx_list.size()) {
496: 			state.finished = true;
497: 			return false;
498: 		}
499: 
500: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
501: 			// this is a special case where we are not interested in the actual contents of the file
502: 			if (state.column_ids[out_col_idx] == COLUMN_IDENTIFIER_ROW_ID) {
503: 				continue;
504: 			}
505: 
506: 			PrepareRowGroupBuffer(state, out_col_idx);
507: 		}
508: 		return true;
509: 	}
510: 
511: 	auto this_output_chunk_rows = MinValue<idx_t>(STANDARD_VECTOR_SIZE, GetGroup(state).num_rows - state.group_offset);
512: 	result.SetCardinality(this_output_chunk_rows);
513: 
514: 	if (this_output_chunk_rows == 0) {
515: 		state.finished = true;
516: 		return false; // end of last group, we are done
517: 	}
518: 
519: 	// we evaluate simple table filters directly in this scan so we can skip decoding column data that's never going to
520: 	// be relevant
521: 	parquet_filter_t filter_mask;
522: 	filter_mask.set();
523: 
524: 	state.define_buf.zero();
525: 	state.repeat_buf.zero();
526: 
527: 	auto define_ptr = (uint8_t *)state.define_buf.ptr;
528: 	auto repeat_ptr = (uint8_t *)state.repeat_buf.ptr;
529: 
530: 	auto root_reader = ((StructColumnReader *)state.root_reader.get());
531: 
532: 	if (state.filters) {
533: 		vector<bool> need_to_read(result.ColumnCount(), true);
534: 
535: 		// first load the columns that are used in filters
536: 		for (auto &filter_col : state.filters->filters) {
537: 			auto file_col_idx = state.column_ids[filter_col.first];
538: 
539: 			if (filter_mask.none()) { // if no rows are left we can stop checking filters
540: 				break;
541: 			}
542: 
543: 			root_reader->GetChildReader(file_col_idx)
544: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[filter_col.first]);
545: 
546: 			need_to_read[filter_col.first] = false;
547: 
548: 			for (auto &filter : filter_col.second) {
549: 				switch (filter.comparison_type) {
550: 				case ExpressionType::COMPARE_EQUAL:
551: 					FilterOperationSwitch<Equals>(result.data[filter_col.first], filter.constant, filter_mask,
552: 					                              this_output_chunk_rows);
553: 					break;
554: 				case ExpressionType::COMPARE_LESSTHAN:
555: 					FilterOperationSwitch<LessThan>(result.data[filter_col.first], filter.constant, filter_mask,
556: 					                                this_output_chunk_rows);
557: 					break;
558: 				case ExpressionType::COMPARE_LESSTHANOREQUALTO:
559: 					FilterOperationSwitch<LessThanEquals>(result.data[filter_col.first], filter.constant, filter_mask,
560: 					                                      this_output_chunk_rows);
561: 					break;
562: 				case ExpressionType::COMPARE_GREATERTHAN:
563: 					FilterOperationSwitch<GreaterThan>(result.data[filter_col.first], filter.constant, filter_mask,
564: 					                                   this_output_chunk_rows);
565: 					break;
566: 				case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
567: 					FilterOperationSwitch<GreaterThanEquals>(result.data[filter_col.first], filter.constant,
568: 					                                         filter_mask, this_output_chunk_rows);
569: 					break;
570: 				default:
571: 					D_ASSERT(0);
572: 				}
573: 			}
574: 		}
575: 
576: 		// we still may have to read some cols
577: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
578: 			if (!need_to_read[out_col_idx]) {
579: 				continue;
580: 			}
581: 			auto file_col_idx = state.column_ids[out_col_idx];
582: 
583: 			if (filter_mask.none()) {
584: 				root_reader->GetChildReader(file_col_idx)->Skip(result.size());
585: 				continue;
586: 			}
587: 			// TODO handle ROWID here, too
588: 			root_reader->GetChildReader(file_col_idx)
589: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[out_col_idx]);
590: 		}
591: 
592: 		idx_t sel_size = 0;
593: 		for (idx_t i = 0; i < this_output_chunk_rows; i++) {
594: 			if (filter_mask[i]) {
595: 				state.sel.set_index(sel_size++, i);
596: 			}
597: 		}
598: 
599: 		result.Slice(state.sel, sel_size);
600: 		result.Verify();
601: 
602: 	} else { // #nofilter, just fricking load the data
603: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
604: 			auto file_col_idx = state.column_ids[out_col_idx];
605: 
606: 			if (file_col_idx == COLUMN_IDENTIFIER_ROW_ID) {
607: 				Value constant_42 = Value::BIGINT(42);
608: 				result.data[out_col_idx].Reference(constant_42);
609: 				continue;
610: 			}
611: 
612: 			root_reader->GetChildReader(file_col_idx)
613: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[out_col_idx]);
614: 		}
615: 	}
616: 
617: 	state.group_offset += this_output_chunk_rows;
618: 	return true;
619: }
620: 
621: } // namespace duckdb
[end of extension/parquet/parquet_reader.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: