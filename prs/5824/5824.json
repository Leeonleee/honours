{
  "repo": "duckdb/duckdb",
  "pull_number": 5824,
  "instance_id": "duckdb__duckdb-5824",
  "issue_numbers": [
    "5759"
  ],
  "base_commit": "2dcdc245566bb9a0fc717bc9d2c3867f911abc6d",
  "patch": "diff --git a/third_party/fsst/libfsst.cpp b/third_party/fsst/libfsst.cpp\nindex e651a6147c82..96a53437fccf 100644\n--- a/third_party/fsst/libfsst.cpp\n+++ b/third_party/fsst/libfsst.cpp\n@@ -454,21 +454,24 @@ static inline size_t compressBulk(SymbolTable &symbolTable, size_t nlines, size_\n #define FSST_SAMPLELINE ((size_t) 512)\n \n // quickly select a uniformly random set of lines such that we have between [FSST_SAMPLETARGET,FSST_SAMPLEMAXSZ) string bytes\n-vector<u8*> makeSample(u8* sampleBuf, u8* strIn[], size_t **lenRef, size_t nlines) {\n-\tsize_t totSize = 0, *lenIn = *lenRef;\n+vector<u8*> makeSample(u8* sampleBuf, u8* strIn[], size_t *lenIn, size_t nlines,\n+                                                    unique_ptr<vector<size_t>>& sample_len_out) {\n+\tsize_t totSize = 0;\n \tvector<u8*> sample;\n \n \tfor(size_t i=0; i<nlines; i++)\n \t\ttotSize += lenIn[i];\n-\n \tif (totSize < FSST_SAMPLETARGET) {\n \t\tfor(size_t i=0; i<nlines; i++)\n \t\t\tsample.push_back(strIn[i]);\n \t} else {\n \t\tsize_t sampleRnd = FSST_HASH(4637947);\n \t\tu8* sampleLim = sampleBuf + FSST_SAMPLETARGET;\n-\t\tsize_t *sampleLen = *lenRef = new size_t[nlines + FSST_SAMPLEMAXSZ/FSST_SAMPLELINE];\n \n+\t\tsample_len_out = unique_ptr<vector<size_t>>(new vector<size_t>());\n+\t\tsample_len_out->reserve(nlines + FSST_SAMPLEMAXSZ/FSST_SAMPLELINE);\n+\n+\t\t// This fails if we have a lot of small strings and a few big ones?\n \t\twhile(sampleBuf < sampleLim) {\n \t\t\t// choose a non-empty line\n \t\t\tsampleRnd = FSST_HASH(sampleRnd);\n@@ -485,7 +488,9 @@ vector<u8*> makeSample(u8* sampleBuf, u8* strIn[], size_t **lenRef, size_t nline\n \t\t\tsize_t len = min(lenIn[linenr]-chunk,FSST_SAMPLELINE);\n \t\t\tmemcpy(sampleBuf, strIn[linenr]+chunk, len);\n \t\t\tsample.push_back(sampleBuf);\n-\t\t\tsampleBuf += *sampleLen++ = len;\n+\n+\t\t\tsample_len_out->push_back(len);\n+\t\t\tsampleBuf += len;\n \t\t}\n \t}\n \treturn sample;\n@@ -493,11 +498,11 @@ vector<u8*> makeSample(u8* sampleBuf, u8* strIn[], size_t **lenRef, size_t nline\n \n extern \"C\" duckdb_fsst_encoder_t* duckdb_fsst_create(size_t n, size_t lenIn[], u8 *strIn[], int zeroTerminated) {\n \tu8* sampleBuf = new u8[FSST_SAMPLEMAXSZ];\n-\tsize_t *sampleLen = lenIn;\n-\tvector<u8*> sample = makeSample(sampleBuf, strIn, &sampleLen, n?n:1); // careful handling of input to get a right-size and representative sample\n+\tunique_ptr<vector<size_t>> sample_sizes;\n+\tvector<u8*> sample = makeSample(sampleBuf, strIn, lenIn, n?n:1, sample_sizes); // careful handling of input to get a right-size and representative sample\n \tEncoder *encoder = new Encoder();\n+\tsize_t* sampleLen = sample_sizes ? sample_sizes->data() : &lenIn[0];\n \tencoder->symbolTable = shared_ptr<SymbolTable>(buildSymbolTable(encoder->counters, sample, sampleLen, zeroTerminated));\n-\tif (sampleLen != lenIn) delete[] sampleLen;\n \tdelete[] sampleBuf;\n \treturn (duckdb_fsst_encoder_t*) encoder;\n }\n",
  "test_patch": "diff --git a/test/sql/storage/compression/fsst/issue_5759.test b/test/sql/storage/compression/fsst/issue_5759.test\nnew file mode 100644\nindex 000000000000..e30e06fb8e05\n--- /dev/null\n+++ b/test/sql/storage/compression/fsst/issue_5759.test\n@@ -0,0 +1,12 @@\n+# name: test/sql/storage/compression/fsst/issue_5759.test\n+# description: Issue #5759: segfault on sample creation\n+# group: [fsst]\n+\n+load __TEST_DIR__/issue_5759.db\n+\n+statement ok\n+pragma force_compression='fsst'\n+\n+# With many short and a few small strings, the sample calculation would overflow\n+statement ok\n+CREATE TABLE trigger5759 AS SELECT CASE WHEN RANDOM() > 0.95 THEN repeat('ab', 1500) ELSE 'c' END FROM range(0,1000);\n\\ No newline at end of file\n",
  "problem_statement": "Segmentation fault (or various memory errors) on WAL application\n### What happens?\r\n\r\nWhen applying a WAL file from an `INSERT INTO ... SELECT FROM ... GROUP BY`, a variety of errors occur, including:\r\n* corrupted double-linked list (Linux / AMD64)\r\n* Segmentation fault (OSX)\r\n* Also the below set of logs (OSX):\r\n```\r\nduckdb(31324,0x119e43600) malloc: *** error for object 0x6: pointer being freed was not allocated\r\nduckdb(31324,0x119e43600) malloc: *** set a breakpoint in malloc_error_break to debug\r\nAbort trap: 6\r\n```\r\n\r\nIf the group-by is removed, it does not occur.\r\n\r\nIf the inserts are broken up using LIMIT/OFFSET with breaks inbetween, this also works.\r\n\r\nWill upload the CSV.\r\n\r\n### To Reproduce\r\n\r\n```\r\nCREATE TABLE temp AS SELECT column0, last(column1) FROM 'anon-data.csv.gz' GROUP BY column0;\r\n```\r\n\r\n### OS:\r\n\r\nOSX, Linux AMD64\r\n\r\n### DuckDB Version:\r\n\r\n* 0.6.1\r\n* Also tried latest completed build off master - a3ea1654263e8f844ba6fad4ebf3f535d7aa52f1\r\n\r\n### DuckDB Client:\r\n\r\nCLI\r\n\r\n### Full Name:\r\n\r\nPaul Ellsworth\r\n\r\n### Affiliation:\r\n\r\nTenable, Inc.\r\n\r\n### Have you tried this on the latest `master` branch?\r\n\r\n- [X] I agree\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] I agree\n",
  "hints_text": "Data used to reproduce:\r\n[anon-data.csv.gz](https://github.com/duckdb/duckdb/files/10280289/anon-data.csv.gz)\nThe group-by clause seems to make the problem show up very repeatedly; however, it _sometimes_ shows up even without it; for example:\r\n\r\n```\r\n$ rm -f test.db ; rm -f test.db.wal ; duckdb test.db \"create table test as select * FROM 'anon-data.csv.gz';\"\r\nduckdb(40732,0x111832600) malloc: *** error for object 0x7fc4cb809404: pointer being freed was not allocated\r\nduckdb(40732,0x111832600) malloc: *** set a breakpoint in malloc_error_break to debug\r\nAbort trap: 6\r\n```\nThanks for the report! I can verify that this seems to be causing a crash in the `FSST` compression algorithm. @samansmink could you perhaps pick this up after the holidays?\nThanks, @Mytherin :) Is there a way to bypass FSST?\n@paulewog you can use this pragma intended for testing: \r\n\r\n```\r\npragma force_compression=\"dictionary\";\r\n```\r\n\r\nthis will disable all other compression schemes except dictionary encoding. Note that this will also disable the other compression schemes though, so your storage size will increase quite a bit.\r\n\r\nCurrently you can only pick one compression scheme to force or disable them all using:\r\n\r\n```\r\npragma force_compression=\"uncompressed\";\r\n```\r\n\nOk, thank you!  I'll test it out, if only to validate :)\n(I can confirm that `pragma force_compression=\"dictionary\";` does make things pass. :) )",
  "created_at": "2023-01-04T15:54:23Z"
}