You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
The Parquet reader considers 'FOO' and 'foo' as duplicate column names when different cases are used for the column name and the partition key.
### What happens?

When a Parquet dataset contains a column names and a partition key which are case-sensitive different but case-insensitive the same, DuckDB fails when reading it.



### To Reproduce

1. Untar this file :
[people.tar.gz](https://github.com/user-attachments/files/18048196/people.tar.gz)

2. use the following SQL

```
.echo on

FROM read_parquet('people/**/*.parquet', hive_partitioning = false);
FROM read_parquet('people/**/*.parquet');
```

3. results : 

```
duckdb-1.1.3 < a.sql

FROM read_parquet('people/**/*.parquet', hive_partitioning = false);
┌─────────┬───────┐
│  col1   │ col2  │
│ varchar │ int32 │
├─────────┼───────┤
│ Hannes  │     2 │
│ Mark    │     1 │
└─────────┴───────┘
Binder Error: table "read_parquet" has duplicate column name "COL1"
```


### OS:

ubuntu 22.04 x86_64 

### DuckDB Version:

1.1.3 and bleeding

### DuckDB Client:

cli 

### Hardware:

_No response_

### Full Name:

Nicolas chuche

### Affiliation:

Ministere de la transition écologique

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a nightly build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have
The Parquet reader considers 'FOO' and 'foo' as duplicate column names when different cases are used for the column name and the partition key.
### What happens?

When a Parquet dataset contains a column names and a partition key which are case-sensitive different but case-insensitive the same, DuckDB fails when reading it.



### To Reproduce

1. Untar this file :
[people.tar.gz](https://github.com/user-attachments/files/18048196/people.tar.gz)

2. use the following SQL

```
.echo on

FROM read_parquet('people/**/*.parquet', hive_partitioning = false);
FROM read_parquet('people/**/*.parquet');
```

3. results : 

```
duckdb-1.1.3 < a.sql

FROM read_parquet('people/**/*.parquet', hive_partitioning = false);
┌─────────┬───────┐
│  col1   │ col2  │
│ varchar │ int32 │
├─────────┼───────┤
│ Hannes  │     2 │
│ Mark    │     1 │
└─────────┴───────┘
Binder Error: table "read_parquet" has duplicate column name "COL1"
```


### OS:

ubuntu 22.04 x86_64 

### DuckDB Version:

1.1.3 and bleeding

### DuckDB Client:

cli 

### Hardware:

_No response_

### Full Name:

Nicolas chuche

### Affiliation:

Ministere de la transition écologique

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a nightly build

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://duckdb.org/docs/installation/) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of src/common/multi_file_reader.cpp]
1: #include "duckdb/common/multi_file_reader.hpp"
2: 
3: #include "duckdb/common/exception.hpp"
4: #include "duckdb/common/hive_partitioning.hpp"
5: #include "duckdb/common/types.hpp"
6: #include "duckdb/common/types/value.hpp"
7: #include "duckdb/function/function_set.hpp"
8: #include "duckdb/function/table_function.hpp"
9: #include "duckdb/main/config.hpp"
10: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
11: #include "duckdb/common/string_util.hpp"
12: 
13: #include <algorithm>
14: 
15: namespace duckdb {
16: 
17: MultiFileReaderGlobalState::~MultiFileReaderGlobalState() {
18: }
19: 
20: MultiFileReader::~MultiFileReader() {
21: }
22: 
23: unique_ptr<MultiFileReader> MultiFileReader::Create(const TableFunction &table_function) {
24: 	unique_ptr<MultiFileReader> res;
25: 	if (table_function.get_multi_file_reader) {
26: 		res = table_function.get_multi_file_reader(table_function);
27: 		res->function_name = table_function.name;
28: 	} else {
29: 		res = make_uniq<MultiFileReader>();
30: 		res->function_name = table_function.name;
31: 	}
32: 	return res;
33: }
34: 
35: unique_ptr<MultiFileReader> MultiFileReader::CreateDefault(const string &function_name) {
36: 	auto res = make_uniq<MultiFileReader>();
37: 	res->function_name = function_name;
38: 	return res;
39: }
40: 
41: Value MultiFileReader::CreateValueFromFileList(const vector<string> &file_list) {
42: 	vector<Value> files;
43: 	for (auto &file : file_list) {
44: 		files.push_back(file);
45: 	}
46: 	return Value::LIST(LogicalType::VARCHAR, std::move(files));
47: }
48: 
49: void MultiFileReader::AddParameters(TableFunction &table_function) {
50: 	table_function.named_parameters["filename"] = LogicalType::ANY;
51: 	table_function.named_parameters["hive_partitioning"] = LogicalType::BOOLEAN;
52: 	table_function.named_parameters["union_by_name"] = LogicalType::BOOLEAN;
53: 	table_function.named_parameters["hive_types"] = LogicalType::ANY;
54: 	table_function.named_parameters["hive_types_autocast"] = LogicalType::BOOLEAN;
55: }
56: 
57: vector<string> MultiFileReader::ParsePaths(const Value &input) {
58: 	if (input.IsNull()) {
59: 		throw ParserException("%s cannot take NULL list as parameter", function_name);
60: 	}
61: 
62: 	if (input.type().id() == LogicalTypeId::VARCHAR) {
63: 		return {StringValue::Get(input)};
64: 	} else if (input.type().id() == LogicalTypeId::LIST) {
65: 		vector<string> paths;
66: 		for (auto &val : ListValue::GetChildren(input)) {
67: 			if (val.IsNull()) {
68: 				throw ParserException("%s reader cannot take NULL input as parameter", function_name);
69: 			}
70: 			if (val.type().id() != LogicalTypeId::VARCHAR) {
71: 				throw ParserException("%s reader can only take a list of strings as a parameter", function_name);
72: 			}
73: 			paths.push_back(StringValue::Get(val));
74: 		}
75: 		return paths;
76: 	} else {
77: 		throw InternalException("Unsupported type for MultiFileReader::ParsePaths called with: '%s'");
78: 	}
79: }
80: 
81: shared_ptr<MultiFileList> MultiFileReader::CreateFileList(ClientContext &context, const vector<string> &paths,
82:                                                           FileGlobOptions options) {
83: 	vector<string> result_files;
84: 
85: 	auto res = make_uniq<GlobMultiFileList>(context, paths, options);
86: 	if (res->GetExpandResult() == FileExpandResult::NO_FILES && options == FileGlobOptions::DISALLOW_EMPTY) {
87: 		throw IOException("%s needs at least one file to read", function_name);
88: 	}
89: 	return std::move(res);
90: }
91: 
92: shared_ptr<MultiFileList> MultiFileReader::CreateFileList(ClientContext &context, const Value &input,
93:                                                           FileGlobOptions options) {
94: 	auto paths = ParsePaths(input);
95: 	return CreateFileList(context, paths, options);
96: }
97: 
98: bool MultiFileReader::ParseOption(const string &key, const Value &val, MultiFileReaderOptions &options,
99:                                   ClientContext &context) {
100: 	auto loption = StringUtil::Lower(key);
101: 	if (loption == "filename") {
102: 		if (val.type() == LogicalType::VARCHAR) {
103: 			// If not, we interpret it as the name of the column containing the filename
104: 			options.filename = true;
105: 			options.filename_column = StringValue::Get(val);
106: 		} else {
107: 			Value boolean_value;
108: 			string error_message;
109: 			if (val.DefaultTryCastAs(LogicalType::BOOLEAN, boolean_value, &error_message)) {
110: 				// If the argument can be cast to boolean, we just interpret it as a boolean
111: 				options.filename = BooleanValue::Get(boolean_value);
112: 			}
113: 		}
114: 	} else if (loption == "hive_partitioning") {
115: 		options.hive_partitioning = BooleanValue::Get(val);
116: 		options.auto_detect_hive_partitioning = false;
117: 	} else if (loption == "union_by_name") {
118: 		options.union_by_name = BooleanValue::Get(val);
119: 	} else if (loption == "hive_types_autocast" || loption == "hive_type_autocast") {
120: 		options.hive_types_autocast = BooleanValue::Get(val);
121: 	} else if (loption == "hive_types" || loption == "hive_type") {
122: 		if (val.type().id() != LogicalTypeId::STRUCT) {
123: 			throw InvalidInputException(
124: 			    "'hive_types' only accepts a STRUCT('name':VARCHAR, ...), but '%s' was provided",
125: 			    val.type().ToString());
126: 		}
127: 		// verify that that all the children of the struct value are VARCHAR
128: 		auto &children = StructValue::GetChildren(val);
129: 		for (idx_t i = 0; i < children.size(); i++) {
130: 			const Value &child = children[i];
131: 			if (child.type().id() != LogicalType::VARCHAR) {
132: 				throw InvalidInputException("hive_types: '%s' must be a VARCHAR, instead: '%s' was provided",
133: 				                            StructType::GetChildName(val.type(), i), child.type().ToString());
134: 			}
135: 			// for every child of the struct, get the logical type
136: 			LogicalType transformed_type = TransformStringToLogicalType(child.ToString(), context);
137: 			const string &name = StructType::GetChildName(val.type(), i);
138: 			options.hive_types_schema[name] = transformed_type;
139: 		}
140: 		D_ASSERT(!options.hive_types_schema.empty());
141: 	} else {
142: 		return false;
143: 	}
144: 	return true;
145: }
146: 
147: unique_ptr<MultiFileList> MultiFileReader::ComplexFilterPushdown(ClientContext &context, MultiFileList &files,
148:                                                                  const MultiFileReaderOptions &options,
149:                                                                  MultiFilePushdownInfo &info,
150:                                                                  vector<unique_ptr<Expression>> &filters) {
151: 	return files.ComplexFilterPushdown(context, options, info, filters);
152: }
153: 
154: unique_ptr<MultiFileList> MultiFileReader::DynamicFilterPushdown(ClientContext &context, const MultiFileList &files,
155:                                                                  const MultiFileReaderOptions &options,
156:                                                                  const vector<string> &names,
157:                                                                  const vector<LogicalType> &types,
158:                                                                  const vector<column_t> &column_ids,
159:                                                                  TableFilterSet &filters) {
160: 	return files.DynamicFilterPushdown(context, options, names, types, column_ids, filters);
161: }
162: 
163: bool MultiFileReader::Bind(MultiFileReaderOptions &options, MultiFileList &files, vector<LogicalType> &return_types,
164:                            vector<string> &names, MultiFileReaderBindData &bind_data) {
165: 	// The Default MultiFileReader can not perform any binding as it uses MultiFileLists with no schema information.
166: 	return false;
167: }
168: 
169: void MultiFileReader::BindOptions(MultiFileReaderOptions &options, MultiFileList &files,
170:                                   vector<LogicalType> &return_types, vector<string> &names,
171:                                   MultiFileReaderBindData &bind_data) {
172: 	// Add generated constant column for filename
173: 	if (options.filename) {
174: 		if (std::find(names.begin(), names.end(), options.filename_column) != names.end()) {
175: 			throw BinderException("Option filename adds column \"%s\", but a column with this name is also in the "
176: 			                      "file. Try setting a different name: filename='<filename column name>'",
177: 			                      options.filename_column);
178: 		}
179: 		bind_data.filename_idx = names.size();
180: 		return_types.emplace_back(LogicalType::VARCHAR);
181: 		names.emplace_back(options.filename_column);
182: 	}
183: 
184: 	// Add generated constant columns from hive partitioning scheme
185: 	if (options.hive_partitioning) {
186: 		D_ASSERT(files.GetExpandResult() != FileExpandResult::NO_FILES);
187: 		auto partitions = HivePartitioning::Parse(files.GetFirstFile());
188: 		// verify that all files have the same hive partitioning scheme
189: 		for (const auto &file : files.Files()) {
190: 			auto file_partitions = HivePartitioning::Parse(file);
191: 			for (auto &part_info : partitions) {
192: 				if (file_partitions.find(part_info.first) == file_partitions.end()) {
193: 					string error = "Hive partition mismatch between file \"%s\" and \"%s\": key \"%s\" not found";
194: 					if (options.auto_detect_hive_partitioning == true) {
195: 						throw InternalException(error + "(hive partitioning was autodetected)", files.GetFirstFile(),
196: 						                        file, part_info.first);
197: 					}
198: 					throw BinderException(error.c_str(), files.GetFirstFile(), file, part_info.first);
199: 				}
200: 			}
201: 			if (partitions.size() != file_partitions.size()) {
202: 				string error_msg = "Hive partition mismatch between file \"%s\" and \"%s\"";
203: 				if (options.auto_detect_hive_partitioning == true) {
204: 					throw InternalException(error_msg + "(hive partitioning was autodetected)", files.GetFirstFile(),
205: 					                        file);
206: 				}
207: 				throw BinderException(error_msg.c_str(), files.GetFirstFile(), file);
208: 			}
209: 		}
210: 
211: 		if (!options.hive_types_schema.empty()) {
212: 			// verify that all hive_types are existing partitions
213: 			options.VerifyHiveTypesArePartitions(partitions);
214: 		}
215: 
216: 		for (auto &part : partitions) {
217: 			idx_t hive_partitioning_index;
218: 			auto lookup = std::find(names.begin(), names.end(), part.first);
219: 			if (lookup != names.end()) {
220: 				// hive partitioning column also exists in file - override
221: 				auto idx = NumericCast<idx_t>(lookup - names.begin());
222: 				hive_partitioning_index = idx;
223: 				return_types[idx] = options.GetHiveLogicalType(part.first);
224: 			} else {
225: 				// hive partitioning column does not exist in file - add a new column containing the key
226: 				hive_partitioning_index = names.size();
227: 				return_types.emplace_back(options.GetHiveLogicalType(part.first));
228: 				names.emplace_back(part.first);
229: 			}
230: 			bind_data.hive_partitioning_indexes.emplace_back(part.first, hive_partitioning_index);
231: 		}
232: 	}
233: }
234: 
235: void MultiFileReader::FinalizeBind(const MultiFileReaderOptions &file_options, const MultiFileReaderBindData &options,
236:                                    const string &filename, const vector<string> &local_names,
237:                                    const vector<LogicalType> &global_types, const vector<string> &global_names,
238:                                    const vector<ColumnIndex> &global_column_ids, MultiFileReaderData &reader_data,
239:                                    ClientContext &context, optional_ptr<MultiFileReaderGlobalState> global_state) {
240: 
241: 	// create a map of name -> column index
242: 	case_insensitive_map_t<idx_t> name_map;
243: 	if (file_options.union_by_name) {
244: 		for (idx_t col_idx = 0; col_idx < local_names.size(); col_idx++) {
245: 			name_map[local_names[col_idx]] = col_idx;
246: 		}
247: 	}
248: 	for (idx_t i = 0; i < global_column_ids.size(); i++) {
249: 		auto &col_idx = global_column_ids[i];
250: 		if (col_idx.IsRowIdColumn()) {
251: 			// row-id
252: 			reader_data.constant_map.emplace_back(i, Value::BIGINT(42));
253: 			continue;
254: 		}
255: 		auto column_id = col_idx.GetPrimaryIndex();
256: 		if (column_id == options.filename_idx) {
257: 			// filename
258: 			reader_data.constant_map.emplace_back(i, Value(filename));
259: 			continue;
260: 		}
261: 		if (!options.hive_partitioning_indexes.empty()) {
262: 			// hive partition constants
263: 			auto partitions = HivePartitioning::Parse(filename);
264: 			D_ASSERT(partitions.size() == options.hive_partitioning_indexes.size());
265: 			bool found_partition = false;
266: 			for (auto &entry : options.hive_partitioning_indexes) {
267: 				if (column_id == entry.index) {
268: 					Value value = file_options.GetHivePartitionValue(partitions[entry.value], entry.value, context);
269: 					reader_data.constant_map.emplace_back(i, value);
270: 					found_partition = true;
271: 					break;
272: 				}
273: 			}
274: 			if (found_partition) {
275: 				continue;
276: 			}
277: 		}
278: 		if (file_options.union_by_name) {
279: 			auto &global_name = global_names[column_id];
280: 			auto entry = name_map.find(global_name);
281: 			bool not_present_in_file = entry == name_map.end();
282: 			if (not_present_in_file) {
283: 				// we need to project a column with name \"global_name\" - but it does not exist in the current file
284: 				// push a NULL value of the specified type
285: 				reader_data.constant_map.emplace_back(i, Value(global_types[column_id]));
286: 				continue;
287: 			}
288: 		}
289: 	}
290: }
291: 
292: unique_ptr<MultiFileReaderGlobalState>
293: MultiFileReader::InitializeGlobalState(ClientContext &context, const MultiFileReaderOptions &file_options,
294:                                        const MultiFileReaderBindData &bind_data, const MultiFileList &file_list,
295:                                        const vector<LogicalType> &global_types, const vector<string> &global_names,
296:                                        const vector<ColumnIndex> &global_column_ids) {
297: 	// By default, the multifilereader does not require any global state
298: 	return nullptr;
299: }
300: 
301: void MultiFileReader::CreateNameMapping(const string &file_name, const vector<LogicalType> &local_types,
302:                                         const vector<string> &local_names, const vector<LogicalType> &global_types,
303:                                         const vector<string> &global_names,
304:                                         const vector<ColumnIndex> &global_column_ids, MultiFileReaderData &reader_data,
305:                                         const string &initial_file,
306:                                         optional_ptr<MultiFileReaderGlobalState> global_state) {
307: 	D_ASSERT(global_types.size() == global_names.size());
308: 	D_ASSERT(local_types.size() == local_names.size());
309: 	// we have expected types: create a map of name -> column index
310: 	case_insensitive_map_t<idx_t> name_map;
311: 	for (idx_t col_idx = 0; col_idx < local_names.size(); col_idx++) {
312: 		name_map[local_names[col_idx]] = col_idx;
313: 	}
314: 	for (idx_t i = 0; i < global_column_ids.size(); i++) {
315: 		// check if this is a constant column
316: 		bool constant = false;
317: 		for (auto &entry : reader_data.constant_map) {
318: 			if (entry.column_id == i) {
319: 				constant = true;
320: 				break;
321: 			}
322: 		}
323: 		if (constant) {
324: 			// this column is constant for this file
325: 			continue;
326: 		}
327: 		// not constant - look up the column in the name map
328: 		auto &global_idx = global_column_ids[i];
329: 		auto global_id = global_idx.GetPrimaryIndex();
330: 		if (global_id >= global_types.size()) {
331: 			throw InternalException(
332: 			    "MultiFileReader::CreatePositionalMapping - global_id is out of range in global_types for this file");
333: 		}
334: 		auto &global_name = global_names[global_id];
335: 		auto entry = name_map.find(global_name);
336: 		if (entry == name_map.end()) {
337: 			string candidate_names;
338: 			for (auto &local_name : local_names) {
339: 				if (!candidate_names.empty()) {
340: 					candidate_names += ", ";
341: 				}
342: 				candidate_names += local_name;
343: 			}
344: 			throw IOException(
345: 			    StringUtil::Format("Failed to read file \"%s\": schema mismatch in glob: column \"%s\" was read from "
346: 			                       "the original file \"%s\", but could not be found in file \"%s\".\nCandidate names: "
347: 			                       "%s\nIf you are trying to "
348: 			                       "read files with different schemas, try setting union_by_name=True",
349: 			                       file_name, global_name, initial_file, file_name, candidate_names));
350: 		}
351: 		// we found the column in the local file - check if the types are the same
352: 		auto local_id = entry->second;
353: 		D_ASSERT(global_id < global_types.size());
354: 		D_ASSERT(local_id < local_types.size());
355: 		auto &global_type = global_types[global_id];
356: 		auto &local_type = local_types[local_id];
357: 		ColumnIndex local_index(local_id);
358: 		if (global_type != local_type) {
359: 			// the types are not the same - add a cast
360: 			reader_data.cast_map[local_id] = global_type;
361: 		} else {
362: 			local_index = ColumnIndex(local_id, global_idx.GetChildIndexes());
363: 		}
364: 		// create the mapping
365: 		reader_data.column_mapping.push_back(i);
366: 		reader_data.column_ids.push_back(local_id);
367: 		reader_data.column_indexes.push_back(std::move(local_index));
368: 	}
369: 
370: 	reader_data.empty_columns = reader_data.column_indexes.empty();
371: }
372: 
373: void MultiFileReader::CreateMapping(const string &file_name, const vector<LogicalType> &local_types,
374:                                     const vector<string> &local_names, const vector<LogicalType> &global_types,
375:                                     const vector<string> &global_names, const vector<ColumnIndex> &global_column_ids,
376:                                     optional_ptr<TableFilterSet> filters, MultiFileReaderData &reader_data,
377:                                     const string &initial_file, const MultiFileReaderBindData &options,
378:                                     optional_ptr<MultiFileReaderGlobalState> global_state) {
379: 	CreateNameMapping(file_name, local_types, local_names, global_types, global_names, global_column_ids, reader_data,
380: 	                  initial_file, global_state);
381: 	CreateFilterMap(global_types, filters, reader_data, global_state);
382: }
383: 
384: void MultiFileReader::CreateFilterMap(const vector<LogicalType> &global_types, optional_ptr<TableFilterSet> filters,
385:                                       MultiFileReaderData &reader_data,
386:                                       optional_ptr<MultiFileReaderGlobalState> global_state) {
387: 	if (filters) {
388: 		auto filter_map_size = global_types.size();
389: 		if (global_state) {
390: 			filter_map_size += global_state->extra_columns.size();
391: 		}
392: 		reader_data.filter_map.resize(filter_map_size);
393: 
394: 		for (idx_t c = 0; c < reader_data.column_mapping.size(); c++) {
395: 			auto map_index = reader_data.column_mapping[c];
396: 			reader_data.filter_map[map_index].index = c;
397: 			reader_data.filter_map[map_index].is_constant = false;
398: 		}
399: 		for (idx_t c = 0; c < reader_data.constant_map.size(); c++) {
400: 			auto constant_index = reader_data.constant_map[c].column_id;
401: 			reader_data.filter_map[constant_index].index = c;
402: 			reader_data.filter_map[constant_index].is_constant = true;
403: 		}
404: 	}
405: }
406: 
407: void MultiFileReader::FinalizeChunk(ClientContext &context, const MultiFileReaderBindData &bind_data,
408:                                     const MultiFileReaderData &reader_data, DataChunk &chunk,
409:                                     optional_ptr<MultiFileReaderGlobalState> global_state) {
410: 	// reference all the constants set up in MultiFileReader::FinalizeBind
411: 	for (auto &entry : reader_data.constant_map) {
412: 		chunk.data[entry.column_id].Reference(entry.value);
413: 	}
414: 	chunk.Verify();
415: }
416: 
417: void MultiFileReader::GetPartitionData(ClientContext &context, const MultiFileReaderBindData &bind_data,
418:                                        const MultiFileReaderData &reader_data,
419:                                        optional_ptr<MultiFileReaderGlobalState> global_state,
420:                                        const OperatorPartitionInfo &partition_info,
421:                                        OperatorPartitionData &partition_data) {
422: 	for (auto &col : partition_info.partition_columns) {
423: 		bool found_constant = false;
424: 		for (auto &constant : reader_data.constant_map) {
425: 			if (constant.column_id == col) {
426: 				found_constant = true;
427: 				partition_data.partition_data.emplace_back(constant.value);
428: 				break;
429: 			}
430: 		}
431: 		if (!found_constant) {
432: 			throw InternalException(
433: 			    "MultiFileReader::GetPartitionData - did not find constant for the given partition");
434: 		}
435: 	}
436: }
437: 
438: TablePartitionInfo MultiFileReader::GetPartitionInfo(ClientContext &context, const MultiFileReaderBindData &bind_data,
439:                                                      TableFunctionPartitionInput &input) {
440: 	// check if all of the columns are in the hive partition set
441: 	for (auto &partition_col : input.partition_ids) {
442: 		// check if this column is in the hive partitioned set
443: 		bool found = false;
444: 		for (auto &partition : bind_data.hive_partitioning_indexes) {
445: 			if (partition.index == partition_col) {
446: 				found = true;
447: 				break;
448: 			}
449: 		}
450: 		if (!found) {
451: 			// the column is not partitioned - hive partitioning alone can't guarantee the groups are partitioned
452: 			return TablePartitionInfo::NOT_PARTITIONED;
453: 		}
454: 	}
455: 	// if all columns are in the hive partitioning set, we know that each partition will only have a single value
456: 	// i.e. if the hive partitioning is by (YEAR, MONTH), each partition will have a single unique (YEAR, MONTH)
457: 	return TablePartitionInfo::SINGLE_VALUE_PARTITIONS;
458: }
459: 
460: TableFunctionSet MultiFileReader::CreateFunctionSet(TableFunction table_function) {
461: 	TableFunctionSet function_set(table_function.name);
462: 	function_set.AddFunction(table_function);
463: 	D_ASSERT(table_function.arguments.size() >= 1 && table_function.arguments[0] == LogicalType::VARCHAR);
464: 	table_function.arguments[0] = LogicalType::LIST(LogicalType::VARCHAR);
465: 	function_set.AddFunction(std::move(table_function));
466: 	return function_set;
467: }
468: 
469: HivePartitioningIndex::HivePartitioningIndex(string value_p, idx_t index) : value(std::move(value_p)), index(index) {
470: }
471: 
472: void MultiFileReaderOptions::AddBatchInfo(BindInfo &bind_info) const {
473: 	bind_info.InsertOption("filename", Value(filename_column));
474: 	bind_info.InsertOption("hive_partitioning", Value::BOOLEAN(hive_partitioning));
475: 	bind_info.InsertOption("auto_detect_hive_partitioning", Value::BOOLEAN(auto_detect_hive_partitioning));
476: 	bind_info.InsertOption("union_by_name", Value::BOOLEAN(union_by_name));
477: 	bind_info.InsertOption("hive_types_autocast", Value::BOOLEAN(hive_types_autocast));
478: }
479: 
480: void UnionByName::CombineUnionTypes(const vector<string> &col_names, const vector<LogicalType> &sql_types,
481:                                     vector<LogicalType> &union_col_types, vector<string> &union_col_names,
482:                                     case_insensitive_map_t<idx_t> &union_names_map) {
483: 	D_ASSERT(col_names.size() == sql_types.size());
484: 
485: 	for (idx_t col = 0; col < col_names.size(); ++col) {
486: 		auto union_find = union_names_map.find(col_names[col]);
487: 
488: 		if (union_find != union_names_map.end()) {
489: 			// given same name , union_col's type must compatible with col's type
490: 			auto &current_type = union_col_types[union_find->second];
491: 			auto compatible_type = LogicalType::ForceMaxLogicalType(current_type, sql_types[col]);
492: 			union_col_types[union_find->second] = compatible_type;
493: 		} else {
494: 			union_names_map[col_names[col]] = union_col_names.size();
495: 			union_col_names.emplace_back(col_names[col]);
496: 			union_col_types.emplace_back(sql_types[col]);
497: 		}
498: 	}
499: }
500: 
501: bool MultiFileReaderOptions::AutoDetectHivePartitioningInternal(MultiFileList &files, ClientContext &context) {
502: 	auto first_file = files.GetFirstFile();
503: 	auto partitions = HivePartitioning::Parse(first_file);
504: 	if (partitions.empty()) {
505: 		// no partitions found in first file
506: 		return false;
507: 	}
508: 
509: 	for (const auto &file : files.Files()) {
510: 		auto new_partitions = HivePartitioning::Parse(file);
511: 		if (new_partitions.size() != partitions.size()) {
512: 			// partition count mismatch
513: 			return false;
514: 		}
515: 		for (auto &part : new_partitions) {
516: 			auto entry = partitions.find(part.first);
517: 			if (entry == partitions.end()) {
518: 				// differing partitions between files
519: 				return false;
520: 			}
521: 		}
522: 	}
523: 	return true;
524: }
525: void MultiFileReaderOptions::AutoDetectHiveTypesInternal(MultiFileList &files, ClientContext &context) {
526: 	const LogicalType candidates[] = {LogicalType::DATE, LogicalType::TIMESTAMP, LogicalType::BIGINT};
527: 
528: 	unordered_map<string, LogicalType> detected_types;
529: 	for (const auto &file : files.Files()) {
530: 		auto partitions = HivePartitioning::Parse(file);
531: 		if (partitions.empty()) {
532: 			return;
533: 		}
534: 
535: 		for (auto &part : partitions) {
536: 			const string &name = part.first;
537: 			if (hive_types_schema.find(name) != hive_types_schema.end()) {
538: 				// type was explicitly provided by the user
539: 				continue;
540: 			}
541: 			LogicalType detected_type = LogicalType::VARCHAR;
542: 			Value value(part.second);
543: 			for (auto &candidate : candidates) {
544: 				const bool success = value.TryCastAs(context, candidate, true);
545: 				if (success) {
546: 					detected_type = candidate;
547: 					break;
548: 				}
549: 			}
550: 			auto entry = detected_types.find(name);
551: 			if (entry == detected_types.end()) {
552: 				// type was not yet detected - insert it
553: 				detected_types.insert(make_pair(name, std::move(detected_type)));
554: 			} else {
555: 				// type was already detected - check if the type matches
556: 				// if not promote to VARCHAR
557: 				if (entry->second != detected_type) {
558: 					entry->second = LogicalType::VARCHAR;
559: 				}
560: 			}
561: 		}
562: 	}
563: 	for (auto &entry : detected_types) {
564: 		hive_types_schema.insert(make_pair(entry.first, std::move(entry.second)));
565: 	}
566: }
567: void MultiFileReaderOptions::AutoDetectHivePartitioning(MultiFileList &files, ClientContext &context) {
568: 	D_ASSERT(files.GetExpandResult() != FileExpandResult::NO_FILES);
569: 	const bool hp_explicitly_disabled = !auto_detect_hive_partitioning && !hive_partitioning;
570: 	const bool ht_enabled = !hive_types_schema.empty();
571: 	if (hp_explicitly_disabled && ht_enabled) {
572: 		throw InvalidInputException("cannot disable hive_partitioning when hive_types is enabled");
573: 	}
574: 	if (ht_enabled && auto_detect_hive_partitioning && !hive_partitioning) {
575: 		// hive_types flag implies hive_partitioning
576: 		hive_partitioning = true;
577: 		auto_detect_hive_partitioning = false;
578: 	}
579: 	if (auto_detect_hive_partitioning) {
580: 		hive_partitioning = AutoDetectHivePartitioningInternal(files, context);
581: 	}
582: 	if (hive_partitioning && hive_types_autocast) {
583: 		AutoDetectHiveTypesInternal(files, context);
584: 	}
585: }
586: void MultiFileReaderOptions::VerifyHiveTypesArePartitions(const std::map<string, string> &partitions) const {
587: 	for (auto &hive_type : hive_types_schema) {
588: 		if (partitions.find(hive_type.first) == partitions.end()) {
589: 			throw InvalidInputException("Unknown hive_type: \"%s\" does not appear to be a partition", hive_type.first);
590: 		}
591: 	}
592: }
593: LogicalType MultiFileReaderOptions::GetHiveLogicalType(const string &hive_partition_column) const {
594: 	if (!hive_types_schema.empty()) {
595: 		auto it = hive_types_schema.find(hive_partition_column);
596: 		if (it != hive_types_schema.end()) {
597: 			return it->second;
598: 		}
599: 	}
600: 	return LogicalType::VARCHAR;
601: }
602: 
603: bool MultiFileReaderOptions::AnySet() {
604: 	return filename || hive_partitioning || union_by_name;
605: }
606: 
607: Value MultiFileReaderOptions::GetHivePartitionValue(const string &value, const string &key,
608:                                                     ClientContext &context) const {
609: 	auto it = hive_types_schema.find(key);
610: 	if (it == hive_types_schema.end()) {
611: 		return HivePartitioning::GetValue(context, key, value, LogicalType::VARCHAR);
612: 	}
613: 	return HivePartitioning::GetValue(context, key, value, it->second);
614: }
615: 
616: } // namespace duckdb
[end of src/common/multi_file_reader.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: