{
  "repo": "duckdb/duckdb",
  "pull_number": 6613,
  "instance_id": "duckdb__duckdb-6613",
  "issue_numbers": [
    "6588"
  ],
  "base_commit": "33cb4bf11ed50a0b94d53f8cd4cae015de2e43ab",
  "patch": "diff --git a/extension/json/json_functions/read_json.cpp b/extension/json/json_functions/read_json.cpp\nindex 5ee3538c8669..ff11124a42f3 100644\n--- a/extension/json/json_functions/read_json.cpp\n+++ b/extension/json/json_functions/read_json.cpp\n@@ -20,6 +20,12 @@ void JSONScan::AutoDetect(ClientContext &context, JSONScanData &bind_data, vecto\n \tidx_t remaining = bind_data.sample_size;\n \twhile (remaining != 0) {\n \t\tallocator.Reset();\n+\n+\t\tif (gstate.file_index >= 10) {\n+\t\t\t// We really shouldn't open more than 10 files when sampling\n+\t\t\tbreak;\n+\t\t}\n+\n \t\tauto read_count = lstate.ReadNext(gstate);\n \t\tif (lstate.scan_count > 1) {\n \t\t\tmore_than_one = true;\n@@ -46,11 +52,6 @@ void JSONScan::AutoDetect(ClientContext &context, JSONScanData &bind_data, vecto\n \t\tnode.InitializeCandidateTypes(bind_data.max_depth);\n \t\tnode.RefineCandidateTypes(values, next, string_vector, allocator, bind_data.date_format_map);\n \t\tremaining -= next;\n-\n-\t\tif (gstate.file_index == 10) {\n-\t\t\t// We really shouldn't open more than 10 files when sampling\n-\t\t\tbreak;\n-\t\t}\n \t}\n \tbind_data.type = original_scan_type;\n \n",
  "test_patch": "diff --git a/test/sql/json/read_json_many_files.test_slow b/test/sql/json/read_json_many_files.test_slow\nindex 3aa3b28aaa40..7b0c5ffaab47 100644\n--- a/test/sql/json/read_json_many_files.test_slow\n+++ b/test/sql/json/read_json_many_files.test_slow\n@@ -5,7 +5,7 @@\n require json\n \n statement ok\n-create table input as select 'Laurens' as name;\n+create table input as select range as a from range(1, 4);\n \n loop i 0 2000\n \n@@ -14,10 +14,7 @@ copy input to '__TEST_DIR__/input${i}.json';\n \n endloop\n \n-statement ok\n-pragma threads=1\n-\n query T\n select count(*) from read_json_auto('__TEST_DIR__/input*.json');\n ----\n-2000\n+6000\n",
  "problem_statement": "[re-opening] read_json_auto: duckdb.IOException: IO Error: Cannot open file \"xxx\": Too many open files\n### What happens?\n\nWhen reading about 1000 json files, I recieve the following error:\r\n\r\n`read_json_auto: duckdb.IOException: IO Error: Cannot open file \"xxx\": Too many open files`\r\n\r\nThis issue has already been reported and a potential fix has been implemented ( https://github.com/duckdb/duckdb/issues/6249). Unfortunately, the issue is still available with duckdb 0.7.1 (and also the most recent pre-realese version of 0.7.2).\n\n### To Reproduce\n\n```python\r\nimport shutil\r\nfrom pathlib import Path\r\n\r\nimport duckdb\r\nimport pandas as pd\r\n\r\nif __name__ == \"__main__\":\r\n    mode = \"json\"  # \"json\", \"csv\"\r\n\r\n    data_dir = Path(\"data\")\r\n    if data_dir.exists():\r\n        shutil.rmtree(data_dir)\r\n    data_dir.mkdir(exist_ok=True)\r\n\r\n    for i in range(1000):\r\n        df = pd.DataFrame({\"a\": [1, 2, 3]})\r\n\r\n        if mode == \"json\":\r\n            df.to_json(data_dir / f\"file_{i}.json\", lines=True, orient=\"records\")\r\n        else:\r\n            df.to_csv(data_dir / f\"file_{i}.csv\")\r\n\r\n    data = duckdb.sql(f\"SELECT COUNT(*) FROM read_{mode}_auto('{data_dir}/*.{mode}');\")\r\n\r\n    print(data.fetchone())\r\n```\r\n\r\nWhen setting mode=\"csv\", the code works as expected.\n\n### OS:\n\nmacOS\n\n### DuckDB Version:\n\n0.7.1\n\n### DuckDB Client:\n\nPython\n\n### Full Name:\n\nTimon Schmelzer\n\n### Affiliation:\n\ngrandcentrix\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "Can confirm; seeing the same on my end. Have been loading in batches in the meantime.\nI have tested this in the Python client, the CLI, and our `unittest` executable, and cannot reproduce this even with 2000 files. When adding print statements to debug, I can verify that all 2000 files are closed (MacOS Ventura 13.0, with an M1 chip).\r\n\r\nI will discuss this with the one who implemented our CSV reader and see if he has a hunch about what is happening on your end. I would love to be able to reproduce and fix this.\nWith some help, I was able to reproduce this! I'm currently investigating.",
  "created_at": "2023-03-07T10:51:20Z"
}