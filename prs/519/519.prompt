You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Query using the LN() function does not terminate
Consider the following statements:
```sql
CREATE TABLE t0(c0 INT);
CREATE TABLE t1(c0 INT);
INSERT INTO t0(c0) VALUES (0);
INSERT INTO t1(c0) VALUES (0), (0), (1), (-1);
SELECT * FROM t0, t1 WHERE LN(t1.c0) < t0.c0; -- does not terminate
```
Unexpectedly, the `SELECT` does not terminate.

I found this bug based on the latest master commit (9795d18b6a6e250a4076fa448f248287e76f5693).

</issue>
<code>
[start of README.md]
1: <img align="left" src="logo/duckdb-logo.png" height="120">
2: 
3: # DuckDB, the SQLite for Analytics
4: [![Travis](https://api.travis-ci.org/cwida/duckdb.svg?branch=master)](https://travis-ci.org/cwida/duckdb)
5: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
6: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
7: 
8: <br>
9: 
10: 
11: # Requirements
12: DuckDB requires [CMake](https://cmake.org) to be installed and a `C++11` compliant compiler. GCC 4.9 and newer, Clang 3.9 and newer and VisualStudio 2017 are tested on each revision.
13: 
14: ## Compiling
15: Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You may run `make unit` and `make allunit` to verify that your version works properly after making changes.
16: 
17: # Usage
18: A command line utility based on `sqlite3` can be found in either `build/release/duckdb_cli` (release, the default) or `build/debug/duckdb_cli` (debug).
19: 
20: # Embedding
21: As DuckDB is an embedded database, there is no database server to launch or client to connect to a running server. However, the database server can be embedded directly into an application using the C or C++ bindings. The main build process creates the shared library `build/release/src/libduckdb.[so|dylib|dll]` that can be linked against. A static library is built as well.
22: 
23: For examples on how to embed DuckDB into your application, see the [examples](https://github.com/cwida/duckdb/tree/master/examples) folder.
24: 
25: ## Benchmarks
26: After compiling, benchmarks can be executed from the root directory by executing `./build/release/benchmark/benchmark_runner`.
27: 
28: ## Standing on the Shoulders of Giants
29: DuckDB is implemented in C++ 11, should compile with GCC and clang, uses CMake to build and [Catch2](https://github.com/catchorg/Catch2) for testing. DuckDB uses some components from various Open-Source databases and draws inspiration from scientific publications. Here is an overview:
30: 
31: * Parser: We use the PostgreSQL parser that was [repackaged as a stand-alone library](https://github.com/lfittl/libpg_query). The translation to our own parse tree is inspired by [Peloton](https://pelotondb.io).
32: * Shell: We have adapted the [SQLite shell](https://sqlite.org/cli.html) to work with DuckDB.
33: * Tests: We use the [SQL Logic Tests from SQLite](https://www.sqlite.org/sqllogictest/doc/trunk/about.wiki) to test DuckDB.
34: * Query fuzzing: We use [SQLsmith](https://github.com/anse1/sqlsmith) to generate random queries for additional testing.
35: * Date Math: We use the date math component from [MonetDB](https://www.monetdb.org).
36: * SQL Window Functions: DuckDB's window functions implementation uses Segment Tree Aggregation as described in the paper "Efficient Processing of Window Functions in Analytical SQL Queries" by Viktor Leis, Kan Kundhikanjana, Alfons Kemper and Thomas Neumann.
37: * Execution engine: The vectorized execution engine is inspired by the paper "MonetDB/X100: Hyper-Pipelining Query Execution" by Peter Boncz, Marcin Zukowski and Niels Nes.
38: * Optimizer: DuckDB's optimizer draws inspiration from the papers "Dynamic programming strikes back" by Guido Moerkotte and Thomas Neumman as well as "Unnesting Arbitrary Queries" by Thomas Neumann and Alfons Kemper.
39: * Concurrency control: Our MVCC implementation is inspired by the paper "Fast Serializable Multi-Version Concurrency Control for Main-Memory Database Systems" by Thomas Neumann, Tobias Mühlbauer and Alfons Kemper.
40: * Regular Expression: DuckDB uses Google's [RE2](https://github.com/google/re2) regular expression engine.
41: 
42: ## Other pages
43: * [Continuous Benchmarking (CB™)](https://www.duckdb.org/benchmarks/index.html), runs TPC-H, TPC-DS and some microbenchmarks on every commit
[end of README.md]
[start of src/execution/index/art/art.cpp]
1: #include "duckdb/execution/index/art/art.hpp"
2: #include "duckdb/execution/expression_executor.hpp"
3: #include "duckdb/common/vector_operations/vector_operations.hpp"
4: #include <algorithm>
5: #include <ctgmath>
6: 
7: using namespace duckdb;
8: using namespace std;
9: 
10: ART::ART(DataTable &table, vector<column_t> column_ids, vector<unique_ptr<Expression>> unbound_expressions,
11:          bool is_unique)
12:     : Index(IndexType::ART, table, column_ids, move(unbound_expressions)), is_unique(is_unique) {
13: 	tree = nullptr;
14: 	expression_result.Initialize(types);
15: 	int n = 1;
16: 	//! little endian if true
17: 	if (*(char *)&n == 1) {
18: 		is_little_endian = true;
19: 	} else {
20: 		is_little_endian = false;
21: 	}
22: 	switch (types[0]) {
23: 	case TypeId::BOOL:
24: 	case TypeId::INT8:
25: 	case TypeId::INT16:
26: 	case TypeId::INT32:
27: 	case TypeId::INT64:
28: 	case TypeId::FLOAT:
29: 	case TypeId::DOUBLE:
30: 	case TypeId::VARCHAR:
31: 		break;
32: 	default:
33: 		throw InvalidTypeException(types[0], "Invalid type for index");
34: 	}
35: }
36: 
37: ART::~ART() {
38: }
39: 
40: bool ART::LeafMatches(Node *node, Key &key, unsigned depth) {
41: 	auto leaf = static_cast<Leaf *>(node);
42: 	Key &leaf_key = *leaf->value;
43: 	for (idx_t i = depth; i < leaf_key.len; i++) {
44: 		if (leaf_key[i] != key[i]) {
45: 			return false;
46: 		}
47: 	}
48: 
49: 	return true;
50: }
51: 
52: unique_ptr<IndexScanState> ART::InitializeScanSinglePredicate(Transaction &transaction, vector<column_t> column_ids,
53:                                                               Value value, ExpressionType expression_type) {
54: 	auto result = make_unique<ARTIndexScanState>(column_ids);
55: 	result->values[0] = value;
56: 	result->expressions[0] = expression_type;
57: 	return move(result);
58: }
59: 
60: unique_ptr<IndexScanState> ART::InitializeScanTwoPredicates(Transaction &transaction, vector<column_t> column_ids,
61:                                                             Value low_value, ExpressionType low_expression_type,
62:                                                             Value high_value, ExpressionType high_expression_type) {
63: 	auto result = make_unique<ARTIndexScanState>(column_ids);
64: 	result->values[0] = low_value;
65: 	result->expressions[0] = low_expression_type;
66: 	result->values[1] = high_value;
67: 	result->expressions[1] = high_expression_type;
68: 	return move(result);
69: }
70: 
71: //===--------------------------------------------------------------------===//
72: // Insert
73: //===--------------------------------------------------------------------===//
74: template <class T>
75: static void generate_keys(Vector &input, idx_t count, vector<unique_ptr<Key>> &keys, bool is_little_endian) {
76: 	VectorData idata;
77: 	input.Orrify(count, idata);
78: 
79: 	auto input_data = (T *)idata.data;
80: 	for (idx_t i = 0; i < count; i++) {
81: 		auto idx = idata.sel->get_index(i);
82: 		if ((*idata.nullmask)[idx]) {
83: 			keys.push_back(nullptr);
84: 		} else {
85: 			keys.push_back(Key::CreateKey<T>(input_data[idx], is_little_endian));
86: 		}
87: 	}
88: }
89: 
90: template <class T>
91: static void concatenate_keys(Vector &input, idx_t count, vector<unique_ptr<Key>> &keys, bool is_little_endian) {
92: 	VectorData idata;
93: 	input.Orrify(count, idata);
94: 
95: 	auto input_data = (T *)idata.data;
96: 	for (idx_t i = 0; i < count; i++) {
97: 		auto idx = idata.sel->get_index(i);
98: 		if ((*idata.nullmask)[idx] || !keys[i]) {
99: 			// either this column is NULL, or the previous column is NULL!
100: 			keys[i] = nullptr;
101: 		} else {
102: 			// concatenate the keys
103: 			auto old_key = move(keys[i]);
104: 			auto new_key = Key::CreateKey<T>(input_data[idx], is_little_endian);
105: 			auto keyLen = old_key->len + new_key->len;
106: 			auto compound_data = unique_ptr<data_t[]>(new data_t[keyLen]);
107: 			memcpy(compound_data.get(), old_key->data.get(), old_key->len);
108: 			memcpy(compound_data.get() + old_key->len, new_key->data.get(), new_key->len);
109: 			keys[i] = make_unique<Key>(move(compound_data), keyLen);
110: 		}
111: 	}
112: }
113: 
114: void ART::GenerateKeys(DataChunk &input, vector<unique_ptr<Key>> &keys) {
115: 	keys.reserve(STANDARD_VECTOR_SIZE);
116: 	// generate keys for the first input column
117: 	switch (input.data[0].type) {
118: 	case TypeId::BOOL:
119: 		generate_keys<bool>(input.data[0], input.size(), keys, is_little_endian);
120: 		break;
121: 	case TypeId::INT8:
122: 		generate_keys<int8_t>(input.data[0], input.size(), keys, is_little_endian);
123: 		break;
124: 	case TypeId::INT16:
125: 		generate_keys<int16_t>(input.data[0], input.size(), keys, is_little_endian);
126: 		break;
127: 	case TypeId::INT32:
128: 		generate_keys<int32_t>(input.data[0], input.size(), keys, is_little_endian);
129: 		break;
130: 	case TypeId::INT64:
131: 		generate_keys<int64_t>(input.data[0], input.size(), keys, is_little_endian);
132: 		break;
133: 	case TypeId::FLOAT:
134: 		generate_keys<float>(input.data[0], input.size(), keys, is_little_endian);
135: 		break;
136: 	case TypeId::DOUBLE:
137: 		generate_keys<double>(input.data[0], input.size(), keys, is_little_endian);
138: 		break;
139: 	case TypeId::VARCHAR:
140: 		generate_keys<string_t>(input.data[0], input.size(), keys, is_little_endian);
141: 		break;
142: 	default:
143: 		throw InvalidTypeException(input.data[0].type, "Invalid type for index");
144: 	}
145: 	for (idx_t i = 1; i < input.column_count(); i++) {
146: 		// for each of the remaining columns, concatenate
147: 		switch (input.data[i].type) {
148: 		case TypeId::BOOL:
149: 			concatenate_keys<bool>(input.data[i], input.size(), keys, is_little_endian);
150: 			break;
151: 		case TypeId::INT8:
152: 			concatenate_keys<int8_t>(input.data[i], input.size(), keys, is_little_endian);
153: 			break;
154: 		case TypeId::INT16:
155: 			concatenate_keys<int16_t>(input.data[i], input.size(), keys, is_little_endian);
156: 			break;
157: 		case TypeId::INT32:
158: 			concatenate_keys<int32_t>(input.data[i], input.size(), keys, is_little_endian);
159: 			break;
160: 		case TypeId::INT64:
161: 			concatenate_keys<int64_t>(input.data[i], input.size(), keys, is_little_endian);
162: 			break;
163: 		case TypeId::FLOAT:
164: 			concatenate_keys<float>(input.data[i], input.size(), keys, is_little_endian);
165: 			break;
166: 		case TypeId::DOUBLE:
167: 			concatenate_keys<double>(input.data[i], input.size(), keys, is_little_endian);
168: 			break;
169: 		case TypeId::VARCHAR:
170: 			concatenate_keys<string_t>(input.data[i], input.size(), keys, is_little_endian);
171: 			break;
172: 		default:
173: 			throw InvalidTypeException(input.data[0].type, "Invalid type for index");
174: 		}
175: 	}
176: }
177: 
178: bool ART::Insert(IndexLock &lock, DataChunk &input, Vector &row_ids) {
179: 	assert(row_ids.type == ROW_TYPE);
180: 	assert(types[0] == input.data[0].type);
181: 
182: 	// generate the keys for the given input
183: 	vector<unique_ptr<Key>> keys;
184: 	GenerateKeys(input, keys);
185: 
186: 	// now insert the elements into the index
187: 	row_ids.Normalify(input.size());
188: 	auto row_identifiers = FlatVector::GetData<row_t>(row_ids);
189: 	idx_t failed_index = INVALID_INDEX;
190: 	for (idx_t i = 0; i < input.size(); i++) {
191: 		if (!keys[i]) {
192: 			continue;
193: 		}
194: 
195: 		row_t row_id = row_identifiers[i];
196: 		if (!Insert(tree, move(keys[i]), 0, row_id)) {
197: 			// failed to insert because of constraint violation
198: 			failed_index = i;
199: 			break;
200: 		}
201: 	}
202: 	if (failed_index != INVALID_INDEX) {
203: 		// failed to insert because of constraint violation: remove previously inserted entries
204: 		// generate keys again
205: 		keys.clear();
206: 		GenerateKeys(input, keys);
207: 		unique_ptr<Key> key;
208: 
209: 		// now erase the entries
210: 		for (idx_t i = 0; i < failed_index; i++) {
211: 			if (!keys[i]) {
212: 				continue;
213: 			}
214: 			row_t row_id = row_identifiers[i];
215: 			Erase(tree, *keys[i], 0, row_id);
216: 		}
217: 		return false;
218: 	}
219: 	return true;
220: }
221: 
222: bool ART::Append(IndexLock &lock, DataChunk &appended_data, Vector &row_identifiers) {
223: 	// first resolve the expressions for the index
224: 	ExecuteExpressions(appended_data, expression_result);
225: 
226: 	// now insert into the index
227: 	return Insert(lock, expression_result, row_identifiers);
228: }
229: 
230: void ART::VerifyAppend(DataChunk &chunk) {
231: 	if (!is_unique) {
232: 		return;
233: 	}
234: 	// unique index, check
235: 	lock_guard<mutex> l(lock);
236: 	// first resolve the expressions for the index
237: 	ExecuteExpressions(chunk, expression_result);
238: 
239: 	// generate the keys for the given input
240: 	vector<unique_ptr<Key>> keys;
241: 	GenerateKeys(expression_result, keys);
242: 
243: 	for (idx_t i = 0; i < chunk.size(); i++) {
244: 		if (!keys[i]) {
245: 			continue;
246: 		}
247: 		if (Lookup(tree, *keys[i], 0) != nullptr) {
248: 			// node already exists in tree
249: 			throw ConstraintException("duplicate key value violates primary key or unique constraint");
250: 		}
251: 	}
252: }
253: 
254: bool ART::InsertToLeaf(Leaf &leaf, row_t row_id) {
255: 	if (is_unique && leaf.num_elements != 0) {
256: 		return false;
257: 	}
258: 	leaf.Insert(row_id);
259: 	return true;
260: }
261: 
262: bool ART::Insert(unique_ptr<Node> &node, unique_ptr<Key> value, unsigned depth, row_t row_id) {
263: 	Key &key = *value;
264: 	if (!node) {
265: 		// node is currently empty, create a leaf here with the key
266: 		node = make_unique<Leaf>(*this, move(value), row_id);
267: 		return true;
268: 	}
269: 
270: 	if (node->type == NodeType::NLeaf) {
271: 		// Replace leaf with Node4 and store both leaves in it
272: 		auto leaf = static_cast<Leaf *>(node.get());
273: 
274: 		Key &existingKey = *leaf->value;
275: 		uint32_t newPrefixLength = 0;
276: 		// Leaf node is already there, update row_id vector
277: 		if (depth + newPrefixLength == existingKey.len && existingKey.len == key.len) {
278: 			return InsertToLeaf(*leaf, row_id);
279: 		}
280: 		while (existingKey[depth + newPrefixLength] == key[depth + newPrefixLength]) {
281: 			newPrefixLength++;
282: 			// Leaf node is already there, update row_id vector
283: 			if (depth + newPrefixLength == existingKey.len && existingKey.len == key.len) {
284: 				return InsertToLeaf(*leaf, row_id);
285: 			}
286: 		}
287: 
288: 		unique_ptr<Node> newNode = make_unique<Node4>(*this, newPrefixLength);
289: 		newNode->prefix_length = newPrefixLength;
290: 		memcpy(newNode->prefix.get(), &key[depth], newPrefixLength);
291: 		Node4::insert(*this, newNode, existingKey[depth + newPrefixLength], node);
292: 		unique_ptr<Node> leaf_node = make_unique<Leaf>(*this, move(value), row_id);
293: 		Node4::insert(*this, newNode, key[depth + newPrefixLength], leaf_node);
294: 		node = move(newNode);
295: 		return true;
296: 	}
297: 
298: 	// Handle prefix of inner node
299: 	if (node->prefix_length) {
300: 		uint32_t mismatchPos = Node::PrefixMismatch(*this, node.get(), key, depth);
301: 		if (mismatchPos != node->prefix_length) {
302: 			// Prefix differs, create new node
303: 			unique_ptr<Node> newNode = make_unique<Node4>(*this, mismatchPos);
304: 			newNode->prefix_length = mismatchPos;
305: 			memcpy(newNode->prefix.get(), node->prefix.get(), mismatchPos);
306: 			// Break up prefix
307: 			auto node_ptr = node.get();
308: 			Node4::insert(*this, newNode, node->prefix[mismatchPos], node);
309: 			node_ptr->prefix_length -= (mismatchPos + 1);
310: 			memmove(node_ptr->prefix.get(), node_ptr->prefix.get() + mismatchPos + 1, node_ptr->prefix_length);
311: 			unique_ptr<Node> leaf_node = make_unique<Leaf>(*this, move(value), row_id);
312: 			Node4::insert(*this, newNode, key[depth + mismatchPos], leaf_node);
313: 			node = move(newNode);
314: 			return true;
315: 		}
316: 		depth += node->prefix_length;
317: 	}
318: 
319: 	// Recurse
320: 	idx_t pos = node->GetChildPos(key[depth]);
321: 	if (pos != INVALID_INDEX) {
322: 		auto child = node->GetChild(pos);
323: 		return Insert(*child, move(value), depth + 1, row_id);
324: 	}
325: 	unique_ptr<Node> newNode = make_unique<Leaf>(*this, move(value), row_id);
326: 	Node::InsertLeaf(*this, node, key[depth], newNode);
327: 	return true;
328: }
329: 
330: //===--------------------------------------------------------------------===//
331: // Delete
332: //===--------------------------------------------------------------------===//
333: void ART::Delete(IndexLock &state, DataChunk &input, Vector &row_ids) {
334: 	// first resolve the expressions
335: 	ExecuteExpressions(input, expression_result);
336: 
337: 	// then generate the keys for the given input
338: 	vector<unique_ptr<Key>> keys;
339: 	GenerateKeys(expression_result, keys);
340: 
341: 	// now erase the elements from the database
342: 	row_ids.Normalify(input.size());
343: 	auto row_identifiers = FlatVector::GetData<row_t>(row_ids);
344: 
345: 	for (idx_t i = 0; i < input.size(); i++) {
346: 		if (!keys[i]) {
347: 			continue;
348: 		}
349: 		Erase(tree, *keys[i], 0, row_identifiers[i]);
350: 		// assert that the entry was erased properly
351: 		assert(!is_unique || Lookup(tree, *keys[i], 0) == nullptr);
352: 	}
353: }
354: 
355: void ART::Erase(unique_ptr<Node> &node, Key &key, unsigned depth, row_t row_id) {
356: 	if (!node) {
357: 		return;
358: 	}
359: 	// Delete a leaf from a tree
360: 	if (node->type == NodeType::NLeaf) {
361: 		// Make sure we have the right leaf
362: 		if (ART::LeafMatches(node.get(), key, depth)) {
363: 			node.reset();
364: 		}
365: 		return;
366: 	}
367: 
368: 	// Handle prefix
369: 	if (node->prefix_length) {
370: 		if (Node::PrefixMismatch(*this, node.get(), key, depth) != node->prefix_length) {
371: 			return;
372: 		}
373: 		depth += node->prefix_length;
374: 	}
375: 	idx_t pos = node->GetChildPos(key[depth]);
376: 	if (pos != INVALID_INDEX) {
377: 		auto child = node->GetChild(pos);
378: 		assert(child);
379: 
380: 		unique_ptr<Node> &child_ref = *child;
381: 		if (child_ref->type == NodeType::NLeaf && LeafMatches(child_ref.get(), key, depth)) {
382: 			// Leaf found, remove entry
383: 			auto leaf = static_cast<Leaf *>(child_ref.get());
384: 			if (leaf->num_elements > 1) {
385: 				// leaf has multiple rows: remove the row from the leaf
386: 				leaf->Remove(row_id);
387: 			} else {
388: 				// Leaf only has one element, delete leaf, decrement node counter and maybe shrink node
389: 				Node::Erase(*this, node, pos);
390: 			}
391: 		} else {
392: 			// Recurse
393: 			Erase(*child, key, depth + 1, row_id);
394: 		}
395: 	}
396: }
397: 
398: //===--------------------------------------------------------------------===//
399: // Point Query
400: //===--------------------------------------------------------------------===//
401: static unique_ptr<Key> CreateKey(ART &art, TypeId type, Value &value) {
402: 	assert(type == value.type);
403: 	switch (type) {
404: 	case TypeId::INT8:
405: 		return Key::CreateKey<int8_t>(value.value_.tinyint, art.is_little_endian);
406: 	case TypeId::INT16:
407: 		return Key::CreateKey<int16_t>(value.value_.smallint, art.is_little_endian);
408: 	case TypeId::INT32:
409: 		return Key::CreateKey<int32_t>(value.value_.integer, art.is_little_endian);
410: 	case TypeId::INT64:
411: 		return Key::CreateKey<int64_t>(value.value_.bigint, art.is_little_endian);
412: 	case TypeId::FLOAT:
413: 		return Key::CreateKey<float>(value.value_.float_, art.is_little_endian);
414: 	case TypeId::DOUBLE:
415: 		return Key::CreateKey<double>(value.value_.double_, art.is_little_endian);
416: 	case TypeId::VARCHAR:
417: 		return Key::CreateKey<string_t>(string_t(value.str_value.c_str(), value.str_value.size()),
418: 		                                art.is_little_endian);
419: 	default:
420: 		throw InvalidTypeException(type, "Invalid type for index");
421: 	}
422: }
423: 
424: void ART::SearchEqual(vector<row_t> &result_ids, ARTIndexScanState *state) {
425: 	unique_ptr<Key> key = CreateKey(*this, types[0], state->values[0]);
426: 	auto leaf = static_cast<Leaf *>(Lookup(tree, *key, 0));
427: 	if (!leaf) {
428: 		return;
429: 	}
430: 	for (idx_t i = 0; i < leaf->num_elements; i++) {
431: 		row_t row_id = leaf->GetRowId(i);
432: 		result_ids.push_back(row_id);
433: 	}
434: }
435: 
436: Node *ART::Lookup(unique_ptr<Node> &node, Key &key, unsigned depth) {
437: 	auto node_val = node.get();
438: 
439: 	while (node_val) {
440: 		if (node_val->type == NodeType::NLeaf) {
441: 			auto leaf = static_cast<Leaf *>(node_val);
442: 			Key &leafKey = *leaf->value;
443: 			//! Check leaf
444: 			for (idx_t i = depth; i < leafKey.len; i++) {
445: 				if (leafKey[i] != key[i]) {
446: 					return nullptr;
447: 				}
448: 			}
449: 			return node_val;
450: 		}
451: 		if (node_val->prefix_length) {
452: 			for (idx_t pos = 0; pos < node_val->prefix_length; pos++) {
453: 				if (key[depth + pos] != node_val->prefix[pos]) {
454: 					return nullptr;
455: 				}
456: 			}
457: 			depth += node_val->prefix_length;
458: 		}
459: 		idx_t pos = node_val->GetChildPos(key[depth]);
460: 		if (pos == INVALID_INDEX) {
461: 			return nullptr;
462: 		}
463: 		node_val = node_val->GetChild(pos)->get();
464: 		assert(node_val);
465: 
466: 		depth++;
467: 	}
468: 
469: 	return nullptr;
470: }
471: 
472: //===--------------------------------------------------------------------===//
473: // Iterator scans
474: //===--------------------------------------------------------------------===//
475: template <bool HAS_BOUND, bool INCLUSIVE>
476: void ART::IteratorScan(ARTIndexScanState *state, Iterator *it, vector<row_t> &result_ids, Key *bound) {
477: 	bool has_next;
478: 	do {
479: 		if (HAS_BOUND) {
480: 			assert(bound);
481: 			if (INCLUSIVE) {
482: 				if (*it->node->value > *bound) {
483: 					break;
484: 				}
485: 			} else {
486: 				if (*it->node->value >= *bound) {
487: 					break;
488: 				}
489: 			}
490: 		}
491: 		for (idx_t i = 0; i < it->node->num_elements; i++) {
492: 			row_t row_id = it->node->GetRowId(i);
493: 			result_ids.push_back(row_id);
494: 		}
495: 		has_next = ART::IteratorNext(*it);
496: 	} while (has_next);
497: }
498: 
499: bool ART::IteratorNext(Iterator &it) {
500: 	// Skip leaf
501: 	if ((it.depth) && ((it.stack[it.depth - 1].node)->type == NodeType::NLeaf)) {
502: 		it.depth--;
503: 	}
504: 
505: 	// Look for the next leaf
506: 	while (it.depth > 0) {
507: 		auto &top = it.stack[it.depth - 1];
508: 		Node *node = top.node;
509: 
510: 		if (node->type == NodeType::NLeaf) {
511: 			// found a leaf: move to next node
512: 			it.node = (Leaf *)node;
513: 			return true;
514: 		}
515: 
516: 		// Find next node
517: 		top.pos = node->GetNextPos(top.pos);
518: 		if (top.pos != INVALID_INDEX) {
519: 			// next node found: go there
520: 			it.stack[it.depth].node = node->GetChild(top.pos)->get();
521: 			it.stack[it.depth].pos = INVALID_INDEX;
522: 			it.depth++;
523: 		} else {
524: 			// no node found: move up the tree
525: 			it.depth--;
526: 		}
527: 	}
528: 	return false;
529: }
530: 
531: //===--------------------------------------------------------------------===//
532: // Greater Than
533: // Returns: True (If found leaf >= key)
534: //          False (Otherwise)
535: //===--------------------------------------------------------------------===//
536: bool ART::Bound(unique_ptr<Node> &n, Key &key, Iterator &it, bool inclusive) {
537: 	it.depth = 0;
538: 	if (!n) {
539: 		return false;
540: 	}
541: 	Node *node = n.get();
542: 
543: 	idx_t depth = 0;
544: 	while (true) {
545: 		auto &top = it.stack[it.depth];
546: 		top.node = node;
547: 		it.depth++;
548: 
549: 		if (node->type == NodeType::NLeaf) {
550: 			// found a leaf node: check if it is bigger or equal than the current key
551: 			auto leaf = static_cast<Leaf *>(node);
552: 			it.node = leaf;
553: 			// if the search is not inclusive the leaf node could still be equal to the current value
554: 			// check if leaf is equal to the current key
555: 			if (*leaf->value == key) {
556: 				// if its not inclusive check if there is a next leaf
557: 				if (!inclusive && !IteratorNext(it)) {
558: 					return false;
559: 				} else {
560: 					return true;
561: 				}
562: 			}
563: 
564: 			if (*leaf->value > key) {
565: 				return true;
566: 			}
567: 			// Leaf is lower than key
568: 			// Check if next leaf is still lower than key
569: 			while (IteratorNext(it)) {
570: 				if (*it.node->value == key) {
571: 					// if its not inclusive check if there is a next leaf
572: 					if (!inclusive && !IteratorNext(it)) {
573: 						return false;
574: 					} else {
575: 						return true;
576: 					}
577: 				} else if (*it.node->value > key) {
578: 					// if its not inclusive check if there is a next leaf
579: 					return true;
580: 				}
581: 			}
582: 			return false;
583: 		}
584: 		uint32_t mismatchPos = Node::PrefixMismatch(*this, node, key, depth);
585: 		if (mismatchPos != node->prefix_length) {
586: 			if (node->prefix[mismatchPos] < key[depth + mismatchPos]) {
587: 				// Less
588: 				it.depth--;
589: 				return IteratorNext(it);
590: 			} else {
591: 				// Greater
592: 				top.pos = INVALID_INDEX;
593: 				return IteratorNext(it);
594: 			}
595: 		}
596: 		// prefix matches, search inside the child for the key
597: 		depth += node->prefix_length;
598: 
599: 		top.pos = node->GetChildGreaterEqual(key[depth]);
600: 
601: 		if (top.pos == INVALID_INDEX) {
602: 			// Find min leaf
603: 			top.pos = node->GetMin();
604: 		}
605: 		node = node->GetChild(top.pos)->get();
606: 		depth++;
607: 	}
608: }
609: 
610: void ART::SearchGreater(vector<row_t> &result_ids, ARTIndexScanState *state, bool inclusive) {
611: 	Iterator *it = &state->iterator;
612: 	auto key = CreateKey(*this, types[0], state->values[0]);
613: 
614: 	// greater than scan: first set the iterator to the node at which we will start our scan by finding the lowest node
615: 	// that satisfies our requirement
616: 	if (!it->start) {
617: 		bool found = ART::Bound(tree, *key, *it, inclusive);
618: 		if (!found) {
619: 			return;
620: 		}
621: 		it->start = true;
622: 	}
623: 	// after that we continue the scan; we don't need to check the bounds as any value following this value is
624: 	// automatically bigger and hence satisfies our predicate
625: 	IteratorScan<false, false>(state, it, result_ids, nullptr);
626: }
627: 
628: //===--------------------------------------------------------------------===//
629: // Less Than
630: //===--------------------------------------------------------------------===//
631: static Leaf &FindMinimum(Iterator &it, Node &node) {
632: 	Node *next = nullptr;
633: 	idx_t pos = 0;
634: 	switch (node.type) {
635: 	case NodeType::NLeaf:
636: 		it.node = (Leaf *)&node;
637: 		return (Leaf &)node;
638: 	case NodeType::N4:
639: 		next = ((Node4 &)node).child[0].get();
640: 		break;
641: 	case NodeType::N16:
642: 		next = ((Node16 &)node).child[0].get();
643: 		break;
644: 	case NodeType::N48: {
645: 		auto &n48 = (Node48 &)node;
646: 		while (n48.childIndex[pos] == Node::EMPTY_MARKER) {
647: 			pos++;
648: 		}
649: 		next = n48.child[n48.childIndex[pos]].get();
650: 		break;
651: 	}
652: 	case NodeType::N256: {
653: 		auto &n256 = (Node256 &)node;
654: 		while (!n256.child[pos]) {
655: 			pos++;
656: 		}
657: 		next = n256.child[pos].get();
658: 		break;
659: 	}
660: 	}
661: 	it.stack[it.depth].node = &node;
662: 	it.stack[it.depth].pos = pos;
663: 	it.depth++;
664: 	return FindMinimum(it, *next);
665: }
666: 
667: void ART::SearchLess(vector<row_t> &result_ids, ARTIndexScanState *state, bool inclusive) {
668: 	if (!tree) {
669: 		return;
670: 	}
671: 
672: 	Iterator *it = &state->iterator;
673: 	auto upper_bound = CreateKey(*this, types[0], state->values[0]);
674: 
675: 	if (!it->start) {
676: 		// first find the minimum value in the ART: we start scanning from this value
677: 		auto &minimum = FindMinimum(state->iterator, *tree);
678: 		// early out min value higher than upper bound query
679: 		if (*minimum.value > *upper_bound) {
680: 			return;
681: 		}
682: 		it->start = true;
683: 	}
684: 	// now continue the scan until we reach the upper bound
685: 	if (inclusive) {
686: 		IteratorScan<true, true>(state, it, result_ids, upper_bound.get());
687: 	} else {
688: 		IteratorScan<true, false>(state, it, result_ids, upper_bound.get());
689: 	}
690: }
691: 
692: //===--------------------------------------------------------------------===//
693: // Closed Range Query
694: //===--------------------------------------------------------------------===//
695: void ART::SearchCloseRange(vector<row_t> &result_ids, ARTIndexScanState *state, bool left_inclusive,
696:                            bool right_inclusive) {
697: 	auto lower_bound = CreateKey(*this, types[0], state->values[0]);
698: 	auto upper_bound = CreateKey(*this, types[0], state->values[1]);
699: 	Iterator *it = &state->iterator;
700: 	// first find the first node that satisfies the left predicate
701: 	if (!it->start) {
702: 		bool found = ART::Bound(tree, *lower_bound, *it, left_inclusive);
703: 		if (!found) {
704: 			return;
705: 		}
706: 		it->start = true;
707: 	}
708: 	// now continue the scan until we reach the upper bound
709: 	if (right_inclusive) {
710: 		IteratorScan<true, true>(state, it, result_ids, upper_bound.get());
711: 	} else {
712: 		IteratorScan<true, false>(state, it, result_ids, upper_bound.get());
713: 	}
714: }
715: 
716: void ART::Scan(Transaction &transaction, TableIndexScanState &table_state, DataChunk &result) {
717: 	auto state = (ARTIndexScanState *)table_state.index_state.get();
718: 
719: 	// scan the index
720: 	if (!state->checked) {
721: 		vector<row_t> result_ids;
722: 		assert(state->values[0].type == types[0]);
723: 
724: 		if (state->values[1].is_null) {
725: 			lock_guard<mutex> l(lock);
726: 			// single predicate
727: 			switch (state->expressions[0]) {
728: 			case ExpressionType::COMPARE_EQUAL:
729: 				SearchEqual(result_ids, state);
730: 				break;
731: 			case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
732: 				SearchGreater(result_ids, state, true);
733: 				break;
734: 			case ExpressionType::COMPARE_GREATERTHAN:
735: 				SearchGreater(result_ids, state, false);
736: 				break;
737: 			case ExpressionType::COMPARE_LESSTHANOREQUALTO:
738: 				SearchLess(result_ids, state, true);
739: 				break;
740: 			case ExpressionType::COMPARE_LESSTHAN:
741: 				SearchLess(result_ids, state, false);
742: 				break;
743: 			default:
744: 				throw NotImplementedException("Operation not implemented");
745: 			}
746: 		} else {
747: 			lock_guard<mutex> l(lock);
748: 			// two predicates
749: 			assert(state->values[1].type == types[0]);
750: 			bool left_inclusive = state->expressions[0] == ExpressionType ::COMPARE_GREATERTHANOREQUALTO;
751: 			bool right_inclusive = state->expressions[1] == ExpressionType ::COMPARE_LESSTHANOREQUALTO;
752: 			SearchCloseRange(result_ids, state, left_inclusive, right_inclusive);
753: 		}
754: 		state->checked = true;
755: 
756: 		if (result_ids.size() == 0) {
757: 			return;
758: 		}
759: 
760: 		// sort the row ids
761: 		sort(result_ids.begin(), result_ids.end());
762: 		// duplicate eliminate the row ids and append them to the row ids of the state
763: 		state->result_ids.reserve(result_ids.size());
764: 
765: 		state->result_ids.push_back(result_ids[0]);
766: 		for (idx_t i = 1; i < result_ids.size(); i++) {
767: 			if (result_ids[i] != result_ids[i - 1]) {
768: 				state->result_ids.push_back(result_ids[i]);
769: 			}
770: 		}
771: 	}
772: 
773: 	if (state->result_index >= state->result_ids.size()) {
774: 		// exhausted all row ids
775: 		return;
776: 	}
777: 
778: 	// create a vector pointing to the current set of row ids
779: 	Vector row_identifiers(ROW_TYPE, (data_ptr_t)&state->result_ids[state->result_index]);
780: 	idx_t scan_count = std::min((idx_t)STANDARD_VECTOR_SIZE, (idx_t)state->result_ids.size() - state->result_index);
781: 
782: 	// fetch the actual values from the base table
783: 	table.Fetch(transaction, result, state->column_ids, row_identifiers, scan_count, table_state);
784: 
785: 	// move to the next set of row ids
786: 	state->result_index += scan_count;
787: }
[end of src/execution/index/art/art.cpp]
[start of src/execution/index/art/leaf.cpp]
1: #include "duckdb/execution/index/art/node.hpp"
2: #include "duckdb/execution/index/art/leaf.hpp"
3: 
4: #include <cstring>
5: 
6: using namespace duckdb;
7: using namespace std;
8: 
9: Leaf::Leaf(ART &art, unique_ptr<Key> value, row_t row_id) : Node(art, NodeType::NLeaf, 0) {
10: 	this->value = move(value);
11: 	this->capacity = 1;
12: 	this->row_ids = unique_ptr<row_t[]>(new row_t[this->capacity]);
13: 	this->row_ids[0] = row_id;
14: 	this->num_elements = 1;
15: }
16: 
17: void Leaf::Insert(row_t row_id) {
18: 	// Grow array
19: 	if (num_elements == capacity) {
20: 		auto new_row_id = unique_ptr<row_t[]>(new row_t[capacity * 2]);
21: 		memcpy(new_row_id.get(), row_ids.get(), capacity * sizeof(row_t));
22: 		capacity *= 2;
23: 		row_ids = move(new_row_id);
24: 	}
25: 	row_ids[num_elements++] = row_id;
26: }
27: 
28: //! TODO: Maybe shrink array dynamically?
29: void Leaf::Remove(row_t row_id) {
30: 	idx_t entry_offset = -1;
31: 	for (idx_t i = 0; i < num_elements; i++) {
32: 		if (row_ids[i] == row_id) {
33: 			entry_offset = i;
34: 			break;
35: 		}
36: 	}
37: 	num_elements--;
38: 	for (idx_t j = entry_offset; j < num_elements; j++) {
39: 		row_ids[j] = row_ids[j + 1];
40: 	}
41: }
[end of src/execution/index/art/leaf.cpp]
[start of src/execution/operator/join/physical_nested_loop_join.cpp]
1: #include "duckdb/execution/operator/join/physical_nested_loop_join.hpp"
2: 
3: #include "duckdb/common/operator/comparison_operators.hpp"
4: #include "duckdb/common/vector_operations/vector_operations.hpp"
5: #include "duckdb/execution/expression_executor.hpp"
6: #include "duckdb/execution/nested_loop_join.hpp"
7: 
8: using namespace std;
9: 
10: namespace duckdb {
11: 
12: class PhysicalNestedLoopJoinState : public PhysicalComparisonJoinState {
13: public:
14: 	PhysicalNestedLoopJoinState(PhysicalOperator *left, PhysicalOperator *right, vector<JoinCondition> &conditions)
15: 	    : PhysicalComparisonJoinState(left, right, conditions), right_chunk(0), has_null(false), left_tuple(0),
16: 	      right_tuple(0) {
17: 	}
18: 
19: 	idx_t right_chunk;
20: 	DataChunk left_join_condition;
21: 	ChunkCollection right_data;
22: 	ChunkCollection right_chunks;
23: 	//! Whether or not the RHS of the nested loop join has NULL values
24: 	bool has_null;
25: 
26: 	idx_t left_tuple;
27: 	idx_t right_tuple;
28: };
29: 
30: PhysicalNestedLoopJoin::PhysicalNestedLoopJoin(LogicalOperator &op, unique_ptr<PhysicalOperator> left,
31:                                                unique_ptr<PhysicalOperator> right, vector<JoinCondition> cond,
32:                                                JoinType join_type)
33:     : PhysicalComparisonJoin(op, PhysicalOperatorType::NESTED_LOOP_JOIN, move(cond), join_type) {
34: 	children.push_back(move(left));
35: 	children.push_back(move(right));
36: }
37: 
38: static bool HasNullValues(DataChunk &chunk) {
39: 	for (idx_t col_idx = 0; col_idx < chunk.column_count(); col_idx++) {
40: 		VectorData vdata;
41: 		chunk.data[col_idx].Orrify(chunk.size(), vdata);
42: 
43: 		if (vdata.nullmask->none()) {
44: 			continue;
45: 		}
46: 		for (idx_t i = 0; i < chunk.size(); i++) {
47: 			auto idx = vdata.sel->get_index(i);
48: 			if ((*vdata.nullmask)[idx]) {
49: 				return true;
50: 			}
51: 		}
52: 	}
53: 	return false;
54: }
55: 
56: template <bool MATCH>
57: void PhysicalJoin::ConstructSemiOrAntiJoinResult(DataChunk &left, DataChunk &result, bool found_match[]) {
58: 	assert(left.column_count() == result.column_count());
59: 	// create the selection vector from the matches that were found
60: 	idx_t result_count = 0;
61: 	SelectionVector sel(STANDARD_VECTOR_SIZE);
62: 	for (idx_t i = 0; i < left.size(); i++) {
63: 		if (found_match[i] == MATCH) {
64: 			sel.set_index(result_count++, i);
65: 		}
66: 	}
67: 	// construct the final result
68: 	if (result_count > 0) {
69: 		// we only return the columns on the left side
70: 		// project them using the result selection vector
71: 		// reference the columns of the left side from the result
72: 		result.Slice(left, sel, result_count);
73: 	} else {
74: 		result.SetCardinality(0);
75: 	}
76: }
77: 
78: void PhysicalJoin::ConstructMarkJoinResult(DataChunk &join_keys, DataChunk &left, DataChunk &result, bool found_match[],
79:                                            bool has_null) {
80: 	// for the initial set of columns we just reference the left side
81: 	result.SetCardinality(left);
82: 	for (idx_t i = 0; i < left.column_count(); i++) {
83: 		result.data[i].Reference(left.data[i]);
84: 	}
85: 	auto &mark_vector = result.data.back();
86: 	mark_vector.vector_type = VectorType::FLAT_VECTOR;
87: 	// first we set the NULL values from the join keys
88: 	// if there is any NULL in the keys, the result is NULL
89: 	auto bool_result = FlatVector::GetData<bool>(mark_vector);
90: 	auto &nullmask = FlatVector::Nullmask(mark_vector);
91: 	for (idx_t col_idx = 0; col_idx < join_keys.column_count(); col_idx++) {
92: 		VectorData jdata;
93: 		join_keys.data[col_idx].Orrify(join_keys.size(), jdata);
94: 		if (jdata.nullmask->any()) {
95: 			for (idx_t i = 0; i < join_keys.size(); i++) {
96: 				auto jidx = jdata.sel->get_index(i);
97: 				nullmask[i] = (*jdata.nullmask)[jidx];
98: 			}
99: 		}
100: 	}
101: 	// now set the remaining entries to either true or false based on whether a match was found
102: 	if (found_match) {
103: 		for (idx_t i = 0; i < left.size(); i++) {
104: 			bool_result[i] = found_match[i];
105: 		}
106: 	} else {
107: 		memset(bool_result, 0, sizeof(bool) * left.size());
108: 	}
109: 	// if the right side contains NULL values, the result of any FALSE becomes NULL
110: 	if (has_null) {
111: 		for (idx_t i = 0; i < left.size(); i++) {
112: 			if (!bool_result[i]) {
113: 				nullmask[i] = true;
114: 			}
115: 		}
116: 	}
117: }
118: 
119: void PhysicalNestedLoopJoin::GetChunkInternal(ClientContext &context, DataChunk &chunk, PhysicalOperatorState *state_) {
120: 	auto state = reinterpret_cast<PhysicalNestedLoopJoinState *>(state_);
121: 
122: 	// first we fully materialize the right child, if we haven't done that yet
123: 	if (state->right_chunks.column_count() == 0) {
124: 		vector<TypeId> condition_types;
125: 		for (auto &cond : conditions) {
126: 			assert(cond.left->return_type == cond.right->return_type);
127: 			condition_types.push_back(cond.left->return_type);
128: 		}
129: 
130: 		auto right_state = children[1]->GetOperatorState();
131: 		auto types = children[1]->GetTypes();
132: 
133: 		DataChunk new_chunk, right_condition;
134: 		new_chunk.Initialize(types);
135: 		right_condition.Initialize(condition_types);
136: 		do {
137: 			children[1]->GetChunk(context, new_chunk, right_state.get());
138: 			if (new_chunk.size() == 0) {
139: 				break;
140: 			}
141: 			// resolve the join expression of the right side
142: 			state->rhs_executor.Execute(new_chunk, right_condition);
143: 
144: 			state->right_data.Append(new_chunk);
145: 			state->right_chunks.Append(right_condition);
146: 		} while (new_chunk.size() > 0);
147: 
148: 		if (state->right_chunks.count == 0) {
149: 			if ((type == JoinType::INNER || type == JoinType::SEMI)) {
150: 				// empty RHS with INNER or SEMI join means empty result set
151: 				return;
152: 			}
153: 		} else {
154: 			// for the MARK join, we check if there are null values in any of the right chunks
155: 			if (type == JoinType::MARK) {
156: 				for (idx_t i = 0; i < state->right_chunks.chunks.size(); i++) {
157: 					if (HasNullValues(*state->right_chunks.chunks[i])) {
158: 						state->has_null = true;
159: 					}
160: 				}
161: 			}
162: 			// initialize the chunks for the join conditions
163: 			state->left_join_condition.Initialize(condition_types);
164: 			state->right_chunk = state->right_chunks.chunks.size() - 1;
165: 			state->right_tuple = state->right_chunks.chunks[state->right_chunk]->size();
166: 		}
167: 	}
168: 
169: 	if (state->right_chunks.count == 0) {
170: 		// empty join, switch on type
171: 		if (type == JoinType::MARK) {
172: 			// pull a chunk from the LHS
173: 			children[0]->GetChunk(context, state->child_chunk, state->child_state.get());
174: 			if (state->child_chunk.size() == 0) {
175: 				return;
176: 			}
177: 			// RHS empty: set FOUND MATCh vector to false
178: 			chunk.Reference(state->child_chunk);
179: 			auto &mark_vector = chunk.data.back();
180: 			mark_vector.vector_type = VectorType::CONSTANT_VECTOR;
181: 			mark_vector.SetValue(0, Value::BOOLEAN(false));
182: 		} else if (type == JoinType::ANTI) {
183: 			// ANTI join, just pull chunk from RHS
184: 			children[0]->GetChunk(context, chunk, state->child_state.get());
185: 		} else if (type == JoinType::LEFT) {
186: 			children[0]->GetChunk(context, state->child_chunk, state->child_state.get());
187: 			if (state->child_chunk.size() == 0) {
188: 				return;
189: 			}
190: 			chunk.Reference(state->child_chunk);
191: 			for (idx_t idx = state->child_chunk.column_count(); idx < chunk.column_count(); idx++) {
192: 				chunk.data[idx].vector_type = VectorType::CONSTANT_VECTOR;
193: 				ConstantVector::SetNull(chunk.data[idx], true);
194: 			}
195: 		} else {
196: 			throw Exception("Unhandled type for empty NL join");
197: 		}
198: 		return;
199: 	}
200: 
201: 	if ((type == JoinType::INNER || type == JoinType::LEFT) &&
202: 	    state->right_chunk >= state->right_chunks.chunks.size()) {
203: 		return;
204: 	}
205: 	// now that we have fully materialized the right child
206: 	// we have to perform the nested loop join
207: 	do {
208: 		// first check if we have to move to the next child on the right isde
209: 		assert(state->right_chunk < state->right_chunks.chunks.size());
210: 		if (state->right_tuple >= state->right_chunks.chunks[state->right_chunk]->size()) {
211: 			// we exhausted the chunk on the right
212: 			state->right_chunk++;
213: 			if (state->right_chunk >= state->right_chunks.chunks.size()) {
214: 				// we exhausted all right chunks!
215: 				// move to the next left chunk
216: 				do {
217: 					children[0]->GetChunk(context, state->child_chunk, state->child_state.get());
218: 					if (state->child_chunk.size() == 0) {
219: 						return;
220: 					}
221: 
222: 					// resolve the left join condition for the current chunk
223: 					state->lhs_executor.Execute(state->child_chunk, state->left_join_condition);
224: 				} while (state->left_join_condition.size() == 0);
225: 
226: 				state->right_chunk = 0;
227: 			}
228: 			// move to the start of this chunk
229: 			state->left_tuple = 0;
230: 			state->right_tuple = 0;
231: 		}
232: 
233: 		switch (type) {
234: 		case JoinType::SEMI:
235: 		case JoinType::ANTI:
236: 		case JoinType::MARK: {
237: 			// MARK, SEMI and ANTI joins are handled separately because they scan the whole RHS in one go
238: 			bool found_match[STANDARD_VECTOR_SIZE] = {false};
239: 			NestedLoopJoinMark::Perform(state->left_join_condition, state->right_chunks, found_match, conditions);
240: 			if (type == JoinType::MARK) {
241: 				// now construct the mark join result from the found matches
242: 				PhysicalJoin::ConstructMarkJoinResult(state->left_join_condition, state->child_chunk, chunk,
243: 				                                      found_match, state->has_null);
244: 			} else if (type == JoinType::SEMI) {
245: 				// construct the semi join result from the found matches
246: 				PhysicalJoin::ConstructSemiOrAntiJoinResult<true>(state->child_chunk, chunk, found_match);
247: 			} else if (type == JoinType::ANTI) {
248: 				PhysicalJoin::ConstructSemiOrAntiJoinResult<false>(state->child_chunk, chunk, found_match);
249: 			}
250: 			// move to the next LHS chunk in the next iteration
251: 			state->right_tuple = state->right_chunks.chunks[state->right_chunk]->size();
252: 			state->right_chunk = state->right_chunks.chunks.size() - 1;
253: 			if (chunk.size() > 0) {
254: 				return;
255: 			} else {
256: 				continue;
257: 			}
258: 		}
259: 		default:
260: 			break;
261: 		}
262: 
263: 		auto &left_chunk = state->child_chunk;
264: 		auto &right_chunk = *state->right_chunks.chunks[state->right_chunk];
265: 		auto &right_data = *state->right_data.chunks[state->right_chunk];
266: 
267: 		// sanity check
268: 		left_chunk.Verify();
269: 		right_chunk.Verify();
270: 		right_data.Verify();
271: 
272: 		// now perform the join
273: 		switch (type) {
274: 		case JoinType::INNER: {
275: 			SelectionVector lvector(STANDARD_VECTOR_SIZE), rvector(STANDARD_VECTOR_SIZE);
276: 			idx_t match_count =
277: 			    NestedLoopJoinInner::Perform(state->left_tuple, state->right_tuple, state->left_join_condition,
278: 			                                 right_chunk, lvector, rvector, conditions);
279: 			// we have finished resolving the join conditions
280: 			if (match_count == 0) {
281: 				// if there are no results, move on
282: 				continue;
283: 			}
284: 			// we have matching tuples!
285: 			// construct the result
286: 			chunk.Slice(state->child_chunk, lvector, match_count);
287: 			chunk.Slice(right_data, rvector, match_count, state->child_chunk.column_count());
288: 			break;
289: 		}
290: 		default:
291: 			throw NotImplementedException("Unimplemented type for nested loop join!");
292: 		}
293: 	} while (chunk.size() == 0);
294: }
295: 
296: unique_ptr<PhysicalOperatorState> PhysicalNestedLoopJoin::GetOperatorState() {
297: 	return make_unique<PhysicalNestedLoopJoinState>(children[0].get(), children[1].get(), conditions);
298: }
299: 
300: } // namespace duckdb
[end of src/execution/operator/join/physical_nested_loop_join.cpp]
[start of src/execution/operator/join/physical_piecewise_merge_join.cpp]
1: #include "duckdb/execution/operator/join/physical_piecewise_merge_join.hpp"
2: 
3: #include "duckdb/common/vector_operations/vector_operations.hpp"
4: #include "duckdb/execution/expression_executor.hpp"
5: #include "duckdb/execution/merge_join.hpp"
6: #include "duckdb/common/operator/comparison_operators.hpp"
7: 
8: using namespace duckdb;
9: using namespace std;
10: 
11: static void OrderVector(Vector &vector, idx_t count, MergeOrder &order);
12: 
13: class PhysicalPiecewiseMergeJoinState : public PhysicalComparisonJoinState {
14: public:
15: 	PhysicalPiecewiseMergeJoinState(PhysicalOperator *left, PhysicalOperator *right, vector<JoinCondition> &conditions)
16: 	    : PhysicalComparisonJoinState(left, right, conditions), initialized(false), left_position(0), right_position(0),
17: 	      right_chunk_index(0), has_null(false) {
18: 	}
19: 
20: 	bool initialized;
21: 	idx_t left_position;
22: 	idx_t right_position;
23: 	idx_t right_chunk_index;
24: 	DataChunk left_chunk;
25: 	DataChunk join_keys;
26: 	MergeOrder left_orders;
27: 	ChunkCollection right_chunks;
28: 	ChunkCollection right_conditions;
29: 	vector<MergeOrder> right_orders;
30: 	bool has_null;
31: };
32: 
33: PhysicalPiecewiseMergeJoin::PhysicalPiecewiseMergeJoin(LogicalOperator &op, unique_ptr<PhysicalOperator> left,
34:                                                        unique_ptr<PhysicalOperator> right, vector<JoinCondition> cond,
35:                                                        JoinType join_type)
36:     : PhysicalComparisonJoin(op, PhysicalOperatorType::PIECEWISE_MERGE_JOIN, move(cond), join_type) {
37: 	// for now we only support one condition!
38: 	assert(conditions.size() == 1);
39: 	for (auto &cond : conditions) {
40: 		// COMPARE NOT EQUAL not supported yet with merge join
41: 		assert(cond.comparison != ExpressionType::COMPARE_NOTEQUAL);
42: 		assert(cond.left->return_type == cond.right->return_type);
43: 		join_key_types.push_back(cond.left->return_type);
44: 	}
45: 	children.push_back(move(left));
46: 	children.push_back(move(right));
47: }
48: 
49: void PhysicalPiecewiseMergeJoin::GetChunkInternal(ClientContext &context, DataChunk &chunk,
50:                                                   PhysicalOperatorState *state_) {
51: 	auto state = reinterpret_cast<PhysicalPiecewiseMergeJoinState *>(state_);
52: 	assert(conditions.size() == 1);
53: 	if (!state->initialized) {
54: 		// create the sorted pieces
55: 		auto right_state = children[1]->GetOperatorState();
56: 		auto types = children[1]->GetTypes();
57: 
58: 		DataChunk right_chunk;
59: 		right_chunk.Initialize(types);
60: 		state->join_keys.Initialize(join_key_types);
61: 		// first fetch the entire right side
62: 		while (true) {
63: 			children[1]->GetChunk(context, right_chunk, right_state.get());
64: 			if (right_chunk.size() == 0) {
65: 				break;
66: 			}
67: 			// resolve the join keys for this chunk
68: 			state->rhs_executor.SetChunk(right_chunk);
69: 
70: 			state->join_keys.Reset();
71: 			state->join_keys.SetCardinality(right_chunk);
72: 			for (idx_t k = 0; k < conditions.size(); k++) {
73: 				// resolve the join key
74: 				state->rhs_executor.ExecuteExpression(k, state->join_keys.data[k]);
75: 			}
76: 			// append the join keys and the chunk to the chunk collection
77: 			state->right_chunks.Append(right_chunk);
78: 			state->right_conditions.Append(state->join_keys);
79: 		}
80: 		if (state->right_chunks.count == 0 && (type == JoinType::INNER || type == JoinType::SEMI)) {
81: 			// empty RHS with INNER or SEMI join means empty result set
82: 			return;
83: 		}
84: 		// now order all the chunks
85: 		state->right_orders.resize(state->right_conditions.chunks.size());
86: 		for (idx_t i = 0; i < state->right_conditions.chunks.size(); i++) {
87: 			auto &chunk_to_order = *state->right_conditions.chunks[i];
88: 			assert(chunk_to_order.column_count() == 1);
89: 			for (idx_t col_idx = 0; col_idx < chunk_to_order.column_count(); col_idx++) {
90: 				OrderVector(chunk_to_order.data[col_idx], chunk_to_order.size(), state->right_orders[i]);
91: 				if (state->right_orders[i].count < chunk_to_order.size()) {
92: 					// the amount of entries in the order vector is smaller than the amount of entries in the vector
93: 					// this only happens if there are NULL values in the right-hand side
94: 					// hence we set the has_null to true (this is required for the MARK join)
95: 					state->has_null = true;
96: 				}
97: 			}
98: 		}
99: 		state->right_chunk_index = state->right_orders.size();
100: 		state->initialized = true;
101: 	}
102: 
103: 	do {
104: 		// check if we have to fetch a child from the left side
105: 		if (state->right_chunk_index == state->right_orders.size()) {
106: 			// fetch the chunk from the left side
107: 			children[0]->GetChunk(context, state->child_chunk, state->child_state.get());
108: 			if (state->child_chunk.size() == 0) {
109: 				return;
110: 			}
111: 
112: 			// resolve the join keys for the left chunk
113: 			state->join_keys.Reset();
114: 			state->lhs_executor.SetChunk(state->child_chunk);
115: 			state->join_keys.SetCardinality(state->child_chunk);
116: 			for (idx_t k = 0; k < conditions.size(); k++) {
117: 				state->lhs_executor.ExecuteExpression(k, state->join_keys.data[k]);
118: 				// sort by join key
119: 				OrderVector(state->join_keys.data[k], state->join_keys.size(), state->left_orders);
120: 			}
121: 			state->right_chunk_index = 0;
122: 			state->left_position = 0;
123: 			state->right_position = 0;
124: 		}
125: 
126: 		ScalarMergeInfo left_info(state->left_orders, state->join_keys.data[0].type, state->left_position);
127: 
128: 		// first check if the join type is MARK, SEMI or ANTI
129: 		// in this case we loop over the entire right collection immediately
130: 		// because we can never return more than STANDARD_VECTOR_SIZE rows from a join
131: 		switch (type) {
132: 		case JoinType::MARK: {
133: 			// MARK join
134: 			if (state->right_chunks.count > 0) {
135: 				ChunkMergeInfo right_info(state->right_conditions, state->right_orders);
136: 				// first perform the MARK join
137: 				// this method uses the LHS to loop over the entire RHS looking for matches
138: 				MergeJoinMark::Perform(left_info, right_info, conditions[0].comparison);
139: 				// now construct the mark join result from the found matches
140: 				PhysicalJoin::ConstructMarkJoinResult(state->join_keys, state->child_chunk, chunk,
141: 				                                      right_info.found_match, state->has_null);
142: 			} else {
143: 				// RHS empty: result is false for everything
144: 				chunk.Reference(state->child_chunk);
145: 				auto &mark_vector = chunk.data.back();
146: 				mark_vector.vector_type = VectorType::CONSTANT_VECTOR;
147: 				mark_vector.SetValue(0, Value::BOOLEAN(false));
148: 			}
149: 			state->right_chunk_index = state->right_orders.size();
150: 			return;
151: 		}
152: 		default:
153: 			// INNER, LEFT OUTER, etc... join that can return >STANDARD_VECTOR_SIZE entries
154: 			break;
155: 		}
156: 
157: 		// perform the actual merge join
158: 		auto &right_chunk = *state->right_chunks.chunks[state->right_chunk_index];
159: 		auto &right_condition_chunk = *state->right_conditions.chunks[state->right_chunk_index];
160: 		auto &right_orders = state->right_orders[state->right_chunk_index];
161: 
162: 		ScalarMergeInfo right(right_orders, right_condition_chunk.data[0].type, state->right_position);
163: 		// perform the merge join
164: 		switch (type) {
165: 		case JoinType::INNER: {
166: 			idx_t result_count = MergeJoinInner::Perform(left_info, right, conditions[0].comparison);
167: 			if (result_count == 0) {
168: 				// exhausted this chunk on the right side
169: 				// move to the next
170: 				state->right_chunk_index++;
171: 				state->left_position = 0;
172: 				state->right_position = 0;
173: 			} else {
174: 				chunk.Slice(state->child_chunk, left_info.result, result_count);
175: 				chunk.Slice(right_chunk, right.result, result_count, state->child_chunk.column_count());
176: 			}
177: 			break;
178: 		}
179: 		default:
180: 			throw NotImplementedException("Unimplemented join type for merge join");
181: 		}
182: 	} while (chunk.size() == 0);
183: }
184: 
185: unique_ptr<PhysicalOperatorState> PhysicalPiecewiseMergeJoin::GetOperatorState() {
186: 	return make_unique<PhysicalPiecewiseMergeJoinState>(children[0].get(), children[1].get(), conditions);
187: }
188: 
189: template <class T, class OP>
190: static sel_t templated_quicksort_initial(T *data, const SelectionVector &sel, const SelectionVector &not_null_sel,
191:                                          idx_t count, SelectionVector &result) {
192: 	// select pivot
193: 	auto pivot_idx = not_null_sel.get_index(0);
194: 	auto dpivot_idx = sel.get_index(pivot_idx);
195: 	sel_t low = 0, high = count - 1;
196: 	// now insert elements
197: 	for (idx_t i = 1; i < count; i++) {
198: 		auto idx = not_null_sel.get_index(i);
199: 		auto didx = sel.get_index(idx);
200: 		if (OP::Operation(data[didx], data[dpivot_idx])) {
201: 			result.set_index(low++, idx);
202: 		} else {
203: 			result.set_index(high--, idx);
204: 		}
205: 	}
206: 	assert(low == high);
207: 	result.set_index(low, pivot_idx);
208: 	return low;
209: }
210: 
211: template <class T, class OP>
212: static void templated_quicksort_inplace(T *data, const SelectionVector &sel, idx_t count, SelectionVector &result,
213:                                         sel_t left, sel_t right) {
214: 	if (left >= right) {
215: 		return;
216: 	}
217: 
218: 	sel_t middle = left + (right - left) / 2;
219: 	sel_t dpivot_idx = sel.get_index(result.get_index(middle));
220: 
221: 	// move the mid point value to the front.
222: 	sel_t i = left + 1;
223: 	sel_t j = right;
224: 
225: 	result.swap(middle, left);
226: 	while (i <= j) {
227: 		while (i <= j && (OP::Operation(data[sel.get_index(result.get_index(i))], data[dpivot_idx]))) {
228: 			i++;
229: 		}
230: 
231: 		while (i <= j && OP::Operation(data[dpivot_idx], data[sel.get_index(result.get_index(j))])) {
232: 			j--;
233: 		}
234: 
235: 		if (i < j) {
236: 			result.swap(i, j);
237: 		}
238: 	}
239: 	result.swap(i - 1, left);
240: 	sel_t part = i - 1;
241: 
242: 	if (part > 0) {
243: 		templated_quicksort_inplace<T, OP>(data, sel, count, result, left, part - 1);
244: 	}
245: 	templated_quicksort_inplace<T, OP>(data, sel, count, result, part + 1, right);
246: }
247: 
248: template <class T, class OP>
249: void templated_quicksort(T *__restrict data, const SelectionVector &sel, const SelectionVector &not_null_sel,
250:                          idx_t count, SelectionVector &result) {
251: 	auto part = templated_quicksort_initial<T, OP>(data, sel, not_null_sel, count, result);
252: 	if (part > count) {
253: 		return;
254: 	}
255: 	templated_quicksort_inplace<T, OP>(data, sel, count, result, 0, part);
256: 	templated_quicksort_inplace<T, OP>(data, sel, count, result, part + 1, count - 1);
257: }
258: 
259: template <class T>
260: static void templated_quicksort(VectorData &vdata, const SelectionVector &not_null_sel, idx_t not_null_count,
261:                                 SelectionVector &result) {
262: 	if (not_null_count == 0) {
263: 		return;
264: 	}
265: 	templated_quicksort<T, duckdb::LessThanEquals>((T *)vdata.data, *vdata.sel, not_null_sel, not_null_count, result);
266: }
267: 
268: void OrderVector(Vector &vector, idx_t count, MergeOrder &order) {
269: 	if (count == 0) {
270: 		order.count = 0;
271: 		return;
272: 	}
273: 	vector.Orrify(count, order.vdata);
274: 	auto &vdata = order.vdata;
275: 
276: 	// first filter out all the non-null values
277: 	idx_t not_null_count = 0;
278: 	SelectionVector not_null(STANDARD_VECTOR_SIZE);
279: 	for (idx_t i = 0; i < count; i++) {
280: 		auto idx = vdata.sel->get_index(i);
281: 		if (!(*vdata.nullmask)[idx]) {
282: 			not_null.set_index(not_null_count++, i);
283: 		}
284: 	}
285: 	order.count = not_null_count;
286: 	order.order.Initialize(STANDARD_VECTOR_SIZE);
287: 	switch (vector.type) {
288: 	case TypeId::BOOL:
289: 	case TypeId::INT8:
290: 		templated_quicksort<int8_t>(vdata, not_null, not_null_count, order.order);
291: 		break;
292: 	case TypeId::INT16:
293: 		templated_quicksort<int16_t>(vdata, not_null, not_null_count, order.order);
294: 		break;
295: 	case TypeId::INT32:
296: 		templated_quicksort<int32_t>(vdata, not_null, not_null_count, order.order);
297: 		break;
298: 	case TypeId::INT64:
299: 		templated_quicksort<int64_t>(vdata, not_null, not_null_count, order.order);
300: 		break;
301: 	case TypeId::FLOAT:
302: 		templated_quicksort<float>(vdata, not_null, not_null_count, order.order);
303: 		break;
304: 	case TypeId::DOUBLE:
305: 		templated_quicksort<double>(vdata, not_null, not_null_count, order.order);
306: 		break;
307: 	case TypeId::VARCHAR:
308: 		templated_quicksort<string_t>(vdata, not_null, not_null_count, order.order);
309: 		break;
310: 	default:
311: 		throw NotImplementedException("Unimplemented type for sort");
312: 	}
313: }
[end of src/execution/operator/join/physical_piecewise_merge_join.cpp]
[start of src/function/scalar/math/numeric.cpp]
1: #include "duckdb/function/scalar/math_functions.hpp"
2: #include "duckdb/common/vector_operations/vector_operations.hpp"
3: 
4: #include <algorithm>
5: #include <cmath>
6: 
7: using namespace duckdb;
8: using namespace std;
9: 
10: namespace duckdb {
11: 
12: //===--------------------------------------------------------------------===//
13: // abs
14: //===--------------------------------------------------------------------===//
15: struct AbsOperator {
16: 	template <class TA, class TR> static inline TR Operation(TA left) {
17: 		return left < 0 ? left * -1 : left;
18: 	}
19: };
20: 
21: void AbsFun::RegisterFunction(BuiltinFunctions &set) {
22: 	ScalarFunctionSet abs("abs");
23: 	for (auto &type : SQLType::NUMERIC) {
24: 		abs.AddFunction(ScalarFunction({type}, type, ScalarFunction::GetScalarUnaryFunction<AbsOperator>(type)));
25: 	}
26: 	set.AddFunction(abs);
27: }
28: 
29: //===--------------------------------------------------------------------===//
30: // sign
31: //===--------------------------------------------------------------------===//
32: struct SignOperator {
33: 	template <class TA, class TR> static inline TR Operation(TA left) {
34: 		if (left == TA(0))
35: 			return 0;
36: 		else if (left > TA(0))
37: 			return 1;
38: 		else
39: 			return -1;
40: 	}
41: };
42: 
43: void SignFun::RegisterFunction(BuiltinFunctions &set) {
44: 	ScalarFunctionSet sign("sign");
45: 	for (auto &type : SQLType::NUMERIC) {
46: 		sign.AddFunction(ScalarFunction({type}, SQLType::TINYINT,
47: 		                                ScalarFunction::GetScalarUnaryFunctionFixedReturn<int8_t, SignOperator>(type)));
48: 	}
49: 	set.AddFunction(sign);
50: }
51: 
52: //===--------------------------------------------------------------------===//
53: // ceil
54: //===--------------------------------------------------------------------===//
55: struct CeilOperator {
56: 	template <class TA, class TR> static inline TR Operation(TA left) {
57: 		return ceil(left);
58: 	}
59: };
60: 
61: void CeilFun::RegisterFunction(BuiltinFunctions &set) {
62: 	ScalarFunctionSet ceil("ceil");
63: 	for (auto &type : SQLType::NUMERIC) {
64: 		scalar_function_t func;
65: 		if (type.IsIntegral()) {
66: 			// ceil on integral type is a nop
67: 			func = ScalarFunction::NopFunction;
68: 		} else {
69: 			func = ScalarFunction::GetScalarUnaryFunction<CeilOperator>(type);
70: 		}
71: 		ceil.AddFunction(ScalarFunction({type}, type, func));
72: 	}
73: 	set.AddFunction(ceil);
74: 	ceil.name = "ceiling";
75: 	set.AddFunction(ceil);
76: }
77: 
78: //===--------------------------------------------------------------------===//
79: // floor
80: //===--------------------------------------------------------------------===//
81: struct FloorOperator {
82: 	template <class TA, class TR> static inline TR Operation(TA left) {
83: 		return floor(left);
84: 	}
85: };
86: 
87: void FloorFun::RegisterFunction(BuiltinFunctions &set) {
88: 	ScalarFunctionSet floor("floor");
89: 	for (auto &type : SQLType::NUMERIC) {
90: 		scalar_function_t func;
91: 		if (type.IsIntegral()) {
92: 			// floor on integral type is a nop
93: 			func = ScalarFunction::NopFunction;
94: 		} else {
95: 			func = ScalarFunction::GetScalarUnaryFunction<FloorOperator>(type);
96: 		}
97: 		floor.AddFunction(ScalarFunction({type}, type, func));
98: 	}
99: 	set.AddFunction(floor);
100: }
101: 
102: //===--------------------------------------------------------------------===//
103: // round
104: //===--------------------------------------------------------------------===//
105: struct RoundOperator {
106: 	template <class TA, class TB, class TR> static inline TR Operation(TA input, TB precision) {
107: 		if (precision < 0) {
108: 			precision = 0;
109: 		}
110: 		TA modifier = pow(10, precision);
111: 		return (round(input * modifier)) / modifier;
112: 	}
113: };
114: 
115: void RoundFun::RegisterFunction(BuiltinFunctions &set) {
116: 	ScalarFunctionSet round("round");
117: 	for (auto &type : SQLType::NUMERIC) {
118: 		scalar_function_t func;
119: 		if (type.IsIntegral()) {
120: 			// round on integral type is a nop
121: 			func = ScalarFunction::NopFunction;
122: 		} else if (type.id == SQLTypeId::FLOAT) {
123: 			func = ScalarFunction::BinaryFunction<float, int32_t, float, RoundOperator>;
124: 		} else {
125: 			assert(type.id == SQLTypeId::DOUBLE || type.id == SQLTypeId::DECIMAL);
126: 			func = ScalarFunction::BinaryFunction<double, int32_t, double, RoundOperator>;
127: 		}
128: 		round.AddFunction(ScalarFunction({type, SQLType::INTEGER}, type, func));
129: 	}
130: 	set.AddFunction(round);
131: }
132: 
133: //===--------------------------------------------------------------------===//
134: // exp
135: //===--------------------------------------------------------------------===//
136: struct ExpOperator {
137: 	template <class TA, class TR> static inline TR Operation(TA left) {
138: 		return exp(left);
139: 	}
140: };
141: 
142: void ExpFun::RegisterFunction(BuiltinFunctions &set) {
143: 	set.AddFunction(ScalarFunction("exp", {SQLType::DOUBLE}, SQLType::DOUBLE,
144: 	                               ScalarFunction::UnaryFunction<double, double, ExpOperator>));
145: }
146: 
147: //===--------------------------------------------------------------------===//
148: // pow
149: //===--------------------------------------------------------------------===//
150: struct PowOperator {
151: 	template <class TA, class TB, class TR> static inline TR Operation(TA base, TB exponent) {
152: 		return pow(base, exponent);
153: 	}
154: };
155: 
156: void PowFun::RegisterFunction(BuiltinFunctions &set) {
157: 	ScalarFunction power_function("pow", {SQLType::DOUBLE, SQLType::DOUBLE}, SQLType::DOUBLE,
158: 	                              ScalarFunction::BinaryFunction<double, double, double, PowOperator>);
159: 	set.AddFunction(power_function);
160: 	power_function.name = "power";
161: 	set.AddFunction(power_function);
162: }
163: 
164: //===--------------------------------------------------------------------===//
165: // sqrt
166: //===--------------------------------------------------------------------===//
167: struct SqrtOperator {
168: 	template <class TA, class TR> static inline TR Operation(TA left) {
169: 		return sqrt(left);
170: 	}
171: };
172: 
173: void SqrtFun::RegisterFunction(BuiltinFunctions &set) {
174: 	set.AddFunction(ScalarFunction("sqrt", {SQLType::DOUBLE}, SQLType::DOUBLE,
175: 	                               ScalarFunction::UnaryFunction<double, double, SqrtOperator>));
176: }
177: 
178: //===--------------------------------------------------------------------===//
179: // ln
180: //===--------------------------------------------------------------------===//
181: struct LnOperator {
182: 	template <class TA, class TR> static inline TR Operation(TA left) {
183: 		return log(left);
184: 	}
185: };
186: 
187: void LnFun::RegisterFunction(BuiltinFunctions &set) {
188: 	set.AddFunction(ScalarFunction("ln", {SQLType::DOUBLE}, SQLType::DOUBLE,
189: 	                               ScalarFunction::UnaryFunction<double, double, LnOperator>));
190: }
191: 
192: //===--------------------------------------------------------------------===//
193: // log
194: //===--------------------------------------------------------------------===//
195: struct Log10Operator {
196: 	template <class TA, class TR> static inline TR Operation(TA left) {
197: 		return log10(left);
198: 	}
199: };
200: 
201: void Log10Fun::RegisterFunction(BuiltinFunctions &set) {
202: 	ScalarFunction log_function("log10", {SQLType::DOUBLE}, SQLType::DOUBLE,
203: 	                            ScalarFunction::UnaryFunction<double, double, Log10Operator>);
204: 	set.AddFunction(log_function);
205: 	// "log" is an alias for "log10"
206: 	log_function.name = "log";
207: 	set.AddFunction(log_function);
208: }
209: 
210: //===--------------------------------------------------------------------===//
211: // log2
212: //===--------------------------------------------------------------------===//
213: struct Log2Operator {
214: 	template <class TA, class TR> static inline TR Operation(TA left) {
215: 		return log2(left);
216: 	}
217: };
218: 
219: void Log2Fun::RegisterFunction(BuiltinFunctions &set) {
220: 	set.AddFunction(ScalarFunction("log2", {SQLType::DOUBLE}, SQLType::DOUBLE,
221: 	                               ScalarFunction::UnaryFunction<double, double, Log2Operator>));
222: }
223: 
224: //===--------------------------------------------------------------------===//
225: // cbrt
226: //===--------------------------------------------------------------------===//
227: struct CbRtOperator {
228: 	template <class TA, class TR> static inline TR Operation(TA left) {
229: 		return cbrt(left);
230: 	}
231: };
232: 
233: void CbrtFun::RegisterFunction(BuiltinFunctions &set) {
234: 	set.AddFunction(ScalarFunction("cbrt", {SQLType::DOUBLE}, SQLType::DOUBLE,
235: 	                               ScalarFunction::UnaryFunction<double, double, CbRtOperator>));
236: }
237: 
238: //===--------------------------------------------------------------------===//
239: // pi
240: //===--------------------------------------------------------------------===//
241: Value pi_value = Value::DOUBLE(PI);
242: 
243: static void pi_function(DataChunk &args, ExpressionState &state, Vector &result) {
244: 	assert(args.column_count() == 0);
245: 	result.Reference(pi_value);
246: }
247: 
248: void PiFun::RegisterFunction(BuiltinFunctions &set) {
249: 	set.AddFunction(ScalarFunction("pi", {}, SQLType::DOUBLE, pi_function));
250: }
251: 
252: //===--------------------------------------------------------------------===//
253: // degrees
254: //===--------------------------------------------------------------------===//
255: struct DegreesOperator {
256: 	template <class TA, class TR> static inline TR Operation(TA left) {
257: 		return left * (180 / PI);
258: 	}
259: };
260: 
261: void DegreesFun::RegisterFunction(BuiltinFunctions &set) {
262: 	set.AddFunction(ScalarFunction("degrees", {SQLType::DOUBLE}, SQLType::DOUBLE,
263: 	                               ScalarFunction::UnaryFunction<double, double, DegreesOperator>));
264: }
265: 
266: //===--------------------------------------------------------------------===//
267: // radians
268: //===--------------------------------------------------------------------===//
269: struct RadiansOperator {
270: 	template <class TA, class TR> static inline TR Operation(TA left) {
271: 		return left * (PI / 180);
272: 	}
273: };
274: 
275: void RadiansFun::RegisterFunction(BuiltinFunctions &set) {
276: 	set.AddFunction(ScalarFunction("radians", {SQLType::DOUBLE}, SQLType::DOUBLE,
277: 	                               ScalarFunction::UnaryFunction<double, double, RadiansOperator>));
278: }
279: 
280: } // namespace duckdb
[end of src/function/scalar/math/numeric.cpp]
[start of src/function/scalar/operators/bitwise.cpp]
1: #include "duckdb/function/scalar/operators.hpp"
2: #include "duckdb/common/vector_operations/vector_operations.hpp"
3: 
4: using namespace duckdb;
5: using namespace std;
6: 
7: namespace duckdb {
8: 
9: //===--------------------------------------------------------------------===//
10: // & [bitwise_and]
11: //===--------------------------------------------------------------------===//
12: struct BitwiseANDOperator {
13: 	template <class TA, class TB, class TR> static inline TR Operation(TA left, TB right) {
14: 		return left & right;
15: 	}
16: };
17: 
18: void BitwiseAndFun::RegisterFunction(BuiltinFunctions &set) {
19: 	ScalarFunctionSet functions("&");
20: 	for (auto &type : SQLType::INTEGRAL) {
21: 		functions.AddFunction(ScalarFunction({type, type}, type,
22: 		                                     ScalarFunction::GetScalarIntegerBinaryFunction<BitwiseANDOperator>(type)));
23: 	}
24: 	set.AddFunction(functions);
25: }
26: 
27: //===--------------------------------------------------------------------===//
28: // | [bitwise_or]
29: //===--------------------------------------------------------------------===//
30: struct BitwiseOROperator {
31: 	template <class TA, class TB, class TR> static inline TR Operation(TA left, TB right) {
32: 		return left | right;
33: 	}
34: };
35: 
36: void BitwiseOrFun::RegisterFunction(BuiltinFunctions &set) {
37: 	ScalarFunctionSet functions("|");
38: 	for (auto &type : SQLType::INTEGRAL) {
39: 		functions.AddFunction(ScalarFunction({type, type}, type,
40: 		                                     ScalarFunction::GetScalarIntegerBinaryFunction<BitwiseOROperator>(type)));
41: 	}
42: 	set.AddFunction(functions);
43: }
44: 
45: //===--------------------------------------------------------------------===//
46: // # [bitwise_xor]
47: //===--------------------------------------------------------------------===//
48: struct BitwiseXOROperator {
49: 	template <class TA, class TB, class TR> static inline TR Operation(TA left, TB right) {
50: 		return left ^ right;
51: 	}
52: };
53: 
54: void BitwiseXorFun::RegisterFunction(BuiltinFunctions &set) {
55: 	ScalarFunctionSet functions("#");
56: 	for (auto &type : SQLType::INTEGRAL) {
57: 		functions.AddFunction(ScalarFunction({type, type}, type,
58: 		                                     ScalarFunction::GetScalarIntegerBinaryFunction<BitwiseXOROperator>(type)));
59: 	}
60: 	set.AddFunction(functions);
61: }
62: 
63: //===--------------------------------------------------------------------===//
64: // << [bitwise_left_shift]
65: //===--------------------------------------------------------------------===//
66: struct BitwiseShiftLeftOperator {
67: 	template <class TA, class TB, class TR> static inline TR Operation(TA left, TB right) {
68: 		return left << right;
69: 	}
70: };
71: 
72: void LeftShiftFun::RegisterFunction(BuiltinFunctions &set) {
73: 	ScalarFunctionSet functions("<<");
74: 	for (auto &type : SQLType::INTEGRAL) {
75: 		functions.AddFunction(ScalarFunction(
76: 		    {type, type}, type, ScalarFunction::GetScalarIntegerBinaryFunction<BitwiseShiftLeftOperator>(type)));
77: 	}
78: 	set.AddFunction(functions);
79: }
80: 
81: //===--------------------------------------------------------------------===//
82: // >> [bitwise_right_shift]
83: //===--------------------------------------------------------------------===//
84: struct BitwiseShiftRightOperator {
85: 	template <class TA, class TB, class TR> static inline TR Operation(TA left, TB right) {
86: 		return left >> right;
87: 	}
88: };
89: 
90: void RightShiftFun::RegisterFunction(BuiltinFunctions &set) {
91: 	ScalarFunctionSet functions(">>");
92: 	for (auto &type : SQLType::INTEGRAL) {
93: 		functions.AddFunction(ScalarFunction(
94: 		    {type, type}, type, ScalarFunction::GetScalarIntegerBinaryFunction<BitwiseShiftRightOperator>(type)));
95: 	}
96: 	set.AddFunction(functions);
97: }
98: 
99: } // namespace duckdb
[end of src/function/scalar/operators/bitwise.cpp]
[start of src/include/duckdb/common/vector_operations/unary_executor.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/common/vector_operations/unary_executor.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/exception.hpp"
12: #include "duckdb/common/types/vector.hpp"
13: #include "duckdb/common/vector_operations/vector_operations.hpp"
14: 
15: #include <functional>
16: 
17: namespace duckdb {
18: 
19: struct UnaryOperatorWrapper {
20: 	template <class FUNC, class OP, class INPUT_TYPE, class RESULT_TYPE>
21: 	static inline RESULT_TYPE Operation(FUNC fun, INPUT_TYPE input) {
22: 		return OP::template Operation<INPUT_TYPE, RESULT_TYPE>(input);
23: 	}
24: };
25: 
26: struct UnaryLambdaWrapper {
27: 	template <class FUNC, class OP, class INPUT_TYPE, class RESULT_TYPE>
28: 	static inline RESULT_TYPE Operation(FUNC fun, INPUT_TYPE input) {
29: 		return fun(input);
30: 	}
31: };
32: 
33: struct UnaryExecutor {
34: private:
35: 	template <class INPUT_TYPE, class RESULT_TYPE, class OPWRAPPER, class OP, class FUNC, bool IGNORE_NULL>
36: 	static inline void ExecuteLoop(INPUT_TYPE *__restrict ldata, RESULT_TYPE *__restrict result_data, idx_t count,
37: 	                               const SelectionVector *__restrict sel_vector, nullmask_t &nullmask,
38: 	                               nullmask_t &result_nullmask, FUNC fun) {
39: 		ASSERT_RESTRICT(ldata, ldata + count, result_data, result_data + count);
40: 
41: 		if (nullmask.any()) {
42: 			for (idx_t i = 0; i < count; i++) {
43: 				auto idx = sel_vector->get_index(i);
44: 				if (!nullmask[idx]) {
45: 					result_data[i] = OPWRAPPER::template Operation<FUNC, OP, INPUT_TYPE, RESULT_TYPE>(fun, ldata[idx]);
46: 				} else {
47: 					result_nullmask[i] = true;
48: 				}
49: 			}
50: 		} else {
51: 			for (idx_t i = 0; i < count; i++) {
52: 				auto idx = sel_vector->get_index(i);
53: 				result_data[i] = OPWRAPPER::template Operation<FUNC, OP, INPUT_TYPE, RESULT_TYPE>(fun, ldata[idx]);
54: 			}
55: 		}
56: 	}
57: 
58: 	template <class INPUT_TYPE, class RESULT_TYPE, class OPWRAPPER, class OP, class FUNC, bool IGNORE_NULL>
59: 	static inline void ExecuteFlat(INPUT_TYPE *__restrict ldata, RESULT_TYPE *__restrict result_data, idx_t count,
60: 	                               nullmask_t &nullmask, nullmask_t &result_nullmask, FUNC fun) {
61: 		ASSERT_RESTRICT(ldata, ldata + count, result_data, result_data + count);
62: 
63: 		if (IGNORE_NULL && nullmask.any()) {
64: 			result_nullmask = nullmask;
65: 			for (idx_t i = 0; i < count; i++) {
66: 				if (!nullmask[i]) {
67: 					result_data[i] = OPWRAPPER::template Operation<FUNC, OP, INPUT_TYPE, RESULT_TYPE>(fun, ldata[i]);
68: 				}
69: 			}
70: 		} else {
71: 			for (idx_t i = 0; i < count; i++) {
72: 				result_data[i] = OPWRAPPER::template Operation<FUNC, OP, INPUT_TYPE, RESULT_TYPE>(fun, ldata[i]);
73: 			}
74: 		}
75: 	}
76: 
77: 	template <class INPUT_TYPE, class RESULT_TYPE, class OPWRAPPER, class OP, class FUNC, bool IGNORE_NULL>
78: 	static inline void ExecuteStandard(Vector &input, Vector &result, idx_t count, FUNC fun) {
79: 		switch (input.vector_type) {
80: 		case VectorType::CONSTANT_VECTOR: {
81: 			result.vector_type = VectorType::CONSTANT_VECTOR;
82: 			auto result_data = ConstantVector::GetData<RESULT_TYPE>(result);
83: 			auto ldata = ConstantVector::GetData<INPUT_TYPE>(input);
84: 
85: 			if (ConstantVector::IsNull(input)) {
86: 				ConstantVector::SetNull(result, true);
87: 			} else {
88: 				ConstantVector::SetNull(result, false);
89: 				*result_data = OPWRAPPER::template Operation<FUNC, OP, INPUT_TYPE, RESULT_TYPE>(fun, *ldata);
90: 			}
91: 			break;
92: 		}
93: 		case VectorType::FLAT_VECTOR: {
94: 			result.vector_type = VectorType::FLAT_VECTOR;
95: 			auto result_data = FlatVector::GetData<RESULT_TYPE>(result);
96: 			auto ldata = FlatVector::GetData<INPUT_TYPE>(input);
97: 
98: 			FlatVector::SetNullmask(result, FlatVector::Nullmask(input));
99: 
100: 			ExecuteFlat<INPUT_TYPE, RESULT_TYPE, OPWRAPPER, OP, FUNC, IGNORE_NULL>(
101: 			    ldata, result_data, count, FlatVector::Nullmask(input), FlatVector::Nullmask(result), fun);
102: 			break;
103: 		}
104: 		default: {
105: 			VectorData vdata;
106: 			input.Orrify(count, vdata);
107: 
108: 			result.vector_type = VectorType::FLAT_VECTOR;
109: 			auto result_data = FlatVector::GetData<RESULT_TYPE>(result);
110: 			auto ldata = (INPUT_TYPE *)vdata.data;
111: 
112: 			ExecuteLoop<INPUT_TYPE, RESULT_TYPE, OPWRAPPER, OP, FUNC, IGNORE_NULL>(
113: 			    ldata, result_data, count, vdata.sel, *vdata.nullmask, FlatVector::Nullmask(result), fun);
114: 			break;
115: 		}
116: 		}
117: 	}
118: 
119: public:
120: 	template <class INPUT_TYPE, class RESULT_TYPE, class OP, bool IGNORE_NULL = false>
121: 	static void Execute(Vector &input, Vector &result, idx_t count) {
122: 		ExecuteStandard<INPUT_TYPE, RESULT_TYPE, UnaryOperatorWrapper, OP, bool, IGNORE_NULL>(input, result, count,
123: 		                                                                                      false);
124: 	}
125: 
126: 	template <class INPUT_TYPE, class RESULT_TYPE, bool IGNORE_NULL = false,
127: 	          class FUNC = std::function<RESULT_TYPE(INPUT_TYPE)>>
128: 	static void Execute(Vector &input, Vector &result, idx_t count, FUNC fun) {
129: 		ExecuteStandard<INPUT_TYPE, RESULT_TYPE, UnaryLambdaWrapper, bool, FUNC, IGNORE_NULL>(input, result, count,
130: 		                                                                                      fun);
131: 	}
132: };
133: 
134: } // namespace duckdb
[end of src/include/duckdb/common/vector_operations/unary_executor.hpp]
[start of src/parser/transform/expression/transform_bool_expr.cpp]
1: #include "duckdb/parser/expression/conjunction_expression.hpp"
2: #include "duckdb/parser/expression/operator_expression.hpp"
3: #include "duckdb/parser/transformer.hpp"
4: 
5: using namespace duckdb;
6: using namespace std;
7: 
8: unique_ptr<ParsedExpression> Transformer::TransformBoolExpr(PGBoolExpr *root) {
9: 	unique_ptr<ParsedExpression> result;
10: 	for (auto node = root->args->head; node != nullptr; node = node->next) {
11: 		auto next = TransformExpression(reinterpret_cast<PGNode *>(node->data.ptr_value));
12: 
13: 		switch (root->boolop) {
14: 		case PG_AND_EXPR: {
15: 			if (!result) {
16: 				result = move(next);
17: 			} else {
18: 				result = make_unique<ConjunctionExpression>(ExpressionType::CONJUNCTION_AND, move(result), move(next));
19: 			}
20: 			break;
21: 		}
22: 		case PG_OR_EXPR: {
23: 			if (!result) {
24: 				result = move(next);
25: 			} else {
26: 				result = make_unique<ConjunctionExpression>(ExpressionType::CONJUNCTION_OR, move(result), move(next));
27: 			}
28: 			break;
29: 		}
30: 		case PG_NOT_EXPR: {
31: 			if (next->type == ExpressionType::COMPARE_IN) {
32: 				// convert COMPARE_IN to COMPARE_NOT_IN
33: 				next->type = ExpressionType::COMPARE_NOT_IN;
34: 				result = move(next);
35: 			} else {
36: 				result = make_unique<OperatorExpression>(ExpressionType::OPERATOR_NOT, move(next));
37: 			}
38: 			break;
39: 		}
40: 		}
41: 	}
42: 	return result;
43: }
[end of src/parser/transform/expression/transform_bool_expr.cpp]
[start of src/planner/binder/tableref/plan_joinref.cpp]
1: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
2: #include "duckdb/planner/expression/bound_comparison_expression.hpp"
3: #include "duckdb/planner/expression/bound_conjunction_expression.hpp"
4: #include "duckdb/planner/expression/bound_constant_expression.hpp"
5: #include "duckdb/planner/expression/bound_operator_expression.hpp"
6: #include "duckdb/planner/expression/bound_subquery_expression.hpp"
7: #include "duckdb/planner/expression_iterator.hpp"
8: #include "duckdb/planner/binder.hpp"
9: #include "duckdb/planner/operator/logical_any_join.hpp"
10: #include "duckdb/planner/operator/logical_comparison_join.hpp"
11: #include "duckdb/planner/operator/logical_cross_product.hpp"
12: #include "duckdb/planner/operator/logical_filter.hpp"
13: #include "duckdb/planner/tableref/bound_joinref.hpp"
14: 
15: using namespace duckdb;
16: using namespace std;
17: 
18: //! Create a JoinCondition from a comparison
19: static bool CreateJoinCondition(Expression &expr, unordered_set<idx_t> &left_bindings,
20:                                 unordered_set<idx_t> &right_bindings, vector<JoinCondition> &conditions) {
21: 	// comparison
22: 	auto &comparison = (BoundComparisonExpression &)expr;
23: 	auto left_side = JoinSide::GetJoinSide(*comparison.left, left_bindings, right_bindings);
24: 	auto right_side = JoinSide::GetJoinSide(*comparison.right, left_bindings, right_bindings);
25: 	if (left_side != JoinSide::BOTH && right_side != JoinSide::BOTH) {
26: 		// join condition can be divided in a left/right side
27: 		JoinCondition condition;
28: 		condition.comparison = expr.type;
29: 		auto left = move(comparison.left);
30: 		auto right = move(comparison.right);
31: 		if (left_side == JoinSide::RIGHT) {
32: 			// left = right, right = left, flip the comparison symbol and reverse sides
33: 			swap(left, right);
34: 			condition.comparison = FlipComparisionExpression(expr.type);
35: 		}
36: 		condition.left = move(left);
37: 		condition.right = move(right);
38: 		conditions.push_back(move(condition));
39: 		return true;
40: 	}
41: 	return false;
42: }
43: 
44: unique_ptr<LogicalOperator> LogicalComparisonJoin::CreateJoin(JoinType type, unique_ptr<LogicalOperator> left_child,
45:                                                               unique_ptr<LogicalOperator> right_child,
46:                                                               unordered_set<idx_t> &left_bindings,
47:                                                               unordered_set<idx_t> &right_bindings,
48:                                                               vector<unique_ptr<Expression>> &expressions) {
49: 	vector<JoinCondition> conditions;
50: 	vector<unique_ptr<Expression>> arbitrary_expressions;
51: 	// first check if we can create
52: 	for (idx_t i = 0; i < expressions.size(); i++) {
53: 		auto &expr = expressions[i];
54: 		auto total_side = JoinSide::GetJoinSide(*expr, left_bindings, right_bindings);
55: 		if (total_side != JoinSide::BOTH) {
56: 			// join condition does not reference both sides, add it as filter under the join
57: 			if (type == JoinType::LEFT && total_side == JoinSide::RIGHT) {
58: 				// filter is on RHS and the join is a LEFT OUTER join, we can push it in the right child
59: 				if (right_child->type != LogicalOperatorType::FILTER) {
60: 					// not a filter yet, push a new empty filter
61: 					auto filter = make_unique<LogicalFilter>();
62: 					filter->AddChild(move(right_child));
63: 					right_child = move(filter);
64: 				}
65: 				// push the expression into the filter
66: 				auto &filter = (LogicalFilter &)*right_child;
67: 				filter.expressions.push_back(move(expr));
68: 				continue;
69: 			}
70: 		} else if (expr->type >= ExpressionType::COMPARE_EQUAL &&
71: 		           expr->type <= ExpressionType::COMPARE_GREATERTHANOREQUALTO) {
72: 			// comparison, check if we can create a comparison JoinCondition
73: 			if (CreateJoinCondition(*expr, left_bindings, right_bindings, conditions)) {
74: 				// successfully created the join condition
75: 				continue;
76: 			}
77: 		} else if (expr->type == ExpressionType::OPERATOR_NOT) {
78: 			auto &not_expr = (BoundOperatorExpression &)*expr;
79: 			assert(not_expr.children.size() == 1);
80: 			ExpressionType child_type = not_expr.children[0]->GetExpressionType();
81: 			// the condition is ON NOT (EXPRESSION)
82: 			// we can transform this to remove the NOT if the child is a Comparison
83: 			// e.g.:
84: 			// ON NOT (X = 3) can be turned into ON (X <> 3)
85: 			// ON NOT (X > 3) can be turned into ON (X <= 3)
86: 			// for non-comparison operators here we just push the filter
87: 			if (child_type >= ExpressionType::COMPARE_EQUAL &&
88: 			    child_type <= ExpressionType::COMPARE_GREATERTHANOREQUALTO) {
89: 				// switcheroo the child condition
90: 				// our join needs to compare explicit left and right sides. So we
91: 				// invert the condition to express NOT, this way we can still use
92: 				// equi-joins
93: 				not_expr.children[0]->type = NegateComparisionExpression(child_type);
94: 				if (CreateJoinCondition(*not_expr.children[0], left_bindings, right_bindings, conditions)) {
95: 					// successfully created the join condition
96: 					continue;
97: 				}
98: 			}
99: 		}
100: 		arbitrary_expressions.push_back(move(expr));
101: 	}
102: 	if (conditions.size() > 0) {
103: 		// we successfully convertedexpressions into JoinConditions
104: 		// create a LogicalComparisonJoin
105: 		auto comp_join = make_unique<LogicalComparisonJoin>(type);
106: 		comp_join->conditions = move(conditions);
107: 		comp_join->children.push_back(move(left_child));
108: 		comp_join->children.push_back(move(right_child));
109: 		if (arbitrary_expressions.size() > 0) {
110: 			// we have some arbitrary expressions as well
111: 			// add them to a filter
112: 			auto filter = make_unique<LogicalFilter>();
113: 			for (auto &expr : arbitrary_expressions) {
114: 				filter->expressions.push_back(move(expr));
115: 			}
116: 			LogicalFilter::SplitPredicates(filter->expressions);
117: 			filter->children.push_back(move(comp_join));
118: 			return move(filter);
119: 		}
120: 		return move(comp_join);
121: 	} else {
122: 		if (arbitrary_expressions.size() == 0) {
123: 			// all conditions were pushed down, add TRUE predicate
124: 			arbitrary_expressions.push_back(make_unique<BoundConstantExpression>(Value::BOOLEAN(true)));
125: 		}
126: 		// if we get here we could not create any JoinConditions
127: 		// turn this into an arbitrary expression join
128: 		auto any_join = make_unique<LogicalAnyJoin>(type);
129: 		// create the condition
130: 		any_join->children.push_back(move(left_child));
131: 		any_join->children.push_back(move(right_child));
132: 		// AND all the arbitrary expressions together
133: 		// do the same with any remaining conditions
134: 		any_join->condition = move(arbitrary_expressions[0]);
135: 		for (idx_t i = 1; i < arbitrary_expressions.size(); i++) {
136: 			any_join->condition = make_unique<BoundConjunctionExpression>(
137: 			    ExpressionType::CONJUNCTION_AND, move(any_join->condition), move(arbitrary_expressions[i]));
138: 		}
139: 		return move(any_join);
140: 	}
141: }
142: 
143: unique_ptr<LogicalOperator> Binder::CreatePlan(BoundJoinRef &ref) {
144: 	auto left = CreatePlan(*ref.left);
145: 	auto right = CreatePlan(*ref.right);
146: 	if (ref.type == JoinType::RIGHT) {
147: 		ref.type = JoinType::LEFT;
148: 		std::swap(left, right);
149: 	}
150: 
151: 	if (ref.type == JoinType::INNER) {
152: 		// inner join, generate a cross product + filter
153: 		// this will be later turned into a proper join by the join order optimizer
154: 		auto cross_product = make_unique<LogicalCrossProduct>();
155: 
156: 		cross_product->AddChild(move(left));
157: 		cross_product->AddChild(move(right));
158: 
159: 		unique_ptr<LogicalOperator> root = move(cross_product);
160: 
161: 		auto filter = make_unique<LogicalFilter>(move(ref.condition));
162: 		// visit the expressions in the filter
163: 		for (idx_t i = 0; i < filter->expressions.size(); i++) {
164: 			PlanSubqueries(&filter->expressions[i], &root);
165: 		}
166: 		filter->AddChild(move(root));
167: 		return move(filter);
168: 	}
169: 
170: 	// split the expressions by the AND clause
171: 	vector<unique_ptr<Expression>> expressions;
172: 	expressions.push_back(move(ref.condition));
173: 	LogicalFilter::SplitPredicates(expressions);
174: 
175: 	// find the table bindings on the LHS and RHS of the join
176: 	unordered_set<idx_t> left_bindings, right_bindings;
177: 	LogicalJoin::GetTableReferences(*left, left_bindings);
178: 	LogicalJoin::GetTableReferences(*right, right_bindings);
179: 	// now create the join operator from the set of join conditions
180: 	auto result = LogicalComparisonJoin::CreateJoin(ref.type, move(left), move(right), left_bindings, right_bindings,
181: 	                                                expressions);
182: 
183: 	LogicalOperator *join;
184: 	if (result->type == LogicalOperatorType::FILTER) {
185: 		join = result->children[0].get();
186: 	} else {
187: 		join = result.get();
188: 	}
189: 
190: 	// we visit the expressions depending on the type of join
191: 	if (join->type == LogicalOperatorType::COMPARISON_JOIN) {
192: 		// comparison join
193: 		// in this join we visit the expressions on the LHS with the LHS as root node
194: 		// and the expressions on the RHS with the RHS as root node
195: 		auto &comp_join = (LogicalComparisonJoin &)*join;
196: 		for (idx_t i = 0; i < comp_join.conditions.size(); i++) {
197: 			PlanSubqueries(&comp_join.conditions[i].left, &comp_join.children[0]);
198: 			PlanSubqueries(&comp_join.conditions[i].right, &comp_join.children[1]);
199: 		}
200: 	} else if (join->type == LogicalOperatorType::ANY_JOIN) {
201: 		auto &any_join = (LogicalAnyJoin &)*join;
202: 		// for the any join we just visit the condition
203: 		if (any_join.condition->HasSubquery()) {
204: 			throw NotImplementedException("Cannot perform non-inner join on subquery!");
205: 		}
206: 	}
207: 	return result;
208: }
[end of src/planner/binder/tableref/plan_joinref.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: