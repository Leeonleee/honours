You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Histogram function returns error on Python API
The following code:
```python
def histogram(con, table, column, bins):
    sql_code = f"""
        SELECT histogram({column}) FROM {table}
    """
    con.execute(sql_code)
    return con.fetchall()
```

Returns the following error:
`RuntimeError: unsupported type: LIST<STRUCT<bucket: INTEGER, count: UBIGINT>>`

</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
2: 
3: ![.github/workflows/main.yml](https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
6: 
7: 
8: ## Installation
9: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
10: 
11: ## Development
12: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
13: 
14: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
15: 
16: 
[end of README.md]
[start of tools/pythonpkg/src/pyresult.cpp]
1: #include "duckdb_python/array_wrapper.hpp"
2: #include "duckdb_python/pyresult.hpp"
3: #include "duckdb/common/types/date.hpp"
4: #include "duckdb/common/types/hugeint.hpp"
5: #include "duckdb/common/types/time.hpp"
6: #include "duckdb/common/types/timestamp.hpp"
7: #include "duckdb/common/arrow.hpp"
8: 
9: #include "datetime.h" // from Python
10: 
11: namespace duckdb {
12: 
13: void DuckDBPyResult::Initialize(py::handle &m) {
14: 	py::class_<DuckDBPyResult>(m, "DuckDBPyResult")
15: 	    .def("close", &DuckDBPyResult::Close)
16: 	    .def("fetchone", &DuckDBPyResult::Fetchone)
17: 	    .def("fetchall", &DuckDBPyResult::Fetchall)
18: 	    .def("fetchnumpy", &DuckDBPyResult::FetchNumpy)
19: 	    .def("fetchdf", &DuckDBPyResult::FetchDF)
20: 	    .def("fetch_df", &DuckDBPyResult::FetchDF)
21: 	    .def("fetch_df_chunk", &DuckDBPyResult::FetchDFChunk)
22: 	    .def("fetch_arrow_table", &DuckDBPyResult::FetchArrowTable)
23: 	    .def("arrow", &DuckDBPyResult::FetchArrowTable)
24: 	    .def("df", &DuckDBPyResult::FetchDF);
25: 
26: 	PyDateTime_IMPORT;
27: }
28: 
29: template <class SRC>
30: static SRC FetchScalar(Vector &src_vec, idx_t offset) {
31: 	auto src_ptr = FlatVector::GetData<SRC>(src_vec);
32: 	return src_ptr[offset];
33: }
34: 
35: py::object DuckDBPyResult::Fetchone() {
36: 	if (!result) {
37: 		throw std::runtime_error("result closed");
38: 	}
39: 	if (!current_chunk || chunk_offset >= current_chunk->size()) {
40: 		current_chunk = result->Fetch();
41: 		chunk_offset = 0;
42: 	}
43: 	if (!current_chunk || current_chunk->size() == 0) {
44: 		return py::none();
45: 	}
46: 	py::tuple res(result->types.size());
47: 
48: 	for (idx_t col_idx = 0; col_idx < result->types.size(); col_idx++) {
49: 		auto &mask = FlatVector::Validity(current_chunk->data[col_idx]);
50: 		if (!mask.RowIsValid(chunk_offset)) {
51: 			res[col_idx] = py::none();
52: 			continue;
53: 		}
54: 		auto val = current_chunk->data[col_idx].GetValue(chunk_offset);
55: 		switch (result->types[col_idx].id()) {
56: 		case LogicalTypeId::BOOLEAN:
57: 			res[col_idx] = val.GetValue<bool>();
58: 			break;
59: 		case LogicalTypeId::TINYINT:
60: 			res[col_idx] = val.GetValue<int8_t>();
61: 			break;
62: 		case LogicalTypeId::SMALLINT:
63: 			res[col_idx] = val.GetValue<int16_t>();
64: 			break;
65: 		case LogicalTypeId::INTEGER:
66: 			res[col_idx] = val.GetValue<int32_t>();
67: 			break;
68: 		case LogicalTypeId::BIGINT:
69: 			res[col_idx] = val.GetValue<int64_t>();
70: 			break;
71: 		case LogicalTypeId::UTINYINT:
72: 			res[col_idx] = val.GetValue<uint8_t>();
73: 			break;
74: 		case LogicalTypeId::USMALLINT:
75: 			res[col_idx] = val.GetValue<uint16_t>();
76: 			break;
77: 		case LogicalTypeId::UINTEGER:
78: 			res[col_idx] = val.GetValue<uint32_t>();
79: 			break;
80: 		case LogicalTypeId::UBIGINT:
81: 			res[col_idx] = val.GetValue<uint64_t>();
82: 			break;
83: 		case LogicalTypeId::HUGEINT: {
84: 			auto hugeint_str = val.GetValue<string>();
85: 			res[col_idx] = PyLong_FromString((char *)hugeint_str.c_str(), nullptr, 10);
86: 			break;
87: 		}
88: 		case LogicalTypeId::FLOAT:
89: 			res[col_idx] = val.GetValue<float>();
90: 			break;
91: 		case LogicalTypeId::DOUBLE:
92: 			res[col_idx] = val.GetValue<double>();
93: 			break;
94: 		case LogicalTypeId::DECIMAL: {
95: 			py::object decimal_py = py::module_::import("decimal").attr("Decimal");
96: 			res[col_idx] = decimal_py(val.ToString());
97: 		} break;
98: 		case LogicalTypeId::VARCHAR:
99: 			res[col_idx] = val.GetValue<string>();
100: 			break;
101: 		case LogicalTypeId::BLOB:
102: 			res[col_idx] = py::bytes(val.GetValueUnsafe<string>());
103: 			break;
104: 		case LogicalTypeId::TIMESTAMP:
105: 		case LogicalTypeId::TIMESTAMP_MS:
106: 		case LogicalTypeId::TIMESTAMP_NS:
107: 		case LogicalTypeId::TIMESTAMP_SEC: {
108: 			D_ASSERT(result->types[col_idx].InternalType() == PhysicalType::INT64);
109: 			auto timestamp = val.GetValueUnsafe<timestamp_t>();
110: 			if (result->types[col_idx].id() == LogicalTypeId::TIMESTAMP_MS) {
111: 				timestamp = Timestamp::FromEpochMs(timestamp.value);
112: 			} else if (result->types[col_idx].id() == LogicalTypeId::TIMESTAMP_NS) {
113: 				timestamp = Timestamp::FromEpochNanoSeconds(timestamp.value);
114: 			} else if (result->types[col_idx].id() == LogicalTypeId::TIMESTAMP_SEC) {
115: 				timestamp = Timestamp::FromEpochSeconds(timestamp.value);
116: 			}
117: 			int32_t year, month, day, hour, min, sec, micros;
118: 			date_t date;
119: 			dtime_t time;
120: 			Timestamp::Convert(timestamp, date, time);
121: 			Date::Convert(date, year, month, day);
122: 			Time::Convert(time, hour, min, sec, micros);
123: 			res[col_idx] = PyDateTime_FromDateAndTime(year, month, day, hour, min, sec, micros);
124: 			break;
125: 		}
126: 		case LogicalTypeId::TIME: {
127: 			D_ASSERT(result->types[col_idx].InternalType() == PhysicalType::INT64);
128: 
129: 			int32_t hour, min, sec, microsec;
130: 			auto time = val.GetValueUnsafe<dtime_t>();
131: 			duckdb::Time::Convert(time, hour, min, sec, microsec);
132: 			res[col_idx] = PyTime_FromTime(hour, min, sec, microsec);
133: 			break;
134: 		}
135: 		case LogicalTypeId::DATE: {
136: 			D_ASSERT(result->types[col_idx].InternalType() == PhysicalType::INT32);
137: 
138: 			auto date = val.GetValueUnsafe<date_t>();
139: 			int32_t year, month, day;
140: 			duckdb::Date::Convert(date, year, month, day);
141: 			res[col_idx] = PyDate_FromDate(year, month, day);
142: 			break;
143: 		}
144: 
145: 		default:
146: 			throw std::runtime_error("unsupported type: " + result->types[col_idx].ToString());
147: 		}
148: 	}
149: 	chunk_offset++;
150: 	return move(res);
151: }
152: 
153: py::list DuckDBPyResult::Fetchall() {
154: 	py::list res;
155: 	while (true) {
156: 		auto fres = Fetchone();
157: 		if (fres.is_none()) {
158: 			break;
159: 		}
160: 		res.append(fres);
161: 	}
162: 	return res;
163: }
164: 
165: py::dict DuckDBPyResult::FetchNumpy(bool stream) {
166: 	if (!result) {
167: 		throw std::runtime_error("result closed");
168: 	}
169: 
170: 	// iterate over the result to materialize the data needed for the NumPy arrays
171: 	idx_t initial_capacity = STANDARD_VECTOR_SIZE * 2;
172: 	if (result->type == QueryResultType::MATERIALIZED_RESULT) {
173: 		// materialized query result: we know exactly how much space we need
174: 		auto &materialized = (MaterializedQueryResult &)*result;
175: 		initial_capacity = materialized.collection.Count();
176: 	}
177: 
178: 	NumpyResultConversion conversion(result->types, initial_capacity);
179: 	if (result->type == QueryResultType::MATERIALIZED_RESULT) {
180: 		auto &materialized = (MaterializedQueryResult &)*result;
181: 		if (!stream) {
182: 			for (auto &chunk : materialized.collection.Chunks()) {
183: 				conversion.Append(*chunk);
184: 			}
185: 			materialized.collection.Reset();
186: 		} else {
187: 			conversion.Append(*materialized.Fetch());
188: 		}
189: 	} else {
190: 		if (!stream) {
191: 			while (true) {
192: 				auto chunk = result->FetchRaw();
193: 				if (!chunk || chunk->size() == 0) {
194: 					// finished
195: 					break;
196: 				}
197: 				conversion.Append(*chunk);
198: 			}
199: 		} else {
200: 			auto chunk = result->FetchRaw();
201: 			if (chunk && chunk->size() > 0) {
202: 				conversion.Append(*chunk);
203: 			}
204: 		}
205: 	}
206: 
207: 	// now that we have materialized the result in contiguous arrays, construct the actual NumPy arrays
208: 	py::dict res;
209: 	for (idx_t col_idx = 0; col_idx < result->types.size(); col_idx++) {
210: 		res[result->names[col_idx].c_str()] = conversion.ToArray(col_idx);
211: 	}
212: 	return res;
213: }
214: 
215: py::object DuckDBPyResult::FetchDF() {
216: 	return py::module::import("pandas").attr("DataFrame").attr("from_dict")(FetchNumpy());
217: }
218: 
219: py::object DuckDBPyResult::FetchDFChunk() {
220: 	return py::module::import("pandas").attr("DataFrame").attr("from_dict")(FetchNumpy(true));
221: }
222: 
223: py::object DuckDBPyResult::FetchArrowTable() {
224: 	if (!result) {
225: 		throw std::runtime_error("result closed");
226: 	}
227: 	py::gil_scoped_acquire acquire;
228: 	auto pyarrow_lib_module = py::module::import("pyarrow").attr("lib");
229: 
230: 	auto batch_import_func = pyarrow_lib_module.attr("RecordBatch").attr("_import_from_c");
231: 	auto from_batches_func = pyarrow_lib_module.attr("Table").attr("from_batches");
232: 	auto schema_import_func = pyarrow_lib_module.attr("Schema").attr("_import_from_c");
233: 	ArrowSchema schema;
234: 	result->ToArrowSchema(&schema);
235: 	auto schema_obj = schema_import_func((uint64_t)&schema);
236: 
237: 	py::list batches;
238: 	while (true) {
239: 		auto data_chunk = result->Fetch();
240: 		if (!data_chunk || data_chunk->size() == 0) {
241: 			break;
242: 		}
243: 		ArrowArray data;
244: 		data_chunk->ToArrowArray(&data);
245: 		ArrowSchema arrow_schema;
246: 		result->ToArrowSchema(&arrow_schema);
247: 		batches.append(batch_import_func((uint64_t)&data, (uint64_t)&arrow_schema));
248: 	}
249: 	return from_batches_func(batches, schema_obj);
250: }
251: 
252: py::list DuckDBPyResult::Description() {
253: 	py::list desc(result->names.size());
254: 	for (idx_t col_idx = 0; col_idx < result->names.size(); col_idx++) {
255: 		py::tuple col_desc(7);
256: 		col_desc[0] = py::str(result->names[col_idx]);
257: 		col_desc[1] = py::none();
258: 		col_desc[2] = py::none();
259: 		col_desc[3] = py::none();
260: 		col_desc[4] = py::none();
261: 		col_desc[5] = py::none();
262: 		col_desc[6] = py::none();
263: 		desc[col_idx] = col_desc;
264: 	}
265: 	return desc;
266: }
267: 
268: void DuckDBPyResult::Close() {
269: 	result = nullptr;
270: }
271: 
272: } // namespace duckdb
[end of tools/pythonpkg/src/pyresult.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: