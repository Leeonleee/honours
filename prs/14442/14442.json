{
  "repo": "duckdb/duckdb",
  "pull_number": 14442,
  "instance_id": "duckdb__duckdb-14442",
  "issue_numbers": [
    "14430"
  ],
  "base_commit": "aed52f5cabe34075c53bcec4407e297124c8d336",
  "patch": "diff --git a/src/execution/perfect_aggregate_hashtable.cpp b/src/execution/perfect_aggregate_hashtable.cpp\nindex 4fa6f08f47b8..c378e61edafb 100644\n--- a/src/execution/perfect_aggregate_hashtable.cpp\n+++ b/src/execution/perfect_aggregate_hashtable.cpp\n@@ -130,7 +130,12 @@ void PerfectAggregateHashTable::AddChunk(DataChunk &groups, DataChunk &payload)\n \t// compute the actual pointer to the data by adding it to the base HT pointer and multiplying by the tuple size\n \tfor (idx_t i = 0; i < groups.size(); i++) {\n \t\tconst auto group = address_data[i];\n-\t\tD_ASSERT(group < total_groups);\n+\t\tif (group >= total_groups) {\n+\t\t\tthrow InvalidInputException(\"Perfect hash aggregate: aggregate group %llu exceeded total groups %llu. This \"\n+\t\t\t                            \"likely means that the statistics in your data source are corrupt.\\n* PRAGMA \"\n+\t\t\t                            \"disable_optimizer to disable optimizations that rely on correct statistics\",\n+\t\t\t                            group, total_groups);\n+\t\t}\n \t\tgroup_is_set[group] = true;\n \t\taddress_data[i] = uintptr_t(data) + group * tuple_size;\n \t}\n",
  "test_patch": "diff --git a/data/parquet-testing/corrupt_stats.parquet b/data/parquet-testing/corrupt_stats.parquet\nnew file mode 100644\nindex 000000000000..26f4bb0cac3d\nBinary files /dev/null and b/data/parquet-testing/corrupt_stats.parquet differ\ndiff --git a/test/sql/copy/parquet/corrupt_stats.test b/test/sql/copy/parquet/corrupt_stats.test\nnew file mode 100644\nindex 000000000000..146c27f9d2e4\n--- /dev/null\n+++ b/test/sql/copy/parquet/corrupt_stats.test\n@@ -0,0 +1,18 @@\n+# name: test/sql/copy/parquet/corrupt_stats.test\n+# description: Issue #14430: group by a timestamp column in a parquet file can cause the process to crash\n+# group: [parquet]\n+\n+require parquet\n+\n+statement error\n+SELECT a FROM 'data/parquet-testing/corrupt_stats.parquet' GROUP BY a;\n+----\n+This likely means that the statistics in your data source are corrupt\n+\n+statement ok\n+PRAGMA disable_optimizer\n+\n+query I\n+SELECT a FROM 'data/parquet-testing/corrupt_stats.parquet' GROUP BY a;\n+----\n+2021-01-01 12:00:00\n",
  "problem_statement": "group by a timestamp column in a parquet file can cause the process to crash\n### What happens?\r\n\r\nGroup by a timestamp column in a parquet file can cause the process to crash.\r\n[demo.parquet.zip](https://github.com/user-attachments/files/17427101/demo.zip)\r\n\r\n\r\n### To Reproduce\r\n\r\n```sql\r\nSELECT a FROM 'demo.parquet' GROUP BY a;\r\n```\r\n\r\n### OS:\r\n\r\nwindows 11, x86_64\r\n\r\n### DuckDB Version:\r\n\r\n1.1.2\r\n\r\n### DuckDB Client:\r\n\r\nCLI\r\n\r\n### Hardware:\r\n\r\n_No response_\r\n\r\n### Full Name:\r\n\r\nJason Jia\r\n\r\n### Affiliation:\r\n\r\npersonal\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have tested with a stable release\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nYes\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\n",
  "hints_text": "",
  "created_at": "2024-10-18T15:21:42Z"
}