{
  "repo": "duckdb/duckdb",
  "pull_number": 5854,
  "instance_id": "duckdb__duckdb-5854",
  "issue_numbers": [
    "5782",
    "5781"
  ],
  "base_commit": "46f8e0b7cee4ebdb0762a676e75cd976ce314cd1",
  "patch": "diff --git a/data/csv/ips.csv.gz b/data/csv/ips.csv.gz\nnew file mode 100644\nindex 000000000000..e781a601a941\nBinary files /dev/null and b/data/csv/ips.csv.gz differ\ndiff --git a/src/function/scalar/list/list_aggregates.cpp b/src/function/scalar/list/list_aggregates.cpp\nindex 011531d5e69a..d780927a1404 100644\n--- a/src/function/scalar/list/list_aggregates.cpp\n+++ b/src/function/scalar/list/list_aggregates.cpp\n@@ -144,7 +144,6 @@ struct UniqueFunctor {\n \n template <class FUNCTION_FUNCTOR, bool IS_AGGR = false>\n static void ListAggregatesFunction(DataChunk &args, ExpressionState &state, Vector &result) {\n-\n \tauto count = args.size();\n \tVector &lists = args.data[0];\n \n@@ -167,6 +166,7 @@ static void ListAggregatesFunction(DataChunk &args, ExpressionState &state, Vect\n \n \tauto lists_size = ListVector::GetListSize(lists);\n \tauto &child_vector = ListVector::GetEntry(lists);\n+\tchild_vector.Flatten(lists_size);\n \n \tUnifiedVectorFormat child_data;\n \tchild_vector.ToUnifiedFormat(lists_size, child_data);\n@@ -216,7 +216,7 @@ static void ListAggregatesFunction(DataChunk &args, ExpressionState &state, Vect\n \t\t\t// states vector is full, update\n \t\t\tif (states_idx == STANDARD_VECTOR_SIZE) {\n \t\t\t\t// update the aggregate state(s)\n-\t\t\t\tVector slice = Vector(child_vector, sel_vector, states_idx);\n+\t\t\t\tVector slice(child_vector, sel_vector, states_idx);\n \t\t\t\taggr.function.update(&slice, aggr_input_data, 1, state_vector_update, states_idx);\n \n \t\t\t\t// reset values\n@@ -232,7 +232,7 @@ static void ListAggregatesFunction(DataChunk &args, ExpressionState &state, Vect\n \n \t// update the remaining elements of the last list(s)\n \tif (states_idx != 0) {\n-\t\tVector slice = Vector(child_vector, sel_vector, states_idx);\n+\t\tVector slice(child_vector, sel_vector, states_idx);\n \t\taggr.function.update(&slice, aggr_input_data, 1, state_vector_update, states_idx);\n \t}\n \n",
  "test_patch": "diff --git a/test/sql/types/nested/list/list_aggregate_dict.test b/test/sql/types/nested/list/list_aggregate_dict.test\nnew file mode 100644\nindex 000000000000..68012ec82da3\n--- /dev/null\n+++ b/test/sql/types/nested/list/list_aggregate_dict.test\n@@ -0,0 +1,29 @@\n+# name: test/sql/types/nested/list/list_aggregate_dict.test\n+# description: Test lists with aggregations on a table with dictionary compression\n+# group: [list]\n+\n+load __TEST_DIR__/store_dict.db\n+\n+statement ok\n+pragma force_compression='dictionary';\n+\n+statement ok\n+CREATE TABLE Hosts (ips varchar[]);\n+\n+statement ok\n+INSERT INTO Hosts SELECT * FROM 'data/csv/ips.csv.gz';\n+\n+query I\n+SELECT min(list_string_agg(ips)) FROM Hosts\n+----\n+248.128.0.0\n+\n+query I\n+SELECT min(ips[1]) FROM Hosts\n+----\n+248.128.0.0\n+\n+query I\n+SELECT min([x[2:4] for x in ips if x[1]::int > 1]) FROM Hosts\n+----\n+[48.]\n",
  "problem_statement": "read_csv_auto() reads a varchar column (starting with '0') as integer \n### What happens?\n\nMy csv file contains a column with zip codes, some starting with a '0':\r\n![image](https://user-images.githubusercontent.com/17407796/209432957-54e7ea06-a05e-4268-9a4f-528f9db2b33e.png)\r\n\r\nread_csv_auto() convert that column as integer, i exepectd a varchar, the leading '0' is lost\r\n![image](https://user-images.githubusercontent.com/17407796/209432994-63cbc91c-561d-412c-9f53-1fb90fc04d59.png)\r\n\n\n### To Reproduce\n\nSELECT * FROM read_csv_auto('test.csv');\r\n\r\n\r\n[test.csv](https://github.com/duckdb/duckdb/files/10298425/test.csv)\r\n\n\n### OS:\n\nx64\n\n### DuckDB Version:\n\n0.6\n\n### DuckDB Client:\n\nDBeaver\n\n### Full Name:\n\neric mauviere\n\n### Affiliation:\n\nicem7\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\nIntermittent segfault with list_string_agg on dictionary-compressed column\n### What happens?\n\nA segmentation fault occurs when using list_string_agg on a dictionary-compressed VARCHAR[] column. It doesn't happen every time, but it seems to happen more often than not.\n\n### To Reproduce\n\nUsing the attached test.db:\r\n\r\n```\r\nduckdb test.db 'SELECT list_string_agg(ips) FROM Hosts'\r\n```\r\n\r\nNote that I created the table using the attached CSV of IPs like this:\r\n```\r\nduckdb test.db \"pragma force_compression='dictionary'; CREATE TABLE Hosts (ips varchar[]); INSERT INTO Hosts SELECT * FROM 'ips.csv';\"\r\n```\r\n\r\n[ips.csv.gz](https://github.com/duckdb/duckdb/files/10297715/ips.csv.gz)\r\n[test.db.gz](https://github.com/duckdb/duckdb/files/10297716/test.db.gz)\n\n### OS:\n\nOSX x64, Linux x64\n\n### DuckDB Version:\n\n0.6.1\n\n### DuckDB Client:\n\nCLI\n\n### Full Name:\n\nPaul Ellsworth\n\n### Affiliation:\n\nTenable, Inc.\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "You likely want to use `read_csv` and specify the columns and their types explicitly\r\nhttps://duckdb.org/docs/data/csv\n> You likely want to use `read_csv` and specify the columns and their types explicitly https://duckdb.org/docs/data/csv\r\n\r\nWell, i know i could, but i love the simplicity of read_csv_auto(), which is also used as a default in many third parties tools (Observable, Tad...)\nAh I see, but this requires context, there are definitely situations where this is the behavior you would want\r\nand besides that you can fix it up yourself with something like `format` (if the length of the CODGEO column is a constant)\nThanks once again for your reactivity! I understand, but i don't know other contexts where it would be appropriate for a 09001 code value to be altered into a number such as 9001. \r\n\r\nWith R for instance, readr::read_delim() reads properly such a file, without any particular specification. \nI am too fond of the R package `readr` and its nice well-chosen defaults for parsing CSV and similar formats - especially its convenient type guessing for column types. \r\n\r\nWhile there are parameters such as ALL_VARCHAR and SAMPLE_SIZE available for use with duckdb's `read_csv_auto` (where the first one disables the type guessing entirely for all columns), it seems those options wouldn't help much in a use case such as this one, where one would like for strings starting with leading zeroes by default to be guessed to be varchar rather than numerical.\r\n\r\nAnother nice thing that `readr` allows for is to specify column types for a subset of the columns and a default for all the rest, making it less onerous in comparison with having to specify all the types for all the columns. Also, when required, the column type specification can be made in a compact way. For [example](https://readr.tidyverse.org/reference/read_delim.html): `read_csv('test.csv', col_types = 'c??')` reads the first column as characters and guesses the type for the next two columns.\nI should also mention that building the same table, except using _uncompressed_ instead of _dictionary_ works fine. :)",
  "created_at": "2023-01-08T12:39:41Z"
}