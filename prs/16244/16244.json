{
  "repo": "duckdb/duckdb",
  "pull_number": 16244,
  "instance_id": "duckdb__duckdb-16244",
  "issue_numbers": [
    "16118"
  ],
  "base_commit": "6415640753e28d21ca5fb340d28a2bf435bab313",
  "patch": "diff --git a/extension/core_functions/aggregate/distributive/string_agg.cpp b/extension/core_functions/aggregate/distributive/string_agg.cpp\nindex b694a23656fa..cdbd9e003aa5 100644\n--- a/extension/core_functions/aggregate/distributive/string_agg.cpp\n+++ b/extension/core_functions/aggregate/distributive/string_agg.cpp\n@@ -44,14 +44,7 @@ struct StringAggFunction {\n \t\tif (!state.dataptr) {\n \t\t\tfinalize_data.ReturnNull();\n \t\t} else {\n-\t\t\ttarget = StringVector::AddString(finalize_data.result, state.dataptr, state.size);\n-\t\t}\n-\t}\n-\n-\ttemplate <class STATE>\n-\tstatic void Destroy(STATE &state, AggregateInputData &aggr_input_data) {\n-\t\tif (state.dataptr) {\n-\t\t\tdelete[] state.dataptr;\n+\t\t\ttarget = string_t(state.dataptr, state.size);\n \t\t}\n \t}\n \n@@ -59,12 +52,12 @@ struct StringAggFunction {\n \t\treturn true;\n \t}\n \n-\tstatic inline void PerformOperation(StringAggState &state, const char *str, const char *sep, idx_t str_size,\n-\t                                    idx_t sep_size) {\n+\tstatic inline void PerformOperation(StringAggState &state, ArenaAllocator &allocator, const char *str,\n+\t                                    const char *sep, idx_t str_size, idx_t sep_size) {\n \t\tif (!state.dataptr) {\n \t\t\t// first iteration: allocate space for the string and copy it into the state\n \t\t\tstate.alloc_size = MaxValue<idx_t>(8, NextPowerOfTwo(str_size));\n-\t\t\tstate.dataptr = new char[state.alloc_size];\n+\t\t\tstate.dataptr = char_ptr_cast(allocator.Allocate(state.alloc_size));\n \t\t\tstate.size = str_size;\n \t\t\tmemcpy(state.dataptr, str, str_size);\n \t\t} else {\n@@ -72,13 +65,12 @@ struct StringAggFunction {\n \t\t\tidx_t required_size = state.size + str_size + sep_size;\n \t\t\tif (required_size > state.alloc_size) {\n \t\t\t\t// no space! allocate extra space\n+\t\t\t\tconst auto old_size = state.alloc_size;\n \t\t\t\twhile (state.alloc_size < required_size) {\n \t\t\t\t\tstate.alloc_size *= 2;\n \t\t\t\t}\n-\t\t\t\tauto new_data = new char[state.alloc_size];\n-\t\t\t\tmemcpy(new_data, state.dataptr, state.size);\n-\t\t\t\tdelete[] state.dataptr;\n-\t\t\t\tstate.dataptr = new_data;\n+\t\t\t\tstate.dataptr =\n+\t\t\t\t    char_ptr_cast(allocator.Reallocate(data_ptr_cast(state.dataptr), old_size, state.alloc_size));\n \t\t\t}\n \t\t\t// copy the separator\n \t\t\tmemcpy(state.dataptr + state.size, sep, sep_size);\n@@ -89,14 +81,15 @@ struct StringAggFunction {\n \t\t}\n \t}\n \n-\tstatic inline void PerformOperation(StringAggState &state, string_t str, optional_ptr<FunctionData> data_p) {\n+\tstatic inline void PerformOperation(StringAggState &state, ArenaAllocator &allocator, string_t str,\n+\t                                    optional_ptr<FunctionData> data_p) {\n \t\tauto &data = data_p->Cast<StringAggBindData>();\n-\t\tPerformOperation(state, str.GetData(), data.sep.c_str(), str.GetSize(), data.sep.size());\n+\t\tPerformOperation(state, allocator, str.GetData(), data.sep.c_str(), str.GetSize(), data.sep.size());\n \t}\n \n \ttemplate <class INPUT_TYPE, class STATE, class OP>\n \tstatic void Operation(STATE &state, const INPUT_TYPE &input, AggregateUnaryInput &unary_input) {\n-\t\tPerformOperation(state, input, unary_input.input.bind_data);\n+\t\tPerformOperation(state, unary_input.input.allocator, input, unary_input.input.bind_data);\n \t}\n \n \ttemplate <class INPUT_TYPE, class STATE, class OP>\n@@ -113,8 +106,8 @@ struct StringAggFunction {\n \t\t\t// source is not set: skip combining\n \t\t\treturn;\n \t\t}\n-\t\tPerformOperation(target, string_t(source.dataptr, UnsafeNumericCast<uint32_t>(source.size)),\n-\t\t                 aggr_input_data.bind_data);\n+\t\tPerformOperation(target, aggr_input_data.allocator,\n+\t\t                 string_t(source.dataptr, UnsafeNumericCast<uint32_t>(source.size)), aggr_input_data.bind_data);\n \t}\n };\n \n@@ -162,8 +155,7 @@ AggregateFunctionSet StringAggFun::GetFunctions() {\n \t    AggregateFunction::UnaryScatterUpdate<StringAggState, string_t, StringAggFunction>,\n \t    AggregateFunction::StateCombine<StringAggState, StringAggFunction>,\n \t    AggregateFunction::StateFinalize<StringAggState, string_t, StringAggFunction>,\n-\t    AggregateFunction::UnaryUpdate<StringAggState, string_t, StringAggFunction>, StringAggBind,\n-\t    AggregateFunction::StateDestroy<StringAggState, StringAggFunction>);\n+\t    AggregateFunction::UnaryUpdate<StringAggState, string_t, StringAggFunction>, StringAggBind);\n \tstring_agg_param.serialize = StringAggSerialize;\n \tstring_agg_param.deserialize = StringAggDeserialize;\n \tstring_agg.AddFunction(string_agg_param);\ndiff --git a/extension/core_functions/aggregate/nested/list.cpp b/extension/core_functions/aggregate/nested/list.cpp\nindex 7b23987d6875..40a917eff7e8 100644\n--- a/extension/core_functions/aggregate/nested/list.cpp\n+++ b/extension/core_functions/aggregate/nested/list.cpp\n@@ -116,7 +116,6 @@ static void ListFinalize(Vector &states_vector, AggregateInputData &aggr_input_d\n \n \t// first iterate over all entries and set up the list entries, and get the newly required total length\n \tfor (idx_t i = 0; i < count; i++) {\n-\n \t\tauto &state = *states[states_data.sel->get_index(i)];\n \t\tconst auto rid = i + offset;\n \t\tresult_data[rid].offset = total_len;\ndiff --git a/extension/core_functions/scalar/list/list_aggregates.cpp b/extension/core_functions/scalar/list/list_aggregates.cpp\nindex 1b2aab71dca7..a5ce49eed759 100644\n--- a/extension/core_functions/scalar/list/list_aggregates.cpp\n+++ b/extension/core_functions/scalar/list/list_aggregates.cpp\n@@ -15,7 +15,17 @@\n \n namespace duckdb {\n \n-// FIXME: use a local state for each thread to increase performance?\n+struct ListAggregatesLocalState : public FunctionLocalState {\n+\texplicit ListAggregatesLocalState(Allocator &allocator) : arena_allocator(allocator) {\n+\t}\n+\n+\tArenaAllocator arena_allocator;\n+};\n+\n+unique_ptr<FunctionLocalState> ListAggregatesInitLocalState(ExpressionState &state, const BoundFunctionExpression &expr,\n+                                                            FunctionData *bind_data) {\n+\treturn make_uniq<ListAggregatesLocalState>(BufferAllocator::Get(state.GetContext()));\n+}\n // FIXME: benchmark the use of simple_update against using update (if applicable)\n \n static unique_ptr<FunctionData> ListAggregatesBindFailure(ScalarFunction &bound_function) {\n@@ -207,7 +217,8 @@ static void ListAggregatesFunction(DataChunk &args, ExpressionState &state, Vect\n \tauto &func_expr = state.expr.Cast<BoundFunctionExpression>();\n \tauto &info = func_expr.bind_info->Cast<ListAggregatesBindData>();\n \tauto &aggr = info.aggr_expr->Cast<BoundAggregateExpression>();\n-\tArenaAllocator allocator(Allocator::DefaultAllocator());\n+\tauto &allocator = ExecuteFunctionState::GetFunctionState(state)->Cast<ListAggregatesLocalState>().arena_allocator;\n+\tallocator.Reset();\n \tAggregateInputData aggr_input_data(aggr.bind_info.get(), allocator);\n \n \tD_ASSERT(aggr.function.update);\n@@ -511,8 +522,9 @@ static unique_ptr<FunctionData> ListUniqueBind(ClientContext &context, ScalarFun\n }\n \n ScalarFunction ListAggregateFun::GetFunction() {\n-\tauto result = ScalarFunction({LogicalType::LIST(LogicalType::ANY), LogicalType::VARCHAR}, LogicalType::ANY,\n-\t                             ListAggregateFunction, ListAggregateBind);\n+\tauto result =\n+\t    ScalarFunction({LogicalType::LIST(LogicalType::ANY), LogicalType::VARCHAR}, LogicalType::ANY,\n+\t                   ListAggregateFunction, ListAggregateBind, nullptr, nullptr, ListAggregatesInitLocalState);\n \tBaseScalarFunction::SetReturnsError(result);\n \tresult.null_handling = FunctionNullHandling::SPECIAL_HANDLING;\n \tresult.varargs = LogicalType::ANY;\n@@ -523,12 +535,12 @@ ScalarFunction ListAggregateFun::GetFunction() {\n \n ScalarFunction ListDistinctFun::GetFunction() {\n \treturn ScalarFunction({LogicalType::LIST(LogicalType::ANY)}, LogicalType::LIST(LogicalType::ANY),\n-\t                      ListDistinctFunction, ListDistinctBind);\n+\t                      ListDistinctFunction, ListDistinctBind, nullptr, nullptr, ListAggregatesInitLocalState);\n }\n \n ScalarFunction ListUniqueFun::GetFunction() {\n \treturn ScalarFunction({LogicalType::LIST(LogicalType::ANY)}, LogicalType::UBIGINT, ListUniqueFunction,\n-\t                      ListUniqueBind);\n+\t                      ListUniqueBind, nullptr, nullptr, ListAggregatesInitLocalState);\n }\n \n } // namespace duckdb\ndiff --git a/extension/json/buffered_json_reader.cpp b/extension/json/buffered_json_reader.cpp\nindex 68a830c7fd12..a57d375771c2 100644\n--- a/extension/json/buffered_json_reader.cpp\n+++ b/extension/json/buffered_json_reader.cpp\n@@ -92,7 +92,7 @@ void JSONFileHandle::ReadAtPosition(char *pointer, idx_t size, idx_t position, b\n \t\tauto &handle = override_handle ? *override_handle.get() : *file_handle.get();\n \t\tif (can_seek) {\n \t\t\thandle.Read(pointer, size, position);\n-\t\t} else if (sample_run) { // Cache the buffer\n+\t\t} else if (file_handle->IsPipe()) { // Cache the buffer\n \t\t\thandle.Read(pointer, size, position);\n \n \t\t\tcached_buffers.emplace_back(allocator.Allocate(size));\n@@ -128,7 +128,7 @@ bool JSONFileHandle::Read(char *pointer, idx_t &read_size, idx_t requested_size,\n \tif (can_seek) {\n \t\tread_size = ReadInternal(pointer, requested_size);\n \t\tread_position += read_size;\n-\t} else if (sample_run) { // Cache the buffer\n+\t} else if (file_handle->IsPipe()) { // Cache the buffer\n \t\tread_size = ReadInternal(pointer, requested_size);\n \t\tif (read_size > 0) {\n \t\t\tcached_buffers.emplace_back(allocator.Allocate(read_size));\ndiff --git a/extension/json/json_extension.cpp b/extension/json/json_extension.cpp\nindex e0665260a5bf..d609fd82ef68 100644\n--- a/extension/json/json_extension.cpp\n+++ b/extension/json/json_extension.cpp\n@@ -17,12 +17,17 @@\n namespace duckdb {\n \n static DefaultMacro json_macros[] = {\n-    {DEFAULT_SCHEMA, \"json_group_array\", {\"x\", nullptr}, {{nullptr, nullptr}}, \"to_json(list(x))\"},\n+    {DEFAULT_SCHEMA,\n+     \"json_group_array\",\n+     {\"x\", nullptr},\n+     {{nullptr, nullptr}},\n+     \"CAST('[' || string_agg(CASE WHEN x IS NULL THEN 'null'::JSON ELSE to_json(x) END, ',') || ']' AS JSON)\"},\n     {DEFAULT_SCHEMA,\n      \"json_group_object\",\n-     {\"name\", \"value\", nullptr},\n+     {\"n\", \"v\", nullptr},\n      {{nullptr, nullptr}},\n-     \"to_json(map(list(name), list(value)))\"},\n+     \"CAST('{' || string_agg(to_json(n::VARCHAR) || ':' || CASE WHEN v IS NULL THEN 'null'::JSON ELSE to_json(v) END, \"\n+     \"',') || '}' AS JSON)\"},\n     {DEFAULT_SCHEMA,\n      \"json_group_structure\",\n      {\"x\", nullptr},\ndiff --git a/src/common/compressed_file_system.cpp b/src/common/compressed_file_system.cpp\nindex b8f032a656e3..d222bf13de12 100644\n--- a/src/common/compressed_file_system.cpp\n+++ b/src/common/compressed_file_system.cpp\n@@ -31,6 +31,8 @@ void CompressedFile::Initialize(bool write) {\n \tstream_data.out_buff_start = stream_data.out_buff.get();\n \tstream_data.out_buff_end = stream_data.out_buff.get();\n \n+\tcurrent_position = 0;\n+\n \tstream_wrapper = compressed_fs.CreateStream();\n \tstream_wrapper->Initialize(*this, write);\n }\ndiff --git a/src/execution/operator/aggregate/physical_window.cpp b/src/execution/operator/aggregate/physical_window.cpp\nindex 8b8b2a162b32..956c50ee18ad 100644\n--- a/src/execution/operator/aggregate/physical_window.cpp\n+++ b/src/execution/operator/aggregate/physical_window.cpp\n@@ -575,6 +575,11 @@ class WindowLocalSourceState : public LocalSourceState {\n \n \texplicit WindowLocalSourceState(WindowGlobalSourceState &gsource);\n \n+\tvoid ReleaseLocalStates() {\n+\t\tauto &local_states = window_hash_group->thread_states.at(task->thread_idx);\n+\t\tlocal_states.clear();\n+\t}\n+\n \t//! Does the task have more work to do?\n \tbool TaskFinished() const {\n \t\treturn !task || task->begin_idx == task->end_idx;\n@@ -792,6 +797,12 @@ void WindowGlobalSourceState::FinishTask(TaskPtr task) {\n }\n \n bool WindowLocalSourceState::TryAssignTask() {\n+\tD_ASSERT(TaskFinished());\n+\tif (task && task->stage == WindowGroupStage::GETDATA) {\n+\t\t// If this state completed the last block in the previous iteration,\n+\t\t// release out local state memory.\n+\t\tReleaseLocalStates();\n+\t}\n \t// Because downstream operators may be using our internal buffers,\n \t// we can't \"finish\" a task until we are about to get the next one.\n \n@@ -888,10 +899,6 @@ void WindowLocalSourceState::GetData(DataChunk &result) {\n \t\t++task->begin_idx;\n \t}\n \n-\t// If that was the last block, release out local state memory.\n-\tif (TaskFinished()) {\n-\t\tlocal_states.clear();\n-\t}\n \tresult.Verify();\n }\n \ndiff --git a/src/function/window/window_constant_aggregator.cpp b/src/function/window/window_constant_aggregator.cpp\nindex 0e09c6f99996..312161223903 100644\n--- a/src/function/window/window_constant_aggregator.cpp\n+++ b/src/function/window/window_constant_aggregator.cpp\n@@ -18,6 +18,10 @@ class WindowConstantAggregatorGlobalState : public WindowAggregatorGlobalState {\n \n \tvoid Finalize(const FrameStats &stats);\n \n+\t~WindowConstantAggregatorGlobalState() override {\n+\t\tstatef.Destroy();\n+\t}\n+\n \t//! Partition starts\n \tvector<idx_t> partition_offsets;\n \t//! Reused result state container for the window functions\n@@ -304,11 +308,7 @@ void WindowConstantAggregator::Finalize(WindowAggregatorState &gstate, WindowAgg\n \tlastate.statef.Combine(gastate.statef);\n \tlastate.statef.Destroy();\n \n-\t//\tLast one out turns off the lights!\n-\tif (++gastate.finalized == gastate.locals) {\n-\t\tgastate.statef.Finalize(*gastate.results);\n-\t\tgastate.statef.Destroy();\n-\t}\n+\tgastate.statef.Finalize(*gastate.results);\n }\n \n unique_ptr<WindowAggregatorState> WindowConstantAggregator::GetLocalState(const WindowAggregatorState &gstate) const {\ndiff --git a/src/function/window/window_distinct_aggregator.cpp b/src/function/window/window_distinct_aggregator.cpp\nindex 1dc940f9533d..77e00a02ee58 100644\n--- a/src/function/window/window_distinct_aggregator.cpp\n+++ b/src/function/window/window_distinct_aggregator.cpp\n@@ -190,6 +190,10 @@ class WindowDistinctAggregatorLocalState : public WindowAggregatorLocalState {\n public:\n \texplicit WindowDistinctAggregatorLocalState(const WindowDistinctAggregatorGlobalState &aggregator);\n \n+\t~WindowDistinctAggregatorLocalState() override {\n+\t\tstatef.Destroy();\n+\t}\n+\n \tvoid Sink(DataChunk &sink_chunk, DataChunk &coll_chunk, idx_t input_idx, optional_ptr<SelectionVector> filter_sel,\n \t          idx_t filtered);\n \tvoid Finalize(WindowAggregatorGlobalState &gastate, CollectionPtr collection) override;\n@@ -740,7 +744,6 @@ void WindowDistinctAggregatorLocalState::Evaluate(const WindowDistinctAggregator\n \n \t//\tFinalise the result aggregates and write to the result\n \tstatef.Finalize(result);\n-\tstatef.Destroy();\n }\n \n unique_ptr<WindowAggregatorState> WindowDistinctAggregator::GetLocalState(const WindowAggregatorState &gstate) const {\ndiff --git a/src/optimizer/column_lifetime_analyzer.cpp b/src/optimizer/column_lifetime_analyzer.cpp\nindex 8b6f1152bf8c..233617b4ced1 100644\n--- a/src/optimizer/column_lifetime_analyzer.cpp\n+++ b/src/optimizer/column_lifetime_analyzer.cpp\n@@ -108,6 +108,7 @@ void ColumnLifetimeAnalyzer::VisitOperator(LogicalOperator &op) {\n \t\t//! When RETURNING is used, a PROJECTION is the top level operator for INSERTS, UPDATES, and DELETES\n \t\t//! We still need to project all values from these operators so the projection\n \t\t//! on top of them can select from only the table values being inserted.\n+\tcase LogicalOperatorType::LOGICAL_GET:\n \tcase LogicalOperatorType::LOGICAL_UNION:\n \tcase LogicalOperatorType::LOGICAL_EXCEPT:\n \tcase LogicalOperatorType::LOGICAL_INTERSECT:\n",
  "test_patch": "diff --git a/test/optimizer/column_lifetime_analyzer/summary_column_lifetime.test b/test/optimizer/column_lifetime_analyzer/summary_column_lifetime.test\nnew file mode 100644\nindex 000000000000..2bb177aca0df\n--- /dev/null\n+++ b/test/optimizer/column_lifetime_analyzer/summary_column_lifetime.test\n@@ -0,0 +1,9 @@\n+# name: test/optimizer/column_lifetime_analyzer/summary_column_lifetime.test\n+# description: Test column lifetime analyzer with SUMMARY (internal issue #4138)\n+# group: [column_lifetime_analyzer]\n+\n+statement ok\n+create table data as select * from range(0,4000) tbl(col);\n+\n+statement ok\n+SELECT * FROM summary((SELECT col FROM data ORDER BY col));\ndiff --git a/test/sql/json/test_json_copy.test_slow b/test/sql/json/test_json_copy.test_slow\nindex 9041e5d4d6ec..56c2c6a36803 100644\n--- a/test/sql/json/test_json_copy.test_slow\n+++ b/test/sql/json/test_json_copy.test_slow\n@@ -1,5 +1,5 @@\n # name: test/sql/json/test_json_copy.test_slow\n-# description: Test JSON COPY using TPC-H\n+# description: Test JSON COPY\n # group: [json]\n \n require json\ndiff --git a/test/sql/json/test_json_copy_tpch.test_slow b/test/sql/json/test_json_copy_tpch.test_slow\nindex ab614e4b7d11..41db483ad968 100644\n--- a/test/sql/json/test_json_copy_tpch.test_slow\n+++ b/test/sql/json/test_json_copy_tpch.test_slow\n@@ -38,7 +38,7 @@ statement ok\n set memory_limit='100mb'\n \n statement ok\n-COPY lineitem from '__TEST_DIR__/lineitem.json' (ARRAY)\n+COPY lineitem FROM '__TEST_DIR__/lineitem.json' (ARRAY)\n \n # 500mb should be enough for the rest\n statement ok\n@@ -49,6 +49,13 @@ PRAGMA tpch(1)\n ----\n <FILE>:extension/tpch/dbgen/answers/sf0.1/q01.csv\n \n+# also test gzipped\n+statement ok\n+COPY lineitem TO '__TEST_DIR__/lineitem.json.gz'\n+\n+statement ok\n+FROM '__TEST_DIR__/lineitem.json.gz'\n+\n statement ok\n rollback\n \n",
  "problem_statement": "v1.2 broke select from gzipped json\n### What happens?\n\nAfter upgrading to v1.2, reading a gzipped json file demanded a large increase in maximum object size and subsequently failed with invalid character. The same json without gzip compression reads fine in both v1.x and v1.2.\n\n### To Reproduce\n\n```\nSELECT * FROM 'fundos_list.json.gz';\n```\n[fundos_list.json.gz](https://github.com/user-attachments/files/18707966/fundos_list.json.gz)\n\n### OS:\n\nWindows 11\n\n### DuckDB Version:\n\n1.2\n\n### DuckDB Client:\n\npython\n\n### Hardware:\n\n_No response_\n\n### Full Name:\n\nLeonardo Horta\n\n### Affiliation:\n\nSparta Fundos de Investimento\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a stable release\n\n### Did you include all relevant data sets for reproducing the issue?\n\nYes\n\n### Did you include all code required to reproduce the issue?\n\n- [x] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [x] Yes, I have\n",
  "hints_text": "",
  "created_at": "2025-02-14T13:57:42Z"
}