You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Column ID mismatch in adaptive filters
This was found by @mrigger's SQLancer (somewhat minified by us)

```SQL
CREATE TABLE t0(c0 DATETIME DEFAULT(0.45428781614730807), c1 DATE, PRIMARY KEY(c1));
CREATE TABLE t1(c0 DOUBLE NOT NULL, c1 BOOLEAN);
insert into t0 (c0, c1) values (NULL, '2019-11-26');
insert into t1 values (42, true);

SELECT t0.rowid, t1.c1, t1.c0 FROM t1, t0 WHERE (((t1.rowid NOT IN (((t1.c1) ::BOOLEAN), ((t1.c0) ::INT1))))AND((false BETWEEN '[' AND t1.c0))) UNION SELECT t0.rowid, t1.c1, t1.c0 FROM t1, t0 WHERE (NOT (((t1.rowid NOT IN (((t1.c1) ::BOOLEAN), ((t1.c0) ::INT1))))AND((false BETWEEN '[' AND t1.c0)))) UNION SELECT t0.rowid, t1.c1, t1.c0 FROM t1, t0 WHERE (((((t1.rowid NOT IN (((t1.c1) ::BOOLEAN), ((t1.c0) ::TINYINT))))AND((false BETWEEN '[' AND t1.c0)))) IS NULL);
```


The issue is an out of bound access in data_table.cpp line 399:

```cpp
auto tf_idx = state.adaptive_filter->permutation[i];
columns[tf_idx]->Select(transaction, state.column_scans[tf_idx], result.data[tf_idx], sel, approved_tuple_count, table_filters[tf_idx]);
```

</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="30">
2: 
3: [![Travis](https://api.travis-ci.org/cwida/duckdb.svg?branch=master)](https://travis-ci.org/cwida/duckdb)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
6: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3901452.svg)](https://zenodo.org/record/3901452)
7: 
8: 
9: ## Installation
10: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
11: 
12: ## Development
13: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
14: 
15: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
16: 
17: 
[end of README.md]
[start of src/storage/data_table.cpp]
1: #include "duckdb/storage/data_table.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/common/exception.hpp"
5: #include "duckdb/common/helper.hpp"
6: #include "duckdb/common/vector_operations/vector_operations.hpp"
7: #include "duckdb/execution/expression_executor.hpp"
8: #include "duckdb/planner/constraints/list.hpp"
9: #include "duckdb/transaction/transaction.hpp"
10: #include "duckdb/transaction/transaction_manager.hpp"
11: #include "duckdb/storage/table/transient_segment.hpp"
12: #include "duckdb/storage/storage_manager.hpp"
13: 
14: using namespace duckdb;
15: using namespace std;
16: using namespace chrono;
17: 
18: DataTable::DataTable(StorageManager &storage, string schema, string table, vector<TypeId> types_,
19:                      unique_ptr<vector<unique_ptr<PersistentSegment>>[]> data)
20:     : info(make_shared<DataTableInfo>(schema, table)), types(types_), storage(storage),
21:       persistent_manager(make_shared<VersionManager>(*info)), transient_manager(make_shared<VersionManager>(*info)),
22:       is_root(true) {
23: 	// set up the segment trees for the column segments
24: 	for (idx_t i = 0; i < types.size(); i++) {
25: 		auto column_data = make_shared<ColumnData>(*storage.buffer_manager, *info);
26: 		column_data->type = types[i];
27: 		column_data->column_idx = i;
28: 		columns.push_back(move(column_data));
29: 	}
30: 
31: 	// initialize the table with the existing data from disk, if any
32: 	if (data && data[0].size() > 0) {
33: 		// first append all the segments to the set of column segments
34: 		for (idx_t i = 0; i < types.size(); i++) {
35: 			columns[i]->Initialize(data[i]);
36: 			if (columns[i]->persistent_rows != columns[0]->persistent_rows) {
37: 				throw Exception("Column length mismatch in table load!");
38: 			}
39: 		}
40: 		persistent_manager->max_row = columns[0]->persistent_rows;
41: 		transient_manager->base_row = persistent_manager->max_row;
42: 	}
43: }
44: 
45: DataTable::DataTable(ClientContext &context, DataTable &parent, ColumnDefinition &new_column, Expression *default_value)
46:     : info(parent.info), types(parent.types), storage(parent.storage), persistent_manager(parent.persistent_manager),
47:       transient_manager(parent.transient_manager), columns(parent.columns), is_root(true) {
48: 	// prevent any new tuples from being added to the parent
49: 	lock_guard<mutex> parent_lock(parent.append_lock);
50: 	// add the new column to this DataTable
51: 	auto new_column_type = GetInternalType(new_column.type);
52: 	idx_t new_column_idx = columns.size();
53: 
54: 	types.push_back(new_column_type);
55: 	auto column_data = make_shared<ColumnData>(*storage.buffer_manager, *info);
56: 	column_data->type = new_column_type;
57: 	column_data->column_idx = new_column_idx;
58: 	columns.push_back(move(column_data));
59: 
60: 	// fill the column with its DEFAULT value, or NULL if none is specified
61: 	idx_t rows_to_write = persistent_manager->max_row + transient_manager->max_row;
62: 	if (rows_to_write > 0) {
63: 		ExpressionExecutor executor;
64: 		DataChunk dummy_chunk;
65: 		Vector result(new_column_type);
66: 		if (!default_value) {
67: 			FlatVector::Nullmask(result).set();
68: 		} else {
69: 			executor.AddExpression(*default_value);
70: 		}
71: 
72: 		ColumnAppendState state;
73: 		columns[new_column_idx]->InitializeAppend(state);
74: 		for (idx_t i = 0; i < rows_to_write; i += STANDARD_VECTOR_SIZE) {
75: 			idx_t rows_in_this_vector = std::min(rows_to_write - i, (idx_t)STANDARD_VECTOR_SIZE);
76: 			if (default_value) {
77: 				dummy_chunk.SetCardinality(rows_in_this_vector);
78: 				executor.ExecuteExpression(dummy_chunk, result);
79: 			}
80: 			columns[new_column_idx]->Append(state, result, rows_in_this_vector);
81: 		}
82: 	}
83: 	// also add this column to client local storage
84: 	Transaction::GetTransaction(context).storage.AddColumn(&parent, this, new_column, default_value);
85: 
86: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
87: 	parent.is_root = false;
88: }
89: 
90: DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t removed_column)
91:     : info(parent.info), types(parent.types), storage(parent.storage), persistent_manager(parent.persistent_manager),
92:       transient_manager(parent.transient_manager), columns(parent.columns), is_root(true) {
93: 	// prevent any new tuples from being added to the parent
94: 	lock_guard<mutex> parent_lock(parent.append_lock);
95: 	// first check if there are any indexes that exist that point to the removed column
96: 	for (auto &index : info->indexes) {
97: 		for (auto &column_id : index->column_ids) {
98: 			if (column_id == removed_column) {
99: 				throw CatalogException("Cannot drop this column: an index depends on it!");
100: 			} else if (column_id > removed_column) {
101: 				throw CatalogException("Cannot drop this column: an index depends on a column after it!");
102: 			}
103: 		}
104: 	}
105: 	// erase the column from this DataTable
106: 	assert(removed_column < types.size());
107: 	types.erase(types.begin() + removed_column);
108: 	columns.erase(columns.begin() + removed_column);
109: 
110: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
111: 	parent.is_root = false;
112: }
113: 
114: DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t changed_idx, SQLType target_type,
115:                      vector<column_t> bound_columns, Expression &cast_expr)
116:     : info(parent.info), types(parent.types), storage(parent.storage), persistent_manager(parent.persistent_manager),
117:       transient_manager(parent.transient_manager), columns(parent.columns), is_root(true) {
118: 
119: 	// prevent any new tuples from being added to the parent
120: 	CreateIndexScanState scan_state;
121: 	parent.InitializeCreateIndexScan(scan_state, bound_columns);
122: 
123: 	// first check if there are any indexes that exist that point to the changed column
124: 	for (auto &index : info->indexes) {
125: 		for (auto &column_id : index->column_ids) {
126: 			if (column_id == changed_idx) {
127: 				throw CatalogException("Cannot change the type of this column: an index depends on it!");
128: 			}
129: 		}
130: 	}
131: 	// change the type in this DataTable
132: 	auto new_type = GetInternalType(target_type);
133: 	types[changed_idx] = new_type;
134: 
135: 	// construct a new column data for this type
136: 	auto column_data = make_shared<ColumnData>(*storage.buffer_manager, *info);
137: 	column_data->type = new_type;
138: 	column_data->column_idx = changed_idx;
139: 
140: 	ColumnAppendState append_state;
141: 	column_data->InitializeAppend(append_state);
142: 
143: 	// scan the original table, and fill the new column with the transformed value
144: 	auto &transaction = Transaction::GetTransaction(context);
145: 
146: 	vector<TypeId> types;
147: 	for (idx_t i = 0; i < bound_columns.size(); i++) {
148: 		if (bound_columns[i] == COLUMN_IDENTIFIER_ROW_ID) {
149: 			types.push_back(ROW_TYPE);
150: 		} else {
151: 			types.push_back(parent.types[bound_columns[i]]);
152: 		}
153: 	}
154: 
155: 	DataChunk scan_chunk;
156: 	scan_chunk.Initialize(types);
157: 
158: 	ExpressionExecutor executor;
159: 	executor.AddExpression(cast_expr);
160: 
161: 	Vector append_vector(new_type);
162: 	while (true) {
163: 		// scan the table
164: 		scan_chunk.Reset();
165: 		parent.CreateIndexScan(scan_state, scan_chunk);
166: 		if (scan_chunk.size() == 0) {
167: 			break;
168: 		}
169: 		// execute the expression
170: 		executor.ExecuteExpression(scan_chunk, append_vector);
171: 		column_data->Append(append_state, append_vector, scan_chunk.size());
172: 	}
173: 	// also add this column to client local storage
174: 	transaction.storage.ChangeType(&parent, this, changed_idx, target_type, bound_columns, cast_expr);
175: 
176: 	columns[changed_idx] = move(column_data);
177: 
178: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
179: 	parent.is_root = false;
180: }
181: 
182: //===--------------------------------------------------------------------===//
183: // Scan
184: //===--------------------------------------------------------------------===//
185: void DataTable::InitializeScan(TableScanState &state, vector<column_t> column_ids,
186:                                unordered_map<idx_t, vector<TableFilter>> *table_filters) {
187: 	// initialize a column scan state for each column
188: 	state.column_scans = unique_ptr<ColumnScanState[]>(new ColumnScanState[column_ids.size()]);
189: 	for (idx_t i = 0; i < column_ids.size(); i++) {
190: 		auto column = column_ids[i];
191: 		if (column != COLUMN_IDENTIFIER_ROW_ID) {
192: 			columns[column]->InitializeScan(state.column_scans[i]);
193: 		}
194: 	}
195: 	state.column_ids = move(column_ids);
196: 	// initialize the chunk scan state
197: 	state.offset = 0;
198: 	state.current_persistent_row = 0;
199: 	state.max_persistent_row = persistent_manager->max_row;
200: 	state.current_transient_row = 0;
201: 	state.max_transient_row = transient_manager->max_row;
202: 	if (table_filters && table_filters->size() > 0) {
203: 		state.adaptive_filter = make_unique<AdaptiveFilter>(*table_filters);
204: 	}
205: }
206: 
207: void DataTable::InitializeScan(Transaction &transaction, TableScanState &state, vector<column_t> column_ids,
208:                                unordered_map<idx_t, vector<TableFilter>> *table_filters) {
209: 	InitializeScan(state, move(column_ids), table_filters);
210: 	transaction.storage.InitializeScan(this, state.local_state);
211: }
212: 
213: void DataTable::Scan(Transaction &transaction, DataChunk &result, TableScanState &state,
214:                      unordered_map<idx_t, vector<TableFilter>> &table_filters) {
215: 	// scan the persistent segments
216: 	while (ScanBaseTable(transaction, result, state, state.current_persistent_row, state.max_persistent_row, 0,
217: 	                     *persistent_manager, table_filters)) {
218: 		if (result.size() > 0) {
219: 			return;
220: 		}
221: 		result.Reset();
222: 	}
223: 	// scan the transient segments
224: 	while (ScanBaseTable(transaction, result, state, state.current_transient_row, state.max_transient_row,
225: 	                     persistent_manager->max_row, *transient_manager, table_filters)) {
226: 		if (result.size() > 0) {
227: 			return;
228: 		}
229: 		result.Reset();
230: 	}
231: 
232: 	// scan the transaction-local segments
233: 	transaction.storage.Scan(state.local_state, state.column_ids, result, &table_filters);
234: }
235: 
236: template <class T> bool checkZonemap(TableScanState &state, TableFilter &table_filter, T constant) {
237: 	T *min = (T *)state.column_scans[table_filter.column_index].current->stats.minimum.get();
238: 	T *max = (T *)state.column_scans[table_filter.column_index].current->stats.maximum.get();
239: 	switch (table_filter.comparison_type) {
240: 	case ExpressionType::COMPARE_EQUAL:
241: 		return constant >= *min && constant <= *max;
242: 	case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
243: 		return constant <= *max;
244: 	case ExpressionType::COMPARE_GREATERTHAN:
245: 		return constant < *max;
246: 	case ExpressionType::COMPARE_LESSTHANOREQUALTO:
247: 		return constant >= *min;
248: 	case ExpressionType::COMPARE_LESSTHAN:
249: 		return constant > *min;
250: 	default:
251: 		throw NotImplementedException("Operation not implemented");
252: 	}
253: }
254: 
255: bool checkZonemapString(TableScanState &state, TableFilter &table_filter, const char *constant) {
256: 	char *min = (char *)state.column_scans[table_filter.column_index].current->stats.minimum.get();
257: 	char *max = (char *)state.column_scans[table_filter.column_index].current->stats.maximum.get();
258: 	int min_comp = strcmp(min, constant);
259: 	int max_comp = strcmp(max, constant);
260: 	switch (table_filter.comparison_type) {
261: 	case ExpressionType::COMPARE_EQUAL:
262: 		return min_comp <= 0 && max_comp >= 0;
263: 	case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
264: 	case ExpressionType::COMPARE_GREATERTHAN:
265: 		return max_comp >= 0;
266: 	case ExpressionType::COMPARE_LESSTHAN:
267: 	case ExpressionType::COMPARE_LESSTHANOREQUALTO:
268: 		return min_comp <= 0;
269: 	default:
270: 		throw NotImplementedException("Operation not implemented");
271: 	}
272: }
273: 
274: bool DataTable::CheckZonemap(TableScanState &state, unordered_map<idx_t, vector<TableFilter>> &table_filters,
275:                              idx_t &current_row) {
276: 	bool readSegment = true;
277: 	for (auto &table_filter : table_filters) {
278: 		for (auto &predicate_constant : table_filter.second) {
279: 			if (!state.column_scans[predicate_constant.column_index].segment_checked) {
280: 				state.column_scans[predicate_constant.column_index].segment_checked = true;
281: 				if (!state.column_scans[predicate_constant.column_index].current) {
282: 					return true;
283: 				}
284: 				switch (state.column_scans[predicate_constant.column_index].current->type) {
285: 				case TypeId::INT8: {
286: 					int8_t constant = predicate_constant.constant.value_.tinyint;
287: 					readSegment &= checkZonemap<int8_t>(state, predicate_constant, constant);
288: 					break;
289: 				}
290: 				case TypeId::INT16: {
291: 					int16_t constant = predicate_constant.constant.value_.smallint;
292: 					readSegment &= checkZonemap<int16_t>(state, predicate_constant, constant);
293: 					break;
294: 				}
295: 				case TypeId::INT32: {
296: 					int32_t constant = predicate_constant.constant.value_.integer;
297: 					readSegment &= checkZonemap<int32_t>(state, predicate_constant, constant);
298: 					break;
299: 				}
300: 				case TypeId::INT64: {
301: 					int64_t constant = predicate_constant.constant.value_.bigint;
302: 					readSegment &= checkZonemap<int64_t>(state, predicate_constant, constant);
303: 					break;
304: 				}
305: 				case TypeId::FLOAT: {
306: 					float constant = predicate_constant.constant.value_.float_;
307: 					readSegment &= checkZonemap<float>(state, predicate_constant, constant);
308: 					break;
309: 				}
310: 				case TypeId::DOUBLE: {
311: 					double constant = predicate_constant.constant.value_.double_;
312: 					readSegment &= checkZonemap<double>(state, predicate_constant, constant);
313: 					break;
314: 				}
315: 				case TypeId::VARCHAR: {
316: 					//! we can only compare the first 7 bytes
317: 					size_t value_size = predicate_constant.constant.str_value.size() > 7
318: 					                        ? 7
319: 					                        : predicate_constant.constant.str_value.size();
320: 					string constant;
321: 					for (size_t i = 0; i < value_size; i++) {
322: 						constant += predicate_constant.constant.str_value[i];
323: 					}
324: 					readSegment &= checkZonemapString(state, predicate_constant, constant.c_str());
325: 					break;
326: 				}
327: 				default:
328: 					throw NotImplementedException("Unimplemented type for uncompressed segment");
329: 				}
330: 			}
331: 			if (!readSegment) {
332: 				//! We can skip this partition
333: 				idx_t vectorsToSkip =
334: 				    ceil((double)(state.column_scans[predicate_constant.column_index].current->count +
335: 				                  state.column_scans[predicate_constant.column_index].current->start - current_row) /
336: 				         STANDARD_VECTOR_SIZE);
337: 				for (idx_t i = 0; i < vectorsToSkip; ++i) {
338: 					state.NextVector();
339: 					current_row += STANDARD_VECTOR_SIZE;
340: 				}
341: 				return false;
342: 			}
343: 		}
344: 	}
345: 
346: 	return true;
347: }
348: 
349: bool DataTable::ScanBaseTable(Transaction &transaction, DataChunk &result, TableScanState &state, idx_t &current_row,
350:                               idx_t max_row, idx_t base_row, VersionManager &manager,
351:                               unordered_map<idx_t, vector<TableFilter>> &table_filters) {
352: 	if (current_row >= max_row) {
353: 		// exceeded the amount of rows to scan
354: 		return false;
355: 	}
356: 	idx_t max_count = std::min((idx_t)STANDARD_VECTOR_SIZE, max_row - current_row);
357: 	idx_t vector_offset = current_row / STANDARD_VECTOR_SIZE;
358: 	//! first check the zonemap if we have to scan this partition
359: 	if (!CheckZonemap(state, table_filters, current_row)) {
360: 		return true;
361: 	}
362: 	// second, scan the version chunk manager to figure out which tuples to load for this transaction
363: 	SelectionVector valid_sel(STANDARD_VECTOR_SIZE);
364: 	idx_t count = manager.GetSelVector(transaction, vector_offset, valid_sel, max_count);
365: 	if (count == 0) {
366: 		// nothing to scan for this vector, skip the entire vector
367: 		state.NextVector();
368: 		current_row += STANDARD_VECTOR_SIZE;
369: 		return true;
370: 	}
371: 	idx_t approved_tuple_count = count;
372: 	if (count == max_count && table_filters.empty()) {
373: 		//! If we don't have any deleted tuples or filters we can just run a regular scan
374: 		for (idx_t i = 0; i < state.column_ids.size(); i++) {
375: 			auto column = state.column_ids[i];
376: 			if (column == COLUMN_IDENTIFIER_ROW_ID) {
377: 				// scan row id
378: 				assert(result.data[i].type == ROW_TYPE);
379: 				result.data[i].Sequence(base_row + current_row, 1);
380: 			} else {
381: 				columns[column]->Scan(transaction, state.column_scans[i], result.data[i]);
382: 			}
383: 		}
384: 	} else {
385: 		SelectionVector sel;
386: 
387: 		if (count != max_count) {
388: 			sel.Initialize(valid_sel);
389: 		} else {
390: 			sel.Initialize(FlatVector::IncrementalSelectionVector);
391: 		}
392: 		//! First, we scan the columns with filters, fetch their data and generate a selection vector.
393: 		//! get runtime statistics
394: 		auto start_time = high_resolution_clock::now();
395: 		for (idx_t i = 0; i < table_filters.size(); i++) {
396: 			auto tf_idx = state.adaptive_filter->permutation[i];
397: 			columns[tf_idx]->Select(transaction, state.column_scans[tf_idx], result.data[tf_idx], sel,
398: 			                        approved_tuple_count, table_filters[tf_idx]);
399: 		}
400: 		for (auto &table_filter : table_filters) {
401: 			result.data[table_filter.first].Slice(sel, approved_tuple_count);
402: 		}
403: 		//! Now we use the selection vector to fetch data for the other columns.
404: 		for (idx_t i = 0; i < state.column_ids.size(); i++) {
405: 			if (table_filters.find(i) == table_filters.end()) {
406: 				auto column = state.column_ids[i];
407: 				if (column == COLUMN_IDENTIFIER_ROW_ID) {
408: 					assert(result.data[i].type == TypeId::INT64);
409: 					result.data[i].vector_type = VectorType::FLAT_VECTOR;
410: 					auto result_data = (int64_t *)FlatVector::GetData(result.data[i]);
411: 					for (size_t sel_idx = 0; sel_idx < approved_tuple_count; sel_idx++) {
412: 						result_data[sel_idx] = base_row + current_row + sel.get_index(sel_idx);
413: 					}
414: 				} else {
415: 					columns[column]->FilterScan(transaction, state.column_scans[i], result.data[i], sel,
416: 					                            approved_tuple_count);
417: 				}
418: 			}
419: 		}
420: 		auto end_time = high_resolution_clock::now();
421: 		if (state.adaptive_filter && table_filters.size() > 1) {
422: 			state.adaptive_filter->AdaptRuntimeStatistics(
423: 			    duration_cast<duration<double>>(end_time - start_time).count());
424: 		}
425: 	}
426: 
427: 	result.SetCardinality(approved_tuple_count);
428: 	current_row += STANDARD_VECTOR_SIZE;
429: 	return true;
430: }
431: 
432: //===--------------------------------------------------------------------===//
433: // Index Scan
434: //===--------------------------------------------------------------------===//
435: void DataTable::InitializeIndexScan(Transaction &transaction, TableIndexScanState &state, Index &index,
436:                                     vector<column_t> column_ids) {
437: 	state.index = &index;
438: 	state.column_ids = move(column_ids);
439: 	transaction.storage.InitializeScan(this, state.local_state);
440: }
441: 
442: void DataTable::InitializeIndexScan(Transaction &transaction, TableIndexScanState &state, Index &index, Value value,
443:                                     ExpressionType expr_type, vector<column_t> column_ids) {
444: 	InitializeIndexScan(transaction, state, index, move(column_ids));
445: 	state.index_state = index.InitializeScanSinglePredicate(transaction, state.column_ids, value, expr_type);
446: }
447: 
448: void DataTable::InitializeIndexScan(Transaction &transaction, TableIndexScanState &state, Index &index, Value low_value,
449:                                     ExpressionType low_type, Value high_value, ExpressionType high_type,
450:                                     vector<column_t> column_ids) {
451: 	InitializeIndexScan(transaction, state, index, move(column_ids));
452: 	state.index_state =
453: 	    index.InitializeScanTwoPredicates(transaction, state.column_ids, low_value, low_type, high_value, high_type);
454: }
455: 
456: void DataTable::IndexScan(Transaction &transaction, DataChunk &result, TableIndexScanState &state) {
457: 	// clear any previously pinned blocks
458: 	state.fetch_state.handles.clear();
459: 	// scan the index
460: 	state.index->Scan(transaction, *this, state, result);
461: 	if (result.size() > 0) {
462: 		return;
463: 	}
464: 	// scan the local structure
465: 	transaction.storage.Scan(state.local_state, state.column_ids, result);
466: }
467: 
468: //===--------------------------------------------------------------------===//
469: // Fetch
470: //===--------------------------------------------------------------------===//
471: void DataTable::Fetch(Transaction &transaction, DataChunk &result, vector<column_t> &column_ids,
472:                       Vector &row_identifiers, idx_t fetch_count, TableIndexScanState &state) {
473: 	// first figure out which row identifiers we should use for this transaction by looking at the VersionManagers
474: 	row_t rows[STANDARD_VECTOR_SIZE];
475: 	idx_t count = FetchRows(transaction, row_identifiers, fetch_count, rows);
476: 
477: 	if (count == 0) {
478: 		// no rows to use
479: 		return;
480: 	}
481: 	// for each of the remaining rows, now fetch the data
482: 	result.SetCardinality(count);
483: 	for (idx_t col_idx = 0; col_idx < column_ids.size(); col_idx++) {
484: 		auto column = column_ids[col_idx];
485: 		if (column == COLUMN_IDENTIFIER_ROW_ID) {
486: 			// row id column: fill in the row ids
487: 			assert(result.data[col_idx].type == TypeId::INT64);
488: 			result.data[col_idx].vector_type = VectorType::FLAT_VECTOR;
489: 			auto data = FlatVector::GetData<row_t>(result.data[col_idx]);
490: 			for (idx_t i = 0; i < count; i++) {
491: 				data[i] = rows[i];
492: 			}
493: 		} else {
494: 			// regular column: fetch data from the base column
495: 			for (idx_t i = 0; i < count; i++) {
496: 				auto row_id = rows[i];
497: 				columns[column]->FetchRow(state.fetch_state, transaction, row_id, result.data[col_idx], i);
498: 			}
499: 		}
500: 	}
501: }
502: 
503: idx_t DataTable::FetchRows(Transaction &transaction, Vector &row_identifiers, idx_t fetch_count, row_t result_rows[]) {
504: 	assert(row_identifiers.type == ROW_TYPE);
505: 
506: 	// obtain a read lock on the version managers
507: 	auto l1 = persistent_manager->lock.GetSharedLock();
508: 	auto l2 = transient_manager->lock.GetSharedLock();
509: 
510: 	// now iterate over the row ids and figure out which rows to use
511: 	idx_t count = 0;
512: 
513: 	auto row_ids = FlatVector::GetData<row_t>(row_identifiers);
514: 	for (idx_t i = 0; i < fetch_count; i++) {
515: 		auto row_id = row_ids[i];
516: 		bool use_row;
517: 		if ((idx_t)row_id < persistent_manager->max_row) {
518: 			// persistent row: use persistent manager
519: 			use_row = persistent_manager->Fetch(transaction, row_id);
520: 		} else {
521: 			// transient row: use transient manager
522: 			use_row = transient_manager->Fetch(transaction, row_id);
523: 		}
524: 		if (use_row) {
525: 			// row is not deleted; use the row
526: 			result_rows[count++] = row_id;
527: 		}
528: 	}
529: 	return count;
530: }
531: 
532: //===--------------------------------------------------------------------===//
533: // Append
534: //===--------------------------------------------------------------------===//
535: static void VerifyNotNullConstraint(TableCatalogEntry &table, Vector &vector, idx_t count, string &col_name) {
536: 	if (VectorOperations::HasNull(vector, count)) {
537: 		throw ConstraintException("NOT NULL constraint failed: %s.%s", table.name.c_str(), col_name.c_str());
538: 	}
539: }
540: 
541: static void VerifyCheckConstraint(TableCatalogEntry &table, Expression &expr, DataChunk &chunk) {
542: 	ExpressionExecutor executor(expr);
543: 	Vector result(TypeId::INT32);
544: 	try {
545: 		executor.ExecuteExpression(chunk, result);
546: 	} catch (Exception &ex) {
547: 		throw ConstraintException("CHECK constraint failed: %s (Error: %s)", table.name.c_str(), ex.what());
548: 	} catch (...) {
549: 		throw ConstraintException("CHECK constraint failed: %s (Unknown Error)", table.name.c_str());
550: 	}
551: 	VectorData vdata;
552: 	result.Orrify(chunk.size(), vdata);
553: 
554: 	auto dataptr = (int32_t *)vdata.data;
555: 	for (idx_t i = 0; i < chunk.size(); i++) {
556: 		auto idx = vdata.sel->get_index(i);
557: 		if (!(*vdata.nullmask)[idx] && dataptr[idx] == 0) {
558: 			throw ConstraintException("CHECK constraint failed: %s", table.name.c_str());
559: 		}
560: 	}
561: }
562: 
563: void DataTable::VerifyAppendConstraints(TableCatalogEntry &table, DataChunk &chunk) {
564: 	for (auto &constraint : table.bound_constraints) {
565: 		switch (constraint->type) {
566: 		case ConstraintType::NOT_NULL: {
567: 			auto &not_null = *reinterpret_cast<BoundNotNullConstraint *>(constraint.get());
568: 			VerifyNotNullConstraint(table, chunk.data[not_null.index], chunk.size(),
569: 			                        table.columns[not_null.index].name);
570: 			break;
571: 		}
572: 		case ConstraintType::CHECK: {
573: 			auto &check = *reinterpret_cast<BoundCheckConstraint *>(constraint.get());
574: 			VerifyCheckConstraint(table, *check.expression, chunk);
575: 			break;
576: 		}
577: 		case ConstraintType::UNIQUE: {
578: 			//! check whether or not the chunk can be inserted into the indexes
579: 			for (auto &index : info->indexes) {
580: 				index->VerifyAppend(chunk);
581: 			}
582: 			break;
583: 		}
584: 		case ConstraintType::FOREIGN_KEY:
585: 		default:
586: 			throw NotImplementedException("Constraint type not implemented!");
587: 		}
588: 	}
589: }
590: 
591: void DataTable::Append(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk) {
592: 	if (chunk.size() == 0) {
593: 		return;
594: 	}
595: 	if (chunk.column_count() != table.columns.size()) {
596: 		throw CatalogException("Mismatch in column count for append");
597: 	}
598: 	if (!is_root) {
599: 		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
600: 	}
601: 
602: 	chunk.Verify();
603: 
604: 	// verify any constraints on the new chunk
605: 	VerifyAppendConstraints(table, chunk);
606: 
607: 	// append to the transaction local data
608: 	auto &transaction = Transaction::GetTransaction(context);
609: 	transaction.storage.Append(this, chunk);
610: }
611: 
612: void DataTable::InitializeAppend(TableAppendState &state) {
613: 	// obtain the append lock for this table
614: 	state.append_lock = unique_lock<mutex>(append_lock);
615: 	if (!is_root) {
616: 		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
617: 	}
618: 	// obtain locks on all indexes for the table
619: 	state.index_locks = unique_ptr<IndexLock[]>(new IndexLock[info->indexes.size()]);
620: 	for (idx_t i = 0; i < info->indexes.size(); i++) {
621: 		info->indexes[i]->InitializeLock(state.index_locks[i]);
622: 	}
623: 	// for each column, initialize the append state
624: 	state.states = unique_ptr<ColumnAppendState[]>(new ColumnAppendState[types.size()]);
625: 	for (idx_t i = 0; i < types.size(); i++) {
626: 		columns[i]->InitializeAppend(state.states[i]);
627: 	}
628: 	state.row_start = transient_manager->max_row;
629: 	state.current_row = state.row_start;
630: }
631: 
632: void DataTable::Append(Transaction &transaction, transaction_t commit_id, DataChunk &chunk, TableAppendState &state) {
633: 	assert(is_root);
634: 	assert(chunk.column_count() == types.size());
635: 	chunk.Verify();
636: 
637: 	// set up the inserted info in the version manager
638: 	transient_manager->Append(transaction, state.current_row, chunk.size(), commit_id);
639: 
640: 	// append the physical data to each of the entries
641: 	for (idx_t i = 0; i < types.size(); i++) {
642: 		columns[i]->Append(state.states[i], chunk.data[i], chunk.size());
643: 	}
644: 	info->cardinality += chunk.size();
645: 	state.current_row += chunk.size();
646: }
647: 
648: void DataTable::RevertAppend(TableAppendState &state) {
649: 	if (state.row_start == state.current_row) {
650: 		// nothing to revert!
651: 		return;
652: 	}
653: 	assert(is_root);
654: 	// revert changes in the base columns
655: 	for (idx_t i = 0; i < types.size(); i++) {
656: 		columns[i]->RevertAppend(state.row_start);
657: 	}
658: 	// adjust the cardinality
659: 	info->cardinality -= state.current_row - state.row_start;
660: 	transient_manager->max_row = state.row_start;
661: 	// revert changes in the transient manager
662: 	transient_manager->RevertAppend(state.row_start, state.current_row);
663: }
664: 
665: //===--------------------------------------------------------------------===//
666: // Indexes
667: //===--------------------------------------------------------------------===//
668: bool DataTable::AppendToIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {
669: 	assert(is_root);
670: 	if (info->indexes.size() == 0) {
671: 		return true;
672: 	}
673: 	// first generate the vector of row identifiers
674: 	Vector row_identifiers(ROW_TYPE);
675: 	VectorOperations::GenerateSequence(row_identifiers, chunk.size(), row_start, 1);
676: 
677: 	idx_t failed_index = INVALID_INDEX;
678: 	// now append the entries to the indices
679: 	for (idx_t i = 0; i < info->indexes.size(); i++) {
680: 		if (!info->indexes[i]->Append(state.index_locks[i], chunk, row_identifiers)) {
681: 			failed_index = i;
682: 			break;
683: 		}
684: 	}
685: 	if (failed_index != INVALID_INDEX) {
686: 		// constraint violation!
687: 		// remove any appended entries from previous indexes (if any)
688: 		for (idx_t i = 0; i < failed_index; i++) {
689: 			info->indexes[i]->Delete(state.index_locks[i], chunk, row_identifiers);
690: 		}
691: 		return false;
692: 	}
693: 	return true;
694: }
695: 
696: void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {
697: 	assert(is_root);
698: 	if (info->indexes.size() == 0) {
699: 		return;
700: 	}
701: 	// first generate the vector of row identifiers
702: 	Vector row_identifiers(ROW_TYPE);
703: 	VectorOperations::GenerateSequence(row_identifiers, chunk.size(), row_start, 1);
704: 
705: 	// now remove the entries from the indices
706: 	RemoveFromIndexes(state, chunk, row_identifiers);
707: }
708: 
709: void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, Vector &row_identifiers) {
710: 	assert(is_root);
711: 	for (idx_t i = 0; i < info->indexes.size(); i++) {
712: 		info->indexes[i]->Delete(state.index_locks[i], chunk, row_identifiers);
713: 	}
714: }
715: 
716: void DataTable::RemoveFromIndexes(Vector &row_identifiers, idx_t count) {
717: 	assert(is_root);
718: 	auto row_ids = FlatVector::GetData<row_t>(row_identifiers);
719: 	// create a selection vector from the row_ids
720: 	SelectionVector sel(STANDARD_VECTOR_SIZE);
721: 	for (idx_t i = 0; i < count; i++) {
722: 		sel.set_index(i, row_ids[i] % STANDARD_VECTOR_SIZE);
723: 	}
724: 
725: 	// fetch the data for these row identifiers
726: 	DataChunk result;
727: 	result.Initialize(types);
728: 	// FIXME: we do not need to fetch all columns, only the columns required by the indices!
729: 	auto states = unique_ptr<ColumnScanState[]>(new ColumnScanState[types.size()]);
730: 	for (idx_t i = 0; i < types.size(); i++) {
731: 		columns[i]->Fetch(states[i], row_ids[0], result.data[i]);
732: 	}
733: 	result.Slice(sel, count);
734: 	for (idx_t i = 0; i < info->indexes.size(); i++) {
735: 		info->indexes[i]->Delete(result, row_identifiers);
736: 	}
737: }
738: 
739: //===--------------------------------------------------------------------===//
740: // Delete
741: //===--------------------------------------------------------------------===//
742: void DataTable::Delete(TableCatalogEntry &table, ClientContext &context, Vector &row_identifiers, idx_t count) {
743: 	assert(row_identifiers.type == ROW_TYPE);
744: 	if (count == 0) {
745: 		return;
746: 	}
747: 
748: 	auto &transaction = Transaction::GetTransaction(context);
749: 
750: 	row_identifiers.Normalify(count);
751: 	auto ids = FlatVector::GetData<row_t>(row_identifiers);
752: 	auto first_id = ids[0];
753: 
754: 	if (first_id >= MAX_ROW_ID) {
755: 		// deletion is in transaction-local storage: push delete into local chunk collection
756: 		transaction.storage.Delete(this, row_identifiers, count);
757: 	} else if ((idx_t)first_id < persistent_manager->max_row) {
758: 		// deletion is in persistent storage: delete in the persistent version manager
759: 		persistent_manager->Delete(transaction, this, row_identifiers, count);
760: 	} else {
761: 		// deletion is in transient storage: delete in the persistent version manager
762: 		transient_manager->Delete(transaction, this, row_identifiers, count);
763: 	}
764: }
765: 
766: //===--------------------------------------------------------------------===//
767: // Update
768: //===--------------------------------------------------------------------===//
769: static void CreateMockChunk(vector<TypeId> &types, vector<column_t> &column_ids, DataChunk &chunk,
770:                             DataChunk &mock_chunk) {
771: 	// construct a mock DataChunk
772: 	mock_chunk.InitializeEmpty(types);
773: 	for (column_t i = 0; i < column_ids.size(); i++) {
774: 		mock_chunk.data[column_ids[i]].Reference(chunk.data[i]);
775: 	}
776: 	mock_chunk.SetCardinality(chunk.size());
777: }
778: 
779: static bool CreateMockChunk(TableCatalogEntry &table, vector<column_t> &column_ids,
780:                             unordered_set<column_t> &desired_column_ids, DataChunk &chunk, DataChunk &mock_chunk) {
781: 	idx_t found_columns = 0;
782: 	// check whether the desired columns are present in the UPDATE clause
783: 	for (column_t i = 0; i < column_ids.size(); i++) {
784: 		if (desired_column_ids.find(column_ids[i]) != desired_column_ids.end()) {
785: 			found_columns++;
786: 		}
787: 	}
788: 	if (found_columns == 0) {
789: 		// no columns were found: no need to check the constraint again
790: 		return false;
791: 	}
792: 	if (found_columns != desired_column_ids.size()) {
793: 		// FIXME: not all columns in UPDATE clause are present!
794: 		// this should not be triggered at all as the binder should add these columns
795: 		throw NotImplementedException(
796: 		    "Not all columns required for the CHECK constraint are present in the UPDATED chunk!");
797: 	}
798: 	// construct a mock DataChunk
799: 	auto types = table.GetTypes();
800: 	CreateMockChunk(types, column_ids, chunk, mock_chunk);
801: 	return true;
802: }
803: 
804: void DataTable::VerifyUpdateConstraints(TableCatalogEntry &table, DataChunk &chunk, vector<column_t> &column_ids) {
805: 	for (auto &constraint : table.bound_constraints) {
806: 		switch (constraint->type) {
807: 		case ConstraintType::NOT_NULL: {
808: 			auto &not_null = *reinterpret_cast<BoundNotNullConstraint *>(constraint.get());
809: 			// check if the constraint is in the list of column_ids
810: 			for (idx_t i = 0; i < column_ids.size(); i++) {
811: 				if (column_ids[i] == not_null.index) {
812: 					// found the column id: check the data in
813: 					VerifyNotNullConstraint(table, chunk.data[i], chunk.size(), table.columns[not_null.index].name);
814: 					break;
815: 				}
816: 			}
817: 			break;
818: 		}
819: 		case ConstraintType::CHECK: {
820: 			auto &check = *reinterpret_cast<BoundCheckConstraint *>(constraint.get());
821: 
822: 			DataChunk mock_chunk;
823: 			if (CreateMockChunk(table, column_ids, check.bound_columns, chunk, mock_chunk)) {
824: 				VerifyCheckConstraint(table, *check.expression, mock_chunk);
825: 			}
826: 			break;
827: 		}
828: 		case ConstraintType::UNIQUE:
829: 		case ConstraintType::FOREIGN_KEY:
830: 			break;
831: 		default:
832: 			throw NotImplementedException("Constraint type not implemented!");
833: 		}
834: 	}
835: 	// update should not be called for indexed columns!
836: 	// instead update should have been rewritten to delete + update on higher layer
837: #ifdef DEBUG
838: 	for (idx_t i = 0; i < info->indexes.size(); i++) {
839: 		assert(!info->indexes[i]->IndexIsUpdated(column_ids));
840: 	}
841: #endif
842: }
843: 
844: void DataTable::Update(TableCatalogEntry &table, ClientContext &context, Vector &row_ids, vector<column_t> &column_ids,
845:                        DataChunk &updates) {
846: 	assert(row_ids.type == ROW_TYPE);
847: 
848: 	updates.Verify();
849: 	if (updates.size() == 0) {
850: 		return;
851: 	}
852: 
853: 	// first verify that no constraints are violated
854: 	VerifyUpdateConstraints(table, updates, column_ids);
855: 
856: 	// now perform the actual update
857: 	auto &transaction = Transaction::GetTransaction(context);
858: 
859: 	updates.Normalify();
860: 	row_ids.Normalify(updates.size());
861: 	auto first_id = FlatVector::GetValue<row_t>(row_ids, 0);
862: 	if (first_id >= MAX_ROW_ID) {
863: 		// update is in transaction-local storage: push update into local storage
864: 		transaction.storage.Update(this, row_ids, column_ids, updates);
865: 		return;
866: 	}
867: 
868: 	for (idx_t i = 0; i < column_ids.size(); i++) {
869: 		auto column = column_ids[i];
870: 		assert(column != COLUMN_IDENTIFIER_ROW_ID);
871: 
872: 		columns[column]->Update(transaction, updates.data[i], row_ids, updates.size());
873: 	}
874: }
875: 
876: //===--------------------------------------------------------------------===//
877: // Create Index Scan
878: //===--------------------------------------------------------------------===//
879: void DataTable::InitializeCreateIndexScan(CreateIndexScanState &state, vector<column_t> column_ids) {
880: 	// we grab the append lock to make sure nothing is appended until AFTER we finish the index scan
881: 	state.append_lock = unique_lock<mutex>(append_lock);
882: 	// get a read lock on the VersionManagers to prevent any further deletions
883: 	state.locks.push_back(persistent_manager->lock.GetSharedLock());
884: 	state.locks.push_back(transient_manager->lock.GetSharedLock());
885: 
886: 	InitializeScan(state, column_ids);
887: }
888: 
889: void DataTable::CreateIndexScan(CreateIndexScanState &state, DataChunk &result) {
890: 	// scan the persistent segments
891: 	if (ScanCreateIndex(state, result, state.current_persistent_row, state.max_persistent_row, 0)) {
892: 		return;
893: 	}
894: 	// scan the transient segments
895: 	if (ScanCreateIndex(state, result, state.current_transient_row, state.max_transient_row,
896: 	                    state.max_persistent_row)) {
897: 		return;
898: 	}
899: }
900: 
901: bool DataTable::ScanCreateIndex(CreateIndexScanState &state, DataChunk &result, idx_t &current_row, idx_t max_row,
902:                                 idx_t base_row) {
903: 	if (current_row >= max_row) {
904: 		return false;
905: 	}
906: 	idx_t count = std::min((idx_t)STANDARD_VECTOR_SIZE, max_row - current_row);
907: 
908: 	// scan the base columns to fetch the actual data
909: 	// note that we insert all data into the index, even if it is marked as deleted
910: 	// FIXME: tuples that are already "cleaned up" do not need to be inserted into the index!
911: 	for (idx_t i = 0; i < state.column_ids.size(); i++) {
912: 		auto column = state.column_ids[i];
913: 		if (column == COLUMN_IDENTIFIER_ROW_ID) {
914: 			// scan row id
915: 			assert(result.data[i].type == ROW_TYPE);
916: 			result.data[i].Sequence(base_row + current_row, 1);
917: 		} else {
918: 			// scan actual base column
919: 			columns[column]->IndexScan(state.column_scans[i], result.data[i]);
920: 		}
921: 	}
922: 	result.SetCardinality(count);
923: 
924: 	current_row += STANDARD_VECTOR_SIZE;
925: 	return count > 0;
926: }
927: 
928: void DataTable::AddIndex(unique_ptr<Index> index, vector<unique_ptr<Expression>> &expressions) {
929: 	DataChunk result;
930: 	result.Initialize(index->types);
931: 
932: 	DataChunk intermediate;
933: 	vector<TypeId> intermediate_types;
934: 	auto column_ids = index->column_ids;
935: 	column_ids.push_back(COLUMN_IDENTIFIER_ROW_ID);
936: 	for (auto &id : index->column_ids) {
937: 		intermediate_types.push_back(types[id]);
938: 	}
939: 	intermediate_types.push_back(ROW_TYPE);
940: 	intermediate.Initialize(intermediate_types);
941: 
942: 	// initialize an index scan
943: 	CreateIndexScanState state;
944: 	InitializeCreateIndexScan(state, column_ids);
945: 
946: 	if (!is_root) {
947: 		throw TransactionException("Transaction conflict: cannot add an index to a table that has been altered!");
948: 	}
949: 
950: 	// now start incrementally building the index
951: 	IndexLock lock;
952: 	index->InitializeLock(lock);
953: 	ExpressionExecutor executor(expressions);
954: 	while (true) {
955: 		intermediate.Reset();
956: 		// scan a new chunk from the table to index
957: 		CreateIndexScan(state, intermediate);
958: 		if (intermediate.size() == 0) {
959: 			// finished scanning for index creation
960: 			// release all locks
961: 			break;
962: 		}
963: 		// resolve the expressions for this chunk
964: 		executor.Execute(intermediate, result);
965: 
966: 		// insert into the index
967: 		if (!index->Insert(lock, result, intermediate.data[intermediate.column_count() - 1])) {
968: 			throw ConstraintException("Cant create unique index, table contains duplicate data on indexed column(s)");
969: 		}
970: 	}
971: 	info->indexes.push_back(move(index));
972: }
[end of src/storage/data_table.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: