You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
insert into ... on conflict, select fails in transaction
### What happens?

When doing a conflicting INSERT with following SELECT in a transaction block, an internal error is raised.
This happens when a UNIQUE constraint is violated. The transaction aborts with the following error as soon as the SELECT statement has been read.

```
INTERNAL Error: No rows in LocalTableStorage row group for scan
This error signals an assertion failure within DuckDB. This usually occurs due to unexpected conditions or errors in the program's logic.
For more information, see https://duckdb.org/docs/dev/internal_errors
```

### To Reproduce

```
create table tbl (a short primary key, b short, unique(b));
insert into tbl(a, b) values(1, 2);

begin transaction;
insert into tbl(a, b) values(1, 2) on conflict do nothing;
select * from tbl where a = 1;
```

At this point, the above error is raised and the db, if opened from file, must be reopened.

### OS:

Windows 11, x64

### DuckDB Version:

v1.0.0 1f98600c2c

### DuckDB Client:

duckdb

### Full Name:

Philipp Kroos

### Affiliation:

Applitec

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://www.duckdb.org/docs/installation) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of src/storage/local_storage.cpp]
1: #include "duckdb/transaction/local_storage.hpp"
2: #include "duckdb/execution/index/art/art.hpp"
3: #include "duckdb/storage/table/append_state.hpp"
4: #include "duckdb/storage/write_ahead_log.hpp"
5: #include "duckdb/common/vector_operations/vector_operations.hpp"
6: #include "duckdb/storage/table/row_group.hpp"
7: #include "duckdb/transaction/duck_transaction.hpp"
8: #include "duckdb/planner/table_filter.hpp"
9: #include "duckdb/storage/partial_block_manager.hpp"
10: 
11: #include "duckdb/storage/table/column_checkpoint_state.hpp"
12: #include "duckdb/storage/table_io_manager.hpp"
13: #include "duckdb/storage/table/scan_state.hpp"
14: 
15: namespace duckdb {
16: 
17: LocalTableStorage::LocalTableStorage(ClientContext &context, DataTable &table)
18:     : table_ref(table), allocator(Allocator::Get(table.db)), deleted_rows(0), optimistic_writer(table),
19:       merged_storage(false) {
20: 	auto types = table.GetTypes();
21: 	auto data_table_info = table.GetDataTableInfo();
22: 	row_groups = make_shared_ptr<RowGroupCollection>(
23: 	    data_table_info, TableIOManager::Get(table).GetBlockManagerForRowData(), types, MAX_ROW_ID, 0);
24: 	row_groups->InitializeEmpty();
25: 
26: 	data_table_info->GetIndexes().BindAndScan<ART>(context, *data_table_info, [&](ART &art) {
27: 		if (art.GetConstraintType() != IndexConstraintType::NONE) {
28: 			// unique index: create a local ART index that maintains the same unique constraint
29: 			vector<unique_ptr<Expression>> unbound_expressions;
30: 			unbound_expressions.reserve(art.unbound_expressions.size());
31: 			for (auto &expr : art.unbound_expressions) {
32: 				unbound_expressions.push_back(expr->Copy());
33: 			}
34: 			indexes.AddIndex(make_uniq<ART>(art.GetIndexName(), art.GetConstraintType(), art.GetColumnIds(),
35: 			                                art.table_io_manager, std::move(unbound_expressions), art.db));
36: 		}
37: 		return false;
38: 	});
39: }
40: 
41: LocalTableStorage::LocalTableStorage(ClientContext &context, DataTable &new_dt, LocalTableStorage &parent,
42:                                      idx_t changed_idx, const LogicalType &target_type,
43:                                      const vector<column_t> &bound_columns, Expression &cast_expr)
44:     : table_ref(new_dt), allocator(Allocator::Get(new_dt.db)), deleted_rows(parent.deleted_rows),
45:       optimistic_writer(new_dt, parent.optimistic_writer), optimistic_writers(std::move(parent.optimistic_writers)),
46:       merged_storage(parent.merged_storage) {
47: 	row_groups = parent.row_groups->AlterType(context, changed_idx, target_type, bound_columns, cast_expr);
48: 	parent.row_groups.reset();
49: 	indexes.Move(parent.indexes);
50: }
51: 
52: LocalTableStorage::LocalTableStorage(DataTable &new_dt, LocalTableStorage &parent, idx_t drop_idx)
53:     : table_ref(new_dt), allocator(Allocator::Get(new_dt.db)), deleted_rows(parent.deleted_rows),
54:       optimistic_writer(new_dt, parent.optimistic_writer), optimistic_writers(std::move(parent.optimistic_writers)),
55:       merged_storage(parent.merged_storage) {
56: 	row_groups = parent.row_groups->RemoveColumn(drop_idx);
57: 	parent.row_groups.reset();
58: 	indexes.Move(parent.indexes);
59: }
60: 
61: LocalTableStorage::LocalTableStorage(ClientContext &context, DataTable &new_dt, LocalTableStorage &parent,
62:                                      ColumnDefinition &new_column, ExpressionExecutor &default_executor)
63:     : table_ref(new_dt), allocator(Allocator::Get(new_dt.db)), deleted_rows(parent.deleted_rows),
64:       optimistic_writer(new_dt, parent.optimistic_writer), optimistic_writers(std::move(parent.optimistic_writers)),
65:       merged_storage(parent.merged_storage) {
66: 	row_groups = parent.row_groups->AddColumn(context, new_column, default_executor);
67: 	parent.row_groups.reset();
68: 	indexes.Move(parent.indexes);
69: }
70: 
71: LocalTableStorage::~LocalTableStorage() {
72: }
73: 
74: void LocalTableStorage::InitializeScan(CollectionScanState &state, optional_ptr<TableFilterSet> table_filters) {
75: 	if (row_groups->GetTotalRows() == 0) {
76: 		throw InternalException("No rows in LocalTableStorage row group for scan");
77: 	}
78: 	row_groups->InitializeScan(state, state.GetColumnIds(), table_filters.get());
79: }
80: 
81: idx_t LocalTableStorage::EstimatedSize() {
82: 	// count the appended rows
83: 	idx_t appended_rows = row_groups->GetTotalRows() - deleted_rows;
84: 
85: 	// get the (estimated) size of a row (no compressions, etc.)
86: 	idx_t row_size = 0;
87: 	auto &types = row_groups->GetTypes();
88: 	for (auto &type : types) {
89: 		row_size += GetTypeIdSize(type.InternalType());
90: 	}
91: 
92: 	// get the index size
93: 	idx_t index_sizes = 0;
94: 	indexes.Scan([&](Index &index) {
95: 		D_ASSERT(index.IsBound());
96: 		index_sizes += index.Cast<BoundIndex>().GetInMemorySize();
97: 		return false;
98: 	});
99: 
100: 	// return the size of the appended rows and the index size
101: 	return appended_rows * row_size + index_sizes;
102: }
103: 
104: void LocalTableStorage::WriteNewRowGroup() {
105: 	if (deleted_rows != 0) {
106: 		// we have deletes - we cannot merge row groups
107: 		return;
108: 	}
109: 	optimistic_writer.WriteNewRowGroup(*row_groups);
110: }
111: 
112: void LocalTableStorage::FlushBlocks() {
113: 	if (!merged_storage && row_groups->GetTotalRows() > Storage::ROW_GROUP_SIZE) {
114: 		optimistic_writer.WriteLastRowGroup(*row_groups);
115: 	}
116: 	optimistic_writer.FinalFlush();
117: }
118: 
119: ErrorData LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, RowGroupCollection &source,
120:                                              TableIndexList &index_list, const vector<LogicalType> &table_types,
121:                                              row_t &start_row) {
122: 	// only need to scan for index append
123: 	// figure out which columns we need to scan for the set of indexes
124: 	auto columns = index_list.GetRequiredColumns();
125: 	// create an empty mock chunk that contains all the correct types for the table
126: 	DataChunk mock_chunk;
127: 	mock_chunk.InitializeEmpty(table_types);
128: 	ErrorData error;
129: 	source.Scan(transaction, columns, [&](DataChunk &chunk) -> bool {
130: 		// construct the mock chunk by referencing the required columns
131: 		for (idx_t i = 0; i < columns.size(); i++) {
132: 			mock_chunk.data[columns[i]].Reference(chunk.data[i]);
133: 		}
134: 		mock_chunk.SetCardinality(chunk);
135: 		// append this chunk to the indexes of the table
136: 		error = DataTable::AppendToIndexes(index_list, mock_chunk, start_row);
137: 		if (error.HasError()) {
138: 			return false;
139: 		}
140: 		start_row += UnsafeNumericCast<row_t>(chunk.size());
141: 		return true;
142: 	});
143: 	return error;
144: }
145: 
146: void LocalTableStorage::AppendToIndexes(DuckTransaction &transaction, TableAppendState &append_state,
147:                                         idx_t append_count, bool append_to_table) {
148: 	auto &table = table_ref.get();
149: 	if (append_to_table) {
150: 		table.InitializeAppend(transaction, append_state);
151: 	}
152: 	ErrorData error;
153: 	if (append_to_table) {
154: 		// appending: need to scan entire
155: 		row_groups->Scan(transaction, [&](DataChunk &chunk) -> bool {
156: 			// append this chunk to the indexes of the table
157: 			error = table.AppendToIndexes(chunk, append_state.current_row);
158: 			if (error.HasError()) {
159: 				return false;
160: 			}
161: 			// append to base table
162: 			table.Append(chunk, append_state);
163: 			return true;
164: 		});
165: 	} else {
166: 		auto data_table_info = table.GetDataTableInfo();
167: 		auto &index_list = data_table_info->GetIndexes();
168: 		error = AppendToIndexes(transaction, *row_groups, index_list, table.GetTypes(), append_state.current_row);
169: 	}
170: 	if (error.HasError()) {
171: 		// need to revert all appended row ids
172: 		row_t current_row = append_state.row_start;
173: 		// remove the data from the indexes, if there are any indexes
174: 		row_groups->Scan(transaction, [&](DataChunk &chunk) -> bool {
175: 			// append this chunk to the indexes of the table
176: 			try {
177: 				table.RemoveFromIndexes(append_state, chunk, current_row);
178: 			} catch (std::exception &ex) { // LCOV_EXCL_START
179: 				error = ErrorData(ex);
180: 				return false;
181: 			} // LCOV_EXCL_STOP
182: 
183: 			current_row += UnsafeNumericCast<row_t>(chunk.size());
184: 			if (current_row >= append_state.current_row) {
185: 				// finished deleting all rows from the index: abort now
186: 				return false;
187: 			}
188: 			return true;
189: 		});
190: 		if (append_to_table) {
191: 			table.RevertAppendInternal(NumericCast<idx_t>(append_state.row_start));
192: 		}
193: 
194: 		// we need to vacuum the indexes to remove any buffers that are now empty
195: 		// due to reverting the appends
196: 		table.VacuumIndexes();
197: 		error.Throw();
198: 	}
199: 	if (append_to_table) {
200: 		table.FinalizeAppend(transaction, append_state);
201: 	}
202: }
203: 
204: OptimisticDataWriter &LocalTableStorage::CreateOptimisticWriter() {
205: 	auto writer = make_uniq<OptimisticDataWriter>(table_ref.get());
206: 	optimistic_writers.push_back(std::move(writer));
207: 	return *optimistic_writers.back();
208: }
209: 
210: void LocalTableStorage::FinalizeOptimisticWriter(OptimisticDataWriter &writer) {
211: 	// remove the writer from the set of optimistic writers
212: 	unique_ptr<OptimisticDataWriter> owned_writer;
213: 	for (idx_t i = 0; i < optimistic_writers.size(); i++) {
214: 		if (optimistic_writers[i].get() == &writer) {
215: 			owned_writer = std::move(optimistic_writers[i]);
216: 			optimistic_writers.erase_at(i);
217: 			break;
218: 		}
219: 	}
220: 	if (!owned_writer) {
221: 		throw InternalException("Error in FinalizeOptimisticWriter - could not find writer");
222: 	}
223: 	optimistic_writer.Merge(*owned_writer);
224: }
225: 
226: void LocalTableStorage::Rollback() {
227: 	for (auto &writer : optimistic_writers) {
228: 		writer->Rollback();
229: 	}
230: 	optimistic_writers.clear();
231: 	optimistic_writer.Rollback();
232: }
233: 
234: //===--------------------------------------------------------------------===//
235: // LocalTableManager
236: //===--------------------------------------------------------------------===//
237: optional_ptr<LocalTableStorage> LocalTableManager::GetStorage(DataTable &table) {
238: 	lock_guard<mutex> l(table_storage_lock);
239: 	auto entry = table_storage.find(table);
240: 	return entry == table_storage.end() ? nullptr : entry->second.get();
241: }
242: 
243: LocalTableStorage &LocalTableManager::GetOrCreateStorage(ClientContext &context, DataTable &table) {
244: 	lock_guard<mutex> l(table_storage_lock);
245: 	auto entry = table_storage.find(table);
246: 	if (entry == table_storage.end()) {
247: 		auto new_storage = make_shared_ptr<LocalTableStorage>(context, table);
248: 		auto storage = new_storage.get();
249: 		table_storage.insert(make_pair(reference<DataTable>(table), std::move(new_storage)));
250: 		return *storage;
251: 	} else {
252: 		return *entry->second.get();
253: 	}
254: }
255: 
256: bool LocalTableManager::IsEmpty() {
257: 	lock_guard<mutex> l(table_storage_lock);
258: 	return table_storage.empty();
259: }
260: 
261: shared_ptr<LocalTableStorage> LocalTableManager::MoveEntry(DataTable &table) {
262: 	lock_guard<mutex> l(table_storage_lock);
263: 	auto entry = table_storage.find(table);
264: 	if (entry == table_storage.end()) {
265: 		return nullptr;
266: 	}
267: 	auto storage_entry = std::move(entry->second);
268: 	table_storage.erase(entry);
269: 	return storage_entry;
270: }
271: 
272: reference_map_t<DataTable, shared_ptr<LocalTableStorage>> LocalTableManager::MoveEntries() {
273: 	lock_guard<mutex> l(table_storage_lock);
274: 	return std::move(table_storage);
275: }
276: 
277: idx_t LocalTableManager::EstimatedSize() {
278: 	lock_guard<mutex> l(table_storage_lock);
279: 	idx_t estimated_size = 0;
280: 	for (auto &storage : table_storage) {
281: 		estimated_size += storage.second->EstimatedSize();
282: 	}
283: 	return estimated_size;
284: }
285: 
286: void LocalTableManager::InsertEntry(DataTable &table, shared_ptr<LocalTableStorage> entry) {
287: 	lock_guard<mutex> l(table_storage_lock);
288: 	D_ASSERT(table_storage.find(table) == table_storage.end());
289: 	table_storage[table] = std::move(entry);
290: }
291: 
292: //===--------------------------------------------------------------------===//
293: // LocalStorage
294: //===--------------------------------------------------------------------===//
295: LocalStorage::LocalStorage(ClientContext &context, DuckTransaction &transaction)
296:     : context(context), transaction(transaction) {
297: }
298: 
299: LocalStorage::CommitState::CommitState() {
300: }
301: 
302: LocalStorage::CommitState::~CommitState() {
303: }
304: 
305: LocalStorage &LocalStorage::Get(DuckTransaction &transaction) {
306: 	return transaction.GetLocalStorage();
307: }
308: 
309: LocalStorage &LocalStorage::Get(ClientContext &context, AttachedDatabase &db) {
310: 	return DuckTransaction::Get(context, db).GetLocalStorage();
311: }
312: 
313: LocalStorage &LocalStorage::Get(ClientContext &context, Catalog &catalog) {
314: 	return LocalStorage::Get(context, catalog.GetAttached());
315: }
316: 
317: void LocalStorage::InitializeScan(DataTable &table, CollectionScanState &state,
318:                                   optional_ptr<TableFilterSet> table_filters) {
319: 	auto storage = table_manager.GetStorage(table);
320: 	if (storage == nullptr) {
321: 		return;
322: 	}
323: 	storage->InitializeScan(state, table_filters);
324: }
325: 
326: void LocalStorage::Scan(CollectionScanState &state, const vector<storage_t> &column_ids, DataChunk &result) {
327: 	state.Scan(transaction, result);
328: }
329: 
330: void LocalStorage::InitializeParallelScan(DataTable &table, ParallelCollectionScanState &state) {
331: 	auto storage = table_manager.GetStorage(table);
332: 	if (!storage) {
333: 		state.max_row = 0;
334: 		state.vector_index = 0;
335: 		state.current_row_group = nullptr;
336: 	} else {
337: 		storage->row_groups->InitializeParallelScan(state);
338: 	}
339: }
340: 
341: bool LocalStorage::NextParallelScan(ClientContext &context, DataTable &table, ParallelCollectionScanState &state,
342:                                     CollectionScanState &scan_state) {
343: 	auto storage = table_manager.GetStorage(table);
344: 	if (!storage) {
345: 		return false;
346: 	}
347: 	return storage->row_groups->NextParallelScan(context, state, scan_state);
348: }
349: 
350: void LocalStorage::InitializeAppend(LocalAppendState &state, DataTable &table) {
351: 	table.InitializeIndexes(context);
352: 	state.storage = &table_manager.GetOrCreateStorage(context, table);
353: 	state.storage->row_groups->InitializeAppend(TransactionData(transaction), state.append_state);
354: }
355: 
356: void LocalStorage::Append(LocalAppendState &state, DataChunk &chunk) {
357: 	// append to unique indices (if any)
358: 	auto storage = state.storage;
359: 	idx_t base_id =
360: 	    NumericCast<idx_t>(MAX_ROW_ID) + storage->row_groups->GetTotalRows() + state.append_state.total_append_count;
361: 	auto error = DataTable::AppendToIndexes(storage->indexes, chunk, NumericCast<row_t>(base_id));
362: 	if (error.HasError()) {
363: 		error.Throw();
364: 	}
365: 
366: 	//! Append the chunk to the local storage
367: 	auto new_row_group = storage->row_groups->Append(chunk, state.append_state);
368: 	//! Check if we should pre-emptively flush blocks to disk
369: 	if (new_row_group) {
370: 		storage->WriteNewRowGroup();
371: 	}
372: }
373: 
374: void LocalStorage::FinalizeAppend(LocalAppendState &state) {
375: 	state.storage->row_groups->FinalizeAppend(state.append_state.transaction, state.append_state);
376: }
377: 
378: void LocalStorage::LocalMerge(DataTable &table, RowGroupCollection &collection) {
379: 	auto &storage = table_manager.GetOrCreateStorage(context, table);
380: 	if (!storage.indexes.Empty()) {
381: 		// append data to indexes if required
382: 		row_t base_id = MAX_ROW_ID + NumericCast<row_t>(storage.row_groups->GetTotalRows());
383: 		auto error = storage.AppendToIndexes(transaction, collection, storage.indexes, table.GetTypes(), base_id);
384: 		if (error.HasError()) {
385: 			error.Throw();
386: 		}
387: 	}
388: 	storage.row_groups->MergeStorage(collection);
389: 	storage.merged_storage = true;
390: }
391: 
392: OptimisticDataWriter &LocalStorage::CreateOptimisticWriter(DataTable &table) {
393: 	auto &storage = table_manager.GetOrCreateStorage(context, table);
394: 	return storage.CreateOptimisticWriter();
395: }
396: 
397: void LocalStorage::FinalizeOptimisticWriter(DataTable &table, OptimisticDataWriter &writer) {
398: 	auto &storage = table_manager.GetOrCreateStorage(context, table);
399: 	storage.FinalizeOptimisticWriter(writer);
400: }
401: 
402: bool LocalStorage::ChangesMade() noexcept {
403: 	return !table_manager.IsEmpty();
404: }
405: 
406: bool LocalStorage::Find(DataTable &table) {
407: 	return table_manager.GetStorage(table) != nullptr;
408: }
409: 
410: idx_t LocalStorage::EstimatedSize() {
411: 	return table_manager.EstimatedSize();
412: }
413: 
414: idx_t LocalStorage::Delete(DataTable &table, Vector &row_ids, idx_t count) {
415: 	auto storage = table_manager.GetStorage(table);
416: 	D_ASSERT(storage);
417: 
418: 	// delete from unique indices (if any)
419: 	if (!storage->indexes.Empty()) {
420: 		storage->row_groups->RemoveFromIndexes(storage->indexes, row_ids, count);
421: 	}
422: 
423: 	auto ids = FlatVector::GetData<row_t>(row_ids);
424: 	idx_t delete_count = storage->row_groups->Delete(TransactionData(0, 0), table, ids, count);
425: 	storage->deleted_rows += delete_count;
426: 	return delete_count;
427: }
428: 
429: void LocalStorage::Update(DataTable &table, Vector &row_ids, const vector<PhysicalIndex> &column_ids,
430:                           DataChunk &updates) {
431: 	auto storage = table_manager.GetStorage(table);
432: 	D_ASSERT(storage);
433: 
434: 	auto ids = FlatVector::GetData<row_t>(row_ids);
435: 	storage->row_groups->Update(TransactionData(0, 0), ids, column_ids, updates);
436: }
437: 
438: void LocalStorage::Flush(DataTable &table, LocalTableStorage &storage) {
439: 	if (storage.is_dropped) {
440: 		return;
441: 	}
442: 	if (storage.row_groups->GetTotalRows() <= storage.deleted_rows) {
443: 		// all rows that we added were deleted
444: 		// rollback any partial blocks that are still outstanding
445: 		storage.Rollback();
446: 		return;
447: 	}
448: 	idx_t append_count = storage.row_groups->GetTotalRows() - storage.deleted_rows;
449: 
450: 	table.InitializeIndexes(context);
451: 
452: 	TableAppendState append_state;
453: 	table.AppendLock(append_state);
454: 	transaction.PushAppend(table, NumericCast<idx_t>(append_state.row_start), append_count);
455: 	if ((append_state.row_start == 0 || storage.row_groups->GetTotalRows() >= MERGE_THRESHOLD) &&
456: 	    storage.deleted_rows == 0) {
457: 		// table is currently empty OR we are bulk appending: move over the storage directly
458: 		// first flush any outstanding blocks
459: 		storage.FlushBlocks();
460: 		// now append to the indexes (if there are any)
461: 		// FIXME: we should be able to merge the transaction-local index directly into the main table index
462: 		// as long we just rewrite some row-ids
463: 		if (table.HasIndexes()) {
464: 			storage.AppendToIndexes(transaction, append_state, append_count, false);
465: 		}
466: 		// finally move over the row groups
467: 		table.MergeStorage(*storage.row_groups, storage.indexes);
468: 	} else {
469: 		// check if we have written data
470: 		// if we have, we cannot merge to disk after all
471: 		// so we need to revert the data we have already written
472: 		storage.Rollback();
473: 		// append to the indexes and append to the base table
474: 		storage.AppendToIndexes(transaction, append_state, append_count, true);
475: 	}
476: 
477: 	// possibly vacuum any excess index data
478: 	table.VacuumIndexes();
479: }
480: 
481: void LocalStorage::Commit() {
482: 	// commit local storage
483: 	// iterate over all entries in the table storage map and commit them
484: 	// after this, the local storage is no longer required and can be cleared
485: 	auto table_storage = table_manager.MoveEntries();
486: 	for (auto &entry : table_storage) {
487: 		auto table = entry.first;
488: 		auto storage = entry.second.get();
489: 		Flush(table, *storage);
490: 		entry.second.reset();
491: 	}
492: }
493: 
494: void LocalStorage::Rollback() {
495: 	// rollback local storage
496: 	// after this, the local storage is no longer required and can be cleared
497: 	auto table_storage = table_manager.MoveEntries();
498: 	for (auto &entry : table_storage) {
499: 		auto storage = entry.second.get();
500: 		if (!storage) {
501: 			continue;
502: 		}
503: 		storage->Rollback();
504: 
505: 		entry.second.reset();
506: 	}
507: }
508: 
509: idx_t LocalStorage::AddedRows(DataTable &table) {
510: 	auto storage = table_manager.GetStorage(table);
511: 	if (!storage) {
512: 		return 0;
513: 	}
514: 	return storage->row_groups->GetTotalRows() - storage->deleted_rows;
515: }
516: 
517: void LocalStorage::DropTable(DataTable &table) {
518: 	auto storage = table_manager.GetStorage(table);
519: 	if (!storage) {
520: 		return;
521: 	}
522: 	storage->is_dropped = true;
523: }
524: 
525: void LocalStorage::MoveStorage(DataTable &old_dt, DataTable &new_dt) {
526: 	// check if there are any pending appends for the old version of the table
527: 	auto new_storage = table_manager.MoveEntry(old_dt);
528: 	if (!new_storage) {
529: 		return;
530: 	}
531: 	// take over the storage from the old entry
532: 	new_storage->table_ref = new_dt;
533: 	table_manager.InsertEntry(new_dt, std::move(new_storage));
534: }
535: 
536: void LocalStorage::AddColumn(DataTable &old_dt, DataTable &new_dt, ColumnDefinition &new_column,
537:                              ExpressionExecutor &default_executor) {
538: 	// check if there are any pending appends for the old version of the table
539: 	auto storage = table_manager.MoveEntry(old_dt);
540: 	if (!storage) {
541: 		return;
542: 	}
543: 	auto new_storage = make_shared_ptr<LocalTableStorage>(context, new_dt, *storage, new_column, default_executor);
544: 	table_manager.InsertEntry(new_dt, std::move(new_storage));
545: }
546: 
547: void LocalStorage::DropColumn(DataTable &old_dt, DataTable &new_dt, idx_t removed_column) {
548: 	// check if there are any pending appends for the old version of the table
549: 	auto storage = table_manager.MoveEntry(old_dt);
550: 	if (!storage) {
551: 		return;
552: 	}
553: 	auto new_storage = make_shared_ptr<LocalTableStorage>(new_dt, *storage, removed_column);
554: 	table_manager.InsertEntry(new_dt, std::move(new_storage));
555: }
556: 
557: void LocalStorage::ChangeType(DataTable &old_dt, DataTable &new_dt, idx_t changed_idx, const LogicalType &target_type,
558:                               const vector<column_t> &bound_columns, Expression &cast_expr) {
559: 	// check if there are any pending appends for the old version of the table
560: 	auto storage = table_manager.MoveEntry(old_dt);
561: 	if (!storage) {
562: 		return;
563: 	}
564: 	auto new_storage = make_shared_ptr<LocalTableStorage>(context, new_dt, *storage, changed_idx, target_type,
565: 	                                                      bound_columns, cast_expr);
566: 	table_manager.InsertEntry(new_dt, std::move(new_storage));
567: }
568: 
569: void LocalStorage::FetchChunk(DataTable &table, Vector &row_ids, idx_t count, const vector<column_t> &col_ids,
570:                               DataChunk &chunk, ColumnFetchState &fetch_state) {
571: 	auto storage = table_manager.GetStorage(table);
572: 	if (!storage) {
573: 		throw InternalException("LocalStorage::FetchChunk - local storage not found");
574: 	}
575: 
576: 	storage->row_groups->Fetch(transaction, chunk, col_ids, row_ids, count, fetch_state);
577: }
578: 
579: TableIndexList &LocalStorage::GetIndexes(DataTable &table) {
580: 	auto storage = table_manager.GetStorage(table);
581: 	if (!storage) {
582: 		throw InternalException("LocalStorage::GetIndexes - local storage not found");
583: 	}
584: 	return storage->indexes;
585: }
586: 
587: void LocalStorage::VerifyNewConstraint(DataTable &parent, const BoundConstraint &constraint) {
588: 	auto storage = table_manager.GetStorage(parent);
589: 	if (!storage) {
590: 		return;
591: 	}
592: 	storage->row_groups->VerifyNewConstraint(parent, constraint);
593: }
594: 
595: } // namespace duckdb
[end of src/storage/local_storage.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: