You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Segmentation fault
### What happens?

Sometimes a segmentation fault when DuckDB is backed by a file, in-memory it does not issue a seg fault.

Out of 277 datasets, only 5 have this issue. With provided dataset, can always be reproduced.

```
(gdb) bt
#0  0x0000555555775ed6 in std::__shared_ptr<duckdb::DataTableInfo, (__gnu_cxx::_Lock_policy)2>::get (this=0x29c) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:1667
#1  0x00005555557688de in duckdb::shared_ptr<duckdb::DataTableInfo, true>::operator* (this=0x29c) at /home/skinkie/Sources/duckdb/src/include/duckdb/common/shared_ptr_ipp.hpp:196
#2  0x000055555621bb98 in duckdb::RowGroupCollection::GetTableInfo (this=0x28c) at /home/skinkie/Sources/duckdb/src/include/duckdb/storage/table/row_group_collection.hpp:129
#3  0x00005555561e5cc6 in duckdb::RowGroup::GetTableInfo (this=0x55555a7159e0) at /home/skinkie/Sources/duckdb/src/storage/table/row_group.cpp:135
#4  0x00005555561e6c1a in duckdb::RowGroup::AddColumn (this=0x55555a7159e0, new_collection=..., new_column=..., executor=..., result=...) at /home/skinkie/Sources/duckdb/src/storage/table/row_group.cpp:305
#5  0x00005555561f019a in duckdb::RowGroupCollection::AddColumn (this=0x55555a1ee9c0, context=..., new_column=..., default_executor=...) at /home/skinkie/Sources/duckdb/src/storage/table/row_group_collection.cpp:1066
#6  0x0000555556279e05 in duckdb::DataTable::DataTable (this=0x55555a80eaf0, context=..., parent=..., new_column=..., default_value=...) at /home/skinkie/Sources/duckdb/src/storage/data_table.cpp:80
#7  0x00005555557a16e8 in std::_Construct<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (__p=0x55555a80eaf0)
    at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/stl_construct.h:119
#8  0x000055555579c4e8 in std::allocator_traits<std::allocator<void> >::construct<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (__p=0x55555a80eaf0)
    at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/alloc_traits.h:657
#9  std::_Sp_counted_ptr_inplace<duckdb::DataTable, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_Sp_counted_ptr_inplace<duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (
    this=0x55555a80eae0, __a=...) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:607
#10 0x00005555557974e8 in std::__shared_count<(__gnu_cxx::_Lock_policy)2>::__shared_count<duckdb::DataTable, std::allocator<void>, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (
    this=0x7fffffffa588, __p=@0x7fffffffa580: 0x0, __a=...) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:969
#11 0x00005555557916e2 in std::__shared_ptr<duckdb::DataTable, (__gnu_cxx::_Lock_policy)2>::__shared_ptr<std::allocator<void>, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (
    this=0x7fffffffa580, __tag=...) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:1713
#12 0x0000555555788805 in std::shared_ptr<duckdb::DataTable>::shared_ptr<std::allocator<void>, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (this=0x7fffffffa580, __tag=...)
    at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr.h:463
#13 0x000055555577cb90 in std::make_shared<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> () at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr.h:1008
#14 0x000055555576f1f0 in duckdb::make_shared_ptr<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> () at /home/skinkie/Sources/duckdb/src/include/duckdb/common/helper.hpp:73
#15 0x0000555555753e98 in duckdb::DuckTableEntry::AddColumn (this=0x55555a196530, context=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_table_entry.cpp:391
#16 0x000055555575271e in duckdb::DuckTableEntry::AlterEntry (this=0x55555a196530, context=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_table_entry.cpp:231
#17 0x00005555557522bc in duckdb::DuckTableEntry::AlterEntry (this=0x55555a196530, transaction=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_table_entry.cpp:184
#18 0x00005555557b6bd8 in duckdb::CatalogSet::AlterEntry (this=0x55555a16be20, transaction=..., name="kv6_import", alter_info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_set.cpp:328
#19 0x000055555574fcf9 in duckdb::DuckSchemaEntry::Alter (this=0x55555a16bd40, transaction=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_schema_entry.cpp:291
#20 0x00005555557b156a in duckdb::Catalog::Alter (this=0x55555a1704e0, transaction=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog.cpp:904
#21 0x00005555557b176b in duckdb::Catalog::Alter (this=0x55555a1704e0, context=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog.cpp:912
#22 0x00005555570b1ff5 in duckdb::PhysicalAlter::GetData (this=0x7fffdc5f55a0, context=..., chunk=..., input=...) at /home/skinkie/Sources/duckdb/src/execution/operator/schema/physical_alter.cpp:13
#23 0x0000555555ff23af in duckdb::PipelineExecutor::GetData (this=0x55555a31e250, chunk=..., input=...) at /home/skinkie/Sources/duckdb/src/parallel/pipeline_executor.cpp:478
#24 0x0000555555ff24a0 in duckdb::PipelineExecutor::FetchFromSource (this=0x55555a31e250, result=...) at /home/skinkie/Sources/duckdb/src/parallel/pipeline_executor.cpp:504
#25 0x0000555555ff1268 in duckdb::PipelineExecutor::Execute (this=0x55555a31e250, max_chunks=50) at /home/skinkie/Sources/duckdb/src/parallel/pipeline_executor.cpp:204
#26 0x0000555555fed597 in duckdb::PipelineTask::ExecuteTask (this=0x7fffdc15fff0, mode=duckdb::TaskExecutionMode::PROCESS_PARTIAL) at /home/skinkie/Sources/duckdb/src/parallel/pipeline.cpp:40
#27 0x0000555555fe7c38 in duckdb::ExecutorTask::Execute (this=0x7fffdc15fff0, mode=duckdb::TaskExecutionMode::PROCESS_PARTIAL) at /home/skinkie/Sources/duckdb/src/parallel/executor_task.cpp:44
#28 0x0000555555feb739 in duckdb::Executor::ExecuteTask (this=0x55555a173350, dry_run=false) at /home/skinkie/Sources/duckdb/src/parallel/executor.cpp:580
#29 0x0000555555eab6d5 in duckdb::ClientContext::ExecuteTaskInternal (this=0x55555a16e960, lock=..., result=..., dry_run=false) at /home/skinkie/Sources/duckdb/src/main/client_context.cpp:557
#30 0x0000555555ec66bd in duckdb::PendingQueryResult::ExecuteTaskInternal (this=0x55555a46b6d0, lock=...) at /home/skinkie/Sources/duckdb/src/main/pending_query_result.cpp:68
#31 0x0000555555ec6751 in duckdb::PendingQueryResult::ExecuteInternal (this=0x55555a46b6d0, lock=...) at /home/skinkie/Sources/duckdb/src/main/pending_query_result.cpp:75
#32 0x0000555555ec6905 in duckdb::PendingQueryResult::Execute (this=0x55555a46b6d0) at /home/skinkie/Sources/duckdb/src/main/pending_query_result.cpp:95
#33 0x0000555555ec72be in duckdb::PreparedStatement::Execute (this=0x55555a4dd5b0, values=..., allow_stream_result=false) at /home/skinkie/Sources/duckdb/src/main/prepared_statement.cpp:85
#34 0x00005555556b91da in duckdb_shell_sqlite3_print_duckbox (pStmt=0x7fffcc091870, max_rows=40, max_width=0, null_value=0x7fffffffc03c "", columnar=0)
    at /home/skinkie/Sources/duckdb/tools/sqlite3_api_wrapper/sqlite3_api_wrapper.cpp:249
#35 0x000055555568be00 in exec_prepared_stmt (pArg=0x7fffffffbf20, pStmt=0x7fffcc091870) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:12752
#36 0x000055555568cc4c in shell_exec (pArg=0x7fffffffbf20, zSql=0x55555a1ef160 "alter table kv6_import add column trip_id varchar(16);", pzErrMsg=0x7fffffffbdb0) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:13087
#37 0x0000555555699c79 in runOneSqlLine (p=0x7fffffffbf20, zSql=0x55555a1ef160 "alter table kv6_import add column trip_id varchar(16);", in=0x55555a31e000, startline=31) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19273
#38 0x000055555569a16f in process_input (p=0x7fffffffbf20) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19384
#39 0x000055555569a49a in process_sqliterc (p=0x7fffffffbf20, sqliterc_override=0x7fffffffd720 "/tmp/script.sql") at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19505
#40 0x000055555569b174 in main (argc=4, argv=0x7fffffffd258) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19951
```

### To Reproduce

```sql
DROP TABLE IF EXISTS kv6_import;
DROP TABLE IF EXISTS tt_import;

CREATE TABLE "kv6_import" (
        "receive"                   TIMESTAMP     NOT NULL,
        "message"                   TIMESTAMP     NOT NULL,
        "vehicle"                   TIMESTAMP     NOT NULL,
        "messagetype"               VARCHAR(10)   NOT NULL,
        "operatingday"              DATE          NOT NULL,
        "dataownercode"             VARCHAR(10)   NOT NULL,
        "lineplanningnumber"        VARCHAR(10),
        "journeynumber"             UINTEGER       NOT NULL,
        "reinforcementnumber"       UTINYINT       NOT NULL,
        "userstopcode"              VARCHAR(10),
        "passagesequencenumber"     TINYINT,
        "distancesincelastuserstop" INTEGER,
        "punctuality"               INTEGER,
        "rd_x"                      VARCHAR(11),
        "rd_y"                      VARCHAR(11),
        "blockcode"                 INTEGER,
        "vehiclenumber"             SMALLINT,
        "wheelchairaccessible"      VARCHAR(5),
        "source"                    VARCHAR(10)   NOT NULL,
        "numberofcoaches"           UTINYINT
);

copy kv6_import from '/tmp/kv6-20240917.log' (DELIMITER ';');

delete from kv6_import where messagetype not in ('ARRIVAL', 'DEPARTURE');

alter table kv6_import add column trip_id varchar(16);

update kv6_import set trip_id = dataownercode || ':' || lineplanningnumber || ':' || journeynumber;

CREATE TABLE "tt_import" (
        "operatingday"          DATE          NOT NULL,
        "trip_id"               VARCHAR(16)   NOT NULL,
        "pointorder"            UTINYINT      NOT NULL,
        "passagesequencenumber" UTINYINT      NOT NULL,
        "userstopcode"          VARCHAR(10)   NOT NULL,
        "targetarrivaltime"     VARCHAR(8)    NOT NULL,
        "targetdeparturetime"   VARCHAR(8)    NOT NULL,
        "recordedarrivaltime"   VARCHAR(8),
        "recordeddeparturetime" VARCHAR(8)
);

copy tt_import from '/tmp/transittimes.csv' (DELIMITER ',');

delete from tt_import where trip_id like 'DOEKSEN:%' or trip_id like 'WSF:%' or trip_id like 'IFF%' or trip_id like 'WPD%' or trip_id like 'TESO%';

alter table tt_import drop column recordedarrivaltime;
alter table tt_import drop column recordeddeparturetime;

update kv6_import set trip_id = w.trip_id from (select tt_import.trip_id as trip_id, z.trip_id as kv6_trip_id, vehiclenumber from tt_import join (select trip_id, userstopcode, vehiclenumber from kv6_import join (select trip_id, min(receive) as receive from kv6_import where trip_id in (select trip_id from kv6_import where trip_id like 'ARR:%' and length(trip_id) = 13  except select trip_id from tt_import) group by trip_id) as y using (trip_id, receive)) as z using (userstopcode) where pointorder = 1 and tt_import.trip_id like z.trip_id[0:8] || '_' || z.trip_id[-5:]) as w where kv6_import.trip_id = kv6_trip_id and kv6_import.vehiclenumber = w.vehiclenumber;

copy (select tt_import.operatingday, tt_import.trip_id, x.reinforcementnumber, x.vehiclenumber, tt_import.userstopcode, tt_import.passagesequencenumber, tt_import.pointorder, tt_import.targetarrivaltime, tt_import.targetdeparturetime, x.arrival as recordedarrivaltime, x.departure as recordeddeparturetime from (select trip_id, reinforcementnumber, vehiclenumber, userstopcode, passagesequencenumber, max(a.receive) as arrival, max(b.receive) as departure from kv6_import as a full outer join kv6_import as b using (trip_id, reinforcementnumber, userstopcode, passagesequencenumber, vehiclenumber) where a.messagetype = 'ARRIVAL' and b.messagetype = 'DEPARTURE' group by trip_id, reinforcementnumber, userstopcode, passagesequencenumber, vehiclenumber) as x right join tt_import using (trip_id, userstopcode, passagesequencenumber) order by trip_id, reinforcementnumber, pointorder) to '/home/skinkie/arriva/volledig-20240917.csv.gz' header;
```

Data:
https://download.stefan.konink.de/duckdb/kv6-20240917.log.gz
https://download.stefan.konink.de/duckdb/transittimes.csv.gz


### OS:

Linux

### DuckDB Version:

main

### DuckDB Client:

cli

### Hardware:

amd64

### Full Name:

Stefan de Konink

### Affiliation:

Stichting OpenGeo

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a source build

### Did you include all relevant data sets for reproducing the issue?

No - Other reason (please specify in the issue body)

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have
Segmentation fault
### What happens?

Sometimes a segmentation fault when DuckDB is backed by a file, in-memory it does not issue a seg fault.

Out of 277 datasets, only 5 have this issue. With provided dataset, can always be reproduced.

```
(gdb) bt
#0  0x0000555555775ed6 in std::__shared_ptr<duckdb::DataTableInfo, (__gnu_cxx::_Lock_policy)2>::get (this=0x29c) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:1667
#1  0x00005555557688de in duckdb::shared_ptr<duckdb::DataTableInfo, true>::operator* (this=0x29c) at /home/skinkie/Sources/duckdb/src/include/duckdb/common/shared_ptr_ipp.hpp:196
#2  0x000055555621bb98 in duckdb::RowGroupCollection::GetTableInfo (this=0x28c) at /home/skinkie/Sources/duckdb/src/include/duckdb/storage/table/row_group_collection.hpp:129
#3  0x00005555561e5cc6 in duckdb::RowGroup::GetTableInfo (this=0x55555a7159e0) at /home/skinkie/Sources/duckdb/src/storage/table/row_group.cpp:135
#4  0x00005555561e6c1a in duckdb::RowGroup::AddColumn (this=0x55555a7159e0, new_collection=..., new_column=..., executor=..., result=...) at /home/skinkie/Sources/duckdb/src/storage/table/row_group.cpp:305
#5  0x00005555561f019a in duckdb::RowGroupCollection::AddColumn (this=0x55555a1ee9c0, context=..., new_column=..., default_executor=...) at /home/skinkie/Sources/duckdb/src/storage/table/row_group_collection.cpp:1066
#6  0x0000555556279e05 in duckdb::DataTable::DataTable (this=0x55555a80eaf0, context=..., parent=..., new_column=..., default_value=...) at /home/skinkie/Sources/duckdb/src/storage/data_table.cpp:80
#7  0x00005555557a16e8 in std::_Construct<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (__p=0x55555a80eaf0)
    at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/stl_construct.h:119
#8  0x000055555579c4e8 in std::allocator_traits<std::allocator<void> >::construct<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (__p=0x55555a80eaf0)
    at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/alloc_traits.h:657
#9  std::_Sp_counted_ptr_inplace<duckdb::DataTable, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_Sp_counted_ptr_inplace<duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (
    this=0x55555a80eae0, __a=...) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:607
#10 0x00005555557974e8 in std::__shared_count<(__gnu_cxx::_Lock_policy)2>::__shared_count<duckdb::DataTable, std::allocator<void>, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (
    this=0x7fffffffa588, __p=@0x7fffffffa580: 0x0, __a=...) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:969
#11 0x00005555557916e2 in std::__shared_ptr<duckdb::DataTable, (__gnu_cxx::_Lock_policy)2>::__shared_ptr<std::allocator<void>, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (
    this=0x7fffffffa580, __tag=...) at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr_base.h:1713
#12 0x0000555555788805 in std::shared_ptr<duckdb::DataTable>::shared_ptr<std::allocator<void>, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> (this=0x7fffffffa580, __tag=...)
    at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr.h:463
#13 0x000055555577cb90 in std::make_shared<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> () at /usr/lib/gcc/x86_64-pc-linux-gnu/14/include/g++-v14/bits/shared_ptr.h:1008
#14 0x000055555576f1f0 in duckdb::make_shared_ptr<duckdb::DataTable, duckdb::ClientContext&, duckdb::DataTable&, duckdb::ColumnDefinition&, duckdb::Expression&> () at /home/skinkie/Sources/duckdb/src/include/duckdb/common/helper.hpp:73
#15 0x0000555555753e98 in duckdb::DuckTableEntry::AddColumn (this=0x55555a196530, context=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_table_entry.cpp:391
#16 0x000055555575271e in duckdb::DuckTableEntry::AlterEntry (this=0x55555a196530, context=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_table_entry.cpp:231
#17 0x00005555557522bc in duckdb::DuckTableEntry::AlterEntry (this=0x55555a196530, transaction=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_table_entry.cpp:184
#18 0x00005555557b6bd8 in duckdb::CatalogSet::AlterEntry (this=0x55555a16be20, transaction=..., name="kv6_import", alter_info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_set.cpp:328
#19 0x000055555574fcf9 in duckdb::DuckSchemaEntry::Alter (this=0x55555a16bd40, transaction=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog_entry/duck_schema_entry.cpp:291
#20 0x00005555557b156a in duckdb::Catalog::Alter (this=0x55555a1704e0, transaction=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog.cpp:904
#21 0x00005555557b176b in duckdb::Catalog::Alter (this=0x55555a1704e0, context=..., info=...) at /home/skinkie/Sources/duckdb/src/catalog/catalog.cpp:912
#22 0x00005555570b1ff5 in duckdb::PhysicalAlter::GetData (this=0x7fffdc5f55a0, context=..., chunk=..., input=...) at /home/skinkie/Sources/duckdb/src/execution/operator/schema/physical_alter.cpp:13
#23 0x0000555555ff23af in duckdb::PipelineExecutor::GetData (this=0x55555a31e250, chunk=..., input=...) at /home/skinkie/Sources/duckdb/src/parallel/pipeline_executor.cpp:478
#24 0x0000555555ff24a0 in duckdb::PipelineExecutor::FetchFromSource (this=0x55555a31e250, result=...) at /home/skinkie/Sources/duckdb/src/parallel/pipeline_executor.cpp:504
#25 0x0000555555ff1268 in duckdb::PipelineExecutor::Execute (this=0x55555a31e250, max_chunks=50) at /home/skinkie/Sources/duckdb/src/parallel/pipeline_executor.cpp:204
#26 0x0000555555fed597 in duckdb::PipelineTask::ExecuteTask (this=0x7fffdc15fff0, mode=duckdb::TaskExecutionMode::PROCESS_PARTIAL) at /home/skinkie/Sources/duckdb/src/parallel/pipeline.cpp:40
#27 0x0000555555fe7c38 in duckdb::ExecutorTask::Execute (this=0x7fffdc15fff0, mode=duckdb::TaskExecutionMode::PROCESS_PARTIAL) at /home/skinkie/Sources/duckdb/src/parallel/executor_task.cpp:44
#28 0x0000555555feb739 in duckdb::Executor::ExecuteTask (this=0x55555a173350, dry_run=false) at /home/skinkie/Sources/duckdb/src/parallel/executor.cpp:580
#29 0x0000555555eab6d5 in duckdb::ClientContext::ExecuteTaskInternal (this=0x55555a16e960, lock=..., result=..., dry_run=false) at /home/skinkie/Sources/duckdb/src/main/client_context.cpp:557
#30 0x0000555555ec66bd in duckdb::PendingQueryResult::ExecuteTaskInternal (this=0x55555a46b6d0, lock=...) at /home/skinkie/Sources/duckdb/src/main/pending_query_result.cpp:68
#31 0x0000555555ec6751 in duckdb::PendingQueryResult::ExecuteInternal (this=0x55555a46b6d0, lock=...) at /home/skinkie/Sources/duckdb/src/main/pending_query_result.cpp:75
#32 0x0000555555ec6905 in duckdb::PendingQueryResult::Execute (this=0x55555a46b6d0) at /home/skinkie/Sources/duckdb/src/main/pending_query_result.cpp:95
#33 0x0000555555ec72be in duckdb::PreparedStatement::Execute (this=0x55555a4dd5b0, values=..., allow_stream_result=false) at /home/skinkie/Sources/duckdb/src/main/prepared_statement.cpp:85
#34 0x00005555556b91da in duckdb_shell_sqlite3_print_duckbox (pStmt=0x7fffcc091870, max_rows=40, max_width=0, null_value=0x7fffffffc03c "", columnar=0)
    at /home/skinkie/Sources/duckdb/tools/sqlite3_api_wrapper/sqlite3_api_wrapper.cpp:249
#35 0x000055555568be00 in exec_prepared_stmt (pArg=0x7fffffffbf20, pStmt=0x7fffcc091870) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:12752
#36 0x000055555568cc4c in shell_exec (pArg=0x7fffffffbf20, zSql=0x55555a1ef160 "alter table kv6_import add column trip_id varchar(16);", pzErrMsg=0x7fffffffbdb0) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:13087
#37 0x0000555555699c79 in runOneSqlLine (p=0x7fffffffbf20, zSql=0x55555a1ef160 "alter table kv6_import add column trip_id varchar(16);", in=0x55555a31e000, startline=31) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19273
#38 0x000055555569a16f in process_input (p=0x7fffffffbf20) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19384
#39 0x000055555569a49a in process_sqliterc (p=0x7fffffffbf20, sqliterc_override=0x7fffffffd720 "/tmp/script.sql") at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19505
#40 0x000055555569b174 in main (argc=4, argv=0x7fffffffd258) at /home/skinkie/Sources/duckdb/tools/shell/shell.c:19951
```

### To Reproduce

```sql
DROP TABLE IF EXISTS kv6_import;
DROP TABLE IF EXISTS tt_import;

CREATE TABLE "kv6_import" (
        "receive"                   TIMESTAMP     NOT NULL,
        "message"                   TIMESTAMP     NOT NULL,
        "vehicle"                   TIMESTAMP     NOT NULL,
        "messagetype"               VARCHAR(10)   NOT NULL,
        "operatingday"              DATE          NOT NULL,
        "dataownercode"             VARCHAR(10)   NOT NULL,
        "lineplanningnumber"        VARCHAR(10),
        "journeynumber"             UINTEGER       NOT NULL,
        "reinforcementnumber"       UTINYINT       NOT NULL,
        "userstopcode"              VARCHAR(10),
        "passagesequencenumber"     TINYINT,
        "distancesincelastuserstop" INTEGER,
        "punctuality"               INTEGER,
        "rd_x"                      VARCHAR(11),
        "rd_y"                      VARCHAR(11),
        "blockcode"                 INTEGER,
        "vehiclenumber"             SMALLINT,
        "wheelchairaccessible"      VARCHAR(5),
        "source"                    VARCHAR(10)   NOT NULL,
        "numberofcoaches"           UTINYINT
);

copy kv6_import from '/tmp/kv6-20240917.log' (DELIMITER ';');

delete from kv6_import where messagetype not in ('ARRIVAL', 'DEPARTURE');

alter table kv6_import add column trip_id varchar(16);

update kv6_import set trip_id = dataownercode || ':' || lineplanningnumber || ':' || journeynumber;

CREATE TABLE "tt_import" (
        "operatingday"          DATE          NOT NULL,
        "trip_id"               VARCHAR(16)   NOT NULL,
        "pointorder"            UTINYINT      NOT NULL,
        "passagesequencenumber" UTINYINT      NOT NULL,
        "userstopcode"          VARCHAR(10)   NOT NULL,
        "targetarrivaltime"     VARCHAR(8)    NOT NULL,
        "targetdeparturetime"   VARCHAR(8)    NOT NULL,
        "recordedarrivaltime"   VARCHAR(8),
        "recordeddeparturetime" VARCHAR(8)
);

copy tt_import from '/tmp/transittimes.csv' (DELIMITER ',');

delete from tt_import where trip_id like 'DOEKSEN:%' or trip_id like 'WSF:%' or trip_id like 'IFF%' or trip_id like 'WPD%' or trip_id like 'TESO%';

alter table tt_import drop column recordedarrivaltime;
alter table tt_import drop column recordeddeparturetime;

update kv6_import set trip_id = w.trip_id from (select tt_import.trip_id as trip_id, z.trip_id as kv6_trip_id, vehiclenumber from tt_import join (select trip_id, userstopcode, vehiclenumber from kv6_import join (select trip_id, min(receive) as receive from kv6_import where trip_id in (select trip_id from kv6_import where trip_id like 'ARR:%' and length(trip_id) = 13  except select trip_id from tt_import) group by trip_id) as y using (trip_id, receive)) as z using (userstopcode) where pointorder = 1 and tt_import.trip_id like z.trip_id[0:8] || '_' || z.trip_id[-5:]) as w where kv6_import.trip_id = kv6_trip_id and kv6_import.vehiclenumber = w.vehiclenumber;

copy (select tt_import.operatingday, tt_import.trip_id, x.reinforcementnumber, x.vehiclenumber, tt_import.userstopcode, tt_import.passagesequencenumber, tt_import.pointorder, tt_import.targetarrivaltime, tt_import.targetdeparturetime, x.arrival as recordedarrivaltime, x.departure as recordeddeparturetime from (select trip_id, reinforcementnumber, vehiclenumber, userstopcode, passagesequencenumber, max(a.receive) as arrival, max(b.receive) as departure from kv6_import as a full outer join kv6_import as b using (trip_id, reinforcementnumber, userstopcode, passagesequencenumber, vehiclenumber) where a.messagetype = 'ARRIVAL' and b.messagetype = 'DEPARTURE' group by trip_id, reinforcementnumber, userstopcode, passagesequencenumber, vehiclenumber) as x right join tt_import using (trip_id, userstopcode, passagesequencenumber) order by trip_id, reinforcementnumber, pointorder) to '/home/skinkie/arriva/volledig-20240917.csv.gz' header;
```

Data:
https://download.stefan.konink.de/duckdb/kv6-20240917.log.gz
https://download.stefan.konink.de/duckdb/transittimes.csv.gz


### OS:

Linux

### DuckDB Version:

main

### DuckDB Client:

cli

### Hardware:

amd64

### Full Name:

Stefan de Konink

### Affiliation:

Stichting OpenGeo

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a source build

### Did you include all relevant data sets for reproducing the issue?

No - Other reason (please specify in the issue body)

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://duckdb.org/docs/installation/) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of src/include/duckdb/storage/table/segment_tree.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/table/segment_tree.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/constants.hpp"
12: #include "duckdb/storage/storage_lock.hpp"
13: #include "duckdb/storage/table/segment_lock.hpp"
14: #include "duckdb/common/vector.hpp"
15: #include "duckdb/common/mutex.hpp"
16: #include "duckdb/common/string_util.hpp"
17: 
18: namespace duckdb {
19: 
20: template <class T>
21: struct SegmentNode {
22: 	idx_t row_start;
23: 	unique_ptr<T> node;
24: };
25: 
26: //! The SegmentTree maintains a list of all segments of a specific column in a table, and allows searching for a segment
27: //! by row number
28: template <class T, bool SUPPORTS_LAZY_LOADING = false>
29: class SegmentTree {
30: private:
31: 	class SegmentIterationHelper;
32: 
33: public:
34: 	explicit SegmentTree() : finished_loading(true) {
35: 	}
36: 	virtual ~SegmentTree() {
37: 	}
38: 
39: 	//! Locks the segment tree. All methods to the segment tree either lock the segment tree, or take an already
40: 	//! obtained lock.
41: 	SegmentLock Lock() {
42: 		return SegmentLock(node_lock);
43: 	}
44: 
45: 	bool IsEmpty(SegmentLock &l) {
46: 		return GetRootSegment(l) == nullptr;
47: 	}
48: 
49: 	//! Gets a pointer to the first segment. Useful for scans.
50: 	T *GetRootSegment() {
51: 		auto l = Lock();
52: 		return GetRootSegment(l);
53: 	}
54: 
55: 	T *GetRootSegment(SegmentLock &l) {
56: 		if (nodes.empty()) {
57: 			LoadNextSegment(l);
58: 		}
59: 		return GetRootSegmentInternal();
60: 	}
61: 	//! Obtains ownership of the data of the segment tree
62: 	vector<SegmentNode<T>> MoveSegments(SegmentLock &l) {
63: 		LoadAllSegments(l);
64: 		return std::move(nodes);
65: 	}
66: 	vector<SegmentNode<T>> MoveSegments() {
67: 		auto l = Lock();
68: 		return MoveSegments(l);
69: 	}
70: 	idx_t GetSegmentCount() {
71: 		auto l = Lock();
72: 		return GetSegmentCount(l);
73: 	}
74: 	idx_t GetSegmentCount(SegmentLock &l) {
75: 		return nodes.size();
76: 	}
77: 	//! Gets a pointer to the nth segment. Negative numbers start from the back.
78: 	T *GetSegmentByIndex(int64_t index) {
79: 		auto l = Lock();
80: 		return GetSegmentByIndex(l, index);
81: 	}
82: 	T *GetSegmentByIndex(SegmentLock &l, int64_t index) {
83: 		if (index < 0) {
84: 			// load all segments
85: 			LoadAllSegments(l);
86: 			index += nodes.size();
87: 			if (index < 0) {
88: 				return nullptr;
89: 			}
90: 			return nodes[UnsafeNumericCast<idx_t>(index)].node.get();
91: 		} else {
92: 			// lazily load segments until we reach the specific segment
93: 			while (idx_t(index) >= nodes.size() && LoadNextSegment(l)) {
94: 			}
95: 			if (idx_t(index) >= nodes.size()) {
96: 				return nullptr;
97: 			}
98: 			return nodes[UnsafeNumericCast<idx_t>(index)].node.get();
99: 		}
100: 	}
101: 	//! Gets the next segment
102: 	T *GetNextSegment(T *segment) {
103: 		if (!SUPPORTS_LAZY_LOADING) {
104: 			return segment->Next();
105: 		}
106: 		if (finished_loading) {
107: 			return segment->Next();
108: 		}
109: 		auto l = Lock();
110: 		return GetNextSegment(l, segment);
111: 	}
112: 	T *GetNextSegment(SegmentLock &l, T *segment) {
113: 		if (!segment) {
114: 			return nullptr;
115: 		}
116: #ifdef DEBUG
117: 		D_ASSERT(nodes[segment->index].node.get() == segment);
118: #endif
119: 		return GetSegmentByIndex(l, UnsafeNumericCast<int64_t>(segment->index + 1));
120: 	}
121: 
122: 	//! Gets a pointer to the last segment. Useful for appends.
123: 	T *GetLastSegment(SegmentLock &l) {
124: 		LoadAllSegments(l);
125: 		if (nodes.empty()) {
126: 			return nullptr;
127: 		}
128: 		return nodes.back().node.get();
129: 	}
130: 	//! Gets a pointer to a specific column segment for the given row
131: 	T *GetSegment(idx_t row_number) {
132: 		auto l = Lock();
133: 		return GetSegment(l, row_number);
134: 	}
135: 	T *GetSegment(SegmentLock &l, idx_t row_number) {
136: 		return nodes[GetSegmentIndex(l, row_number)].node.get();
137: 	}
138: 
139: 	//! Append a column segment to the tree
140: 	void AppendSegmentInternal(SegmentLock &l, unique_ptr<T> segment) {
141: 		D_ASSERT(segment);
142: 		// add the node to the list of nodes
143: 		if (!nodes.empty()) {
144: 			nodes.back().node->next = segment.get();
145: 		}
146: 		SegmentNode<T> node;
147: 		segment->index = nodes.size();
148: 		node.row_start = segment->start;
149: 		node.node = std::move(segment);
150: 		nodes.push_back(std::move(node));
151: 	}
152: 	void AppendSegment(unique_ptr<T> segment) {
153: 		auto l = Lock();
154: 		AppendSegment(l, std::move(segment));
155: 	}
156: 	void AppendSegment(SegmentLock &l, unique_ptr<T> segment) {
157: 		LoadAllSegments(l);
158: 		AppendSegmentInternal(l, std::move(segment));
159: 	}
160: 	//! Debug method, check whether the segment is in the segment tree
161: 	bool HasSegment(T *segment) {
162: 		auto l = Lock();
163: 		return HasSegment(l, segment);
164: 	}
165: 	bool HasSegment(SegmentLock &, T *segment) {
166: 		return segment->index < nodes.size() && nodes[segment->index].node.get() == segment;
167: 	}
168: 
169: 	//! Replace this tree with another tree, taking over its nodes in-place
170: 	void Replace(SegmentTree<T> &other) {
171: 		auto l = Lock();
172: 		Replace(l, other);
173: 	}
174: 	void Replace(SegmentLock &l, SegmentTree<T> &other) {
175: 		other.LoadAllSegments(l);
176: 		nodes = std::move(other.nodes);
177: 	}
178: 
179: 	//! Erase all segments after a specific segment
180: 	void EraseSegments(SegmentLock &l, idx_t segment_start) {
181: 		LoadAllSegments(l);
182: 		if (segment_start >= nodes.size() - 1) {
183: 			return;
184: 		}
185: 		nodes.erase(nodes.begin() + UnsafeNumericCast<int64_t>(segment_start) + 1, nodes.end());
186: 	}
187: 
188: 	//! Get the segment index of the column segment for the given row
189: 	idx_t GetSegmentIndex(SegmentLock &l, idx_t row_number) {
190: 		idx_t segment_index;
191: 		if (TryGetSegmentIndex(l, row_number, segment_index)) {
192: 			return segment_index;
193: 		}
194: 		string error;
195: 		error = StringUtil::Format("Attempting to find row number \"%lld\" in %lld nodes\n", row_number, nodes.size());
196: 		for (idx_t i = 0; i < nodes.size(); i++) {
197: 			error += StringUtil::Format("Node %lld: Start %lld, Count %lld", i, nodes[i].row_start,
198: 			                            nodes[i].node->count.load());
199: 		}
200: 		throw InternalException("Could not find node in column segment tree!\n%s%s", error, Exception::GetStackTrace());
201: 	}
202: 
203: 	bool TryGetSegmentIndex(SegmentLock &l, idx_t row_number, idx_t &result) {
204: 		// load segments until the row number is within bounds
205: 		while (nodes.empty() || (row_number >= (nodes.back().row_start + nodes.back().node->count))) {
206: 			if (!LoadNextSegment(l)) {
207: 				break;
208: 			}
209: 		}
210: 		if (nodes.empty()) {
211: 			return false;
212: 		}
213: 		idx_t lower = 0;
214: 		idx_t upper = nodes.size() - 1;
215: 		// binary search to find the node
216: 		while (lower <= upper) {
217: 			idx_t index = (lower + upper) / 2;
218: 			D_ASSERT(index < nodes.size());
219: 			auto &entry = nodes[index];
220: 			D_ASSERT(entry.row_start == entry.node->start);
221: 			if (row_number < entry.row_start) {
222: 				upper = index - 1;
223: 			} else if (row_number >= entry.row_start + entry.node->count) {
224: 				lower = index + 1;
225: 			} else {
226: 				result = index;
227: 				return true;
228: 			}
229: 		}
230: 		return false;
231: 	}
232: 
233: 	void Verify(SegmentLock &) {
234: #ifdef DEBUG
235: 		idx_t base_start = nodes.empty() ? 0 : nodes[0].node->start;
236: 		for (idx_t i = 0; i < nodes.size(); i++) {
237: 			D_ASSERT(nodes[i].row_start == nodes[i].node->start);
238: 			D_ASSERT(nodes[i].node->start == base_start);
239: 			base_start += nodes[i].node->count;
240: 		}
241: #endif
242: 	}
243: 	void Verify() {
244: #ifdef DEBUG
245: 		auto l = Lock();
246: 		Verify(l);
247: #endif
248: 	}
249: 
250: 	SegmentIterationHelper Segments() {
251: 		return SegmentIterationHelper(*this);
252: 	}
253: 
254: 	void Reinitialize() {
255: 		if (nodes.empty()) {
256: 			return;
257: 		}
258: 		idx_t offset = nodes[0].node->start;
259: 		for (auto &entry : nodes) {
260: 			if (entry.node->start != offset) {
261: 				throw InternalException("In SegmentTree::Reinitialize - gap found between nodes!");
262: 			}
263: 			entry.row_start = offset;
264: 			offset += entry.node->count;
265: 		}
266: 	}
267: 
268: protected:
269: 	atomic<bool> finished_loading;
270: 
271: 	//! Load the next segment - only used when lazily loading
272: 	virtual unique_ptr<T> LoadSegment() {
273: 		return nullptr;
274: 	}
275: 
276: private:
277: 	//! The nodes in the tree, can be binary searched
278: 	vector<SegmentNode<T>> nodes;
279: 	//! Lock to access or modify the nodes
280: 	mutex node_lock;
281: 
282: private:
283: 	T *GetRootSegmentInternal() {
284: 		return nodes.empty() ? nullptr : nodes[0].node.get();
285: 	}
286: 
287: 	class SegmentIterationHelper {
288: 	public:
289: 		explicit SegmentIterationHelper(SegmentTree &tree) : tree(tree) {
290: 		}
291: 
292: 	private:
293: 		SegmentTree &tree;
294: 
295: 	private:
296: 		class SegmentIterator {
297: 		public:
298: 			SegmentIterator(SegmentTree &tree_p, T *current_p) : tree(tree_p), current(current_p) {
299: 			}
300: 
301: 			SegmentTree &tree;
302: 			T *current;
303: 
304: 		public:
305: 			void Next() {
306: 				current = tree.GetNextSegment(current);
307: 			}
308: 
309: 			SegmentIterator &operator++() {
310: 				Next();
311: 				return *this;
312: 			}
313: 			bool operator!=(const SegmentIterator &other) const {
314: 				return current != other.current;
315: 			}
316: 			T &operator*() const {
317: 				D_ASSERT(current);
318: 				return *current;
319: 			}
320: 		};
321: 
322: 	public:
323: 		SegmentIterator begin() { // NOLINT: match stl API
324: 			return SegmentIterator(tree, tree.GetRootSegment());
325: 		}
326: 		SegmentIterator end() { // NOLINT: match stl API
327: 			return SegmentIterator(tree, nullptr);
328: 		}
329: 	};
330: 
331: 	//! Load the next segment, if there are any left to load
332: 	bool LoadNextSegment(SegmentLock &l) {
333: 		if (!SUPPORTS_LAZY_LOADING) {
334: 			return false;
335: 		}
336: 		if (finished_loading) {
337: 			return false;
338: 		}
339: 		auto result = LoadSegment();
340: 		if (result) {
341: 			AppendSegmentInternal(l, std::move(result));
342: 			return true;
343: 		}
344: 		return false;
345: 	}
346: 
347: 	//! Load all segments, if there are any left to load
348: 	void LoadAllSegments(SegmentLock &l) {
349: 		if (!SUPPORTS_LAZY_LOADING) {
350: 			return;
351: 		}
352: 		while (LoadNextSegment(l)) {
353: 		}
354: 	}
355: };
356: 
357: } // namespace duckdb
[end of src/include/duckdb/storage/table/segment_tree.hpp]
[start of src/storage/table/row_group_collection.cpp]
1: #include "duckdb/storage/table/row_group_collection.hpp"
2: 
3: #include "duckdb/common/serializer/binary_deserializer.hpp"
4: #include "duckdb/execution/expression_executor.hpp"
5: #include "duckdb/execution/index/bound_index.hpp"
6: #include "duckdb/execution/task_error_manager.hpp"
7: #include "duckdb/main/client_context.hpp"
8: #include "duckdb/parallel/task_executor.hpp"
9: #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
10: #include "duckdb/storage/checkpoint/table_data_writer.hpp"
11: #include "duckdb/storage/data_table.hpp"
12: #include "duckdb/storage/metadata/metadata_reader.hpp"
13: #include "duckdb/storage/table/append_state.hpp"
14: #include "duckdb/storage/table/column_checkpoint_state.hpp"
15: #include "duckdb/storage/table/persistent_table_data.hpp"
16: #include "duckdb/storage/table/row_group_segment_tree.hpp"
17: #include "duckdb/storage/table/scan_state.hpp"
18: #include "duckdb/storage/table_storage_info.hpp"
19: 
20: namespace duckdb {
21: 
22: //===--------------------------------------------------------------------===//
23: // Row Group Segment Tree
24: //===--------------------------------------------------------------------===//
25: RowGroupSegmentTree::RowGroupSegmentTree(RowGroupCollection &collection)
26:     : SegmentTree<RowGroup, true>(), collection(collection), current_row_group(0), max_row_group(0) {
27: }
28: RowGroupSegmentTree::~RowGroupSegmentTree() {
29: }
30: 
31: void RowGroupSegmentTree::Initialize(PersistentTableData &data) {
32: 	D_ASSERT(data.row_group_count > 0);
33: 	current_row_group = 0;
34: 	max_row_group = data.row_group_count;
35: 	finished_loading = false;
36: 	reader = make_uniq<MetadataReader>(collection.GetMetadataManager(), data.block_pointer);
37: }
38: 
39: unique_ptr<RowGroup> RowGroupSegmentTree::LoadSegment() {
40: 	if (current_row_group >= max_row_group) {
41: 		reader.reset();
42: 		finished_loading = true;
43: 		return nullptr;
44: 	}
45: 	BinaryDeserializer deserializer(*reader);
46: 	deserializer.Begin();
47: 	auto row_group_pointer = RowGroup::Deserialize(deserializer);
48: 	deserializer.End();
49: 	current_row_group++;
50: 	return make_uniq<RowGroup>(collection, std::move(row_group_pointer));
51: }
52: 
53: //===--------------------------------------------------------------------===//
54: // Row Group Collection
55: //===--------------------------------------------------------------------===//
56: RowGroupCollection::RowGroupCollection(shared_ptr<DataTableInfo> info_p, BlockManager &block_manager,
57:                                        vector<LogicalType> types_p, idx_t row_start_p, idx_t total_rows_p)
58:     : block_manager(block_manager), total_rows(total_rows_p), info(std::move(info_p)), types(std::move(types_p)),
59:       row_start(row_start_p), allocation_size(0) {
60: 	row_groups = make_shared_ptr<RowGroupSegmentTree>(*this);
61: }
62: 
63: idx_t RowGroupCollection::GetTotalRows() const {
64: 	return total_rows.load();
65: }
66: 
67: const vector<LogicalType> &RowGroupCollection::GetTypes() const {
68: 	return types;
69: }
70: 
71: Allocator &RowGroupCollection::GetAllocator() const {
72: 	return Allocator::Get(info->GetDB());
73: }
74: 
75: AttachedDatabase &RowGroupCollection::GetAttached() {
76: 	return GetTableInfo().GetDB();
77: }
78: 
79: MetadataManager &RowGroupCollection::GetMetadataManager() {
80: 	return GetBlockManager().GetMetadataManager();
81: }
82: 
83: //===--------------------------------------------------------------------===//
84: // Initialize
85: //===--------------------------------------------------------------------===//
86: void RowGroupCollection::Initialize(PersistentTableData &data) {
87: 	D_ASSERT(this->row_start == 0);
88: 	auto l = row_groups->Lock();
89: 	this->total_rows = data.total_rows;
90: 	row_groups->Initialize(data);
91: 	stats.Initialize(types, data);
92: }
93: 
94: void RowGroupCollection::Initialize(PersistentCollectionData &data) {
95: 	stats.InitializeEmpty(types);
96: 	auto l = row_groups->Lock();
97: 	for (auto &row_group_data : data.row_group_data) {
98: 		auto row_group = make_uniq<RowGroup>(*this, row_group_data);
99: 		row_group->MergeIntoStatistics(stats);
100: 		total_rows += row_group->count;
101: 		row_groups->AppendSegment(l, std::move(row_group));
102: 	}
103: }
104: 
105: void RowGroupCollection::InitializeEmpty() {
106: 	stats.InitializeEmpty(types);
107: }
108: 
109: void RowGroupCollection::AppendRowGroup(SegmentLock &l, idx_t start_row) {
110: 	D_ASSERT(start_row >= row_start);
111: 	auto new_row_group = make_uniq<RowGroup>(*this, start_row, 0U);
112: 	new_row_group->InitializeEmpty(types);
113: 	row_groups->AppendSegment(l, std::move(new_row_group));
114: }
115: 
116: RowGroup *RowGroupCollection::GetRowGroup(int64_t index) {
117: 	return (RowGroup *)row_groups->GetSegmentByIndex(index);
118: }
119: 
120: void RowGroupCollection::Verify() {
121: #ifdef DEBUG
122: 	idx_t current_total_rows = 0;
123: 	row_groups->Verify();
124: 	for (auto &row_group : row_groups->Segments()) {
125: 		row_group.Verify();
126: 		D_ASSERT(&row_group.GetCollection() == this);
127: 		D_ASSERT(row_group.start == this->row_start + current_total_rows);
128: 		current_total_rows += row_group.count;
129: 	}
130: 	D_ASSERT(current_total_rows == total_rows.load());
131: #endif
132: }
133: 
134: //===--------------------------------------------------------------------===//
135: // Scan
136: //===--------------------------------------------------------------------===//
137: void RowGroupCollection::InitializeScan(CollectionScanState &state, const vector<column_t> &column_ids,
138:                                         TableFilterSet *table_filters) {
139: 	auto row_group = row_groups->GetRootSegment();
140: 	D_ASSERT(row_group);
141: 	state.row_groups = row_groups.get();
142: 	state.max_row = row_start + total_rows;
143: 	state.Initialize(GetTypes());
144: 	while (row_group && !row_group->InitializeScan(state)) {
145: 		row_group = row_groups->GetNextSegment(row_group);
146: 	}
147: }
148: 
149: void RowGroupCollection::InitializeCreateIndexScan(CreateIndexScanState &state) {
150: 	state.segment_lock = row_groups->Lock();
151: }
152: 
153: void RowGroupCollection::InitializeScanWithOffset(CollectionScanState &state, const vector<column_t> &column_ids,
154:                                                   idx_t start_row, idx_t end_row) {
155: 	auto row_group = row_groups->GetSegment(start_row);
156: 	D_ASSERT(row_group);
157: 	state.row_groups = row_groups.get();
158: 	state.max_row = end_row;
159: 	state.Initialize(GetTypes());
160: 	idx_t start_vector = (start_row - row_group->start) / STANDARD_VECTOR_SIZE;
161: 	if (!row_group->InitializeScanWithOffset(state, start_vector)) {
162: 		throw InternalException("Failed to initialize row group scan with offset");
163: 	}
164: }
165: 
166: bool RowGroupCollection::InitializeScanInRowGroup(CollectionScanState &state, RowGroupCollection &collection,
167:                                                   RowGroup &row_group, idx_t vector_index, idx_t max_row) {
168: 	state.max_row = max_row;
169: 	state.row_groups = collection.row_groups.get();
170: 	if (!state.column_scans) {
171: 		// initialize the scan state
172: 		state.Initialize(collection.GetTypes());
173: 	}
174: 	return row_group.InitializeScanWithOffset(state, vector_index);
175: }
176: 
177: void RowGroupCollection::InitializeParallelScan(ParallelCollectionScanState &state) {
178: 	state.collection = this;
179: 	state.current_row_group = row_groups->GetRootSegment();
180: 	state.vector_index = 0;
181: 	state.max_row = row_start + total_rows;
182: 	state.batch_index = 0;
183: 	state.processed_rows = 0;
184: }
185: 
186: bool RowGroupCollection::NextParallelScan(ClientContext &context, ParallelCollectionScanState &state,
187:                                           CollectionScanState &scan_state) {
188: 	while (true) {
189: 		idx_t vector_index;
190: 		idx_t max_row;
191: 		RowGroupCollection *collection;
192: 		RowGroup *row_group;
193: 		{
194: 			// select the next row group to scan from the parallel state
195: 			lock_guard<mutex> l(state.lock);
196: 			if (!state.current_row_group || state.current_row_group->count == 0) {
197: 				// no more data left to scan
198: 				break;
199: 			}
200: 			collection = state.collection;
201: 			row_group = state.current_row_group;
202: 			if (ClientConfig::GetConfig(context).verify_parallelism) {
203: 				vector_index = state.vector_index;
204: 				max_row = state.current_row_group->start +
205: 				          MinValue<idx_t>(state.current_row_group->count,
206: 				                          STANDARD_VECTOR_SIZE * state.vector_index + STANDARD_VECTOR_SIZE);
207: 				D_ASSERT(vector_index * STANDARD_VECTOR_SIZE < state.current_row_group->count);
208: 				state.vector_index++;
209: 				if (state.vector_index * STANDARD_VECTOR_SIZE >= state.current_row_group->count) {
210: 					state.current_row_group = row_groups->GetNextSegment(state.current_row_group);
211: 					state.vector_index = 0;
212: 				}
213: 			} else {
214: 				state.processed_rows += state.current_row_group->count;
215: 				vector_index = 0;
216: 				max_row = state.current_row_group->start + state.current_row_group->count;
217: 				state.current_row_group = row_groups->GetNextSegment(state.current_row_group);
218: 			}
219: 			max_row = MinValue<idx_t>(max_row, state.max_row);
220: 			scan_state.batch_index = ++state.batch_index;
221: 		}
222: 		D_ASSERT(collection);
223: 		D_ASSERT(row_group);
224: 
225: 		// initialize the scan for this row group
226: 		bool need_to_scan = InitializeScanInRowGroup(scan_state, *collection, *row_group, vector_index, max_row);
227: 		if (!need_to_scan) {
228: 			// skip this row group
229: 			continue;
230: 		}
231: 		return true;
232: 	}
233: 	lock_guard<mutex> l(state.lock);
234: 	scan_state.batch_index = state.batch_index;
235: 	return false;
236: }
237: 
238: bool RowGroupCollection::Scan(DuckTransaction &transaction, const vector<column_t> &column_ids,
239:                               const std::function<bool(DataChunk &chunk)> &fun) {
240: 	vector<LogicalType> scan_types;
241: 	for (idx_t i = 0; i < column_ids.size(); i++) {
242: 		scan_types.push_back(types[column_ids[i]]);
243: 	}
244: 	DataChunk chunk;
245: 	chunk.Initialize(GetAllocator(), scan_types);
246: 
247: 	// initialize the scan
248: 	TableScanState state;
249: 	state.Initialize(column_ids, nullptr);
250: 	InitializeScan(state.local_state, column_ids, nullptr);
251: 
252: 	while (true) {
253: 		chunk.Reset();
254: 		state.local_state.Scan(transaction, chunk);
255: 		if (chunk.size() == 0) {
256: 			return true;
257: 		}
258: 		if (!fun(chunk)) {
259: 			return false;
260: 		}
261: 	}
262: }
263: 
264: bool RowGroupCollection::Scan(DuckTransaction &transaction, const std::function<bool(DataChunk &chunk)> &fun) {
265: 	vector<column_t> column_ids;
266: 	column_ids.reserve(types.size());
267: 	for (idx_t i = 0; i < types.size(); i++) {
268: 		column_ids.push_back(i);
269: 	}
270: 	return Scan(transaction, column_ids, fun);
271: }
272: 
273: //===--------------------------------------------------------------------===//
274: // Fetch
275: //===--------------------------------------------------------------------===//
276: void RowGroupCollection::Fetch(TransactionData transaction, DataChunk &result, const vector<column_t> &column_ids,
277:                                const Vector &row_identifiers, idx_t fetch_count, ColumnFetchState &state) {
278: 	// figure out which row_group to fetch from
279: 	auto row_ids = FlatVector::GetData<row_t>(row_identifiers);
280: 	idx_t count = 0;
281: 	for (idx_t i = 0; i < fetch_count; i++) {
282: 		auto row_id = row_ids[i];
283: 		RowGroup *row_group;
284: 		{
285: 			idx_t segment_index;
286: 			auto l = row_groups->Lock();
287: 			if (!row_groups->TryGetSegmentIndex(l, UnsafeNumericCast<idx_t>(row_id), segment_index)) {
288: 				// in parallel append scenarios it is possible for the row_id
289: 				continue;
290: 			}
291: 			row_group = row_groups->GetSegmentByIndex(l, UnsafeNumericCast<int64_t>(segment_index));
292: 		}
293: 		if (!row_group->Fetch(transaction, UnsafeNumericCast<idx_t>(row_id) - row_group->start)) {
294: 			continue;
295: 		}
296: 		row_group->FetchRow(transaction, state, column_ids, row_id, result, count);
297: 		count++;
298: 	}
299: 	result.SetCardinality(count);
300: }
301: 
302: //===--------------------------------------------------------------------===//
303: // Append
304: //===--------------------------------------------------------------------===//
305: TableAppendState::TableAppendState()
306:     : row_group_append_state(*this), total_append_count(0), start_row_group(nullptr), transaction(0, 0) {
307: }
308: 
309: TableAppendState::~TableAppendState() {
310: }
311: 
312: bool RowGroupCollection::IsEmpty() const {
313: 	auto l = row_groups->Lock();
314: 	return IsEmpty(l);
315: }
316: 
317: bool RowGroupCollection::IsEmpty(SegmentLock &l) const {
318: 	return row_groups->IsEmpty(l);
319: }
320: 
321: void RowGroupCollection::InitializeAppend(TransactionData transaction, TableAppendState &state) {
322: 	state.row_start = UnsafeNumericCast<row_t>(total_rows.load());
323: 	state.current_row = state.row_start;
324: 	state.total_append_count = 0;
325: 
326: 	// start writing to the row_groups
327: 	auto l = row_groups->Lock();
328: 	if (IsEmpty(l)) {
329: 		// empty row group collection: empty first row group
330: 		AppendRowGroup(l, row_start);
331: 	}
332: 	state.start_row_group = row_groups->GetLastSegment(l);
333: 	D_ASSERT(this->row_start + total_rows == state.start_row_group->start + state.start_row_group->count);
334: 	state.start_row_group->InitializeAppend(state.row_group_append_state);
335: 	state.transaction = transaction;
336: 
337: 	// initialize thread-local stats so we have less lock contention when updating distinct statistics
338: 	state.stats = TableStatistics();
339: 	state.stats.InitializeEmpty(types);
340: }
341: 
342: void RowGroupCollection::InitializeAppend(TableAppendState &state) {
343: 	TransactionData tdata(0, 0);
344: 	InitializeAppend(tdata, state);
345: }
346: 
347: bool RowGroupCollection::Append(DataChunk &chunk, TableAppendState &state) {
348: 	D_ASSERT(chunk.ColumnCount() == types.size());
349: 	chunk.Verify();
350: 
351: 	bool new_row_group = false;
352: 	idx_t total_append_count = chunk.size();
353: 	idx_t remaining = chunk.size();
354: 	state.total_append_count += total_append_count;
355: 	while (true) {
356: 		auto current_row_group = state.row_group_append_state.row_group;
357: 		// check how much we can fit into the current row_group
358: 		idx_t append_count =
359: 		    MinValue<idx_t>(remaining, Storage::ROW_GROUP_SIZE - state.row_group_append_state.offset_in_row_group);
360: 		if (append_count > 0) {
361: 			auto previous_allocation_size = current_row_group->GetAllocationSize();
362: 			current_row_group->Append(state.row_group_append_state, chunk, append_count);
363: 			allocation_size += current_row_group->GetAllocationSize() - previous_allocation_size;
364: 			// merge the stats
365: 			current_row_group->MergeIntoStatistics(stats);
366: 		}
367: 		remaining -= append_count;
368: 		if (remaining > 0) {
369: 			// we expect max 1 iteration of this loop (i.e. a single chunk should never overflow more than one
370: 			// row_group)
371: 			D_ASSERT(chunk.size() == remaining + append_count);
372: 			// slice the input chunk
373: 			if (remaining < chunk.size()) {
374: 				chunk.Slice(append_count, remaining);
375: 			}
376: 			// append a new row_group
377: 			new_row_group = true;
378: 			auto next_start = current_row_group->start + state.row_group_append_state.offset_in_row_group;
379: 
380: 			auto l = row_groups->Lock();
381: 			AppendRowGroup(l, next_start);
382: 			// set up the append state for this row_group
383: 			auto last_row_group = row_groups->GetLastSegment(l);
384: 			last_row_group->InitializeAppend(state.row_group_append_state);
385: 			continue;
386: 		} else {
387: 			break;
388: 		}
389: 	}
390: 	state.current_row += row_t(total_append_count);
391: 	auto local_stats_lock = state.stats.GetLock();
392: 	for (idx_t col_idx = 0; col_idx < types.size(); col_idx++) {
393: 		state.stats.GetStats(*local_stats_lock, col_idx).UpdateDistinctStatistics(chunk.data[col_idx], chunk.size());
394: 	}
395: 	return new_row_group;
396: }
397: 
398: void RowGroupCollection::FinalizeAppend(TransactionData transaction, TableAppendState &state) {
399: 	auto remaining = state.total_append_count;
400: 	auto row_group = state.start_row_group;
401: 	while (remaining > 0) {
402: 		auto append_count = MinValue<idx_t>(remaining, Storage::ROW_GROUP_SIZE - row_group->count);
403: 		row_group->AppendVersionInfo(transaction, append_count);
404: 		remaining -= append_count;
405: 		row_group = row_groups->GetNextSegment(row_group);
406: 	}
407: 	total_rows += state.total_append_count;
408: 
409: 	state.total_append_count = 0;
410: 	state.start_row_group = nullptr;
411: 
412: 	auto global_stats_lock = stats.GetLock();
413: 	auto local_stats_lock = state.stats.GetLock();
414: 	for (idx_t col_idx = 0; col_idx < types.size(); col_idx++) {
415: 		auto &global_stats = stats.GetStats(*global_stats_lock, col_idx);
416: 		if (!global_stats.HasDistinctStats()) {
417: 			continue;
418: 		}
419: 		auto &local_stats = state.stats.GetStats(*local_stats_lock, col_idx);
420: 		if (!local_stats.HasDistinctStats()) {
421: 			continue;
422: 		}
423: 		global_stats.DistinctStats().Merge(local_stats.DistinctStats());
424: 	}
425: 
426: 	Verify();
427: }
428: 
429: void RowGroupCollection::CommitAppend(transaction_t commit_id, idx_t row_start, idx_t count) {
430: 	auto row_group = row_groups->GetSegment(row_start);
431: 	D_ASSERT(row_group);
432: 	idx_t current_row = row_start;
433: 	idx_t remaining = count;
434: 	while (true) {
435: 		idx_t start_in_row_group = current_row - row_group->start;
436: 		idx_t append_count = MinValue<idx_t>(row_group->count - start_in_row_group, remaining);
437: 
438: 		row_group->CommitAppend(commit_id, start_in_row_group, append_count);
439: 
440: 		current_row += append_count;
441: 		remaining -= append_count;
442: 		if (remaining == 0) {
443: 			break;
444: 		}
445: 		row_group = row_groups->GetNextSegment(row_group);
446: 	}
447: }
448: 
449: void RowGroupCollection::RevertAppendInternal(idx_t start_row) {
450: 	total_rows = start_row;
451: 
452: 	auto l = row_groups->Lock();
453: 	idx_t segment_count = row_groups->GetSegmentCount(l);
454: 	if (segment_count == 0) {
455: 		// we have no segments to revert
456: 		return;
457: 	}
458: 	idx_t segment_index;
459: 	// find the segment index that the start row belongs to
460: 	if (!row_groups->TryGetSegmentIndex(l, start_row, segment_index)) {
461: 		// revert from the last segment
462: 		segment_index = segment_count - 1;
463: 	}
464: 	auto &segment = *row_groups->GetSegmentByIndex(l, UnsafeNumericCast<int64_t>(segment_index));
465: 
466: 	// remove any segments AFTER this segment: they should be deleted entirely
467: 	row_groups->EraseSegments(l, segment_index);
468: 
469: 	segment.next = nullptr;
470: 	segment.RevertAppend(start_row);
471: }
472: 
473: void RowGroupCollection::CleanupAppend(transaction_t lowest_transaction, idx_t start, idx_t count) {
474: 	auto row_group = row_groups->GetSegment(start);
475: 	D_ASSERT(row_group);
476: 	idx_t current_row = start;
477: 	idx_t remaining = count;
478: 	while (true) {
479: 		idx_t start_in_row_group = current_row - row_group->start;
480: 		idx_t append_count = MinValue<idx_t>(row_group->count - start_in_row_group, remaining);
481: 
482: 		row_group->CleanupAppend(lowest_transaction, start_in_row_group, append_count);
483: 
484: 		current_row += append_count;
485: 		remaining -= append_count;
486: 		if (remaining == 0) {
487: 			break;
488: 		}
489: 		row_group = row_groups->GetNextSegment(row_group);
490: 	}
491: }
492: 
493: bool RowGroupCollection::IsPersistent() const {
494: 	for (auto &row_group : row_groups->Segments()) {
495: 		if (!row_group.IsPersistent()) {
496: 			return false;
497: 		}
498: 	}
499: 	return true;
500: }
501: 
502: void RowGroupCollection::MergeStorage(RowGroupCollection &data, optional_ptr<DataTable> table,
503:                                       optional_ptr<StorageCommitState> commit_state) {
504: 	D_ASSERT(data.types == types);
505: 	auto start_index = row_start + total_rows.load();
506: 	auto index = start_index;
507: 	auto segments = data.row_groups->MoveSegments();
508: 
509: 	// check if the row groups we are merging are optimistically written
510: 	// if all row groups are optimistically written we keep around the block pointers
511: 	unique_ptr<PersistentCollectionData> row_group_data;
512: 	idx_t optimistically_written_count = 0;
513: 	if (commit_state) {
514: 		for (auto &entry : segments) {
515: 			auto &row_group = *entry.node;
516: 			if (!row_group.IsPersistent()) {
517: 				break;
518: 			}
519: 			optimistically_written_count += row_group.count;
520: 		}
521: 		if (optimistically_written_count > 0) {
522: 			row_group_data = make_uniq<PersistentCollectionData>();
523: 		}
524: 	}
525: 	for (auto &entry : segments) {
526: 		auto &row_group = entry.node;
527: 		row_group->MoveToCollection(*this, index);
528: 
529: 		if (commit_state && (index - start_index) < optimistically_written_count) {
530: 			// serialize the block pointers of this row group
531: 			auto persistent_data = row_group->SerializeRowGroupInfo();
532: 			persistent_data.types = types;
533: 			row_group_data->row_group_data.push_back(std::move(persistent_data));
534: 		}
535: 		index += row_group->count;
536: 		row_groups->AppendSegment(std::move(row_group));
537: 	}
538: 	if (commit_state && optimistically_written_count > 0) {
539: 		// if we have serialized the row groups - push the serialized block pointers into the commit state
540: 		commit_state->AddRowGroupData(*table, start_index, optimistically_written_count, std::move(row_group_data));
541: 	}
542: 	stats.MergeStats(data.stats);
543: 	total_rows += data.total_rows.load();
544: }
545: 
546: //===--------------------------------------------------------------------===//
547: // Delete
548: //===--------------------------------------------------------------------===//
549: idx_t RowGroupCollection::Delete(TransactionData transaction, DataTable &table, row_t *ids, idx_t count) {
550: 	idx_t delete_count = 0;
551: 	// delete is in the row groups
552: 	// we need to figure out for each id to which row group it belongs
553: 	// usually all (or many) ids belong to the same row group
554: 	// we iterate over the ids and check for every id if it belongs to the same row group as their predecessor
555: 	idx_t pos = 0;
556: 	do {
557: 		idx_t start = pos;
558: 		auto row_group = row_groups->GetSegment(UnsafeNumericCast<idx_t>(ids[start]));
559: 		for (pos++; pos < count; pos++) {
560: 			D_ASSERT(ids[pos] >= 0);
561: 			// check if this id still belongs to this row group
562: 			if (idx_t(ids[pos]) < row_group->start) {
563: 				// id is before row_group start -> it does not
564: 				break;
565: 			}
566: 			if (idx_t(ids[pos]) >= row_group->start + row_group->count) {
567: 				// id is after row group end -> it does not
568: 				break;
569: 			}
570: 		}
571: 		delete_count += row_group->Delete(transaction, table, ids + start, pos - start);
572: 	} while (pos < count);
573: 	return delete_count;
574: }
575: 
576: //===--------------------------------------------------------------------===//
577: // Update
578: //===--------------------------------------------------------------------===//
579: void RowGroupCollection::Update(TransactionData transaction, row_t *ids, const vector<PhysicalIndex> &column_ids,
580:                                 DataChunk &updates) {
581: 	idx_t pos = 0;
582: 	do {
583: 		idx_t start = pos;
584: 		auto row_group = row_groups->GetSegment(UnsafeNumericCast<idx_t>(ids[pos]));
585: 		row_t base_id =
586: 		    UnsafeNumericCast<row_t>(row_group->start + ((UnsafeNumericCast<idx_t>(ids[pos]) - row_group->start) /
587: 		                                                 STANDARD_VECTOR_SIZE * STANDARD_VECTOR_SIZE));
588: 		auto max_id = MinValue<row_t>(base_id + STANDARD_VECTOR_SIZE,
589: 		                              UnsafeNumericCast<row_t>(row_group->start + row_group->count));
590: 		for (pos++; pos < updates.size(); pos++) {
591: 			D_ASSERT(ids[pos] >= 0);
592: 			// check if this id still belongs to this vector in this row group
593: 			if (ids[pos] < base_id) {
594: 				// id is before vector start -> it does not
595: 				break;
596: 			}
597: 			if (ids[pos] >= max_id) {
598: 				// id is after the maximum id in this vector -> it does not
599: 				break;
600: 			}
601: 		}
602: 		row_group->Update(transaction, updates, ids, start, pos - start, column_ids);
603: 
604: 		auto l = stats.GetLock();
605: 		for (idx_t i = 0; i < column_ids.size(); i++) {
606: 			auto column_id = column_ids[i];
607: 			stats.MergeStats(*l, column_id.index, *row_group->GetStatistics(column_id.index));
608: 		}
609: 	} while (pos < updates.size());
610: }
611: 
612: void RowGroupCollection::RemoveFromIndexes(TableIndexList &indexes, Vector &row_identifiers, idx_t count) {
613: 	auto row_ids = FlatVector::GetData<row_t>(row_identifiers);
614: 
615: 	// initialize the fetch state
616: 	// FIXME: we do not need to fetch all columns, only the columns required by the indices!
617: 	TableScanState state;
618: 	vector<column_t> column_ids;
619: 	column_ids.reserve(types.size());
620: 	for (idx_t i = 0; i < types.size(); i++) {
621: 		column_ids.push_back(i);
622: 	}
623: 	state.Initialize(std::move(column_ids));
624: 	state.table_state.max_row = row_start + total_rows;
625: 
626: 	// initialize the fetch chunk
627: 	DataChunk result;
628: 	result.Initialize(GetAllocator(), types);
629: 
630: 	SelectionVector sel(STANDARD_VECTOR_SIZE);
631: 	// now iterate over the row ids
632: 	for (idx_t r = 0; r < count;) {
633: 		result.Reset();
634: 		// figure out which row_group to fetch from
635: 		auto row_id = row_ids[r];
636: 		auto row_group = row_groups->GetSegment(UnsafeNumericCast<idx_t>(row_id));
637: 		auto row_group_vector_idx = (UnsafeNumericCast<idx_t>(row_id) - row_group->start) / STANDARD_VECTOR_SIZE;
638: 		auto base_row_id = row_group_vector_idx * STANDARD_VECTOR_SIZE + row_group->start;
639: 
640: 		// fetch the current vector
641: 		state.table_state.Initialize(GetTypes());
642: 		row_group->InitializeScanWithOffset(state.table_state, row_group_vector_idx);
643: 		row_group->ScanCommitted(state.table_state, result, TableScanType::TABLE_SCAN_COMMITTED_ROWS);
644: 		result.Verify();
645: 
646: 		// check for any remaining row ids if they also fall into this vector
647: 		// we try to fetch handle as many rows as possible at the same time
648: 		idx_t sel_count = 0;
649: 		for (; r < count; r++) {
650: 			idx_t current_row = idx_t(row_ids[r]);
651: 			if (current_row < base_row_id || current_row >= base_row_id + result.size()) {
652: 				// this row-id does not fall into the current chunk - break
653: 				break;
654: 			}
655: 			auto row_in_vector = current_row - base_row_id;
656: 			D_ASSERT(row_in_vector < result.size());
657: 			sel.set_index(sel_count++, row_in_vector);
658: 		}
659: 		D_ASSERT(sel_count > 0);
660: 		// slice the vector with all rows that are present in this vector and erase from the index
661: 		result.Slice(sel, sel_count);
662: 
663: 		indexes.Scan([&](Index &index) {
664: 			if (index.IsBound()) {
665: 				index.Cast<BoundIndex>().Delete(result, row_identifiers);
666: 			} else {
667: 				throw MissingExtensionException(
668: 				    "Cannot delete from index '%s', unknown index type '%s'. You need to load the "
669: 				    "extension that provides this index type before table '%s' can be modified.",
670: 				    index.GetIndexName(), index.GetIndexType(), info->GetTableName());
671: 			}
672: 			return false;
673: 		});
674: 	}
675: }
676: 
677: void RowGroupCollection::UpdateColumn(TransactionData transaction, Vector &row_ids, const vector<column_t> &column_path,
678:                                       DataChunk &updates) {
679: 	auto first_id = FlatVector::GetValue<row_t>(row_ids, 0);
680: 	if (first_id >= MAX_ROW_ID) {
681: 		throw NotImplementedException("Cannot update a column-path on transaction local data");
682: 	}
683: 	// find the row_group this id belongs to
684: 	auto primary_column_idx = column_path[0];
685: 	auto row_group = row_groups->GetSegment(UnsafeNumericCast<idx_t>(first_id));
686: 	row_group->UpdateColumn(transaction, updates, row_ids, column_path);
687: 
688: 	auto lock = stats.GetLock();
689: 	row_group->MergeIntoStatistics(primary_column_idx, stats.GetStats(*lock, primary_column_idx).Statistics());
690: }
691: 
692: //===--------------------------------------------------------------------===//
693: // Checkpoint State
694: //===--------------------------------------------------------------------===//
695: struct CollectionCheckpointState {
696: 	CollectionCheckpointState(RowGroupCollection &collection, TableDataWriter &writer,
697: 	                          vector<SegmentNode<RowGroup>> &segments, TableStatistics &global_stats)
698: 	    : collection(collection), writer(writer), executor(writer.GetScheduler()), segments(segments),
699: 	      global_stats(global_stats) {
700: 		writers.resize(segments.size());
701: 		write_data.resize(segments.size());
702: 	}
703: 
704: 	RowGroupCollection &collection;
705: 	TableDataWriter &writer;
706: 	TaskExecutor executor;
707: 	vector<SegmentNode<RowGroup>> &segments;
708: 	vector<unique_ptr<RowGroupWriter>> writers;
709: 	vector<RowGroupWriteData> write_data;
710: 	TableStatistics &global_stats;
711: 	mutex write_lock;
712: };
713: 
714: class BaseCheckpointTask : public BaseExecutorTask {
715: public:
716: 	explicit BaseCheckpointTask(CollectionCheckpointState &checkpoint_state)
717: 	    : BaseExecutorTask(checkpoint_state.executor), checkpoint_state(checkpoint_state) {
718: 	}
719: 
720: protected:
721: 	CollectionCheckpointState &checkpoint_state;
722: };
723: 
724: class CheckpointTask : public BaseCheckpointTask {
725: public:
726: 	CheckpointTask(CollectionCheckpointState &checkpoint_state, idx_t index)
727: 	    : BaseCheckpointTask(checkpoint_state), index(index) {
728: 	}
729: 
730: 	void ExecuteTask() override {
731: 		auto &entry = checkpoint_state.segments[index];
732: 		auto &row_group = *entry.node;
733: 		checkpoint_state.writers[index] = checkpoint_state.writer.GetRowGroupWriter(*entry.node);
734: 		checkpoint_state.write_data[index] = row_group.WriteToDisk(*checkpoint_state.writers[index]);
735: 	}
736: 
737: private:
738: 	idx_t index;
739: };
740: 
741: //===--------------------------------------------------------------------===//
742: // Vacuum
743: //===--------------------------------------------------------------------===//
744: struct VacuumState {
745: 	bool can_vacuum_deletes = false;
746: 	idx_t row_start = 0;
747: 	idx_t next_vacuum_idx = 0;
748: 	vector<idx_t> row_group_counts;
749: };
750: 
751: class VacuumTask : public BaseCheckpointTask {
752: public:
753: 	VacuumTask(CollectionCheckpointState &checkpoint_state, VacuumState &vacuum_state, idx_t segment_idx,
754: 	           idx_t merge_count, idx_t target_count, idx_t merge_rows, idx_t row_start)
755: 	    : BaseCheckpointTask(checkpoint_state), vacuum_state(vacuum_state), segment_idx(segment_idx),
756: 	      merge_count(merge_count), target_count(target_count), merge_rows(merge_rows), row_start(row_start) {
757: 	}
758: 
759: 	void ExecuteTask() override {
760: 		auto &collection = checkpoint_state.collection;
761: 		auto &types = collection.GetTypes();
762: 		// create the new set of target row groups (initially empty)
763: 		vector<unique_ptr<RowGroup>> new_row_groups;
764: 		vector<idx_t> append_counts;
765: 		idx_t row_group_rows = merge_rows;
766: 		idx_t start = row_start;
767: 		for (idx_t target_idx = 0; target_idx < target_count; target_idx++) {
768: 			idx_t current_row_group_rows = MinValue<idx_t>(row_group_rows, Storage::ROW_GROUP_SIZE);
769: 			auto new_row_group = make_uniq<RowGroup>(collection, start, current_row_group_rows);
770: 			new_row_group->InitializeEmpty(types);
771: 			new_row_groups.push_back(std::move(new_row_group));
772: 			append_counts.push_back(0);
773: 
774: 			row_group_rows -= current_row_group_rows;
775: 			start += current_row_group_rows;
776: 		}
777: 
778: 		DataChunk scan_chunk;
779: 		scan_chunk.Initialize(Allocator::DefaultAllocator(), types);
780: 
781: 		vector<column_t> column_ids;
782: 		for (idx_t c = 0; c < types.size(); c++) {
783: 			column_ids.push_back(c);
784: 		}
785: 
786: 		idx_t current_append_idx = 0;
787: 
788: 		// fill the new row group with the merged rows
789: 		TableAppendState append_state;
790: 		new_row_groups[current_append_idx]->InitializeAppend(append_state.row_group_append_state);
791: 
792: 		TableScanState scan_state;
793: 		scan_state.Initialize(column_ids);
794: 		scan_state.table_state.Initialize(types);
795: 		scan_state.table_state.max_row = idx_t(-1);
796: 		idx_t merged_groups = 0;
797: 		idx_t total_row_groups = vacuum_state.row_group_counts.size();
798: 		for (idx_t c_idx = segment_idx; merged_groups < merge_count && c_idx < total_row_groups; c_idx++) {
799: 			if (vacuum_state.row_group_counts[c_idx] == 0) {
800: 				continue;
801: 			}
802: 			merged_groups++;
803: 
804: 			auto &current_row_group = *checkpoint_state.segments[c_idx].node;
805: 
806: 			current_row_group.InitializeScan(scan_state.table_state);
807: 			while (true) {
808: 				scan_chunk.Reset();
809: 
810: 				current_row_group.ScanCommitted(scan_state.table_state, scan_chunk,
811: 				                                TableScanType::TABLE_SCAN_LATEST_COMMITTED_ROWS);
812: 				if (scan_chunk.size() == 0) {
813: 					break;
814: 				}
815: 				scan_chunk.Flatten();
816: 				idx_t remaining = scan_chunk.size();
817: 				while (remaining > 0) {
818: 					idx_t append_count =
819: 					    MinValue<idx_t>(remaining, Storage::ROW_GROUP_SIZE - append_counts[current_append_idx]);
820: 					new_row_groups[current_append_idx]->Append(append_state.row_group_append_state, scan_chunk,
821: 					                                           append_count);
822: 					append_counts[current_append_idx] += append_count;
823: 					remaining -= append_count;
824: 					const bool row_group_full = append_counts[current_append_idx] == Storage::ROW_GROUP_SIZE;
825: 					const bool last_row_group = current_append_idx + 1 >= new_row_groups.size();
826: 					if (remaining > 0 || (row_group_full && !last_row_group)) {
827: 						// move to the next row group
828: 						current_append_idx++;
829: 						new_row_groups[current_append_idx]->InitializeAppend(append_state.row_group_append_state);
830: 						// slice chunk for the next append
831: 						scan_chunk.Slice(append_count, remaining);
832: 					}
833: 				}
834: 			}
835: 			// drop the row group after merging
836: 			current_row_group.CommitDrop();
837: 			checkpoint_state.segments[c_idx].node.reset();
838: 		}
839: 		idx_t total_append_count = 0;
840: 		for (idx_t target_idx = 0; target_idx < target_count; target_idx++) {
841: 			auto &row_group = new_row_groups[target_idx];
842: 			row_group->Verify();
843: 
844: 			// assign the new row group to the current segment
845: 			checkpoint_state.segments[segment_idx + target_idx].node = std::move(row_group);
846: 			total_append_count += append_counts[target_idx];
847: 		}
848: 		if (total_append_count != merge_rows) {
849: 			throw InternalException("Mismatch in row group count vs verify count in RowGroupCollection::Checkpoint");
850: 		}
851: 		// merging is complete - execute checkpoint tasks of the target row groups
852: 		for (idx_t i = 0; i < target_count; i++) {
853: 			auto checkpoint_task = collection.GetCheckpointTask(checkpoint_state, segment_idx + i);
854: 			checkpoint_task->ExecuteTask();
855: 		}
856: 	}
857: 
858: private:
859: 	VacuumState &vacuum_state;
860: 	idx_t segment_idx;
861: 	idx_t merge_count;
862: 	idx_t target_count;
863: 	idx_t merge_rows;
864: 	idx_t row_start;
865: };
866: 
867: void RowGroupCollection::InitializeVacuumState(CollectionCheckpointState &checkpoint_state, VacuumState &state,
868:                                                vector<SegmentNode<RowGroup>> &segments) {
869: 	bool is_full_checkpoint = checkpoint_state.writer.GetCheckpointType() == CheckpointType::FULL_CHECKPOINT;
870: 	// currently we can only vacuum deletes if we are doing a full checkpoint and there are no indexes
871: 	state.can_vacuum_deletes = info->GetIndexes().Empty() && is_full_checkpoint;
872: 	if (!state.can_vacuum_deletes) {
873: 		return;
874: 	}
875: 	// obtain the set of committed row counts for each row group
876: 	state.row_group_counts.reserve(segments.size());
877: 	for (auto &entry : segments) {
878: 		auto &row_group = *entry.node;
879: 		auto row_group_count = row_group.GetCommittedRowCount();
880: 		if (row_group_count == 0) {
881: 			// empty row group - we can drop it entirely
882: 			row_group.CommitDrop();
883: 			entry.node.reset();
884: 		}
885: 		state.row_group_counts.push_back(row_group_count);
886: 	}
887: }
888: 
889: bool RowGroupCollection::ScheduleVacuumTasks(CollectionCheckpointState &checkpoint_state, VacuumState &state,
890:                                              idx_t segment_idx, bool schedule_vacuum) {
891: 	static constexpr const idx_t MAX_MERGE_COUNT = 3;
892: 
893: 	if (!state.can_vacuum_deletes) {
894: 		// we cannot vacuum deletes - cannot vacuum
895: 		return false;
896: 	}
897: 	if (segment_idx < state.next_vacuum_idx) {
898: 		// this segment is being vacuumed by a previously scheduled task
899: 		return true;
900: 	}
901: 	if (state.row_group_counts[segment_idx] == 0) {
902: 		// segment was already dropped - skip
903: 		D_ASSERT(!checkpoint_state.segments[segment_idx].node);
904: 		return false;
905: 	}
906: 	if (!schedule_vacuum) {
907: 		return false;
908: 	}
909: 	idx_t merge_rows;
910: 	idx_t next_idx = 0;
911: 	idx_t merge_count;
912: 	idx_t target_count;
913: 	bool perform_merge = false;
914: 	// check if we can merge row groups adjacent to the current segment_idx
915: 	// we try merging row groups into batches of 1-3 row groups
916: 	// our goal is to reduce the amount of row groups
917: 	// hence we target_count should be less than merge_count for a marge to be worth it
918: 	// we greedily prefer to merge to the lowest target_count
919: 	// i.e. we prefer to merge 2 row groups into 1, than 3 row groups into 2
920: 	for (target_count = 1; target_count <= MAX_MERGE_COUNT; target_count++) {
921: 		auto total_target_size = target_count * Storage::ROW_GROUP_SIZE;
922: 		merge_count = 0;
923: 		merge_rows = 0;
924: 		for (next_idx = segment_idx; next_idx < checkpoint_state.segments.size(); next_idx++) {
925: 			if (state.row_group_counts[next_idx] == 0) {
926: 				continue;
927: 			}
928: 			if (merge_rows + state.row_group_counts[next_idx] > total_target_size) {
929: 				// does not fit
930: 				break;
931: 			}
932: 			// we can merge this row group together with the other row group
933: 			merge_rows += state.row_group_counts[next_idx];
934: 			merge_count++;
935: 		}
936: 		if (target_count < merge_count) {
937: 			// we can reduce "merge_count" row groups to "target_count"
938: 			// perform the merge at this level
939: 			perform_merge = true;
940: 			break;
941: 		}
942: 	}
943: 	if (!perform_merge) {
944: 		return false;
945: 	}
946: 	// schedule the vacuum task
947: 	auto vacuum_task = make_uniq<VacuumTask>(checkpoint_state, state, segment_idx, merge_count, target_count,
948: 	                                         merge_rows, state.row_start);
949: 	checkpoint_state.executor.ScheduleTask(std::move(vacuum_task));
950: 	// skip vacuuming by the row groups we have merged
951: 	state.next_vacuum_idx = next_idx;
952: 	state.row_start += merge_rows;
953: 	return true;
954: }
955: 
956: //===--------------------------------------------------------------------===//
957: // Checkpoint
958: //===--------------------------------------------------------------------===//
959: unique_ptr<CheckpointTask> RowGroupCollection::GetCheckpointTask(CollectionCheckpointState &checkpoint_state,
960:                                                                  idx_t segment_idx) {
961: 	return make_uniq<CheckpointTask>(checkpoint_state, segment_idx);
962: }
963: 
964: void RowGroupCollection::Checkpoint(TableDataWriter &writer, TableStatistics &global_stats) {
965: 	auto segments = row_groups->MoveSegments();
966: 	auto l = row_groups->Lock();
967: 
968: 	CollectionCheckpointState checkpoint_state(*this, writer, segments, global_stats);
969: 
970: 	VacuumState vacuum_state;
971: 	InitializeVacuumState(checkpoint_state, vacuum_state, segments);
972: 	// schedule tasks
973: 	idx_t total_vacuum_tasks = 0;
974: 	auto &config = DBConfig::GetConfig(writer.GetDatabase());
975: 	for (idx_t segment_idx = 0; segment_idx < segments.size(); segment_idx++) {
976: 		auto &entry = segments[segment_idx];
977: 		auto vacuum_tasks = ScheduleVacuumTasks(checkpoint_state, vacuum_state, segment_idx,
978: 		                                        total_vacuum_tasks < config.options.max_vacuum_tasks);
979: 		if (vacuum_tasks) {
980: 			// vacuum tasks were scheduled - don't schedule a checkpoint task yet
981: 			total_vacuum_tasks++;
982: 			continue;
983: 		}
984: 		if (!entry.node) {
985: 			// row group was vacuumed/dropped - skip
986: 			continue;
987: 		}
988: 		// schedule a checkpoint task for this row group
989: 		entry.node->MoveToCollection(*this, vacuum_state.row_start);
990: 		auto checkpoint_task = GetCheckpointTask(checkpoint_state, segment_idx);
991: 		checkpoint_state.executor.ScheduleTask(std::move(checkpoint_task));
992: 		vacuum_state.row_start += entry.node->count;
993: 	}
994: 	// all tasks have been scheduled - execute tasks until we are done
995: 	checkpoint_state.executor.WorkOnTasks();
996: 
997: 	// no errors - finalize the row groups
998: 	idx_t new_total_rows = 0;
999: 	for (idx_t segment_idx = 0; segment_idx < segments.size(); segment_idx++) {
1000: 		auto &entry = segments[segment_idx];
1001: 		if (!entry.node) {
1002: 			// row group was vacuumed/dropped - skip
1003: 			continue;
1004: 		}
1005: 		auto &row_group = *entry.node;
1006: 		auto row_group_writer = std::move(checkpoint_state.writers[segment_idx]);
1007: 		if (!row_group_writer) {
1008: 			throw InternalException("Missing row group writer for index %llu", segment_idx);
1009: 		}
1010: 		auto pointer =
1011: 		    row_group.Checkpoint(std::move(checkpoint_state.write_data[segment_idx]), *row_group_writer, global_stats);
1012: 		writer.AddRowGroup(std::move(pointer), std::move(row_group_writer));
1013: 		row_groups->AppendSegment(l, std::move(entry.node));
1014: 		new_total_rows += row_group.count;
1015: 	}
1016: 	total_rows = new_total_rows;
1017: }
1018: 
1019: //===--------------------------------------------------------------------===//
1020: // CommitDrop
1021: //===--------------------------------------------------------------------===//
1022: void RowGroupCollection::CommitDropColumn(idx_t index) {
1023: 	for (auto &row_group : row_groups->Segments()) {
1024: 		row_group.CommitDropColumn(index);
1025: 	}
1026: }
1027: 
1028: void RowGroupCollection::CommitDropTable() {
1029: 	for (auto &row_group : row_groups->Segments()) {
1030: 		row_group.CommitDrop();
1031: 	}
1032: }
1033: 
1034: //===--------------------------------------------------------------------===//
1035: // GetColumnSegmentInfo
1036: //===--------------------------------------------------------------------===//
1037: vector<ColumnSegmentInfo> RowGroupCollection::GetColumnSegmentInfo() {
1038: 	vector<ColumnSegmentInfo> result;
1039: 	for (auto &row_group : row_groups->Segments()) {
1040: 		row_group.GetColumnSegmentInfo(row_group.index, result);
1041: 	}
1042: 	return result;
1043: }
1044: 
1045: //===--------------------------------------------------------------------===//
1046: // Alter
1047: //===--------------------------------------------------------------------===//
1048: shared_ptr<RowGroupCollection> RowGroupCollection::AddColumn(ClientContext &context, ColumnDefinition &new_column,
1049:                                                              ExpressionExecutor &default_executor) {
1050: 	idx_t new_column_idx = types.size();
1051: 	auto new_types = types;
1052: 	new_types.push_back(new_column.GetType());
1053: 	auto result =
1054: 	    make_shared_ptr<RowGroupCollection>(info, block_manager, std::move(new_types), row_start, total_rows.load());
1055: 
1056: 	DataChunk dummy_chunk;
1057: 	Vector default_vector(new_column.GetType());
1058: 
1059: 	result->stats.InitializeAddColumn(stats, new_column.GetType());
1060: 	auto lock = result->stats.GetLock();
1061: 	auto &new_column_stats = result->stats.GetStats(*lock, new_column_idx);
1062: 
1063: 	// fill the column with its DEFAULT value, or NULL if none is specified
1064: 	auto new_stats = make_uniq<SegmentStatistics>(new_column.GetType());
1065: 	for (auto &current_row_group : row_groups->Segments()) {
1066: 		auto new_row_group = current_row_group.AddColumn(*result, new_column, default_executor, default_vector);
1067: 		// merge in the statistics
1068: 		new_row_group->MergeIntoStatistics(new_column_idx, new_column_stats.Statistics());
1069: 
1070: 		result->row_groups->AppendSegment(std::move(new_row_group));
1071: 	}
1072: 	return result;
1073: }
1074: 
1075: shared_ptr<RowGroupCollection> RowGroupCollection::RemoveColumn(idx_t col_idx) {
1076: 	D_ASSERT(col_idx < types.size());
1077: 	auto new_types = types;
1078: 	new_types.erase_at(col_idx);
1079: 
1080: 	auto result =
1081: 	    make_shared_ptr<RowGroupCollection>(info, block_manager, std::move(new_types), row_start, total_rows.load());
1082: 	result->stats.InitializeRemoveColumn(stats, col_idx);
1083: 
1084: 	for (auto &current_row_group : row_groups->Segments()) {
1085: 		auto new_row_group = current_row_group.RemoveColumn(*result, col_idx);
1086: 		result->row_groups->AppendSegment(std::move(new_row_group));
1087: 	}
1088: 	return result;
1089: }
1090: 
1091: shared_ptr<RowGroupCollection> RowGroupCollection::AlterType(ClientContext &context, idx_t changed_idx,
1092:                                                              const LogicalType &target_type,
1093:                                                              vector<column_t> bound_columns, Expression &cast_expr) {
1094: 	D_ASSERT(changed_idx < types.size());
1095: 	auto new_types = types;
1096: 	new_types[changed_idx] = target_type;
1097: 
1098: 	auto result =
1099: 	    make_shared_ptr<RowGroupCollection>(info, block_manager, std::move(new_types), row_start, total_rows.load());
1100: 	result->stats.InitializeAlterType(stats, changed_idx, target_type);
1101: 
1102: 	vector<LogicalType> scan_types;
1103: 	for (idx_t i = 0; i < bound_columns.size(); i++) {
1104: 		if (bound_columns[i] == COLUMN_IDENTIFIER_ROW_ID) {
1105: 			scan_types.emplace_back(LogicalType::ROW_TYPE);
1106: 		} else {
1107: 			scan_types.push_back(types[bound_columns[i]]);
1108: 		}
1109: 	}
1110: 	DataChunk scan_chunk;
1111: 	scan_chunk.Initialize(GetAllocator(), scan_types);
1112: 
1113: 	ExpressionExecutor executor(context);
1114: 	executor.AddExpression(cast_expr);
1115: 
1116: 	TableScanState scan_state;
1117: 	scan_state.Initialize(bound_columns);
1118: 	scan_state.table_state.max_row = row_start + total_rows;
1119: 
1120: 	// now alter the type of the column within all of the row_groups individually
1121: 	auto lock = result->stats.GetLock();
1122: 	auto &changed_stats = result->stats.GetStats(*lock, changed_idx);
1123: 	for (auto &current_row_group : row_groups->Segments()) {
1124: 		auto new_row_group = current_row_group.AlterType(*result, target_type, changed_idx, executor,
1125: 		                                                 scan_state.table_state, scan_chunk);
1126: 		new_row_group->MergeIntoStatistics(changed_idx, changed_stats.Statistics());
1127: 		result->row_groups->AppendSegment(std::move(new_row_group));
1128: 	}
1129: 
1130: 	return result;
1131: }
1132: 
1133: void RowGroupCollection::VerifyNewConstraint(DataTable &parent, const BoundConstraint &constraint) {
1134: 	if (total_rows == 0) {
1135: 		return;
1136: 	}
1137: 	// scan the original table, check if there's any null value
1138: 	auto &not_null_constraint = constraint.Cast<BoundNotNullConstraint>();
1139: 	vector<LogicalType> scan_types;
1140: 	auto physical_index = not_null_constraint.index.index;
1141: 	D_ASSERT(physical_index < types.size());
1142: 	scan_types.push_back(types[physical_index]);
1143: 	DataChunk scan_chunk;
1144: 	scan_chunk.Initialize(GetAllocator(), scan_types);
1145: 
1146: 	CreateIndexScanState state;
1147: 	vector<column_t> cids;
1148: 	cids.push_back(physical_index);
1149: 	// Use ScanCommitted to scan the latest committed data
1150: 	state.Initialize(cids, nullptr);
1151: 	InitializeScan(state.table_state, cids, nullptr);
1152: 	InitializeCreateIndexScan(state);
1153: 	while (true) {
1154: 		scan_chunk.Reset();
1155: 		state.table_state.ScanCommitted(scan_chunk, state.segment_lock,
1156: 		                                TableScanType::TABLE_SCAN_COMMITTED_ROWS_OMIT_PERMANENTLY_DELETED);
1157: 		if (scan_chunk.size() == 0) {
1158: 			break;
1159: 		}
1160: 		// Check constraint
1161: 		if (VectorOperations::HasNull(scan_chunk.data[0], scan_chunk.size())) {
1162: 			throw ConstraintException("NOT NULL constraint failed: %s.%s", info->GetTableName(),
1163: 			                          parent.Columns()[physical_index].GetName());
1164: 		}
1165: 	}
1166: }
1167: 
1168: //===--------------------------------------------------------------------===//
1169: // Statistics
1170: //===--------------------------------------------------------------------===//
1171: void RowGroupCollection::CopyStats(TableStatistics &other_stats) {
1172: 	stats.CopyStats(other_stats);
1173: }
1174: 
1175: unique_ptr<BaseStatistics> RowGroupCollection::CopyStats(column_t column_id) {
1176: 	return stats.CopyStats(column_id);
1177: }
1178: 
1179: void RowGroupCollection::SetDistinct(column_t column_id, unique_ptr<DistinctStatistics> distinct_stats) {
1180: 	D_ASSERT(column_id != COLUMN_IDENTIFIER_ROW_ID);
1181: 	auto stats_lock = stats.GetLock();
1182: 	stats.GetStats(*stats_lock, column_id).SetDistinct(std::move(distinct_stats));
1183: }
1184: 
1185: } // namespace duckdb
[end of src/storage/table/row_group_collection.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: