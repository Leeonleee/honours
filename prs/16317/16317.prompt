You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Parquet Write value count mismatch when writing DELTA_BINARY_PACKED
### What happens?

Writing out a Parquet file using COPY hits an assertion failure. Works in DuckDB 1.2, but not in main HEAD compiled 2025/02/18.

Data source: https://noaa-ghcn-pds.s3.amazonaws.com/index.html#parquet/by_year/

### To Reproduce

```sql
COPY (
    SELECT * FROM read_parquet('**/*.parquet', union_by_name = true)
    WHERE year
    BETWEEN 2010 AND 2015 ORDER BY element, obs_time
)
TO 'weather_v2_zstd_2025_02_18_HEAD.parquet'
(PARQUET_VERSION V2, COMPRESSION 'zstd');
```

```
INTERNAL Error:
value count mismatch when writing DELTA_BINARY_PACKED

Stack Trace:

0        duckdb::Exception::Exception(duckdb::ExceptionType, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&) + 64
1        duckdb::InternalException::InternalException(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&) + 20
2        duckdb::DbpEncoder::FinishWrite(duckdb::WriteStream&) + 128
3        duckdb::StandardColumnWriter<duckdb::string_t, duckdb::string_t, duckdb::ParquetStringOperator>::FlushPageState(duckdb::WriteStream&, duckdb::ColumnWriterPageState*) + 268
4        duckdb::PrimitiveColumnWriter::FlushPage(duckdb::PrimitiveColumnWriterState&) + 124
5        duckdb::PrimitiveColumnWriter::NextPage(duckdb::PrimitiveColumnWriterState&) + 52
6        duckdb::PrimitiveColumnWriter::Write(duckdb::ColumnWriterState&, duckdb::Vector&, unsigned long long) + 204
7        duckdb::ParquetWriter::PrepareRowGroup(duckdb::ColumnDataCollection&, duckdb::PreparedRowGroup&) + 6036
8        duckdb::ParquetWritePrepareBatch(duckdb::ClientContext&, duckdb::FunctionData&, duckdb::GlobalFunctionData&, duckdb::unique_ptr<duckdb::ColumnDataCollection, std::__1::default_delete<duckdb::ColumnDataCollection>, true>) + 144
9        duckdb::PrepareBatchTask::Execute(duckdb::PhysicalBatchCopyToFile const&, duckdb::ClientContext&, duckdb::GlobalSinkState&) + 132
10       duckdb::PhysicalBatchCopyToFile::ExecuteTask(duckdb::ClientContext&, duckdb::GlobalSinkState&) const + 232
11       duckdb::ProcessRemainingBatchesTask::ExecuteTask(duckdb::TaskExecutionMode) + 32
12       duckdb::ExecutorTask::Execute(duckdb::TaskExecutionMode) + 236
13       duckdb::TaskScheduler::ExecuteForever(std::__1::atomic<bool>*) + 612
14       void* std::__1::__thread_proxy[abi:ne180100]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) + 56
15       _pthread_start + 136
16       thread_start + 8

This error signals an assertion failure within DuckDB. This usually occurs due to unexpected conditions or errors in the program's logic.
For more information, see https://duckdb.org/docs/dev/internal_errors
```

### OS:

macOS Sequoia 15.3.1

### DuckDB Version:

main/HEAD e249a40c8be4709c6aae693bc8bf2c012cc3d6b2

### DuckDB Client:

CLI

### Hardware:

_No response_

### Full Name:

Alejandro Wainzinger

### Affiliation:

N/A

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a source build

### Did you include all relevant data sets for reproducing the issue?

No - I cannot easily share my data sets due to their large size

### Did you include all code required to reproduce the issue?

- [x] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [x] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdb.org/docs/api/r#duckplyr-dplyr-api).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://duckdb.org/docs/installation/) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of extension/parquet/column_writer.cpp]
1: #include "column_writer.hpp"
2: 
3: #include "duckdb.hpp"
4: #include "geo_parquet.hpp"
5: #include "parquet_dbp_encoder.hpp"
6: #include "parquet_dlba_encoder.hpp"
7: #include "parquet_rle_bp_decoder.hpp"
8: #include "parquet_rle_bp_encoder.hpp"
9: #include "parquet_bss_encoder.hpp"
10: #include "parquet_statistics.hpp"
11: #include "parquet_writer.hpp"
12: #ifndef DUCKDB_AMALGAMATION
13: #include "duckdb/common/exception.hpp"
14: #include "duckdb/common/operator/comparison_operators.hpp"
15: #include "duckdb/common/serializer/buffered_file_writer.hpp"
16: #include "duckdb/common/serializer/memory_stream.hpp"
17: #include "duckdb/common/serializer/write_stream.hpp"
18: #include "duckdb/common/string_map_set.hpp"
19: #include "duckdb/common/types/hugeint.hpp"
20: #include "duckdb/common/types/time.hpp"
21: #include "duckdb/common/types/timestamp.hpp"
22: #include "duckdb/common/types/uhugeint.hpp"
23: #include "duckdb/execution/expression_executor.hpp"
24: #endif
25: 
26: #include "brotli/encode.h"
27: #include "lz4.hpp"
28: #include "miniz_wrapper.hpp"
29: #include "snappy.h"
30: #include "zstd.h"
31: #include "zstd/common/xxhash.hpp"
32: 
33: #include <cmath>
34: 
35: namespace duckdb {
36: 
37: using namespace duckdb_parquet; // NOLINT
38: using namespace duckdb_miniz;   // NOLINT
39: 
40: using duckdb_parquet::CompressionCodec;
41: using duckdb_parquet::ConvertedType;
42: using duckdb_parquet::Encoding;
43: using duckdb_parquet::FieldRepetitionType;
44: using duckdb_parquet::FileMetaData;
45: using duckdb_parquet::PageHeader;
46: using duckdb_parquet::PageType;
47: using ParquetRowGroup = duckdb_parquet::RowGroup;
48: using duckdb_parquet::Type;
49: 
50: #define PARQUET_DEFINE_VALID 65535
51: 
52: //===--------------------------------------------------------------------===//
53: // ColumnWriterStatistics
54: //===--------------------------------------------------------------------===//
55: ColumnWriterStatistics::~ColumnWriterStatistics() {
56: }
57: 
58: bool ColumnWriterStatistics::HasStats() {
59: 	return false;
60: }
61: 
62: string ColumnWriterStatistics::GetMin() {
63: 	return string();
64: }
65: 
66: string ColumnWriterStatistics::GetMax() {
67: 	return string();
68: }
69: 
70: string ColumnWriterStatistics::GetMinValue() {
71: 	return string();
72: }
73: 
74: string ColumnWriterStatistics::GetMaxValue() {
75: 	return string();
76: }
77: 
78: //===--------------------------------------------------------------------===//
79: // RleBpEncoder
80: //===--------------------------------------------------------------------===//
81: RleBpEncoder::RleBpEncoder(uint32_t bit_width)
82:     : byte_width((bit_width + 7) / 8), byte_count(idx_t(-1)), run_count(idx_t(-1)) {
83: }
84: 
85: // we always RLE everything (for now)
86: void RleBpEncoder::BeginPrepare(uint32_t first_value) {
87: 	byte_count = 0;
88: 	run_count = 1;
89: 	current_run_count = 1;
90: 	last_value = first_value;
91: }
92: 
93: void RleBpEncoder::FinishRun() {
94: 	// last value, or value has changed
95: 	// write out the current run
96: 	byte_count += ParquetDecodeUtils::GetVarintSize(current_run_count << 1) + byte_width;
97: 	current_run_count = 1;
98: 	run_count++;
99: }
100: 
101: void RleBpEncoder::PrepareValue(uint32_t value) {
102: 	if (value != last_value) {
103: 		FinishRun();
104: 		last_value = value;
105: 	} else {
106: 		current_run_count++;
107: 	}
108: }
109: 
110: void RleBpEncoder::FinishPrepare() {
111: 	FinishRun();
112: }
113: 
114: idx_t RleBpEncoder::GetByteCount() {
115: 	D_ASSERT(byte_count != idx_t(-1));
116: 	return byte_count;
117: }
118: 
119: void RleBpEncoder::BeginWrite(WriteStream &writer, uint32_t first_value) {
120: 	// start the RLE runs
121: 	last_value = first_value;
122: 	current_run_count = 1;
123: }
124: 
125: void RleBpEncoder::WriteRun(WriteStream &writer) {
126: 	// write the header of the run
127: 	ParquetDecodeUtils::VarintEncode(current_run_count << 1, writer);
128: 	// now write the value
129: 	D_ASSERT(last_value >> (byte_width * 8) == 0);
130: 	switch (byte_width) {
131: 	case 1:
132: 		writer.Write<uint8_t>(last_value);
133: 		break;
134: 	case 2:
135: 		writer.Write<uint16_t>(last_value);
136: 		break;
137: 	case 3:
138: 		writer.Write<uint8_t>(last_value & 0xFF);
139: 		writer.Write<uint8_t>((last_value >> 8) & 0xFF);
140: 		writer.Write<uint8_t>((last_value >> 16) & 0xFF);
141: 		break;
142: 	case 4:
143: 		writer.Write<uint32_t>(last_value);
144: 		break;
145: 	default:
146: 		throw InternalException("unsupported byte width for RLE encoding");
147: 	}
148: 	current_run_count = 1;
149: }
150: 
151: void RleBpEncoder::WriteValue(WriteStream &writer, uint32_t value) {
152: 	if (value != last_value) {
153: 		WriteRun(writer);
154: 		last_value = value;
155: 	} else {
156: 		current_run_count++;
157: 	}
158: }
159: 
160: void RleBpEncoder::FinishWrite(WriteStream &writer) {
161: 	WriteRun(writer);
162: }
163: 
164: //===--------------------------------------------------------------------===//
165: // ColumnWriter
166: //===--------------------------------------------------------------------===//
167: ColumnWriter::ColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, idx_t max_repeat,
168:                            idx_t max_define, bool can_have_nulls)
169:     : writer(writer), schema_idx(schema_idx), schema_path(std::move(schema_path_p)), max_repeat(max_repeat),
170:       max_define(max_define), can_have_nulls(can_have_nulls) {
171: }
172: ColumnWriter::~ColumnWriter() {
173: }
174: 
175: ColumnWriterState::~ColumnWriterState() {
176: }
177: 
178: void ColumnWriter::CompressPage(MemoryStream &temp_writer, size_t &compressed_size, data_ptr_t &compressed_data,
179:                                 unique_ptr<data_t[]> &compressed_buf) {
180: 	switch (writer.GetCodec()) {
181: 	case CompressionCodec::UNCOMPRESSED:
182: 		compressed_size = temp_writer.GetPosition();
183: 		compressed_data = temp_writer.GetData();
184: 		break;
185: 
186: 	case CompressionCodec::SNAPPY: {
187: 		compressed_size = duckdb_snappy::MaxCompressedLength(temp_writer.GetPosition());
188: 		compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
189: 		duckdb_snappy::RawCompress(const_char_ptr_cast(temp_writer.GetData()), temp_writer.GetPosition(),
190: 		                           char_ptr_cast(compressed_buf.get()), &compressed_size);
191: 		compressed_data = compressed_buf.get();
192: 		D_ASSERT(compressed_size <= duckdb_snappy::MaxCompressedLength(temp_writer.GetPosition()));
193: 		break;
194: 	}
195: 	case CompressionCodec::LZ4_RAW: {
196: 		compressed_size = duckdb_lz4::LZ4_compressBound(UnsafeNumericCast<int32_t>(temp_writer.GetPosition()));
197: 		compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
198: 		compressed_size = duckdb_lz4::LZ4_compress_default(
199: 		    const_char_ptr_cast(temp_writer.GetData()), char_ptr_cast(compressed_buf.get()),
200: 		    UnsafeNumericCast<int32_t>(temp_writer.GetPosition()), UnsafeNumericCast<int32_t>(compressed_size));
201: 		compressed_data = compressed_buf.get();
202: 		break;
203: 	}
204: 	case CompressionCodec::GZIP: {
205: 		MiniZStream s;
206: 		compressed_size = s.MaxCompressedLength(temp_writer.GetPosition());
207: 		compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
208: 		s.Compress(const_char_ptr_cast(temp_writer.GetData()), temp_writer.GetPosition(),
209: 		           char_ptr_cast(compressed_buf.get()), &compressed_size);
210: 		compressed_data = compressed_buf.get();
211: 		break;
212: 	}
213: 	case CompressionCodec::ZSTD: {
214: 		compressed_size = duckdb_zstd::ZSTD_compressBound(temp_writer.GetPosition());
215: 		compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
216: 		compressed_size = duckdb_zstd::ZSTD_compress((void *)compressed_buf.get(), compressed_size,
217: 		                                             (const void *)temp_writer.GetData(), temp_writer.GetPosition(),
218: 		                                             UnsafeNumericCast<int32_t>(writer.CompressionLevel()));
219: 		compressed_data = compressed_buf.get();
220: 		break;
221: 	}
222: 	case CompressionCodec::BROTLI: {
223: 
224: 		compressed_size = duckdb_brotli::BrotliEncoderMaxCompressedSize(temp_writer.GetPosition());
225: 		compressed_buf = unique_ptr<data_t[]>(new data_t[compressed_size]);
226: 
227: 		duckdb_brotli::BrotliEncoderCompress(BROTLI_DEFAULT_QUALITY, BROTLI_DEFAULT_WINDOW, BROTLI_DEFAULT_MODE,
228: 		                                     temp_writer.GetPosition(), temp_writer.GetData(), &compressed_size,
229: 		                                     compressed_buf.get());
230: 		compressed_data = compressed_buf.get();
231: 
232: 		break;
233: 	}
234: 	default:
235: 		throw InternalException("Unsupported codec for Parquet Writer");
236: 	}
237: 
238: 	if (compressed_size > idx_t(NumericLimits<int32_t>::Maximum())) {
239: 		throw InternalException("Parquet writer: %d compressed page size out of range for type integer",
240: 		                        temp_writer.GetPosition());
241: 	}
242: }
243: 
244: void ColumnWriter::HandleRepeatLevels(ColumnWriterState &state, ColumnWriterState *parent, idx_t count,
245:                                       idx_t max_repeat) const {
246: 	if (!parent) {
247: 		// no repeat levels without a parent node
248: 		return;
249: 	}
250: 	while (state.repetition_levels.size() < parent->repetition_levels.size()) {
251: 		state.repetition_levels.push_back(parent->repetition_levels[state.repetition_levels.size()]);
252: 	}
253: }
254: 
255: void ColumnWriter::HandleDefineLevels(ColumnWriterState &state, ColumnWriterState *parent, const ValidityMask &validity,
256:                                       const idx_t count, const uint16_t define_value, const uint16_t null_value) const {
257: 	if (parent) {
258: 		// parent node: inherit definition level from the parent
259: 		idx_t vector_index = 0;
260: 		while (state.definition_levels.size() < parent->definition_levels.size()) {
261: 			idx_t current_index = state.definition_levels.size();
262: 			if (parent->definition_levels[current_index] != PARQUET_DEFINE_VALID) {
263: 				state.definition_levels.push_back(parent->definition_levels[current_index]);
264: 			} else if (validity.RowIsValid(vector_index)) {
265: 				state.definition_levels.push_back(define_value);
266: 			} else {
267: 				if (!can_have_nulls) {
268: 					throw IOException("Parquet writer: map key column is not allowed to contain NULL values");
269: 				}
270: 				state.null_count++;
271: 				state.definition_levels.push_back(null_value);
272: 			}
273: 			if (parent->is_empty.empty() || !parent->is_empty[current_index]) {
274: 				vector_index++;
275: 			}
276: 		}
277: 	} else {
278: 		// no parent: set definition levels only from this validity mask
279: 		for (idx_t i = 0; i < count; i++) {
280: 			const auto is_null = !validity.RowIsValid(i);
281: 			state.definition_levels.emplace_back(is_null ? null_value : define_value);
282: 			state.null_count += is_null;
283: 		}
284: 		if (!can_have_nulls && state.null_count != 0) {
285: 			throw IOException("Parquet writer: map key column is not allowed to contain NULL values");
286: 		}
287: 	}
288: }
289: 
290: class ColumnWriterPageState {
291: public:
292: 	virtual ~ColumnWriterPageState() {
293: 	}
294: 
295: public:
296: 	template <class TARGET>
297: 	TARGET &Cast() {
298: 		DynamicCastCheck<TARGET>(this);
299: 		return reinterpret_cast<TARGET &>(*this);
300: 	}
301: 	template <class TARGET>
302: 	const TARGET &Cast() const {
303: 		D_ASSERT(dynamic_cast<const TARGET *>(this));
304: 		return reinterpret_cast<const TARGET &>(*this);
305: 	}
306: };
307: 
308: struct PageInformation {
309: 	idx_t offset = 0;
310: 	idx_t row_count = 0;
311: 	idx_t empty_count = 0;
312: 	idx_t estimated_page_size = 0;
313: };
314: 
315: struct PageWriteInformation {
316: 	PageHeader page_header;
317: 	unique_ptr<MemoryStream> temp_writer;
318: 	unique_ptr<ColumnWriterPageState> page_state;
319: 	idx_t write_page_idx = 0;
320: 	idx_t write_count = 0;
321: 	idx_t max_write_count = 0;
322: 	size_t compressed_size;
323: 	data_ptr_t compressed_data;
324: 	unique_ptr<data_t[]> compressed_buf;
325: };
326: 
327: class BasicColumnWriterState : public ColumnWriterState {
328: public:
329: 	BasicColumnWriterState(duckdb_parquet::RowGroup &row_group, idx_t col_idx)
330: 	    : row_group(row_group), col_idx(col_idx) {
331: 		page_info.emplace_back();
332: 	}
333: 	~BasicColumnWriterState() override = default;
334: 
335: 	duckdb_parquet::RowGroup &row_group;
336: 	idx_t col_idx;
337: 	vector<PageInformation> page_info;
338: 	vector<PageWriteInformation> write_info;
339: 	unique_ptr<ColumnWriterStatistics> stats_state;
340: 	idx_t current_page = 0;
341: 
342: 	unique_ptr<ParquetBloomFilter> bloom_filter;
343: };
344: 
345: //===--------------------------------------------------------------------===//
346: // BasicColumnWriter
347: // A base class for writing all non-compound types (ex. numerics, strings)
348: //===--------------------------------------------------------------------===//
349: class BasicColumnWriter : public ColumnWriter {
350: public:
351: 	BasicColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path, idx_t max_repeat,
352: 	                  idx_t max_define, bool can_have_nulls)
353: 	    : ColumnWriter(writer, schema_idx, std::move(schema_path), max_repeat, max_define, can_have_nulls) {
354: 	}
355: 
356: 	~BasicColumnWriter() override = default;
357: 
358: 	//! We limit the uncompressed page size to 100MB
359: 	//! The max size in Parquet is 2GB, but we choose a more conservative limit
360: 	static constexpr const idx_t MAX_UNCOMPRESSED_PAGE_SIZE = 100000000;
361: 	//! Dictionary pages must be below 2GB. Unlike data pages, there's only one dictionary page.
362: 	//! For this reason we go with a much higher, but still a conservative upper bound of 1GB;
363: 	static constexpr const idx_t MAX_UNCOMPRESSED_DICT_PAGE_SIZE = 1e9;
364: 	//! If the dictionary has this many entries, we stop creating the dictionary
365: 	static constexpr const idx_t DICTIONARY_ANALYZE_THRESHOLD = 1e4;
366: 	//! The maximum size a key entry in an RLE page takes
367: 	static constexpr const idx_t MAX_DICTIONARY_KEY_SIZE = sizeof(uint32_t);
368: 	//! The size of encoding the string length
369: 	static constexpr const idx_t STRING_LENGTH_SIZE = sizeof(uint32_t);
370: 
371: public:
372: 	unique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::RowGroup &row_group) override;
373: 	void Prepare(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;
374: 	void BeginWrite(ColumnWriterState &state) override;
375: 	void Write(ColumnWriterState &state, Vector &vector, idx_t count) override;
376: 	void FinalizeWrite(ColumnWriterState &state) override;
377: 
378: protected:
379: 	static void WriteLevels(WriteStream &temp_writer, const unsafe_vector<uint16_t> &levels, idx_t max_value,
380: 	                        idx_t start_offset, idx_t count);
381: 
382: 	virtual duckdb_parquet::Encoding::type GetEncoding(BasicColumnWriterState &state);
383: 
384: 	void NextPage(BasicColumnWriterState &state);
385: 	void FlushPage(BasicColumnWriterState &state);
386: 
387: 	//! Initializes the state used to track statistics during writing. Only used for scalar types.
388: 	virtual unique_ptr<ColumnWriterStatistics> InitializeStatsState();
389: 
390: 	//! Initialize the writer for a specific page. Only used for scalar types.
391: 	virtual unique_ptr<ColumnWriterPageState> InitializePageState(BasicColumnWriterState &state, idx_t page_idx);
392: 
393: 	//! Flushes the writer for a specific page. Only used for scalar types.
394: 	virtual void FlushPageState(WriteStream &temp_writer, ColumnWriterPageState *state);
395: 
396: 	//! Retrieves the row size of a vector at the specified location. Only used for scalar types.
397: 	virtual idx_t GetRowSize(const Vector &vector, const idx_t index, const BasicColumnWriterState &state) const;
398: 	//! Writes a (subset of a) vector to the specified serializer. Only used for scalar types.
399: 	virtual void WriteVector(WriteStream &temp_writer, ColumnWriterStatistics *stats, ColumnWriterPageState *page_state,
400: 	                         Vector &vector, idx_t chunk_start, idx_t chunk_end) = 0;
401: 
402: 	virtual bool HasDictionary(BasicColumnWriterState &state_p) {
403: 		return false;
404: 	}
405: 	//! The number of elements in the dictionary
406: 	virtual idx_t DictionarySize(BasicColumnWriterState &state_p);
407: 	void WriteDictionary(BasicColumnWriterState &state, unique_ptr<MemoryStream> temp_writer, idx_t row_count);
408: 	virtual void FlushDictionary(BasicColumnWriterState &state, ColumnWriterStatistics *stats);
409: 
410: 	void SetParquetStatistics(BasicColumnWriterState &state, duckdb_parquet::ColumnChunk &column);
411: 	void RegisterToRowGroup(duckdb_parquet::RowGroup &row_group);
412: };
413: 
414: unique_ptr<ColumnWriterState> BasicColumnWriter::InitializeWriteState(duckdb_parquet::RowGroup &row_group) {
415: 	auto result = make_uniq<BasicColumnWriterState>(row_group, row_group.columns.size());
416: 	RegisterToRowGroup(row_group);
417: 	return std::move(result);
418: }
419: 
420: void BasicColumnWriter::RegisterToRowGroup(duckdb_parquet::RowGroup &row_group) {
421: 	duckdb_parquet::ColumnChunk column_chunk;
422: 	column_chunk.__isset.meta_data = true;
423: 	column_chunk.meta_data.codec = writer.GetCodec();
424: 	column_chunk.meta_data.path_in_schema = schema_path;
425: 	column_chunk.meta_data.num_values = 0;
426: 	column_chunk.meta_data.type = writer.GetType(schema_idx);
427: 	row_group.columns.push_back(std::move(column_chunk));
428: }
429: 
430: unique_ptr<ColumnWriterPageState> BasicColumnWriter::InitializePageState(BasicColumnWriterState &state,
431:                                                                          idx_t page_idx) {
432: 	return nullptr;
433: }
434: 
435: void BasicColumnWriter::FlushPageState(WriteStream &temp_writer, ColumnWriterPageState *state) {
436: }
437: 
438: void BasicColumnWriter::Prepare(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {
439: 	auto &state = state_p.Cast<BasicColumnWriterState>();
440: 	auto &col_chunk = state.row_group.columns[state.col_idx];
441: 
442: 	idx_t start = 0;
443: 	idx_t vcount = parent ? parent->definition_levels.size() - state.definition_levels.size() : count;
444: 	idx_t parent_index = state.definition_levels.size();
445: 	auto &validity = FlatVector::Validity(vector);
446: 	HandleRepeatLevels(state, parent, count, max_repeat);
447: 	HandleDefineLevels(state, parent, validity, count, max_define, max_define - 1);
448: 
449: 	idx_t vector_index = 0;
450: 	reference<PageInformation> page_info_ref = state.page_info.back();
451: 	for (idx_t i = start; i < vcount; i++) {
452: 		auto &page_info = page_info_ref.get();
453: 		page_info.row_count++;
454: 		col_chunk.meta_data.num_values++;
455: 		if (parent && !parent->is_empty.empty() && parent->is_empty[parent_index + i]) {
456: 			page_info.empty_count++;
457: 			continue;
458: 		}
459: 		if (validity.RowIsValid(vector_index)) {
460: 			page_info.estimated_page_size += GetRowSize(vector, vector_index, state);
461: 			if (page_info.estimated_page_size >= MAX_UNCOMPRESSED_PAGE_SIZE) {
462: 				PageInformation new_info;
463: 				new_info.offset = page_info.offset + page_info.row_count;
464: 				state.page_info.push_back(new_info);
465: 				page_info_ref = state.page_info.back();
466: 			}
467: 		}
468: 		vector_index++;
469: 	}
470: }
471: 
472: duckdb_parquet::Encoding::type BasicColumnWriter::GetEncoding(BasicColumnWriterState &state) {
473: 	return Encoding::PLAIN;
474: }
475: 
476: void BasicColumnWriter::BeginWrite(ColumnWriterState &state_p) {
477: 	auto &state = state_p.Cast<BasicColumnWriterState>();
478: 
479: 	// set up the page write info
480: 	state.stats_state = InitializeStatsState();
481: 	for (idx_t page_idx = 0; page_idx < state.page_info.size(); page_idx++) {
482: 		auto &page_info = state.page_info[page_idx];
483: 		if (page_info.row_count == 0) {
484: 			D_ASSERT(page_idx + 1 == state.page_info.size());
485: 			state.page_info.erase_at(page_idx);
486: 			break;
487: 		}
488: 		PageWriteInformation write_info;
489: 		// set up the header
490: 		auto &hdr = write_info.page_header;
491: 		hdr.compressed_page_size = 0;
492: 		hdr.uncompressed_page_size = 0;
493: 		hdr.type = PageType::DATA_PAGE;
494: 		hdr.__isset.data_page_header = true;
495: 
496: 		hdr.data_page_header.num_values = UnsafeNumericCast<int32_t>(page_info.row_count);
497: 		hdr.data_page_header.encoding = GetEncoding(state);
498: 		hdr.data_page_header.definition_level_encoding = Encoding::RLE;
499: 		hdr.data_page_header.repetition_level_encoding = Encoding::RLE;
500: 
501: 		write_info.temp_writer = make_uniq<MemoryStream>(
502: 		    Allocator::Get(writer.GetContext()),
503: 		    MaxValue<idx_t>(NextPowerOfTwo(page_info.estimated_page_size), MemoryStream::DEFAULT_INITIAL_CAPACITY));
504: 		write_info.write_count = page_info.empty_count;
505: 		write_info.max_write_count = page_info.row_count;
506: 		write_info.page_state = InitializePageState(state, page_idx);
507: 
508: 		write_info.compressed_size = 0;
509: 		write_info.compressed_data = nullptr;
510: 
511: 		state.write_info.push_back(std::move(write_info));
512: 	}
513: 
514: 	// start writing the first page
515: 	NextPage(state);
516: }
517: 
518: void BasicColumnWriter::WriteLevels(WriteStream &temp_writer, const unsafe_vector<uint16_t> &levels, idx_t max_value,
519:                                     idx_t offset, idx_t count) {
520: 	if (levels.empty() || count == 0) {
521: 		return;
522: 	}
523: 
524: 	// write the levels using the RLE-BP encoding
525: 	auto bit_width = RleBpDecoder::ComputeBitWidth((max_value));
526: 	RleBpEncoder rle_encoder(bit_width);
527: 
528: 	rle_encoder.BeginPrepare(levels[offset]);
529: 	for (idx_t i = offset + 1; i < offset + count; i++) {
530: 		rle_encoder.PrepareValue(levels[i]);
531: 	}
532: 	rle_encoder.FinishPrepare();
533: 
534: 	// start off by writing the byte count as a uint32_t
535: 	temp_writer.Write<uint32_t>(rle_encoder.GetByteCount());
536: 	rle_encoder.BeginWrite(temp_writer, levels[offset]);
537: 	for (idx_t i = offset + 1; i < offset + count; i++) {
538: 		rle_encoder.WriteValue(temp_writer, levels[i]);
539: 	}
540: 	rle_encoder.FinishWrite(temp_writer);
541: }
542: 
543: void BasicColumnWriter::NextPage(BasicColumnWriterState &state) {
544: 	if (state.current_page > 0) {
545: 		// need to flush the current page
546: 		FlushPage(state);
547: 	}
548: 	if (state.current_page >= state.write_info.size()) {
549: 		state.current_page = state.write_info.size() + 1;
550: 		return;
551: 	}
552: 	auto &page_info = state.page_info[state.current_page];
553: 	auto &write_info = state.write_info[state.current_page];
554: 	state.current_page++;
555: 
556: 	auto &temp_writer = *write_info.temp_writer;
557: 
558: 	// write the repetition levels
559: 	WriteLevels(temp_writer, state.repetition_levels, max_repeat, page_info.offset, page_info.row_count);
560: 
561: 	// write the definition levels
562: 	WriteLevels(temp_writer, state.definition_levels, max_define, page_info.offset, page_info.row_count);
563: }
564: 
565: void BasicColumnWriter::FlushPage(BasicColumnWriterState &state) {
566: 	D_ASSERT(state.current_page > 0);
567: 	if (state.current_page > state.write_info.size()) {
568: 		return;
569: 	}
570: 
571: 	// compress the page info
572: 	auto &write_info = state.write_info[state.current_page - 1];
573: 	auto &temp_writer = *write_info.temp_writer;
574: 	auto &hdr = write_info.page_header;
575: 
576: 	FlushPageState(temp_writer, write_info.page_state.get());
577: 
578: 	// now that we have finished writing the data we know the uncompressed size
579: 	if (temp_writer.GetPosition() > idx_t(NumericLimits<int32_t>::Maximum())) {
580: 		throw InternalException("Parquet writer: %d uncompressed page size out of range for type integer",
581: 		                        temp_writer.GetPosition());
582: 	}
583: 	hdr.uncompressed_page_size = UnsafeNumericCast<int32_t>(temp_writer.GetPosition());
584: 
585: 	// compress the data
586: 	CompressPage(temp_writer, write_info.compressed_size, write_info.compressed_data, write_info.compressed_buf);
587: 	hdr.compressed_page_size = UnsafeNumericCast<int32_t>(write_info.compressed_size);
588: 	D_ASSERT(hdr.uncompressed_page_size > 0);
589: 	D_ASSERT(hdr.compressed_page_size > 0);
590: 
591: 	if (write_info.compressed_buf) {
592: 		// if the data has been compressed, we no longer need the uncompressed data
593: 		D_ASSERT(write_info.compressed_buf.get() == write_info.compressed_data);
594: 		write_info.temp_writer.reset();
595: 	}
596: }
597: 
598: unique_ptr<ColumnWriterStatistics> BasicColumnWriter::InitializeStatsState() {
599: 	return make_uniq<ColumnWriterStatistics>();
600: }
601: 
602: idx_t BasicColumnWriter::GetRowSize(const Vector &vector, const idx_t index,
603:                                     const BasicColumnWriterState &state) const {
604: 	throw InternalException("GetRowSize unsupported for struct/list column writers");
605: }
606: 
607: void BasicColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t count) {
608: 	auto &state = state_p.Cast<BasicColumnWriterState>();
609: 
610: 	idx_t remaining = count;
611: 	idx_t offset = 0;
612: 	while (remaining > 0) {
613: 		auto &write_info = state.write_info[state.current_page - 1];
614: 		if (!write_info.temp_writer) {
615: 			throw InternalException("Writes are not correctly aligned!?");
616: 		}
617: 		auto &temp_writer = *write_info.temp_writer;
618: 		idx_t write_count = MinValue<idx_t>(remaining, write_info.max_write_count - write_info.write_count);
619: 		D_ASSERT(write_count > 0);
620: 
621: 		WriteVector(temp_writer, state.stats_state.get(), write_info.page_state.get(), vector, offset,
622: 		            offset + write_count);
623: 
624: 		write_info.write_count += write_count;
625: 		if (write_info.write_count == write_info.max_write_count) {
626: 			NextPage(state);
627: 		}
628: 		offset += write_count;
629: 		remaining -= write_count;
630: 	}
631: }
632: 
633: void BasicColumnWriter::SetParquetStatistics(BasicColumnWriterState &state, duckdb_parquet::ColumnChunk &column_chunk) {
634: 	if (!state.stats_state) {
635: 		return;
636: 	}
637: 	if (max_repeat == 0) {
638: 		column_chunk.meta_data.statistics.null_count = NumericCast<int64_t>(state.null_count);
639: 		column_chunk.meta_data.statistics.__isset.null_count = true;
640: 		column_chunk.meta_data.__isset.statistics = true;
641: 	}
642: 	// set min/max/min_value/max_value
643: 	// this code is not going to win any beauty contests, but well
644: 	auto min = state.stats_state->GetMin();
645: 	if (!min.empty()) {
646: 		column_chunk.meta_data.statistics.min = std::move(min);
647: 		column_chunk.meta_data.statistics.__isset.min = true;
648: 		column_chunk.meta_data.__isset.statistics = true;
649: 	}
650: 	auto max = state.stats_state->GetMax();
651: 	if (!max.empty()) {
652: 		column_chunk.meta_data.statistics.max = std::move(max);
653: 		column_chunk.meta_data.statistics.__isset.max = true;
654: 		column_chunk.meta_data.__isset.statistics = true;
655: 	}
656: 	if (state.stats_state->HasStats()) {
657: 		column_chunk.meta_data.statistics.min_value = state.stats_state->GetMinValue();
658: 		column_chunk.meta_data.statistics.__isset.min_value = true;
659: 		column_chunk.meta_data.__isset.statistics = true;
660: 
661: 		column_chunk.meta_data.statistics.max_value = state.stats_state->GetMaxValue();
662: 		column_chunk.meta_data.statistics.__isset.max_value = true;
663: 		column_chunk.meta_data.__isset.statistics = true;
664: 	}
665: 	if (HasDictionary(state)) {
666: 		column_chunk.meta_data.statistics.distinct_count = UnsafeNumericCast<int64_t>(DictionarySize(state));
667: 		column_chunk.meta_data.statistics.__isset.distinct_count = true;
668: 		column_chunk.meta_data.__isset.statistics = true;
669: 	}
670: 	for (const auto &write_info : state.write_info) {
671: 		// only care about data page encodings, data_page_header.encoding is meaningless for dict
672: 		if (write_info.page_header.type != PageType::DATA_PAGE &&
673: 		    write_info.page_header.type != PageType::DATA_PAGE_V2) {
674: 			continue;
675: 		}
676: 		column_chunk.meta_data.encodings.push_back(write_info.page_header.data_page_header.encoding);
677: 	}
678: }
679: 
680: void BasicColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {
681: 	auto &state = state_p.Cast<BasicColumnWriterState>();
682: 	auto &column_chunk = state.row_group.columns[state.col_idx];
683: 
684: 	// flush the last page (if any remains)
685: 	FlushPage(state);
686: 
687: 	auto &column_writer = writer.GetWriter();
688: 	auto start_offset = column_writer.GetTotalWritten();
689: 	// flush the dictionary
690: 	if (HasDictionary(state)) {
691: 		column_chunk.meta_data.statistics.distinct_count = UnsafeNumericCast<int64_t>(DictionarySize(state));
692: 		column_chunk.meta_data.statistics.__isset.distinct_count = true;
693: 		column_chunk.meta_data.dictionary_page_offset = UnsafeNumericCast<int64_t>(column_writer.GetTotalWritten());
694: 		column_chunk.meta_data.__isset.dictionary_page_offset = true;
695: 		FlushDictionary(state, state.stats_state.get());
696: 	}
697: 
698: 	// record the start position of the pages for this column
699: 	column_chunk.meta_data.data_page_offset = 0;
700: 	SetParquetStatistics(state, column_chunk);
701: 
702: 	// write the individual pages to disk
703: 	idx_t total_uncompressed_size = 0;
704: 	for (auto &write_info : state.write_info) {
705: 		// set the data page offset whenever we see the *first* data page
706: 		if (column_chunk.meta_data.data_page_offset == 0 && (write_info.page_header.type == PageType::DATA_PAGE ||
707: 		                                                     write_info.page_header.type == PageType::DATA_PAGE_V2)) {
708: 			column_chunk.meta_data.data_page_offset = UnsafeNumericCast<int64_t>(column_writer.GetTotalWritten());
709: 			;
710: 		}
711: 		D_ASSERT(write_info.page_header.uncompressed_page_size > 0);
712: 		auto header_start_offset = column_writer.GetTotalWritten();
713: 		writer.Write(write_info.page_header);
714: 		// total uncompressed size in the column chunk includes the header size (!)
715: 		total_uncompressed_size += column_writer.GetTotalWritten() - header_start_offset;
716: 		total_uncompressed_size += write_info.page_header.uncompressed_page_size;
717: 		writer.WriteData(write_info.compressed_data, write_info.compressed_size);
718: 	}
719: 	column_chunk.meta_data.total_compressed_size =
720: 	    UnsafeNumericCast<int64_t>(column_writer.GetTotalWritten() - start_offset);
721: 	column_chunk.meta_data.total_uncompressed_size = UnsafeNumericCast<int64_t>(total_uncompressed_size);
722: 	state.row_group.total_byte_size += column_chunk.meta_data.total_uncompressed_size;
723: 
724: 	if (state.bloom_filter) {
725: 		writer.BufferBloomFilter(state.col_idx, std::move(state.bloom_filter));
726: 	}
727: 	// which row group is this?
728: }
729: 
730: void BasicColumnWriter::FlushDictionary(BasicColumnWriterState &state, ColumnWriterStatistics *stats) {
731: 	throw InternalException("This page does not have a dictionary");
732: }
733: 
734: idx_t BasicColumnWriter::DictionarySize(BasicColumnWriterState &state) {
735: 	throw InternalException("This page does not have a dictionary");
736: }
737: 
738: void BasicColumnWriter::WriteDictionary(BasicColumnWriterState &state, unique_ptr<MemoryStream> temp_writer,
739:                                         idx_t row_count) {
740: 	D_ASSERT(temp_writer);
741: 	D_ASSERT(temp_writer->GetPosition() > 0);
742: 
743: 	// write the dictionary page header
744: 	PageWriteInformation write_info;
745: 	// set up the header
746: 	auto &hdr = write_info.page_header;
747: 	hdr.uncompressed_page_size = UnsafeNumericCast<int32_t>(temp_writer->GetPosition());
748: 	hdr.type = PageType::DICTIONARY_PAGE;
749: 	hdr.__isset.dictionary_page_header = true;
750: 
751: 	hdr.dictionary_page_header.encoding = Encoding::PLAIN;
752: 	hdr.dictionary_page_header.is_sorted = false;
753: 	hdr.dictionary_page_header.num_values = UnsafeNumericCast<int32_t>(row_count);
754: 
755: 	write_info.temp_writer = std::move(temp_writer);
756: 	write_info.write_count = 0;
757: 	write_info.max_write_count = 0;
758: 
759: 	// compress the contents of the dictionary page
760: 	CompressPage(*write_info.temp_writer, write_info.compressed_size, write_info.compressed_data,
761: 	             write_info.compressed_buf);
762: 	hdr.compressed_page_size = UnsafeNumericCast<int32_t>(write_info.compressed_size);
763: 
764: 	// insert the dictionary page as the first page to write for this column
765: 	state.write_info.insert(state.write_info.begin(), std::move(write_info));
766: }
767: 
768: //===--------------------------------------------------------------------===//
769: // Standard Column Writer
770: //===--------------------------------------------------------------------===//
771: template <class SRC, class T, class OP>
772: class NumericStatisticsState : public ColumnWriterStatistics {
773: public:
774: 	NumericStatisticsState() : min(NumericLimits<T>::Maximum()), max(NumericLimits<T>::Minimum()) {
775: 	}
776: 
777: 	T min;
778: 	T max;
779: 
780: public:
781: 	bool HasStats() override {
782: 		return min <= max;
783: 	}
784: 
785: 	string GetMin() override {
786: 		return NumericLimits<SRC>::IsSigned() ? GetMinValue() : string();
787: 	}
788: 	string GetMax() override {
789: 		return NumericLimits<SRC>::IsSigned() ? GetMaxValue() : string();
790: 	}
791: 	string GetMinValue() override {
792: 		return HasStats() ? string(char_ptr_cast(&min), sizeof(T)) : string();
793: 	}
794: 	string GetMaxValue() override {
795: 		return HasStats() ? string(char_ptr_cast(&max), sizeof(T)) : string();
796: 	}
797: };
798: 
799: struct BaseParquetOperator {
800: 	template <class SRC, class TGT>
801: 	static void WriteToStream(const TGT &input, WriteStream &ser) {
802: 		ser.WriteData(const_data_ptr_cast(&input), sizeof(TGT));
803: 	}
804: 
805: 	template <class SRC, class TGT>
806: 	static uint64_t XXHash64(const TGT &target_value) {
807: 		return duckdb_zstd::XXH64(&target_value, sizeof(target_value), 0);
808: 	}
809: 
810: 	template <class SRC, class TGT>
811: 	static unique_ptr<ColumnWriterStatistics> InitializeStats() {
812: 		return nullptr;
813: 	}
814: 
815: 	template <class SRC, class TGT>
816: 	static void HandleStats(ColumnWriterStatistics *stats, TGT target_value) {
817: 	}
818: 
819: 	template <class SRC, class TGT>
820: 	static idx_t GetRowSize(const Vector &, idx_t) {
821: 		return sizeof(TGT);
822: 	}
823: };
824: 
825: struct ParquetCastOperator : public BaseParquetOperator {
826: 	template <class SRC, class TGT>
827: 	static TGT Operation(SRC input) {
828: 		return TGT(input);
829: 	}
830: 	template <class SRC, class TGT>
831: 	static unique_ptr<ColumnWriterStatistics> InitializeStats() {
832: 		return make_uniq<NumericStatisticsState<SRC, TGT, BaseParquetOperator>>();
833: 	}
834: 
835: 	template <class SRC, class TGT>
836: 	static void HandleStats(ColumnWriterStatistics *stats, TGT target_value) {
837: 		auto &numeric_stats = (NumericStatisticsState<SRC, TGT, BaseParquetOperator> &)*stats;
838: 		if (LessThan::Operation(target_value, numeric_stats.min)) {
839: 			numeric_stats.min = target_value;
840: 		}
841: 		if (GreaterThan::Operation(target_value, numeric_stats.max)) {
842: 			numeric_stats.max = target_value;
843: 		}
844: 	}
845: };
846: 
847: struct ParquetTimestampNSOperator : public ParquetCastOperator {
848: 	template <class SRC, class TGT>
849: 	static TGT Operation(SRC input) {
850: 		return TGT(input);
851: 	}
852: };
853: 
854: struct ParquetTimestampSOperator : public ParquetCastOperator {
855: 	template <class SRC, class TGT>
856: 	static TGT Operation(SRC input) {
857: 		return Timestamp::FromEpochSecondsPossiblyInfinite(input).value;
858: 	}
859: };
860: 
861: class StringStatisticsState : public ColumnWriterStatistics {
862: 	static constexpr const idx_t MAX_STRING_STATISTICS_SIZE = 10000;
863: 
864: public:
865: 	StringStatisticsState() : has_stats(false), values_too_big(false), min(), max() {
866: 	}
867: 
868: 	bool has_stats;
869: 	bool values_too_big;
870: 	string min;
871: 	string max;
872: 
873: public:
874: 	bool HasStats() override {
875: 		return has_stats;
876: 	}
877: 
878: 	void Update(const string_t &val) {
879: 		if (values_too_big) {
880: 			return;
881: 		}
882: 		auto str_len = val.GetSize();
883: 		if (str_len > MAX_STRING_STATISTICS_SIZE) {
884: 			// we avoid gathering stats when individual string values are too large
885: 			// this is because the statistics are copied into the Parquet file meta data in uncompressed format
886: 			// ideally we avoid placing several mega or giga-byte long strings there
887: 			// we put a threshold of 10KB, if we see strings that exceed this threshold we avoid gathering stats
888: 			values_too_big = true;
889: 			has_stats = false;
890: 			min = string();
891: 			max = string();
892: 			return;
893: 		}
894: 		if (!has_stats || LessThan::Operation(val, string_t(min))) {
895: 			min = val.GetString();
896: 		}
897: 		if (!has_stats || GreaterThan::Operation(val, string_t(max))) {
898: 			max = val.GetString();
899: 		}
900: 		has_stats = true;
901: 	}
902: 
903: 	string GetMin() override {
904: 		return GetMinValue();
905: 	}
906: 	string GetMax() override {
907: 		return GetMaxValue();
908: 	}
909: 	string GetMinValue() override {
910: 		return HasStats() ? min : string();
911: 	}
912: 	string GetMaxValue() override {
913: 		return HasStats() ? max : string();
914: 	}
915: };
916: 
917: struct ParquetStringOperator : public BaseParquetOperator {
918: 	template <class SRC, class TGT>
919: 	static TGT Operation(SRC input) {
920: 		return input;
921: 	}
922: 
923: 	template <class SRC, class TGT>
924: 	static unique_ptr<ColumnWriterStatistics> InitializeStats() {
925: 		return make_uniq<StringStatisticsState>();
926: 	}
927: 
928: 	template <class SRC, class TGT>
929: 	static void HandleStats(ColumnWriterStatistics *stats, TGT target_value) {
930: 		auto &string_stats = stats->Cast<StringStatisticsState>();
931: 		string_stats.Update(target_value);
932: 	}
933: 
934: 	template <class SRC, class TGT>
935: 	static void WriteToStream(const TGT &target_value, WriteStream &ser) {
936: 		ser.Write<uint32_t>(target_value.GetSize());
937: 		ser.WriteData(const_data_ptr_cast(target_value.GetData()), target_value.GetSize());
938: 	}
939: 
940: 	template <class SRC, class TGT>
941: 	static uint64_t XXHash64(const TGT &target_value) {
942: 		return duckdb_zstd::XXH64(target_value.GetData(), target_value.GetSize(), 0);
943: 	}
944: 
945: 	template <class SRC, class TGT>
946: 	static idx_t GetRowSize(const Vector &vector, idx_t index) {
947: 		return FlatVector::GetData<string_t>(vector)[index].GetSize();
948: 	}
949: };
950: 
951: struct ParquetIntervalTargetType {
952: 	static constexpr const idx_t PARQUET_INTERVAL_SIZE = 12;
953: 	data_t bytes[PARQUET_INTERVAL_SIZE];
954: };
955: 
956: struct ParquetIntervalOperator : public BaseParquetOperator {
957: 	template <class SRC, class TGT>
958: 	static TGT Operation(SRC input) {
959: 
960: 		if (input.days < 0 || input.months < 0 || input.micros < 0) {
961: 			throw IOException("Parquet files do not support negative intervals");
962: 		}
963: 		TGT result;
964: 		Store<uint32_t>(input.months, result.bytes);
965: 		Store<uint32_t>(input.days, result.bytes + sizeof(uint32_t));
966: 		Store<uint32_t>(input.micros / 1000, result.bytes + sizeof(uint32_t) * 2);
967: 		return result;
968: 	}
969: 
970: 	template <class SRC, class TGT>
971: 	static void WriteToStream(const TGT &target_value, WriteStream &ser) {
972: 		ser.WriteData(target_value.bytes, ParquetIntervalTargetType::PARQUET_INTERVAL_SIZE);
973: 	}
974: 
975: 	template <class SRC, class TGT>
976: 	static uint64_t XXHash64(const TGT &target_value) {
977: 		return duckdb_zstd::XXH64(target_value.bytes, ParquetIntervalTargetType::PARQUET_INTERVAL_SIZE, 0);
978: 	}
979: };
980: 
981: struct ParquetUUIDTargetType {
982: 	static constexpr const idx_t PARQUET_UUID_SIZE = 16;
983: 	data_t bytes[PARQUET_UUID_SIZE];
984: };
985: 
986: struct ParquetUUIDOperator : public BaseParquetOperator {
987: 	template <class SRC, class TGT>
988: 	static TGT Operation(SRC input) {
989: 		TGT result;
990: 		uint64_t high_bytes = input.upper ^ (int64_t(1) << 63);
991: 		uint64_t low_bytes = input.lower;
992: 		for (idx_t i = 0; i < sizeof(uint64_t); i++) {
993: 			auto shift_count = (sizeof(uint64_t) - i - 1) * 8;
994: 			result.bytes[i] = (high_bytes >> shift_count) & 0xFF;
995: 		}
996: 		for (idx_t i = 0; i < sizeof(uint64_t); i++) {
997: 			auto shift_count = (sizeof(uint64_t) - i - 1) * 8;
998: 			result.bytes[sizeof(uint64_t) + i] = (low_bytes >> shift_count) & 0xFF;
999: 		}
1000: 		return result;
1001: 	}
1002: 
1003: 	template <class SRC, class TGT>
1004: 	static void WriteToStream(const TGT &target_value, WriteStream &ser) {
1005: 		ser.WriteData(target_value.bytes, ParquetUUIDTargetType::PARQUET_UUID_SIZE);
1006: 	}
1007: 
1008: 	template <class SRC, class TGT>
1009: 	static uint64_t XXHash64(const TGT &target_value) {
1010: 		return duckdb_zstd::XXH64(target_value.bytes, ParquetUUIDTargetType::PARQUET_UUID_SIZE, 0);
1011: 	}
1012: };
1013: 
1014: struct ParquetTimeTZOperator : public BaseParquetOperator {
1015: 	template <class SRC, class TGT>
1016: 	static TGT Operation(SRC input) {
1017: 		return input.time().micros;
1018: 	}
1019: };
1020: 
1021: struct ParquetHugeintOperator : public BaseParquetOperator {
1022: 	template <class SRC, class TGT>
1023: 	static TGT Operation(SRC input) {
1024: 		return Hugeint::Cast<double>(input);
1025: 	}
1026: 
1027: 	template <class SRC, class TGT>
1028: 	static unique_ptr<ColumnWriterStatistics> InitializeStats() {
1029: 		return make_uniq<ColumnWriterStatistics>();
1030: 	}
1031: 
1032: 	template <class SRC, class TGT>
1033: 	static void HandleStats(ColumnWriterStatistics *stats, TGT target_value) {
1034: 	}
1035: };
1036: 
1037: struct ParquetUhugeintOperator : public BaseParquetOperator {
1038: 	template <class SRC, class TGT>
1039: 	static TGT Operation(SRC input) {
1040: 		return Uhugeint::Cast<double>(input);
1041: 	}
1042: 
1043: 	template <class SRC, class TGT>
1044: 	static unique_ptr<ColumnWriterStatistics> InitializeStats() {
1045: 		return make_uniq<ColumnWriterStatistics>();
1046: 	}
1047: 
1048: 	template <class SRC, class TGT>
1049: 	static void HandleStats(ColumnWriterStatistics *stats, TGT target_value) {
1050: 	}
1051: };
1052: 
1053: template <class SRC, class TGT, class OP = ParquetCastOperator>
1054: static void TemplatedWritePlain(Vector &col, ColumnWriterStatistics *stats, const idx_t chunk_start,
1055:                                 const idx_t chunk_end, const ValidityMask &mask, WriteStream &ser) {
1056: 
1057: 	const auto *ptr = FlatVector::GetData<SRC>(col);
1058: 	for (idx_t r = chunk_start; r < chunk_end; r++) {
1059: 		if (!mask.RowIsValid(r)) {
1060: 			continue;
1061: 		}
1062: 		TGT target_value = OP::template Operation<SRC, TGT>(ptr[r]);
1063: 		OP::template HandleStats<SRC, TGT>(stats, target_value);
1064: 		OP::template WriteToStream<SRC, TGT>(target_value, ser);
1065: 	}
1066: }
1067: 
1068: template <class T>
1069: class StandardColumnWriterState : public BasicColumnWriterState {
1070: public:
1071: 	StandardColumnWriterState(duckdb_parquet::RowGroup &row_group, idx_t col_idx)
1072: 	    : BasicColumnWriterState(row_group, col_idx) {
1073: 	}
1074: 	~StandardColumnWriterState() override = default;
1075: 
1076: 	// analysis state for integer values for DELTA_BINARY_PACKED/DELTA_LENGTH_BYTE_ARRAY
1077: 	idx_t total_value_count = 0;
1078: 	idx_t total_string_size = 0;
1079: 	uint32_t key_bit_width = 0;
1080: 
1081: 	unordered_map<T, uint32_t> dictionary;
1082: 	duckdb_parquet::Encoding::type encoding;
1083: };
1084: 
1085: template <class SRC, class TGT>
1086: class StandardWriterPageState : public ColumnWriterPageState {
1087: public:
1088: 	explicit StandardWriterPageState(const idx_t total_value_count, const idx_t total_string_size,
1089: 	                                 Encoding::type encoding_p, const unordered_map<SRC, uint32_t> &dictionary_p)
1090: 	    : encoding(encoding_p), dbp_initialized(false), dbp_encoder(total_value_count), dlba_initialized(false),
1091: 	      dlba_encoder(total_value_count, total_string_size), bss_encoder(total_value_count, sizeof(TGT)),
1092: 	      dictionary(dictionary_p), dict_written_value(false),
1093: 	      dict_bit_width(RleBpDecoder::ComputeBitWidth(dictionary.size())), dict_encoder(dict_bit_width) {
1094: 	}
1095: 	duckdb_parquet::Encoding::type encoding;
1096: 
1097: 	bool dbp_initialized;
1098: 	DbpEncoder dbp_encoder;
1099: 
1100: 	bool dlba_initialized;
1101: 	DlbaEncoder dlba_encoder;
1102: 
1103: 	BssEncoder bss_encoder;
1104: 
1105: 	const unordered_map<SRC, uint32_t> &dictionary;
1106: 	bool dict_written_value;
1107: 	uint32_t dict_bit_width;
1108: 	RleBpEncoder dict_encoder;
1109: };
1110: 
1111: namespace dbp_encoder {
1112: 
1113: template <class T>
1114: void BeginWrite(DbpEncoder &encoder, WriteStream &writer, const T &first_value) {
1115: 	throw InternalException("Can't write type to DELTA_BINARY_PACKED column");
1116: }
1117: 
1118: template <>
1119: void BeginWrite(DbpEncoder &encoder, WriteStream &writer, const int64_t &first_value) {
1120: 	encoder.BeginWrite(writer, first_value);
1121: }
1122: 
1123: template <>
1124: void BeginWrite(DbpEncoder &encoder, WriteStream &writer, const int32_t &first_value) {
1125: 	BeginWrite(encoder, writer, UnsafeNumericCast<int64_t>(first_value));
1126: }
1127: 
1128: template <>
1129: void BeginWrite(DbpEncoder &encoder, WriteStream &writer, const uint64_t &first_value) {
1130: 	encoder.BeginWrite(writer, UnsafeNumericCast<int64_t>(first_value));
1131: }
1132: 
1133: template <>
1134: void BeginWrite(DbpEncoder &encoder, WriteStream &writer, const uint32_t &first_value) {
1135: 	BeginWrite(encoder, writer, UnsafeNumericCast<int64_t>(first_value));
1136: }
1137: 
1138: template <class T>
1139: void WriteValue(DbpEncoder &encoder, WriteStream &writer, const T &value) {
1140: 	throw InternalException("Can't write type to DELTA_BINARY_PACKED column");
1141: }
1142: 
1143: template <>
1144: void WriteValue(DbpEncoder &encoder, WriteStream &writer, const int64_t &value) {
1145: 	encoder.WriteValue(writer, value);
1146: }
1147: 
1148: template <>
1149: void WriteValue(DbpEncoder &encoder, WriteStream &writer, const int32_t &value) {
1150: 	WriteValue(encoder, writer, UnsafeNumericCast<int64_t>(value));
1151: }
1152: 
1153: template <>
1154: void WriteValue(DbpEncoder &encoder, WriteStream &writer, const uint64_t &value) {
1155: 	encoder.WriteValue(writer, UnsafeNumericCast<int64_t>(value));
1156: }
1157: 
1158: template <>
1159: void WriteValue(DbpEncoder &encoder, WriteStream &writer, const uint32_t &value) {
1160: 	WriteValue(encoder, writer, UnsafeNumericCast<int64_t>(value));
1161: }
1162: 
1163: } // namespace dbp_encoder
1164: 
1165: namespace dlba_encoder {
1166: 
1167: template <class T>
1168: void BeginWrite(DlbaEncoder &encoder, WriteStream &writer, const T &first_value) {
1169: 	throw InternalException("Can't write type to DELTA_LENGTH_BYTE_ARRAY column");
1170: }
1171: 
1172: template <>
1173: void BeginWrite(DlbaEncoder &encoder, WriteStream &writer, const string_t &first_value) {
1174: 	encoder.BeginWrite(writer, first_value);
1175: }
1176: 
1177: template <class T>
1178: void WriteValue(DlbaEncoder &encoder, WriteStream &writer, const T &value) {
1179: 	throw InternalException("Can't write type to DELTA_LENGTH_BYTE_ARRAY column");
1180: }
1181: 
1182: template <>
1183: void WriteValue(DlbaEncoder &encoder, WriteStream &writer, const string_t &value) {
1184: 	encoder.WriteValue(writer, value);
1185: }
1186: 
1187: // helpers to get size from strings
1188: template <class SRC>
1189: static idx_t GetDlbaStringSize(const SRC &src_value) {
1190: 	return 0;
1191: }
1192: 
1193: template <>
1194: idx_t GetDlbaStringSize(const string_t &src_value) {
1195: 	return src_value.GetSize();
1196: }
1197: 
1198: } // namespace dlba_encoder
1199: 
1200: namespace bss_encoder {
1201: 
1202: template <class T>
1203: void WriteValue(BssEncoder &encoder, const T &value) {
1204: 	throw InternalException("Can't write type to BYTE_STREAM_SPLIT column");
1205: }
1206: 
1207: template <>
1208: void WriteValue(BssEncoder &encoder, const float &value) {
1209: 	encoder.WriteValue(value);
1210: }
1211: 
1212: template <>
1213: void WriteValue(BssEncoder &encoder, const double &value) {
1214: 	encoder.WriteValue(value);
1215: }
1216: 
1217: } // namespace bss_encoder
1218: 
1219: template <class SRC, class TGT, class OP = ParquetCastOperator>
1220: class StandardColumnWriter : public BasicColumnWriter {
1221: public:
1222: 	StandardColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, // NOLINT
1223: 	                     idx_t max_repeat, idx_t max_define, bool can_have_nulls)
1224: 	    : BasicColumnWriter(writer, schema_idx, std::move(schema_path_p), max_repeat, max_define, can_have_nulls) {
1225: 	}
1226: 	~StandardColumnWriter() override = default;
1227: 
1228: public:
1229: 	unique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::RowGroup &row_group) override {
1230: 		auto result = make_uniq<StandardColumnWriterState<SRC>>(row_group, row_group.columns.size());
1231: 		result->encoding = Encoding::RLE_DICTIONARY;
1232: 		RegisterToRowGroup(row_group);
1233: 		return std::move(result);
1234: 	}
1235: 
1236: 	unique_ptr<ColumnWriterPageState> InitializePageState(BasicColumnWriterState &state_p, idx_t page_idx) override {
1237: 		auto &state = state_p.Cast<StandardColumnWriterState<SRC>>();
1238: 		const auto &page_info = state_p.page_info[page_idx];
1239: 		auto result = make_uniq<StandardWriterPageState<SRC, TGT>>(
1240: 		    page_info.row_count - page_info.empty_count, state.total_string_size, state.encoding, state.dictionary);
1241: 		return std::move(result);
1242: 	}
1243: 
1244: 	void FlushPageState(WriteStream &temp_writer, ColumnWriterPageState *state_p) override {
1245: 		auto &page_state = state_p->Cast<StandardWriterPageState<SRC, TGT>>();
1246: 		switch (page_state.encoding) {
1247: 		case Encoding::DELTA_BINARY_PACKED:
1248: 			if (!page_state.dbp_initialized) {
1249: 				dbp_encoder::BeginWrite<int64_t>(page_state.dbp_encoder, temp_writer, 0);
1250: 			}
1251: 			page_state.dbp_encoder.FinishWrite(temp_writer);
1252: 			break;
1253: 		case Encoding::RLE_DICTIONARY:
1254: 			D_ASSERT(page_state.dict_bit_width != 0);
1255: 			if (!page_state.dict_written_value) {
1256: 				// all values are null
1257: 				// just write the bit width
1258: 				temp_writer.Write<uint8_t>(page_state.dict_bit_width);
1259: 				return;
1260: 			}
1261: 			page_state.dict_encoder.FinishWrite(temp_writer);
1262: 			break;
1263: 		case Encoding::DELTA_LENGTH_BYTE_ARRAY:
1264: 			if (!page_state.dlba_initialized) {
1265: 				dlba_encoder::BeginWrite<string_t>(page_state.dlba_encoder, temp_writer, string_t(""));
1266: 			}
1267: 			page_state.dlba_encoder.FinishWrite(temp_writer);
1268: 			break;
1269: 		case Encoding::BYTE_STREAM_SPLIT:
1270: 			page_state.bss_encoder.FinishWrite(temp_writer);
1271: 			break;
1272: 		case Encoding::PLAIN:
1273: 			break;
1274: 		default:
1275: 			throw InternalException("Unknown encoding");
1276: 		}
1277: 	}
1278: 
1279: 	Encoding::type GetEncoding(BasicColumnWriterState &state_p) override {
1280: 		auto &state = state_p.Cast<StandardColumnWriterState<SRC>>();
1281: 		return state.encoding;
1282: 	}
1283: 
1284: 	bool HasAnalyze() override {
1285: 		return true;
1286: 	}
1287: 
1288: 	void Analyze(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) override {
1289: 		auto &state = state_p.Cast<StandardColumnWriterState<SRC>>();
1290: 
1291: 		auto data_ptr = FlatVector::GetData<SRC>(vector);
1292: 		idx_t vector_index = 0;
1293: 		uint32_t new_value_index = state.dictionary.size();
1294: 
1295: 		const bool check_parent_empty = parent && !parent->is_empty.empty();
1296: 		const idx_t parent_index = state.definition_levels.size();
1297: 
1298: 		const idx_t vcount =
1299: 		    check_parent_empty ? parent->definition_levels.size() - state.definition_levels.size() : count;
1300: 
1301: 		const auto &validity = FlatVector::Validity(vector);
1302: 
1303: 		for (idx_t i = 0; i < vcount; i++) {
1304: 			if (check_parent_empty && parent->is_empty[parent_index + i]) {
1305: 				continue;
1306: 			}
1307: 			if (validity.RowIsValid(vector_index)) {
1308: 				const auto &src_value = data_ptr[vector_index];
1309: 				if (state.dictionary.size() <= writer.DictionarySizeLimit()) {
1310: 					if (state.dictionary.find(src_value) == state.dictionary.end()) {
1311: 						state.dictionary[src_value] = new_value_index;
1312: 						new_value_index++;
1313: 					}
1314: 				}
1315: 				state.total_value_count++;
1316: 				state.total_string_size += dlba_encoder::GetDlbaStringSize(src_value);
1317: 			}
1318: 			vector_index++;
1319: 		}
1320: 	}
1321: 
1322: 	void FinalizeAnalyze(ColumnWriterState &state_p) override {
1323: 		const auto type = writer.GetType(schema_idx);
1324: 
1325: 		auto &state = state_p.Cast<StandardColumnWriterState<SRC>>();
1326: 		if (state.dictionary.size() == 0 || state.dictionary.size() > writer.DictionarySizeLimit()) {
1327: 			if (writer.GetParquetVersion() == ParquetVersion::V1) {
1328: 				// Can't do the cool stuff for V1
1329: 				state.encoding = Encoding::PLAIN;
1330: 			} else {
1331: 				// If we aren't doing dictionary encoding, these encodings are virtually always better than PLAIN
1332: 				switch (type) {
1333: 				case Type::type::INT32:
1334: 				case Type::type::INT64:
1335: 					state.encoding = Encoding::DELTA_BINARY_PACKED;
1336: 					break;
1337: 				case Type::type::BYTE_ARRAY:
1338: 					state.encoding = Encoding::DELTA_LENGTH_BYTE_ARRAY;
1339: 					break;
1340: 				case Type::type::FLOAT:
1341: 				case Type::type::DOUBLE:
1342: 					state.encoding = Encoding::BYTE_STREAM_SPLIT;
1343: 					break;
1344: 				default:
1345: 					state.encoding = Encoding::PLAIN;
1346: 				}
1347: 			}
1348: 			state.dictionary.clear();
1349: 		} else {
1350: 			state.key_bit_width = RleBpDecoder::ComputeBitWidth(state.dictionary.size());
1351: 		}
1352: 	}
1353: 
1354: 	unique_ptr<ColumnWriterStatistics> InitializeStatsState() override {
1355: 		return OP::template InitializeStats<SRC, TGT>();
1356: 	}
1357: 
1358: 	bool HasDictionary(BasicColumnWriterState &state_p) override {
1359: 		auto &state = state_p.Cast<StandardColumnWriterState<SRC>>();
1360: 		return state.encoding == Encoding::RLE_DICTIONARY;
1361: 	}
1362: 
1363: 	idx_t DictionarySize(BasicColumnWriterState &state_p) override {
1364: 		auto &state = state_p.Cast<StandardColumnWriterState<SRC>>();
1365: 		return state.dictionary.size();
1366: 	}
1367: 
1368: 	void WriteVector(WriteStream &temp_writer, ColumnWriterStatistics *stats, ColumnWriterPageState *page_state_p,
1369: 	                 Vector &input_column, idx_t chunk_start, idx_t chunk_end) override {
1370: 		auto &page_state = page_state_p->Cast<StandardWriterPageState<SRC, TGT>>();
1371: 
1372: 		const auto &mask = FlatVector::Validity(input_column);
1373: 		const auto *data_ptr = FlatVector::GetData<SRC>(input_column);
1374: 
1375: 		switch (page_state.encoding) {
1376: 		case Encoding::RLE_DICTIONARY: {
1377: 			for (idx_t r = chunk_start; r < chunk_end; r++) {
1378: 				if (!mask.RowIsValid(r)) {
1379: 					continue;
1380: 				}
1381: 				auto &src_val = data_ptr[r];
1382: 				auto value_index = page_state.dictionary.at(src_val);
1383: 				if (!page_state.dict_written_value) {
1384: 					// first value
1385: 					// write the bit-width as a one-byte entry
1386: 					temp_writer.Write<uint8_t>(page_state.dict_bit_width);
1387: 					// now begin writing the actual value
1388: 					page_state.dict_encoder.BeginWrite(temp_writer, value_index);
1389: 					page_state.dict_written_value = true;
1390: 				} else {
1391: 					page_state.dict_encoder.WriteValue(temp_writer, value_index);
1392: 				}
1393: 			}
1394: 			break;
1395: 		}
1396: 		case Encoding::DELTA_BINARY_PACKED: {
1397: 			idx_t r = chunk_start;
1398: 			if (!page_state.dbp_initialized) {
1399: 				// find first non-null value
1400: 				for (; r < chunk_end; r++) {
1401: 					if (!mask.RowIsValid(r)) {
1402: 						continue;
1403: 					}
1404: 					const TGT target_value = OP::template Operation<SRC, TGT>(data_ptr[r]);
1405: 					OP::template HandleStats<SRC, TGT>(stats, target_value);
1406: 					dbp_encoder::BeginWrite(page_state.dbp_encoder, temp_writer, target_value);
1407: 					page_state.dbp_initialized = true;
1408: 					r++; // skip over
1409: 					break;
1410: 				}
1411: 			}
1412: 
1413: 			for (; r < chunk_end; r++) {
1414: 				if (!mask.RowIsValid(r)) {
1415: 					continue;
1416: 				}
1417: 				const TGT target_value = OP::template Operation<SRC, TGT>(data_ptr[r]);
1418: 				OP::template HandleStats<SRC, TGT>(stats, target_value);
1419: 				dbp_encoder::WriteValue(page_state.dbp_encoder, temp_writer, target_value);
1420: 			}
1421: 			break;
1422: 		}
1423: 		case Encoding::DELTA_LENGTH_BYTE_ARRAY: {
1424: 			idx_t r = chunk_start;
1425: 			if (!page_state.dlba_initialized) {
1426: 				// find first non-null value
1427: 				for (; r < chunk_end; r++) {
1428: 					if (!mask.RowIsValid(r)) {
1429: 						continue;
1430: 					}
1431: 					const TGT target_value = OP::template Operation<SRC, TGT>(data_ptr[r]);
1432: 					OP::template HandleStats<SRC, TGT>(stats, target_value);
1433: 					dlba_encoder::BeginWrite(page_state.dlba_encoder, temp_writer, target_value);
1434: 					page_state.dlba_initialized = true;
1435: 					r++; // skip over
1436: 					break;
1437: 				}
1438: 			}
1439: 
1440: 			for (; r < chunk_end; r++) {
1441: 				if (!mask.RowIsValid(r)) {
1442: 					continue;
1443: 				}
1444: 				const TGT target_value = OP::template Operation<SRC, TGT>(data_ptr[r]);
1445: 				OP::template HandleStats<SRC, TGT>(stats, target_value);
1446: 				dlba_encoder::WriteValue(page_state.dlba_encoder, temp_writer, target_value);
1447: 			}
1448: 			break;
1449: 		}
1450: 		case Encoding::BYTE_STREAM_SPLIT: {
1451: 			for (idx_t r = chunk_start; r < chunk_end; r++) {
1452: 				if (!mask.RowIsValid(r)) {
1453: 					continue;
1454: 				}
1455: 				const TGT target_value = OP::template Operation<SRC, TGT>(data_ptr[r]);
1456: 				OP::template HandleStats<SRC, TGT>(stats, target_value);
1457: 				bss_encoder::WriteValue(page_state.bss_encoder, target_value);
1458: 			}
1459: 			break;
1460: 		}
1461: 		case Encoding::PLAIN: {
1462: 			D_ASSERT(page_state.encoding == Encoding::PLAIN);
1463: 			TemplatedWritePlain<SRC, TGT, OP>(input_column, stats, chunk_start, chunk_end, mask, temp_writer);
1464: 			break;
1465: 		}
1466: 		default:
1467: 			throw InternalException("Unknown encoding");
1468: 		}
1469: 	}
1470: 
1471: 	void FlushDictionary(BasicColumnWriterState &state_p, ColumnWriterStatistics *stats) override {
1472: 		auto &state = state_p.Cast<StandardColumnWriterState<SRC>>();
1473: 
1474: 		D_ASSERT(state.encoding == Encoding::RLE_DICTIONARY);
1475: 
1476: 		// first we need to sort the values in index order
1477: 		auto values = vector<SRC>(state.dictionary.size());
1478: 		for (const auto &entry : state.dictionary) {
1479: 			values[entry.second] = entry.first;
1480: 		}
1481: 
1482: 		state.bloom_filter =
1483: 		    make_uniq<ParquetBloomFilter>(state.dictionary.size(), writer.BloomFilterFalsePositiveRatio());
1484: 
1485: 		// first write the contents of the dictionary page to a temporary buffer
1486: 		auto temp_writer = make_uniq<MemoryStream>(
1487: 		    Allocator::Get(writer.GetContext()), MaxValue<idx_t>(NextPowerOfTwo(state.dictionary.size() * sizeof(TGT)),
1488: 		                                                         MemoryStream::DEFAULT_INITIAL_CAPACITY));
1489: 		for (idx_t r = 0; r < values.size(); r++) {
1490: 			const TGT target_value = OP::template Operation<SRC, TGT>(values[r]);
1491: 			// update the statistics
1492: 			OP::template HandleStats<SRC, TGT>(stats, target_value);
1493: 			// update the bloom filter
1494: 			auto hash = OP::template XXHash64<SRC, TGT>(target_value);
1495: 			state.bloom_filter->FilterInsert(hash);
1496: 			// actually write the dictionary value
1497: 			OP::template WriteToStream<SRC, TGT>(target_value, *temp_writer);
1498: 		}
1499: 		// flush the dictionary page and add it to the to-be-written pages
1500: 		WriteDictionary(state, std::move(temp_writer), values.size());
1501: 		// bloom filter will be queued for writing in ParquetWriter::BufferBloomFilter one level up
1502: 	}
1503: 
1504: 	idx_t GetRowSize(const Vector &vector, const idx_t index, const BasicColumnWriterState &state_p) const override {
1505: 		auto &state = state_p.Cast<StandardColumnWriterState<SRC>>();
1506: 		if (state.encoding == Encoding::RLE_DICTIONARY) {
1507: 			return (state.key_bit_width + 7) / 8;
1508: 		} else {
1509: 			return OP::template GetRowSize<SRC, TGT>(vector, index);
1510: 		}
1511: 	}
1512: };
1513: 
1514: //===--------------------------------------------------------------------===//
1515: // Boolean Column Writer
1516: //===--------------------------------------------------------------------===//
1517: class BooleanStatisticsState : public ColumnWriterStatistics {
1518: public:
1519: 	BooleanStatisticsState() : min(true), max(false) {
1520: 	}
1521: 
1522: 	bool min;
1523: 	bool max;
1524: 
1525: public:
1526: 	bool HasStats() override {
1527: 		return !(min && !max);
1528: 	}
1529: 
1530: 	string GetMin() override {
1531: 		return GetMinValue();
1532: 	}
1533: 	string GetMax() override {
1534: 		return GetMaxValue();
1535: 	}
1536: 	string GetMinValue() override {
1537: 		return HasStats() ? string(const_char_ptr_cast(&min), sizeof(bool)) : string();
1538: 	}
1539: 	string GetMaxValue() override {
1540: 		return HasStats() ? string(const_char_ptr_cast(&max), sizeof(bool)) : string();
1541: 	}
1542: };
1543: 
1544: class BooleanWriterPageState : public ColumnWriterPageState {
1545: public:
1546: 	uint8_t byte = 0;
1547: 	uint8_t byte_pos = 0;
1548: };
1549: 
1550: class BooleanColumnWriter : public BasicColumnWriter {
1551: public:
1552: 	BooleanColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, idx_t max_repeat,
1553: 	                    idx_t max_define, bool can_have_nulls)
1554: 	    : BasicColumnWriter(writer, schema_idx, std::move(schema_path_p), max_repeat, max_define, can_have_nulls) {
1555: 	}
1556: 	~BooleanColumnWriter() override = default;
1557: 
1558: public:
1559: 	unique_ptr<ColumnWriterStatistics> InitializeStatsState() override {
1560: 		return make_uniq<BooleanStatisticsState>();
1561: 	}
1562: 
1563: 	void WriteVector(WriteStream &temp_writer, ColumnWriterStatistics *stats_p, ColumnWriterPageState *state_p,
1564: 	                 Vector &input_column, idx_t chunk_start, idx_t chunk_end) override {
1565: 		auto &stats = stats_p->Cast<BooleanStatisticsState>();
1566: 		auto &state = state_p->Cast<BooleanWriterPageState>();
1567: 		auto &mask = FlatVector::Validity(input_column);
1568: 
1569: 		auto *ptr = FlatVector::GetData<bool>(input_column);
1570: 		for (idx_t r = chunk_start; r < chunk_end; r++) {
1571: 			if (mask.RowIsValid(r)) {
1572: 				// only encode if non-null
1573: 				if (ptr[r]) {
1574: 					stats.max = true;
1575: 					state.byte |= 1 << state.byte_pos;
1576: 				} else {
1577: 					stats.min = false;
1578: 				}
1579: 				state.byte_pos++;
1580: 
1581: 				if (state.byte_pos == 8) {
1582: 					temp_writer.Write<uint8_t>(state.byte);
1583: 					state.byte = 0;
1584: 					state.byte_pos = 0;
1585: 				}
1586: 			}
1587: 		}
1588: 	}
1589: 
1590: 	unique_ptr<ColumnWriterPageState> InitializePageState(BasicColumnWriterState &state, idx_t page_idx) override {
1591: 		return make_uniq<BooleanWriterPageState>();
1592: 	}
1593: 
1594: 	void FlushPageState(WriteStream &temp_writer, ColumnWriterPageState *state_p) override {
1595: 		auto &state = state_p->Cast<BooleanWriterPageState>();
1596: 		if (state.byte_pos > 0) {
1597: 			temp_writer.Write<uint8_t>(state.byte);
1598: 			state.byte = 0;
1599: 			state.byte_pos = 0;
1600: 		}
1601: 	}
1602: 
1603: 	idx_t GetRowSize(const Vector &vector, const idx_t index, const BasicColumnWriterState &state) const override {
1604: 		return sizeof(bool);
1605: 	}
1606: };
1607: 
1608: //===--------------------------------------------------------------------===//
1609: // Decimal Column Writer
1610: //===--------------------------------------------------------------------===//
1611: static void WriteParquetDecimal(hugeint_t input, data_ptr_t result) {
1612: 	bool positive = input >= 0;
1613: 	// numbers are stored as two's complement so some muckery is required
1614: 	if (!positive) {
1615: 		input = NumericLimits<hugeint_t>::Maximum() + input + 1;
1616: 	}
1617: 	uint64_t high_bytes = uint64_t(input.upper);
1618: 	uint64_t low_bytes = input.lower;
1619: 
1620: 	for (idx_t i = 0; i < sizeof(uint64_t); i++) {
1621: 		auto shift_count = (sizeof(uint64_t) - i - 1) * 8;
1622: 		result[i] = (high_bytes >> shift_count) & 0xFF;
1623: 	}
1624: 	for (idx_t i = 0; i < sizeof(uint64_t); i++) {
1625: 		auto shift_count = (sizeof(uint64_t) - i - 1) * 8;
1626: 		result[sizeof(uint64_t) + i] = (low_bytes >> shift_count) & 0xFF;
1627: 	}
1628: 	if (!positive) {
1629: 		result[0] |= 0x80;
1630: 	}
1631: }
1632: 
1633: class FixedDecimalStatistics : public ColumnWriterStatistics {
1634: public:
1635: 	FixedDecimalStatistics() : min(NumericLimits<hugeint_t>::Maximum()), max(NumericLimits<hugeint_t>::Minimum()) {
1636: 	}
1637: 
1638: 	hugeint_t min;
1639: 	hugeint_t max;
1640: 
1641: public:
1642: 	string GetStats(hugeint_t &input) {
1643: 		data_t buffer[16];
1644: 		WriteParquetDecimal(input, buffer);
1645: 		return string(const_char_ptr_cast(buffer), 16);
1646: 	}
1647: 
1648: 	bool HasStats() override {
1649: 		return min <= max;
1650: 	}
1651: 
1652: 	void Update(hugeint_t &val) {
1653: 		if (LessThan::Operation(val, min)) {
1654: 			min = val;
1655: 		}
1656: 		if (GreaterThan::Operation(val, max)) {
1657: 			max = val;
1658: 		}
1659: 	}
1660: 
1661: 	string GetMin() override {
1662: 		return GetMinValue();
1663: 	}
1664: 	string GetMax() override {
1665: 		return GetMaxValue();
1666: 	}
1667: 	string GetMinValue() override {
1668: 		return HasStats() ? GetStats(min) : string();
1669: 	}
1670: 	string GetMaxValue() override {
1671: 		return HasStats() ? GetStats(max) : string();
1672: 	}
1673: };
1674: 
1675: class FixedDecimalColumnWriter : public BasicColumnWriter {
1676: public:
1677: 	FixedDecimalColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, idx_t max_repeat,
1678: 	                         idx_t max_define, bool can_have_nulls)
1679: 	    : BasicColumnWriter(writer, schema_idx, std::move(schema_path_p), max_repeat, max_define, can_have_nulls) {
1680: 	}
1681: 	~FixedDecimalColumnWriter() override = default;
1682: 
1683: public:
1684: 	unique_ptr<ColumnWriterStatistics> InitializeStatsState() override {
1685: 		return make_uniq<FixedDecimalStatistics>();
1686: 	}
1687: 
1688: 	void WriteVector(WriteStream &temp_writer, ColumnWriterStatistics *stats_p, ColumnWriterPageState *page_state,
1689: 	                 Vector &input_column, idx_t chunk_start, idx_t chunk_end) override {
1690: 		auto &mask = FlatVector::Validity(input_column);
1691: 		auto *ptr = FlatVector::GetData<hugeint_t>(input_column);
1692: 		auto &stats = stats_p->Cast<FixedDecimalStatistics>();
1693: 
1694: 		data_t temp_buffer[16];
1695: 		for (idx_t r = chunk_start; r < chunk_end; r++) {
1696: 			if (mask.RowIsValid(r)) {
1697: 				stats.Update(ptr[r]);
1698: 				WriteParquetDecimal(ptr[r], temp_buffer);
1699: 				temp_writer.WriteData(temp_buffer, 16);
1700: 			}
1701: 		}
1702: 	}
1703: 
1704: 	idx_t GetRowSize(const Vector &vector, const idx_t index, const BasicColumnWriterState &state) const override {
1705: 		return sizeof(hugeint_t);
1706: 	}
1707: };
1708: 
1709: //===--------------------------------------------------------------------===//
1710: // WKB Column Writer
1711: //===--------------------------------------------------------------------===//
1712: // Used to store the metadata for a WKB-encoded geometry column when writing
1713: // GeoParquet files.
1714: class WKBColumnWriterState final : public StandardColumnWriterState<string_t> {
1715: public:
1716: 	WKBColumnWriterState(ClientContext &context, duckdb_parquet::RowGroup &row_group, idx_t col_idx)
1717: 	    : StandardColumnWriterState(row_group, col_idx), geo_data(), geo_data_writer(context) {
1718: 	}
1719: 
1720: 	GeoParquetColumnMetadata geo_data;
1721: 	GeoParquetColumnMetadataWriter geo_data_writer;
1722: };
1723: 
1724: class WKBColumnWriter final : public StandardColumnWriter<string_t, string_t, ParquetStringOperator> {
1725: public:
1726: 	WKBColumnWriter(ClientContext &context_p, ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p,
1727: 	                idx_t max_repeat, idx_t max_define, bool can_have_nulls, string name)
1728: 	    : StandardColumnWriter(writer, schema_idx, std::move(schema_path_p), max_repeat, max_define, can_have_nulls),
1729: 	      column_name(std::move(name)), context(context_p) {
1730: 
1731: 		this->writer.GetGeoParquetData().RegisterGeometryColumn(column_name);
1732: 	}
1733: 
1734: 	unique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::RowGroup &row_group) override {
1735: 		auto result = make_uniq<WKBColumnWriterState>(context, row_group, row_group.columns.size());
1736: 		result->encoding = Encoding::RLE_DICTIONARY;
1737: 		RegisterToRowGroup(row_group);
1738: 		return std::move(result);
1739: 	}
1740: 
1741: 	void Write(ColumnWriterState &state, Vector &vector, idx_t count) override {
1742: 		StandardColumnWriter::Write(state, vector, count);
1743: 
1744: 		auto &geo_state = state.Cast<WKBColumnWriterState>();
1745: 		geo_state.geo_data_writer.Update(geo_state.geo_data, vector, count);
1746: 	}
1747: 
1748: 	void FinalizeWrite(ColumnWriterState &state) override {
1749: 		StandardColumnWriter::FinalizeWrite(state);
1750: 
1751: 		// Add the geodata object to the writer
1752: 		const auto &geo_state = state.Cast<WKBColumnWriterState>();
1753: 
1754: 		// Merge this state's geo column data with the writer's geo column data
1755: 		writer.GetGeoParquetData().FlushColumnMeta(column_name, geo_state.geo_data);
1756: 	}
1757: 
1758: private:
1759: 	string column_name;
1760: 	ClientContext &context;
1761: };
1762: 
1763: //===--------------------------------------------------------------------===//
1764: // Enum Column Writer
1765: //===--------------------------------------------------------------------===//
1766: class EnumWriterPageState : public ColumnWriterPageState {
1767: public:
1768: 	explicit EnumWriterPageState(uint32_t bit_width) : encoder(bit_width), written_value(false) {
1769: 	}
1770: 
1771: 	RleBpEncoder encoder;
1772: 	bool written_value;
1773: };
1774: 
1775: class EnumColumnWriter : public BasicColumnWriter {
1776: public:
1777: 	EnumColumnWriter(ParquetWriter &writer, LogicalType enum_type_p, idx_t schema_idx, vector<string> schema_path_p,
1778: 	                 idx_t max_repeat, idx_t max_define, bool can_have_nulls)
1779: 	    : BasicColumnWriter(writer, schema_idx, std::move(schema_path_p), max_repeat, max_define, can_have_nulls),
1780: 	      enum_type(std::move(enum_type_p)) {
1781: 		bit_width = RleBpDecoder::ComputeBitWidth(EnumType::GetSize(enum_type));
1782: 	}
1783: 	~EnumColumnWriter() override = default;
1784: 
1785: 	LogicalType enum_type;
1786: 	uint32_t bit_width;
1787: 
1788: public:
1789: 	unique_ptr<ColumnWriterStatistics> InitializeStatsState() override {
1790: 		return make_uniq<StringStatisticsState>();
1791: 	}
1792: 
1793: 	template <class T>
1794: 	void WriteEnumInternal(WriteStream &temp_writer, Vector &input_column, idx_t chunk_start, idx_t chunk_end,
1795: 	                       EnumWriterPageState &page_state) {
1796: 		auto &mask = FlatVector::Validity(input_column);
1797: 		auto *ptr = FlatVector::GetData<T>(input_column);
1798: 		for (idx_t r = chunk_start; r < chunk_end; r++) {
1799: 			if (mask.RowIsValid(r)) {
1800: 				if (!page_state.written_value) {
1801: 					// first value
1802: 					// write the bit-width as a one-byte entry
1803: 					temp_writer.Write<uint8_t>(bit_width);
1804: 					// now begin writing the actual value
1805: 					page_state.encoder.BeginWrite(temp_writer, ptr[r]);
1806: 					page_state.written_value = true;
1807: 				} else {
1808: 					page_state.encoder.WriteValue(temp_writer, ptr[r]);
1809: 				}
1810: 			}
1811: 		}
1812: 	}
1813: 
1814: 	void WriteVector(WriteStream &temp_writer, ColumnWriterStatistics *stats_p, ColumnWriterPageState *page_state_p,
1815: 	                 Vector &input_column, idx_t chunk_start, idx_t chunk_end) override {
1816: 		auto &page_state = page_state_p->Cast<EnumWriterPageState>();
1817: 		switch (enum_type.InternalType()) {
1818: 		case PhysicalType::UINT8:
1819: 			WriteEnumInternal<uint8_t>(temp_writer, input_column, chunk_start, chunk_end, page_state);
1820: 			break;
1821: 		case PhysicalType::UINT16:
1822: 			WriteEnumInternal<uint16_t>(temp_writer, input_column, chunk_start, chunk_end, page_state);
1823: 			break;
1824: 		case PhysicalType::UINT32:
1825: 			WriteEnumInternal<uint32_t>(temp_writer, input_column, chunk_start, chunk_end, page_state);
1826: 			break;
1827: 		default:
1828: 			throw InternalException("Unsupported internal enum type");
1829: 		}
1830: 	}
1831: 
1832: 	unique_ptr<ColumnWriterPageState> InitializePageState(BasicColumnWriterState &state, idx_t page_idx) override {
1833: 		return make_uniq<EnumWriterPageState>(bit_width);
1834: 	}
1835: 
1836: 	void FlushPageState(WriteStream &temp_writer, ColumnWriterPageState *state_p) override {
1837: 		auto &page_state = state_p->Cast<EnumWriterPageState>();
1838: 		if (!page_state.written_value) {
1839: 			// all values are null
1840: 			// just write the bit width
1841: 			temp_writer.Write<uint8_t>(bit_width);
1842: 			return;
1843: 		}
1844: 		page_state.encoder.FinishWrite(temp_writer);
1845: 	}
1846: 
1847: 	duckdb_parquet::Encoding::type GetEncoding(BasicColumnWriterState &state) override {
1848: 		return Encoding::RLE_DICTIONARY;
1849: 	}
1850: 
1851: 	bool HasDictionary(BasicColumnWriterState &state) override {
1852: 		return true;
1853: 	}
1854: 
1855: 	idx_t DictionarySize(BasicColumnWriterState &state_p) override {
1856: 		return EnumType::GetSize(enum_type);
1857: 	}
1858: 
1859: 	void FlushDictionary(BasicColumnWriterState &state, ColumnWriterStatistics *stats_p) override {
1860: 		auto &stats = stats_p->Cast<StringStatisticsState>();
1861: 		// write the enum values to a dictionary page
1862: 		auto &enum_values = EnumType::GetValuesInsertOrder(enum_type);
1863: 		auto enum_count = EnumType::GetSize(enum_type);
1864: 		auto string_values = FlatVector::GetData<string_t>(enum_values);
1865: 		// first write the contents of the dictionary page to a temporary buffer
1866: 		auto temp_writer = make_uniq<MemoryStream>(Allocator::Get(writer.GetContext()));
1867: 		for (idx_t r = 0; r < enum_count; r++) {
1868: 			D_ASSERT(!FlatVector::IsNull(enum_values, r));
1869: 			// update the statistics
1870: 			stats.Update(string_values[r]);
1871: 			// write this string value to the dictionary
1872: 			temp_writer->Write<uint32_t>(string_values[r].GetSize());
1873: 			temp_writer->WriteData(const_data_ptr_cast(string_values[r].GetData()), string_values[r].GetSize());
1874: 		}
1875: 		// flush the dictionary page and add it to the to-be-written pages
1876: 		WriteDictionary(state, std::move(temp_writer), enum_count);
1877: 	}
1878: 
1879: 	idx_t GetRowSize(const Vector &vector, const idx_t index, const BasicColumnWriterState &state) const override {
1880: 		return (bit_width + 7) / 8;
1881: 	}
1882: };
1883: 
1884: //===--------------------------------------------------------------------===//
1885: // Struct Column Writer
1886: //===--------------------------------------------------------------------===//
1887: class StructColumnWriter : public ColumnWriter {
1888: public:
1889: 	StructColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, idx_t max_repeat,
1890: 	                   idx_t max_define, vector<unique_ptr<ColumnWriter>> child_writers_p, bool can_have_nulls)
1891: 	    : ColumnWriter(writer, schema_idx, std::move(schema_path_p), max_repeat, max_define, can_have_nulls),
1892: 	      child_writers(std::move(child_writers_p)) {
1893: 	}
1894: 	~StructColumnWriter() override = default;
1895: 
1896: 	vector<unique_ptr<ColumnWriter>> child_writers;
1897: 
1898: public:
1899: 	unique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::RowGroup &row_group) override;
1900: 	bool HasAnalyze() override;
1901: 	void Analyze(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;
1902: 	void FinalizeAnalyze(ColumnWriterState &state) override;
1903: 	void Prepare(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;
1904: 
1905: 	void BeginWrite(ColumnWriterState &state) override;
1906: 	void Write(ColumnWriterState &state, Vector &vector, idx_t count) override;
1907: 	void FinalizeWrite(ColumnWriterState &state) override;
1908: };
1909: 
1910: class StructColumnWriterState : public ColumnWriterState {
1911: public:
1912: 	StructColumnWriterState(duckdb_parquet::RowGroup &row_group, idx_t col_idx)
1913: 	    : row_group(row_group), col_idx(col_idx) {
1914: 	}
1915: 	~StructColumnWriterState() override = default;
1916: 
1917: 	duckdb_parquet::RowGroup &row_group;
1918: 	idx_t col_idx;
1919: 	vector<unique_ptr<ColumnWriterState>> child_states;
1920: };
1921: 
1922: unique_ptr<ColumnWriterState> StructColumnWriter::InitializeWriteState(duckdb_parquet::RowGroup &row_group) {
1923: 	auto result = make_uniq<StructColumnWriterState>(row_group, row_group.columns.size());
1924: 
1925: 	result->child_states.reserve(child_writers.size());
1926: 	for (auto &child_writer : child_writers) {
1927: 		result->child_states.push_back(child_writer->InitializeWriteState(row_group));
1928: 	}
1929: 	return std::move(result);
1930: }
1931: 
1932: bool StructColumnWriter::HasAnalyze() {
1933: 	for (auto &child_writer : child_writers) {
1934: 		if (child_writer->HasAnalyze()) {
1935: 			return true;
1936: 		}
1937: 	}
1938: 	return false;
1939: }
1940: 
1941: void StructColumnWriter::Analyze(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {
1942: 	auto &state = state_p.Cast<StructColumnWriterState>();
1943: 	auto &child_vectors = StructVector::GetEntries(vector);
1944: 	for (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {
1945: 		// Need to check again. It might be that just one child needs it but the rest not
1946: 		if (child_writers[child_idx]->HasAnalyze()) {
1947: 			child_writers[child_idx]->Analyze(*state.child_states[child_idx], &state_p, *child_vectors[child_idx],
1948: 			                                  count);
1949: 		}
1950: 	}
1951: }
1952: 
1953: void StructColumnWriter::FinalizeAnalyze(ColumnWriterState &state_p) {
1954: 	auto &state = state_p.Cast<StructColumnWriterState>();
1955: 	for (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {
1956: 		// Need to check again. It might be that just one child needs it but the rest not
1957: 		if (child_writers[child_idx]->HasAnalyze()) {
1958: 			child_writers[child_idx]->FinalizeAnalyze(*state.child_states[child_idx]);
1959: 		}
1960: 	}
1961: }
1962: 
1963: void StructColumnWriter::Prepare(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {
1964: 	auto &state = state_p.Cast<StructColumnWriterState>();
1965: 
1966: 	auto &validity = FlatVector::Validity(vector);
1967: 	if (parent) {
1968: 		// propagate empty entries from the parent
1969: 		while (state.is_empty.size() < parent->is_empty.size()) {
1970: 			state.is_empty.push_back(parent->is_empty[state.is_empty.size()]);
1971: 		}
1972: 	}
1973: 	HandleRepeatLevels(state_p, parent, count, max_repeat);
1974: 	HandleDefineLevels(state_p, parent, validity, count, PARQUET_DEFINE_VALID, max_define - 1);
1975: 	auto &child_vectors = StructVector::GetEntries(vector);
1976: 	for (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {
1977: 		child_writers[child_idx]->Prepare(*state.child_states[child_idx], &state_p, *child_vectors[child_idx], count);
1978: 	}
1979: }
1980: 
1981: void StructColumnWriter::BeginWrite(ColumnWriterState &state_p) {
1982: 	auto &state = state_p.Cast<StructColumnWriterState>();
1983: 	for (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {
1984: 		child_writers[child_idx]->BeginWrite(*state.child_states[child_idx]);
1985: 	}
1986: }
1987: 
1988: void StructColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t count) {
1989: 	auto &state = state_p.Cast<StructColumnWriterState>();
1990: 	auto &child_vectors = StructVector::GetEntries(vector);
1991: 	for (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {
1992: 		child_writers[child_idx]->Write(*state.child_states[child_idx], *child_vectors[child_idx], count);
1993: 	}
1994: }
1995: 
1996: void StructColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {
1997: 	auto &state = state_p.Cast<StructColumnWriterState>();
1998: 	for (idx_t child_idx = 0; child_idx < child_writers.size(); child_idx++) {
1999: 		// we add the null count of the struct to the null count of the children
2000: 		state.child_states[child_idx]->null_count += state_p.null_count;
2001: 		child_writers[child_idx]->FinalizeWrite(*state.child_states[child_idx]);
2002: 	}
2003: }
2004: 
2005: //===--------------------------------------------------------------------===//
2006: // List Column Writer
2007: //===--------------------------------------------------------------------===//
2008: class ListColumnWriter : public ColumnWriter {
2009: public:
2010: 	ListColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, idx_t max_repeat,
2011: 	                 idx_t max_define, unique_ptr<ColumnWriter> child_writer_p, bool can_have_nulls)
2012: 	    : ColumnWriter(writer, schema_idx, std::move(schema_path_p), max_repeat, max_define, can_have_nulls),
2013: 	      child_writer(std::move(child_writer_p)) {
2014: 	}
2015: 	~ListColumnWriter() override = default;
2016: 
2017: 	unique_ptr<ColumnWriter> child_writer;
2018: 
2019: public:
2020: 	unique_ptr<ColumnWriterState> InitializeWriteState(duckdb_parquet::RowGroup &row_group) override;
2021: 	bool HasAnalyze() override;
2022: 	void Analyze(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;
2023: 	void FinalizeAnalyze(ColumnWriterState &state) override;
2024: 	void Prepare(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;
2025: 
2026: 	void BeginWrite(ColumnWriterState &state) override;
2027: 	void Write(ColumnWriterState &state, Vector &vector, idx_t count) override;
2028: 	void FinalizeWrite(ColumnWriterState &state) override;
2029: };
2030: 
2031: class ListColumnWriterState : public ColumnWriterState {
2032: public:
2033: 	ListColumnWriterState(duckdb_parquet::RowGroup &row_group, idx_t col_idx) : row_group(row_group), col_idx(col_idx) {
2034: 	}
2035: 	~ListColumnWriterState() override = default;
2036: 
2037: 	duckdb_parquet::RowGroup &row_group;
2038: 	idx_t col_idx;
2039: 	unique_ptr<ColumnWriterState> child_state;
2040: 	idx_t parent_index = 0;
2041: };
2042: 
2043: unique_ptr<ColumnWriterState> ListColumnWriter::InitializeWriteState(duckdb_parquet::RowGroup &row_group) {
2044: 	auto result = make_uniq<ListColumnWriterState>(row_group, row_group.columns.size());
2045: 	result->child_state = child_writer->InitializeWriteState(row_group);
2046: 	return std::move(result);
2047: }
2048: 
2049: bool ListColumnWriter::HasAnalyze() {
2050: 	return child_writer->HasAnalyze();
2051: }
2052: void ListColumnWriter::Analyze(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {
2053: 	auto &state = state_p.Cast<ListColumnWriterState>();
2054: 	auto &list_child = ListVector::GetEntry(vector);
2055: 	auto list_count = ListVector::GetListSize(vector);
2056: 	child_writer->Analyze(*state.child_state, &state_p, list_child, list_count);
2057: }
2058: 
2059: void ListColumnWriter::FinalizeAnalyze(ColumnWriterState &state_p) {
2060: 	auto &state = state_p.Cast<ListColumnWriterState>();
2061: 	child_writer->FinalizeAnalyze(*state.child_state);
2062: }
2063: 
2064: idx_t GetConsecutiveChildList(Vector &list, Vector &result, idx_t offset, idx_t count) {
2065: 	// returns a consecutive child list that fully flattens and repeats all required elements
2066: 	auto &validity = FlatVector::Validity(list);
2067: 	auto list_entries = FlatVector::GetData<list_entry_t>(list);
2068: 	bool is_consecutive = true;
2069: 	idx_t total_length = 0;
2070: 	for (idx_t c = offset; c < offset + count; c++) {
2071: 		if (!validity.RowIsValid(c)) {
2072: 			continue;
2073: 		}
2074: 		if (list_entries[c].offset != total_length) {
2075: 			is_consecutive = false;
2076: 		}
2077: 		total_length += list_entries[c].length;
2078: 	}
2079: 	if (is_consecutive) {
2080: 		// already consecutive - leave it as-is
2081: 		return total_length;
2082: 	}
2083: 	SelectionVector sel(total_length);
2084: 	idx_t index = 0;
2085: 	for (idx_t c = offset; c < offset + count; c++) {
2086: 		if (!validity.RowIsValid(c)) {
2087: 			continue;
2088: 		}
2089: 		for (idx_t k = 0; k < list_entries[c].length; k++) {
2090: 			sel.set_index(index++, list_entries[c].offset + k);
2091: 		}
2092: 	}
2093: 	result.Slice(sel, total_length);
2094: 	result.Flatten(total_length);
2095: 	return total_length;
2096: }
2097: 
2098: void ListColumnWriter::Prepare(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {
2099: 	auto &state = state_p.Cast<ListColumnWriterState>();
2100: 
2101: 	auto list_data = FlatVector::GetData<list_entry_t>(vector);
2102: 	auto &validity = FlatVector::Validity(vector);
2103: 
2104: 	// write definition levels and repeats
2105: 	idx_t start = 0;
2106: 	idx_t vcount = parent ? parent->definition_levels.size() - state.parent_index : count;
2107: 	idx_t vector_index = 0;
2108: 	for (idx_t i = start; i < vcount; i++) {
2109: 		idx_t parent_index = state.parent_index + i;
2110: 		if (parent && !parent->is_empty.empty() && parent->is_empty[parent_index]) {
2111: 			state.definition_levels.push_back(parent->definition_levels[parent_index]);
2112: 			state.repetition_levels.push_back(parent->repetition_levels[parent_index]);
2113: 			state.is_empty.push_back(true);
2114: 			continue;
2115: 		}
2116: 		auto first_repeat_level =
2117: 		    parent && !parent->repetition_levels.empty() ? parent->repetition_levels[parent_index] : max_repeat;
2118: 		if (parent && parent->definition_levels[parent_index] != PARQUET_DEFINE_VALID) {
2119: 			state.definition_levels.push_back(parent->definition_levels[parent_index]);
2120: 			state.repetition_levels.push_back(first_repeat_level);
2121: 			state.is_empty.push_back(true);
2122: 		} else if (validity.RowIsValid(vector_index)) {
2123: 			// push the repetition levels
2124: 			if (list_data[vector_index].length == 0) {
2125: 				state.definition_levels.push_back(max_define);
2126: 				state.is_empty.push_back(true);
2127: 			} else {
2128: 				state.definition_levels.push_back(PARQUET_DEFINE_VALID);
2129: 				state.is_empty.push_back(false);
2130: 			}
2131: 			state.repetition_levels.push_back(first_repeat_level);
2132: 			for (idx_t k = 1; k < list_data[vector_index].length; k++) {
2133: 				state.repetition_levels.push_back(max_repeat + 1);
2134: 				state.definition_levels.push_back(PARQUET_DEFINE_VALID);
2135: 				state.is_empty.push_back(false);
2136: 			}
2137: 		} else {
2138: 			if (!can_have_nulls) {
2139: 				throw IOException("Parquet writer: map key column is not allowed to contain NULL values");
2140: 			}
2141: 			state.definition_levels.push_back(max_define - 1);
2142: 			state.repetition_levels.push_back(first_repeat_level);
2143: 			state.is_empty.push_back(true);
2144: 		}
2145: 		vector_index++;
2146: 	}
2147: 	state.parent_index += vcount;
2148: 
2149: 	auto &list_child = ListVector::GetEntry(vector);
2150: 	Vector child_list(list_child);
2151: 	auto child_length = GetConsecutiveChildList(vector, child_list, 0, count);
2152: 	child_writer->Prepare(*state.child_state, &state_p, child_list, child_length);
2153: }
2154: 
2155: void ListColumnWriter::BeginWrite(ColumnWriterState &state_p) {
2156: 	auto &state = state_p.Cast<ListColumnWriterState>();
2157: 	child_writer->BeginWrite(*state.child_state);
2158: }
2159: 
2160: void ListColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t count) {
2161: 	auto &state = state_p.Cast<ListColumnWriterState>();
2162: 
2163: 	auto &list_child = ListVector::GetEntry(vector);
2164: 	Vector child_list(list_child);
2165: 	auto child_length = GetConsecutiveChildList(vector, child_list, 0, count);
2166: 	child_writer->Write(*state.child_state, child_list, child_length);
2167: }
2168: 
2169: void ListColumnWriter::FinalizeWrite(ColumnWriterState &state_p) {
2170: 	auto &state = state_p.Cast<ListColumnWriterState>();
2171: 	child_writer->FinalizeWrite(*state.child_state);
2172: }
2173: 
2174: //===--------------------------------------------------------------------===//
2175: // Array Column Writer
2176: //===--------------------------------------------------------------------===//
2177: class ArrayColumnWriter : public ListColumnWriter {
2178: public:
2179: 	ArrayColumnWriter(ParquetWriter &writer, idx_t schema_idx, vector<string> schema_path_p, idx_t max_repeat,
2180: 	                  idx_t max_define, unique_ptr<ColumnWriter> child_writer_p, bool can_have_nulls)
2181: 	    : ListColumnWriter(writer, schema_idx, std::move(schema_path_p), max_repeat, max_define,
2182: 	                       std::move(child_writer_p), can_have_nulls) {
2183: 	}
2184: 	~ArrayColumnWriter() override = default;
2185: 
2186: public:
2187: 	void Analyze(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;
2188: 	void Prepare(ColumnWriterState &state, ColumnWriterState *parent, Vector &vector, idx_t count) override;
2189: 	void Write(ColumnWriterState &state, Vector &vector, idx_t count) override;
2190: };
2191: 
2192: void ArrayColumnWriter::Analyze(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {
2193: 	auto &state = state_p.Cast<ListColumnWriterState>();
2194: 	auto &array_child = ArrayVector::GetEntry(vector);
2195: 	auto array_size = ArrayType::GetSize(vector.GetType());
2196: 	child_writer->Analyze(*state.child_state, &state_p, array_child, array_size * count);
2197: }
2198: 
2199: void ArrayColumnWriter::Prepare(ColumnWriterState &state_p, ColumnWriterState *parent, Vector &vector, idx_t count) {
2200: 	auto &state = state_p.Cast<ListColumnWriterState>();
2201: 
2202: 	auto array_size = ArrayType::GetSize(vector.GetType());
2203: 	auto &validity = FlatVector::Validity(vector);
2204: 
2205: 	// write definition levels and repeats
2206: 	// the main difference between this and ListColumnWriter::Prepare is that we need to make sure to write out
2207: 	// repetition levels and definitions for the child elements of the array even if the array itself is NULL.
2208: 	idx_t start = 0;
2209: 	idx_t vcount = parent ? parent->definition_levels.size() - state.parent_index : count;
2210: 	idx_t vector_index = 0;
2211: 	for (idx_t i = start; i < vcount; i++) {
2212: 		idx_t parent_index = state.parent_index + i;
2213: 		if (parent && !parent->is_empty.empty() && parent->is_empty[parent_index]) {
2214: 			state.definition_levels.push_back(parent->definition_levels[parent_index]);
2215: 			state.repetition_levels.push_back(parent->repetition_levels[parent_index]);
2216: 			state.is_empty.push_back(true);
2217: 			continue;
2218: 		}
2219: 		auto first_repeat_level =
2220: 		    parent && !parent->repetition_levels.empty() ? parent->repetition_levels[parent_index] : max_repeat;
2221: 		if (parent && parent->definition_levels[parent_index] != PARQUET_DEFINE_VALID) {
2222: 			state.definition_levels.push_back(parent->definition_levels[parent_index]);
2223: 			state.repetition_levels.push_back(first_repeat_level);
2224: 			state.is_empty.push_back(false);
2225: 			for (idx_t k = 1; k < array_size; k++) {
2226: 				state.repetition_levels.push_back(max_repeat + 1);
2227: 				state.definition_levels.push_back(parent->definition_levels[parent_index]);
2228: 				state.is_empty.push_back(false);
2229: 			}
2230: 		} else if (validity.RowIsValid(vector_index)) {
2231: 			// push the repetition levels
2232: 			state.definition_levels.push_back(PARQUET_DEFINE_VALID);
2233: 			state.is_empty.push_back(false);
2234: 
2235: 			state.repetition_levels.push_back(first_repeat_level);
2236: 			for (idx_t k = 1; k < array_size; k++) {
2237: 				state.repetition_levels.push_back(max_repeat + 1);
2238: 				state.definition_levels.push_back(PARQUET_DEFINE_VALID);
2239: 				state.is_empty.push_back(false);
2240: 			}
2241: 		} else {
2242: 			state.definition_levels.push_back(max_define - 1);
2243: 			state.repetition_levels.push_back(first_repeat_level);
2244: 			state.is_empty.push_back(false);
2245: 			for (idx_t k = 1; k < array_size; k++) {
2246: 				state.repetition_levels.push_back(max_repeat + 1);
2247: 				state.definition_levels.push_back(max_define - 1);
2248: 				state.is_empty.push_back(false);
2249: 			}
2250: 		}
2251: 		vector_index++;
2252: 	}
2253: 	state.parent_index += vcount;
2254: 
2255: 	auto &array_child = ArrayVector::GetEntry(vector);
2256: 	child_writer->Prepare(*state.child_state, &state_p, array_child, count * array_size);
2257: }
2258: 
2259: void ArrayColumnWriter::Write(ColumnWriterState &state_p, Vector &vector, idx_t count) {
2260: 	auto &state = state_p.Cast<ListColumnWriterState>();
2261: 	auto array_size = ArrayType::GetSize(vector.GetType());
2262: 	auto &array_child = ArrayVector::GetEntry(vector);
2263: 	child_writer->Write(*state.child_state, array_child, count * array_size);
2264: }
2265: 
2266: // special double/float class to deal with dictionary encoding and NaN equality
2267: struct double_na_equal {
2268: 	double_na_equal() : val(0) {
2269: 	}
2270: 	explicit double_na_equal(const double val_p) : val(val_p) {
2271: 	}
2272: 	// NOLINTNEXTLINE: allow implicit conversion to double
2273: 	operator double() const {
2274: 		return val;
2275: 	}
2276: 
2277: 	bool operator==(const double &right) const {
2278: 		if (std::isnan(val) && std::isnan(right)) {
2279: 			return true;
2280: 		}
2281: 		return val == right;
2282: 	}
2283: 	double val;
2284: };
2285: 
2286: struct float_na_equal {
2287: 	float_na_equal() : val(0) {
2288: 	}
2289: 	explicit float_na_equal(const float val_p) : val(val_p) {
2290: 	}
2291: 	// NOLINTNEXTLINE: allow implicit conversion to float
2292: 	operator float() const {
2293: 		return val;
2294: 	}
2295: 	bool operator==(const float &right) const {
2296: 		if (std::isnan(val) && std::isnan(right)) {
2297: 			return true;
2298: 		}
2299: 		return val == right;
2300: 	}
2301: 	float val;
2302: };
2303: 
2304: //===--------------------------------------------------------------------===//
2305: // Create Column Writer
2306: //===--------------------------------------------------------------------===//
2307: 
2308: unique_ptr<ColumnWriter> ColumnWriter::CreateWriterRecursive(ClientContext &context,
2309:                                                              vector<duckdb_parquet::SchemaElement> &schemas,
2310:                                                              ParquetWriter &writer, const LogicalType &type,
2311:                                                              const string &name, vector<string> schema_path,
2312:                                                              optional_ptr<const ChildFieldIDs> field_ids,
2313:                                                              idx_t max_repeat, idx_t max_define, bool can_have_nulls) {
2314: 	auto null_type = can_have_nulls ? FieldRepetitionType::OPTIONAL : FieldRepetitionType::REQUIRED;
2315: 	if (!can_have_nulls) {
2316: 		max_define--;
2317: 	}
2318: 	idx_t schema_idx = schemas.size();
2319: 
2320: 	optional_ptr<const FieldID> field_id;
2321: 	optional_ptr<const ChildFieldIDs> child_field_ids;
2322: 	if (field_ids) {
2323: 		auto field_id_it = field_ids->ids->find(name);
2324: 		if (field_id_it != field_ids->ids->end()) {
2325: 			field_id = &field_id_it->second;
2326: 			child_field_ids = &field_id->child_field_ids;
2327: 		}
2328: 	}
2329: 
2330: 	if (type.id() == LogicalTypeId::STRUCT || type.id() == LogicalTypeId::UNION) {
2331: 		auto &child_types = StructType::GetChildTypes(type);
2332: 		// set up the schema element for this struct
2333: 		duckdb_parquet::SchemaElement schema_element;
2334: 		schema_element.repetition_type = null_type;
2335: 		schema_element.num_children = UnsafeNumericCast<int32_t>(child_types.size());
2336: 		schema_element.__isset.num_children = true;
2337: 		schema_element.__isset.type = false;
2338: 		schema_element.__isset.repetition_type = true;
2339: 		schema_element.name = name;
2340: 		if (field_id && field_id->set) {
2341: 			schema_element.__isset.field_id = true;
2342: 			schema_element.field_id = field_id->field_id;
2343: 		}
2344: 		schemas.push_back(std::move(schema_element));
2345: 		schema_path.push_back(name);
2346: 
2347: 		// construct the child types recursively
2348: 		vector<unique_ptr<ColumnWriter>> child_writers;
2349: 		child_writers.reserve(child_types.size());
2350: 		for (auto &child_type : child_types) {
2351: 			child_writers.push_back(CreateWriterRecursive(context, schemas, writer, child_type.second, child_type.first,
2352: 			                                              schema_path, child_field_ids, max_repeat, max_define + 1));
2353: 		}
2354: 		return make_uniq<StructColumnWriter>(writer, schema_idx, std::move(schema_path), max_repeat, max_define,
2355: 		                                     std::move(child_writers), can_have_nulls);
2356: 	}
2357: 	if (type.id() == LogicalTypeId::LIST || type.id() == LogicalTypeId::ARRAY) {
2358: 		auto is_list = type.id() == LogicalTypeId::LIST;
2359: 		auto &child_type = is_list ? ListType::GetChildType(type) : ArrayType::GetChildType(type);
2360: 		// set up the two schema elements for the list
2361: 		// for some reason we only set the converted type in the OPTIONAL element
2362: 		// first an OPTIONAL element
2363: 		duckdb_parquet::SchemaElement optional_element;
2364: 		optional_element.repetition_type = null_type;
2365: 		optional_element.num_children = 1;
2366: 		optional_element.converted_type = ConvertedType::LIST;
2367: 		optional_element.__isset.num_children = true;
2368: 		optional_element.__isset.type = false;
2369: 		optional_element.__isset.repetition_type = true;
2370: 		optional_element.__isset.converted_type = true;
2371: 		optional_element.name = name;
2372: 		if (field_id && field_id->set) {
2373: 			optional_element.__isset.field_id = true;
2374: 			optional_element.field_id = field_id->field_id;
2375: 		}
2376: 		schemas.push_back(std::move(optional_element));
2377: 		schema_path.push_back(name);
2378: 
2379: 		// then a REPEATED element
2380: 		duckdb_parquet::SchemaElement repeated_element;
2381: 		repeated_element.repetition_type = FieldRepetitionType::REPEATED;
2382: 		repeated_element.num_children = 1;
2383: 		repeated_element.__isset.num_children = true;
2384: 		repeated_element.__isset.type = false;
2385: 		repeated_element.__isset.repetition_type = true;
2386: 		repeated_element.name = is_list ? "list" : "array";
2387: 		schemas.push_back(std::move(repeated_element));
2388: 		schema_path.emplace_back(is_list ? "list" : "array");
2389: 
2390: 		auto child_writer = CreateWriterRecursive(context, schemas, writer, child_type, "element", schema_path,
2391: 		                                          child_field_ids, max_repeat + 1, max_define + 2);
2392: 		if (is_list) {
2393: 			return make_uniq<ListColumnWriter>(writer, schema_idx, std::move(schema_path), max_repeat, max_define,
2394: 			                                   std::move(child_writer), can_have_nulls);
2395: 		} else {
2396: 			return make_uniq<ArrayColumnWriter>(writer, schema_idx, std::move(schema_path), max_repeat, max_define,
2397: 			                                    std::move(child_writer), can_have_nulls);
2398: 		}
2399: 	}
2400: 	if (type.id() == LogicalTypeId::MAP) {
2401: 		// map type
2402: 		// maps are stored as follows:
2403: 		// <map-repetition> group <name> (MAP) {
2404: 		// 	repeated group key_value {
2405: 		// 		required <key-type> key;
2406: 		// 		<value-repetition> <value-type> value;
2407: 		// 	}
2408: 		// }
2409: 		// top map element
2410: 		duckdb_parquet::SchemaElement top_element;
2411: 		top_element.repetition_type = null_type;
2412: 		top_element.num_children = 1;
2413: 		top_element.converted_type = ConvertedType::MAP;
2414: 		top_element.__isset.repetition_type = true;
2415: 		top_element.__isset.num_children = true;
2416: 		top_element.__isset.converted_type = true;
2417: 		top_element.__isset.type = false;
2418: 		top_element.name = name;
2419: 		if (field_id && field_id->set) {
2420: 			top_element.__isset.field_id = true;
2421: 			top_element.field_id = field_id->field_id;
2422: 		}
2423: 		schemas.push_back(std::move(top_element));
2424: 		schema_path.push_back(name);
2425: 
2426: 		// key_value element
2427: 		duckdb_parquet::SchemaElement kv_element;
2428: 		kv_element.repetition_type = FieldRepetitionType::REPEATED;
2429: 		kv_element.num_children = 2;
2430: 		kv_element.__isset.repetition_type = true;
2431: 		kv_element.__isset.num_children = true;
2432: 		kv_element.__isset.type = false;
2433: 		kv_element.name = "key_value";
2434: 		schemas.push_back(std::move(kv_element));
2435: 		schema_path.emplace_back("key_value");
2436: 
2437: 		// construct the child types recursively
2438: 		vector<LogicalType> kv_types {MapType::KeyType(type), MapType::ValueType(type)};
2439: 		vector<string> kv_names {"key", "value"};
2440: 		vector<unique_ptr<ColumnWriter>> child_writers;
2441: 		child_writers.reserve(2);
2442: 		for (idx_t i = 0; i < 2; i++) {
2443: 			// key needs to be marked as REQUIRED
2444: 			bool is_key = i == 0;
2445: 			auto child_writer = CreateWriterRecursive(context, schemas, writer, kv_types[i], kv_names[i], schema_path,
2446: 			                                          child_field_ids, max_repeat + 1, max_define + 2, !is_key);
2447: 
2448: 			child_writers.push_back(std::move(child_writer));
2449: 		}
2450: 		auto struct_writer = make_uniq<StructColumnWriter>(writer, schema_idx, schema_path, max_repeat, max_define,
2451: 		                                                   std::move(child_writers), can_have_nulls);
2452: 		return make_uniq<ListColumnWriter>(writer, schema_idx, schema_path, max_repeat, max_define,
2453: 		                                   std::move(struct_writer), can_have_nulls);
2454: 	}
2455: 	duckdb_parquet::SchemaElement schema_element;
2456: 	schema_element.type = ParquetWriter::DuckDBTypeToParquetType(type);
2457: 	schema_element.repetition_type = null_type;
2458: 	schema_element.__isset.num_children = false;
2459: 	schema_element.__isset.type = true;
2460: 	schema_element.__isset.repetition_type = true;
2461: 	schema_element.name = name;
2462: 	if (field_id && field_id->set) {
2463: 		schema_element.__isset.field_id = true;
2464: 		schema_element.field_id = field_id->field_id;
2465: 	}
2466: 	ParquetWriter::SetSchemaProperties(type, schema_element);
2467: 	schemas.push_back(std::move(schema_element));
2468: 	schema_path.push_back(name);
2469: 	if (type.id() == LogicalTypeId::BLOB && type.GetAlias() == "WKB_BLOB" &&
2470: 	    GeoParquetFileMetadata::IsGeoParquetConversionEnabled(context)) {
2471: 		return make_uniq<WKBColumnWriter>(context, writer, schema_idx, std::move(schema_path), max_repeat, max_define,
2472: 		                                  can_have_nulls, name);
2473: 	}
2474: 
2475: 	switch (type.id()) {
2476: 	case LogicalTypeId::BOOLEAN:
2477: 		return make_uniq<BooleanColumnWriter>(writer, schema_idx, std::move(schema_path), max_repeat, max_define,
2478: 		                                      can_have_nulls);
2479: 	case LogicalTypeId::TINYINT:
2480: 		return make_uniq<StandardColumnWriter<int8_t, int32_t>>(writer, schema_idx, std::move(schema_path), max_repeat,
2481: 		                                                        max_define, can_have_nulls);
2482: 	case LogicalTypeId::SMALLINT:
2483: 		return make_uniq<StandardColumnWriter<int16_t, int32_t>>(writer, schema_idx, std::move(schema_path), max_repeat,
2484: 		                                                         max_define, can_have_nulls);
2485: 	case LogicalTypeId::INTEGER:
2486: 	case LogicalTypeId::DATE:
2487: 		return make_uniq<StandardColumnWriter<int32_t, int32_t>>(writer, schema_idx, std::move(schema_path), max_repeat,
2488: 		                                                         max_define, can_have_nulls);
2489: 	case LogicalTypeId::BIGINT:
2490: 	case LogicalTypeId::TIME:
2491: 	case LogicalTypeId::TIMESTAMP:
2492: 	case LogicalTypeId::TIMESTAMP_TZ:
2493: 	case LogicalTypeId::TIMESTAMP_MS:
2494: 		return make_uniq<StandardColumnWriter<int64_t, int64_t>>(writer, schema_idx, std::move(schema_path), max_repeat,
2495: 		                                                         max_define, can_have_nulls);
2496: 	case LogicalTypeId::TIME_TZ:
2497: 		return make_uniq<StandardColumnWriter<dtime_tz_t, int64_t, ParquetTimeTZOperator>>(
2498: 		    writer, schema_idx, std::move(schema_path), max_repeat, max_define, can_have_nulls);
2499: 	case LogicalTypeId::HUGEINT:
2500: 		return make_uniq<StandardColumnWriter<hugeint_t, double, ParquetHugeintOperator>>(
2501: 		    writer, schema_idx, std::move(schema_path), max_repeat, max_define, can_have_nulls);
2502: 	case LogicalTypeId::UHUGEINT:
2503: 		return make_uniq<StandardColumnWriter<uhugeint_t, double, ParquetUhugeintOperator>>(
2504: 		    writer, schema_idx, std::move(schema_path), max_repeat, max_define, can_have_nulls);
2505: 	case LogicalTypeId::TIMESTAMP_NS:
2506: 		return make_uniq<StandardColumnWriter<int64_t, int64_t, ParquetTimestampNSOperator>>(
2507: 		    writer, schema_idx, std::move(schema_path), max_repeat, max_define, can_have_nulls);
2508: 	case LogicalTypeId::TIMESTAMP_SEC:
2509: 		return make_uniq<StandardColumnWriter<int64_t, int64_t, ParquetTimestampSOperator>>(
2510: 		    writer, schema_idx, std::move(schema_path), max_repeat, max_define, can_have_nulls);
2511: 	case LogicalTypeId::UTINYINT:
2512: 		return make_uniq<StandardColumnWriter<uint8_t, int32_t>>(writer, schema_idx, std::move(schema_path), max_repeat,
2513: 		                                                         max_define, can_have_nulls);
2514: 	case LogicalTypeId::USMALLINT:
2515: 		return make_uniq<StandardColumnWriter<uint16_t, int32_t>>(writer, schema_idx, std::move(schema_path),
2516: 		                                                          max_repeat, max_define, can_have_nulls);
2517: 	case LogicalTypeId::UINTEGER:
2518: 		return make_uniq<StandardColumnWriter<uint32_t, uint32_t>>(writer, schema_idx, std::move(schema_path),
2519: 		                                                           max_repeat, max_define, can_have_nulls);
2520: 	case LogicalTypeId::UBIGINT:
2521: 		return make_uniq<StandardColumnWriter<uint64_t, uint64_t>>(writer, schema_idx, std::move(schema_path),
2522: 		                                                           max_repeat, max_define, can_have_nulls);
2523: 	case LogicalTypeId::FLOAT:
2524: 		return make_uniq<StandardColumnWriter<float_na_equal, float>>(writer, schema_idx, std::move(schema_path),
2525: 		                                                              max_repeat, max_define, can_have_nulls);
2526: 	case LogicalTypeId::DOUBLE:
2527: 		return make_uniq<StandardColumnWriter<double_na_equal, double>>(writer, schema_idx, std::move(schema_path),
2528: 		                                                                max_repeat, max_define, can_have_nulls);
2529: 	case LogicalTypeId::DECIMAL:
2530: 		switch (type.InternalType()) {
2531: 		case PhysicalType::INT16:
2532: 			return make_uniq<StandardColumnWriter<int16_t, int32_t>>(writer, schema_idx, std::move(schema_path),
2533: 			                                                         max_repeat, max_define, can_have_nulls);
2534: 		case PhysicalType::INT32:
2535: 			return make_uniq<StandardColumnWriter<int32_t, int32_t>>(writer, schema_idx, std::move(schema_path),
2536: 			                                                         max_repeat, max_define, can_have_nulls);
2537: 		case PhysicalType::INT64:
2538: 			return make_uniq<StandardColumnWriter<int64_t, int64_t>>(writer, schema_idx, std::move(schema_path),
2539: 			                                                         max_repeat, max_define, can_have_nulls);
2540: 		default:
2541: 			return make_uniq<FixedDecimalColumnWriter>(writer, schema_idx, std::move(schema_path), max_repeat,
2542: 			                                           max_define, can_have_nulls);
2543: 		}
2544: 	case LogicalTypeId::BLOB:
2545: 	case LogicalTypeId::VARCHAR:
2546: 		return make_uniq<StandardColumnWriter<string_t, string_t, ParquetStringOperator>>(
2547: 		    writer, schema_idx, std::move(schema_path), max_repeat, max_define, can_have_nulls);
2548: 	case LogicalTypeId::UUID:
2549: 		return make_uniq<StandardColumnWriter<hugeint_t, ParquetUUIDTargetType, ParquetUUIDOperator>>(
2550: 		    writer, schema_idx, std::move(schema_path), max_repeat, max_define, can_have_nulls);
2551: 	case LogicalTypeId::INTERVAL:
2552: 		return make_uniq<StandardColumnWriter<interval_t, ParquetIntervalTargetType, ParquetIntervalOperator>>(
2553: 		    writer, schema_idx, std::move(schema_path), max_repeat, max_define, can_have_nulls);
2554: 	case LogicalTypeId::ENUM:
2555: 		return make_uniq<EnumColumnWriter>(writer, type, schema_idx, std::move(schema_path), max_repeat, max_define,
2556: 		                                   can_have_nulls);
2557: 	default:
2558: 		throw InternalException("Unsupported type \"%s\" in Parquet writer", type.ToString());
2559: 	}
2560: }
2561: 
2562: template <>
2563: struct NumericLimits<float_na_equal> {
2564: 	static constexpr float Minimum() {
2565: 		return std::numeric_limits<float>::lowest();
2566: 	};
2567: 	static constexpr float Maximum() {
2568: 		return std::numeric_limits<float>::max();
2569: 	};
2570: 	static constexpr bool IsSigned() {
2571: 		return std::is_signed<float>::value;
2572: 	}
2573: 	static constexpr bool IsIntegral() {
2574: 		return std::is_integral<float>::value;
2575: 	}
2576: };
2577: 
2578: template <>
2579: struct NumericLimits<double_na_equal> {
2580: 	static constexpr double Minimum() {
2581: 		return std::numeric_limits<double>::lowest();
2582: 	};
2583: 	static constexpr double Maximum() {
2584: 		return std::numeric_limits<double>::max();
2585: 	};
2586: 	static constexpr bool IsSigned() {
2587: 		return std::is_signed<double>::value;
2588: 	}
2589: 	static constexpr bool IsIntegral() {
2590: 		return std::is_integral<double>::value;
2591: 	}
2592: };
2593: 
2594: } // namespace duckdb
2595: 
2596: namespace std {
2597: template <>
2598: struct hash<duckdb::ParquetIntervalTargetType> {
2599: 	size_t operator()(const duckdb::ParquetIntervalTargetType &val) const {
2600: 		return duckdb::Hash(duckdb::const_char_ptr_cast(val.bytes),
2601: 		                    duckdb::ParquetIntervalTargetType::PARQUET_INTERVAL_SIZE);
2602: 	}
2603: };
2604: 
2605: template <>
2606: struct hash<duckdb::ParquetUUIDTargetType> {
2607: 	size_t operator()(const duckdb::ParquetUUIDTargetType &val) const {
2608: 		return duckdb::Hash(duckdb::const_char_ptr_cast(val.bytes), duckdb::ParquetUUIDTargetType::PARQUET_UUID_SIZE);
2609: 	}
2610: };
2611: 
2612: template <>
2613: struct hash<duckdb::float_na_equal> {
2614: 	size_t operator()(const duckdb::float_na_equal &val) const {
2615: 		if (std::isnan(val.val)) {
2616: 			return duckdb::Hash<float>(std::numeric_limits<float>::quiet_NaN());
2617: 		}
2618: 		return duckdb::Hash<float>(val.val);
2619: 	}
2620: };
2621: 
2622: template <>
2623: struct hash<duckdb::double_na_equal> {
2624: 	inline size_t operator()(const duckdb::double_na_equal &val) const {
2625: 		if (std::isnan(val.val)) {
2626: 			return duckdb::Hash<double>(std::numeric_limits<double>::quiet_NaN());
2627: 		}
2628: 		return duckdb::Hash<double>(val.val);
2629: 	}
2630: };
2631: } // namespace std
[end of extension/parquet/column_writer.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: