{
  "repo": "duckdb/duckdb",
  "pull_number": 6436,
  "instance_id": "duckdb__duckdb-6436",
  "issue_numbers": [
    "6343"
  ],
  "base_commit": "baf04ba00975fb8eff8f49206d0e3095836be326",
  "patch": "diff --git a/src/common/bind_helpers.cpp b/src/common/bind_helpers.cpp\nindex 5567bcb6741f..9074b3e7ff58 100644\n--- a/src/common/bind_helpers.cpp\n+++ b/src/common/bind_helpers.cpp\n@@ -4,6 +4,7 @@\n #include \"duckdb/common/exception.hpp\"\n #include \"duckdb/common/types/value.hpp\"\n #include \"duckdb/common/case_insensitive_map.hpp\"\n+#include <numeric>\n \n namespace duckdb {\n \n@@ -64,4 +65,58 @@ vector<bool> ParseColumnList(const Value &value, vector<string> &names, const st\n \treturn ParseColumnList(children, names, loption);\n }\n \n+vector<idx_t> ParseColumnsOrdered(const vector<Value> &set, vector<string> &names, const string &loption) {\n+\tvector<idx_t> result;\n+\n+\tif (set.empty()) {\n+\t\tthrow BinderException(\"\\\"%s\\\" expects a column list or * as parameter\", loption);\n+\t}\n+\n+\t// Maps option to bool indicating if its found and the index in the original set\n+\tcase_insensitive_map_t<std::pair<bool, idx_t>> option_map;\n+\tfor (idx_t i = 0; i < set.size(); i++) {\n+\t\toption_map[set[i].ToString()] = {false, i};\n+\t}\n+\tresult.resize(option_map.size());\n+\n+\tfor (idx_t i = 0; i < names.size(); i++) {\n+\t\tauto entry = option_map.find(names[i]);\n+\t\tif (entry != option_map.end()) {\n+\t\t\tresult[entry->second.second] = i;\n+\t\t\tentry->second.first = true;\n+\t\t}\n+\t}\n+\tfor (auto &entry : option_map) {\n+\t\tif (!entry.second.first) {\n+\t\t\tthrow BinderException(\"\\\"%s\\\" expected to find %s, but it was not found in the table\", loption,\n+\t\t\t                      entry.first.c_str());\n+\t\t}\n+\t}\n+\treturn result;\n+}\n+\n+vector<idx_t> ParseColumnsOrdered(const Value &value, vector<string> &names, const string &loption) {\n+\tvector<idx_t> result;\n+\n+\t// Only accept a list of arguments\n+\tif (value.type().id() != LogicalTypeId::LIST) {\n+\t\t// Support a single argument if it's '*'\n+\t\tif (value.type().id() == LogicalTypeId::VARCHAR && value.GetValue<string>() == \"*\") {\n+\t\t\tresult.resize(names.size(), 0);\n+\t\t\tstd::iota(std::begin(result), std::end(result), 0);\n+\t\t\treturn result;\n+\t\t}\n+\t\tthrow BinderException(\"\\\"%s\\\" expects a column list or * as parameter\", loption);\n+\t}\n+\tauto &children = ListValue::GetChildren(value);\n+\t// accept '*' as single argument\n+\tif (children.size() == 1 && children[0].type().id() == LogicalTypeId::VARCHAR &&\n+\t    children[0].GetValue<string>() == \"*\") {\n+\t\tresult.resize(names.size(), 0);\n+\t\tstd::iota(std::begin(result), std::end(result), 0);\n+\t\treturn result;\n+\t}\n+\treturn ParseColumnsOrdered(children, names, loption);\n+}\n+\n } // namespace duckdb\ndiff --git a/src/include/duckdb/common/bind_helpers.hpp b/src/include/duckdb/common/bind_helpers.hpp\nindex 2f7e9e623de9..992047628c6c 100644\n--- a/src/include/duckdb/common/bind_helpers.hpp\n+++ b/src/include/duckdb/common/bind_helpers.hpp\n@@ -17,5 +17,7 @@ class Value;\n Value ConvertVectorToValue(vector<Value> set);\n vector<bool> ParseColumnList(const vector<Value> &set, vector<string> &names, const string &option_name);\n vector<bool> ParseColumnList(const Value &value, vector<string> &names, const string &option_name);\n+vector<idx_t> ParseColumnsOrdered(const vector<Value> &set, vector<string> &names, const string &loption);\n+vector<idx_t> ParseColumnsOrdered(const Value &value, vector<string> &names, const string &loption);\n \n } // namespace duckdb\ndiff --git a/src/planner/binder/statement/bind_copy.cpp b/src/planner/binder/statement/bind_copy.cpp\nindex 5f34cc0bf40c..66f666431207 100644\n--- a/src/planner/binder/statement/bind_copy.cpp\n+++ b/src/planner/binder/statement/bind_copy.cpp\n@@ -23,16 +23,6 @@\n \n namespace duckdb {\n \n-static vector<idx_t> ColumnListToIndices(const vector<bool> &vec) {\n-\tvector<idx_t> ret;\n-\tfor (idx_t i = 0; i < vec.size(); i++) {\n-\t\tif (vec[i]) {\n-\t\t\tret.push_back(i);\n-\t\t}\n-\t}\n-\treturn ret;\n-}\n-\n vector<string> GetUniqueNames(const vector<string> &original_names) {\n \tunordered_set<string> name_set;\n \tvector<string> unique_names;\n@@ -115,7 +105,7 @@ BoundStatement Binder::BindCopyTo(CopyStatement &stmt) {\n \t\t}\n \t\tif (loption == \"partition_by\") {\n \t\t\tauto converted = ConvertVectorToValue(std::move(option.second));\n-\t\t\tpartition_cols = ColumnListToIndices(ParseColumnList(converted, select_node.names, loption));\n+\t\t\tpartition_cols = ParseColumnsOrdered(converted, select_node.names, loption);\n \t\t\tcontinue;\n \t\t}\n \t\tstmt.info->options[option.first] = option.second;\n",
  "test_patch": "diff --git a/test/sql/copy/partitioned/hive_partitioned_write.test b/test/sql/copy/partitioned/hive_partitioned_write.test\nindex 3d42cb21a0c0..31257e7eddf9 100644\n--- a/test/sql/copy/partitioned/hive_partitioned_write.test\n+++ b/test/sql/copy/partitioned/hive_partitioned_write.test\n@@ -77,7 +77,7 @@ statement ok\n COPY test TO '__TEST_DIR__/partitioned4' (FORMAT PARQUET, PARTITION_BY part_col);\n \n query III\n-SELECT part_col, value_col, value2_col FROM parquet_scan('__TEST_DIR__/partitioned4/*/*.parquet', HIVE_PARTITIONING=1) WHERE part_col=0 ORDER BY value2_col;\n+SELECT part_col, value_col, value2_col FROM parquet_scan('__TEST_DIR__/partitioned4/part_col=*/*.parquet', HIVE_PARTITIONING=1) WHERE part_col=0 ORDER BY value2_col;\n ----\n 0\t1\t0\n 0\t3\t2\n@@ -151,4 +151,45 @@ SELECT part_col, value_col, value2_col FROM '__TEST_DIR__/partitioned8/part_col=\n 0\t3\t2\n 0\t0\t4\n 0\t2\t6\n-0\t4\t8\n\\ No newline at end of file\n+0\t4\t8\n+\n+# Order matters!\n+statement ok\n+COPY test TO '__TEST_DIR__/partitioned9' (FORMAT PARQUET, PARTITION_BY (part_col, value_col));\n+\n+query I\n+SELECT min(value2_col) as min_val\n+FROM parquet_scan('__TEST_DIR__/partitioned9/part_col=*/value_col=*/*.parquet', FILENAME=1)\n+GROUP BY filename\n+ORDER BY min_val\n+----\n+0\n+1\n+2\n+3\n+4\n+5\n+6\n+7\n+8\n+9\n+\n+statement ok\n+COPY test TO '__TEST_DIR__/partitioned10' (FORMAT PARQUET, PARTITION_BY (value_col, part_col));\n+\n+query I\n+SELECT min(value2_col) as min_val\n+FROM parquet_scan('__TEST_DIR__/partitioned10/value_col=*/part_col=*/*.parquet', FILENAME=1)\n+GROUP BY filename\n+ORDER BY min_val\n+----\n+0\n+1\n+2\n+3\n+4\n+5\n+6\n+7\n+8\n+9\n\\ No newline at end of file\ndiff --git a/test/sql/copy/partitioned/hive_partitioned_write.test_slow b/test/sql/copy/partitioned/hive_partitioned_write.test_slow\nindex db68bb6d639d..827be7314829 100644\n--- a/test/sql/copy/partitioned/hive_partitioned_write.test_slow\n+++ b/test/sql/copy/partitioned/hive_partitioned_write.test_slow\n@@ -21,13 +21,13 @@ call dbgen(sf=1);\n \n # Partition by 2 columns\n statement ok\n-COPY lineitem TO '__TEST_DIR__/lineitem_sf1_partitioned' (FORMAT PARQUET, PARTITION_BY (l_returnflag, l_linestatus));\n+COPY lineitem TO '__TEST_DIR__/lineitem_sf1_partitioned' (FORMAT PARQUET, PARTITION_BY (l_linestatus, l_returnflag));\n \n statement ok\n DROP TABLE lineitem;\n \n statement ok\n-CREATE VIEW lineitem as SELECT * FROM parquet_scan('__TEST_DIR__/lineitem_sf1_partitioned/*/*/*.parquet', HIVE_PARTITIONING=1);\n+CREATE VIEW lineitem as SELECT * FROM parquet_scan('__TEST_DIR__/lineitem_sf1_partitioned/l_linestatus=*/l_returnflag=*/*.parquet', HIVE_PARTITIONING=1);\n \n loop i 1 9\n \n",
  "problem_statement": "COPY TO PARTITION BY doesn't use the partition order when writing files\n### What happens?\r\n\r\nI have a query like\r\n\r\n```sql\r\nCOPY (SELECT * FROM parquet_scan('s3://${S3_BUCKET_NAME}/incoming/organization_id=*/domain_name=*/event_date=${yesterday}/*.parquet', HIVE_PARTITIONING = 1)) TO 's3://${S3_BUCKET_NAME}/aggregated' (FORMAT PARQUET, PARTITION_BY (organization_id, domain_name, event_date), ALLOW_OVERWRITE TRUE);\r\n```\r\n\r\nMy expectation (maybe incorrectly) was that the order of the partition columns is also used when producing the S3 keys. Turns out it doesn't, but apparently uses the lexical order of the columns (domain_name, event_date, organization_id) instead. I think would make sense from a query pattern / data hierarchy perspective that the given column order is respected.\r\n\r\n\r\n### To Reproduce\r\n\r\nUse a partitioned data source in S3, and try to repartition it as described above.\r\n\r\n### OS:\r\n\r\nAmazon Linux\r\n\r\n### DuckDB Version:\r\n\r\nlatest master\r\n\r\n### DuckDB Client:\r\n\r\nNode.js\r\n\r\n### Full Name:\r\n\r\nTobi M.\r\n\r\n### Affiliation:\r\n\r\nnone\r\n\r\n### Have you tried this on the latest `master` branch?\r\n\r\n- [X] I agree\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] I agree\n",
  "hints_text": "",
  "created_at": "2023-02-23T09:42:45Z"
}