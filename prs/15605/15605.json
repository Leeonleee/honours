{
  "repo": "duckdb/duckdb",
  "pull_number": 15605,
  "instance_id": "duckdb__duckdb-15605",
  "issue_numbers": [
    "15504"
  ],
  "base_commit": "acdbf60889033d2701a5fef360a19963cafea471",
  "patch": "diff --git a/extension/parquet/column_reader.cpp b/extension/parquet/column_reader.cpp\nindex cf1f5ba504c4..8a2112c356dd 100644\n--- a/extension/parquet/column_reader.cpp\n+++ b/extension/parquet/column_reader.cpp\n@@ -135,6 +135,14 @@ const SchemaElement &ColumnReader::Schema() const {\n \treturn schema;\n }\n \n+optional_ptr<const SchemaElement> ColumnReader::GetParentSchema() const {\n+\treturn parent_schema;\n+}\n+\n+void ColumnReader::SetParentSchema(const SchemaElement &parent_schema_p) {\n+\tparent_schema = &parent_schema_p;\n+}\n+\n idx_t ColumnReader::FileIdx() const {\n \treturn file_idx;\n }\ndiff --git a/extension/parquet/include/column_reader.hpp b/extension/parquet/include/column_reader.hpp\nindex dd89cadd5cd8..23d4fc3d4b6b 100644\n--- a/extension/parquet/include/column_reader.hpp\n+++ b/extension/parquet/include/column_reader.hpp\n@@ -57,6 +57,9 @@ class ColumnReader {\n \tParquetReader &Reader();\n \tconst LogicalType &Type() const;\n \tconst SchemaElement &Schema() const;\n+\toptional_ptr<const SchemaElement> GetParentSchema() const;\n+\tvoid SetParentSchema(const SchemaElement &parent_schema);\n+\n \tidx_t FileIdx() const;\n \tidx_t MaxDefine() const;\n \tidx_t MaxRepeat() const;\n@@ -140,6 +143,7 @@ class ColumnReader {\n \n protected:\n \tconst SchemaElement &schema;\n+\toptional_ptr<const SchemaElement> parent_schema;\n \n \tidx_t file_idx;\n \tidx_t max_define;\ndiff --git a/extension/parquet/parquet_extension.cpp b/extension/parquet/parquet_extension.cpp\nindex 8cbea80909d4..500b899198d7 100644\n--- a/extension/parquet/parquet_extension.cpp\n+++ b/extension/parquet/parquet_extension.cpp\n@@ -313,9 +313,15 @@ static void InitializeParquetReader(ParquetReader &reader, const ParquetReadBind\n \tunordered_map<uint32_t, idx_t> field_id_to_column_index;\n \tauto &column_readers = reader.root_reader->Cast<StructColumnReader>().child_readers;\n \tfor (idx_t column_index = 0; column_index < column_readers.size(); column_index++) {\n-\t\tauto &column_schema = column_readers[column_index]->Schema();\n+\t\tauto &column_reader = *column_readers[column_index];\n+\t\tauto &column_schema = column_reader.Schema();\n \t\tif (column_schema.__isset.field_id) {\n \t\t\tfield_id_to_column_index[column_schema.field_id] = column_index;\n+\t\t} else if (column_reader.GetParentSchema()) {\n+\t\t\tauto &parent_column_schema = *column_reader.GetParentSchema();\n+\t\t\tif (parent_column_schema.__isset.field_id) {\n+\t\t\t\tfield_id_to_column_index[parent_column_schema.field_id] = column_index;\n+\t\t\t}\n \t\t}\n \t}\n \ndiff --git a/extension/parquet/parquet_reader.cpp b/extension/parquet/parquet_reader.cpp\nindex e6ce7aa92d51..1d2565a2dea9 100644\n--- a/extension/parquet/parquet_reader.cpp\n+++ b/extension/parquet/parquet_reader.cpp\n@@ -418,9 +418,10 @@ unique_ptr<ColumnReader> ParquetReader::CreateReaderRecursive(ClientContext &con\n \t\t}\n \t\tif (is_repeated) {\n \t\t\tresult_type = LogicalType::LIST(result_type);\n-\t\t\treturn make_uniq<ListColumnReader>(*this, result_type, s_ele, this_idx, max_define, max_repeat,\n-\t\t\t                                   std::move(result));\n+\t\t\tresult = make_uniq<ListColumnReader>(*this, result_type, s_ele, this_idx, max_define, max_repeat,\n+\t\t\t                                     std::move(result));\n \t\t}\n+\t\tresult->SetParentSchema(s_ele);\n \t\treturn result;\n \t} else { // leaf node\n \t\tif (!s_ele.__isset.type) {\ndiff --git a/src/execution/aggregate_hashtable.cpp b/src/execution/aggregate_hashtable.cpp\nindex 8a2dd448ce78..198b77facfc0 100644\n--- a/src/execution/aggregate_hashtable.cpp\n+++ b/src/execution/aggregate_hashtable.cpp\n@@ -131,6 +131,9 @@ void GroupedAggregateHashTable::Abandon() {\n \t// Start over\n \tClearPointerTable();\n \tcount = 0;\n+\n+\t// Resetting the id ensures the dict state is reset properly when needed\n+\tstate.dict_state.dictionary_id = string();\n }\n \n void GroupedAggregateHashTable::Repartition() {\ndiff --git a/src/function/scalar/sequence/nextval.cpp b/src/function/scalar/sequence/nextval.cpp\nindex 7e19dae7d6bf..6738383dc50f 100644\n--- a/src/function/scalar/sequence/nextval.cpp\n+++ b/src/function/scalar/sequence/nextval.cpp\n@@ -91,6 +91,9 @@ static void NextValFunction(DataChunk &args, ExpressionState &state, Vector &res\n \n static unique_ptr<FunctionData> NextValBind(ScalarFunctionBindInput &bind_input, ScalarFunction &,\n                                             vector<unique_ptr<Expression>> &arguments) {\n+\tif (arguments[0]->HasParameter() || arguments[0]->return_type.id() == LogicalTypeId::UNKNOWN) {\n+\t\tthrow ParameterNotResolvedException();\n+\t}\n \tif (!arguments[0]->IsFoldable()) {\n \t\tthrow NotImplementedException(\n \t\t    \"currval/nextval requires a constant sequence - non-constant sequences are no longer supported\");\ndiff --git a/src/function/scalar/string/length.cpp b/src/function/scalar/string/length.cpp\nindex f2ec8bae1ff0..5386464199d3 100644\n--- a/src/function/scalar/string/length.cpp\n+++ b/src/function/scalar/string/length.cpp\n@@ -110,12 +110,14 @@ static void ArrayLengthFunction(DataChunk &args, ExpressionState &state, Vector\n \n static unique_ptr<FunctionData> ArrayOrListLengthBind(ClientContext &context, ScalarFunction &bound_function,\n                                                       vector<unique_ptr<Expression>> &arguments) {\n-\tif (arguments[0]->HasParameter()) {\n+\tif (arguments[0]->HasParameter() || arguments[0]->return_type.id() == LogicalTypeId::UNKNOWN) {\n \t\tthrow ParameterNotResolvedException();\n \t}\n-\tif (arguments[0]->return_type.id() == LogicalTypeId::ARRAY) {\n+\n+\tconst auto &arg_type = arguments[0]->return_type.id();\n+\tif (arg_type == LogicalTypeId::ARRAY) {\n \t\tbound_function.function = ArrayLengthFunction;\n-\t} else if (arguments[0]->return_type.id() == LogicalTypeId::LIST) {\n+\t} else if (arg_type == LogicalTypeId::LIST) {\n \t\tbound_function.function = ListLengthFunction;\n \t} else {\n \t\t// Unreachable\n@@ -183,7 +185,7 @@ static void ArrayLengthBinaryFunction(DataChunk &args, ExpressionState &state, V\n \n static unique_ptr<FunctionData> ArrayOrListLengthBinaryBind(ClientContext &context, ScalarFunction &bound_function,\n                                                             vector<unique_ptr<Expression>> &arguments) {\n-\tif (arguments[0]->HasParameter()) {\n+\tif (arguments[0]->HasParameter() || arguments[0]->return_type.id() == LogicalTypeId::UNKNOWN) {\n \t\tthrow ParameterNotResolvedException();\n \t}\n \tauto type = arguments[0]->return_type;\ndiff --git a/src/planner/binder/expression/bind_operator_expression.cpp b/src/planner/binder/expression/bind_operator_expression.cpp\nindex 81addd4069ef..9326c8849622 100644\n--- a/src/planner/binder/expression/bind_operator_expression.cpp\n+++ b/src/planner/binder/expression/bind_operator_expression.cpp\n@@ -134,6 +134,9 @@ BindResult ExpressionBinder::BindExpression(OperatorExpression &op, idx_t depth)\n \t\tD_ASSERT(op.children[0]->GetExpressionClass() == ExpressionClass::BOUND_EXPRESSION);\n \t\tD_ASSERT(op.children[1]->GetExpressionClass() == ExpressionClass::BOUND_EXPRESSION);\n \t\tauto &extract_exp = BoundExpression::GetExpression(*op.children[0]);\n+\t\tif (extract_exp->HasParameter() || extract_exp->return_type.id() == LogicalTypeId::UNKNOWN) {\n+\t\t\tthrow ParameterNotResolvedException();\n+\t\t}\n \t\tauto &name_exp = BoundExpression::GetExpression(*op.children[1]);\n \t\tconst auto &extract_expr_type = extract_exp->return_type;\n \t\tif (extract_expr_type.id() != LogicalTypeId::STRUCT && extract_expr_type.id() != LogicalTypeId::UNION &&\ndiff --git a/src/planner/binder/statement/bind_create.cpp b/src/planner/binder/statement/bind_create.cpp\nindex 495a687e67f6..8d53446fc625 100644\n--- a/src/planner/binder/statement/bind_create.cpp\n+++ b/src/planner/binder/statement/bind_create.cpp\n@@ -196,7 +196,7 @@ SchemaCatalogEntry &Binder::BindCreateFunctionInfo(CreateInfo &info) {\n \t\t\tif (param.IsQualified()) {\n \t\t\t\tthrow BinderException(\"Invalid parameter name '%s': must be unqualified\", param.ToString());\n \t\t\t}\n-\t\t\tdummy_types.emplace_back(LogicalType::SQLNULL);\n+\t\t\tdummy_types.emplace_back(LogicalType::UNKNOWN);\n \t\t\tdummy_names.push_back(param.GetColumnName());\n \t\t}\n \t\t// default parameters\ndiff --git a/src/storage/arena_allocator.cpp b/src/storage/arena_allocator.cpp\nindex a67eeb2869cd..000444f755f6 100644\n--- a/src/storage/arena_allocator.cpp\n+++ b/src/storage/arena_allocator.cpp\n@@ -107,12 +107,13 @@ data_ptr_t ArenaAllocator::Reallocate(data_ptr_t pointer, idx_t old_size, idx_t\n \t\treturn pointer;\n \t}\n \n-\tauto head_ptr = head->data.get() + head->current_position;\n+\tconst auto head_ptr = head->data.get() + head->current_position - old_size;\n+\tint64_t current_position = NumericCast<int64_t>(head->current_position);\n \tint64_t diff = NumericCast<int64_t>(size) - NumericCast<int64_t>(old_size);\n-\tif (pointer == head_ptr && (size < old_size || NumericCast<int64_t>(head->current_position) + diff <=\n-\t                                                   NumericCast<int64_t>(head->maximum_size))) {\n+\tif (pointer == head_ptr &&\n+\t    (size < old_size || current_position + diff <= NumericCast<int64_t>(head->maximum_size))) {\n \t\t// passed pointer is the head pointer, and the diff fits on the current chunk\n-\t\thead->current_position += NumericCast<idx_t>(diff);\n+\t\thead->current_position = NumericCast<idx_t>(current_position + diff);\n \t\treturn pointer;\n \t} else {\n \t\t// allocate new memory\n",
  "test_patch": "diff --git a/test/issues/general/test_15432.test b/test/issues/general/test_15432.test\nnew file mode 100644\nindex 000000000000..521bbcd84ce6\n--- /dev/null\n+++ b/test/issues/general/test_15432.test\n@@ -0,0 +1,34 @@\n+# name: test/issues/general/test_15432.test\n+# description: Issue 15432 - Regression bug (V1.1.4 vs V1.0 and V1.0): binder error references non-existent call to sum()\n+# group: [general]\n+\n+statement ok\n+pragma enable_verification;\n+\n+statement ok\n+create or replace function transpose(lst) as (\n+\tselect list_transform(\n+\t\trange(1, 1+length(lst[1])),\n+\t\tj -> list_transform(\n+\t\t\trange(1, length(lst)+1),\n+\t\t\ti -> lst[i][j]\n+\t\t)\n+\t)\n+);\n+\n+statement ok\n+create or replace function centroid(points) as (\n+\tlist_transform(\n+\t\ttranspose(points),\n+\t\tx -> list_sum(x) / length(points)\n+\t)\n+);\n+\n+query II\n+select\n+\tpoints,\n+\tcentroid(points)\n+from\n+\tunnest([[[1], [2], [3]]]) t(points);\n+----\n+[[1], [2], [3]]\t[2.0]\ndiff --git a/test/parquet/test_parquet_schema.test b/test/parquet/test_parquet_schema.test\nindex b7f41cb6ac19..3df1a2736fd7 100644\n--- a/test/parquet/test_parquet_schema.test\n+++ b/test/parquet/test_parquet_schema.test\n@@ -268,3 +268,12 @@ ORDER BY filename\n ----\n 5\t3\t2\t1\tmultifile1.parquet\n 1\t4\t5\tNULL\tmultifile2.parquet\n+\n+# issue 15504\n+statement ok\n+COPY (select 1 as id, list_value('a', 'b', 'c') as arr, { key: 1, v1: 'a', v2: 'b' } as s) TO '__TEST_DIR__/15504.parquet' (field_ids { 'id': 0, 'arr': 1, 's': 2 });\n+\n+query III\n+SELECT * FROM read_parquet('__TEST_DIR__/15504.parquet', schema=map { 0: { name: 'id', type: 'int32', default_value: NULL }, 1: { name: 'arr', type: 'varchar[]', default_value: NULL }, 2: { name: 's', type: 'STRUCT(key INT, v1 TEXT, v2 TEXT)', default_value: NULL } });\n+----\n+1\t[a, b, c]\t{'key': 1, 'v1': a, 'v2': b}\ndiff --git a/test/sql/catalog/function/test_complex_macro.test b/test/sql/catalog/function/test_complex_macro.test\nindex 7211801e9c26..1bbe101d000c 100644\n--- a/test/sql/catalog/function/test_complex_macro.test\n+++ b/test/sql/catalog/function/test_complex_macro.test\n@@ -62,10 +62,13 @@ statement error\n SELECT IFELSE(1,IFELSE(1,b,1),a) FROM integers\n ----\n \n-# conflicting bindings\n-statement error\n+statement ok\n CREATE MACRO f1(x) AS (SELECT MIN(a) + x FROM integers)\n+\n+query I\n+select f1(42) from integers;\n ----\n+43\n \n # macro in GROUP BY\n statement ok\n",
  "problem_statement": "ARRAY or LIST return null when read_parquet with schema parameter\n### What happens?\r\n\r\nI am using duck db to test the analysis of nested fields, When using read_parquet with schema parameters to read a Parquet file, the array / list type returns null.\r\nI'm not sure if this is a bug or if there's a problem with my usage.\r\n\r\n### To Reproduce\r\n\r\n```\r\nCOPY (select 1 as id, list_value('a', 'b', 'c') as arr, { key: 1, v1: 'a', v2: 'b' } as s) TO './test.parquet' (field_ids { 'id': 0, 'arr': 1, 's': 2 });\r\nSELECT * FROM read_parquet('./test.parquet', schema=map { 0: { name: 'id', type: 'int32', default_value: NULL }, 1: { name: 'arr', type: 'varchar[]', default_value: NULL }, 2: { name: 's', type: 'STRUCT(key INT, v1 TEXT, v2 TEXT)', default_value: NULL } });\r\n```\r\nThe Output:\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  id   \u2502    arr    \u2502                       s                       \u2502\r\n\u2502 int32 \u2502 varchar[] \u2502 struct(\"key\" integer, v1 varchar, v2 varchar) \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502     1 \u2502           \u2502 {'key': 1, 'v1': a, 'v2': b}                  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nif read_parquet without schema\uff0cthe result is normal\r\n```\r\nSELECT * FROM read_parquet('./test.parquet');\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  id   \u2502    arr    \u2502                       s                       \u2502\r\n\u2502 int32 \u2502 varchar[] \u2502 struct(\"key\" integer, v1 varchar, v2 varchar) \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502     1 \u2502 [a, b, c] \u2502 {'key': 1, 'v1': a, 'v2': b}                  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nfull case:\r\n<img width=\"1870\" alt=\"image\" src=\"https://github.com/user-attachments/assets/2ddf246b-ff08-42ae-8507-3ee643407e0a\" />\r\n\r\n\r\n### OS:\r\n\r\nosx aarch64\r\n\r\n### DuckDB Version:\r\n\r\n1.3.0, 1.0.0\r\n\r\n### DuckDB Client:\r\n\r\ncli\r\n\r\n### Hardware:\r\n\r\n_No response_\r\n\r\n### Full Name:\r\n\r\njun\r\n\r\n### Affiliation:\r\n\r\njdy\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have not tested with any build\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nYes\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [ ] Yes, I have\n",
  "hints_text": "The reason for this problem may be as follows\r\n\r\nThe parquet schema was:\r\n```\r\nSchema:\r\nmessage duckdb_schema {\r\n  optional int32 id (INTEGER(32,true)) = 0;\r\n  optional group arr (LIST) = 1 {\r\n    repeated group list {\r\n      optional binary element (STRING);\r\n    }\r\n  }\r\n  optional group s = 2 {\r\n    optional int32 key (INTEGER(32,true));\r\n    optional binary v1 (STRING);\r\n    optional binary v2 (STRING);\r\n  }\r\n}\r\n```\r\n\r\nIn the [ParquetReader::CreateReaderRecursive](https://github.com/duckdb/duckdb/blob/e1db888231f0cbe856d75ee8db6e677a2e19dd4a/extension/parquet/parquet_reader.cpp#L298) method, it will be rollup when there only one child reader\r\n\r\nhttps://github.com/duckdb/duckdb/blob/e1db888231f0cbe856d75ee8db6e677a2e19dd4a/extension/parquet/parquet_reader.cpp#L410-L418\r\n\r\nSo the child reader of root will be like this in the end\r\n```\r\nStruct:\r\n  - name: id, field_id: 0\r\n  - name: list, field_id: (not set)\r\n  - name: s, field_id: 2\r\n```\r\n\r\nIn the [InitializeParquetReader](https://github.com/duckdb/duckdb/blob/e1db888231f0cbe856d75ee8db6e677a2e19dd4a/extension/parquet/parquet_extension.cpp#L287) method, the column will be checked for existence based on the root reader, and if not, the default value will be returned.\r\n\r\nThe relevant code is as follows:\r\nhttps://github.com/duckdb/duckdb/blob/e1db888231f0cbe856d75ee8db6e677a2e19dd4a/extension/parquet/parquet_extension.cpp#L314-L320\r\n\r\nhttps://github.com/duckdb/duckdb/blob/e1db888231f0cbe856d75ee8db6e677a2e19dd4a/extension/parquet/parquet_extension.cpp#L349-L354\r\n\r\nMy C++ is poor, and I\u2019m not sure if the reason is correct.",
  "created_at": "2025-01-08T08:30:37Z"
}