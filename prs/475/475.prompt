You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Benchmark Runner without parameters
The Benchmark Runner crashes while running without parameters (e.g.  ./benchmark_runner), a segmentation fault happens in some sqlite tests, for example: sqlite_StringAggShort and sqlite_StringPrefix.

Follows an output of the Valgrind:

-------------------------
|| sqlite_StringPrefix ||
-------------------------
Cold run...DONE
1/5...==25043== Invalid read of size 1
==25043==    at 0x4A1030: StringPrefixBenchmark::VerifyResult(duckdb::QueryResult*) (in /home/tkepe/git/duckdb/build/release/benchmark/benchmark_runner)
==25043==    by 0x49B4A4: duckdb::SQLiteBenchmark::Verify(duckdb::BenchmarkState*) (in /home/tkepe/git/duckdb/build/release/benchmark/benchmark_runner)
==25043==    by 0x45C6CD: duckdb::BenchmarkRunner::RunBenchmark(duckdb::Benchmark*) (in /home/tkepe/git/duckdb/build/release/benchmark/benchmark_runner)
==25043==    by 0x45CBCE: duckdb::BenchmarkRunner::RunBenchmarks() (in /home/tkepe/git/duckdb/build/release/benchmark/benchmark_runner)
==25043==    by 0x461B6D: run_benchmarks(BenchmarkConfiguration const&) (in /home/tkepe/git/duckdb/build/release/benchmark/benchmark_runner)
==25043==    by 0x44C8DE: main (in /home/tkepe/git/duckdb/build/release/benchmark/benchmark_runner)
==25043==  Address 0x58 is not stack'd, malloc'd or (recently) free'd
==25043== 
==25043== 
==25043== Process terminating with default action of signal 11 (SIGSEGV)
==25043==  Access not within mapped region at address 0x58
==25043==    at 0x4A1030: StringPrefixBenchmark::VerifyResult(duckdb::QueryResult*) (in /home/tkepe/git/duckdb/build/release/benchmark/benchmark_runner)
==25043==    by 0x49B4A4: duckdb::SQLiteBenchmark::Verify(duckdb::BenchmarkState*) (in /home/tkepe/git/duckdb/build/release/benchmark/benchmark_runner)
==25043==    by 0x45C6CD: duckdb::BenchmarkRunner::RunBenchmark(duckdb::Benchmark*) (in /home/tkepe/git/duckdb/build/release/benchmark/benchmark_runner)
==25043==    by 0x45CBCE: duckdb::BenchmarkRunner::RunBenchmarks() (in /home/tkepe/git/duckdb/build/release/benchmark/benchmark_runner)
==25043==    by 0x461B6D: run_benchmarks(BenchmarkConfiguration const&) (in /home/tkepe/git/duckdb/build/release/benchmark/benchmark_runner)
==25043==    by 0x44C8DE: main (in /home/tkepe/git/duckdb/build/release/benchmark/benchmark_runner)
==25043==  If you believe this happened as a result of a stack
==25043==  overflow in your program's main thread (unlikely but
==25043==  possible), you can try to increase the size of the
==25043==  main thread stack using the --main-stacksize= flag.
==25043==  The main thread stack size used in this run was 8388608.
==25043== 
==25043== HEAP SUMMARY:
==25043==     in use at exit: 1,954,037 bytes in 528 blocks
==25043==   total heap usage: 3,043,022 allocs, 3,042,494 frees, 176,662,345 bytes allocated
==25043== 
==25043== LEAK SUMMARY:
==25043==    definitely lost: 0 bytes in 0 blocks
==25043==    indirectly lost: 0 bytes in 0 blocks
==25043==      possibly lost: 1,816,824 bytes in 416 blocks
==25043==    still reachable: 137,213 bytes in 112 blocks
==25043==                       of which reachable via heuristic:
==25043==                         stdstring          : 1,605 bytes in 41 blocks
==25043==                         length64           : 132,816 bytes in 47 blocks
==25043==         suppressed: 0 bytes in 0 blocks
==25043== Rerun with --leak-check=full to see details of leaked memory
==25043== 
==25043== For counts of detected and suppressed errors, rerun with: -v
==25043== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 0 from 0)

</issue>
<code>
[start of README.md]
1: <img align="left" src="logo/duckdb-logo.png" height="120">
2: 
3: # DuckDB, the SQLite for Analytics
4: [![Travis](https://api.travis-ci.org/cwida/duckdb.svg?branch=master)](https://travis-ci.org/cwida/duckdb)
5: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
6: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
7: 
8: <br>
9: 
10: 
11: # Requirements
12: DuckDB requires [CMake](https://cmake.org) to be installed and a `C++11` compliant compiler. GCC 4.9 and newer, Clang 3.9 and newer and VisualStudio 2017 are tested on each revision.
13: 
14: ## Compiling
15: Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You may run `make unit` and `make allunit` to verify that your version works properly after making changes.
16: 
17: # Usage
18: A command line utility based on `sqlite3` can be found in either `build/release/duckdb_cli` (release, the default) or `build/debug/duckdb_cli` (debug).
19: 
20: # Embedding
21: As DuckDB is an embedded database, there is no database server to launch or client to connect to a running server. However, the database server can be embedded directly into an application using the C or C++ bindings. The main build process creates the shared library `build/release/src/libduckdb.[so|dylib|dll]` that can be linked against. A static library is built as well.
22: 
23: For examples on how to embed DuckDB into your application, see the [examples](https://github.com/cwida/duckdb/tree/master/examples) folder.
24: 
25: ## Benchmarks
26: After compiling, benchmarks can be executed from the root directory by executing `./build/release/benchmark/benchmark_runner`.
27: 
28: ## Standing on the Shoulders of Giants
29: DuckDB is implemented in C++ 11, should compile with GCC and clang, uses CMake to build and [Catch2](https://github.com/catchorg/Catch2) for testing. DuckDB uses some components from various Open-Source databases and draws inspiration from scientific publications. Here is an overview:
30: 
31: * Parser: We use the PostgreSQL parser that was [repackaged as a stand-alone library](https://github.com/lfittl/libpg_query). The translation to our own parse tree is inspired by [Peloton](https://pelotondb.io).
32: * Shell: We have adapted the [SQLite shell](https://sqlite.org/cli.html) to work with DuckDB.
33: * Tests: We use the [SQL Logic Tests from SQLite](https://www.sqlite.org/sqllogictest/doc/trunk/about.wiki) to test DuckDB.
34: * Query fuzzing: We use [SQLsmith](https://github.com/anse1/sqlsmith) to generate random queries for additional testing.
35: * Date Math: We use the date math component from [MonetDB](https://www.monetdb.org).
36: * SQL Window Functions: DuckDB's window functions implementation uses Segment Tree Aggregation as described in the paper "Efficient Processing of Window Functions in Analytical SQL Queries" by Viktor Leis, Kan Kundhikanjana, Alfons Kemper and Thomas Neumann.
37: * Execution engine: The vectorized execution engine is inspired by the paper "MonetDB/X100: Hyper-Pipelining Query Execution" by Peter Boncz, Marcin Zukowski and Niels Nes.
38: * Optimizer: DuckDB's optimizer draws inspiration from the papers "Dynamic programming strikes back" by Guido Moerkotte and Thomas Neumman as well as "Unnesting Arbitrary Queries" by Thomas Neumann and Alfons Kemper.
39: * Concurrency control: Our MVCC implementation is inspired by the paper "Fast Serializable Multi-Version Concurrency Control for Main-Memory Database Systems" by Thomas Neumann, Tobias Mühlbauer and Alfons Kemper.
40: * Regular Expression: DuckDB uses Google's [RE2](https://github.com/google/re2) regular expression engine.
41: 
42: ## Other pages
43: * [Continuous Benchmarking (CB™)](https://www.duckdb.org/benchmarks/index.html), runs TPC-H, TPC-DS and some microbenchmarks on every commit
[end of README.md]
[start of benchmark/dbwrapper/sqlite_benchmark.cpp]
1: #include "sqlite_benchmark.hpp"
2: 
3: #include "sqlite_transfer.hpp"
4: 
5: using namespace duckdb;
6: using namespace std;
7: 
8: SQLiteBenchmark::SQLiteBenchmark(unique_ptr<DuckDBBenchmark> duckdb)
9:     : Benchmark(true, "sqlite_" + duckdb->name, "sqlite_" + duckdb->group), duckdb_benchmark(move(duckdb)) {
10: }
11: 
12: unique_ptr<BenchmarkState> SQLiteBenchmark::Initialize() {
13: 	auto sqlite_state = make_unique<SQLiteBenchmarkState>();
14: 	// first load the data into DuckDB
15: 	auto duckdb_benchmark_state = duckdb_benchmark->Initialize();
16: 	auto &duckdb_state = (DuckDBBenchmarkState &)*duckdb_benchmark_state;
17: 	if (sqlite3_open(":memory:", &sqlite_state->db) != SQLITE_OK) {
18: 		return nullptr;
19: 	}
20: 	// then transfer the data to SQLite
21: 	sqlite::TransferDatabase(duckdb_state.conn, sqlite_state->db);
22: 	// get the types of the query
23: 	auto duckdb_result = duckdb_state.conn.Query(duckdb_benchmark->GetQuery());
24: 	if (!duckdb_result->success) {
25: 		return nullptr;
26: 	}
27: 	sqlite_state->types = duckdb_result->sql_types;
28: 	return move(sqlite_state);
29: }
30: 
31: void SQLiteBenchmark::Run(BenchmarkState *state_) {
32: 	auto state = (SQLiteBenchmarkState *)state_;
33: 	auto query = duckdb_benchmark->GetQuery();
34: 	state->result = sqlite::QueryDatabase(state->types, state->db, query, state->interrupt);
35: }
36: 
37: void SQLiteBenchmark::Cleanup(BenchmarkState *state_) {
38: }
39: 
40: string SQLiteBenchmark::Verify(BenchmarkState *state_) {
41: 	auto state = (SQLiteBenchmarkState *)state_;
42: 	return duckdb_benchmark->VerifyResult(state->result.get());
43: }
44: 
45: string SQLiteBenchmark::GetLogOutput(BenchmarkState *state_) {
46: 	return "";
47: }
48: 
49: void SQLiteBenchmark::Interrupt(BenchmarkState *state_) {
50: 	auto state = (SQLiteBenchmarkState *)state_;
51: 	state->interrupt = 1;
52: }
53: 
54: string SQLiteBenchmark::BenchmarkInfo() {
55: 	return duckdb_benchmark->BenchmarkInfo();
56: }
[end of benchmark/dbwrapper/sqlite_benchmark.cpp]
[start of src/function/aggregate/distributive/first.cpp]
1: #include "duckdb/function/aggregate/distributive_functions.hpp"
2: #include "duckdb/common/exception.hpp"
3: #include "duckdb/common/types/null_value.hpp"
4: #include "duckdb/common/vector_operations/vector_operations.hpp"
5: 
6: using namespace std;
7: 
8: namespace duckdb {
9: 
10: template <class T> struct FirstState {
11: 	bool is_set;
12: 	T value;
13: };
14: 
15: struct FirstFunction {
16: 	template <class STATE> static void Initialize(STATE *state) {
17: 		state->is_set = false;
18: 	}
19: 
20: 	template <class INPUT_TYPE, class STATE, class OP>
21: 	static void Operation(STATE *state, INPUT_TYPE *input, nullmask_t &nullmask, idx_t idx) {
22: 		if (!state->is_set) {
23: 			state->is_set = true;
24: 			if (nullmask[idx]) {
25: 				state->value = NullValue<INPUT_TYPE>();
26: 			} else {
27: 				state->value = input[idx];
28: 			}
29: 		}
30: 	}
31: 
32: 	template <class INPUT_TYPE, class STATE, class OP>
33: 	static void ConstantOperation(STATE *state, INPUT_TYPE *input, nullmask_t &nullmask, idx_t count) {
34: 		Operation<INPUT_TYPE, STATE, OP>(state, input, nullmask, 0);
35: 	}
36: 
37: 	template <class STATE, class OP> static void Combine(STATE source, STATE *target) {
38: 		if (!target->is_set) {
39: 			*target = source;
40: 		}
41: 	}
42: 
43: 	template <class T, class STATE>
44: 	static void Finalize(Vector &result, STATE *state, T *target, nullmask_t &nullmask, idx_t idx) {
45: 		if (!state->is_set || IsNullValue<T>(state->value)) {
46: 			nullmask[idx] = true;
47: 		} else {
48: 			target[idx] = state->value;
49: 		}
50: 	}
51: 
52: 	static bool IgnoreNull() {
53: 		return false;
54: 	}
55: };
56: 
57: template <class T> static AggregateFunction GetFirstAggregateTemplated(SQLType type) {
58: 	return AggregateFunction::UnaryAggregate<FirstState<T>, T, T, FirstFunction>(type, type);
59: }
60: 
61: AggregateFunction FirstFun::GetFunction(SQLType type) {
62: 	switch (type.id) {
63: 	case SQLTypeId::BOOLEAN:
64: 		return GetFirstAggregateTemplated<bool>(type);
65: 	case SQLTypeId::TINYINT:
66: 		return GetFirstAggregateTemplated<int8_t>(type);
67: 	case SQLTypeId::SMALLINT:
68: 		return GetFirstAggregateTemplated<int16_t>(type);
69: 	case SQLTypeId::INTEGER:
70: 		return GetFirstAggregateTemplated<int32_t>(type);
71: 	case SQLTypeId::BIGINT:
72: 		return GetFirstAggregateTemplated<int64_t>(type);
73: 	case SQLTypeId::FLOAT:
74: 		return GetFirstAggregateTemplated<float>(type);
75: 	case SQLTypeId::DOUBLE:
76: 		return GetFirstAggregateTemplated<double>(type);
77: 	case SQLTypeId::DECIMAL:
78: 		return GetFirstAggregateTemplated<double>(type);
79: 	case SQLTypeId::DATE:
80: 		return GetFirstAggregateTemplated<date_t>(type);
81: 	case SQLTypeId::TIMESTAMP:
82: 		return GetFirstAggregateTemplated<timestamp_t>(type);
83: 	case SQLTypeId::VARCHAR:
84: 		return GetFirstAggregateTemplated<string_t>(type);
85: 	default:
86: 		throw NotImplementedException("Unimplemented type for FIRST aggregate");
87: 	}
88: }
89: 
90: void FirstFun::RegisterFunction(BuiltinFunctions &set) {
91: 	AggregateFunctionSet first("first");
92: 	for (auto type : SQLType::ALL_TYPES) {
93: 		first.AddFunction(FirstFun::GetFunction(type));
94: 	}
95: 	set.AddFunction(first);
96: }
97: 
98: } // namespace duckdb
[end of src/function/aggregate/distributive/first.cpp]
[start of src/main/client_context.cpp]
1: #include "duckdb/main/client_context.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/common/serializer/buffered_deserializer.hpp"
5: #include "duckdb/common/serializer/buffered_serializer.hpp"
6: #include "duckdb/execution/physical_plan_generator.hpp"
7: #include "duckdb/main/database.hpp"
8: #include "duckdb/main/materialized_query_result.hpp"
9: #include "duckdb/main/query_result.hpp"
10: #include "duckdb/main/stream_query_result.hpp"
11: #include "duckdb/optimizer/optimizer.hpp"
12: #include "duckdb/parser/parser.hpp"
13: #include "duckdb/parser/expression/constant_expression.hpp"
14: #include "duckdb/parser/statement/drop_statement.hpp"
15: #include "duckdb/parser/statement/execute_statement.hpp"
16: #include "duckdb/parser/statement/explain_statement.hpp"
17: #include "duckdb/parser/statement/prepare_statement.hpp"
18: #include "duckdb/planner/operator/logical_execute.hpp"
19: #include "duckdb/planner/planner.hpp"
20: #include "duckdb/transaction/transaction_manager.hpp"
21: #include "duckdb/transaction/transaction.hpp"
22: #include "duckdb/storage/data_table.hpp"
23: #include "duckdb/main/appender.hpp"
24: 
25: using namespace duckdb;
26: using namespace std;
27: 
28: ClientContext::ClientContext(DuckDB &database)
29:     : db(database), transaction(*database.transaction_manager), interrupted(false), catalog(*database.catalog),
30:       temporary_objects(make_unique<SchemaCatalogEntry>(db.catalog.get(), TEMP_SCHEMA)),
31:       prepared_statements(make_unique<CatalogSet>(*db.catalog)), open_result(nullptr) {
32: 	random_device rd;
33: 	random_engine.seed(rd());
34: }
35: 
36: void ClientContext::Cleanup() {
37: 	lock_guard<mutex> client_guard(context_lock);
38: 	if (is_invalidated || !prepared_statements) {
39: 		return;
40: 	}
41: 	if (transaction.HasActiveTransaction()) {
42: 		ActiveTransaction().active_query = MAXIMUM_QUERY_ID;
43: 		if (!transaction.IsAutoCommit()) {
44: 			transaction.Rollback();
45: 		}
46: 	}
47: 	assert(prepared_statements);
48: 	db.transaction_manager->AddCatalogSet(*this, move(prepared_statements));
49: 	// invalidate any prepared statements
50: 	for (auto &statement : prepared_statement_objects) {
51: 		statement->is_invalidated = true;
52: 	}
53: 	for (auto &appender : appenders) {
54: 		appender->Invalidate("Connection has been closed!", false);
55: 	}
56: 	CleanupInternal();
57: }
58: 
59: void ClientContext::RegisterAppender(Appender *appender) {
60: 	lock_guard<mutex> client_guard(context_lock);
61: 	if (is_invalidated) {
62: 		throw Exception("Database that this connection belongs to has been closed!");
63: 	}
64: 	appenders.insert(appender);
65: }
66: 
67: void ClientContext::RemoveAppender(Appender *appender) {
68: 	lock_guard<mutex> client_guard(context_lock);
69: 	if (is_invalidated) {
70: 		return;
71: 	}
72: 	appenders.erase(appender);
73: }
74: 
75: unique_ptr<DataChunk> ClientContext::Fetch() {
76: 	lock_guard<mutex> client_guard(context_lock);
77: 	if (!open_result) {
78: 		// no result to fetch from
79: 		return nullptr;
80: 	}
81: 	if (is_invalidated) {
82: 		// ClientContext is invalidated: database has been closed
83: 		open_result->error = "Database that this connection belongs to has been closed!";
84: 		open_result->success = false;
85: 		return nullptr;
86: 	}
87: 	try {
88: 		// fetch the chunk and return it
89: 		auto chunk = FetchInternal();
90: 		return chunk;
91: 	} catch (Exception &ex) {
92: 		open_result->error = ex.what();
93: 	} catch (...) {
94: 		open_result->error = "Unhandled exception in Fetch";
95: 	}
96: 	open_result->success = false;
97: 	CleanupInternal();
98: 	return nullptr;
99: }
100: 
101: string ClientContext::FinalizeQuery(bool success) {
102: 	profiler.EndQuery();
103: 
104: 	execution_context.Reset();
105: 
106: 	string error;
107: 	if (transaction.HasActiveTransaction()) {
108: 		ActiveTransaction().active_query = MAXIMUM_QUERY_ID;
109: 		try {
110: 			if (transaction.IsAutoCommit()) {
111: 				if (success) {
112: 					// query was successful: commit
113: 					transaction.Commit();
114: 				} else {
115: 					// query was unsuccessful: rollback
116: 					transaction.Rollback();
117: 				}
118: 			}
119: 		} catch (Exception &ex) {
120: 			error = ex.what();
121: 		} catch (...) {
122: 			error = "Unhandled exception!";
123: 		}
124: 	}
125: 	return error;
126: }
127: 
128: void ClientContext::CleanupInternal() {
129: 	if (!open_result) {
130: 		// no result currently open
131: 		return;
132: 	}
133: 
134: 	auto error = FinalizeQuery(open_result->success);
135: 	if (open_result->success) {
136: 		// if an error occurred while committing report it in the result
137: 		open_result->error = error;
138: 		open_result->success = error.empty();
139: 	}
140: 
141: 	open_result->is_open = false;
142: 	open_result = nullptr;
143: }
144: 
145: unique_ptr<DataChunk> ClientContext::FetchInternal() {
146: 	assert(execution_context.physical_plan);
147: 	auto chunk = make_unique<DataChunk>();
148: 	// run the plan to get the next chunks
149: 	execution_context.physical_plan->InitializeChunk(*chunk);
150: 	execution_context.physical_plan->GetChunk(*this, *chunk, execution_context.physical_state.get());
151: 	return chunk;
152: }
153: 
154: unique_ptr<PreparedStatementData> ClientContext::CreatePreparedStatement(const string &query,
155:                                                                          unique_ptr<SQLStatement> statement) {
156: 	StatementType statement_type = statement->type;
157: 	auto result = make_unique<PreparedStatementData>(statement_type);
158: 
159: 	profiler.StartPhase("planner");
160: 	Planner planner(*this);
161: 	planner.CreatePlan(move(statement));
162: 	assert(planner.plan);
163: 	profiler.EndPhase();
164: 
165: 	auto plan = move(planner.plan);
166: 	// extract the result column names from the plan
167: 	result->read_only = planner.read_only;
168: 	result->requires_valid_transaction = planner.requires_valid_transaction;
169: 	result->names = planner.names;
170: 	result->sql_types = planner.sql_types;
171: 	result->value_map = move(planner.value_map);
172: 
173: #ifdef DEBUG
174: 	if (enable_optimizer) {
175: #endif
176: 		profiler.StartPhase("optimizer");
177: 		Optimizer optimizer(planner.binder, *this);
178: 		plan = optimizer.Optimize(move(plan));
179: 		assert(plan);
180: 		profiler.EndPhase();
181: #ifdef DEBUG
182: 	}
183: #endif
184: 
185: 	profiler.StartPhase("physical_planner");
186: 	// now convert logical query plan into a physical query plan
187: 	PhysicalPlanGenerator physical_planner(*this);
188: 	auto physical_plan = physical_planner.CreatePlan(move(plan));
189: 	profiler.EndPhase();
190: 
191: 	result->dependencies = move(physical_planner.dependencies);
192: 	result->types = physical_plan->types;
193: 	result->plan = move(physical_plan);
194: 	return result;
195: }
196: 
197: unique_ptr<QueryResult> ClientContext::ExecutePreparedStatement(const string &query, PreparedStatementData &statement,
198:                                                                 vector<Value> bound_values, bool allow_stream_result) {
199: 	if (ActiveTransaction().is_invalidated && statement.requires_valid_transaction) {
200: 		throw Exception("Current transaction is aborted (please ROLLBACK)");
201: 	}
202: 	if (db.access_mode == AccessMode::READ_ONLY && !statement.read_only) {
203: 		throw Exception(StringUtil::Format("Cannot execute statement of type \"%s\" in read-only mode!",
204: 		                                   StatementTypeToString(statement.statement_type).c_str()));
205: 	}
206: 
207: 	// bind the bound values before execution
208: 	statement.Bind(move(bound_values));
209: 
210: 	bool create_stream_result = statement.statement_type == StatementType::SELECT && allow_stream_result;
211: 
212: 	// store the physical plan in the context for calls to Fetch()
213: 	execution_context.physical_plan = move(statement.plan);
214: 	execution_context.physical_state = execution_context.physical_plan->GetOperatorState();
215: 
216: 	auto types = execution_context.physical_plan->GetTypes();
217: 	assert(types.size() == statement.sql_types.size());
218: 
219: 	if (create_stream_result) {
220: 		// successfully compiled SELECT clause and it is the last statement
221: 		// return a StreamQueryResult so the client can call Fetch() on it and stream the result
222: 		return make_unique<StreamQueryResult>(statement.statement_type, *this, statement.sql_types, types,
223: 		                                      statement.names);
224: 	}
225: 	// create a materialized result by continuously fetching
226: 	auto result =
227: 	    make_unique<MaterializedQueryResult>(statement.statement_type, statement.sql_types, types, statement.names);
228: 	while (true) {
229: 		auto chunk = FetchInternal();
230: 		if (chunk->size() == 0) {
231: 			break;
232: 		}
233: 		result->collection.Append(*chunk);
234: 	}
235: 	return move(result);
236: }
237: 
238: void ClientContext::InitialCleanup() {
239: 	if (is_invalidated) {
240: 		throw Exception("Database that this connection belongs to has been closed!");
241: 	}
242: 	//! Cleanup any open results and reset the interrupted flag
243: 	CleanupInternal();
244: 	interrupted = false;
245: }
246: 
247: unique_ptr<PreparedStatement> ClientContext::Prepare(string query) {
248: 	lock_guard<mutex> client_guard(context_lock);
249: 	// prepare the query
250: 	try {
251: 		InitialCleanup();
252: 
253: 		// first parse the query
254: 		Parser parser;
255: 		parser.ParseQuery(query.c_str());
256: 		if (parser.statements.size() == 0) {
257: 			throw Exception("No statement to prepare!");
258: 		}
259: 		if (parser.statements.size() > 1) {
260: 			throw Exception("Cannot prepare multiple statements at once!");
261: 		}
262: 		// now write the prepared statement data into the catalog
263: 		string prepare_name = "____duckdb_internal_prepare_" + to_string(prepare_count);
264: 		prepare_count++;
265: 		// create a prepare statement out of the underlying statement
266: 		auto prepare = make_unique<PrepareStatement>();
267: 		prepare->name = prepare_name;
268: 		prepare->statement = move(parser.statements[0]);
269: 
270: 		// now perform the actual PREPARE query
271: 		auto result = RunStatement(query, move(prepare), false);
272: 		if (!result->success) {
273: 			throw Exception(result->error);
274: 		}
275: 		auto prepared_catalog = (PreparedStatementCatalogEntry *)prepared_statements->GetRootEntry(prepare_name);
276: 		auto prepared_object = make_unique<PreparedStatement>(this, prepare_name, query, *prepared_catalog->prepared,
277: 		                                                      parser.n_prepared_parameters);
278: 		prepared_statement_objects.insert(prepared_object.get());
279: 		return prepared_object;
280: 	} catch (Exception &ex) {
281: 		return make_unique<PreparedStatement>(ex.what());
282: 	}
283: }
284: 
285: unique_ptr<QueryResult> ClientContext::Execute(string name, vector<Value> &values, bool allow_stream_result,
286:                                                string query) {
287: 	lock_guard<mutex> client_guard(context_lock);
288: 	try {
289: 		InitialCleanup();
290: 	} catch (std::exception &ex) {
291: 		return make_unique<MaterializedQueryResult>(ex.what());
292: 	}
293: 
294: 	// create the execute statement
295: 	auto execute = make_unique<ExecuteStatement>();
296: 	execute->name = name;
297: 	for (auto &val : values) {
298: 		execute->values.push_back(make_unique<ConstantExpression>(SQLTypeFromInternalType(val.type), val));
299: 	}
300: 
301: 	return RunStatement(query, move(execute), allow_stream_result);
302: }
303: void ClientContext::RemovePreparedStatement(PreparedStatement *statement) {
304: 	lock_guard<mutex> client_guard(context_lock);
305: 	if (!statement->success || statement->is_invalidated || is_invalidated) {
306: 		return;
307: 	}
308: 	try {
309: 		InitialCleanup();
310: 	} catch (...) {
311: 		return;
312: 	}
313: 	// erase the object from the list of prepared statements
314: 	prepared_statement_objects.erase(statement);
315: 	// drop it from the catalog
316: 	auto deallocate_statement = make_unique<DropStatement>();
317: 	deallocate_statement->info->type = CatalogType::PREPARED_STATEMENT;
318: 	deallocate_statement->info->name = statement->name;
319: 	string query = "DEALLOCATE " + statement->name;
320: 	RunStatement(query, move(deallocate_statement), false);
321: }
322: 
323: unique_ptr<QueryResult> ClientContext::RunStatementInternal(const string &query, unique_ptr<SQLStatement> statement,
324:                                                             bool allow_stream_result) {
325: 	// prepare the query for execution
326: 	auto prepared = CreatePreparedStatement(query, move(statement));
327: 	// by default, no values are bound
328: 	vector<Value> bound_values;
329: 	// execute the prepared statement
330: 	return ExecutePreparedStatement(query, *prepared, move(bound_values), allow_stream_result);
331: }
332: 
333: unique_ptr<QueryResult> ClientContext::RunStatement(const string &query, unique_ptr<SQLStatement> statement,
334:                                                     bool allow_stream_result) {
335: 	unique_ptr<QueryResult> result;
336: 	// check if we are on AutoCommit. In this case we should start a transaction.
337: 	if (transaction.IsAutoCommit()) {
338: 		transaction.BeginTransaction();
339: 	}
340: 	ActiveTransaction().active_query = db.transaction_manager->GetQueryNumber();
341: 	if (statement->type == StatementType::SELECT && query_verification_enabled) {
342: 		// query verification is enabled:
343: 		// create a copy of the statement and verify the original statement
344: 		auto copied_statement = ((SelectStatement &)*statement).Copy();
345: 		string error = VerifyQuery(query, move(statement));
346: 		if (!error.empty()) {
347: 			// query failed: abort now
348: 			FinalizeQuery(false);
349: 			// error in verifying query
350: 			return make_unique<MaterializedQueryResult>(error);
351: 		}
352: 		statement = move(copied_statement);
353: 	}
354: 	// start the profiler
355: 	profiler.StartQuery(query, *statement);
356: 	try {
357: 		result = RunStatementInternal(query, move(statement), allow_stream_result);
358: 	} catch (StandardException &ex) {
359: 		// standard exceptions do not invalidate the current transaction
360: 		result = make_unique<MaterializedQueryResult>(ex.what());
361: 	} catch (std::exception &ex) {
362: 		// other types of exceptions do invalidate the current transaction
363: 		if (transaction.HasActiveTransaction()) {
364: 			ActiveTransaction().is_invalidated = true;
365: 		}
366: 		result = make_unique<MaterializedQueryResult>(ex.what());
367: 	}
368: 	if (!result->success) {
369: 		// initial failures should always be reported as MaterializedResult
370: 		assert(result->type != QueryResultType::STREAM_RESULT);
371: 		// query failed: abort now
372: 		FinalizeQuery(false);
373: 		return result;
374: 	}
375: 	// query succeeded, append to list of results
376: 	if (result->type == QueryResultType::STREAM_RESULT) {
377: 		// store as currently open result if it is a stream result
378: 		this->open_result = (StreamQueryResult *)result.get();
379: 	} else {
380: 		// finalize the query if it is not a stream result
381: 		string error = FinalizeQuery(true);
382: 		if (!error.empty()) {
383: 			// failure in committing transaction
384: 			return make_unique<MaterializedQueryResult>(error);
385: 		}
386: 	}
387: 	return result;
388: }
389: 
390: unique_ptr<QueryResult> ClientContext::RunStatements(const string &query, vector<unique_ptr<SQLStatement>> &statements,
391:                                                      bool allow_stream_result) {
392: 	// now we have a list of statements
393: 	// iterate over them and execute them one by one
394: 	unique_ptr<QueryResult> result;
395: 	QueryResult *last_result = nullptr;
396: 	for (idx_t i = 0; i < statements.size(); i++) {
397: 		auto &statement = statements[i];
398: 		bool is_last_statement = i + 1 == statements.size();
399: 		auto current_result = RunStatement(query, move(statement), allow_stream_result && is_last_statement);
400: 		// now append the result to the list of results
401: 		if (!last_result) {
402: 			// first result of the query
403: 			result = move(current_result);
404: 			last_result = result.get();
405: 		} else {
406: 			// later results; attach to the result chain
407: 			last_result->next = move(current_result);
408: 			last_result = last_result->next.get();
409: 		}
410: 	}
411: 	return result;
412: }
413: 
414: unique_ptr<QueryResult> ClientContext::Query(string query, bool allow_stream_result) {
415: 	lock_guard<mutex> client_guard(context_lock);
416: 
417: 	Parser parser;
418: 	try {
419: 		InitialCleanup();
420: 		// parse the query and transform it into a set of statements
421: 		parser.ParseQuery(query.c_str());
422: 	} catch (std::exception &ex) {
423: 		return make_unique<MaterializedQueryResult>(ex.what());
424: 	}
425: 
426: 	if (parser.statements.size() == 0) {
427: 		// no statements, return empty successful result
428: 		return make_unique<MaterializedQueryResult>(StatementType::INVALID);
429: 	}
430: 
431: 	return RunStatements(query, parser.statements, allow_stream_result);
432: }
433: 
434: void ClientContext::Interrupt() {
435: 	interrupted = true;
436: }
437: 
438: void ClientContext::EnableProfiling() {
439: 	lock_guard<mutex> client_guard(context_lock);
440: 	profiler.Enable();
441: }
442: 
443: void ClientContext::DisableProfiling() {
444: 	lock_guard<mutex> client_guard(context_lock);
445: 	profiler.Disable();
446: }
447: 
448: void ClientContext::Invalidate() {
449: 	// interrupt any running query before attempting to obtain the lock
450: 	// this way we don't have to wait for the entire query to finish
451: 	Interrupt();
452: 	// now obtain the context lock
453: 	lock_guard<mutex> client_guard(context_lock);
454: 	// invalidate this context and the TransactionManager
455: 	is_invalidated = true;
456: 	transaction.Invalidate();
457: 	// also close any open result
458: 	if (open_result) {
459: 		open_result->is_open = false;
460: 	}
461: 	// and close any open appenders
462: 	for (auto &appender : appenders) {
463: 		appender->Invalidate("Database that this appender belongs to has been closed!", false);
464: 	}
465: 	appenders.clear();
466: }
467: 
468: string ClientContext::VerifyQuery(string query, unique_ptr<SQLStatement> statement) {
469: 	assert(statement->type == StatementType::SELECT);
470: 	// aggressive query verification
471: 
472: 	// the purpose of this function is to test correctness of otherwise hard to test features:
473: 	// Copy() of statements and expressions
474: 	// Serialize()/Deserialize() of expressions
475: 	// Hash() of expressions
476: 	// Equality() of statements and expressions
477: 	// Correctness of plans both with and without optimizers
478: 
479: 	// copy the statement
480: 	auto select_stmt = (SelectStatement *)statement.get();
481: 	auto copied_stmt = select_stmt->Copy();
482: 	auto unoptimized_stmt = select_stmt->Copy();
483: 
484: 	BufferedSerializer serializer;
485: 	select_stmt->Serialize(serializer);
486: 	BufferedDeserializer source(serializer);
487: 	auto deserialized_stmt = SelectStatement::Deserialize(source);
488: 	// all the statements should be equal
489: 	assert(copied_stmt->Equals(statement.get()));
490: 	assert(deserialized_stmt->Equals(statement.get()));
491: 	assert(copied_stmt->Equals(deserialized_stmt.get()));
492: 
493: 	// now perform checking on the expressions
494: #ifdef DEBUG
495: 	auto &orig_expr_list = select_stmt->node->GetSelectList();
496: 	auto &de_expr_list = deserialized_stmt->node->GetSelectList();
497: 	auto &cp_expr_list = copied_stmt->node->GetSelectList();
498: 	assert(orig_expr_list.size() == de_expr_list.size() && cp_expr_list.size() == de_expr_list.size());
499: 	for (idx_t i = 0; i < orig_expr_list.size(); i++) {
500: 		// check that the expressions are equivalent
501: 		assert(orig_expr_list[i]->Equals(de_expr_list[i].get()));
502: 		assert(orig_expr_list[i]->Equals(cp_expr_list[i].get()));
503: 		assert(de_expr_list[i]->Equals(cp_expr_list[i].get()));
504: 		// check that the hashes are equivalent too
505: 		assert(orig_expr_list[i]->Hash() == de_expr_list[i]->Hash());
506: 		assert(orig_expr_list[i]->Hash() == cp_expr_list[i]->Hash());
507: 	}
508: 	// now perform additional checking within the expressions
509: 	for (idx_t outer_idx = 0; outer_idx < orig_expr_list.size(); outer_idx++) {
510: 		auto hash = orig_expr_list[outer_idx]->Hash();
511: 		for (idx_t inner_idx = 0; inner_idx < orig_expr_list.size(); inner_idx++) {
512: 			auto hash2 = orig_expr_list[inner_idx]->Hash();
513: 			if (hash != hash2) {
514: 				// if the hashes are not equivalent, the expressions should not be equivalent
515: 				assert(!orig_expr_list[outer_idx]->Equals(orig_expr_list[inner_idx].get()));
516: 			}
517: 		}
518: 	}
519: #endif
520: 
521: 	// disable profiling if it is enabled
522: 	bool profiling_is_enabled = profiler.IsEnabled();
523: 	if (profiling_is_enabled) {
524: 		profiler.Disable();
525: 	}
526: 
527: 	// see below
528: 	auto statement_copy_for_explain = select_stmt->Copy();
529: 
530: 	auto original_result = make_unique<MaterializedQueryResult>(StatementType::SELECT),
531: 	     copied_result = make_unique<MaterializedQueryResult>(StatementType::SELECT),
532: 	     deserialized_result = make_unique<MaterializedQueryResult>(StatementType::SELECT),
533: 	     unoptimized_result = make_unique<MaterializedQueryResult>(StatementType::SELECT);
534: 	// execute the original statement
535: 	try {
536: 		auto result = RunStatementInternal(query, move(statement), false);
537: 		original_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
538: 	} catch (Exception &ex) {
539: 		original_result->error = ex.what();
540: 		original_result->success = false;
541: 	}
542: 
543: 	// check explain, only if q does not already contain EXPLAIN
544: 	if (original_result->success) {
545: 		auto explain_q = "EXPLAIN " + query;
546: 		auto explain_stmt = make_unique<ExplainStatement>(move(statement_copy_for_explain));
547: 		try {
548: 			RunStatementInternal(explain_q, move(explain_stmt), false);
549: 		} catch (std::exception &ex) {
550: 			return "EXPLAIN failed but query did not (" + string(ex.what()) + ")";
551: 		}
552: 	}
553: 
554: 	// now execute the copied statement
555: 	try {
556: 		auto result = RunStatementInternal(query, move(copied_stmt), false);
557: 		copied_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
558: 	} catch (Exception &ex) {
559: 		copied_result->error = ex.what();
560: 	}
561: 	// now execute the deserialized statement
562: 	try {
563: 		auto result = RunStatementInternal(query, move(deserialized_stmt), false);
564: 		deserialized_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
565: 	} catch (Exception &ex) {
566: 		deserialized_result->error = ex.what();
567: 	}
568: 	// now execute the unoptimized statement
569: 	enable_optimizer = false;
570: 	try {
571: 		auto result = RunStatementInternal(query, move(unoptimized_stmt), false);
572: 		unoptimized_result = unique_ptr_cast<QueryResult, MaterializedQueryResult>(move(result));
573: 	} catch (Exception &ex) {
574: 		unoptimized_result->error = ex.what();
575: 	}
576: 
577: 	enable_optimizer = true;
578: 	if (profiling_is_enabled) {
579: 		profiler.Enable();
580: 	}
581: 
582: 	// now compare the results
583: 	// the results of all four runs should be identical
584: 	if (!original_result->collection.Equals(copied_result->collection)) {
585: 		string result = "Copied result differs from original result!\n";
586: 		result += "Original Result:\n" + original_result->ToString();
587: 		result += "Copied Result\n" + copied_result->ToString();
588: 		return result;
589: 	}
590: 	if (!original_result->collection.Equals(deserialized_result->collection)) {
591: 		string result = "Deserialized result differs from original result!\n";
592: 		result += "Original Result:\n" + original_result->ToString();
593: 		result += "Deserialized Result\n" + deserialized_result->ToString();
594: 		return result;
595: 	}
596: 	if (!original_result->collection.Equals(unoptimized_result->collection)) {
597: 		string result = "Unoptimized result differs from original result!\n";
598: 		result += "Original Result:\n" + original_result->ToString();
599: 		result += "Unoptimized Result\n" + unoptimized_result->ToString();
600: 		return result;
601: 	}
602: 
603: 	return "";
604: }
605: 
606: unique_ptr<TableDescription> ClientContext::TableInfo(string schema_name, string table_name) {
607: 	lock_guard<mutex> client_guard(context_lock);
608: 	if (is_invalidated || (transaction.HasActiveTransaction() && transaction.ActiveTransaction().is_invalidated)) {
609: 		return nullptr;
610: 	}
611: 	// check if we are on AutoCommit. In this case we should start a transaction
612: 	if (transaction.IsAutoCommit()) {
613: 		transaction.BeginTransaction();
614: 	}
615: 	unique_ptr<TableDescription> result;
616: 	try {
617: 		// obtain the table info
618: 		auto table = db.catalog->GetEntry<TableCatalogEntry>(*this, schema_name, table_name);
619: 		// write the table info to the result
620: 		result = make_unique<TableDescription>();
621: 		result->schema = schema_name;
622: 		result->table = table_name;
623: 		for (auto &column : table->columns) {
624: 			result->columns.push_back(ColumnDefinition(column.name, column.type));
625: 		}
626: 	} catch (...) {
627: 		// table not found!
628: 		result = nullptr;
629: 	}
630: 
631: 	if (transaction.IsAutoCommit()) {
632: 		transaction.Commit();
633: 	}
634: 	return result;
635: }
636: 
637: void ClientContext::Append(TableDescription &description, DataChunk &chunk) {
638: 	lock_guard<mutex> client_guard(context_lock);
639: 	if (is_invalidated) {
640: 		throw Exception("Failed to append: database has been closed!");
641: 	}
642: 	if (transaction.HasActiveTransaction() && transaction.ActiveTransaction().is_invalidated) {
643: 		throw Exception("Failed to append: transaction has been invalidated!");
644: 	}
645: 	// check if we are on AutoCommit. In this case we should start a transaction
646: 	if (transaction.IsAutoCommit()) {
647: 		transaction.BeginTransaction();
648: 	}
649: 	try {
650: 		auto table_entry = db.catalog->GetEntry<TableCatalogEntry>(*this, description.schema, description.table);
651: 		// verify that the table columns and types match up
652: 		if (description.columns.size() != table_entry->columns.size()) {
653: 			throw Exception("Failed to append: table entry has different number of columns!");
654: 		}
655: 		for (idx_t i = 0; i < description.columns.size(); i++) {
656: 			if (description.columns[i].type != table_entry->columns[i].type) {
657: 				throw Exception("Failed to append: table entry has different number of columns!");
658: 			}
659: 		}
660: 		table_entry->storage->Append(*table_entry, *this, chunk);
661: 	} catch (Exception &ex) {
662: 		if (transaction.IsAutoCommit()) {
663: 			transaction.Rollback();
664: 		} else {
665: 			transaction.Invalidate();
666: 		}
667: 		throw ex;
668: 	}
669: 	if (transaction.IsAutoCommit()) {
670: 		transaction.Commit();
671: 	}
672: }
[end of src/main/client_context.cpp]
[start of tools/dbtransfer/sqlite_transfer.cpp]
1: #include "sqlite_transfer.hpp"
2: 
3: #include "duckdb/common/types/date.hpp"
4: 
5: using namespace duckdb;
6: using namespace std;
7: 
8: namespace sqlite {
9: 
10: bool TransferDatabase(Connection &con, sqlite3 *sqlite) {
11: 	char *error;
12: 	// start the SQLite transaction
13: 	if (sqlite3_exec(sqlite, "BEGIN TRANSACTION", nullptr, nullptr, &error) != SQLITE_OK) {
14: 		return false;
15: 	}
16: 
17: 	// query the list of tables
18: 	auto table_list = con.Query("SELECT name, sql FROM sqlite_master();");
19: 
20: 	for (size_t i = 0; i < table_list->collection.count; i++) {
21: 		auto name = table_list->GetValue(0, i).ToString();
22: 		auto sql = table_list->GetValue(1, i).ToString();
23: 
24: 		// for each table, first create the table in sqlite
25: 		if (sqlite3_exec(sqlite, sql.c_str(), nullptr, nullptr, &error) != SQLITE_OK) {
26: 			return false;
27: 		}
28: 
29: 		// now transfer the actual data
30: 		// first get the data from DuckDB
31: 		auto result = con.Query("SELECT * FROM " + name);
32: 		// create the prepared statement based on the result
33: 		stringstream prepared;
34: 		prepared << "INSERT INTO " << name << " (";
35: 		for (size_t j = 0; j < result->types.size(); j++) {
36: 			prepared << result->names[j];
37: 			if (j + 1 != result->types.size()) {
38: 				prepared << ",";
39: 			}
40: 		}
41: 		prepared << ") VALUES (";
42: 		for (size_t j = 0; j < result->types.size(); j++) {
43: 			prepared << "?";
44: 			if (j + 1 != result->types.size()) {
45: 				prepared << ",";
46: 			}
47: 		}
48: 		prepared << ");";
49: 
50: 		auto insert_statement = prepared.str();
51: 		sqlite3_stmt *stmt;
52: 		if (sqlite3_prepare_v2(sqlite, insert_statement.c_str(), -1, &stmt, nullptr) != SQLITE_OK) {
53: 			return false;
54: 		}
55: 
56: 		auto &types = result->sql_types;
57: 		for (size_t k = 0; k < result->collection.count; k++) {
58: 			int rc = SQLITE_ERROR;
59: 			for (size_t j = 0; j < types.size(); j++) {
60: 				size_t bind_index = j + 1;
61: 				auto value = result->GetValue(j, k);
62: 				if (value.is_null) {
63: 					rc = sqlite3_bind_null(stmt, bind_index);
64: 				} else {
65: 					// bind based on the type
66: 					switch (types[j].id) {
67: 					case SQLTypeId::BOOLEAN:
68: 					case SQLTypeId::TINYINT:
69: 					case SQLTypeId::SMALLINT:
70: 					case SQLTypeId::INTEGER:
71: 						rc = sqlite3_bind_int(stmt, bind_index, (int)value.GetValue<int64_t>());
72: 						break;
73: 					case SQLTypeId::BIGINT:
74: 						rc = sqlite3_bind_int64(stmt, bind_index, (sqlite3_int64)value.GetValue<int64_t>());
75: 						break;
76: 					case SQLTypeId::DATE: {
77: 						auto date_str = value.ToString() + " 00:00:00";
78: 						rc = sqlite3_bind_text(stmt, bind_index, date_str.c_str(), -1, SQLITE_TRANSIENT);
79: 						break;
80: 					}
81: 					case SQLTypeId::TIMESTAMP:
82: 						// TODO
83: 						throw NotImplementedException("Transferring timestamps is not supported yet");
84: 					case SQLTypeId::DECIMAL:
85: 						rc = sqlite3_bind_double(stmt, bind_index, value.value_.double_);
86: 						break;
87: 					case SQLTypeId::VARCHAR:
88: 						rc = sqlite3_bind_text(stmt, bind_index, value.ToString().c_str(), -1, SQLITE_TRANSIENT);
89: 						break;
90: 					default:
91: 						break;
92: 					}
93: 				}
94: 				if (rc != SQLITE_OK) {
95: 					return false;
96: 				}
97: 			}
98: 			rc = sqlite3_step(stmt);
99: 			if (rc != SQLITE_DONE) {
100: 				return false;
101: 			}
102: 			if (sqlite3_reset(stmt) != SQLITE_OK) {
103: 				return false;
104: 			}
105: 		}
106: 		sqlite3_finalize(stmt);
107: 	}
108: 	// commit the SQLite transaction
109: 	if (sqlite3_exec(sqlite, "COMMIT", nullptr, nullptr, &error) != SQLITE_OK) {
110: 		return false;
111: 	}
112: 	return true;
113: }
114: 
115: unique_ptr<QueryResult> QueryDatabase(vector<SQLType> result_types, sqlite3 *sqlite, std::string query,
116:                                       volatile int &interrupt) {
117: 	// prepare the SQL statement
118: 	sqlite3_stmt *stmt;
119: 	if (sqlite3_prepare_v2(sqlite, query.c_str(), -1, &stmt, nullptr) != SQLITE_OK) {
120: 		return nullptr;
121: 	}
122: 	int col_count = sqlite3_column_count(stmt);
123: 	vector<string> names;
124: 	for (int i = 0; i < col_count; i++) {
125: 		names.push_back(sqlite3_column_name(stmt, i));
126: 	}
127: 	// figure out the types of the columns
128: 	// construct the types of the result
129: 	vector<TypeId> typeids;
130: 	for (auto &tp : result_types) {
131: 		typeids.push_back(GetInternalType(tp));
132: 	}
133: 
134: 	// construct the result
135: 	auto result = make_unique<MaterializedQueryResult>(StatementType::SELECT, result_types, typeids, std::move(names));
136: 	DataChunk result_chunk;
137: 	result_chunk.Initialize(typeids);
138: 	int rc = SQLITE_ERROR;
139: 	while ((rc = sqlite3_step(stmt)) == SQLITE_ROW && interrupt == 0) {
140: 		// get the value for each of the columns
141: 		idx_t result_idx = result_chunk.size();
142: 		for (int i = 0; i < col_count; i++) {
143: 			if (sqlite3_column_type(stmt, i) == SQLITE_NULL) {
144: 				// NULL value
145: 				FlatVector::Nullmask(result_chunk.data[i])[result_idx] = true;
146: 			} else {
147: 				auto dataptr = FlatVector::GetData(result_chunk.data[i]);
148: 				// normal value, convert type
149: 				switch (result_types[i].id) {
150: 				case SQLTypeId::BOOLEAN:
151: 					((int8_t *)dataptr)[result_idx] = sqlite3_column_int(stmt, i) == 0 ? 0 : 1;
152: 					break;
153: 				case SQLTypeId::TINYINT:
154: 					((int8_t *)dataptr)[result_idx] = (int8_t)sqlite3_column_int(stmt, i);
155: 					break;
156: 				case SQLTypeId::SMALLINT:
157: 					((int16_t *)dataptr)[result_idx] = (int16_t)sqlite3_column_int(stmt, i);
158: 					break;
159: 				case SQLTypeId::INTEGER:
160: 					((int32_t *)dataptr)[result_idx] = (int32_t)sqlite3_column_int(stmt, i);
161: 					break;
162: 				case SQLTypeId::BIGINT:
163: 					((int64_t *)dataptr)[result_idx] = (int64_t)sqlite3_column_int64(stmt, i);
164: 					break;
165: 				case SQLTypeId::DECIMAL:
166: 					((double *)dataptr)[result_idx] = (double)sqlite3_column_double(stmt, i);
167: 					break;
168: 				case SQLTypeId::VARCHAR: {
169: 					Value result((char *)sqlite3_column_text(stmt, i));
170: 					result_chunk.SetValue(i, result_idx, result);
171: 					break;
172: 				}
173: 				case SQLTypeId::DATE: {
174: 					auto unix_time = sqlite3_column_int64(stmt, i);
175: 					((date_t *)dataptr)[result_idx] = Date::EpochToDate(unix_time);
176: 					break;
177: 				}
178: 				default:
179: 					throw NotImplementedException("Unimplemented type for SQLite -> DuckDB type conversion");
180: 				}
181: 			}
182: 		}
183: 		result_chunk.SetCardinality(result_idx + 1);
184: 		if (result_chunk.size() == STANDARD_VECTOR_SIZE) {
185: 			// chunk is filled
186: 			// flush the chunk to the result
187: 			result->collection.Append(result_chunk);
188: 			result_chunk.Reset();
189: 		}
190: 	}
191: 	if (rc != SQLITE_DONE) {
192: 		// failed
193: 		return nullptr;
194: 	}
195: 	if (result_chunk.size() > 0) {
196: 		// final append of any leftover data
197: 		result->collection.Append(result_chunk);
198: 		result_chunk.Reset();
199: 	}
200: 	sqlite3_finalize(stmt);
201: 	return move(result);
202: }
203: 
204: }; // namespace sqlite
[end of tools/dbtransfer/sqlite_transfer.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: