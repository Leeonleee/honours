You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Range filters on empty/scarcely populated value ranges
I encountered an issue with filter conditions on a ~30M row dataset. The problem arises when using a `WHERE x BETWEEN value1 AND value2` or `WHERE x > value1 AND x < value2` condition on value ranges which are not present (or only very scarcely) in the dataset. While scanning for values within this range, a Stack overflow error occurs - on my system at about 1GB of memory consumption by the process. Some preliminary debugging lead me to the Adaptive Filters, but I get a bit lost beyond that. 

The following code should help to reproduce the issue. 

First generate some data:
```
import pandas as pd
import numpy as np
np.random.seed(1)
df = pd.DataFrame(np.random.uniform(-10,10,size=(30000000, 4)), columns=list('xyzn'))
df.to_csv("test.csv",index=False)

import duckdb
con = duckdb.connect(database='testdb', read_only=False)
con.execute("CREATE TABLE test AS SELECT * FROM read_csv_auto('test.csv');")
con.close()
```

Then, either execute in the python client:
```
con = duckdb.connect(database='testdb', read_only=False)
con.execute("SELECT * FROM test WHERE x > -11.0 and x < -10.0 LIMIT 10;")

```
Or in the duckdb cli:
```
.open testdb
 SELECT * FROM test WHERE x > -11.0 and x < -10.0 LIMIT 10;

```

</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="30">
2: 
3: [![Travis](https://api.travis-ci.org/cwida/duckdb.svg?branch=master)](https://travis-ci.org/cwida/duckdb)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![Coverage Status](https://coveralls.io/repos/github/cwida/duckdb/badge.svg?branch=master)](https://coveralls.io/github/cwida/duckdb?branch=master)
6: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3901452.svg)](https://zenodo.org/record/3901452)
7: 
8: 
9: ## Installation
10: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
11: 
12: ## Development
13: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
14: 
15: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
16: 
17: 
[end of README.md]
[start of scripts/runsqlsmith.py]
1: 
2: # run SQL smith and collect breaking queries
3: import os
4: import re
5: import subprocess
6: 
7: def run_sqlsmith():
8: 	subprocess.call(['build/debug/third_party/sqlsmith/sqlsmith', '--duckdb=:memory:'])
9: 
10: def get_file(i):
11: 	return 'sqlsmith-queries/sqlsmith-%s.sql' % str(i)
12: 
13: i = 1
14: os.system('mkdir -p sqlsmith-queries')
15: while os.path.isfile(get_file(i)):
16: 	i += 1
17: while True:
18: 	# run SQL smith
19: 	run_sqlsmith()
20: 	# get the breaking query
21: 	with open('sqlsmith.log', 'r') as f:
22: 		text = re.sub('[ \t\n]+', ' ', f.read())
23: 
24: 	with open(get_file(i), 'w+') as f:
25: 		f.write(text)
26: 		f.write('\n')
27: 	i += 1
28: 
[end of scripts/runsqlsmith.py]
[start of src/common/types/vector.cpp]
1: #include <cstring> // strlen() on Solaris
2: 
3: #include "duckdb/common/types/vector.hpp"
4: 
5: #include "duckdb/common/assert.hpp"
6: #include "duckdb/common/exception.hpp"
7: #include "duckdb/common/printer.hpp"
8: #include "duckdb/common/vector_operations/vector_operations.hpp"
9: #include "duckdb/common/types/chunk_collection.hpp"
10: #include "duckdb/common/serializer.hpp"
11: #include "duckdb/common/types/null_value.hpp"
12: 
13: using namespace std;
14: 
15: namespace duckdb {
16: 
17: Vector::Vector(TypeId type, bool create_data, bool zero_data)
18:     : vector_type(VectorType::FLAT_VECTOR), type(type), data(nullptr) {
19: 	if (create_data) {
20: 		Initialize(type, zero_data);
21: 	}
22: }
23: 
24: Vector::Vector(TypeId type) : Vector(type, true, false) {
25: }
26: 
27: Vector::Vector(TypeId type, data_ptr_t dataptr) : vector_type(VectorType::FLAT_VECTOR), type(type), data(dataptr) {
28: 	if (dataptr && type == TypeId::INVALID) {
29: 		throw InvalidTypeException(type, "Cannot create a vector of type INVALID!");
30: 	}
31: }
32: 
33: Vector::Vector(Value value) : vector_type(VectorType::CONSTANT_VECTOR) {
34: 	Reference(value);
35: }
36: 
37: Vector::Vector() : vector_type(VectorType::FLAT_VECTOR), type(TypeId::INVALID), data(nullptr) {
38: }
39: 
40: Vector::Vector(Vector &&other) noexcept
41:     : vector_type(other.vector_type), type(other.type), data(other.data), nullmask(other.nullmask),
42:       buffer(move(other.buffer)), auxiliary(move(other.auxiliary)) {
43: }
44: 
45: void Vector::Reference(Value &value) {
46: 	vector_type = VectorType::CONSTANT_VECTOR;
47: 	type = value.type;
48: 	buffer = VectorBuffer::CreateConstantVector(type);
49: 	auxiliary.reset();
50: 	data = buffer->GetData();
51: 	SetValue(0, value);
52: }
53: 
54: void Vector::Reference(Vector &other) {
55: 	vector_type = other.vector_type;
56: 	buffer = other.buffer;
57: 	auxiliary = other.auxiliary;
58: 	data = other.data;
59: 	type = other.type;
60: 	nullmask = other.nullmask;
61: }
62: 
63: void Vector::Slice(Vector &other, idx_t offset) {
64: 	if (other.vector_type == VectorType::CONSTANT_VECTOR) {
65: 		Reference(other);
66: 		return;
67: 	}
68: 	assert(other.vector_type == VectorType::FLAT_VECTOR);
69: 
70: 	// create a reference to the other vector
71: 	Reference(other);
72: 	if (offset > 0) {
73: 		data = data + GetTypeIdSize(type) * offset;
74: 		nullmask <<= offset;
75: 	}
76: }
77: 
78: void Vector::Slice(Vector &other, const SelectionVector &sel, idx_t count) {
79: 	Reference(other);
80: 	Slice(sel, count);
81: }
82: 
83: void Vector::Slice(const SelectionVector &sel, idx_t count) {
84: 	if (vector_type == VectorType::CONSTANT_VECTOR) {
85: 		// dictionary on a constant is just a constant
86: 		return;
87: 	}
88: 	if (vector_type == VectorType::DICTIONARY_VECTOR) {
89: 		// already a dictionary, slice the current dictionary
90: 		auto &current_sel = DictionaryVector::SelVector(*this);
91: 		auto sliced_dictionary = current_sel.Slice(sel, count);
92: 		buffer = make_unique<DictionaryBuffer>(move(sliced_dictionary));
93: 		return;
94: 	}
95: 	auto child_ref = make_buffer<VectorChildBuffer>();
96: 	child_ref->data.Reference(*this);
97: 
98: 	auto dict_buffer = make_unique<DictionaryBuffer>(sel);
99: 	buffer = move(dict_buffer);
100: 	auxiliary = move(child_ref);
101: 	vector_type = VectorType::DICTIONARY_VECTOR;
102: }
103: 
104: void Vector::Slice(const SelectionVector &sel, idx_t count, sel_cache_t &cache) {
105: 	if (vector_type == VectorType::DICTIONARY_VECTOR) {
106: 		// dictionary vector: need to merge dictionaries
107: 		// check if we have a cached entry
108: 		auto &current_sel = DictionaryVector::SelVector(*this);
109: 		auto target_data = current_sel.data();
110: 		auto entry = cache.find(target_data);
111: 		if (entry != cache.end()) {
112: 			// cached entry exists: use that
113: 			this->buffer = entry->second;
114: 		} else {
115: 			Slice(sel, count);
116: 			cache[target_data] = this->buffer;
117: 		}
118: 	} else {
119: 		Slice(sel, count);
120: 	}
121: }
122: 
123: void Vector::Initialize(TypeId new_type, bool zero_data) {
124: 	if (new_type != TypeId::INVALID) {
125: 		type = new_type;
126: 	}
127: 	vector_type = VectorType::FLAT_VECTOR;
128: 	buffer.reset();
129: 	auxiliary.reset();
130: 	nullmask.reset();
131: 	if (GetTypeIdSize(type) > 0) {
132: 		buffer = VectorBuffer::CreateStandardVector(type);
133: 		data = buffer->GetData();
134: 		if (zero_data) {
135: 			memset(data, 0, STANDARD_VECTOR_SIZE * GetTypeIdSize(type));
136: 		}
137: 	}
138: }
139: 
140: void Vector::SetValue(idx_t index, Value val) {
141: 	if (vector_type == VectorType::DICTIONARY_VECTOR) {
142: 		// dictionary: apply dictionary and forward to child
143: 		auto &sel_vector = DictionaryVector::SelVector(*this);
144: 		auto &child = DictionaryVector::Child(*this);
145: 		return child.SetValue(sel_vector.get_index(index), move(val));
146: 	}
147: 	Value newVal = val.CastAs(type);
148: 
149: 	nullmask[index] = newVal.is_null;
150: 	if (newVal.is_null) {
151: 		return;
152: 	}
153: 	switch (type) {
154: 	case TypeId::BOOL:
155: 		((bool *)data)[index] = newVal.value_.boolean;
156: 		break;
157: 	case TypeId::INT8:
158: 		((int8_t *)data)[index] = newVal.value_.tinyint;
159: 		break;
160: 	case TypeId::INT16:
161: 		((int16_t *)data)[index] = newVal.value_.smallint;
162: 		break;
163: 	case TypeId::INT32:
164: 		((int32_t *)data)[index] = newVal.value_.integer;
165: 		break;
166: 	case TypeId::INT64:
167: 		((int64_t *)data)[index] = newVal.value_.bigint;
168: 		break;
169: 	case TypeId::FLOAT:
170: 		((float *)data)[index] = newVal.value_.float_;
171: 		break;
172: 	case TypeId::DOUBLE:
173: 		((double *)data)[index] = newVal.value_.double_;
174: 		break;
175: 	case TypeId::POINTER:
176: 		((uintptr_t *)data)[index] = newVal.value_.pointer;
177: 		break;
178: 	case TypeId::VARCHAR: {
179: 		((string_t *)data)[index] = StringVector::AddBlob(*this, newVal.str_value);
180: 		break;
181: 	}
182: 	case TypeId::STRUCT: {
183: 		if (!auxiliary || StructVector::GetEntries(*this).size() == 0) {
184: 			for (size_t i = 0; i < val.struct_value.size(); i++) {
185: 				auto &struct_child = val.struct_value[i];
186: 				auto cv = make_unique<Vector>(struct_child.second.type);
187: 				cv->vector_type = vector_type;
188: 				StructVector::AddEntry(*this, struct_child.first, move(cv));
189: 			}
190: 		}
191: 
192: 		auto &children = StructVector::GetEntries(*this);
193: 		assert(children.size() == val.struct_value.size());
194: 
195: 		for (size_t i = 0; i < val.struct_value.size(); i++) {
196: 			auto &struct_child = val.struct_value[i];
197: 			assert(vector_type == VectorType::CONSTANT_VECTOR || vector_type == VectorType::FLAT_VECTOR);
198: 			auto &vec_child = children[i];
199: 			assert(vec_child.first == struct_child.first);
200: 			vec_child.second->SetValue(index, struct_child.second);
201: 		}
202: 	} break;
203: 
204: 	case TypeId::LIST: {
205: 		if (!auxiliary) {
206: 			auto cc = make_unique<ChunkCollection>();
207: 			ListVector::SetEntry(*this, move(cc));
208: 		}
209: 		auto &child_cc = ListVector::GetEntry(*this);
210: 		// TODO optimization: in-place update if fits
211: 		auto offset = child_cc.count;
212: 		if (val.list_value.size() > 0) {
213: 			idx_t append_idx = 0;
214: 			while (append_idx < val.list_value.size()) {
215: 				idx_t this_append_len = min((idx_t)STANDARD_VECTOR_SIZE, val.list_value.size() - append_idx);
216: 
217: 				DataChunk child_append_chunk;
218: 				child_append_chunk.SetCardinality(this_append_len);
219: 				vector<TypeId> types;
220: 				types.push_back(val.list_value[0].type);
221: 				child_append_chunk.Initialize(types);
222: 				for (idx_t i = 0; i < this_append_len; i++) {
223: 					child_append_chunk.data[0].SetValue(i, val.list_value[i + append_idx]);
224: 				}
225: 				child_cc.Append(child_append_chunk);
226: 				append_idx += this_append_len;
227: 			}
228: 		}
229: 		// now set the pointer
230: 		auto &entry = ((list_entry_t *)data)[index];
231: 		entry.length = val.list_value.size();
232: 		entry.offset = offset;
233: 	} break;
234: 	default:
235: 		throw NotImplementedException("Unimplemented type for Vector::SetValue");
236: 	}
237: }
238: 
239: Value Vector::GetValue(idx_t index) const {
240: 	if (vector_type == VectorType::CONSTANT_VECTOR) {
241: 		index = 0;
242: 	} else if (vector_type == VectorType::DICTIONARY_VECTOR) {
243: 		// dictionary: apply dictionary and forward to child
244: 		auto &sel_vector = DictionaryVector::SelVector(*this);
245: 		auto &child = DictionaryVector::Child(*this);
246: 		return child.GetValue(sel_vector.get_index(index));
247: 	} else {
248: 		assert(vector_type == VectorType::FLAT_VECTOR);
249: 	}
250: 
251: 	if (nullmask[index]) {
252: 		return Value(type);
253: 	}
254: 	switch (type) {
255: 	case TypeId::BOOL:
256: 		return Value::BOOLEAN(((bool *)data)[index]);
257: 	case TypeId::INT8:
258: 		return Value::TINYINT(((int8_t *)data)[index]);
259: 	case TypeId::INT16:
260: 		return Value::SMALLINT(((int16_t *)data)[index]);
261: 	case TypeId::INT32:
262: 		return Value::INTEGER(((int32_t *)data)[index]);
263: 	case TypeId::INT64:
264: 		return Value::BIGINT(((int64_t *)data)[index]);
265: 	case TypeId::HASH:
266: 		return Value::HASH(((hash_t *)data)[index]);
267: 	case TypeId::POINTER:
268: 		return Value::POINTER(((uintptr_t *)data)[index]);
269: 	case TypeId::FLOAT:
270: 		return Value::FLOAT(((float *)data)[index]);
271: 	case TypeId::DOUBLE:
272: 		return Value::DOUBLE(((double *)data)[index]);
273: 	case TypeId::VARCHAR: {
274: 		auto str = ((string_t *)data)[index];
275: 		// avoiding implicit cast and double conversion
276: 		return Value::BLOB(str.GetString(), false);
277: 	}
278: 	case TypeId::STRUCT: {
279: 		Value ret(TypeId::STRUCT);
280: 		ret.is_null = false;
281: 		// we can derive the value schema from the vector schema
282: 		for (auto &struct_child : StructVector::GetEntries(*this)) {
283: 			ret.struct_value.push_back(pair<string, Value>(struct_child.first, struct_child.second->GetValue(index)));
284: 		}
285: 		return ret;
286: 	}
287: 	case TypeId::LIST: {
288: 		Value ret(TypeId::LIST);
289: 		ret.is_null = false;
290: 		auto offlen = ((list_entry_t *)data)[index];
291: 		auto &child_cc = ListVector::GetEntry(*this);
292: 		for (idx_t i = offlen.offset; i < offlen.offset + offlen.length; i++) {
293: 			ret.list_value.push_back(child_cc.GetValue(0, i));
294: 		}
295: 		return ret;
296: 	}
297: 	default:
298: 		throw NotImplementedException("Unimplemented type for value access");
299: 	}
300: }
301: 
302: string VectorTypeToString(VectorType type) {
303: 	switch (type) {
304: 	case VectorType::FLAT_VECTOR:
305: 		return "FLAT";
306: 	case VectorType::SEQUENCE_VECTOR:
307: 		return "SEQUENCE";
308: 	case VectorType::DICTIONARY_VECTOR:
309: 		return "DICTIONARY";
310: 	case VectorType::CONSTANT_VECTOR:
311: 		return "CONSTANT";
312: 	default:
313: 		return "UNKNOWN";
314: 	}
315: }
316: 
317: string Vector::ToString(idx_t count) const {
318: 	string retval = VectorTypeToString(vector_type) + " " + TypeIdToString(type) + ": " + to_string(count) + " = [ ";
319: 	switch (vector_type) {
320: 	case VectorType::FLAT_VECTOR:
321: 	case VectorType::DICTIONARY_VECTOR:
322: 		for (idx_t i = 0; i < count; i++) {
323: 			retval += GetValue(i).ToString() + (i == count - 1 ? "" : ", ");
324: 		}
325: 		break;
326: 	case VectorType::CONSTANT_VECTOR:
327: 		retval += GetValue(0).ToString();
328: 		break;
329: 	case VectorType::SEQUENCE_VECTOR: {
330: 		int64_t start, increment;
331: 		SequenceVector::GetSequence(*this, start, increment);
332: 		for (idx_t i = 0; i < count; i++) {
333: 			retval += to_string(start + increment * i) + (i == count - 1 ? "" : ", ");
334: 		}
335: 		break;
336: 	}
337: 	default:
338: 		retval += "UNKNOWN VECTOR TYPE";
339: 		break;
340: 	}
341: 	retval += "]";
342: 	return retval;
343: }
344: 
345: void Vector::Print(idx_t count) {
346: 	Printer::Print(ToString(count));
347: }
348: 
349: string Vector::ToString() const {
350: 	string retval = VectorTypeToString(vector_type) + " " + TypeIdToString(type) + ": (UNKNOWN COUNT) [ ";
351: 	switch (vector_type) {
352: 	case VectorType::FLAT_VECTOR:
353: 	case VectorType::DICTIONARY_VECTOR:
354: 		break;
355: 	case VectorType::CONSTANT_VECTOR:
356: 		retval += GetValue(0).ToString();
357: 		break;
358: 	case VectorType::SEQUENCE_VECTOR: {
359: 		break;
360: 	}
361: 	default:
362: 		retval += "UNKNOWN VECTOR TYPE";
363: 		break;
364: 	}
365: 	retval += "]";
366: 	return retval;
367: }
368: 
369: void Vector::Print() {
370: 	Printer::Print(ToString());
371: }
372: 
373: template <class T> static void flatten_constant_vector_loop(data_ptr_t data, data_ptr_t old_data, idx_t count) {
374: 	auto constant = *((T *)old_data);
375: 	auto output = (T *)data;
376: 	for (idx_t i = 0; i < count; i++) {
377: 		output[i] = constant;
378: 	}
379: }
380: 
381: void Vector::Normalify(idx_t count) {
382: 	switch (vector_type) {
383: 	case VectorType::FLAT_VECTOR:
384: 		// already a flat vector
385: 		break;
386: 	case VectorType::DICTIONARY_VECTOR: {
387: 		// create a new flat vector of this type
388: 		Vector other(type);
389: 		// now copy the data of this vector to the other vector, removing the selection vector in the process
390: 		VectorOperations::Copy(*this, other, count, 0, 0);
391: 		// create a reference to the data in the other vector
392: 		this->Reference(other);
393: 		break;
394: 	}
395: 	case VectorType::CONSTANT_VECTOR: {
396: 		vector_type = VectorType::FLAT_VECTOR;
397: 		// allocate a new buffer for the vector
398: 		auto old_buffer = move(buffer);
399: 		auto old_data = data;
400: 		buffer = VectorBuffer::CreateStandardVector(type);
401: 		data = buffer->GetData();
402: 		if (nullmask[0]) {
403: 			// constant NULL, set nullmask
404: 			nullmask.set();
405: 			return;
406: 		}
407: 		// non-null constant: have to repeat the constant
408: 		switch (type) {
409: 		case TypeId::BOOL:
410: 		case TypeId::INT8:
411: 			flatten_constant_vector_loop<int8_t>(data, old_data, count);
412: 			break;
413: 		case TypeId::INT16:
414: 			flatten_constant_vector_loop<int16_t>(data, old_data, count);
415: 			break;
416: 		case TypeId::INT32:
417: 			flatten_constant_vector_loop<int32_t>(data, old_data, count);
418: 			break;
419: 		case TypeId::INT64:
420: 			flatten_constant_vector_loop<int64_t>(data, old_data, count);
421: 			break;
422: 		case TypeId::FLOAT:
423: 			flatten_constant_vector_loop<float>(data, old_data, count);
424: 			break;
425: 		case TypeId::DOUBLE:
426: 			flatten_constant_vector_loop<double>(data, old_data, count);
427: 			break;
428: 		case TypeId::HASH:
429: 			flatten_constant_vector_loop<hash_t>(data, old_data, count);
430: 			break;
431: 		case TypeId::POINTER:
432: 			flatten_constant_vector_loop<uintptr_t>(data, old_data, count);
433: 			break;
434: 		case TypeId::VARCHAR:
435: 			flatten_constant_vector_loop<string_t>(data, old_data, count);
436: 			break;
437: 		case TypeId::LIST: {
438: 			flatten_constant_vector_loop<list_entry_t>(data, old_data, count);
439: 			break;
440: 		}
441: 		case TypeId::STRUCT: {
442: 			for (auto &child : StructVector::GetEntries(*this)) {
443: 				assert(child.second->vector_type == VectorType::CONSTANT_VECTOR);
444: 				child.second->Normalify(count);
445: 			}
446: 		} break;
447: 		default:
448: 			throw NotImplementedException("Unimplemented type for VectorOperations::Normalify");
449: 		}
450: 		break;
451: 	}
452: 	case VectorType::SEQUENCE_VECTOR: {
453: 		int64_t start, increment;
454: 		SequenceVector::GetSequence(*this, start, increment);
455: 
456: 		vector_type = VectorType::FLAT_VECTOR;
457: 		buffer = VectorBuffer::CreateStandardVector(type);
458: 		data = buffer->GetData();
459: 		VectorOperations::GenerateSequence(*this, count, start, increment);
460: 		break;
461: 	}
462: 	default:
463: 		throw NotImplementedException("FIXME: unimplemented type for normalify");
464: 	}
465: }
466: 
467: void Vector::Normalify(const SelectionVector &sel, idx_t count) {
468: 	switch (vector_type) {
469: 	case VectorType::FLAT_VECTOR:
470: 		// already a flat vector
471: 		break;
472: 	case VectorType::SEQUENCE_VECTOR: {
473: 		int64_t start, increment;
474: 		SequenceVector::GetSequence(*this, start, increment);
475: 
476: 		vector_type = VectorType::FLAT_VECTOR;
477: 		buffer = VectorBuffer::CreateStandardVector(type);
478: 		data = buffer->GetData();
479: 		VectorOperations::GenerateSequence(*this, count, sel, start, increment);
480: 		break;
481: 	}
482: 	default:
483: 		throw NotImplementedException("Unimplemented type for normalify with selection vector");
484: 	}
485: }
486: 
487: void Vector::Orrify(idx_t count, VectorData &data) {
488: 	switch (vector_type) {
489: 	case VectorType::DICTIONARY_VECTOR: {
490: 		auto &sel = DictionaryVector::SelVector(*this);
491: 		auto &child = DictionaryVector::Child(*this);
492: 		if (child.vector_type == VectorType::FLAT_VECTOR) {
493: 			data.sel = &sel;
494: 			data.data = FlatVector::GetData(child);
495: 			data.nullmask = &FlatVector::Nullmask(child);
496: 		} else {
497: 			// dictionary with non-flat child: create a new reference to the child and normalify it
498: 			auto new_aux = make_unique<VectorChildBuffer>();
499: 			new_aux->data.Reference(child);
500: 			new_aux->data.Normalify(sel, count);
501: 
502: 			data.sel = &sel;
503: 			data.data = FlatVector::GetData(new_aux->data);
504: 			data.nullmask = &FlatVector::Nullmask(new_aux->data);
505: 			this->auxiliary = move(new_aux);
506: 		}
507: 		break;
508: 	}
509: 	case VectorType::CONSTANT_VECTOR:
510: 		data.sel = &ConstantVector::ZeroSelectionVector;
511: 		data.data = ConstantVector::GetData(*this);
512: 		data.nullmask = &nullmask;
513: 		break;
514: 	default:
515: 		Normalify(count);
516: 		data.sel = &FlatVector::IncrementalSelectionVector;
517: 		data.data = FlatVector::GetData(*this);
518: 		data.nullmask = &nullmask;
519: 		break;
520: 	}
521: }
522: 
523: void Vector::Sequence(int64_t start, int64_t increment) {
524: 	vector_type = VectorType::SEQUENCE_VECTOR;
525: 	this->buffer = make_buffer<VectorBuffer>(sizeof(int64_t) * 2);
526: 	auto data = (int64_t *)buffer->GetData();
527: 	data[0] = start;
528: 	data[1] = increment;
529: 	nullmask.reset();
530: 	auxiliary.reset();
531: }
532: 
533: void Vector::Serialize(idx_t count, Serializer &serializer) {
534: 	if (TypeIsConstantSize(type)) {
535: 		// constant size type: simple copy
536: 		idx_t write_size = GetTypeIdSize(type) * count;
537: 		auto ptr = unique_ptr<data_t[]>(new data_t[write_size]);
538: 		VectorOperations::WriteToStorage(*this, count, ptr.get());
539: 		serializer.WriteData(ptr.get(), write_size);
540: 	} else {
541: 		VectorData vdata;
542: 		Orrify(count, vdata);
543: 
544: 		switch (type) {
545: 		case TypeId::VARCHAR: {
546: 			auto strings = (string_t *)vdata.data;
547: 			for (idx_t i = 0; i < count; i++) {
548: 				auto idx = vdata.sel->get_index(i);
549: 				auto source = (*vdata.nullmask)[idx] ? NullValue<const char *>() : strings[idx].GetData();
550: 				serializer.WriteString(source);
551: 			}
552: 			break;
553: 		}
554: 		default:
555: 			throw NotImplementedException("Unimplemented type for Vector::Serialize!");
556: 		}
557: 	}
558: }
559: 
560: void Vector::Deserialize(idx_t count, Deserializer &source) {
561: 	if (TypeIsConstantSize(type)) {
562: 		// constant size type: read fixed amount of data from
563: 		auto column_size = GetTypeIdSize(type) * count;
564: 		auto ptr = unique_ptr<data_t[]>(new data_t[column_size]);
565: 		source.ReadData(ptr.get(), column_size);
566: 
567: 		VectorOperations::ReadFromStorage(ptr.get(), count, *this);
568: 	} else {
569: 		auto strings = FlatVector::GetData<string_t>(*this);
570: 		auto &nullmask = FlatVector::Nullmask(*this);
571: 		for (idx_t i = 0; i < count; i++) {
572: 			// read the strings
573: 			auto str = source.Read<string>();
574: 			// now add the string to the StringHeap of the vector
575: 			// and write the pointer into the vector
576: 			if (IsNullValue<const char *>((const char *)str.c_str())) {
577: 				nullmask[i] = true;
578: 			} else {
579: 				strings[i] = StringVector::AddString(*this, str);
580: 			}
581: 		}
582: 	}
583: }
584: 
585: void Vector::UTFVerify(const SelectionVector &sel, idx_t count) {
586: #ifdef DEBUG
587: 	if (count == 0) {
588: 		return;
589: 	}
590: 	if (type == TypeId::VARCHAR) {
591: 		// we just touch all the strings and let the sanitizer figure out if any
592: 		// of them are deallocated/corrupt
593: 		switch (vector_type) {
594: 		case VectorType::CONSTANT_VECTOR: {
595: 			auto string = ConstantVector::GetData<string_t>(*this);
596: 			if (!ConstantVector::IsNull(*this)) {
597: 				string->Verify();
598: 			}
599: 			break;
600: 		}
601: 		case VectorType::FLAT_VECTOR: {
602: 			auto strings = FlatVector::GetData<string_t>(*this);
603: 			for (idx_t i = 0; i < count; i++) {
604: 				auto oidx = sel.get_index(i);
605: 				if (!nullmask[oidx]) {
606: 					strings[oidx].Verify();
607: 				}
608: 			}
609: 			break;
610: 		}
611: 		default:
612: 			break;
613: 		}
614: 	}
615: #endif
616: }
617: 
618: void Vector::UTFVerify(idx_t count) {
619: 	UTFVerify(FlatVector::IncrementalSelectionVector, count);
620: }
621: 
622: void Vector::Verify(const SelectionVector &sel, idx_t count) {
623: #ifdef DEBUG
624: 	if (count == 0) {
625: 		return;
626: 	}
627: 	if (vector_type == VectorType::DICTIONARY_VECTOR) {
628: 		auto &child = DictionaryVector::Child(*this);
629: 		auto &dict_sel = DictionaryVector::SelVector(*this);
630: 		for (idx_t i = 0; i < count; i++) {
631: 			auto oidx = sel.get_index(i);
632: 			auto idx = dict_sel.get_index(oidx);
633: 			assert(idx < STANDARD_VECTOR_SIZE);
634: 		}
635: 		// merge the selection vectors and verify the child
636: 		auto new_buffer = dict_sel.Slice(sel, count);
637: 		SelectionVector new_sel(new_buffer);
638: 		child.Verify(new_sel, count);
639: 		return;
640: 	}
641: 	if (type == TypeId::DOUBLE) {
642: 		// verify that there are no INF or NAN values
643: 		switch (vector_type) {
644: 		case VectorType::CONSTANT_VECTOR: {
645: 			auto dbl = ConstantVector::GetData<double>(*this);
646: 			if (!ConstantVector::IsNull(*this)) {
647: 				assert(Value::DoubleIsValid(*dbl));
648: 			}
649: 			break;
650: 		}
651: 		case VectorType::FLAT_VECTOR: {
652: 			auto doubles = FlatVector::GetData<double>(*this);
653: 			for (idx_t i = 0; i < count; i++) {
654: 				auto oidx = sel.get_index(i);
655: 				if (!nullmask[oidx]) {
656: 					assert(Value::DoubleIsValid(doubles[oidx]));
657: 				}
658: 			}
659: 			break;
660: 		}
661: 		default:
662: 			break;
663: 		}
664: 	}
665: 
666: 	if (type == TypeId::STRUCT) {
667: 		if (vector_type == VectorType::FLAT_VECTOR || vector_type == VectorType::CONSTANT_VECTOR) {
668: 			auto &children = StructVector::GetEntries(*this);
669: 			assert(children.size() > 0);
670: 			for (auto &child : children) {
671: 				child.second->Verify(sel, count);
672: 			}
673: 		}
674: 	}
675: 
676: 	if (type == TypeId::LIST) {
677: 		if (vector_type == VectorType::CONSTANT_VECTOR) {
678: 			if (!ConstantVector::IsNull(*this)) {
679: 				ListVector::GetEntry(*this).Verify();
680: 				auto le = ConstantVector::GetData<list_entry_t>(*this);
681: 				assert(le->offset + le->length <= ListVector::GetEntry(*this).count);
682: 			}
683: 		} else if (vector_type == VectorType::FLAT_VECTOR) {
684: 			if (ListVector::HasEntry(*this)) {
685: 				ListVector::GetEntry(*this).Verify();
686: 			}
687: 			auto list_data = FlatVector::GetData<list_entry_t>(*this);
688: 			for (idx_t i = 0; i < count; i++) {
689: 				auto idx = sel.get_index(i);
690: 				auto &le = list_data[idx];
691: 				if (!nullmask[idx]) {
692: 					assert(le.offset + le.length <= ListVector::GetEntry(*this).count);
693: 				}
694: 			}
695: 		}
696: 	}
697: // TODO verify list and struct
698: #endif
699: }
700: 
701: void Vector::Verify(idx_t count) {
702: 	Verify(FlatVector::IncrementalSelectionVector, count);
703: }
704: 
705: string_t StringVector::AddString(Vector &vector, const char *data, idx_t len) {
706: 	return StringVector::AddString(vector, string_t(data, len));
707: }
708: 
709: string_t StringVector::AddString(Vector &vector, const char *data) {
710: 	return StringVector::AddString(vector, string_t(data, strlen(data)));
711: }
712: 
713: string_t StringVector::AddString(Vector &vector, const string &data) {
714: 	return StringVector::AddString(vector, string_t(data.c_str(), data.size()));
715: }
716: 
717: string_t StringVector::AddString(Vector &vector, string_t data) {
718: 	assert(vector.type == TypeId::VARCHAR);
719: 	if (data.IsInlined()) {
720: 		// string will be inlined: no need to store in string heap
721: 		return data;
722: 	}
723: 	if (!vector.auxiliary) {
724: 		vector.auxiliary = make_buffer<VectorStringBuffer>();
725: 	}
726: 	assert(vector.auxiliary->type == VectorBufferType::STRING_BUFFER);
727: 	auto &string_buffer = (VectorStringBuffer &)*vector.auxiliary;
728: 	return string_buffer.AddString(data);
729: }
730: 
731: string_t StringVector::AddBlob(Vector &vector, string_t data) {
732: 	assert(vector.type == TypeId::VARCHAR);
733: 	if (data.IsInlined()) {
734: 		// string will be inlined: no need to store in string heap
735: 		return data;
736: 	}
737: 	if (!vector.auxiliary) {
738: 		vector.auxiliary = make_buffer<VectorStringBuffer>();
739: 	}
740: 	assert(vector.auxiliary->type == VectorBufferType::STRING_BUFFER);
741: 	auto &string_buffer = (VectorStringBuffer &)*vector.auxiliary;
742: 	return string_buffer.AddBlob(data);
743: }
744: 
745: string_t StringVector::EmptyString(Vector &vector, idx_t len) {
746: 	assert(vector.type == TypeId::VARCHAR);
747: 	if (len < string_t::INLINE_LENGTH) {
748: 		return string_t(len);
749: 	}
750: 	if (!vector.auxiliary) {
751: 		vector.auxiliary = make_buffer<VectorStringBuffer>();
752: 	}
753: 	assert(vector.auxiliary->type == VectorBufferType::STRING_BUFFER);
754: 	auto &string_buffer = (VectorStringBuffer &)*vector.auxiliary;
755: 	return string_buffer.EmptyString(len);
756: }
757: 
758: void StringVector::AddHeapReference(Vector &vector, Vector &other) {
759: 	assert(vector.type == TypeId::VARCHAR);
760: 	assert(other.type == TypeId::VARCHAR);
761: 
762: 	if (other.vector_type == VectorType::DICTIONARY_VECTOR) {
763: 		StringVector::AddHeapReference(vector, DictionaryVector::Child(other));
764: 		return;
765: 	}
766: 	if (!other.auxiliary) {
767: 		return;
768: 	}
769: 	if (!vector.auxiliary) {
770: 		vector.auxiliary = make_buffer<VectorStringBuffer>();
771: 	}
772: 	assert(vector.auxiliary->type == VectorBufferType::STRING_BUFFER);
773: 	assert(other.auxiliary->type == VectorBufferType::STRING_BUFFER);
774: 	auto &string_buffer = (VectorStringBuffer &)*vector.auxiliary;
775: 	string_buffer.AddHeapReference(other.auxiliary);
776: }
777: 
778: bool StructVector::HasEntries(const Vector &vector) {
779: 	assert(vector.type == TypeId::STRUCT);
780: 	assert(vector.vector_type == VectorType::FLAT_VECTOR || vector.vector_type == VectorType::CONSTANT_VECTOR);
781: 	assert(vector.auxiliary == nullptr || vector.auxiliary->type == VectorBufferType::STRUCT_BUFFER);
782: 	return vector.auxiliary != nullptr;
783: }
784: 
785: child_list_t<unique_ptr<Vector>> &StructVector::GetEntries(const Vector &vector) {
786: 	assert(vector.type == TypeId::STRUCT);
787: 	assert(vector.vector_type == VectorType::FLAT_VECTOR || vector.vector_type == VectorType::CONSTANT_VECTOR);
788: 	assert(vector.auxiliary);
789: 	assert(vector.auxiliary->type == VectorBufferType::STRUCT_BUFFER);
790: 	return ((VectorStructBuffer *)vector.auxiliary.get())->GetChildren();
791: }
792: 
793: void StructVector::AddEntry(Vector &vector, string name, unique_ptr<Vector> entry) {
794: 	// TODO asser that an entry with this name does not already exist
795: 	assert(vector.type == TypeId::STRUCT);
796: 	assert(vector.vector_type == VectorType::FLAT_VECTOR || vector.vector_type == VectorType::CONSTANT_VECTOR);
797: 	if (!vector.auxiliary) {
798: 		vector.auxiliary = make_buffer<VectorStructBuffer>();
799: 	}
800: 	assert(vector.auxiliary);
801: 	assert(vector.auxiliary->type == VectorBufferType::STRUCT_BUFFER);
802: 	((VectorStructBuffer *)vector.auxiliary.get())->AddChild(name, move(entry));
803: }
804: 
805: bool ListVector::HasEntry(const Vector &vector) {
806: 	assert(vector.type == TypeId::LIST);
807: 	if (vector.vector_type == VectorType::DICTIONARY_VECTOR) {
808: 		auto &child = DictionaryVector::Child(vector);
809: 		return ListVector::HasEntry(child);
810: 	}
811: 	assert(vector.vector_type == VectorType::FLAT_VECTOR || vector.vector_type == VectorType::CONSTANT_VECTOR);
812: 	return vector.auxiliary != nullptr;
813: }
814: 
815: ChunkCollection &ListVector::GetEntry(const Vector &vector) {
816: 	assert(vector.type == TypeId::LIST);
817: 	if (vector.vector_type == VectorType::DICTIONARY_VECTOR) {
818: 		auto &child = DictionaryVector::Child(vector);
819: 		return ListVector::GetEntry(child);
820: 	}
821: 	assert(vector.vector_type == VectorType::FLAT_VECTOR || vector.vector_type == VectorType::CONSTANT_VECTOR);
822: 	assert(vector.auxiliary);
823: 	assert(vector.auxiliary->type == VectorBufferType::LIST_BUFFER);
824: 	return ((VectorListBuffer *)vector.auxiliary.get())->GetChild();
825: }
826: 
827: void ListVector::SetEntry(Vector &vector, unique_ptr<ChunkCollection> cc) {
828: 	assert(vector.type == TypeId::LIST);
829: 	assert(vector.vector_type == VectorType::FLAT_VECTOR || vector.vector_type == VectorType::CONSTANT_VECTOR);
830: 	if (!vector.auxiliary) {
831: 		vector.auxiliary = make_buffer<VectorListBuffer>();
832: 	}
833: 	assert(vector.auxiliary);
834: 	assert(vector.auxiliary->type == VectorBufferType::LIST_BUFFER);
835: 	((VectorListBuffer *)vector.auxiliary.get())->SetChild(move(cc));
836: }
837: 
838: } // namespace duckdb
[end of src/common/types/vector.cpp]
[start of src/planner/binder/tableref/plan_joinref.cpp]
1: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
2: #include "duckdb/planner/expression/bound_comparison_expression.hpp"
3: #include "duckdb/planner/expression/bound_conjunction_expression.hpp"
4: #include "duckdb/planner/expression/bound_constant_expression.hpp"
5: #include "duckdb/planner/expression/bound_operator_expression.hpp"
6: #include "duckdb/planner/expression/bound_subquery_expression.hpp"
7: #include "duckdb/planner/expression_iterator.hpp"
8: #include "duckdb/planner/binder.hpp"
9: #include "duckdb/planner/operator/logical_any_join.hpp"
10: #include "duckdb/planner/operator/logical_comparison_join.hpp"
11: #include "duckdb/planner/operator/logical_cross_product.hpp"
12: #include "duckdb/planner/operator/logical_filter.hpp"
13: #include "duckdb/planner/tableref/bound_joinref.hpp"
14: 
15: using namespace duckdb;
16: using namespace std;
17: 
18: //! Create a JoinCondition from a comparison
19: static bool CreateJoinCondition(Expression &expr, unordered_set<idx_t> &left_bindings,
20:                                 unordered_set<idx_t> &right_bindings, vector<JoinCondition> &conditions) {
21: 	// comparison
22: 	auto &comparison = (BoundComparisonExpression &)expr;
23: 	auto left_side = JoinSide::GetJoinSide(*comparison.left, left_bindings, right_bindings);
24: 	auto right_side = JoinSide::GetJoinSide(*comparison.right, left_bindings, right_bindings);
25: 	if (left_side != JoinSide::BOTH && right_side != JoinSide::BOTH) {
26: 		// join condition can be divided in a left/right side
27: 		JoinCondition condition;
28: 		condition.comparison = expr.type;
29: 		auto left = move(comparison.left);
30: 		auto right = move(comparison.right);
31: 		if (left_side == JoinSide::RIGHT) {
32: 			// left = right, right = left, flip the comparison symbol and reverse sides
33: 			swap(left, right);
34: 			condition.comparison = FlipComparisionExpression(expr.type);
35: 		}
36: 		condition.left = move(left);
37: 		condition.right = move(right);
38: 		conditions.push_back(move(condition));
39: 		return true;
40: 	}
41: 	return false;
42: }
43: 
44: unique_ptr<LogicalOperator> LogicalComparisonJoin::CreateJoin(JoinType type, unique_ptr<LogicalOperator> left_child,
45:                                                               unique_ptr<LogicalOperator> right_child,
46:                                                               unordered_set<idx_t> &left_bindings,
47:                                                               unordered_set<idx_t> &right_bindings,
48:                                                               vector<unique_ptr<Expression>> &expressions) {
49: 	vector<JoinCondition> conditions;
50: 	vector<unique_ptr<Expression>> arbitrary_expressions;
51: 	// first check if we can create
52: 	for (idx_t i = 0; i < expressions.size(); i++) {
53: 		auto &expr = expressions[i];
54: 		auto total_side = JoinSide::GetJoinSide(*expr, left_bindings, right_bindings);
55: 		if (total_side != JoinSide::BOTH) {
56: 			// join condition does not reference both sides, add it as filter under the join
57: 			if (type == JoinType::LEFT && total_side == JoinSide::RIGHT) {
58: 				// filter is on RHS and the join is a LEFT OUTER join, we can push it in the right child
59: 				if (right_child->type != LogicalOperatorType::FILTER) {
60: 					// not a filter yet, push a new empty filter
61: 					auto filter = make_unique<LogicalFilter>();
62: 					filter->AddChild(move(right_child));
63: 					right_child = move(filter);
64: 				}
65: 				// push the expression into the filter
66: 				auto &filter = (LogicalFilter &)*right_child;
67: 				filter.expressions.push_back(move(expr));
68: 				continue;
69: 			}
70: 		} else if (expr->type >= ExpressionType::COMPARE_EQUAL &&
71: 		           expr->type <= ExpressionType::COMPARE_GREATERTHANOREQUALTO) {
72: 			// comparison, check if we can create a comparison JoinCondition
73: 			if (CreateJoinCondition(*expr, left_bindings, right_bindings, conditions)) {
74: 				// successfully created the join condition
75: 				continue;
76: 			}
77: 		}
78: 		arbitrary_expressions.push_back(move(expr));
79: 	}
80: 	if (conditions.size() > 0) {
81: 		// we successfully convertedexpressions into JoinConditions
82: 		// create a LogicalComparisonJoin
83: 		auto comp_join = make_unique<LogicalComparisonJoin>(type);
84: 		comp_join->conditions = move(conditions);
85: 		comp_join->children.push_back(move(left_child));
86: 		comp_join->children.push_back(move(right_child));
87: 		if (arbitrary_expressions.size() > 0) {
88: 			// we have some arbitrary expressions as well
89: 			// add them to a filter
90: 			auto filter = make_unique<LogicalFilter>();
91: 			for (auto &expr : arbitrary_expressions) {
92: 				filter->expressions.push_back(move(expr));
93: 			}
94: 			LogicalFilter::SplitPredicates(filter->expressions);
95: 			filter->children.push_back(move(comp_join));
96: 			return move(filter);
97: 		}
98: 		return move(comp_join);
99: 	} else {
100: 		if (arbitrary_expressions.size() == 0) {
101: 			// all conditions were pushed down, add TRUE predicate
102: 			arbitrary_expressions.push_back(make_unique<BoundConstantExpression>(Value::BOOLEAN(true)));
103: 		}
104: 		// if we get here we could not create any JoinConditions
105: 		// turn this into an arbitrary expression join
106: 		auto any_join = make_unique<LogicalAnyJoin>(type);
107: 		// create the condition
108: 		any_join->children.push_back(move(left_child));
109: 		any_join->children.push_back(move(right_child));
110: 		// AND all the arbitrary expressions together
111: 		// do the same with any remaining conditions
112: 		any_join->condition = move(arbitrary_expressions[0]);
113: 		for (idx_t i = 1; i < arbitrary_expressions.size(); i++) {
114: 			any_join->condition = make_unique<BoundConjunctionExpression>(
115: 			    ExpressionType::CONJUNCTION_AND, move(any_join->condition), move(arbitrary_expressions[i]));
116: 		}
117: 		return move(any_join);
118: 	}
119: }
120: 
121: unique_ptr<LogicalOperator> Binder::CreatePlan(BoundJoinRef &ref) {
122: 	auto left = CreatePlan(*ref.left);
123: 	auto right = CreatePlan(*ref.right);
124: 	if (ref.type == JoinType::RIGHT) {
125: 		ref.type = JoinType::LEFT;
126: 		std::swap(left, right);
127: 	}
128: 
129: 	if (ref.type == JoinType::INNER) {
130: 		// inner join, generate a cross product + filter
131: 		// this will be later turned into a proper join by the join order optimizer
132: 		auto cross_product = make_unique<LogicalCrossProduct>();
133: 
134: 		cross_product->AddChild(move(left));
135: 		cross_product->AddChild(move(right));
136: 
137: 		unique_ptr<LogicalOperator> root = move(cross_product);
138: 
139: 		auto filter = make_unique<LogicalFilter>(move(ref.condition));
140: 		// visit the expressions in the filter
141: 		for (idx_t i = 0; i < filter->expressions.size(); i++) {
142: 			PlanSubqueries(&filter->expressions[i], &root);
143: 		}
144: 		filter->AddChild(move(root));
145: 		return move(filter);
146: 	}
147: 
148: 	// split the expressions by the AND clause
149: 	vector<unique_ptr<Expression>> expressions;
150: 	expressions.push_back(move(ref.condition));
151: 	LogicalFilter::SplitPredicates(expressions);
152: 
153: 	// find the table bindings on the LHS and RHS of the join
154: 	unordered_set<idx_t> left_bindings, right_bindings;
155: 	LogicalJoin::GetTableReferences(*left, left_bindings);
156: 	LogicalJoin::GetTableReferences(*right, right_bindings);
157: 	// now create the join operator from the set of join conditions
158: 	auto result = LogicalComparisonJoin::CreateJoin(ref.type, move(left), move(right), left_bindings, right_bindings,
159: 	                                                expressions);
160: 
161: 	LogicalOperator *join;
162: 	if (result->type == LogicalOperatorType::FILTER) {
163: 		join = result->children[0].get();
164: 	} else {
165: 		join = result.get();
166: 	}
167: 
168: 	// we visit the expressions depending on the type of join
169: 	if (join->type == LogicalOperatorType::COMPARISON_JOIN) {
170: 		// comparison join
171: 		// in this join we visit the expressions on the LHS with the LHS as root node
172: 		// and the expressions on the RHS with the RHS as root node
173: 		auto &comp_join = (LogicalComparisonJoin &)*join;
174: 		for (idx_t i = 0; i < comp_join.conditions.size(); i++) {
175: 			PlanSubqueries(&comp_join.conditions[i].left, &comp_join.children[0]);
176: 			PlanSubqueries(&comp_join.conditions[i].right, &comp_join.children[1]);
177: 		}
178: 	} else if (join->type == LogicalOperatorType::ANY_JOIN) {
179: 		auto &any_join = (LogicalAnyJoin &)*join;
180: 		// for the any join we just visit the condition
181: 		if (any_join.condition->HasSubquery()) {
182: 			throw NotImplementedException("Cannot perform non-inner join on subquery!");
183: 		}
184: 	}
185: 	return result;
186: }
[end of src/planner/binder/tableref/plan_joinref.cpp]
[start of src/planner/logical_operator_visitor.cpp]
1: #include "duckdb/planner/logical_operator_visitor.hpp"
2: 
3: #include "duckdb/planner/expression/list.hpp"
4: #include "duckdb/planner/expression_iterator.hpp"
5: #include "duckdb/planner/operator/list.hpp"
6: 
7: using namespace duckdb;
8: using namespace std;
9: 
10: void LogicalOperatorVisitor::VisitOperator(LogicalOperator &op) {
11: 	VisitOperatorChildren(op);
12: 	VisitOperatorExpressions(op);
13: }
14: 
15: void LogicalOperatorVisitor::VisitOperatorChildren(LogicalOperator &op) {
16: 	for (auto &child : op.children) {
17: 		VisitOperator(*child);
18: 	}
19: }
20: 
21: void LogicalOperatorVisitor::VisitOperatorExpressions(LogicalOperator &op) {
22: 	switch (op.type) {
23: 	case LogicalOperatorType::EXPRESSION_GET: {
24: 		auto &get = (LogicalExpressionGet &)op;
25: 		for (auto &expr_list : get.expressions) {
26: 			for (auto &expr : expr_list) {
27: 				VisitExpression(&expr);
28: 			}
29: 		}
30: 		break;
31: 	}
32: 	case LogicalOperatorType::ORDER_BY: {
33: 		auto &order = (LogicalOrder &)op;
34: 		for (auto &node : order.orders) {
35: 			VisitExpression(&node.expression);
36: 		}
37: 		break;
38: 	}
39: 	case LogicalOperatorType::TOP_N: {
40: 		auto &order = (LogicalTopN &)op;
41: 		for (auto &node : order.orders) {
42: 			VisitExpression(&node.expression);
43: 		}
44: 		break;
45: 	}
46: 	case LogicalOperatorType::DISTINCT: {
47: 		auto &distinct = (LogicalDistinct &)op;
48: 		for (auto &target : distinct.distinct_targets) {
49: 			VisitExpression(&target);
50: 		}
51: 		break;
52: 	}
53: 	case LogicalOperatorType::DELIM_JOIN:
54: 	case LogicalOperatorType::COMPARISON_JOIN: {
55: 		auto &join = (LogicalComparisonJoin &)op;
56: 		for (auto &cond : join.conditions) {
57: 			VisitExpression(&cond.left);
58: 			VisitExpression(&cond.right);
59: 		}
60: 		break;
61: 	}
62: 	case LogicalOperatorType::ANY_JOIN: {
63: 		auto &join = (LogicalAnyJoin &)op;
64: 		VisitExpression(&join.condition);
65: 		break;
66: 	}
67: 	case LogicalOperatorType::AGGREGATE_AND_GROUP_BY: {
68: 		auto &aggr = (LogicalAggregate &)op;
69: 		for (idx_t i = 0; i < aggr.groups.size(); i++) {
70: 			VisitExpression(&aggr.groups[i]);
71: 		}
72: 		break;
73: 	}
74: 	default:
75: 		break;
76: 	}
77: 	for (idx_t i = 0; i < op.expressions.size(); i++) {
78: 		VisitExpression(&op.expressions[i]);
79: 	}
80: }
81: 
82: void LogicalOperatorVisitor::VisitExpression(unique_ptr<Expression> *expression) {
83: 	auto &expr = **expression;
84: 	unique_ptr<Expression> result;
85: 	switch (expr.GetExpressionClass()) {
86: 	case ExpressionClass::BOUND_AGGREGATE:
87: 		result = VisitReplace((BoundAggregateExpression &)expr, expression);
88: 		break;
89: 	case ExpressionClass::BOUND_BETWEEN:
90: 		result = VisitReplace((BoundBetweenExpression &)expr, expression);
91: 		break;
92: 	case ExpressionClass::BOUND_CASE:
93: 		result = VisitReplace((BoundCaseExpression &)expr, expression);
94: 		break;
95: 	case ExpressionClass::BOUND_CAST:
96: 		result = VisitReplace((BoundCastExpression &)expr, expression);
97: 		break;
98: 	case ExpressionClass::BOUND_COLUMN_REF:
99: 		result = VisitReplace((BoundColumnRefExpression &)expr, expression);
100: 		break;
101: 	case ExpressionClass::BOUND_COMPARISON:
102: 		result = VisitReplace((BoundComparisonExpression &)expr, expression);
103: 		break;
104: 	case ExpressionClass::BOUND_CONJUNCTION:
105: 		result = VisitReplace((BoundConjunctionExpression &)expr, expression);
106: 		break;
107: 	case ExpressionClass::BOUND_CONSTANT:
108: 		result = VisitReplace((BoundConstantExpression &)expr, expression);
109: 		break;
110: 	case ExpressionClass::BOUND_FUNCTION:
111: 		result = VisitReplace((BoundFunctionExpression &)expr, expression);
112: 		break;
113: 	case ExpressionClass::BOUND_SUBQUERY:
114: 		result = VisitReplace((BoundSubqueryExpression &)expr, expression);
115: 		break;
116: 	case ExpressionClass::BOUND_OPERATOR:
117: 		result = VisitReplace((BoundOperatorExpression &)expr, expression);
118: 		break;
119: 	case ExpressionClass::BOUND_PARAMETER:
120: 		result = VisitReplace((BoundParameterExpression &)expr, expression);
121: 		break;
122: 	case ExpressionClass::BOUND_REF:
123: 		result = VisitReplace((BoundReferenceExpression &)expr, expression);
124: 		break;
125: 	case ExpressionClass::BOUND_DEFAULT:
126: 		result = VisitReplace((BoundDefaultExpression &)expr, expression);
127: 		break;
128: 	case ExpressionClass::COMMON_SUBEXPRESSION:
129: 		result = VisitReplace((CommonSubExpression &)expr, expression);
130: 		break;
131: 	case ExpressionClass::BOUND_WINDOW:
132: 		result = VisitReplace((BoundWindowExpression &)expr, expression);
133: 		break;
134: 	case ExpressionClass::BOUND_UNNEST:
135: 		result = VisitReplace((BoundUnnestExpression &)expr, expression);
136: 		break;
137: 	default:
138: 		assert(0);
139: 	}
140: 	if (result) {
141: 		*expression = move(result);
142: 	} else {
143: 		// visit the children of this node
144: 		VisitExpressionChildren(expr);
145: 	}
146: }
147: 
148: void LogicalOperatorVisitor::VisitExpressionChildren(Expression &expr) {
149: 	ExpressionIterator::EnumerateChildren(expr, [&](unique_ptr<Expression> expr) -> unique_ptr<Expression> {
150: 		VisitExpression(&expr);
151: 		return move(expr);
152: 	});
153: }
154: 
155: unique_ptr<Expression> LogicalOperatorVisitor::VisitReplace(BoundAggregateExpression &expr,
156:                                                             unique_ptr<Expression> *expr_ptr) {
157: 	return nullptr;
158: }
159: 
160: unique_ptr<Expression> LogicalOperatorVisitor::VisitReplace(BoundBetweenExpression &expr,
161:                                                             unique_ptr<Expression> *expr_ptr) {
162: 	return nullptr;
163: }
164: 
165: unique_ptr<Expression> LogicalOperatorVisitor::VisitReplace(BoundCaseExpression &expr,
166:                                                             unique_ptr<Expression> *expr_ptr) {
167: 	return nullptr;
168: }
169: 
170: unique_ptr<Expression> LogicalOperatorVisitor::VisitReplace(BoundCastExpression &expr,
171:                                                             unique_ptr<Expression> *expr_ptr) {
172: 	return nullptr;
173: }
174: 
175: unique_ptr<Expression> LogicalOperatorVisitor::VisitReplace(BoundColumnRefExpression &expr,
176:                                                             unique_ptr<Expression> *expr_ptr) {
177: 	return nullptr;
178: }
179: 
180: unique_ptr<Expression> LogicalOperatorVisitor::VisitReplace(BoundComparisonExpression &expr,
181:                                                             unique_ptr<Expression> *expr_ptr) {
182: 	return nullptr;
183: }
184: 
185: unique_ptr<Expression> LogicalOperatorVisitor::VisitReplace(BoundConjunctionExpression &expr,
186:                                                             unique_ptr<Expression> *expr_ptr) {
187: 	return nullptr;
188: }
189: 
190: unique_ptr<Expression> LogicalOperatorVisitor::VisitReplace(BoundConstantExpression &expr,
191:                                                             unique_ptr<Expression> *expr_ptr) {
192: 	return nullptr;
193: }
194: 
195: unique_ptr<Expression> LogicalOperatorVisitor::VisitReplace(BoundDefaultExpression &expr,
196:                                                             unique_ptr<Expression> *expr_ptr) {
197: 	return nullptr;
198: }
199: 
200: unique_ptr<Expression> LogicalOperatorVisitor::VisitReplace(BoundFunctionExpression &expr,
201:                                                             unique_ptr<Expression> *expr_ptr) {
202: 	return nullptr;
203: }
204: 
205: unique_ptr<Expression> LogicalOperatorVisitor::VisitReplace(BoundOperatorExpression &expr,
206:                                                             unique_ptr<Expression> *expr_ptr) {
207: 	return nullptr;
208: }
209: 
210: unique_ptr<Expression> LogicalOperatorVisitor::VisitReplace(BoundParameterExpression &expr,
211:                                                             unique_ptr<Expression> *expr_ptr) {
212: 	return nullptr;
213: }
214: 
215: unique_ptr<Expression> LogicalOperatorVisitor::VisitReplace(BoundReferenceExpression &expr,
216:                                                             unique_ptr<Expression> *expr_ptr) {
217: 	return nullptr;
218: }
219: 
220: unique_ptr<Expression> LogicalOperatorVisitor::VisitReplace(BoundSubqueryExpression &expr,
221:                                                             unique_ptr<Expression> *expr_ptr) {
222: 	return nullptr;
223: }
224: 
225: unique_ptr<Expression> LogicalOperatorVisitor::VisitReplace(BoundWindowExpression &expr,
226:                                                             unique_ptr<Expression> *expr_ptr) {
227: 	return nullptr;
228: }
229: 
230: unique_ptr<Expression> LogicalOperatorVisitor::VisitReplace(BoundUnnestExpression &expr,
231:                                                             unique_ptr<Expression> *expr_ptr) {
232: 	return nullptr;
233: }
234: 
235: unique_ptr<Expression> LogicalOperatorVisitor::VisitReplace(CommonSubExpression &expr,
236:                                                             unique_ptr<Expression> *expr_ptr) {
237: 	return nullptr;
238: }
[end of src/planner/logical_operator_visitor.cpp]
[start of src/storage/data_table.cpp]
1: #include "duckdb/storage/data_table.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/common/exception.hpp"
5: #include "duckdb/common/helper.hpp"
6: #include "duckdb/common/vector_operations/vector_operations.hpp"
7: #include "duckdb/execution/expression_executor.hpp"
8: #include "duckdb/planner/constraints/list.hpp"
9: #include "duckdb/transaction/transaction.hpp"
10: #include "duckdb/transaction/transaction_manager.hpp"
11: #include "duckdb/storage/table/transient_segment.hpp"
12: #include "duckdb/storage/storage_manager.hpp"
13: 
14: using namespace duckdb;
15: using namespace std;
16: using namespace chrono;
17: 
18: DataTable::DataTable(StorageManager &storage, string schema, string table, vector<TypeId> types_,
19:                      unique_ptr<vector<unique_ptr<PersistentSegment>>[]> data)
20:     : info(make_shared<DataTableInfo>(schema, table)), types(types_), storage(storage),
21:       persistent_manager(make_shared<VersionManager>(*info)), transient_manager(make_shared<VersionManager>(*info)),
22:       is_root(true) {
23: 	// set up the segment trees for the column segments
24: 	for (idx_t i = 0; i < types.size(); i++) {
25: 		auto column_data = make_shared<ColumnData>(*storage.buffer_manager, *info);
26: 		column_data->type = types[i];
27: 		column_data->column_idx = i;
28: 		columns.push_back(move(column_data));
29: 	}
30: 
31: 	// initialize the table with the existing data from disk, if any
32: 	if (data && data[0].size() > 0) {
33: 		// first append all the segments to the set of column segments
34: 		for (idx_t i = 0; i < types.size(); i++) {
35: 			columns[i]->Initialize(data[i]);
36: 			if (columns[i]->persistent_rows != columns[0]->persistent_rows) {
37: 				throw Exception("Column length mismatch in table load!");
38: 			}
39: 		}
40: 		persistent_manager->max_row = columns[0]->persistent_rows;
41: 		transient_manager->base_row = persistent_manager->max_row;
42: 	}
43: }
44: 
45: DataTable::DataTable(ClientContext &context, DataTable &parent, ColumnDefinition &new_column, Expression *default_value)
46:     : info(parent.info), types(parent.types), storage(parent.storage), persistent_manager(parent.persistent_manager),
47:       transient_manager(parent.transient_manager), columns(parent.columns), is_root(true) {
48: 	// prevent any new tuples from being added to the parent
49: 	lock_guard<mutex> parent_lock(parent.append_lock);
50: 	// add the new column to this DataTable
51: 	auto new_column_type = GetInternalType(new_column.type);
52: 	idx_t new_column_idx = columns.size();
53: 
54: 	types.push_back(new_column_type);
55: 	auto column_data = make_shared<ColumnData>(*storage.buffer_manager, *info);
56: 	column_data->type = new_column_type;
57: 	column_data->column_idx = new_column_idx;
58: 	columns.push_back(move(column_data));
59: 
60: 	// fill the column with its DEFAULT value, or NULL if none is specified
61: 	idx_t rows_to_write = persistent_manager->max_row + transient_manager->max_row;
62: 	if (rows_to_write > 0) {
63: 		ExpressionExecutor executor;
64: 		DataChunk dummy_chunk;
65: 		Vector result(new_column_type);
66: 		if (!default_value) {
67: 			FlatVector::Nullmask(result).set();
68: 		} else {
69: 			executor.AddExpression(*default_value);
70: 		}
71: 
72: 		ColumnAppendState state;
73: 		columns[new_column_idx]->InitializeAppend(state);
74: 		for (idx_t i = 0; i < rows_to_write; i += STANDARD_VECTOR_SIZE) {
75: 			idx_t rows_in_this_vector = std::min(rows_to_write - i, (idx_t)STANDARD_VECTOR_SIZE);
76: 			if (default_value) {
77: 				dummy_chunk.SetCardinality(rows_in_this_vector);
78: 				executor.ExecuteExpression(dummy_chunk, result);
79: 			}
80: 			columns[new_column_idx]->Append(state, result, rows_in_this_vector);
81: 		}
82: 	}
83: 	// also add this column to client local storage
84: 	Transaction::GetTransaction(context).storage.AddColumn(&parent, this, new_column, default_value);
85: 
86: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
87: 	parent.is_root = false;
88: }
89: 
90: DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t removed_column)
91:     : info(parent.info), types(parent.types), storage(parent.storage), persistent_manager(parent.persistent_manager),
92:       transient_manager(parent.transient_manager), columns(parent.columns), is_root(true) {
93: 	// prevent any new tuples from being added to the parent
94: 	lock_guard<mutex> parent_lock(parent.append_lock);
95: 	// first check if there are any indexes that exist that point to the removed column
96: 	for (auto &index : info->indexes) {
97: 		for (auto &column_id : index->column_ids) {
98: 			if (column_id == removed_column) {
99: 				throw CatalogException("Cannot drop this column: an index depends on it!");
100: 			} else if (column_id > removed_column) {
101: 				throw CatalogException("Cannot drop this column: an index depends on a column after it!");
102: 			}
103: 		}
104: 	}
105: 	// erase the column from this DataTable
106: 	assert(removed_column < types.size());
107: 	types.erase(types.begin() + removed_column);
108: 	columns.erase(columns.begin() + removed_column);
109: 
110: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
111: 	parent.is_root = false;
112: }
113: 
114: DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t changed_idx, SQLType target_type,
115:                      vector<column_t> bound_columns, Expression &cast_expr)
116:     : info(parent.info), types(parent.types), storage(parent.storage), persistent_manager(parent.persistent_manager),
117:       transient_manager(parent.transient_manager), columns(parent.columns), is_root(true) {
118: 
119: 	// prevent any new tuples from being added to the parent
120: 	CreateIndexScanState scan_state;
121: 	parent.InitializeCreateIndexScan(scan_state, bound_columns);
122: 
123: 	// first check if there are any indexes that exist that point to the changed column
124: 	for (auto &index : info->indexes) {
125: 		for (auto &column_id : index->column_ids) {
126: 			if (column_id == changed_idx) {
127: 				throw CatalogException("Cannot change the type of this column: an index depends on it!");
128: 			}
129: 		}
130: 	}
131: 	// change the type in this DataTable
132: 	auto new_type = GetInternalType(target_type);
133: 	types[changed_idx] = new_type;
134: 
135: 	// construct a new column data for this type
136: 	auto column_data = make_shared<ColumnData>(*storage.buffer_manager, *info);
137: 	column_data->type = new_type;
138: 	column_data->column_idx = changed_idx;
139: 
140: 	ColumnAppendState append_state;
141: 	column_data->InitializeAppend(append_state);
142: 
143: 	// scan the original table, and fill the new column with the transformed value
144: 	auto &transaction = Transaction::GetTransaction(context);
145: 
146: 	vector<TypeId> types;
147: 	for (idx_t i = 0; i < bound_columns.size(); i++) {
148: 		if (bound_columns[i] == COLUMN_IDENTIFIER_ROW_ID) {
149: 			types.push_back(ROW_TYPE);
150: 		} else {
151: 			types.push_back(parent.types[bound_columns[i]]);
152: 		}
153: 	}
154: 
155: 	DataChunk scan_chunk;
156: 	scan_chunk.Initialize(types);
157: 
158: 	ExpressionExecutor executor;
159: 	executor.AddExpression(cast_expr);
160: 
161: 	Vector append_vector(new_type);
162: 	while (true) {
163: 		// scan the table
164: 		scan_chunk.Reset();
165: 		parent.CreateIndexScan(scan_state, scan_chunk);
166: 		if (scan_chunk.size() == 0) {
167: 			break;
168: 		}
169: 		// execute the expression
170: 		executor.ExecuteExpression(scan_chunk, append_vector);
171: 		column_data->Append(append_state, append_vector, scan_chunk.size());
172: 	}
173: 	// also add this column to client local storage
174: 	transaction.storage.ChangeType(&parent, this, changed_idx, target_type, bound_columns, cast_expr);
175: 
176: 	columns[changed_idx] = move(column_data);
177: 
178: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
179: 	parent.is_root = false;
180: }
181: 
182: //===--------------------------------------------------------------------===//
183: // Scan
184: //===--------------------------------------------------------------------===//
185: void DataTable::InitializeScan(TableScanState &state, vector<column_t> column_ids,
186:                                unordered_map<idx_t, vector<TableFilter>> *table_filters) {
187: 	// initialize a column scan state for each column
188: 	state.column_scans = unique_ptr<ColumnScanState[]>(new ColumnScanState[column_ids.size()]);
189: 	for (idx_t i = 0; i < column_ids.size(); i++) {
190: 		auto column = column_ids[i];
191: 		if (column != COLUMN_IDENTIFIER_ROW_ID) {
192: 			columns[column]->InitializeScan(state.column_scans[i]);
193: 		}
194: 	}
195: 	state.column_ids = move(column_ids);
196: 	// initialize the chunk scan state
197: 	state.offset = 0;
198: 	state.current_persistent_row = 0;
199: 	state.max_persistent_row = persistent_manager->max_row;
200: 	state.current_transient_row = 0;
201: 	state.max_transient_row = transient_manager->max_row;
202: 	if (table_filters && table_filters->size() > 0) {
203: 		state.adaptive_filter = make_unique<AdaptiveFilter>(*table_filters);
204: 	}
205: }
206: 
207: void DataTable::InitializeScan(Transaction &transaction, TableScanState &state, vector<column_t> column_ids,
208:                                unordered_map<idx_t, vector<TableFilter>> *table_filters) {
209: 	InitializeScan(state, move(column_ids), table_filters);
210: 	transaction.storage.InitializeScan(this, state.local_state);
211: }
212: 
213: void DataTable::Scan(Transaction &transaction, DataChunk &result, TableScanState &state,
214:                      unordered_map<idx_t, vector<TableFilter>> &table_filters) {
215: 	// scan the persistent segments
216: 	while (ScanBaseTable(transaction, result, state, state.current_persistent_row, state.max_persistent_row, 0,
217: 	                     *persistent_manager, table_filters)) {
218: 		if (result.size() > 0) {
219: 			return;
220: 		}
221: 	}
222: 	// scan the transient segments
223: 	while (ScanBaseTable(transaction, result, state, state.current_transient_row, state.max_transient_row,
224: 	                     persistent_manager->max_row, *transient_manager, table_filters)) {
225: 		if (result.size() > 0) {
226: 			return;
227: 		}
228: 	}
229: 
230: 	// scan the transaction-local segments
231: 	transaction.storage.Scan(state.local_state, state.column_ids, result, &table_filters);
232: }
233: 
234: template <class T> bool checkZonemap(TableScanState &state, TableFilter &table_filter, T constant) {
235: 	T *min = (T *)state.column_scans[table_filter.column_index].current->stats.minimum.get();
236: 	T *max = (T *)state.column_scans[table_filter.column_index].current->stats.maximum.get();
237: 	switch (table_filter.comparison_type) {
238: 	case ExpressionType::COMPARE_EQUAL:
239: 		return constant >= *min && constant <= *max;
240: 	case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
241: 		return constant <= *max;
242: 	case ExpressionType::COMPARE_GREATERTHAN:
243: 		return constant < *max;
244: 	case ExpressionType::COMPARE_LESSTHANOREQUALTO:
245: 		return constant >= *min;
246: 	case ExpressionType::COMPARE_LESSTHAN:
247: 		return constant > *min;
248: 	default:
249: 		throw NotImplementedException("Operation not implemented");
250: 	}
251: }
252: 
253: bool checkZonemapString(TableScanState &state, TableFilter &table_filter, const char *constant) {
254: 	char *min = (char *)state.column_scans[table_filter.column_index].current->stats.minimum.get();
255: 	char *max = (char *)state.column_scans[table_filter.column_index].current->stats.maximum.get();
256: 	int min_comp = strcmp(min, constant);
257: 	int max_comp = strcmp(max, constant);
258: 	switch (table_filter.comparison_type) {
259: 	case ExpressionType::COMPARE_EQUAL:
260: 		return min_comp <= 0 && max_comp >= 0;
261: 	case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
262: 	case ExpressionType::COMPARE_GREATERTHAN:
263: 		return max_comp >= 0;
264: 	case ExpressionType::COMPARE_LESSTHAN:
265: 	case ExpressionType::COMPARE_LESSTHANOREQUALTO:
266: 		return min_comp <= 0;
267: 	default:
268: 		throw NotImplementedException("Operation not implemented");
269: 	}
270: }
271: 
272: bool DataTable::CheckZonemap(TableScanState &state, unordered_map<idx_t, vector<TableFilter>> &table_filters,
273:                              idx_t &current_row) {
274: 	bool readSegment = true;
275: 	for (auto &table_filter : table_filters) {
276: 		for (auto &predicate_constant : table_filter.second) {
277: 			if (!state.column_scans[predicate_constant.column_index].segment_checked) {
278: 				state.column_scans[predicate_constant.column_index].segment_checked = true;
279: 				if (!state.column_scans[predicate_constant.column_index].current) {
280: 					return true;
281: 				}
282: 				switch (state.column_scans[predicate_constant.column_index].current->type) {
283: 				case TypeId::INT8: {
284: 					int8_t constant = predicate_constant.constant.value_.tinyint;
285: 					readSegment &= checkZonemap<int8_t>(state, predicate_constant, constant);
286: 					break;
287: 				}
288: 				case TypeId::INT16: {
289: 					int16_t constant = predicate_constant.constant.value_.smallint;
290: 					readSegment &= checkZonemap<int16_t>(state, predicate_constant, constant);
291: 					break;
292: 				}
293: 				case TypeId::INT32: {
294: 					int32_t constant = predicate_constant.constant.value_.integer;
295: 					readSegment &= checkZonemap<int32_t>(state, predicate_constant, constant);
296: 					break;
297: 				}
298: 				case TypeId::INT64: {
299: 					int64_t constant = predicate_constant.constant.value_.bigint;
300: 					readSegment &= checkZonemap<int64_t>(state, predicate_constant, constant);
301: 					break;
302: 				}
303: 				case TypeId::FLOAT: {
304: 					float constant = predicate_constant.constant.value_.float_;
305: 					readSegment &= checkZonemap<float>(state, predicate_constant, constant);
306: 					break;
307: 				}
308: 				case TypeId::DOUBLE: {
309: 					double constant = predicate_constant.constant.value_.double_;
310: 					readSegment &= checkZonemap<double>(state, predicate_constant, constant);
311: 					break;
312: 				}
313: 				case TypeId::VARCHAR: {
314: 					//! we can only compare the first 7 bytes
315: 					size_t value_size = predicate_constant.constant.str_value.size() > 7
316: 					                        ? 7
317: 					                        : predicate_constant.constant.str_value.size();
318: 					string constant;
319: 					for (size_t i = 0; i < value_size; i++) {
320: 						constant += predicate_constant.constant.str_value[i];
321: 					}
322: 					readSegment &= checkZonemapString(state, predicate_constant, constant.c_str());
323: 					break;
324: 				}
325: 				default:
326: 					throw NotImplementedException("Unimplemented type for uncompressed segment");
327: 				}
328: 			}
329: 			if (!readSegment) {
330: 				//! We can skip this partition
331: 				idx_t vectorsToSkip =
332: 				    ceil((double)(state.column_scans[predicate_constant.column_index].current->count +
333: 				                  state.column_scans[predicate_constant.column_index].current->start - current_row) /
334: 				         STANDARD_VECTOR_SIZE);
335: 				for (idx_t i = 0; i < vectorsToSkip; ++i) {
336: 					state.NextVector();
337: 					current_row += STANDARD_VECTOR_SIZE;
338: 				}
339: 				return false;
340: 			}
341: 		}
342: 	}
343: 
344: 	return true;
345: }
346: 
347: bool DataTable::ScanBaseTable(Transaction &transaction, DataChunk &result, TableScanState &state, idx_t &current_row,
348:                               idx_t max_row, idx_t base_row, VersionManager &manager,
349:                               unordered_map<idx_t, vector<TableFilter>> &table_filters) {
350: 	if (current_row >= max_row) {
351: 		// exceeded the amount of rows to scan
352: 		return false;
353: 	}
354: 	idx_t max_count = std::min((idx_t)STANDARD_VECTOR_SIZE, max_row - current_row);
355: 	idx_t vector_offset = current_row / STANDARD_VECTOR_SIZE;
356: 	//! first check the zonemap if we have to scan this partition
357: 	if (!CheckZonemap(state, table_filters, current_row)) {
358: 		return true;
359: 	}
360: 	// second, scan the version chunk manager to figure out which tuples to load for this transaction
361: 	SelectionVector valid_sel(STANDARD_VECTOR_SIZE);
362: 	idx_t count = manager.GetSelVector(transaction, vector_offset, valid_sel, max_count);
363: 	if (count == 0) {
364: 		// nothing to scan for this vector, skip the entire vector
365: 		state.NextVector();
366: 		current_row += STANDARD_VECTOR_SIZE;
367: 		return true;
368: 	}
369: 	idx_t approved_tuple_count = count;
370: 	if (count == max_count && table_filters.empty()) {
371: 		//! If we don't have any deleted tuples or filters we can just run a regular scan
372: 		for (idx_t i = 0; i < state.column_ids.size(); i++) {
373: 			auto column = state.column_ids[i];
374: 			if (column == COLUMN_IDENTIFIER_ROW_ID) {
375: 				// scan row id
376: 				assert(result.data[i].type == ROW_TYPE);
377: 				result.data[i].Sequence(base_row + current_row, 1);
378: 			} else {
379: 				columns[column]->Scan(transaction, state.column_scans[i], result.data[i]);
380: 			}
381: 		}
382: 	} else {
383: 		SelectionVector sel;
384: 
385: 		if (count != max_count) {
386: 			sel.Initialize(valid_sel);
387: 		} else {
388: 			sel.Initialize(FlatVector::IncrementalSelectionVector);
389: 		}
390: 		//! First, we scan the columns with filters, fetch their data and generate a selection vector.
391: 		//! get runtime statistics
392: 		auto start_time = high_resolution_clock::now();
393: 		for (idx_t i = 0; i < table_filters.size(); i++) {
394: 			auto tf_idx = state.adaptive_filter->permutation[i];
395: 			columns[tf_idx]->Select(transaction, state.column_scans[tf_idx], result.data[tf_idx], sel,
396: 			                        approved_tuple_count, table_filters[tf_idx]);
397: 		}
398: 		for (auto &table_filter : table_filters) {
399: 			result.data[table_filter.first].Slice(sel, approved_tuple_count);
400: 		}
401: 		//! Now we use the selection vector to fetch data for the other columns.
402: 		for (idx_t i = 0; i < state.column_ids.size(); i++) {
403: 			if (table_filters.find(i) == table_filters.end()) {
404: 				auto column = state.column_ids[i];
405: 				if (column == COLUMN_IDENTIFIER_ROW_ID) {
406: 					assert(result.data[i].type == TypeId::INT64);
407: 					result.data[i].vector_type = VectorType::FLAT_VECTOR;
408: 					auto result_data = (int64_t *)FlatVector::GetData(result.data[i]);
409: 					for (size_t sel_idx = 0; sel_idx < approved_tuple_count; sel_idx++) {
410: 						result_data[sel_idx] = base_row + current_row + sel.get_index(sel_idx);
411: 					}
412: 				} else {
413: 					columns[column]->FilterScan(transaction, state.column_scans[i], result.data[i], sel,
414: 					                            approved_tuple_count);
415: 				}
416: 			}
417: 		}
418: 		auto end_time = high_resolution_clock::now();
419: 		if (state.adaptive_filter && table_filters.size() > 1) {
420: 			state.adaptive_filter->AdaptRuntimeStatistics(
421: 			    duration_cast<duration<double>>(end_time - start_time).count());
422: 		}
423: 	}
424: 
425: 	result.SetCardinality(approved_tuple_count);
426: 	current_row += STANDARD_VECTOR_SIZE;
427: 	return true;
428: }
429: 
430: //===--------------------------------------------------------------------===//
431: // Index Scan
432: //===--------------------------------------------------------------------===//
433: void DataTable::InitializeIndexScan(Transaction &transaction, TableIndexScanState &state, Index &index,
434:                                     vector<column_t> column_ids) {
435: 	state.index = &index;
436: 	state.column_ids = move(column_ids);
437: 	transaction.storage.InitializeScan(this, state.local_state);
438: }
439: 
440: void DataTable::InitializeIndexScan(Transaction &transaction, TableIndexScanState &state, Index &index, Value value,
441:                                     ExpressionType expr_type, vector<column_t> column_ids) {
442: 	InitializeIndexScan(transaction, state, index, move(column_ids));
443: 	state.index_state = index.InitializeScanSinglePredicate(transaction, state.column_ids, value, expr_type);
444: }
445: 
446: void DataTable::InitializeIndexScan(Transaction &transaction, TableIndexScanState &state, Index &index, Value low_value,
447:                                     ExpressionType low_type, Value high_value, ExpressionType high_type,
448:                                     vector<column_t> column_ids) {
449: 	InitializeIndexScan(transaction, state, index, move(column_ids));
450: 	state.index_state =
451: 	    index.InitializeScanTwoPredicates(transaction, state.column_ids, low_value, low_type, high_value, high_type);
452: }
453: 
454: void DataTable::IndexScan(Transaction &transaction, DataChunk &result, TableIndexScanState &state) {
455: 	// clear any previously pinned blocks
456: 	state.fetch_state.handles.clear();
457: 	// scan the index
458: 	state.index->Scan(transaction, *this, state, result);
459: 	if (result.size() > 0) {
460: 		return;
461: 	}
462: 	// scan the local structure
463: 	transaction.storage.Scan(state.local_state, state.column_ids, result);
464: }
465: 
466: //===--------------------------------------------------------------------===//
467: // Fetch
468: //===--------------------------------------------------------------------===//
469: void DataTable::Fetch(Transaction &transaction, DataChunk &result, vector<column_t> &column_ids,
470:                       Vector &row_identifiers, idx_t fetch_count, TableIndexScanState &state) {
471: 	// first figure out which row identifiers we should use for this transaction by looking at the VersionManagers
472: 	row_t rows[STANDARD_VECTOR_SIZE];
473: 	idx_t count = FetchRows(transaction, row_identifiers, fetch_count, rows);
474: 
475: 	if (count == 0) {
476: 		// no rows to use
477: 		return;
478: 	}
479: 	// for each of the remaining rows, now fetch the data
480: 	result.SetCardinality(count);
481: 	for (idx_t col_idx = 0; col_idx < column_ids.size(); col_idx++) {
482: 		auto column = column_ids[col_idx];
483: 		if (column == COLUMN_IDENTIFIER_ROW_ID) {
484: 			// row id column: fill in the row ids
485: 			assert(result.data[col_idx].type == TypeId::INT64);
486: 			result.data[col_idx].vector_type = VectorType::FLAT_VECTOR;
487: 			auto data = FlatVector::GetData<row_t>(result.data[col_idx]);
488: 			for (idx_t i = 0; i < count; i++) {
489: 				data[i] = rows[i];
490: 			}
491: 		} else {
492: 			// regular column: fetch data from the base column
493: 			for (idx_t i = 0; i < count; i++) {
494: 				auto row_id = rows[i];
495: 				columns[column]->FetchRow(state.fetch_state, transaction, row_id, result.data[col_idx], i);
496: 			}
497: 		}
498: 	}
499: }
500: 
501: idx_t DataTable::FetchRows(Transaction &transaction, Vector &row_identifiers, idx_t fetch_count, row_t result_rows[]) {
502: 	assert(row_identifiers.type == ROW_TYPE);
503: 
504: 	// obtain a read lock on the version managers
505: 	auto l1 = persistent_manager->lock.GetSharedLock();
506: 	auto l2 = transient_manager->lock.GetSharedLock();
507: 
508: 	// now iterate over the row ids and figure out which rows to use
509: 	idx_t count = 0;
510: 
511: 	auto row_ids = FlatVector::GetData<row_t>(row_identifiers);
512: 	for (idx_t i = 0; i < fetch_count; i++) {
513: 		auto row_id = row_ids[i];
514: 		bool use_row;
515: 		if ((idx_t)row_id < persistent_manager->max_row) {
516: 			// persistent row: use persistent manager
517: 			use_row = persistent_manager->Fetch(transaction, row_id);
518: 		} else {
519: 			// transient row: use transient manager
520: 			use_row = transient_manager->Fetch(transaction, row_id);
521: 		}
522: 		if (use_row) {
523: 			// row is not deleted; use the row
524: 			result_rows[count++] = row_id;
525: 		}
526: 	}
527: 	return count;
528: }
529: 
530: //===--------------------------------------------------------------------===//
531: // Append
532: //===--------------------------------------------------------------------===//
533: static void VerifyNotNullConstraint(TableCatalogEntry &table, Vector &vector, idx_t count, string &col_name) {
534: 	if (VectorOperations::HasNull(vector, count)) {
535: 		throw ConstraintException("NOT NULL constraint failed: %s.%s", table.name.c_str(), col_name.c_str());
536: 	}
537: }
538: 
539: static void VerifyCheckConstraint(TableCatalogEntry &table, Expression &expr, DataChunk &chunk) {
540: 	ExpressionExecutor executor(expr);
541: 	Vector result(TypeId::INT32);
542: 	try {
543: 		executor.ExecuteExpression(chunk, result);
544: 	} catch (Exception &ex) {
545: 		throw ConstraintException("CHECK constraint failed: %s (Error: %s)", table.name.c_str(), ex.what());
546: 	} catch (...) {
547: 		throw ConstraintException("CHECK constraint failed: %s (Unknown Error)", table.name.c_str());
548: 	}
549: 	VectorData vdata;
550: 	result.Orrify(chunk.size(), vdata);
551: 
552: 	auto dataptr = (int32_t *)vdata.data;
553: 	for (idx_t i = 0; i < chunk.size(); i++) {
554: 		auto idx = vdata.sel->get_index(i);
555: 		if (!(*vdata.nullmask)[idx] && dataptr[idx] == 0) {
556: 			throw ConstraintException("CHECK constraint failed: %s", table.name.c_str());
557: 		}
558: 	}
559: }
560: 
561: void DataTable::VerifyAppendConstraints(TableCatalogEntry &table, DataChunk &chunk) {
562: 	for (auto &constraint : table.bound_constraints) {
563: 		switch (constraint->type) {
564: 		case ConstraintType::NOT_NULL: {
565: 			auto &not_null = *reinterpret_cast<BoundNotNullConstraint *>(constraint.get());
566: 			VerifyNotNullConstraint(table, chunk.data[not_null.index], chunk.size(),
567: 			                        table.columns[not_null.index].name);
568: 			break;
569: 		}
570: 		case ConstraintType::CHECK: {
571: 			auto &check = *reinterpret_cast<BoundCheckConstraint *>(constraint.get());
572: 			VerifyCheckConstraint(table, *check.expression, chunk);
573: 			break;
574: 		}
575: 		case ConstraintType::UNIQUE: {
576: 			//! check whether or not the chunk can be inserted into the indexes
577: 			for (auto &index : info->indexes) {
578: 				index->VerifyAppend(chunk);
579: 			}
580: 			break;
581: 		}
582: 		case ConstraintType::FOREIGN_KEY:
583: 		default:
584: 			throw NotImplementedException("Constraint type not implemented!");
585: 		}
586: 	}
587: }
588: 
589: void DataTable::Append(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk) {
590: 	if (chunk.size() == 0) {
591: 		return;
592: 	}
593: 	if (chunk.column_count() != table.columns.size()) {
594: 		throw CatalogException("Mismatch in column count for append");
595: 	}
596: 	if (!is_root) {
597: 		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
598: 	}
599: 
600: 	chunk.Verify();
601: 
602: 	// verify any constraints on the new chunk
603: 	VerifyAppendConstraints(table, chunk);
604: 
605: 	// append to the transaction local data
606: 	auto &transaction = Transaction::GetTransaction(context);
607: 	transaction.storage.Append(this, chunk);
608: }
609: 
610: void DataTable::InitializeAppend(TableAppendState &state) {
611: 	// obtain the append lock for this table
612: 	state.append_lock = unique_lock<mutex>(append_lock);
613: 	if (!is_root) {
614: 		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
615: 	}
616: 	// obtain locks on all indexes for the table
617: 	state.index_locks = unique_ptr<IndexLock[]>(new IndexLock[info->indexes.size()]);
618: 	for (idx_t i = 0; i < info->indexes.size(); i++) {
619: 		info->indexes[i]->InitializeLock(state.index_locks[i]);
620: 	}
621: 	// for each column, initialize the append state
622: 	state.states = unique_ptr<ColumnAppendState[]>(new ColumnAppendState[types.size()]);
623: 	for (idx_t i = 0; i < types.size(); i++) {
624: 		columns[i]->InitializeAppend(state.states[i]);
625: 	}
626: 	state.row_start = transient_manager->max_row;
627: 	state.current_row = state.row_start;
628: }
629: 
630: void DataTable::Append(Transaction &transaction, transaction_t commit_id, DataChunk &chunk, TableAppendState &state) {
631: 	assert(is_root);
632: 	assert(chunk.column_count() == types.size());
633: 	chunk.Verify();
634: 
635: 	// set up the inserted info in the version manager
636: 	transient_manager->Append(transaction, state.current_row, chunk.size(), commit_id);
637: 
638: 	// append the physical data to each of the entries
639: 	for (idx_t i = 0; i < types.size(); i++) {
640: 		columns[i]->Append(state.states[i], chunk.data[i], chunk.size());
641: 	}
642: 	info->cardinality += chunk.size();
643: 	state.current_row += chunk.size();
644: }
645: 
646: void DataTable::RevertAppend(TableAppendState &state) {
647: 	if (state.row_start == state.current_row) {
648: 		// nothing to revert!
649: 		return;
650: 	}
651: 	assert(is_root);
652: 	// revert changes in the base columns
653: 	for (idx_t i = 0; i < types.size(); i++) {
654: 		columns[i]->RevertAppend(state.row_start);
655: 	}
656: 	// adjust the cardinality
657: 	info->cardinality -= state.current_row - state.row_start;
658: 	transient_manager->max_row = state.row_start;
659: 	// revert changes in the transient manager
660: 	transient_manager->RevertAppend(state.row_start, state.current_row);
661: }
662: 
663: //===--------------------------------------------------------------------===//
664: // Indexes
665: //===--------------------------------------------------------------------===//
666: bool DataTable::AppendToIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {
667: 	assert(is_root);
668: 	if (info->indexes.size() == 0) {
669: 		return true;
670: 	}
671: 	// first generate the vector of row identifiers
672: 	Vector row_identifiers(ROW_TYPE);
673: 	VectorOperations::GenerateSequence(row_identifiers, chunk.size(), row_start, 1);
674: 
675: 	idx_t failed_index = INVALID_INDEX;
676: 	// now append the entries to the indices
677: 	for (idx_t i = 0; i < info->indexes.size(); i++) {
678: 		if (!info->indexes[i]->Append(state.index_locks[i], chunk, row_identifiers)) {
679: 			failed_index = i;
680: 			break;
681: 		}
682: 	}
683: 	if (failed_index != INVALID_INDEX) {
684: 		// constraint violation!
685: 		// remove any appended entries from previous indexes (if any)
686: 		for (idx_t i = 0; i < failed_index; i++) {
687: 			info->indexes[i]->Delete(state.index_locks[i], chunk, row_identifiers);
688: 		}
689: 		return false;
690: 	}
691: 	return true;
692: }
693: 
694: void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {
695: 	assert(is_root);
696: 	if (info->indexes.size() == 0) {
697: 		return;
698: 	}
699: 	// first generate the vector of row identifiers
700: 	Vector row_identifiers(ROW_TYPE);
701: 	VectorOperations::GenerateSequence(row_identifiers, chunk.size(), row_start, 1);
702: 
703: 	// now remove the entries from the indices
704: 	RemoveFromIndexes(state, chunk, row_identifiers);
705: }
706: 
707: void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, Vector &row_identifiers) {
708: 	assert(is_root);
709: 	for (idx_t i = 0; i < info->indexes.size(); i++) {
710: 		info->indexes[i]->Delete(state.index_locks[i], chunk, row_identifiers);
711: 	}
712: }
713: 
714: void DataTable::RemoveFromIndexes(Vector &row_identifiers, idx_t count) {
715: 	assert(is_root);
716: 	auto row_ids = FlatVector::GetData<row_t>(row_identifiers);
717: 	// create a selection vector from the row_ids
718: 	SelectionVector sel(STANDARD_VECTOR_SIZE);
719: 	for (idx_t i = 0; i < count; i++) {
720: 		sel.set_index(i, row_ids[i] % STANDARD_VECTOR_SIZE);
721: 	}
722: 
723: 	// fetch the data for these row identifiers
724: 	DataChunk result;
725: 	result.Initialize(types);
726: 	// FIXME: we do not need to fetch all columns, only the columns required by the indices!
727: 	auto states = unique_ptr<ColumnScanState[]>(new ColumnScanState[types.size()]);
728: 	for (idx_t i = 0; i < types.size(); i++) {
729: 		columns[i]->Fetch(states[i], row_ids[0], result.data[i]);
730: 	}
731: 	result.Slice(sel, count);
732: 	for (idx_t i = 0; i < info->indexes.size(); i++) {
733: 		info->indexes[i]->Delete(result, row_identifiers);
734: 	}
735: }
736: 
737: //===--------------------------------------------------------------------===//
738: // Delete
739: //===--------------------------------------------------------------------===//
740: void DataTable::Delete(TableCatalogEntry &table, ClientContext &context, Vector &row_identifiers, idx_t count) {
741: 	assert(row_identifiers.type == ROW_TYPE);
742: 	if (count == 0) {
743: 		return;
744: 	}
745: 
746: 	auto &transaction = Transaction::GetTransaction(context);
747: 
748: 	row_identifiers.Normalify(count);
749: 	auto ids = FlatVector::GetData<row_t>(row_identifiers);
750: 	auto first_id = ids[0];
751: 
752: 	if (first_id >= MAX_ROW_ID) {
753: 		// deletion is in transaction-local storage: push delete into local chunk collection
754: 		transaction.storage.Delete(this, row_identifiers, count);
755: 	} else if ((idx_t)first_id < persistent_manager->max_row) {
756: 		// deletion is in persistent storage: delete in the persistent version manager
757: 		persistent_manager->Delete(transaction, this, row_identifiers, count);
758: 	} else {
759: 		// deletion is in transient storage: delete in the persistent version manager
760: 		transient_manager->Delete(transaction, this, row_identifiers, count);
761: 	}
762: }
763: 
764: //===--------------------------------------------------------------------===//
765: // Update
766: //===--------------------------------------------------------------------===//
767: static void CreateMockChunk(vector<TypeId> &types, vector<column_t> &column_ids, DataChunk &chunk,
768:                             DataChunk &mock_chunk) {
769: 	// construct a mock DataChunk
770: 	mock_chunk.InitializeEmpty(types);
771: 	for (column_t i = 0; i < column_ids.size(); i++) {
772: 		mock_chunk.data[column_ids[i]].Reference(chunk.data[i]);
773: 	}
774: 	mock_chunk.SetCardinality(chunk.size());
775: }
776: 
777: static bool CreateMockChunk(TableCatalogEntry &table, vector<column_t> &column_ids,
778:                             unordered_set<column_t> &desired_column_ids, DataChunk &chunk, DataChunk &mock_chunk) {
779: 	idx_t found_columns = 0;
780: 	// check whether the desired columns are present in the UPDATE clause
781: 	for (column_t i = 0; i < column_ids.size(); i++) {
782: 		if (desired_column_ids.find(column_ids[i]) != desired_column_ids.end()) {
783: 			found_columns++;
784: 		}
785: 	}
786: 	if (found_columns == 0) {
787: 		// no columns were found: no need to check the constraint again
788: 		return false;
789: 	}
790: 	if (found_columns != desired_column_ids.size()) {
791: 		// FIXME: not all columns in UPDATE clause are present!
792: 		// this should not be triggered at all as the binder should add these columns
793: 		throw NotImplementedException(
794: 		    "Not all columns required for the CHECK constraint are present in the UPDATED chunk!");
795: 	}
796: 	// construct a mock DataChunk
797: 	auto types = table.GetTypes();
798: 	CreateMockChunk(types, column_ids, chunk, mock_chunk);
799: 	return true;
800: }
801: 
802: void DataTable::VerifyUpdateConstraints(TableCatalogEntry &table, DataChunk &chunk, vector<column_t> &column_ids) {
803: 	for (auto &constraint : table.bound_constraints) {
804: 		switch (constraint->type) {
805: 		case ConstraintType::NOT_NULL: {
806: 			auto &not_null = *reinterpret_cast<BoundNotNullConstraint *>(constraint.get());
807: 			// check if the constraint is in the list of column_ids
808: 			for (idx_t i = 0; i < column_ids.size(); i++) {
809: 				if (column_ids[i] == not_null.index) {
810: 					// found the column id: check the data in
811: 					VerifyNotNullConstraint(table, chunk.data[i], chunk.size(), table.columns[not_null.index].name);
812: 					break;
813: 				}
814: 			}
815: 			break;
816: 		}
817: 		case ConstraintType::CHECK: {
818: 			auto &check = *reinterpret_cast<BoundCheckConstraint *>(constraint.get());
819: 
820: 			DataChunk mock_chunk;
821: 			if (CreateMockChunk(table, column_ids, check.bound_columns, chunk, mock_chunk)) {
822: 				VerifyCheckConstraint(table, *check.expression, mock_chunk);
823: 			}
824: 			break;
825: 		}
826: 		case ConstraintType::UNIQUE:
827: 		case ConstraintType::FOREIGN_KEY:
828: 			break;
829: 		default:
830: 			throw NotImplementedException("Constraint type not implemented!");
831: 		}
832: 	}
833: 	// update should not be called for indexed columns!
834: 	// instead update should have been rewritten to delete + update on higher layer
835: #ifdef DEBUG
836: 	for (idx_t i = 0; i < info->indexes.size(); i++) {
837: 		assert(!info->indexes[i]->IndexIsUpdated(column_ids));
838: 	}
839: #endif
840: }
841: 
842: void DataTable::Update(TableCatalogEntry &table, ClientContext &context, Vector &row_ids, vector<column_t> &column_ids,
843:                        DataChunk &updates) {
844: 	assert(row_ids.type == ROW_TYPE);
845: 
846: 	updates.Verify();
847: 	if (updates.size() == 0) {
848: 		return;
849: 	}
850: 
851: 	// first verify that no constraints are violated
852: 	VerifyUpdateConstraints(table, updates, column_ids);
853: 
854: 	// now perform the actual update
855: 	auto &transaction = Transaction::GetTransaction(context);
856: 
857: 	updates.Normalify();
858: 	row_ids.Normalify(updates.size());
859: 	auto first_id = FlatVector::GetValue<row_t>(row_ids, 0);
860: 	if (first_id >= MAX_ROW_ID) {
861: 		// update is in transaction-local storage: push update into local storage
862: 		transaction.storage.Update(this, row_ids, column_ids, updates);
863: 		return;
864: 	}
865: 
866: 	for (idx_t i = 0; i < column_ids.size(); i++) {
867: 		auto column = column_ids[i];
868: 		assert(column != COLUMN_IDENTIFIER_ROW_ID);
869: 
870: 		columns[column]->Update(transaction, updates.data[i], row_ids, updates.size());
871: 	}
872: }
873: 
874: //===--------------------------------------------------------------------===//
875: // Create Index Scan
876: //===--------------------------------------------------------------------===//
877: void DataTable::InitializeCreateIndexScan(CreateIndexScanState &state, vector<column_t> column_ids) {
878: 	// we grab the append lock to make sure nothing is appended until AFTER we finish the index scan
879: 	state.append_lock = unique_lock<mutex>(append_lock);
880: 	// get a read lock on the VersionManagers to prevent any further deletions
881: 	state.locks.push_back(persistent_manager->lock.GetSharedLock());
882: 	state.locks.push_back(transient_manager->lock.GetSharedLock());
883: 
884: 	InitializeScan(state, column_ids);
885: }
886: 
887: void DataTable::CreateIndexScan(CreateIndexScanState &state, DataChunk &result) {
888: 	// scan the persistent segments
889: 	if (ScanCreateIndex(state, result, state.current_persistent_row, state.max_persistent_row, 0)) {
890: 		return;
891: 	}
892: 	// scan the transient segments
893: 	if (ScanCreateIndex(state, result, state.current_transient_row, state.max_transient_row,
894: 	                    state.max_persistent_row)) {
895: 		return;
896: 	}
897: }
898: 
899: bool DataTable::ScanCreateIndex(CreateIndexScanState &state, DataChunk &result, idx_t &current_row, idx_t max_row,
900:                                 idx_t base_row) {
901: 	if (current_row >= max_row) {
902: 		return false;
903: 	}
904: 	idx_t count = std::min((idx_t)STANDARD_VECTOR_SIZE, max_row - current_row);
905: 
906: 	// scan the base columns to fetch the actual data
907: 	// note that we insert all data into the index, even if it is marked as deleted
908: 	// FIXME: tuples that are already "cleaned up" do not need to be inserted into the index!
909: 	for (idx_t i = 0; i < state.column_ids.size(); i++) {
910: 		auto column = state.column_ids[i];
911: 		if (column == COLUMN_IDENTIFIER_ROW_ID) {
912: 			// scan row id
913: 			assert(result.data[i].type == ROW_TYPE);
914: 			result.data[i].Sequence(base_row + current_row, 1);
915: 		} else {
916: 			// scan actual base column
917: 			columns[column]->IndexScan(state.column_scans[i], result.data[i]);
918: 		}
919: 	}
920: 	result.SetCardinality(count);
921: 
922: 	current_row += STANDARD_VECTOR_SIZE;
923: 	return count > 0;
924: }
925: 
926: void DataTable::AddIndex(unique_ptr<Index> index, vector<unique_ptr<Expression>> &expressions) {
927: 	DataChunk result;
928: 	result.Initialize(index->types);
929: 
930: 	DataChunk intermediate;
931: 	vector<TypeId> intermediate_types;
932: 	auto column_ids = index->column_ids;
933: 	column_ids.push_back(COLUMN_IDENTIFIER_ROW_ID);
934: 	for (auto &id : index->column_ids) {
935: 		intermediate_types.push_back(types[id]);
936: 	}
937: 	intermediate_types.push_back(ROW_TYPE);
938: 	intermediate.Initialize(intermediate_types);
939: 
940: 	// initialize an index scan
941: 	CreateIndexScanState state;
942: 	InitializeCreateIndexScan(state, column_ids);
943: 
944: 	if (!is_root) {
945: 		throw TransactionException("Transaction conflict: cannot add an index to a table that has been altered!");
946: 	}
947: 
948: 	// now start incrementally building the index
949: 	IndexLock lock;
950: 	index->InitializeLock(lock);
951: 	ExpressionExecutor executor(expressions);
952: 	while (true) {
953: 		intermediate.Reset();
954: 		// scan a new chunk from the table to index
955: 		CreateIndexScan(state, intermediate);
956: 		if (intermediate.size() == 0) {
957: 			// finished scanning for index creation
958: 			// release all locks
959: 			break;
960: 		}
961: 		// resolve the expressions for this chunk
962: 		executor.Execute(intermediate, result);
963: 
964: 		// insert into the index
965: 		if (!index->Insert(lock, result, intermediate.data[intermediate.column_count() - 1])) {
966: 			throw ConstraintException("Cant create unique index, table contains duplicate data on indexed column(s)");
967: 		}
968: 	}
969: 	info->indexes.push_back(move(index));
970: }
[end of src/storage/data_table.cpp]
[start of third_party/sqlsmith/duckdb.cc]
1: #include "duckdb.hh"
2: #include "dbgen.hpp"
3: 
4: #include <cassert>
5: #include <cstring>
6: #include <iostream>
7: #include <stdexcept>
8: #include <thread>
9: #include <chrono>
10: 
11: #include <regex>
12: 
13: using namespace duckdb;
14: using namespace std;
15: 
16: static regex e_syntax("Query Error: syntax error at or near .*");
17: 
18: duckdb_connection::duckdb_connection(string &conninfo) {
19: 	// in-memory database
20: 	database = make_unique<DuckDB>(nullptr);
21: 	connection = make_unique<Connection>(*database);
22: }
23: 
24: void duckdb_connection::q(const char *query) {
25: 	auto result = connection->Query(query);
26: 	if (!result->success) {
27: 		throw runtime_error(result->error);
28: 	}
29: }
30: 
31: schema_duckdb::schema_duckdb(std::string &conninfo, bool no_catalog) : duckdb_connection(conninfo) {
32: 	// generate empty TPC-H schema
33: 	tpch::dbgen(0, *database);
34: 
35: 	cerr << "Loading tables...";
36: 	auto result = connection->Query("SELECT * FROM sqlite_master() WHERE type IN ('table', 'view')");
37: 	if (!result->success) {
38: 		throw runtime_error(result->error);
39: 	}
40: 	for (size_t i = 0; i < result->collection.count; i++) {
41: 		auto type = result->collection.GetValue(0, i).str_value;
42: 		auto name = result->collection.GetValue(2, i).str_value;
43: 		bool view = type == "view";
44: 		table tab(name, "main", !view, !view);
45: 		tables.push_back(tab);
46: 	}
47: 	cerr << "done." << endl;
48: 
49: 	if (tables.size() == 0) {
50: 		throw std::runtime_error("No tables available in catalog!");
51: 	}
52: 
53: 	cerr << "Loading columns and constraints...";
54: 
55: 	for (auto t = tables.begin(); t != tables.end(); ++t) {
56: 		result = connection->Query("PRAGMA table_info('" + t->name + "')");
57: 		if (!result->success) {
58: 			throw runtime_error(result->error);
59: 		}
60: 		for (size_t i = 0; i < result->collection.count; i++) {
61: 			auto name = result->collection.GetValue(1, i).str_value;
62: 			auto type = result->collection.GetValue(2, i).str_value;
63: 			column c(name, sqltype::get(type));
64: 			t->columns().push_back(c);
65: 		}
66: 	}
67: 
68: 	cerr << "done." << endl;
69: 
70: #define BINOP(n, t)                                                                                                    \
71: 	do {                                                                                                               \
72: 		op o(#n, sqltype::get(#t), sqltype::get(#t), sqltype::get(#t));                                                \
73: 		register_operator(o);                                                                                          \
74: 	} while (0)
75: 
76: 	// BINOP(||, TEXT);
77: 	BINOP(*, INTEGER);
78: 	BINOP(/, INTEGER);
79: 
80: 	BINOP(+, INTEGER);
81: 	BINOP(-, INTEGER);
82: 
83: 	// BINOP(>>, INTEGER);
84: 	// BINOP(<<, INTEGER);
85: 
86: 	// BINOP(&, INTEGER);
87: 	// BINOP(|, INTEGER);
88: 
89: 	BINOP(<, INTEGER);
90: 	BINOP(<=, INTEGER);
91: 	BINOP(>, INTEGER);
92: 	BINOP(>=, INTEGER);
93: 
94: 	BINOP(=, INTEGER);
95: 	BINOP(<>, INTEGER);
96: 	BINOP(IS, INTEGER);
97: 	BINOP(IS NOT, INTEGER);
98: 
99: 	BINOP(AND, INTEGER);
100: 	BINOP(OR, INTEGER);
101: 
102: #define FUNC(n, r)                                                                                                     \
103: 	do {                                                                                                               \
104: 		routine proc("", "", sqltype::get(#r), #n);                                                                    \
105: 		register_routine(proc);                                                                                        \
106: 	} while (0)
107: 
108: #define FUNC1(n, r, a)                                                                                                 \
109: 	do {                                                                                                               \
110: 		routine proc("", "", sqltype::get(#r), #n);                                                                    \
111: 		proc.argtypes.push_back(sqltype::get(#a));                                                                     \
112: 		register_routine(proc);                                                                                        \
113: 	} while (0)
114: 
115: #define FUNC2(n, r, a, b)                                                                                              \
116: 	do {                                                                                                               \
117: 		routine proc("", "", sqltype::get(#r), #n);                                                                    \
118: 		proc.argtypes.push_back(sqltype::get(#a));                                                                     \
119: 		proc.argtypes.push_back(sqltype::get(#b));                                                                     \
120: 		register_routine(proc);                                                                                        \
121: 	} while (0)
122: 
123: #define FUNC3(n, r, a, b, c)                                                                                           \
124: 	do {                                                                                                               \
125: 		routine proc("", "", sqltype::get(#r), #n);                                                                    \
126: 		proc.argtypes.push_back(sqltype::get(#a));                                                                     \
127: 		proc.argtypes.push_back(sqltype::get(#b));                                                                     \
128: 		proc.argtypes.push_back(sqltype::get(#c));                                                                     \
129: 		register_routine(proc);                                                                                        \
130: 	} while (0)
131: 
132: 	// FUNC(last_insert_rowid, INTEGER);
133: 	// FUNC(random, INTEGER);
134: 	// FUNC(sqlite_source_id, TEXT);
135: 	// FUNC(sqlite_version, TEXT);
136: 	// FUNC(total_changes, INTEGER);
137: 
138: 	FUNC1(abs, INTEGER, REAL);
139: 	// FUNC1(hex, TEXT, TEXT);
140: 	// FUNC1(length, INTEGER, TEXT);
141: 	// FUNC1(lower, TEXT, TEXT);
142: 	// FUNC1(ltrim, TEXT, TEXT);
143: 	// FUNC1(quote, TEXT, TEXT);
144: 	// FUNC1(randomblob, TEXT, INTEGER);
145: 	// FUNC1(round, INTEGER, REAL);
146: 	// FUNC1(rtrim, TEXT, TEXT);
147: 	// FUNC1(soundex, TEXT, TEXT);
148: 	// FUNC1(sqlite_compileoption_get, TEXT, INTEGER);
149: 	// FUNC1(sqlite_compileoption_used, INTEGER, TEXT);
150: 	// FUNC1(trim, TEXT, TEXT);
151: 	// FUNC1(typeof, TEXT, INTEGER);
152: 	// FUNC1(typeof, TEXT, NUMERIC);
153: 	// FUNC1(typeof, TEXT, REAL);
154: 	// FUNC1(typeof, TEXT, TEXT);
155: 	// FUNC1(unicode, INTEGER, TEXT);
156: 	// FUNC1(upper, TEXT, TEXT);
157: 	// FUNC1(zeroblob, TEXT, INTEGER);
158: 
159: 	// FUNC2(glob, INTEGER, TEXT, TEXT);
160: 	// FUNC2(instr, INTEGER, TEXT, TEXT);
161: 	// FUNC2(like, INTEGER, TEXT, TEXT);
162: 	// FUNC2(ltrim, TEXT, TEXT, TEXT);
163: 	// FUNC2(rtrim, TEXT, TEXT, TEXT);
164: 	// FUNC2(trim, TEXT, TEXT, TEXT);
165: 	// FUNC2(round, INTEGER, REAL, INTEGER);
166: 	// FUNC2(substr, TEXT, TEXT, INTEGER);
167: 
168: 	// FUNC3(substr, TEXT, TEXT, INTEGER, INTEGER);
169: 	// FUNC3(replace, TEXT, TEXT, TEXT, TEXT);
170: 
171: #define AGG(n, r, a)                                                                                                   \
172: 	do {                                                                                                               \
173: 		routine proc("", "", sqltype::get(#r), #n);                                                                    \
174: 		proc.argtypes.push_back(sqltype::get(#a));                                                                     \
175: 		register_aggregate(proc);                                                                                      \
176: 	} while (0)
177: 
178: 	AGG(avg, INTEGER, INTEGER);
179: 	AGG(avg, REAL, REAL);
180: 	AGG(count, INTEGER, REAL);
181: 	AGG(count, INTEGER, TEXT);
182: 	AGG(count, INTEGER, INTEGER);
183: 	// AGG(group_concat, TEXT, TEXT);
184: 	AGG(max, REAL, REAL);
185: 	AGG(max, INTEGER, INTEGER);
186: 	AGG(min, REAL, REAL);
187: 	AGG(min, INTEGER, INTEGER);
188: 	AGG(sum, REAL, REAL);
189: 	AGG(sum, INTEGER, INTEGER);
190: 	// AGG(total, REAL, INTEGER);
191: 	// AGG(total, REAL, REAL);
192: 
193: 	booltype = sqltype::get("INTEGER");
194: 	inttype = sqltype::get("INTEGER");
195: 
196: 	internaltype = sqltype::get("internal");
197: 	arraytype = sqltype::get("ARRAY");
198: 
199: 	true_literal = "1";
200: 	false_literal = "0";
201: 
202: 	generate_indexes();
203: }
204: 
205: dut_duckdb::dut_duckdb(std::string &conninfo) : duckdb_connection(conninfo) {
206: 	cerr << "Generating TPC-H...";
207: 	tpch::dbgen(0.1, *database);
208: 	cerr << "done." << endl;
209: 	// q("PRAGMA main.auto_vacuum = 2");
210: }
211: 
212: volatile bool is_active = false;
213: // timeout is 10ms * TIMEOUT_TICKS
214: #define TIMEOUT_TICKS 50
215: 
216: void sleep_thread(Connection *connection) {
217: 	for (size_t i = 0; i < TIMEOUT_TICKS && is_active; i++) {
218: 		std::this_thread::sleep_for(std::chrono::milliseconds(10));
219: 	}
220: 	if (is_active) {
221: 		connection->Interrupt();
222: 	}
223: }
224: 
225: void dut_duckdb::test(const std::string &stmt) {
226: 	is_active = true;
227: 	thread interrupt_thread(sleep_thread, connection.get());
228: 	auto result = connection->Query(stmt);
229: 	is_active = false;
230: 	interrupt_thread.join();
231: 
232: 	if (!result->success) {
233: 		auto error = result->error.c_str();
234: 		try {
235: 			if (regex_match(error, e_syntax))
236: 				throw dut::syntax(error);
237: 			else
238: 				throw dut::failure(error);
239: 		} catch (dut::failure &e) {
240: 			throw;
241: 		}
242: 	}
243: }
[end of third_party/sqlsmith/duckdb.cc]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: