You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
`INTERNAL Error: Unimplemented type for normalify` in select query
### What happens?

An internal error is thrown in a select query.

Test case generated with SQLancer. 


### To Reproduce

```sql
CREATE TABLE t0(c0 BOOLEAN, PRIMARY KEY(c0));
CREATE TABLE t63(c0 VARCHAR COLLATE C, PRIMARY KEY(c0));
insert into t0(c0) values (0.7);
insert into t63(c0) values ('1');
SELECT t63.c0 FROM t0 NATURAL LEFT JOIN t63;
```

### OS:

Arch Linux x86_64

### DuckDB Version:

v1.1.0 fa5c2fe15f

### DuckDB Client:

duckdb cli

### Hardware:

_No response_

### Full Name:

Ming Wei Tan

### Affiliation:

National University of Singapore

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have
`INTERNAL Error: Unimplemented type for normalify` in select query
### What happens?

An internal error is thrown in a select query.

Test case generated with SQLancer. 


### To Reproduce

```sql
CREATE TABLE t0(c0 BOOLEAN, PRIMARY KEY(c0));
CREATE TABLE t63(c0 VARCHAR COLLATE C, PRIMARY KEY(c0));
insert into t0(c0) values (0.7);
insert into t63(c0) values ('1');
SELECT t63.c0 FROM t0 NATURAL LEFT JOIN t63;
```

### OS:

Arch Linux x86_64

### DuckDB Version:

v1.1.0 fa5c2fe15f

### DuckDB Client:

duckdb cli

### Hardware:

_No response_

### Full Name:

Ming Wei Tan

### Affiliation:

National University of Singapore

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://duckdb.org/docs/installation) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of src/execution/operator/aggregate/physical_ungrouped_aggregate.cpp]
1: #include "duckdb/execution/operator/aggregate/physical_ungrouped_aggregate.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/aggregate_function_catalog_entry.hpp"
4: #include "duckdb/common/algorithm.hpp"
5: #include "duckdb/common/unordered_set.hpp"
6: #include "duckdb/common/vector_operations/vector_operations.hpp"
7: #include "duckdb/execution/expression_executor.hpp"
8: #include "duckdb/execution/operator/aggregate/aggregate_object.hpp"
9: #include "duckdb/execution/operator/aggregate/distinct_aggregate_data.hpp"
10: #include "duckdb/execution/radix_partitioned_hashtable.hpp"
11: #include "duckdb/main/client_context.hpp"
12: #include "duckdb/parallel/base_pipeline_event.hpp"
13: #include "duckdb/parallel/interrupt.hpp"
14: #include "duckdb/parallel/thread_context.hpp"
15: #include "duckdb/parallel/executor_task.hpp"
16: #include "duckdb/planner/expression/bound_aggregate_expression.hpp"
17: #include "duckdb/planner/expression/bound_reference_expression.hpp"
18: #include "duckdb/execution/operator/aggregate/ungrouped_aggregate_state.hpp"
19: 
20: #include <functional>
21: 
22: namespace duckdb {
23: 
24: PhysicalUngroupedAggregate::PhysicalUngroupedAggregate(vector<LogicalType> types,
25:                                                        vector<unique_ptr<Expression>> expressions,
26:                                                        idx_t estimated_cardinality)
27:     : PhysicalOperator(PhysicalOperatorType::UNGROUPED_AGGREGATE, std::move(types), estimated_cardinality),
28:       aggregates(std::move(expressions)) {
29: 
30: 	distinct_collection_info = DistinctAggregateCollectionInfo::Create(aggregates);
31: 	if (!distinct_collection_info) {
32: 		return;
33: 	}
34: 	distinct_data = make_uniq<DistinctAggregateData>(*distinct_collection_info);
35: }
36: 
37: //===--------------------------------------------------------------------===//
38: // Ungrouped Aggregate State
39: //===--------------------------------------------------------------------===//
40: UngroupedAggregateState::UngroupedAggregateState(const vector<unique_ptr<Expression>> &aggregate_expressions)
41:     : aggregate_expressions(aggregate_expressions) {
42: 	counts = make_uniq_array<atomic<idx_t>>(aggregate_expressions.size());
43: 	for (idx_t i = 0; i < aggregate_expressions.size(); i++) {
44: 		auto &aggregate = aggregate_expressions[i];
45: 		D_ASSERT(aggregate->GetExpressionClass() == ExpressionClass::BOUND_AGGREGATE);
46: 		auto &aggr = aggregate->Cast<BoundAggregateExpression>();
47: 		auto state = make_unsafe_uniq_array_uninitialized<data_t>(aggr.function.state_size(aggr.function));
48: 		aggr.function.initialize(aggr.function, state.get());
49: 		aggregate_data.push_back(std::move(state));
50: 		bind_data.push_back(aggr.bind_info.get());
51: 		destructors.push_back(aggr.function.destructor);
52: #ifdef DEBUG
53: 		counts[i] = 0;
54: #endif
55: 	}
56: }
57: UngroupedAggregateState::~UngroupedAggregateState() {
58: 	D_ASSERT(destructors.size() == aggregate_data.size());
59: 	for (idx_t i = 0; i < destructors.size(); i++) {
60: 		if (!destructors[i]) {
61: 			continue;
62: 		}
63: 		Vector state_vector(Value::POINTER(CastPointerToValue(aggregate_data[i].get())));
64: 		state_vector.SetVectorType(VectorType::FLAT_VECTOR);
65: 
66: 		ArenaAllocator allocator(Allocator::DefaultAllocator());
67: 		AggregateInputData aggr_input_data(bind_data[i], allocator);
68: 		destructors[i](state_vector, aggr_input_data, 1);
69: 	}
70: }
71: 
72: void UngroupedAggregateState::Move(UngroupedAggregateState &other) {
73: 	other.aggregate_data = std::move(aggregate_data);
74: 	other.destructors = std::move(destructors);
75: }
76: 
77: //===--------------------------------------------------------------------===//
78: // Global State
79: //===--------------------------------------------------------------------===//
80: class UngroupedAggregateGlobalSinkState : public GlobalSinkState {
81: public:
82: 	UngroupedAggregateGlobalSinkState(const PhysicalUngroupedAggregate &op, ClientContext &client)
83: 	    : state(BufferAllocator::Get(client), op.aggregates), finished(false) {
84: 		if (op.distinct_data) {
85: 			distinct_state = make_uniq<DistinctAggregateState>(*op.distinct_data, client);
86: 		}
87: 	}
88: 
89: 	//! The global aggregate state
90: 	GlobalUngroupedAggregateState state;
91: 	//! Whether or not the aggregate is finished
92: 	bool finished;
93: 	//! The data related to the distinct aggregates (if there are any)
94: 	unique_ptr<DistinctAggregateState> distinct_state;
95: };
96: 
97: ArenaAllocator &GlobalUngroupedAggregateState::CreateAllocator() const {
98: 	lock_guard<mutex> glock(lock);
99: 	stored_allocators.emplace_back(make_uniq<ArenaAllocator>(client_allocator));
100: 	return *stored_allocators.back();
101: }
102: 
103: void GlobalUngroupedAggregateState::Combine(LocalUngroupedAggregateState &other) {
104: 	lock_guard<mutex> glock(lock);
105: 	for (idx_t aggr_idx = 0; aggr_idx < state.aggregate_expressions.size(); aggr_idx++) {
106: 		auto &aggregate = state.aggregate_expressions[aggr_idx]->Cast<BoundAggregateExpression>();
107: 
108: 		if (aggregate.IsDistinct()) {
109: 			continue;
110: 		}
111: 
112: 		Vector source_state(Value::POINTER(CastPointerToValue(other.state.aggregate_data[aggr_idx].get())));
113: 		Vector dest_state(Value::POINTER(CastPointerToValue(state.aggregate_data[aggr_idx].get())));
114: 
115: 		AggregateInputData aggr_input_data(aggregate.bind_info.get(), allocator,
116: 		                                   AggregateCombineType::ALLOW_DESTRUCTIVE);
117: 		aggregate.function.combine(source_state, dest_state, aggr_input_data, 1);
118: #ifdef DEBUG
119: 		state.counts[aggr_idx] += other.state.counts[aggr_idx];
120: #endif
121: 	}
122: }
123: 
124: void GlobalUngroupedAggregateState::CombineDistinct(LocalUngroupedAggregateState &other,
125:                                                     DistinctAggregateData &distinct_data) {
126: 	lock_guard<mutex> glock(lock);
127: 	for (idx_t aggr_idx = 0; aggr_idx < state.aggregate_expressions.size(); aggr_idx++) {
128: 		if (!distinct_data.IsDistinct(aggr_idx)) {
129: 			continue;
130: 		}
131: 
132: 		auto &aggregate = state.aggregate_expressions[aggr_idx]->Cast<BoundAggregateExpression>();
133: 		AggregateInputData aggr_input_data(aggregate.bind_info.get(), allocator,
134: 		                                   AggregateCombineType::ALLOW_DESTRUCTIVE);
135: 
136: 		Vector state_vec(Value::POINTER(CastPointerToValue(other.state.aggregate_data[aggr_idx].get())));
137: 		Vector combined_vec(Value::POINTER(CastPointerToValue(state.aggregate_data[aggr_idx].get())));
138: 		aggregate.function.combine(state_vec, combined_vec, aggr_input_data, 1);
139: #ifdef DEBUG
140: 		state.counts[aggr_idx] += other.state.counts[aggr_idx];
141: #endif
142: 	}
143: }
144: 
145: //===--------------------------------------------------------------------===//
146: // Local State
147: //===--------------------------------------------------------------------===//
148: LocalUngroupedAggregateState::LocalUngroupedAggregateState(GlobalUngroupedAggregateState &gstate)
149:     : allocator(gstate.CreateAllocator()), state(gstate.state.aggregate_expressions) {
150: }
151: 
152: class UngroupedAggregateLocalSinkState : public LocalSinkState {
153: public:
154: 	UngroupedAggregateLocalSinkState(const PhysicalUngroupedAggregate &op, const vector<LogicalType> &child_types,
155: 	                                 UngroupedAggregateGlobalSinkState &gstate_p, ExecutionContext &context)
156: 	    : state(gstate_p.state), child_executor(context.client), aggregate_input_chunk(), filter_set() {
157: 		auto &gstate = gstate_p.Cast<UngroupedAggregateGlobalSinkState>();
158: 
159: 		auto &allocator = BufferAllocator::Get(context.client);
160: 		InitializeDistinctAggregates(op, gstate, context);
161: 
162: 		vector<LogicalType> payload_types;
163: 		vector<AggregateObject> aggregate_objects;
164: 		for (auto &aggregate : op.aggregates) {
165: 			D_ASSERT(aggregate->GetExpressionClass() == ExpressionClass::BOUND_AGGREGATE);
166: 			auto &aggr = aggregate->Cast<BoundAggregateExpression>();
167: 			// initialize the payload chunk
168: 			for (auto &child : aggr.children) {
169: 				payload_types.push_back(child->return_type);
170: 				child_executor.AddExpression(*child);
171: 			}
172: 			aggregate_objects.emplace_back(&aggr);
173: 		}
174: 		if (!payload_types.empty()) { // for select count(*) from t; there is no payload at all
175: 			aggregate_input_chunk.Initialize(allocator, payload_types);
176: 		}
177: 		filter_set.Initialize(context.client, aggregate_objects, child_types);
178: 	}
179: 
180: 	//! The local aggregate state
181: 	LocalUngroupedAggregateState state;
182: 	//! The executor
183: 	ExpressionExecutor child_executor;
184: 	//! The payload chunk, containing all the Vectors for the aggregates
185: 	DataChunk aggregate_input_chunk;
186: 	//! Aggregate filter data set
187: 	AggregateFilterDataSet filter_set;
188: 	//! The local sink states of the distinct aggregates hash tables
189: 	vector<unique_ptr<LocalSinkState>> radix_states;
190: 
191: public:
192: 	void Reset() {
193: 		aggregate_input_chunk.Reset();
194: 	}
195: 	void InitializeDistinctAggregates(const PhysicalUngroupedAggregate &op,
196: 	                                  const UngroupedAggregateGlobalSinkState &gstate, ExecutionContext &context) {
197: 
198: 		if (!op.distinct_data) {
199: 			return;
200: 		}
201: 		auto &data = *op.distinct_data;
202: 		auto &state = *gstate.distinct_state;
203: 		D_ASSERT(!data.radix_tables.empty());
204: 
205: 		const idx_t aggregate_count = state.radix_states.size();
206: 		radix_states.resize(aggregate_count);
207: 
208: 		auto &distinct_info = *op.distinct_collection_info;
209: 
210: 		for (auto &idx : distinct_info.indices) {
211: 			idx_t table_idx = distinct_info.table_map[idx];
212: 			if (data.radix_tables[table_idx] == nullptr) {
213: 				// This aggregate has identical input as another aggregate, so no table is created for it
214: 				continue;
215: 			}
216: 			auto &radix_table = *data.radix_tables[table_idx];
217: 			radix_states[table_idx] = radix_table.GetLocalSinkState(context);
218: 		}
219: 	}
220: };
221: 
222: //===--------------------------------------------------------------------===//
223: // Sink
224: //===--------------------------------------------------------------------===//
225: bool PhysicalUngroupedAggregate::SinkOrderDependent() const {
226: 	for (auto &expr : aggregates) {
227: 		auto &aggr = expr->Cast<BoundAggregateExpression>();
228: 		if (aggr.function.order_dependent == AggregateOrderDependent::ORDER_DEPENDENT) {
229: 			return true;
230: 		}
231: 	}
232: 	return false;
233: }
234: 
235: unique_ptr<GlobalSinkState> PhysicalUngroupedAggregate::GetGlobalSinkState(ClientContext &context) const {
236: 	return make_uniq<UngroupedAggregateGlobalSinkState>(*this, context);
237: }
238: 
239: unique_ptr<LocalSinkState> PhysicalUngroupedAggregate::GetLocalSinkState(ExecutionContext &context) const {
240: 	D_ASSERT(sink_state);
241: 	auto &gstate = sink_state->Cast<UngroupedAggregateGlobalSinkState>();
242: 	return make_uniq<UngroupedAggregateLocalSinkState>(*this, children[0]->GetTypes(), gstate, context);
243: }
244: 
245: void PhysicalUngroupedAggregate::SinkDistinct(ExecutionContext &context, DataChunk &chunk,
246:                                               OperatorSinkInput &input) const {
247: 	auto &sink = input.local_state.Cast<UngroupedAggregateLocalSinkState>();
248: 	auto &global_sink = input.global_state.Cast<UngroupedAggregateGlobalSinkState>();
249: 	D_ASSERT(distinct_data);
250: 	auto &distinct_state = *global_sink.distinct_state;
251: 	auto &distinct_info = *distinct_collection_info;
252: 	auto &distinct_indices = distinct_info.Indices();
253: 
254: 	DataChunk empty_chunk;
255: 
256: 	auto &distinct_filter = distinct_info.Indices();
257: 
258: 	for (auto &idx : distinct_indices) {
259: 		auto &aggregate = aggregates[idx]->Cast<BoundAggregateExpression>();
260: 
261: 		idx_t table_idx = distinct_info.table_map[idx];
262: 		if (!distinct_data->radix_tables[table_idx]) {
263: 			// This distinct aggregate shares its data with another
264: 			continue;
265: 		}
266: 		D_ASSERT(distinct_data->radix_tables[table_idx]);
267: 		auto &radix_table = *distinct_data->radix_tables[table_idx];
268: 		auto &radix_global_sink = *distinct_state.radix_states[table_idx];
269: 		auto &radix_local_sink = *sink.radix_states[table_idx];
270: 		OperatorSinkInput sink_input {radix_global_sink, radix_local_sink, input.interrupt_state};
271: 
272: 		if (aggregate.filter) {
273: 			// The hashtable can apply a filter, but only on the payload
274: 			// And in our case, we need to filter the groups (the distinct aggr children)
275: 
276: 			// Apply the filter before inserting into the hashtable
277: 			auto &filtered_data = sink.filter_set.GetFilterData(idx);
278: 			idx_t count = filtered_data.ApplyFilter(chunk);
279: 			filtered_data.filtered_payload.SetCardinality(count);
280: 
281: 			radix_table.Sink(context, filtered_data.filtered_payload, sink_input, empty_chunk, distinct_filter);
282: 		} else {
283: 			radix_table.Sink(context, chunk, sink_input, empty_chunk, distinct_filter);
284: 		}
285: 	}
286: }
287: 
288: SinkResultType PhysicalUngroupedAggregate::Sink(ExecutionContext &context, DataChunk &chunk,
289:                                                 OperatorSinkInput &input) const {
290: 	auto &sink = input.local_state.Cast<UngroupedAggregateLocalSinkState>();
291: 
292: 	// perform the aggregation inside the local state
293: 	sink.Reset();
294: 
295: 	if (distinct_data) {
296: 		SinkDistinct(context, chunk, input);
297: 	}
298: 
299: 	DataChunk &payload_chunk = sink.aggregate_input_chunk;
300: 
301: 	idx_t payload_idx = 0;
302: 	idx_t next_payload_idx = 0;
303: 
304: 	for (idx_t aggr_idx = 0; aggr_idx < aggregates.size(); aggr_idx++) {
305: 		auto &aggregate = aggregates[aggr_idx]->Cast<BoundAggregateExpression>();
306: 
307: 		payload_idx = next_payload_idx;
308: 		next_payload_idx = payload_idx + aggregate.children.size();
309: 
310: 		if (aggregate.IsDistinct()) {
311: 			continue;
312: 		}
313: 
314: 		idx_t payload_cnt = 0;
315: 		// resolve the filter (if any)
316: 		if (aggregate.filter) {
317: 			auto &filtered_data = sink.filter_set.GetFilterData(aggr_idx);
318: 			auto count = filtered_data.ApplyFilter(chunk);
319: 
320: 			sink.child_executor.SetChunk(filtered_data.filtered_payload);
321: 			payload_chunk.SetCardinality(count);
322: 		} else {
323: 			sink.child_executor.SetChunk(chunk);
324: 			payload_chunk.SetCardinality(chunk);
325: 		}
326: 
327: 		// resolve the child expressions of the aggregate (if any)
328: 		for (idx_t i = 0; i < aggregate.children.size(); ++i) {
329: 			sink.child_executor.ExecuteExpression(payload_idx + payload_cnt,
330: 			                                      payload_chunk.data[payload_idx + payload_cnt]);
331: 			payload_cnt++;
332: 		}
333: 
334: 		sink.state.Sink(payload_chunk, payload_idx, aggr_idx);
335: 	}
336: 	return SinkResultType::NEED_MORE_INPUT;
337: }
338: 
339: void LocalUngroupedAggregateState::Sink(DataChunk &payload_chunk, idx_t payload_idx, idx_t aggr_idx) {
340: #ifdef DEBUG
341: 	state.counts[aggr_idx] += payload_chunk.size();
342: #endif
343: 	auto &aggregate = state.aggregate_expressions[aggr_idx]->Cast<BoundAggregateExpression>();
344: 	idx_t payload_cnt = aggregate.children.size();
345: 	auto start_of_input = payload_cnt == 0 ? nullptr : &payload_chunk.data[payload_idx];
346: 	AggregateInputData aggr_input_data(state.bind_data[aggr_idx], allocator);
347: 	aggregate.function.simple_update(start_of_input, aggr_input_data, payload_cnt, state.aggregate_data[aggr_idx].get(),
348: 	                                 payload_chunk.size());
349: }
350: 
351: //===--------------------------------------------------------------------===//
352: // Combine
353: //===--------------------------------------------------------------------===//
354: void PhysicalUngroupedAggregate::CombineDistinct(ExecutionContext &context, OperatorSinkCombineInput &input) const {
355: 	auto &gstate = input.global_state.Cast<UngroupedAggregateGlobalSinkState>();
356: 	auto &lstate = input.local_state.Cast<UngroupedAggregateLocalSinkState>();
357: 
358: 	if (!distinct_data) {
359: 		return;
360: 	}
361: 	auto &distinct_state = gstate.distinct_state;
362: 	auto table_count = distinct_data->radix_tables.size();
363: 	for (idx_t table_idx = 0; table_idx < table_count; table_idx++) {
364: 		D_ASSERT(distinct_data->radix_tables[table_idx]);
365: 		auto &radix_table = *distinct_data->radix_tables[table_idx];
366: 		auto &radix_global_sink = *distinct_state->radix_states[table_idx];
367: 		auto &radix_local_sink = *lstate.radix_states[table_idx];
368: 
369: 		radix_table.Combine(context, radix_global_sink, radix_local_sink);
370: 	}
371: }
372: 
373: SinkCombineResultType PhysicalUngroupedAggregate::Combine(ExecutionContext &context,
374:                                                           OperatorSinkCombineInput &input) const {
375: 	auto &gstate = input.global_state.Cast<UngroupedAggregateGlobalSinkState>();
376: 	auto &lstate = input.local_state.Cast<UngroupedAggregateLocalSinkState>();
377: 	D_ASSERT(!gstate.finished);
378: 
379: 	// finalize: combine the local state into the global state
380: 	// all aggregates are combinable: we might be doing a parallel aggregate
381: 	// use the combine method to combine the partial aggregates
382: 	OperatorSinkCombineInput distinct_input {gstate, lstate, input.interrupt_state};
383: 	CombineDistinct(context, distinct_input);
384: 
385: 	gstate.state.Combine(lstate.state);
386: 
387: 	auto &client_profiler = QueryProfiler::Get(context.client);
388: 	context.thread.profiler.Flush(*this);
389: 	client_profiler.Flush(context.thread.profiler);
390: 
391: 	return SinkCombineResultType::FINISHED;
392: }
393: 
394: //===--------------------------------------------------------------------===//
395: // Finalize
396: //===--------------------------------------------------------------------===//
397: class UngroupedDistinctAggregateFinalizeEvent : public BasePipelineEvent {
398: public:
399: 	UngroupedDistinctAggregateFinalizeEvent(ClientContext &context, const PhysicalUngroupedAggregate &op_p,
400: 	                                        UngroupedAggregateGlobalSinkState &gstate_p, Pipeline &pipeline_p)
401: 	    : BasePipelineEvent(pipeline_p), context(context), op(op_p), gstate(gstate_p), tasks_scheduled(0),
402: 	      tasks_done(0) {
403: 	}
404: 
405: public:
406: 	void Schedule() override;
407: 	void FinalizeTask() {
408: 		lock_guard<mutex> finalize(lock);
409: 		D_ASSERT(!gstate.finished);
410: 		D_ASSERT(tasks_done < tasks_scheduled);
411: 		if (++tasks_done == tasks_scheduled) {
412: 			gstate.finished = true;
413: 		}
414: 	}
415: 
416: private:
417: 	ClientContext &context;
418: 
419: 	const PhysicalUngroupedAggregate &op;
420: 	UngroupedAggregateGlobalSinkState &gstate;
421: 
422: 	mutex lock;
423: 	idx_t tasks_scheduled;
424: 	idx_t tasks_done;
425: 
426: public:
427: 	vector<unique_ptr<GlobalSourceState>> global_source_states;
428: };
429: 
430: class UngroupedDistinctAggregateFinalizeTask : public ExecutorTask {
431: public:
432: 	UngroupedDistinctAggregateFinalizeTask(Executor &executor, shared_ptr<Event> event_p,
433: 	                                       const PhysicalUngroupedAggregate &op,
434: 	                                       UngroupedAggregateGlobalSinkState &state_p)
435: 	    : ExecutorTask(executor, std::move(event_p)), op(op), gstate(state_p), aggregate_state(gstate.state) {
436: 	}
437: 
438: 	TaskExecutionResult ExecuteTask(TaskExecutionMode mode) override;
439: 
440: private:
441: 	TaskExecutionResult AggregateDistinct();
442: 
443: private:
444: 	const PhysicalUngroupedAggregate &op;
445: 	UngroupedAggregateGlobalSinkState &gstate;
446: 
447: 	// Distinct aggregation state
448: 	LocalUngroupedAggregateState aggregate_state;
449: 	idx_t aggregation_idx = 0;
450: 	unique_ptr<LocalSourceState> radix_table_lstate;
451: 	bool blocked = false;
452: };
453: 
454: void UngroupedDistinctAggregateFinalizeEvent::Schedule() {
455: 	D_ASSERT(gstate.distinct_state);
456: 	auto &aggregates = op.aggregates;
457: 	auto &distinct_data = *op.distinct_data;
458: 
459: 	idx_t n_tasks = 0;
460: 	idx_t payload_idx = 0;
461: 	idx_t next_payload_idx = 0;
462: 	for (idx_t agg_idx = 0; agg_idx < aggregates.size(); agg_idx++) {
463: 		auto &aggregate = aggregates[agg_idx]->Cast<BoundAggregateExpression>();
464: 
465: 		// Forward the payload idx
466: 		payload_idx = next_payload_idx;
467: 		next_payload_idx = payload_idx + aggregate.children.size();
468: 
469: 		// If aggregate is not distinct, skip it
470: 		if (!distinct_data.IsDistinct(agg_idx)) {
471: 			global_source_states.push_back(nullptr);
472: 			continue;
473: 		}
474: 		D_ASSERT(distinct_data.info.table_map.count(agg_idx));
475: 
476: 		// Create global state for scanning
477: 		auto table_idx = distinct_data.info.table_map.at(agg_idx);
478: 		auto &radix_table_p = *distinct_data.radix_tables[table_idx];
479: 		n_tasks += radix_table_p.MaxThreads(*gstate.distinct_state->radix_states[table_idx]);
480: 		global_source_states.push_back(radix_table_p.GetGlobalSourceState(context));
481: 	}
482: 	n_tasks = MaxValue<idx_t>(n_tasks, 1);
483: 	n_tasks = MinValue<idx_t>(n_tasks, NumericCast<idx_t>(TaskScheduler::GetScheduler(context).NumberOfThreads()));
484: 
485: 	vector<shared_ptr<Task>> tasks;
486: 	for (idx_t i = 0; i < n_tasks; i++) {
487: 		tasks.push_back(
488: 		    make_uniq<UngroupedDistinctAggregateFinalizeTask>(pipeline->executor, shared_from_this(), op, gstate));
489: 		tasks_scheduled++;
490: 	}
491: 	SetTasks(std::move(tasks));
492: }
493: 
494: TaskExecutionResult UngroupedDistinctAggregateFinalizeTask::ExecuteTask(TaskExecutionMode mode) {
495: 	auto res = AggregateDistinct();
496: 	if (res == TaskExecutionResult::TASK_BLOCKED) {
497: 		return res;
498: 	}
499: 	event->FinishTask();
500: 	return TaskExecutionResult::TASK_FINISHED;
501: }
502: 
503: TaskExecutionResult UngroupedDistinctAggregateFinalizeTask::AggregateDistinct() {
504: 	D_ASSERT(gstate.distinct_state);
505: 	auto &distinct_state = *gstate.distinct_state;
506: 	auto &distinct_data = *op.distinct_data;
507: 
508: 	auto &aggregates = op.aggregates;
509: 	auto &state = aggregate_state;
510: 
511: 	// Thread-local contexts
512: 	ThreadContext thread_context(executor.context);
513: 	ExecutionContext execution_context(executor.context, thread_context, nullptr);
514: 
515: 	auto &finalize_event = event->Cast<UngroupedDistinctAggregateFinalizeEvent>();
516: 
517: 	// Now loop through the distinct aggregates, scanning the distinct HTs
518: 
519: 	// This needs to be preserved in case the radix_table.GetData blocks
520: 	auto &agg_idx = aggregation_idx;
521: 
522: 	for (; agg_idx < aggregates.size(); agg_idx++) {
523: 		auto &aggregate = aggregates[agg_idx]->Cast<BoundAggregateExpression>();
524: 
525: 		// If aggregate is not distinct, skip it
526: 		if (!distinct_data.IsDistinct(agg_idx)) {
527: 			continue;
528: 		}
529: 
530: 		const auto table_idx = distinct_data.info.table_map.at(agg_idx);
531: 		auto &radix_table = *distinct_data.radix_tables[table_idx];
532: 		if (!blocked) {
533: 			// Because we can block, we need to make sure we preserve this state
534: 			radix_table_lstate = radix_table.GetLocalSourceState(execution_context);
535: 		}
536: 		auto &lstate = *radix_table_lstate;
537: 
538: 		auto &sink = *distinct_state.radix_states[table_idx];
539: 		InterruptState interrupt_state(shared_from_this());
540: 		OperatorSourceInput source_input {*finalize_event.global_source_states[agg_idx], lstate, interrupt_state};
541: 
542: 		DataChunk output_chunk;
543: 		output_chunk.Initialize(executor.context, distinct_state.distinct_output_chunks[table_idx]->GetTypes());
544: 
545: 		DataChunk payload_chunk;
546: 		payload_chunk.InitializeEmpty(distinct_data.grouped_aggregate_data[table_idx]->group_types);
547: 		payload_chunk.SetCardinality(0);
548: 
549: 		while (true) {
550: 			output_chunk.Reset();
551: 
552: 			auto res = radix_table.GetData(execution_context, output_chunk, sink, source_input);
553: 			if (res == SourceResultType::FINISHED) {
554: 				D_ASSERT(output_chunk.size() == 0);
555: 				break;
556: 			} else if (res == SourceResultType::BLOCKED) {
557: 				blocked = true;
558: 				return TaskExecutionResult::TASK_BLOCKED;
559: 			}
560: 
561: 			// We dont need to resolve the filter, we already did this in Sink
562: 			idx_t payload_cnt = aggregate.children.size();
563: 			for (idx_t i = 0; i < payload_cnt; i++) {
564: 				payload_chunk.data[i].Reference(output_chunk.data[i]);
565: 			}
566: 			payload_chunk.SetCardinality(output_chunk);
567: 
568: 			// Update the aggregate state
569: 			state.Sink(payload_chunk, 0, agg_idx);
570: 		}
571: 		blocked = false;
572: 	}
573: 
574: 	// After scanning the distinct HTs, we can combine the thread-local agg states with the thread-global
575: 	gstate.state.CombineDistinct(state, distinct_data);
576: 	finalize_event.FinalizeTask();
577: 	return TaskExecutionResult::TASK_FINISHED;
578: }
579: 
580: SinkFinalizeType PhysicalUngroupedAggregate::FinalizeDistinct(Pipeline &pipeline, Event &event, ClientContext &context,
581:                                                               GlobalSinkState &gstate_p) const {
582: 	auto &gstate = gstate_p.Cast<UngroupedAggregateGlobalSinkState>();
583: 	D_ASSERT(distinct_data);
584: 	auto &distinct_state = *gstate.distinct_state;
585: 
586: 	for (idx_t table_idx = 0; table_idx < distinct_data->radix_tables.size(); table_idx++) {
587: 		auto &radix_table_p = distinct_data->radix_tables[table_idx];
588: 		auto &radix_state = *distinct_state.radix_states[table_idx];
589: 		radix_table_p->Finalize(context, radix_state);
590: 	}
591: 	auto new_event = make_shared_ptr<UngroupedDistinctAggregateFinalizeEvent>(context, *this, gstate, pipeline);
592: 	event.InsertEvent(std::move(new_event));
593: 	return SinkFinalizeType::READY;
594: }
595: 
596: SinkFinalizeType PhysicalUngroupedAggregate::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
597:                                                       OperatorSinkFinalizeInput &input) const {
598: 	auto &gstate = input.global_state.Cast<UngroupedAggregateGlobalSinkState>();
599: 
600: 	if (distinct_data) {
601: 		return FinalizeDistinct(pipeline, event, context, input.global_state);
602: 	}
603: 
604: 	D_ASSERT(!gstate.finished);
605: 	gstate.finished = true;
606: 	return SinkFinalizeType::READY;
607: }
608: 
609: //===--------------------------------------------------------------------===//
610: // Source
611: //===--------------------------------------------------------------------===//
612: void VerifyNullHandling(DataChunk &chunk, UngroupedAggregateState &state,
613:                         const vector<unique_ptr<Expression>> &aggregates) {
614: #ifdef DEBUG
615: 	for (idx_t aggr_idx = 0; aggr_idx < aggregates.size(); aggr_idx++) {
616: 		auto &aggr = aggregates[aggr_idx]->Cast<BoundAggregateExpression>();
617: 		if (state.counts[aggr_idx] == 0 && aggr.function.null_handling == FunctionNullHandling::DEFAULT_NULL_HANDLING) {
618: 			// Default is when 0 values go in, NULL comes out
619: 			UnifiedVectorFormat vdata;
620: 			chunk.data[aggr_idx].ToUnifiedFormat(1, vdata);
621: 			D_ASSERT(!vdata.validity.RowIsValid(vdata.sel->get_index(0)));
622: 		}
623: 	}
624: #endif
625: }
626: 
627: void GlobalUngroupedAggregateState::Finalize(DataChunk &result) {
628: 	result.SetCardinality(1);
629: 	for (idx_t aggr_idx = 0; aggr_idx < state.aggregate_expressions.size(); aggr_idx++) {
630: 		auto &aggregate = state.aggregate_expressions[aggr_idx]->Cast<BoundAggregateExpression>();
631: 
632: 		Vector state_vector(Value::POINTER(CastPointerToValue(state.aggregate_data[aggr_idx].get())));
633: 		AggregateInputData aggr_input_data(aggregate.bind_info.get(), allocator);
634: 		aggregate.function.finalize(state_vector, aggr_input_data, result.data[aggr_idx], 1, 0);
635: 	}
636: }
637: 
638: SourceResultType PhysicalUngroupedAggregate::GetData(ExecutionContext &context, DataChunk &chunk,
639:                                                      OperatorSourceInput &input) const {
640: 	auto &gstate = sink_state->Cast<UngroupedAggregateGlobalSinkState>();
641: 	D_ASSERT(gstate.finished);
642: 
643: 	// initialize the result chunk with the aggregate values
644: 	gstate.state.Finalize(chunk);
645: 	VerifyNullHandling(chunk, gstate.state.state, aggregates);
646: 
647: 	return SourceResultType::FINISHED;
648: }
649: 
650: InsertionOrderPreservingMap<string> PhysicalUngroupedAggregate::ParamsToString() const {
651: 	InsertionOrderPreservingMap<string> result;
652: 	string aggregate_info;
653: 	for (idx_t i = 0; i < aggregates.size(); i++) {
654: 		auto &aggregate = aggregates[i]->Cast<BoundAggregateExpression>();
655: 		if (i > 0) {
656: 			aggregate_info += "\n";
657: 		}
658: 		aggregate_info += aggregates[i]->GetName();
659: 		if (aggregate.filter) {
660: 			aggregate_info += " Filter: " + aggregate.filter->GetName();
661: 		}
662: 	}
663: 	result["Aggregates"] = aggregate_info;
664: 	return result;
665: }
666: 
667: } // namespace duckdb
[end of src/execution/operator/aggregate/physical_ungrouped_aggregate.cpp]
[start of src/optimizer/join_filter_pushdown_optimizer.cpp]
1: #include "duckdb/optimizer/join_filter_pushdown_optimizer.hpp"
2: #include "duckdb/planner/operator/logical_comparison_join.hpp"
3: #include "duckdb/planner/operator/logical_get.hpp"
4: #include "duckdb/planner/operator/logical_projection.hpp"
5: #include "duckdb/execution/operator/join/join_filter_pushdown.hpp"
6: #include "duckdb/planner/expression/bound_columnref_expression.hpp"
7: #include "duckdb/core_functions/aggregate/distributive_functions.hpp"
8: #include "duckdb/optimizer/optimizer.hpp"
9: #include "duckdb/function/function_binder.hpp"
10: #include "duckdb/execution/operator/join/physical_comparison_join.hpp"
11: #include "duckdb/planner/expression/bound_aggregate_expression.hpp"
12: 
13: namespace duckdb {
14: 
15: JoinFilterPushdownOptimizer::JoinFilterPushdownOptimizer(Optimizer &optimizer) : optimizer(optimizer) {
16: }
17: 
18: void JoinFilterPushdownOptimizer::GenerateJoinFilters(LogicalComparisonJoin &join) {
19: 	switch (join.join_type) {
20: 	case JoinType::MARK:
21: 	case JoinType::SINGLE:
22: 	case JoinType::LEFT:
23: 	case JoinType::OUTER:
24: 	case JoinType::ANTI:
25: 	case JoinType::RIGHT_ANTI:
26: 	case JoinType::RIGHT_SEMI:
27: 		// cannot generate join filters for these join types
28: 		// mark/single - cannot change cardinality of probe side
29: 		// left/outer always need to include every row from probe side
30: 		// FIXME: anti/right_anti - we could do this, but need to invert the join conditions
31: 		return;
32: 	default:
33: 		break;
34: 	}
35: 	// re-order conditions here - otherwise this will happen later on and invalidate the indexes we generate
36: 	PhysicalComparisonJoin::ReorderConditions(join.conditions);
37: 	auto pushdown_info = make_uniq<JoinFilterPushdownInfo>();
38: 	for (idx_t cond_idx = 0; cond_idx < join.conditions.size(); cond_idx++) {
39: 		auto &cond = join.conditions[cond_idx];
40: 		if (cond.comparison != ExpressionType::COMPARE_EQUAL) {
41: 			// only equality supported for now
42: 			continue;
43: 		}
44: 		if (cond.left->type != ExpressionType::BOUND_COLUMN_REF) {
45: 			// only bound column ref supported for now
46: 			continue;
47: 		}
48: 		if (cond.left->return_type.IsNested()) {
49: 			// nested columns are not supported for pushdown
50: 			continue;
51: 		}
52: 		if (cond.left->return_type.id() == LogicalTypeId::INTERVAL) {
53: 			// interval is not supported for pushdown
54: 			continue;
55: 		}
56: 		JoinFilterPushdownColumn pushdown_col;
57: 		pushdown_col.join_condition = cond_idx;
58: 
59: 		auto &colref = cond.left->Cast<BoundColumnRefExpression>();
60: 		pushdown_col.probe_column_index = colref.binding;
61: 		pushdown_info->filters.push_back(pushdown_col);
62: 	}
63: 	if (pushdown_info->filters.empty()) {
64: 		// could not generate any filters - bail-out
65: 		return;
66: 	}
67: 	// find the child LogicalGet (if possible)
68: 	reference<LogicalOperator> probe_source(*join.children[0]);
69: 	while (probe_source.get().type != LogicalOperatorType::LOGICAL_GET) {
70: 		auto &probe_child = probe_source.get();
71: 		switch (probe_child.type) {
72: 		case LogicalOperatorType::LOGICAL_LIMIT:
73: 		case LogicalOperatorType::LOGICAL_FILTER:
74: 		case LogicalOperatorType::LOGICAL_ORDER_BY:
75: 		case LogicalOperatorType::LOGICAL_TOP_N:
76: 		case LogicalOperatorType::LOGICAL_DISTINCT:
77: 		case LogicalOperatorType::LOGICAL_COMPARISON_JOIN:
78: 		case LogicalOperatorType::LOGICAL_CROSS_PRODUCT:
79: 			// does not affect probe side - continue into left child
80: 			// FIXME: we can probably recurse into more operators here (e.g. window, set operation, unnest)
81: 			probe_source = *probe_child.children[0];
82: 			break;
83: 		case LogicalOperatorType::LOGICAL_PROJECTION: {
84: 			// projection - check if we all of the expressions are only column references
85: 			auto &proj = probe_source.get().Cast<LogicalProjection>();
86: 			for (auto &filter : pushdown_info->filters) {
87: 				if (filter.probe_column_index.table_index != proj.table_index) {
88: 					// index does not belong to this projection - bail-out
89: 					return;
90: 				}
91: 				auto &expr = *proj.expressions[filter.probe_column_index.column_index];
92: 				if (expr.type != ExpressionType::BOUND_COLUMN_REF) {
93: 					// not a simple column ref - bail-out
94: 					return;
95: 				}
96: 				// column-ref - pass through the new column binding
97: 				auto &colref = expr.Cast<BoundColumnRefExpression>();
98: 				filter.probe_column_index = colref.binding;
99: 			}
100: 			probe_source = *probe_child.children[0];
101: 			break;
102: 		}
103: 		default:
104: 			// unsupported child type
105: 			return;
106: 		}
107: 	}
108: 	// found the LogicalGet
109: 	auto &get = probe_source.get().Cast<LogicalGet>();
110: 	if (!get.function.filter_pushdown) {
111: 		// filter pushdown is not supported - bail-out
112: 		return;
113: 	}
114: 	for (auto &filter : pushdown_info->filters) {
115: 		if (filter.probe_column_index.table_index != get.table_index) {
116: 			// the filter does not apply to the probe side here - bail-out
117: 			return;
118: 		}
119: 	}
120: 	// pushdown can be performed
121: 	// set up the dynamic filters (if we don't have any yet)
122: 	if (!get.dynamic_filters) {
123: 		get.dynamic_filters = make_shared_ptr<DynamicTableFilterSet>();
124: 	}
125: 	pushdown_info->dynamic_filters = get.dynamic_filters;
126: 
127: 	// set up the min/max aggregates for each of the filters
128: 	vector<AggregateFunction> aggr_functions;
129: 	aggr_functions.push_back(MinFun::GetFunction());
130: 	aggr_functions.push_back(MaxFun::GetFunction());
131: 	for (auto &filter : pushdown_info->filters) {
132: 		for (auto &aggr : aggr_functions) {
133: 			FunctionBinder function_binder(optimizer.GetContext());
134: 			vector<unique_ptr<Expression>> aggr_children;
135: 			aggr_children.push_back(join.conditions[filter.join_condition].right->Copy());
136: 			auto aggr_expr = function_binder.BindAggregateFunction(aggr, std::move(aggr_children), nullptr,
137: 			                                                       AggregateType::NON_DISTINCT);
138: 			pushdown_info->min_max_aggregates.push_back(std::move(aggr_expr));
139: 		}
140: 	}
141: 
142: 	// set up the filter pushdown in the join itself
143: 	join.filter_pushdown = std::move(pushdown_info);
144: }
145: 
146: void JoinFilterPushdownOptimizer::VisitOperator(LogicalOperator &op) {
147: 	if (op.type == LogicalOperatorType::LOGICAL_COMPARISON_JOIN) {
148: 		// comparison join - try to generate join filters (if possible)
149: 		GenerateJoinFilters(op.Cast<LogicalComparisonJoin>());
150: 	}
151: 	LogicalOperatorVisitor::VisitOperator(op);
152: }
153: 
154: } // namespace duckdb
[end of src/optimizer/join_filter_pushdown_optimizer.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: