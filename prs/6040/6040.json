{
  "repo": "duckdb/duckdb",
  "pull_number": 6040,
  "instance_id": "duckdb__duckdb-6040",
  "issue_numbers": [
    "5983"
  ],
  "base_commit": "cbc9b8280049493d27cd258ccccb5191117e476b",
  "patch": "diff --git a/src/catalog/catalog.cpp b/src/catalog/catalog.cpp\nindex 8a2706710acb..e4d0c37fec42 100644\n--- a/src/catalog/catalog.cpp\n+++ b/src/catalog/catalog.cpp\n@@ -625,6 +625,19 @@ vector<SchemaCatalogEntry *> Catalog::GetSchemas(ClientContext &context) {\n \treturn schemas;\n }\n \n+bool Catalog::TypeExists(ClientContext &context, const string &catalog_name, const string &schema, const string &name) {\n+\tCatalogEntry *entry;\n+\tentry = GetEntry(context, CatalogType::TYPE_ENTRY, catalog_name, schema, name, true);\n+\tif (!entry) {\n+\t\t// look in the system catalog\n+\t\tentry = GetEntry(context, CatalogType::TYPE_ENTRY, SYSTEM_CATALOG, schema, name, true);\n+\t\tif (!entry) {\n+\t\t\treturn false;\n+\t\t}\n+\t}\n+\treturn true;\n+}\n+\n vector<SchemaCatalogEntry *> Catalog::GetSchemas(ClientContext &context, const string &catalog_name) {\n \tvector<Catalog *> catalogs;\n \tif (IsInvalidCatalog(catalog_name)) {\ndiff --git a/src/catalog/catalog_entry/duck_table_entry.cpp b/src/catalog/catalog_entry/duck_table_entry.cpp\nindex 728dbc885eac..39c532e940e6 100644\n--- a/src/catalog/catalog_entry/duck_table_entry.cpp\n+++ b/src/catalog/catalog_entry/duck_table_entry.cpp\n@@ -682,27 +682,8 @@ void DuckTableEntry::SetAsRoot() {\n \tstorage->info->table = name;\n }\n \n-void DuckTableEntry::CommitAlter(AlterInfo &info) {\n-\tD_ASSERT(info.type == AlterType::ALTER_TABLE);\n-\tauto &alter_table = (AlterTableInfo &)info;\n-\tstring column_name;\n-\tswitch (alter_table.alter_table_type) {\n-\tcase AlterTableType::REMOVE_COLUMN: {\n-\t\tauto &remove_info = (RemoveColumnInfo &)alter_table;\n-\t\tcolumn_name = remove_info.removed_column;\n-\t\tbreak;\n-\t}\n-\tcase AlterTableType::ALTER_COLUMN_TYPE: {\n-\t\tauto &change_info = (ChangeColumnTypeInfo &)alter_table;\n-\t\tcolumn_name = change_info.column_name;\n-\t\tbreak;\n-\t}\n-\tdefault:\n-\t\tbreak;\n-\t}\n-\tif (column_name.empty()) {\n-\t\treturn;\n-\t}\n+void DuckTableEntry::CommitAlter(string &column_name) {\n+\tD_ASSERT(!column_name.empty());\n \tidx_t removed_index = DConstants::INVALID_INDEX;\n \tfor (auto &col : columns.Logical()) {\n \t\tif (col.Name() == column_name) {\ndiff --git a/src/catalog/catalog_entry/type_catalog_entry.cpp b/src/catalog/catalog_entry/type_catalog_entry.cpp\nindex 5ac8f8107f61..cce42a942cb4 100644\n--- a/src/catalog/catalog_entry/type_catalog_entry.cpp\n+++ b/src/catalog/catalog_entry/type_catalog_entry.cpp\n@@ -23,7 +23,13 @@ void TypeCatalogEntry::Serialize(Serializer &serializer) {\n \tFieldWriter writer(serializer);\n \twriter.WriteString(schema->name);\n \twriter.WriteString(name);\n-\twriter.WriteSerializable(user_type);\n+\tif (user_type.id() == LogicalTypeId::ENUM) {\n+\t\t// We have to serialize Enum Values\n+\t\twriter.AddField();\n+\t\tuser_type.SerializeEnumType(writer.GetSerializer());\n+\t} else {\n+\t\twriter.WriteSerializable(user_type);\n+\t}\n \twriter.Finalize();\n }\n \n@@ -43,7 +49,7 @@ string TypeCatalogEntry::ToSQL() {\n \tstd::stringstream ss;\n \tswitch (user_type.id()) {\n \tcase (LogicalTypeId::ENUM): {\n-\t\tVector values_insert_order(EnumType::GetValuesInsertOrder(user_type));\n+\t\tauto &values_insert_order = EnumType::GetValuesInsertOrder(user_type);\n \t\tidx_t size = EnumType::GetSize(user_type);\n \t\tss << \"CREATE TYPE \";\n \t\tss << KeywordHelper::WriteOptionallyQuoted(name);\ndiff --git a/src/catalog/catalog_set.cpp b/src/catalog/catalog_set.cpp\nindex 20a0dcf9787d..6f203121d914 100644\n--- a/src/catalog/catalog_set.cpp\n+++ b/src/catalog/catalog_set.cpp\n@@ -258,6 +258,7 @@ bool CatalogSet::AlterEntry(CatalogTransaction transaction, const string &name,\n \n \t// serialize the AlterInfo into a temporary buffer\n \tBufferedSerializer serializer;\n+\tserializer.WriteString(alter_info->GetColumnName());\n \talter_info->Serialize(serializer);\n \tBinaryData serialized_alter = serializer.GetData();\n \ndiff --git a/src/common/arrow/arrow_appender.cpp b/src/common/arrow/arrow_appender.cpp\nindex 5857279b1891..4c2c9b5af827 100644\n--- a/src/common/arrow/arrow_appender.cpp\n+++ b/src/common/arrow/arrow_appender.cpp\n@@ -185,11 +185,51 @@ struct ArrowScalarData : public ArrowScalarBaseData<TGT, SRC, OP> {\n //===--------------------------------------------------------------------===//\n template <class TGT>\n struct ArrowEnumData : public ArrowScalarBaseData<TGT> {\n+\tstatic idx_t GetLength(string_t input) {\n+\t\treturn input.GetSize();\n+\t}\n+\tstatic void WriteData(data_ptr_t target, string_t input) {\n+\t\tmemcpy(target, input.GetDataUnsafe(), input.GetSize());\n+\t}\n+\tstatic void EnumAppendVector(ArrowAppendData &append_data, const Vector &input, idx_t size) {\n+\t\tD_ASSERT(input.GetVectorType() == VectorType::FLAT_VECTOR);\n+\n+\t\t// resize the validity mask and set up the validity buffer for iteration\n+\t\tResizeValidity(append_data.validity, append_data.row_count + size);\n+\n+\t\t// resize the offset buffer - the offset buffer holds the offsets into the child array\n+\t\tappend_data.main_buffer.resize(append_data.main_buffer.size() + sizeof(uint32_t) * (size + 1));\n+\t\tauto data = (string_t *)FlatVector::GetData<string_t>(input);\n+\t\tauto offset_data = (uint32_t *)append_data.main_buffer.data();\n+\t\tif (append_data.row_count == 0) {\n+\t\t\t// first entry\n+\t\t\toffset_data[0] = 0;\n+\t\t}\n+\t\t// now append the string data to the auxiliary buffer\n+\t\t// the auxiliary buffer's length depends on the string lengths, so we resize as required\n+\t\tauto last_offset = offset_data[append_data.row_count];\n+\t\tfor (idx_t i = 0; i < size; i++) {\n+\t\t\tauto offset_idx = append_data.row_count + i + 1;\n+\n+\t\t\tauto string_length = GetLength(data[i]);\n+\n+\t\t\t// append the offset data\n+\t\t\tauto current_offset = last_offset + string_length;\n+\t\t\toffset_data[offset_idx] = current_offset;\n+\n+\t\t\t// resize the string buffer if required, and write the string data\n+\t\t\tappend_data.aux_buffer.resize(current_offset);\n+\t\t\tWriteData(append_data.aux_buffer.data() + last_offset, data[i]);\n+\n+\t\t\tlast_offset = current_offset;\n+\t\t}\n+\t\tappend_data.row_count += size;\n+\t}\n \tstatic void Initialize(ArrowAppendData &result, const LogicalType &type, idx_t capacity) {\n \t\tresult.main_buffer.reserve(capacity * sizeof(TGT));\n \t\t// construct the enum child data\n \t\tauto enum_data = InitializeArrowChild(LogicalType::VARCHAR, EnumType::GetSize(type));\n-\t\tenum_data->append_vector(*enum_data, EnumType::GetValuesInsertOrder(type), EnumType::GetSize(type));\n+\t\tEnumAppendVector(*enum_data, EnumType::GetValuesInsertOrder(type), EnumType::GetSize(type));\n \t\tresult.child_data.push_back(std::move(enum_data));\n \t}\n \ndiff --git a/src/common/field_writer.cpp b/src/common/field_writer.cpp\nindex 2579995a335b..1396ed45818c 100644\n--- a/src/common/field_writer.cpp\n+++ b/src/common/field_writer.cpp\n@@ -8,6 +8,7 @@ namespace duckdb {\n FieldWriter::FieldWriter(Serializer &serializer_p)\n     : serializer(serializer_p), buffer(make_unique<BufferedSerializer>()), field_count(0), finalized(false) {\n \tbuffer->SetVersion(serializer.GetVersion());\n+\tbuffer->is_query_plan = serializer.is_query_plan;\n }\n \n FieldWriter::~FieldWriter() {\ndiff --git a/src/common/serializer/buffered_deserializer.cpp b/src/common/serializer/buffered_deserializer.cpp\nindex c4f7044982f5..e1636eb806f7 100644\n--- a/src/common/serializer/buffered_deserializer.cpp\n+++ b/src/common/serializer/buffered_deserializer.cpp\n@@ -20,4 +20,8 @@ void BufferedDeserializer::ReadData(data_ptr_t buffer, idx_t read_size) {\n \tptr += read_size;\n }\n \n+ClientContext &BufferedContextDeserializer::GetContext() {\n+\treturn context;\n+}\n+\n } // namespace duckdb\ndiff --git a/src/common/serializer/buffered_file_reader.cpp b/src/common/serializer/buffered_file_reader.cpp\nindex 57c403379bcd..c7e5a3c7e1ef 100644\n--- a/src/common/serializer/buffered_file_reader.cpp\n+++ b/src/common/serializer/buffered_file_reader.cpp\n@@ -7,8 +7,10 @@\n \n namespace duckdb {\n \n-BufferedFileReader::BufferedFileReader(FileSystem &fs, const char *path, FileLockType lock_type, FileOpener *opener)\n-    : fs(fs), data(unique_ptr<data_t[]>(new data_t[FILE_BUFFER_SIZE])), offset(0), read_data(0), total_read(0) {\n+BufferedFileReader::BufferedFileReader(FileSystem &fs, const char *path, ClientContext *context, FileLockType lock_type,\n+                                       FileOpener *opener)\n+    : fs(fs), data(unique_ptr<data_t[]>(new data_t[FILE_BUFFER_SIZE])), offset(0), read_data(0), context(context),\n+      total_read(0) {\n \thandle = fs.OpenFile(path, FileFlags::FILE_FLAGS_READ, lock_type, FileSystem::DEFAULT_COMPRESSION, opener);\n \tfile_size = fs.GetFileSize(*handle);\n }\n@@ -54,4 +56,15 @@ uint64_t BufferedFileReader::CurrentOffset() {\n \treturn total_read + offset;\n }\n \n+ClientContext &BufferedFileReader::GetContext() {\n+\tif (!context) {\n+\t\tthrow InternalException(\"Trying to acquire a client context that does not exist\");\n+\t}\n+\treturn *context;\n+}\n+\n+Catalog *BufferedFileReader::GetCatalog() {\n+\treturn catalog;\n+}\n+\n } // namespace duckdb\ndiff --git a/src/common/types.cpp b/src/common/types.cpp\nindex cb47d222ae4e..71a23e2f6cbf 100644\n--- a/src/common/types.cpp\n+++ b/src/common/types.cpp\n@@ -1,7 +1,9 @@\n #include \"duckdb/common/types.hpp\"\n \n #include \"duckdb/catalog/catalog.hpp\"\n+#include \"duckdb/catalog/catalog_entry/schema_catalog_entry.hpp\"\n #include \"duckdb/catalog/catalog_entry/type_catalog_entry.hpp\"\n+#include \"duckdb/catalog/catalog_search_path.hpp\"\n #include \"duckdb/catalog/default/default_types.hpp\"\n #include \"duckdb/common/exception.hpp\"\n #include \"duckdb/common/field_writer.hpp\"\n@@ -17,6 +19,11 @@\n #include \"duckdb/common/types/vector.hpp\"\n #include \"duckdb/common/unordered_map.hpp\"\n #include \"duckdb/function/cast_rules.hpp\"\n+#include \"duckdb/main/attached_database.hpp\"\n+#include \"duckdb/main/client_context.hpp\"\n+#include \"duckdb/main/client_data.hpp\"\n+#include \"duckdb/main/database.hpp\"\n+#include \"duckdb/main/database_manager.hpp\"\n #include \"duckdb/parser/keyword_helper.hpp\"\n #include \"duckdb/parser/parser.hpp\"\n \n@@ -785,7 +792,7 @@ LogicalType LogicalType::MaxLogicalType(const LogicalType &left, const LogicalTy\n \t\tchild_list_t<LogicalType> child_types;\n \t\tfor (idx_t i = 0; i < left_child_types.size(); i++) {\n \t\t\tauto child_type = MaxLogicalType(left_child_types[i].second, right_child_types[i].second);\n-\t\t\tchild_types.push_back(make_pair(left_child_types[i].first, std::move(child_type)));\n+\t\t\tchild_types.emplace_back(left_child_types[i].first, std::move(child_type));\n \t\t}\n \n \t\treturn LogicalType::STRUCT(std::move(child_types));\n@@ -797,7 +804,7 @@ LogicalType LogicalType::MaxLogicalType(const LogicalType &left, const LogicalTy\n \t\t\t// return the \"larger\" type, with the most members\n \t\t\treturn left_member_count > right_member_count ? left : right;\n \t\t}\n-\t\t// otherwise, keep left, dont try to meld the two together.\n+\t\t// otherwise, keep left, don't try to meld the two together.\n \t\treturn left;\n \t}\n \t// types are equal but no extra specifier: just return the type\n@@ -838,17 +845,6 @@ bool ApproxEqual(double ldecimal, double rdecimal) {\n //===--------------------------------------------------------------------===//\n // Extra Type Info\n //===--------------------------------------------------------------------===//\n-enum class ExtraTypeInfoType : uint8_t {\n-\tINVALID_TYPE_INFO = 0,\n-\tGENERIC_TYPE_INFO = 1,\n-\tDECIMAL_TYPE_INFO = 2,\n-\tSTRING_TYPE_INFO = 3,\n-\tLIST_TYPE_INFO = 4,\n-\tSTRUCT_TYPE_INFO = 5,\n-\tENUM_TYPE_INFO = 6,\n-\tUSER_TYPE_INFO = 7,\n-\tAGGREGATE_STATE_TYPE_INFO = 8\n-};\n \n struct ExtraTypeInfo {\n \texplicit ExtraTypeInfo(ExtraTypeInfoType type) : type(type) {\n@@ -941,6 +937,10 @@ TypeCatalogEntry *LogicalType::GetCatalog(const LogicalType &type) {\n \treturn ((ExtraTypeInfo &)*info).catalog_entry;\n }\n \n+ExtraTypeInfoType LogicalType::GetExtraTypeInfoType(const ExtraTypeInfo &type) {\n+\treturn type.type;\n+}\n+\n //===--------------------------------------------------------------------===//\n // Decimal Type\n //===--------------------------------------------------------------------===//\n@@ -1108,7 +1108,7 @@ struct StructTypeInfo : public ExtraTypeInfo {\n \t\tfor (uint32_t i = 0; i < child_types_size; i++) {\n \t\t\tauto name = source.Read<string>();\n \t\t\tauto type = LogicalType::Deserialize(source);\n-\t\t\tchild_list.push_back(make_pair(std::move(name), std::move(type)));\n+\t\t\tchild_list.emplace_back(std::move(name), std::move(type));\n \t\t}\n \t\treturn make_shared<StructTypeInfo>(std::move(child_list));\n \t}\n@@ -1227,8 +1227,8 @@ LogicalType LogicalType::MAP(LogicalType child) {\n \n LogicalType LogicalType::MAP(LogicalType key, LogicalType value) {\n \tchild_list_t<LogicalType> child_types;\n-\tchild_types.push_back({\"key\", std::move(key)});\n-\tchild_types.push_back({\"value\", std::move(value)});\n+\tchild_types.emplace_back(\"key\", std::move(key));\n+\tchild_types.emplace_back(\"value\", std::move(value));\n \treturn LogicalType::MAP(LogicalType::STRUCT(std::move(child_types)));\n }\n \n@@ -1247,7 +1247,7 @@ const LogicalType &MapType::ValueType(const LogicalType &type) {\n //===--------------------------------------------------------------------===//\n \n LogicalType LogicalType::UNION(child_list_t<LogicalType> members) {\n-\tD_ASSERT(members.size() > 0);\n+\tD_ASSERT(!members.empty());\n \tD_ASSERT(members.size() <= UnionType::MAX_UNION_MEMBERS);\n \t// union types always have a hidden \"tag\" field in front\n \tmembers.insert(members.begin(), {\"\", LogicalType::TINYINT});\n@@ -1270,7 +1270,7 @@ const string &UnionType::GetMemberName(const LogicalType &type, idx_t index) {\n }\n \n idx_t UnionType::GetMemberCount(const LogicalType &type) {\n-\t// dont count the \"tag\" field\n+\t// don't count the \"tag\" field\n \treturn StructType::GetChildTypes(type).size() - 1;\n }\n const child_list_t<LogicalType> UnionType::CopyMemberTypes(const LogicalType &type) {\n@@ -1326,13 +1326,27 @@ enum EnumDictType : uint8_t { INVALID = 0, VECTOR_DICT = 1 };\n \n struct EnumTypeInfo : public ExtraTypeInfo {\n \texplicit EnumTypeInfo(string enum_name_p, Vector &values_insert_order_p, idx_t dict_size_p)\n-\t    : ExtraTypeInfo(ExtraTypeInfoType::ENUM_TYPE_INFO), dict_type(EnumDictType::VECTOR_DICT),\n-\t      enum_name(std::move(enum_name_p)), values_insert_order(values_insert_order_p), dict_size(dict_size_p) {\n-\t}\n-\tEnumDictType dict_type;\n-\tstring enum_name;\n-\tVector values_insert_order;\n-\tidx_t dict_size;\n+\t    : ExtraTypeInfo(ExtraTypeInfoType::ENUM_TYPE_INFO), values_insert_order(values_insert_order_p),\n+\t      dict_type(EnumDictType::VECTOR_DICT), enum_name(std::move(enum_name_p)), dict_size(dict_size_p) {\n+\t}\n+\n+\tconst EnumDictType &GetEnumDictType() {\n+\t\treturn dict_type;\n+\t};\n+\tconst string &GetEnumName() {\n+\t\treturn enum_name;\n+\t};\n+\tconst string GetSchemaName() const {\n+\t\treturn catalog_entry ? catalog_entry->schema->name : \"\";\n+\t};\n+\tconst Vector &GetValuesInsertOrder() {\n+\t\treturn values_insert_order;\n+\t};\n+\tconst idx_t &GetDictSize() {\n+\t\treturn dict_size;\n+\t};\n+\tEnumTypeInfo(const EnumTypeInfo &) = delete;\n+\tEnumTypeInfo &operator=(const EnumTypeInfo &) = delete;\n \n protected:\n \t// Equalities are only used in enums with different catalog entries\n@@ -1362,12 +1376,39 @@ struct EnumTypeInfo : public ExtraTypeInfo {\n \t\tif (dict_type != EnumDictType::VECTOR_DICT) {\n \t\t\tthrow InternalException(\"Cannot serialize non-vector dictionary ENUM types\");\n \t\t}\n-\t\twriter.WriteField<uint32_t>(dict_size);\n-\t\twriter.WriteString(enum_name);\n-\t\t((Vector &)values_insert_order).Serialize(dict_size, writer.GetSerializer());\n+\t\tbool serialize_internals = GetSchemaName().empty() || writer.GetSerializer().is_query_plan;\n+\t\tEnumType::Serialize(writer, *this, serialize_internals);\n \t}\n+\n+\tVector values_insert_order;\n+\n+private:\n+\tEnumDictType dict_type;\n+\tstring enum_name;\n+\tidx_t dict_size;\n };\n \n+// If this type is primarily stored in the catalog or not. Enums from Pandas/Factors are not in the catalog.\n+\n+void EnumType::Serialize(FieldWriter &writer, const ExtraTypeInfo &type_info, bool serialize_internals) {\n+\tD_ASSERT(type_info.type == ExtraTypeInfoType::ENUM_TYPE_INFO);\n+\tauto &enum_info = (EnumTypeInfo &)type_info;\n+\t// Store Schema Name\n+\twriter.WriteString(enum_info.GetSchemaName());\n+\t// Store Enum Name\n+\twriter.WriteString(enum_info.GetEnumName());\n+\t// Store If we are serializing the internals\n+\twriter.WriteField<bool>(serialize_internals);\n+\tif (serialize_internals) {\n+\t\t// We must serialize the internals\n+\t\tauto dict_size = enum_info.GetDictSize();\n+\t\t// Store Dictionary Size\n+\t\twriter.WriteField<uint32_t>(dict_size);\n+\t\t// Store Vector Order By Insertion\n+\t\t((Vector &)enum_info.GetValuesInsertOrder()).Serialize(dict_size, writer.GetSerializer());\n+\t}\n+}\n+\n template <class T>\n struct EnumTypeInfoTemplated : public EnumTypeInfo {\n \texplicit EnumTypeInfoTemplated(const string &enum_name_p, Vector &values_insert_order_p, idx_t size_p)\n@@ -1391,13 +1432,21 @@ struct EnumTypeInfoTemplated : public EnumTypeInfo {\n \t\t}\n \t}\n \n-\tstatic shared_ptr<EnumTypeInfoTemplated> Deserialize(FieldReader &reader, uint32_t size) {\n-\t\tauto enum_name = reader.ReadRequired<string>();\n+\tstatic shared_ptr<EnumTypeInfoTemplated> Deserialize(FieldReader &reader, uint32_t size, string enum_name) {\n+\n \t\tVector values_insert_order(LogicalType::VARCHAR, size);\n \t\tvalues_insert_order.Deserialize(size, reader.GetSource());\n \t\treturn make_shared<EnumTypeInfoTemplated>(std::move(enum_name), values_insert_order, size);\n \t}\n \n+\tstring_map_t<T> &GetValues() {\n+\t\treturn values;\n+\t}\n+\n+\tEnumTypeInfoTemplated(const EnumTypeInfoTemplated &) = delete;\n+\tEnumTypeInfoTemplated &operator=(const EnumTypeInfoTemplated &) = delete;\n+\n+private:\n \tstring_map_t<T> values;\n };\n \n@@ -1405,7 +1454,7 @@ const string &EnumType::GetTypeName(const LogicalType &type) {\n \tD_ASSERT(type.id() == LogicalTypeId::ENUM);\n \tauto info = type.AuxInfo();\n \tD_ASSERT(info);\n-\treturn ((EnumTypeInfo &)*info).enum_name;\n+\treturn ((EnumTypeInfo &)*info).GetEnumName();\n }\n \n static PhysicalType EnumVectorDictType(idx_t size) {\n@@ -1454,11 +1503,11 @@ int64_t EnumType::GetPos(const LogicalType &type, const string_t &key) {\n \tauto info = type.AuxInfo();\n \tswitch (type.InternalType()) {\n \tcase PhysicalType::UINT8:\n-\t\treturn TemplatedGetPos(((EnumTypeInfoTemplated<uint8_t> &)*info).values, key);\n+\t\treturn TemplatedGetPos(((EnumTypeInfoTemplated<uint8_t> &)*info).GetValues(), key);\n \tcase PhysicalType::UINT16:\n-\t\treturn TemplatedGetPos(((EnumTypeInfoTemplated<uint16_t> &)*info).values, key);\n+\t\treturn TemplatedGetPos(((EnumTypeInfoTemplated<uint16_t> &)*info).GetValues(), key);\n \tcase PhysicalType::UINT32:\n-\t\treturn TemplatedGetPos(((EnumTypeInfoTemplated<uint32_t> &)*info).values, key);\n+\t\treturn TemplatedGetPos(((EnumTypeInfoTemplated<uint32_t> &)*info).GetValues(), key);\n \tdefault:\n \t\tthrow InternalException(\"ENUM can only have unsigned integers (except UINT64) as physical types\");\n \t}\n@@ -1466,22 +1515,22 @@ int64_t EnumType::GetPos(const LogicalType &type, const string_t &key) {\n \n const string EnumType::GetValue(const Value &val) {\n \tauto info = val.type().AuxInfo();\n-\tauto &values_insert_order = ((EnumTypeInfo &)*info).values_insert_order;\n+\tauto &values_insert_order = ((EnumTypeInfo &)*info).GetValuesInsertOrder();\n \treturn StringValue::Get(values_insert_order.GetValue(val.GetValue<uint32_t>()));\n }\n \n-Vector &EnumType::GetValuesInsertOrder(const LogicalType &type) {\n+const Vector &EnumType::GetValuesInsertOrder(const LogicalType &type) {\n \tD_ASSERT(type.id() == LogicalTypeId::ENUM);\n \tauto info = type.AuxInfo();\n \tD_ASSERT(info);\n-\treturn ((EnumTypeInfo &)*info).values_insert_order;\n+\treturn ((EnumTypeInfo &)*info).GetValuesInsertOrder();\n }\n \n idx_t EnumType::GetSize(const LogicalType &type) {\n \tD_ASSERT(type.id() == LogicalTypeId::ENUM);\n \tauto info = type.AuxInfo();\n \tD_ASSERT(info);\n-\treturn ((EnumTypeInfo &)*info).dict_size;\n+\treturn ((EnumTypeInfo &)*info).GetDictSize();\n }\n \n void EnumType::SetCatalog(LogicalType &type, TypeCatalogEntry *catalog_entry) {\n@@ -1497,13 +1546,18 @@ TypeCatalogEntry *EnumType::GetCatalog(const LogicalType &type) {\n \treturn ((EnumTypeInfo &)*info).catalog_entry;\n }\n \n+string EnumType::GetSchemaName(const LogicalType &type) {\n+\tauto catalog_entry = EnumType::GetCatalog(type);\n+\treturn catalog_entry ? catalog_entry->schema->name : \"\";\n+}\n+\n PhysicalType EnumType::GetPhysicalType(const LogicalType &type) {\n \tD_ASSERT(type.id() == LogicalTypeId::ENUM);\n \tauto aux_info = type.AuxInfo();\n \tD_ASSERT(aux_info);\n \tauto &info = (EnumTypeInfo &)*aux_info;\n-\tD_ASSERT(info.dict_type == EnumDictType::VECTOR_DICT);\n-\treturn EnumVectorDictType(info.dict_size);\n+\tD_ASSERT(info.GetEnumDictType() == EnumDictType::VECTOR_DICT);\n+\treturn EnumVectorDictType(info.GetDictSize());\n }\n \n //===--------------------------------------------------------------------===//\n@@ -1549,20 +1603,40 @@ shared_ptr<ExtraTypeInfo> ExtraTypeInfo::Deserialize(FieldReader &reader) {\n \t\textra_info = UserTypeInfo::Deserialize(reader);\n \t\tbreak;\n \tcase ExtraTypeInfoType::ENUM_TYPE_INFO: {\n-\t\tauto enum_size = reader.ReadRequired<uint32_t>();\n-\t\tauto enum_internal_type = EnumVectorDictType(enum_size);\n-\t\tswitch (enum_internal_type) {\n-\t\tcase PhysicalType::UINT8:\n-\t\t\textra_info = EnumTypeInfoTemplated<uint8_t>::Deserialize(reader, enum_size);\n-\t\t\tbreak;\n-\t\tcase PhysicalType::UINT16:\n-\t\t\textra_info = EnumTypeInfoTemplated<uint16_t>::Deserialize(reader, enum_size);\n-\t\t\tbreak;\n-\t\tcase PhysicalType::UINT32:\n-\t\t\textra_info = EnumTypeInfoTemplated<uint32_t>::Deserialize(reader, enum_size);\n+\t\tauto schema_name = reader.ReadRequired<string>();\n+\t\tauto enum_name = reader.ReadRequired<string>();\n+\t\tauto deserialize_internals = reader.ReadRequired<bool>();\n+\t\tif (!deserialize_internals) {\n+\t\t\t// this means the enum should already be in the catalog.\n+\t\t\tauto &client_context = reader.GetSource().GetContext();\n+\t\t\t// See if the serializer has a catalog\n+\t\t\tauto catalog = reader.GetSource().GetCatalog();\n+\t\t\tif (catalog) {\n+\t\t\t\tauto enum_type = catalog->GetType(client_context, schema_name, enum_name, true);\n+\t\t\t\tif (enum_type != LogicalType::INVALID) {\n+\t\t\t\t\textra_info = enum_type.GetAuxInfoShrPtr();\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tif (!extra_info) {\n+\t\t\t\tthrow InternalException(\"Could not find ENUM in the Catalog to deserialize\");\n+\t\t\t}\n \t\t\tbreak;\n-\t\tdefault:\n-\t\t\tthrow InternalException(\"Invalid Physical Type for ENUMs\");\n+\t\t} else {\n+\t\t\tauto enum_size = reader.ReadRequired<uint32_t>();\n+\t\t\tauto enum_internal_type = EnumVectorDictType(enum_size);\n+\t\t\tswitch (enum_internal_type) {\n+\t\t\tcase PhysicalType::UINT8:\n+\t\t\t\textra_info = EnumTypeInfoTemplated<uint8_t>::Deserialize(reader, enum_size, enum_name);\n+\t\t\t\tbreak;\n+\t\t\tcase PhysicalType::UINT16:\n+\t\t\t\textra_info = EnumTypeInfoTemplated<uint16_t>::Deserialize(reader, enum_size, enum_name);\n+\t\t\t\tbreak;\n+\t\t\tcase PhysicalType::UINT32:\n+\t\t\t\textra_info = EnumTypeInfoTemplated<uint32_t>::Deserialize(reader, enum_size, enum_name);\n+\t\t\t\tbreak;\n+\t\t\tdefault:\n+\t\t\t\tthrow InternalException(\"Invalid Physical Type for ENUMs\");\n+\t\t\t}\n \t\t}\n \t} break;\n \tcase ExtraTypeInfoType::AGGREGATE_STATE_TYPE_INFO:\n@@ -1592,6 +1666,15 @@ void LogicalType::Serialize(Serializer &serializer) const {\n \twriter.Finalize();\n }\n \n+void LogicalType::SerializeEnumType(Serializer &serializer) const {\n+\tFieldWriter writer(serializer);\n+\twriter.WriteField<LogicalTypeId>(id_);\n+\twriter.WriteField<ExtraTypeInfoType>(type_info_->type);\n+\tEnumType::Serialize(writer, *type_info_, true);\n+\twriter.WriteString(type_info_->alias);\n+\twriter.Finalize();\n+}\n+\n LogicalType LogicalType::Deserialize(Deserializer &source) {\n \tFieldReader reader(source);\n \tauto id = reader.ReadRequired<LogicalTypeId>();\ndiff --git a/src/execution/operator/schema/physical_create_type.cpp b/src/execution/operator/schema/physical_create_type.cpp\nindex 82b07075c5b2..98cba1126144 100644\n--- a/src/execution/operator/schema/physical_create_type.cpp\n+++ b/src/execution/operator/schema/physical_create_type.cpp\n@@ -15,10 +15,11 @@ PhysicalCreateType::PhysicalCreateType(unique_ptr<CreateTypeInfo> info, idx_t es\n //===--------------------------------------------------------------------===//\n class CreateTypeGlobalState : public GlobalSinkState {\n public:\n-\texplicit CreateTypeGlobalState(ClientContext &context) : collection(context, {LogicalType::VARCHAR}) {\n+\texplicit CreateTypeGlobalState(ClientContext &context) : result(LogicalType::VARCHAR) {\n \t}\n-\n-\tColumnDataCollection collection;\n+\tVector result;\n+\tidx_t size = 0;\n+\tidx_t capacity = STANDARD_VECTOR_SIZE;\n };\n \n unique_ptr<GlobalSinkState> PhysicalCreateType::GetGlobalSinkState(ClientContext &context) const {\n@@ -28,7 +29,7 @@ unique_ptr<GlobalSinkState> PhysicalCreateType::GetGlobalSinkState(ClientContext\n SinkResultType PhysicalCreateType::Sink(ExecutionContext &context, GlobalSinkState &gstate_p, LocalSinkState &lstate_p,\n                                         DataChunk &input) const {\n \tauto &gstate = (CreateTypeGlobalState &)gstate_p;\n-\tidx_t total_row_count = gstate.collection.Count() + input.size();\n+\tidx_t total_row_count = gstate.size + input.size();\n \tif (total_row_count > NumericLimits<uint32_t>::Maximum()) {\n \t\tthrow InvalidInputException(\"Attempted to create ENUM of size %llu, which exceeds the maximum size of %llu\",\n \t\t                            total_row_count, NumericLimits<uint32_t>::Maximum());\n@@ -36,15 +37,23 @@ SinkResultType PhysicalCreateType::Sink(ExecutionContext &context, GlobalSinkSta\n \tUnifiedVectorFormat sdata;\n \tinput.data[0].ToUnifiedFormat(input.size(), sdata);\n \n+\tif (total_row_count > gstate.capacity) {\n+\t\t// We must resize our result vector\n+\t\tgstate.result.Resize(gstate.capacity, gstate.capacity * 2);\n+\t\tgstate.capacity *= 2;\n+\t}\n+\n+\tauto src_ptr = (string_t *)sdata.data;\n+\tauto result_ptr = FlatVector::GetData<string_t>(gstate.result);\n \t// Input vector has NULL value, we just throw an exception\n \tfor (idx_t i = 0; i < input.size(); i++) {\n \t\tidx_t idx = sdata.sel->get_index(i);\n \t\tif (!sdata.validity.RowIsValid(idx)) {\n \t\t\tthrow InvalidInputException(\"Attempted to create ENUM type with NULL value!\");\n \t\t}\n+\t\tresult_ptr[gstate.size++] =\n+\t\t    StringVector::AddStringOrBlob(gstate.result, src_ptr[idx].GetDataUnsafe(), src_ptr[idx].GetSize());\n \t}\n-\n-\tgstate.collection.Append(input);\n \treturn SinkResultType::NEED_MORE_INPUT;\n }\n \n@@ -72,44 +81,15 @@ void PhysicalCreateType::GetData(ExecutionContext &context, DataChunk &chunk, Gl\n \n \tif (IsSink()) {\n \t\tD_ASSERT(info->type == LogicalType::INVALID);\n-\n \t\tauto &g_sink_state = (CreateTypeGlobalState &)*sink_state;\n-\t\tauto &collection = g_sink_state.collection;\n-\n-\t\tidx_t total_row_count = collection.Count();\n-\n-\t\tColumnDataScanState scan_state;\n-\t\tcollection.InitializeScan(scan_state);\n-\n-\t\tDataChunk scan_chunk;\n-\t\tcollection.InitializeScanChunk(scan_chunk);\n-\n-\t\tVector result(LogicalType::VARCHAR, total_row_count);\n-\t\tauto result_ptr = FlatVector::GetData<string_t>(result);\n-\n-\t\tidx_t offset = 0;\n-\t\twhile (collection.Scan(scan_state, scan_chunk)) {\n-\t\t\tidx_t src_row_count = scan_chunk.size();\n-\t\t\tauto &src_vec = scan_chunk.data[0];\n-\t\t\tD_ASSERT(src_vec.GetVectorType() == VectorType::FLAT_VECTOR);\n-\t\t\tD_ASSERT(src_vec.GetType().id() == LogicalType::VARCHAR);\n-\n-\t\t\tauto src_ptr = FlatVector::GetData<string_t>(src_vec);\n-\n-\t\t\tfor (idx_t i = 0; i < src_row_count; i++) {\n-\t\t\t\tidx_t target_index = offset + i;\n-\t\t\t\tresult_ptr[target_index] =\n-\t\t\t\t    StringVector::AddStringOrBlob(result, src_ptr[i].GetDataUnsafe(), src_ptr[i].GetSize());\n-\t\t\t}\n-\n-\t\t\toffset += src_row_count;\n-\t\t}\n-\n-\t\tinfo->type = LogicalType::ENUM(info->name, result, total_row_count);\n+\t\tinfo->type = LogicalType::ENUM(info->name, g_sink_state.result, g_sink_state.size);\n \t}\n \n \tauto &catalog = Catalog::GetCatalog(context.client, info->catalog);\n-\tcatalog.CreateType(context.client, info.get());\n+\tauto catalog_entry = catalog.CreateType(context.client, info.get());\n+\tD_ASSERT(catalog_entry->type == CatalogType::TYPE_ENTRY);\n+\tauto catalog_type = (TypeCatalogEntry *)catalog_entry;\n+\tLogicalType::SetCatalog(info->type, catalog_type);\n \tstate.finished = true;\n }\n \ndiff --git a/src/include/duckdb/catalog/catalog.hpp b/src/include/duckdb/catalog/catalog.hpp\nindex f9cd465982f7..be017851f47d 100644\n--- a/src/include/duckdb/catalog/catalog.hpp\n+++ b/src/include/duckdb/catalog/catalog.hpp\n@@ -205,6 +205,9 @@ class Catalog {\n \tDUCKDB_API static LogicalType GetType(ClientContext &context, const string &catalog_name, const string &schema,\n \t                                      const string &name);\n \n+\tstatic bool TypeExists(ClientContext &context, const string &catalog_name, const string &schema,\n+\t                       const string &name);\n+\n \ttemplate <class T>\n \tT *GetEntry(ClientContext &context, const string &schema_name, const string &name, bool if_exists = false,\n \t            QueryErrorContext error_context = QueryErrorContext()) {\ndiff --git a/src/include/duckdb/catalog/catalog_entry/duck_table_entry.hpp b/src/include/duckdb/catalog/catalog_entry/duck_table_entry.hpp\nindex 43996027ccaf..b1a126bd7cb2 100644\n--- a/src/include/duckdb/catalog/catalog_entry/duck_table_entry.hpp\n+++ b/src/include/duckdb/catalog/catalog_entry/duck_table_entry.hpp\n@@ -35,7 +35,7 @@ class DuckTableEntry : public TableCatalogEntry {\n \n \tvoid SetAsRoot() override;\n \n-\tvoid CommitAlter(AlterInfo &info);\n+\tvoid CommitAlter(string &column_name);\n \tvoid CommitDrop();\n \n \tTableFunction GetScanFunction(ClientContext &context, unique_ptr<FunctionData> &bind_data) override;\ndiff --git a/src/include/duckdb/common/field_writer.hpp b/src/include/duckdb/common/field_writer.hpp\nindex e9cae65d06bf..b7a81c017910 100644\n--- a/src/include/duckdb/common/field_writer.hpp\n+++ b/src/include/duckdb/common/field_writer.hpp\n@@ -25,7 +25,7 @@ struct IndexWriteOperation {\n \n class FieldWriter {\n public:\n-\tDUCKDB_API FieldWriter(Serializer &serializer);\n+\tDUCKDB_API explicit FieldWriter(Serializer &serializer);\n \tDUCKDB_API ~FieldWriter();\n \n public:\n@@ -128,11 +128,11 @@ class FieldWriter {\n \t\treturn *buffer;\n \t}\n \n-private:\n \tvoid AddField() {\n \t\tfield_count++;\n \t}\n \n+private:\n \ttemplate <class T>\n \tvoid Write(const T &element) {\n \t\tWriteData((const_data_ptr_t)&element, sizeof(T));\n@@ -152,7 +152,7 @@ DUCKDB_API void FieldWriter::Write(const string &val);\n \n class FieldDeserializer : public Deserializer {\n public:\n-\tFieldDeserializer(Deserializer &root);\n+\texplicit FieldDeserializer(Deserializer &root);\n \n public:\n \tvoid ReadData(data_ptr_t buffer, idx_t read_size) override;\n@@ -163,6 +163,14 @@ class FieldDeserializer : public Deserializer {\n \t\treturn root;\n \t}\n \n+\tClientContext &GetContext() override {\n+\t\treturn root.GetContext();\n+\t}\n+\n+\tCatalog *GetCatalog() override {\n+\t\treturn root.GetCatalog();\n+\t}\n+\n private:\n \tDeserializer &root;\n \tidx_t remaining_data;\n@@ -177,7 +185,7 @@ struct IndexReadOperation {\n \n class FieldReader {\n public:\n-\tDUCKDB_API FieldReader(Deserializer &source);\n+\tDUCKDB_API explicit FieldReader(Deserializer &source);\n \tDUCKDB_API ~FieldReader();\n \n public:\ndiff --git a/src/include/duckdb/common/serializer.hpp b/src/include/duckdb/common/serializer.hpp\nindex 87ff24d69334..0b66ed3f96d8 100644\n--- a/src/include/duckdb/common/serializer.hpp\n+++ b/src/include/duckdb/common/serializer.hpp\n@@ -8,6 +8,7 @@\n \n #pragma once\n \n+#include \"duckdb/catalog/catalog.hpp\"\n #include \"duckdb/common/common.hpp\"\n #include \"duckdb/common/exception.hpp\"\n #include \"duckdb/common/vector.hpp\"\n@@ -21,6 +22,8 @@ class Serializer {\n \tuint64_t version = 0L;\n \n public:\n+\tbool is_query_plan = false;\n+\n \tvirtual ~Serializer() {\n \t}\n \n@@ -111,6 +114,16 @@ class Deserializer {\n \t//! Reads [read_size] bytes into the buffer\n \tvirtual void ReadData(data_ptr_t buffer, idx_t read_size) = 0;\n \n+\t//! Gets the context for the deserializer\n+\tvirtual ClientContext &GetContext() {\n+\t\tthrow InternalException(\"This deserializer does not have a client-context\");\n+\t};\n+\n+\t//! Gets the catalog for the deserializer\n+\tvirtual Catalog *GetCatalog() {\n+\t\treturn nullptr;\n+\t};\n+\n \ttemplate <class T>\n \tT Read() {\n \t\tT value;\ndiff --git a/src/include/duckdb/common/serializer/buffered_deserializer.hpp b/src/include/duckdb/common/serializer/buffered_deserializer.hpp\nindex 4399c7a397db..58e41dbba678 100644\n--- a/src/include/duckdb/common/serializer/buffered_deserializer.hpp\n+++ b/src/include/duckdb/common/serializer/buffered_deserializer.hpp\n@@ -26,14 +26,16 @@ class BufferedDeserializer : public Deserializer {\n \tvoid ReadData(data_ptr_t buffer, uint64_t read_size) override;\n };\n \n-class BufferentContextDeserializer : public BufferedDeserializer {\n+class BufferedContextDeserializer : public BufferedDeserializer {\n public:\n-\tBufferentContextDeserializer(ClientContext &context_p, data_ptr_t ptr, idx_t data_size)\n+\tBufferedContextDeserializer(ClientContext &context_p, data_ptr_t ptr, idx_t data_size)\n \t    : BufferedDeserializer(ptr, data_size), context(context_p) {\n \t}\n \n public:\n \tClientContext &context;\n+\n+\tClientContext &GetContext() override;\n };\n \n } // namespace duckdb\ndiff --git a/src/include/duckdb/common/serializer/buffered_file_reader.hpp b/src/include/duckdb/common/serializer/buffered_file_reader.hpp\nindex 6267d0e7bed5..d7132e6e500e 100644\n--- a/src/include/duckdb/common/serializer/buffered_file_reader.hpp\n+++ b/src/include/duckdb/common/serializer/buffered_file_reader.hpp\n@@ -14,14 +14,16 @@ namespace duckdb {\n \n class BufferedFileReader : public Deserializer {\n public:\n-\tBufferedFileReader(FileSystem &fs, const char *path, FileLockType lock_type = FileLockType::READ_LOCK,\n-\t                   FileOpener *opener = nullptr);\n+\tBufferedFileReader(FileSystem &fs, const char *path, ClientContext *context,\n+\t                   FileLockType lock_type = FileLockType::READ_LOCK, FileOpener *opener = nullptr);\n \n \tFileSystem &fs;\n \tunique_ptr<data_t[]> data;\n \tidx_t offset;\n \tidx_t read_data;\n \tunique_ptr<FileHandle> handle;\n+\tClientContext *context;\n+\tCatalog *catalog = nullptr;\n \n public:\n \tvoid ReadData(data_ptr_t buffer, uint64_t read_size) override;\n@@ -35,6 +37,10 @@ class BufferedFileReader : public Deserializer {\n \tvoid Seek(uint64_t location);\n \tuint64_t CurrentOffset();\n \n+\tClientContext &GetContext() override;\n+\n+\tCatalog *GetCatalog() override;\n+\n private:\n \tidx_t file_size;\n \tidx_t total_read;\ndiff --git a/src/include/duckdb/common/types.hpp b/src/include/duckdb/common/types.hpp\nindex 2de4a0147ecd..781f109447b3 100644\n--- a/src/include/duckdb/common/types.hpp\n+++ b/src/include/duckdb/common/types.hpp\n@@ -23,6 +23,20 @@ class Value;\n class TypeCatalogEntry;\n class Vector;\n class ClientContext;\n+class FieldWriter;\n+\n+//! Extra Type Info Type\n+enum class ExtraTypeInfoType : uint8_t {\n+\tINVALID_TYPE_INFO = 0,\n+\tGENERIC_TYPE_INFO = 1,\n+\tDECIMAL_TYPE_INFO = 2,\n+\tSTRING_TYPE_INFO = 3,\n+\tLIST_TYPE_INFO = 4,\n+\tSTRUCT_TYPE_INFO = 5,\n+\tENUM_TYPE_INFO = 6,\n+\tUSER_TYPE_INFO = 7,\n+\tAGGREGATE_STATE_TYPE_INFO = 8\n+};\n \n struct hugeint_t {\n public:\n@@ -297,6 +311,11 @@ struct LogicalType {\n \tinline const ExtraTypeInfo *AuxInfo() const {\n \t\treturn type_info_.get();\n \t}\n+\n+\tinline shared_ptr<ExtraTypeInfo> GetAuxInfoShrPtr() const {\n+\t\treturn type_info_;\n+\t}\n+\n \tinline void CopyAuxInfo(const LogicalType& other) {\n \t\ttype_info_ = other.type_info_;\n \t}\n@@ -324,6 +343,9 @@ struct LogicalType {\n \n \t//! Serializes a LogicalType to a stand-alone binary blob\n \tDUCKDB_API void Serialize(Serializer &serializer) const;\n+\n+\tDUCKDB_API void SerializeEnumType(Serializer &serializer) const;\n+\n \t//! Deserializes a blob back into an LogicalType\n \tDUCKDB_API static LogicalType Deserialize(Deserializer &source);\n \n@@ -349,6 +371,8 @@ struct LogicalType {\n \tDUCKDB_API static void SetCatalog(LogicalType &type, TypeCatalogEntry* catalog_entry);\n \tDUCKDB_API static TypeCatalogEntry* GetCatalog(const LogicalType &type);\n \n+\tDUCKDB_API static ExtraTypeInfoType GetExtraTypeInfoType(const ExtraTypeInfo &type);\n+\n \t//! Gets the decimal properties of a numeric type. Fails if the type is not numeric.\n \tDUCKDB_API bool GetDecimalProperties(uint8_t &width, uint8_t &scale) const;\n \n@@ -441,12 +465,14 @@ struct UserType{\n struct EnumType{\n \tDUCKDB_API static const string &GetTypeName(const LogicalType &type);\n \tDUCKDB_API static int64_t GetPos(const LogicalType &type, const string_t& key);\n-\tDUCKDB_API static Vector &GetValuesInsertOrder(const LogicalType &type);\n+\tDUCKDB_API static const Vector &GetValuesInsertOrder(const LogicalType &type);\n \tDUCKDB_API static idx_t GetSize(const LogicalType &type);\n \tDUCKDB_API static const string GetValue(const Value &val);\n \tDUCKDB_API static void SetCatalog(LogicalType &type, TypeCatalogEntry* catalog_entry);\n \tDUCKDB_API static TypeCatalogEntry* GetCatalog(const LogicalType &type);\n+\tDUCKDB_API static string GetSchemaName(const LogicalType &type);\n \tDUCKDB_API static PhysicalType GetPhysicalType(const LogicalType &type);\n+\tDUCKDB_API static void Serialize(FieldWriter& writer, const ExtraTypeInfo& type_info, bool serialize_internals);\n };\n \n struct StructType {\ndiff --git a/src/include/duckdb/main/connection_manager.hpp b/src/include/duckdb/main/connection_manager.hpp\nindex 3f0981d8e911..11495742de50 100644\n--- a/src/include/duckdb/main/connection_manager.hpp\n+++ b/src/include/duckdb/main/connection_manager.hpp\n@@ -47,6 +47,8 @@ class ConnectionManager {\n \t\treturn result;\n \t}\n \n+\tClientContext *GetConnection(DatabaseInstance *db);\n+\n \tstatic ConnectionManager &Get(DatabaseInstance &db);\n \tstatic ConnectionManager &Get(ClientContext &context);\n \ndiff --git a/src/include/duckdb/parser/parsed_data/alter_info.hpp b/src/include/duckdb/parser/parsed_data/alter_info.hpp\nindex 293c32236d45..4fe4ebd836d7 100644\n--- a/src/include/duckdb/parser/parsed_data/alter_info.hpp\n+++ b/src/include/duckdb/parser/parsed_data/alter_info.hpp\n@@ -59,6 +59,9 @@ struct AlterInfo : public ParseInfo {\n \tvoid Serialize(Serializer &serializer) const;\n \tvirtual void Serialize(FieldWriter &writer) const = 0;\n \tstatic unique_ptr<AlterInfo> Deserialize(Deserializer &source);\n+\tvirtual string GetColumnName() const {\n+\t\treturn \"\";\n+\t};\n \n \tAlterEntryData GetAlterEntryData() const;\n };\ndiff --git a/src/include/duckdb/parser/parsed_data/alter_table_info.hpp b/src/include/duckdb/parser/parsed_data/alter_table_info.hpp\nindex 76e93ea5f026..ea068cdafd64 100644\n--- a/src/include/duckdb/parser/parsed_data/alter_table_info.hpp\n+++ b/src/include/duckdb/parser/parsed_data/alter_table_info.hpp\n@@ -137,6 +137,9 @@ struct RemoveColumnInfo : public AlterTableInfo {\n \tunique_ptr<AlterInfo> Copy() const override;\n \tvoid SerializeAlterTable(FieldWriter &writer) const override;\n \tstatic unique_ptr<AlterInfo> Deserialize(FieldReader &reader, AlterEntryData data);\n+\tstring GetColumnName() const override {\n+\t\treturn removed_column;\n+\t};\n };\n \n //===--------------------------------------------------------------------===//\n@@ -158,6 +161,9 @@ struct ChangeColumnTypeInfo : public AlterTableInfo {\n \tunique_ptr<AlterInfo> Copy() const override;\n \tvoid SerializeAlterTable(FieldWriter &writer) const override;\n \tstatic unique_ptr<AlterInfo> Deserialize(FieldReader &reader, AlterEntryData data);\n+\tstring GetColumnName() const override {\n+\t\treturn column_name;\n+\t};\n };\n \n //===--------------------------------------------------------------------===//\ndiff --git a/src/include/duckdb/storage/checkpoint_manager.hpp b/src/include/duckdb/storage/checkpoint_manager.hpp\nindex 82f1a14c7273..5310c5c5489d 100644\n--- a/src/include/duckdb/storage/checkpoint_manager.hpp\n+++ b/src/include/duckdb/storage/checkpoint_manager.hpp\n@@ -44,7 +44,7 @@ class CheckpointWriter {\n \tvirtual void WriteMacro(ScalarMacroCatalogEntry &table);\n \tvirtual void WriteTableMacro(TableMacroCatalogEntry &table);\n \tvirtual void WriteIndex(IndexCatalogEntry &index_catalog);\n-\tvirtual void WriteType(TypeCatalogEntry &table);\n+\tvirtual void WriteType(TypeCatalogEntry &type);\n };\n \n class CheckpointReader {\ndiff --git a/src/include/duckdb/storage/meta_block_reader.hpp b/src/include/duckdb/storage/meta_block_reader.hpp\nindex 1531e0c496f6..9ec1f2435b12 100644\n--- a/src/include/duckdb/storage/meta_block_reader.hpp\n+++ b/src/include/duckdb/storage/meta_block_reader.hpp\n@@ -36,7 +36,14 @@ class MetaBlockReader : public Deserializer {\n \t//! Read content of size read_size into the buffer\n \tvoid ReadData(data_ptr_t buffer, idx_t read_size) override;\n \n+\tClientContext &GetContext() override;\n+\tCatalog *GetCatalog() override;\n+\tvoid SetCatalog(Catalog *catalog_p);\n+\tvoid SetContext(ClientContext *context_p);\n+\n private:\n \tvoid ReadNewBlock(block_id_t id);\n+\tCatalog *catalog = nullptr;\n+\tClientContext *context = nullptr;\n };\n } // namespace duckdb\ndiff --git a/src/include/duckdb/storage/write_ahead_log.hpp b/src/include/duckdb/storage/write_ahead_log.hpp\nindex 653445241987..cff6ca35822e 100644\n--- a/src/include/duckdb/storage/write_ahead_log.hpp\n+++ b/src/include/duckdb/storage/write_ahead_log.hpp\n@@ -137,7 +137,7 @@ class WriteAheadLog {\n \t//! Sets the table used for subsequent insert/delete/update commands\n \tvoid WriteSetTable(string &schema, string &table);\n \n-\tvoid WriteAlter(AlterInfo &info);\n+\tvoid WriteAlter(data_ptr_t ptr, idx_t data_size);\n \n \tvoid WriteInsert(DataChunk &chunk);\n \tvoid WriteDelete(DataChunk &chunk);\ndiff --git a/src/main/database.cpp b/src/main/database.cpp\nindex c0d0dc8a2ab2..8c072a96cf9f 100644\n--- a/src/main/database.cpp\n+++ b/src/main/database.cpp\n@@ -115,6 +115,15 @@ ConnectionManager &ConnectionManager::Get(DatabaseInstance &db) {\n \treturn db.GetConnectionManager();\n }\n \n+ClientContext *ConnectionManager::GetConnection(DatabaseInstance *db) {\n+\tfor (auto &conn : connections) {\n+\t\tif (conn.first->db.get() == db) {\n+\t\t\treturn conn.first;\n+\t\t}\n+\t}\n+\treturn nullptr;\n+}\n+\n ConnectionManager &ConnectionManager::Get(ClientContext &context) {\n \treturn ConnectionManager::Get(DatabaseInstance::GetDatabase(context));\n }\ndiff --git a/src/planner/logical_operator.cpp b/src/planner/logical_operator.cpp\nindex d70e4663b22b..da98caf67dc4 100644\n--- a/src/planner/logical_operator.cpp\n+++ b/src/planner/logical_operator.cpp\n@@ -128,6 +128,8 @@ void LogicalOperator::Verify(ClientContext &context) {\n \t\t\tcontinue;\n \t\t}\n \t\tBufferedSerializer serializer;\n+\t\t// We are serializing a query plan\n+\t\tserializer.is_query_plan = true;\n \t\ttry {\n \t\t\texpressions[expr_idx]->Serialize(serializer);\n \t\t} catch (NotImplementedException &ex) {\n@@ -136,7 +138,7 @@ void LogicalOperator::Verify(ClientContext &context) {\n \t\t}\n \n \t\tauto data = serializer.GetData();\n-\t\tauto deserializer = BufferedDeserializer(data.data.get(), data.size);\n+\t\tauto deserializer = BufferedContextDeserializer(context, data.data.get(), data.size);\n \n \t\tPlanDeserializationState state(context);\n \t\tauto deserialized_expression = Expression::Deserialize(deserializer, state);\n@@ -371,7 +373,7 @@ unique_ptr<LogicalOperator> LogicalOperator::Copy(ClientContext &context) const\n \t\t                              std::string(ex.what()));\n \t}\n \tauto data = logical_op_serializer.GetData();\n-\tauto logical_op_deserializer = BufferedDeserializer(data.data.get(), data.size);\n+\tauto logical_op_deserializer = BufferedContextDeserializer(context, data.data.get(), data.size);\n \tPlanDeserializationState state(context);\n \tauto op_copy = LogicalOperator::Deserialize(logical_op_deserializer, state);\n \treturn op_copy;\ndiff --git a/src/planner/planner.cpp b/src/planner/planner.cpp\nindex 8f3b5087e797..588f527148ca 100644\n--- a/src/planner/planner.cpp\n+++ b/src/planner/planner.cpp\n@@ -184,6 +184,7 @@ void Planner::VerifyPlan(ClientContext &context, unique_ptr<LogicalOperator> &op\n \t}\n \n \tBufferedSerializer serializer;\n+\tserializer.is_query_plan = true;\n \ttry {\n \t\top->Serialize(serializer);\n \t} catch (NotImplementedException &ex) {\n@@ -191,7 +192,7 @@ void Planner::VerifyPlan(ClientContext &context, unique_ptr<LogicalOperator> &op\n \t\treturn;\n \t}\n \tauto data = serializer.GetData();\n-\tauto deserializer = BufferedDeserializer(data.data.get(), data.size);\n+\tauto deserializer = BufferedContextDeserializer(context, data.data.get(), data.size);\n \n \tPlanDeserializationState state(context);\n \tauto new_plan = LogicalOperator::Deserialize(deserializer, state);\ndiff --git a/src/storage/checkpoint_manager.cpp b/src/storage/checkpoint_manager.cpp\nindex 4c63549e599a..6123fa0ba19d 100644\n--- a/src/storage/checkpoint_manager.cpp\n+++ b/src/storage/checkpoint_manager.cpp\n@@ -128,6 +128,8 @@ void SingleFileCheckpointReader::LoadFromStorage() {\n \tcon.BeginTransaction();\n \t// create the MetaBlockReader to read from the storage\n \tMetaBlockReader reader(block_manager, meta_block);\n+\treader.SetCatalog(&catalog.GetAttached().GetCatalog());\n+\treader.SetContext(con.context.get());\n \tLoadCheckpoint(*con.context, reader);\n \tcon.Commit();\n }\n@@ -395,13 +397,16 @@ void CheckpointReader::ReadIndex(ClientContext &context, MetaBlockReader &reader\n //===--------------------------------------------------------------------===//\n // Custom Types\n //===--------------------------------------------------------------------===//\n-void CheckpointWriter::WriteType(TypeCatalogEntry &table) {\n-\ttable.Serialize(GetMetaBlockWriter());\n+void CheckpointWriter::WriteType(TypeCatalogEntry &type) {\n+\ttype.Serialize(GetMetaBlockWriter());\n }\n \n void CheckpointReader::ReadType(ClientContext &context, MetaBlockReader &reader) {\n \tauto info = TypeCatalogEntry::Deserialize(reader);\n-\tcatalog.CreateType(context, info.get());\n+\tauto catalog_entry = (TypeCatalogEntry *)catalog.CreateType(context, info.get());\n+\tif (info->type.id() == LogicalTypeId::ENUM) {\n+\t\tEnumType::SetCatalog(info->type, catalog_entry);\n+\t}\n }\n \n //===--------------------------------------------------------------------===//\ndiff --git a/src/storage/meta_block_reader.cpp b/src/storage/meta_block_reader.cpp\nindex 30cfafa08418..058eedc55bd8 100644\n--- a/src/storage/meta_block_reader.cpp\n+++ b/src/storage/meta_block_reader.cpp\n@@ -1,5 +1,7 @@\n #include \"duckdb/storage/meta_block_reader.hpp\"\n #include \"duckdb/storage/buffer_manager.hpp\"\n+#include \"duckdb/main/connection_manager.hpp\"\n+#include \"duckdb/main/database.hpp\"\n \n #include <cstring>\n \n@@ -34,6 +36,16 @@ void MetaBlockReader::ReadData(data_ptr_t buffer, idx_t read_size) {\n \toffset += read_size;\n }\n \n+ClientContext &MetaBlockReader::GetContext() {\n+\tif (!context) {\n+\t\tthrow InternalException(\"Meta Block Reader is missing context\");\n+\t}\n+\treturn *context;\n+}\n+Catalog *MetaBlockReader::GetCatalog() {\n+\treturn catalog;\n+}\n+\n void MetaBlockReader::ReadNewBlock(block_id_t id) {\n \tauto &buffer_manager = block_manager.buffer_manager;\n \n@@ -52,4 +64,14 @@ void MetaBlockReader::ReadNewBlock(block_id_t id) {\n \toffset = sizeof(block_id_t);\n }\n \n+void MetaBlockReader::SetCatalog(Catalog *catalog_p) {\n+\tD_ASSERT(!catalog);\n+\tcatalog = catalog_p;\n+}\n+\n+void MetaBlockReader::SetContext(ClientContext *context_p) {\n+\tD_ASSERT(!context);\n+\tcontext = context_p;\n+}\n+\n } // namespace duckdb\ndiff --git a/src/storage/storage_info.cpp b/src/storage/storage_info.cpp\nindex 2e2fe2621247..9ecee7109d78 100644\n--- a/src/storage/storage_info.cpp\n+++ b/src/storage/storage_info.cpp\n@@ -2,7 +2,7 @@\n \n namespace duckdb {\n \n-const uint64_t VERSION_NUMBER = 46;\n+const uint64_t VERSION_NUMBER = 47;\n \n struct StorageVersionInfo {\n \tconst char *version_name;\ndiff --git a/src/storage/wal_replay.cpp b/src/storage/wal_replay.cpp\nindex c75ce674b502..559b31fb124c 100644\n--- a/src/storage/wal_replay.cpp\n+++ b/src/storage/wal_replay.cpp\n@@ -24,17 +24,19 @@\n namespace duckdb {\n \n bool WriteAheadLog::Replay(AttachedDatabase &database, string &path) {\n-\tauto initial_reader = make_unique<BufferedFileReader>(FileSystem::Get(database), path.c_str());\n+\tConnection con(database.GetDatabase());\n+\tauto initial_reader = make_unique<BufferedFileReader>(FileSystem::Get(database), path.c_str(), con.context.get());\n \tif (initial_reader->Finished()) {\n \t\t// WAL is empty\n \t\treturn false;\n \t}\n-\tConnection con(database.GetDatabase());\n+\n \tcon.BeginTransaction();\n \n \t// first deserialize the WAL to look for a checkpoint flag\n \t// if there is a checkpoint flag, we might have already flushed the contents of the WAL to disk\n \tReplayState checkpoint_state(database, *con.context, *initial_reader);\n+\tinitial_reader->catalog = &checkpoint_state.catalog;\n \tcheckpoint_state.deserialize_only = true;\n \ttry {\n \t\twhile (true) {\n@@ -70,7 +72,8 @@ bool WriteAheadLog::Replay(AttachedDatabase &database, string &path) {\n \t}\n \n \t// we need to recover from the WAL: actually set up the replay state\n-\tBufferedFileReader reader(FileSystem::Get(database), path.c_str());\n+\tBufferedFileReader reader(FileSystem::Get(database), path.c_str(), con.context.get());\n+\treader.catalog = &checkpoint_state.catalog;\n \tReplayState state(database, *con.context, reader);\n \n \t// replay the WAL\n@@ -192,6 +195,7 @@ void ReplayState::ReplayEntry(WALType entry_type) {\n // Replay Table\n //===--------------------------------------------------------------------===//\n void ReplayState::ReplayCreateTable() {\n+\n \tauto info = TableCatalogEntry::Deserialize(source, context);\n \tif (deserialize_only) {\n \t\treturn;\n@@ -278,10 +282,9 @@ void ReplayState::ReplayDropSchema() {\n //===--------------------------------------------------------------------===//\n void ReplayState::ReplayCreateType() {\n \tauto info = TypeCatalogEntry::Deserialize(source);\n-\tif (deserialize_only) {\n+\tif (Catalog::TypeExists(context, info->catalog, info->schema, info->name)) {\n \t\treturn;\n \t}\n-\n \tcatalog.CreateType(context, info.get());\n }\n \ndiff --git a/src/storage/write_ahead_log.cpp b/src/storage/write_ahead_log.cpp\nindex 7294431e4a7a..dfb11e159aaf 100644\n--- a/src/storage/write_ahead_log.cpp\n+++ b/src/storage/write_ahead_log.cpp\n@@ -281,12 +281,12 @@ void WriteAheadLog::WriteUpdate(DataChunk &chunk, const vector<column_t> &column\n //===--------------------------------------------------------------------===//\n // Write ALTER Statement\n //===--------------------------------------------------------------------===//\n-void WriteAheadLog::WriteAlter(AlterInfo &info) {\n+void WriteAheadLog::WriteAlter(data_ptr_t ptr, idx_t data_size) {\n \tif (skip_writing) {\n \t\treturn;\n \t}\n \twriter->Write<WALType>(WALType::ALTER_INFO);\n-\tinfo.Serialize(*writer);\n+\twriter->WriteData(ptr, data_size);\n }\n \n //===--------------------------------------------------------------------===//\ndiff --git a/src/transaction/commit_state.cpp b/src/transaction/commit_state.cpp\nindex 44f7bfa4da81..527861b71ae6 100644\n--- a/src/transaction/commit_state.cpp\n+++ b/src/transaction/commit_state.cpp\n@@ -46,12 +46,16 @@ void CommitState::WriteCatalogEntry(CatalogEntry *entry, data_ptr_t dataptr) {\n \t\t\t// ALTER TABLE statement, read the extra data after the entry\n \t\t\tauto extra_data_size = Load<idx_t>(dataptr);\n \t\t\tauto extra_data = (data_ptr_t)(dataptr + sizeof(idx_t));\n-\t\t\t// deserialize it\n+\n \t\t\tBufferedDeserializer source(extra_data, extra_data_size);\n-\t\t\tauto info = AlterInfo::Deserialize(source);\n-\t\t\t// write the alter table in the log\n-\t\t\ttable_entry->CommitAlter(*info);\n-\t\t\tlog->WriteAlter(*info);\n+\t\t\tstring column_name = source.Read<string>();\n+\n+\t\t\tif (!column_name.empty()) {\n+\t\t\t\t// write the alter table in the log\n+\t\t\t\ttable_entry->CommitAlter(column_name);\n+\t\t\t}\n+\n+\t\t\tlog->WriteAlter(source.ptr, source.endptr - source.ptr);\n \t\t} else {\n \t\t\t// CREATE TABLE statement\n \t\t\tlog->WriteCreateTable((TableCatalogEntry *)parent);\n@@ -71,9 +75,9 @@ void CommitState::WriteCatalogEntry(CatalogEntry *entry, data_ptr_t dataptr) {\n \t\t\tauto extra_data = (data_ptr_t)(dataptr + sizeof(idx_t));\n \t\t\t// deserialize it\n \t\t\tBufferedDeserializer source(extra_data, extra_data_size);\n-\t\t\tauto info = AlterInfo::Deserialize(source);\n+\t\t\tstring column_name = source.Read<string>();\n \t\t\t// write the alter table in the log\n-\t\t\tlog->WriteAlter(*info);\n+\t\t\tlog->WriteAlter(source.ptr, source.endptr - source.ptr);\n \t\t} else {\n \t\t\tlog->WriteCreateView((ViewCatalogEntry *)parent);\n \t\t}\n",
  "test_patch": "diff --git a/test/api/serialized_plans/test_plan_serialization_bwc.cpp b/test/api/serialized_plans/test_plan_serialization_bwc.cpp\nindex b74a32d0c091..ee0496acbf1e 100644\n--- a/test/api/serialized_plans/test_plan_serialization_bwc.cpp\n+++ b/test/api/serialized_plans/test_plan_serialization_bwc.cpp\n@@ -64,7 +64,8 @@ TEST_CASE(\"Test deserialized plans from file\", \"[.][serialization]\") {\n \tDuckDB db;\n \tConnection con(db);\n \tload_db(con);\n-\tBufferedFileReader deserializer(db.GetFileSystem(), get_full_file_name(\"serialized_plans.binary\").c_str());\n+\tBufferedFileReader deserializer(db.GetFileSystem(), get_full_file_name(\"serialized_plans.binary\").c_str(),\n+\t                                con.context.get());\n \tdeserializer.SetVersion(deserializer.Read<uint64_t>());\n \n \tstd::ifstream queries(get_full_file_name(\"queries.sql\"));\ndiff --git a/test/api/test_plan_serialization_across_versions.cpp b/test/api/test_plan_serialization_across_versions.cpp\nindex 6fa1d4d75004..b7548e04de67 100644\n--- a/test/api/test_plan_serialization_across_versions.cpp\n+++ b/test/api/test_plan_serialization_across_versions.cpp\n@@ -90,7 +90,7 @@ static void test_helper(const V version_compatible_value, const uint64_t sourceV\n \twriter.Finalize();\n \n \tauto data = serializer.GetData();\n-\tauto deserializer = BufferedDeserializer(data.data.get(), data.size);\n+\tauto deserializer = BufferedContextDeserializer(*con.context, data.data.get(), data.size);\n \tdeserializer.SetVersion(deserializer.Read<uint64_t>());\n \tINFO(\"target version: \" << deserializer.GetVersion());\n \tFieldReader reader(deserializer);\ndiff --git a/test/sql/attach/attach_enums.test b/test/sql/attach/attach_enums.test\nindex 6ecb3e48c7b5..3c1e0363ac21 100644\n--- a/test/sql/attach/attach_enums.test\n+++ b/test/sql/attach/attach_enums.test\n@@ -37,3 +37,28 @@ query TT\n select * from db1.person\n ----\n Moe\thappy\n+\n+statement ok\n+ATTACH '__TEST_DIR__/attach_enums_2.db' AS db2\n+\n+statement ok\n+CREATE TYPE db2.mood AS ENUM ('ble','grr','kkcry');\n+\n+statement ok\n+CREATE TABLE db2.person (\n+    name text,\n+    current_mood mood\n+);\n+\n+statement ok\n+INSERT INTO db2.person VALUES ('Moe', 'kkcry');\n+\n+query TT\n+select * from db1.person\n+----\n+Moe\thappy\n+\n+query TT\n+select * from db2.person\n+----\n+Moe\tkkcry\n\\ No newline at end of file\ndiff --git a/test/sql/storage/wal/wal_store_rename_table.test b/test/sql/storage/wal/wal_store_rename_table.test\nindex b8eded7b682e..bd7adc0316f3 100644\n--- a/test/sql/storage/wal/wal_store_rename_table.test\n+++ b/test/sql/storage/wal/wal_store_rename_table.test\n@@ -71,6 +71,7 @@ COMMIT\n restart\n \n # after a restart, the renamed table is still here\n+\n query I\n SELECT a FROM new_name ORDER BY 1\n ----\ndiff --git a/test/sql/storage_version/storage_version.db b/test/sql/storage_version/storage_version.db\nindex 33da4532e71a..d1f820152564 100644\nBinary files a/test/sql/storage_version/storage_version.db and b/test/sql/storage_version/storage_version.db differ\ndiff --git a/test/sql/types/enum/test_5983.test b/test/sql/types/enum/test_5983.test\nnew file mode 100644\nindex 000000000000..76e36effed6a\n--- /dev/null\n+++ b/test/sql/types/enum/test_5983.test\n@@ -0,0 +1,26 @@\n+# name: test/sql/types/enum/test_5983.test\n+# description: Test ENUM blowup\n+# group: [enum]\n+\n+require tpch\n+\n+load __TEST_DIR__/test_enum_blowup.db\n+\n+\n+statement ok\n+CALL DBGEN(sf=0.01);\n+\n+statement ok\n+create type orderkey_enum as enum (Select (l_orderkey/4)::VARCHAR from lineitem);\n+\n+statement ok\n+create table t2 (c1 orderkey_enum);\n+\n+statement ok\n+insert into t2 (select (l_orderkey/4)::VARCHAR from lineitem);\n+\n+statement ok\n+CREATE TYPE l_comment_enum as ENUM(select l_comment from lineitem);\n+\n+statement ok\n+CREATE TABLE lineitem2 (comment l_comment_enum);\n\\ No newline at end of file\n",
  "problem_statement": "Enums functionality fails when memory limit is low\n### What happens?\r\n\r\nConverting a table with varchar columns to a table with enum columns increases the size of a duckdb database by more than double. The operation does not complete either. This happens when the size of the database is close to the memory limit of the duckdb persistent session.\r\n### To Reproduce\r\n\r\n```bash\r\n./build/debug/duckdb mem_limit_enum.duckdb\r\n```\r\n\r\n```SQL\r\nset memory_limit='2GB';\r\nload tpch;\r\ncall dbgen(sf=2);\r\nSELECT count(distinct(l_comment)) from lineitem;\r\nCREATE TYPE l_comment_enum as ENUM(select l_comment from lineitem); # statement 1\r\nCREATE TABLE lineitem2 (comment l_comment_enum); # statement 2\r\n```\r\nAfter statement 2, the creation of the table increases the size of the database by over 50%.\r\nYou can keep creating tables to show this\r\n```\r\nINSERT INTO lineitem2 (select l_comment from lineitem);\r\n```\r\nObserve as the size of mem_limit_enum.duckdb increases to over 5GB and the insert statement does not complete.\r\n\r\n\r\n\r\n### OS:\r\n\r\nMacOs Monterey\r\n\r\n### DuckDB Version:\r\n\r\nv0.6.2-dev1218\r\n\r\n### DuckDB Client:\r\n\r\nCLI\r\n\r\n### Full Name:\r\n\r\nTom Ebergen\r\n\r\n### Affiliation:\r\n\r\nDuckDB labs\r\n\r\n### Have you tried this on the latest `master` branch?\r\n\r\n- [X] I agree\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] I agree\n",
  "hints_text": "I suspect this has to do with the size of the enum and it being serialized in separate places. The `l_comment` enum must be at least 1GB by itself. I think exceeding the memory limit is not that much of a problem - memory managing enums would create a lot of complications. Perhaps we should hard-limit enums to some reasonable value like 10MB of string data?\nI agree with Mytherin that managing the memory of an ENUM can be challenging. \r\n\r\nImplementing a hard limit may prevent users from creating an ENUM, but perhaps we should consider making it a configurable option.\r\n\r\nOne potential solution for making ENUM memory management easier is to use an ART in place of an unordered map. However, this might have nasty consequences for the OG memory footprint. (:\nSo the specific case where this is causing issues for me is a enum type with 1000000 (1 mil) values in a table with 1,000,000,000 (1 billion) rows.\r\n\r\nEach enum has a length of 3 - 8 characters. I'll see if I can reproduce an example using a tpch dataset\nI wonder if we might be serializing types in too many places in general - which wouldn't be a problem for regular types but becomes a problem with larger enums. \n@pdet Here are steps to reproduce where only 750,001 enums are created for a table that will have 11mil rows eventually. I also did not need to change my memory limit (16GB by default).\r\n\r\n```\r\ncall dbgen(sf=2);\r\nselect distinct(l_orderkey/4) from lineitem; -- shows the amount of values we will have in our enum\r\ncreate type orderkey_enum as enum (Select (l_orderkey/4)::VARCHAR from lineitem);\r\ncreate table t2 (c1 orderkey_enum);\r\ninsert into t2 (select (l_orderkey/4)::VARCHAR from lineitem); -- this blows up the size of the database\r\n```",
  "created_at": "2023-01-30T15:50:25Z"
}