You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes.
In the patch file, please make sure to include the line numbers and a blank line at the end so it can be applied using git apply.

<issue>
Seeing a single value changed after inserting ~1.5million row polars df to duckdb and then querying back out
### What happens?

When I write these 9 polars dataframes (saved as parquet files here: https://drive.google.com/drive/folders/18-xFUBLvPyux-erwETaATOV_fUpkLp4B?usp=sharing) to a duckdb table, I find that one single value from one single column is changed when I read the data back out.

### To Reproduce

First, download these 9 parquet files to your local machine: https://drive.google.com/drive/folders/18-xFUBLvPyux-erwETaATOV_fUpkLp4B?usp=sharing

Then, run the following code:
```
import duckdb
import polars as pl
import os
from datetime import date
from zipfile import ZipFile

### Constants
FILESDIR = "path_you_saved_downloaded_parquet_files_to" 
DAYS = range(1,10)
DUCKDB_FILE = os.path.join("path_you_saved_downloaded_parquet_files_to", "so_test.ddb")

### Read data from disk and write to duckdb
for day in DAYS:
    pldf = pl.read_parquet(os.path.join(FILESDIR, f"f{day}_masked.parquet"))
    with duckdb.connect(database=DUCKDB_FILE, read_only=False) as conn:
        if day == 1:
            conn.execute(f"CREATE OR REPLACE TABLE so_test AS SELECT * FROM pldf")
        else:
            conn.execute(f"INSERT INTO so_test SELECT * FROM pldf")        

### Verify that the data has changed by comparing a specific row from last df to its record in duckdb
PROBLEM_ROW = 1108956

print(pldf.slice(PROBLEM_ROW,1))  # hour=4.0
with duckdb.connect(database=DUCKDB_FILE, read_only=False) as conn:
    print(conn.execute(f"SELECT * from so_test WHERE (DAILY_FILE_ROW_ORDER = {PROBLEM_ROW}) and (DAILY_FILE_DATE = '2021-01-09')").pl())  # hour=3.0 now
```

Below is a screenshot of what I see when I run the code above. Note the rows are identical except `hour` has been changed from `4.0` to `3.0` (Also, I see `hour` for that row has been changed to `3.0` even if I read the result as a pandas `.df()`). 
<img width="834" alt="screenshot for duckdb issue" src="https://github.com/user-attachments/assets/ee217594-528c-49d3-861e-016758a19c02">


### OS:

Windows Server 2019 Datacenter, Version 10.0.17763 Build 17763 x64

### DuckDB Version:

1.1.0

### DuckDB Client:

python (python version 3.9.6)

### Hardware:

_No response_

### Full Name:

Max Epstein

### Affiliation:

Max Power Consulting, LLC

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have
Seeing a single value changed after inserting ~1.5million row polars df to duckdb and then querying back out
### What happens?

When I write these 9 polars dataframes (saved as parquet files here: https://drive.google.com/drive/folders/18-xFUBLvPyux-erwETaATOV_fUpkLp4B?usp=sharing) to a duckdb table, I find that one single value from one single column is changed when I read the data back out.

### To Reproduce

First, download these 9 parquet files to your local machine: https://drive.google.com/drive/folders/18-xFUBLvPyux-erwETaATOV_fUpkLp4B?usp=sharing

Then, run the following code:
```
import duckdb
import polars as pl
import os
from datetime import date
from zipfile import ZipFile

### Constants
FILESDIR = "path_you_saved_downloaded_parquet_files_to" 
DAYS = range(1,10)
DUCKDB_FILE = os.path.join("path_you_saved_downloaded_parquet_files_to", "so_test.ddb")

### Read data from disk and write to duckdb
for day in DAYS:
    pldf = pl.read_parquet(os.path.join(FILESDIR, f"f{day}_masked.parquet"))
    with duckdb.connect(database=DUCKDB_FILE, read_only=False) as conn:
        if day == 1:
            conn.execute(f"CREATE OR REPLACE TABLE so_test AS SELECT * FROM pldf")
        else:
            conn.execute(f"INSERT INTO so_test SELECT * FROM pldf")        

### Verify that the data has changed by comparing a specific row from last df to its record in duckdb
PROBLEM_ROW = 1108956

print(pldf.slice(PROBLEM_ROW,1))  # hour=4.0
with duckdb.connect(database=DUCKDB_FILE, read_only=False) as conn:
    print(conn.execute(f"SELECT * from so_test WHERE (DAILY_FILE_ROW_ORDER = {PROBLEM_ROW}) and (DAILY_FILE_DATE = '2021-01-09')").pl())  # hour=3.0 now
```

Below is a screenshot of what I see when I run the code above. Note the rows are identical except `hour` has been changed from `4.0` to `3.0` (Also, I see `hour` for that row has been changed to `3.0` even if I read the result as a pandas `.df()`). 
<img width="834" alt="screenshot for duckdb issue" src="https://github.com/user-attachments/assets/ee217594-528c-49d3-861e-016758a19c02">


### OS:

Windows Server 2019 Datacenter, Version 10.0.17763 Build 17763 x64

### DuckDB Version:

1.1.0

### DuckDB Client:

python (python version 3.9.6)

### Hardware:

_No response_

### Full Name:

Max Epstein

### Affiliation:

Max Power Consulting, LLC

### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.

I have tested with a stable release

### Did you include all relevant data sets for reproducing the issue?

Yes

### Did you include all code required to reproduce the issue?

- [X] Yes, I have

### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?

- [X] Yes, I have

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <picture>
3:     <source media="(prefers-color-scheme: light)" srcset="logo/DuckDB_Logo-horizontal.svg">
4:     <source media="(prefers-color-scheme: dark)" srcset="logo/DuckDB_Logo-horizontal-dark-mode.svg">
5:     <img alt="DuckDB logo" src="logo/DuckDB_Logo-horizontal.svg" height="100">
6:   </picture>
7: </div>
8: <br>
9: 
10: <p align="center">
11:   <a href="https://github.com/duckdb/duckdb/actions"><img src="https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main" alt="Github Actions Badge"></a>
12:   <a href="https://discord.gg/tcvwpjfnZx"><img src="https://shields.io/discord/909674491309850675" alt="discord" /></a>
13:   <a href="https://github.com/duckdb/duckdb/releases/"><img src="https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white" alt="Latest Release"></a>
14: </p>
15: 
16: ## DuckDB
17: 
18: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/guides/sql_features/friendly_sql).
19: 
20: DuckDB is available as a [standalone CLI application](https://duckdb.org/docs/api/cli/overview) and has clients for [Python](https://duckdb.org/docs/api/python/overview), [R](https://duckdb.org/docs/api/r), [Java](https://duckdb.org/docs/api/java), [Wasm](https://duckdb.org/docs/api/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdblabs.github.io/duckplyr/).
21: 
22: For more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/).
23: 
24: ## Installation
25: 
26: If you want to install DuckDB, please see [our installation page](https://duckdb.org/docs/installation) for instructions.
27: 
28: ## Data Import
29: 
30: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
31: 
32: ```sql
33: SELECT * FROM 'myfile.csv';
34: SELECT * FROM 'myfile.parquet';
35: ```
36: 
37: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
38: 
39: ## SQL Reference
40: 
41: The documentation contains a [SQL introduction and reference](https://duckdb.org/docs/sql/introduction).
42: 
43: ## Development
44: 
45: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).
46: 
47: Please also refer to our [Build Guide](https://duckdb.org/dev/building) and [Contribution Guide](CONTRIBUTING.md).
48: 
49: ## Support
50: 
51: See the [Support Options](https://duckdblabs.com/support/) page.
[end of README.md]
[start of src/storage/compression/rle.cpp]
1: #include "duckdb/common/types/null_value.hpp"
2: #include "duckdb/function/compression/compression.hpp"
3: #include "duckdb/function/compression_function.hpp"
4: #include "duckdb/storage/buffer_manager.hpp"
5: #include "duckdb/storage/table/column_data_checkpointer.hpp"
6: #include "duckdb/storage/table/column_segment.hpp"
7: #include "duckdb/storage/table/scan_state.hpp"
8: 
9: #include <functional>
10: 
11: namespace duckdb {
12: 
13: using rle_count_t = uint16_t;
14: 
15: //===--------------------------------------------------------------------===//
16: // Analyze
17: //===--------------------------------------------------------------------===//
18: struct EmptyRLEWriter {
19: 	template <class VALUE_TYPE>
20: 	static void Operation(VALUE_TYPE value, rle_count_t count, void *dataptr, bool is_null) {
21: 	}
22: };
23: 
24: template <class T>
25: struct RLEState {
26: 	RLEState() : seen_count(0), last_value(NullValue<T>()), last_seen_count(0), dataptr(nullptr) {
27: 	}
28: 
29: 	idx_t seen_count;
30: 	T last_value;
31: 	rle_count_t last_seen_count;
32: 	void *dataptr;
33: 	bool all_null = true;
34: 
35: public:
36: 	template <class OP>
37: 	void Flush() {
38: 		OP::template Operation<T>(last_value, last_seen_count, dataptr, all_null);
39: 	}
40: 
41: 	template <class OP = EmptyRLEWriter>
42: 	void Update(const T *data, ValidityMask &validity, idx_t idx) {
43: 		if (validity.RowIsValid(idx)) {
44: 			if (all_null) {
45: 				// no value seen yet
46: 				// assign the current value, and increment the seen_count
47: 				// note that we increment last_seen_count rather than setting it to 1
48: 				// this is intentional: this is the first VALID value we see
49: 				// but it might not be the first value in case of nulls!
50: 				last_value = data[idx];
51: 				seen_count++;
52: 				last_seen_count++;
53: 				all_null = false;
54: 			} else if (last_value == data[idx]) {
55: 				// the last value is identical to this value: increment the last_seen_count
56: 				last_seen_count++;
57: 			} else {
58: 				// the values are different
59: 				// issue the callback on the last value
60: 				Flush<OP>();
61: 
62: 				// increment the seen_count and put the new value into the RLE slot
63: 				last_value = data[idx];
64: 				seen_count++;
65: 				last_seen_count = 1;
66: 			}
67: 		} else {
68: 			// NULL value: we merely increment the last_seen_count
69: 			last_seen_count++;
70: 		}
71: 		if (last_seen_count == NumericLimits<rle_count_t>::Maximum()) {
72: 			// we have seen the same value so many times in a row we are at the limit of what fits in our count
73: 			// write away the value and move to the next value
74: 			Flush<OP>();
75: 			last_seen_count = 0;
76: 			seen_count++;
77: 		}
78: 	}
79: };
80: 
81: template <class T>
82: struct RLEAnalyzeState : public AnalyzeState {
83: 	explicit RLEAnalyzeState(const CompressionInfo &info) : AnalyzeState(info) {
84: 	}
85: 
86: 	RLEState<T> state;
87: };
88: 
89: template <class T>
90: unique_ptr<AnalyzeState> RLEInitAnalyze(ColumnData &col_data, PhysicalType type) {
91: 	CompressionInfo info(col_data.GetBlockManager().GetBlockSize());
92: 	return make_uniq<RLEAnalyzeState<T>>(info);
93: }
94: 
95: template <class T>
96: bool RLEAnalyze(AnalyzeState &state, Vector &input, idx_t count) {
97: 	auto &rle_state = state.template Cast<RLEAnalyzeState<T>>();
98: 	UnifiedVectorFormat vdata;
99: 	input.ToUnifiedFormat(count, vdata);
100: 
101: 	auto data = UnifiedVectorFormat::GetData<T>(vdata);
102: 	for (idx_t i = 0; i < count; i++) {
103: 		auto idx = vdata.sel->get_index(i);
104: 		rle_state.state.Update(data, vdata.validity, idx);
105: 	}
106: 	return true;
107: }
108: 
109: template <class T>
110: idx_t RLEFinalAnalyze(AnalyzeState &state) {
111: 	auto &rle_state = state.template Cast<RLEAnalyzeState<T>>();
112: 	return (sizeof(rle_count_t) + sizeof(T)) * rle_state.state.seen_count;
113: }
114: 
115: //===--------------------------------------------------------------------===//
116: // Compress
117: //===--------------------------------------------------------------------===//
118: struct RLEConstants {
119: 	static constexpr const idx_t RLE_HEADER_SIZE = sizeof(uint64_t);
120: };
121: 
122: template <class T, bool WRITE_STATISTICS>
123: struct RLECompressState : public CompressionState {
124: 	struct RLEWriter {
125: 		template <class VALUE_TYPE>
126: 		static void Operation(VALUE_TYPE value, rle_count_t count, void *dataptr, bool is_null) {
127: 			auto state = reinterpret_cast<RLECompressState<T, WRITE_STATISTICS> *>(dataptr);
128: 			state->WriteValue(value, count, is_null);
129: 		}
130: 	};
131: 
132: 	idx_t MaxRLECount() {
133: 		auto entry_size = sizeof(T) + sizeof(rle_count_t);
134: 		return (info.GetBlockSize() - RLEConstants::RLE_HEADER_SIZE) / entry_size;
135: 	}
136: 
137: 	RLECompressState(ColumnDataCheckpointer &checkpointer_p, const CompressionInfo &info)
138: 	    : CompressionState(info), checkpointer(checkpointer_p),
139: 	      function(checkpointer.GetCompressionFunction(CompressionType::COMPRESSION_RLE)) {
140: 		CreateEmptySegment(checkpointer.GetRowGroup().start);
141: 
142: 		state.dataptr = (void *)this;
143: 		max_rle_count = MaxRLECount();
144: 	}
145: 
146: 	void CreateEmptySegment(idx_t row_start) {
147: 		auto &db = checkpointer.GetDatabase();
148: 		auto &type = checkpointer.GetType();
149: 
150: 		auto column_segment =
151: 		    ColumnSegment::CreateTransientSegment(db, type, row_start, info.GetBlockSize(), info.GetBlockSize());
152: 		column_segment->function = function;
153: 		current_segment = std::move(column_segment);
154: 
155: 		auto &buffer_manager = BufferManager::GetBufferManager(db);
156: 		handle = buffer_manager.Pin(current_segment->block);
157: 	}
158: 
159: 	void Append(UnifiedVectorFormat &vdata, idx_t count) {
160: 		auto data = UnifiedVectorFormat::GetData<T>(vdata);
161: 		for (idx_t i = 0; i < count; i++) {
162: 			auto idx = vdata.sel->get_index(i);
163: 			state.template Update<RLECompressState<T, WRITE_STATISTICS>::RLEWriter>(data, vdata.validity, idx);
164: 		}
165: 	}
166: 
167: 	void WriteValue(T value, rle_count_t count, bool is_null) {
168: 		// write the RLE entry
169: 		auto handle_ptr = handle.Ptr() + RLEConstants::RLE_HEADER_SIZE;
170: 		auto data_pointer = reinterpret_cast<T *>(handle_ptr);
171: 		auto index_pointer = reinterpret_cast<rle_count_t *>(handle_ptr + max_rle_count * sizeof(T));
172: 		data_pointer[entry_count] = value;
173: 		index_pointer[entry_count] = count;
174: 		entry_count++;
175: 
176: 		// update meta data
177: 		if (WRITE_STATISTICS && !is_null) {
178: 			current_segment->stats.statistics.UpdateNumericStats<T>(value);
179: 		}
180: 		current_segment->count += count;
181: 
182: 		if (entry_count == max_rle_count) {
183: 			// we have finished writing this segment: flush it and create a new segment
184: 			auto row_start = current_segment->start + current_segment->count;
185: 			FlushSegment();
186: 			CreateEmptySegment(row_start);
187: 			entry_count = 0;
188: 		}
189: 	}
190: 
191: 	void FlushSegment() {
192: 		// flush the segment
193: 		// we compact the segment by moving the counts so they are directly next to the values
194: 		idx_t counts_size = sizeof(rle_count_t) * entry_count;
195: 		idx_t original_rle_offset = RLEConstants::RLE_HEADER_SIZE + max_rle_count * sizeof(T);
196: 		idx_t minimal_rle_offset = AlignValue(RLEConstants::RLE_HEADER_SIZE + sizeof(T) * entry_count);
197: 		idx_t total_segment_size = minimal_rle_offset + counts_size;
198: 		auto data_ptr = handle.Ptr();
199: 		memmove(data_ptr + minimal_rle_offset, data_ptr + original_rle_offset, counts_size);
200: 		// store the final RLE offset within the segment
201: 		Store<uint64_t>(minimal_rle_offset, data_ptr);
202: 		handle.Destroy();
203: 
204: 		auto &state = checkpointer.GetCheckpointState();
205: 		state.FlushSegment(std::move(current_segment), total_segment_size);
206: 	}
207: 
208: 	void Finalize() {
209: 		state.template Flush<RLECompressState<T, WRITE_STATISTICS>::RLEWriter>();
210: 
211: 		FlushSegment();
212: 		current_segment.reset();
213: 	}
214: 
215: 	ColumnDataCheckpointer &checkpointer;
216: 	CompressionFunction &function;
217: 	unique_ptr<ColumnSegment> current_segment;
218: 	BufferHandle handle;
219: 
220: 	RLEState<T> state;
221: 	idx_t entry_count = 0;
222: 	idx_t max_rle_count;
223: };
224: 
225: template <class T, bool WRITE_STATISTICS>
226: unique_ptr<CompressionState> RLEInitCompression(ColumnDataCheckpointer &checkpointer, unique_ptr<AnalyzeState> state) {
227: 	return make_uniq<RLECompressState<T, WRITE_STATISTICS>>(checkpointer, state->info);
228: }
229: 
230: template <class T, bool WRITE_STATISTICS>
231: void RLECompress(CompressionState &state_p, Vector &scan_vector, idx_t count) {
232: 	auto &state = state_p.Cast<RLECompressState<T, WRITE_STATISTICS>>();
233: 	UnifiedVectorFormat vdata;
234: 	scan_vector.ToUnifiedFormat(count, vdata);
235: 
236: 	state.Append(vdata, count);
237: }
238: 
239: template <class T, bool WRITE_STATISTICS>
240: void RLEFinalizeCompress(CompressionState &state_p) {
241: 	auto &state = state_p.Cast<RLECompressState<T, WRITE_STATISTICS>>();
242: 	state.Finalize();
243: }
244: 
245: //===--------------------------------------------------------------------===//
246: // Scan
247: //===--------------------------------------------------------------------===//
248: template <class T>
249: struct RLEScanState : public SegmentScanState {
250: 	explicit RLEScanState(ColumnSegment &segment) {
251: 		auto &buffer_manager = BufferManager::GetBufferManager(segment.db);
252: 		handle = buffer_manager.Pin(segment.block);
253: 		entry_pos = 0;
254: 		position_in_entry = 0;
255: 		rle_count_offset = UnsafeNumericCast<uint32_t>(Load<uint64_t>(handle.Ptr() + segment.GetBlockOffset()));
256: 		D_ASSERT(rle_count_offset <= segment.GetBlockManager().GetBlockSize());
257: 	}
258: 
259: 	void Skip(ColumnSegment &segment, idx_t skip_count) {
260: 		auto data = handle.Ptr() + segment.GetBlockOffset();
261: 		auto index_pointer = reinterpret_cast<rle_count_t *>(data + rle_count_offset);
262: 
263: 		for (idx_t i = 0; i < skip_count; i++) {
264: 			// assign the current value
265: 			position_in_entry++;
266: 			if (position_in_entry >= index_pointer[entry_pos]) {
267: 				// handled all entries in this RLE value
268: 				// move to the next entry
269: 				entry_pos++;
270: 				position_in_entry = 0;
271: 			}
272: 		}
273: 	}
274: 
275: 	BufferHandle handle;
276: 	idx_t entry_pos;
277: 	idx_t position_in_entry;
278: 	uint32_t rle_count_offset;
279: };
280: 
281: template <class T>
282: unique_ptr<SegmentScanState> RLEInitScan(ColumnSegment &segment) {
283: 	auto result = make_uniq<RLEScanState<T>>(segment);
284: 	return std::move(result);
285: }
286: 
287: //===--------------------------------------------------------------------===//
288: // Scan base data
289: //===--------------------------------------------------------------------===//
290: template <class T>
291: void RLESkip(ColumnSegment &segment, ColumnScanState &state, idx_t skip_count) {
292: 	auto &scan_state = state.scan_state->Cast<RLEScanState<T>>();
293: 	scan_state.Skip(segment, skip_count);
294: }
295: 
296: template <bool ENTIRE_VECTOR>
297: static bool CanEmitConstantVector(idx_t position, idx_t run_length, idx_t scan_count) {
298: 	if (!ENTIRE_VECTOR) {
299: 		return false;
300: 	}
301: 	if (scan_count != STANDARD_VECTOR_SIZE) {
302: 		// Only when we can fill an entire Vector can we emit a ConstantVector, because subsequent scans require the
303: 		// input Vector to be flat
304: 		return false;
305: 	}
306: 	D_ASSERT(position < run_length);
307: 	auto remaining_in_run = run_length - position;
308: 	// The amount of values left in this run are equal or greater than the amount of values we need to scan
309: 	return remaining_in_run >= scan_count;
310: }
311: 
312: template <class T>
313: inline static void ForwardToNextRun(RLEScanState<T> &scan_state) {
314: 	// handled all entries in this RLE value
315: 	// move to the next entry
316: 	scan_state.entry_pos++;
317: 	scan_state.position_in_entry = 0;
318: }
319: 
320: template <class T>
321: inline static bool ExhaustedRun(RLEScanState<T> &scan_state, rle_count_t *index_pointer) {
322: 	return scan_state.position_in_entry >= index_pointer[scan_state.entry_pos];
323: }
324: 
325: template <class T>
326: static void RLEScanConstant(RLEScanState<T> &scan_state, rle_count_t *index_pointer, T *data_pointer, idx_t scan_count,
327:                             Vector &result) {
328: 	result.SetVectorType(VectorType::CONSTANT_VECTOR);
329: 	auto result_data = ConstantVector::GetData<T>(result);
330: 	result_data[0] = data_pointer[scan_state.entry_pos];
331: 	scan_state.position_in_entry += scan_count;
332: 	if (ExhaustedRun(scan_state, index_pointer)) {
333: 		ForwardToNextRun(scan_state);
334: 	}
335: 	return;
336: }
337: 
338: template <class T, bool ENTIRE_VECTOR>
339: void RLEScanPartialInternal(ColumnSegment &segment, ColumnScanState &state, idx_t scan_count, Vector &result,
340:                             idx_t result_offset) {
341: 	auto &scan_state = state.scan_state->Cast<RLEScanState<T>>();
342: 
343: 	auto data = scan_state.handle.Ptr() + segment.GetBlockOffset();
344: 	auto data_pointer = reinterpret_cast<T *>(data + RLEConstants::RLE_HEADER_SIZE);
345: 	auto index_pointer = reinterpret_cast<rle_count_t *>(data + scan_state.rle_count_offset);
346: 
347: 	// If we are scanning an entire Vector and it contains only a single run
348: 	if (CanEmitConstantVector<ENTIRE_VECTOR>(scan_state.position_in_entry, index_pointer[scan_state.entry_pos],
349: 	                                         scan_count)) {
350: 		RLEScanConstant<T>(scan_state, index_pointer, data_pointer, scan_count, result);
351: 		return;
352: 	}
353: 
354: 	auto result_data = FlatVector::GetData<T>(result);
355: 	result.SetVectorType(VectorType::FLAT_VECTOR);
356: 	for (idx_t i = 0; i < scan_count; i++) {
357: 		// assign the current value
358: 		result_data[result_offset + i] = data_pointer[scan_state.entry_pos];
359: 		scan_state.position_in_entry++;
360: 		if (ExhaustedRun(scan_state, index_pointer)) {
361: 			ForwardToNextRun(scan_state);
362: 		}
363: 	}
364: }
365: 
366: template <class T>
367: void RLEScanPartial(ColumnSegment &segment, ColumnScanState &state, idx_t scan_count, Vector &result,
368:                     idx_t result_offset) {
369: 	return RLEScanPartialInternal<T, false>(segment, state, scan_count, result, result_offset);
370: }
371: 
372: template <class T>
373: void RLEScan(ColumnSegment &segment, ColumnScanState &state, idx_t scan_count, Vector &result) {
374: 	RLEScanPartialInternal<T, true>(segment, state, scan_count, result, 0);
375: }
376: 
377: //===--------------------------------------------------------------------===//
378: // Fetch
379: //===--------------------------------------------------------------------===//
380: template <class T>
381: void RLEFetchRow(ColumnSegment &segment, ColumnFetchState &state, row_t row_id, Vector &result, idx_t result_idx) {
382: 	RLEScanState<T> scan_state(segment);
383: 	scan_state.Skip(segment, NumericCast<idx_t>(row_id));
384: 
385: 	auto data = scan_state.handle.Ptr() + segment.GetBlockOffset();
386: 	auto data_pointer = reinterpret_cast<T *>(data + RLEConstants::RLE_HEADER_SIZE);
387: 	auto result_data = FlatVector::GetData<T>(result);
388: 	result_data[result_idx] = data_pointer[scan_state.entry_pos];
389: }
390: 
391: //===--------------------------------------------------------------------===//
392: // Get Function
393: //===--------------------------------------------------------------------===//
394: template <class T, bool WRITE_STATISTICS = true>
395: CompressionFunction GetRLEFunction(PhysicalType data_type) {
396: 	return CompressionFunction(CompressionType::COMPRESSION_RLE, data_type, RLEInitAnalyze<T>, RLEAnalyze<T>,
397: 	                           RLEFinalAnalyze<T>, RLEInitCompression<T, WRITE_STATISTICS>,
398: 	                           RLECompress<T, WRITE_STATISTICS>, RLEFinalizeCompress<T, WRITE_STATISTICS>,
399: 	                           RLEInitScan<T>, RLEScan<T>, RLEScanPartial<T>, RLEFetchRow<T>, RLESkip<T>);
400: }
401: 
402: CompressionFunction RLEFun::GetFunction(PhysicalType type) {
403: 	switch (type) {
404: 	case PhysicalType::BOOL:
405: 	case PhysicalType::INT8:
406: 		return GetRLEFunction<int8_t>(type);
407: 	case PhysicalType::INT16:
408: 		return GetRLEFunction<int16_t>(type);
409: 	case PhysicalType::INT32:
410: 		return GetRLEFunction<int32_t>(type);
411: 	case PhysicalType::INT64:
412: 		return GetRLEFunction<int64_t>(type);
413: 	case PhysicalType::INT128:
414: 		return GetRLEFunction<hugeint_t>(type);
415: 	case PhysicalType::UINT128:
416: 		return GetRLEFunction<uhugeint_t>(type);
417: 	case PhysicalType::UINT8:
418: 		return GetRLEFunction<uint8_t>(type);
419: 	case PhysicalType::UINT16:
420: 		return GetRLEFunction<uint16_t>(type);
421: 	case PhysicalType::UINT32:
422: 		return GetRLEFunction<uint32_t>(type);
423: 	case PhysicalType::UINT64:
424: 		return GetRLEFunction<uint64_t>(type);
425: 	case PhysicalType::FLOAT:
426: 		return GetRLEFunction<float>(type);
427: 	case PhysicalType::DOUBLE:
428: 		return GetRLEFunction<double>(type);
429: 	case PhysicalType::LIST:
430: 		return GetRLEFunction<uint64_t, false>(type);
431: 	default:
432: 		throw InternalException("Unsupported type for RLE");
433: 	}
434: }
435: 
436: bool RLEFun::TypeIsSupported(const PhysicalType physical_type) {
437: 	switch (physical_type) {
438: 	case PhysicalType::BOOL:
439: 	case PhysicalType::INT8:
440: 	case PhysicalType::INT16:
441: 	case PhysicalType::INT32:
442: 	case PhysicalType::INT64:
443: 	case PhysicalType::INT128:
444: 	case PhysicalType::UINT8:
445: 	case PhysicalType::UINT16:
446: 	case PhysicalType::UINT32:
447: 	case PhysicalType::UINT64:
448: 	case PhysicalType::UINT128:
449: 	case PhysicalType::FLOAT:
450: 	case PhysicalType::DOUBLE:
451: 	case PhysicalType::LIST:
452: 		return true;
453: 	default:
454: 		return false;
455: 	}
456: }
457: 
458: } // namespace duckdb
[end of src/storage/compression/rle.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: