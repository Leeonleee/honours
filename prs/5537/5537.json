{
  "repo": "duckdb/duckdb",
  "pull_number": 5537,
  "instance_id": "duckdb__duckdb-5537",
  "issue_numbers": [
    "5501",
    "5501"
  ],
  "base_commit": "8948cead74f4fb8bd024791793ed47f22c7263d9",
  "patch": "diff --git a/.github/workflows/LinuxRelease.yml b/.github/workflows/LinuxRelease.yml\nindex 269e949c3444..33aed9f0e063 100644\n--- a/.github/workflows/LinuxRelease.yml\n+++ b/.github/workflows/LinuxRelease.yml\n@@ -545,6 +545,40 @@ jobs:\n       shell: bash\n       run: python3.7 scripts/exported_symbols_check.py build/release/src/libduckdb*.so\n \n+ linux-memory-leaks:\n+    name: Linux Memory Leaks\n+    runs-on: ubuntu-20.04\n+    needs: linux-release-64\n+    env:\n+      GEN: ninja\n+\n+    steps:\n+    - uses: actions/checkout@v3\n+      with:\n+        fetch-depth: 0\n+\n+    - uses: actions/setup-python@v2\n+      with:\n+        python-version: '3.7'\n+\n+    - name: Install Ninja\n+      shell: bash\n+      run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build\n+\n+    - name: Setup Ccache\n+      uses: hendrikmuhs/ccache-action@main\n+      with:\n+        key: ${{ github.job }}\n+\n+    - name: Build\n+      shell: bash\n+      run: make\n+\n+    - name: Test\n+      shell: bash\n+      run: |\n+        python3 test/memoryleak/test_memory_leaks.py\n+\n  linux-httpfs:\n     name: Linux HTTPFS\n     runs-on: ubuntu-20.04\ndiff --git a/src/catalog/catalog_set.cpp b/src/catalog/catalog_set.cpp\nindex ec07f6eca4e3..f0233112457a 100644\n--- a/src/catalog/catalog_set.cpp\n+++ b/src/catalog/catalog_set.cpp\n@@ -10,6 +10,7 @@\n #include \"duckdb/common/string_util.hpp\"\n #include \"duckdb/parser/column_definition.hpp\"\n #include \"duckdb/parser/expression/constant_expression.hpp\"\n+#include \"duckdb/catalog/mapping_value.hpp\"\n \n namespace duckdb {\n \n@@ -23,27 +24,44 @@ namespace duckdb {\n class EntryDropper {\n public:\n \t//! Both constructor and destructor are privates because they should only be called by DropEntryDependencies\n-\texplicit EntryDropper(CatalogSet &catalog_set, idx_t entry_index)\n-\t    : catalog_set(catalog_set), entry_index(entry_index) {\n-\t\told_deleted = catalog_set.entries[entry_index].get()->deleted;\n+\texplicit EntryDropper(EntryIndex &entry_index_p) : entry_index(entry_index_p) {\n+\t\told_deleted = entry_index.GetEntry()->deleted;\n \t}\n \n \t~EntryDropper() {\n-\t\tcatalog_set.entries[entry_index].get()->deleted = old_deleted;\n+\t\tentry_index.GetEntry()->deleted = old_deleted;\n \t}\n \n private:\n-\t//! The current catalog_set\n-\tCatalogSet &catalog_set;\n \t//! Keeps track of the state of the entry before starting the delete\n \tbool old_deleted;\n \t//! Index of entry to be deleted\n-\tidx_t entry_index;\n+\tEntryIndex &entry_index;\n };\n \n CatalogSet::CatalogSet(Catalog &catalog, unique_ptr<DefaultGenerator> defaults)\n     : catalog(catalog), defaults(move(defaults)) {\n }\n+CatalogSet::~CatalogSet() {\n+}\n+\n+EntryIndex CatalogSet::PutEntry(idx_t entry_index, unique_ptr<CatalogEntry> entry) {\n+\tif (entries.find(entry_index) != entries.end()) {\n+\t\tthrow InternalException(\"Entry with entry index \\\"%llu\\\" already exists\", entry_index);\n+\t}\n+\tentries.insert(make_pair(entry_index, EntryValue(move(entry))));\n+\treturn EntryIndex(*this, entry_index);\n+}\n+\n+void CatalogSet::PutEntry(EntryIndex index, unique_ptr<CatalogEntry> catalog_entry) {\n+\tauto entry = entries.find(index.GetIndex());\n+\tif (entry == entries.end()) {\n+\t\tthrow InternalException(\"Entry with entry index \\\"%llu\\\" does not exist\", index.GetIndex());\n+\t}\n+\tcatalog_entry->child = move(entry->second.entry);\n+\tcatalog_entry->child->parent = catalog_entry.get();\n+\tentry->second.entry = move(catalog_entry);\n+}\n \n bool CatalogSet::CreateEntry(ClientContext &context, const string &name, unique_ptr<CatalogEntry> value,\n                              unordered_set<CatalogEntry *> &dependencies) {\n@@ -54,7 +72,7 @@ bool CatalogSet::CreateEntry(ClientContext &context, const string &name, unique_\n \tunique_lock<mutex> read_lock(catalog_lock);\n \n \t// first check if the entry exists in the unordered set\n-\tidx_t entry_index;\n+\tidx_t index;\n \tauto mapping_value = GetMapping(context, name);\n \tif (mapping_value == nullptr || mapping_value->deleted) {\n \t\t// if it does not: entry has never been created\n@@ -68,17 +86,17 @@ bool CatalogSet::CreateEntry(ClientContext &context, const string &name, unique_\n \t\t// first create a dummy deleted entry for this entry\n \t\t// so transactions started before the commit of this transaction don't\n \t\t// see it yet\n-\t\tentry_index = current_entry++;\n \t\tauto dummy_node = make_unique<CatalogEntry>(CatalogType::INVALID, value->catalog, name);\n \t\tdummy_node->timestamp = 0;\n \t\tdummy_node->deleted = true;\n \t\tdummy_node->set = this;\n \n-\t\tentries[entry_index] = move(dummy_node);\n-\t\tPutMapping(context, name, entry_index);\n+\t\tauto entry_index = PutEntry(current_entry++, move(dummy_node));\n+\t\tindex = entry_index.GetIndex();\n+\t\tPutMapping(context, name, move(entry_index));\n \t} else {\n-\t\tentry_index = mapping_value->index;\n-\t\tauto &current = *entries[entry_index];\n+\t\tindex = mapping_value->index.GetIndex();\n+\t\tauto &current = *mapping_value->index.GetEntry();\n \t\t// if it does, we have to check version numbers\n \t\tif (HasConflict(context, current.timestamp)) {\n \t\t\t// current version has been written to by a currently active\n@@ -100,16 +118,16 @@ bool CatalogSet::CreateEntry(ClientContext &context, const string &name, unique_\n \t// now add the dependency set of this object to the dependency manager\n \tcatalog.dependency_manager->AddObject(context, value.get(), dependencies);\n \n-\tvalue->child = move(entries[entry_index]);\n-\tvalue->child->parent = value.get();\n+\tauto value_ptr = value.get();\n+\tEntryIndex entry_index(*this, index);\n+\tPutEntry(move(entry_index), move(value));\n \t// push the old entry in the undo buffer for this transaction\n-\ttransaction.PushCatalogEntry(value->child.get());\n-\tentries[entry_index] = move(value);\n+\ttransaction.PushCatalogEntry(value_ptr->child.get());\n \treturn true;\n }\n \n-bool CatalogSet::GetEntryInternal(ClientContext &context, idx_t entry_index, CatalogEntry *&catalog_entry) {\n-\tcatalog_entry = entries[entry_index].get();\n+bool CatalogSet::GetEntryInternal(ClientContext &context, EntryIndex &entry_index, CatalogEntry *&catalog_entry) {\n+\tcatalog_entry = entry_index.GetEntry().get();\n \t// if it does: we have to retrieve the entry and to check version numbers\n \tif (HasConflict(context, catalog_entry->timestamp)) {\n \t\t// current version has been written to by a currently active\n@@ -125,21 +143,22 @@ bool CatalogSet::GetEntryInternal(ClientContext &context, idx_t entry_index, Cat\n \treturn true;\n }\n \n-bool CatalogSet::GetEntryInternal(ClientContext &context, const string &name, idx_t &entry_index,\n+bool CatalogSet::GetEntryInternal(ClientContext &context, const string &name, EntryIndex *entry_index,\n                                   CatalogEntry *&catalog_entry) {\n \tauto mapping_value = GetMapping(context, name);\n \tif (mapping_value == nullptr || mapping_value->deleted) {\n \t\t// the entry does not exist, check if we can create a default entry\n \t\treturn false;\n \t}\n-\tentry_index = mapping_value->index;\n-\treturn GetEntryInternal(context, entry_index, catalog_entry);\n+\tif (entry_index) {\n+\t\t*entry_index = mapping_value->index.Copy();\n+\t}\n+\treturn GetEntryInternal(context, mapping_value->index, catalog_entry);\n }\n \n bool CatalogSet::AlterOwnership(ClientContext &context, ChangeOwnershipInfo *info) {\n-\tidx_t entry_index;\n \tCatalogEntry *entry;\n-\tif (!GetEntryInternal(context, info->name, entry_index, entry)) {\n+\tif (!GetEntryInternal(context, info->name, nullptr, entry)) {\n \t\treturn false;\n \t}\n \n@@ -159,9 +178,9 @@ bool CatalogSet::AlterEntry(ClientContext &context, const string &name, AlterInf\n \tlock_guard<mutex> write_lock(catalog.write_lock);\n \n \t// first check if the entry exists in the unordered set\n-\tidx_t entry_index;\n+\tEntryIndex entry_index;\n \tCatalogEntry *entry;\n-\tif (!GetEntryInternal(context, name, entry_index, entry)) {\n+\tif (!GetEntryInternal(context, name, &entry_index, entry)) {\n \t\treturn false;\n \t}\n \tif (entry->internal) {\n@@ -184,7 +203,7 @@ bool CatalogSet::AlterEntry(ClientContext &context, const string &name, AlterInf\n \tif (value->name != original_name) {\n \t\tauto mapping_value = GetMapping(context, value->name);\n \t\tif (mapping_value && !mapping_value->deleted) {\n-\t\t\tauto original_entry = GetEntryForTransaction(context, entries[mapping_value->index].get());\n+\t\t\tauto original_entry = GetEntryForTransaction(context, mapping_value->index.GetEntry().get());\n \t\t\tif (!original_entry->deleted) {\n \t\t\t\tentry->UndoAlter(context, alter_info);\n \t\t\t\tstring rename_err_msg =\n@@ -196,25 +215,22 @@ bool CatalogSet::AlterEntry(ClientContext &context, const string &name, AlterInf\n \n \tif (value->name != original_name) {\n \t\t// Do PutMapping and DeleteMapping after dependency check\n-\t\tPutMapping(context, value->name, entry_index);\n+\t\tPutMapping(context, value->name, entry_index.Copy());\n \t\tDeleteMapping(context, original_name);\n \t}\n \n \tvalue->timestamp = transaction.transaction_id;\n-\tvalue->child = move(entries[entry_index]);\n-\tvalue->child->parent = value.get();\n \tvalue->set = this;\n+\tauto new_entry = value.get();\n+\tPutEntry(move(entry_index), move(value));\n \n \t// serialize the AlterInfo into a temporary buffer\n \tBufferedSerializer serializer;\n \talter_info->Serialize(serializer);\n \tBinaryData serialized_alter = serializer.GetData();\n \n-\tauto new_entry = value.get();\n-\n \t// push the old entry in the undo buffer for this transaction\n-\ttransaction.PushCatalogEntry(value->child.get(), serialized_alter.data.get(), serialized_alter.size);\n-\tentries[entry_index] = move(value);\n+\ttransaction.PushCatalogEntry(new_entry->child.get(), serialized_alter.data.get(), serialized_alter.size);\n \n \t// Check the dependency manager to verify that there are no conflicting dependencies with this alter\n \t// Note that we do this AFTER the new entry has been entirely set up in the catalog set\n@@ -225,13 +241,13 @@ bool CatalogSet::AlterEntry(ClientContext &context, const string &name, AlterInf\n \treturn true;\n }\n \n-void CatalogSet::DropEntryDependencies(ClientContext &context, idx_t entry_index, CatalogEntry &entry, bool cascade) {\n-\n+void CatalogSet::DropEntryDependencies(ClientContext &context, EntryIndex &entry_index, CatalogEntry &entry,\n+                                       bool cascade) {\n \t// Stores the deleted value of the entry before starting the process\n-\tEntryDropper dropper(*this, entry_index);\n+\tEntryDropper dropper(entry_index);\n \n \t// To correctly delete the object and its dependencies, it temporarily is set to deleted.\n-\tentries[entry_index].get()->deleted = true;\n+\tentry_index.GetEntry()->deleted = true;\n \n \t// check any dependencies of this object\n \tentry.catalog->dependency_manager->DropObject(context, &entry, cascade);\n@@ -241,7 +257,7 @@ void CatalogSet::DropEntryDependencies(ClientContext &context, idx_t entry_index\n \t// dropper.~EntryDropper()\n }\n \n-void CatalogSet::DropEntryInternal(ClientContext &context, idx_t entry_index, CatalogEntry &entry, bool cascade) {\n+void CatalogSet::DropEntryInternal(ClientContext &context, EntryIndex entry_index, CatalogEntry &entry, bool cascade) {\n \tauto &transaction = Transaction::GetTransaction(context);\n \n \tDropEntryDependencies(context, entry_index, entry, cascade);\n@@ -251,24 +267,22 @@ void CatalogSet::DropEntryInternal(ClientContext &context, idx_t entry_index, Ca\n \t// and point it at the dummy node\n \tauto value = make_unique<CatalogEntry>(CatalogType::DELETED_ENTRY, entry.catalog, entry.name);\n \tvalue->timestamp = transaction.transaction_id;\n-\tvalue->child = move(entries[entry_index]);\n-\tvalue->child->parent = value.get();\n \tvalue->set = this;\n \tvalue->deleted = true;\n+\tauto value_ptr = value.get();\n+\tPutEntry(move(entry_index), move(value));\n \n \t// push the old entry in the undo buffer for this transaction\n-\ttransaction.PushCatalogEntry(value->child.get());\n-\n-\tentries[entry_index] = move(value);\n+\ttransaction.PushCatalogEntry(value_ptr->child.get());\n }\n \n bool CatalogSet::DropEntry(ClientContext &context, const string &name, bool cascade) {\n \t// lock the catalog for writing\n \tlock_guard<mutex> write_lock(catalog.write_lock);\n \t// we can only delete an entry that exists\n-\tidx_t entry_index;\n+\tEntryIndex entry_index;\n \tCatalogEntry *entry;\n-\tif (!GetEntryInternal(context, name, entry_index, entry)) {\n+\tif (!GetEntryInternal(context, name, &entry_index, entry)) {\n \t\treturn false;\n \t}\n \tif (entry->internal) {\n@@ -276,7 +290,7 @@ bool CatalogSet::DropEntry(ClientContext &context, const string &name, bool casc\n \t}\n \n \tlock_guard<mutex> read_lock(catalog_lock);\n-\tDropEntryInternal(context, entry_index, *entry, cascade);\n+\tDropEntryInternal(context, move(entry_index), *entry, cascade);\n \treturn true;\n }\n \n@@ -294,12 +308,10 @@ void CatalogSet::CleanupEntry(CatalogEntry *catalog_entry) {\n \t\tif (parent->deleted && !parent->child && !parent->parent) {\n \t\t\tauto mapping_entry = mapping.find(parent->name);\n \t\t\tD_ASSERT(mapping_entry != mapping.end());\n-\t\t\tauto index = mapping_entry->second->index;\n-\t\t\tauto entry = entries.find(index);\n-\t\t\tD_ASSERT(entry != entries.end());\n-\t\t\tif (entry->second.get() == parent) {\n+\t\t\tauto entry = mapping_entry->second->index.GetEntry().get();\n+\t\t\tD_ASSERT(entry);\n+\t\t\tif (entry == parent) {\n \t\t\t\tmapping.erase(mapping_entry);\n-\t\t\t\tentries.erase(entry);\n \t\t\t}\n \t\t}\n \t}\n@@ -333,9 +345,9 @@ MappingValue *CatalogSet::GetMapping(ClientContext &context, const string &name,\n \treturn mapping_value;\n }\n \n-void CatalogSet::PutMapping(ClientContext &context, const string &name, idx_t entry_index) {\n+void CatalogSet::PutMapping(ClientContext &context, const string &name, EntryIndex entry_index) {\n \tauto entry = mapping.find(name);\n-\tauto new_value = make_unique<MappingValue>(entry_index);\n+\tauto new_value = make_unique<MappingValue>(move(entry_index));\n \tnew_value->timestamp = Transaction::GetTransaction(context).transaction_id;\n \tif (entry != mapping.end()) {\n \t\tif (HasConflict(context, entry->second->timestamp)) {\n@@ -350,7 +362,7 @@ void CatalogSet::PutMapping(ClientContext &context, const string &name, idx_t en\n void CatalogSet::DeleteMapping(ClientContext &context, const string &name) {\n \tauto entry = mapping.find(name);\n \tD_ASSERT(entry != mapping.end());\n-\tauto delete_marker = make_unique<MappingValue>(entry->second->index);\n+\tauto delete_marker = make_unique<MappingValue>(entry->second->index.Copy());\n \tdelete_marker->deleted = true;\n \tdelete_marker->timestamp = Transaction::GetTransaction(context).transaction_id;\n \tdelete_marker->child = move(entry->second);\n@@ -418,15 +430,14 @@ CatalogEntry *CatalogSet::CreateEntryInternal(ClientContext &context, unique_ptr\n \t\treturn nullptr;\n \t}\n \tauto &name = entry->name;\n-\tauto entry_index = current_entry++;\n \tauto catalog_entry = entry.get();\n \n \tentry->set = this;\n \tentry->timestamp = 0;\n \n-\tPutMapping(context, name, entry_index);\n+\tauto entry_index = PutEntry(current_entry++, move(entry));\n+\tPutMapping(context, name, move(entry_index));\n \tmapping[name]->timestamp = 0;\n-\tentries[entry_index] = move(entry);\n \treturn catalog_entry;\n }\n \n@@ -465,7 +476,7 @@ CatalogEntry *CatalogSet::GetEntry(ClientContext &context, const string &name) {\n \t\t// we found an entry for this name\n \t\t// check the version numbers\n \n-\t\tauto catalog_entry = entries[mapping_value->index].get();\n+\t\tauto catalog_entry = mapping_value->index.GetEntry().get();\n \t\tCatalogEntry *current = GetEntryForTransaction(context, catalog_entry);\n \t\tif (current->deleted || (current->name != name && !UseTimestamp(context, mapping_value->timestamp))) {\n \t\t\treturn nullptr;\n@@ -574,7 +585,7 @@ void CatalogSet::Undo(CatalogEntry *entry) {\n \t\t// otherwise we need to update the base entry tables\n \t\tauto &name = entry->name;\n \t\tto_be_removed_node->child->SetAsRoot();\n-\t\tentries[mapping[name]->index] = move(to_be_removed_node->child);\n+\t\tmapping[name]->index.GetEntry() = move(to_be_removed_node->child);\n \t\tentry->parent = nullptr;\n \t}\n \n@@ -589,7 +600,7 @@ void CatalogSet::Undo(CatalogEntry *entry) {\n \t\t}\n \t}\n \t// we mark the catalog as being modified, since this action can lead to e.g. tables being dropped\n-\tentry->catalog->ModifyCatalog();\n+\tcatalog.ModifyCatalog();\n }\n \n void CatalogSet::CreateDefaultEntries(ClientContext &context, unique_lock<mutex> &lock) {\n@@ -622,7 +633,7 @@ void CatalogSet::Scan(ClientContext &context, const std::function<void(CatalogEn\n \tCreateDefaultEntries(context, lock);\n \n \tfor (auto &kv : entries) {\n-\t\tauto entry = kv.second.get();\n+\t\tauto entry = kv.second.entry.get();\n \t\tentry = GetEntryForTransaction(context, entry);\n \t\tif (!entry->deleted) {\n \t\t\tcallback(entry);\n@@ -634,7 +645,7 @@ void CatalogSet::Scan(const std::function<void(CatalogEntry *)> &callback) {\n \t// lock the catalog set\n \tlock_guard<mutex> lock(catalog_lock);\n \tfor (auto &kv : entries) {\n-\t\tauto entry = kv.second.get();\n+\t\tauto entry = kv.second.entry.get();\n \t\tentry = GetCommittedEntry(entry);\n \t\tif (!entry->deleted) {\n \t\t\tcallback(entry);\ndiff --git a/src/catalog/dependency_manager.cpp b/src/catalog/dependency_manager.cpp\nindex a8244d57461c..116d16cbafb9 100644\n--- a/src/catalog/dependency_manager.cpp\n+++ b/src/catalog/dependency_manager.cpp\n@@ -6,6 +6,7 @@\n #include \"duckdb/main/client_context.hpp\"\n #include \"duckdb/main/database.hpp\"\n #include \"duckdb/parser/expression/constant_expression.hpp\"\n+#include \"duckdb/catalog/mapping_value.hpp\"\n \n namespace duckdb {\n \n@@ -16,12 +17,11 @@ void DependencyManager::AddObject(ClientContext &context, CatalogEntry *object,\n                                   unordered_set<CatalogEntry *> &dependencies) {\n \t// check for each object in the sources if they were not deleted yet\n \tfor (auto &dependency : dependencies) {\n-\t\tidx_t entry_index;\n \t\tCatalogEntry *catalog_entry;\n \t\tif (!dependency->set) {\n \t\t\tthrow InternalException(\"Dependency has no set\");\n \t\t}\n-\t\tif (!dependency->set->GetEntryInternal(context, dependency->name, entry_index, catalog_entry)) {\n+\t\tif (!dependency->set->GetEntryInternal(context, dependency->name, nullptr, catalog_entry)) {\n \t\t\tthrow InternalException(\"Dependency has already been deleted?\");\n \t\t}\n \t}\n@@ -49,10 +49,9 @@ void DependencyManager::DropObject(ClientContext &context, CatalogEntry *object,\n \t\tif (mapping_value == nullptr) {\n \t\t\tcontinue;\n \t\t}\n-\t\tidx_t entry_index = mapping_value->index;\n \t\tCatalogEntry *dependency_entry;\n \n-\t\tif (!catalog_set.GetEntryInternal(context, entry_index, dependency_entry)) {\n+\t\tif (!catalog_set.GetEntryInternal(context, mapping_value->index, dependency_entry)) {\n \t\t\t// the dependent object was already deleted, no conflict\n \t\t\tcontinue;\n \t\t}\n@@ -60,7 +59,7 @@ void DependencyManager::DropObject(ClientContext &context, CatalogEntry *object,\n \t\tif (cascade || dep.dependency_type == DependencyType::DEPENDENCY_AUTOMATIC ||\n \t\t    dep.dependency_type == DependencyType::DEPENDENCY_OWNS) {\n \t\t\t// cascade: drop the dependent object\n-\t\t\tcatalog_set.DropEntryInternal(context, entry_index, *dependency_entry, cascade);\n+\t\t\tcatalog_set.DropEntryInternal(context, mapping_value->index.Copy(), *dependency_entry, cascade);\n \t\t} else {\n \t\t\t// no cascade and there are objects that depend on this object: throw error\n \t\t\tthrow DependencyException(\"Cannot drop entry \\\"%s\\\" because there are entries that \"\n@@ -80,9 +79,8 @@ void DependencyManager::AlterObject(ClientContext &context, CatalogEntry *old_ob\n \tfor (auto &dep : dependent_objects) {\n \t\t// look up the entry in the catalog set\n \t\tauto &catalog_set = *dep.entry->set;\n-\t\tidx_t entry_index;\n \t\tCatalogEntry *dependency_entry;\n-\t\tif (!catalog_set.GetEntryInternal(context, dep.entry->name, entry_index, dependency_entry)) {\n+\t\tif (!catalog_set.GetEntryInternal(context, dep.entry->name, nullptr, dependency_entry)) {\n \t\t\t// the dependent object was already deleted, no conflict\n \t\t\tcontinue;\n \t\t}\ndiff --git a/src/execution/physical_plan/plan_create_table.cpp b/src/execution/physical_plan/plan_create_table.cpp\nindex c4d0d1f5dc5b..4ab874b5b626 100644\n--- a/src/execution/physical_plan/plan_create_table.cpp\n+++ b/src/execution/physical_plan/plan_create_table.cpp\n@@ -8,6 +8,7 @@\n #include \"duckdb/main/config.hpp\"\n #include \"duckdb/execution/operator/persistent/physical_batch_insert.hpp\"\n #include \"duckdb/planner/constraints/bound_check_constraint.hpp\"\n+#include \"duckdb/parallel/task_scheduler.hpp\"\n \n namespace duckdb {\n \n@@ -22,13 +23,14 @@ unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalCreateTabl\n \n \t\tbool parallel_streaming_insert = !PreserveInsertionOrder(*plan);\n \t\tbool use_batch_index = UseBatchIndex(*plan);\n+\t\tauto num_threads = TaskScheduler::GetScheduler(context).NumberOfThreads();\n \t\tunique_ptr<PhysicalOperator> create;\n \t\tif (!parallel_streaming_insert && use_batch_index) {\n \t\t\tcreate = make_unique<PhysicalBatchInsert>(op, op.schema, move(op.info), op.estimated_cardinality);\n \n \t\t} else {\n \t\t\tcreate = make_unique<PhysicalInsert>(op, op.schema, move(op.info), op.estimated_cardinality,\n-\t\t\t                                     parallel_streaming_insert);\n+\t\t\t                                     parallel_streaming_insert && num_threads > 1);\n \t\t}\n \n \t\tD_ASSERT(op.children.size() == 1);\ndiff --git a/src/execution/physical_plan/plan_insert.cpp b/src/execution/physical_plan/plan_insert.cpp\nindex 9f6fe5e9197a..3cd94d5df6d4 100644\n--- a/src/execution/physical_plan/plan_insert.cpp\n+++ b/src/execution/physical_plan/plan_insert.cpp\n@@ -53,6 +53,7 @@ unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalInsert &op\n \n \tbool parallel_streaming_insert = !PreserveInsertionOrder(*plan);\n \tbool use_batch_index = UseBatchIndex(*plan);\n+\tauto num_threads = TaskScheduler::GetScheduler(context).NumberOfThreads();\n \tif (op.return_chunk) {\n \t\t// not supported for RETURNING (yet?)\n \t\tparallel_streaming_insert = false;\n@@ -64,7 +65,8 @@ unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalInsert &op\n \t\t                                          op.estimated_cardinality);\n \t} else {\n \t\tinsert = make_unique<PhysicalInsert>(op.types, op.table, op.column_index_map, move(op.bound_defaults),\n-\t\t                                     op.estimated_cardinality, op.return_chunk, parallel_streaming_insert);\n+\t\t                                     op.estimated_cardinality, op.return_chunk,\n+\t\t                                     parallel_streaming_insert && num_threads > 1);\n \t}\n \tif (plan) {\n \t\tinsert->children.push_back(move(plan));\ndiff --git a/src/include/duckdb/catalog/catalog_set.hpp b/src/include/duckdb/catalog/catalog_set.hpp\nindex fb175d8ced8c..1042a783a3ba 100644\n--- a/src/include/duckdb/catalog/catalog_set.hpp\n+++ b/src/include/duckdb/catalog/catalog_set.hpp\n@@ -26,27 +26,46 @@ namespace duckdb {\n struct AlterInfo;\n \n class ClientContext;\n+struct MappingValue;\n+struct EntryIndex;\n \n typedef unordered_map<CatalogSet *, unique_lock<mutex>> set_lock_map_t;\n \n-struct MappingValue {\n-\texplicit MappingValue(idx_t index_) : index(index_), timestamp(0), deleted(false), parent(nullptr) {\n+struct EntryValue {\n+\tEntryValue() {\n+\t\tthrow InternalException(\"EntryValue called without a catalog entry\");\n \t}\n \n-\tidx_t index;\n-\ttransaction_t timestamp;\n-\tbool deleted;\n-\tunique_ptr<MappingValue> child;\n-\tMappingValue *parent;\n+\texplicit EntryValue(unique_ptr<CatalogEntry> entry_p) : entry(move(entry_p)), reference_count(0) {\n+\t}\n+\t//! enable move constructors\n+\tEntryValue(EntryValue &&other) noexcept {\n+\t\tSwap(other);\n+\t}\n+\tEntryValue &operator=(EntryValue &&other) noexcept {\n+\t\tSwap(other);\n+\t\treturn *this;\n+\t}\n+\tvoid Swap(EntryValue &other) {\n+\t\tstd::swap(entry, other.entry);\n+\t\tidx_t count = reference_count;\n+\t\treference_count = other.reference_count.load();\n+\t\tother.reference_count = count;\n+\t}\n+\n+\tunique_ptr<CatalogEntry> entry;\n+\tatomic<idx_t> reference_count;\n };\n \n //! The Catalog Set stores (key, value) map of a set of CatalogEntries\n class CatalogSet {\n \tfriend class DependencyManager;\n \tfriend class EntryDropper;\n+\tfriend struct EntryIndex;\n \n public:\n \tDUCKDB_API explicit CatalogSet(Catalog &catalog, unique_ptr<DefaultGenerator> defaults = nullptr);\n+\t~CatalogSet();\n \n \t//! Create an entry in the catalog set. Returns whether or not it was\n \t//! successful.\n@@ -87,7 +106,6 @@ class CatalogSet {\n \tDUCKDB_API static bool HasConflict(ClientContext &context, transaction_t timestamp);\n \tDUCKDB_API static bool UseTimestamp(ClientContext &context, transaction_t timestamp);\n \n-\tCatalogEntry *GetEntryFromIndex(idx_t index);\n \tvoid UpdateTimestamp(CatalogEntry *entry, transaction_t timestamp);\n \n private:\n@@ -100,29 +118,32 @@ class CatalogSet {\n \t//! Given a root entry, gets the entry valid for this transaction\n \tCatalogEntry *GetEntryForTransaction(ClientContext &context, CatalogEntry *current);\n \tCatalogEntry *GetCommittedEntry(CatalogEntry *current);\n-\tbool GetEntryInternal(ClientContext &context, const string &name, idx_t &entry_index, CatalogEntry *&entry);\n-\tbool GetEntryInternal(ClientContext &context, idx_t entry_index, CatalogEntry *&entry);\n+\tbool GetEntryInternal(ClientContext &context, const string &name, EntryIndex *entry_index, CatalogEntry *&entry);\n+\tbool GetEntryInternal(ClientContext &context, EntryIndex &entry_index, CatalogEntry *&entry);\n \t//! Drops an entry from the catalog set; must hold the catalog_lock to safely call this\n-\tvoid DropEntryInternal(ClientContext &context, idx_t entry_index, CatalogEntry &entry, bool cascade);\n+\tvoid DropEntryInternal(ClientContext &context, EntryIndex entry_index, CatalogEntry &entry, bool cascade);\n \tCatalogEntry *CreateEntryInternal(ClientContext &context, unique_ptr<CatalogEntry> entry);\n \tMappingValue *GetMapping(ClientContext &context, const string &name, bool get_latest = false);\n-\tvoid PutMapping(ClientContext &context, const string &name, idx_t entry_index);\n+\tvoid PutMapping(ClientContext &context, const string &name, EntryIndex entry_index);\n \tvoid DeleteMapping(ClientContext &context, const string &name);\n-\tvoid DropEntryDependencies(ClientContext &context, idx_t entry_index, CatalogEntry &entry, bool cascade);\n+\tvoid DropEntryDependencies(ClientContext &context, EntryIndex &entry_index, CatalogEntry &entry, bool cascade);\n \n \t//! Create all default entries\n \tvoid CreateDefaultEntries(ClientContext &context, unique_lock<mutex> &lock);\n \t//! Attempt to create a default entry with the specified name. Returns the entry if successful, nullptr otherwise.\n \tCatalogEntry *CreateDefaultEntry(ClientContext &context, const string &name, unique_lock<mutex> &lock);\n \n+\tEntryIndex PutEntry(idx_t entry_index, unique_ptr<CatalogEntry> entry);\n+\tvoid PutEntry(EntryIndex index, unique_ptr<CatalogEntry> entry);\n+\n private:\n \tCatalog &catalog;\n \t//! The catalog lock is used to make changes to the data\n \tmutex catalog_lock;\n+\t//! The set of catalog entries\n+\tunordered_map<idx_t, EntryValue> entries;\n \t//! Mapping of string to catalog entry\n \tcase_insensitive_map_t<unique_ptr<MappingValue>> mapping;\n-\t//! The set of catalog entries\n-\tunordered_map<idx_t, unique_ptr<CatalogEntry>> entries;\n \t//! The current catalog entry index\n \tidx_t current_entry = 0;\n \t//! The generator used to generate default internal entries\ndiff --git a/src/include/duckdb/catalog/mapping_value.hpp b/src/include/duckdb/catalog/mapping_value.hpp\nnew file mode 100644\nindex 000000000000..c614bd57f695\n--- /dev/null\n+++ b/src/include/duckdb/catalog/mapping_value.hpp\n@@ -0,0 +1,91 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// duckdb/catalog/mapping_value.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"duckdb/catalog/catalog_entry.hpp\"\n+#include \"duckdb/catalog/catalog_set.hpp\"\n+\n+namespace duckdb {\n+struct AlterInfo;\n+\n+class ClientContext;\n+\n+struct EntryIndex {\n+\tEntryIndex() : catalog(nullptr), index(DConstants::INVALID_INDEX) {\n+\t}\n+\tEntryIndex(CatalogSet &catalog, idx_t index) : catalog(&catalog), index(index) {\n+\t\tauto entry = catalog.entries.find(index);\n+\t\tif (entry == catalog.entries.end()) {\n+\t\t\tthrow InternalException(\"EntryIndex - Catalog entry not found in constructor!?\");\n+\t\t}\n+\t\tcatalog.entries[index].reference_count++;\n+\t}\n+\t~EntryIndex() {\n+\t\tif (!catalog) {\n+\t\t\treturn;\n+\t\t}\n+\t\tauto entry = catalog->entries.find(index);\n+\t\tD_ASSERT(entry != catalog->entries.end());\n+\t\tauto remaining_ref = --entry->second.reference_count;\n+\t\tif (remaining_ref == 0) {\n+\t\t\tcatalog->entries.erase(index);\n+\t\t}\n+\t\tcatalog = nullptr;\n+\t}\n+\t// disable copy constructors\n+\tEntryIndex(const EntryIndex &other) = delete;\n+\tEntryIndex &operator=(const EntryIndex &) = delete;\n+\t//! enable move constructors\n+\tEntryIndex(EntryIndex &&other) noexcept {\n+\t\tcatalog = nullptr;\n+\t\tindex = DConstants::INVALID_INDEX;\n+\t\tstd::swap(catalog, other.catalog);\n+\t\tstd::swap(index, other.index);\n+\t}\n+\tEntryIndex &operator=(EntryIndex &&other) noexcept {\n+\t\tstd::swap(catalog, other.catalog);\n+\t\tstd::swap(index, other.index);\n+\t\treturn *this;\n+\t}\n+\n+\tunique_ptr<CatalogEntry> &GetEntry() {\n+\t\tauto entry = catalog->entries.find(index);\n+\t\tif (entry == catalog->entries.end()) {\n+\t\t\tthrow InternalException(\"EntryIndex - Catalog entry not found!?\");\n+\t\t}\n+\t\treturn entry->second.entry;\n+\t}\n+\tidx_t GetIndex() {\n+\t\treturn index;\n+\t}\n+\tEntryIndex Copy() {\n+\t\tif (catalog) {\n+\t\t\treturn EntryIndex(*catalog, index);\n+\t\t} else {\n+\t\t\treturn EntryIndex();\n+\t\t}\n+\t}\n+\n+private:\n+\tCatalogSet *catalog;\n+\tidx_t index;\n+};\n+\n+struct MappingValue {\n+\texplicit MappingValue(EntryIndex index_p) : index(move(index_p)), timestamp(0), deleted(false), parent(nullptr) {\n+\t}\n+\n+\tEntryIndex index;\n+\ttransaction_t timestamp;\n+\tbool deleted;\n+\tunique_ptr<MappingValue> child;\n+\tMappingValue *parent;\n+};\n+\n+} // namespace duckdb\ndiff --git a/src/storage/buffer_manager.cpp b/src/storage/buffer_manager.cpp\nindex 0ad546b44771..8359446b7b0e 100644\n--- a/src/storage/buffer_manager.cpp\n+++ b/src/storage/buffer_manager.cpp\n@@ -74,6 +74,7 @@ BlockHandle::~BlockHandle() {\n \t} else {\n \t\tD_ASSERT(memory_charge.size == 0);\n \t}\n+\tbuffer_manager.PurgeQueue();\n \tblock_manager.UnregisterBlock(block_id, can_destroy);\n }\n \n@@ -235,7 +236,7 @@ void BufferManager::SetTemporaryDirectory(string new_dir) {\n \n BufferManager::BufferManager(DatabaseInstance &db, string tmp, idx_t maximum_memory)\n     : db(db), current_memory(0), maximum_memory(maximum_memory), temp_directory(move(tmp)),\n-      queue(make_unique<EvictionQueue>()), temporary_id(MAXIMUM_BLOCK),\n+      queue(make_unique<EvictionQueue>()), temporary_id(MAXIMUM_BLOCK), queue_insertions(0),\n       buffer_allocator(BufferAllocatorAllocate, BufferAllocatorFree, BufferAllocatorRealloc,\n                        make_unique<BufferAllocatorData>(*this)) {\n \ttemp_block_manager = make_unique<InMemoryBlockManager>(*this);\ndiff --git a/src/storage/single_file_block_manager.cpp b/src/storage/single_file_block_manager.cpp\nindex aaaa101acd6a..3a3006577be8 100644\n--- a/src/storage/single_file_block_manager.cpp\n+++ b/src/storage/single_file_block_manager.cpp\n@@ -238,6 +238,7 @@ block_id_t SingleFileBlockManager::GetFreeBlockId() {\n void SingleFileBlockManager::MarkBlockAsFree(block_id_t block_id) {\n \tlock_guard<mutex> lock(block_lock);\n \tD_ASSERT(block_id >= 0);\n+\tD_ASSERT(block_id < max_block);\n \tD_ASSERT(free_list.find(block_id) == free_list.end());\n \tmulti_use_blocks.erase(block_id);\n \tfree_list.insert(block_id);\n@@ -246,6 +247,7 @@ void SingleFileBlockManager::MarkBlockAsFree(block_id_t block_id) {\n void SingleFileBlockManager::MarkBlockAsModified(block_id_t block_id) {\n \tlock_guard<mutex> lock(block_lock);\n \tD_ASSERT(block_id >= 0);\n+\tD_ASSERT(block_id < max_block);\n \n \t// check if the block is a multi-use block\n \tauto entry = multi_use_blocks.find(block_id);\n@@ -268,6 +270,8 @@ void SingleFileBlockManager::MarkBlockAsModified(block_id_t block_id) {\n \n void SingleFileBlockManager::IncreaseBlockReferenceCount(block_id_t block_id) {\n \tlock_guard<mutex> lock(block_lock);\n+\tD_ASSERT(block_id >= 0);\n+\tD_ASSERT(block_id < max_block);\n \tD_ASSERT(free_list.find(block_id) == free_list.end());\n \tauto entry = multi_use_blocks.find(block_id);\n \tif (entry != multi_use_blocks.end()) {\ndiff --git a/src/storage/table/column_segment.cpp b/src/storage/table/column_segment.cpp\nindex e06a41279ff8..113266ab66a6 100644\n--- a/src/storage/table/column_segment.cpp\n+++ b/src/storage/table/column_segment.cpp\n@@ -31,7 +31,7 @@ unique_ptr<ColumnSegment> ColumnSegment::CreatePersistentSegment(DatabaseInstanc\n \t\tblock = block_manager.RegisterBlock(block_id);\n \t}\n \tauto segment_size = Storage::BLOCK_SIZE;\n-\treturn make_unique<ColumnSegment>(db, block, type, ColumnSegmentType::PERSISTENT, start, count, function,\n+\treturn make_unique<ColumnSegment>(db, move(block), type, ColumnSegmentType::PERSISTENT, start, count, function,\n \t                                  move(statistics), block_id, offset, segment_size);\n }\n \n@@ -47,7 +47,7 @@ unique_ptr<ColumnSegment> ColumnSegment::CreateTransientSegment(DatabaseInstance\n \t} else {\n \t\tblock = buffer_manager.RegisterMemory(segment_size, false);\n \t}\n-\treturn make_unique<ColumnSegment>(db, block, type, ColumnSegmentType::TRANSIENT, start, 0, function, nullptr,\n+\treturn make_unique<ColumnSegment>(db, move(block), type, ColumnSegmentType::TRANSIENT, start, 0, function, nullptr,\n \t                                  INVALID_BLOCK, 0, segment_size);\n }\n \n",
  "test_patch": "diff --git a/test/CMakeLists.txt b/test/CMakeLists.txt\nindex ad8f5c96f751..f01a8d628a7a 100644\n--- a/test/CMakeLists.txt\n+++ b/test/CMakeLists.txt\n@@ -9,6 +9,7 @@ add_subdirectory(arrow_roundtrip)\n add_subdirectory(common)\n add_subdirectory(extension)\n add_subdirectory(helpers)\n+add_subdirectory(memoryleak)\n add_subdirectory(sql)\n add_subdirectory(sqlite)\n add_subdirectory(ossfuzz)\ndiff --git a/test/include/test_helpers.hpp b/test/include/test_helpers.hpp\nindex 8157da01c46e..09ebc8ef80fe 100644\n--- a/test/include/test_helpers.hpp\n+++ b/test/include/test_helpers.hpp\n@@ -20,6 +20,7 @@ namespace duckdb {\n \n bool TestForceStorage();\n bool TestForceReload();\n+bool TestMemoryLeaks();\n void RegisterSqllogictests();\n \n void DeleteDatabase(string path);\ndiff --git a/test/memoryleak/CMakeLists.txt b/test/memoryleak/CMakeLists.txt\nnew file mode 100644\nindex 000000000000..4d59184b5e20\n--- /dev/null\n+++ b/test/memoryleak/CMakeLists.txt\n@@ -0,0 +1,4 @@\n+add_library_unity(test_memory_leak OBJECT test_temporary_tables.cpp)\n+set(ALL_OBJECT_FILES\n+    ${ALL_OBJECT_FILES} $<TARGET_OBJECTS:test_memory_leak>\n+    PARENT_SCOPE)\ndiff --git a/test/memoryleak/README.md b/test/memoryleak/README.md\nnew file mode 100644\nindex 000000000000..35fae0eebf92\n--- /dev/null\n+++ b/test/memoryleak/README.md\n@@ -0,0 +1,32 @@\n+## Usage\n+\n+Run all memory leak tests:\n+\n+```bash\n+python3 test/memoryleak/test_memory_leaks.py\n+```\n+\n+Run a specific test:\n+\n+```bash\n+python3 test/memoryleak/test_memory_leaks.py --test=\"Test temporary table leaks (#5501)\"\n+```\n+\n+Running tests in the debugger requires passing the `--memory-leak-tests` flag:\n+\n+```bash\n+build/debug/test/unittest \"Test temporary table leaks (#5501)\" --memory-leak-tests\n+```\n+\n+## Description\n+\n+The memory leak folder contains memory leak tests. These are tests that run forever (i.e. they contain a `while true` loop). The tests are disabled unless the `--memory-leak-tests` flag is passed to the test runner.\n+\n+The core idea of the tests is that they perform operations in a loop that should not increase memory of the system (e.g. they might create tables and then drop them again, or create connections and then destroy them).\n+\n+A separate Python script is used to run these tests (`test/memoryleak/test_memory_leaks.py`). The Python script measures the resident set size of the unittest using the `ps` system call.\n+\n+* The script measures memory usage of the test - if the memory usage does not stabilize within the timeout the test is considered a failure.\n+* Stabilized memory usage means that the trend of memory usage has not been going up in the past 10 seconds\n+* The exact threshold of what \"going up\" means is determined by `--threshold-percentage` and `--threshold-absolute`\n+* The timeout is determined by `--timeout`\n\\ No newline at end of file\ndiff --git a/test/memoryleak/test_memory_leaks.py b/test/memoryleak/test_memory_leaks.py\nnew file mode 100644\nindex 000000000000..a0aac2400a8f\n--- /dev/null\n+++ b/test/memoryleak/test_memory_leaks.py\n@@ -0,0 +1,133 @@\n+import subprocess\n+import time\n+import argparse\n+import os\n+\n+parser = argparse.ArgumentParser(description='Runs the memory leak tests')\n+\n+parser.add_argument('--unittest', dest='unittest',\n+                    action='store', help='Path to unittest executable', default='build/release/test/unittest')\n+parser.add_argument('--test', dest='test',\n+                    action='store', help='The name of the tests to run (* is all)', default='*')\n+parser.add_argument('--timeout', dest='timeout',\n+                    action='store', help='The maximum time to run the test and measure memory usage (in seconds)', default=60)\n+parser.add_argument('--threshold-percentage', dest='threshold_percentage',\n+                    action='store', help='The percentage threshold before we consider an increase a regression', default=0.01)\n+parser.add_argument('--threshold-absolute', dest='threshold_absolute',\n+                    action='store', help='The absolute threshold before we consider an increase a regression', default=1000)\n+parser.add_argument('--verbose', dest='verbose',\n+                    action='store', help='Verbose output', default=True)\n+\n+args = parser.parse_args()\n+\n+unittest_program = args.unittest\n+test_filter = args.test\n+test_time = int(args.timeout)\n+verbose = args.verbose\n+measurements_per_second = 1.0\n+\n+# get a list of all unittests\n+proc = subprocess.Popen([unittest_program, '-l', '[memoryleak]'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n+stdout = proc.stdout.read().decode('utf8')\n+stderr = proc.stderr.read().decode('utf8')\n+if proc.returncode is not None and proc.returncode != 0:\n+    print(\"Failed to run program \" + unittest_program)\n+    print(proc.returncode)\n+    print(stdout)\n+    print(stderr)\n+    exit(1)\n+\n+test_cases = []\n+first_line = True\n+for line in stdout.splitlines():\n+    if first_line:\n+        first_line = False\n+        continue\n+    if len(line.strip()) == 0:\n+        continue\n+    splits = line.rsplit('\\t', 1)\n+    if test_filter == '*' or test_filter in splits[0]:\n+        test_cases.append(splits[0])\n+\n+if len(test_cases) == 0:\n+    print(f\"No tests matching filter \\\"{test_filter}\\\" found\")\n+    exit(0)\n+\n+def sizeof_fmt(num, suffix=\"B\"):\n+    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\", \"E\", \"Z\"]:\n+        if abs(num) < 1000.0:\n+            return f\"{num:3.1f}{unit}{suffix}\"\n+        num /= 1000.0\n+    return f\"{num:.1f}Yi{suffix}\"\n+\n+def run_test(test_case):\n+    # launch the unittest program\n+    proc = subprocess.Popen([unittest_program, test_case, '--memory-leak-tests'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n+    pid = proc.pid\n+\n+    # capture the memory output for the duration of the program running\n+    leak = True\n+    rss = []\n+    for i in range(int(test_time * measurements_per_second)):\n+        time.sleep(1.0 / measurements_per_second)\n+        if proc.poll() is not None:\n+            print(\"------------------------------------------------\")\n+            print(\"                    FAILURE                     \")\n+            print(\"------------------------------------------------\")\n+            print(\"                    stdout:                     \")\n+            print(\"------------------------------------------------\")\n+            print(proc.stdout.read().decode('utf8'))\n+            print(\"------------------------------------------------\")\n+            print(\"                    stderr:                     \")\n+            print(\"------------------------------------------------\")\n+            print(proc.stderr.read().decode('utf8'))\n+            exit(1)\n+        ps_proc = subprocess.Popen(f'ps -o rss= -p {pid}'.split(' '), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n+        res = ps_proc.stdout.read().decode('utf8').strip()\n+        rss.append(int(res))\n+        if not has_memory_leak(rss):\n+            leak = False\n+            break\n+\n+    proc.terminate()\n+    if leak:\n+        print(\"------------------------------------------------\")\n+        print(\"                     ERROR                      \")\n+        print(\"------------------------------------------------\")\n+        print(f\"Memory leak detected in test case \\\"{test_case}\\\"\")\n+        print(\"------------------------------------------------\")\n+    elif verbose:\n+        print(\"------------------------------------------------\")\n+        print(\"                    Success!                    \")\n+        print(\"------------------------------------------------\")\n+        print(\"------------------------------------------------\")\n+        print(f\"No memory leaks detected in test case \\\"{test_case}\\\"\")\n+        print(\"------------------------------------------------\")\n+    if verbose or leak:\n+        print(\"Observed memory usages\")\n+        print(\"------------------------------------------------\")\n+        for i in range(len(rss)):\n+            memory = rss[i]\n+            print(f\"{i / measurements_per_second}: {sizeof_fmt(memory)}\")\n+        if leak:\n+            exit(1)\n+\n+\n+def has_memory_leak(rss):\n+    measurement_count = int(measurements_per_second * 10)\n+    if len(rss) <= measurement_count:\n+        # not enough measurements yet\n+        return True\n+    differences = []\n+    for i in range(1, len(rss)):\n+        differences.append(rss[i] - rss[i - 1])\n+    max_memory = max(rss)\n+    sum_differences = sum(differences[-measurement_count:])\n+    return sum_differences > (max_memory * args.threshold_percentage + args.threshold_absolute)\n+\n+try:\n+    for index, test in enumerate(test_cases):\n+        print(f\"[{index}/{len(test_cases)}] {test}\")\n+        run_test(test)\n+finally:\n+    os.system('killall -9 unittest')\ndiff --git a/test/memoryleak/test_temporary_tables.cpp b/test/memoryleak/test_temporary_tables.cpp\nnew file mode 100644\nindex 000000000000..7cb996753d99\n--- /dev/null\n+++ b/test/memoryleak/test_temporary_tables.cpp\n@@ -0,0 +1,78 @@\n+#include \"catch.hpp\"\n+#include \"duckdb/common/file_system.hpp\"\n+#include \"duckdb/storage/buffer_manager.hpp\"\n+#include \"duckdb/storage/storage_info.hpp\"\n+#include \"test_helpers.hpp\"\n+#include \"duckdb/main/client_context.hpp\"\n+#include \"duckdb/main/config.hpp\"\n+\n+using namespace duckdb;\n+using namespace std;\n+\n+TEST_CASE(\"Test in-memory database scanning from tables\", \"[memoryleak]\") {\n+\tif (!TestMemoryLeaks()) {\n+\t\treturn;\n+\t}\n+\tDuckDB db;\n+\tConnection con(db);\n+\tREQUIRE_NO_FAIL(\n+\t    con.Query(\"create table t1 as select i, concat('thisisalongstring', i) s from range(1000000) t(i);\"));\n+\twhile (true) {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"SELECT * FROM t1\"));\n+\t}\n+}\n+\n+TEST_CASE(\"Rollback create table\", \"[memoryleak]\") {\n+\tif (!TestMemoryLeaks()) {\n+\t\treturn;\n+\t}\n+\tDBConfig config;\n+\tconfig.options.load_extensions = false;\n+\tDuckDB db(\":memory:\", &config);\n+\tConnection con(db);\n+\twhile (true) {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"BEGIN\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE t2(i INT);\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ROLLBACK\"));\n+\t}\n+}\n+\n+TEST_CASE(\"DB temporary table insertion\", \"[memoryleak]\") {\n+\tif (!TestMemoryLeaks()) {\n+\t\treturn;\n+\t}\n+\tauto db_path = TestCreatePath(\"memory_leak_temp_table.db\");\n+\tDeleteDatabase(db_path);\n+\n+\tDuckDB db(db_path);\n+\t{\n+\t\tConnection con(db);\n+\t\tREQUIRE_NO_FAIL(con.Query(\"SET threads=8;\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"SET preserve_insertion_order=false;\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"SET force_compression='uncompressed';\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"create table t1 as select i from range(1000000) t(i);\"));\n+\t}\n+\tConnection con(db);\n+\twhile (true) {\n+\t\tREQUIRE_NO_FAIL(con.Query(\"BEGIN\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"CREATE OR REPLACE TEMPORARY TABLE t2(i int)\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"insert into t2 SELECT * FROM t1;\"));\n+\t\tREQUIRE_NO_FAIL(con.Query(\"ROLLBACk\"));\n+\t}\n+}\n+\n+// FIXME: broken right now - we need to flush/merge deletes to fix this\n+// TEST_CASE(\"Insert and delete data repeatedly\", \"[memoryleak]\") {\n+//\tif (!TestMemoryLeaks()) {\n+//\t\treturn;\n+//\t}\n+//\tDBConfig config;\n+//\tconfig.options.load_extensions = false;\n+//\tDuckDB db(\":memory:\", &config);\n+//\tConnection con(db);\n+//\tREQUIRE_NO_FAIL(con.Query(\"CREATE TABLE t1(i INT);\"));\n+//\twhile (true) {\n+//\t\tREQUIRE_NO_FAIL(con.Query(\"INSERT INTO t1 SELECT * FROM range(100000)\"));\n+//\t\tREQUIRE_NO_FAIL(con.Query(\"DELETE FROM t1\"));\n+//\t}\n+//}\ndiff --git a/test/unittest.cpp b/test/unittest.cpp\nindex 16c80c3e0505..e1731d061364 100644\n--- a/test/unittest.cpp\n+++ b/test/unittest.cpp\n@@ -2,6 +2,7 @@\n #include \"catch.hpp\"\n \n #include \"duckdb/common/file_system.hpp\"\n+#include \"duckdb/common/string_util.hpp\"\n #include \"test_helpers.hpp\"\n \n using namespace duckdb;\n@@ -9,6 +10,7 @@ using namespace duckdb;\n namespace duckdb {\n static bool test_force_storage = false;\n static bool test_force_reload = false;\n+static bool test_memory_leaks = false;\n \n bool TestForceStorage() {\n \treturn test_force_storage;\n@@ -18,6 +20,10 @@ bool TestForceReload() {\n \treturn test_force_reload;\n }\n \n+bool TestMemoryLeaks() {\n+\treturn test_memory_leaks;\n+}\n+\n } // namespace duckdb\n \n int main(int argc, char *argv[]) {\n@@ -30,6 +36,9 @@ int main(int argc, char *argv[]) {\n \t\t\ttest_force_storage = true;\n \t\t} else if (string(argv[i]) == \"--force-reload\" || string(argv[i]) == \"--force-restart\") {\n \t\t\ttest_force_reload = true;\n+\t\t} else if (StringUtil::StartsWith(string(argv[i]), \"--memory-leak\") ||\n+\t\t           StringUtil::StartsWith(string(argv[i]), \"--test-memory-leak\")) {\n+\t\t\ttest_memory_leaks = true;\n \t\t} else if (string(argv[i]) == \"--test-dir\") {\n \t\t\ttest_directory = string(argv[++i]);\n \t\t} else {\n",
  "problem_statement": "Some Temp Table Memory Resource May Not Be Released.\n### What happens?\n\nIn file mode, if copy some data from a table stored in db file into a temporary table, some unknown memory resource is not released.\r\n\r\nIt is hard to be noticed in a short time, but certainly be seen in several hours.\n\n### To Reproduce\n\n1. build a db in file mode.\r\n2. create a table T0 and load some table.\r\n3. In an infinite loop:\r\n    3.1 create a connection\r\n    3.2 create a temporary table T1 (\"CREATE TEMP TABLE T1 ...\") which has the same definition with T0.\r\n    3.3 \"INSERT INTO T1 SELECT * FROM T0\"\r\n\r\nAt the end of each loop, the connection is destroyed,  theoretically all resource (including) memory should be released, however, memory increased linearly( Memory usage, which is gotten by \"PRAGMA database_size\" counted by DB keep stable.)\r\n\n\n### OS:\n\ncentos 7\n\n### DuckDB Version:\n\n0.6.0\n\n### DuckDB Client:\n\nC++\n\n### Full Name:\n\nwenfeng\n\n### Affiliation:\n\n--\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\nSome Temp Table Memory Resource May Not Be Released.\n### What happens?\n\nIn file mode, if copy some data from a table stored in db file into a temporary table, some unknown memory resource is not released.\r\n\r\nIt is hard to be noticed in a short time, but certainly be seen in several hours.\n\n### To Reproduce\n\n1. build a db in file mode.\r\n2. create a table T0 and load some table.\r\n3. In an infinite loop:\r\n    3.1 create a connection\r\n    3.2 create a temporary table T1 (\"CREATE TEMP TABLE T1 ...\") which has the same definition with T0.\r\n    3.3 \"INSERT INTO T1 SELECT * FROM T0\"\r\n\r\nAt the end of each loop, the connection is destroyed,  theoretically all resource (including) memory should be released, however, memory increased linearly( Memory usage, which is gotten by \"PRAGMA database_size\" counted by DB keep stable.)\r\n\n\n### OS:\n\ncentos 7\n\n### DuckDB Version:\n\n0.6.0\n\n### DuckDB Client:\n\nC++\n\n### Full Name:\n\nwenfeng\n\n### Affiliation:\n\n--\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "\n",
  "created_at": "2022-11-29T14:28:26Z"
}