{
  "repo": "duckdb/duckdb",
  "pull_number": 7466,
  "instance_id": "duckdb__duckdb-7466",
  "issue_numbers": [
    "7377",
    "7377"
  ],
  "base_commit": "12105f78c12509c7386f7dc9668eaa43339c87b6",
  "patch": "diff --git a/benchmark/tpch/csv/lineitem_csv_gzip.benchmark b/benchmark/tpch/csv/lineitem_csv_gzip.benchmark\nnew file mode 100644\nindex 000000000000..c650cd5f53cc\n--- /dev/null\n+++ b/benchmark/tpch/csv/lineitem_csv_gzip.benchmark\n@@ -0,0 +1,24 @@\n+# name: benchmark/tpch/csv/lineitem_csv_gzip.benchmark\n+# description: Read Lineitem CSV gzipped with auto-detection\n+# group: [csv]\n+\n+name Read Lineitem CSV gzipped with auto-detection\n+group csv\n+\n+require tpch\n+\n+# create the CSV file\n+load\n+CREATE SCHEMA tpch_schema;\n+CALL dbgen(sf=1, schema='tpch_schema');\n+COPY tpch_schema.lineitem TO '${BENCHMARK_DIR}/lineitem.csv.gz' (DELIMITER '|', HEADER);\n+DROP SCHEMA tpch_schema CASCADE;\n+\n+run\n+CREATE OR REPLACE TABLE lineitem AS (SELECT * FROM read_csv_auto(['${BENCHMARK_DIR}/lineitem.csv.gz']));\n+\n+cleanup\n+DROP TABLE IF EXISTS lineitem;\n+\n+result I\n+6001215\ndiff --git a/benchmark/tpch/csv/lineitem_csv_gzip_sample.benchmark b/benchmark/tpch/csv/lineitem_csv_gzip_sample.benchmark\nnew file mode 100644\nindex 000000000000..266fd3fb1198\n--- /dev/null\n+++ b/benchmark/tpch/csv/lineitem_csv_gzip_sample.benchmark\n@@ -0,0 +1,24 @@\n+# name: benchmark/tpch/csv/lineitem_csv_gzip_sample.benchmark\n+# description: Read Lineitem CSV gzipped with auto-detection sampling the entire file\n+# group: [csv]\n+\n+name Read Lineitem CSV gzipped with auto-detection sampling the entire file\n+group csv\n+\n+require tpch\n+\n+# create the CSV file\n+load\n+CREATE SCHEMA tpch_schema;\n+CALL dbgen(sf=1, schema='tpch_schema');\n+COPY tpch_schema.lineitem TO '${BENCHMARK_DIR}/lineitem.csv.gz' (FORMAT CSV, DELIMITER '|', HEADER);\n+DROP SCHEMA tpch_schema CASCADE;\n+\n+run\n+CREATE OR REPLACE TABLE lineitem AS (SELECT * FROM read_csv_auto('${BENCHMARK_DIR}/lineitem.csv.gz', sample_size=-1));\n+\n+cleanup\n+DROP TABLE IF EXISTS lineitem;\n+\n+result I\n+6001215\ndiff --git a/extension/httpfs/s3fs.cpp b/extension/httpfs/s3fs.cpp\nindex 396c6720a8fa..19f093e79d66 100644\n--- a/extension/httpfs/s3fs.cpp\n+++ b/extension/httpfs/s3fs.cpp\n@@ -526,8 +526,9 @@ void S3FileSystem::GetQueryParam(const string &key, string &param, duckdb_httpli\n }\n \n void S3FileSystem::ReadQueryParams(const string &url_query_param, S3AuthParams &params) {\n-\tif (url_query_param.empty())\n+\tif (url_query_param.empty()) {\n \t\treturn;\n+\t}\n \n \tduckdb_httplib_openssl::Params query_params;\n \tduckdb_httplib_openssl::detail::parse_query_text(url_query_param, query_params);\ndiff --git a/src/common/virtual_file_system.cpp b/src/common/virtual_file_system.cpp\nindex c69abd149ddf..aa7bd837efc7 100644\n--- a/src/common/virtual_file_system.cpp\n+++ b/src/common/virtual_file_system.cpp\n@@ -15,6 +15,10 @@ unique_ptr<FileHandle> VirtualFileSystem::OpenFile(const string &path, uint8_t f\n \tif (compression == FileCompressionType::AUTO_DETECT) {\n \t\t// auto detect compression settings based on file name\n \t\tauto lower_path = StringUtil::Lower(path);\n+\t\tif (StringUtil::EndsWith(lower_path, \".tmp\")) {\n+\t\t\t// strip .tmp\n+\t\t\tlower_path = lower_path.substr(0, lower_path.length() - 4);\n+\t\t}\n \t\tif (StringUtil::EndsWith(lower_path, \".gz\")) {\n \t\t\tcompression = FileCompressionType::GZIP;\n \t\t} else if (StringUtil::EndsWith(lower_path, \".zst\")) {\ndiff --git a/src/execution/operator/persistent/CMakeLists.txt b/src/execution/operator/persistent/CMakeLists.txt\nindex a0ecdcadf3cb..d33f348d7ab5 100644\n--- a/src/execution/operator/persistent/CMakeLists.txt\n+++ b/src/execution/operator/persistent/CMakeLists.txt\n@@ -5,6 +5,7 @@ add_library_unity(\n   buffered_csv_reader.cpp\n   parallel_csv_reader.cpp\n   csv_buffer.cpp\n+  csv_file_handle.cpp\n   csv_reader_options.cpp\n   physical_fixed_batch_copy.cpp\n   physical_batch_copy_to_file.cpp\ndiff --git a/src/execution/operator/persistent/base_csv_reader.cpp b/src/execution/operator/persistent/base_csv_reader.cpp\nindex 10b53d265656..1edd6b497bc8 100644\n--- a/src/execution/operator/persistent/base_csv_reader.cpp\n+++ b/src/execution/operator/persistent/base_csv_reader.cpp\n@@ -34,7 +34,7 @@ string BaseCSVReader::GetLineNumberStr(idx_t line_error, bool is_line_estimated,\n \n BaseCSVReader::BaseCSVReader(ClientContext &context_p, BufferedCSVReaderOptions options_p,\n                              const vector<LogicalType> &requested_types)\n-    : context(context_p), fs(FileSystem::GetFileSystem(context)), allocator(Allocator::Get(context)),\n+    : context(context_p), fs(FileSystem::GetFileSystem(context)), allocator(BufferAllocator::Get(context)),\n       options(std::move(options_p)) {\n }\n \n@@ -42,12 +42,7 @@ BaseCSVReader::~BaseCSVReader() {\n }\n \n unique_ptr<CSVFileHandle> BaseCSVReader::OpenCSV(const BufferedCSVReaderOptions &options_p) {\n-\tauto file_handle = fs.OpenFile(options_p.file_path.c_str(), FileFlags::FILE_FLAGS_READ, FileLockType::NO_LOCK,\n-\t                               options_p.compression);\n-\tif (file_handle->CanSeek()) {\n-\t\tfile_handle->Reset();\n-\t}\n-\treturn make_uniq<CSVFileHandle>(std::move(file_handle));\n+\treturn CSVFileHandle::OpenFile(fs, allocator, options_p.file_path, options_p.compression, true);\n }\n \n void BaseCSVReader::InitParseChunk(idx_t num_cols) {\ndiff --git a/src/execution/operator/persistent/buffered_csv_reader.cpp b/src/execution/operator/persistent/buffered_csv_reader.cpp\nindex 784b145c8c81..3f988b2e5a68 100644\n--- a/src/execution/operator/persistent/buffered_csv_reader.cpp\n+++ b/src/execution/operator/persistent/buffered_csv_reader.cpp\n@@ -239,18 +239,13 @@ void BufferedCSVReader::Initialize(const vector<LogicalType> &requested_types) {\n \t\tif (return_types.empty()) {\n \t\t\tthrow InvalidInputException(\"Failed to detect column types from CSV: is the file a valid CSV file?\");\n \t\t}\n-\t\tif (cached_chunks.empty()) {\n-\t\t\tJumpToBeginning(options.skip_rows, options.header);\n-\t\t}\n+\t\tJumpToBeginning(options.skip_rows, options.header);\n \t} else {\n \t\treturn_types = requested_types;\n \t\tResetBuffer();\n \t\tSkipRowsAndReadHeader(options.skip_rows, options.header);\n \t}\n \tInitParseChunk(return_types.size());\n-\t// we only need reset support during the automatic CSV type detection\n-\t// since reset support might require caching (in the case of streams), we disable it for the remainder\n-\tfile_handle->DisableReset();\n }\n \n void BufferedCSVReader::ResetBuffer() {\n@@ -262,13 +257,7 @@ void BufferedCSVReader::ResetBuffer() {\n }\n \n void BufferedCSVReader::ResetStream() {\n-\tif (!file_handle->CanSeek()) {\n-\t\t// seeking to the beginning appears to not be supported in all compiler/os-scenarios,\n-\t\t// so we have to create a new stream source here for now\n-\t\tfile_handle->Reset();\n-\t} else {\n-\t\tfile_handle->Seek(0);\n-\t}\n+\tfile_handle->Reset();\n \tlinenr = 0;\n \tlinenr_estimated = false;\n \tbytes_per_line_avg = 0;\n@@ -332,7 +321,7 @@ bool BufferedCSVReader::JumpToNextSample() {\n \n \t// if we deal with any other sources than plaintext files, jumping_samples can be tricky. In that case\n \t// we just read x continuous chunks from the stream TODO: make jumps possible for zipfiles.\n-\tif (!file_handle->PlainFileSource() || !jumping_samples) {\n+\tif (!file_handle->OnDiskFile() || !jumping_samples) {\n \t\tsample_chunk_idx++;\n \t\treturn true;\n \t}\n@@ -802,21 +791,6 @@ vector<LogicalType> BufferedCSVReader::RefineTypeDetection(const vector<LogicalT\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n-\n-\t\t\tif (!jumping_samples) {\n-\t\t\t\tif ((sample_chunk_idx)*options.sample_chunk_size <= options.buffer_size) {\n-\t\t\t\t\t// cache parse chunk\n-\t\t\t\t\t// create a new chunk and fill it with the remainder\n-\t\t\t\t\tauto chunk = make_uniq<DataChunk>();\n-\t\t\t\t\tauto parse_chunk_types = parse_chunk.GetTypes();\n-\t\t\t\t\tchunk->Move(parse_chunk);\n-\t\t\t\t\tcached_chunks.push(std::move(chunk));\n-\t\t\t\t} else {\n-\t\t\t\t\twhile (!cached_chunks.empty()) {\n-\t\t\t\t\t\tcached_chunks.pop();\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n \t\t}\n \n \t\t// set sql types\n@@ -1474,17 +1448,6 @@ bool BufferedCSVReader::ReadBuffer(idx_t &start, idx_t &line_start) {\n }\n \n void BufferedCSVReader::ParseCSV(DataChunk &insert_chunk) {\n-\t// if no auto-detect or auto-detect with jumping samples, we have nothing cached and start from the beginning\n-\tif (cached_chunks.empty()) {\n-\t\tcached_buffers.clear();\n-\t} else {\n-\t\tauto &chunk = cached_chunks.front();\n-\t\tparse_chunk.Move(*chunk);\n-\t\tcached_chunks.pop();\n-\t\tFlush(insert_chunk);\n-\t\treturn;\n-\t}\n-\n \tstring error_message;\n \tif (!TryParseCSV(ParserMode::PARSING, insert_chunk, error_message)) {\n \t\tthrow InvalidInputException(error_message);\ndiff --git a/src/execution/operator/persistent/csv_file_handle.cpp b/src/execution/operator/persistent/csv_file_handle.cpp\nnew file mode 100644\nindex 000000000000..b319c20bc636\n--- /dev/null\n+++ b/src/execution/operator/persistent/csv_file_handle.cpp\n@@ -0,0 +1,158 @@\n+#include \"duckdb/execution/operator/persistent/csv_file_handle.hpp\"\n+\n+namespace duckdb {\n+\n+CSVFileHandle::CSVFileHandle(FileSystem &fs, Allocator &allocator, unique_ptr<FileHandle> file_handle_p,\n+                             const string &path_p, FileCompressionType compression, bool enable_reset)\n+    : fs(fs), allocator(allocator), file_handle(std::move(file_handle_p)), path(path_p), compression(compression),\n+      reset_enabled(enable_reset) {\n+\tcan_seek = file_handle->CanSeek();\n+\ton_disk_file = file_handle->OnDiskFile();\n+\tfile_size = file_handle->GetFileSize();\n+}\n+\n+unique_ptr<FileHandle> CSVFileHandle::OpenFileHandle(FileSystem &fs, Allocator &allocator, const string &path,\n+                                                     FileCompressionType compression) {\n+\tauto file_handle = fs.OpenFile(path.c_str(), FileFlags::FILE_FLAGS_READ, FileLockType::NO_LOCK, compression);\n+\tif (file_handle->CanSeek()) {\n+\t\tfile_handle->Reset();\n+\t}\n+\treturn file_handle;\n+}\n+\n+unique_ptr<CSVFileHandle> CSVFileHandle::OpenFile(FileSystem &fs, Allocator &allocator, const string &path,\n+                                                  FileCompressionType compression, bool enable_reset) {\n+\tauto file_handle = CSVFileHandle::OpenFileHandle(fs, allocator, path, compression);\n+\treturn make_uniq<CSVFileHandle>(fs, allocator, std::move(file_handle), path, compression, enable_reset);\n+}\n+\n+bool CSVFileHandle::CanSeek() {\n+\treturn can_seek;\n+}\n+\n+void CSVFileHandle::Seek(idx_t position) {\n+\tif (!can_seek) {\n+\t\tthrow InternalException(\"Cannot seek in this file\");\n+\t}\n+\tfile_handle->Seek(position);\n+}\n+\n+idx_t CSVFileHandle::SeekPosition() {\n+\tif (!can_seek) {\n+\t\tthrow InternalException(\"Cannot seek in this file\");\n+\t}\n+\treturn file_handle->SeekPosition();\n+}\n+\n+void CSVFileHandle::Reset() {\n+\trequested_bytes = 0;\n+\tread_position = 0;\n+\tif (can_seek) {\n+\t\t// we can seek - reset the file handle\n+\t\tfile_handle->Reset();\n+\t} else if (on_disk_file) {\n+\t\t// we cannot seek but it is an on-disk file - re-open the file\n+\t\tfile_handle = CSVFileHandle::OpenFileHandle(fs, allocator, path, compression);\n+\t} else {\n+\t\tif (!reset_enabled) {\n+\t\t\tthrow InternalException(\"Reset called but reset is not enabled for this CSV Handle\");\n+\t\t}\n+\t\tread_position = 0;\n+\t}\n+}\n+bool CSVFileHandle::OnDiskFile() {\n+\treturn on_disk_file;\n+}\n+\n+idx_t CSVFileHandle::FileSize() {\n+\treturn file_size;\n+}\n+\n+bool CSVFileHandle::FinishedReading() {\n+\treturn requested_bytes >= file_size;\n+}\n+\n+idx_t CSVFileHandle::Read(void *buffer, idx_t nr_bytes) {\n+\trequested_bytes += nr_bytes;\n+\tif (on_disk_file || can_seek) {\n+\t\t// if this is a plain file source OR we can seek we are not caching anything\n+\t\treturn file_handle->Read(buffer, nr_bytes);\n+\t}\n+\t// not a plain file source: we need to do some bookkeeping around the reset functionality\n+\tidx_t result_offset = 0;\n+\tif (read_position < buffer_size) {\n+\t\t// we need to read from our cached buffer\n+\t\tauto buffer_read_count = MinValue<idx_t>(nr_bytes, buffer_size - read_position);\n+\t\tmemcpy(buffer, cached_buffer.get() + read_position, buffer_read_count);\n+\t\tresult_offset += buffer_read_count;\n+\t\tread_position += buffer_read_count;\n+\t\tif (result_offset == nr_bytes) {\n+\t\t\treturn nr_bytes;\n+\t\t}\n+\t} else if (!reset_enabled && cached_buffer.IsSet()) {\n+\t\t// reset is disabled, but we still have cached data\n+\t\t// we can remove any cached data\n+\t\tcached_buffer.Reset();\n+\t\tbuffer_size = 0;\n+\t\tbuffer_capacity = 0;\n+\t\tread_position = 0;\n+\t}\n+\t// we have data left to read from the file\n+\t// read directly into the buffer\n+\tauto bytes_read = file_handle->Read((char *)buffer + result_offset, nr_bytes - result_offset);\n+\tfile_size = file_handle->GetFileSize();\n+\tread_position += bytes_read;\n+\tif (reset_enabled) {\n+\t\t// if reset caching is enabled, we need to cache the bytes that we have read\n+\t\tif (buffer_size + bytes_read >= buffer_capacity) {\n+\t\t\t// no space; first enlarge the buffer\n+\t\t\tbuffer_capacity = MaxValue<idx_t>(NextPowerOfTwo(buffer_size + bytes_read), buffer_capacity * 2);\n+\n+\t\t\tauto new_buffer = allocator.Allocate(buffer_capacity);\n+\t\t\tif (buffer_size > 0) {\n+\t\t\t\tmemcpy(new_buffer.get(), cached_buffer.get(), buffer_size);\n+\t\t\t}\n+\t\t\tcached_buffer = std::move(new_buffer);\n+\t\t}\n+\t\tmemcpy(cached_buffer.get() + buffer_size, (char *)buffer + result_offset, bytes_read);\n+\t\tbuffer_size += bytes_read;\n+\t}\n+\n+\treturn result_offset + bytes_read;\n+}\n+\n+string CSVFileHandle::ReadLine() {\n+\tbool carriage_return = false;\n+\tstring result;\n+\tchar buffer[1];\n+\twhile (true) {\n+\t\tidx_t bytes_read = Read(buffer, 1);\n+\t\tif (bytes_read == 0) {\n+\t\t\treturn result;\n+\t\t}\n+\t\tif (carriage_return) {\n+\t\t\tif (buffer[0] != '\\n') {\n+\t\t\t\tif (!file_handle->CanSeek()) {\n+\t\t\t\t\tthrow BinderException(\n+\t\t\t\t\t    \"Carriage return newlines not supported when reading CSV files in which we cannot seek\");\n+\t\t\t\t}\n+\t\t\t\tfile_handle->Seek(file_handle->SeekPosition() - 1);\n+\t\t\t\treturn result;\n+\t\t\t}\n+\t\t}\n+\t\tif (buffer[0] == '\\n') {\n+\t\t\treturn result;\n+\t\t}\n+\t\tif (buffer[0] != '\\r') {\n+\t\t\tresult += buffer[0];\n+\t\t} else {\n+\t\t\tcarriage_return = true;\n+\t\t}\n+\t}\n+}\n+\n+void CSVFileHandle::DisableReset() {\n+\tthis->reset_enabled = false;\n+}\n+\n+} // namespace duckdb\ndiff --git a/src/function/table/read_csv.cpp b/src/function/table/read_csv.cpp\nindex a398d1184309..01b5d3a4f5eb 100644\n--- a/src/function/table/read_csv.cpp\n+++ b/src/function/table/read_csv.cpp\n@@ -4,7 +4,6 @@\n #include \"duckdb/main/database.hpp\"\n #include \"duckdb/common/string_util.hpp\"\n #include \"duckdb/common/enum_util.hpp\"\n-#include \"duckdb/common/hive_partitioning.hpp\"\n #include \"duckdb/common/union_by_name.hpp\"\n #include \"duckdb/main/config.hpp\"\n #include \"duckdb/parser/expression/constant_expression.hpp\"\n@@ -15,7 +14,6 @@\n #include \"duckdb/common/multi_file_reader.hpp\"\n #include \"duckdb/main/client_data.hpp\"\n #include \"duckdb/execution/operator/persistent/csv_line_info.hpp\"\n-\n #include <limits>\n \n namespace duckdb {\n@@ -23,11 +21,8 @@ namespace duckdb {\n unique_ptr<CSVFileHandle> ReadCSV::OpenCSV(const string &file_path, FileCompressionType compression,\n                                            ClientContext &context) {\n \tauto &fs = FileSystem::GetFileSystem(context);\n-\tauto file_handle = fs.OpenFile(file_path.c_str(), FileFlags::FILE_FLAGS_READ, FileLockType::NO_LOCK, compression);\n-\tif (file_handle->CanSeek()) {\n-\t\tfile_handle->Reset();\n-\t}\n-\treturn make_uniq<CSVFileHandle>(std::move(file_handle), false);\n+\tauto &allocator = BufferAllocator::Get(context);\n+\treturn CSVFileHandle::OpenFile(fs, allocator, file_path, compression, false);\n }\n \n void ReadCSVData::FinalizeRead(ClientContext &context) {\n@@ -238,14 +233,6 @@ static unique_ptr<FunctionData> ReadCSVBind(ClientContext &context, TableFunctio\n \t} else {\n \t\tresult->reader_bind = MultiFileReader::BindOptions(options.file_options, result->files, return_types, names);\n \t}\n-\tauto &fs = FileSystem::GetFileSystem(context);\n-\tfor (auto &file : result->files) {\n-\t\tif (fs.IsPipe(file)) {\n-\t\t\tresult->is_pipe = true;\n-\t\t\tresult->single_threaded = true;\n-\t\t\tbreak;\n-\t\t}\n-\t}\n \tresult->return_types = return_types;\n \tresult->return_names = names;\n \tresult->FinalizeRead(context);\n@@ -265,7 +252,7 @@ static unique_ptr<FunctionData> ReadCSVAutoBind(ClientContext &context, TableFun\n struct ParallelCSVGlobalState : public GlobalTableFunctionState {\n public:\n \tParallelCSVGlobalState(ClientContext &context, unique_ptr<CSVFileHandle> file_handle_p,\n-\t                       vector<string> &files_path_p, idx_t system_threads_p, idx_t buffer_size_p,\n+\t                       const vector<string> &files_path_p, idx_t system_threads_p, idx_t buffer_size_p,\n \t                       idx_t rows_to_skip, bool force_parallelism_p, vector<column_t> column_ids_p, bool has_header)\n \t    : file_handle(std::move(file_handle_p)), system_threads(system_threads_p), buffer_size(buffer_size_p),\n \t      force_parallelism(force_parallelism_p), column_ids(std::move(column_ids_p)),\n@@ -278,6 +265,7 @@ struct ParallelCSVGlobalState : public GlobalTableFunctionState {\n \t\t}\n \t\tfile_size = file_handle->FileSize();\n \t\tfirst_file_size = file_size;\n+\t\ton_disk_file = file_handle->OnDiskFile();\n \t\tbytes_read = 0;\n \t\tif (buffer_size < file_size || file_size == 0) {\n \t\t\tbytes_per_local_state = buffer_size / ParallelCSVGlobalState::MaxThreads();\n@@ -335,7 +323,7 @@ struct ParallelCSVGlobalState : public GlobalTableFunctionState {\n \n \tbool Finished();\n \n-\tdouble GetProgress(ReadCSVData &bind_data) const {\n+\tdouble GetProgress(const ReadCSVData &bind_data) const {\n \t\tidx_t total_files = bind_data.files.size();\n \n \t\t// get the progress WITHIN the current file\n@@ -369,6 +357,8 @@ struct ParallelCSVGlobalState : public GlobalTableFunctionState {\n \tidx_t bytes_per_local_state;\n \t//! Size of first file\n \tidx_t first_file_size;\n+\t//! Whether or not this is an on-disk file\n+\tbool on_disk_file = true;\n \t//! Basically max number of threads in DuckDB\n \tidx_t system_threads;\n \t//! Size of the buffers\n@@ -402,7 +392,7 @@ struct ParallelCSVGlobalState : public GlobalTableFunctionState {\n };\n \n idx_t ParallelCSVGlobalState::MaxThreads() const {\n-\tif (force_parallelism) {\n+\tif (force_parallelism || !on_disk_file) {\n \t\treturn system_threads;\n \t}\n \tidx_t one_mb = 1000000; // We initialize max one thread per Mb\n@@ -628,7 +618,7 @@ idx_t LineInfo::GetLine(idx_t batch_idx, idx_t line_error, idx_t file_idx, idx_t\n \n static unique_ptr<GlobalTableFunctionState> ParallelCSVInitGlobal(ClientContext &context,\n                                                                   TableFunctionInitInput &input) {\n-\tauto &bind_data = (ReadCSVData &)*input.bind_data;\n+\tauto &bind_data = input.bind_data->CastNoConst<ReadCSVData>();\n \tif (bind_data.files.empty()) {\n \t\t// This can happen when a filename based filter pushdown has eliminated all possible files for this scan.\n \t\treturn make_uniq<ParallelCSVGlobalState>();\n@@ -636,7 +626,15 @@ static unique_ptr<GlobalTableFunctionState> ParallelCSVInitGlobal(ClientContext\n \tunique_ptr<CSVFileHandle> file_handle;\n \n \tbind_data.options.file_path = bind_data.files[0];\n-\tfile_handle = ReadCSV::OpenCSV(bind_data.options.file_path, bind_data.options.compression, context);\n+\n+\tif (bind_data.initial_reader) {\n+\t\tfile_handle = std::move(bind_data.initial_reader->file_handle);\n+\t\tfile_handle->Reset();\n+\t\tfile_handle->DisableReset();\n+\t\tbind_data.initial_reader.reset();\n+\t} else {\n+\t\tfile_handle = ReadCSV::OpenCSV(bind_data.options.file_path, bind_data.options.compression, context);\n+\t}\n \treturn make_uniq<ParallelCSVGlobalState>(\n \t    context, std::move(file_handle), bind_data.files, context.db->NumberOfThreads(), bind_data.options.buffer_size,\n \t    bind_data.options.skip_rows, ClientConfig::GetConfig(context).verify_parallelism, input.column_ids,\n@@ -738,7 +736,7 @@ struct SingleThreadedCSVState : public GlobalTableFunctionState {\n \t\treturn total_files;\n \t}\n \n-\tdouble GetProgress(ReadCSVData &bind_data) const {\n+\tdouble GetProgress(const ReadCSVData &bind_data) const {\n \t\tD_ASSERT(total_files == bind_data.files.size());\n \t\tD_ASSERT(progress_in_files <= total_files * 100);\n \t\treturn (double(progress_in_files) / double(total_files));\n@@ -746,6 +744,16 @@ struct SingleThreadedCSVState : public GlobalTableFunctionState {\n \n \tunique_ptr<BufferedCSVReader> GetCSVReader(ClientContext &context, ReadCSVData &bind_data, idx_t &file_index,\n \t                                           idx_t &total_size) {\n+\t\tauto reader = GetCSVReaderInternal(context, bind_data, file_index, total_size);\n+\t\tif (reader) {\n+\t\t\treader->file_handle->DisableReset();\n+\t\t}\n+\t\treturn reader;\n+\t}\n+\n+private:\n+\tunique_ptr<BufferedCSVReader> GetCSVReaderInternal(ClientContext &context, ReadCSVData &bind_data,\n+\t                                                   idx_t &file_index, idx_t &total_size) {\n \t\tBufferedCSVReaderOptions options;\n \t\t{\n \t\t\tlock_guard<mutex> l(csv_lock);\n@@ -799,14 +807,14 @@ struct SingleThreadedCSVLocalState : public LocalTableFunctionState {\n \n static unique_ptr<GlobalTableFunctionState> SingleThreadedCSVInit(ClientContext &context,\n                                                                   TableFunctionInitInput &input) {\n-\tauto &bind_data = (ReadCSVData &)*input.bind_data;\n+\tauto &bind_data = input.bind_data->CastNoConst<ReadCSVData>();\n \tauto result = make_uniq<SingleThreadedCSVState>(bind_data.files.size());\n \tif (bind_data.files.empty()) {\n \t\t// This can happen when a filename based filter pushdown has eliminated all possible files for this scan.\n \t\treturn std::move(result);\n \t} else {\n \t\tbind_data.options.file_path = bind_data.files[0];\n-\t\tif (bind_data.initial_reader && bind_data.is_pipe) {\n+\t\tif (bind_data.initial_reader) {\n \t\t\t// If this is a pipe and an initial reader already exists due to read_csv_auto\n \t\t\t// We must re-use it, since we can't restart the reader due for it being a pipe.\n \t\t\tresult->initial_reader = std::move(bind_data.initial_reader);\n@@ -904,7 +912,7 @@ static void SingleThreadedCSVFunction(ClientContext &context, TableFunctionInput\n // Read CSV Functions\n //===--------------------------------------------------------------------===//\n static unique_ptr<GlobalTableFunctionState> ReadCSVInitGlobal(ClientContext &context, TableFunctionInitInput &input) {\n-\tauto &bind_data = (ReadCSVData &)*input.bind_data;\n+\tauto &bind_data = input.bind_data->Cast<ReadCSVData>();\n \tif (bind_data.single_threaded) {\n \t\treturn SingleThreadedCSVInit(context, input);\n \t} else {\n@@ -914,7 +922,7 @@ static unique_ptr<GlobalTableFunctionState> ReadCSVInitGlobal(ClientContext &con\n \n unique_ptr<LocalTableFunctionState> ReadCSVInitLocal(ExecutionContext &context, TableFunctionInitInput &input,\n                                                      GlobalTableFunctionState *global_state_p) {\n-\tauto &csv_data = (ReadCSVData &)*input.bind_data;\n+\tauto &csv_data = input.bind_data->Cast<ReadCSVData>();\n \tif (csv_data.single_threaded) {\n \t\treturn SingleThreadedReadCSVInitLocal(context, input, global_state_p);\n \t} else {\n@@ -923,7 +931,7 @@ unique_ptr<LocalTableFunctionState> ReadCSVInitLocal(ExecutionContext &context,\n }\n \n static void ReadCSVFunction(ClientContext &context, TableFunctionInput &data_p, DataChunk &output) {\n-\tauto &bind_data = (ReadCSVData &)*data_p.bind_data;\n+\tauto &bind_data = data_p.bind_data->Cast<ReadCSVData>();\n \tif (bind_data.single_threaded) {\n \t\tSingleThreadedCSVFunction(context, data_p, output);\n \t} else {\n@@ -933,7 +941,7 @@ static void ReadCSVFunction(ClientContext &context, TableFunctionInput &data_p,\n \n static idx_t CSVReaderGetBatchIndex(ClientContext &context, const FunctionData *bind_data_p,\n                                     LocalTableFunctionState *local_state, GlobalTableFunctionState *global_state) {\n-\tauto &bind_data = (ReadCSVData &)*bind_data_p;\n+\tauto &bind_data = bind_data_p->Cast<ReadCSVData>();\n \tif (bind_data.single_threaded) {\n \t\tauto &data = local_state->Cast<SingleThreadedCSVLocalState>();\n \t\treturn data.file_index;\n@@ -980,28 +988,28 @@ static void ReadCSVAddNamedParameters(TableFunction &table_function) {\n \n double CSVReaderProgress(ClientContext &context, const FunctionData *bind_data_p,\n                          const GlobalTableFunctionState *global_state) {\n-\tauto &bind_data = (ReadCSVData &)*bind_data_p;\n+\tauto &bind_data = bind_data_p->Cast<ReadCSVData>();\n \tif (bind_data.single_threaded) {\n-\t\tauto &data = (SingleThreadedCSVState &)*global_state;\n+\t\tauto &data = global_state->Cast<SingleThreadedCSVState>();\n \t\treturn data.GetProgress(bind_data);\n \t} else {\n-\t\tauto &data = (const ParallelCSVGlobalState &)*global_state;\n+\t\tauto &data = global_state->Cast<ParallelCSVGlobalState>();\n \t\treturn data.GetProgress(bind_data);\n \t}\n }\n \n void CSVComplexFilterPushdown(ClientContext &context, LogicalGet &get, FunctionData *bind_data_p,\n                               vector<unique_ptr<Expression>> &filters) {\n-\tauto data = (ReadCSVData *)bind_data_p;\n+\tauto &data = bind_data_p->Cast<ReadCSVData>();\n \tauto reset_reader =\n-\t    MultiFileReader::ComplexFilterPushdown(context, data->files, data->options.file_options, get, filters);\n+\t    MultiFileReader::ComplexFilterPushdown(context, data.files, data.options.file_options, get, filters);\n \tif (reset_reader) {\n-\t\tMultiFileReader::PruneReaders(*data);\n+\t\tMultiFileReader::PruneReaders(data);\n \t}\n }\n \n unique_ptr<NodeStatistics> CSVReaderCardinality(ClientContext &context, const FunctionData *bind_data_p) {\n-\tauto &bind_data = (ReadCSVData &)*bind_data_p;\n+\tauto &bind_data = bind_data_p->Cast<ReadCSVData>();\n \tidx_t per_file_cardinality = 0;\n \tif (bind_data.initial_reader && bind_data.initial_reader->file_handle) {\n \t\tauto estimated_row_width = (bind_data.csv_types.size() * 5);\n@@ -1086,7 +1094,7 @@ void BufferedCSVReaderOptions::Deserialize(FieldReader &reader) {\n }\n \n static void CSVReaderSerialize(FieldWriter &writer, const FunctionData *bind_data_p, const TableFunction &function) {\n-\tauto &bind_data = (ReadCSVData &)*bind_data_p;\n+\tauto &bind_data = bind_data_p->Cast<ReadCSVData>();\n \twriter.WriteList<string>(bind_data.files);\n \twriter.WriteRegularSerializableList<LogicalType>(bind_data.csv_types);\n \twriter.WriteList<string>(bind_data.csv_names);\ndiff --git a/src/include/duckdb/common/allocator.hpp b/src/include/duckdb/common/allocator.hpp\nindex 2b4228b19de1..77edb89826f2 100644\n--- a/src/include/duckdb/common/allocator.hpp\n+++ b/src/include/duckdb/common/allocator.hpp\n@@ -54,6 +54,9 @@ class AllocatedData {\n \tidx_t GetSize() const {\n \t\treturn allocated_size;\n \t}\n+\tbool IsSet() {\n+\t\treturn pointer;\n+\t}\n \tvoid Reset();\n \n private:\ndiff --git a/src/include/duckdb/common/compressed_file_system.hpp b/src/include/duckdb/common/compressed_file_system.hpp\nindex d34a6fa3136d..86a91fb46032 100644\n--- a/src/include/duckdb/common/compressed_file_system.hpp\n+++ b/src/include/duckdb/common/compressed_file_system.hpp\n@@ -59,7 +59,7 @@ class CompressedFileSystem : public FileSystem {\n class CompressedFile : public FileHandle {\n public:\n \tDUCKDB_API CompressedFile(CompressedFileSystem &fs, unique_ptr<FileHandle> child_handle_p, const string &path);\n-\tDUCKDB_API virtual ~CompressedFile() override;\n+\tDUCKDB_API ~CompressedFile() override;\n \n \tCompressedFileSystem &compressed_fs;\n \tunique_ptr<FileHandle> child_handle;\ndiff --git a/src/include/duckdb/common/optional_ptr.hpp b/src/include/duckdb/common/optional_ptr.hpp\nindex a91e333b229a..82b845a8e34c 100644\n--- a/src/include/duckdb/common/optional_ptr.hpp\n+++ b/src/include/duckdb/common/optional_ptr.hpp\n@@ -9,6 +9,7 @@\n #pragma once\n \n #include \"duckdb/common/exception.hpp\"\n+#include \"duckdb/common/unique_ptr.hpp\"\n \n namespace duckdb {\n \ndiff --git a/src/include/duckdb/execution/operator/persistent/base_csv_reader.hpp b/src/include/duckdb/execution/operator/persistent/base_csv_reader.hpp\nindex 0e14ac005344..6b06e353b9f3 100644\n--- a/src/include/duckdb/execution/operator/persistent/base_csv_reader.hpp\n+++ b/src/include/duckdb/execution/operator/persistent/base_csv_reader.hpp\n@@ -61,8 +61,6 @@ class BaseCSVReader {\n \n \tDataChunk parse_chunk;\n \n-\tstd::queue<unique_ptr<DataChunk>> cached_chunks;\n-\n \tParserMode mode;\n \n public:\ndiff --git a/src/include/duckdb/execution/operator/persistent/csv_file_handle.hpp b/src/include/duckdb/execution/operator/persistent/csv_file_handle.hpp\nindex 1e36e8537723..d24f491be18b 100644\n--- a/src/include/duckdb/execution/operator/persistent/csv_file_handle.hpp\n+++ b/src/include/duckdb/execution/operator/persistent/csv_file_handle.hpp\n@@ -11,152 +11,52 @@\n #include \"duckdb/common/file_system.hpp\"\n #include \"duckdb/common/mutex.hpp\"\n #include \"duckdb/common/helper.hpp\"\n+#include \"duckdb/common/allocator.hpp\"\n \n namespace duckdb {\n+class Allocator;\n+class FileSystem;\n \n struct CSVFileHandle {\n public:\n-\texplicit CSVFileHandle(unique_ptr<FileHandle> file_handle_p, bool enable_reset = true)\n-\t    : file_handle(std::move(file_handle_p)), reset_enabled(enable_reset) {\n-\t\tcan_seek = file_handle->CanSeek();\n-\t\tplain_file_source = file_handle->OnDiskFile() && can_seek;\n-\t\tfile_size = file_handle->GetFileSize();\n-\t}\n+\tCSVFileHandle(FileSystem &fs, Allocator &allocator, unique_ptr<FileHandle> file_handle_p, const string &path_p,\n+\t              FileCompressionType compression, bool enable_reset = true);\n \n-\tbool CanSeek() {\n-\t\treturn can_seek;\n-\t}\n-\tvoid Seek(idx_t position) {\n-\t\tif (!can_seek) {\n-\t\t\tthrow InternalException(\"Cannot seek in this file\");\n-\t\t}\n-\t\tfile_handle->Seek(position);\n-\t}\n-\tidx_t SeekPosition() {\n-\t\tif (!can_seek) {\n-\t\t\tthrow InternalException(\"Cannot seek in this file\");\n-\t\t}\n-\t\treturn file_handle->SeekPosition();\n-\t}\n-\tvoid Reset() {\n-\t\tif (plain_file_source) {\n-\t\t\tfile_handle->Reset();\n-\t\t} else {\n-\t\t\tif (!reset_enabled) {\n-\t\t\t\tthrow InternalException(\"Reset called but reset is not enabled for this CSV Handle\");\n-\t\t\t}\n-\t\t\tread_position = 0;\n-\t\t}\n-\t}\n-\tbool PlainFileSource() {\n-\t\treturn plain_file_source;\n-\t}\n-\n-\tbool OnDiskFile() {\n-\t\treturn file_handle->OnDiskFile();\n-\t}\n-\n-\tidx_t FileSize() {\n-\t\treturn file_size;\n-\t}\n+\tmutex main_mutex;\n \n-\tbool FinishedReading() {\n-\t\treturn requested_bytes >= file_size;\n-\t}\n+public:\n+\tbool CanSeek();\n+\tvoid Seek(idx_t position);\n+\tidx_t SeekPosition();\n+\tvoid Reset();\n+\tbool OnDiskFile();\n \n-\tidx_t Read(void *buffer, idx_t nr_bytes) {\n-\t\trequested_bytes += nr_bytes;\n-\t\tif (!plain_file_source) {\n-\t\t\t// not a plain file source: we need to do some bookkeeping around the reset functionality\n-\t\t\tidx_t result_offset = 0;\n-\t\t\tif (read_position < buffer_size) {\n-\t\t\t\t// we need to read from our cached buffer\n-\t\t\t\tauto buffer_read_count = MinValue<idx_t>(nr_bytes, buffer_size - read_position);\n-\t\t\t\tmemcpy(buffer, cached_buffer.get() + read_position, buffer_read_count);\n-\t\t\t\tresult_offset += buffer_read_count;\n-\t\t\t\tread_position += buffer_read_count;\n-\t\t\t\tif (result_offset == nr_bytes) {\n-\t\t\t\t\treturn nr_bytes;\n-\t\t\t\t}\n-\t\t\t} else if (!reset_enabled && cached_buffer) {\n-\t\t\t\t// reset is disabled, but we still have cached data\n-\t\t\t\t// we can remove any cached data\n-\t\t\t\tcached_buffer.reset();\n-\t\t\t\tbuffer_size = 0;\n-\t\t\t\tbuffer_capacity = 0;\n-\t\t\t\tread_position = 0;\n-\t\t\t}\n-\t\t\t// we have data left to read from the file\n-\t\t\t// read directly into the buffer\n-\t\t\tauto bytes_read = file_handle->Read((char *)buffer + result_offset, nr_bytes - result_offset);\n-\t\t\tfile_size = file_handle->GetFileSize();\n-\t\t\tread_position += bytes_read;\n-\t\t\tif (reset_enabled) {\n-\t\t\t\t// if reset caching is enabled, we need to cache the bytes that we have read\n-\t\t\t\tif (buffer_size + bytes_read >= buffer_capacity) {\n-\t\t\t\t\t// no space; first enlarge the buffer\n-\t\t\t\t\tbuffer_capacity = MaxValue<idx_t>(NextPowerOfTwo(buffer_size + bytes_read), buffer_capacity * 2);\n+\tidx_t FileSize();\n \n-\t\t\t\t\tauto new_buffer = make_unsafe_array<data_t>(buffer_capacity);\n-\t\t\t\t\tif (buffer_size > 0) {\n-\t\t\t\t\t\tmemcpy(new_buffer.get(), cached_buffer.get(), buffer_size);\n-\t\t\t\t\t}\n-\t\t\t\t\tcached_buffer = std::move(new_buffer);\n-\t\t\t\t}\n-\t\t\t\tmemcpy(cached_buffer.get() + buffer_size, (char *)buffer + result_offset, bytes_read);\n-\t\t\t\tbuffer_size += bytes_read;\n-\t\t\t}\n+\tbool FinishedReading();\n \n-\t\t\treturn result_offset + bytes_read;\n-\t\t} else {\n-\t\t\treturn file_handle->Read(buffer, nr_bytes);\n-\t\t}\n-\t}\n+\tidx_t Read(void *buffer, idx_t nr_bytes);\n \n-\tstring ReadLine() {\n-\t\tbool carriage_return = false;\n-\t\tstring result;\n-\t\tchar buffer[1];\n-\t\twhile (true) {\n-\t\t\tidx_t bytes_read = Read(buffer, 1);\n-\t\t\tif (bytes_read == 0) {\n-\t\t\t\treturn result;\n-\t\t\t}\n-\t\t\tif (carriage_return) {\n-\t\t\t\tif (buffer[0] != '\\n') {\n-\t\t\t\t\tif (!file_handle->CanSeek()) {\n-\t\t\t\t\t\tthrow BinderException(\n-\t\t\t\t\t\t    \"Carriage return newlines not supported when reading CSV files in which we cannot seek\");\n-\t\t\t\t\t}\n-\t\t\t\t\tfile_handle->Seek(file_handle->SeekPosition() - 1);\n-\t\t\t\t\treturn result;\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tif (buffer[0] == '\\n') {\n-\t\t\t\treturn result;\n-\t\t\t}\n-\t\t\tif (buffer[0] != '\\r') {\n-\t\t\t\tresult += buffer[0];\n-\t\t\t} else {\n-\t\t\t\tcarriage_return = true;\n-\t\t\t}\n-\t\t}\n-\t}\n+\tstring ReadLine();\n+\tvoid DisableReset();\n \n-\tvoid DisableReset() {\n-\t\tthis->reset_enabled = false;\n-\t}\n-\tmutex main_mutex;\n-\tidx_t count = 0;\n+\tstatic unique_ptr<FileHandle> OpenFileHandle(FileSystem &fs, Allocator &allocator, const string &path,\n+\t                                             FileCompressionType compression);\n+\tstatic unique_ptr<CSVFileHandle> OpenFile(FileSystem &fs, Allocator &allocator, const string &path,\n+\t                                          FileCompressionType compression, bool enable_reset);\n \n private:\n+\tFileSystem &fs;\n+\tAllocator &allocator;\n \tunique_ptr<FileHandle> file_handle;\n+\tstring path;\n+\tFileCompressionType compression;\n \tbool reset_enabled = true;\n \tbool can_seek = false;\n-\tbool plain_file_source = false;\n+\tbool on_disk_file = false;\n \tidx_t file_size = 0;\n \t// reset support\n-\tunsafe_array_ptr<data_t> cached_buffer;\n+\tAllocatedData cached_buffer;\n \tidx_t read_position = 0;\n \tidx_t buffer_size = 0;\n \tidx_t buffer_capacity = 0;\ndiff --git a/src/include/duckdb/function/table/read_csv.hpp b/src/include/duckdb/function/table/read_csv.hpp\nindex 89055f58b6c7..99d18a15cc0a 100644\n--- a/src/include/duckdb/function/table/read_csv.hpp\n+++ b/src/include/duckdb/function/table/read_csv.hpp\n@@ -65,7 +65,7 @@ struct ColumnInfo {\n \t\tnames = std::move(names_p);\n \t\ttypes = std::move(types_p);\n \t}\n-\tvoid Serialize(FieldWriter &writer) {\n+\tvoid Serialize(FieldWriter &writer) const {\n \t\twriter.WriteList<string>(names);\n \t\twriter.WriteRegularSerializableList<LogicalType>(types);\n \t}\n@@ -99,8 +99,6 @@ struct ReadCSVData : public BaseCSVData {\n \tbool single_threaded = false;\n \t//! Reader bind data\n \tMultiFileReaderBindData reader_bind;\n-\t//! If any file is a pipe\n-\tbool is_pipe = false;\n \tvector<ColumnInfo> column_info;\n \n \tvoid Initialize(unique_ptr<BufferedCSVReader> &reader) {\ndiff --git a/src/include/duckdb/storage/buffer/buffer_handle.hpp b/src/include/duckdb/storage/buffer/buffer_handle.hpp\nindex 56b9537a849d..ed15fce883be 100644\n--- a/src/include/duckdb/storage/buffer/buffer_handle.hpp\n+++ b/src/include/duckdb/storage/buffer/buffer_handle.hpp\n@@ -9,6 +9,7 @@\n #pragma once\n \n #include \"duckdb/storage/storage_info.hpp\"\n+#include \"duckdb/common/file_buffer.hpp\"\n \n namespace duckdb {\n class BlockHandle;\n@@ -30,9 +31,15 @@ class BufferHandle {\n \t//! Returns whether or not the BufferHandle is valid.\n \tDUCKDB_API bool IsValid() const;\n \t//! Returns a pointer to the buffer data. Handle must be valid.\n-\tDUCKDB_API data_ptr_t Ptr() const;\n+\tinline data_ptr_t Ptr() const {\n+\t\tD_ASSERT(IsValid());\n+\t\treturn node->buffer;\n+\t}\n \t//! Returns a pointer to the buffer data. Handle must be valid.\n-\tDUCKDB_API data_ptr_t Ptr();\n+\tinline data_ptr_t Ptr() {\n+\t\tD_ASSERT(IsValid());\n+\t\treturn node->buffer;\n+\t}\n \t//! Gets the underlying file buffer. Handle must be valid.\n \tDUCKDB_API FileBuffer &GetFileBuffer();\n \t//! Destroys the buffer handle\ndiff --git a/src/storage/buffer/buffer_handle.cpp b/src/storage/buffer/buffer_handle.cpp\nindex 43f57e1974c1..dc3be3f28445 100644\n--- a/src/storage/buffer/buffer_handle.cpp\n+++ b/src/storage/buffer/buffer_handle.cpp\n@@ -7,7 +7,8 @@ namespace duckdb {\n BufferHandle::BufferHandle() : handle(nullptr), node(nullptr) {\n }\n \n-BufferHandle::BufferHandle(shared_ptr<BlockHandle> handle, FileBuffer *node) : handle(std::move(handle)), node(node) {\n+BufferHandle::BufferHandle(shared_ptr<BlockHandle> handle_p, FileBuffer *node_p)\n+    : handle(std::move(handle_p)), node(node_p) {\n }\n \n BufferHandle::BufferHandle(BufferHandle &&other) noexcept {\n@@ -29,16 +30,6 @@ bool BufferHandle::IsValid() const {\n \treturn node != nullptr;\n }\n \n-data_ptr_t BufferHandle::Ptr() const {\n-\tD_ASSERT(IsValid());\n-\treturn node->buffer;\n-}\n-\n-data_ptr_t BufferHandle::Ptr() {\n-\tD_ASSERT(IsValid());\n-\treturn node->buffer;\n-}\n-\n void BufferHandle::Destroy() {\n \tif (!handle || !IsValid()) {\n \t\treturn;\n",
  "test_patch": "diff --git a/test/sql/copy/csv/copy_to_overwrite.test b/test/sql/copy/csv/copy_to_overwrite.test\nnew file mode 100644\nindex 000000000000..b3fb3eed571b\n--- /dev/null\n+++ b/test/sql/copy/csv/copy_to_overwrite.test\n@@ -0,0 +1,67 @@\n+# name: test/sql/copy/csv/copy_to_overwrite.test\n+# description: Test copy to overwriting behavior\n+# group: [csv]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+SET threads=1;\n+\n+# run with and without preservation of insertion order\n+loop i 0 2\n+\n+statement ok\n+COPY (SELECT * FROM range(5) t(i)) TO '__TEST_DIR__/copy_to_overwrite.csv' (HEADER)\n+\n+query I\n+SELECT * FROM '__TEST_DIR__/copy_to_overwrite.csv'\n+----\n+0\n+1\n+2\n+3\n+4\n+\n+statement ok\n+COPY (SELECT * FROM range(5, 10) t(i)) TO '__TEST_DIR__/copy_to_overwrite.csv' (HEADER)\n+\n+query I\n+SELECT * FROM '__TEST_DIR__/copy_to_overwrite.csv'\n+----\n+5\n+6\n+7\n+8\n+9\n+\n+# gzip\n+statement ok\n+COPY (SELECT * FROM range(5) t(i)) TO '__TEST_DIR__/copy_to_overwrite.csv.gz' (HEADER)\n+\n+query I\n+SELECT * FROM '__TEST_DIR__/copy_to_overwrite.csv.gz'\n+----\n+0\n+1\n+2\n+3\n+4\n+\n+statement ok\n+COPY (SELECT * FROM range(5, 10) t(i)) TO '__TEST_DIR__/copy_to_overwrite.csv.gz' (HEADER)\n+\n+query I\n+SELECT * FROM '__TEST_DIR__/copy_to_overwrite.csv.gz'\n+----\n+5\n+6\n+7\n+8\n+9\n+\n+statement ok\n+SET preserve_insertion_order=false\n+\n+endloop\n+\ndiff --git a/test/sql/copy/csv/test_copy_gzip.test b/test/sql/copy/csv/test_copy_gzip.test\nindex c1fe76dda8f0..e4ee7e630790 100644\n--- a/test/sql/copy/csv/test_copy_gzip.test\n+++ b/test/sql/copy/csv/test_copy_gzip.test\n@@ -37,3 +37,15 @@ SELECT l_partkey FROM lineitem WHERE l_orderkey=1 ORDER BY l_linenumber\n 2132\n 24027\n 15635\n+\n+# round trip\n+statement ok\n+COPY lineitem TO '__TEST_DIR__/lineitem1k.csv.gz' (DELIMITER '|', HEADER);\n+\n+statement ok\n+CREATE TABLE lineitem_rt AS FROM '__TEST_DIR__/lineitem1k.csv.gz';\n+\n+query I\n+SELECT COUNT(*) FROM (FROM lineitem EXCEPT FROM lineitem_rt)\n+----\n+0\ndiff --git a/test/sql/copy/s3/README.md b/test/sql/copy/s3/README.md\nindex 4216bacaec80..2c4f07ad8f83 100644\n--- a/test/sql/copy/s3/README.md\n+++ b/test/sql/copy/s3/README.md\n@@ -24,7 +24,7 @@ Then run the test server in the back-ground using Docker. Note that Docker must\n \n \n ```bash\n- ./scripts/run_s3_test_server.sh\n+./scripts/run_s3_test_server.sh\n ```\n \n Now set up the following environment variables to enable running of the tests.\ndiff --git a/test/sql/tpch/gzip_csv_auto_detect.test_slow b/test/sql/tpch/gzip_csv_auto_detect.test_slow\nnew file mode 100644\nindex 000000000000..4998ecc1b13e\n--- /dev/null\n+++ b/test/sql/tpch/gzip_csv_auto_detect.test_slow\n@@ -0,0 +1,27 @@\n+# name: test/sql/tpch/gzip_csv_auto_detect.test_slow\n+# description: Issue #7377 - DuckDB significantly exceeds memory_limit when ingesting .csv.gz file with sample_size=-1\n+# group: [tpch]\n+\n+require tpch\n+\n+# write lineitem to lineitem.csv.gz\n+statement ok\n+CALL dbgen(sf=1);\n+\n+statement ok\n+COPY lineitem TO '__TEST_DIR__/lineitem.csv.gz' (HEADER)\n+\n+load __TEST_DIR__/store_tpch_auto_detect.db\n+\n+# now read the gzip compressed file with sample_size=-1\n+statement ok\n+SET memory_limit='500MB';\n+\n+statement ok\n+CREATE OR REPLACE TABLE lineitem AS (SELECT * FROM read_csv_auto(['__TEST_DIR__/lineitem.csv.gz'], sample_size=-1));\n+\n+# run Q01 to verify the file was loaded correctly\n+query I\n+PRAGMA tpch(1)\n+----\n+<FILE>:extension/tpch/dbgen/answers/sf1/q01.csv\n",
  "problem_statement": "DuckDB significantly exceeds `memory_limit` when ingesting `.csv.gz` file with `sample_size=-1`\n### What happens?\n\nI have a 308MB `.csv.gz` file (2.6GB uncompressed) that I'm ingesting using `read_csv_auto` with the `memory_limit` set to 2GB (but have tested at higher limits as well, observing the same behavior). \r\n\r\nWhen I ingest it using `sample_size=-1`, I'm observing peak memory usage of 10.7GB. After ingestion completes, memory usage settles at 4.4GB, still much higher than the memory limit.\r\n\r\nWhen I ingest it with the default `sample_size`, I'm observing peak memory usage of 2.4GB (slightly higher than the memory limit). After ingestion completes, memory usage settles at 1.6GB, below the memory limit.\r\n\r\nSo this issue covers several bugs, probably related:\r\n\r\n- Using `sample_size=-1` on a `.csv.gz` file consumes significantly more memory than even the uncompressed file size\r\n- Using `sample_size=-1` makes DuckDB significantly exceed the `memory_limit` even after ingestion has completed\r\n- Using `read_csv_auto` moderately exceeds the `memory_limit` even when using the default `sample_size`\n\n### To Reproduce\n\nUnfortunately, I can't share the `.csv.gz` file in question. Here is the sequence of commands I'm running:\r\n\r\n```\r\nroot@ubuntu:~# rm data.db\r\nroot@ubuntu:~# ./duckdb data.db\r\nv0.7.2-dev2740 bb37f3fd06\r\nEnter \".help\" for usage hints.\r\nD SET memory_limit='2GB';\r\nD CREATE OR REPLACE TABLE tmp AS (SELECT * FROM read_csv_auto(['data.csv.gz'], sample_size=-1));\r\n```\r\n\r\nI monitored the memory usage separately using `top`.\r\n\r\nI have experimented with various values for `memory_limit` and `sample_size`, and experienced variants of these bugs in all cases.\n\n### OS:\n\nReproduced on Ubuntu 22.04.2 LTS (x64) and macOS 13.2.1 (aarch64)\n\n### DuckDB Version:\n\nReproduced on v0.7.1 and latest nightly (v0.7.2-dev2740 bb37f3fd06)\n\n### DuckDB Client:\n\nDuckDB CLI\n\n### Full Name:\n\nBenjamin Egelund-M\u00fcller\n\n### Affiliation:\n\nRill Data\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\nDuckDB significantly exceeds `memory_limit` when ingesting `.csv.gz` file with `sample_size=-1`\n### What happens?\n\nI have a 308MB `.csv.gz` file (2.6GB uncompressed) that I'm ingesting using `read_csv_auto` with the `memory_limit` set to 2GB (but have tested at higher limits as well, observing the same behavior). \r\n\r\nWhen I ingest it using `sample_size=-1`, I'm observing peak memory usage of 10.7GB. After ingestion completes, memory usage settles at 4.4GB, still much higher than the memory limit.\r\n\r\nWhen I ingest it with the default `sample_size`, I'm observing peak memory usage of 2.4GB (slightly higher than the memory limit). After ingestion completes, memory usage settles at 1.6GB, below the memory limit.\r\n\r\nSo this issue covers several bugs, probably related:\r\n\r\n- Using `sample_size=-1` on a `.csv.gz` file consumes significantly more memory than even the uncompressed file size\r\n- Using `sample_size=-1` makes DuckDB significantly exceed the `memory_limit` even after ingestion has completed\r\n- Using `read_csv_auto` moderately exceeds the `memory_limit` even when using the default `sample_size`\n\n### To Reproduce\n\nUnfortunately, I can't share the `.csv.gz` file in question. Here is the sequence of commands I'm running:\r\n\r\n```\r\nroot@ubuntu:~# rm data.db\r\nroot@ubuntu:~# ./duckdb data.db\r\nv0.7.2-dev2740 bb37f3fd06\r\nEnter \".help\" for usage hints.\r\nD SET memory_limit='2GB';\r\nD CREATE OR REPLACE TABLE tmp AS (SELECT * FROM read_csv_auto(['data.csv.gz'], sample_size=-1));\r\n```\r\n\r\nI monitored the memory usage separately using `top`.\r\n\r\nI have experimented with various values for `memory_limit` and `sample_size`, and experienced variants of these bugs in all cases.\n\n### OS:\n\nReproduced on Ubuntu 22.04.2 LTS (x64) and macOS 13.2.1 (aarch64)\n\n### DuckDB Version:\n\nReproduced on v0.7.1 and latest nightly (v0.7.2-dev2740 bb37f3fd06)\n\n### DuckDB Client:\n\nDuckDB CLI\n\n### Full Name:\n\nBenjamin Egelund-M\u00fcller\n\n### Affiliation:\n\nRill Data\n\n### Have you tried this on the latest `master` branch?\n\n- [X] I agree\n\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\n\n- [X] I agree\n",
  "hints_text": "\n",
  "created_at": "2023-05-11T18:45:48Z"
}