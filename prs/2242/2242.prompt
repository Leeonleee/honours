You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Exception thrown when inserting and updating a text column in the same row in a transaction
**What does happen?**
If you insert and update a row in the same transaction, it will fail for columns of type 'text' with the error 
"Unsupported type for in-place update", thrown here: 

https://github.com/duckdb/duckdb/blob/33299a4f8583c648a7473e61757fe61f86b5ddbe/src/storage/local_storage.cpp#L274-L300

**What should happen?**
The row should be updated.

**To Reproduce**

```
BEGIN TRANSACTION;
CREATE TABLE test (id INTEGER, name TEXT);
INSERT INTO test VALUES (1, 'Bob');
UPDATE test SET name = 'Alice' Where id = 1;
```

Note that this succeeds if you COMMIT before the UPDATE statement.

**Environment (please complete the following information):**
 - OS: Linux/Ubuntu 20.04
 - DuckDB Version: 0.2.7

Command line and JDBC.

**Before submitting**
- [X] Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?
- [ ] Not tried against master, but an inspection of [local_storage.cpp](https://github.com/duckdb/duckdb/blob/33299a4f8583c648a7473e61757fe61f86b5ddbe/src/storage/local_storage.cpp#L298) suggests that this case remains unimplemented for all types except INT8, INT16, INT32, INT64, FLOAT, and DOUBLE.

</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
2: 
3: ![.github/workflows/main.yml](https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![codecov](https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN)](https://codecov.io/gh/duckdb/duckdb)
6: 
7: 
8: ## Installation
9: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
10: 
11: ## Development
12: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
13: 
14: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
15: 
16: 
[end of README.md]
[start of src/storage/local_storage.cpp]
1: #include "duckdb/transaction/local_storage.hpp"
2: #include "duckdb/execution/index/art/art.hpp"
3: #include "duckdb/storage/table/append_state.hpp"
4: #include "duckdb/storage/write_ahead_log.hpp"
5: #include "duckdb/common/vector_operations/vector_operations.hpp"
6: #include "duckdb/storage/table/row_group.hpp"
7: #include "duckdb/transaction/transaction.hpp"
8: #include "duckdb/planner/table_filter.hpp"
9: 
10: #include "duckdb/storage/table/column_segment.hpp"
11: 
12: namespace duckdb {
13: 
14: LocalTableStorage::LocalTableStorage(DataTable &table) : table(table) {
15: 	Clear();
16: }
17: 
18: LocalTableStorage::~LocalTableStorage() {
19: }
20: 
21: void LocalTableStorage::InitializeScan(LocalScanState &state, TableFilterSet *table_filters) {
22: 	if (collection.ChunkCount() == 0) {
23: 		// nothing to scan
24: 		return;
25: 	}
26: 	state.SetStorage(this);
27: 
28: 	state.chunk_index = 0;
29: 	state.max_index = collection.ChunkCount() - 1;
30: 	state.last_chunk_count = collection.Chunks().back()->size();
31: 	state.table_filters = table_filters;
32: }
33: 
34: idx_t LocalTableStorage::EstimatedSize() {
35: 	idx_t appended_rows = collection.Count() - deleted_rows;
36: 	if (appended_rows == 0) {
37: 		return 0;
38: 	}
39: 	idx_t row_size = 0;
40: 	for (auto &type : collection.Types()) {
41: 		row_size += GetTypeIdSize(type.InternalType());
42: 	}
43: 	return appended_rows * row_size;
44: }
45: 
46: LocalScanState::~LocalScanState() {
47: 	SetStorage(nullptr);
48: }
49: 
50: void LocalScanState::SetStorage(LocalTableStorage *new_storage) {
51: 	if (storage != nullptr) {
52: 		D_ASSERT(storage->active_scans > 0);
53: 		storage->active_scans--;
54: 	}
55: 	storage = new_storage;
56: 	if (storage) {
57: 		storage->active_scans++;
58: 	}
59: }
60: 
61: void LocalTableStorage::Clear() {
62: 	collection.Reset();
63: 	deleted_entries.clear();
64: 	indexes.clear();
65: 	deleted_rows = 0;
66: 	table.info->indexes.Scan([&](Index &index) {
67: 		D_ASSERT(index.type == IndexType::ART);
68: 		auto &art = (ART &)index;
69: 		if (art.is_unique) {
70: 			// unique index: create a local ART index that maintains the same unique constraint
71: 			vector<unique_ptr<Expression>> unbound_expressions;
72: 			for (auto &expr : art.unbound_expressions) {
73: 				unbound_expressions.push_back(expr->Copy());
74: 			}
75: 			indexes.push_back(make_unique<ART>(art.column_ids, move(unbound_expressions), true));
76: 		}
77: 		return false;
78: 	});
79: }
80: 
81: void LocalStorage::InitializeScan(DataTable *table, LocalScanState &state, TableFilterSet *table_filters) {
82: 	auto entry = table_storage.find(table);
83: 	if (entry == table_storage.end()) {
84: 		// no local storage for table: set scan to nullptr
85: 		state.SetStorage(nullptr);
86: 		return;
87: 	}
88: 	auto storage = entry->second.get();
89: 	storage->InitializeScan(state, table_filters);
90: }
91: 
92: void LocalStorage::Scan(LocalScanState &state, const vector<column_t> &column_ids, DataChunk &result) {
93: 	auto storage = state.GetStorage();
94: 	if (!storage || state.chunk_index > state.max_index) {
95: 		// nothing left to scan
96: 		result.Reset();
97: 		return;
98: 	}
99: 	auto &chunk = storage->collection.GetChunk(state.chunk_index);
100: 	idx_t chunk_count = state.chunk_index == state.max_index ? state.last_chunk_count : chunk.size();
101: 	idx_t count = chunk_count;
102: 
103: 	// first create a selection vector from the deleted entries (if any)
104: 	SelectionVector valid_sel(STANDARD_VECTOR_SIZE);
105: 	auto entry = storage->deleted_entries.find(state.chunk_index);
106: 	if (entry != storage->deleted_entries.end()) {
107: 		// deleted entries! create a selection vector
108: 		auto deleted = entry->second.get();
109: 		idx_t new_count = 0;
110: 		for (idx_t i = 0; i < count; i++) {
111: 			if (!deleted[i]) {
112: 				valid_sel.set_index(new_count++, i);
113: 			}
114: 		}
115: 		if (new_count == 0 && count > 0) {
116: 			// all entries in this chunk were deleted: continue to next chunk
117: 			state.chunk_index++;
118: 			Scan(state, column_ids, result);
119: 			return;
120: 		}
121: 		count = new_count;
122: 	}
123: 
124: 	SelectionVector sel;
125: 	if (count != chunk_count) {
126: 		sel.Initialize(valid_sel);
127: 	} else {
128: 		sel.Initialize(FlatVector::INCREMENTAL_SELECTION_VECTOR);
129: 	}
130: 	// now scan the vectors of the chunk
131: 	for (idx_t i = 0; i < column_ids.size(); i++) {
132: 		auto id = column_ids[i];
133: 		if (id == COLUMN_IDENTIFIER_ROW_ID) {
134: 			// row identifier: return a sequence of rowids starting from MAX_ROW_ID plus the row offset in the chunk
135: 			result.data[i].Sequence(MAX_ROW_ID + state.chunk_index * STANDARD_VECTOR_SIZE, 1);
136: 		} else {
137: 			result.data[i].Reference(chunk.data[id]);
138: 		}
139: 		idx_t approved_tuple_count = count;
140: 		if (state.table_filters) {
141: 			auto column_filters = state.table_filters->filters.find(i);
142: 			if (column_filters != state.table_filters->filters.end()) {
143: 				//! We have filters to apply here
144: 				auto &mask = FlatVector::Validity(result.data[i]);
145: 				ColumnSegment::FilterSelection(sel, result.data[i], *column_filters->second, approved_tuple_count,
146: 				                               mask);
147: 				count = approved_tuple_count;
148: 			}
149: 		}
150: 	}
151: 	if (count == 0) {
152: 		// all entries in this chunk were filtered:: Continue on next chunk
153: 		state.chunk_index++;
154: 		Scan(state, column_ids, result);
155: 		return;
156: 	}
157: 	if (count == chunk_count) {
158: 		result.SetCardinality(count);
159: 	} else {
160: 		result.Slice(sel, count);
161: 	}
162: 	state.chunk_index++;
163: }
164: 
165: void LocalStorage::Append(DataTable *table, DataChunk &chunk) {
166: 	auto entry = table_storage.find(table);
167: 	LocalTableStorage *storage;
168: 	if (entry == table_storage.end()) {
169: 		auto new_storage = make_unique<LocalTableStorage>(*table);
170: 		storage = new_storage.get();
171: 		table_storage.insert(make_pair(table, move(new_storage)));
172: 	} else {
173: 		storage = entry->second.get();
174: 	}
175: 	// append to unique indices (if any)
176: 	if (!storage->indexes.empty()) {
177: 		idx_t base_id = MAX_ROW_ID + storage->collection.Count();
178: 
179: 		// first generate the vector of row identifiers
180: 		Vector row_ids(LOGICAL_ROW_TYPE);
181: 		VectorOperations::GenerateSequence(row_ids, chunk.size(), base_id, 1);
182: 
183: 		// now append the entries to the indices
184: 		for (auto &index : storage->indexes) {
185: 			if (!index->Append(chunk, row_ids)) {
186: 				throw ConstraintException("PRIMARY KEY or UNIQUE constraint violated: duplicated key");
187: 			}
188: 		}
189: 	}
190: 	//! Append to the chunk
191: 	storage->collection.Append(chunk);
192: 	if (storage->active_scans == 0 && storage->collection.Count() >= RowGroup::ROW_GROUP_SIZE * 2) {
193: 		// flush to base storage
194: 		Flush(*table, *storage);
195: 	}
196: }
197: 
198: LocalTableStorage *LocalStorage::GetStorage(DataTable *table) {
199: 	auto entry = table_storage.find(table);
200: 	D_ASSERT(entry != table_storage.end());
201: 	return entry->second.get();
202: }
203: 
204: idx_t LocalStorage::EstimatedSize() {
205: 	idx_t estimated_size = 0;
206: 	for (auto &storage : table_storage) {
207: 		estimated_size += storage.second->EstimatedSize();
208: 	}
209: 	return estimated_size;
210: }
211: 
212: static idx_t GetChunk(Vector &row_ids) {
213: 	auto ids = FlatVector::GetData<row_t>(row_ids);
214: 	auto first_id = ids[0] - MAX_ROW_ID;
215: 
216: 	return first_id / STANDARD_VECTOR_SIZE;
217: }
218: 
219: idx_t LocalStorage::Delete(DataTable *table, Vector &row_ids, idx_t count) {
220: 	auto storage = GetStorage(table);
221: 	// figure out the chunk from which these row ids came
222: 	idx_t chunk_idx = GetChunk(row_ids);
223: 	D_ASSERT(chunk_idx < storage->collection.ChunkCount());
224: 
225: 	// get a pointer to the deleted entries for this chunk
226: 	bool *deleted;
227: 	auto entry = storage->deleted_entries.find(chunk_idx);
228: 	if (entry == storage->deleted_entries.end()) {
229: 		// nothing deleted yet, add the deleted entries
230: 		auto del_entries = unique_ptr<bool[]>(new bool[STANDARD_VECTOR_SIZE]);
231: 		memset(del_entries.get(), 0, sizeof(bool) * STANDARD_VECTOR_SIZE);
232: 		deleted = del_entries.get();
233: 		storage->deleted_entries.insert(make_pair(chunk_idx, move(del_entries)));
234: 	} else {
235: 		deleted = entry->second.get();
236: 	}
237: 
238: 	// now actually mark the entries as deleted in the deleted vector
239: 	idx_t base_index = MAX_ROW_ID + chunk_idx * STANDARD_VECTOR_SIZE;
240: 
241: 	idx_t deleted_count = 0;
242: 	auto ids = FlatVector::GetData<row_t>(row_ids);
243: 	for (idx_t i = 0; i < count; i++) {
244: 		auto id = ids[i] - base_index;
245: 		if (!deleted[id]) {
246: 			deleted_count++;
247: 		}
248: 		deleted[id] = true;
249: 	}
250: 	storage->deleted_rows += deleted_count;
251: 	return deleted_count;
252: }
253: 
254: template <class T>
255: static void TemplatedUpdateLoop(Vector &data_vector, Vector &update_vector, Vector &row_ids, idx_t count,
256:                                 idx_t base_index) {
257: 	VectorData udata;
258: 	update_vector.Orrify(count, udata);
259: 
260: 	auto target = FlatVector::GetData<T>(data_vector);
261: 	auto &mask = FlatVector::Validity(data_vector);
262: 	auto ids = FlatVector::GetData<row_t>(row_ids);
263: 	auto updates = (T *)udata.data;
264: 
265: 	for (idx_t i = 0; i < count; i++) {
266: 		auto uidx = udata.sel->get_index(i);
267: 
268: 		auto id = ids[i] - base_index;
269: 		target[id] = updates[uidx];
270: 		mask.Set(id, udata.validity.RowIsValid(uidx));
271: 	}
272: }
273: 
274: static void UpdateChunk(Vector &data, Vector &updates, Vector &row_ids, idx_t count, idx_t base_index) {
275: 	D_ASSERT(data.GetType() == updates.GetType());
276: 	D_ASSERT(row_ids.GetType() == LOGICAL_ROW_TYPE);
277: 
278: 	switch (data.GetType().InternalType()) {
279: 	case PhysicalType::INT8:
280: 		TemplatedUpdateLoop<int8_t>(data, updates, row_ids, count, base_index);
281: 		break;
282: 	case PhysicalType::INT16:
283: 		TemplatedUpdateLoop<int16_t>(data, updates, row_ids, count, base_index);
284: 		break;
285: 	case PhysicalType::INT32:
286: 		TemplatedUpdateLoop<int32_t>(data, updates, row_ids, count, base_index);
287: 		break;
288: 	case PhysicalType::INT64:
289: 		TemplatedUpdateLoop<int64_t>(data, updates, row_ids, count, base_index);
290: 		break;
291: 	case PhysicalType::FLOAT:
292: 		TemplatedUpdateLoop<float>(data, updates, row_ids, count, base_index);
293: 		break;
294: 	case PhysicalType::DOUBLE:
295: 		TemplatedUpdateLoop<double>(data, updates, row_ids, count, base_index);
296: 		break;
297: 	default:
298: 		throw Exception("Unsupported type for in-place update");
299: 	}
300: }
301: 
302: void LocalStorage::Update(DataTable *table, Vector &row_ids, const vector<column_t> &column_ids, DataChunk &data) {
303: 	auto storage = GetStorage(table);
304: 	// figure out the chunk from which these row ids came
305: 	idx_t chunk_idx = GetChunk(row_ids);
306: 	D_ASSERT(chunk_idx < storage->collection.ChunkCount());
307: 
308: 	idx_t base_index = MAX_ROW_ID + chunk_idx * STANDARD_VECTOR_SIZE;
309: 
310: 	// now perform the actual update
311: 	auto &chunk = storage->collection.GetChunk(chunk_idx);
312: 	for (idx_t i = 0; i < column_ids.size(); i++) {
313: 		auto col_idx = column_ids[i];
314: 		UpdateChunk(chunk.data[col_idx], data.data[i], row_ids, data.size(), base_index);
315: 	}
316: }
317: 
318: template <class T>
319: bool LocalStorage::ScanTableStorage(DataTable &table, LocalTableStorage &storage, T &&fun) {
320: 	vector<column_t> column_ids;
321: 	for (idx_t i = 0; i < table.types.size(); i++) {
322: 		column_ids.push_back(i);
323: 	}
324: 
325: 	DataChunk chunk;
326: 	chunk.Initialize(table.types);
327: 
328: 	// initialize the scan
329: 	LocalScanState state;
330: 	storage.InitializeScan(state);
331: 
332: 	while (true) {
333: 		Scan(state, column_ids, chunk);
334: 		if (chunk.size() == 0) {
335: 			return true;
336: 		}
337: 		if (!fun(chunk)) {
338: 			return false;
339: 		}
340: 	}
341: }
342: 
343: void LocalStorage::Flush(DataTable &table, LocalTableStorage &storage) {
344: 	if (storage.collection.Count() <= storage.deleted_rows) {
345: 		return;
346: 	}
347: 	idx_t append_count = storage.collection.Count() - storage.deleted_rows;
348: 	TableAppendState append_state;
349: 	table.InitializeAppend(transaction, append_state, append_count);
350: 
351: 	bool constraint_violated = false;
352: 	ScanTableStorage(table, storage, [&](DataChunk &chunk) -> bool {
353: 		// append this chunk to the indexes of the table
354: 		if (!table.AppendToIndexes(append_state, chunk, append_state.current_row)) {
355: 			constraint_violated = true;
356: 			return false;
357: 		}
358: 		// append to base table
359: 		table.Append(transaction, chunk, append_state);
360: 		return true;
361: 	});
362: 	if (constraint_violated) {
363: 		// need to revert the append
364: 		row_t current_row = append_state.row_start;
365: 		// remove the data from the indexes, if there are any indexes
366: 		ScanTableStorage(table, storage, [&](DataChunk &chunk) -> bool {
367: 			// append this chunk to the indexes of the table
368: 			table.RemoveFromIndexes(append_state, chunk, current_row);
369: 
370: 			current_row += chunk.size();
371: 			if (current_row >= append_state.current_row) {
372: 				// finished deleting all rows from the index: abort now
373: 				return false;
374: 			}
375: 			return true;
376: 		});
377: 		table.RevertAppendInternal(append_state.row_start, append_count);
378: 		storage.Clear();
379: 		throw ConstraintException("PRIMARY KEY or UNIQUE constraint violated: duplicated key");
380: 	}
381: 	storage.Clear();
382: 	transaction.PushAppend(&table, append_state.row_start, append_count);
383: }
384: 
385: void LocalStorage::Commit(LocalStorage::CommitState &commit_state, Transaction &transaction, WriteAheadLog *log,
386:                           transaction_t commit_id) {
387: 	// commit local storage, iterate over all entries in the table storage map
388: 	for (auto &entry : table_storage) {
389: 		auto table = entry.first;
390: 		auto storage = entry.second.get();
391: 		Flush(*table, *storage);
392: 	}
393: 	// finished commit: clear local storage
394: 	table_storage.clear();
395: }
396: 
397: void LocalStorage::AddColumn(DataTable *old_dt, DataTable *new_dt, ColumnDefinition &new_column,
398:                              Expression *default_value) {
399: 	// check if there are any pending appends for the old version of the table
400: 	auto entry = table_storage.find(old_dt);
401: 	if (entry == table_storage.end()) {
402: 		return;
403: 	}
404: 	// take over the storage from the old entry
405: 	auto new_storage = move(entry->second);
406: 
407: 	// now add the new column filled with the default value to all chunks
408: 	auto new_column_type = new_column.type;
409: 	ExpressionExecutor executor;
410: 	DataChunk dummy_chunk;
411: 	if (default_value) {
412: 		executor.AddExpression(*default_value);
413: 	}
414: 
415: 	new_storage->collection.Types().push_back(new_column_type);
416: 	for (idx_t chunk_idx = 0; chunk_idx < new_storage->collection.ChunkCount(); chunk_idx++) {
417: 		auto &chunk = new_storage->collection.GetChunk(chunk_idx);
418: 		Vector result(new_column_type);
419: 		if (default_value) {
420: 			dummy_chunk.SetCardinality(chunk.size());
421: 			executor.ExecuteExpression(dummy_chunk, result);
422: 		} else {
423: 			FlatVector::Validity(result).SetAllInvalid(chunk.size());
424: 		}
425: 		result.Normalify(chunk.size());
426: 		chunk.data.push_back(move(result));
427: 	}
428: 
429: 	table_storage.erase(entry);
430: 	table_storage[new_dt] = move(new_storage);
431: }
432: 
433: void LocalStorage::ChangeType(DataTable *old_dt, DataTable *new_dt, idx_t changed_idx, const LogicalType &target_type,
434:                               const vector<column_t> &bound_columns, Expression &cast_expr) {
435: 	// check if there are any pending appends for the old version of the table
436: 	auto entry = table_storage.find(old_dt);
437: 	if (entry == table_storage.end()) {
438: 		return;
439: 	}
440: 	throw NotImplementedException("FIXME: ALTER TYPE with transaction local data not currently supported");
441: }
442: 
443: } // namespace duckdb
[end of src/storage/local_storage.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: