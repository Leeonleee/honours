{
  "repo": "duckdb/duckdb",
  "pull_number": 11774,
  "instance_id": "duckdb__duckdb-11774",
  "issue_numbers": [
    "11743"
  ],
  "base_commit": "2eb4c3f54895b62f3594ff48dd303308016b4227",
  "patch": "diff --git a/extension/parquet/column_reader.cpp b/extension/parquet/column_reader.cpp\nindex e02feca15208..08471fdf3fed 100644\n--- a/extension/parquet/column_reader.cpp\n+++ b/extension/parquet/column_reader.cpp\n@@ -12,6 +12,7 @@\n #include \"row_number_column_reader.hpp\"\n #include \"snappy.h\"\n #include \"string_column_reader.hpp\"\n+#include \"null_column_reader.hpp\"\n #include \"struct_column_reader.hpp\"\n #include \"templated_column_reader.hpp\"\n #include \"utf8proc_wrapper.hpp\"\n@@ -1534,6 +1535,8 @@ unique_ptr<ColumnReader> ColumnReader::CreateReader(ParquetReader &reader, const\n \t\treturn make_uniq<UUIDColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n \tcase LogicalTypeId::INTERVAL:\n \t\treturn make_uniq<IntervalColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n+\tcase LogicalTypeId::SQLNULL:\n+\t\treturn make_uniq<NullColumnReader>(reader, type_p, schema_p, file_idx_p, max_define, max_repeat);\n \tdefault:\n \t\tbreak;\n \t}\ndiff --git a/extension/parquet/include/null_column_reader.hpp b/extension/parquet/include/null_column_reader.hpp\nnew file mode 100644\nindex 000000000000..567efee3e171\n--- /dev/null\n+++ b/extension/parquet/include/null_column_reader.hpp\n@@ -0,0 +1,54 @@\n+//===----------------------------------------------------------------------===//\n+//                         DuckDB\n+//\n+// null_column_reader.hpp\n+//\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#pragma once\n+\n+#include \"column_reader.hpp\"\n+#include \"duckdb/common/helper.hpp\"\n+\n+namespace duckdb {\n+\n+class NullColumnReader : public ColumnReader {\n+public:\n+\tstatic constexpr const PhysicalType TYPE = PhysicalType::INVALID;\n+\n+public:\n+\tNullColumnReader(ParquetReader &reader, LogicalType type_p, const SchemaElement &schema_p, idx_t schema_idx_p,\n+\t                 idx_t max_define_p, idx_t max_repeat_p)\n+\t    : ColumnReader(reader, std::move(type_p), schema_p, schema_idx_p, max_define_p, max_repeat_p) {};\n+\n+\tshared_ptr<ResizeableBuffer> dict;\n+\n+public:\n+\tvoid Dictionary(shared_ptr<ResizeableBuffer> data, idx_t num_entries) override {\n+\t\tdict = std::move(data);\n+\t}\n+\n+\tvoid Offsets(uint32_t *offsets, uint8_t *defines, uint64_t num_values, parquet_filter_t &filter,\n+\t             idx_t result_offset, Vector &result) override {\n+\t\tauto &result_mask = FlatVector::Validity(result);\n+\n+\t\tfor (idx_t row_idx = 0; row_idx < num_values; row_idx++) {\n+\t\t\tresult_mask.SetInvalid(row_idx + result_offset);\n+\t\t}\n+\t}\n+\n+\tvoid Plain(shared_ptr<ByteBuffer> plain_data, uint8_t *defines, uint64_t num_values, parquet_filter_t &filter,\n+\t           idx_t result_offset, Vector &result) override {\n+\t\t(void)defines;\n+\t\t(void)plain_data;\n+\t\t(void)filter;\n+\n+\t\tauto &result_mask = FlatVector::Validity(result);\n+\t\tfor (idx_t row_idx = 0; row_idx < num_values; row_idx++) {\n+\t\t\tresult_mask.SetInvalid(row_idx + result_offset);\n+\t\t}\n+\t}\n+};\n+\n+} // namespace duckdb\ndiff --git a/extension/parquet/parquet_reader.cpp b/extension/parquet/parquet_reader.cpp\nindex 896bc11b390c..7ff71fe9edcf 100644\n--- a/extension/parquet/parquet_reader.cpp\n+++ b/extension/parquet/parquet_reader.cpp\n@@ -243,12 +243,14 @@ LogicalType ParquetReader::DeriveLogicalType(const SchemaElement &s_ele, bool bi\n \t\t\treturn LogicalType::INTERVAL;\n \t\tcase ConvertedType::JSON:\n \t\t\treturn LogicalType::VARCHAR;\n+\t\tcase ConvertedType::NULL_TYPE:\n+\t\t\treturn LogicalTypeId::SQLNULL;\n \t\tcase ConvertedType::MAP:\n \t\tcase ConvertedType::MAP_KEY_VALUE:\n \t\tcase ConvertedType::LIST:\n \t\tcase ConvertedType::BSON:\n \t\tdefault:\n-\t\t\tthrow IOException(\"Unsupported converted type\");\n+\t\t\tthrow IOException(\"Unsupported converted type (%d)\", (int32_t)s_ele.converted_type);\n \t\t}\n \t} else {\n \t\t// no converted type set\ndiff --git a/third_party/parquet/parquet_types.cpp b/third_party/parquet/parquet_types.cpp\nindex 22299be39509..e6eeb774d061 100644\n--- a/third_party/parquet/parquet_types.cpp\n+++ b/third_party/parquet/parquet_types.cpp\n@@ -113,6 +113,9 @@ std::ostream &operator<<(std::ostream &out, const ConvertedType::type &val) {\n \tcase ConvertedType::INTERVAL:\n \t\tout << \"INTERVAL\";\n \t\treturn out;\n+\tcase ConvertedType::NULL_TYPE:\n+\t\tout << \"NULL\";\n+\t\treturn out;\n \t\t//  no default for compiler error on missing enum\n \t}\n \tout << static_cast<int>(val);\ndiff --git a/third_party/parquet/parquet_types.h b/third_party/parquet/parquet_types.h\nindex 55bc8c0e26c5..af109ee97e6f 100644\n--- a/third_party/parquet/parquet_types.h\n+++ b/third_party/parquet/parquet_types.h\n@@ -61,7 +61,8 @@ struct ConvertedType {\n     INT_64 = 18,\n     JSON = 19,\n     BSON = 20,\n-    INTERVAL = 21\n+    INTERVAL = 21,\n+    NULL_TYPE = 24\n   };\n };\n \n",
  "test_patch": "diff --git a/data/parquet-testing/empty.parquet b/data/parquet-testing/empty.parquet\nnew file mode 100644\nindex 000000000000..5b79f06661e6\nBinary files /dev/null and b/data/parquet-testing/empty.parquet differ\ndiff --git a/test/parquet/test_legacy_empty_pandas_parquet.test b/test/parquet/test_legacy_empty_pandas_parquet.test\nnew file mode 100644\nindex 000000000000..5c0df8ca1b63\n--- /dev/null\n+++ b/test/parquet/test_legacy_empty_pandas_parquet.test\n@@ -0,0 +1,9 @@\n+# name: test/parquet/test_legacy_empty_pandas_parquet.test\n+# group: [parquet]\n+\n+require parquet\n+\n+# This file includes the unsupported NULL (24) ConvertedType\n+# Which is not supported by the spec, but written by some ancient versions of Pandas (pre-2020)\n+statement ok\n+select * from 'data/parquet-testing/empty.parquet'\n",
  "problem_statement": "Duckdb (Python) 0.9.1 + 0.10.2 fails to read empty parquet files\n### What happens?\n\nReading an empty parquet file in duckdb causes the following error. Tested in both 0.9.1 and 10.0.2:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nIOException                               Traceback (most recent call last)\r\n/tmp/ipykernel_14881/3933748703.py in <module>\r\n----> 1 ddb.read_parquet('empty.parq')\r\n\r\nIOException: IO Error: Unsupported converted type\r\n```\r\n\r\nReading the same file in pandas/pyarrow returns an empty dataframe with expected columns.\n\n### To Reproduce\n\nAssuming an empty parquet file, `empty.parq` in the same directory:\r\n\r\n```\r\nimport sys\r\nsys.path.insert(0, '/usr/bin/lib/duckdb/0.10.2')\r\nimport duckdb as ddb\r\n\r\nddb.read_parquet('empty.parq')\r\n```\n\n### OS:\n\nRHEL\n\n### DuckDB Version:\n\n0.10.2\n\n### DuckDB Client:\n\n3.7\n\n### Full Name:\n\nCody Bordelon\n\n### Affiliation:\n\nNone\n\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\n\nI have tested with a stable release\n\n### Did you include all relevant data sets for reproducing the issue?\n\nNot applicable - the reproduction does not require a data set\n\n### Did you include all code required to reproduce the issue?\n\n- [X] Yes, I have\n\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\n\n- [X] Yes, I have\n",
  "hints_text": "Reading the place this exception is thrown from, it has nothing to do with the file being empty, just the type is unsupported\r\nhttps://github.com/duckdb/duckdb/blob/f29b2db1bbaa750b5273f1323568404830fb1f3c/extension/parquet/parquet_reader.cpp#L141\r\n\r\nCan you provide the file ?\nHi @Tishj thanks for the response.\r\n\r\nI didn't provide the file because all I did to test was:\r\n\r\n```\r\nimport pandas as pd\r\npd.DataFrame(column={'col1'}).to_parquet('empty.parq')\r\n```\r\n\r\nand then tried to read that in. If needed, I can recreate this and provide.\n> Hi @Tishj thanks for the response.\r\n> \r\n> I didn't provide the file because all I did to test was:\r\n> \r\n> ```\r\n> import pandas as pd\r\n> pd.DataFrame(column={'col1'}).to_parquet('empty.parq')\r\n> ```\r\n> \r\n> and then tried to read that in. If needed, I can recreate this and provide.\r\n\r\n```\r\n    pd.DataFrame(column={'col1'}).to_parquet('tmp/empty.parquet')\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nTypeError: DataFrame.__init__() got an unexpected keyword argument 'column'\r\n```\nAplogies, should be:\r\n\r\n```\r\npd.DataFrame(columns={'col1'}).to_parquet('empty.parq')\r\n```\n> Aplogies, should be:\r\n> \r\n> ```\r\n> pd.DataFrame(columns={'col1'}).to_parquet('empty.parq')\r\n> ```\r\n\r\n```\r\n    raise ValueError(\"columns cannot be a set\")\r\nValueError: columns cannot be a set\r\n```\nGithub will not allow me to upload parquet files it seems. I am unsure why you see that error when trying to write an empty parquet file in pandas - I can only offer that this is my current pandas version:\r\n\r\n```\r\n>>> pandas.__version__\r\n'0.24.0'\r\n```\nMy local pandas has this version:\r\n```\r\n>>> pd.__version__\r\n'2.2.2'\r\n```\nPlease provide a reproduction in any future issues you open here or anywhere else \ud83d\udc4d \r\n\r\nIn this case the script to recreate the empty parquet file would have been helpful\r\nAnd since you're using an ancient version of pandas (from the start of 2019) that would have been interesting as well\r\n\r\nI did get `pandas==0.24.0` to work in a Linux VM, running python 3.7 (also deprecated) after first installing `numpy==1.16.0`\r\n\r\nOnly to be faced with:\r\n```\r\nImportError: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\r\npyarrow or fastparquet is required for parquet support\r\n```\r\n\r\nSo mentioning the version of those libraries would also be required to reproduce this as pandas simply delegates the creation of parquet to one of these\nI am unable to reproduce the file for you because Github won't allow parquet file uploads. It should be a fairly straightforward operation to write an empty parquet file though. If there is some other avenue for providing the file to you, I will happily send it.\nYou could try WeTransfer, that has always worked well, even for medium sized files\nhttps://we.tl/t-e09cVyyKBH\nThanks, I can reproduce the issue\nAwesome thanks @Tishj please let me know if you need anything else on my end\nI believe I have narrowed down this issue to something being different between pandas versions:\r\n\r\nold:\r\n\r\n```\r\nPython 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import pandas as pd\r\n>>> import duckdb as ddb\r\n>>> ddb.__version__\r\n'0.10.2'\r\n>>> pd.__version__\r\n'2.2.2'\r\n>>> pd.DataFrame(columns=['col1']).to_parquet('temp.parq')\r\n>>> ddb.read_parquet('temp.parq')\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  col1  \u2502\r\n\u2502 int32  \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 0 rows \u2502\r\n```\r\n\r\nNew:\r\n```\r\nPython 3.7.0 (default, Oct  9 2018, 10:31:47) \r\n[GCC 7.3.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import pandas as pd\r\n>>> pd.DataFrame(columns={'col1'}).to_parquet(\"temp2.parq\")\r\n>>> import duckdb as ddb\r\n>>> ddb.read_parquet('temp2.parq')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nduckdb.duckdb.IOException: IO Error: Unsupported converted type\r\n>>> pd.__version__\r\n'0.24.0'\r\n>>> ddb.__version__\r\n'0.9.1'\r\n```\r\n\r\ncc @Tishj \nHI @Tishj I have some more info which may be useful. I was able to download the source code and build duckdb on my side directly. Issue recreated here:\r\n\r\n```\r\n./duckdb\r\nv0.10.3-dev280 4750ce2d7a\r\nEnter \".help\" for usage hints.\r\nConnected to a transient in-memory database.\r\nUse \".open FILENAME\" to reopen on a persistent database.\r\nD select * from read_parquet('temp2.parq');\r\nINT32\r\nIO Error: Unsupported converted type\r\nD \r\n```\r\n\r\nAfter that, I used some basic prints to figure out what the converted type was being read in as:\r\n\r\n```\r\nLogicalType ParquetReader::DeriveLogicalType(const SchemaElement &s_ele, bool binary_as_string) {\r\n\tstd::cout << s_ele.converted_type;\r\n        std::cout << s_ele.type;\r\n```\r\n\r\nresults in\r\n\r\n```\r\nConnected to a transient in-memory database.\r\nUse \".open FILENAME\" to reopen on a persistent database.\r\nD select * from read_parquet('temp2.parq');\r\n24\r\nINT32\r\nIO Error: Unsupported converted type\r\n```\r\n\r\nSo it seems whatever the old pandas version was writing is being converted to a type of `24` which is obviously not a covered case in the switch for that function. Note that the regular `type` is coming across as an INT32 which is very confusing here.\r\n\r\nAs a test, I added a case for this:\r\n\r\n```\r\n\t\t// case handles empty parquet files written in old pandas versions\r\n\t\tcase 24:\r\n\t\t\tif (s_ele.type == Type::INT32) {\r\n\t\t\t\treturn LogicalType::TINYINT;\r\n\t\t\t} else {\r\n\t\t\t\tthrow IOException(\"INT8 converted type can only be set for value of Type::INT32\");\r\n\t\t\t}\r\n```\r\n\r\nand now I can read the file in fine:\r\n\r\n```\r\nD select * from read_parquet('temp2.parq');\r\n24\r\nINT_32\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 col1 \u2502 __index_level_0__ \u2502\r\n\u2502 int8 \u2502       int8        \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502          0 rows          \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nD \r\n```\r\n\r\nThe problem here is that I'm not sure where the `converted_type` is being set here, but it seems like it should be a normal `ConvertedType::Int32` object but for some reason it is not. \r\n\r\nHope this helps!\nThanks, that's also what my findings were\r\n`24` does not appear to be a valid `converted_type` enum constant.\r\n\r\nI don't have that much experience with the Parquet code, but this seems like the file that Pandas produced is corrupted\r\nIt might be that the only change this brings about is improving the error message we throw there to include the problematic `converted_type` value\nI'm not really sure that its a \"corrupted\" file - my guess would be that its just how old pandas use to write out empty columns\r\n\r\nhttps://stackoverflow.com/questions/21287624/convert-pandas-column-containing-nans-to-dtype-int\r\n\r\nAlso, I have tested this across multiple machines/environments and it seems consistent when using that pandas verison that the type always comes across as `24`.\r\n\r\nMy guess would be that somewhere in the duckdb code we are taking the INT32 type from pandas and not properly setting the ConvertedType but I'm not sure how it gets 24\nYea I believe that it's consistent, but according to the Parquet spec the produced file is corrupted\r\n\r\nLooking at the array of types:\r\n```\r\n  std::__1::vector<duckdb_parquet::format::SchemaElement, std::__1::allocator<duckdb_parquet::format::SchemaElement> > = size=3 {\r\n    [0] = {\r\n      type = BOOLEAN\r\n      type_length = 0\r\n      repetition_type = REQUIRED\r\n      name = \"schema\"\r\n      num_children = 2\r\n      converted_type = UTF8\r\n      scale = 0\r\n      precision = 0\r\n      field_id = 0\r\n      logicalType = {\r\n        STRING = {}\r\n        MAP = {}\r\n        LIST = {}\r\n        ENUM = {}\r\n        DECIMAL = (scale = 0, precision = 0)\r\n        DATE = {}\r\n        TIME = {\r\n          isAdjustedToUTC = false\r\n          unit = {\r\n            MILLIS ={...}\r\n            MICROS ={...}\r\n            NANOS ={...}\r\n            __isset ={...}\r\n          }\r\n        }\r\n        TIMESTAMP = {\r\n          isAdjustedToUTC = false\r\n          unit = {\r\n            MILLIS ={...}\r\n            MICROS ={...}\r\n            NANOS ={...}\r\n            __isset ={...}\r\n          }\r\n        }\r\n        INTEGER = (bitWidth = '\\0', isSigned = false)\r\n        UNKNOWN = {}\r\n        JSON = {}\r\n        BSON = {}\r\n        UUID = {}\r\n        __isset = {\r\n          STRING = false\r\n          MAP = false\r\n          LIST = false\r\n          ENUM = false\r\n          DECIMAL = false\r\n          DATE = false\r\n          TIME = false\r\n          TIMESTAMP = false\r\n          INTEGER = false\r\n          UNKNOWN = false\r\n          JSON = false\r\n          BSON = false\r\n          UUID = false\r\n        }\r\n      }\r\n      __isset = {\r\n        type = false\r\n        type_length = false\r\n        repetition_type = true\r\n        num_children = true\r\n        converted_type = false\r\n        scale = false\r\n        precision = false\r\n        field_id = false\r\n        logicalType = false\r\n      }\r\n    }\r\n    [1] = {\r\n      type = INT32\r\n      type_length = 0\r\n      repetition_type = OPTIONAL\r\n      name = \"col1\"\r\n      num_children = 0\r\n      converted_type = TIME_MICROS | INT_16\r\n      scale = 0\r\n      precision = 0\r\n      field_id = 0\r\n      logicalType = {\r\n        STRING = {}\r\n        MAP = {}\r\n        LIST = {}\r\n        ENUM = {}\r\n        DECIMAL = (scale = 0, precision = 0)\r\n        DATE = {}\r\n        TIME = {\r\n          isAdjustedToUTC = false\r\n          unit = {\r\n            MILLIS ={...}\r\n            MICROS ={...}\r\n            NANOS ={...}\r\n            __isset ={...}\r\n          }\r\n        }\r\n        TIMESTAMP = {\r\n          isAdjustedToUTC = false\r\n          unit = {\r\n            MILLIS ={...}\r\n            MICROS ={...}\r\n            NANOS ={...}\r\n            __isset ={...}\r\n          }\r\n        }\r\n        INTEGER = (bitWidth = '\\0', isSigned = false)\r\n        UNKNOWN = {}\r\n        JSON = {}\r\n        BSON = {}\r\n        UUID = {}\r\n        __isset = {\r\n          STRING = false\r\n          MAP = false\r\n          LIST = false\r\n          ENUM = false\r\n          DECIMAL = false\r\n          DATE = false\r\n          TIME = false\r\n          TIMESTAMP = false\r\n          INTEGER = false\r\n          UNKNOWN = false\r\n          JSON = false\r\n          BSON = false\r\n          UUID = false\r\n        }\r\n      }\r\n      __isset = {\r\n        type = true\r\n        type_length = false\r\n        repetition_type = true\r\n        num_children = false\r\n        converted_type = true\r\n        scale = false\r\n        precision = false\r\n        field_id = false\r\n        logicalType = false\r\n      }\r\n    }\r\n    [2] = {\r\n      type = INT32\r\n      type_length = 0\r\n      repetition_type = OPTIONAL\r\n      name = \"__index_level_0__\"\r\n      num_children = 0\r\n      converted_type = TIME_MICROS | INT_16\r\n      scale = 0\r\n      precision = 0\r\n      field_id = 0\r\n      logicalType = {\r\n        STRING = {}\r\n        MAP = {}\r\n        LIST = {}\r\n        ENUM = {}\r\n        DECIMAL = (scale = 0, precision = 0)\r\n        DATE = {}\r\n        TIME = {\r\n          isAdjustedToUTC = false\r\n          unit = {\r\n            MILLIS ={...}\r\n            MICROS ={...}\r\n            NANOS ={...}\r\n            __isset ={...}\r\n          }\r\n        }\r\n        TIMESTAMP = {\r\n          isAdjustedToUTC = false\r\n          unit = {\r\n            MILLIS ={...}\r\n            MICROS ={...}\r\n            NANOS ={...}\r\n            __isset ={...}\r\n          }\r\n        }\r\n        INTEGER = (bitWidth = '\\0', isSigned = false)\r\n        UNKNOWN = {}\r\n        JSON = {}\r\n        BSON = {}\r\n        UUID = {}\r\n        __isset = {\r\n          STRING = false\r\n          MAP = false\r\n          LIST = false\r\n          ENUM = false\r\n          DECIMAL = false\r\n          DATE = false\r\n          TIME = false\r\n          TIMESTAMP = false\r\n          INTEGER = false\r\n          UNKNOWN = false\r\n          JSON = false\r\n          BSON = false\r\n          UUID = false\r\n        }\r\n      }\r\n      __isset = {\r\n        type = true\r\n        type_length = false\r\n        repetition_type = true\r\n        num_children = false\r\n        converted_type = true\r\n        scale = false\r\n        precision = false\r\n        field_id = false\r\n        logicalType = false\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThe top level struct indicates that there are 2 columns\r\n`\"col1\"` and `\"__index_level_0__\"`",
  "created_at": "2024-04-22T18:10:36Z"
}