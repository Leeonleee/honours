You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
TPC-H Q6 fails on Parquet input
I ran the following query from Python:

```
WITH lineitem AS (SELECT * FROM "0.parquet")
SELECT
    sum(l_extendedprice * l_discount) AS revenue
FROM
    lineitem
WHERE
    l_shipdate >= CAST('1994-01-01' AS date)
    AND l_shipdate < CAST('1995-01-01' AS date)
    AND l_discount BETWEEN 0.05
    AND 0.07
    AND l_quantity < 24;
```

with the following Parquet file: https://drive.google.com/file/d/1kdsASWJ0KEdP8IDigHN-9ibVJBbHlVSP/view?usp=sharing

and got the following exception:

```
RuntimeError: Not implemented Error: Unsupported type for filter FLAT DECIMAL(15,2): (UNKNOWN COUNT) [ ]
```

The Parquet files were obtained by using `CALL dbgen(sf=1);` and then exporting each TPC-H table to Arrow format, which I then wrote to Parquet format (using `pyarrow.parquet` -- I know that DuckDB can also export Parquet files, but this gave me the control I was looking for). 

**Environment (please complete the following information):**
 - Linux gcc 9.3
 - git master


</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
2: 
3: ![.github/workflows/main.yml](https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![codecov](https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN)](https://codecov.io/gh/duckdb/duckdb)
6: 
7: 
8: ## Installation
9: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
10: 
11: ## Development
12: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
13: 
14: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
15: 
16: 
[end of README.md]
[start of extension/parquet/parquet_reader.cpp]
1: #include "parquet_reader.hpp"
2: #include "parquet_timestamp.hpp"
3: #include "parquet_statistics.hpp"
4: #include "column_reader.hpp"
5: 
6: #include "boolean_column_reader.hpp"
7: #include "callback_column_reader.hpp"
8: #include "decimal_column_reader.hpp"
9: #include "list_column_reader.hpp"
10: #include "string_column_reader.hpp"
11: #include "struct_column_reader.hpp"
12: #include "templated_column_reader.hpp"
13: 
14: #include "thrift_tools.hpp"
15: 
16: #include "parquet_file_metadata_cache.hpp"
17: 
18: #include "duckdb.hpp"
19: #ifndef DUCKDB_AMALGAMATION
20: #include "duckdb/planner/table_filter.hpp"
21: #include "duckdb/planner/filter/constant_filter.hpp"
22: #include "duckdb/planner/filter/null_filter.hpp"
23: #include "duckdb/planner/filter/conjunction_filter.hpp"
24: #include "duckdb/common/file_system.hpp"
25: #include "duckdb/common/string_util.hpp"
26: #include "duckdb/common/types/date.hpp"
27: #include "duckdb/common/pair.hpp"
28: 
29: #include "duckdb/storage/object_cache.hpp"
30: #endif
31: 
32: #include <sstream>
33: #include <cassert>
34: #include <chrono>
35: #include <cstring>
36: #include <iostream>
37: 
38: namespace duckdb {
39: 
40: using duckdb_parquet::format::ColumnChunk;
41: using duckdb_parquet::format::ConvertedType;
42: using duckdb_parquet::format::FieldRepetitionType;
43: using duckdb_parquet::format::FileMetaData;
44: using ParquetRowGroup = duckdb_parquet::format::RowGroup;
45: using duckdb_parquet::format::SchemaElement;
46: using duckdb_parquet::format::Statistics;
47: using duckdb_parquet::format::Type;
48: 
49: static unique_ptr<duckdb_apache::thrift::protocol::TProtocol> CreateThriftProtocol(Allocator &allocator,
50:                                                                                    FileHandle &file_handle) {
51: 	auto transport = make_shared<ThriftFileTransport>(allocator, file_handle);
52: 	return make_unique<duckdb_apache::thrift::protocol::TCompactProtocolT<ThriftFileTransport>>(move(transport));
53: }
54: 
55: static shared_ptr<ParquetFileMetadataCache> LoadMetadata(Allocator &allocator, FileHandle &file_handle) {
56: 	auto current_time = std::chrono::system_clock::to_time_t(std::chrono::system_clock::now());
57: 
58: 	auto proto = CreateThriftProtocol(allocator, file_handle);
59: 	auto &transport = ((ThriftFileTransport &)*proto->getTransport());
60: 	auto file_size = transport.GetSize();
61: 	if (file_size < 12) {
62: 		throw InvalidInputException("File '%s' too small to be a Parquet file", file_handle.path);
63: 	}
64: 
65: 	ResizeableBuffer buf;
66: 	buf.resize(allocator, 8);
67: 	buf.zero();
68: 
69: 	transport.SetLocation(file_size - 8);
70: 	transport.read((uint8_t *)buf.ptr, 8);
71: 
72: 	if (strncmp(buf.ptr + 4, "PAR1", 4) != 0) {
73: 		throw InvalidInputException("No magic bytes found at end of file '%s'", file_handle.path);
74: 	}
75: 	// read four-byte footer length from just before the end magic bytes
76: 	auto footer_len = *(uint32_t *)buf.ptr;
77: 	if (footer_len <= 0 || file_size < 12 + footer_len) {
78: 		throw InvalidInputException("Footer length error in file '%s'", file_handle.path);
79: 	}
80: 	auto metadata_pos = file_size - (footer_len + 8);
81: 	transport.SetLocation(metadata_pos);
82: 	transport.Prefetch(metadata_pos, footer_len);
83: 
84: 	auto metadata = make_unique<FileMetaData>();
85: 	metadata->read(proto.get());
86: 	return make_shared<ParquetFileMetadataCache>(move(metadata), current_time);
87: }
88: 
89: static LogicalType DeriveLogicalType(const SchemaElement &s_ele) {
90: 	// inner node
91: 	D_ASSERT(s_ele.__isset.type && s_ele.num_children == 0);
92: 	switch (s_ele.type) {
93: 	case Type::BOOLEAN:
94: 		return LogicalType::BOOLEAN;
95: 	case Type::INT32:
96: 		if (s_ele.__isset.converted_type) {
97: 			switch (s_ele.converted_type) {
98: 			case ConvertedType::DATE:
99: 				return LogicalType::DATE;
100: 			case ConvertedType::UINT_8:
101: 				return LogicalType::UTINYINT;
102: 			case ConvertedType::UINT_16:
103: 				return LogicalType::USMALLINT;
104: 			default:
105: 				return LogicalType::INTEGER;
106: 			}
107: 		}
108: 		return LogicalType::INTEGER;
109: 	case Type::INT64:
110: 		if (s_ele.__isset.converted_type) {
111: 			switch (s_ele.converted_type) {
112: 			case ConvertedType::TIMESTAMP_MICROS:
113: 			case ConvertedType::TIMESTAMP_MILLIS:
114: 				return LogicalType::TIMESTAMP;
115: 			case ConvertedType::UINT_32:
116: 				return LogicalType::UINTEGER;
117: 			case ConvertedType::UINT_64:
118: 				return LogicalType::UBIGINT;
119: 			default:
120: 				return LogicalType::BIGINT;
121: 			}
122: 		}
123: 		return LogicalType::BIGINT;
124: 
125: 	case Type::INT96: // always a timestamp it would seem
126: 		return LogicalType::TIMESTAMP;
127: 	case Type::FLOAT:
128: 		return LogicalType::FLOAT;
129: 	case Type::DOUBLE:
130: 		return LogicalType::DOUBLE;
131: 	case Type::BYTE_ARRAY:
132: 	case Type::FIXED_LEN_BYTE_ARRAY:
133: 		if (s_ele.type == Type::FIXED_LEN_BYTE_ARRAY && !s_ele.__isset.type_length) {
134: 			return LogicalType::INVALID;
135: 		}
136: 		if (s_ele.__isset.converted_type) {
137: 			switch (s_ele.converted_type) {
138: 			case ConvertedType::DECIMAL:
139: 				if (s_ele.type == Type::FIXED_LEN_BYTE_ARRAY && s_ele.__isset.scale && s_ele.__isset.type_length) {
140: 					return LogicalType::DECIMAL(s_ele.precision, s_ele.scale);
141: 				}
142: 				return LogicalType::INVALID;
143: 
144: 			case ConvertedType::UTF8:
145: 				return LogicalType::VARCHAR;
146: 			default:
147: 				return LogicalType::BLOB;
148: 			}
149: 		}
150: 		return LogicalType::BLOB;
151: 	default:
152: 		return LogicalType::INVALID;
153: 	}
154: }
155: 
156: static unique_ptr<ColumnReader> CreateReaderRecursive(ParquetReader &reader, const FileMetaData *file_meta_data,
157:                                                       idx_t depth, idx_t max_define, idx_t max_repeat,
158:                                                       idx_t &next_schema_idx, idx_t &next_file_idx) {
159: 	D_ASSERT(file_meta_data);
160: 	D_ASSERT(next_schema_idx < file_meta_data->schema.size());
161: 	auto &s_ele = file_meta_data->schema[next_schema_idx];
162: 	auto this_idx = next_schema_idx;
163: 
164: 	if (s_ele.__isset.repetition_type) {
165: 		if (s_ele.repetition_type != FieldRepetitionType::REQUIRED) {
166: 			max_define++;
167: 		}
168: 		if (s_ele.repetition_type == FieldRepetitionType::REPEATED) {
169: 			max_repeat++;
170: 		}
171: 	}
172: 
173: 	if (!s_ele.__isset.type) { // inner node
174: 		if (s_ele.num_children == 0) {
175: 			throw std::runtime_error("Node has no children but should");
176: 		}
177: 		child_list_t<LogicalType> child_types;
178: 		vector<unique_ptr<ColumnReader>> child_readers;
179: 
180: 		idx_t c_idx = 0;
181: 		while (c_idx < (idx_t)s_ele.num_children) {
182: 			next_schema_idx++;
183: 
184: 			auto &child_ele = file_meta_data->schema[next_schema_idx];
185: 
186: 			auto child_reader = CreateReaderRecursive(reader, file_meta_data, depth + 1, max_define, max_repeat,
187: 			                                          next_schema_idx, next_file_idx);
188: 			child_types.push_back(make_pair(child_ele.name, child_reader->Type()));
189: 			child_readers.push_back(move(child_reader));
190: 
191: 			c_idx++;
192: 		}
193: 		D_ASSERT(!child_types.empty());
194: 		unique_ptr<ColumnReader> result;
195: 		LogicalType result_type;
196: 		// if we only have a single child no reason to create a struct ay
197: 		if (child_types.size() > 1 || depth == 0) {
198: 			result_type = LogicalType::STRUCT(move(child_types));
199: 			result = make_unique<StructColumnReader>(reader, result_type, s_ele, this_idx, max_define, max_repeat,
200: 			                                         move(child_readers));
201: 		} else {
202: 			// if we have a struct with only a single type, pull up
203: 			result_type = child_types[0].second;
204: 			result = move(child_readers[0]);
205: 		}
206: 		if (s_ele.repetition_type == FieldRepetitionType::REPEATED) {
207: 			result_type = LogicalType::LIST(result_type);
208: 			return make_unique<ListColumnReader>(reader, result_type, s_ele, this_idx, max_define, max_repeat,
209: 			                                     move(result));
210: 		}
211: 		return result;
212: 	} else { // leaf node
213: 		// TODO check return value of derive type or should we only do this on read()
214: 		return ColumnReader::CreateReader(reader, DeriveLogicalType(s_ele), s_ele, next_file_idx++, max_define,
215: 		                                  max_repeat);
216: 	}
217: }
218: 
219: // TODO we don't need readers for columns we are not going to read ay
220: static unique_ptr<ColumnReader> CreateReader(ParquetReader &reader, const FileMetaData *file_meta_data) {
221: 	idx_t next_schema_idx = 0;
222: 	idx_t next_file_idx = 0;
223: 
224: 	auto ret = CreateReaderRecursive(reader, file_meta_data, 0, 0, 0, next_schema_idx, next_file_idx);
225: 	D_ASSERT(next_schema_idx == file_meta_data->schema.size() - 1);
226: 	D_ASSERT(file_meta_data->row_groups.empty() || next_file_idx == file_meta_data->row_groups[0].columns.size());
227: 	return ret;
228: }
229: 
230: void ParquetReader::InitializeSchema(const vector<LogicalType> &expected_types_p, const string &initial_filename_p) {
231: 	auto file_meta_data = GetFileMetadata();
232: 
233: 	if (file_meta_data->__isset.encryption_algorithm) {
234: 		throw FormatException("Encrypted Parquet files are not supported");
235: 	}
236: 	// check if we like this schema
237: 	if (file_meta_data->schema.size() < 2) {
238: 		throw FormatException("Need at least one non-root column in the file");
239: 	}
240: 
241: 	bool has_expected_types = !expected_types_p.empty();
242: 	auto root_reader = CreateReader(*this, file_meta_data);
243: 
244: 	auto &root_type = root_reader->Type();
245: 	auto &child_types = StructType::GetChildTypes(root_type);
246: 	D_ASSERT(root_type.id() == LogicalTypeId::STRUCT);
247: 	if (has_expected_types && child_types.size() != expected_types_p.size()) {
248: 		throw FormatException("column count mismatch");
249: 	}
250: 	idx_t col_idx = 0;
251: 	for (auto &type_pair : child_types) {
252: 		if (has_expected_types && expected_types_p[col_idx] != type_pair.second) {
253: 			if (initial_filename_p.empty()) {
254: 				throw FormatException("column \"%d\" in parquet file is of type %s, could not auto cast to "
255: 				                      "expected type %s for this column",
256: 				                      col_idx, type_pair.second, expected_types_p[col_idx].ToString());
257: 			} else {
258: 				throw FormatException("schema mismatch in Parquet glob: column \"%d\" in parquet file is of type "
259: 				                      "%s, but in the original file \"%s\" this column is of type \"%s\"",
260: 				                      col_idx, type_pair.second, initial_filename_p,
261: 				                      expected_types_p[col_idx].ToString());
262: 			}
263: 		} else {
264: 			names.push_back(type_pair.first);
265: 			return_types.push_back(type_pair.second);
266: 		}
267: 		col_idx++;
268: 	}
269: 	D_ASSERT(!names.empty());
270: 	D_ASSERT(!return_types.empty());
271: }
272: 
273: ParquetReader::ParquetReader(Allocator &allocator_p, unique_ptr<FileHandle> file_handle_p,
274:                              const vector<LogicalType> &expected_types_p, const string &initial_filename_p)
275:     : allocator(allocator_p) {
276: 	file_name = file_handle_p->path;
277: 	file_handle = move(file_handle_p);
278: 	metadata = LoadMetadata(allocator, *file_handle);
279: 	InitializeSchema(expected_types_p, initial_filename_p);
280: }
281: 
282: ParquetReader::ParquetReader(ClientContext &context_p, string file_name_p, const vector<LogicalType> &expected_types_p,
283:                              const string &initial_filename_p)
284:     : allocator(Allocator::Get(context_p)) {
285: 	auto &fs = FileSystem::GetFileSystem(context_p);
286: 	file_name = move(file_name_p);
287: 	file_handle = fs.OpenFile(file_name, FileFlags::FILE_FLAGS_READ);
288: 	// If object cached is disabled
289: 	// or if this file has cached metadata
290: 	// or if the cached version already expired
291: 
292: 	auto last_modify_time = fs.GetLastModifiedTime(*file_handle);
293: 	if (!ObjectCache::ObjectCacheEnabled(context_p)) {
294: 		metadata = LoadMetadata(allocator, *file_handle);
295: 	} else {
296: 		metadata =
297: 		    std::dynamic_pointer_cast<ParquetFileMetadataCache>(ObjectCache::GetObjectCache(context_p).Get(file_name));
298: 		if (!metadata || (last_modify_time + 10 >= metadata->read_time)) {
299: 			metadata = LoadMetadata(allocator, *file_handle);
300: 			ObjectCache::GetObjectCache(context_p).Put(file_name, metadata);
301: 		}
302: 	}
303: 
304: 	InitializeSchema(expected_types_p, initial_filename_p);
305: }
306: 
307: ParquetReader::~ParquetReader() {
308: }
309: 
310: const FileMetaData *ParquetReader::GetFileMetadata() {
311: 	D_ASSERT(metadata);
312: 	D_ASSERT(metadata->metadata);
313: 	return metadata->metadata.get();
314: }
315: 
316: // TODO also somewhat ugly, perhaps this can be moved to the column reader too
317: unique_ptr<BaseStatistics> ParquetReader::ReadStatistics(ParquetReader &reader, LogicalType &type,
318:                                                          column_t file_col_idx, const FileMetaData *file_meta_data) {
319: 	unique_ptr<BaseStatistics> column_stats;
320: 	auto root_reader = CreateReader(reader, file_meta_data);
321: 	auto column_reader = ((StructColumnReader *)root_reader.get())->GetChildReader(file_col_idx);
322: 
323: 	for (auto &row_group : file_meta_data->row_groups) {
324: 		auto chunk_stats = column_reader->Stats(row_group.columns);
325: 		if (!chunk_stats) {
326: 			return nullptr;
327: 		}
328: 		if (!column_stats) {
329: 			column_stats = move(chunk_stats);
330: 		} else {
331: 			column_stats->Merge(*chunk_stats);
332: 		}
333: 	}
334: 	return column_stats;
335: }
336: 
337: const ParquetRowGroup &ParquetReader::GetGroup(ParquetReaderScanState &state) {
338: 	auto file_meta_data = GetFileMetadata();
339: 	D_ASSERT(state.current_group >= 0 && (idx_t)state.current_group < state.group_idx_list.size());
340: 	D_ASSERT(state.group_idx_list[state.current_group] >= 0 &&
341: 	         state.group_idx_list[state.current_group] < file_meta_data->row_groups.size());
342: 	return file_meta_data->row_groups[state.group_idx_list[state.current_group]];
343: }
344: 
345: void ParquetReader::PrepareRowGroupBuffer(ParquetReaderScanState &state, idx_t out_col_idx) {
346: 	auto &group = GetGroup(state);
347: 
348: 	auto column_reader = ((StructColumnReader *)state.root_reader.get())->GetChildReader(state.column_ids[out_col_idx]);
349: 
350: 	// TODO move this to columnreader too
351: 	if (state.filters) {
352: 		auto stats = column_reader->Stats(group.columns);
353: 		// filters contain output chunk index, not file col idx!
354: 		auto filter_entry = state.filters->filters.find(out_col_idx);
355: 		if (stats && filter_entry != state.filters->filters.end()) {
356: 			bool skip_chunk = false;
357: 			auto &filter = *filter_entry->second;
358: 			auto prune_result = filter.CheckStatistics(*stats);
359: 			if (prune_result == FilterPropagateResult::FILTER_ALWAYS_FALSE) {
360: 				skip_chunk = true;
361: 			}
362: 			if (skip_chunk) {
363: 				state.group_offset = group.num_rows;
364: 				return;
365: 				// this effectively will skip this chunk
366: 			}
367: 		}
368: 	}
369: 
370: 	state.root_reader->InitializeRead(group.columns, *state.thrift_file_proto);
371: }
372: 
373: idx_t ParquetReader::NumRows() {
374: 	return GetFileMetadata()->num_rows;
375: }
376: 
377: idx_t ParquetReader::NumRowGroups() {
378: 	return GetFileMetadata()->row_groups.size();
379: }
380: 
381: void ParquetReader::InitializeScan(ParquetReaderScanState &state, vector<column_t> column_ids,
382:                                    vector<idx_t> groups_to_read, TableFilterSet *filters) {
383: 	state.current_group = -1;
384: 	state.finished = false;
385: 	state.column_ids = move(column_ids);
386: 	state.group_offset = 0;
387: 	state.group_idx_list = move(groups_to_read);
388: 	state.filters = filters;
389: 	state.sel.Initialize(STANDARD_VECTOR_SIZE);
390: 	state.file_handle = file_handle->file_system.OpenFile(file_handle->path, FileFlags::FILE_FLAGS_READ);
391: 	state.thrift_file_proto = CreateThriftProtocol(allocator, *state.file_handle);
392: 	state.root_reader = CreateReader(*this, GetFileMetadata());
393: 
394: 	state.define_buf.resize(allocator, STANDARD_VECTOR_SIZE);
395: 	state.repeat_buf.resize(allocator, STANDARD_VECTOR_SIZE);
396: }
397: 
398: void FilterIsNull(Vector &v, parquet_filter_t &filter_mask, idx_t count) {
399: 	auto &mask = FlatVector::Validity(v);
400: 	if (mask.AllValid()) {
401: 		filter_mask.reset();
402: 	} else {
403: 		for (idx_t i = 0; i < count; i++) {
404: 			filter_mask[i] = filter_mask[i] && !mask.RowIsValid(i);
405: 		}
406: 	}
407: }
408: 
409: void FilterIsNotNull(Vector &v, parquet_filter_t &filter_mask, idx_t count) {
410: 	auto &mask = FlatVector::Validity(v);
411: 	if (!mask.AllValid()) {
412: 		for (idx_t i = 0; i < count; i++) {
413: 			filter_mask[i] = filter_mask[i] && mask.RowIsValid(i);
414: 		}
415: 	}
416: }
417: 
418: template <class T, class OP>
419: void TemplatedFilterOperation(Vector &v, T constant, parquet_filter_t &filter_mask, idx_t count) {
420: 	D_ASSERT(v.GetVectorType() == VectorType::FLAT_VECTOR); // we just created the damn thing it better be
421: 
422: 	auto v_ptr = FlatVector::GetData<T>(v);
423: 	auto &mask = FlatVector::Validity(v);
424: 
425: 	if (!mask.AllValid()) {
426: 		for (idx_t i = 0; i < count; i++) {
427: 			if (mask.RowIsValid(i)) {
428: 				filter_mask[i] = filter_mask[i] && OP::Operation(v_ptr[i], constant);
429: 			}
430: 		}
431: 	} else {
432: 		for (idx_t i = 0; i < count; i++) {
433: 			filter_mask[i] = filter_mask[i] && OP::Operation(v_ptr[i], constant);
434: 		}
435: 	}
436: }
437: 
438: template <class OP>
439: static void FilterOperationSwitch(Vector &v, Value &constant, parquet_filter_t &filter_mask, idx_t count) {
440: 	if (filter_mask.none() || count == 0) {
441: 		return;
442: 	}
443: 	switch (v.GetType().id()) {
444: 	case LogicalTypeId::BOOLEAN:
445: 		TemplatedFilterOperation<bool, OP>(v, constant.value_.boolean, filter_mask, count);
446: 		break;
447: 	case LogicalTypeId::UTINYINT:
448: 		TemplatedFilterOperation<uint8_t, OP>(v, constant.value_.utinyint, filter_mask, count);
449: 		break;
450: 	case LogicalTypeId::USMALLINT:
451: 		TemplatedFilterOperation<uint16_t, OP>(v, constant.value_.usmallint, filter_mask, count);
452: 		break;
453: 	case LogicalTypeId::UINTEGER:
454: 		TemplatedFilterOperation<uint32_t, OP>(v, constant.value_.uinteger, filter_mask, count);
455: 		break;
456: 	case LogicalTypeId::UBIGINT:
457: 		TemplatedFilterOperation<uint64_t, OP>(v, constant.value_.ubigint, filter_mask, count);
458: 		break;
459: 	case LogicalTypeId::INTEGER:
460: 		TemplatedFilterOperation<int32_t, OP>(v, constant.value_.integer, filter_mask, count);
461: 		break;
462: 	case LogicalTypeId::BIGINT:
463: 		TemplatedFilterOperation<int64_t, OP>(v, constant.value_.bigint, filter_mask, count);
464: 		break;
465: 	case LogicalTypeId::FLOAT:
466: 		TemplatedFilterOperation<float, OP>(v, constant.value_.float_, filter_mask, count);
467: 		break;
468: 	case LogicalTypeId::DOUBLE:
469: 		TemplatedFilterOperation<double, OP>(v, constant.value_.double_, filter_mask, count);
470: 		break;
471: 	case LogicalTypeId::DATE:
472: 		TemplatedFilterOperation<date_t, OP>(v, constant.value_.date, filter_mask, count);
473: 		break;
474: 	case LogicalTypeId::TIMESTAMP:
475: 		TemplatedFilterOperation<timestamp_t, OP>(v, constant.value_.timestamp, filter_mask, count);
476: 		break;
477: 	case LogicalTypeId::BLOB:
478: 	case LogicalTypeId::VARCHAR:
479: 		TemplatedFilterOperation<string_t, OP>(v, string_t(constant.str_value), filter_mask, count);
480: 		break;
481: 	default:
482: 		throw NotImplementedException("Unsupported type for filter %s", v.ToString());
483: 	}
484: }
485: 
486: static void ApplyFilter(Vector &v, TableFilter &filter, parquet_filter_t &filter_mask, idx_t count) {
487: 	switch (filter.filter_type) {
488: 	case TableFilterType::CONJUNCTION_AND: {
489: 		auto &conjunction = (ConjunctionAndFilter &)filter;
490: 		for (auto &child_filter : conjunction.child_filters) {
491: 			ApplyFilter(v, *child_filter, filter_mask, count);
492: 		}
493: 		break;
494: 	}
495: 	case TableFilterType::CONJUNCTION_OR: {
496: 		auto &conjunction = (ConjunctionOrFilter &)filter;
497: 		for (auto &child_filter : conjunction.child_filters) {
498: 			parquet_filter_t child_mask = filter_mask;
499: 			ApplyFilter(v, *child_filter, child_mask, count);
500: 			filter_mask |= child_mask;
501: 		}
502: 		break;
503: 	}
504: 	case TableFilterType::CONSTANT_COMPARISON: {
505: 		auto &constant_filter = (ConstantFilter &)filter;
506: 		switch (constant_filter.comparison_type) {
507: 		case ExpressionType::COMPARE_EQUAL:
508: 			FilterOperationSwitch<Equals>(v, constant_filter.constant, filter_mask, count);
509: 			break;
510: 		case ExpressionType::COMPARE_LESSTHAN:
511: 			FilterOperationSwitch<LessThan>(v, constant_filter.constant, filter_mask, count);
512: 			break;
513: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO:
514: 			FilterOperationSwitch<LessThanEquals>(v, constant_filter.constant, filter_mask, count);
515: 			break;
516: 		case ExpressionType::COMPARE_GREATERTHAN:
517: 			FilterOperationSwitch<GreaterThan>(v, constant_filter.constant, filter_mask, count);
518: 			break;
519: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
520: 			FilterOperationSwitch<GreaterThanEquals>(v, constant_filter.constant, filter_mask, count);
521: 			break;
522: 		default:
523: 			D_ASSERT(0);
524: 		}
525: 		break;
526: 	}
527: 	case TableFilterType::IS_NOT_NULL:
528: 		FilterIsNotNull(v, filter_mask, count);
529: 		break;
530: 	case TableFilterType::IS_NULL:
531: 		FilterIsNull(v, filter_mask, count);
532: 		break;
533: 	default:
534: 		D_ASSERT(0);
535: 		break;
536: 	}
537: }
538: 
539: void ParquetReader::Scan(ParquetReaderScanState &state, DataChunk &result) {
540: 	while (ScanInternal(state, result)) {
541: 		if (result.size() > 0) {
542: 			break;
543: 		}
544: 		result.Reset();
545: 	}
546: }
547: 
548: bool ParquetReader::ScanInternal(ParquetReaderScanState &state, DataChunk &result) {
549: 	if (state.finished) {
550: 		return false;
551: 	}
552: 
553: 	// see if we have to switch to the next row group in the parquet file
554: 	if (state.current_group < 0 || (int64_t)state.group_offset >= GetGroup(state).num_rows) {
555: 		state.current_group++;
556: 		state.group_offset = 0;
557: 
558: 		if ((idx_t)state.current_group == state.group_idx_list.size()) {
559: 			state.finished = true;
560: 			return false;
561: 		}
562: 
563: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
564: 			// this is a special case where we are not interested in the actual contents of the file
565: 			if (state.column_ids[out_col_idx] == COLUMN_IDENTIFIER_ROW_ID) {
566: 				continue;
567: 			}
568: 
569: 			PrepareRowGroupBuffer(state, out_col_idx);
570: 		}
571: 		return true;
572: 	}
573: 
574: 	auto this_output_chunk_rows = MinValue<idx_t>(STANDARD_VECTOR_SIZE, GetGroup(state).num_rows - state.group_offset);
575: 	result.SetCardinality(this_output_chunk_rows);
576: 
577: 	if (this_output_chunk_rows == 0) {
578: 		state.finished = true;
579: 		return false; // end of last group, we are done
580: 	}
581: 
582: 	// we evaluate simple table filters directly in this scan so we can skip decoding column data that's never going to
583: 	// be relevant
584: 	parquet_filter_t filter_mask;
585: 	filter_mask.set();
586: 
587: 	state.define_buf.zero();
588: 	state.repeat_buf.zero();
589: 
590: 	auto define_ptr = (uint8_t *)state.define_buf.ptr;
591: 	auto repeat_ptr = (uint8_t *)state.repeat_buf.ptr;
592: 
593: 	auto root_reader = ((StructColumnReader *)state.root_reader.get());
594: 
595: 	if (state.filters) {
596: 		vector<bool> need_to_read(result.ColumnCount(), true);
597: 
598: 		// first load the columns that are used in filters
599: 		for (auto &filter_col : state.filters->filters) {
600: 			auto file_col_idx = state.column_ids[filter_col.first];
601: 
602: 			if (filter_mask.none()) { // if no rows are left we can stop checking filters
603: 				break;
604: 			}
605: 
606: 			root_reader->GetChildReader(file_col_idx)
607: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[filter_col.first]);
608: 
609: 			need_to_read[filter_col.first] = false;
610: 
611: 			ApplyFilter(result.data[filter_col.first], *filter_col.second, filter_mask, this_output_chunk_rows);
612: 		}
613: 
614: 		// we still may have to read some cols
615: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
616: 			if (!need_to_read[out_col_idx]) {
617: 				continue;
618: 			}
619: 			auto file_col_idx = state.column_ids[out_col_idx];
620: 
621: 			if (filter_mask.none()) {
622: 				root_reader->GetChildReader(file_col_idx)->Skip(result.size());
623: 				continue;
624: 			}
625: 			// TODO handle ROWID here, too
626: 			root_reader->GetChildReader(file_col_idx)
627: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[out_col_idx]);
628: 		}
629: 
630: 		idx_t sel_size = 0;
631: 		for (idx_t i = 0; i < this_output_chunk_rows; i++) {
632: 			if (filter_mask[i]) {
633: 				state.sel.set_index(sel_size++, i);
634: 			}
635: 		}
636: 
637: 		result.Slice(state.sel, sel_size);
638: 		result.Verify();
639: 
640: 	} else { // #nofilter, just fricking load the data
641: 		for (idx_t out_col_idx = 0; out_col_idx < result.ColumnCount(); out_col_idx++) {
642: 			auto file_col_idx = state.column_ids[out_col_idx];
643: 
644: 			if (file_col_idx == COLUMN_IDENTIFIER_ROW_ID) {
645: 				Value constant_42 = Value::BIGINT(42);
646: 				result.data[out_col_idx].Reference(constant_42);
647: 				continue;
648: 			}
649: 
650: 			root_reader->GetChildReader(file_col_idx)
651: 			    ->Read(result.size(), filter_mask, define_ptr, repeat_ptr, result.data[out_col_idx]);
652: 		}
653: 	}
654: 
655: 	state.group_offset += this_output_chunk_rows;
656: 	return true;
657: }
658: 
659: } // namespace duckdb
[end of extension/parquet/parquet_reader.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: