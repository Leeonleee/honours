{
  "repo": "duckdb/duckdb",
  "pull_number": 15615,
  "instance_id": "duckdb__duckdb-15615",
  "issue_numbers": [
    "15601",
    "15601"
  ],
  "base_commit": "ce33de966a521d1a6e86ec9579e133ff2b2534f4",
  "patch": "diff --git a/data/json/15601/bunch_of_key_collisions.json b/data/json/15601/bunch_of_key_collisions.json\nnew file mode 100644\nindex 000000000000..5be180543052\n--- /dev/null\n+++ b/data/json/15601/bunch_of_key_collisions.json\n@@ -0,0 +1,1 @@\n+{\"duck_1_1\": 42, \"duck\": 43, \"Duck\": 44, \"duck_1\": 45, \"Duck_1\": 46, \"Duck_1_1\": 47}\ndiff --git a/data/json/15601/fragment1.json b/data/json/15601/fragment1.json\nnew file mode 100644\nindex 000000000000..c745c4e33c7c\n--- /dev/null\n+++ b/data/json/15601/fragment1.json\n@@ -0,0 +1,2 @@\n+{\"id\":\"19276\",\"Consignee\":\"zz af9\",\"Time\":\"30-04-2024 10:06:21\",\"DocketNumber\":\"6364174\",\"Image\":[\"abc\"],\"Time_1\":\"30-04-2024 09:32:13\"}\n+{\"id\":\"19281\",\"Consignee\":\"zz af9\",\"Time\":\"30-04-2024 11:38:13\",\"DocketNumber\":\"6364181\",\"Image\":[\"def\"],\"Time_1\":\"30-04-2024 11:01:27\"}\ndiff --git a/data/json/15601/fragment2.json b/data/json/15601/fragment2.json\nnew file mode 100644\nindex 000000000000..c2744a30d1db\n--- /dev/null\n+++ b/data/json/15601/fragment2.json\n@@ -0,0 +1,3 @@\n+{\"id\":\"19421\",\"Comments\":null,\"Consignee\":\"zz zz\",\"Driver\":null,\"Time\":\"30-04-2024 17:04:01\",\"image\":[\"zz\"],\"DocketNumber\":\"11\",\"Image_1\":[\"zzz\"],\"Time_1\":\"30-04-2024 17:00:59\"}\n+{\"id\":\"19426\",\"Comments\":\"delivered\",\"Consignee\":null,\"Driver\":\"zz zz\",\"Time\":\"01-05-2024 08:31:27\",\"image\":[\"zz\"],\"DocketNumber\":\"zxc\",\"Image_1\":[\"zzz\"],\"Time_1\":\"01-05-2024 08:31:01\"}\n+{\"id\":\"8182\",\"Comments\":null,\"Consignee\":\"\",\"Driver\":null,\"Time\":\"26-03-2024 18:03:33\",\"image\":null,\"signature\":[\"xx\"],\"DocketNumber\":\"x D xc\",\"Image_1\":null,\"Time_1\":\"26-03-2024 18:03:33\"}\ndiff --git a/extension/json/json_functions/read_json.cpp b/extension/json/json_functions/read_json.cpp\nindex 78c1b43795db..0dfe0039198b 100644\n--- a/extension/json/json_functions/read_json.cpp\n+++ b/extension/json/json_functions/read_json.cpp\n@@ -306,14 +306,14 @@ unique_ptr<FunctionData> ReadJSONBind(ClientContext &context, TableFunctionBindI\n \t\t// JSON may contain columns such as \"id\" and \"Id\", which are duplicates for us due to case-insensitivity\n \t\t// We rename them so we can parse the file anyway. Note that we can't change bind_data->names,\n \t\t// because the JSON reader gets columns by exact name, not position\n-\t\tcase_insensitive_map_t<idx_t> name_count_map;\n-\t\tfor (auto &name : names) {\n-\t\t\tauto it = name_count_map.find(name);\n-\t\t\tif (it == name_count_map.end()) {\n-\t\t\t\tname_count_map[name] = 1;\n-\t\t\t} else {\n-\t\t\t\tname = StringUtil::Format(\"%s_%llu\", name, it->second++);\n+\t\tcase_insensitive_map_t<idx_t> name_collision_count;\n+\t\tfor (auto &col_name : names) {\n+\t\t\t// Taken from CSV header_detection.cpp\n+\t\t\twhile (name_collision_count.find(col_name) != name_collision_count.end()) {\n+\t\t\t\tname_collision_count[col_name] += 1;\n+\t\t\t\tcol_name = col_name + \"_\" + to_string(name_collision_count[col_name]);\n \t\t\t}\n+\t\t\tname_collision_count[col_name] = 0;\n \t\t}\n \t}\n \n",
  "test_patch": "diff --git a/test/sql/json/issues/issue15601.test b/test/sql/json/issues/issue15601.test\nnew file mode 100644\nindex 000000000000..106db3bdd82b\n--- /dev/null\n+++ b/test/sql/json/issues/issue15601.test\n@@ -0,0 +1,16 @@\n+# name: test/sql/json/issues/issue15601.test\n+# description: Test issue 15601 - JSON reader fails with duplicate column name when reading multiple JSON files of slightly different casing\n+# group: [issues]\n+\n+require json\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+# original from the issue\n+statement ok\n+FROM 'data/json/15601/fragment*.json'\n+\n+# created an even worse example\n+statement ok\n+FROM 'data/json/15601/bunch_of_key_collisions.json'\n",
  "problem_statement": "JSON reader fails with duplicate column name when reading multiple JSON files of slightly different casing\n### What happens?\r\n\r\nWhen reading multiple JSON files using a single FROM \"fragment*.json\" query, DuckDB throws a duplicate column name \"Image_1\" error. It appears that columns (JSON object keys) with differing case\u2014e.g., \"Image\" vs. \"image\"\u2014may trigger this collision.\r\n\r\n### To Reproduce\r\n\r\n\r\nI have two JSON files with partially overlapping schemas. One file uses a key `\"Image\"`, and the other uses `\"image\"`, plus there is a key named `\"Image_1\"` in the second file. When I attempt to query all JSON files at once using a wildcard pattern, DuckDB complains about a duplicate column name.\r\n\r\n**File: `fragment1.json`**\r\n```json\r\n{\"id\":\"19276\",\"Consignee\":\"zz af9\",\"Time\":\"30-04-2024 10:06:21\",\"DocketNumber\":\"6364174\",\"Image\":[\"abc\"],\"Time_1\":\"30-04-2024 09:32:13\"}\r\n{\"id\":\"19281\",\"Consignee\":\"zz af9\",\"Time\":\"30-04-2024 11:38:13\",\"DocketNumber\":\"6364181\",\"Image\":[\"def\"],\"Time_1\":\"30-04-2024 11:01:27\"}\r\n```\r\n\r\n**File: `fragment2.json`**\r\n```json\r\n{\"id\":\"19421\",\"Comments\":null,\"Consignee\":\"zz zz\",\"Driver\":null,\"Time\":\"30-04-2024 17:04:01\",\"image\":[\"zz\"],\"DocketNumber\":\"11\",\"Image_1\":[\"zzz\"],\"Time_1\":\"30-04-2024 17:00:59\"}\r\n{\"id\":\"19426\",\"Comments\":\"delivered\",\"Consignee\":null,\"Driver\":\"zz zz\",\"Time\":\"01-05-2024 08:31:27\",\"image\":[\"zz\"],\"DocketNumber\":\"zxc\",\"Image_1\":[\"zzz\"],\"Time_1\":\"01-05-2024 08:31:01\"}\r\n{\"id\":\"8182\",\"Comments\":null,\"Consignee\":\"\",\"Driver\":null,\"Time\":\"26-03-2024 18:03:33\",\"image\":null,\"signature\":[\"xx\"],\"DocketNumber\":\"x D xc\",\"Image_1\":null,\"Time_1\":\"26-03-2024 18:03:33\"}\r\n```\r\n\r\n### Python script to reproduce\r\n\r\n```python\r\nimport duckdb\r\nimport os\r\n\r\ndef main():\r\n    # Create two JSON files with the sample data\r\n    with open('fragment1.json', 'w') as f:\r\n        f.write('''\\\r\n{\"id\":\"19276\",\"Consignee\":\"zz af9\",\"Time\":\"30-04-2024 10:06:21\",\"DocketNumber\":\"6364174\",\"Image\":[\"abc\"],\"Time_1\":\"30-04-2024 09:32:13\"}\r\n{\"id\":\"19281\",\"Consignee\":\"zz af9\",\"Time\":\"30-04-2024 11:38:13\",\"DocketNumber\":\"6364181\",\"Image\":[\"def\"],\"Time_1\":\"30-04-2024 11:01:27\"}\r\n''')\r\n\r\n    with open('fragment2.json', 'w') as f:\r\n        f.write('''\\\r\n{\"id\":\"19421\",\"Comments\":null,\"Consignee\":\"zz zz\",\"Driver\":null,\"Time\":\"30-04-2024 17:04:01\",\"image\":[\"zz\"],\"DocketNumber\":\"11\",\"Image_1\":[\"zzz\"],\"Time_1\":\"30-04-2024 17:00:59\"}\r\n{\"id\":\"19426\",\"Comments\":\"delivered\",\"Consignee\":null,\"Driver\":\"zz zz\",\"Time\":\"01-05-2024 08:31:27\",\"image\":[\"zz\"],\"DocketNumber\":\"zxc\",\"Image_1\":[\"zzz\"],\"Time_1\":\"01-05-2024 08:31:01\"}\r\n{\"id\":\"8182\",\"Comments\":null,\"Consignee\":\"\",\"Driver\":null,\"Time\":\"26-03-2024 18:03:33\",\"image\":null,\"signature\":[\"xx\"],\"DocketNumber\":\"x D xc\",\"Image_1\":null,\"Time_1\":\"26-03-2024 18:03:33\"}\r\n''')\r\n\r\n    con = duckdb.connect()\r\n    try:\r\n        # This query should fail due to a duplicate column name\r\n        con.execute(\"SELECT * FROM 'fragment*.json';\")\r\n        results = con.fetchall()\r\n        print(\"Query succeeded (unexpectedly). Results:\", results)\r\n    except Exception as e:\r\n        print(\"Query failed as expected with error:\")\r\n        print(e)\r\n\r\n    # Cleanup test files\r\n    os.remove('fragment1.json')\r\n    os.remove('fragment2.json')\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n### Expected vs. Actual Behavior\r\n\r\n- **Expected**: DuckDB should read all rows from both JSON files, merging columns appropriately (with `NULL` where columns are missing).\r\n- **Actual**: A `duplicate column name \"Image_1\"` error appears:\r\n  ```Binder Error: table \"fragment*.json\" has duplicate column name \"Image_1\"```\r\n\r\n### OS:\r\n\r\nWindows 11 amd64\r\n\r\n### DuckDB Version:\r\n\r\n1.1.3\r\n\r\n### DuckDB Client:\r\n\r\nPython\r\n\r\n### Hardware:\r\n\r\n_No response_\r\n\r\n### Full Name:\r\n\r\nMaitland Marshall\r\n\r\n### Affiliation:\r\n\r\nMAIT DEV\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have tested with a stable release\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nYes\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\nJSON reader fails with duplicate column name when reading multiple JSON files of slightly different casing\n### What happens?\r\n\r\nWhen reading multiple JSON files using a single FROM \"fragment*.json\" query, DuckDB throws a duplicate column name \"Image_1\" error. It appears that columns (JSON object keys) with differing case\u2014e.g., \"Image\" vs. \"image\"\u2014may trigger this collision.\r\n\r\n### To Reproduce\r\n\r\n\r\nI have two JSON files with partially overlapping schemas. One file uses a key `\"Image\"`, and the other uses `\"image\"`, plus there is a key named `\"Image_1\"` in the second file. When I attempt to query all JSON files at once using a wildcard pattern, DuckDB complains about a duplicate column name.\r\n\r\n**File: `fragment1.json`**\r\n```json\r\n{\"id\":\"19276\",\"Consignee\":\"zz af9\",\"Time\":\"30-04-2024 10:06:21\",\"DocketNumber\":\"6364174\",\"Image\":[\"abc\"],\"Time_1\":\"30-04-2024 09:32:13\"}\r\n{\"id\":\"19281\",\"Consignee\":\"zz af9\",\"Time\":\"30-04-2024 11:38:13\",\"DocketNumber\":\"6364181\",\"Image\":[\"def\"],\"Time_1\":\"30-04-2024 11:01:27\"}\r\n```\r\n\r\n**File: `fragment2.json`**\r\n```json\r\n{\"id\":\"19421\",\"Comments\":null,\"Consignee\":\"zz zz\",\"Driver\":null,\"Time\":\"30-04-2024 17:04:01\",\"image\":[\"zz\"],\"DocketNumber\":\"11\",\"Image_1\":[\"zzz\"],\"Time_1\":\"30-04-2024 17:00:59\"}\r\n{\"id\":\"19426\",\"Comments\":\"delivered\",\"Consignee\":null,\"Driver\":\"zz zz\",\"Time\":\"01-05-2024 08:31:27\",\"image\":[\"zz\"],\"DocketNumber\":\"zxc\",\"Image_1\":[\"zzz\"],\"Time_1\":\"01-05-2024 08:31:01\"}\r\n{\"id\":\"8182\",\"Comments\":null,\"Consignee\":\"\",\"Driver\":null,\"Time\":\"26-03-2024 18:03:33\",\"image\":null,\"signature\":[\"xx\"],\"DocketNumber\":\"x D xc\",\"Image_1\":null,\"Time_1\":\"26-03-2024 18:03:33\"}\r\n```\r\n\r\n### Python script to reproduce\r\n\r\n```python\r\nimport duckdb\r\nimport os\r\n\r\ndef main():\r\n    # Create two JSON files with the sample data\r\n    with open('fragment1.json', 'w') as f:\r\n        f.write('''\\\r\n{\"id\":\"19276\",\"Consignee\":\"zz af9\",\"Time\":\"30-04-2024 10:06:21\",\"DocketNumber\":\"6364174\",\"Image\":[\"abc\"],\"Time_1\":\"30-04-2024 09:32:13\"}\r\n{\"id\":\"19281\",\"Consignee\":\"zz af9\",\"Time\":\"30-04-2024 11:38:13\",\"DocketNumber\":\"6364181\",\"Image\":[\"def\"],\"Time_1\":\"30-04-2024 11:01:27\"}\r\n''')\r\n\r\n    with open('fragment2.json', 'w') as f:\r\n        f.write('''\\\r\n{\"id\":\"19421\",\"Comments\":null,\"Consignee\":\"zz zz\",\"Driver\":null,\"Time\":\"30-04-2024 17:04:01\",\"image\":[\"zz\"],\"DocketNumber\":\"11\",\"Image_1\":[\"zzz\"],\"Time_1\":\"30-04-2024 17:00:59\"}\r\n{\"id\":\"19426\",\"Comments\":\"delivered\",\"Consignee\":null,\"Driver\":\"zz zz\",\"Time\":\"01-05-2024 08:31:27\",\"image\":[\"zz\"],\"DocketNumber\":\"zxc\",\"Image_1\":[\"zzz\"],\"Time_1\":\"01-05-2024 08:31:01\"}\r\n{\"id\":\"8182\",\"Comments\":null,\"Consignee\":\"\",\"Driver\":null,\"Time\":\"26-03-2024 18:03:33\",\"image\":null,\"signature\":[\"xx\"],\"DocketNumber\":\"x D xc\",\"Image_1\":null,\"Time_1\":\"26-03-2024 18:03:33\"}\r\n''')\r\n\r\n    con = duckdb.connect()\r\n    try:\r\n        # This query should fail due to a duplicate column name\r\n        con.execute(\"SELECT * FROM 'fragment*.json';\")\r\n        results = con.fetchall()\r\n        print(\"Query succeeded (unexpectedly). Results:\", results)\r\n    except Exception as e:\r\n        print(\"Query failed as expected with error:\")\r\n        print(e)\r\n\r\n    # Cleanup test files\r\n    os.remove('fragment1.json')\r\n    os.remove('fragment2.json')\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n### Expected vs. Actual Behavior\r\n\r\n- **Expected**: DuckDB should read all rows from both JSON files, merging columns appropriately (with `NULL` where columns are missing).\r\n- **Actual**: A `duplicate column name \"Image_1\"` error appears:\r\n  ```Binder Error: table \"fragment*.json\" has duplicate column name \"Image_1\"```\r\n\r\n### OS:\r\n\r\nWindows 11 amd64\r\n\r\n### DuckDB Version:\r\n\r\n1.1.3\r\n\r\n### DuckDB Client:\r\n\r\nPython\r\n\r\n### Hardware:\r\n\r\n_No response_\r\n\r\n### Full Name:\r\n\r\nMaitland Marshall\r\n\r\n### Affiliation:\r\n\r\nMAIT DEV\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have tested with a stable release\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nYes\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\n",
  "hints_text": "\n",
  "created_at": "2025-01-08T13:46:12Z"
}