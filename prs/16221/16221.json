{
  "repo": "duckdb/duckdb",
  "pull_number": 16221,
  "instance_id": "duckdb__duckdb-16221",
  "issue_numbers": [
    "13552",
    "13552"
  ],
  "base_commit": "8234e7175cb50b35eecb537320d7f3eeef849c86",
  "patch": "diff --git a/src/planner/binder/tableref/bind_pivot.cpp b/src/planner/binder/tableref/bind_pivot.cpp\nindex 3899ecdd6ce1..c2891e9c54d9 100644\n--- a/src/planner/binder/tableref/bind_pivot.cpp\n+++ b/src/planner/binder/tableref/bind_pivot.cpp\n@@ -606,6 +606,7 @@ unique_ptr<SelectNode> Binder::BindUnpivot(Binder &child_binder, PivotRef &ref,\n \t\t}\n \t}\n \n+\tvector<string> select_names;\n \tfor (auto &col_expr : all_columns) {\n \t\tif (col_expr->GetExpressionType() != ExpressionType::COLUMN_REF) {\n \t\t\tthrow InternalException(\"Unexpected child of pivot source - not a ColumnRef\");\n@@ -615,6 +616,7 @@ unique_ptr<SelectNode> Binder::BindUnpivot(Binder &child_binder, PivotRef &ref,\n \t\tauto entry = handled_columns.find(column_name);\n \t\tif (entry == handled_columns.end()) {\n \t\t\t// not handled - add to the set of regularly selected columns\n+\t\t\tselect_names.push_back(col_expr->GetName());\n \t\t\tselect_node->select_list.push_back(std::move(col_expr));\n \t\t} else {\n \t\t\tname_map[column_name] = column_name;\n@@ -662,28 +664,70 @@ unique_ptr<SelectNode> Binder::BindUnpivot(Binder &child_binder, PivotRef &ref,\n \t\tunpivot_expressions.push_back(std::move(expressions));\n \t}\n \n+\tidx_t column_count = select_names.size();\n+\tidx_t unnest_count = unpivot_expressions.size();\n+\t// add the names for the generated unpivot lists\n+\tselect_names.push_back(\"unpivot_names\");\n+\tfor (idx_t i = 0; i < unpivot_expressions.size(); i++) {\n+\t\tif (i > 0) {\n+\t\t\tselect_names.push_back(\"unpivot_list_\" + std::to_string(i + 1));\n+\t\t} else {\n+\t\t\tselect_names.push_back(\"unpivot_list\");\n+\t\t}\n+\t}\n+\n+\t// now de-duplicate the names\n+\tQueryResult::DeduplicateColumns(select_names);\n+\n \t// construct the UNNEST expression for the set of names (constant)\n \tauto unpivot_list = Value::LIST(LogicalType::VARCHAR, std::move(unpivot_names));\n \tauto unpivot_name_expr = make_uniq<ConstantExpression>(std::move(unpivot_list));\n-\tvector<unique_ptr<ParsedExpression>> unnest_name_children;\n-\tunnest_name_children.push_back(std::move(unpivot_name_expr));\n-\tauto unnest_name_expr = make_uniq<FunctionExpression>(\"unnest\", std::move(unnest_name_children));\n-\tunnest_name_expr->SetAlias(unpivot.unpivot_names[0]);\n-\tselect_node->select_list.push_back(std::move(unnest_name_expr));\n+\tunpivot_name_expr->alias = select_names[column_count];\n+\tselect_node->select_list.push_back(std::move(unpivot_name_expr));\n \n-\t// construct the UNNEST expression for the set of unpivoted columns\n+\t// construct the unpivot lists for the set of unpivoted columns\n \tif (ref.unpivot_names.size() != unpivot_expressions.size()) {\n \t\tthrow BinderException(ref, \"UNPIVOT name count mismatch - got %d names but %d expressions\",\n \t\t                      ref.unpivot_names.size(), unpivot_expressions.size());\n \t}\n \tfor (idx_t i = 0; i < unpivot_expressions.size(); i++) {\n \t\tauto list_expr = make_uniq<FunctionExpression>(\"unpivot_list\", std::move(unpivot_expressions[i]));\n+\t\tlist_expr->alias = select_names[column_count + 1 + i];\n+\t\tselect_node->select_list.push_back(std::move(list_expr));\n+\t}\n+\n+\t// move the unpivot lists into a subquery\n+\tauto result_node = make_uniq<SelectNode>();\n+\tauto sub_select = make_uniq<SelectStatement>();\n+\tsub_select->node = std::move(select_node);\n+\tauto subquery = make_uniq<SubqueryRef>(std::move(sub_select));\n+\tsubquery->alias = \"unpivot\";\n+\n+\tresult_node->from_table = std::move(subquery);\n+\n+\t// construct the final UNNEST calls which generate the final unpivot result\n+\tfor (idx_t i = 0; i < column_count; i++) {\n+\t\tauto select_col = make_uniq<ColumnRefExpression>(std::move(select_names[i]));\n+\t\tresult_node->select_list.push_back(std::move(select_col));\n+\t}\n+\n+\tauto unpivot_name_list = make_uniq<ColumnRefExpression>(std::move(select_names[column_count]));\n+\tvector<unique_ptr<ParsedExpression>> unnest_name_children;\n+\tunnest_name_children.push_back(std::move(unpivot_name_list));\n+\tauto unnest_name_expr = make_uniq<FunctionExpression>(\"unnest\", std::move(unnest_name_children));\n+\tunnest_name_expr->SetAlias(unpivot.unpivot_names[0]);\n+\tresult_node->select_list.push_back(std::move(unnest_name_expr));\n+\n+\tfor (idx_t i = 0; i < unnest_count; i++) {\n+\t\tauto unpivot_internal_name = std::move(select_names[column_count + 1 + i]);\n+\n+\t\tauto unpivot_list_ref = make_uniq<ColumnRefExpression>(std::move(unpivot_internal_name));\n \t\tvector<unique_ptr<ParsedExpression>> unnest_val_children;\n-\t\tunnest_val_children.push_back(std::move(list_expr));\n+\t\tunnest_val_children.push_back(std::move(unpivot_list_ref));\n \t\tauto unnest_val_expr = make_uniq<FunctionExpression>(\"unnest\", std::move(unnest_val_children));\n \t\tauto unnest_name = i < ref.column_name_alias.size() ? ref.column_name_alias[i] : ref.unpivot_names[i];\n \t\tunnest_val_expr->SetAlias(unnest_name);\n-\t\tselect_node->select_list.push_back(std::move(unnest_val_expr));\n+\t\tresult_node->select_list.push_back(std::move(unnest_val_expr));\n \t\tif (!ref.include_nulls) {\n \t\t\t// if we are running with EXCLUDE NULLS we need to add an IS NOT NULL filter\n \t\t\tauto colref = make_uniq<ColumnRefExpression>(unnest_name);\n@@ -696,7 +740,7 @@ unique_ptr<SelectNode> Binder::BindUnpivot(Binder &child_binder, PivotRef &ref,\n \t\t\t}\n \t\t}\n \t}\n-\treturn select_node;\n+\treturn result_node;\n }\n \n unique_ptr<BoundTableRef> Binder::Bind(PivotRef &ref) {\n",
  "test_patch": "diff --git a/test/sql/pivot/unpivot_internal_names.test b/test/sql/pivot/unpivot_internal_names.test\nnew file mode 100644\nindex 000000000000..50c5c7ea3ce6\n--- /dev/null\n+++ b/test/sql/pivot/unpivot_internal_names.test\n@@ -0,0 +1,19 @@\n+# name: test/sql/pivot/unpivot_internal_names.test\n+# description: Test unpivoting on a table with names used internally by the unpivot operator\n+# group: [pivot]\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+statement ok\n+CREATE TABLE unpivot_names(unpivot_names VARCHAR, unpivot_list VARCHAR, unpivot_list_2 VARCHAR, col1 INT, col2 INT, col3 INT);\n+\n+statement ok\n+INSERT INTO unpivot_names VALUES ('unpivot_names', 'unpivot_list', 'unpivot_list_2', 1, 2, 3);\n+\n+query IIIII\n+UNPIVOT unpivot_names ON COLUMNS('col*')\n+----\n+unpivot_names\tunpivot_list\tunpivot_list_2\tcol1\t1\n+unpivot_names\tunpivot_list\tunpivot_list_2\tcol2\t2\n+unpivot_names\tunpivot_list\tunpivot_list_2\tcol3\t3\n",
  "problem_statement": "UNPIVOT statement is extremely slow when working with many columns\n### What happens?\r\n\r\nI'm trying to unpivot table with 30k+ columns. It is extremely slow on DuckDB. \r\nIn comparision similar operation in Pandas takes few seconds.\r\n\r\nFor 10k columns it takes 117 seconds on DuckDB vs 2,7 sec on Pandas\r\n\r\n\r\n### To Reproduce\r\n\r\nFor reporting purposes please find this example:\r\n\r\nData generation\r\n```sql\r\ncreate table r as select id, concat('column_', id) as column_name, concat('value_', id) as  column_value from (select unnest(generate_series(1,10000)) as id);\r\ncreate table pivoted as pivot r on column_name using first(id) group by id;\r\n```\r\n\r\nBenchmark:\r\n```sql\r\nselect count(*) from (unpivot pivoted on columns(* exclude id) into name 'column_name' value 'column_value');\r\n```\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 count_star() \u2502\r\n\u2502    int64     \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502        10000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nRun Time (s): real 117.393 user 215.706619 sys 18.343984\r\n```\r\n\r\nPandas for comparison\r\n```\r\nimport pandas as pd\r\nimport time\r\n\r\n# Step 1: Create the DataFrame\r\ndata = {\r\n    'id': range(10000),\r\n    'column_name': ['column_' + str(i) for i in range(10000)],\r\n    'column_value': range(10000)\r\n}\r\n\r\ndf = pd.DataFrame(data)\r\n\r\n# Step 2: Pivot the DataFrame\r\npivot_df = df.pivot(index='id', columns='column_name', values='column_value')\r\n\r\n# Step 3: Melt the DataFrame back\r\nstart = time.time()\r\nmelted_df = pivot_df.reset_index().melt(id_vars='id', var_name='column_name', value_name='column_value')\r\nprint(time.time() - start)\r\n# 2.771090269088745\r\n```\r\n\r\n### OS:\r\n\r\nMacOS\r\n\r\n### DuckDB Version:\r\n\r\n1.0.0\r\n\r\n### DuckDB Client:\r\n\r\nCLI\r\n\r\n### Full Name:\r\n\r\nMaciej Bry\u0144ski\r\n\r\n### Affiliation:\r\n\r\nCledar\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have tested with a stable release\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nYes\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\nUNPIVOT statement is extremely slow when working with many columns\n### What happens?\r\n\r\nI'm trying to unpivot table with 30k+ columns. It is extremely slow on DuckDB. \r\nIn comparision similar operation in Pandas takes few seconds.\r\n\r\nFor 10k columns it takes 117 seconds on DuckDB vs 2,7 sec on Pandas\r\n\r\n\r\n### To Reproduce\r\n\r\nFor reporting purposes please find this example:\r\n\r\nData generation\r\n```sql\r\ncreate table r as select id, concat('column_', id) as column_name, concat('value_', id) as  column_value from (select unnest(generate_series(1,10000)) as id);\r\ncreate table pivoted as pivot r on column_name using first(id) group by id;\r\n```\r\n\r\nBenchmark:\r\n```sql\r\nselect count(*) from (unpivot pivoted on columns(* exclude id) into name 'column_name' value 'column_value');\r\n```\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 count_star() \u2502\r\n\u2502    int64     \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502        10000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nRun Time (s): real 117.393 user 215.706619 sys 18.343984\r\n```\r\n\r\nPandas for comparison\r\n```\r\nimport pandas as pd\r\nimport time\r\n\r\n# Step 1: Create the DataFrame\r\ndata = {\r\n    'id': range(10000),\r\n    'column_name': ['column_' + str(i) for i in range(10000)],\r\n    'column_value': range(10000)\r\n}\r\n\r\ndf = pd.DataFrame(data)\r\n\r\n# Step 2: Pivot the DataFrame\r\npivot_df = df.pivot(index='id', columns='column_name', values='column_value')\r\n\r\n# Step 3: Melt the DataFrame back\r\nstart = time.time()\r\nmelted_df = pivot_df.reset_index().melt(id_vars='id', var_name='column_name', value_name='column_value')\r\nprint(time.time() - start)\r\n# 2.771090269088745\r\n```\r\n\r\n### OS:\r\n\r\nMacOS\r\n\r\n### DuckDB Version:\r\n\r\n1.0.0\r\n\r\n### DuckDB Client:\r\n\r\nCLI\r\n\r\n### Full Name:\r\n\r\nMaciej Bry\u0144ski\r\n\r\n### Affiliation:\r\n\r\nCledar\r\n\r\n### What is the latest build you tested with? If possible, we recommend testing with the latest nightly build.\r\n\r\nI have tested with a stable release\r\n\r\n### Did you include all relevant data sets for reproducing the issue?\r\n\r\nYes\r\n\r\n### Did you include all code required to reproduce the issue?\r\n\r\n- [X] Yes, I have\r\n\r\n### Did you include all relevant configuration (e.g., CPU architecture, Python version, Linux distribution) to reproduce the issue?\r\n\r\n- [X] Yes, I have\n",
  "hints_text": "DuckDB 1.1.0 - no changes\r\n``` \r\nD select count(*) from (unpivot pivoted on columns(* exclude id) into name 'column_name' value 'column_value');\r\n100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 count_star() \u2502\r\n\u2502    int64     \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502        10000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nRun Time (s): real 109.126 user 90.234428 sys 16.978885\r\n```\nDuckDB 1.1.0 - no changes\r\n``` \r\nD select count(*) from (unpivot pivoted on columns(* exclude id) into name 'column_name' value 'column_value');\r\n100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 count_star() \u2502\r\n\u2502    int64     \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502        10000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nRun Time (s): real 109.126 user 90.234428 sys 16.978885\r\n```",
  "created_at": "2025-02-13T09:39:22Z"
}