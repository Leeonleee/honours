You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
NullPointer at duckdb/src/common/types/vector.cpp:995:20
#### What happens?
```
/root/duckdb/src/common/types/vector.cpp:995:20: runtime error: member call on null pointer of type 'duckdb::string_t'
SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /root/duckdb/src/common/types/vector.cpp:995:20 in 
```

#### To Reproduce
```sql
CREATE TABLE strings(t0 INTEGER, a TEXT, b TEXT, UNIQUE (t0, a));
CREATE TABLE c2(c0 INTEGER, c1 INTEGER UNIQUE, FOREIGN KEY (c0) REFERENCES strings(t0));
INSERT INTO c2 VALUES (1, 101), (2, 102);
```

#### Environment (please complete the following information):
 - OS: linux
 - DuckDB Version: v0.3.3-dev1395 80ae1e12d
 - DuckDB Client: /usr/local/bin/duckdb

#### Before Submitting

- [x] **Have you tried this on the latest `master` branch?**
* **Python**: `pip install duckdb --upgrade --pre`
* **R**: `install.packages("https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_r_src.tar.gz", repos = NULL)`
* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.

- [x] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**

#### ASAN detail
```
AddressSanitizer:DEADLYSIGNAL
=================================================================
==42723==ERROR: AddressSanitizer: SEGV on unknown address 0x000000000000 (pc 0x0000018d7787 bp 0x000000000001 sp 0x7ffd7373c320 T0)
==42723==The signal is caused by a READ memory access.
==42723==Hint: address points to the zero page.
    #0 0x18d7787 in void duckdb::ConcatenateKeys<duckdb::string_t>(duckdb::Vector&, unsigned long, std::vector<std::unique_ptr<duckdb::Key, std::default_delete<duckdb::Key> >, std::allocator<std::unique_ptr<duckdb::Key, std::default_delete<duckdb::Key> > > >&, bool) /root/duckdb/src/execution/index/art/art.cpp:108:37
    #1 0x18d7787 in duckdb::ART::GenerateKeys(duckdb::DataChunk&, std::vector<std::unique_ptr<duckdb::Key, std::default_delete<duckdb::Key> >, std::allocator<std::unique_ptr<duckdb::Key, std::default_delete<duckdb::Key> > > >&) /root/duckdb/src/execution/index/art/art.cpp:205:4
    #2 0x18cda72 in duckdb::ART::VerifyExistence(duckdb::DataChunk&, duckdb::VerifyExistenceType, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*) /root/duckdb/src/execution/index/art/art.cpp:871:2
    #3 0x64095f in duckdb::VerifyForeignKeyConstraint(duckdb::BoundForeignKeyConstraint const&, duckdb::ClientContext&, duckdb::DataChunk&, bool)::$_18::operator()(duckdb::Index&) const /root/duckdb/src/storage/data_table.cpp:500:11
    #4 0x64095f in void duckdb::TableIndexList::Scan<duckdb::VerifyForeignKeyConstraint(duckdb::BoundForeignKeyConstraint const&, duckdb::ClientContext&, duckdb::DataChunk&, bool)::$_18>(duckdb::VerifyForeignKeyConstraint(duckdb::BoundForeignKeyConstraint const&, duckdb::ClientContext&, duckdb::DataChunk&, bool)::$_18&&) /root/duckdb/src/include/duckdb/storage/data_table.hpp:44:8
    #5 0x64095f in duckdb::VerifyForeignKeyConstraint(duckdb::BoundForeignKeyConstraint const&, duckdb::ClientContext&, duckdb::DataChunk&, bool) /root/duckdb/src/storage/data_table.cpp:497:16
    #6 0x61ae95 in duckdb::VerifyAppendForeignKeyConstraint(duckdb::BoundForeignKeyConstraint const&, duckdb::ClientContext&, duckdb::DataChunk&) /root/duckdb/src/storage/data_table.cpp:558:2
    #7 0x61ae95 in duckdb::DataTable::VerifyAppendConstraints(duckdb::TableCatalogEntry&, duckdb::ClientContext&, duckdb::DataChunk&) /root/duckdb/src/storage/data_table.cpp:592:5
    #8 0x61c622 in duckdb::DataTable::Append(duckdb::TableCatalogEntry&, duckdb::ClientContext&, duckdb::DataChunk&) /root/duckdb/src/storage/data_table.cpp:616:2
    #9 0x18c0b64 in duckdb::PhysicalInsert::Sink(duckdb::ExecutionContext&, duckdb::GlobalSinkState&, duckdb::LocalSinkState&, duckdb::DataChunk&) const /root/duckdb/src/execution/operator/persistent/physical_insert.cpp:78:18
    #10 0x5f5a40 in duckdb::PipelineExecutor::ExecutePushInternal(duckdb::DataChunk&, unsigned long) /root/duckdb/src/parallel/pipeline_executor.cpp:103:38
    #11 0x5ff186 in duckdb::PipelineExecutor::Execute(unsigned long) /root/duckdb/src/parallel/pipeline_executor.cpp:61:17
    #12 0x5ff186 in duckdb::PipelineTask::ExecuteTask(duckdb::TaskExecutionMode) /root/duckdb/src/parallel/pipeline.cpp:42:39
    #13 0x5e0934 in duckdb::ExecutorTask::Execute(duckdb::TaskExecutionMode) /root/duckdb/src/parallel/executor_task.cpp:17:10
    #14 0x5ed394 in duckdb::Executor::ExecuteTask() /root/duckdb/src/parallel/executor.cpp:350:24
    #15 0x5322d1 in duckdb::ClientContext::ExecuteTaskInternal(duckdb::ClientContextLock&, duckdb::PendingQueryResult&) /root/duckdb/src/main/client_context.cpp:336:41
    #16 0x533ea3 in duckdb::PendingQueryResult::ExecuteTaskInternal(duckdb::ClientContextLock&) /root/duckdb/src/main/pending_query_result.cpp:45:18
    #17 0x533ea3 in duckdb::PendingQueryResult::ExecuteInternal(duckdb::ClientContextLock&, bool) /root/duckdb/src/main/pending_query_result.cpp:50:9
    #18 0x518357 in duckdb::ClientContext::ExecutePendingQueryInternal(duckdb::ClientContextLock&, duckdb::PendingQueryResult&, bool) /root/duckdb/src/main/client_context.cpp:700:15
    #19 0x518357 in duckdb::ClientContext::Query(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool) /root/duckdb/src/main/client_context.cpp:639:21
    #20 0x519c06 in duckdb::Connection::Query(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /root/duckdb/src/main/connection.cpp:71:25
    #21 0x4e57c8 in duckdb_query /root/duckdb/src/main/capi/duckdb-c.cpp:67:22
    #22 0x4e10d3 in run_one_statement_new(void*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /mnt/jingzhou_workspace/sqlsim/client_new/duckdb/client_new.cpp:57:13
    #23 0x4d3984 in run_testcase(void*&, char const*, int) /root/autodriver.cpp:102:14
    #24 0x4d8601 in do_fuzz()::$_3::operator()() const /root/autodriver.cpp:227:13
    #25 0x4d809e in do_fuzz() /root/autodriver.cpp:235:13
    #26 0x4d8b43 in main /root/autodriver.cpp:292:9
    #27 0x7fc18e43f0b2 in __libc_start_main /build/glibc-sMfBJT/glibc-2.31/csu/../csu/libc-start.c:308:16
    #28 0x4247ad in _start (/root/autodriver+0x4247ad)

AddressSanitizer can not provide additional info.
SUMMARY: AddressSanitizer: SEGV /root/duckdb/src/execution/index/art/art.cpp:108:37 in void duckdb::ConcatenateKeys<duckdb::string_t>(duckdb::Vector&, unsigned long, std::vector<std::unique_ptr<duckdb::Key, std::default_delete<duckdb::Key> >, std::allocator<std::unique_ptr<duckdb::Key, std::default_delete<duckdb::Key> > > >&, bool)
==42723==ABORTING
```
NullPointer at duckdb/src/common/types/vector.cpp:995:20
#### What happens?
```
/root/duckdb/src/common/types/vector.cpp:995:20: runtime error: member call on null pointer of type 'duckdb::string_t'
SUMMARY: UndefinedBehaviorSanitizer: undefined-behavior /root/duckdb/src/common/types/vector.cpp:995:20 in 
```

#### To Reproduce
```sql
CREATE TABLE strings(t0 INTEGER, a TEXT, b TEXT, UNIQUE (t0, a));
CREATE TABLE c2(c0 INTEGER, c1 INTEGER UNIQUE, FOREIGN KEY (c0) REFERENCES strings(t0));
INSERT INTO c2 VALUES (1, 101), (2, 102);
```

#### Environment (please complete the following information):
 - OS: linux
 - DuckDB Version: v0.3.3-dev1395 80ae1e12d
 - DuckDB Client: /usr/local/bin/duckdb

#### Before Submitting

- [x] **Have you tried this on the latest `master` branch?**
* **Python**: `pip install duckdb --upgrade --pre`
* **R**: `install.packages("https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_r_src.tar.gz", repos = NULL)`
* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.

- [x] **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**

#### ASAN detail
```
AddressSanitizer:DEADLYSIGNAL
=================================================================
==42723==ERROR: AddressSanitizer: SEGV on unknown address 0x000000000000 (pc 0x0000018d7787 bp 0x000000000001 sp 0x7ffd7373c320 T0)
==42723==The signal is caused by a READ memory access.
==42723==Hint: address points to the zero page.
    #0 0x18d7787 in void duckdb::ConcatenateKeys<duckdb::string_t>(duckdb::Vector&, unsigned long, std::vector<std::unique_ptr<duckdb::Key, std::default_delete<duckdb::Key> >, std::allocator<std::unique_ptr<duckdb::Key, std::default_delete<duckdb::Key> > > >&, bool) /root/duckdb/src/execution/index/art/art.cpp:108:37
    #1 0x18d7787 in duckdb::ART::GenerateKeys(duckdb::DataChunk&, std::vector<std::unique_ptr<duckdb::Key, std::default_delete<duckdb::Key> >, std::allocator<std::unique_ptr<duckdb::Key, std::default_delete<duckdb::Key> > > >&) /root/duckdb/src/execution/index/art/art.cpp:205:4
    #2 0x18cda72 in duckdb::ART::VerifyExistence(duckdb::DataChunk&, duckdb::VerifyExistenceType, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*) /root/duckdb/src/execution/index/art/art.cpp:871:2
    #3 0x64095f in duckdb::VerifyForeignKeyConstraint(duckdb::BoundForeignKeyConstraint const&, duckdb::ClientContext&, duckdb::DataChunk&, bool)::$_18::operator()(duckdb::Index&) const /root/duckdb/src/storage/data_table.cpp:500:11
    #4 0x64095f in void duckdb::TableIndexList::Scan<duckdb::VerifyForeignKeyConstraint(duckdb::BoundForeignKeyConstraint const&, duckdb::ClientContext&, duckdb::DataChunk&, bool)::$_18>(duckdb::VerifyForeignKeyConstraint(duckdb::BoundForeignKeyConstraint const&, duckdb::ClientContext&, duckdb::DataChunk&, bool)::$_18&&) /root/duckdb/src/include/duckdb/storage/data_table.hpp:44:8
    #5 0x64095f in duckdb::VerifyForeignKeyConstraint(duckdb::BoundForeignKeyConstraint const&, duckdb::ClientContext&, duckdb::DataChunk&, bool) /root/duckdb/src/storage/data_table.cpp:497:16
    #6 0x61ae95 in duckdb::VerifyAppendForeignKeyConstraint(duckdb::BoundForeignKeyConstraint const&, duckdb::ClientContext&, duckdb::DataChunk&) /root/duckdb/src/storage/data_table.cpp:558:2
    #7 0x61ae95 in duckdb::DataTable::VerifyAppendConstraints(duckdb::TableCatalogEntry&, duckdb::ClientContext&, duckdb::DataChunk&) /root/duckdb/src/storage/data_table.cpp:592:5
    #8 0x61c622 in duckdb::DataTable::Append(duckdb::TableCatalogEntry&, duckdb::ClientContext&, duckdb::DataChunk&) /root/duckdb/src/storage/data_table.cpp:616:2
    #9 0x18c0b64 in duckdb::PhysicalInsert::Sink(duckdb::ExecutionContext&, duckdb::GlobalSinkState&, duckdb::LocalSinkState&, duckdb::DataChunk&) const /root/duckdb/src/execution/operator/persistent/physical_insert.cpp:78:18
    #10 0x5f5a40 in duckdb::PipelineExecutor::ExecutePushInternal(duckdb::DataChunk&, unsigned long) /root/duckdb/src/parallel/pipeline_executor.cpp:103:38
    #11 0x5ff186 in duckdb::PipelineExecutor::Execute(unsigned long) /root/duckdb/src/parallel/pipeline_executor.cpp:61:17
    #12 0x5ff186 in duckdb::PipelineTask::ExecuteTask(duckdb::TaskExecutionMode) /root/duckdb/src/parallel/pipeline.cpp:42:39
    #13 0x5e0934 in duckdb::ExecutorTask::Execute(duckdb::TaskExecutionMode) /root/duckdb/src/parallel/executor_task.cpp:17:10
    #14 0x5ed394 in duckdb::Executor::ExecuteTask() /root/duckdb/src/parallel/executor.cpp:350:24
    #15 0x5322d1 in duckdb::ClientContext::ExecuteTaskInternal(duckdb::ClientContextLock&, duckdb::PendingQueryResult&) /root/duckdb/src/main/client_context.cpp:336:41
    #16 0x533ea3 in duckdb::PendingQueryResult::ExecuteTaskInternal(duckdb::ClientContextLock&) /root/duckdb/src/main/pending_query_result.cpp:45:18
    #17 0x533ea3 in duckdb::PendingQueryResult::ExecuteInternal(duckdb::ClientContextLock&, bool) /root/duckdb/src/main/pending_query_result.cpp:50:9
    #18 0x518357 in duckdb::ClientContext::ExecutePendingQueryInternal(duckdb::ClientContextLock&, duckdb::PendingQueryResult&, bool) /root/duckdb/src/main/client_context.cpp:700:15
    #19 0x518357 in duckdb::ClientContext::Query(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool) /root/duckdb/src/main/client_context.cpp:639:21
    #20 0x519c06 in duckdb::Connection::Query(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /root/duckdb/src/main/connection.cpp:71:25
    #21 0x4e57c8 in duckdb_query /root/duckdb/src/main/capi/duckdb-c.cpp:67:22
    #22 0x4e10d3 in run_one_statement_new(void*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /mnt/jingzhou_workspace/sqlsim/client_new/duckdb/client_new.cpp:57:13
    #23 0x4d3984 in run_testcase(void*&, char const*, int) /root/autodriver.cpp:102:14
    #24 0x4d8601 in do_fuzz()::$_3::operator()() const /root/autodriver.cpp:227:13
    #25 0x4d809e in do_fuzz() /root/autodriver.cpp:235:13
    #26 0x4d8b43 in main /root/autodriver.cpp:292:9
    #27 0x7fc18e43f0b2 in __libc_start_main /build/glibc-sMfBJT/glibc-2.31/csu/../csu/libc-start.c:308:16
    #28 0x4247ad in _start (/root/autodriver+0x4247ad)

AddressSanitizer can not provide additional info.
SUMMARY: AddressSanitizer: SEGV /root/duckdb/src/execution/index/art/art.cpp:108:37 in void duckdb::ConcatenateKeys<duckdb::string_t>(duckdb::Vector&, unsigned long, std::vector<std::unique_ptr<duckdb::Key, std::default_delete<duckdb::Key> >, std::allocator<std::unique_ptr<duckdb::Key, std::default_delete<duckdb::Key> > > >&, bool)
==42723==ABORTING
```

</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
44: 
45: 
[end of README.md]
[start of src/include/duckdb/storage/data_table.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/storage/data_table.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/common/enums/index_type.hpp"
12: #include "duckdb/common/types/data_chunk.hpp"
13: #include "duckdb/storage/index.hpp"
14: #include "duckdb/storage/table_statistics.hpp"
15: #include "duckdb/storage/block.hpp"
16: #include "duckdb/storage/table/column_segment.hpp"
17: #include "duckdb/transaction/local_storage.hpp"
18: #include "duckdb/storage/table/persistent_table_data.hpp"
19: #include "duckdb/storage/table/row_group.hpp"
20: #include "duckdb/common/enums/scan_options.hpp"
21: 
22: #include "duckdb/common/atomic.hpp"
23: #include "duckdb/common/mutex.hpp"
24: 
25: namespace duckdb {
26: class ClientContext;
27: class ColumnDefinition;
28: class DataTable;
29: class RowGroup;
30: class StorageManager;
31: class TableCatalogEntry;
32: class Transaction;
33: class WriteAheadLog;
34: class TableDataWriter;
35: 
36: class TableIndexList {
37: public:
38: 	//! Scan the catalog set, invoking the callback method for every entry
39: 	template <class T>
40: 	void Scan(T &&callback) {
41: 		// lock the catalog set
42: 		lock_guard<mutex> lock(indexes_lock);
43: 		for (auto &index : indexes) {
44: 			if (callback(*index)) {
45: 				break;
46: 			}
47: 		}
48: 	}
49: 
50: 	void AddIndex(unique_ptr<Index> index) {
51: 		D_ASSERT(index);
52: 		lock_guard<mutex> lock(indexes_lock);
53: 		indexes.push_back(move(index));
54: 	}
55: 
56: 	void RemoveIndex(Index *index) {
57: 		D_ASSERT(index);
58: 		lock_guard<mutex> lock(indexes_lock);
59: 
60: 		for (idx_t index_idx = 0; index_idx < indexes.size(); index_idx++) {
61: 			auto &index_entry = indexes[index_idx];
62: 			if (index_entry.get() == index) {
63: 				indexes.erase(indexes.begin() + index_idx);
64: 				break;
65: 			}
66: 		}
67: 	}
68: 
69: 	bool Empty() {
70: 		lock_guard<mutex> lock(indexes_lock);
71: 		return indexes.empty();
72: 	}
73: 
74: 	idx_t Count() {
75: 		lock_guard<mutex> lock(indexes_lock);
76: 		return indexes.size();
77: 	}
78: 
79: private:
80: 	//! Indexes associated with the current table
81: 	mutex indexes_lock;
82: 	vector<unique_ptr<Index>> indexes;
83: };
84: 
85: struct DataTableInfo {
86: 	DataTableInfo(DatabaseInstance &db, string schema, string table)
87: 	    : db(db), cardinality(0), schema(move(schema)), table(move(table)) {
88: 	}
89: 
90: 	//! The database instance of the table
91: 	DatabaseInstance &db;
92: 	//! The amount of elements in the table. Note that this number signifies the amount of COMMITTED entries in the
93: 	//! table. It can be inaccurate inside of transactions. More work is needed to properly support that.
94: 	atomic<idx_t> cardinality;
95: 	// schema of the table
96: 	string schema;
97: 	// name of the table
98: 	string table;
99: 
100: 	TableIndexList indexes;
101: 
102: 	bool IsTemporary() {
103: 		return schema == TEMP_SCHEMA;
104: 	}
105: };
106: 
107: struct ParallelTableScanState {
108: 	RowGroup *current_row_group;
109: 	idx_t vector_index;
110: 	idx_t max_row;
111: 	LocalScanState local_state;
112: 	bool transaction_local_data;
113: };
114: 
115: //! DataTable represents a physical table on disk
116: class DataTable {
117: public:
118: 	//! Constructs a new data table from an (optional) set of persistent segments
119: 	DataTable(DatabaseInstance &db, const string &schema, const string &table,
120: 	          vector<ColumnDefinition> column_definitions_p, unique_ptr<PersistentTableData> data = nullptr);
121: 	//! Constructs a DataTable as a delta on an existing data table with a newly added column
122: 	DataTable(ClientContext &context, DataTable &parent, ColumnDefinition &new_column, Expression *default_value);
123: 	//! Constructs a DataTable as a delta on an existing data table but with one column removed
124: 	DataTable(ClientContext &context, DataTable &parent, idx_t removed_column);
125: 	//! Constructs a DataTable as a delta on an existing data table but with one column changed type
126: 	DataTable(ClientContext &context, DataTable &parent, idx_t changed_idx, const LogicalType &target_type,
127: 	          vector<column_t> bound_columns, Expression &cast_expr);
128: 
129: 	shared_ptr<DataTableInfo> info;
130: 
131: 	vector<ColumnDefinition> column_definitions;
132: 
133: 	//! A reference to the database instance
134: 	DatabaseInstance &db;
135: 
136: public:
137: 	//! Returns a list of types of the table
138: 	vector<LogicalType> GetTypes();
139: 
140: 	void InitializeScan(TableScanState &state, const vector<column_t> &column_ids,
141: 	                    TableFilterSet *table_filter = nullptr);
142: 	void InitializeScan(Transaction &transaction, TableScanState &state, const vector<column_t> &column_ids,
143: 	                    TableFilterSet *table_filters = nullptr);
144: 
145: 	//! Returns the maximum amount of threads that should be assigned to scan this data table
146: 	idx_t MaxThreads(ClientContext &context);
147: 	void InitializeParallelScan(ClientContext &context, ParallelTableScanState &state);
148: 	bool NextParallelScan(ClientContext &context, ParallelTableScanState &state, TableScanState &scan_state,
149: 	                      const vector<column_t> &column_ids);
150: 
151: 	//! Scans up to STANDARD_VECTOR_SIZE elements from the table starting
152: 	//! from offset and store them in result. Offset is incremented with how many
153: 	//! elements were returned.
154: 	//! Returns true if all pushed down filters were executed during data fetching
155: 	void Scan(Transaction &transaction, DataChunk &result, TableScanState &state, vector<column_t> &column_ids);
156: 
157: 	//! Fetch data from the specific row identifiers from the base table
158: 	void Fetch(Transaction &transaction, DataChunk &result, const vector<column_t> &column_ids, Vector &row_ids,
159: 	           idx_t fetch_count, ColumnFetchState &state);
160: 
161: 	//! Append a DataChunk to the table. Throws an exception if the columns don't match the tables' columns.
162: 	void Append(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk);
163: 	//! Delete the entries with the specified row identifier from the table
164: 	idx_t Delete(TableCatalogEntry &table, ClientContext &context, Vector &row_ids, idx_t count);
165: 	//! Update the entries with the specified row identifier from the table
166: 	void Update(TableCatalogEntry &table, ClientContext &context, Vector &row_ids, const vector<column_t> &column_ids,
167: 	            DataChunk &data);
168: 	//! Update a single (sub-)column along a column path
169: 	//! The column_path vector is a *path* towards a column within the table
170: 	//! i.e. if we have a table with a single column S STRUCT(A INT, B INT)
171: 	//! and we update the validity mask of "S.B"
172: 	//! the column path is:
173: 	//! 0 (first column of table)
174: 	//! -> 1 (second subcolumn of struct)
175: 	//! -> 0 (first subcolumn of INT)
176: 	//! This method should only be used from the WAL replay. It does not verify update constraints.
177: 	void UpdateColumn(TableCatalogEntry &table, ClientContext &context, Vector &row_ids,
178: 	                  const vector<column_t> &column_path, DataChunk &updates);
179: 
180: 	//! Add an index to the DataTable
181: 	void AddIndex(unique_ptr<Index> index, const vector<unique_ptr<Expression>> &expressions);
182: 
183: 	//! Begin appending structs to this table, obtaining necessary locks, etc
184: 	void InitializeAppend(Transaction &transaction, TableAppendState &state, idx_t append_count);
185: 	//! Append a chunk to the table using the AppendState obtained from BeginAppend
186: 	void Append(Transaction &transaction, DataChunk &chunk, TableAppendState &state);
187: 	//! Commit the append
188: 	void CommitAppend(transaction_t commit_id, idx_t row_start, idx_t count);
189: 	//! Write a segment of the table to the WAL
190: 	void WriteToLog(WriteAheadLog &log, idx_t row_start, idx_t count);
191: 	//! Revert a set of appends made by the given AppendState, used to revert appends in the event of an error during
192: 	//! commit (e.g. because of an I/O exception)
193: 	void RevertAppend(idx_t start_row, idx_t count);
194: 	void RevertAppendInternal(idx_t start_row, idx_t count);
195: 
196: 	void ScanTableSegment(idx_t start_row, idx_t count, const std::function<void(DataChunk &chunk)> &function);
197: 
198: 	//! Append a chunk with the row ids [row_start, ..., row_start + chunk.size()] to all indexes of the table, returns
199: 	//! whether or not the append succeeded
200: 	bool AppendToIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start);
201: 	//! Remove a chunk with the row ids [row_start, ..., row_start + chunk.size()] from all indexes of the table
202: 	void RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start);
203: 	//! Remove the chunk with the specified set of row identifiers from all indexes of the table
204: 	void RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, Vector &row_identifiers);
205: 	//! Remove the row identifiers from all the indexes of the table
206: 	void RemoveFromIndexes(Vector &row_identifiers, idx_t count);
207: 
208: 	void SetAsRoot() {
209: 		this->is_root = true;
210: 	}
211: 
212: 	unique_ptr<BaseStatistics> GetStatistics(ClientContext &context, column_t column_id);
213: 
214: 	//! Checkpoint the table to the specified table data writer
215: 	BlockPointer Checkpoint(TableDataWriter &writer);
216: 	void CommitDropTable();
217: 	void CommitDropColumn(idx_t index);
218: 
219: 	idx_t GetTotalRows();
220: 
221: 	//! Appends an empty row_group to the table
222: 	void AppendRowGroup(idx_t start_row);
223: 
224: 	vector<vector<Value>> GetStorageInfo();
225: 
226: private:
227: 	//! Verify constraints with a chunk from the Append containing all columns of the table
228: 	void VerifyAppendConstraints(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk);
229: 	//! Verify constraints with a chunk from the Update containing only the specified column_ids
230: 	void VerifyUpdateConstraints(TableCatalogEntry &table, DataChunk &chunk, const vector<column_t> &column_ids);
231: 	//! Verify constraints with a chunk from the Delete containing all columns of the table
232: 	void VerifyDeleteConstraints(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk);
233: 
234: 	void InitializeScanWithOffset(TableScanState &state, const vector<column_t> &column_ids, idx_t start_row,
235: 	                              idx_t end_row);
236: 	bool InitializeScanInRowGroup(TableScanState &state, const vector<column_t> &column_ids,
237: 	                              TableFilterSet *table_filters, RowGroup *row_group, idx_t vector_index,
238: 	                              idx_t max_row);
239: 	bool ScanBaseTable(Transaction &transaction, DataChunk &result, TableScanState &state);
240: 
241: 	//! The CreateIndexScan is a special scan that is used to create an index on the table, it keeps locks on the table
242: 	void InitializeCreateIndexScan(CreateIndexScanState &state, const vector<column_t> &column_ids);
243: 	bool ScanCreateIndex(CreateIndexScanState &state, DataChunk &result, TableScanType type);
244: 
245: private:
246: 	//! Lock for appending entries to the table
247: 	mutex append_lock;
248: 	//! The number of rows in the table
249: 	atomic<idx_t> total_rows;
250: 	//! The segment trees holding the various row_groups of the table
251: 	shared_ptr<SegmentTree> row_groups;
252: 	//! Column statistics
253: 	vector<unique_ptr<BaseStatistics>> column_stats;
254: 	//! The statistics lock
255: 	mutex stats_lock;
256: 	//! Whether or not the data table is the root DataTable for this table; the root DataTable is the newest version
257: 	//! that can be appended to
258: 	atomic<bool> is_root;
259: };
260: } // namespace duckdb
[end of src/include/duckdb/storage/data_table.hpp]
[start of src/planner/binder/statement/bind_create.cpp]
1: #include "duckdb/catalog/catalog.hpp"
2: #include "duckdb/catalog/catalog_search_path.hpp"
3: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
4: #include "duckdb/catalog/catalog_entry/type_catalog_entry.hpp"
5: #include "duckdb/main/client_context.hpp"
6: #include "duckdb/main/database.hpp"
7: #include "duckdb/parser/expression/constant_expression.hpp"
8: #include "duckdb/parser/expression/subquery_expression.hpp"
9: #include "duckdb/parser/parsed_data/create_index_info.hpp"
10: #include "duckdb/parser/parsed_data/create_macro_info.hpp"
11: #include "duckdb/parser/parsed_data/create_view_info.hpp"
12: #include "duckdb/parser/parsed_expression_iterator.hpp"
13: #include "duckdb/parser/statement/create_statement.hpp"
14: #include "duckdb/planner/binder.hpp"
15: #include "duckdb/planner/bound_query_node.hpp"
16: #include "duckdb/planner/expression_binder/aggregate_binder.hpp"
17: #include "duckdb/planner/expression_binder/index_binder.hpp"
18: #include "duckdb/planner/expression_binder/select_binder.hpp"
19: #include "duckdb/planner/operator/logical_create.hpp"
20: #include "duckdb/planner/operator/logical_create_index.hpp"
21: #include "duckdb/planner/operator/logical_create_table.hpp"
22: #include "duckdb/planner/operator/logical_get.hpp"
23: #include "duckdb/planner/parsed_data/bound_create_function_info.hpp"
24: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
25: #include "duckdb/planner/query_node/bound_select_node.hpp"
26: #include "duckdb/planner/tableref/bound_basetableref.hpp"
27: #include "duckdb/parser/constraints/foreign_key_constraint.hpp"
28: #include "duckdb/function/scalar_macro_function.hpp"
29: 
30: namespace duckdb {
31: 
32: SchemaCatalogEntry *Binder::BindSchema(CreateInfo &info) {
33: 	if (info.schema.empty()) {
34: 		info.schema = info.temporary ? TEMP_SCHEMA : context.catalog_search_path->GetDefault();
35: 	}
36: 
37: 	if (!info.temporary) {
38: 		// non-temporary create: not read only
39: 		if (info.schema == TEMP_SCHEMA) {
40: 			throw ParserException("Only TEMPORARY table names can use the \"temp\" schema");
41: 		}
42: 		this->read_only = false;
43: 	} else {
44: 		if (info.schema != TEMP_SCHEMA) {
45: 			throw ParserException("TEMPORARY table names can *only* use the \"%s\" schema", TEMP_SCHEMA);
46: 		}
47: 	}
48: 	// fetch the schema in which we want to create the object
49: 	auto schema_obj = Catalog::GetCatalog(context).GetSchema(context, info.schema);
50: 	D_ASSERT(schema_obj->type == CatalogType::SCHEMA_ENTRY);
51: 	info.schema = schema_obj->name;
52: 	return schema_obj;
53: }
54: 
55: void Binder::BindCreateViewInfo(CreateViewInfo &base) {
56: 	// bind the view as if it were a query so we can catch errors
57: 	// note that we bind the original, and replace the original with a copy
58: 	// this is because the original has
59: 	this->can_contain_nulls = true;
60: 
61: 	auto copy = base.query->Copy();
62: 	auto query_node = Bind(*base.query);
63: 	base.query = unique_ptr_cast<SQLStatement, SelectStatement>(move(copy));
64: 	if (base.aliases.size() > query_node.names.size()) {
65: 		throw BinderException("More VIEW aliases than columns in query result");
66: 	}
67: 	// fill up the aliases with the remaining names of the bound query
68: 	for (idx_t i = base.aliases.size(); i < query_node.names.size(); i++) {
69: 		base.aliases.push_back(query_node.names[i]);
70: 	}
71: 	base.types = query_node.types;
72: }
73: 
74: SchemaCatalogEntry *Binder::BindCreateFunctionInfo(CreateInfo &info) {
75: 	auto &base = (CreateMacroInfo &)info;
76: 	auto &scalar_function = (ScalarMacroFunction &)*base.function;
77: 
78: 	if (scalar_function.expression->HasParameter()) {
79: 		throw BinderException("Parameter expressions within macro's are not supported!");
80: 	}
81: 
82: 	// create macro binding in order to bind the function
83: 	vector<LogicalType> dummy_types;
84: 	vector<string> dummy_names;
85: 	// positional parameters
86: 	for (idx_t i = 0; i < base.function->parameters.size(); i++) {
87: 		auto param = (ColumnRefExpression &)*base.function->parameters[i];
88: 		if (param.IsQualified()) {
89: 			throw BinderException("Invalid parameter name '%s': must be unqualified", param.ToString());
90: 		}
91: 		dummy_types.emplace_back(LogicalType::SQLNULL);
92: 		dummy_names.push_back(param.GetColumnName());
93: 	}
94: 	// default parameters
95: 	for (auto it = base.function->default_parameters.begin(); it != base.function->default_parameters.end(); it++) {
96: 		auto &val = (ConstantExpression &)*it->second;
97: 		dummy_types.push_back(val.value.type());
98: 		dummy_names.push_back(it->first);
99: 	}
100: 	auto this_macro_binding = make_unique<MacroBinding>(dummy_types, dummy_names, base.name);
101: 	macro_binding = this_macro_binding.get();
102: 	ExpressionBinder::QualifyColumnNames(*this, scalar_function.expression);
103: 
104: 	// create a copy of the expression because we do not want to alter the original
105: 	auto expression = scalar_function.expression->Copy();
106: 
107: 	// bind it to verify the function was defined correctly
108: 	string error;
109: 	auto sel_node = make_unique<BoundSelectNode>();
110: 	auto group_info = make_unique<BoundGroupInformation>();
111: 	SelectBinder binder(*this, context, *sel_node, *group_info);
112: 	error = binder.Bind(&expression, 0, false);
113: 
114: 	if (!error.empty()) {
115: 		throw BinderException(error);
116: 	}
117: 
118: 	return BindSchema(info);
119: }
120: 
121: void Binder::BindLogicalType(ClientContext &context, LogicalType &type, const string &schema) {
122: 	if (type.id() == LogicalTypeId::LIST) {
123: 		auto child_type = ListType::GetChildType(type);
124: 		BindLogicalType(context, child_type, schema);
125: 		type = LogicalType::LIST(child_type);
126: 	} else if (type.id() == LogicalTypeId::STRUCT || type.id() == LogicalTypeId::MAP) {
127: 		auto child_types = StructType::GetChildTypes(type);
128: 		for (auto &child_type : child_types) {
129: 			BindLogicalType(context, child_type.second, schema);
130: 		}
131: 		// Generate new Struct/Map Type
132: 		if (type.id() == LogicalTypeId::STRUCT) {
133: 			type = LogicalType::STRUCT(child_types);
134: 		} else {
135: 			type = LogicalType::MAP(child_types);
136: 		}
137: 	} else if (type.id() == LogicalTypeId::USER) {
138: 		auto &user_type_name = UserType::GetTypeName(type);
139: 		auto user_type_catalog = (TypeCatalogEntry *)context.db->GetCatalog().GetEntry(context, CatalogType::TYPE_ENTRY,
140: 		                                                                               schema, user_type_name, true);
141: 		if (!user_type_catalog) {
142: 			throw NotImplementedException("DataType %s not supported yet...\n", user_type_name);
143: 		}
144: 		type = user_type_catalog->user_type;
145: 		EnumType::SetCatalog(type, user_type_catalog);
146: 	} else if (type.id() == LogicalTypeId::ENUM) {
147: 		auto &enum_type_name = EnumType::GetTypeName(type);
148: 		auto enum_type_catalog = (TypeCatalogEntry *)context.db->GetCatalog().GetEntry(context, CatalogType::TYPE_ENTRY,
149: 		                                                                               schema, enum_type_name, true);
150: 		EnumType::SetCatalog(type, enum_type_catalog);
151: 	}
152: }
153: 
154: BoundStatement Binder::Bind(CreateStatement &stmt) {
155: 	BoundStatement result;
156: 	result.names = {"Count"};
157: 	result.types = {LogicalType::BIGINT};
158: 
159: 	auto catalog_type = stmt.info->type;
160: 	switch (catalog_type) {
161: 	case CatalogType::SCHEMA_ENTRY:
162: 		result.plan = make_unique<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_SCHEMA, move(stmt.info));
163: 		break;
164: 	case CatalogType::VIEW_ENTRY: {
165: 		auto &base = (CreateViewInfo &)*stmt.info;
166: 		// bind the schema
167: 		auto schema = BindSchema(*stmt.info);
168: 		BindCreateViewInfo(base);
169: 		result.plan = make_unique<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_VIEW, move(stmt.info), schema);
170: 		break;
171: 	}
172: 	case CatalogType::SEQUENCE_ENTRY: {
173: 		auto schema = BindSchema(*stmt.info);
174: 		result.plan = make_unique<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_SEQUENCE, move(stmt.info), schema);
175: 		break;
176: 	}
177: 	case CatalogType::TABLE_MACRO_ENTRY: {
178: 		auto schema = BindSchema(*stmt.info);
179: 		result.plan = make_unique<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_MACRO, move(stmt.info), schema);
180: 		break;
181: 	}
182: 	case CatalogType::MACRO_ENTRY: {
183: 		auto schema = BindCreateFunctionInfo(*stmt.info);
184: 		result.plan = make_unique<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_MACRO, move(stmt.info), schema);
185: 		break;
186: 	}
187: 	case CatalogType::INDEX_ENTRY: {
188: 		auto &base = (CreateIndexInfo &)*stmt.info;
189: 
190: 		// visit the table reference
191: 		auto bound_table = Bind(*base.table);
192: 		if (bound_table->type != TableReferenceType::BASE_TABLE) {
193: 			throw BinderException("Can only delete from base table!");
194: 		}
195: 		auto &table_binding = (BoundBaseTableRef &)*bound_table;
196: 		auto table = table_binding.table;
197: 		// bind the index expressions
198: 		vector<unique_ptr<Expression>> expressions;
199: 		IndexBinder binder(*this, context);
200: 		for (auto &expr : base.expressions) {
201: 			expressions.push_back(binder.Bind(expr));
202: 		}
203: 
204: 		auto plan = CreatePlan(*bound_table);
205: 		if (plan->type != LogicalOperatorType::LOGICAL_GET) {
206: 			throw BinderException("Cannot create index on a view!");
207: 		}
208: 		auto &get = (LogicalGet &)*plan;
209: 		for (auto &column_id : get.column_ids) {
210: 			if (column_id == COLUMN_IDENTIFIER_ROW_ID) {
211: 				throw BinderException("Cannot create an index on the rowid!");
212: 			}
213: 		}
214: 		// this gives us a logical table scan
215: 		// we take the required columns from here
216: 		// create the logical operator
217: 		result.plan = make_unique<LogicalCreateIndex>(*table, get.column_ids, move(expressions),
218: 		                                              unique_ptr_cast<CreateInfo, CreateIndexInfo>(move(stmt.info)));
219: 		break;
220: 	}
221: 	case CatalogType::TABLE_ENTRY: {
222: 		// If there is a foreign key constraint, resolve primary key column's index from primary key column's name
223: 		auto &create_info = (CreateTableInfo &)*stmt.info;
224: 		auto &catalog = Catalog::GetCatalog(context);
225: 		for (idx_t i = 0; i < create_info.constraints.size(); i++) {
226: 			auto &cond = create_info.constraints[i];
227: 			if (cond->type != ConstraintType::FOREIGN_KEY) {
228: 				continue;
229: 			}
230: 			auto &fk = (ForeignKeyConstraint &)*cond;
231: 			if (fk.info.type != ForeignKeyType::FK_TYPE_FOREIGN_KEY_TABLE) {
232: 				continue;
233: 			}
234: 			D_ASSERT(fk.info.pk_keys.empty() && !fk.pk_columns.empty());
235: 			if (create_info.table == fk.info.table) {
236: 				fk.info.type = ForeignKeyType::FK_TYPE_SELF_REFERENCE_TABLE;
237: 			} else {
238: 				// have to resolve referenced table
239: 				auto pk_table_entry_ptr = catalog.GetEntry<TableCatalogEntry>(context, fk.info.schema, fk.info.table);
240: 				D_ASSERT(!fk.pk_columns.empty() && fk.info.pk_keys.empty());
241: 				for (auto &keyname : fk.pk_columns) {
242: 					auto entry = pk_table_entry_ptr->name_map.find(keyname);
243: 					if (entry == pk_table_entry_ptr->name_map.end()) {
244: 						throw ParserException("column \"%s\" named in key does not exist", keyname);
245: 					}
246: 					fk.info.pk_keys.push_back(entry->second);
247: 				}
248: 			}
249: 		}
250: 		// We first check if there are any user types, if yes we check to which custom types they refer.
251: 		auto bound_info = BindCreateTableInfo(move(stmt.info));
252: 		auto root = move(bound_info->query);
253: 
254: 		// create the logical operator
255: 		auto &schema = bound_info->schema;
256: 		auto create_table = make_unique<LogicalCreateTable>(schema, move(bound_info));
257: 		if (root) {
258: 			create_table->children.push_back(move(root));
259: 		}
260: 		result.plan = move(create_table);
261: 		break;
262: 	}
263: 	case CatalogType::TYPE_ENTRY: {
264: 		auto schema = BindSchema(*stmt.info);
265: 		result.plan = make_unique<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_TYPE, move(stmt.info), schema);
266: 		break;
267: 	}
268: 	default:
269: 		throw Exception("Unrecognized type!");
270: 	}
271: 	this->allow_stream_result = false;
272: 	return result;
273: }
274: 
275: } // namespace duckdb
[end of src/planner/binder/statement/bind_create.cpp]
[start of src/planner/binder/statement/bind_create_table.cpp]
1: #include "duckdb/parser/constraints/list.hpp"
2: #include "duckdb/parser/expression/cast_expression.hpp"
3: #include "duckdb/planner/binder.hpp"
4: #include "duckdb/planner/constraints/list.hpp"
5: #include "duckdb/planner/expression/bound_constant_expression.hpp"
6: #include "duckdb/planner/expression_binder/check_binder.hpp"
7: #include "duckdb/planner/expression_binder/constant_binder.hpp"
8: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
9: #include "duckdb/catalog/catalog_entry/type_catalog_entry.hpp"
10: #include "duckdb/catalog/dependency_manager.hpp"
11: 
12: #include <algorithm>
13: 
14: namespace duckdb {
15: 
16: static void CreateColumnMap(BoundCreateTableInfo &info, bool allow_duplicate_names) {
17: 	auto &base = (CreateTableInfo &)*info.base;
18: 
19: 	for (uint64_t oid = 0; oid < base.columns.size(); oid++) {
20: 		auto &col = base.columns[oid];
21: 		if (allow_duplicate_names) {
22: 			idx_t index = 1;
23: 			string base_name = col.name;
24: 			while (info.name_map.find(col.name) != info.name_map.end()) {
25: 				col.name = base_name + ":" + to_string(index++);
26: 			}
27: 		} else {
28: 			if (info.name_map.find(col.name) != info.name_map.end()) {
29: 				throw CatalogException("Column with name %s already exists!", col.name);
30: 			}
31: 		}
32: 
33: 		info.name_map[col.name] = oid;
34: 		col.oid = oid;
35: 	}
36: }
37: 
38: static void BindConstraints(Binder &binder, BoundCreateTableInfo &info) {
39: 	auto &base = (CreateTableInfo &)*info.base;
40: 
41: 	bool has_primary_key = false;
42: 	vector<idx_t> primary_keys;
43: 	for (idx_t i = 0; i < base.constraints.size(); i++) {
44: 		auto &cond = base.constraints[i];
45: 		switch (cond->type) {
46: 		case ConstraintType::CHECK: {
47: 			auto bound_constraint = make_unique<BoundCheckConstraint>();
48: 			// check constraint: bind the expression
49: 			CheckBinder check_binder(binder, binder.context, base.table, base.columns, bound_constraint->bound_columns);
50: 			auto &check = (CheckConstraint &)*cond;
51: 			// create a copy of the unbound expression because the binding destroys the constraint
52: 			auto unbound_expression = check.expression->Copy();
53: 			// now bind the constraint and create a new BoundCheckConstraint
54: 			bound_constraint->expression = check_binder.Bind(check.expression);
55: 			info.bound_constraints.push_back(move(bound_constraint));
56: 			// move the unbound constraint back into the original check expression
57: 			check.expression = move(unbound_expression);
58: 			break;
59: 		}
60: 		case ConstraintType::NOT_NULL: {
61: 			auto &not_null = (NotNullConstraint &)*cond;
62: 			info.bound_constraints.push_back(make_unique<BoundNotNullConstraint>(not_null.index));
63: 			break;
64: 		}
65: 		case ConstraintType::UNIQUE: {
66: 			auto &unique = (UniqueConstraint &)*cond;
67: 			// have to resolve columns of the unique constraint
68: 			vector<idx_t> keys;
69: 			unordered_set<idx_t> key_set;
70: 			if (unique.index != DConstants::INVALID_INDEX) {
71: 				D_ASSERT(unique.index < base.columns.size());
72: 				// unique constraint is given by single index
73: 				unique.columns.push_back(base.columns[unique.index].name);
74: 				keys.push_back(unique.index);
75: 				key_set.insert(unique.index);
76: 			} else {
77: 				// unique constraint is given by list of names
78: 				// have to resolve names
79: 				D_ASSERT(!unique.columns.empty());
80: 				for (auto &keyname : unique.columns) {
81: 					auto entry = info.name_map.find(keyname);
82: 					if (entry == info.name_map.end()) {
83: 						throw ParserException("column \"%s\" named in key does not exist", keyname);
84: 					}
85: 					if (key_set.find(entry->second) != key_set.end()) {
86: 						throw ParserException("column \"%s\" appears twice in "
87: 						                      "primary key constraint",
88: 						                      keyname);
89: 					}
90: 					keys.push_back(entry->second);
91: 					key_set.insert(entry->second);
92: 				}
93: 			}
94: 
95: 			if (unique.is_primary_key) {
96: 				// we can only have one primary key per table
97: 				if (has_primary_key) {
98: 					throw ParserException("table \"%s\" has more than one primary key", base.table);
99: 				}
100: 				has_primary_key = true;
101: 				primary_keys = keys;
102: 			}
103: 			info.bound_constraints.push_back(
104: 			    make_unique<BoundUniqueConstraint>(move(keys), move(key_set), unique.is_primary_key));
105: 			break;
106: 		}
107: 		case ConstraintType::FOREIGN_KEY: {
108: 			auto &fk = (ForeignKeyConstraint &)*cond;
109: 			D_ASSERT((fk.info.type == ForeignKeyType::FK_TYPE_FOREIGN_KEY_TABLE && !fk.info.pk_keys.empty()) ||
110: 			         (fk.info.type == ForeignKeyType::FK_TYPE_PRIMARY_KEY_TABLE && !fk.info.pk_keys.empty()) ||
111: 			         fk.info.type == ForeignKeyType::FK_TYPE_SELF_REFERENCE_TABLE);
112: 			if (fk.info.type == ForeignKeyType::FK_TYPE_SELF_REFERENCE_TABLE && fk.info.pk_keys.empty()) {
113: 				for (auto &keyname : fk.pk_columns) {
114: 					auto entry = info.name_map.find(keyname);
115: 					if (entry == info.name_map.end()) {
116: 						throw ParserException("column \"%s\" named in key does not exist", keyname);
117: 					}
118: 					fk.info.pk_keys.push_back(entry->second);
119: 				}
120: 			}
121: 			if (fk.info.fk_keys.empty()) {
122: 				for (auto &keyname : fk.fk_columns) {
123: 					auto entry = info.name_map.find(keyname);
124: 					if (entry == info.name_map.end()) {
125: 						throw ParserException("column \"%s\" named in key does not exist", keyname);
126: 					}
127: 					fk.info.fk_keys.push_back(entry->second);
128: 				}
129: 			}
130: 			unordered_set<idx_t> fk_key_set, pk_key_set;
131: 			for (idx_t i = 0; i < fk.info.pk_keys.size(); i++) {
132: 				pk_key_set.insert(fk.info.pk_keys[i]);
133: 			}
134: 			for (idx_t i = 0; i < fk.info.fk_keys.size(); i++) {
135: 				fk_key_set.insert(fk.info.fk_keys[i]);
136: 			}
137: 			info.bound_constraints.push_back(make_unique<BoundForeignKeyConstraint>(fk.info, pk_key_set, fk_key_set));
138: 			break;
139: 		}
140: 		default:
141: 			throw NotImplementedException("unrecognized constraint type in bind");
142: 		}
143: 	}
144: 	if (has_primary_key) {
145: 		// if there is a primary key index, also create a NOT NULL constraint for each of the columns
146: 		for (auto &column_index : primary_keys) {
147: 			base.constraints.push_back(make_unique<NotNullConstraint>(column_index));
148: 			info.bound_constraints.push_back(make_unique<BoundNotNullConstraint>(column_index));
149: 		}
150: 	}
151: }
152: 
153: void Binder::BindDefaultValues(vector<ColumnDefinition> &columns, vector<unique_ptr<Expression>> &bound_defaults) {
154: 	for (idx_t i = 0; i < columns.size(); i++) {
155: 		unique_ptr<Expression> bound_default;
156: 		if (columns[i].default_value) {
157: 			// we bind a copy of the DEFAULT value because binding is destructive
158: 			// and we want to keep the original expression around for serialization
159: 			auto default_copy = columns[i].default_value->Copy();
160: 			ConstantBinder default_binder(*this, context, "DEFAULT value");
161: 			default_binder.target_type = columns[i].type;
162: 			bound_default = default_binder.Bind(default_copy);
163: 		} else {
164: 			// no default value specified: push a default value of constant null
165: 			bound_default = make_unique<BoundConstantExpression>(Value(columns[i].type));
166: 		}
167: 		bound_defaults.push_back(move(bound_default));
168: 	}
169: }
170: 
171: unique_ptr<BoundCreateTableInfo> Binder::BindCreateTableInfo(unique_ptr<CreateInfo> info) {
172: 	auto &base = (CreateTableInfo &)*info;
173: 
174: 	auto result = make_unique<BoundCreateTableInfo>(move(info));
175: 	result->schema = BindSchema(*result->base);
176: 	if (base.query) {
177: 		// construct the result object
178: 		auto query_obj = Bind(*base.query);
179: 		result->query = move(query_obj.plan);
180: 
181: 		// construct the set of columns based on the names and types of the query
182: 		auto &names = query_obj.names;
183: 		auto &sql_types = query_obj.types;
184: 		D_ASSERT(names.size() == sql_types.size());
185: 		for (idx_t i = 0; i < names.size(); i++) {
186: 			base.columns.emplace_back(names[i], sql_types[i]);
187: 		}
188: 		// create the name map for the statement
189: 		CreateColumnMap(*result, true);
190: 	} else {
191: 		// create the name map for the statement
192: 		CreateColumnMap(*result, false);
193: 		// bind any constraints
194: 		BindConstraints(*this, *result);
195: 		// bind the default values
196: 		BindDefaultValues(base.columns, result->bound_defaults);
197: 	}
198: 	// bind collations to detect any unsupported collation errors
199: 	for (auto &column : base.columns) {
200: 		ExpressionBinder::TestCollation(context, StringType::GetCollation(column.type));
201: 		BindLogicalType(context, column.type);
202: 		if (column.type.id() == LogicalTypeId::ENUM) {
203: 			// We add a catalog dependency
204: 			auto enum_dependency = EnumType::GetCatalog(column.type);
205: 			if (enum_dependency) {
206: 				// Only if the ENUM comes from a create type
207: 				result->dependencies.insert(enum_dependency);
208: 			}
209: 		}
210: 	}
211: 	this->allow_stream_result = false;
212: 	return result;
213: }
214: 
215: } // namespace duckdb
[end of src/planner/binder/statement/bind_create_table.cpp]
[start of src/storage/data_table.cpp]
1: #include "duckdb/storage/data_table.hpp"
2: 
3: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
4: #include "duckdb/common/exception.hpp"
5: #include "duckdb/common/helper.hpp"
6: #include "duckdb/common/vector_operations/vector_operations.hpp"
7: #include "duckdb/execution/expression_executor.hpp"
8: #include "duckdb/main/client_context.hpp"
9: #include "duckdb/planner/constraints/list.hpp"
10: #include "duckdb/planner/table_filter.hpp"
11: #include "duckdb/storage/storage_manager.hpp"
12: #include "duckdb/storage/table/row_group.hpp"
13: #include "duckdb/storage/table/persistent_table_data.hpp"
14: #include "duckdb/transaction/transaction.hpp"
15: #include "duckdb/transaction/transaction_manager.hpp"
16: #include "duckdb/storage/checkpoint/table_data_writer.hpp"
17: #include "duckdb/storage/table/standard_column_data.hpp"
18: 
19: #include "duckdb/common/chrono.hpp"
20: 
21: namespace duckdb {
22: 
23: DataTable::DataTable(DatabaseInstance &db, const string &schema, const string &table,
24:                      vector<ColumnDefinition> column_definitions_p, unique_ptr<PersistentTableData> data)
25:     : info(make_shared<DataTableInfo>(db, schema, table)), column_definitions(move(column_definitions_p)), db(db),
26:       total_rows(0), is_root(true) {
27: 	// initialize the table with the existing data from disk, if any
28: 	this->row_groups = make_shared<SegmentTree>();
29: 	auto types = GetTypes();
30: 	if (data && !data->row_groups.empty()) {
31: 		for (auto &row_group_pointer : data->row_groups) {
32: 			auto new_row_group = make_unique<RowGroup>(db, *info, types, row_group_pointer);
33: 			auto row_group_count = new_row_group->start + new_row_group->count;
34: 			if (row_group_count > total_rows) {
35: 				total_rows = row_group_count;
36: 			}
37: 			row_groups->AppendSegment(move(new_row_group));
38: 		}
39: 		column_stats = move(data->column_stats);
40: 		if (column_stats.size() != types.size()) { // LCOV_EXCL_START
41: 			throw IOException("Table statistics column count is not aligned with table column count. Corrupt file?");
42: 		} // LCOV_EXCL_STOP
43: 	}
44: 	if (column_stats.empty()) {
45: 		D_ASSERT(total_rows == 0);
46: 
47: 		AppendRowGroup(0);
48: 		for (auto &type : types) {
49: 			column_stats.push_back(BaseStatistics::CreateEmpty(type));
50: 		}
51: 	} else {
52: 		D_ASSERT(column_stats.size() == types.size());
53: 		D_ASSERT(row_groups->GetRootSegment() != nullptr);
54: 	}
55: }
56: 
57: void DataTable::AppendRowGroup(idx_t start_row) {
58: 	auto types = GetTypes();
59: 	auto new_row_group = make_unique<RowGroup>(db, *info, start_row, 0);
60: 	new_row_group->InitializeEmpty(types);
61: 	row_groups->AppendSegment(move(new_row_group));
62: }
63: 
64: DataTable::DataTable(ClientContext &context, DataTable &parent, ColumnDefinition &new_column, Expression *default_value)
65:     : info(parent.info), db(parent.db), total_rows(parent.total_rows.load()), is_root(true) {
66: 	for (auto &column_def : parent.column_definitions) {
67: 		column_definitions.emplace_back(column_def.Copy());
68: 	}
69: 	// prevent any new tuples from being added to the parent
70: 	lock_guard<mutex> parent_lock(parent.append_lock);
71: 	// add the new column to this DataTable
72: 	auto new_column_type = new_column.type;
73: 	auto new_column_idx = parent.column_definitions.size();
74: 
75: 	// set up the statistics
76: 	for (idx_t i = 0; i < parent.column_stats.size(); i++) {
77: 		column_stats.push_back(parent.column_stats[i]->Copy());
78: 	}
79: 	column_stats.push_back(BaseStatistics::CreateEmpty(new_column_type));
80: 
81: 	// add the column definitions from this DataTable
82: 	column_definitions.emplace_back(new_column.Copy());
83: 
84: 	auto &transaction = Transaction::GetTransaction(context);
85: 
86: 	ExpressionExecutor executor;
87: 	DataChunk dummy_chunk;
88: 	Vector result(new_column_type);
89: 	if (!default_value) {
90: 		FlatVector::Validity(result).SetAllInvalid(STANDARD_VECTOR_SIZE);
91: 	} else {
92: 		executor.AddExpression(*default_value);
93: 	}
94: 
95: 	// fill the column with its DEFAULT value, or NULL if none is specified
96: 	auto new_stats = make_unique<SegmentStatistics>(new_column.type);
97: 	this->row_groups = make_shared<SegmentTree>();
98: 	auto current_row_group = (RowGroup *)parent.row_groups->GetRootSegment();
99: 	while (current_row_group) {
100: 		auto new_row_group = current_row_group->AddColumn(context, new_column, executor, default_value, result);
101: 		// merge in the statistics
102: 		column_stats[new_column_idx]->Merge(*new_row_group->GetStatistics(new_column_idx));
103: 
104: 		row_groups->AppendSegment(move(new_row_group));
105: 		current_row_group = (RowGroup *)current_row_group->next.get();
106: 	}
107: 
108: 	// also add this column to client local storage
109: 	transaction.storage.AddColumn(&parent, this, new_column, default_value);
110: 
111: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
112: 	parent.is_root = false;
113: }
114: 
115: DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t removed_column)
116:     : info(parent.info), db(parent.db), total_rows(parent.total_rows.load()), is_root(true) {
117: 	// prevent any new tuples from being added to the parent
118: 	lock_guard<mutex> parent_lock(parent.append_lock);
119: 
120: 	for (auto &column_def : parent.column_definitions) {
121: 		column_definitions.emplace_back(column_def.Copy());
122: 	}
123: 	// first check if there are any indexes that exist that point to the removed column
124: 	info->indexes.Scan([&](Index &index) {
125: 		for (auto &column_id : index.column_ids) {
126: 			if (column_id == removed_column) {
127: 				throw CatalogException("Cannot drop this column: an index depends on it!");
128: 			} else if (column_id > removed_column) {
129: 				throw CatalogException("Cannot drop this column: an index depends on a column after it!");
130: 			}
131: 		}
132: 		return false;
133: 	});
134: 
135: 	// erase the stats from this DataTable
136: 	for (idx_t i = 0; i < parent.column_stats.size(); i++) {
137: 		if (i != removed_column) {
138: 			column_stats.push_back(parent.column_stats[i]->Copy());
139: 		}
140: 	}
141: 
142: 	// erase the column definitions from this DataTable
143: 	D_ASSERT(removed_column < column_definitions.size());
144: 	column_definitions.erase(column_definitions.begin() + removed_column);
145: 
146: 	// alter the row_groups and remove the column from each of them
147: 	this->row_groups = make_shared<SegmentTree>();
148: 	auto current_row_group = (RowGroup *)parent.row_groups->GetRootSegment();
149: 	while (current_row_group) {
150: 		auto new_row_group = current_row_group->RemoveColumn(removed_column);
151: 		row_groups->AppendSegment(move(new_row_group));
152: 		current_row_group = (RowGroup *)current_row_group->next.get();
153: 	}
154: 
155: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
156: 	parent.is_root = false;
157: }
158: 
159: DataTable::DataTable(ClientContext &context, DataTable &parent, idx_t changed_idx, const LogicalType &target_type,
160:                      vector<column_t> bound_columns, Expression &cast_expr)
161:     : info(parent.info), db(parent.db), total_rows(parent.total_rows.load()), is_root(true) {
162: 	// prevent any tuples from being added to the parent
163: 	lock_guard<mutex> lock(append_lock);
164: 	for (auto &column_def : parent.column_definitions) {
165: 		column_definitions.emplace_back(column_def.Copy());
166: 	}
167: 	// first check if there are any indexes that exist that point to the changed column
168: 	info->indexes.Scan([&](Index &index) {
169: 		for (auto &column_id : index.column_ids) {
170: 			if (column_id == changed_idx) {
171: 				throw CatalogException("Cannot change the type of this column: an index depends on it!");
172: 			}
173: 		}
174: 		return false;
175: 	});
176: 
177: 	// change the type in this DataTable
178: 	column_definitions[changed_idx].type = target_type;
179: 
180: 	// set up the statistics for the table
181: 	// the column that had its type changed will have the new statistics computed during conversion
182: 	for (idx_t i = 0; i < column_definitions.size(); i++) {
183: 		if (i == changed_idx) {
184: 			column_stats.push_back(BaseStatistics::CreateEmpty(column_definitions[i].type));
185: 		} else {
186: 			column_stats.push_back(parent.column_stats[i]->Copy());
187: 		}
188: 	}
189: 
190: 	// scan the original table, and fill the new column with the transformed value
191: 	auto &transaction = Transaction::GetTransaction(context);
192: 
193: 	vector<LogicalType> scan_types;
194: 	for (idx_t i = 0; i < bound_columns.size(); i++) {
195: 		if (bound_columns[i] == COLUMN_IDENTIFIER_ROW_ID) {
196: 			scan_types.emplace_back(LogicalType::ROW_TYPE);
197: 		} else {
198: 			scan_types.push_back(parent.column_definitions[bound_columns[i]].type);
199: 		}
200: 	}
201: 	DataChunk scan_chunk;
202: 	scan_chunk.Initialize(scan_types);
203: 
204: 	ExpressionExecutor executor;
205: 	executor.AddExpression(cast_expr);
206: 
207: 	TableScanState scan_state;
208: 	scan_state.column_ids = bound_columns;
209: 	scan_state.max_row = total_rows;
210: 
211: 	// now alter the type of the column within all of the row_groups individually
212: 	this->row_groups = make_shared<SegmentTree>();
213: 	auto current_row_group = (RowGroup *)parent.row_groups->GetRootSegment();
214: 	while (current_row_group) {
215: 		auto new_row_group =
216: 		    current_row_group->AlterType(context, target_type, changed_idx, executor, scan_state, scan_chunk);
217: 		column_stats[changed_idx]->Merge(*new_row_group->GetStatistics(changed_idx));
218: 		row_groups->AppendSegment(move(new_row_group));
219: 		current_row_group = (RowGroup *)current_row_group->next.get();
220: 	}
221: 
222: 	transaction.storage.ChangeType(&parent, this, changed_idx, target_type, bound_columns, cast_expr);
223: 
224: 	// this table replaces the previous table, hence the parent is no longer the root DataTable
225: 	parent.is_root = false;
226: }
227: 
228: vector<LogicalType> DataTable::GetTypes() {
229: 	vector<LogicalType> types;
230: 	for (auto &it : column_definitions) {
231: 		types.push_back(it.type);
232: 	}
233: 	return types;
234: }
235: 
236: //===--------------------------------------------------------------------===//
237: // Scan
238: //===--------------------------------------------------------------------===//
239: void DataTable::InitializeScan(TableScanState &state, const vector<column_t> &column_ids,
240:                                TableFilterSet *table_filters) {
241: 	// initialize a column scan state for each column
242: 	// initialize the chunk scan state
243: 	auto row_group = (RowGroup *)row_groups->GetRootSegment();
244: 	state.column_ids = column_ids;
245: 	state.max_row = total_rows;
246: 	state.table_filters = table_filters;
247: 	if (table_filters) {
248: 		D_ASSERT(!table_filters->filters.empty());
249: 		state.adaptive_filter = make_unique<AdaptiveFilter>(table_filters);
250: 	}
251: 	while (row_group && !row_group->InitializeScan(state.row_group_scan_state)) {
252: 		row_group = (RowGroup *)row_group->next.get();
253: 	}
254: }
255: 
256: void DataTable::InitializeScan(Transaction &transaction, TableScanState &state, const vector<column_t> &column_ids,
257:                                TableFilterSet *table_filters) {
258: 	InitializeScan(state, column_ids, table_filters);
259: 	transaction.storage.InitializeScan(this, state.local_state, table_filters);
260: }
261: 
262: void DataTable::InitializeScanWithOffset(TableScanState &state, const vector<column_t> &column_ids, idx_t start_row,
263:                                          idx_t end_row) {
264: 
265: 	auto row_group = (RowGroup *)row_groups->GetSegment(start_row);
266: 	state.column_ids = column_ids;
267: 	state.max_row = end_row;
268: 	state.table_filters = nullptr;
269: 	idx_t start_vector = (start_row - row_group->start) / STANDARD_VECTOR_SIZE;
270: 	if (!row_group->InitializeScanWithOffset(state.row_group_scan_state, start_vector)) {
271: 		throw InternalException("Failed to initialize row group scan with offset");
272: 	}
273: }
274: 
275: bool DataTable::InitializeScanInRowGroup(TableScanState &state, const vector<column_t> &column_ids,
276:                                          TableFilterSet *table_filters, RowGroup *row_group, idx_t vector_index,
277:                                          idx_t max_row) {
278: 	state.column_ids = column_ids;
279: 	state.max_row = max_row;
280: 	state.table_filters = table_filters;
281: 	if (table_filters) {
282: 		D_ASSERT(!table_filters->filters.empty());
283: 		state.adaptive_filter = make_unique<AdaptiveFilter>(table_filters);
284: 	}
285: 	return row_group->InitializeScanWithOffset(state.row_group_scan_state, vector_index);
286: }
287: 
288: idx_t DataTable::MaxThreads(ClientContext &context) {
289: 	idx_t parallel_scan_vector_count = RowGroup::ROW_GROUP_VECTOR_COUNT;
290: 	if (ClientConfig::GetConfig(context).verify_parallelism) {
291: 		parallel_scan_vector_count = 1;
292: 	}
293: 	idx_t parallel_scan_tuple_count = STANDARD_VECTOR_SIZE * parallel_scan_vector_count;
294: 
295: 	return total_rows / parallel_scan_tuple_count + 1;
296: }
297: 
298: void DataTable::InitializeParallelScan(ClientContext &context, ParallelTableScanState &state) {
299: 	state.current_row_group = (RowGroup *)row_groups->GetRootSegment();
300: 	state.transaction_local_data = false;
301: 	// figure out the max row we can scan for both the regular and the transaction-local storage
302: 	state.max_row = total_rows;
303: 	state.local_state.max_index = 0;
304: 	auto &transaction = Transaction::GetTransaction(context);
305: 	transaction.storage.InitializeScan(this, state.local_state, nullptr);
306: }
307: 
308: bool DataTable::NextParallelScan(ClientContext &context, ParallelTableScanState &state, TableScanState &scan_state,
309:                                  const vector<column_t> &column_ids) {
310: 	while (state.current_row_group) {
311: 		idx_t vector_index;
312: 		idx_t max_row;
313: 		if (ClientConfig::GetConfig(context).verify_parallelism) {
314: 			vector_index = state.vector_index;
315: 			max_row = state.current_row_group->start +
316: 			          MinValue<idx_t>(state.current_row_group->count,
317: 			                          STANDARD_VECTOR_SIZE * state.vector_index + STANDARD_VECTOR_SIZE);
318: 		} else {
319: 			vector_index = 0;
320: 			max_row = state.current_row_group->start + state.current_row_group->count;
321: 		}
322: 		max_row = MinValue<idx_t>(max_row, state.max_row);
323: 		bool need_to_scan = InitializeScanInRowGroup(scan_state, column_ids, scan_state.table_filters,
324: 		                                             state.current_row_group, vector_index, max_row);
325: 		if (ClientConfig::GetConfig(context).verify_parallelism) {
326: 			state.vector_index++;
327: 			if (state.vector_index * STANDARD_VECTOR_SIZE >= state.current_row_group->count) {
328: 				state.current_row_group = (RowGroup *)state.current_row_group->next.get();
329: 				state.vector_index = 0;
330: 			}
331: 		} else {
332: 			state.current_row_group = (RowGroup *)state.current_row_group->next.get();
333: 		}
334: 		if (!need_to_scan) {
335: 			// filters allow us to skip this row group: move to the next row group
336: 			continue;
337: 		}
338: 		return true;
339: 	}
340: 	if (!state.transaction_local_data) {
341: 		auto &transaction = Transaction::GetTransaction(context);
342: 		// create a task for scanning the local data
343: 		scan_state.row_group_scan_state.max_row = 0;
344: 		scan_state.max_row = 0;
345: 		transaction.storage.InitializeScan(this, scan_state.local_state, scan_state.table_filters);
346: 		scan_state.local_state.max_index = state.local_state.max_index;
347: 		scan_state.local_state.last_chunk_count = state.local_state.last_chunk_count;
348: 		state.transaction_local_data = true;
349: 		return true;
350: 	} else {
351: 		// finished all scans: no more scans remaining
352: 		return false;
353: 	}
354: }
355: 
356: void DataTable::Scan(Transaction &transaction, DataChunk &result, TableScanState &state, vector<column_t> &column_ids) {
357: 	// scan the persistent segments
358: 	if (ScanBaseTable(transaction, result, state)) {
359: 		D_ASSERT(result.size() > 0);
360: 		return;
361: 	}
362: 
363: 	// scan the transaction-local segments
364: 	transaction.storage.Scan(state.local_state, column_ids, result);
365: }
366: 
367: bool DataTable::ScanBaseTable(Transaction &transaction, DataChunk &result, TableScanState &state) {
368: 	auto current_row_group = state.row_group_scan_state.row_group;
369: 	while (current_row_group) {
370: 		current_row_group->Scan(transaction, state.row_group_scan_state, result);
371: 		if (result.size() > 0) {
372: 			return true;
373: 		} else {
374: 			do {
375: 				current_row_group = state.row_group_scan_state.row_group = (RowGroup *)current_row_group->next.get();
376: 				if (current_row_group) {
377: 					bool scan_row_group = current_row_group->InitializeScan(state.row_group_scan_state);
378: 					if (scan_row_group) {
379: 						// skip this row group
380: 						break;
381: 					}
382: 				}
383: 			} while (current_row_group);
384: 		}
385: 	}
386: 	return false;
387: }
388: 
389: //===--------------------------------------------------------------------===//
390: // Fetch
391: //===--------------------------------------------------------------------===//
392: void DataTable::Fetch(Transaction &transaction, DataChunk &result, const vector<column_t> &column_ids,
393:                       Vector &row_identifiers, idx_t fetch_count, ColumnFetchState &state) {
394: 	// figure out which row_group to fetch from
395: 	auto row_ids = FlatVector::GetData<row_t>(row_identifiers);
396: 	idx_t count = 0;
397: 	for (idx_t i = 0; i < fetch_count; i++) {
398: 		auto row_id = row_ids[i];
399: 		auto row_group = (RowGroup *)row_groups->GetSegment(row_id);
400: 		if (!row_group->Fetch(transaction, row_id - row_group->start)) {
401: 			continue;
402: 		}
403: 		row_group->FetchRow(transaction, state, column_ids, row_id, result, count);
404: 		count++;
405: 	}
406: 	result.SetCardinality(count);
407: }
408: 
409: //===--------------------------------------------------------------------===//
410: // Append
411: //===--------------------------------------------------------------------===//
412: static void VerifyNotNullConstraint(TableCatalogEntry &table, Vector &vector, idx_t count, string &col_name) {
413: 	if (VectorOperations::HasNull(vector, count)) {
414: 		throw ConstraintException("NOT NULL constraint failed: %s.%s", table.name, col_name);
415: 	}
416: }
417: 
418: static void VerifyCheckConstraint(TableCatalogEntry &table, Expression &expr, DataChunk &chunk) {
419: 	ExpressionExecutor executor(expr);
420: 	Vector result(LogicalType::INTEGER);
421: 	try {
422: 		executor.ExecuteExpression(chunk, result);
423: 	} catch (std::exception &ex) {
424: 		throw ConstraintException("CHECK constraint failed: %s (Error: %s)", table.name, ex.what());
425: 	} catch (...) { // LCOV_EXCL_START
426: 		throw ConstraintException("CHECK constraint failed: %s (Unknown Error)", table.name);
427: 	} // LCOV_EXCL_STOP
428: 	VectorData vdata;
429: 	result.Orrify(chunk.size(), vdata);
430: 
431: 	auto dataptr = (int32_t *)vdata.data;
432: 	for (idx_t i = 0; i < chunk.size(); i++) {
433: 		auto idx = vdata.sel->get_index(i);
434: 		if (vdata.validity.RowIsValid(idx) && dataptr[idx] == 0) {
435: 			throw ConstraintException("CHECK constraint failed: %s", table.name);
436: 		}
437: 	}
438: }
439: 
440: static bool FindColumnIndex(const vector<idx_t> *keys_ptr, const vector<column_t> &index_column_ids) {
441: 	for (idx_t i = 0; i < keys_ptr->size(); i++) {
442: 		bool is_found = false;
443: 		for (idx_t j = 0; j < index_column_ids.size(); j++) {
444: 			if ((*keys_ptr)[i] == index_column_ids[j]) {
445: 				is_found = true;
446: 				break;
447: 			}
448: 		}
449: 		if (!is_found) {
450: 			return false;
451: 		}
452: 	}
453: 
454: 	return true;
455: }
456: 
457: static void VerifyForeignKeyConstraint(const BoundForeignKeyConstraint &bfk, ClientContext &context, DataChunk &chunk,
458:                                        bool is_append) {
459: 	const vector<idx_t> *src_keys_ptr = &bfk.info.fk_keys;
460: 	const vector<idx_t> *dst_keys_ptr = &bfk.info.pk_keys;
461: 	if (!is_append) {
462: 		src_keys_ptr = &bfk.info.pk_keys;
463: 		dst_keys_ptr = &bfk.info.fk_keys;
464: 	}
465: 
466: 	auto table_entry_ptr =
467: 	    Catalog::GetCatalog(context).GetEntry<TableCatalogEntry>(context, bfk.info.schema, bfk.info.table);
468: 	if (table_entry_ptr == nullptr) {
469: 		throw InternalException("Can't find table \"%s\" in foreign key constraint", bfk.info.table);
470: 	}
471: 
472: 	// make the data chunk to check
473: 	vector<LogicalType> types;
474: 	for (idx_t i = 0; i < table_entry_ptr->columns.size(); i++) {
475: 		types.emplace_back(table_entry_ptr->columns[i].type);
476: 	}
477: 	DataChunk dst_chunk;
478: 	dst_chunk.InitializeEmpty(types);
479: 	for (idx_t i = 0; i < src_keys_ptr->size(); i++) {
480: 		dst_chunk.data[(*dst_keys_ptr)[i]].Reference(chunk.data[(*src_keys_ptr)[i]]);
481: 	}
482: 	dst_chunk.SetCardinality(chunk.size());
483: 	auto data_table = table_entry_ptr->storage.get();
484: 
485: 	idx_t count = dst_chunk.size();
486: 	if (count <= 0) {
487: 		return;
488: 	}
489: 
490: 	// we need to look at the error messages concurrently in data table's index and transaction local storage's index
491: 	vector<string> err_msgs, tran_err_msgs;
492: 	err_msgs.resize(count);
493: 	tran_err_msgs.resize(count);
494: 
495: 	// check whether or not the chunk can be inserted or deleted into the referenced table' storage
496: 	TableIndexList &table_indices = data_table->info->indexes;
497: 	table_indices.Scan([&](Index &index) {
498: 		if (FindColumnIndex(dst_keys_ptr, index.column_ids)) {
499: 			if (is_append) {
500: 				index.VerifyAppendForeignKey(dst_chunk, err_msgs.data());
501: 			} else {
502: 				index.VerifyDeleteForeignKey(dst_chunk, err_msgs.data());
503: 			}
504: 		}
505: 		return false;
506: 	});
507: 
508: 	// check whether or not the chunk can be inserted or deleted into the referenced table' transaction local storage
509: 	auto &transaction = Transaction::GetTransaction(context);
510: 	bool transaction_check = transaction.storage.Find(data_table);
511: 	if (transaction_check) {
512: 		vector<unique_ptr<Index>> &transact_index_vec = transaction.storage.GetIndexes(data_table);
513: 		for (idx_t i = 0; i < transact_index_vec.size(); i++) {
514: 			if (FindColumnIndex(dst_keys_ptr, transact_index_vec[i]->column_ids)) {
515: 				if (is_append) {
516: 					transact_index_vec[i]->VerifyAppendForeignKey(dst_chunk, tran_err_msgs.data());
517: 				} else {
518: 					transact_index_vec[i]->VerifyDeleteForeignKey(dst_chunk, tran_err_msgs.data());
519: 				}
520: 			}
521: 		}
522: 	}
523: 
524: 	// we need to look at the error messages concurrently in data table's index and transaction local storage's index
525: 	for (idx_t i = 0; i < count; i++) {
526: 		if (!transaction_check) {
527: 			// if there is no transaction-local data we only need to check if there is an error message in the main
528: 			// index
529: 			if (!err_msgs[i].empty()) {
530: 				throw ConstraintException(err_msgs[i]);
531: 			} else {
532: 				continue;
533: 			}
534: 		}
535: 		if (is_append) {
536: 			// if we are appending we need to check to ensure the foreign key exists in either the transaction-local
537: 			// storage or the main table
538: 			if (!err_msgs[i].empty() && !tran_err_msgs[i].empty()) {
539: 				throw ConstraintException(err_msgs[i]);
540: 			} else {
541: 				continue;
542: 			}
543: 		}
544: 		// if we are deleting we need to ensure the foreign key DOES NOT exist in EITHER the transaction-local storage
545: 		// OR the main table
546: 		if (!err_msgs[i].empty() || !tran_err_msgs[i].empty()) {
547: 			string &err_msg = err_msgs[i];
548: 			if (err_msg.empty()) {
549: 				err_msg = tran_err_msgs[i];
550: 			}
551: 			throw ConstraintException(err_msg);
552: 		}
553: 	}
554: }
555: 
556: static void VerifyAppendForeignKeyConstraint(const BoundForeignKeyConstraint &bfk, ClientContext &context,
557:                                              DataChunk &chunk) {
558: 	VerifyForeignKeyConstraint(bfk, context, chunk, true);
559: }
560: 
561: static void VerifyDeleteForeignKeyConstraint(const BoundForeignKeyConstraint &bfk, ClientContext &context,
562:                                              DataChunk &chunk) {
563: 	VerifyForeignKeyConstraint(bfk, context, chunk, false);
564: }
565: 
566: void DataTable::VerifyAppendConstraints(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk) {
567: 	for (auto &constraint : table.bound_constraints) {
568: 		switch (constraint->type) {
569: 		case ConstraintType::NOT_NULL: {
570: 			auto &not_null = *reinterpret_cast<BoundNotNullConstraint *>(constraint.get());
571: 			VerifyNotNullConstraint(table, chunk.data[not_null.index], chunk.size(),
572: 			                        table.columns[not_null.index].name);
573: 			break;
574: 		}
575: 		case ConstraintType::CHECK: {
576: 			auto &check = *reinterpret_cast<BoundCheckConstraint *>(constraint.get());
577: 			VerifyCheckConstraint(table, *check.expression, chunk);
578: 			break;
579: 		}
580: 		case ConstraintType::UNIQUE: {
581: 			//! check whether or not the chunk can be inserted into the indexes
582: 			info->indexes.Scan([&](Index &index) {
583: 				index.VerifyAppend(chunk);
584: 				return false;
585: 			});
586: 			break;
587: 		}
588: 		case ConstraintType::FOREIGN_KEY: {
589: 			auto &bfk = *reinterpret_cast<BoundForeignKeyConstraint *>(constraint.get());
590: 			if (bfk.info.type == ForeignKeyType::FK_TYPE_FOREIGN_KEY_TABLE ||
591: 			    bfk.info.type == ForeignKeyType::FK_TYPE_SELF_REFERENCE_TABLE) {
592: 				VerifyAppendForeignKeyConstraint(bfk, context, chunk);
593: 			}
594: 			break;
595: 		}
596: 		default:
597: 			throw NotImplementedException("Constraint type not implemented!");
598: 		}
599: 	}
600: }
601: 
602: void DataTable::Append(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk) {
603: 	if (chunk.size() == 0) {
604: 		return;
605: 	}
606: 	if (chunk.ColumnCount() != table.columns.size()) {
607: 		throw InternalException("Mismatch in column count for append");
608: 	}
609: 	if (!is_root) {
610: 		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
611: 	}
612: 
613: 	chunk.Verify();
614: 
615: 	// verify any constraints on the new chunk
616: 	VerifyAppendConstraints(table, context, chunk);
617: 
618: 	// append to the transaction local data
619: 	auto &transaction = Transaction::GetTransaction(context);
620: 	transaction.storage.Append(this, chunk);
621: }
622: 
623: void DataTable::InitializeAppend(Transaction &transaction, TableAppendState &state, idx_t append_count) {
624: 	// obtain the append lock for this table
625: 	state.append_lock = unique_lock<mutex>(append_lock);
626: 	if (!is_root) {
627: 		throw TransactionException("Transaction conflict: adding entries to a table that has been altered!");
628: 	}
629: 	state.row_start = total_rows;
630: 	state.current_row = state.row_start;
631: 	state.remaining_append_count = append_count;
632: 
633: 	// start writing to the row_groups
634: 	lock_guard<mutex> row_group_lock(row_groups->node_lock);
635: 	auto last_row_group = (RowGroup *)row_groups->GetLastSegment();
636: 	D_ASSERT(total_rows == last_row_group->start + last_row_group->count);
637: 	last_row_group->InitializeAppend(transaction, state.row_group_append_state, state.remaining_append_count);
638: 	total_rows += append_count;
639: }
640: 
641: void DataTable::Append(Transaction &transaction, DataChunk &chunk, TableAppendState &state) {
642: 	D_ASSERT(is_root);
643: 	D_ASSERT(chunk.ColumnCount() == column_definitions.size());
644: 	chunk.Verify();
645: 
646: 	idx_t append_count = chunk.size();
647: 	idx_t remaining = chunk.size();
648: 	while (true) {
649: 		auto current_row_group = state.row_group_append_state.row_group;
650: 		// check how much we can fit into the current row_group
651: 		idx_t append_count =
652: 		    MinValue<idx_t>(remaining, RowGroup::ROW_GROUP_SIZE - state.row_group_append_state.offset_in_row_group);
653: 		if (append_count > 0) {
654: 			current_row_group->Append(state.row_group_append_state, chunk, append_count);
655: 			// merge the stats
656: 			lock_guard<mutex> stats_guard(stats_lock);
657: 			for (idx_t i = 0; i < column_definitions.size(); i++) {
658: 				column_stats[i]->Merge(*current_row_group->GetStatistics(i));
659: 			}
660: 		}
661: 		state.remaining_append_count -= append_count;
662: 		remaining -= append_count;
663: 		if (remaining > 0) {
664: 			// we expect max 1 iteration of this loop (i.e. a single chunk should never overflow more than one
665: 			// row_group)
666: 			D_ASSERT(chunk.size() == remaining + append_count);
667: 			// slice the input chunk
668: 			if (remaining < chunk.size()) {
669: 				SelectionVector sel(STANDARD_VECTOR_SIZE);
670: 				for (idx_t i = 0; i < remaining; i++) {
671: 					sel.set_index(i, append_count + i);
672: 				}
673: 				chunk.Slice(sel, remaining);
674: 			}
675: 			// append a new row_group
676: 			AppendRowGroup(current_row_group->start + current_row_group->count);
677: 			// set up the append state for this row_group
678: 			lock_guard<mutex> row_group_lock(row_groups->node_lock);
679: 			auto last_row_group = (RowGroup *)row_groups->GetLastSegment();
680: 			last_row_group->InitializeAppend(transaction, state.row_group_append_state, state.remaining_append_count);
681: 			continue;
682: 		} else {
683: 			break;
684: 		}
685: 	}
686: 	state.current_row += append_count;
687: }
688: 
689: void DataTable::ScanTableSegment(idx_t row_start, idx_t count, const std::function<void(DataChunk &chunk)> &function) {
690: 	idx_t end = row_start + count;
691: 
692: 	vector<column_t> column_ids;
693: 	vector<LogicalType> types;
694: 	for (idx_t i = 0; i < this->column_definitions.size(); i++) {
695: 		column_ids.push_back(i);
696: 		types.push_back(column_definitions[i].type);
697: 	}
698: 	DataChunk chunk;
699: 	chunk.Initialize(types);
700: 
701: 	CreateIndexScanState state;
702: 
703: 	idx_t row_start_aligned = row_start / STANDARD_VECTOR_SIZE * STANDARD_VECTOR_SIZE;
704: 	InitializeScanWithOffset(state, column_ids, row_start_aligned, row_start + count);
705: 
706: 	idx_t current_row = row_start_aligned;
707: 	while (current_row < end) {
708: 		ScanCreateIndex(state, chunk, TableScanType::TABLE_SCAN_COMMITTED_ROWS);
709: 		if (chunk.size() == 0) {
710: 			break;
711: 		}
712: 		idx_t end_row = current_row + chunk.size();
713: 		// figure out if we need to write the entire chunk or just part of it
714: 		idx_t chunk_start = MaxValue<idx_t>(current_row, row_start);
715: 		idx_t chunk_end = MinValue<idx_t>(end_row, end);
716: 		D_ASSERT(chunk_start < chunk_end);
717: 		idx_t chunk_count = chunk_end - chunk_start;
718: 		if (chunk_count != chunk.size()) {
719: 			// need to slice the chunk before insert
720: 			auto start_in_chunk = chunk_start % STANDARD_VECTOR_SIZE;
721: 			SelectionVector sel(start_in_chunk, chunk_count);
722: 			chunk.Slice(sel, chunk_count);
723: 			chunk.Verify();
724: 		}
725: 		function(chunk);
726: 		chunk.Reset();
727: 		current_row = end_row;
728: 	}
729: }
730: 
731: void DataTable::WriteToLog(WriteAheadLog &log, idx_t row_start, idx_t count) {
732: 	log.WriteSetTable(info->schema, info->table);
733: 	ScanTableSegment(row_start, count, [&](DataChunk &chunk) { log.WriteInsert(chunk); });
734: }
735: 
736: void DataTable::CommitAppend(transaction_t commit_id, idx_t row_start, idx_t count) {
737: 	lock_guard<mutex> lock(append_lock);
738: 
739: 	auto row_group = (RowGroup *)row_groups->GetSegment(row_start);
740: 	idx_t current_row = row_start;
741: 	idx_t remaining = count;
742: 	while (true) {
743: 		idx_t start_in_row_group = current_row - row_group->start;
744: 		idx_t append_count = MinValue<idx_t>(row_group->count - start_in_row_group, remaining);
745: 
746: 		row_group->CommitAppend(commit_id, start_in_row_group, append_count);
747: 
748: 		current_row += append_count;
749: 		remaining -= append_count;
750: 		if (remaining == 0) {
751: 			break;
752: 		}
753: 		row_group = (RowGroup *)row_group->next.get();
754: 	}
755: 	info->cardinality += count;
756: }
757: 
758: void DataTable::RevertAppendInternal(idx_t start_row, idx_t count) {
759: 	if (count == 0) {
760: 		// nothing to revert!
761: 		return;
762: 	}
763: 	if (total_rows != start_row + count) {
764: 		// interleaved append: don't do anything
765: 		// in this case the rows will stay as "inserted by transaction X", but will never be committed
766: 		// they will never be used by any other transaction and will essentially leave a gap
767: 		// this situation is rare, and as such we don't care about optimizing it (yet?)
768: 		// it only happens if C1 appends a lot of data -> C2 appends a lot of data -> C1 rolls back
769: 		return;
770: 	}
771: 	// adjust the cardinality
772: 	info->cardinality = start_row;
773: 	total_rows = start_row;
774: 	D_ASSERT(is_root);
775: 	// revert appends made to row_groups
776: 	lock_guard<mutex> tree_lock(row_groups->node_lock);
777: 	// find the segment index that the current row belongs to
778: 	idx_t segment_index = row_groups->GetSegmentIndex(start_row);
779: 	auto segment = row_groups->nodes[segment_index].node;
780: 	auto &info = (RowGroup &)*segment;
781: 
782: 	// remove any segments AFTER this segment: they should be deleted entirely
783: 	if (segment_index < row_groups->nodes.size() - 1) {
784: 		row_groups->nodes.erase(row_groups->nodes.begin() + segment_index + 1, row_groups->nodes.end());
785: 	}
786: 	info.next = nullptr;
787: 	info.RevertAppend(start_row);
788: }
789: 
790: void DataTable::RevertAppend(idx_t start_row, idx_t count) {
791: 	lock_guard<mutex> lock(append_lock);
792: 
793: 	if (!info->indexes.Empty()) {
794: 		idx_t current_row_base = start_row;
795: 		row_t row_data[STANDARD_VECTOR_SIZE];
796: 		Vector row_identifiers(LogicalType::ROW_TYPE, (data_ptr_t)row_data);
797: 		ScanTableSegment(start_row, count, [&](DataChunk &chunk) {
798: 			for (idx_t i = 0; i < chunk.size(); i++) {
799: 				row_data[i] = current_row_base + i;
800: 			}
801: 			info->indexes.Scan([&](Index &index) {
802: 				index.Delete(chunk, row_identifiers);
803: 				return false;
804: 			});
805: 			current_row_base += chunk.size();
806: 		});
807: 	}
808: 	RevertAppendInternal(start_row, count);
809: }
810: 
811: //===--------------------------------------------------------------------===//
812: // Indexes
813: //===--------------------------------------------------------------------===//
814: bool DataTable::AppendToIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {
815: 	D_ASSERT(is_root);
816: 	if (info->indexes.Empty()) {
817: 		return true;
818: 	}
819: 	// first generate the vector of row identifiers
820: 	Vector row_identifiers(LogicalType::ROW_TYPE);
821: 	VectorOperations::GenerateSequence(row_identifiers, chunk.size(), row_start, 1);
822: 
823: 	vector<Index *> already_appended;
824: 	bool append_failed = false;
825: 	// now append the entries to the indices
826: 	info->indexes.Scan([&](Index &index) {
827: 		if (!index.Append(chunk, row_identifiers)) {
828: 			append_failed = true;
829: 			return true;
830: 		}
831: 		already_appended.push_back(&index);
832: 		return false;
833: 	});
834: 
835: 	if (append_failed) {
836: 		// constraint violation!
837: 		// remove any appended entries from previous indexes (if any)
838: 
839: 		for (auto *index : already_appended) {
840: 			index->Delete(chunk, row_identifiers);
841: 		}
842: 
843: 		return false;
844: 	}
845: 	return true;
846: }
847: 
848: void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, row_t row_start) {
849: 	D_ASSERT(is_root);
850: 	if (info->indexes.Empty()) {
851: 		return;
852: 	}
853: 	// first generate the vector of row identifiers
854: 	Vector row_identifiers(LogicalType::ROW_TYPE);
855: 	VectorOperations::GenerateSequence(row_identifiers, chunk.size(), row_start, 1);
856: 
857: 	// now remove the entries from the indices
858: 	RemoveFromIndexes(state, chunk, row_identifiers);
859: }
860: 
861: void DataTable::RemoveFromIndexes(TableAppendState &state, DataChunk &chunk, Vector &row_identifiers) {
862: 	D_ASSERT(is_root);
863: 	info->indexes.Scan([&](Index &index) {
864: 		index.Delete(chunk, row_identifiers);
865: 		return false;
866: 	});
867: }
868: 
869: void DataTable::RemoveFromIndexes(Vector &row_identifiers, idx_t count) {
870: 	D_ASSERT(is_root);
871: 	auto row_ids = FlatVector::GetData<row_t>(row_identifiers);
872: 
873: 	// figure out which row_group to fetch from
874: 	auto row_group = (RowGroup *)row_groups->GetSegment(row_ids[0]);
875: 	auto row_group_vector_idx = (row_ids[0] - row_group->start) / STANDARD_VECTOR_SIZE;
876: 	auto base_row_id = row_group_vector_idx * STANDARD_VECTOR_SIZE + row_group->start;
877: 
878: 	// create a selection vector from the row_ids
879: 	SelectionVector sel(STANDARD_VECTOR_SIZE);
880: 	for (idx_t i = 0; i < count; i++) {
881: 		auto row_in_vector = row_ids[i] - base_row_id;
882: 		D_ASSERT(row_in_vector < STANDARD_VECTOR_SIZE);
883: 		sel.set_index(i, row_in_vector);
884: 	}
885: 
886: 	// now fetch the columns from that row_group
887: 	// FIXME: we do not need to fetch all columns, only the columns required by the indices!
888: 	TableScanState state;
889: 	state.max_row = total_rows;
890: 	auto types = GetTypes();
891: 	for (idx_t i = 0; i < types.size(); i++) {
892: 		state.column_ids.push_back(i);
893: 	}
894: 	DataChunk result;
895: 	result.Initialize(types);
896: 
897: 	row_group->InitializeScanWithOffset(state.row_group_scan_state, row_group_vector_idx);
898: 	row_group->ScanCommitted(state.row_group_scan_state, result,
899: 	                         TableScanType::TABLE_SCAN_COMMITTED_ROWS_DISALLOW_UPDATES);
900: 	result.Slice(sel, count);
901: 
902: 	info->indexes.Scan([&](Index &index) {
903: 		index.Delete(result, row_identifiers);
904: 		return false;
905: 	});
906: }
907: 
908: void DataTable::VerifyDeleteConstraints(TableCatalogEntry &table, ClientContext &context, DataChunk &chunk) {
909: 	for (auto &constraint : table.bound_constraints) {
910: 		switch (constraint->type) {
911: 		case ConstraintType::NOT_NULL:
912: 		case ConstraintType::CHECK:
913: 		case ConstraintType::UNIQUE:
914: 			break;
915: 		case ConstraintType::FOREIGN_KEY: {
916: 			auto &bfk = *reinterpret_cast<BoundForeignKeyConstraint *>(constraint.get());
917: 			if (bfk.info.type == ForeignKeyType::FK_TYPE_PRIMARY_KEY_TABLE ||
918: 			    bfk.info.type == ForeignKeyType::FK_TYPE_SELF_REFERENCE_TABLE) {
919: 				VerifyDeleteForeignKeyConstraint(bfk, context, chunk);
920: 			}
921: 			break;
922: 		}
923: 		default:
924: 			throw NotImplementedException("Constraint type not implemented!");
925: 		}
926: 	}
927: }
928: 
929: //===--------------------------------------------------------------------===//
930: // Delete
931: //===--------------------------------------------------------------------===//
932: idx_t DataTable::Delete(TableCatalogEntry &table, ClientContext &context, Vector &row_identifiers, idx_t count) {
933: 	D_ASSERT(row_identifiers.GetType().InternalType() == ROW_TYPE);
934: 	if (count == 0) {
935: 		return 0;
936: 	}
937: 
938: 	auto &transaction = Transaction::GetTransaction(context);
939: 
940: 	row_identifiers.Normalify(count);
941: 	auto ids = FlatVector::GetData<row_t>(row_identifiers);
942: 	auto first_id = ids[0];
943: 
944: 	// verify any constraints on the delete rows
945: 	DataChunk verify_chunk;
946: 	if (first_id >= MAX_ROW_ID) {
947: 		transaction.storage.FetchChunk(this, row_identifiers, count, verify_chunk);
948: 	} else {
949: 		ColumnFetchState fetch_state;
950: 		vector<column_t> col_ids;
951: 		vector<LogicalType> types;
952: 		for (idx_t i = 0; i < column_definitions.size(); i++) {
953: 			col_ids.push_back(column_definitions[i].oid);
954: 			types.emplace_back(column_definitions[i].type);
955: 		}
956: 		verify_chunk.Initialize(types);
957: 		Fetch(transaction, verify_chunk, col_ids, row_identifiers, count, fetch_state);
958: 	}
959: 	VerifyDeleteConstraints(table, context, verify_chunk);
960: 
961: 	if (first_id >= MAX_ROW_ID) {
962: 		// deletion is in transaction-local storage: push delete into local chunk collection
963: 		return transaction.storage.Delete(this, row_identifiers, count);
964: 	} else {
965: 		idx_t delete_count = 0;
966: 		// delete is in the row groups
967: 		// we need to figure out for each id to which row group it belongs
968: 		// usually all (or many) ids belong to the same row group
969: 		// we iterate over the ids and check for every id if it belongs to the same row group as their predecessor
970: 		idx_t pos = 0;
971: 		do {
972: 			idx_t start = pos;
973: 			auto row_group = (RowGroup *)row_groups->GetSegment(ids[pos]);
974: 			for (pos++; pos < count; pos++) {
975: 				D_ASSERT(ids[pos] >= 0);
976: 				// check if this id still belongs to this row group
977: 				if (idx_t(ids[pos]) < row_group->start) {
978: 					// id is before row_group start -> it does not
979: 					break;
980: 				}
981: 				if (idx_t(ids[pos]) >= row_group->start + row_group->count) {
982: 					// id is after row group end -> it does not
983: 					break;
984: 				}
985: 			}
986: 			delete_count += row_group->Delete(transaction, this, ids + start, pos - start);
987: 		} while (pos < count);
988: 		return delete_count;
989: 	}
990: }
991: 
992: //===--------------------------------------------------------------------===//
993: // Update
994: //===--------------------------------------------------------------------===//
995: static void CreateMockChunk(vector<LogicalType> &types, const vector<column_t> &column_ids, DataChunk &chunk,
996:                             DataChunk &mock_chunk) {
997: 	// construct a mock DataChunk
998: 	mock_chunk.InitializeEmpty(types);
999: 	for (column_t i = 0; i < column_ids.size(); i++) {
1000: 		mock_chunk.data[column_ids[i]].Reference(chunk.data[i]);
1001: 	}
1002: 	mock_chunk.SetCardinality(chunk.size());
1003: }
1004: 
1005: static bool CreateMockChunk(TableCatalogEntry &table, const vector<column_t> &column_ids,
1006:                             unordered_set<column_t> &desired_column_ids, DataChunk &chunk, DataChunk &mock_chunk) {
1007: 	idx_t found_columns = 0;
1008: 	// check whether the desired columns are present in the UPDATE clause
1009: 	for (column_t i = 0; i < column_ids.size(); i++) {
1010: 		if (desired_column_ids.find(column_ids[i]) != desired_column_ids.end()) {
1011: 			found_columns++;
1012: 		}
1013: 	}
1014: 	if (found_columns == 0) {
1015: 		// no columns were found: no need to check the constraint again
1016: 		return false;
1017: 	}
1018: 	if (found_columns != desired_column_ids.size()) {
1019: 		// FIXME: not all columns in UPDATE clause are present!
1020: 		// this should not be triggered at all as the binder should add these columns
1021: 		throw InternalException("Not all columns required for the CHECK constraint are present in the UPDATED chunk!");
1022: 	}
1023: 	// construct a mock DataChunk
1024: 	auto types = table.GetTypes();
1025: 	CreateMockChunk(types, column_ids, chunk, mock_chunk);
1026: 	return true;
1027: }
1028: 
1029: void DataTable::VerifyUpdateConstraints(TableCatalogEntry &table, DataChunk &chunk,
1030:                                         const vector<column_t> &column_ids) {
1031: 	for (auto &constraint : table.bound_constraints) {
1032: 		switch (constraint->type) {
1033: 		case ConstraintType::NOT_NULL: {
1034: 			auto &not_null = *reinterpret_cast<BoundNotNullConstraint *>(constraint.get());
1035: 			// check if the constraint is in the list of column_ids
1036: 			for (idx_t i = 0; i < column_ids.size(); i++) {
1037: 				if (column_ids[i] == not_null.index) {
1038: 					// found the column id: check the data in
1039: 					VerifyNotNullConstraint(table, chunk.data[i], chunk.size(), table.columns[not_null.index].name);
1040: 					break;
1041: 				}
1042: 			}
1043: 			break;
1044: 		}
1045: 		case ConstraintType::CHECK: {
1046: 			auto &check = *reinterpret_cast<BoundCheckConstraint *>(constraint.get());
1047: 
1048: 			DataChunk mock_chunk;
1049: 			if (CreateMockChunk(table, column_ids, check.bound_columns, chunk, mock_chunk)) {
1050: 				VerifyCheckConstraint(table, *check.expression, mock_chunk);
1051: 			}
1052: 			break;
1053: 		}
1054: 		case ConstraintType::UNIQUE:
1055: 		case ConstraintType::FOREIGN_KEY:
1056: 			break;
1057: 		default:
1058: 			throw NotImplementedException("Constraint type not implemented!");
1059: 		}
1060: 	}
1061: 	// update should not be called for indexed columns!
1062: 	// instead update should have been rewritten to delete + update on higher layer
1063: #ifdef DEBUG
1064: 	info->indexes.Scan([&](Index &index) {
1065: 		D_ASSERT(!index.IndexIsUpdated(column_ids));
1066: 		return false;
1067: 	});
1068: 
1069: #endif
1070: }
1071: 
1072: void DataTable::Update(TableCatalogEntry &table, ClientContext &context, Vector &row_ids,
1073:                        const vector<column_t> &column_ids, DataChunk &updates) {
1074: 	D_ASSERT(row_ids.GetType().InternalType() == ROW_TYPE);
1075: 
1076: 	auto count = updates.size();
1077: 	updates.Verify();
1078: 	if (count == 0) {
1079: 		return;
1080: 	}
1081: 
1082: 	if (!is_root) {
1083: 		throw TransactionException("Transaction conflict: cannot update a table that has been altered!");
1084: 	}
1085: 
1086: 	// first verify that no constraints are violated
1087: 	VerifyUpdateConstraints(table, updates, column_ids);
1088: 
1089: 	// now perform the actual update
1090: 	auto &transaction = Transaction::GetTransaction(context);
1091: 
1092: 	updates.Normalify();
1093: 	row_ids.Normalify(count);
1094: 	auto ids = FlatVector::GetData<row_t>(row_ids);
1095: 	auto first_id = FlatVector::GetValue<row_t>(row_ids, 0);
1096: 	if (first_id >= MAX_ROW_ID) {
1097: 		// update is in transaction-local storage: push update into local storage
1098: 		transaction.storage.Update(this, row_ids, column_ids, updates);
1099: 		return;
1100: 	}
1101: 
1102: 	// update is in the row groups
1103: 	// we need to figure out for each id to which row group it belongs
1104: 	// usually all (or many) ids belong to the same row group
1105: 	// we iterate over the ids and check for every id if it belongs to the same row group as their predecessor
1106: 	idx_t pos = 0;
1107: 	do {
1108: 		idx_t start = pos;
1109: 		auto row_group = (RowGroup *)row_groups->GetSegment(ids[pos]);
1110: 		row_t base_id =
1111: 		    row_group->start + ((ids[pos] - row_group->start) / STANDARD_VECTOR_SIZE * STANDARD_VECTOR_SIZE);
1112: 		for (pos++; pos < count; pos++) {
1113: 			D_ASSERT(ids[pos] >= 0);
1114: 			// check if this id still belongs to this vector
1115: 			if (ids[pos] < base_id) {
1116: 				// id is before vector start -> it does not
1117: 				break;
1118: 			}
1119: 			if (ids[pos] >= base_id + STANDARD_VECTOR_SIZE) {
1120: 				// id is after vector end -> it does not
1121: 				break;
1122: 			}
1123: 		}
1124: 		row_group->Update(transaction, updates, ids, start, pos - start, column_ids);
1125: 
1126: 		lock_guard<mutex> stats_guard(stats_lock);
1127: 		for (idx_t i = 0; i < column_ids.size(); i++) {
1128: 			auto column_id = column_ids[i];
1129: 			column_stats[column_id]->Merge(*row_group->GetStatistics(column_id));
1130: 		}
1131: 	} while (pos < count);
1132: }
1133: 
1134: void DataTable::UpdateColumn(TableCatalogEntry &table, ClientContext &context, Vector &row_ids,
1135:                              const vector<column_t> &column_path, DataChunk &updates) {
1136: 	D_ASSERT(row_ids.GetType().InternalType() == ROW_TYPE);
1137: 	D_ASSERT(updates.ColumnCount() == 1);
1138: 	updates.Verify();
1139: 	if (updates.size() == 0) {
1140: 		return;
1141: 	}
1142: 
1143: 	if (!is_root) {
1144: 		throw TransactionException("Transaction conflict: cannot update a table that has been altered!");
1145: 	}
1146: 
1147: 	// now perform the actual update
1148: 	auto &transaction = Transaction::GetTransaction(context);
1149: 
1150: 	updates.Normalify();
1151: 	row_ids.Normalify(updates.size());
1152: 	auto first_id = FlatVector::GetValue<row_t>(row_ids, 0);
1153: 	if (first_id >= MAX_ROW_ID) {
1154: 		throw NotImplementedException("Cannot update a column-path on transaction local data");
1155: 	}
1156: 	// find the row_group this id belongs to
1157: 	auto primary_column_idx = column_path[0];
1158: 	auto row_group = (RowGroup *)row_groups->GetSegment(first_id);
1159: 	row_group->UpdateColumn(transaction, updates, row_ids, column_path);
1160: 
1161: 	lock_guard<mutex> stats_guard(stats_lock);
1162: 	column_stats[primary_column_idx]->Merge(*row_group->GetStatistics(primary_column_idx));
1163: }
1164: 
1165: //===--------------------------------------------------------------------===//
1166: // Create Index Scan
1167: //===--------------------------------------------------------------------===//
1168: void DataTable::InitializeCreateIndexScan(CreateIndexScanState &state, const vector<column_t> &column_ids) {
1169: 	// we grab the append lock to make sure nothing is appended until AFTER we finish the index scan
1170: 	state.append_lock = std::unique_lock<mutex>(append_lock);
1171: 	state.delete_lock = std::unique_lock<mutex>(row_groups->node_lock);
1172: 
1173: 	InitializeScan(state, column_ids);
1174: }
1175: 
1176: bool DataTable::ScanCreateIndex(CreateIndexScanState &state, DataChunk &result, TableScanType type) {
1177: 	auto current_row_group = state.row_group_scan_state.row_group;
1178: 	while (current_row_group) {
1179: 		current_row_group->ScanCommitted(state.row_group_scan_state, result, type);
1180: 		if (result.size() > 0) {
1181: 			return true;
1182: 		} else {
1183: 			current_row_group = state.row_group_scan_state.row_group = (RowGroup *)current_row_group->next.get();
1184: 			if (current_row_group) {
1185: 				current_row_group->InitializeScan(state.row_group_scan_state);
1186: 			}
1187: 		}
1188: 	}
1189: 	return false;
1190: }
1191: 
1192: void DataTable::AddIndex(unique_ptr<Index> index, const vector<unique_ptr<Expression>> &expressions) {
1193: 	DataChunk result;
1194: 	result.Initialize(index->logical_types);
1195: 
1196: 	DataChunk intermediate;
1197: 	vector<LogicalType> intermediate_types;
1198: 	auto column_ids = index->column_ids;
1199: 	column_ids.push_back(COLUMN_IDENTIFIER_ROW_ID);
1200: 	for (auto &id : index->column_ids) {
1201: 		intermediate_types.push_back(column_definitions[id].type);
1202: 	}
1203: 	intermediate_types.emplace_back(LogicalType::ROW_TYPE);
1204: 	intermediate.Initialize(intermediate_types);
1205: 
1206: 	// initialize an index scan
1207: 	CreateIndexScanState state;
1208: 	InitializeCreateIndexScan(state, column_ids);
1209: 
1210: 	if (!is_root) {
1211: 		throw TransactionException("Transaction conflict: cannot add an index to a table that has been altered!");
1212: 	}
1213: 
1214: 	// now start incrementally building the index
1215: 	{
1216: 		IndexLock lock;
1217: 		index->InitializeLock(lock);
1218: 		ExpressionExecutor executor(expressions);
1219: 		while (true) {
1220: 			intermediate.Reset();
1221: 			// scan a new chunk from the table to index
1222: 			ScanCreateIndex(state, intermediate, TableScanType::TABLE_SCAN_COMMITTED_ROWS_OMIT_PERMANENTLY_DELETED);
1223: 			if (intermediate.size() == 0) {
1224: 				// finished scanning for index creation
1225: 				// release all locks
1226: 				break;
1227: 			}
1228: 			// resolve the expressions for this chunk
1229: 			executor.Execute(intermediate, result);
1230: 
1231: 			// insert into the index
1232: 			if (!index->Insert(lock, result, intermediate.data[intermediate.ColumnCount() - 1])) {
1233: 				throw ConstraintException(
1234: 				    "Cant create unique index, table contains duplicate data on indexed column(s)");
1235: 			}
1236: 		}
1237: 	}
1238: 	info->indexes.AddIndex(move(index));
1239: }
1240: 
1241: unique_ptr<BaseStatistics> DataTable::GetStatistics(ClientContext &context, column_t column_id) {
1242: 	if (column_id == COLUMN_IDENTIFIER_ROW_ID) {
1243: 		return nullptr;
1244: 	}
1245: 	lock_guard<mutex> stats_guard(stats_lock);
1246: 	return column_stats[column_id]->Copy();
1247: }
1248: 
1249: //===--------------------------------------------------------------------===//
1250: // Checkpoint
1251: //===--------------------------------------------------------------------===//
1252: BlockPointer DataTable::Checkpoint(TableDataWriter &writer) {
1253: 	// checkpoint each individual row group
1254: 	// FIXME: we might want to combine adjacent row groups in case they have had deletions...
1255: 	vector<unique_ptr<BaseStatistics>> global_stats;
1256: 	for (idx_t i = 0; i < column_definitions.size(); i++) {
1257: 		global_stats.push_back(BaseStatistics::CreateEmpty(column_definitions[i].type));
1258: 	}
1259: 
1260: 	auto row_group = (RowGroup *)row_groups->GetRootSegment();
1261: 	vector<RowGroupPointer> row_group_pointers;
1262: 	while (row_group) {
1263: 		auto pointer = row_group->Checkpoint(writer, global_stats);
1264: 		row_group_pointers.push_back(move(pointer));
1265: 		row_group = (RowGroup *)row_group->next.get();
1266: 	}
1267: 	// store the current position in the metadata writer
1268: 	// this is where the row groups for this table start
1269: 	auto &meta_writer = writer.GetMetaWriter();
1270: 	auto pointer = meta_writer.GetBlockPointer();
1271: 
1272: 	for (auto &stats : global_stats) {
1273: 		stats->Serialize(meta_writer);
1274: 	}
1275: 	// now start writing the row group pointers to disk
1276: 	meta_writer.Write<uint64_t>(row_group_pointers.size());
1277: 	for (auto &row_group_pointer : row_group_pointers) {
1278: 		RowGroup::Serialize(row_group_pointer, meta_writer);
1279: 	}
1280: 	return pointer;
1281: }
1282: 
1283: void DataTable::CommitDropColumn(idx_t index) {
1284: 	auto segment = (RowGroup *)row_groups->GetRootSegment();
1285: 	while (segment) {
1286: 		segment->CommitDropColumn(index);
1287: 		segment = (RowGroup *)segment->next.get();
1288: 	}
1289: }
1290: 
1291: idx_t DataTable::GetTotalRows() {
1292: 	return total_rows;
1293: }
1294: 
1295: void DataTable::CommitDropTable() {
1296: 	// commit a drop of this table: mark all blocks as modified so they can be reclaimed later on
1297: 	auto segment = (RowGroup *)row_groups->GetRootSegment();
1298: 	while (segment) {
1299: 		segment->CommitDrop();
1300: 		segment = (RowGroup *)segment->next.get();
1301: 	}
1302: }
1303: 
1304: //===--------------------------------------------------------------------===//
1305: // GetStorageInfo
1306: //===--------------------------------------------------------------------===//
1307: vector<vector<Value>> DataTable::GetStorageInfo() {
1308: 	vector<vector<Value>> result;
1309: 
1310: 	auto row_group = (RowGroup *)row_groups->GetRootSegment();
1311: 	idx_t row_group_index = 0;
1312: 	while (row_group) {
1313: 		row_group->GetStorageInfo(row_group_index, result);
1314: 		row_group_index++;
1315: 
1316: 		row_group = (RowGroup *)row_group->next.get();
1317: 	}
1318: 
1319: 	return result;
1320: }
1321: 
1322: } // namespace duckdb
[end of src/storage/data_table.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: