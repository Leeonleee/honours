{
  "repo": "duckdb/duckdb",
  "pull_number": 11785,
  "instance_id": "duckdb__duckdb-11785",
  "issue_numbers": [
    "11566"
  ],
  "base_commit": "414f20bbadbfa9545c8b707b13b41ce50e41e1de",
  "patch": "diff --git a/src/core_functions/aggregate/distributive/arg_min_max.cpp b/src/core_functions/aggregate/distributive/arg_min_max.cpp\nindex 01f677e9bfac..ff82da99c898 100644\n--- a/src/core_functions/aggregate/distributive/arg_min_max.cpp\n+++ b/src/core_functions/aggregate/distributive/arg_min_max.cpp\n@@ -1,12 +1,12 @@\n-#include \"duckdb/core_functions/aggregate/distributive_functions.hpp\"\n #include \"duckdb/common/exception.hpp\"\n+#include \"duckdb/common/operator/comparison_operators.hpp\"\n #include \"duckdb/common/vector_operations/vector_operations.hpp\"\n+#include \"duckdb/core_functions/aggregate/distributive_functions.hpp\"\n #include \"duckdb/function/cast/cast_function_set.hpp\"\n #include \"duckdb/function/function_set.hpp\"\n #include \"duckdb/planner/expression/bound_aggregate_expression.hpp\"\n #include \"duckdb/planner/expression/bound_comparison_expression.hpp\"\n #include \"duckdb/planner/expression_binder.hpp\"\n-#include \"duckdb/common/operator/comparison_operators.hpp\"\n \n namespace duckdb {\n \n@@ -174,7 +174,9 @@ struct ArgMinMaxBase {\n \n \tstatic unique_ptr<FunctionData> Bind(ClientContext &context, AggregateFunction &function,\n \t                                     vector<unique_ptr<Expression>> &arguments) {\n-\t\tExpressionBinder::PushCollation(context, arguments[1], arguments[1]->return_type, false);\n+\t\tif (arguments[1]->return_type.InternalType() == PhysicalType::VARCHAR) {\n+\t\t\tExpressionBinder::PushCollation(context, arguments[1], arguments[1]->return_type, false);\n+\t\t}\n \t\tfunction.arguments[0] = arguments[0]->return_type;\n \t\tfunction.return_type = arguments[0]->return_type;\n \t\treturn nullptr;\n@@ -318,9 +320,7 @@ AggregateFunction GetArgMinMaxFunctionInternal(const LogicalType &by_type, const\n \tif (type.InternalType() == PhysicalType::VARCHAR || by_type.InternalType() == PhysicalType::VARCHAR) {\n \t\tfunction.destructor = AggregateFunction::StateDestroy<STATE, OP>;\n \t}\n-\tif (by_type.InternalType() == PhysicalType::VARCHAR) {\n-\t\tfunction.bind = OP::Bind;\n-\t}\n+\tfunction.bind = OP::Bind;\n \treturn function;\n }\n \ndiff --git a/src/execution/operator/scan/physical_expression_scan.cpp b/src/execution/operator/scan/physical_expression_scan.cpp\nindex 5e08bc9471fc..c0e91ae2be42 100644\n--- a/src/execution/operator/scan/physical_expression_scan.cpp\n+++ b/src/execution/operator/scan/physical_expression_scan.cpp\n@@ -1,6 +1,7 @@\n #include \"duckdb/execution/operator/scan/physical_expression_scan.hpp\"\n-#include \"duckdb/parallel/thread_context.hpp\"\n+\n #include \"duckdb/execution/expression_executor.hpp\"\n+#include \"duckdb/parallel/thread_context.hpp\"\n \n namespace duckdb {\n \n@@ -27,8 +28,7 @@ OperatorResultType PhysicalExpressionScan::Execute(ExecutionContext &context, Da\n \tfor (; chunk.size() + input.size() <= STANDARD_VECTOR_SIZE && state.expression_index < expressions.size();\n \t     state.expression_index++) {\n \t\tstate.temp_chunk.Reset();\n-\t\tEvaluateExpression(context.client, state.expression_index, &input, state.temp_chunk);\n-\t\tchunk.Append(state.temp_chunk);\n+\t\tEvaluateExpression(context.client, state.expression_index, &input, chunk, &state.temp_chunk);\n \t}\n \tif (state.expression_index < expressions.size()) {\n \t\treturn OperatorResultType::HAVE_MORE_OUTPUT;\n@@ -38,15 +38,30 @@ OperatorResultType PhysicalExpressionScan::Execute(ExecutionContext &context, Da\n \t}\n }\n \n-void PhysicalExpressionScan::EvaluateExpression(ClientContext &context, idx_t expression_idx, DataChunk *child_chunk,\n-                                                DataChunk &result) const {\n+void PhysicalExpressionScan::EvaluateExpression(ClientContext &context, idx_t expression_idx,\n+                                                optional_ptr<DataChunk> child_chunk, DataChunk &result,\n+                                                optional_ptr<DataChunk> temp_chunk_ptr) const {\n+\tif (temp_chunk_ptr) {\n+\t\tEvaluateExpressionInternal(context, expression_idx, child_chunk, result, *temp_chunk_ptr);\n+\t} else {\n+\t\tDataChunk temp_chunk;\n+\t\ttemp_chunk.Initialize(Allocator::Get(context), GetTypes());\n+\t\tEvaluateExpressionInternal(context, expression_idx, child_chunk, result, temp_chunk);\n+\t}\n+}\n+\n+void PhysicalExpressionScan::EvaluateExpressionInternal(ClientContext &context, idx_t expression_idx,\n+                                                        optional_ptr<DataChunk> child_chunk, DataChunk &result,\n+                                                        DataChunk &temp_chunk) const {\n \tExpressionExecutor executor(context, expressions[expression_idx]);\n \tif (child_chunk) {\n \t\tchild_chunk->Verify();\n-\t\texecutor.Execute(*child_chunk, result);\n+\t\texecutor.Execute(*child_chunk, temp_chunk);\n \t} else {\n-\t\texecutor.Execute(result);\n+\t\texecutor.Execute(temp_chunk);\n \t}\n+\t// Need to append because \"executor\" might be holding state (e.g., strings), which go out of scope here\n+\tresult.Append(temp_chunk);\n }\n \n bool PhysicalExpressionScan::IsFoldable() const {\ndiff --git a/src/execution/physical_plan/plan_distinct.cpp b/src/execution/physical_plan/plan_distinct.cpp\nindex e0bb12d688c9..355169c33aa5 100644\n--- a/src/execution/physical_plan/plan_distinct.cpp\n+++ b/src/execution/physical_plan/plan_distinct.cpp\n@@ -67,6 +67,7 @@ unique_ptr<PhysicalOperator> PhysicalPlanGenerator::CreatePlan(LogicalDistinct &\n \t\t\t\tbool changes_made = false;\n \t\t\t\tauto new_expr = OrderedAggregateOptimizer::Apply(context, *first_aggregate, groups, changes_made);\n \t\t\t\tif (new_expr) {\n+\t\t\t\t\tD_ASSERT(new_expr->return_type == first_aggregate->return_type);\n \t\t\t\t\tD_ASSERT(new_expr->type == ExpressionType::BOUND_AGGREGATE);\n \t\t\t\t\tfirst_aggregate = unique_ptr_cast<Expression, BoundAggregateExpression>(std::move(new_expr));\n \t\t\t\t}\ndiff --git a/src/execution/radix_partitioned_hashtable.cpp b/src/execution/radix_partitioned_hashtable.cpp\nindex 595c1c4c2c83..a3242fc71951 100644\n--- a/src/execution/radix_partitioned_hashtable.cpp\n+++ b/src/execution/radix_partitioned_hashtable.cpp\n@@ -165,8 +165,8 @@ class RadixHTGlobalSinkState : public GlobalSinkState {\n \tbool finalized;\n \t//! Whether we are doing an external aggregation\n \tatomic<bool> external;\n-\t//! Threads that have called Sink\n-\tatomic<idx_t> active_threads;\n+\t//! Whether the aggregation is single-threaded\n+\tconst idx_t number_of_threads;\n \t//! If any thread has called combine\n \tatomic<bool> any_combined;\n \n@@ -192,7 +192,8 @@ class RadixHTGlobalSinkState : public GlobalSinkState {\n \n RadixHTGlobalSinkState::RadixHTGlobalSinkState(ClientContext &context_p, const RadixPartitionedHashTable &radix_ht_p)\n     : context(context_p), temporary_memory_state(TemporaryMemoryManager::Get(context).Register(context)),\n-      radix_ht(radix_ht_p), config(context, *this), finalized(false), external(false), active_threads(0),\n+      radix_ht(radix_ht_p), config(context, *this), finalized(false), external(false),\n+      number_of_threads(NumericCast<idx_t>(TaskScheduler::GetScheduler(context).NumberOfThreads())),\n       any_combined(false), finalize_done(0), scan_pin_properties(TupleDataPinProperties::DESTROY_AFTER_DONE),\n       count_before_combining(0), max_partition_size(0) {\n \n@@ -230,6 +231,7 @@ void RadixHTGlobalSinkState::Destroy() {\n \t}\n \n \t// There are aggregates with destructors: Call the destructor for each of the aggregates\n+\tlock_guard<mutex> guard(lock);\n \tRowOperationsState row_state(*stored_allocators.back());\n \tfor (auto &partition : partitions) {\n \t\tauto &data_collection = *partition->data;\n@@ -358,8 +360,7 @@ void RadixPartitionedHashTable::PopulateGroupChunk(DataChunk &group_chunk, DataC\n \tgroup_chunk.Verify();\n }\n \n-bool MaybeRepartition(ClientContext &context, RadixHTGlobalSinkState &gstate, RadixHTLocalSinkState &lstate,\n-                      const idx_t &active_threads) {\n+bool MaybeRepartition(ClientContext &context, RadixHTGlobalSinkState &gstate, RadixHTLocalSinkState &lstate) {\n \tauto &config = gstate.config;\n \tauto &ht = *lstate.ht;\n \tauto &partitioned_data = ht.GetPartitionedData();\n@@ -367,19 +368,19 @@ bool MaybeRepartition(ClientContext &context, RadixHTGlobalSinkState &gstate, Ra\n \t// Check if we're approaching the memory limit\n \tauto &temporary_memory_state = *gstate.temporary_memory_state;\n \tconst auto total_size = partitioned_data->SizeInBytes() + ht.Capacity() * sizeof(aggr_ht_entry_t);\n-\tidx_t thread_limit = temporary_memory_state.GetReservation() / active_threads;\n+\tidx_t thread_limit = temporary_memory_state.GetReservation() / gstate.number_of_threads;\n \tif (total_size > thread_limit) {\n \t\t// We're over the thread memory limit\n \t\tif (!gstate.external) {\n \t\t\t// We haven't yet triggered out-of-core behavior, but maybe we don't have to, grab the lock and check again\n \t\t\tlock_guard<mutex> guard(gstate.lock);\n-\t\t\tthread_limit = temporary_memory_state.GetReservation() / active_threads;\n+\t\t\tthread_limit = temporary_memory_state.GetReservation() / gstate.number_of_threads;\n \t\t\tif (total_size > thread_limit) {\n \t\t\t\t// Out-of-core would be triggered below, try to increase the reservation\n \t\t\t\tauto remaining_size =\n-\t\t\t\t    MaxValue<idx_t>(active_threads * total_size, temporary_memory_state.GetRemainingSize());\n+\t\t\t\t    MaxValue<idx_t>(gstate.number_of_threads * total_size, temporary_memory_state.GetRemainingSize());\n \t\t\t\ttemporary_memory_state.SetRemainingSize(context, 2 * remaining_size);\n-\t\t\t\tthread_limit = temporary_memory_state.GetReservation() / active_threads;\n+\t\t\t\tthread_limit = temporary_memory_state.GetReservation() / gstate.number_of_threads;\n \t\t\t}\n \t\t}\n \t}\n@@ -402,7 +403,7 @@ bool MaybeRepartition(ClientContext &context, RadixHTGlobalSinkState &gstate, Ra\n \t}\n \n \t// We can go external when there is only one active thread, but we shouldn't repartition here\n-\tif (active_threads < 2) {\n+\tif (gstate.number_of_threads < 2) {\n \t\treturn false;\n \t}\n \n@@ -412,7 +413,7 @@ bool MaybeRepartition(ClientContext &context, RadixHTGlobalSinkState &gstate, Ra\n \n \tconst auto row_size_per_partition =\n \t    partitioned_data->Count() * partitioned_data->GetLayout().GetRowWidth() / partition_count;\n-\tif (row_size_per_partition > config.BLOCK_FILL_FACTOR * Storage::BLOCK_SIZE) {\n+\tif (row_size_per_partition > NumericCast<idx_t>(config.BLOCK_FILL_FACTOR * Storage::BLOCK_SIZE)) {\n \t\t// We crossed our block filling threshold, try to increment radix bits\n \t\tconfig.SetRadixBits(current_radix_bits + config.REPARTITION_RADIX_BITS);\n \t}\n@@ -437,7 +438,6 @@ void RadixPartitionedHashTable::Sink(ExecutionContext &context, DataChunk &chunk\n \tauto &lstate = input.local_state.Cast<RadixHTLocalSinkState>();\n \tif (!lstate.ht) {\n \t\tlstate.ht = CreateHT(context.client, gstate.config.sink_capacity, gstate.config.GetRadixBits());\n-\t\tgstate.active_threads++;\n \t}\n \n \tauto &group_chunk = lstate.group_chunk;\n@@ -450,8 +450,7 @@ void RadixPartitionedHashTable::Sink(ExecutionContext &context, DataChunk &chunk\n \t\treturn; // We can fit another chunk\n \t}\n \n-\tconst idx_t active_threads = gstate.active_threads;\n-\tif (active_threads > 2) {\n+\tif (gstate.number_of_threads > 2) {\n \t\t// 'Reset' the HT without taking its data, we can just keep appending to the same collection\n \t\t// This only works because we never resize the HT\n \t\tht.ClearPointerTable();\n@@ -460,7 +459,7 @@ void RadixPartitionedHashTable::Sink(ExecutionContext &context, DataChunk &chunk\n \t}\n \n \t// Check if we need to repartition\n-\tauto repartitioned = MaybeRepartition(context.client, gstate, lstate, active_threads);\n+\tauto repartitioned = MaybeRepartition(context.client, gstate, lstate);\n \n \tif (repartitioned && ht.Count() != 0) {\n \t\t// We repartitioned, but we didn't clear the pointer table / reset the count because we're on 1 or 2 threads\n@@ -481,7 +480,7 @@ void RadixPartitionedHashTable::Combine(ExecutionContext &context, GlobalSinkSta\n \n \t// Set any_combined, then check one last time whether we need to repartition\n \tgstate.any_combined = true;\n-\tMaybeRepartition(context.client, gstate, lstate, gstate.active_threads);\n+\tMaybeRepartition(context.client, gstate, lstate);\n \n \tauto &ht = *lstate.ht;\n \tht.UnpinData();\n@@ -513,7 +512,7 @@ void RadixPartitionedHashTable::Finalize(ClientContext &context, GlobalSinkState\n \t\tgstate.count_before_combining = uncombined_data.Count();\n \n \t\t// If true there is no need to combine, it was all done by a single thread in a single HT\n-\t\tconst auto single_ht = !gstate.external && gstate.active_threads == 1;\n+\t\tconst auto single_ht = !gstate.external && gstate.number_of_threads == 1;\n \n \t\tauto &uncombined_partition_data = uncombined_data.GetPartitions();\n \t\tconst auto n_partitions = uncombined_partition_data.size();\n@@ -591,7 +590,6 @@ class RadixHTGlobalSourceState : public GlobalSourceState {\n \tvector<column_t> column_ids;\n \n \t//! For synchronizing tasks\n-\tmutex lock;\n \tidx_t task_idx;\n \tatomic<idx_t> task_done;\n };\n@@ -651,7 +649,7 @@ RadixHTGlobalSourceState::RadixHTGlobalSourceState(ClientContext &context_p, con\n SourceResultType RadixHTGlobalSourceState::AssignTask(RadixHTGlobalSinkState &sink, RadixHTLocalSourceState &lstate,\n                                                       InterruptState &interrupt_state) {\n \t// First, try to get a partition index\n-\tlock_guard<mutex> gstate_guard(lock);\n+\tlock_guard<mutex> gstate_guard(sink.lock);\n \tif (finished) {\n \t\treturn SourceResultType::FINISHED;\n \t}\n@@ -720,7 +718,7 @@ void RadixHTLocalSourceState::Finalize(RadixHTGlobalSinkState &sink, RadixHTGlob\n \t\t// However, we will limit the initial capacity so we don't do a huge over-allocation\n \t\tconst auto n_threads = NumericCast<idx_t>(TaskScheduler::GetScheduler(gstate.context).NumberOfThreads());\n \t\tconst auto memory_limit = BufferManager::GetBufferManager(gstate.context).GetMaxMemory();\n-\t\tconst idx_t thread_limit = NumericCast<idx_t>(0.6 * memory_limit / n_threads);\n+\t\tconst idx_t thread_limit = NumericCast<idx_t>(0.6 * double(memory_limit) / double(n_threads));\n \n \t\tconst idx_t size_per_entry = partition.data->SizeInBytes() / MaxValue<idx_t>(partition.data->Count(), 1) +\n \t\t                             idx_t(GroupedAggregateHashTable::LOAD_FACTOR * sizeof(aggr_ht_entry_t));\n@@ -745,7 +743,7 @@ void RadixHTLocalSourceState::Finalize(RadixHTGlobalSinkState &sink, RadixHTGlob\n \tpartition.data->Combine(*ht->GetPartitionedData()->GetPartitions()[0]);\n \n \t// Update thread-global state\n-\tlock_guard<mutex> global_guard(gstate.lock);\n+\tlock_guard<mutex> global_guard(sink.lock);\n \tsink.stored_allocators.emplace_back(ht->GetAggregateAllocator());\n \tconst auto finalizes_done = ++sink.finalize_done;\n \tD_ASSERT(finalizes_done <= sink.partitions.size());\n@@ -785,7 +783,7 @@ void RadixHTLocalSourceState::Scan(RadixHTGlobalSinkState &sink, RadixHTGlobalSo\n \t\t\tdata_collection.Reset();\n \t\t}\n \t\tscan_status = RadixHTScanStatus::DONE;\n-\t\tlock_guard<mutex> gstate_guard(gstate.lock);\n+\t\tlock_guard<mutex> gstate_guard(sink.lock);\n \t\tif (++gstate.task_done == sink.partitions.size()) {\n \t\t\tgstate.finished = true;\n \t\t}\n@@ -912,10 +910,10 @@ double RadixPartitionedHashTable::GetProgress(ClientContext &, GlobalSinkState &\n \t}\n \n \t// Get scan progress, weigh it 1x\n-\ttotal_progress += 1.0 * gstate.task_done;\n+\ttotal_progress += 1.0 * double(gstate.task_done);\n \n \t// Divide by 3x for the weights, and the number of partitions to get a value between 0 and 1 again\n-\ttotal_progress /= 3.0 * sink.partitions.size();\n+\ttotal_progress /= 3.0 * double(sink.partitions.size());\n \n \t// Multiply by 100 to get a percentage\n \treturn 100.0 * total_progress;\ndiff --git a/src/include/duckdb/execution/operator/scan/physical_expression_scan.hpp b/src/include/duckdb/execution/operator/scan/physical_expression_scan.hpp\nindex a6f0d1ae76bb..b934bd69eeca 100644\n--- a/src/include/duckdb/execution/operator/scan/physical_expression_scan.hpp\n+++ b/src/include/duckdb/execution/operator/scan/physical_expression_scan.hpp\n@@ -39,8 +39,12 @@ class PhysicalExpressionScan : public PhysicalOperator {\n \n public:\n \tbool IsFoldable() const;\n-\tvoid EvaluateExpression(ClientContext &context, idx_t expression_idx, DataChunk *child_chunk,\n-\t                        DataChunk &result) const;\n+\tvoid EvaluateExpression(ClientContext &context, idx_t expression_idx, optional_ptr<DataChunk> child_chunk,\n+\t                        DataChunk &result, optional_ptr<DataChunk> temp_chunk_ptr = nullptr) const;\n+\n+private:\n+\tvoid EvaluateExpressionInternal(ClientContext &context, idx_t expression_idx, optional_ptr<DataChunk> child_chunk,\n+\t                                DataChunk &result, DataChunk &temp_chunk) const;\n };\n \n } // namespace duckdb\n",
  "test_patch": "diff --git a/test/issues/general/test_11391.test b/test/issues/general/test_11391.test\nindex 637c901a6dee..0a228b112280 100644\n--- a/test/issues/general/test_11391.test\n+++ b/test/issues/general/test_11391.test\n@@ -1,5 +1,5 @@\n # name: test/issues/general/test_11391.test\n-# description: Issue 1091: Catalog Error with nested CTEs\n+# description: Issue 11391: Catalog Error with nested CTEs\n # group: [general]\n \n query I\ndiff --git a/test/issues/general/test_11566.test b/test/issues/general/test_11566.test\nnew file mode 100644\nindex 000000000000..8dd5a792b8c7\n--- /dev/null\n+++ b/test/issues/general/test_11566.test\n@@ -0,0 +1,18 @@\n+# name: test/issues/general/test_11566.test\n+# description: Issue 11566: Assertion failure when using DISTINCT ON + ORDER BY with JSON column\n+# group: [general]\n+\n+require json\n+\n+statement ok\n+PRAGMA enable_verification\n+\n+query I\n+SELECT typeof(arg_min({foo: 'bar'}::JSON, 1));\n+----\n+JSON\n+\n+query II\n+SELECT DISTINCT ON (my_row_id) my_row_id, value FROM (SELECT * FROM (VALUES ('1', {foo: 'bar'}::JSON, 1), ('1', {foo: 'baz'}::JSON, 2), ) AS t(my_row_id, value, idx)) ORDER BY idx;\n+----\n+1\t{\"foo\":\"bar\"}\ndiff --git a/test/sql/aggregate/aggregates/test_state_export.test b/test/sql/aggregate/aggregates/test_state_export.test\nindex c9a03f028b8d..64980e3e2787 100644\n--- a/test/sql/aggregate/aggregates/test_state_export.test\n+++ b/test/sql/aggregate/aggregates/test_state_export.test\n@@ -111,6 +111,10 @@ SELECT finalize(count(*) EXPORT_STATE), finalize(count(d) EXPORT_STATE), finaliz\n \n # more aggregates\n \n+# we skip these for now as argmin/argmax now has a custom binder (so that it can work with extension types like JSON)\n+# otherwise we get \"Binder Error: Cannot use EXPORT_STATE on aggregate functions with custom binders\"\n+mode skip\n+\n query II nosort res7\n select argmin(a,b), argmax(a,b) from (values (1,1), (2,2), (8,8), (10,10)) s(a,b);\n ----\n@@ -119,6 +123,8 @@ query II nosort res7\n select FINALIZE(argmin(a,b) EXPORT_STATE), FINALIZE(argmax(a,b) EXPORT_STATE) from (values (1,1), (2,2), (8,8), (10,10)) s(a,b);\n ----\n \n+mode unskip\n+\n query IIIIIII nosort res8\n SELECT g, first(d), last(d), fsum(d), favg(d), product(d), bit_xor(d), bool_and(d > 5) FROM dummy GROUP BY g ORDER BY g;\n ----\n",
  "problem_statement": "Assertion failure when using DISTINCT ON + ORDER BY with JSON column (NodeJS)\n### What happens?\r\n\r\nRunning a query (using the NodeJS library) that uses `DISTINCT ON ... ORDER BY` where one of the columns being returned is of type `JSON` fails with the following error:\r\n\r\n```\r\nAssertion failed: (other.GetType() == GetType()), function Reference, file vector.cpp, line 136.\r\n```\r\n\r\nThe error doesn't occur if either the `ORDER BY` or `JSON` columns are removed. The error also doesn't occur in the DuckDB shell.\r\n\r\n### To Reproduce\r\n\r\n```javascript\r\nconst duckdb = require('duckdb');\r\nconst db = new duckdb.Database(':memory:');\r\n\r\nconst query = `\r\n  SELECT DISTINCT ON (row_id)\r\n    row_id,\r\n    value\r\n  FROM (\r\n    SELECT * FROM (VALUES\r\n      ('1', {foo: 'bar'}::JSON, 1),\r\n      ('1', {foo: 'baz'}::JSON, 2),\r\n    ) AS t(row_id, value, idx)\r\n  )\r\n  ORDER BY idx;\r\n`;\r\n\r\ndb.all(query, function (err, res) {\r\n  console.log({ err, res });\r\n});\r\n\r\n```\r\n\r\n### OS:\r\n\r\nMacOS (Apple Silicon)\r\n\r\n### DuckDB Version:\r\n\r\nv0.10.1\r\n\r\n### DuckDB Client:\r\n\r\nNodeJS\r\n\r\n### Full Name:\r\n\r\nTong Liu\r\n\r\n### Affiliation:\r\n\r\nWatershed\r\n\r\n### Have you tried this on the latest [nightly build](https://duckdb.org/docs/installation/?version=main)?\r\n\r\nYes\r\n\r\n### Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n\r\n- [X] Yes, I have\n",
  "hints_text": "",
  "created_at": "2024-04-23T09:52:47Z"
}