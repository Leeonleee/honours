You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
Segfault when creating table with foreign key constraint
#### What happens?

When creating a table with a `FOREIGN KEY` constraint, DuckDB segfaults ~~if the constraint references a nonexistent table.~~ see followup comment -- even in a minimal case where the referenced table exists I'm getting a segfault.

#### To Reproduce

given an `init.sql` file containing the following:

```sql
CREATE TABLE routes (
	route_id TEXT PRIMARY KEY,
	agency_id TEXT,
	FOREIGN KEY (agency_id) REFERENCES agency,
);
```

running

```sh
duckdb < init.sql
```

outputs

```
Segmentation fault (core dumped)
```

instead of a segfault, as a user I would expect an informative error message that would allow me to diagnose the issue.

#### Environment (please complete the following information):
 - OS: Linux
 - DuckDB Version: 0.3.4
 - DuckDB Client: CLI

#### Before Submitting

- **Have you tried this on the latest `master` branch?**

* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.

  :warning: fyi, the link in the above "other platforms" point seems to just point to a four-year-old source tarball: https://github.com/duckdb/duckdb/releases/tag/master-builds

  the [link to download binaries built off master on the duckdb website](https://duckdb.org/docs/installation/) also fails with a 404 for me: https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_cli-linux-amd64.zip

  given these points, I don't know how to test against master, sorry. I'm not going to attempt to build it myself -- we're just evaluating whether DuckDB is in a usable state for our company (I would love for it to be but have to admit landing on this issue in the first few minutes and then hitting broken downloads on the website is not a promising start).

- **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**

Segfault when creating table with foreign key constraint
#### What happens?

When creating a table with a `FOREIGN KEY` constraint, DuckDB segfaults ~~if the constraint references a nonexistent table.~~ see followup comment -- even in a minimal case where the referenced table exists I'm getting a segfault.

#### To Reproduce

given an `init.sql` file containing the following:

```sql
CREATE TABLE routes (
	route_id TEXT PRIMARY KEY,
	agency_id TEXT,
	FOREIGN KEY (agency_id) REFERENCES agency,
);
```

running

```sh
duckdb < init.sql
```

outputs

```
Segmentation fault (core dumped)
```

instead of a segfault, as a user I would expect an informative error message that would allow me to diagnose the issue.

#### Environment (please complete the following information):
 - OS: Linux
 - DuckDB Version: 0.3.4
 - DuckDB Client: CLI

#### Before Submitting

- **Have you tried this on the latest `master` branch?**

* **Other Platforms**: You can find binaries [here](https://github.com/duckdb/duckdb/releases/tag/master-builds) or compile from source.

  :warning: fyi, the link in the above "other platforms" point seems to just point to a four-year-old source tarball: https://github.com/duckdb/duckdb/releases/tag/master-builds

  the [link to download binaries built off master on the duckdb website](https://duckdb.org/docs/installation/) also fails with a 404 for me: https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_cli-linux-amd64.zip

  given these points, I don't know how to test against master, sorry. I'm not going to attempt to build it myself -- we're just evaluating whether DuckDB is in a usable state for our company (I would love for it to be but have to admit landing on this issue in the first few minutes and then hitting broken downloads on the website is not a promising start).

- **Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?**


</issue>
<code>
[start of README.md]
1: <div align="center">
2:   <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
3: </div>
4: <p>&nbsp;</p>
5: 
6: <p align="center">
7:   <a href="https://github.com/duckdb/duckdb/actions">
8:     <img src="https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master" alt=".github/workflows/main.yml">
9:   </a>
10:   <a href="https://www.codefactor.io/repository/github/cwida/duckdb">
11:     <img src="https://www.codefactor.io/repository/github/cwida/duckdb/badge" alt="CodeFactor"/>
12:   </a>
13:   <a href="https://app.codecov.io/gh/duckdb/duckdb">
14:     <img src="https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN" alt="codecov"/>
15:   </a>
16:   <a href="https://discord.gg/tcvwpjfnZx">
17:     <img src="https://shields.io/discord/909674491309850675" alt="discord" />
18:   </a>
19: </p>
20: 
21: ## DuckDB
22: DuckDB is a high-performance analytical database system. It is designed to be fast, reliable and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. For more information on the goals of DuckDB, please refer to [the Why DuckDB page on our website](https://duckdb.org/why_duckdb).
23: 
24: ## Installation
25: If you want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
26: 
27: ## Data Import
28: For CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:
29: 
30: ```sql
31: SELECT * FROM 'myfile.csv';
32: SELECT * FROM 'myfile.parquet';
33: ```
34: 
35: Refer to our [Data Import](https://duckdb.org/docs/data/overview) section for more information.
36: 
37: ## SQL Reference
38: The [website](https://duckdb.org/docs/sql/introduction) contains a reference of functions and SQL constructs available in DuckDB.
39: 
40: ## Development
41: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
42: 
43: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
44: 
45: 
[end of README.md]
[start of src/execution/index/art/art.cpp]
1: #include "duckdb/execution/index/art/art.hpp"
2: 
3: #include "duckdb/common/radix.hpp"
4: #include "duckdb/common/vector_operations/vector_operations.hpp"
5: #include "duckdb/execution/expression_executor.hpp"
6: 
7: #include <algorithm>
8: #include <cstring>
9: #include <ctgmath>
10: 
11: namespace duckdb {
12: 
13: ART::ART(const vector<column_t> &column_ids, const vector<unique_ptr<Expression>> &unbound_expressions,
14:          IndexConstraintType constraint_type)
15:     : Index(IndexType::ART, column_ids, unbound_expressions, constraint_type) {
16: 	tree = nullptr;
17: 	expression_result.Initialize(logical_types);
18: 	is_little_endian = IsLittleEndian();
19: 	for (idx_t i = 0; i < types.size(); i++) {
20: 		switch (types[i]) {
21: 		case PhysicalType::BOOL:
22: 		case PhysicalType::INT8:
23: 		case PhysicalType::INT16:
24: 		case PhysicalType::INT32:
25: 		case PhysicalType::INT64:
26: 		case PhysicalType::INT128:
27: 		case PhysicalType::UINT8:
28: 		case PhysicalType::UINT16:
29: 		case PhysicalType::UINT32:
30: 		case PhysicalType::UINT64:
31: 		case PhysicalType::FLOAT:
32: 		case PhysicalType::DOUBLE:
33: 		case PhysicalType::VARCHAR:
34: 			break;
35: 		default:
36: 			throw InvalidTypeException(logical_types[i], "Invalid type for index");
37: 		}
38: 	}
39: }
40: 
41: ART::~ART() {
42: }
43: 
44: bool ART::LeafMatches(Node *node, Key &key, unsigned depth) {
45: 	auto leaf = static_cast<Leaf *>(node);
46: 	Key &leaf_key = *leaf->value;
47: 	for (idx_t i = depth; i < leaf_key.len; i++) {
48: 		if (leaf_key[i] != key[i]) {
49: 			return false;
50: 		}
51: 	}
52: 
53: 	return true;
54: }
55: 
56: unique_ptr<IndexScanState> ART::InitializeScanSinglePredicate(Transaction &transaction, Value value,
57:                                                               ExpressionType expression_type) {
58: 	auto result = make_unique<ARTIndexScanState>();
59: 	result->values[0] = value;
60: 	result->expressions[0] = expression_type;
61: 	return move(result);
62: }
63: 
64: unique_ptr<IndexScanState> ART::InitializeScanTwoPredicates(Transaction &transaction, Value low_value,
65:                                                             ExpressionType low_expression_type, Value high_value,
66:                                                             ExpressionType high_expression_type) {
67: 	auto result = make_unique<ARTIndexScanState>();
68: 	result->values[0] = low_value;
69: 	result->expressions[0] = low_expression_type;
70: 	result->values[1] = high_value;
71: 	result->expressions[1] = high_expression_type;
72: 	return move(result);
73: }
74: 
75: //===--------------------------------------------------------------------===//
76: // Insert
77: //===--------------------------------------------------------------------===//
78: template <class T>
79: static void TemplatedGenerateKeys(Vector &input, idx_t count, vector<unique_ptr<Key>> &keys, bool is_little_endian) {
80: 	VectorData idata;
81: 	input.Orrify(count, idata);
82: 
83: 	auto input_data = (T *)idata.data;
84: 	for (idx_t i = 0; i < count; i++) {
85: 		auto idx = idata.sel->get_index(i);
86: 		if (idata.validity.RowIsValid(idx)) {
87: 			keys.push_back(Key::CreateKey<T>(input_data[idx], is_little_endian));
88: 		} else {
89: 			keys.push_back(nullptr);
90: 		}
91: 	}
92: }
93: 
94: template <class T>
95: static void ConcatenateKeys(Vector &input, idx_t count, vector<unique_ptr<Key>> &keys, bool is_little_endian) {
96: 	VectorData idata;
97: 	input.Orrify(count, idata);
98: 
99: 	auto input_data = (T *)idata.data;
100: 	for (idx_t i = 0; i < count; i++) {
101: 		auto idx = idata.sel->get_index(i);
102: 		if (!idata.validity.RowIsValid(idx) || !keys[i]) {
103: 			// either this column is NULL, or the previous column is NULL!
104: 			keys[i] = nullptr;
105: 		} else {
106: 			// concatenate the keys
107: 			auto old_key = move(keys[i]);
108: 			auto new_key = Key::CreateKey<T>(input_data[idx], is_little_endian);
109: 			auto key_len = old_key->len + new_key->len;
110: 			auto compound_data = unique_ptr<data_t[]>(new data_t[key_len]);
111: 			memcpy(compound_data.get(), old_key->data.get(), old_key->len);
112: 			memcpy(compound_data.get() + old_key->len, new_key->data.get(), new_key->len);
113: 			keys[i] = make_unique<Key>(move(compound_data), key_len);
114: 		}
115: 	}
116: }
117: 
118: void ART::GenerateKeys(DataChunk &input, vector<unique_ptr<Key>> &keys) {
119: 	keys.reserve(STANDARD_VECTOR_SIZE);
120: 	// generate keys for the first input column
121: 	switch (input.data[0].GetType().InternalType()) {
122: 	case PhysicalType::BOOL:
123: 		TemplatedGenerateKeys<bool>(input.data[0], input.size(), keys, is_little_endian);
124: 		break;
125: 	case PhysicalType::INT8:
126: 		TemplatedGenerateKeys<int8_t>(input.data[0], input.size(), keys, is_little_endian);
127: 		break;
128: 	case PhysicalType::INT16:
129: 		TemplatedGenerateKeys<int16_t>(input.data[0], input.size(), keys, is_little_endian);
130: 		break;
131: 	case PhysicalType::INT32:
132: 		TemplatedGenerateKeys<int32_t>(input.data[0], input.size(), keys, is_little_endian);
133: 		break;
134: 	case PhysicalType::INT64:
135: 		TemplatedGenerateKeys<int64_t>(input.data[0], input.size(), keys, is_little_endian);
136: 		break;
137: 	case PhysicalType::INT128:
138: 		TemplatedGenerateKeys<hugeint_t>(input.data[0], input.size(), keys, is_little_endian);
139: 		break;
140: 	case PhysicalType::UINT8:
141: 		TemplatedGenerateKeys<uint8_t>(input.data[0], input.size(), keys, is_little_endian);
142: 		break;
143: 	case PhysicalType::UINT16:
144: 		TemplatedGenerateKeys<uint16_t>(input.data[0], input.size(), keys, is_little_endian);
145: 		break;
146: 	case PhysicalType::UINT32:
147: 		TemplatedGenerateKeys<uint32_t>(input.data[0], input.size(), keys, is_little_endian);
148: 		break;
149: 	case PhysicalType::UINT64:
150: 		TemplatedGenerateKeys<uint64_t>(input.data[0], input.size(), keys, is_little_endian);
151: 		break;
152: 	case PhysicalType::FLOAT:
153: 		TemplatedGenerateKeys<float>(input.data[0], input.size(), keys, is_little_endian);
154: 		break;
155: 	case PhysicalType::DOUBLE:
156: 		TemplatedGenerateKeys<double>(input.data[0], input.size(), keys, is_little_endian);
157: 		break;
158: 	case PhysicalType::VARCHAR:
159: 		TemplatedGenerateKeys<string_t>(input.data[0], input.size(), keys, is_little_endian);
160: 		break;
161: 	default:
162: 		throw InternalException("Invalid type for index");
163: 	}
164: 
165: 	for (idx_t i = 1; i < input.ColumnCount(); i++) {
166: 		// for each of the remaining columns, concatenate
167: 		switch (input.data[i].GetType().InternalType()) {
168: 		case PhysicalType::BOOL:
169: 			ConcatenateKeys<bool>(input.data[i], input.size(), keys, is_little_endian);
170: 			break;
171: 		case PhysicalType::INT8:
172: 			ConcatenateKeys<int8_t>(input.data[i], input.size(), keys, is_little_endian);
173: 			break;
174: 		case PhysicalType::INT16:
175: 			ConcatenateKeys<int16_t>(input.data[i], input.size(), keys, is_little_endian);
176: 			break;
177: 		case PhysicalType::INT32:
178: 			ConcatenateKeys<int32_t>(input.data[i], input.size(), keys, is_little_endian);
179: 			break;
180: 		case PhysicalType::INT64:
181: 			ConcatenateKeys<int64_t>(input.data[i], input.size(), keys, is_little_endian);
182: 			break;
183: 		case PhysicalType::INT128:
184: 			ConcatenateKeys<hugeint_t>(input.data[i], input.size(), keys, is_little_endian);
185: 			break;
186: 		case PhysicalType::UINT8:
187: 			ConcatenateKeys<uint8_t>(input.data[i], input.size(), keys, is_little_endian);
188: 			break;
189: 		case PhysicalType::UINT16:
190: 			ConcatenateKeys<uint16_t>(input.data[i], input.size(), keys, is_little_endian);
191: 			break;
192: 		case PhysicalType::UINT32:
193: 			ConcatenateKeys<uint32_t>(input.data[i], input.size(), keys, is_little_endian);
194: 			break;
195: 		case PhysicalType::UINT64:
196: 			ConcatenateKeys<uint64_t>(input.data[i], input.size(), keys, is_little_endian);
197: 			break;
198: 		case PhysicalType::FLOAT:
199: 			ConcatenateKeys<float>(input.data[i], input.size(), keys, is_little_endian);
200: 			break;
201: 		case PhysicalType::DOUBLE:
202: 			ConcatenateKeys<double>(input.data[i], input.size(), keys, is_little_endian);
203: 			break;
204: 		case PhysicalType::VARCHAR:
205: 			ConcatenateKeys<string_t>(input.data[i], input.size(), keys, is_little_endian);
206: 			break;
207: 		default:
208: 			throw InternalException("Invalid type for index");
209: 		}
210: 	}
211: }
212: 
213: bool ART::Insert(IndexLock &lock, DataChunk &input, Vector &row_ids) {
214: 	D_ASSERT(row_ids.GetType().InternalType() == ROW_TYPE);
215: 	D_ASSERT(logical_types[0] == input.data[0].GetType());
216: 
217: 	// generate the keys for the given input
218: 	vector<unique_ptr<Key>> keys;
219: 	GenerateKeys(input, keys);
220: 
221: 	// now insert the elements into the index
222: 	row_ids.Normalify(input.size());
223: 	auto row_identifiers = FlatVector::GetData<row_t>(row_ids);
224: 	idx_t failed_index = DConstants::INVALID_INDEX;
225: 	for (idx_t i = 0; i < input.size(); i++) {
226: 		if (!keys[i]) {
227: 			continue;
228: 		}
229: 
230: 		row_t row_id = row_identifiers[i];
231: 		if (!Insert(tree, move(keys[i]), 0, row_id)) {
232: 			// failed to insert because of constraint violation
233: 			failed_index = i;
234: 			break;
235: 		}
236: 	}
237: 	if (failed_index != DConstants::INVALID_INDEX) {
238: 		// failed to insert because of constraint violation: remove previously inserted entries
239: 		// generate keys again
240: 		keys.clear();
241: 		GenerateKeys(input, keys);
242: 		unique_ptr<Key> key;
243: 
244: 		// now erase the entries
245: 		for (idx_t i = 0; i < failed_index; i++) {
246: 			if (!keys[i]) {
247: 				continue;
248: 			}
249: 			row_t row_id = row_identifiers[i];
250: 			Erase(tree, *keys[i], 0, row_id);
251: 		}
252: 		return false;
253: 	}
254: 	return true;
255: }
256: 
257: bool ART::Append(IndexLock &lock, DataChunk &appended_data, Vector &row_identifiers) {
258: 	DataChunk expression_result;
259: 	expression_result.Initialize(logical_types);
260: 
261: 	// first resolve the expressions for the index
262: 	ExecuteExpressions(appended_data, expression_result);
263: 
264: 	// now insert into the index
265: 	return Insert(lock, expression_result, row_identifiers);
266: }
267: 
268: void ART::VerifyAppend(DataChunk &chunk) {
269: 	VerifyExistence(chunk, VerifyExistenceType::APPEND);
270: }
271: 
272: void ART::VerifyAppendForeignKey(DataChunk &chunk, string *err_msg_ptr) {
273: 	VerifyExistence(chunk, VerifyExistenceType::APPEND_FK, err_msg_ptr);
274: }
275: 
276: void ART::VerifyDeleteForeignKey(DataChunk &chunk, string *err_msg_ptr) {
277: 	VerifyExistence(chunk, VerifyExistenceType::DELETE_FK, err_msg_ptr);
278: }
279: 
280: bool ART::InsertToLeaf(Leaf &leaf, row_t row_id) {
281: 	if (IsUnique() && leaf.num_elements != 0) {
282: 		return false;
283: 	}
284: 	leaf.Insert(row_id);
285: 	return true;
286: }
287: 
288: bool ART::Insert(unique_ptr<Node> &node, unique_ptr<Key> value, unsigned depth, row_t row_id) {
289: 	Key &key = *value;
290: 	if (!node) {
291: 		// node is currently empty, create a leaf here with the key
292: 		node = make_unique<Leaf>(*this, move(value), row_id);
293: 		return true;
294: 	}
295: 
296: 	if (node->type == NodeType::NLeaf) {
297: 		// Replace leaf with Node4 and store both leaves in it
298: 		auto leaf = static_cast<Leaf *>(node.get());
299: 
300: 		Key &existing_key = *leaf->value;
301: 		uint32_t new_prefix_length = 0;
302: 		// Leaf node is already there, update row_id vector
303: 		if (depth + new_prefix_length == existing_key.len && existing_key.len == key.len) {
304: 			return InsertToLeaf(*leaf, row_id);
305: 		}
306: 		while (existing_key[depth + new_prefix_length] == key[depth + new_prefix_length]) {
307: 			new_prefix_length++;
308: 			// Leaf node is already there, update row_id vector
309: 			if (depth + new_prefix_length == existing_key.len && existing_key.len == key.len) {
310: 				return InsertToLeaf(*leaf, row_id);
311: 			}
312: 		}
313: 
314: 		unique_ptr<Node> new_node = make_unique<Node4>(*this, new_prefix_length);
315: 		new_node->prefix_length = new_prefix_length;
316: 		memcpy(new_node->prefix.get(), &key[depth], new_prefix_length);
317: 		Node4::Insert(*this, new_node, existing_key[depth + new_prefix_length], node);
318: 		unique_ptr<Node> leaf_node = make_unique<Leaf>(*this, move(value), row_id);
319: 		Node4::Insert(*this, new_node, key[depth + new_prefix_length], leaf_node);
320: 		node = move(new_node);
321: 		return true;
322: 	}
323: 
324: 	// Handle prefix of inner node
325: 	if (node->prefix_length) {
326: 		uint32_t mismatch_pos = Node::PrefixMismatch(*this, node.get(), key, depth);
327: 		if (mismatch_pos != node->prefix_length) {
328: 			// Prefix differs, create new node
329: 			unique_ptr<Node> new_node = make_unique<Node4>(*this, mismatch_pos);
330: 			new_node->prefix_length = mismatch_pos;
331: 			memcpy(new_node->prefix.get(), node->prefix.get(), mismatch_pos);
332: 			// Break up prefix
333: 			auto node_ptr = node.get();
334: 			Node4::Insert(*this, new_node, node->prefix[mismatch_pos], node);
335: 			node_ptr->prefix_length -= (mismatch_pos + 1);
336: 			memmove(node_ptr->prefix.get(), node_ptr->prefix.get() + mismatch_pos + 1, node_ptr->prefix_length);
337: 			unique_ptr<Node> leaf_node = make_unique<Leaf>(*this, move(value), row_id);
338: 			Node4::Insert(*this, new_node, key[depth + mismatch_pos], leaf_node);
339: 			node = move(new_node);
340: 			return true;
341: 		}
342: 		depth += node->prefix_length;
343: 	}
344: 
345: 	// Recurse
346: 	idx_t pos = node->GetChildPos(key[depth]);
347: 	if (pos != DConstants::INVALID_INDEX) {
348: 		auto child = node->GetChild(pos);
349: 		return Insert(*child, move(value), depth + 1, row_id);
350: 	}
351: 	unique_ptr<Node> new_node = make_unique<Leaf>(*this, move(value), row_id);
352: 	Node::InsertLeaf(*this, node, key[depth], new_node);
353: 	return true;
354: }
355: 
356: //===--------------------------------------------------------------------===//
357: // Delete
358: //===--------------------------------------------------------------------===//
359: void ART::Delete(IndexLock &state, DataChunk &input, Vector &row_ids) {
360: 	DataChunk expression_result;
361: 	expression_result.Initialize(logical_types);
362: 
363: 	// first resolve the expressions
364: 	ExecuteExpressions(input, expression_result);
365: 
366: 	// then generate the keys for the given input
367: 	vector<unique_ptr<Key>> keys;
368: 	GenerateKeys(expression_result, keys);
369: 
370: 	// now erase the elements from the database
371: 	row_ids.Normalify(input.size());
372: 	auto row_identifiers = FlatVector::GetData<row_t>(row_ids);
373: 
374: 	for (idx_t i = 0; i < input.size(); i++) {
375: 		if (!keys[i]) {
376: 			continue;
377: 		}
378: 		Erase(tree, *keys[i], 0, row_identifiers[i]);
379: #ifdef DEBUG
380: 		auto node = Lookup(tree, *keys[i], 0);
381: 		if (node) {
382: 			auto leaf = static_cast<Leaf *>(node);
383: 			for (idx_t k = 0; k < leaf->num_elements; k++) {
384: 				D_ASSERT(leaf->GetRowId(k) != row_identifiers[i]);
385: 			}
386: 		}
387: #endif
388: 	}
389: }
390: 
391: void ART::Erase(unique_ptr<Node> &node, Key &key, unsigned depth, row_t row_id) {
392: 	if (!node) {
393: 		return;
394: 	}
395: 	// Delete a leaf from a tree
396: 	if (node->type == NodeType::NLeaf) {
397: 		// Make sure we have the right leaf
398: 		if (ART::LeafMatches(node.get(), key, depth)) {
399: 			auto leaf = static_cast<Leaf *>(node.get());
400: 			leaf->Remove(row_id);
401: 			if (leaf->num_elements == 0) {
402: 				node.reset();
403: 			}
404: 		}
405: 		return;
406: 	}
407: 
408: 	// Handle prefix
409: 	if (node->prefix_length) {
410: 		if (Node::PrefixMismatch(*this, node.get(), key, depth) != node->prefix_length) {
411: 			return;
412: 		}
413: 		depth += node->prefix_length;
414: 	}
415: 	idx_t pos = node->GetChildPos(key[depth]);
416: 	if (pos != DConstants::INVALID_INDEX) {
417: 		auto child = node->GetChild(pos);
418: 		D_ASSERT(child);
419: 
420: 		unique_ptr<Node> &child_ref = *child;
421: 		if (child_ref->type == NodeType::NLeaf && LeafMatches(child_ref.get(), key, depth)) {
422: 			// Leaf found, remove entry
423: 			auto leaf = static_cast<Leaf *>(child_ref.get());
424: 			leaf->Remove(row_id);
425: 			if (leaf->num_elements == 0) {
426: 				// Leaf is empty, delete leaf, decrement node counter and maybe shrink node
427: 				Node::Erase(*this, node, pos);
428: 			}
429: 		} else {
430: 			// Recurse
431: 			Erase(*child, key, depth + 1, row_id);
432: 		}
433: 	}
434: }
435: 
436: //===--------------------------------------------------------------------===//
437: // Point Query
438: //===--------------------------------------------------------------------===//
439: static unique_ptr<Key> CreateKey(ART &art, PhysicalType type, Value &value) {
440: 	D_ASSERT(type == value.type().InternalType());
441: 	switch (type) {
442: 	case PhysicalType::BOOL:
443: 		return Key::CreateKey<bool>(value, art.is_little_endian);
444: 	case PhysicalType::INT8:
445: 		return Key::CreateKey<int8_t>(value, art.is_little_endian);
446: 	case PhysicalType::INT16:
447: 		return Key::CreateKey<int16_t>(value, art.is_little_endian);
448: 	case PhysicalType::INT32:
449: 		return Key::CreateKey<int32_t>(value, art.is_little_endian);
450: 	case PhysicalType::INT64:
451: 		return Key::CreateKey<int64_t>(value, art.is_little_endian);
452: 	case PhysicalType::UINT8:
453: 		return Key::CreateKey<uint8_t>(value, art.is_little_endian);
454: 	case PhysicalType::UINT16:
455: 		return Key::CreateKey<uint16_t>(value, art.is_little_endian);
456: 	case PhysicalType::UINT32:
457: 		return Key::CreateKey<uint32_t>(value, art.is_little_endian);
458: 	case PhysicalType::UINT64:
459: 		return Key::CreateKey<uint64_t>(value, art.is_little_endian);
460: 	case PhysicalType::INT128:
461: 		return Key::CreateKey<hugeint_t>(value, art.is_little_endian);
462: 	case PhysicalType::FLOAT:
463: 		return Key::CreateKey<float>(value, art.is_little_endian);
464: 	case PhysicalType::DOUBLE:
465: 		return Key::CreateKey<double>(value, art.is_little_endian);
466: 	case PhysicalType::VARCHAR:
467: 		return Key::CreateKey<string_t>(value, art.is_little_endian);
468: 	default:
469: 		throw InternalException("Invalid type for index");
470: 	}
471: }
472: 
473: bool ART::SearchEqual(ARTIndexScanState *state, idx_t max_count, vector<row_t> &result_ids) {
474: 	auto key = CreateKey(*this, types[0], state->values[0]);
475: 	auto leaf = static_cast<Leaf *>(Lookup(tree, *key, 0));
476: 	if (!leaf) {
477: 		return true;
478: 	}
479: 	if (leaf->num_elements > max_count) {
480: 		return false;
481: 	}
482: 	for (idx_t i = 0; i < leaf->num_elements; i++) {
483: 		row_t row_id = leaf->GetRowId(i);
484: 		result_ids.push_back(row_id);
485: 	}
486: 	return true;
487: }
488: 
489: void ART::SearchEqualJoinNoFetch(Value &equal_value, idx_t &result_size) {
490: 	//! We need to look for a leaf
491: 	auto key = CreateKey(*this, types[0], equal_value);
492: 	auto leaf = static_cast<Leaf *>(Lookup(tree, *key, 0));
493: 	if (!leaf) {
494: 		return;
495: 	}
496: 	result_size = leaf->num_elements;
497: }
498: 
499: Node *ART::Lookup(unique_ptr<Node> &node, Key &key, unsigned depth) {
500: 	auto node_val = node.get();
501: 
502: 	while (node_val) {
503: 		if (node_val->type == NodeType::NLeaf) {
504: 			auto leaf = static_cast<Leaf *>(node_val);
505: 			Key &leaf_key = *leaf->value;
506: 			//! Check leaf
507: 			for (idx_t i = depth; i < leaf_key.len; i++) {
508: 				if (leaf_key[i] != key[i]) {
509: 					return nullptr;
510: 				}
511: 			}
512: 			return node_val;
513: 		}
514: 		if (node_val->prefix_length) {
515: 			for (idx_t pos = 0; pos < node_val->prefix_length; pos++) {
516: 				if (key[depth + pos] != node_val->prefix[pos]) {
517: 					return nullptr;
518: 				}
519: 			}
520: 			depth += node_val->prefix_length;
521: 		}
522: 		idx_t pos = node_val->GetChildPos(key[depth]);
523: 		if (pos == DConstants::INVALID_INDEX) {
524: 			return nullptr;
525: 		}
526: 		node_val = node_val->GetChild(pos)->get();
527: 		D_ASSERT(node_val);
528: 
529: 		depth++;
530: 	}
531: 
532: 	return nullptr;
533: }
534: 
535: //===--------------------------------------------------------------------===//
536: // Iterator scans
537: //===--------------------------------------------------------------------===//
538: template <bool HAS_BOUND, bool INCLUSIVE>
539: bool ART::IteratorScan(ARTIndexScanState *state, Iterator *it, Key *bound, idx_t max_count, vector<row_t> &result_ids) {
540: 	bool has_next;
541: 	do {
542: 		if (HAS_BOUND) {
543: 			D_ASSERT(bound);
544: 			if (INCLUSIVE) {
545: 				if (*it->node->value > *bound) {
546: 					break;
547: 				}
548: 			} else {
549: 				if (*it->node->value >= *bound) {
550: 					break;
551: 				}
552: 			}
553: 		}
554: 		if (result_ids.size() + it->node->num_elements > max_count) {
555: 			// adding these elements would exceed the max count
556: 			return false;
557: 		}
558: 		for (idx_t i = 0; i < it->node->num_elements; i++) {
559: 			row_t row_id = it->node->GetRowId(i);
560: 			result_ids.push_back(row_id);
561: 		}
562: 		has_next = ART::IteratorNext(*it);
563: 	} while (has_next);
564: 	return true;
565: }
566: 
567: void Iterator::SetEntry(idx_t entry_depth, IteratorEntry entry) {
568: 	if (stack.size() < entry_depth + 1) {
569: 		stack.resize(MaxValue<idx_t>(8, MaxValue<idx_t>(entry_depth + 1, stack.size() * 2)));
570: 	}
571: 	stack[entry_depth] = entry;
572: }
573: 
574: bool ART::IteratorNext(Iterator &it) {
575: 	// Skip leaf
576: 	if ((it.depth) && ((it.stack[it.depth - 1].node)->type == NodeType::NLeaf)) {
577: 		it.depth--;
578: 	}
579: 
580: 	// Look for the next leaf
581: 	while (it.depth > 0) {
582: 		auto &top = it.stack[it.depth - 1];
583: 		Node *node = top.node;
584: 
585: 		if (node->type == NodeType::NLeaf) {
586: 			// found a leaf: move to next node
587: 			it.node = (Leaf *)node;
588: 			return true;
589: 		}
590: 
591: 		// Find next node
592: 		top.pos = node->GetNextPos(top.pos);
593: 		if (top.pos != DConstants::INVALID_INDEX) {
594: 			// next node found: go there
595: 			it.SetEntry(it.depth, IteratorEntry(node->GetChild(top.pos)->get(), DConstants::INVALID_INDEX));
596: 			it.depth++;
597: 		} else {
598: 			// no node found: move up the tree
599: 			it.depth--;
600: 		}
601: 	}
602: 	return false;
603: }
604: 
605: //===--------------------------------------------------------------------===//
606: // Greater Than
607: // Returns: True (If found leaf >= key)
608: //          False (Otherwise)
609: //===--------------------------------------------------------------------===//
610: bool ART::Bound(unique_ptr<Node> &n, Key &key, Iterator &it, bool inclusive) {
611: 	it.depth = 0;
612: 	bool equal = false;
613: 	if (!n) {
614: 		return false;
615: 	}
616: 	Node *node = n.get();
617: 
618: 	idx_t depth = 0;
619: 	while (true) {
620: 		it.SetEntry(it.depth, IteratorEntry(node, 0));
621: 		auto &top = it.stack[it.depth];
622: 		it.depth++;
623: 		if (!equal) {
624: 			while (node->type != NodeType::NLeaf) {
625: 				node = node->GetChild(node->GetMin())->get();
626: 				auto &c_top = it.stack[it.depth];
627: 				c_top.node = node;
628: 				it.depth++;
629: 			}
630: 		}
631: 		if (node->type == NodeType::NLeaf) {
632: 			// found a leaf node: check if it is bigger or equal than the current key
633: 			auto leaf = static_cast<Leaf *>(node);
634: 			it.node = leaf;
635: 			// if the search is not inclusive the leaf node could still be equal to the current value
636: 			// check if leaf is equal to the current key
637: 			if (*leaf->value == key) {
638: 				// if its not inclusive check if there is a next leaf
639: 				if (!inclusive && !IteratorNext(it)) {
640: 					return false;
641: 				} else {
642: 					return true;
643: 				}
644: 			}
645: 
646: 			if (*leaf->value > key) {
647: 				return true;
648: 			}
649: 			// Leaf is lower than key
650: 			// Check if next leaf is still lower than key
651: 			while (IteratorNext(it)) {
652: 				if (*it.node->value == key) {
653: 					// if its not inclusive check if there is a next leaf
654: 					if (!inclusive && !IteratorNext(it)) {
655: 						return false;
656: 					} else {
657: 						return true;
658: 					}
659: 				} else if (*it.node->value > key) {
660: 					// if its not inclusive check if there is a next leaf
661: 					return true;
662: 				}
663: 			}
664: 			return false;
665: 		}
666: 		uint32_t mismatch_pos = Node::PrefixMismatch(*this, node, key, depth);
667: 		if (mismatch_pos != node->prefix_length) {
668: 			if (node->prefix[mismatch_pos] < key[depth + mismatch_pos]) {
669: 				// Less
670: 				it.depth--;
671: 				return IteratorNext(it);
672: 			} else {
673: 				// Greater
674: 				top.pos = DConstants::INVALID_INDEX;
675: 				return IteratorNext(it);
676: 			}
677: 		}
678: 		// prefix matches, search inside the child for the key
679: 		depth += node->prefix_length;
680: 
681: 		top.pos = node->GetChildGreaterEqual(key[depth], equal);
682: 		if (top.pos == DConstants::INVALID_INDEX) {
683: 			// Find min leaf
684: 			top.pos = node->GetMin();
685: 		}
686: 		node = node->GetChild(top.pos)->get();
687: 		//! This means all children of this node qualify as geq
688: 
689: 		depth++;
690: 	}
691: }
692: 
693: bool ART::SearchGreater(ARTIndexScanState *state, bool inclusive, idx_t max_count, vector<row_t> &result_ids) {
694: 	Iterator *it = &state->iterator;
695: 	auto key = CreateKey(*this, types[0], state->values[0]);
696: 
697: 	// greater than scan: first set the iterator to the node at which we will start our scan by finding the lowest node
698: 	// that satisfies our requirement
699: 	if (!it->start) {
700: 		bool found = ART::Bound(tree, *key, *it, inclusive);
701: 		if (!found) {
702: 			return true;
703: 		}
704: 		it->start = true;
705: 	}
706: 	// after that we continue the scan; we don't need to check the bounds as any value following this value is
707: 	// automatically bigger and hence satisfies our predicate
708: 	return IteratorScan<false, false>(state, it, nullptr, max_count, result_ids);
709: }
710: 
711: //===--------------------------------------------------------------------===//
712: // Less Than
713: //===--------------------------------------------------------------------===//
714: static Leaf &FindMinimum(Iterator &it, Node &node) {
715: 	Node *next = nullptr;
716: 	idx_t pos = 0;
717: 	switch (node.type) {
718: 	case NodeType::NLeaf:
719: 		it.node = (Leaf *)&node;
720: 		return (Leaf &)node;
721: 	case NodeType::N4:
722: 		next = ((Node4 &)node).child[0].get();
723: 		break;
724: 	case NodeType::N16:
725: 		next = ((Node16 &)node).child[0].get();
726: 		break;
727: 	case NodeType::N48: {
728: 		auto &n48 = (Node48 &)node;
729: 		while (n48.child_index[pos] == Node::EMPTY_MARKER) {
730: 			pos++;
731: 		}
732: 		next = n48.child[n48.child_index[pos]].get();
733: 		break;
734: 	}
735: 	case NodeType::N256: {
736: 		auto &n256 = (Node256 &)node;
737: 		while (!n256.child[pos]) {
738: 			pos++;
739: 		}
740: 		next = n256.child[pos].get();
741: 		break;
742: 	}
743: 	}
744: 	it.SetEntry(it.depth, IteratorEntry(&node, pos));
745: 	it.depth++;
746: 	return FindMinimum(it, *next);
747: }
748: 
749: bool ART::SearchLess(ARTIndexScanState *state, bool inclusive, idx_t max_count, vector<row_t> &result_ids) {
750: 	if (!tree) {
751: 		return true;
752: 	}
753: 
754: 	Iterator *it = &state->iterator;
755: 	auto upper_bound = CreateKey(*this, types[0], state->values[0]);
756: 
757: 	if (!it->start) {
758: 		// first find the minimum value in the ART: we start scanning from this value
759: 		auto &minimum = FindMinimum(state->iterator, *tree);
760: 		// early out min value higher than upper bound query
761: 		if (*minimum.value > *upper_bound) {
762: 			return true;
763: 		}
764: 		it->start = true;
765: 	}
766: 	// now continue the scan until we reach the upper bound
767: 	if (inclusive) {
768: 		return IteratorScan<true, true>(state, it, upper_bound.get(), max_count, result_ids);
769: 	} else {
770: 		return IteratorScan<true, false>(state, it, upper_bound.get(), max_count, result_ids);
771: 	}
772: }
773: 
774: //===--------------------------------------------------------------------===//
775: // Closed Range Query
776: //===--------------------------------------------------------------------===//
777: bool ART::SearchCloseRange(ARTIndexScanState *state, bool left_inclusive, bool right_inclusive, idx_t max_count,
778:                            vector<row_t> &result_ids) {
779: 	auto lower_bound = CreateKey(*this, types[0], state->values[0]);
780: 	auto upper_bound = CreateKey(*this, types[0], state->values[1]);
781: 	Iterator *it = &state->iterator;
782: 	// first find the first node that satisfies the left predicate
783: 	if (!it->start) {
784: 		bool found = ART::Bound(tree, *lower_bound, *it, left_inclusive);
785: 		if (!found) {
786: 			return true;
787: 		}
788: 		it->start = true;
789: 	}
790: 	// now continue the scan until we reach the upper bound
791: 	if (right_inclusive) {
792: 		return IteratorScan<true, true>(state, it, upper_bound.get(), max_count, result_ids);
793: 	} else {
794: 		return IteratorScan<true, false>(state, it, upper_bound.get(), max_count, result_ids);
795: 	}
796: }
797: 
798: bool ART::Scan(Transaction &transaction, DataTable &table, IndexScanState &table_state, idx_t max_count,
799:                vector<row_t> &result_ids) {
800: 	auto state = (ARTIndexScanState *)&table_state;
801: 
802: 	D_ASSERT(state->values[0].type().InternalType() == types[0]);
803: 
804: 	vector<row_t> row_ids;
805: 	bool success = true;
806: 	if (state->values[1].IsNull()) {
807: 		lock_guard<mutex> l(lock);
808: 		// single predicate
809: 		switch (state->expressions[0]) {
810: 		case ExpressionType::COMPARE_EQUAL:
811: 			success = SearchEqual(state, max_count, row_ids);
812: 			break;
813: 		case ExpressionType::COMPARE_GREATERTHANOREQUALTO:
814: 			success = SearchGreater(state, true, max_count, row_ids);
815: 			break;
816: 		case ExpressionType::COMPARE_GREATERTHAN:
817: 			success = SearchGreater(state, false, max_count, row_ids);
818: 			break;
819: 		case ExpressionType::COMPARE_LESSTHANOREQUALTO:
820: 			success = SearchLess(state, true, max_count, row_ids);
821: 			break;
822: 		case ExpressionType::COMPARE_LESSTHAN:
823: 			success = SearchLess(state, false, max_count, row_ids);
824: 			break;
825: 		default:
826: 			throw InternalException("Operation not implemented");
827: 		}
828: 	} else {
829: 		lock_guard<mutex> l(lock);
830: 		// two predicates
831: 		D_ASSERT(state->values[1].type().InternalType() == types[0]);
832: 		bool left_inclusive = state->expressions[0] == ExpressionType ::COMPARE_GREATERTHANOREQUALTO;
833: 		bool right_inclusive = state->expressions[1] == ExpressionType ::COMPARE_LESSTHANOREQUALTO;
834: 		success = SearchCloseRange(state, left_inclusive, right_inclusive, max_count, row_ids);
835: 	}
836: 	if (!success) {
837: 		return false;
838: 	}
839: 	if (row_ids.empty()) {
840: 		return true;
841: 	}
842: 	// sort the row ids
843: 	sort(row_ids.begin(), row_ids.end());
844: 	// duplicate eliminate the row ids and append them to the row ids of the state
845: 	result_ids.reserve(row_ids.size());
846: 
847: 	result_ids.push_back(row_ids[0]);
848: 	for (idx_t i = 1; i < row_ids.size(); i++) {
849: 		if (row_ids[i] != row_ids[i - 1]) {
850: 			result_ids.push_back(row_ids[i]);
851: 		}
852: 	}
853: 	return true;
854: }
855: 
856: void ART::VerifyExistence(DataChunk &chunk, VerifyExistenceType verify_type, string *err_msg_ptr) {
857: 	if (verify_type != VerifyExistenceType::DELETE_FK && !IsUnique()) {
858: 		return;
859: 	}
860: 
861: 	DataChunk expression_result;
862: 	expression_result.Initialize(logical_types);
863: 
864: 	// unique index, check
865: 	lock_guard<mutex> l(lock);
866: 	// first resolve the expressions for the index
867: 	ExecuteExpressions(chunk, expression_result);
868: 
869: 	// generate the keys for the given input
870: 	vector<unique_ptr<Key>> keys;
871: 	GenerateKeys(expression_result, keys);
872: 
873: 	for (idx_t i = 0; i < chunk.size(); i++) {
874: 		if (!keys[i]) {
875: 			continue;
876: 		}
877: 		Node *node_ptr = Lookup(tree, *keys[i], 0);
878: 		bool throw_exception =
879: 		    verify_type == VerifyExistenceType::APPEND_FK ? node_ptr == nullptr : node_ptr != nullptr;
880: 		if (!throw_exception) {
881: 			continue;
882: 		}
883: 		string key_name;
884: 		for (idx_t k = 0; k < expression_result.ColumnCount(); k++) {
885: 			if (k > 0) {
886: 				key_name += ", ";
887: 			}
888: 			key_name += unbound_expressions[k]->GetName() + ": " + expression_result.data[k].GetValue(i).ToString();
889: 		}
890: 		string exception_msg;
891: 		switch (verify_type) {
892: 		case VerifyExistenceType::APPEND: {
893: 			// node already exists in tree
894: 			string type = IsPrimary() ? "primary key" : "unique";
895: 			exception_msg = "duplicate key \"" + key_name + "\" violates ";
896: 			exception_msg += type + " constraint";
897: 			break;
898: 		}
899: 		case VerifyExistenceType::APPEND_FK: {
900: 			// found node no exists in tree
901: 			exception_msg =
902: 			    "violates foreign key constraint because key \"" + key_name + "\" no exist in referenced table";
903: 			break;
904: 		}
905: 		case VerifyExistenceType::DELETE_FK: {
906: 			// found node exists in tree
907: 			exception_msg =
908: 			    "violates foreign key constraint because key \"" + key_name + "\" exist in table has foreign key";
909: 			break;
910: 		}
911: 		}
912: 		if (err_msg_ptr) {
913: 			err_msg_ptr[i] = exception_msg;
914: 		} else {
915: 			throw ConstraintException(exception_msg);
916: 		}
917: 	}
918: }
919: 
920: } // namespace duckdb
[end of src/execution/index/art/art.cpp]
[start of src/parser/transform/constraint/transform_constraint.cpp]
1: #include "duckdb/parser/column_definition.hpp"
2: #include "duckdb/parser/constraint.hpp"
3: #include "duckdb/parser/constraints/list.hpp"
4: #include "duckdb/parser/transformer.hpp"
5: 
6: namespace duckdb {
7: 
8: unique_ptr<Constraint> Transformer::TransformConstraint(duckdb_libpgquery::PGListCell *cell) {
9: 	auto constraint = reinterpret_cast<duckdb_libpgquery::PGConstraint *>(cell->data.ptr_value);
10: 	switch (constraint->contype) {
11: 	case duckdb_libpgquery::PG_CONSTR_UNIQUE:
12: 	case duckdb_libpgquery::PG_CONSTR_PRIMARY: {
13: 		bool is_primary_key = constraint->contype == duckdb_libpgquery::PG_CONSTR_PRIMARY;
14: 		vector<string> columns;
15: 		for (auto kc = constraint->keys->head; kc; kc = kc->next) {
16: 			columns.emplace_back(reinterpret_cast<duckdb_libpgquery::PGValue *>(kc->data.ptr_value)->val.str);
17: 		}
18: 		return make_unique<UniqueConstraint>(columns, is_primary_key);
19: 	}
20: 	case duckdb_libpgquery::PG_CONSTR_CHECK: {
21: 		auto expression = TransformExpression(constraint->raw_expr);
22: 		if (expression->HasSubquery()) {
23: 			throw ParserException("subqueries prohibited in CHECK constraints");
24: 		}
25: 		return make_unique<CheckConstraint>(TransformExpression(constraint->raw_expr));
26: 	}
27: 	case duckdb_libpgquery::PG_CONSTR_FOREIGN: {
28: 		ForeignKeyInfo fk_info;
29: 		fk_info.type = ForeignKeyType::FK_TYPE_FOREIGN_KEY_TABLE;
30: 		if (constraint->pktable->schemaname) {
31: 			fk_info.schema = constraint->pktable->schemaname;
32: 		} else {
33: 			fk_info.schema = "";
34: 		}
35: 		fk_info.table = constraint->pktable->relname;
36: 		vector<string> pk_columns, fk_columns;
37: 		for (auto kc = constraint->fk_attrs->head; kc; kc = kc->next) {
38: 			fk_columns.emplace_back(reinterpret_cast<duckdb_libpgquery::PGValue *>(kc->data.ptr_value)->val.str);
39: 		}
40: 		for (auto kc = constraint->pk_attrs->head; kc; kc = kc->next) {
41: 			pk_columns.emplace_back(reinterpret_cast<duckdb_libpgquery::PGValue *>(kc->data.ptr_value)->val.str);
42: 		}
43: 		if (pk_columns.size() != fk_columns.size()) {
44: 			throw ParserException("The number of referencing and referenced columns for foreign keys must be the same");
45: 		}
46: 		if (fk_columns.empty()) {
47: 			throw ParserException("The set of referencing and referenced columns for foreign keys must be not empty");
48: 		}
49: 		return make_unique<ForeignKeyConstraint>(pk_columns, fk_columns, move(fk_info));
50: 	}
51: 	default:
52: 		throw NotImplementedException("Constraint type not handled yet!");
53: 	}
54: }
55: 
56: unique_ptr<Constraint> Transformer::TransformConstraint(duckdb_libpgquery::PGListCell *cell, ColumnDefinition &column,
57:                                                         idx_t index) {
58: 	auto constraint = reinterpret_cast<duckdb_libpgquery::PGConstraint *>(cell->data.ptr_value);
59: 	D_ASSERT(constraint);
60: 	switch (constraint->contype) {
61: 	case duckdb_libpgquery::PG_CONSTR_NOTNULL:
62: 		return make_unique<NotNullConstraint>(index);
63: 	case duckdb_libpgquery::PG_CONSTR_CHECK:
64: 		return TransformConstraint(cell);
65: 	case duckdb_libpgquery::PG_CONSTR_PRIMARY:
66: 		return make_unique<UniqueConstraint>(index, true);
67: 	case duckdb_libpgquery::PG_CONSTR_UNIQUE:
68: 		return make_unique<UniqueConstraint>(index, false);
69: 	case duckdb_libpgquery::PG_CONSTR_NULL:
70: 		return nullptr;
71: 	case duckdb_libpgquery::PG_CONSTR_DEFAULT:
72: 		column.default_value = TransformExpression(constraint->raw_expr);
73: 		return nullptr;
74: 	case duckdb_libpgquery::PG_CONSTR_COMPRESSION:
75: 		column.compression_type = CompressionTypeFromString(constraint->compression_name);
76: 		if (column.compression_type == CompressionType::COMPRESSION_AUTO) {
77: 			throw ParserException("Unrecognized option for column compression, expected none, uncompressed, rle, "
78: 			                      "dictionary, pfor, bitpacking or fsst");
79: 		}
80: 		return nullptr;
81: 	case duckdb_libpgquery::PG_CONSTR_FOREIGN:
82: 	default:
83: 		throw NotImplementedException("Constraint not implemented!");
84: 	}
85: }
86: 
87: } // namespace duckdb
[end of src/parser/transform/constraint/transform_constraint.cpp]
[start of src/planner/binder/statement/bind_create.cpp]
1: #include "duckdb/catalog/catalog.hpp"
2: #include "duckdb/catalog/catalog_search_path.hpp"
3: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
4: #include "duckdb/catalog/catalog_entry/type_catalog_entry.hpp"
5: #include "duckdb/main/client_context.hpp"
6: #include "duckdb/main/database.hpp"
7: #include "duckdb/parser/expression/constant_expression.hpp"
8: #include "duckdb/parser/expression/subquery_expression.hpp"
9: #include "duckdb/parser/parsed_data/create_index_info.hpp"
10: #include "duckdb/parser/parsed_data/create_macro_info.hpp"
11: #include "duckdb/parser/parsed_data/create_view_info.hpp"
12: #include "duckdb/parser/parsed_expression_iterator.hpp"
13: #include "duckdb/parser/statement/create_statement.hpp"
14: #include "duckdb/planner/binder.hpp"
15: #include "duckdb/planner/bound_query_node.hpp"
16: #include "duckdb/planner/expression_binder/aggregate_binder.hpp"
17: #include "duckdb/planner/expression_binder/index_binder.hpp"
18: #include "duckdb/planner/expression_binder/select_binder.hpp"
19: #include "duckdb/planner/operator/logical_create.hpp"
20: #include "duckdb/planner/operator/logical_create_index.hpp"
21: #include "duckdb/planner/operator/logical_create_table.hpp"
22: #include "duckdb/planner/operator/logical_get.hpp"
23: #include "duckdb/planner/parsed_data/bound_create_function_info.hpp"
24: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
25: #include "duckdb/planner/query_node/bound_select_node.hpp"
26: #include "duckdb/planner/tableref/bound_basetableref.hpp"
27: #include "duckdb/parser/constraints/foreign_key_constraint.hpp"
28: #include "duckdb/function/scalar_macro_function.hpp"
29: #include "duckdb/storage/data_table.hpp"
30: 
31: namespace duckdb {
32: 
33: SchemaCatalogEntry *Binder::BindSchema(CreateInfo &info) {
34: 	if (info.schema.empty()) {
35: 		info.schema = info.temporary ? TEMP_SCHEMA : context.catalog_search_path->GetDefault();
36: 	}
37: 
38: 	if (!info.temporary) {
39: 		// non-temporary create: not read only
40: 		if (info.schema == TEMP_SCHEMA) {
41: 			throw ParserException("Only TEMPORARY table names can use the \"temp\" schema");
42: 		}
43: 		this->read_only = false;
44: 	} else {
45: 		if (info.schema != TEMP_SCHEMA) {
46: 			throw ParserException("TEMPORARY table names can *only* use the \"%s\" schema", TEMP_SCHEMA);
47: 		}
48: 	}
49: 	// fetch the schema in which we want to create the object
50: 	auto schema_obj = Catalog::GetCatalog(context).GetSchema(context, info.schema);
51: 	D_ASSERT(schema_obj->type == CatalogType::SCHEMA_ENTRY);
52: 	info.schema = schema_obj->name;
53: 	return schema_obj;
54: }
55: 
56: void Binder::BindCreateViewInfo(CreateViewInfo &base) {
57: 	// bind the view as if it were a query so we can catch errors
58: 	// note that we bind the original, and replace the original with a copy
59: 	// this is because the original has
60: 	this->can_contain_nulls = true;
61: 
62: 	auto copy = base.query->Copy();
63: 	auto query_node = Bind(*base.query);
64: 	base.query = unique_ptr_cast<SQLStatement, SelectStatement>(move(copy));
65: 	if (base.aliases.size() > query_node.names.size()) {
66: 		throw BinderException("More VIEW aliases than columns in query result");
67: 	}
68: 	// fill up the aliases with the remaining names of the bound query
69: 	for (idx_t i = base.aliases.size(); i < query_node.names.size(); i++) {
70: 		base.aliases.push_back(query_node.names[i]);
71: 	}
72: 	base.types = query_node.types;
73: }
74: 
75: SchemaCatalogEntry *Binder::BindCreateFunctionInfo(CreateInfo &info) {
76: 	auto &base = (CreateMacroInfo &)info;
77: 	auto &scalar_function = (ScalarMacroFunction &)*base.function;
78: 
79: 	if (scalar_function.expression->HasParameter()) {
80: 		throw BinderException("Parameter expressions within macro's are not supported!");
81: 	}
82: 
83: 	// create macro binding in order to bind the function
84: 	vector<LogicalType> dummy_types;
85: 	vector<string> dummy_names;
86: 	// positional parameters
87: 	for (idx_t i = 0; i < base.function->parameters.size(); i++) {
88: 		auto param = (ColumnRefExpression &)*base.function->parameters[i];
89: 		if (param.IsQualified()) {
90: 			throw BinderException("Invalid parameter name '%s': must be unqualified", param.ToString());
91: 		}
92: 		dummy_types.emplace_back(LogicalType::SQLNULL);
93: 		dummy_names.push_back(param.GetColumnName());
94: 	}
95: 	// default parameters
96: 	for (auto it = base.function->default_parameters.begin(); it != base.function->default_parameters.end(); it++) {
97: 		auto &val = (ConstantExpression &)*it->second;
98: 		dummy_types.push_back(val.value.type());
99: 		dummy_names.push_back(it->first);
100: 	}
101: 	auto this_macro_binding = make_unique<MacroBinding>(dummy_types, dummy_names, base.name);
102: 	macro_binding = this_macro_binding.get();
103: 	ExpressionBinder::QualifyColumnNames(*this, scalar_function.expression);
104: 
105: 	// create a copy of the expression because we do not want to alter the original
106: 	auto expression = scalar_function.expression->Copy();
107: 
108: 	// bind it to verify the function was defined correctly
109: 	string error;
110: 	auto sel_node = make_unique<BoundSelectNode>();
111: 	auto group_info = make_unique<BoundGroupInformation>();
112: 	SelectBinder binder(*this, context, *sel_node, *group_info);
113: 	error = binder.Bind(&expression, 0, false);
114: 
115: 	if (!error.empty()) {
116: 		throw BinderException(error);
117: 	}
118: 
119: 	return BindSchema(info);
120: }
121: 
122: void Binder::BindLogicalType(ClientContext &context, LogicalType &type, const string &schema) {
123: 	if (type.id() == LogicalTypeId::LIST) {
124: 		auto child_type = ListType::GetChildType(type);
125: 		BindLogicalType(context, child_type, schema);
126: 		type = LogicalType::LIST(child_type);
127: 	} else if (type.id() == LogicalTypeId::STRUCT || type.id() == LogicalTypeId::MAP) {
128: 		auto child_types = StructType::GetChildTypes(type);
129: 		for (auto &child_type : child_types) {
130: 			BindLogicalType(context, child_type.second, schema);
131: 		}
132: 		// Generate new Struct/Map Type
133: 		if (type.id() == LogicalTypeId::STRUCT) {
134: 			type = LogicalType::STRUCT(child_types);
135: 		} else {
136: 			type = LogicalType::MAP(child_types);
137: 		}
138: 	} else if (type.id() == LogicalTypeId::USER) {
139: 		auto &user_type_name = UserType::GetTypeName(type);
140: 		auto user_type_catalog = (TypeCatalogEntry *)context.db->GetCatalog().GetEntry(context, CatalogType::TYPE_ENTRY,
141: 		                                                                               schema, user_type_name, true);
142: 		if (!user_type_catalog) {
143: 			throw NotImplementedException("DataType %s not supported yet...\n", user_type_name);
144: 		}
145: 		type = user_type_catalog->user_type;
146: 		EnumType::SetCatalog(type, user_type_catalog);
147: 	} else if (type.id() == LogicalTypeId::ENUM) {
148: 		auto &enum_type_name = EnumType::GetTypeName(type);
149: 		auto enum_type_catalog = (TypeCatalogEntry *)context.db->GetCatalog().GetEntry(context, CatalogType::TYPE_ENTRY,
150: 		                                                                               schema, enum_type_name, true);
151: 		EnumType::SetCatalog(type, enum_type_catalog);
152: 	}
153: }
154: 
155: BoundStatement Binder::Bind(CreateStatement &stmt) {
156: 	BoundStatement result;
157: 	result.names = {"Count"};
158: 	result.types = {LogicalType::BIGINT};
159: 
160: 	auto catalog_type = stmt.info->type;
161: 	switch (catalog_type) {
162: 	case CatalogType::SCHEMA_ENTRY:
163: 		result.plan = make_unique<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_SCHEMA, move(stmt.info));
164: 		break;
165: 	case CatalogType::VIEW_ENTRY: {
166: 		auto &base = (CreateViewInfo &)*stmt.info;
167: 		// bind the schema
168: 		auto schema = BindSchema(*stmt.info);
169: 		BindCreateViewInfo(base);
170: 		result.plan = make_unique<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_VIEW, move(stmt.info), schema);
171: 		break;
172: 	}
173: 	case CatalogType::SEQUENCE_ENTRY: {
174: 		auto schema = BindSchema(*stmt.info);
175: 		result.plan = make_unique<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_SEQUENCE, move(stmt.info), schema);
176: 		break;
177: 	}
178: 	case CatalogType::TABLE_MACRO_ENTRY: {
179: 		auto schema = BindSchema(*stmt.info);
180: 		result.plan = make_unique<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_MACRO, move(stmt.info), schema);
181: 		break;
182: 	}
183: 	case CatalogType::MACRO_ENTRY: {
184: 		auto schema = BindCreateFunctionInfo(*stmt.info);
185: 		result.plan = make_unique<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_MACRO, move(stmt.info), schema);
186: 		break;
187: 	}
188: 	case CatalogType::INDEX_ENTRY: {
189: 		auto &base = (CreateIndexInfo &)*stmt.info;
190: 
191: 		// visit the table reference
192: 		auto bound_table = Bind(*base.table);
193: 		if (bound_table->type != TableReferenceType::BASE_TABLE) {
194: 			throw BinderException("Can only delete from base table!");
195: 		}
196: 		auto &table_binding = (BoundBaseTableRef &)*bound_table;
197: 		auto table = table_binding.table;
198: 		// bind the index expressions
199: 		vector<unique_ptr<Expression>> expressions;
200: 		IndexBinder binder(*this, context);
201: 		for (auto &expr : base.expressions) {
202: 			expressions.push_back(binder.Bind(expr));
203: 		}
204: 
205: 		auto plan = CreatePlan(*bound_table);
206: 		if (plan->type != LogicalOperatorType::LOGICAL_GET) {
207: 			throw BinderException("Cannot create index on a view!");
208: 		}
209: 		auto &get = (LogicalGet &)*plan;
210: 		for (auto &column_id : get.column_ids) {
211: 			if (column_id == COLUMN_IDENTIFIER_ROW_ID) {
212: 				throw BinderException("Cannot create an index on the rowid!");
213: 			}
214: 		}
215: 		// this gives us a logical table scan
216: 		// we take the required columns from here
217: 		// create the logical operator
218: 		result.plan = make_unique<LogicalCreateIndex>(*table, get.column_ids, move(expressions),
219: 		                                              unique_ptr_cast<CreateInfo, CreateIndexInfo>(move(stmt.info)));
220: 		break;
221: 	}
222: 	case CatalogType::TABLE_ENTRY: {
223: 		// If there is a foreign key constraint, resolve primary key column's index from primary key column's name
224: 		auto &create_info = (CreateTableInfo &)*stmt.info;
225: 		auto &catalog = Catalog::GetCatalog(context);
226: 		for (idx_t i = 0; i < create_info.constraints.size(); i++) {
227: 			auto &cond = create_info.constraints[i];
228: 			if (cond->type != ConstraintType::FOREIGN_KEY) {
229: 				continue;
230: 			}
231: 			auto &fk = (ForeignKeyConstraint &)*cond;
232: 			if (fk.info.type != ForeignKeyType::FK_TYPE_FOREIGN_KEY_TABLE) {
233: 				continue;
234: 			}
235: 			D_ASSERT(fk.info.pk_keys.empty() && !fk.pk_columns.empty());
236: 			if (create_info.table == fk.info.table) {
237: 				fk.info.type = ForeignKeyType::FK_TYPE_SELF_REFERENCE_TABLE;
238: 			} else {
239: 				// have to resolve referenced table
240: 				auto pk_table_entry_ptr = catalog.GetEntry<TableCatalogEntry>(context, fk.info.schema, fk.info.table);
241: 				D_ASSERT(!fk.pk_columns.empty() && fk.info.pk_keys.empty());
242: 				for (auto &keyname : fk.pk_columns) {
243: 					auto entry = pk_table_entry_ptr->name_map.find(keyname);
244: 					if (entry == pk_table_entry_ptr->name_map.end()) {
245: 						throw BinderException("column \"%s\" named in key does not exist", keyname);
246: 					}
247: 					fk.info.pk_keys.push_back(entry->second);
248: 				}
249: 				auto index = pk_table_entry_ptr->storage->info->indexes.FindForeignKeyIndex(
250: 				    fk.info.pk_keys, ForeignKeyType::FK_TYPE_PRIMARY_KEY_TABLE);
251: 				if (!index) {
252: 					auto fk_column_names = StringUtil::Join(fk.pk_columns, ",");
253: 					throw BinderException("Failed to create foreign key on %s(%s): no UNIQUE or PRIMARY KEY constraint "
254: 					                      "present on these columns",
255: 					                      pk_table_entry_ptr->name, fk_column_names);
256: 				}
257: 			}
258: 		}
259: 		// We first check if there are any user types, if yes we check to which custom types they refer.
260: 		auto bound_info = BindCreateTableInfo(move(stmt.info));
261: 		auto root = move(bound_info->query);
262: 
263: 		// create the logical operator
264: 		auto &schema = bound_info->schema;
265: 		auto create_table = make_unique<LogicalCreateTable>(schema, move(bound_info));
266: 		if (root) {
267: 			create_table->children.push_back(move(root));
268: 		}
269: 		result.plan = move(create_table);
270: 		break;
271: 	}
272: 	case CatalogType::TYPE_ENTRY: {
273: 		auto schema = BindSchema(*stmt.info);
274: 		result.plan = make_unique<LogicalCreate>(LogicalOperatorType::LOGICAL_CREATE_TYPE, move(stmt.info), schema);
275: 		break;
276: 	}
277: 	default:
278: 		throw Exception("Unrecognized type!");
279: 	}
280: 	this->allow_stream_result = false;
281: 	return result;
282: }
283: 
284: } // namespace duckdb
[end of src/planner/binder/statement/bind_create.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: