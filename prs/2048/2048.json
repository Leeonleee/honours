{
  "repo": "duckdb/duckdb",
  "pull_number": 2048,
  "instance_id": "duckdb__duckdb-2048",
  "issue_numbers": [
    "2028"
  ],
  "base_commit": "dfb1bf381ad1741c68cb2bd6872776b3c86648ff",
  "patch": "diff --git a/src/catalog/catalog_entry/macro_catalog_entry.cpp b/src/catalog/catalog_entry/macro_catalog_entry.cpp\nindex f888221b3a71..3bd003aa76a6 100644\n--- a/src/catalog/catalog_entry/macro_catalog_entry.cpp\n+++ b/src/catalog/catalog_entry/macro_catalog_entry.cpp\n@@ -4,7 +4,14 @@\n \n namespace duckdb {\n \n+MacroCatalogEntry::MacroCatalogEntry(Catalog *catalog, SchemaCatalogEntry *schema, CreateMacroInfo *info)\n+    : StandardEntry(CatalogType::MACRO_ENTRY, schema, catalog, info->name), function(move(info->function)) {\n+\tthis->temporary = info->temporary;\n+\tthis->internal = info->internal;\n+}\n+\n void MacroCatalogEntry::Serialize(Serializer &serializer) {\n+\tD_ASSERT(!internal);\n \tserializer.WriteString(schema->name);\n \tserializer.WriteString(name);\n \tfunction->expression->Serialize(serializer);\ndiff --git a/src/catalog/catalog_entry/table_catalog_entry.cpp b/src/catalog/catalog_entry/table_catalog_entry.cpp\nindex 9f14bdf2d102..1f8d94006599 100644\n--- a/src/catalog/catalog_entry/table_catalog_entry.cpp\n+++ b/src/catalog/catalog_entry/table_catalog_entry.cpp\n@@ -400,6 +400,7 @@ vector<LogicalType> TableCatalogEntry::GetTypes() {\n }\n \n void TableCatalogEntry::Serialize(Serializer &serializer) {\n+\tD_ASSERT(!internal);\n \tserializer.WriteString(schema->name);\n \tserializer.WriteString(name);\n \tD_ASSERT(columns.size() <= NumericLimits<uint32_t>::Maximum());\ndiff --git a/src/include/duckdb/catalog/catalog_entry/macro_catalog_entry.hpp b/src/include/duckdb/catalog/catalog_entry/macro_catalog_entry.hpp\nindex 2386a1eee624..0d02a71ebb87 100644\n--- a/src/include/duckdb/catalog/catalog_entry/macro_catalog_entry.hpp\n+++ b/src/include/duckdb/catalog/catalog_entry/macro_catalog_entry.hpp\n@@ -18,9 +18,7 @@ namespace duckdb {\n //! A macro function in the catalog\n class MacroCatalogEntry : public StandardEntry {\n public:\n-\tMacroCatalogEntry(Catalog *catalog, SchemaCatalogEntry *schema, CreateMacroInfo *info)\n-\t    : StandardEntry(CatalogType::MACRO_ENTRY, schema, catalog, info->name), function(move(info->function)) {\n-\t}\n+\tMacroCatalogEntry(Catalog *catalog, SchemaCatalogEntry *schema, CreateMacroInfo *info);\n \n \t//! The macro function\n \tunique_ptr<MacroFunction> function;\ndiff --git a/src/storage/checkpoint_manager.cpp b/src/storage/checkpoint_manager.cpp\nindex 537456e7d559..6ac5220d21d0 100644\n--- a/src/storage/checkpoint_manager.cpp\n+++ b/src/storage/checkpoint_manager.cpp\n@@ -131,6 +131,9 @@ void CheckpointManager::WriteSchema(SchemaCatalogEntry &schema) {\n \tvector<TableCatalogEntry *> tables;\n \tvector<ViewCatalogEntry *> views;\n \tschema.Scan(CatalogType::TABLE_ENTRY, [&](CatalogEntry *entry) {\n+\t\tif (entry->internal) {\n+\t\t\treturn;\n+\t\t}\n \t\tif (entry->type == CatalogType::TABLE_ENTRY) {\n \t\t\ttables.push_back((TableCatalogEntry *)entry);\n \t\t} else if (entry->type == CatalogType::VIEW_ENTRY) {\n@@ -140,11 +143,16 @@ void CheckpointManager::WriteSchema(SchemaCatalogEntry &schema) {\n \t\t}\n \t});\n \tvector<SequenceCatalogEntry *> sequences;\n-\tschema.Scan(CatalogType::SEQUENCE_ENTRY,\n-\t            [&](CatalogEntry *entry) { sequences.push_back((SequenceCatalogEntry *)entry); });\n+\tschema.Scan(CatalogType::SEQUENCE_ENTRY, [&](CatalogEntry *entry) {\n+\t\tD_ASSERT(!entry->internal);\n+\t\tsequences.push_back((SequenceCatalogEntry *)entry);\n+\t});\n \n \tvector<MacroCatalogEntry *> macros;\n \tschema.Scan(CatalogType::SCALAR_FUNCTION_ENTRY, [&](CatalogEntry *entry) {\n+\t\tif (entry->internal) {\n+\t\t\treturn;\n+\t\t}\n \t\tif (entry->type == CatalogType::MACRO_ENTRY) {\n \t\t\tmacros.push_back((MacroCatalogEntry *)entry);\n \t\t}\n",
  "test_patch": "diff --git a/test/sql/storage/test_show_tables_persistent.test b/test/sql/storage/test_show_tables_persistent.test\nnew file mode 100644\nindex 000000000000..99c9ed38232d\n--- /dev/null\n+++ b/test/sql/storage/test_show_tables_persistent.test\n@@ -0,0 +1,36 @@\n+# name: test/sql/storage/test_show_tables_persistent.test\n+# description: Test SHOW TABLES between restarts\n+# group: [storage]\n+\n+# load the DB from disk\n+load __TEST_DIR__/show_tables.db\n+\n+query I\n+show tables\n+----\n+\n+query I\n+select current_user\n+----\n+duckdb\n+\n+statement ok\n+create table anno as select 42\n+\n+query I\n+show tables\n+----\n+anno\n+\n+statement ok\n+drop table if exists anno\n+\n+query I\n+show tables\n+----\n+\n+restart\n+\n+query I\n+show tables\n+----\n",
  "problem_statement": "`show tables` returns system tables after dropping user tables and reconnect the database file (starting from version 0.2.7)\n**What does happen?**\r\nbefore version 0.2.7, `show tables` is consistent and only return user tables, since version 0.2.7, it returns system tables if I reconnect the database file\r\n\r\n**What should happen?**\r\nonly return user defined tables\r\n\r\n**To Reproduce**\r\nrun this `test.py` file with any `test.parquet`:\r\n```python\r\nimport duckdb\r\n\r\nconn = duckdb.connect('./test.db')\r\ncursor = conn.cursor()\r\n\r\nprint('empty: ', cursor.execute('show tables').fetchall())\r\n\r\ncursor.execute(\"create table anno as select * from parquet_scan('./test.parquet')\")\r\n\r\nprint('anno table created: ', cursor.execute('show tables').fetchall())\r\n\r\ncursor.execute(\"drop table if exists anno\")\r\n\r\nprint('anno table dropped: ', cursor.execute('show tables').fetchall())\r\n\r\ncursor.close()\r\nconn.close()\r\n\r\n\r\nconn = duckdb.connect('./test.db')\r\ncursor = conn.cursor()\r\n\r\nprint('tables empty ? ', cursor.execute('show tables').fetchall())\r\n\r\ncursor.close()\r\nconn.close()\r\n``` \r\n\r\nfor version 0.2.6, it prints:\r\n```bash\r\nempty:  []\r\nanno table created:  [('anno',)]\r\nanno table dropped:  []\r\ntables empty ?  []\r\n```\r\n\r\nfor version 0.2.7, it prints:\r\n```bash\r\nempty:  []\r\nanno table created:  [('anno',)]\r\nanno table dropped:  []\r\ntables empty ?  [('columns',), ('duckdb_columns',), ('duckdb_constraints',), ('duckdb_indexes',), ('duckdb_schemas',), ('duckdb_tables',), ('duckdb_types',), ('duckdb_views',), ('pg_am',), ('pg_attrdef',), ('pg_attribute',), ('pg_class',), ('pg_constraint',), ('pg_depend',), ('pg_description',), ('pg_enum',), ('pg_index',), ('pg_indexes',), ('pg_namespace',), ('pg_sequence',), ('pg_sequences',), ('pg_tables',), ('pg_tablespace',), ('pg_type',), ('pg_views',), ('pragma_database_list',), ('schemata',), ('sqlite_master',), ('sqlite_schema',), ('sqlite_temp_master',), ('sqlite_temp_schema',), ('tables',)]\r\n```\r\n\r\n\r\n**Environment (please complete the following information):**\r\n - OS: OSX Catalina 10.15.4\r\n - DuckDB Version: 0.2.7 vs 0.2.6\r\n\r\n**Before submitting**\r\n- [x] Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?\r\n- [ ] Have you tried this on the latest `master` branch? In case you cannot compile, you may find some binaries here: https://github.com/duckdb/duckdb/releases/tag/master-builds\r\n\n",
  "hints_text": "",
  "created_at": "2021-07-26T11:51:43Z"
}