You will be provided with a partial code base and an issue statement explaining a problem to resolve.
In your output, give nothing but the code (no markdown) so that your output can be copy pasted and run immediately with no changes

<issue>
`show tables` returns system tables after dropping user tables and reconnect the database file (starting from version 0.2.7)
**What does happen?**
before version 0.2.7, `show tables` is consistent and only return user tables, since version 0.2.7, it returns system tables if I reconnect the database file

**What should happen?**
only return user defined tables

**To Reproduce**
run this `test.py` file with any `test.parquet`:
```python
import duckdb

conn = duckdb.connect('./test.db')
cursor = conn.cursor()

print('empty: ', cursor.execute('show tables').fetchall())

cursor.execute("create table anno as select * from parquet_scan('./test.parquet')")

print('anno table created: ', cursor.execute('show tables').fetchall())

cursor.execute("drop table if exists anno")

print('anno table dropped: ', cursor.execute('show tables').fetchall())

cursor.close()
conn.close()


conn = duckdb.connect('./test.db')
cursor = conn.cursor()

print('tables empty ? ', cursor.execute('show tables').fetchall())

cursor.close()
conn.close()
``` 

for version 0.2.6, it prints:
```bash
empty:  []
anno table created:  [('anno',)]
anno table dropped:  []
tables empty ?  []
```

for version 0.2.7, it prints:
```bash
empty:  []
anno table created:  [('anno',)]
anno table dropped:  []
tables empty ?  [('columns',), ('duckdb_columns',), ('duckdb_constraints',), ('duckdb_indexes',), ('duckdb_schemas',), ('duckdb_tables',), ('duckdb_types',), ('duckdb_views',), ('pg_am',), ('pg_attrdef',), ('pg_attribute',), ('pg_class',), ('pg_constraint',), ('pg_depend',), ('pg_description',), ('pg_enum',), ('pg_index',), ('pg_indexes',), ('pg_namespace',), ('pg_sequence',), ('pg_sequences',), ('pg_tables',), ('pg_tablespace',), ('pg_type',), ('pg_views',), ('pragma_database_list',), ('schemata',), ('sqlite_master',), ('sqlite_schema',), ('sqlite_temp_master',), ('sqlite_temp_schema',), ('tables',)]
```


**Environment (please complete the following information):**
 - OS: OSX Catalina 10.15.4
 - DuckDB Version: 0.2.7 vs 0.2.6

**Before submitting**
- [x] Have you tried the steps to reproduce? Do they include all relevant data and configuration? Does the issue you report still appear there?
- [ ] Have you tried this on the latest `master` branch? In case you cannot compile, you may find some binaries here: https://github.com/duckdb/duckdb/releases/tag/master-builds


</issue>
<code>
[start of README.md]
1: <img src="https://duckdb.org/images/DuckDB_Logo_dl.png" height="50">
2: 
3: ![.github/workflows/main.yml](https://github.com/cwida/duckdb/workflows/.github/workflows/main.yml/badge.svg?branch=master)
4: [![CodeFactor](https://www.codefactor.io/repository/github/cwida/duckdb/badge)](https://www.codefactor.io/repository/github/cwida/duckdb)
5: [![codecov](https://codecov.io/gh/duckdb/duckdb/branch/master/graph/badge.svg?token=FaxjcfFghN)](https://codecov.io/gh/duckdb/duckdb)
6: 
7: 
8: ## Installation
9: If you just want to install and use DuckDB, please see [our website](https://www.duckdb.org) for installation and usage instructions.
10: 
11: ## Development
12: For development, DuckDB requires [CMake](https://cmake.org), Python3 and a `C++11` compliant compiler. Run `make` in the root directory to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`.
13: 
14: Please also refer to our [Contribution Guide](CONTRIBUTING.md).
15: 
16: 
[end of README.md]
[start of src/catalog/catalog_entry/macro_catalog_entry.cpp]
1: #include "duckdb/catalog/catalog_entry/macro_catalog_entry.hpp"
2: 
3: #include "duckdb/common/serializer.hpp"
4: 
5: namespace duckdb {
6: 
7: void MacroCatalogEntry::Serialize(Serializer &serializer) {
8: 	serializer.WriteString(schema->name);
9: 	serializer.WriteString(name);
10: 	function->expression->Serialize(serializer);
11: 	serializer.Write<uint32_t>((uint32_t)function->parameters.size());
12: 	for (auto &param : function->parameters) {
13: 		param->Serialize(serializer);
14: 	}
15: 	serializer.Write<uint32_t>((uint32_t)function->default_parameters.size());
16: 	for (auto &kv : function->default_parameters) {
17: 		serializer.WriteString(kv.first);
18: 		kv.second->Serialize(serializer);
19: 	}
20: }
21: 
22: unique_ptr<CreateMacroInfo> MacroCatalogEntry::Deserialize(Deserializer &source) {
23: 	auto info = make_unique<CreateMacroInfo>();
24: 	info->schema = source.Read<string>();
25: 	info->name = source.Read<string>();
26: 	info->function = make_unique<MacroFunction>(ParsedExpression::Deserialize(source));
27: 	auto param_count = source.Read<uint32_t>();
28: 	for (idx_t i = 0; i < param_count; i++) {
29: 		info->function->parameters.push_back(ParsedExpression::Deserialize(source));
30: 	}
31: 	auto default_param_count = source.Read<uint32_t>();
32: 	for (idx_t i = 0; i < default_param_count; i++) {
33: 		auto name = source.Read<string>();
34: 		info->function->default_parameters[name] = ParsedExpression::Deserialize(source);
35: 	}
36: 	return info;
37: }
38: 
39: } // namespace duckdb
[end of src/catalog/catalog_entry/macro_catalog_entry.cpp]
[start of src/catalog/catalog_entry/table_catalog_entry.cpp]
1: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
2: 
3: #include "duckdb/catalog/catalog.hpp"
4: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
5: #include "duckdb/common/exception.hpp"
6: #include "duckdb/common/serializer.hpp"
7: #include "duckdb/main/connection.hpp"
8: #include "duckdb/main/database.hpp"
9: #include "duckdb/parser/constraints/list.hpp"
10: #include "duckdb/parser/parsed_data/alter_table_info.hpp"
11: #include "duckdb/planner/constraints/bound_not_null_constraint.hpp"
12: #include "duckdb/planner/constraints/bound_unique_constraint.hpp"
13: #include "duckdb/planner/constraints/bound_check_constraint.hpp"
14: #include "duckdb/planner/expression/bound_constant_expression.hpp"
15: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
16: #include "duckdb/storage/storage_manager.hpp"
17: #include "duckdb/planner/binder.hpp"
18: 
19: #include "duckdb/execution/index/art/art.hpp"
20: #include "duckdb/parser/expression/columnref_expression.hpp"
21: #include "duckdb/planner/expression/bound_reference_expression.hpp"
22: #include "duckdb/parser/parsed_expression_iterator.hpp"
23: #include "duckdb/planner/expression_binder/alter_binder.hpp"
24: #include "duckdb/parser/keyword_helper.hpp"
25: 
26: #include <algorithm>
27: #include <sstream>
28: 
29: namespace duckdb {
30: 
31: void TableCatalogEntry::AddLowerCaseAliases(unordered_map<string, column_t> &name_map) {
32: 	unordered_map<string, column_t> extra_lowercase_names;
33: 	for (auto &entry : name_map) {
34: 		auto lcase = StringUtil::Lower(entry.first);
35: 		// check the lowercase name map if there already exists a lowercase version
36: 		if (extra_lowercase_names.find(lcase) == extra_lowercase_names.end()) {
37: 			// not yet: add the mapping
38: 			extra_lowercase_names[lcase] = entry.second;
39: 		} else {
40: 			// the lowercase already exists: set it to invalid index
41: 			extra_lowercase_names[lcase] = INVALID_INDEX;
42: 		}
43: 	}
44: 	// for any new lowercase names, add them to the original name map
45: 	for (auto &entry : extra_lowercase_names) {
46: 		if (entry.second != INVALID_INDEX) {
47: 			name_map[entry.first] = entry.second;
48: 		}
49: 	}
50: }
51: 
52: TableCatalogEntry::TableCatalogEntry(Catalog *catalog, SchemaCatalogEntry *schema, BoundCreateTableInfo *info,
53:                                      std::shared_ptr<DataTable> inherited_storage)
54:     : StandardEntry(CatalogType::TABLE_ENTRY, schema, catalog, info->Base().table), storage(move(inherited_storage)),
55:       columns(move(info->Base().columns)), constraints(move(info->Base().constraints)),
56:       bound_constraints(move(info->bound_constraints)), name_map(info->name_map) {
57: 	this->temporary = info->Base().temporary;
58: 	// add lower case aliases
59: 	AddLowerCaseAliases(name_map);
60: 	// add the "rowid" alias, if there is no rowid column specified in the table
61: 	if (name_map.find("rowid") == name_map.end()) {
62: 		name_map["rowid"] = COLUMN_IDENTIFIER_ROW_ID;
63: 	}
64: 	if (!storage) {
65: 		// create the physical storage
66: 		storage = make_shared<DataTable>(catalog->db, schema->name, name, GetTypes(), move(info->data));
67: 
68: 		// create the unique indexes for the UNIQUE and PRIMARY KEY constraints
69: 		for (idx_t i = 0; i < bound_constraints.size(); i++) {
70: 			auto &constraint = bound_constraints[i];
71: 			if (constraint->type == ConstraintType::UNIQUE) {
72: 				// unique constraint: create a unique index
73: 				auto &unique = (BoundUniqueConstraint &)*constraint;
74: 				// fetch types and create expressions for the index from the columns
75: 				vector<column_t> column_ids;
76: 				vector<unique_ptr<Expression>> unbound_expressions;
77: 				vector<unique_ptr<Expression>> bound_expressions;
78: 				idx_t key_nr = 0;
79: 				for (auto &key : unique.keys) {
80: 					D_ASSERT(key < columns.size());
81: 
82: 					unbound_expressions.push_back(make_unique<BoundColumnRefExpression>(
83: 					    columns[key].name, columns[key].type, ColumnBinding(0, column_ids.size())));
84: 
85: 					bound_expressions.push_back(make_unique<BoundReferenceExpression>(columns[key].type, key_nr++));
86: 					column_ids.push_back(key);
87: 				}
88: 				// create an adaptive radix tree around the expressions
89: 				auto art = make_unique<ART>(column_ids, move(unbound_expressions), true, unique.is_primary_key);
90: 				storage->AddIndex(move(art), bound_expressions);
91: 			}
92: 		}
93: 	}
94: }
95: 
96: bool TableCatalogEntry::ColumnExists(const string &name) {
97: 	return name_map.find(name) != name_map.end();
98: }
99: 
100: unique_ptr<CatalogEntry> TableCatalogEntry::AlterEntry(ClientContext &context, AlterInfo *info) {
101: 	D_ASSERT(!internal);
102: 	if (info->type != AlterType::ALTER_TABLE) {
103: 		throw CatalogException("Can only modify table with ALTER TABLE statement");
104: 	}
105: 	auto table_info = (AlterTableInfo *)info;
106: 	switch (table_info->alter_table_type) {
107: 	case AlterTableType::RENAME_COLUMN: {
108: 		auto rename_info = (RenameColumnInfo *)table_info;
109: 		return RenameColumn(context, *rename_info);
110: 	}
111: 	case AlterTableType::RENAME_TABLE: {
112: 		auto rename_info = (RenameTableInfo *)table_info;
113: 		auto copied_table = Copy(context);
114: 		copied_table->name = rename_info->new_table_name;
115: 		return copied_table;
116: 	}
117: 	case AlterTableType::ADD_COLUMN: {
118: 		auto add_info = (AddColumnInfo *)table_info;
119: 		return AddColumn(context, *add_info);
120: 	}
121: 	case AlterTableType::REMOVE_COLUMN: {
122: 		auto remove_info = (RemoveColumnInfo *)table_info;
123: 		return RemoveColumn(context, *remove_info);
124: 	}
125: 	case AlterTableType::SET_DEFAULT: {
126: 		auto set_default_info = (SetDefaultInfo *)table_info;
127: 		return SetDefault(context, *set_default_info);
128: 	}
129: 	case AlterTableType::ALTER_COLUMN_TYPE: {
130: 		auto change_type_info = (ChangeColumnTypeInfo *)table_info;
131: 		return ChangeColumnType(context, *change_type_info);
132: 	}
133: 	default:
134: 		throw InternalException("Unrecognized alter table type!");
135: 	}
136: }
137: 
138: static void RenameExpression(ParsedExpression &expr, RenameColumnInfo &info) {
139: 	if (expr.type == ExpressionType::COLUMN_REF) {
140: 		auto &colref = (ColumnRefExpression &)expr;
141: 		if (colref.column_name == info.old_name) {
142: 			colref.column_name = info.new_name;
143: 		}
144: 	}
145: 	ParsedExpressionIterator::EnumerateChildren(
146: 	    expr, [&](const ParsedExpression &child) { RenameExpression((ParsedExpression &)child, info); });
147: }
148: 
149: unique_ptr<CatalogEntry> TableCatalogEntry::RenameColumn(ClientContext &context, RenameColumnInfo &info) {
150: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
151: 	create_info->temporary = temporary;
152: 	bool found = false;
153: 	for (idx_t i = 0; i < columns.size(); i++) {
154: 		ColumnDefinition copy = columns[i].Copy();
155: 
156: 		create_info->columns.push_back(move(copy));
157: 		if (info.old_name == columns[i].name) {
158: 			D_ASSERT(!found);
159: 			create_info->columns[i].name = info.new_name;
160: 			found = true;
161: 		}
162: 	}
163: 	if (!found) {
164: 		throw CatalogException("Table does not have a column with name \"%s\"", info.name);
165: 	}
166: 	for (idx_t c_idx = 0; c_idx < constraints.size(); c_idx++) {
167: 		auto copy = constraints[c_idx]->Copy();
168: 		switch (copy->type) {
169: 		case ConstraintType::NOT_NULL:
170: 			// NOT NULL constraint: no adjustments necessary
171: 			break;
172: 		case ConstraintType::CHECK: {
173: 			// CHECK constraint: need to rename column references that refer to the renamed column
174: 			auto &check = (CheckConstraint &)*copy;
175: 			RenameExpression(*check.expression, info);
176: 			break;
177: 		}
178: 		case ConstraintType::UNIQUE: {
179: 			// UNIQUE constraint: possibly need to rename columns
180: 			auto &unique = (UniqueConstraint &)*copy;
181: 			for (idx_t i = 0; i < unique.columns.size(); i++) {
182: 				if (unique.columns[i] == info.old_name) {
183: 					unique.columns[i] = info.new_name;
184: 				}
185: 			}
186: 			break;
187: 		}
188: 		default:
189: 			throw InternalException("Unsupported constraint for entry!");
190: 		}
191: 		create_info->constraints.push_back(move(copy));
192: 	}
193: 	auto binder = Binder::CreateBinder(context);
194: 	auto bound_create_info = binder->BindCreateTableInfo(move(create_info));
195: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
196: }
197: 
198: unique_ptr<CatalogEntry> TableCatalogEntry::AddColumn(ClientContext &context, AddColumnInfo &info) {
199: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
200: 	create_info->temporary = temporary;
201: 	for (idx_t i = 0; i < columns.size(); i++) {
202: 		create_info->columns.push_back(columns[i].Copy());
203: 	}
204: 	info.new_column.oid = columns.size();
205: 	create_info->columns.push_back(info.new_column.Copy());
206: 
207: 	auto binder = Binder::CreateBinder(context);
208: 	auto bound_create_info = binder->BindCreateTableInfo(move(create_info));
209: 	auto new_storage =
210: 	    make_shared<DataTable>(context, *storage, info.new_column, bound_create_info->bound_defaults.back().get());
211: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
212: 	                                      new_storage);
213: }
214: 
215: unique_ptr<CatalogEntry> TableCatalogEntry::RemoveColumn(ClientContext &context, RemoveColumnInfo &info) {
216: 	idx_t removed_index = INVALID_INDEX;
217: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
218: 	create_info->temporary = temporary;
219: 	for (idx_t i = 0; i < columns.size(); i++) {
220: 		if (columns[i].name == info.removed_column) {
221: 			D_ASSERT(removed_index == INVALID_INDEX);
222: 			removed_index = i;
223: 			continue;
224: 		}
225: 		create_info->columns.push_back(columns[i].Copy());
226: 	}
227: 	if (removed_index == INVALID_INDEX) {
228: 		if (!info.if_exists) {
229: 			throw CatalogException("Table does not have a column with name \"%s\"", info.removed_column);
230: 		}
231: 		return nullptr;
232: 	}
233: 	if (create_info->columns.empty()) {
234: 		throw CatalogException("Cannot drop column: table only has one column remaining!");
235: 	}
236: 	// handle constraints for the new table
237: 	D_ASSERT(constraints.size() == bound_constraints.size());
238: 	for (idx_t constr_idx = 0; constr_idx < constraints.size(); constr_idx++) {
239: 		auto &constraint = constraints[constr_idx];
240: 		auto &bound_constraint = bound_constraints[constr_idx];
241: 		switch (bound_constraint->type) {
242: 		case ConstraintType::NOT_NULL: {
243: 			auto &not_null_constraint = (BoundNotNullConstraint &)*bound_constraint;
244: 			if (not_null_constraint.index != removed_index) {
245: 				// the constraint is not about this column: we need to copy it
246: 				// we might need to shift the index back by one though, to account for the removed column
247: 				idx_t new_index = not_null_constraint.index;
248: 				if (not_null_constraint.index > removed_index) {
249: 					new_index -= 1;
250: 				}
251: 				create_info->constraints.push_back(make_unique<NotNullConstraint>(new_index));
252: 			}
253: 			break;
254: 		}
255: 		case ConstraintType::CHECK: {
256: 			// CHECK constraint
257: 			auto &bound_check = (BoundCheckConstraint &)*bound_constraint;
258: 			// check if the removed column is part of the check constraint
259: 			if (bound_check.bound_columns.find(removed_index) != bound_check.bound_columns.end()) {
260: 				if (bound_check.bound_columns.size() > 1) {
261: 					// CHECK constraint that concerns mult
262: 					throw CatalogException(
263: 					    "Cannot drop column \"%s\" because there is a CHECK constraint that depends on it",
264: 					    info.removed_column);
265: 				} else {
266: 					// CHECK constraint that ONLY concerns this column, strip the constraint
267: 				}
268: 			} else {
269: 				// check constraint does not concern the removed column: simply re-add it
270: 				create_info->constraints.push_back(constraint->Copy());
271: 			}
272: 			break;
273: 		}
274: 		case ConstraintType::UNIQUE: {
275: 			auto copy = constraint->Copy();
276: 			auto &unique = (UniqueConstraint &)*copy;
277: 			if (unique.index != INVALID_INDEX) {
278: 				if (unique.index == removed_index) {
279: 					throw CatalogException(
280: 					    "Cannot drop column \"%s\" because there is a UNIQUE constraint that depends on it",
281: 					    info.removed_column);
282: 				} else if (unique.index > removed_index) {
283: 					unique.index--;
284: 				}
285: 			}
286: 			create_info->constraints.push_back(move(copy));
287: 			break;
288: 		}
289: 		default:
290: 			throw InternalException("Unsupported constraint for entry!");
291: 		}
292: 	}
293: 
294: 	auto binder = Binder::CreateBinder(context);
295: 	auto bound_create_info = binder->BindCreateTableInfo(move(create_info));
296: 	auto new_storage = make_shared<DataTable>(context, *storage, removed_index);
297: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
298: 	                                      new_storage);
299: }
300: 
301: unique_ptr<CatalogEntry> TableCatalogEntry::SetDefault(ClientContext &context, SetDefaultInfo &info) {
302: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
303: 	bool found = false;
304: 	for (idx_t i = 0; i < columns.size(); i++) {
305: 		auto copy = columns[i].Copy();
306: 		if (info.column_name == copy.name) {
307: 			// set the default value of this column
308: 			copy.default_value = info.expression ? info.expression->Copy() : nullptr;
309: 			found = true;
310: 		}
311: 		create_info->columns.push_back(move(copy));
312: 	}
313: 	if (!found) {
314: 		throw BinderException("Table \"%s\" does not have a column with name \"%s\"", info.name, info.column_name);
315: 	}
316: 
317: 	for (idx_t i = 0; i < constraints.size(); i++) {
318: 		auto constraint = constraints[i]->Copy();
319: 		create_info->constraints.push_back(move(constraint));
320: 	}
321: 
322: 	auto binder = Binder::CreateBinder(context);
323: 	auto bound_create_info = binder->BindCreateTableInfo(move(create_info));
324: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
325: }
326: 
327: unique_ptr<CatalogEntry> TableCatalogEntry::ChangeColumnType(ClientContext &context, ChangeColumnTypeInfo &info) {
328: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
329: 	idx_t change_idx = INVALID_INDEX;
330: 	for (idx_t i = 0; i < columns.size(); i++) {
331: 		auto copy = columns[i].Copy();
332: 		if (info.column_name == copy.name) {
333: 			// set the default value of this column
334: 			change_idx = i;
335: 			copy.type = info.target_type;
336: 		}
337: 		create_info->columns.push_back(move(copy));
338: 	}
339: 	if (change_idx == INVALID_INDEX) {
340: 		throw BinderException("Table \"%s\" does not have a column with name \"%s\"", info.name, info.column_name);
341: 	}
342: 
343: 	for (idx_t i = 0; i < constraints.size(); i++) {
344: 		auto constraint = constraints[i]->Copy();
345: 		switch (constraint->type) {
346: 		case ConstraintType::CHECK: {
347: 			auto &bound_check = (BoundCheckConstraint &)*bound_constraints[i];
348: 			if (bound_check.bound_columns.find(change_idx) != bound_check.bound_columns.end()) {
349: 				throw BinderException("Cannot change the type of a column that has a CHECK constraint specified");
350: 			}
351: 			break;
352: 		}
353: 		case ConstraintType::NOT_NULL:
354: 			break;
355: 		case ConstraintType::UNIQUE: {
356: 			auto &bound_unique = (BoundUniqueConstraint &)*bound_constraints[i];
357: 			if (bound_unique.key_set.find(change_idx) != bound_unique.key_set.end()) {
358: 				throw BinderException(
359: 				    "Cannot change the type of a column that has a UNIQUE or PRIMARY KEY constraint specified");
360: 			}
361: 			break;
362: 		}
363: 		default:
364: 			throw InternalException("Unsupported constraint for entry!");
365: 		}
366: 		create_info->constraints.push_back(move(constraint));
367: 	}
368: 
369: 	auto binder = Binder::CreateBinder(context);
370: 	// bind the specified expression
371: 	vector<column_t> bound_columns;
372: 	AlterBinder expr_binder(*binder, context, name, columns, bound_columns, info.target_type);
373: 	auto expression = info.expression->Copy();
374: 	auto bound_expression = expr_binder.Bind(expression);
375: 	auto bound_create_info = binder->BindCreateTableInfo(move(create_info));
376: 	if (bound_columns.empty()) {
377: 		bound_columns.push_back(COLUMN_IDENTIFIER_ROW_ID);
378: 	}
379: 
380: 	auto new_storage =
381: 	    make_shared<DataTable>(context, *storage, change_idx, info.target_type, move(bound_columns), *bound_expression);
382: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(),
383: 	                                      new_storage);
384: }
385: 
386: ColumnDefinition &TableCatalogEntry::GetColumn(const string &name) {
387: 	auto entry = name_map.find(name);
388: 	if (entry == name_map.end() || entry->second == COLUMN_IDENTIFIER_ROW_ID) {
389: 		throw CatalogException("Column with name %s does not exist!", name);
390: 	}
391: 	return columns[entry->second];
392: }
393: 
394: vector<LogicalType> TableCatalogEntry::GetTypes() {
395: 	vector<LogicalType> types;
396: 	for (auto &it : columns) {
397: 		types.push_back(it.type);
398: 	}
399: 	return types;
400: }
401: 
402: void TableCatalogEntry::Serialize(Serializer &serializer) {
403: 	serializer.WriteString(schema->name);
404: 	serializer.WriteString(name);
405: 	D_ASSERT(columns.size() <= NumericLimits<uint32_t>::Maximum());
406: 	serializer.Write<uint32_t>((uint32_t)columns.size());
407: 	for (auto &column : columns) {
408: 		column.Serialize(serializer);
409: 	}
410: 	D_ASSERT(constraints.size() <= NumericLimits<uint32_t>::Maximum());
411: 	serializer.Write<uint32_t>((uint32_t)constraints.size());
412: 	for (auto &constraint : constraints) {
413: 		constraint->Serialize(serializer);
414: 	}
415: }
416: 
417: string TableCatalogEntry::ToSQL() {
418: 	std::stringstream ss;
419: 
420: 	ss << "CREATE TABLE ";
421: 
422: 	if (schema->name != DEFAULT_SCHEMA) {
423: 		ss << KeywordHelper::WriteOptionallyQuoted(schema->name) << ".";
424: 	}
425: 
426: 	ss << KeywordHelper::WriteOptionallyQuoted(name) << "(";
427: 
428: 	// find all columns that have NOT NULL specified, but are NOT primary key columns
429: 	unordered_set<idx_t> not_null_columns;
430: 	unordered_set<idx_t> unique_columns;
431: 	unordered_set<idx_t> pk_columns;
432: 	unordered_set<string> multi_key_pks;
433: 	vector<string> extra_constraints;
434: 	for (auto &constraint : constraints) {
435: 		if (constraint->type == ConstraintType::NOT_NULL) {
436: 			auto &not_null = (NotNullConstraint &)*constraint;
437: 			not_null_columns.insert(not_null.index);
438: 		} else if (constraint->type == ConstraintType::UNIQUE) {
439: 			auto &pk = (UniqueConstraint &)*constraint;
440: 			vector<string> constraint_columns = pk.columns;
441: 			if (pk.index != INVALID_INDEX) {
442: 				// no columns specified: single column constraint
443: 				if (pk.is_primary_key) {
444: 					pk_columns.insert(pk.index);
445: 				} else {
446: 					unique_columns.insert(pk.index);
447: 				}
448: 			} else {
449: 				// multi-column constraint, this constraint needs to go at the end after all columns
450: 				if (pk.is_primary_key) {
451: 					// multi key pk column: insert set of columns into multi_key_pks
452: 					for (auto &col : pk.columns) {
453: 						multi_key_pks.insert(col);
454: 					}
455: 				}
456: 				extra_constraints.push_back(constraint->ToString());
457: 			}
458: 		} else {
459: 			extra_constraints.push_back(constraint->ToString());
460: 		}
461: 	}
462: 
463: 	for (idx_t i = 0; i < columns.size(); i++) {
464: 		if (i > 0) {
465: 			ss << ", ";
466: 		}
467: 		auto &column = columns[i];
468: 		ss << KeywordHelper::WriteOptionallyQuoted(column.name) << " " << column.type.ToString();
469: 		bool not_null = not_null_columns.find(column.oid) != not_null_columns.end();
470: 		bool is_single_key_pk = pk_columns.find(column.oid) != pk_columns.end();
471: 		bool is_multi_key_pk = multi_key_pks.find(column.name) != multi_key_pks.end();
472: 		bool is_unique = unique_columns.find(column.oid) != unique_columns.end();
473: 		if (not_null && !is_single_key_pk && !is_multi_key_pk) {
474: 			// NOT NULL but not a primary key column
475: 			ss << " NOT NULL";
476: 		}
477: 		if (is_single_key_pk) {
478: 			// single column pk: insert constraint here
479: 			ss << " PRIMARY KEY";
480: 		}
481: 		if (is_unique) {
482: 			// single column unique: insert constraint here
483: 			ss << " UNIQUE";
484: 		}
485: 		if (column.default_value) {
486: 			ss << " DEFAULT(" << column.default_value->ToString() << ")";
487: 		}
488: 	}
489: 	// print any extra constraints that still need to be printed
490: 	for (auto &extra_constraint : extra_constraints) {
491: 		ss << ", ";
492: 		ss << extra_constraint;
493: 	}
494: 
495: 	ss << ");";
496: 	return ss.str();
497: }
498: 
499: unique_ptr<CreateTableInfo> TableCatalogEntry::Deserialize(Deserializer &source) {
500: 	auto info = make_unique<CreateTableInfo>();
501: 
502: 	info->schema = source.Read<string>();
503: 	info->table = source.Read<string>();
504: 	auto column_count = source.Read<uint32_t>();
505: 
506: 	for (uint32_t i = 0; i < column_count; i++) {
507: 		auto column = ColumnDefinition::Deserialize(source);
508: 		info->columns.push_back(move(column));
509: 	}
510: 	auto constraint_count = source.Read<uint32_t>();
511: 
512: 	for (uint32_t i = 0; i < constraint_count; i++) {
513: 		auto constraint = Constraint::Deserialize(source);
514: 		info->constraints.push_back(move(constraint));
515: 	}
516: 	return info;
517: }
518: 
519: unique_ptr<CatalogEntry> TableCatalogEntry::Copy(ClientContext &context) {
520: 	auto create_info = make_unique<CreateTableInfo>(schema->name, name);
521: 	for (idx_t i = 0; i < columns.size(); i++) {
522: 		create_info->columns.push_back(columns[i].Copy());
523: 	}
524: 
525: 	for (idx_t i = 0; i < constraints.size(); i++) {
526: 		auto constraint = constraints[i]->Copy();
527: 		create_info->constraints.push_back(move(constraint));
528: 	}
529: 
530: 	auto binder = Binder::CreateBinder(context);
531: 	auto bound_create_info = binder->BindCreateTableInfo(move(create_info));
532: 	return make_unique<TableCatalogEntry>(catalog, schema, (BoundCreateTableInfo *)bound_create_info.get(), storage);
533: }
534: 
535: void TableCatalogEntry::SetAsRoot() {
536: 	storage->SetAsRoot();
537: }
538: 
539: void TableCatalogEntry::CommitAlter(AlterInfo &info) {
540: 	D_ASSERT(info.type == AlterType::ALTER_TABLE);
541: 	auto &alter_table = (AlterTableInfo &)info;
542: 	string column_name;
543: 	switch (alter_table.alter_table_type) {
544: 	case AlterTableType::REMOVE_COLUMN: {
545: 		auto &remove_info = (RemoveColumnInfo &)alter_table;
546: 		column_name = remove_info.removed_column;
547: 		break;
548: 	}
549: 	case AlterTableType::ALTER_COLUMN_TYPE: {
550: 		auto &change_info = (ChangeColumnTypeInfo &)alter_table;
551: 		column_name = change_info.column_name;
552: 		break;
553: 	}
554: 	default:
555: 		break;
556: 	}
557: 	if (column_name.empty()) {
558: 		return;
559: 	}
560: 	idx_t removed_index = INVALID_INDEX;
561: 	for (idx_t i = 0; i < columns.size(); i++) {
562: 		if (columns[i].name == column_name) {
563: 			D_ASSERT(removed_index == INVALID_INDEX);
564: 			removed_index = i;
565: 			continue;
566: 		}
567: 	}
568: 	D_ASSERT(removed_index != INVALID_INDEX);
569: 	storage->CommitDropColumn(removed_index);
570: }
571: 
572: void TableCatalogEntry::CommitDrop() {
573: 	storage->CommitDropTable();
574: }
575: 
576: } // namespace duckdb
[end of src/catalog/catalog_entry/table_catalog_entry.cpp]
[start of src/include/duckdb/catalog/catalog_entry/macro_catalog_entry.hpp]
1: //===----------------------------------------------------------------------===//
2: //                         DuckDB
3: //
4: // duckdb/catalog/catalog_entry/macro_catalog_entry.hpp
5: //
6: //
7: //===----------------------------------------------------------------------===//
8: 
9: #pragma once
10: 
11: #include "duckdb/catalog/catalog_set.hpp"
12: #include "duckdb/catalog/standard_entry.hpp"
13: #include "duckdb/function/macro_function.hpp"
14: #include "duckdb/parser/parsed_data/create_macro_info.hpp"
15: 
16: namespace duckdb {
17: 
18: //! A macro function in the catalog
19: class MacroCatalogEntry : public StandardEntry {
20: public:
21: 	MacroCatalogEntry(Catalog *catalog, SchemaCatalogEntry *schema, CreateMacroInfo *info)
22: 	    : StandardEntry(CatalogType::MACRO_ENTRY, schema, catalog, info->name), function(move(info->function)) {
23: 	}
24: 
25: 	//! The macro function
26: 	unique_ptr<MacroFunction> function;
27: 
28: public:
29: 	//! Serialize the meta information of the MacroCatalogEntry a serializer
30: 	virtual void Serialize(Serializer &serializer);
31: 	//! Deserializes to a CreateMacroInfo
32: 	static unique_ptr<CreateMacroInfo> Deserialize(Deserializer &source);
33: };
34: } // namespace duckdb
[end of src/include/duckdb/catalog/catalog_entry/macro_catalog_entry.hpp]
[start of src/storage/checkpoint_manager.cpp]
1: #include "duckdb/storage/checkpoint_manager.hpp"
2: #include "duckdb/storage/block_manager.hpp"
3: #include "duckdb/storage/meta_block_reader.hpp"
4: 
5: #include "duckdb/common/serializer.hpp"
6: #include "duckdb/common/vector_operations/vector_operations.hpp"
7: #include "duckdb/common/types/null_value.hpp"
8: 
9: #include "duckdb/catalog/catalog.hpp"
10: #include "duckdb/catalog/catalog_entry/macro_catalog_entry.hpp"
11: #include "duckdb/catalog/catalog_entry/schema_catalog_entry.hpp"
12: #include "duckdb/catalog/catalog_entry/sequence_catalog_entry.hpp"
13: #include "duckdb/catalog/catalog_entry/table_catalog_entry.hpp"
14: #include "duckdb/catalog/catalog_entry/view_catalog_entry.hpp"
15: 
16: #include "duckdb/parser/parsed_data/create_schema_info.hpp"
17: #include "duckdb/parser/parsed_data/create_table_info.hpp"
18: #include "duckdb/parser/parsed_data/create_view_info.hpp"
19: 
20: #include "duckdb/planner/binder.hpp"
21: #include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
22: 
23: #include "duckdb/main/client_context.hpp"
24: #include "duckdb/main/connection.hpp"
25: #include "duckdb/main/database.hpp"
26: 
27: #include "duckdb/transaction/transaction_manager.hpp"
28: 
29: #include "duckdb/storage/checkpoint/table_data_writer.hpp"
30: #include "duckdb/storage/checkpoint/table_data_reader.hpp"
31: #include "duckdb/main/config.hpp"
32: 
33: namespace duckdb {
34: 
35: CheckpointManager::CheckpointManager(DatabaseInstance &db) : db(db) {
36: }
37: 
38: void CheckpointManager::CreateCheckpoint() {
39: 	auto &config = DBConfig::GetConfig(db);
40: 	auto &storage_manager = StorageManager::GetStorageManager(db);
41: 	if (storage_manager.InMemory()) {
42: 		return;
43: 	}
44: 	// assert that the checkpoint manager hasn't been used before
45: 	D_ASSERT(!metadata_writer);
46: 
47: 	auto &block_manager = BlockManager::GetBlockManager(db);
48: 	block_manager.StartCheckpoint();
49: 
50: 	//! Set up the writers for the checkpoints
51: 	metadata_writer = make_unique<MetaBlockWriter>(db);
52: 	tabledata_writer = make_unique<MetaBlockWriter>(db);
53: 
54: 	// get the id of the first meta block
55: 	block_id_t meta_block = metadata_writer->block->id;
56: 
57: 	vector<SchemaCatalogEntry *> schemas;
58: 	// we scan the set of committed schemas
59: 	auto &catalog = Catalog::GetCatalog(db);
60: 	catalog.schemas->Scan([&](CatalogEntry *entry) { schemas.push_back((SchemaCatalogEntry *)entry); });
61: 	// write the actual data into the database
62: 	// write the amount of schemas
63: 	metadata_writer->Write<uint32_t>(schemas.size());
64: 	for (auto &schema : schemas) {
65: 		WriteSchema(*schema);
66: 	}
67: 	// flush the meta data to disk
68: 	metadata_writer->Flush();
69: 	tabledata_writer->Flush();
70: 
71: 	// write a checkpoint flag to the WAL
72: 	// this protects against the rare event that the database crashes AFTER writing the file, but BEFORE truncating the
73: 	// WAL we write an entry CHECKPOINT "meta_block_id" into the WAL upon loading, if we see there is an entry
74: 	// CHECKPOINT "meta_block_id", and the id MATCHES the head idin the file we know that the database was successfully
75: 	// checkpointed, so we know that we should avoid replaying the WAL to avoid duplicating data
76: 	auto wal = storage_manager.GetWriteAheadLog();
77: 	wal->WriteCheckpoint(meta_block);
78: 	wal->Flush();
79: 
80: 	if (config.checkpoint_abort == CheckpointAbort::DEBUG_ABORT_BEFORE_HEADER) {
81: 		throw IOException("Checkpoint aborted before header write because of PRAGMA checkpoint_abort flag");
82: 	}
83: 
84: 	// finally write the updated header
85: 	DatabaseHeader header;
86: 	header.meta_block = meta_block;
87: 	block_manager.WriteHeader(header);
88: 
89: 	if (config.checkpoint_abort == CheckpointAbort::DEBUG_ABORT_BEFORE_TRUNCATE) {
90: 		throw IOException("Checkpoint aborted before truncate because of PRAGMA checkpoint_abort flag");
91: 	}
92: 
93: 	// truncate the WAL
94: 	wal->Truncate(0);
95: 
96: 	// mark all blocks written as part of the metadata as modified
97: 	for (auto &block_id : metadata_writer->written_blocks) {
98: 		block_manager.MarkBlockAsModified(block_id);
99: 	}
100: 	for (auto &block_id : tabledata_writer->written_blocks) {
101: 		block_manager.MarkBlockAsModified(block_id);
102: 	}
103: }
104: 
105: void CheckpointManager::LoadFromStorage() {
106: 	auto &block_manager = BlockManager::GetBlockManager(db);
107: 	block_id_t meta_block = block_manager.GetMetaBlock();
108: 	if (meta_block < 0) {
109: 		// storage is empty
110: 		return;
111: 	}
112: 
113: 	Connection con(db);
114: 	con.BeginTransaction();
115: 	// create the MetaBlockReader to read from the storage
116: 	MetaBlockReader reader(db, meta_block);
117: 	uint32_t schema_count = reader.Read<uint32_t>();
118: 	for (uint32_t i = 0; i < schema_count; i++) {
119: 		ReadSchema(*con.context, reader);
120: 	}
121: 	con.Commit();
122: }
123: 
124: //===--------------------------------------------------------------------===//
125: // Schema
126: //===--------------------------------------------------------------------===//
127: void CheckpointManager::WriteSchema(SchemaCatalogEntry &schema) {
128: 	// write the schema data
129: 	schema.Serialize(*metadata_writer);
130: 	// then, we fetch the tables/views/sequences information
131: 	vector<TableCatalogEntry *> tables;
132: 	vector<ViewCatalogEntry *> views;
133: 	schema.Scan(CatalogType::TABLE_ENTRY, [&](CatalogEntry *entry) {
134: 		if (entry->type == CatalogType::TABLE_ENTRY) {
135: 			tables.push_back((TableCatalogEntry *)entry);
136: 		} else if (entry->type == CatalogType::VIEW_ENTRY) {
137: 			views.push_back((ViewCatalogEntry *)entry);
138: 		} else {
139: 			throw NotImplementedException("Catalog type for entries");
140: 		}
141: 	});
142: 	vector<SequenceCatalogEntry *> sequences;
143: 	schema.Scan(CatalogType::SEQUENCE_ENTRY,
144: 	            [&](CatalogEntry *entry) { sequences.push_back((SequenceCatalogEntry *)entry); });
145: 
146: 	vector<MacroCatalogEntry *> macros;
147: 	schema.Scan(CatalogType::SCALAR_FUNCTION_ENTRY, [&](CatalogEntry *entry) {
148: 		if (entry->type == CatalogType::MACRO_ENTRY) {
149: 			macros.push_back((MacroCatalogEntry *)entry);
150: 		}
151: 	});
152: 
153: 	// write the sequences
154: 	metadata_writer->Write<uint32_t>(sequences.size());
155: 	for (auto &seq : sequences) {
156: 		WriteSequence(*seq);
157: 	}
158: 	// now write the tables
159: 	metadata_writer->Write<uint32_t>(tables.size());
160: 	for (auto &table : tables) {
161: 		WriteTable(*table);
162: 	}
163: 	// now write the views
164: 	metadata_writer->Write<uint32_t>(views.size());
165: 	for (auto &view : views) {
166: 		WriteView(*view);
167: 	}
168: 	// finally write the macro's
169: 	metadata_writer->Write<uint32_t>(macros.size());
170: 	for (auto &macro : macros) {
171: 		WriteMacro(*macro);
172: 	}
173: }
174: 
175: void CheckpointManager::ReadSchema(ClientContext &context, MetaBlockReader &reader) {
176: 	auto &catalog = Catalog::GetCatalog(db);
177: 
178: 	// read the schema and create it in the catalog
179: 	auto info = SchemaCatalogEntry::Deserialize(reader);
180: 	// we set create conflict to ignore to ignore the failure of recreating the main schema
181: 	info->on_conflict = OnCreateConflict::IGNORE_ON_CONFLICT;
182: 	catalog.CreateSchema(context, info.get());
183: 
184: 	// read the sequences
185: 	uint32_t seq_count = reader.Read<uint32_t>();
186: 	for (uint32_t i = 0; i < seq_count; i++) {
187: 		ReadSequence(context, reader);
188: 	}
189: 	// read the table count and recreate the tables
190: 	uint32_t table_count = reader.Read<uint32_t>();
191: 	for (uint32_t i = 0; i < table_count; i++) {
192: 		ReadTable(context, reader);
193: 	}
194: 	// now read the views
195: 	uint32_t view_count = reader.Read<uint32_t>();
196: 	for (uint32_t i = 0; i < view_count; i++) {
197: 		ReadView(context, reader);
198: 	}
199: 	// finally read the macro's
200: 	uint32_t macro_count = reader.Read<uint32_t>();
201: 	for (uint32_t i = 0; i < macro_count; i++) {
202: 		ReadMacro(context, reader);
203: 	}
204: }
205: 
206: //===--------------------------------------------------------------------===//
207: // Views
208: //===--------------------------------------------------------------------===//
209: void CheckpointManager::WriteView(ViewCatalogEntry &view) {
210: 	view.Serialize(*metadata_writer);
211: }
212: 
213: void CheckpointManager::ReadView(ClientContext &context, MetaBlockReader &reader) {
214: 	auto info = ViewCatalogEntry::Deserialize(reader);
215: 
216: 	auto &catalog = Catalog::GetCatalog(db);
217: 	catalog.CreateView(context, info.get());
218: }
219: 
220: //===--------------------------------------------------------------------===//
221: // Sequences
222: //===--------------------------------------------------------------------===//
223: void CheckpointManager::WriteSequence(SequenceCatalogEntry &seq) {
224: 	seq.Serialize(*metadata_writer);
225: }
226: 
227: void CheckpointManager::ReadSequence(ClientContext &context, MetaBlockReader &reader) {
228: 	auto info = SequenceCatalogEntry::Deserialize(reader);
229: 
230: 	auto &catalog = Catalog::GetCatalog(db);
231: 	catalog.CreateSequence(context, info.get());
232: }
233: 
234: //===--------------------------------------------------------------------===//
235: // Macro's
236: //===--------------------------------------------------------------------===//
237: void CheckpointManager::WriteMacro(MacroCatalogEntry &macro) {
238: 	macro.Serialize(*metadata_writer);
239: }
240: 
241: void CheckpointManager::ReadMacro(ClientContext &context, MetaBlockReader &reader) {
242: 	auto info = MacroCatalogEntry::Deserialize(reader);
243: 
244: 	auto &catalog = Catalog::GetCatalog(db);
245: 	catalog.CreateFunction(context, info.get());
246: }
247: 
248: //===--------------------------------------------------------------------===//
249: // Table Metadata
250: //===--------------------------------------------------------------------===//
251: void CheckpointManager::WriteTable(TableCatalogEntry &table) {
252: 	// write the table meta data
253: 	table.Serialize(*metadata_writer);
254: 	// now we need to write the table data
255: 	TableDataWriter writer(db, table, *tabledata_writer);
256: 	auto pointer = writer.WriteTableData();
257: 
258: 	//! write the block pointer for the table info
259: 	metadata_writer->Write<block_id_t>(pointer.block_id);
260: 	metadata_writer->Write<uint64_t>(pointer.offset);
261: }
262: 
263: void CheckpointManager::ReadTable(ClientContext &context, MetaBlockReader &reader) {
264: 	// deserialize the table meta data
265: 	auto info = TableCatalogEntry::Deserialize(reader);
266: 	// bind the info
267: 	auto binder = Binder::CreateBinder(context);
268: 	auto bound_info = binder->BindCreateTableInfo(move(info));
269: 
270: 	// now read the actual table data and place it into the create table info
271: 	auto block_id = reader.Read<block_id_t>();
272: 	auto offset = reader.Read<uint64_t>();
273: 	MetaBlockReader table_data_reader(db, block_id);
274: 	table_data_reader.offset = offset;
275: 	TableDataReader data_reader(table_data_reader, *bound_info);
276: 	data_reader.ReadTableData();
277: 
278: 	// finally create the table in the catalog
279: 	auto &catalog = Catalog::GetCatalog(db);
280: 	catalog.CreateTable(context, bound_info.get());
281: }
282: 
283: } // namespace duckdb
[end of src/storage/checkpoint_manager.cpp]
</code>
Here is an example of a patch file. It consists of changes to the code base. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files.
<patch>
--- a/file.cpp
+++ b/file.cpp
@@ -3,35 +3,44 @@
 #include <cstdlib>
 
 int euclidean(int a, int b) {
-    while (b) {
-        int temp = b;
-        b = a % b;
-        a = temp;
+    if (b == 0) {
+        return a;
     }
-    return a;
+    return euclidean(b, a % b);
 }
 
 std::vector<std::pair<int, int>> bresenham(int x0, int y0, int x1, int y1) {
     std::vector<std::pair<int, int>> points;
     int dx = abs(x1 - x0);
     int dy = abs(y1 - y0);
+    int x = x0, y = y0;
     int sx = (x0 < x1) ? 1 : -1;
     int sy = (y0 < y1) ? 1 : -1;
-    int err = dx - dy;
 
-    while (true) {
-        points.emplace_back(x0, y0);
-        if (x0 == x1 && y0 == y1) break;
-        int e2 = 2 * err;
-        if (e2 > -dy) {
+    if (dx > dy) {
+        int err = dx / 2;
+        while (x != x1) {
+            points.emplace_back(x, y);
             err -= dy;
-            x0 += sx;
+            if (err < 0) {
+                y += sy;
+                err += dx;
+            }
+            x += sx;
         }
-        if (e2 < dx) {
-            err += dx;
-            y0 += sy;
+    } else {
+        int err = dy / 2;
+        while (y != y1) {
+            points.emplace_back(x, y);
+            err -= dx;
+            if (err < 0) {
+                x += sx;
+                err += dy;
+            }
+            y += sy;
         }
     }
 
+    points.emplace_back(x, y);
     return points;
 }

</patch>

I need you to solve the provided issue by generating a single patch file that I can apply directly to this repository using git apply.
Please respond with a single patch file in the format shown above.
Make sure to only include the patch file contents so that the contents of your output can be copied into a patch file and applied directly

Respond below: