diff --git a/src/server/dragonfly_test.cc b/src/server/dragonfly_test.cc
index 276bcd87b992..83f8c3bdb375 100644
--- a/src/server/dragonfly_test.cc
+++ b/src/server/dragonfly_test.cc
@@ -26,8 +26,8 @@ ABSL_DECLARE_FLAG(float, mem_defrag_threshold);
 ABSL_DECLARE_FLAG(float, mem_defrag_waste_threshold);
 ABSL_DECLARE_FLAG(uint32_t, mem_defrag_check_sec_interval);
 ABSL_DECLARE_FLAG(std::vector<std::string>, rename_command);
-ABSL_DECLARE_FLAG(double, oom_deny_ratio);
 ABSL_DECLARE_FLAG(bool, lua_resp2_legacy_float);
+ABSL_DECLARE_FLAG(double, eviction_memory_budget_threshold);
 
 namespace dfly {
 
@@ -456,19 +456,22 @@ TEST_F(DflyEngineTest, OOM) {
 /// Reproduces the case where items with expiry data were evicted,
 /// and then written with the same key.
 TEST_F(DflyEngineTest, Bug207) {
-  max_memory_limit = 300000;
+  max_memory_limit = 300000 * 4;
 
+  // The threshold is set to 0.3 to trigger eviction earlier and prevent OOM.
   absl::FlagSaver fs;
-  absl::SetFlag(&FLAGS_oom_deny_ratio, 4);
-  ResetService();
+  absl::SetFlag(&FLAGS_eviction_memory_budget_threshold, 0.3);
 
   shard_set->TEST_EnableCacheMode();
 
+  /* The value should be large enough to avoid being inlined. Heartbeat evicts only objects for
+   * which HasAllocated() returns true. */
+  std::string value(1000, '.');
+
   ssize_t i = 0;
   RespExpr resp;
-  for (; i < 10000; ++i) {
-    resp = Run({"setex", StrCat("key", i), "30", "bar"});
-    // we evict some items because 5000 is too much when max_memory_limit is 300000.
+  for (; i < 1000; ++i) {
+    resp = Run({"setex", StrCat("key", i), "30", value});
     ASSERT_EQ(resp, "OK");
   }
 
@@ -489,10 +492,7 @@ TEST_F(DflyEngineTest, Bug207) {
 }
 
 TEST_F(DflyEngineTest, StickyEviction) {
-  max_memory_limit = 300000;
-  absl::FlagSaver fs;
-  absl::SetFlag(&FLAGS_oom_deny_ratio, 4);
-  ResetService();
+  max_memory_limit = 600000;  // 0.6mb
   shard_set->TEST_EnableCacheMode();
 
   string tmp_val(100, '.');
diff --git a/tests/dragonfly/generic_test.py b/tests/dragonfly/generic_test.py
index c4ceef3d6398..25da08a42fc2 100644
--- a/tests/dragonfly/generic_test.py
+++ b/tests/dragonfly/generic_test.py
@@ -148,14 +148,12 @@ async def test_reply_guard_oom(df_factory, df_seeder_factory):
 
 @pytest.mark.asyncio
 async def test_denyoom_commands(df_factory):
-    df_server = df_factory.create(
-        proactor_threads=1, maxmemory="256mb", oom_deny_commands="get", oom_deny_ratio=0.7
-    )
+    df_server = df_factory.create(proactor_threads=1, maxmemory="256mb", oom_deny_commands="get")
     df_server.start()
     client = df_server.client()
     await client.execute_command("DEBUG POPULATE 7000 size 44000")
 
-    min_deny = 250 * 1024 * 1024  # 250mb
+    min_deny = 256 * 1024 * 1024  # 256mb
     info = await client.info("memory")
     print(f'Used memory {info["used_memory"]}, rss {info["used_memory_rss"]}')
     assert info["used_memory"] > min_deny, "Weak testcase: too little used memory"
diff --git a/tests/dragonfly/memory_test.py b/tests/dragonfly/memory_test.py
index 1105f1d83408..438f5296ffb7 100644
--- a/tests/dragonfly/memory_test.py
+++ b/tests/dragonfly/memory_test.py
@@ -6,6 +6,31 @@
 from .instance import DflyInstance, DflyInstanceFactory
 
 
+async def calculate_estimated_connection_memory(
+    async_client: aioredis.Redis, df_server: DflyInstance
+):
+    memory_info = await async_client.info("memory")
+    already_used_rss_memory = memory_info["used_memory_rss"]
+
+    connections_number = 100
+    connections = []
+    for _ in range(connections_number):
+        conn = aioredis.Redis(port=df_server.port)
+        await conn.ping()
+        connections.append(conn)
+
+    await asyncio.sleep(1)  # Wait RSS update
+
+    memory_info = await async_client.info("memory")
+    estimated_connections_memory = memory_info["used_memory_rss"] - already_used_rss_memory
+
+    # Close test connection
+    for conn in connections:
+        await conn.close()
+
+    return estimated_connections_memory // connections_number
+
+
 @pytest.mark.opt_only
 @pytest.mark.parametrize(
     "type, keys, val_size, elements",
@@ -160,3 +185,108 @@ async def test_eval_with_oom(df_factory: DflyInstanceFactory):
     info = await client.info("memory")
     logging.debug(f'Used memory {info["used_memory"]}, rss {info["used_memory_rss"]}')
     assert rss_before_eval * 1.01 > info["used_memory_rss"]
+
+
+@pytest.mark.asyncio
+@dfly_args(
+    {
+        "proactor_threads": 1,
+        "cache_mode": "true",
+        "maxmemory": "256mb",
+        "rss_oom_deny_ratio": 0.5,
+        "max_eviction_per_heartbeat": 1000,
+    }
+)
+async def test_cache_eviction_with_rss_deny_oom(
+    async_client: aioredis.Redis,
+    df_server: DflyInstance,
+):
+    """
+    Test to verify that cache eviction is triggered even if used memory is small but rss memory is above limit
+    """
+
+    max_memory = 256 * 1024 * 1024  # 256 MB
+    rss_max_memory = int(max_memory * 0.5)  # 50% of max memory
+
+    data_fill_size = int(0.55 * rss_max_memory)  # 55% of rss_max_memory
+    rss_increase_size = int(0.55 * rss_max_memory)  # 55% of max rss_max_memory
+
+    key_size = 1024 * 5  # 5 kb
+    num_keys = data_fill_size // key_size
+
+    await asyncio.sleep(1)  # Wait for RSS update
+
+    estimated_connection_memory = await calculate_estimated_connection_memory(
+        async_client, df_server
+    )
+    num_connections = rss_increase_size // estimated_connection_memory
+
+    logging.info(
+        f"Estimated connection memory: {estimated_connection_memory}. Number of connections: {num_connections}."
+    )
+
+    # Fill data to 55% of rss max memory
+    await async_client.execute_command("DEBUG", "POPULATE", num_keys, "key", key_size)
+
+    await asyncio.sleep(1)  # Wait for RSS heartbeat update
+
+    # First test that eviction is not triggered without connection creation
+    stats_info = await async_client.info("stats")
+    assert stats_info["evicted_keys"] == 0, "No eviction should start yet."
+
+    # Test that used memory is less than 90% of max memory
+    memory_info = await async_client.info("memory")
+    assert (
+        memory_info["used_memory"] < max_memory * 0.9
+    ), "Used memory should be less than 90% of max memory."
+    assert (
+        memory_info["used_memory_rss"] < rss_max_memory * 0.9
+    ), "RSS memory should be less than 90% of rss max memory (max_memory * rss_oom_deny_ratio)."
+
+    # Disable heartbeat eviction
+    await async_client.execute_command("CONFIG SET enable_heartbeat_eviction false")
+
+    # Increase RSS memory by 55% of rss max memory
+    # We can simulate RSS increase by creating new connections
+    connections = []
+    for _ in range(num_connections):
+        conn = aioredis.Redis(port=df_server.port)
+        await conn.ping()
+        connections.append(conn)
+
+    await asyncio.sleep(1)
+
+    # Check that RSS memory is above rss limit
+    memory_info = await async_client.info("memory")
+    assert (
+        memory_info["used_memory_rss"] >= rss_max_memory * 0.9
+    ), "RSS memory should exceed 90% of the maximum RSS memory limit (max_memory * rss_oom_deny_ratio)."
+
+    # Enable heartbeat eviction
+    await async_client.execute_command("CONFIG SET enable_heartbeat_eviction true")
+
+    await asyncio.sleep(1)  # Wait for RSS heartbeat update
+    await async_client.execute_command("MEMORY DECOMMIT")
+    await asyncio.sleep(1)  # Wait for RSS update
+
+    # Get RSS memory after creating new connections
+    memory_info = await async_client.info("memory")
+    stats_info = await async_client.info("stats")
+
+    logging.info(f'Evicted keys number: {stats_info["evicted_keys"]}. Total keys: {num_keys}.')
+
+    assert (
+        memory_info["used_memory"] < data_fill_size
+    ), "Used memory should be less than initial fill size due to eviction."
+
+    assert (
+        memory_info["used_memory_rss"] < rss_max_memory * 0.9
+    ), "RSS memory should be less than 90% of rss max memory (max_memory * rss_oom_deny_ratio) after eviction."
+
+    # Check that eviction has occurred
+    assert (
+        stats_info["evicted_keys"] > 0
+    ), "Eviction should have occurred due to rss memory pressure."
+
+    for conn in connections:
+        await conn.close()
