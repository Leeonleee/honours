{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 3702,
  "instance_id": "dragonflydb__dragonfly-3702",
  "issue_numbers": [
    "3616"
  ],
  "base_commit": "abf3acec4ae2105ac5c8d33658c2561e85b71ede",
  "patch": "diff --git a/src/server/engine_shard.cc b/src/server/engine_shard.cc\nindex 6fb0bb6b01a6..1c23e9196392 100644\n--- a/src/server/engine_shard.cc\n+++ b/src/server/engine_shard.cc\n@@ -699,9 +699,6 @@ void EngineShard::RunPeriodic(std::chrono::milliseconds period_ms,\n                               std::function<void()> shard_handler) {\n   VLOG(1) << \"RunPeriodic with period \" << period_ms.count() << \"ms\";\n \n-  bool runs_global_periodic = (shard_id() == 0);  // Only shard 0 runs global periodic.\n-  unsigned global_count = 0;\n-  int64_t last_stats_time = time(nullptr);\n   int64_t last_heartbeat_ms = INT64_MAX;\n   int64_t last_handler_ms = 0;\n \n@@ -721,33 +718,6 @@ void EngineShard::RunPeriodic(std::chrono::milliseconds period_ms,\n       last_handler_ms = last_heartbeat_ms;\n       shard_handler();\n     }\n-\n-    if (runs_global_periodic) {\n-      ++global_count;\n-\n-      // Every 8 runs, update the global stats.\n-      if (global_count % 8 == 0) {\n-        DVLOG(2) << \"Global periodic\";\n-\n-        uint64_t mem_current = used_mem_current.load(std::memory_order_relaxed);\n-\n-        // Single writer, so no races.\n-        if (mem_current > used_mem_peak.load(memory_order_relaxed))\n-          used_mem_peak.store(mem_current, memory_order_relaxed);\n-\n-        int64_t cur_time = time(nullptr);\n-        if (cur_time != last_stats_time) {\n-          last_stats_time = cur_time;\n-          io::Result<io::StatusData> sdata_res = io::ReadStatusInfo();\n-          if (sdata_res) {\n-            size_t total_rss = sdata_res->vm_rss + sdata_res->hugetlb_pages;\n-            rss_mem_current.store(total_rss, memory_order_relaxed);\n-            if (rss_mem_peak.load(memory_order_relaxed) < total_rss)\n-              rss_mem_peak.store(total_rss, memory_order_relaxed);\n-          }\n-        }\n-      }\n-    }\n   }\n }\n \ndiff --git a/src/server/main_service.cc b/src/server/main_service.cc\nindex be0f002558c3..b391c29f37ce 100644\n--- a/src/server/main_service.cc\n+++ b/src/server/main_service.cc\n@@ -103,6 +103,11 @@ ABSL_FLAG(double, oom_deny_ratio, 1.1,\n           \"commands with flag denyoom will return OOM when the ratio between maxmemory and used \"\n           \"memory is above this value\");\n \n+ABSL_FLAG(double, rss_oom_deny_ratio, 1.25,\n+          \"When the ratio between maxmemory and RSS memory exceeds this value, commands marked as \"\n+          \"DENYOOM will fail with OOM error and new connections to non-admin port will be \"\n+          \"rejected. Negative value disables this feature.\");\n+\n ABSL_FLAG(size_t, serialization_max_chunk_size, 0,\n           \"Maximum size of a value that may be serialized at once during snapshotting or full \"\n           \"sync. Values bigger than this threshold will be serialized using streaming \"\n@@ -859,6 +864,16 @@ Service::Service(ProactorPool* pp)\n   engine_varz.emplace(\"engine\", [this] { return GetVarzStats(); });\n }\n \n+void SetOomDenyRatioOnAllThreads(double ratio) {\n+  auto cb = [ratio](unsigned, auto*) { ServerState::tlocal()->oom_deny_ratio = ratio; };\n+  shard_set->pool()->AwaitBrief(cb);\n+}\n+\n+void SetRssOomDenyRatioOnAllThreads(double ratio) {\n+  auto cb = [ratio](unsigned, auto*) { ServerState::tlocal()->rss_oom_deny_ratio = ratio; };\n+  shard_set->pool()->AwaitBrief(cb);\n+}\n+\n Service::~Service() {\n   delete shard_set;\n   shard_set = nullptr;\n@@ -884,7 +899,22 @@ void Service::Init(util::AcceptServer* acceptor, std::vector<facade::Listener*>\n   config_registry.RegisterMutable(\"masteruser\");\n   config_registry.RegisterMutable(\"max_eviction_per_heartbeat\");\n   config_registry.RegisterMutable(\"max_segment_to_consider\");\n-  config_registry.RegisterMutable(\"oom_deny_ratio\");\n+\n+  config_registry.RegisterMutable(\"oom_deny_ratio\", [](const absl::CommandLineFlag& flag) {\n+    auto res = flag.TryGet<double>();\n+    if (res.has_value()) {\n+      SetOomDenyRatioOnAllThreads(*res);\n+    }\n+    return res.has_value();\n+  });\n+\n+  config_registry.RegisterMutable(\"rss_oom_deny_ratio\", [](const absl::CommandLineFlag& flag) {\n+    auto res = flag.TryGet<double>();\n+    if (res.has_value()) {\n+      SetRssOomDenyRatioOnAllThreads(*res);\n+    }\n+    return res.has_value();\n+  });\n   config_registry.RegisterMutable(\"pipeline_squash\");\n   config_registry.RegisterMutable(\"pipeline_queue_limit\",\n                                   [pool = &pp_](const absl::CommandLineFlag& flag) {\n@@ -925,7 +955,12 @@ void Service::Init(util::AcceptServer* acceptor, std::vector<facade::Listener*>\n   }\n \n   // Initialize shard_set with a global callback running once in a while in the shard threads.\n-  shard_set->Init(shard_num, [this] { server_family_.GetDflyCmd()->BreakStalledFlowsInShard(); });\n+  shard_set->Init(shard_num, [this] {\n+    server_family_.GetDflyCmd()->BreakStalledFlowsInShard();\n+    server_family_.UpdateMemoryGlobalStats();\n+  });\n+  SetOomDenyRatioOnAllThreads(absl::GetFlag(FLAGS_oom_deny_ratio));\n+  SetRssOomDenyRatioOnAllThreads(absl::GetFlag(FLAGS_rss_oom_deny_ratio));\n \n   // Requires that shard_set will be initialized before because server_family_.Init might\n   // load the snapshot.\n@@ -1046,22 +1081,30 @@ static optional<ErrorReply> VerifyConnectionAclStatus(const CommandId* cid,\n   return nullopt;\n }\n \n-optional<ErrorReply> Service::VerifyCommandExecution(const CommandId* cid,\n-                                                     const ConnectionContext* cntx,\n-                                                     CmdArgList tail_args) {\n+bool ShouldDenyOnOOM(const CommandId* cid) {\n   ServerState& etl = *ServerState::tlocal();\n-\n   if ((cid->opt_mask() & CO::DENYOOM) && etl.is_master) {\n     uint64_t start_ns = absl::GetCurrentTimeNanos();\n+    auto memory_stats = etl.GetMemoryUsage(start_ns);\n \n-    uint64_t used_memory = etl.GetUsedMemory(start_ns);\n-    double oom_deny_ratio = GetFlag(FLAGS_oom_deny_ratio);\n-    if (used_memory > (max_memory_limit * oom_deny_ratio)) {\n-      DLOG(WARNING) << \"Out of memory, used \" << used_memory << \" vs limit \" << max_memory_limit;\n+    if (memory_stats.used_mem > (max_memory_limit * etl.oom_deny_ratio) ||\n+        (etl.rss_oom_deny_ratio > 0 &&\n+         memory_stats.rss_mem > (max_memory_limit * etl.rss_oom_deny_ratio))) {\n+      DLOG(WARNING) << \"Out of memory, used \" << memory_stats.used_mem << \" ,rss \"\n+                    << memory_stats.rss_mem << \" ,limit \" << max_memory_limit;\n       etl.stats.oom_error_cmd_cnt++;\n-      return facade::ErrorReply{kOutOfMemory};\n+      return true;\n     }\n   }\n+  return false;\n+}\n+\n+optional<ErrorReply> Service::VerifyCommandExecution(const CommandId* cid,\n+                                                     const ConnectionContext* cntx,\n+                                                     CmdArgList tail_args) {\n+  if (ShouldDenyOnOOM(cid)) {\n+    return facade::ErrorReply{kOutOfMemory};\n+  }\n \n   return VerifyConnectionAclStatus(cid, cntx, \"ACL rules changed between the MULTI and EXEC\",\n                                    tail_args);\n@@ -1383,7 +1426,9 @@ bool Service::InvokeCmd(const CommandId* cid, CmdArgList tail_args, ConnectionCo\n   }\n \n   std::string reason = cntx->reply_builder()->ConsumeLastError();\n+\n   if (!reason.empty()) {\n+    VLOG(2) << FailedCommandToString(cid->name(), tail_args, reason);\n     LOG_EVERY_T(WARNING, 1) << FailedCommandToString(cid->name(), tail_args, reason);\n   }\n \n@@ -2295,12 +2340,12 @@ void Service::Exec(CmdArgList args, ConnectionContext* cntx) {\n   }\n \n   if (scheduled) {\n-    VLOG(1) << \"Exec unlocking \" << exec_info.body.size() << \" commands\";\n+    VLOG(2) << \"Exec unlocking \" << exec_info.body.size() << \" commands\";\n     cntx->transaction->UnlockMulti();\n   }\n \n   cntx->cid = exec_cid_;\n-  VLOG(1) << \"Exec completed\";\n+  VLOG(2) << \"Exec completed\";\n }\n \n void Service::Publish(CmdArgList args, ConnectionContext* cntx) {\ndiff --git a/src/server/server_family.cc b/src/server/server_family.cc\nindex 8df5e1edf3e3..1fbb10baa937 100644\n--- a/src/server/server_family.cc\n+++ b/src/server/server_family.cc\n@@ -134,6 +134,8 @@ ABSL_DECLARE_FLAG(bool, tls);\n ABSL_DECLARE_FLAG(string, tls_ca_cert_file);\n ABSL_DECLARE_FLAG(string, tls_ca_cert_dir);\n ABSL_DECLARE_FLAG(int, replica_priority);\n+ABSL_DECLARE_FLAG(double, oom_deny_ratio);\n+ABSL_DECLARE_FLAG(double, rss_oom_deny_ratio);\n \n bool AbslParseFlag(std::string_view in, ReplicaOfFlag* flag, std::string* err) {\n #define RETURN_ON_ERROR(cond, m)                                           \\\n@@ -959,11 +961,6 @@ void ServerFamily::Shutdown() {\n   }\n \n   pb_task_->Await([this] {\n-    if (stats_caching_task_) {\n-      pb_task_->CancelPeriodic(stats_caching_task_);\n-      stats_caching_task_ = 0;\n-    }\n-\n     auto ec = journal_->Close();\n     LOG_IF(ERROR, ec) << \"Error closing journal \" << ec;\n \n@@ -978,6 +975,57 @@ void ServerFamily::Shutdown() {\n   });\n }\n \n+bool ServerFamily::HasPrivilegedInterface() {\n+  for (auto* listener : listeners_) {\n+    if (listener->IsPrivilegedInterface()) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+void ServerFamily::UpdateMemoryGlobalStats() {\n+  ShardId sid = EngineShard::tlocal()->shard_id();\n+  if (sid != 0) {  // This function is executed periodicaly on all shards. To ensure the logic\n+                   // bellow runs only on one shard we return is the shard is not 0.\n+    return;\n+  }\n+\n+  uint64_t mem_current = used_mem_current.load(std::memory_order_relaxed);\n+  if (mem_current > used_mem_peak.load(memory_order_relaxed)) {\n+    used_mem_peak.store(mem_current, memory_order_relaxed);\n+  }\n+\n+  io::Result<io::StatusData> sdata_res = io::ReadStatusInfo();\n+  if (sdata_res) {\n+    size_t total_rss = sdata_res->vm_rss + sdata_res->hugetlb_pages;\n+    rss_mem_current.store(total_rss, memory_order_relaxed);\n+    if (rss_mem_peak.load(memory_order_relaxed) < total_rss) {\n+      rss_mem_peak.store(total_rss, memory_order_relaxed);\n+    }\n+    double rss_oom_deny_ratio = ServerState::tlocal()->rss_oom_deny_ratio;\n+    if (rss_oom_deny_ratio > 0) {\n+      size_t memory_limit = max_memory_limit * rss_oom_deny_ratio;\n+      if (total_rss > memory_limit && accepting_connections_ && HasPrivilegedInterface()) {\n+        for (auto* listener : listeners_) {\n+          if (!listener->IsPrivilegedInterface()) {\n+            listener->socket()->proactor()->Await([listener]() { listener->pause_accepting(); });\n+          }\n+        }\n+        accepting_connections_ = false;\n+\n+      } else if (total_rss < memory_limit && !accepting_connections_) {\n+        for (auto* listener : listeners_) {\n+          if (!listener->IsPrivilegedInterface()) {\n+            listener->socket()->proactor()->Await([listener]() { listener->resume_accepting(); });\n+          }\n+        }\n+        accepting_connections_ = true;\n+      }\n+    }\n+  }\n+}\n+\n struct AggregateLoadResult {\n   AggregateError first_error;\n   std::atomic<size_t> keys_read;\n@@ -2278,7 +2326,7 @@ void ServerFamily::Info(CmdArgList args, ConnectionContext* cntx) {\n     append(\"garbage_collected\", m.events.garbage_collected);\n     append(\"bump_ups\", m.events.bumpups);\n     append(\"stash_unloaded\", m.events.stash_unloaded);\n-    append(\"oom_rejections\", m.events.insertion_rejections);\n+    append(\"oom_rejections\", m.events.insertion_rejections + m.coordinator_stats.oom_error_cmd_cnt);\n     append(\"traverse_ttl_sec\", m.traverse_ttl_per_sec);\n     append(\"delete_ttl_sec\", m.delete_ttl_per_sec);\n     append(\"keyspace_hits\", m.events.hits);\ndiff --git a/src/server/server_family.h b/src/server/server_family.h\nindex 52052237f3e8..b89fda512142 100644\n--- a/src/server/server_family.h\n+++ b/src/server/server_family.h\n@@ -260,7 +260,10 @@ class ServerFamily {\n   // Sets the server to replicate another instance. Does not flush the database beforehand!\n   void Replicate(std::string_view host, std::string_view port);\n \n+  void UpdateMemoryGlobalStats();\n+\n  private:\n+  bool HasPrivilegedInterface();\n   void JoinSnapshotSchedule();\n   void LoadFromSnapshot() ABSL_LOCKS_EXCLUDED(loading_stats_mu_);\n \n@@ -330,11 +333,11 @@ class ServerFamily {\n   util::fb2::Fiber snapshot_schedule_fb_;\n   std::optional<util::fb2::Future<GenericError>> load_result_;\n \n-  uint32_t stats_caching_task_ = 0;\n   Service& service_;\n \n   util::AcceptServer* acceptor_ = nullptr;\n   std::vector<facade::Listener*> listeners_;\n+  bool accepting_connections_ = true;\n   util::ProactorBase* pb_task_ = nullptr;\n \n   mutable util::fb2::Mutex replicaof_mu_, save_mu_;\ndiff --git a/src/server/server_state.cc b/src/server/server_state.cc\nindex 0bded3d31cea..d4dab12d2a40 100644\n--- a/src/server/server_state.cc\n+++ b/src/server/server_state.cc\n@@ -113,13 +113,14 @@ void ServerState::Destroy() {\n   state_ = nullptr;\n }\n \n-uint64_t ServerState::GetUsedMemory(uint64_t now_ns) {\n+ServerState::MemoryUsageStats ServerState::GetMemoryUsage(uint64_t now_ns) {\n   static constexpr uint64_t kCacheEveryNs = 1000;\n   if (now_ns > used_mem_last_update_ + kCacheEveryNs) {\n     used_mem_last_update_ = now_ns;\n-    used_mem_cached_ = used_mem_current.load(std::memory_order_relaxed);\n+    memory_stats_cached_.used_mem = used_mem_current.load(std::memory_order_relaxed);\n+    memory_stats_cached_.rss_mem = rss_mem_current.load(std::memory_order_relaxed);\n   }\n-  return used_mem_cached_;\n+  return memory_stats_cached_;\n }\n \n bool ServerState::AllowInlineScheduling() const {\ndiff --git a/src/server/server_state.h b/src/server/server_state.h\nindex ecb392e745c5..1c678c162014 100644\n--- a/src/server/server_state.h\n+++ b/src/server/server_state.h\n@@ -174,7 +174,11 @@ class ServerState {  // public struct - to allow initialization.\n     gstate_ = s;\n   }\n \n-  uint64_t GetUsedMemory(uint64_t now_ns);\n+  struct MemoryUsageStats {\n+    uint64_t used_mem = 0;\n+    uint64_t rss_mem = 0;\n+  };\n+  MemoryUsageStats GetMemoryUsage(uint64_t now_ns);\n \n   bool AllowInlineScheduling() const;\n \n@@ -283,6 +287,8 @@ class ServerState {  // public struct - to allow initialization.\n \n   // Exec descriptor frequency count for this thread.\n   absl::flat_hash_map<std::string, unsigned> exec_freq_count;\n+  double oom_deny_ratio;\n+  double rss_oom_deny_ratio;\n \n  private:\n   int64_t live_transactions_ = 0;\n@@ -311,8 +317,9 @@ class ServerState {  // public struct - to allow initialization.\n \n   absl::flat_hash_map<std::string, base::Histogram> call_latency_histos_;\n   uint32_t thread_index_ = 0;\n-  uint64_t used_mem_cached_ = 0;  // thread local cache of used_mem_current\n+\n   uint64_t used_mem_last_update_ = 0;\n+  MemoryUsageStats memory_stats_cached_;  // thread local cache of used and rss memory current\n \n   static __thread ServerState* state_;\n };\n",
  "test_patch": "diff --git a/src/server/dragonfly_test.cc b/src/server/dragonfly_test.cc\nindex df5827c527fe..ee61612c1754 100644\n--- a/src/server/dragonfly_test.cc\n+++ b/src/server/dragonfly_test.cc\n@@ -457,9 +457,12 @@ TEST_F(DflyEngineTest, OOM) {\n /// and then written with the same key.\n TEST_F(DflyEngineTest, Bug207) {\n   max_memory_limit = 300000;\n-  shard_set->TEST_EnableCacheMode();\n+\n   absl::FlagSaver fs;\n   absl::SetFlag(&FLAGS_oom_deny_ratio, 4);\n+  ResetService();\n+\n+  shard_set->TEST_EnableCacheMode();\n \n   ssize_t i = 0;\n   RespExpr resp;\n@@ -486,11 +489,11 @@ TEST_F(DflyEngineTest, Bug207) {\n }\n \n TEST_F(DflyEngineTest, StickyEviction) {\n-  shard_set->TEST_EnableCacheMode();\n+  max_memory_limit = 300000;\n   absl::FlagSaver fs;\n   absl::SetFlag(&FLAGS_oom_deny_ratio, 4);\n-\n-  max_memory_limit = 300000;\n+  ResetService();\n+  shard_set->TEST_EnableCacheMode();\n \n   string tmp_val(100, '.');\n \ndiff --git a/src/server/test_utils.cc b/src/server/test_utils.cc\nindex 8659ba409352..b24b4f7ef2c0 100644\n--- a/src/server/test_utils.cc\n+++ b/src/server/test_utils.cc\n@@ -28,6 +28,7 @@ extern \"C\" {\n using namespace std;\n \n ABSL_DECLARE_FLAG(string, dbfilename);\n+ABSL_DECLARE_FLAG(double, rss_oom_deny_ratio);\n ABSL_DECLARE_FLAG(uint32_t, num_shards);\n ABSL_FLAG(bool, force_epoll, false, \"If true, uses epoll api instead iouring to run tests\");\n ABSL_DECLARE_FLAG(size_t, acllog_max_len);\n@@ -152,6 +153,7 @@ BaseFamilyTest::~BaseFamilyTest() {\n void BaseFamilyTest::SetUpTestSuite() {\n   kInitSegmentLog = 1;\n \n+  absl::SetFlag(&FLAGS_rss_oom_deny_ratio, -1);\n   absl::SetFlag(&FLAGS_dbfilename, \"\");\n   init_zmalloc_threadlocal(mi_heap_get_backing());\n \ndiff --git a/tests/dragonfly/generic_test.py b/tests/dragonfly/generic_test.py\nindex 2de1cab1e5f9..5b95d23cd534 100644\n--- a/tests/dragonfly/generic_test.py\n+++ b/tests/dragonfly/generic_test.py\n@@ -125,11 +125,15 @@ async def test_restricted_commands(df_factory):\n @pytest.mark.asyncio\n async def test_reply_guard_oom(df_factory, df_seeder_factory):\n     master = df_factory.create(\n-        proactor_threads=1, cache_mode=\"true\", maxmemory=\"256mb\", enable_heartbeat_eviction=\"false\"\n+        proactor_threads=1,\n+        cache_mode=\"true\",\n+        maxmemory=\"256mb\",\n+        enable_heartbeat_eviction=\"false\",\n+        rss_oom_deny_ratio=2,\n     )\n     df_factory.start_all([master])\n     c_master = master.client()\n-    await c_master.execute_command(\"DEBUG POPULATE 6000 size 44000\")\n+    await c_master.execute_command(\"DEBUG POPULATE 6000 size 40000\")\n \n     seeder = df_seeder_factory.create(\n         port=master.port, keys=5000, val_size=1000, stop_on_failure=False\ndiff --git a/tests/dragonfly/memory_test.py b/tests/dragonfly/memory_test.py\nindex 060af26af150..f2f82587eb49 100644\n--- a/tests/dragonfly/memory_test.py\n+++ b/tests/dragonfly/memory_test.py\n@@ -2,6 +2,7 @@\n from redis import asyncio as aioredis\n from .utility import *\n import logging\n+from . import dfly_args\n \n \n @pytest.mark.opt_only\n@@ -47,3 +48,62 @@ async def test_rss_used_mem_gap(df_factory, type, keys, val_size, elements):\n         assert delta < max_unaccounted\n \n     await disconnect_clients(client)\n+\n+\n+@pytest.mark.asyncio\n+@dfly_args(\n+    {\n+        \"maxmemory\": \"512mb\",\n+        \"proactor_threads\": 2,\n+        \"rss_oom_deny_ratio\": 0.5,\n+    }\n+)\n+@pytest.mark.parametrize(\"admin_port\", [0, 1112])\n+async def test_rss_oom_ratio(df_factory, admin_port):\n+    \"\"\"\n+    Test dragonfly rejects denyoom commands and new connections when rss memory is above maxmemory*rss_oom_deny_ratio\n+    Test dragonfly does not rejects when rss memory goes below threshold\n+    \"\"\"\n+    df_server = df_factory.create(admin_port=admin_port)\n+    df_server.start()\n+\n+    client = aioredis.Redis(port=df_server.port)\n+    await client.execute_command(\"DEBUG POPULATE 10000 key 40000 RAND\")\n+\n+    await asyncio.sleep(1)  # Wait for another RSS heartbeat update in Dragonfly\n+\n+    port = df_server.admin_port if admin_port else df_server.port\n+    new_client = aioredis.Redis(port=port)\n+    await new_client.ping()\n+\n+    info = await new_client.info(\"memory\")\n+    logging.debug(f'Used memory {info[\"used_memory\"]}, rss {info[\"used_memory_rss\"]}')\n+\n+    reject_limit = 256 * 1024 * 1024  # 256mb\n+    assert info[\"used_memory_rss\"] > reject_limit\n+\n+    # get command from existing connection should not be rejected\n+    await client.execute_command(\"get x\")\n+\n+    # reject set due to oom\n+    with pytest.raises(redis.exceptions.ResponseError):\n+        await client.execute_command(\"set x y\")\n+\n+    if admin_port:\n+        # new client create should also fail if admin port was set\n+        client = aioredis.Redis(port=df_server.port)\n+        with pytest.raises(redis.exceptions.ConnectionError):\n+            await client.ping()\n+\n+    # flush to free memory\n+    await new_client.flushall()\n+\n+    await asyncio.sleep(2)  # Wait for another RSS heartbeat update in Dragonfly\n+\n+    info = await new_client.info(\"memory\")\n+    logging.debug(f'Used memory {info[\"used_memory\"]}, rss {info[\"used_memory_rss\"]}')\n+    assert info[\"used_memory_rss\"] < reject_limit\n+\n+    # new client create shoud not fail after memory usage decrease\n+    client = aioredis.Redis(port=df_server.port)\n+    await client.execute_command(\"set x y\")\ndiff --git a/tests/dragonfly/replication_test.py b/tests/dragonfly/replication_test.py\nindex b87979855a6f..e2d3264b6446 100644\n--- a/tests/dragonfly/replication_test.py\n+++ b/tests/dragonfly/replication_test.py\n@@ -2085,6 +2085,7 @@ async def test_policy_based_eviction_propagation(df_factory, df_seeder_factory):\n         maxmemory=\"512mb\",\n         logtostdout=\"true\",\n         enable_heartbeat_eviction=\"false\",\n+        rss_oom_deny_ratio=1.3,\n     )\n     replica = df_factory.create(proactor_threads=2)\n     df_factory.start_all([master, replica])\ndiff --git a/tests/dragonfly/snapshot_test.py b/tests/dragonfly/snapshot_test.py\nindex 017a4134c30d..e44dd29f7566 100644\n--- a/tests/dragonfly/snapshot_test.py\n+++ b/tests/dragonfly/snapshot_test.py\n@@ -567,6 +567,7 @@ async def test_tiered_entries_throttle(async_client: aioredis.Redis):\n         (\"LIST\"),\n     ],\n )\n+@pytest.mark.slow\n async def test_big_value_serialization_memory_limit(df_factory, query):\n     dbfilename = f\"dump_{tmp_file_name()}\"\n     instance = df_factory.create(dbfilename=dbfilename)\n",
  "problem_statement": "add additional guard like oom_deny_ratio but for rss \n1. And gradually deprecate oom_deny_ratio\r\n2. Also stop adding connections on the main port once we are low on memory (critical state).\n",
  "hints_text": "@ashotland  FYI",
  "created_at": "2024-09-12T13:03:31Z",
  "modified_files": [
    "src/server/engine_shard.cc",
    "src/server/main_service.cc",
    "src/server/server_family.cc",
    "src/server/server_family.h",
    "src/server/server_state.cc",
    "src/server/server_state.h"
  ],
  "modified_test_files": [
    "src/server/dragonfly_test.cc",
    "src/server/test_utils.cc",
    "tests/dragonfly/generic_test.py",
    "tests/dragonfly/memory_test.py",
    "tests/dragonfly/replication_test.py",
    "tests/dragonfly/snapshot_test.py"
  ]
}