{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 3779,
  "instance_id": "dragonflydb__dragonfly-3779",
  "issue_numbers": [
    "3679"
  ],
  "base_commit": "9aadc0cd2b4bf6e49e935808c51083811c5c01e4",
  "patch": "diff --git a/src/server/server_family.cc b/src/server/server_family.cc\nindex 58a6fbce36fd..e01cd2a9df95 100644\n--- a/src/server/server_family.cc\n+++ b/src/server/server_family.cc\n@@ -2839,6 +2839,13 @@ void ServerFamily::ReplTakeOver(CmdArgList args, ConnectionContext* cntx) {\n }\n \n void ServerFamily::ReplConf(CmdArgList args, ConnectionContext* cntx) {\n+  {\n+    util::fb2::LockGuard lk(replicaof_mu_);\n+    if (!ServerState::tlocal()->is_master) {\n+      return cntx->SendError(\"Replicating a replica is unsupported\");\n+    }\n+  }\n+\n   if (args.size() % 2 == 1)\n     goto err;\n   for (unsigned i = 0; i < args.size(); i += 2) {\n",
  "test_patch": "diff --git a/tests/dragonfly/replication_test.py b/tests/dragonfly/replication_test.py\nindex 95cf423974ef..dfc63bcad46c 100644\n--- a/tests/dragonfly/replication_test.py\n+++ b/tests/dragonfly/replication_test.py\n@@ -2663,3 +2663,25 @@ async def check_master_status():\n     assert await seeder.compare(capture, port=master.port)\n \n     await disconnect_clients(c_master, c_replica)\n+\n+\n+@pytest.mark.asyncio\n+async def test_replica_of_replica(df_factory):\n+    # Can't connect a replica to a replica, but OK to connect 2 replicas to the same master\n+    master = df_factory.create(proactor_threads=2)\n+    replica = df_factory.create(proactor_threads=2)\n+    replica2 = df_factory.create(proactor_threads=2)\n+\n+    df_factory.start_all([master, replica, replica2])\n+\n+    c_replica = replica.client()\n+    c_replica2 = replica2.client()\n+\n+    assert await c_replica.execute_command(f\"REPLICAOF localhost {master.port}\") == \"OK\"\n+\n+    with pytest.raises(redis.exceptions.ResponseError):\n+        await c_replica2.execute_command(f\"REPLICAOF localhost {replica.port}\")\n+\n+    assert await c_replica2.execute_command(f\"REPLICAOF localhost {master.port}\") == \"OK\"\n+\n+    await disconnect_clients(c_replica, c_replica2)\n",
  "problem_statement": "Replica hangs in full sync\n**Describe the bug**\r\nWe sometimes see replicas getting stuck in full sync as part of our test suite\r\n\r\nSuch as the database items graphs show:\r\n![image](https://github.com/user-attachments/assets/4b02739b-fce0-4d73-ae32-ca6f71dbeba6)\r\n\r\nWhere the replica gets most of the keys but they just hangs without completing the sync\r\n\r\nThe replica logs show:\r\n```\r\nI20240909 12:02:25.220778  1843 replica.cc:566] Started full sync with 10.0.43.58:9999\r\nI20240909 12:04:46.989950  1843 rdb_load.cc:2050] Read RDB_OPCODE_FULLSYNC_END\r\nI20240909 12:06:08.022770  1842 rdb_load.cc:2050] Read RDB_OPCODE_FULLSYNC_END\r\nI20240909 12:06:23.528414  1845 rdb_load.cc:2050] Read RDB_OPCODE_FULLSYNC_END\r\nI20240909 12:06:40.811838  1841 rdb_load.cc:2050] Read RDB_OPCODE_FULLSYNC_END\r\nI20240909 12:07:00.670682  1847 rdb_load.cc:2050] Read RDB_OPCODE_FULLSYNC_END\r\n\r\n(5 minutes pass before we delete the datastore and replica disconnects)\r\n\r\nI20240909 12:11:54.027276  1844 rdb_load.cc:1141] Error while calling src_->Read(mb)\r\nI20240909 12:11:54.027280  1843 rdb_load.cc:2205] Error reading from source: system:103 1 bytes\r\nI20240909 12:11:54.027284  1845 rdb_load.cc:2205] Error reading from source: system:103 1 bytes\r\nI20240909 12:11:54.027343  1844 rdb_load.cc:2549] ReadObj error system:103 for key test:61158666\r\nI20240909 12:11:54.027280  1840 rdb_load.cc:1160] Error while calling src_->ReadAtLeast(mb, size)\r\nI20240909 12:11:54.027284  1846 rdb_load.cc:1141] Error while calling src_->Read(mb)\r\nI20240909 12:11:54.027350  1843 rdb_load.cc:1999] Error while calling FetchType()\r\nI20240909 12:11:54.027383  1840 rdb_load.cc:2549] ReadObj error system:103 for key test:23694841\r\nI20240909 12:11:54.027402  1842 rdb_load.cc:2205] Error reading from source: system:103 1 bytes\r\nI20240909 12:11:54.027402  1846 rdb_load.cc:2549] ReadObj error system:103 for key test:72764920\r\nI20240909 12:11:54.027292  1841 rdb_load.cc:2205] Error reading from source: system:103 1 bytes\r\nI20240909 12:11:54.027352  1845 rdb_load.cc:1999] Error while calling FetchType()\r\nI20240909 12:11:54.027402  1847 rdb_load.cc:2205] Error reading from source: system:103 1 bytes\r\nI20240909 12:11:54.027453  1842 rdb_load.cc:1999] Error while calling FetchType()\r\nI20240909 12:11:54.027494  1847 rdb_load.cc:1999] Error while calling FetchType()\r\nI20240909 12:11:54.027479  1841 rdb_load.cc:1999] Error while calling FetchType()\r\nW20240909 12:11:54.027691  1843 replica.cc:243] Error syncing with 10.0.43.58:9999 system:103 Software caused connection abort\r\n```\r\n\r\nSo it seems to get all the keys from the full sync but never transitions to stable sync?\r\n\r\n(I can send full datastore logs over if needed)\r\n\r\n**To Reproduce**\r\nI don't have a reliable way to reproduce\r\n\r\nThis sometimes happens in our test case, where we populate a datastore with two replicas with 75m keys (~75GB), then kill the master (`SIGKILL`) to one of the replicas is promoted to master and the other becomes a replica of the new master\r\n\r\nThe new replica connects to the new master, but then hangs as described above\r\n\r\nIt probably isn't a lot to go on, but we can enable any logs you suggest if it helps debug the issue\r\n\r\n**Environment (please complete the following information):**\r\n- Ubuntu AWS x2gd.2xlarge\r\n- Dragonfly v1.22.0\n",
  "hints_text": "how do you know it hangs? maybe it's master that hangs?\r\nin any case, is it possible to call uncoditionally \"info all\" on both master and replica before deleting the datastore?\r\nand then of course printing both responses into the test logs.\n> how do you know it hangs? maybe it's master that hangs?\r\n\r\nI don't know - just it never completes the sync\r\n\r\n> in any case, is it possible to call uncoditionally \"info all\" on both master and replica before deleting the datastore?\r\n\r\nYep sure will add (Edit: Actually it's only the 'nightly' suite that fails which only runs once a day, so I've just updated to not delete the datastore if it fails to we can inspect before cleaning up manually)\nhow do you check if sync was completed? based on the \"info\" command?\nyeah, once the replica is connected to the expected master and 'sync in progress' is false\nThis happened again (twice actually) and captured `INFO` this time\r\n[info.zip](https://github.com/user-attachments/files/17062289/info.zip)\r\n\r\nReplica shows sync in progress for over a day:\r\n```\r\n# Replication\r\nrole:replica\r\nmaster_host:10.0.37.98\r\nmaster_port:9999\r\nmaster_link_status:up\r\nmaster_last_io_seconds_ago:98035\r\nmaster_sync_in_progress:1\r\nmaster_replid:8040121f00740ce4f57f695be5a82ce557cd56e4\r\nslave_priority:100\r\nslave_read_only:1\r\n```\r\n\r\nThe datastore replica was stuck in this state for hours, again the latest replica logs shows:\r\n```\r\nI20240918 12:30:41.798735  1800 replica.cc:566] Started full sync with 10.0.37.98:9999\r\nI20240918 12:34:24.742923  1799 rdb_load.cc:2050] Read RDB_OPCODE_FULLSYNC_END\r\nI20240918 12:34:27.340220  1800 rdb_load.cc:2050] Read RDB_OPCODE_FULLSYNC_END\r\nI20240918 12:34:32.594964  1801 rdb_load.cc:2050] Read RDB_OPCODE_FULLSYNC_END\r\nI20240918 12:34:38.525820  1795 rdb_load.cc:2050] Read RDB_OPCODE_FULLSYNC_END\r\nI20240918 12:35:11.956713  1796 rdb_load.cc:2050] Read RDB_OPCODE_FULLSYNC_END\r\n```\nMaster shows 2 replicas.\r\nrole:master\r\nconnected_slaves:2\r\nslave0:ip=10.0.34.241,port=6385,state=full_sync,lag=0\r\nslave1:ip=10.0.41.27,port=6385,state=stable_sync,lag=0\r\nmaster_replid:8040121f00740ce4f57f695be5a82ce557cd56e4\r\n\r\nDo you happen to know which one got stuck?\r\n\r\nOn Thu, Sep 19, 2024 at 6:47\u202fPM Andy Dunstall ***@***.***>\r\nwrote:\r\n\r\n> This happened again and captured INFO this time\r\n> info.zip <https://github.com/user-attachments/files/17062289/info.zip>\r\n>\r\n> The datastore replica was stuck in this state for hours, again the latest\r\n> replica logs shows:\r\n>\r\n> I20240918 12:30:41.798735  1800 replica.cc:566] Started full sync with 10.0.37.98:9999\r\n> I20240918 12:34:24.742923  1799 rdb_load.cc:2050] Read RDB_OPCODE_FULLSYNC_END\r\n> I20240918 12:34:27.340220  1800 rdb_load.cc:2050] Read RDB_OPCODE_FULLSYNC_END\r\n> I20240918 12:34:32.594964  1801 rdb_load.cc:2050] Read RDB_OPCODE_FULLSYNC_END\r\n> I20240918 12:34:38.525820  1795 rdb_load.cc:2050] Read RDB_OPCODE_FULLSYNC_END\r\n> I20240918 12:35:11.956713  1796 rdb_load.cc:2050] Read RDB_OPCODE_FULLSYNC_END\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/dragonflydb/dragonfly/issues/3679#issuecomment-2361388418>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AA4BFCCAT7VCFPNRSRH3PBLZXLW2DAVCNFSM6AAAAABN4SAWHGVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGNRRGM4DQNBRHA>\r\n> .\r\n> You are receiving this because you commented.Message ID:\r\n> ***@***.***>\r\n>\r\n\r\n\r\n-- \r\nRoman Gershman\r\nCTO\r\n---\r\n*www.dragonflydb.io <http://www.dragonflydb.io>*\r\n\n> Do you happen to know which one got stuck?\r\n\r\n10.0.34.241\r\n\r\nIt was in staging so we have full instance logs, metrics and state etc if it's useful\nyeah, they are useful. Please attach here.\nLooking through control plane logs to get the exact sequence of events (as this only happens in one particular test).\r\n\r\nFocusing on datastore dst_esfncx612, it starts with nodes:\r\n* node_a3wu4qd9x (A) (10.0.32.212)\r\n* node_va2ek40gc (B) (10.0.45.89)\r\n\r\nThen is updated to updated to:\r\n* node_19wtbsfj5 (C) (10.0.43.24)\r\n* node_b8w20be2k (D) (10.0.43.59)\r\n\r\nWith steps:\r\n1. Datastore created\r\n2. Nodes A and B are ready, where A is master a B is a replica of A\r\n3. Datastore populated with 75GB\r\n4. Datastore updated which provisions C and D, where only C is configured as a replica of A (D waits for C to sync before also replicating)\r\n5. Node A (the master) crashes (manually killed with SIGKILL to test datastore recovery during updates)\r\n6. Node C is reconfigured as a replica of B\r\n7. Node B is reconfigured as a master (note in this case node C is configured to replicate B before B is a master, not sure if that matters?)\r\n8. Node A recovers as a replica of node B\r\n\r\nThen node A syncs with B, but node C hangs.\r\n\r\nTherefore\r\n* Node C is configured as a replica of B, before B is configured as a master\r\n* Node B has two parallel full syncs from both A and C (A syncs but C hangs)\r\n\r\nAre both of those cases ok?\r\n\r\nRather than upload full logs and metrics here (which contains internal info), probably easiest to download with dfcloud? Quickly comparing logs of successful cases vs failed cases, the master (node B above) always seems to log `rdb_save.cc:1271] Error writing to sink Input/output error` in the failed case (checked the 3 recent error cases)\nWe do not support replicating a replica (Node C is configured as a replica of B, before B is configured as a master).\r\nI believe that the change in dragonfly should be that we will reply with error when running replicaof on host that is not master\n> We do not support replicating a replica\r\n\r\nAh ok thanks - will update control plane (FWIW the test often still succeed following the above steps, i.e. replicating a replica)\nHi @andydunstall \r\nDo you think it's safe to close this issue for now, and reopen it should it reoccur?\r\nOr is there anything still pending that I've missed?\n@chakaz  I think we should reject transitive replication from replica - if we do not support it.\n> Do you think it's safe to close this issue for now, and reopen it should it reoccur?\r\n\r\nYep sure I've patched control plane\n> Hi @andydunstall Do you think it's safe to close this issue for now, and reopen it should it reoccur? Or is there anything still pending that I've missed?\r\n\r\n@chakaz please see my comment to Andy above\nYes, I'm on it!",
  "created_at": "2024-09-24T11:23:15Z",
  "modified_files": [
    "src/server/server_family.cc"
  ],
  "modified_test_files": [
    "tests/dragonfly/replication_test.py"
  ]
}