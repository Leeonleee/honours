{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4630,
  "instance_id": "dragonflydb__dragonfly-4630",
  "issue_numbers": [
    "4628"
  ],
  "base_commit": "7e47cfc5ca49cdb5781b40f593c8dfb304482fdb",
  "patch": "diff --git a/src/server/debugcmd.cc b/src/server/debugcmd.cc\nindex aa2b6b9c1693..f5ee1ff01f77 100644\n--- a/src/server/debugcmd.cc\n+++ b/src/server/debugcmd.cc\n@@ -510,7 +510,7 @@ void DebugCmd::Run(CmdArgList args, facade::SinkReplyBuilder* builder) {\n         \"TX\",\n         \"    Performs transaction analysis per shard.\",\n         \"TRAFFIC <path> | [STOP]\",\n-        \"    Starts traffic logging to the specified path. If path is not specified,\"\n+        \"    Starts traffic logging to the specified path. If path is not specified,\",\n         \"    traffic logging is stopped.\",\n         \"RECVSIZE [<tid> | ENABLE | DISABLE]\",\n         \"    Prints the histogram of the received request sizes on the given thread\",\ndiff --git a/src/server/server_family.cc b/src/server/server_family.cc\nindex 3dbd43af4d05..32a95194ede0 100644\n--- a/src/server/server_family.cc\n+++ b/src/server/server_family.cc\n@@ -361,35 +361,6 @@ void ClientList(CmdArgList args, absl::Span<facade::Listener*> listeners, SinkRe\n   return rb->SendVerbatimString(result);\n }\n \n-void ClientPauseCmd(CmdArgList args, vector<facade::Listener*> listeners, SinkReplyBuilder* builder,\n-                    ConnectionContext* cntx) {\n-  CmdArgParser parser(args);\n-\n-  auto timeout = parser.Next<uint64_t>();\n-  ClientPause pause_state = ClientPause::ALL;\n-  if (parser.HasNext()) {\n-    pause_state = parser.MapNext(\"WRITE\", ClientPause::WRITE, \"ALL\", ClientPause::ALL);\n-  }\n-  if (auto err = parser.Error(); err) {\n-    return builder->SendError(err->MakeReply());\n-  }\n-\n-  const auto timeout_ms = timeout * 1ms;\n-  auto is_pause_in_progress = [end_time = chrono::steady_clock::now() + timeout_ms] {\n-    return ServerState::tlocal()->gstate() != GlobalState::SHUTTING_DOWN &&\n-           chrono::steady_clock::now() < end_time;\n-  };\n-\n-  if (auto pause_fb_opt =\n-          Pause(listeners, cntx->ns, cntx->conn(), pause_state, std::move(is_pause_in_progress));\n-      pause_fb_opt) {\n-    pause_fb_opt->Detach();\n-    builder->SendOk();\n-  } else {\n-    builder->SendError(\"Failed to pause all running clients\");\n-  }\n-}\n-\n void ClientTracking(CmdArgList args, SinkReplyBuilder* builder, ConnectionContext* cntx) {\n   auto* rb = static_cast<RedisReplyBuilder*>(builder);\n   if (!rb->IsResp3())\n@@ -728,7 +699,8 @@ void SlowLogGet(dfly::CmdArgList args, std::string_view sub_cmd, util::ProactorP\n \n std::optional<fb2::Fiber> Pause(std::vector<facade::Listener*> listeners, Namespace* ns,\n                                 facade::Connection* conn, ClientPause pause_state,\n-                                std::function<bool()> is_pause_in_progress) {\n+                                std::function<bool()> is_pause_in_progress,\n+                                std::function<void()> maybe_cleanup) {\n   // Track connections and set pause state to be able to wait untill all running transactions read\n   // the new pause state. Exlude already paused commands from the busy count. Exlude tracking\n   // blocked connections because: a) If the connection is blocked it is puased. b) We read pause\n@@ -758,23 +730,28 @@ std::optional<fb2::Fiber> Pause(std::vector<facade::Listener*> listeners, Namesp\n   shard_set->RunBriefInParallel(\n       [ns](EngineShard* shard) { ns->GetDbSlice(shard->shard_id()).SetExpireAllowed(false); });\n \n-  return fb2::Fiber(\"client_pause\", [is_pause_in_progress, pause_state, ns]() mutable {\n-    // On server shutdown we sleep 10ms to make sure all running task finish, therefore 10ms steps\n-    // ensure this fiber will not left hanging .\n-    constexpr auto step = 10ms;\n-    while (is_pause_in_progress()) {\n-      ThisFiber::SleepFor(step);\n-    }\n-\n-    ServerState& etl = *ServerState::tlocal();\n-    if (etl.gstate() != GlobalState::SHUTTING_DOWN) {\n-      shard_set->pool()->AwaitFiberOnAll([pause_state](util::ProactorBase* pb) {\n-        ServerState::tlocal()->SetPauseState(pause_state, false);\n-      });\n-      shard_set->RunBriefInParallel(\n-          [ns](EngineShard* shard) { ns->GetDbSlice(shard->shard_id()).SetExpireAllowed(true); });\n-    }\n-  });\n+  return fb2::Fiber(\"client_pause\",\n+                    [is_pause_in_progress, pause_state, ns, maybe_cleanup]() mutable {\n+                      // On server shutdown we sleep 10ms to make sure all running task finish,\n+                      // therefore 10ms steps ensure this fiber will not left hanging .\n+                      constexpr auto step = 10ms;\n+                      while (is_pause_in_progress()) {\n+                        ThisFiber::SleepFor(step);\n+                      }\n+\n+                      ServerState& etl = *ServerState::tlocal();\n+                      if (etl.gstate() != GlobalState::SHUTTING_DOWN) {\n+                        shard_set->pool()->AwaitFiberOnAll([pause_state](util::ProactorBase* pb) {\n+                          ServerState::tlocal()->SetPauseState(pause_state, false);\n+                        });\n+                        shard_set->RunBriefInParallel([ns](EngineShard* shard) {\n+                          ns->GetDbSlice(shard->shard_id()).SetExpireAllowed(true);\n+                        });\n+                      }\n+                      if (maybe_cleanup) {\n+                        maybe_cleanup();\n+                      }\n+                    });\n }\n \n ServerFamily::ServerFamily(Service* service) : service_(*service) {\n@@ -984,6 +961,8 @@ void ServerFamily::Shutdown() {\n     });\n   }\n \n+  client_pause_ec_.await([this] { return active_pauses_.load() == 0; });\n+\n   pb_task_->Await([this] {\n     auto ec = journal_->Close();\n     LOG_IF(ERROR, ec) << \"Error closing journal \" << ec;\n@@ -1882,6 +1861,54 @@ void ServerFamily::Auth(CmdArgList args, const CommandContext& cmd_cntx) {\n   }\n }\n \n+void ServerFamily::ClientUnPauseCmd(CmdArgList args, SinkReplyBuilder* builder) {\n+  if (!args.empty()) {\n+    builder->SendError(facade::kSyntaxErr);\n+    return;\n+  }\n+  is_c_pause_in_progress_.store(false, std::memory_order_relaxed);\n+  builder->SendOk();\n+}\n+\n+void ClientHelp(SinkReplyBuilder* builder) {\n+  string_view help_arr[] = {\n+      \"CLIENT <subcommand> [<arg> [value] [opt] ...]. Subcommands are:\",\n+      \"CACHING (YES|NO)\",\n+      \"    Enable/disable tracking of the keys for next command in OPTIN/OPTOUT modes.\",\n+      \"GETNAME\",\n+      \"    Return the name of the current connection.\",\n+      \"ID\",\n+      \"    Return the ID of the current connection.\",\n+      \"KILL <ip:port>\",\n+      \"    Kill connection made from <ip:port>.\",\n+      \"KILL <option> <value> [<option> <value> [...]]\",\n+      \"    Kill connections. Options are:\",\n+      \"    * ADDR (<ip:port>|<unixsocket>:0)\",\n+      \"      Kill connections made from the specified address\",\n+      \"    * LADDR (<ip:port>|<unixsocket>:0)\",\n+      \"      Kill connections made to specified local address\",\n+      \"    * ID <client-id>\",\n+      \"      Kill connections by client id.\",\n+      \"LIST\",\n+      \"    Return information about client connections.\",\n+      \"UNPAUSE\",\n+      \"    Stop the current client pause, resuming traffic.\",\n+      \"PAUSE <timeout> [WRITE|ALL]\",\n+      \"    Suspend all, or just write, clients for <timeout> milliseconds.\",\n+      \"SETNAME <name>\",\n+      \"    Assign the name <name> to the current connection.\",\n+      \"SETINFO <option> <value>\",\n+      \"Set client meta attr. Options are:\",\n+      \"    * LIB-NAME: the client lib name.\",\n+      \"    * LIB-VER: the client lib version.\",\n+      \"TRACKING (ON|OFF) [OPTIN] [OPTOUT] [NOLOOP]\",\n+      \"    Control server assisted client side caching.\",\n+      \"HELP\",\n+      \"    Print this help.\"};\n+  auto* rb = static_cast<RedisReplyBuilder*>(builder);\n+  return rb->SendSimpleStrArr(help_arr);\n+}\n+\n void ServerFamily::Client(CmdArgList args, const CommandContext& cmd_cntx) {\n   string sub_cmd = absl::AsciiStrToUpper(ArgS(args, 0));\n   CmdArgList sub_args = args.subspan(1);\n@@ -1895,7 +1922,9 @@ void ServerFamily::Client(CmdArgList args, const CommandContext& cmd_cntx) {\n   } else if (sub_cmd == \"LIST\") {\n     return ClientList(sub_args, absl::MakeSpan(listeners_), builder, cntx);\n   } else if (sub_cmd == \"PAUSE\") {\n-    return ClientPauseCmd(sub_args, GetNonPriviligedListeners(), builder, cntx);\n+    return ClientPauseCmd(sub_args, builder, cntx);\n+  } else if (sub_cmd == \"UNPAUSE\") {\n+    return ClientUnPauseCmd(sub_args, builder);\n   } else if (sub_cmd == \"TRACKING\") {\n     return ClientTracking(sub_args, builder, cntx);\n   } else if (sub_cmd == \"KILL\") {\n@@ -1906,6 +1935,8 @@ void ServerFamily::Client(CmdArgList args, const CommandContext& cmd_cntx) {\n     return ClientSetInfo(sub_args, builder, cntx);\n   } else if (sub_cmd == \"ID\") {\n     return ClientId(sub_args, builder, cntx);\n+  } else if (sub_cmd == \"HELP\") {\n+    return ClientHelp(builder);\n   }\n \n   LOG_FIRST_N(ERROR, 10) << \"Subcommand \" << sub_cmd << \" not supported\";\n@@ -3179,6 +3210,43 @@ void ServerFamily::Module(CmdArgList args, const CommandContext& cmd_cntx) {\n   rb->SendLong(20'000);  // we target v2\n }\n \n+void ServerFamily::ClientPauseCmd(CmdArgList args, SinkReplyBuilder* builder,\n+                                  ConnectionContext* cntx) {\n+  CmdArgParser parser(args);\n+  auto listeners = GetNonPriviligedListeners();\n+\n+  auto timeout = parser.Next<uint64_t>();\n+  ClientPause pause_state = ClientPause::ALL;\n+  if (parser.HasNext()) {\n+    pause_state = parser.MapNext(\"WRITE\", ClientPause::WRITE, \"ALL\", ClientPause::ALL);\n+  }\n+  if (auto err = parser.Error(); err) {\n+    return builder->SendError(err->MakeReply());\n+  }\n+\n+  const auto timeout_ms = timeout * 1ms;\n+  auto is_pause_in_progress = [this, end_time = chrono::steady_clock::now() + timeout_ms] {\n+    return ServerState::tlocal()->gstate() != GlobalState::SHUTTING_DOWN &&\n+           chrono::steady_clock::now() < end_time && is_c_pause_in_progress_.load();\n+  };\n+\n+  auto cleanup = [this] {\n+    active_pauses_.fetch_sub(1);\n+    client_pause_ec_.notify();\n+  };\n+\n+  if (auto pause_fb_opt = Pause(listeners, cntx->ns, cntx->conn(), pause_state,\n+                                std::move(is_pause_in_progress), cleanup);\n+      pause_fb_opt) {\n+    is_c_pause_in_progress_.store(true);\n+    active_pauses_.fetch_add(1);\n+    pause_fb_opt->Detach();\n+    builder->SendOk();\n+  } else {\n+    builder->SendError(\"Failed to pause all running clients\");\n+  }\n+}\n+\n #define HFUNC(x) SetHandler(HandlerFunc(this, &ServerFamily::x))\n \n namespace acl {\ndiff --git a/src/server/server_family.h b/src/server/server_family.h\nindex a2b7cf7f82fc..e63d4dbe7fec 100644\n--- a/src/server/server_family.h\n+++ b/src/server/server_family.h\n@@ -324,6 +324,9 @@ class ServerFamily {\n \n   static bool DoAuth(ConnectionContext* cntx, std::string_view username, std::string_view password);\n \n+  void ClientPauseCmd(CmdArgList args, SinkReplyBuilder* builder, ConnectionContext* cntx);\n+  void ClientUnPauseCmd(CmdArgList args, SinkReplyBuilder* builder);\n+\n   util::fb2::Fiber snapshot_schedule_fb_;\n   util::fb2::Fiber load_fiber_;\n \n@@ -358,6 +361,12 @@ class ServerFamily {\n   std::unique_ptr<util::fb2::FiberQueueThreadPool> fq_threadpool_;\n   std::shared_ptr<detail::SnapshotStorage> snapshot_storage_;\n \n+  std::atomic<bool> is_c_pause_in_progress_ = false;\n+  // We need this because if dragonfly shuts down during pause, ServerState will destruct\n+  // before the dettached fiber Pause() causing a seg fault.\n+  std::atomic<size_t> active_pauses_ = 0;\n+  util::fb2::EventCount client_pause_ec_;\n+\n   // protected by save_mu_\n   util::fb2::Fiber bg_save_fb_;\n \n@@ -371,6 +380,7 @@ class ServerFamily {\n // Reusable CLIENT PAUSE implementation that blocks while polling is_pause_in_progress\n std::optional<util::fb2::Fiber> Pause(std::vector<facade::Listener*> listeners, Namespace* ns,\n                                       facade::Connection* conn, ClientPause pause_state,\n-                                      std::function<bool()> is_pause_in_progress);\n+                                      std::function<bool()> is_pause_in_progress,\n+                                      std::function<void()> maybe_cleanup = {});\n \n }  // namespace dfly\n",
  "test_patch": "diff --git a/tests/dragonfly/connection_test.py b/tests/dragonfly/connection_test.py\nindex a239e384e32c..f53d913f1e79 100755\n--- a/tests/dragonfly/connection_test.py\n+++ b/tests/dragonfly/connection_test.py\n@@ -1160,3 +1160,55 @@ async def push_pipeline(bad_actor_client, size=1):\n             info = await good_client.info()\n \n     assert info[\"dispatch_queue_bytes\"] == 0\n+\n+\n+async def test_client_unpause(df_factory):\n+    server = df_factory.create()\n+    server.start()\n+\n+    async_client = server.client()\n+    await async_client.client_pause(3000, all=False)\n+\n+    async def set_foo():\n+        client = server.client()\n+        async with async_timeout.timeout(2):\n+            await client.execute_command(\"SET\", \"foo\", \"bar\")\n+\n+    p1 = asyncio.create_task(set_foo())\n+\n+    await asyncio.sleep(0.5)\n+    assert not p1.done()\n+\n+    async with async_timeout.timeout(0.5):\n+        await async_client.client_unpause()\n+\n+    async with async_timeout.timeout(0.5):\n+        await p1\n+        assert p1.done()\n+\n+    await async_client.client_pause(1, all=False)\n+    await asyncio.sleep(2)\n+    server.stop()\n+\n+\n+async def test_client_pause_b2b(async_client):\n+    async with async_timeout.timeout(1):\n+        await async_client.client_pause(2000, all=False)\n+        await async_client.client_pause(2000, all=False)\n+\n+\n+async def test_client_unpause_after_pause_all(async_client):\n+    await async_client.client_pause(2000, all=True)\n+    # Blocks and waits\n+    res = await async_client.client_unpause()\n+    assert res == \"OK\"\n+    await async_client.client_pause(2000, all=False)\n+    res = await async_client.client_unpause()\n+\n+\n+async def test_client_detached_crash(df_factory):\n+    server = df_factory.create(proactor_threads=1)\n+    server.start()\n+    async_client = server.client()\n+    await async_client.client_pause(2, all=False)\n+    server.stop()\n",
  "problem_statement": "Implement CLIENT UNPAUSE command\n**Did you search GitHub Issues and GitHub Discussions First?**\nYes\n\n**Is your feature request related to a problem? Please describe.**\n\n- I saw CLIENT PAUSE is supported\n- We use PAUSE/UNPAUSE when failing over from one node to another so that we can minimalise data loss. The process looks like this:\n  - CLIENT PAUSE on current master\n  - wait 3 seconds, or when replica offset = master offset\n  - failover\n  - verify that sentinels all agree on who the new master is\n  - CLIENT UNPAUSE on old master to allow sentinel to reconfigure it to replicate from the new master\n- It's not clear on the docs on the available subcommands of CLIENT\n  - https://www.dragonflydb.io/docs/command-reference/server-management/client\n  - I'm having to reference when is on the compatibility page: https://www.dragonflydb.io/docs/command-reference/compatibility\n\n**Describe the solution you'd like**\nThe ability to unpause client pauses, so that we do not have to wait for the timeout as specified in `CLIENT PAUSE <TIMEOUT>`.\n\n**Describe alternatives you've considered**\nN/A\n\n**Additional context**\nRedis supports this command: https://redis.io/docs/latest/commands/client-unpause/\n\n",
  "hints_text": "Hi @jdheyburn \n\nAre you interested in subcommand `UNPAUSE` only ? \n\nWe do support CLIENT PAUSE` but not `UNPAUSE` I am afraid (although I believe this should be very easy to add so I might be able to help on this). \n\nAre there any other CLIENT subcommands that you are interested in using ?\n\nLast, I agree we need to update our docs :)\n@kostasrim Thanks for the quick reply!\n\nYes it is just `UNPAUSE` required for now. I had a look at the compatibility for CLIENT, and I saw `LIST` was fully supported, which is useful for extracting client data from redis-exporter when export-client-list is enabled.\n\nI would like to refer to the below issue, which discusses implementing CLIENT HELP, which would help to understand what options are available. Especially since the warning message is a bit confusing\n\n```\n$ redis-cli CLIENT HELP\n(error) ERR Unknown subcommand or wrong number of arguments for 'HELP'. Try CLIENT HELP.\n```\n\n- https://github.com/dragonflydb/dragonfly/issues/3799\n@jdheyburn sounds good. Once I get free bandwidth I will ping. Bear with me for a day or two :) ",
  "created_at": "2025-02-19T18:04:56Z",
  "modified_files": [
    "src/server/debugcmd.cc",
    "src/server/server_family.cc",
    "src/server/server_family.h"
  ],
  "modified_test_files": [
    "tests/dragonfly/connection_test.py"
  ]
}