{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 469,
  "instance_id": "dragonflydb__dragonfly-469",
  "issue_numbers": [
    "468"
  ],
  "base_commit": "214c10f165c3ae3f84077d2605ad51ed965c3634",
  "patch": "diff --git a/src/server/list_family.cc b/src/server/list_family.cc\nindex 2eb90926636f..3518a6509d17 100644\n--- a/src/server/list_family.cc\n+++ b/src/server/list_family.cc\n@@ -209,9 +209,7 @@ OpStatus BPopper::Run(Transaction* t, unsigned msec) {\n   time_point tp =\n       msec ? chrono::steady_clock::now() + chrono::milliseconds(msec) : time_point::max();\n   bool is_multi = t->IsMulti();\n-  if (!is_multi) {\n-    t->Schedule();\n-  }\n+  t->Schedule();\n \n   auto* stats = ServerState::tl_connection_stats();\n \n@@ -352,8 +350,8 @@ OpResult<string> Peek(const OpArgs& op_args, string_view key, ListDir dir, bool\n \n   quicklist* ql = GetQL(it_res.value()->second);\n   quicklistEntry entry = container_utils::QLEntry();\n-  quicklistIter* iter = (dir == ListDir::LEFT) ? quicklistGetIterator(ql, AL_START_HEAD) :\n-          quicklistGetIterator(ql, AL_START_TAIL);\n+  quicklistIter* iter = (dir == ListDir::LEFT) ? quicklistGetIterator(ql, AL_START_HEAD)\n+                                               : quicklistGetIterator(ql, AL_START_TAIL);\n   CHECK(quicklistNext(iter, &entry));\n   quicklistReleaseIterator(iter);\n \n@@ -848,7 +846,7 @@ void ListFamily::MoveGeneric(ConnectionContext* cntx, string_view src, string_vi\n \n   if (cntx->transaction->unique_shard_cnt() == 1) {\n     auto cb = [&](Transaction* t, EngineShard* shard) {\n-        return OpMoveSingleShard(t->GetOpArgs(shard), src, dest, src_dir, dest_dir);\n+      return OpMoveSingleShard(t->GetOpArgs(shard), src, dest, src_dir, dest_dir);\n     };\n \n     result = cntx->transaction->ScheduleSingleHopT(std::move(cb));\n@@ -865,11 +863,11 @@ void ListFamily::MoveGeneric(ConnectionContext* cntx, string_view src, string_vi\n     //\n     cntx->transaction->Schedule();\n     auto cb = [&](Transaction* t, EngineShard* shard) {\n-        auto args = t->ShardArgsInShard(shard->shard_id());\n-        DCHECK_EQ(1u, args.size());\n-        bool is_dest = args.front() == dest;\n-        find_res[is_dest] = Peek(t->GetOpArgs(shard), args.front(), src_dir, !is_dest);\n-        return OpStatus::OK;\n+      auto args = t->ShardArgsInShard(shard->shard_id());\n+      DCHECK_EQ(1u, args.size());\n+      bool is_dest = args.front() == dest;\n+      find_res[is_dest] = Peek(t->GetOpArgs(shard), args.front(), src_dir, !is_dest);\n+      return OpStatus::OK;\n     };\n \n     cntx->transaction->Execute(move(cb), false);\n@@ -881,18 +879,18 @@ void ListFamily::MoveGeneric(ConnectionContext* cntx, string_view src, string_vi\n     } else {\n       // Everything is ok, lets proceed with the mutations.\n       auto cb = [&](Transaction* t, EngineShard* shard) {\n-          auto args = t->ShardArgsInShard(shard->shard_id());\n-          bool is_dest = args.front() == dest;\n-          OpArgs op_args = t->GetOpArgs(shard);\n-\n-          if (is_dest) {\n-            string_view val{find_res[0].value()};\n-            absl::Span<string_view> span{&val, 1};\n-            OpPush(op_args, args.front(), dest_dir, false, span);\n-          } else {\n-            OpPop(op_args, args.front(), src_dir, 1, false);\n-          }\n-          return OpStatus::OK;\n+        auto args = t->ShardArgsInShard(shard->shard_id());\n+        bool is_dest = args.front() == dest;\n+        OpArgs op_args = t->GetOpArgs(shard);\n+\n+        if (is_dest) {\n+          string_view val{find_res[0].value()};\n+          absl::Span<string_view> span{&val, 1};\n+          OpPush(op_args, args.front(), dest_dir, false, span);\n+        } else {\n+          OpPop(op_args, args.front(), src_dir, 1, false);\n+        }\n+        return OpStatus::OK;\n       };\n       cntx->transaction->Execute(move(cb), true);\n       result = std::move(find_res[0].value());\ndiff --git a/src/server/main_service.cc b/src/server/main_service.cc\nindex 107a3502dd6c..f0036b5e0f9e 100644\n--- a/src/server/main_service.cc\n+++ b/src/server/main_service.cc\n@@ -632,8 +632,10 @@ void Service::DispatchCommand(CmdArgList args, facade::ConnectionContext* cntx)\n           return (*cntx)->SendError(\"script tried accessing undeclared key\");\n         }\n       }\n+\n       dfly_cntx->transaction->SetExecCmd(cid);\n       OpStatus st = dfly_cntx->transaction->InitByArgs(dfly_cntx->conn_state.db_index, args);\n+\n       if (st != OpStatus::OK) {\n         return (*cntx)->SendError(st);\n       }\n@@ -1154,7 +1156,7 @@ void Service::Exec(CmdArgList args, ConnectionContext* cntx) {\n         }\n       }\n       scmd.descr->Invoke(cmd_arg_list, cntx);\n-      if (rb->GetError())\n+      if (rb->GetError())  // checks for i/o error, not logical error.\n         break;\n     }\n \ndiff --git a/src/server/transaction.cc b/src/server/transaction.cc\nindex 7f38c2aaf8ad..eefd5ecf8b81 100644\n--- a/src/server/transaction.cc\n+++ b/src/server/transaction.cc\n@@ -48,7 +48,7 @@ Transaction::Transaction(const CommandId* cid) : cid_(cid) {\n     multi_->multi_opts = cid->opt_mask();\n \n     if (cmd_name == \"EVAL\" || cmd_name == \"EVALSHA\") {\n-      multi_->incremental = false;  // we lock all the keys at once.\n+      multi_->is_expanding = false;  // we lock all the keys at once.\n     }\n   }\n }\n@@ -105,7 +105,7 @@ OpStatus Transaction::InitByArgs(DbIndex index, CmdArgList args) {\n   DCHECK_LT(key_index.start, args.size());\n   DCHECK_GT(key_index.start, 0u);\n \n-  bool incremental_locking = multi_ && multi_->incremental;\n+  bool incremental_locking = multi_ && multi_->is_expanding;\n   bool single_key = !multi_ && key_index.HasSingleKey();\n   bool needs_reverse_mapping = cid_->opt_mask() & CO::REVERSE_MAPPING;\n \n@@ -151,8 +151,12 @@ OpStatus Transaction::InitByArgs(DbIndex index, CmdArgList args) {\n \n   if (multi_) {\n     mode = Mode();\n+    multi_->keys.clear();\n     tmp_space.uniq_keys.clear();\n     DCHECK_LT(int(mode), 2);\n+\n+    // With EVAL, we call this function for EVAL itself as well as for each command\n+    // for eval. currently, we lock everything only during the eval call.\n     should_record_locks = incremental_locking || !multi_->locks_recorded;\n   }\n \n@@ -175,7 +179,11 @@ OpStatus Transaction::InitByArgs(DbIndex index, CmdArgList args) {\n       shard_index[sid].original_index.push_back(i - 1);\n \n     if (should_record_locks && tmp_space.uniq_keys.insert(key).second) {\n-      multi_->locks[key].cnt[int(mode)]++;\n+      if (multi_->is_expanding) {\n+        multi_->keys.push_back(key);\n+      } else {\n+        multi_->locks[key].cnt[int(mode)]++;\n+      }\n     };\n \n     if (key_index.step == 2) {  // value\n@@ -272,13 +280,12 @@ void Transaction::SetExecCmd(const CommandId* cid) {\n   DCHECK(multi_);\n   DCHECK(!cb_);\n \n-  // The order is important, we are Schedule() for multi transaction before overriding cid_.\n-  // TODO: The flow is ugly. I should introduce a proper interface for Multi transactions\n+  // The order is important, we call Schedule for multi transaction before overriding cid_.\n+  // TODO: The flow is ugly. Consider introducing a proper interface for Multi transactions\n   // like SetupMulti/TurndownMulti. We already have UnlockMulti that should be part of\n   // TurndownMulti.\n-\n   if (txid_ == 0) {\n-    Schedule();\n+    ScheduleInternal();\n   }\n \n   unique_shard_cnt_ = 0;\n@@ -313,7 +320,7 @@ bool Transaction::RunInShard(EngineShard* shard) {\n \n   bool was_suspended = sd.local_mask & SUSPENDED_Q;\n   bool awaked_prerun = (sd.local_mask & AWAKED_Q) != 0;\n-  bool incremental_lock = multi_ && multi_->incremental;\n+  bool incremental_lock = multi_ && multi_->is_expanding;\n \n   // For multi we unlock transaction (i.e. its keys) in UnlockMulti() call.\n   // Therefore we differentiate between concluding, which says that this specific\n@@ -341,7 +348,7 @@ bool Transaction::RunInShard(EngineShard* shard) {\n     // if transaction is suspended (blocked in watched queue), then it's a noop.\n     OpStatus status = OpStatus::OK;\n \n-    if (!was_suspended)  {\n+    if (!was_suspended) {\n       status = cb_(this, shard);\n     }\n \n@@ -527,6 +534,16 @@ void Transaction::ScheduleInternal() {\n   }\n }\n \n+void Transaction::LockMulti() {\n+  DCHECK(multi_ && multi_->is_expanding);\n+\n+  IntentLock::Mode mode = Mode();\n+  for (auto key : multi_->keys) {\n+    multi_->locks[key].cnt[int(mode)]++;\n+  }\n+  multi_->keys.clear();\n+}\n+\n // Optimized \"Schedule and execute\" function for the most common use-case of a single hop\n // transactions like set/mset/mget etc. Does not apply for more complicated cases like RENAME or\n // BLPOP where a data must be read from multiple shards before performing another hop.\n@@ -574,8 +591,14 @@ OpStatus Transaction::ScheduleSingleHop(RunnableType cb) {\n     shard_set->Add(unique_shard_id_, std::move(schedule_cb));  // serves as a barrier.\n   } else {\n     // Transaction spans multiple shards or it's global (like flushdb) or multi.\n-    if (!multi_)\n+    // Note that the logic here is a bit different from the public Schedule() function.\n+    if (multi_) {\n+      if (multi_->is_expanding)\n+        LockMulti();\n+    } else {\n       ScheduleInternal();\n+    }\n+\n     ExecuteAsync();\n   }\n \n@@ -616,6 +639,14 @@ void Transaction::UnlockMulti() {\n   VLOG(1) << \"UnlockMultiEnd \" << DebugId();\n }\n \n+void Transaction::Schedule() {\n+  if (multi_ && multi_->is_expanding) {\n+    LockMulti();\n+  } else {\n+    ScheduleInternal();\n+  }\n+}\n+\n // Runs in coordinator thread.\n void Transaction::Execute(RunnableType cb, bool conclude) {\n   DCHECK(coordinator_state_ & COORD_SCHED);\n@@ -1137,8 +1168,8 @@ bool Transaction::NotifySuspended(TxId committed_txid, ShardId sid) {\n     return false;\n   }\n \n-  DVLOG(1) << \"NotifySuspended \" << DebugId() << \", local_mask:\" << local_mask\n-           << \" by \" << committed_txid;\n+  DVLOG(1) << \"NotifySuspended \" << DebugId() << \", local_mask:\" << local_mask << \" by \"\n+           << committed_txid;\n \n   // local_mask could be awaked (i.e. not suspended) if the transaction has been\n   // awakened by another key or awakened by the same key multiple times.\ndiff --git a/src/server/transaction.h b/src/server/transaction.h\nindex 81dfda06d1a6..d43b35727805 100644\n--- a/src/server/transaction.h\n+++ b/src/server/transaction.h\n@@ -24,8 +24,8 @@ namespace dfly {\n class EngineShard;\n class BlockingController;\n \n-using facade::OpStatus;\n using facade::OpResult;\n+using facade::OpStatus;\n \n class Transaction {\n   friend class BlockingController;\n@@ -101,9 +101,7 @@ class Transaction {\n \n   // Schedules a transaction. Usually used for multi-hop transactions like Rename or BLPOP.\n   // For single hop, use ScheduleSingleHop instead.\n-  void Schedule() {\n-    ScheduleInternal();\n-  }\n+  void Schedule();\n \n   // if conclude is true, removes the transaction from the pending queue.\n   void Execute(RunnableType cb, bool conclude);\n@@ -188,7 +186,6 @@ class Transaction {\n   }\n \n  private:\n-\n   struct LockCnt {\n     unsigned cnt[2] = {0, 0};\n   };\n@@ -200,6 +197,7 @@ class Transaction {\n   }\n \n   void ScheduleInternal();\n+  void LockMulti();\n \n   void ExpireBlocking();\n   void ExecuteAsync();\n@@ -259,12 +257,14 @@ class Transaction {\n   };\n   enum { kPerShardSize = sizeof(PerShardData) };\n \n-\n   struct Multi {\n     absl::flat_hash_map<std::string_view, LockCnt> locks;\n+    std::vector<std::string_view> keys;\n+\n     uint32_t multi_opts = 0;  // options of the parent transaction.\n \n-    bool incremental = true;\n+    // Whether this transaction can lock more keys during its progress.\n+    bool is_expanding = true;\n     bool locks_recorded = false;\n   };\n \n",
  "test_patch": "diff --git a/src/server/dragonfly_test.cc b/src/server/dragonfly_test.cc\nindex 4c370f1c0fb2..c20e9f5d07c0 100644\n--- a/src/server/dragonfly_test.cc\n+++ b/src/server/dragonfly_test.cc\n@@ -195,7 +195,10 @@ TEST_F(DflyEngineTest, MultiWeirdCommands) {\n }\n \n TEST_F(DflyEngineTest, MultiRename) {\n-  RespExpr resp = Run({\"multi\"});\n+  RespExpr resp = Run({\"mget\", kKey1, kKey4});\n+  ASSERT_EQ(1, GetDebugInfo().shards_count);\n+\n+  resp = Run({\"multi\"});\n   ASSERT_EQ(resp, \"OK\");\n   Run({\"set\", kKey1, \"1\"});\n \n@@ -205,9 +208,21 @@ TEST_F(DflyEngineTest, MultiRename) {\n \n   ASSERT_THAT(resp, ArrLen(2));\n   EXPECT_THAT(resp.GetVec(), ElementsAre(\"OK\", \"OK\"));\n-  ASSERT_FALSE(service_->IsLocked(0, kKey1));\n-  ASSERT_FALSE(service_->IsLocked(0, kKey4));\n-  ASSERT_FALSE(service_->IsShardSetLocked());\n+\n+  // Now rename with keys spawning multiple shards.\n+  Run({\"mget\", kKey4, kKey2});\n+  ASSERT_EQ(2, GetDebugInfo().shards_count);\n+\n+  Run({\"multi\"});\n+  resp = Run({\"rename\", kKey4, kKey2});\n+  ASSERT_EQ(resp, \"QUEUED\");\n+  resp = Run({\"exec\"});\n+  EXPECT_EQ(resp, \"OK\");\n+\n+  EXPECT_FALSE(service_->IsLocked(0, kKey1));\n+  EXPECT_FALSE(service_->IsLocked(0, kKey2));\n+  EXPECT_FALSE(service_->IsLocked(0, kKey4));\n+  EXPECT_FALSE(service_->IsShardSetLocked());\n }\n \n TEST_F(DflyEngineTest, MultiHop) {\n@@ -264,7 +279,9 @@ TEST_F(DflyEngineTest, FlushDb) {\n }\n \n TEST_F(DflyEngineTest, Eval) {\n-  auto resp = Run({\"incrby\", \"foo\", \"42\"});\n+  RespExpr resp;\n+\n+  resp = Run({\"incrby\", \"foo\", \"42\"});\n   EXPECT_THAT(resp, IntArg(42));\n \n   resp = Run({\"eval\", \"return redis.call('get', 'foo')\", \"0\"});\n@@ -277,6 +294,7 @@ TEST_F(DflyEngineTest, Eval) {\n \n   resp = Run({\"eval\", \"return redis.call('get', 'foo')\", \"1\", \"foo\"});\n   EXPECT_THAT(resp, \"42\");\n+  ASSERT_FALSE(service_->IsLocked(0, \"foo\"));\n \n   resp = Run({\"eval\", \"return redis.call('get', KEYS[1])\", \"1\", \"foo\"});\n   EXPECT_THAT(resp, \"42\");\n@@ -670,6 +688,22 @@ TEST_F(DflyEngineTest, Watch) {\n   ASSERT_THAT(Run({\"exec\"}), kExecSuccess);\n }\n \n+TEST_F(DflyEngineTest, Bug468) {\n+  RespExpr resp = Run({\"multi\"});\n+  ASSERT_EQ(resp, \"OK\");\n+  resp = Run({\"SET\", \"foo\", \"bar\", \"EX\", \"moo\"});\n+  ASSERT_EQ(resp, \"QUEUED\");\n+\n+  resp = Run({\"exec\"});\n+  ASSERT_THAT(resp, ErrArg(\"not an integer\"));\n+  ASSERT_FALSE(service_->IsLocked(0, \"foo\"));\n+\n+  resp = Run({\"eval\", \"return redis.call('set', 'foo', 'bar', 'EX', 'moo')\", \"1\", \"foo\"});\n+  ASSERT_THAT(resp, ErrArg(\"not an integer\"));\n+\n+  ASSERT_FALSE(service_->IsLocked(0, \"foo\"));\n+}\n+\n // TODO: to test transactions with a single shard since then all transactions become local.\n // To consider having a parameter in dragonfly engine controlling number of shards\n // unconditionally from number of cpus. TO TEST BLPOP under multi for single/multi argument case.\n",
  "problem_statement": "Invalid command inside multi cause a crash\n**Describe the bug**\r\nThis issue is related to locking.\r\nWhen we running a pipeline of commands (multi..exec sequence with the CLI), there is not why to do a \"deep\" test for the command validation, the only validation is that there is at least the correct minimum number of args. \r\nWhen the command passes this initial test it would be saved to the list of the commands that we would later execute inside \"dfly_cntx->conn_state.exec_info.body.push_back\". \r\nLater when the execute arrive, it would start executing these commands.\r\nThe execution will invoke each of these using the call to \"scmd.descr->Invoke(cmd_arg_list, cntx);\"\r\nNote that this call do not return a result.\r\nThe last part is to release any lock that was taken during this processing.\r\nSo the call is made here \"cntx->transaction->UnlockMulti();\"\r\nThis assumes that each command was executed successfully. And so it passes on every key that was processes during the steps above (see transaction.cc UnlockMulti), the releases the locks.\r\nThe problem that this trigger is that if one of the commands failed because it was invalid args (either the type of the args or some other invalid value), it would never acquire a lock, and so the lock release function \"UnlockMultiShardCb\" that is calling DbTable::Release, will fail because the command that was not actually executed never actually got a lock, and the release function is not expecting inconsistency.\r\nOne option for this is to assume that we may trying to release locks that never happened. This has the potential to not releasing locks, or to hide some other issue.\r\nOther option is to try and make sure that we have knowledge about which keys were actually locked and not to try to release those keys that never actually took a lock.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. MULTI\r\n2. SET foo bar EX EX PX oiuqwer\r\n3. EXEC\r\n4. See error\r\n\r\n**Expected behavior**\r\nThis should not crash it should not report an error about the invalid argument to the set command.\r\n\r\n\r\n**Environment (please complete the following information):**\r\n - OS: ubuntu 20.04\r\n - Kernel: Linux dfly2 5.15.0-52-generic #58-Ubuntu SMP Thu Oct 13 08:03:55 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux\r\n - Bare Metal\r\n - Dragonfly Version: 0.10.0\r\n\r\n**Reproducible Code Snippet**\r\n```\r\n#!/usr/bin/env python3\r\n\r\nimport asyncio\r\nimport aioredis\r\nimport async_timeout\r\n\r\nDB_INDEX = 1\r\n\r\n\r\nasync def run_pipeline_mode(pool, messages):\r\n    try:\r\n        conn = aioredis.Redis(connection_pool=pool)\r\n        pipe = conn.pipeline()\r\n        for key, val in messages.items():\r\n            pipe.set(key, val)\r\n   #     pipe.get(key)\r\n        pipe.set(\"foo\", \"bar\", \"EX\", \"oiuqwer\") \r\n        result = await pipe.execute()\r\n\r\n        print(f\"got result from the pipeline of {result} with len = {len(result)}\")\r\n        if len(result) != len(messages):\r\n            return False, f\"number of results from pipe {len(result)} != expected {len(messages)}\"\r\n        elif False in result:\r\n            return False, \"expecting to successfully get all result good, but some failed\"\r\n        else:\r\n            return True, \"all command processed successfully\"\r\n    except Exception as e:\r\n        print(f\"failed to run command: error: {e}\")\r\n        return False, str(e)\r\n\r\n\r\ndef test_pipeline_support():\r\n    def generate(max):\r\n        for i in range(max):\r\n            yield f\"key{i}\", f\"value={i}\"\r\n\r\n    messages = {a: b for a, b in generate(1)}\r\n    loop = asyncio.new_event_loop()\r\n    async_pool = aioredis.ConnectionPool(host=\"localhost\", port=6379,\r\n                                         db=DB_INDEX, decode_responses=True, max_connections=16)\r\n    success, message = loop.run_until_complete(\r\n        run_pipeline_mode(async_pool, messages))\r\n    #assert success, message\r\n    return success\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    print(\"starting the test\")\r\n    state = test_pipeline_support()\r\n    print(f\"finish successfully - {state}\")\r\n\r\n```\r\n\r\n\n",
  "hints_text": "There is new branch for this issue [issue-468](https://github.com/dragonflydb/dragonfly/tree/issue-468) was created.\r\nPlease don't sync this with the main branch as we would like to make sure that we can replication this.\r\nYou have a unit test call \"MultiIssue468\" under dragonfly_test.cc that will be able to trigger this issue,\r\nThere is a script call \"fail_multi_invalid_command.py\" under tests/integration that can trigger this issue as well.",
  "created_at": "2022-11-08T14:27:02Z",
  "modified_files": [
    "src/server/list_family.cc",
    "src/server/main_service.cc",
    "src/server/transaction.cc",
    "src/server/transaction.h"
  ],
  "modified_test_files": [
    "src/server/dragonfly_test.cc"
  ]
}