{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 3790,
  "instance_id": "dragonflydb__dragonfly-3790",
  "issue_numbers": [
    "3708"
  ],
  "base_commit": "734be214071e18b9f2eb5c0ebfe8257248f1cc77",
  "patch": "diff --git a/src/server/db_slice.cc b/src/server/db_slice.cc\nindex 5d7db9d38dba..b6c25e10f822 100644\n--- a/src/server/db_slice.cc\n+++ b/src/server/db_slice.cc\n@@ -1451,6 +1451,10 @@ void DbSlice::ResetEvents() {\n   events_ = {};\n }\n \n+void DbSlice::SetNotifyKeyspaceEvents(std::string_view notify_keyspace_events) {\n+  expired_keys_events_recording_ = !notify_keyspace_events.empty();\n+}\n+\n void DbSlice::SendInvalidationTrackingMessage(std::string_view key) {\n   if (client_tracking_map_.empty())\n     return;\ndiff --git a/src/server/db_slice.h b/src/server/db_slice.h\nindex 546faac2b67b..16799530a95a 100644\n--- a/src/server/db_slice.h\n+++ b/src/server/db_slice.h\n@@ -511,6 +511,10 @@ class DbSlice {\n     return pt->Traverse(cursor, std::forward<Cb>(cb));\n   }\n \n+  // Does not check for non supported events. Callers must parse the string and reject it\n+  // if it's not empty and not EX.\n+  void SetNotifyKeyspaceEvents(std::string_view notify_keyspace_events);\n+\n  private:\n   void PreUpdate(DbIndex db_ind, Iterator it, std::string_view key);\n   void PostUpdate(DbIndex db_ind, Iterator it, std::string_view key, size_t orig_size);\ndiff --git a/src/server/main_service.cc b/src/server/main_service.cc\nindex 3e4724c4519e..7ea727f6c748 100644\n--- a/src/server/main_service.cc\n+++ b/src/server/main_service.cc\n@@ -931,6 +931,25 @@ void Service::Init(util::AcceptServer* acceptor, std::vector<facade::Listener*>\n   config_registry.RegisterMutable(\"table_growth_margin\");\n   config_registry.RegisterMutable(\"tcp_keepalive\");\n \n+  config_registry.RegisterMutable(\n+      \"notify_keyspace_events\", [pool = &pp_](const absl::CommandLineFlag& flag) {\n+        auto res = flag.TryGet<std::string>();\n+        if (!res.has_value() || (!res->empty() && !absl::EqualsIgnoreCase(*res, \"EX\"))) {\n+          return false;\n+        }\n+\n+        pool->AwaitBrief([&res](unsigned, auto*) {\n+          auto* shard = EngineShard::tlocal();\n+          if (shard) {\n+            auto shard_id = shard->shard_id();\n+            auto& db_slice = namespaces.GetDefaultNamespace().GetDbSlice(shard_id);\n+            db_slice.SetNotifyKeyspaceEvents(*res);\n+          }\n+        });\n+\n+        return true;\n+      });\n+\n   serialization_max_chunk_size = GetFlag(FLAGS_serialization_max_chunk_size);\n   uint32_t shard_num = GetFlag(FLAGS_num_shards);\n   if (shard_num == 0 || shard_num > pp_.size()) {\n",
  "test_patch": "diff --git a/tests/dragonfly/connection_test.py b/tests/dragonfly/connection_test.py\nindex 4d9c124c2abf..83f5bab7f53f 100755\n--- a/tests/dragonfly/connection_test.py\n+++ b/tests/dragonfly/connection_test.py\n@@ -411,20 +411,15 @@ async def subscribe_worker():\n     await async_pool.disconnect()\n \n \n-@dfly_args({\"notify_keyspace_events\": \"Ex\"})\n-async def test_keyspace_events(async_client: aioredis.Redis):\n-    pclient = async_client.pubsub()\n-    await pclient.subscribe(\"__keyevent@0__:expired\")\n-\n+async def produce_expiring_keys(async_client: aioredis.Redis):\n     keys = []\n     for i in range(10, 50):\n         keys.append(f\"k{i}\")\n         await async_client.set(keys[-1], \"X\", px=200 + i * 10)\n+    return keys\n \n-    # We don't support immediate expiration:\n-    # keys += ['immediate']\n-    # await async_client.set(keys[-1], 'Y', exat=123) # expired 50 years ago\n \n+async def collect_expiring_events(pclient, keys):\n     events = []\n     async for message in pclient.listen():\n         if message[\"type\"] == \"subscribe\":\n@@ -433,10 +428,50 @@ async def test_keyspace_events(async_client: aioredis.Redis):\n         events.append(message)\n         if len(events) >= len(keys):\n             break\n+    return events\n+\n+\n+@dfly_args({\"notify_keyspace_events\": \"Ex\"})\n+async def test_keyspace_events(async_client: aioredis.Redis):\n+    pclient = async_client.pubsub()\n+    await pclient.subscribe(\"__keyevent@0__:expired\")\n+\n+    keys = await produce_expiring_keys(async_client)\n+\n+    # We don't support immediate expiration:\n+    # keys += ['immediate']\n+    # await async_client.set(keys[-1], 'Y', exat=123) # expired 50 years ago\n+\n+    events = await collect_expiring_events(pclient, keys)\n \n     assert set(ev[\"data\"] for ev in events) == set(keys)\n \n \n+async def test_keyspace_events_config_set(async_client: aioredis.Redis):\n+    # nonsense does not make sense as argument, we only accept ex or empty string\n+    with pytest.raises((ResponseError)):\n+        await async_client.config_set(\"notify_keyspace_events\", \"nonsense\")\n+\n+    await async_client.config_set(\"notify_keyspace_events\", \"ex\")\n+    pclient = async_client.pubsub()\n+    await pclient.subscribe(\"__keyevent@0__:expired\")\n+\n+    keys = await produce_expiring_keys(async_client)\n+\n+    events = await collect_expiring_events(pclient, keys)\n+\n+    assert set(ev[\"data\"] for ev in events) == set(keys)\n+\n+    keys = await produce_expiring_keys(async_client)\n+    await async_client.config_set(\"notify_keyspace_events\", \"\")\n+    try:\n+        async with async_timeout.timeout(1):\n+            await collect_expiring_events(pclient, keys)\n+        assert False\n+    except:\n+        pass\n+\n+\n async def test_big_command(df_server, size=8 * 1024):\n     reader, writer = await asyncio.open_connection(\"127.0.0.1\", df_server.port)\n \n",
  "problem_statement": "Additional request around notify-keyspace-events to support Spring Data\n**Is your feature request related to a problem? Please describe.**\r\nSpring Data Redis repository likes to set config values and when it can't it bombs out on start.\r\n\r\nhttps://github.com/spring-projects/spring-data-redis/issues/2670\r\n\r\n```\r\nCaused by: io.lettuce.core.RedisCommandExecutionException: ERR Unknown option or number of arguments for CONFIG SET - 'notify-keyspace-events'\r\n    at io.lettuce.core.internal.ExceptionFactory.createExecutionException(ExceptionFactory.java:147) ~[lettuce-core-6.3.2.RELEASE.jar!/:6.3.2.RELEASE/8941aea]\r\n    at io.lettuce.core.internal.ExceptionFactory.createExecutionException(ExceptionFactory.java:116) ~[lettuce-core-6.3.2.RELEASE.jar!/:6.3.2.RELEASE/8941aea]\r\n    at io.lettuce.core.protocol.AsyncCommand.completeResult(AsyncCommand.java:120) ~[lettuce-core-6.3.2.RELEASE.jar!/:6.3.2.RELEASE/8941aea]\r\n    at io.lettuce.core.protocol.AsyncCommand.complete(AsyncCommand.java:111) ~[lettuce-core-6.3.2.RELEASE.jar!/:6.3.2.RELEASE/8941aea]\r\n    at io.lettuce.core.protocol.CommandWrapper.complete(CommandWrapper.java:63) ~[lettuce-core-6.3.2.RELEASE.jar!/:6.3.2.RELEASE/8941aea]\r\n    at io.lettuce.core.protocol.CommandHandler.complete(CommandHandler.java:745) ~[lettuce-core-6.3.2.RELEASE.jar!/:6.3.2.RELEASE/8941aea]\r\n    at io.lettuce.core.protocol.CommandHandler.decode(CommandHandler.java:680) ~[lettuce-core-6.3.2.RELEASE.jar!/:6.3.2.RELEASE/8941aea]\r\n    at io.lettuce.core.protocol.CommandHandler.channelRead(CommandHandler.java:597) ~[lettuce-core-6.3.2.RELEASE.jar!/:6.3.2.RELEASE/8941aea]\r\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442) ~[netty-transport-4.1.110.Final.jar!/:4.1.110.Final]\r\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.110.Final.jar!/:4.1.110.Final]\r\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.110.Final.jar!/:4.1.110.Final]\r\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1407) ~[netty-transport-4.1.110.Final.jar!/:4.1.110.Final]\r\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440) ~[netty-transport-4.1.110.Final.jar!/:4.1.110.Final]\r\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.110.Final.jar!/:4.1.110.Final]\r\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:918) ~[netty-transport-4.1.110.Final.jar!/:4.1.110.Final]\r\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) ~[netty-transport-4.1.110.Final.jar!/:4.1.110.Final]\r\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788) ~[netty-transport-4.1.110.Final.jar!/:4.1.110.Final]\r\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724) ~[netty-transport-4.1.110.Final.jar!/:4.1.110.Final]\r\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650) ~[netty-transport-4.1.110.Final.jar!/:4.1.110.Final]\r\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) ~[netty-transport-4.1.110.Final.jar!/:4.1.110.Final]\r\n    at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994) ~[netty-common-4.1.110.Final.jar!/:4.1.110.Final]\r\n    at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.110.Final.jar!/:4.1.110.Final]\r\n    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.110.Final.jar!/:4.1.110.Final]\r\n    at java.base/java.lang.Thread.run(Thread.java:840) ~[na:na]\r\n```\r\n\nI'm trying to use Redis (and in this case DragonflyDB) as a shared session store for a high availability SpringBoot based application.\n\r\n**Describe the solution you'd like**\r\nI'd like to be able to use Spring Data Redis with DragonflyDB.\r\n\r\n**Describe alternatives you've considered**\r\nKeep using Redis sentinel and become increasingly despondent with the experience in EKS.\r\n\r\n**Additional context**\r\n\r\nRelated to https://github.com/dragonflydb/dragonfly/issues/1657\r\n\nhttps://www.shinyproxy.io/documentation/configuration/#session-persistence-and-aws-elasticache - upstream configuration details for elasticache that disables this by default so explicit configuration is needed there.\n\r\n_Using DragonflyDB 1.22.1 via the Operator with 3 replicas._\r\n\n",
  "hints_text": "Hi @danielloader \r\nI am closing as we already have open issue for supporting keyspace notifications\r\n#3618 \n@adiholden  I think the issue here is different. We currently allow enabling the notifications only via a flag, and the issue here is about enabling it via \"config set\"\nIf the functionality works via a flag I can always follow the same guide as AWS elasticache and enable that out of band in the cache side. \r\n\r\nThat said yes the original ask was that spring data just works without additional hurdles.\r\n\r\nEdit: What's the flag configuration to enable it?\r\n\r\n\nSame as with redis: notify-keyspace-events. Please note we only support Ex option\nThanks, yeah in which case without `EA` I guess how I get it configured is a bit premature then.",
  "created_at": "2024-09-25T10:43:08Z",
  "modified_files": [
    "src/server/db_slice.cc",
    "src/server/db_slice.h",
    "src/server/main_service.cc"
  ],
  "modified_test_files": [
    "tests/dragonfly/connection_test.py"
  ]
}