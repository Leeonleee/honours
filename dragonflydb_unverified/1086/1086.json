{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 1086,
  "instance_id": "dragonflydb__dragonfly-1086",
  "issue_numbers": [
    "1045"
  ],
  "base_commit": "13199184764bd55b151b69f7e8db1821d727b8b3",
  "patch": "diff --git a/src/server/debugcmd.cc b/src/server/debugcmd.cc\nindex 20e951940bf3..932ba2915c35 100644\n--- a/src/server/debugcmd.cc\n+++ b/src/server/debugcmd.cc\n@@ -170,13 +170,9 @@ void DebugCmd::Reload(CmdArgList args) {\n \n   if (save) {\n     string err_details;\n-    const CommandId* cid = sf_.service().FindCmd(\"SAVE\");\n-    CHECK_NOTNULL(cid);\n-    intrusive_ptr<Transaction> trans(new Transaction{cid, ServerState::tlocal()->thread_index()});\n-    trans->InitByArgs(0, {});\n     VLOG(1) << \"Performing save\";\n \n-    GenericError ec = sf_.DoSave(absl::GetFlag(FLAGS_df_snapshot_format), trans.get());\n+    GenericError ec = sf_.DoSave();\n     if (ec) {\n       return (*cntx_)->SendError(ec.Format());\n     }\ndiff --git a/src/server/server_family.cc b/src/server/server_family.cc\nindex 7836ed6b43b9..8642551d4fd9 100644\n--- a/src/server/server_family.cc\n+++ b/src/server/server_family.cc\n@@ -465,7 +465,7 @@ void ServerFamily::Init(util::AcceptServer* acceptor, util::ListenerInterface* m\n   if (!save_time.empty()) {\n     std::optional<SnapshotSpec> spec = ParseSaveSchedule(save_time);\n     if (spec) {\n-      snapshot_fiber_ = service_.proactor_pool().GetNextProactor()->LaunchFiber(\n+      snapshot_schedule_fb_ = service_.proactor_pool().GetNextProactor()->LaunchFiber(\n           [save_spec = std::move(spec.value()), this] { SnapshotScheduling(save_spec); });\n     } else {\n       LOG(WARNING) << \"Invalid snapshot time specifier \" << save_time;\n@@ -479,9 +479,18 @@ void ServerFamily::Shutdown() {\n   if (load_result_.valid())\n     load_result_.wait();\n \n-  is_snapshot_done_.Notify();\n-  if (snapshot_fiber_.IsJoinable()) {\n-    snapshot_fiber_.Join();\n+  schedule_done_.Notify();\n+  if (snapshot_schedule_fb_.IsJoinable()) {\n+    snapshot_schedule_fb_.Join();\n+  }\n+\n+  if (save_on_shutdown_ && !absl::GetFlag(FLAGS_dbfilename).empty()) {\n+    shard_set->pool()->GetNextProactor()->Await([this] {\n+      GenericError ec = DoSave();\n+      if (ec) {\n+        LOG(WARNING) << \"Failed to perform snapshot \" << ec.Format();\n+      }\n+    });\n   }\n \n   pb_task_->Await([this] {\n@@ -606,7 +615,7 @@ Future<std::error_code> ServerFamily::Load(const std::string& load_path) {\n void ServerFamily::SnapshotScheduling(const SnapshotSpec& spec) {\n   const auto loop_sleep_time = std::chrono::seconds(20);\n   while (true) {\n-    if (is_snapshot_done_.WaitFor(loop_sleep_time)) {\n+    if (schedule_done_.WaitFor(loop_sleep_time)) {\n       break;\n     }\n \n@@ -627,13 +636,7 @@ void ServerFamily::SnapshotScheduling(const SnapshotSpec& spec) {\n       continue;\n     }\n \n-    const CommandId* cid = service().FindCmd(\"SAVE\");\n-    CHECK_NOTNULL(cid);\n-    boost::intrusive_ptr<Transaction> trans(\n-        new Transaction{cid, ServerState::tlocal()->thread_index()});\n-    trans->InitByArgs(0, {});\n-\n-    GenericError ec = DoSave(absl::GetFlag(FLAGS_df_snapshot_format), trans.get());\n+    GenericError ec = DoSave();\n     if (ec) {\n       LOG(WARNING) << \"Failed to perform snapshot \" << ec.Format();\n     }\n@@ -663,9 +666,6 @@ error_code ServerFamily::LoadRdb(const std::string& rdb_file) {\n   } else {\n     ec = res.error();\n   }\n-\n-  service_.SwitchState(GlobalState::LOADING, GlobalState::ACTIVE);\n-\n   return ec;\n }\n \n@@ -922,6 +922,15 @@ error_code DoPartialSave(PartialSaveOpts opts, const dfly::StringVec& scripts,\n   return local_ec;\n }\n \n+GenericError ServerFamily::DoSave() {\n+  const CommandId* cid = service().FindCmd(\"SAVE\");\n+  CHECK_NOTNULL(cid);\n+  boost::intrusive_ptr<Transaction> trans(\n+      new Transaction{cid, ServerState::tlocal()->thread_index()});\n+  trans->InitByArgs(0, {});\n+  return DoSave(absl::GetFlag(FLAGS_df_snapshot_format), trans.get());\n+}\n+\n GenericError ServerFamily::DoSave(bool new_version, Transaction* trans) {\n   fs::path dir_path(GetFlag(FLAGS_dir));\n   AggregateGenericError ec;\n@@ -2034,6 +2043,22 @@ void ServerFamily::Latency(CmdArgList args, ConnectionContext* cntx) {\n }\n \n void ServerFamily::_Shutdown(CmdArgList args, ConnectionContext* cntx) {\n+  if (args.size() > 1) {\n+    (*cntx)->SendError(kSyntaxErr);\n+    return;\n+  }\n+\n+  if (args.size() == 1) {\n+    auto sub_cmd = ArgS(args, 0);\n+    if (absl::EqualsIgnoreCase(sub_cmd, \"SAVE\")) {\n+    } else if (absl::EqualsIgnoreCase(sub_cmd, \"NOSAVE\")) {\n+      save_on_shutdown_ = false;\n+    } else {\n+      (*cntx)->SendError(kSyntaxErr);\n+      return;\n+    }\n+  }\n+\n   CHECK_NOTNULL(acceptor_)->Stop();\n   (*cntx)->SendOk();\n }\n@@ -2077,7 +2102,7 @@ void ServerFamily::Register(CommandRegistry* registry) {\n             << CI{\"LATENCY\", CO::NOSCRIPT | CO::LOADING | CO::FAST, -2, 0, 0, 0}.HFUNC(Latency)\n             << CI{\"MEMORY\", kMemOpts, -2, 0, 0, 0}.HFUNC(Memory)\n             << CI{\"SAVE\", CO::ADMIN | CO::GLOBAL_TRANS, -1, 0, 0, 0}.HFUNC(Save)\n-            << CI{\"SHUTDOWN\", CO::ADMIN | CO::NOSCRIPT | CO::LOADING, 1, 0, 0, 0}.HFUNC(_Shutdown)\n+            << CI{\"SHUTDOWN\", CO::ADMIN | CO::NOSCRIPT | CO::LOADING, -1, 0, 0, 0}.HFUNC(_Shutdown)\n             << CI{\"SLAVEOF\", kReplicaOpts, 3, 0, 0, 0}.HFUNC(ReplicaOf)\n             << CI{\"READONLY\", CO::READONLY, 1, 0, 0, 0}.HFUNC(ReadOnly)\n             << CI{\"REPLICAOF\", kReplicaOpts, 3, 0, 0, 0}.HFUNC(ReplicaOf)\ndiff --git a/src/server/server_family.h b/src/server/server_family.h\nindex be6ba113badb..431b91ccdcc8 100644\n--- a/src/server/server_family.h\n+++ b/src/server/server_family.h\n@@ -91,6 +91,10 @@ class ServerFamily {\n   // if new_version is true, saves DF specific, non redis compatible snapshot.\n   GenericError DoSave(bool new_version, Transaction* transaction);\n \n+  // Calls DoSave with a default generated transaction and with the format\n+  // specified in --df_snapshot_format\n+  GenericError DoSave();\n+\n   // Burns down and destroy all the data from the database.\n   // if kDbAll is passed, burns all the databases to the ground.\n   std::error_code Drakarys(Transaction* transaction, DbIndex db_ind);\n@@ -161,7 +165,7 @@ class ServerFamily {\n \n   void SnapshotScheduling(const SnapshotSpec& time);\n \n-  Fiber snapshot_fiber_;\n+  Fiber snapshot_schedule_fb_;\n   Future<std::error_code> load_result_;\n \n   uint32_t stats_caching_task_ = 0;\n@@ -186,7 +190,11 @@ class ServerFamily {\n   std::shared_ptr<LastSaveInfo> last_save_info_;  // protected by save_mu_;\n   std::atomic_bool is_saving_{false};\n \n-  Done is_snapshot_done_;\n+  // Used to override save on shutdown behavior that is usually set\n+  // be --dbfilename.\n+  bool save_on_shutdown_{true};\n+\n+  Done schedule_done_;\n   std::unique_ptr<FiberQueueThreadPool> fq_threadpool_;\n };\n \n",
  "test_patch": "diff --git a/src/server/rdb_test.cc b/src/server/rdb_test.cc\nindex 8c73418157be..04fd2097ca85 100644\n--- a/src/server/rdb_test.cc\n+++ b/src/server/rdb_test.cc\n@@ -38,18 +38,21 @@ namespace dfly {\n \n class RdbTest : public BaseFamilyTest {\n  protected:\n-  static void SetUpTestSuite();\n   void TearDown();\n+  void SetUp();\n \n   io::FileSource GetSource(string name);\n };\n \n-void RdbTest::SetUpTestSuite() {\n-  BaseFamilyTest::SetUpTestSuite();\n+void RdbTest::SetUp() {\n   SetFlag(&FLAGS_dbfilename, \"rdbtestdump\");\n+  BaseFamilyTest::SetUp();\n }\n \n void RdbTest::TearDown() {\n+  // Disable save on shutdown\n+  SetFlag(&FLAGS_dbfilename, \"\");\n+\n   auto rdb_files = io::StatFiles(\"rdbtestdump*\");\n   CHECK(rdb_files);\n   for (const auto& fl : *rdb_files) {\ndiff --git a/tests/dragonfly/replication_test.py b/tests/dragonfly/replication_test.py\nindex 62cb5035ae5e..78c8cc6297b4 100644\n--- a/tests/dragonfly/replication_test.py\n+++ b/tests/dragonfly/replication_test.py\n@@ -36,9 +36,9 @@\n @pytest.mark.asyncio\n @pytest.mark.parametrize(\"t_master, t_replicas, seeder_config\", replication_cases)\n async def test_replication_all(df_local_factory, df_seeder_factory, t_master, t_replicas, seeder_config):\n-    master = df_local_factory.create(port=BASE_PORT, proactor_threads=t_master)\n+    master = df_local_factory.create(port=BASE_PORT, proactor_threads=t_master, dbfilename=\"\")\n     replicas = [\n-        df_local_factory.create(port=BASE_PORT+i+1, proactor_threads=t)\n+        df_local_factory.create(port=BASE_PORT+i+1, proactor_threads=t, dbfilename=\"\")\n         for i, t in enumerate(t_replicas)\n     ]\n \n@@ -148,10 +148,10 @@ async def check_data(seeder, replicas, c_replicas):\n @pytest.mark.asyncio\n @pytest.mark.parametrize(\"t_master, t_crash_fs, t_crash_ss, t_disonnect, n_keys\", disconnect_cases)\n async def test_disconnect_replica(df_local_factory: DflyInstanceFactory, df_seeder_factory, t_master, t_crash_fs, t_crash_ss, t_disonnect, n_keys):\n-    master = df_local_factory.create(port=BASE_PORT, proactor_threads=t_master)\n+    master = df_local_factory.create(port=BASE_PORT, proactor_threads=t_master, dbfilename=\"\")\n     replicas = [\n         (df_local_factory.create(\n-            port=BASE_PORT+i+1, proactor_threads=t), crash_fs)\n+            port=BASE_PORT+i+1, proactor_threads=t, dbfilename=\"\"), crash_fs)\n         for i, (t, crash_fs) in enumerate(\n             chain(\n                 zip(t_crash_fs, repeat(DISCONNECT_CRASH_FULL_SYNC)),\n@@ -284,10 +284,10 @@ async def disconnect(replica, c_replica, crash_type):\n @pytest.mark.asyncio\n @pytest.mark.parametrize(\"t_master, t_replicas, n_random_crashes, n_keys\", master_crash_cases)\n async def test_disconnect_master(df_local_factory, df_seeder_factory, t_master, t_replicas, n_random_crashes, n_keys):\n-    master = df_local_factory.create(port=1111, proactor_threads=t_master)\n+    master = df_local_factory.create(port=1111, proactor_threads=t_master, dbfilename=\"\")\n     replicas = [\n         df_local_factory.create(\n-            port=BASE_PORT+i+1, proactor_threads=t)\n+            port=BASE_PORT+i+1, proactor_threads=t, dbfilename=\"\")\n         for i, t in enumerate(t_replicas)\n     ]\n \n@@ -398,8 +398,8 @@ async def test_cancel_replication_immediately(df_local_factory, df_seeder_factor\n     \"\"\"\n     COMMANDS_TO_ISSUE = 40\n \n-    replica = df_local_factory.create(port=BASE_PORT, v=1)\n-    masters = [df_local_factory.create(port=BASE_PORT+i+1) for i in range(4)]\n+    replica = df_local_factory.create(port=BASE_PORT, dbfilename=\"\")\n+    masters = [df_local_factory.create(port=BASE_PORT+i+1, dbfilename=\"\") for i in range(4)]\n     seeders = [df_seeder_factory.create(port=m.port) for m in masters]\n \n     df_local_factory.start_all([replica] + masters)\ndiff --git a/tests/dragonfly/snapshot_test.py b/tests/dragonfly/snapshot_test.py\nindex ac5b4e7e639e..4f7eb93d76d8 100644\n--- a/tests/dragonfly/snapshot_test.py\n+++ b/tests/dragonfly/snapshot_test.py\n@@ -4,6 +4,7 @@\n import glob\n import aioredis\n from pathlib import Path\n+import aioredis\n \n from . import dfly_args\n from .utility import DflySeeder, wait_available_async\n@@ -18,7 +19,8 @@ def setup(self, tmp_dir: Path):\n         self.tmp_dir = tmp_dir\n \n     def get_main_file(self, pattern):\n-        def is_main(f): return \"summary\" in f if pattern.endswith(\"dfs\") else True\n+        def is_main(f): return \"summary\" in f if pattern.endswith(\n+            \"dfs\") else True\n         files = glob.glob(str(self.tmp_dir.absolute()) + '/' + pattern)\n         possible_mains = list(filter(is_main, files))\n         assert len(possible_mains) == 1, possible_mains\n@@ -92,6 +94,8 @@ async def test_snapshot(self, df_seeder_factory, async_client, df_server):\n         assert await seeder.compare(start_capture)\n \n # We spawn instances manually, so reduce memory usage of default to minimum\n+\n+\n @dfly_args({\"proactor_threads\": \"1\"})\n class TestDflyAutoLoadSnapshot(SnapshotTestBase):\n     \"\"\"Test automatic loading of dump files on startup with timestamp\"\"\"\n@@ -138,7 +142,8 @@ def setup(self, tmp_dir: Path):\n \n     @pytest.mark.asyncio\n     async def test_snapshot(self, df_seeder_factory, df_server):\n-        seeder = df_seeder_factory.create(port=df_server.port, keys=10, multi_transaction_probability=0)\n+        seeder = df_seeder_factory.create(\n+            port=df_server.port, keys=10, multi_transaction_probability=0)\n         await seeder.run(target_deviation=0.5)\n \n         time.sleep(60)\n@@ -156,9 +161,34 @@ def setup(self, tmp_dir: Path):\n \n     @pytest.mark.asyncio\n     async def test_snapshot(self, df_local_factory):\n-        df_server = df_local_factory.create(dbfilename=\"../../../../etc/passwd\")\n+        df_server = df_local_factory.create(\n+            dbfilename=\"../../../../etc/passwd\")\n         try:\n             df_server.start()\n             assert False, \"Server should not start correctly\"\n         except Exception as e:\n             pass\n+\n+\n+@dfly_args({**BASIC_ARGS, \"dbfilename\": \"test-shutdown\"})\n+class TestDflySnapshotOnShutdown(SnapshotTestBase):\n+    \"\"\"Test multi file snapshot\"\"\"\n+    @pytest.fixture(autouse=True)\n+    def setup(self, tmp_dir: Path):\n+        self.tmp_dir = tmp_dir\n+\n+    @pytest.mark.asyncio\n+    async def test_snapshot(self, df_seeder_factory, df_server):\n+        seeder = df_seeder_factory.create(port=df_server.port, **SEEDER_ARGS)\n+        await seeder.run(target_deviation=0.1)\n+\n+        start_capture = await seeder.capture()\n+\n+        df_server.stop()\n+        df_server.start()\n+\n+        a_client = aioredis.Redis(port=df_server.port)\n+        await wait_available_async(a_client)\n+        await a_client.connection_pool.disconnect()\n+\n+        assert await seeder.compare(start_capture)\n",
  "problem_statement": "Save on SIGTERM and SIGINT\n**Did you search GitHub Issues and GitHub Discussions First?**\r\nYes, although there are a lot, and i might have overlooked something.\r\n\r\n**Is your feature request related to a problem? Please describe.**\r\nWhen updating Dragonfly (by increasing the version-number in the `docker-compose.yml` and running `docker compose up -d`), the updated instance is empty\r\n\r\n**Describe the solution you'd like**\r\nI'd like to have Dragonfly perform a `save` when receiving a SIGTERM or SIGINT, similar to how [Redis handles these signals](https://redis.io/docs/reference/signals#sigterm-and-sigint).\r\nIf i'm not mistaken that should create a dump that should automatically be restored when the new Dragonfly instance starts up, so no data is lost.\r\n\r\n**Describe alternatives you've considered**\r\nFor my personal use case, i'll create a small update script that tells Dragonfly to `save` before recreating the container.\r\n\r\n**Additional context**\r\nIt's not a big deal, but having Dragonfly save itself on SIGTERM/SIGINT would be convinient.\n",
  "hints_text": "\r\nFor those looking for a resolution in the mean time, before a shut down use SAVE\r\n\r\nIE\r\nredis-cli -h localhost -p 6379 SAVE\r\n\r\n\r\n",
  "created_at": "2023-04-13T13:52:58Z",
  "modified_files": [
    "src/server/debugcmd.cc",
    "src/server/server_family.cc",
    "src/server/server_family.h"
  ],
  "modified_test_files": [
    "src/server/rdb_test.cc",
    "tests/dragonfly/replication_test.py",
    "tests/dragonfly/snapshot_test.py"
  ]
}