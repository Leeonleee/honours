{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4116,
  "instance_id": "dragonflydb__dragonfly-4116",
  "issue_numbers": [
    "4113"
  ],
  "base_commit": "0facd6fd8f6d5d8d145bc799a21a0422f63d4e6c",
  "patch": "diff --git a/src/server/main_service.cc b/src/server/main_service.cc\nindex c5664d1a25b2..6fa009ef0fad 100644\n--- a/src/server/main_service.cc\n+++ b/src/server/main_service.cc\n@@ -1426,15 +1426,14 @@ size_t Service::DispatchManyCommands(absl::Span<CmdArgList> args_list, SinkReply\n \n     // MULTI...EXEC commands need to be collected into a single context, so squashing is not\n     // possible\n-    const bool is_multi =\n-        dfly_cntx->conn_state.exec_info.IsCollecting() || CO::IsTransKind(ArgS(args, 0));\n+    const bool is_multi = dfly_cntx->conn_state.exec_info.IsCollecting() || CO::IsTransKind(cmd);\n \n     // Generally, executing any multi-transactions (including eval) is not possible because they\n     // might request a stricter multi mode than non-atomic which is used for squashing.\n     // TODO: By allowing promoting non-atomic multit transactions to lock-ahead for specific command\n     // invocations, we can potentially execute multiple eval in parallel, which is very powerful\n     // paired with shardlocal eval\n-    const bool is_eval = CO::IsEvalKind(ArgS(args, 0));\n+    const bool is_eval = CO::IsEvalKind(cmd);\n \n     const bool is_blocking = cid != nullptr && cid->IsBlocking();\n \n",
  "test_patch": "diff --git a/tests/dragonfly/connection_test.py b/tests/dragonfly/connection_test.py\nindex 1c3cc89ef3d9..a59c4a39cd20 100755\n--- a/tests/dragonfly/connection_test.py\n+++ b/tests/dragonfly/connection_test.py\n@@ -783,6 +783,16 @@ async def test_tls_reject(\n         await client.ping()\n \n \n+@dfly_args({\"proactor_threads\": \"4\", \"pipeline_squash\": 1})\n+async def test_squashed_pipeline_eval(async_client: aioredis.Redis):\n+    p = async_client.pipeline(transaction=False)\n+    for _ in range(5):\n+        # Deliberately lowcase EVAL to test that it is not squashed\n+        p.execute_command(\"eval\", \"return redis.call('set', KEYS[1], 'value')\", 1, \"key\")\n+    res = await p.execute()\n+    assert res == [\"OK\"] * 5\n+\n+\n @dfly_args({\"proactor_threads\": \"4\", \"pipeline_squash\": 10})\n async def test_squashed_pipeline(async_client: aioredis.Redis):\n     p = async_client.pipeline(transaction=False)\n",
  "problem_statement": "v1.25.0 crashes when running some BullMQ tests\n**Describe the bug**\r\nSince v1.25.0 released today, BullMQ's test suite produces a hard crash in dragonfly's server.\r\n\r\n**To Reproduce**\r\n\r\nSteps to reproduce the behavior:\r\nJust run the test suite but one of the tests that reproduces the issue is the first test in test_concurrency.ts: https://github.com/taskforcesh/bullmq/blob/master/tests/test_concurrency.ts#L33\r\n\r\n**Expected behavior**\r\nShould not crash and test should pass\r\n\r\n**Screenshots**\r\n\r\n**Environment (please complete the following information):**\r\n - OS:MacOs but same issue in Ubuntu.\r\n - Kernel: # Command: `uname -a`\r\n - Containerized?: Docker\r\n - Dragonfly Version: 1.25.0\r\n\r\n**Reproducible Code Snippet**\r\n\r\n**Additional context**\r\nThis is the output of the server when it crashes:\r\n```\r\n docker run -p 6379:6379 --ulimit memlock=-1 docker.dragonflydb.io/dragonflydb/dragonfly --force_epoll --cluster_mode=emulated --lock_on_hashtags --proactor_threads=4\r\nI20241111 12:02:18.918148     1 init.cc:78] dragonfly running in opt mode.\r\nI20241111 12:02:18.920707     1 dfly_main.cc:693] Starting dragonfly df-v1.25.0-a50679576230724c9e4f528c382870666dee48a9\r\n* Logs will be written to the first available of the following paths:\r\n/tmp/dragonfly.*\r\n./dragonfly.*\r\n* For the available flags type dragonfly [--help | --helpfull]\r\n* Documentation can be found at: https://www.dragonflydb.io/docs\r\nW20241111 12:02:18.923547     1 dfly_main.cc:732] SWAP is enabled. Consider disabling it when running Dragonfly.\r\nI20241111 12:02:18.923569     1 dfly_main.cc:737] maxmemory has not been specified. Deciding myself....\r\nI20241111 12:02:18.923571     1 dfly_main.cc:746] Found 5.86GiB available memory. Setting maxmemory to 4.69GiB\r\nI20241111 12:02:18.926821     1 proactor_pool.cc:147] Running 4 io threads\r\nI20241111 12:02:18.934221     1 server_family.cc:835] Host OS: Linux 6.6.32-linuxkit aarch64 with 4 threads\r\nI20241111 12:02:18.941165     1 snapshot_storage.cc:181] Load snapshot: Searching for snapshot in directory: \"/data\"\r\nW20241111 12:02:18.941254     1 server_family.cc:949] Load snapshot: No snapshot found\r\nI20241111 12:02:18.947126    11 listener_interface.cc:101] sock[11] AcceptServer - listening on port 6379\r\nW20241111 12:02:22.767335    13 main_service.cc:1409]  COMMAND DOCS failed with reason: syntax error\r\nF20241111 12:14:50.497730    10 common.cc:461] Check failed: !state->squashing_info\r\n*** Check failure stack trace: ***\r\n    @     0xaaaac56dbd6c  google::LogMessage::SendToLog()\r\n    @     0xaaaac56d4e50  google::LogMessage::Flush()\r\n    @     0xaaaac56d678c  google::LogMessageFatal::~LogMessageFatal()\r\n    @     0xaaaac50650dc  dfly::BorrowedInterpreter::BorrowedInterpreter()\r\n    @     0xaaaac4db5798  dfly::Service::Eval()\r\n    @     0xaaaac505809c  dfly::CommandId::Invoke()\r\n    @     0xaaaac4dac150  dfly::Service::InvokeCmd()\r\n    @     0xaaaac4ec2e0c  dfly::MultiCommandSquasher::SquashedHopCb()\r\n    @     0xaaaac4ec3a54  _ZNSt17_Function_handlerIFvvEZN4dfly20MultiCommandSquasher15ExecuteSquashedEPN6facade17RedisReplyBuilderEEUlvE1_E9_M_invokeERKSt9_Any_data\r\n    @     0xaaaac55266a8  util::fb2::FiberQueue::Run()\r\n    @     0xaaaac50b6dac  _ZN5boost7context6detail11fiber_entryINS1_12fiber_recordINS0_5fiberEN4util3fb219FixedStackAllocatorEZNS6_6detail15WorkerFiberImplIZN4dfly9TaskQueue5StartESt17basic_string_viewIcSt11char_traitsIcEEEUlvE_JEEC4IS7_EESF_RKNS0_12preallocatedEOT_OSG_EUlOS4_E_EEEEvNS1_10transfer_tE\r\n    @     0xaaaac55434d4  make_fcontext\r\n*** SIGABRT received at time=1731327290 on cpu 0 ***\r\nPC: @     0xffff93f0f200  (unknown)  (unknown)\r\n    @     0xaaaac572f924        480  absl::lts_20240116::AbslFailureSignalHandler()\r\n    @     0xffff941927a0       4960  (unknown)\r\n    @     0xffff93eca67c        208  gsignal\r\n    @     0xffff93eb7130         32  abort\r\n    @     0xaaaac56e162c        336  google::DumpStackTraceAndExit()\r\n    @     0xaaaac56d544c        192  google::LogMessage::Fail()\r\n    @     0xaaaac56dbd6c         16  google::LogMessage::SendToLog()\r\n    @     0xaaaac56d4e50        208  google::LogMessage::Flush()\r\n    @     0xaaaac56d678c         80  google::LogMessageFatal::~LogMessageFatal()\r\n    @     0xaaaac50650dc         16  dfly::BorrowedInterpreter::BorrowedInterpreter()\r\n    @     0xaaaac4db5798        160  dfly::Service::Eval()\r\n    @     0xaaaac505809c        208  dfly::CommandId::Invoke()\r\n    @     0xaaaac4dac150        112  dfly::Service::InvokeCmd()\r\n    @     0xaaaac4ec2e0c        672  dfly::MultiCommandSquasher::SquashedHopCb()\r\n    @     0xaaaac4ec3a54       1072  std::_Function_handler<>::_M_invoke()\r\n    @     0xaaaac55266a8         32  util::fb2::FiberQueue::Run()\r\n    @     0xaaaac50b6dac        288  boost::context::detail::fiber_entry<>()\r\n[failure_signal_handler.cc : 345] RAW: Signal 5 raised at PC=0xffff93eb71ec while already in AbslFailureSignalHandler()\r\n*** SIGTRAP received at time=1731327290 on cpu 0 ***\r\nPC: @     0xffff93eb71ec  (unknown)  abort\r\n    @     0xaaaac572f924        480  absl::lts_20240116::AbslFailureSignalHandler()\r\n    @     0xffff941927a0       4960  (unknown)\r\n    @     0xaaaac56e162c        336  google::DumpStackTraceAndExit()\r\n    @     0xaaaac56d544c        192  google::LogMessage::Fail()\r\n    @     0xaaaac56dbd6c         16  google::LogMessage::SendToLog()\r\n    @     0xaaaac56d4e50        208  google::LogMessage::Flush()\r\n    @     0xaaaac56d678c         80  google::LogMessageFatal::~LogMessageFatal()\r\n    @     0xaaaac50650dc         16  dfly::BorrowedInterpreter::BorrowedInterpreter()\r\n    @     0xaaaac4db5798        160  dfly::Service::Eval()\r\n    @     0xaaaac505809c        208  dfly::CommandId::Invoke()\r\n    @     0xaaaac4dac150        112  dfly::Service::InvokeCmd()\r\n    @     0xaaaac4ec2e0c        672  dfly::MultiCommandSquasher::SquashedHopCb()\r\n    @     0xaaaac4ec3a54       1072  std::_Function_handler<>::_M_invoke()\r\n    @     0xaaaac55266a8         32  util::fb2::FiberQueue::Run()\r\n    @     0xaaaac50b6dac        288  boost::context::detail::fiber_entry<>()\r\n```\r\n\r\nThis is the last command sent to Dragonfly and that triggers the crash:\r\n```\r\n1731325576.1345656 [0 192.168.65.1:32473] \"EVAL\" \"--[[\\n  Adds a job to the queue by doing the following:\\n    - Increases the job counter if needed.\\n    - Creates a new job key with the job data.\\n    - if delayed:\\n      - computes timestamp.\\n      - adds to delayed zset.\\n      - Emits a global event 'delayed' if the job is delayed.\\n    - if not delayed\\n      - Adds the jobId to the wait/paused list in one of three ways:\\n         - LIFO\\n         - FIFO\\n         - prioritized.\\n      - Adds the job to the \\\"added\\\" list so that workers gets notified.\\n    Input:\\n      KEYS[1] 'wait',\\n      KEYS[2] 'paused'\\n      KEYS[3] 'meta'\\n      KEYS[4] 'id'\\n      KEYS[5] 'completed'\\n      KEYS[6] 'active'\\n      KEYS[7] events stream key\\n      KEYS[8] marker key\\n      ARGV[1] msgpacked arguments array\\n            [1]  key prefix,\\n            [2]  custom id (will not generate one automatically)\\n            [3]  name\\n            [4]  timestamp\\n            [5]  parentKey?\\n            [6]  waitChildrenKey key.\\n            [7]  parent dependencies key.\\n            [8]  parent? {id, queueKey}\\n            [9]  repeat job key\\n            [10] deduplication key\\n      ARGV[2] Json stringified job data\\n      ARGV[3] msgpacked options\\n      Output:\\n        jobId  - OK\\n        -5     - Missing parent key\\n]]\\nlocal eventsKey = KEYS[7]\\nlocal jobId\\nlocal jobIdKey\\nlocal rcall = redis.call\\nlocal args = cmsgpack.unpack(ARGV[1])\\nlocal data = ARGV[2]\\nlocal opts = cmsgpack.unpack(ARGV[3])\\nlocal parentKey = args[5]\\nlocal parent = args[8]\\nlocal repeatJobKey = args[9]\\nlocal deduplicationKey = args[10]\\nlocal parentData\\n-- Includes\\n--[[\\n  Function to add job in target list and add marker if needed.\\n]]\\n-- Includes\\n--[[\\n  Add marker if needed when a job is available.\\n]]\\nlocal function addBaseMarkerIfNeeded(markerKey, isPausedOrMaxed)\\n  if not isPausedOrMaxed then\\n    rcall(\\\"ZADD\\\", markerKey, 0, \\\"0\\\")\\n  end  \\nend\\nlocal function addJobInTargetList(targetKey, markerKey, pushCmd, isPausedOrMaxed, jobId)\\n  rcall(pushCmd, targetKey, jobId)\\n  addBaseMarkerIfNeeded(markerKey, isPausedOrMaxed)\\nend\\n--[[\\n  Function to debounce a job.\\n]]\\nlocal function deduplicateJob(prefixKey, deduplicationOpts, jobId, deduplicationKey, eventsKey, maxEvents)\\n  local deduplicationId = deduplicationOpts and deduplicationOpts['id']\\n  if deduplicationId then\\n    local ttl = deduplicationOpts['ttl']\\n    local deduplicationKeyExists\\n    if ttl then\\n      deduplicationKeyExists = not rcall('SET', deduplicationKey, jobId, 'PX', ttl, 'NX')\\n    else\\n      deduplicationKeyExists = not rcall('SET', deduplicationKey, jobId, 'NX')\\n    end\\n    if deduplicationKeyExists then\\n      local currentDebounceJobId = rcall('GET', deduplicationKey)\\n      rcall(\\\"XADD\\\", eventsKey, \\\"MAXLEN\\\", \\\"~\\\", maxEvents, \\\"*\\\", \\\"event\\\",\\n        \\\"debounced\\\", \\\"jobId\\\", currentDebounceJobId, \\\"debounceId\\\", deduplicationId)\\n      rcall(\\\"XADD\\\", eventsKey, \\\"MAXLEN\\\", \\\"~\\\", maxEvents, \\\"*\\\", \\\"event\\\",\\n        \\\"deduplicated\\\", \\\"jobId\\\", currentDebounceJobId, \\\"deduplicationId\\\", deduplicationId)\\n      return currentDebounceJobId\\n    end\\n  end\\nend\\n--[[\\n  Function to get max events value or set by default 10000.\\n]]\\nlocal function getOrSetMaxEvents(metaKey)\\n    local maxEvents = rcall(\\\"HGET\\\", metaKey, \\\"opts.maxLenEvents\\\")\\n    if not maxEvents then\\n        maxEvents = 10000\\n        rcall(\\\"HSET\\\", metaKey, \\\"opts.maxLenEvents\\\", maxEvents)\\n    end\\n    return maxEvents\\nend\\n--[[\\n  Function to check for the meta.paused key to decide if we are paused or not\\n  (since an empty list and !EXISTS are not really the same).\\n]]\\nlocal function getTargetQueueList(queueMetaKey, activeKey, waitKey, pausedKey)\\n  local queueAttributes = rcall(\\\"HMGET\\\", queueMetaKey, \\\"paused\\\", \\\"concurrency\\\")\\n  if queueAttributes[1] then\\n    return pausedKey, true\\n  else\\n    if queueAttributes[2] then\\n      local activeCount = rcall(\\\"LLEN\\\", activeKey)\\n      if activeCount >= tonumber(queueAttributes[2]) then\\n        return waitKey, true\\n      else\\n        return waitKey, false\\n      end\\n    end\\n  end\\n  return waitKey, false\\nend\\n--[[\\n  Function to handle the case when job is duplicated.\\n]]\\n-- Includes\\n--[[\\n    This function is used to update the parent's dependencies if the job\\n    is already completed and about to be ignored. The parent must get its\\n    dependencies updated to avoid the parent job being stuck forever in \\n    the waiting-children state.\\n]]\\n-- Includes\\n--[[\\n  Validate and move or add dependencies to parent.\\n]]\\n-- Includes\\n--[[\\n  Validate and move parent to active if needed.\\n]]\\n-- Includes\\n--[[\\n  Add delay marker if needed.\\n]]\\n-- Includes\\n--[[\\n  Function to return the next delayed job timestamp.\\n]]\\nlocal function getNextDelayedTimestamp(delayedKey)\\n  local result = rcall(\\\"ZRANGE\\\", delayedKey, 0, 0, \\\"WITHSCORES\\\")\\n  if #result then\\n    local nextTimestamp = tonumber(result[2])\\n    if nextTimestamp ~= nil then \\n      return nextTimestamp / 0x1000\\n    end\\n  end\\nend\\nlocal function addDelayMarkerIfNeeded(markerKey, delayedKey)\\n  local nextTimestamp = getNextDelayedTimestamp(delayedKey)\\n  if nextTimestamp ~= nil then\\n    -- Replace the score of the marker with the newest known\\n    -- next timestamp.\\n    rcall(\\\"ZADD\\\", markerKey, nextTimestamp, \\\"1\\\")\\n  end\\nend\\n--[[\\n  Function to add job considering priority.\\n]]\\n-- Includes\\nlocal function addJobWithPriority(markerKey, prioritizedKey, priority, jobId, priorityCounterKey,\\n  isPausedOrMaxed)\\n  local prioCounter = rcall(\\\"INCR\\\", priorityCounterKey)\\n  local score = priority * 0x100000000 + prioCounter % 0x100000000\\n  rcall(\\\"ZADD\\\", prioritizedKey, score, jobId)\\n  addBaseMarkerIfNeeded(markerKey, isPausedOrMaxed)\\nend\\n--[[\\n  Function to check if queue is paused or maxed\\n  (since an empty list and !EXISTS are not really the same).\\n]]\\nlocal function isQueuePausedOrMaxed(queueMetaKey, activeKey)\\n  local queueAttributes = rcall(\\\"HMGET\\\", queueMetaKey, \\\"paused\\\", \\\"concurrency\\\")\\n  if queueAttributes[1] then\\n    return true\\n  else\\n    if queueAttributes[2] then\\n      local activeCount = rcall(\\\"LLEN\\\", activeKey)\\n      return activeCount >= tonumber(queueAttributes[2])\\n    end\\n  end\\n  return false\\nend\\nlocal function moveParentToWaitIfNeeded(parentQueueKey, parentDependenciesKey,\\n                                        parentKey, parentId, timestamp)\\n    local isParentActive = rcall(\\\"ZSCORE\\\",\\n                                 parentQueueKey .. \\\":waiting-children\\\", parentId)\\n    if rcall(\\\"SCARD\\\", parentDependenciesKey) == 0 and isParentActive then\\n        rcall(\\\"ZREM\\\", parentQueueKey .. \\\":waiting-children\\\", parentId)\\n        local parentWaitKey = parentQueueKey .. \\\":wait\\\"\\n        local parentPausedKey = parentQueueKey .. \\\":paused\\\"\\n        local parentActiveKey = parentQueueKey .. \\\":active\\\"\\n        local parentMetaKey = parentQueueKey .. \\\":meta\\\"\\n        local parentMarkerKey = parentQueueKey .. \\\":marker\\\"\\n        local jobAttributes = rcall(\\\"HMGET\\\", parentKey, \\\"priority\\\", \\\"delay\\\")\\n        local priority = tonumber(jobAttributes[1]) or 0\\n        local delay = tonumber(jobAttributes[2]) or 0\\n        if delay > 0 then\\n            local delayedTimestamp = tonumber(timestamp) + delay\\n            local score = delayedTimestamp * 0x1000\\n            local parentDelayedKey = parentQueueKey .. \\\":delayed\\\"\\n            rcall(\\\"ZADD\\\", parentDelayedKey, score, parentId)\\n            rcall(\\\"XADD\\\", parentQueueKey .. \\\":events\\\", \\\"*\\\", \\\"event\\\", \\\"delayed\\\",\\n                  \\\"jobId\\\", parentId, \\\"delay\\\", delayedTimestamp)\\n            addDelayMarkerIfNeeded(parentMarkerKey, parentDelayedKey)\\n        else\\n            if priority == 0 then\\n                local parentTarget, isParentPausedOrMaxed =\\n                    getTargetQueueList(parentMetaKey, parentActiveKey, parentWaitKey,\\n                                       parentPausedKey)\\n                addJobInTargetList(parentTarget, parentMarkerKey, \\\"RPUSH\\\", isParentPausedOrMaxed,\\n                    parentId)\\n            else\\n                local isPausedOrMaxed = isQueuePausedOrMaxed(parentMetaKey, parentActiveKey)\\n                addJobWithPriority(parentMarkerKey,\\n                                   parentQueueKey .. \\\":prioritized\\\", priority,\\n                                   parentId, parentQueueKey .. \\\":pc\\\", isPausedOrMaxed)\\n            end\\n            rcall(\\\"XADD\\\", parentQueueKey .. \\\":events\\\", \\\"*\\\", \\\"event\\\", \\\"waiting\\\",\\n                  \\\"jobId\\\", parentId, \\\"prev\\\", \\\"waiting-children\\\")\\n        end\\n    end\\nend\\nlocal function updateParentDepsIfNeeded(parentKey, parentQueueKey, parentDependenciesKey,\\n  parentId, jobIdKey, returnvalue, timestamp )\\n  local processedSet = parentKey .. \\\":processed\\\"\\n  rcall(\\\"HSET\\\", processedSet, jobIdKey, returnvalue)\\n  moveParentToWaitIfNeeded(parentQueueKey, parentDependenciesKey, parentKey, parentId, timestamp)\\nend\\nlocal function updateExistingJobsParent(parentKey, parent, parentData,\\n                                        parentDependenciesKey, completedKey,\\n                                        jobIdKey, jobId, timestamp)\\n    if parentKey ~= nil then\\n        if rcall(\\\"ZSCORE\\\", completedKey, jobId) ~= false then\\n            local returnvalue = rcall(\\\"HGET\\\", jobIdKey, \\\"returnvalue\\\")\\n            updateParentDepsIfNeeded(parentKey, parent['queueKey'],\\n                                     parentDependenciesKey, parent['id'],\\n                                     jobIdKey, returnvalue, timestamp)\\n        else\\n            if parentDependenciesKey ~= nil then\\n                rcall(\\\"SADD\\\", parentDependenciesKey, jobIdKey)\\n            end\\n        end\\n        rcall(\\\"HMSET\\\", jobIdKey, \\\"parentKey\\\", parentKey, \\\"parent\\\", parentData)\\n    end\\nend\\nlocal function handleDuplicatedJob(jobKey, jobId, currentParentKey, currentParent,\\n  parentData, parentDependenciesKey, completedKey, eventsKey, maxEvents, timestamp)\\n  local existedParentKey = rcall(\\\"HGET\\\", jobKey, \\\"parentKey\\\")\\n  if not existedParentKey or existedParentKey == currentParentKey then\\n    updateExistingJobsParent(currentParentKey, currentParent, parentData,\\n      parentDependenciesKey, completedKey, jobKey,\\n      jobId, timestamp)\\n  else\\n    if currentParentKey ~= nil and currentParentKey ~= existedParentKey\\n      and (rcall(\\\"EXISTS\\\", existedParentKey) == 1) then\\n      return -7\\n    end\\n  end\\n  rcall(\\\"XADD\\\", eventsKey, \\\"MAXLEN\\\", \\\"~\\\", maxEvents, \\\"*\\\", \\\"event\\\",\\n    \\\"duplicated\\\", \\\"jobId\\\", jobId)\\n  return jobId .. \\\"\\\" -- convert to string\\nend\\n--[[\\n  Function to store a job\\n]]\\nlocal function storeJob(eventsKey, jobIdKey, jobId, name, data, opts, timestamp,\\n                        parentKey, parentData, repeatJobKey)\\n    local jsonOpts = cjson.encode(opts)\\n    local delay = opts['delay'] or 0\\n    local priority = opts['priority'] or 0\\n    local debounceId = opts['de'] and opts['de']['id']\\n    local optionalValues = {}\\n    if parentKey ~= nil then\\n        table.insert(optionalValues, \\\"parentKey\\\")\\n        table.insert(optionalValues, parentKey)\\n        table.insert(optionalValues, \\\"parent\\\")\\n        table.insert(optionalValues, parentData)\\n    end\\n    if repeatJobKey ~= nil then\\n        table.insert(optionalValues, \\\"rjk\\\")\\n        table.insert(optionalValues, repeatJobKey)\\n    end\\n    if debounceId then\\n        table.insert(optionalValues, \\\"deid\\\")\\n        table.insert(optionalValues, debounceId)\\n    end\\n    rcall(\\\"HMSET\\\", jobIdKey, \\\"name\\\", name, \\\"data\\\", data, \\\"opts\\\", jsonOpts,\\n          \\\"timestamp\\\", timestamp, \\\"delay\\\", delay, \\\"priority\\\", priority,\\n          unpack(optionalValues))\\n    rcall(\\\"XADD\\\", eventsKey, \\\"*\\\", \\\"event\\\", \\\"added\\\", \\\"jobId\\\", jobId, \\\"name\\\", name)\\n    return delay, priority\\nend\\nif parentKey ~= nil then\\n    if rcall(\\\"EXISTS\\\", parentKey) ~= 1 then return -5 end\\n    parentData = cjson.encode(parent)\\nend\\nlocal jobCounter = rcall(\\\"INCR\\\", KEYS[4])\\nlocal metaKey = KEYS[3]\\nlocal maxEvents = getOrSetMaxEvents(metaKey)\\nlocal parentDependenciesKey = args[7]\\nlocal timestamp = args[4]\\nif args[2] == \\\"\\\" then\\n    jobId = jobCounter\\n    jobIdKey = args[1] .. jobId\\nelse\\n    jobId = args[2]\\n    jobIdKey = args[1] .. jobId\\n    if rcall(\\\"EXISTS\\\", jobIdKey) == 1 then\\n        return handleDuplicatedJob(jobIdKey, jobId, parentKey, parent,\\n            parentData, parentDependenciesKey, KEYS[5], eventsKey,\\n            maxEvents, timestamp)\\n    end\\nend\\nlocal deduplicationJobId = deduplicateJob(args[1], opts['de'],\\n  jobId, deduplicationKey, eventsKey, maxEvents)\\nif deduplicationJobId then\\n  return deduplicationJobId\\nend\\n-- Store the job.\\nstoreJob(eventsKey, jobIdKey, jobId, args[3], ARGV[2], opts, timestamp,\\n         parentKey, parentData, repeatJobKey)\\nlocal target, isPausedOrMaxed = getTargetQueueList(metaKey, KEYS[6], KEYS[1], KEYS[2])\\n-- LIFO or FIFO\\nlocal pushCmd = opts['lifo'] and 'RPUSH' or 'LPUSH'\\naddJobInTargetList(target, KEYS[8], pushCmd, isPausedOrMaxed, jobId)\\n-- Emit waiting event\\nrcall(\\\"XADD\\\", eventsKey, \\\"MAXLEN\\\", \\\"~\\\", maxEvents, \\\"*\\\", \\\"event\\\", \\\"waiting\\\",\\n      \\\"jobId\\\", jobId)\\n-- Check if this job is a child of another job, if so add it to the parents dependencies\\nif parentDependenciesKey ~= nil then\\n    rcall(\\\"SADD\\\", parentDependenciesKey, jobIdKey)\\nend\\nreturn jobId .. \\\"\\\" -- convert to string\\n\" \"8\" \"{b}:test-2ec5c4ed-f994-4d0b-9516-08c4f423d244:wait\" \"{b}:test-2ec5c4ed-f994-4d0b-9516-08c4f423d244:paused\" \"{b}:test-2ec5c4ed-f994-4d0b-9516-08c4f423d244:meta\" \"{b}:test-2ec5c4ed-f994-4d0b-9516-08c4f423d244:id\" \"{b}:test-2ec5c4ed-f994-4d0b-9516-08c4f423d244:completed\" \"{b}:test-2ec5c4ed-f994-4d0b-9516-08c4f423d244:active\" \"{b}:test-2ec5c4ed-f994-4d0b-9516-08c4f423d244:events\" \"{b}:test-2ec5c4ed-f994-4d0b-9516-08c4f423d244:marker\" \"\\x9a\\xd9.{b}:test-2ec5c4ed-f994-4d0b-9516-08c4f423d244:\\xa0\\xa4test\\xcbBy1\\xb0\\xbc<@\\x00\\xc0\\xc0\\xc0\\xc0\\xc0\\xc0\" \"{\\\"foo\\\":\\\"bar0\\\"}\" \"\\xde\\x00\\x04\\xa8attempts\\x00\\xa5jobId\\xc0\\xa2tm\\xc0\\xa7backoff\\xc0\"\r\nError: Server closed the connection\r\n```\r\n\r\nLet me know if you need more info.\r\n\r\n\n",
  "hints_text": "@manast  thanks for notifying us. Can you please remind me how to run the test suite?\nnm, figured this out from your actions:\r\n```\r\nyarn install --frozen-lockfile --non-interactive\r\nyarn build\r\nBULLMQ_TEST_PREFIX={b} yarn test\r\n```\r\nwith local dragonfly: `./dragonfly --dbfilename= --noversion_check --maxmemory=8G  --cluster_mode=emulated --lock_on_hashtags --logtostderr` \nHi there. This same issue broke our production :D Waiting for a fix. Reverting to `ghcr.io/dragonflydb/dragonfly:v1.24.0` works. Would probably be good to push a revert to 1.25.1 to fix deployments fetching the latest version.",
  "created_at": "2024-11-11T19:15:18Z",
  "modified_files": [
    "src/server/main_service.cc"
  ],
  "modified_test_files": [
    "tests/dragonfly/connection_test.py"
  ]
}