{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 3281,
  "instance_id": "dragonflydb__dragonfly-3281",
  "issue_numbers": [
    "3252"
  ],
  "base_commit": "ff5c89d0ad11f9b2a9786fdaa833a62f035e1e56",
  "patch": "diff --git a/src/server/db_slice.cc b/src/server/db_slice.cc\nindex 30bbed6f8066..2e815e47ff79 100644\n--- a/src/server/db_slice.cc\n+++ b/src/server/db_slice.cc\n@@ -708,14 +708,10 @@ void DbSlice::FlushSlots(cluster::SlotRanges slot_ranges) {\n }\n \n void DbSlice::FlushDbIndexes(const std::vector<DbIndex>& indexes) {\n-  // Async cleanup can only be performed if no tiered entries exist\n-  bool async_cleanup = true;\n-  for (DbIndex index : indexes) {\n-    async_cleanup &= db_arr_[index]->stats.tiered_entries == 0;\n-  }\n+  bool clear_tiered = owner_->tiered_storage() != nullptr;\n \n-  if (!async_cleanup)\n-    ClearEntriesOnFlush(indexes, db_arr_, false);\n+  if (clear_tiered)\n+    ClearOffloadedEntries(indexes, db_arr_);\n \n   DbTableArray flush_db_arr(db_arr_.size());\n   for (DbIndex index : indexes) {\n@@ -729,9 +725,7 @@ void DbSlice::FlushDbIndexes(const std::vector<DbIndex>& indexes) {\n   }\n \n   CHECK(fetched_items_.empty());\n-  auto cb = [this, async_cleanup, indexes, flush_db_arr = std::move(flush_db_arr)]() mutable {\n-    if (async_cleanup)\n-      ClearEntriesOnFlush(indexes, flush_db_arr, true);\n+  auto cb = [this, indexes, flush_db_arr = std::move(flush_db_arr)]() mutable {\n     flush_db_arr.clear();\n     ServerState::tlocal()->DecommitMemory(ServerState::kDataHeap | ServerState::kBackingHeap |\n                                           ServerState::kGlibcmalloc);\n@@ -1408,24 +1402,31 @@ void DbSlice::InvalidateSlotWatches(const cluster::SlotSet& slot_ids) {\n   }\n }\n \n-void DbSlice::ClearEntriesOnFlush(absl::Span<const DbIndex> indices, const DbTableArray& db_arr,\n-                                  bool async) {\n-  for (auto index : indices) {\n+void DbSlice::ClearOffloadedEntries(absl::Span<const DbIndex> indices, const DbTableArray& db_arr) {\n+  // Currently being used only for tiered storage.\n+  TieredStorage* tiered_storage = shard_owner()->tiered_storage();\n+  string scratch;\n+  for (DbIndex index : indices) {\n     const auto& db_ptr = db_arr[index];\n-    if (!db_ptr || db_ptr->stats.tiered_entries == 0)\n+    if (!db_ptr)\n       continue;\n \n     // Delete all tiered entries\n     PrimeTable::Cursor cursor;\n     do {\n       cursor = db_ptr->prime.Traverse(cursor, [&](PrimeIterator it) {\n-        if (it->second.IsExternal())\n-          PerformDeletion(it, db_ptr.get());\n+        if (it->second.IsExternal()) {\n+          tiered_storage->Delete(index, &it->second);\n+        } else if (it->second.HasIoPending()) {\n+          tiered_storage->CancelStash(index, it->first.GetSlice(&scratch), &it->second);\n+        }\n       });\n-    } while (cursor && db_ptr->stats.tiered_entries > 0);\n+    } while (cursor);\n \n-    // Wait for delete operations to finish in sync\n-    while (!async && db_ptr->stats.tiered_entries > 0) {\n+    // Wait for delete operations to finish in sync.\n+    // TODO: the logic inside tiered_storage that updates tiered_entries is somewhat fragile.\n+    // To revisit it, otherwise we may have deadlocks around this code.\n+    while (db_ptr->stats.tiered_entries > 0) {\n       LOG_EVERY_T(ERROR, 0.5) << \"Long wait for tiered entry delete on flush\";\n       ThisFiber::SleepFor(1ms);\n     }\ndiff --git a/src/server/db_slice.h b/src/server/db_slice.h\nindex 72c113820ba4..66b007cece47 100644\n--- a/src/server/db_slice.h\n+++ b/src/server/db_slice.h\n@@ -494,10 +494,8 @@ class DbSlice {\n   // Invalidate all watched keys for given slots. Used on FlushSlots.\n   void InvalidateSlotWatches(const cluster::SlotSet& slot_ids);\n \n-  // Properly clear db_arr before deleting it. If async is set, it's called from a detached fiber\n-  // after swapping the db.\n-  void ClearEntriesOnFlush(absl::Span<const DbIndex> indices, const DbTableArray& db_arr,\n-                           bool async);\n+  // Clear tiered storage entries for the specified indices.\n+  void ClearOffloadedEntries(absl::Span<const DbIndex> indices, const DbTableArray& db_arr);\n \n   void PerformDeletion(Iterator del_it, ExpIterator exp_it, DbTable* table);\n \ndiff --git a/src/server/tiered_storage.cc b/src/server/tiered_storage.cc\nindex 6989f929d4f8..79ae602f9684 100644\n--- a/src/server/tiered_storage.cc\n+++ b/src/server/tiered_storage.cc\n@@ -60,25 +60,11 @@ class TieredStorage::ShardOpManager : public tiering::OpManager {\n     cache_fetched_ = absl::GetFlag(FLAGS_tiered_storage_cache_fetched);\n   }\n \n-  // Called before overriding value with segment\n-  void RecordAdded(DbTableStats* stats, const PrimeValue& pv, tiering::DiskSegment segment) {\n-    stats->AddTypeMemoryUsage(pv.ObjType(), -pv.MallocUsed());\n-    stats->tiered_entries++;\n-    stats->tiered_used_bytes += segment.length;\n-  }\n-\n-  // Called after setting new value in place of previous segment\n-  void RecordDeleted(DbTableStats* stats, const PrimeValue& pv, tiering::DiskSegment segment) {\n-    stats->AddTypeMemoryUsage(pv.ObjType(), pv.MallocUsed());\n-    stats->tiered_entries--;\n-    stats->tiered_used_bytes -= segment.length;\n-  }\n-\n   // Find entry by key in db_slice and store external segment in place of original value.\n   // Update memory stats\n   void SetExternal(OpManager::KeyRef key, tiering::DiskSegment segment) {\n     if (auto pv = Find(key); pv) {\n-      RecordAdded(db_slice_->MutableStats(key.first), *pv, segment);\n+      RecordAdded(db_slice_->MutableStats(key.first), *pv, segment.length);\n \n       pv->SetIoPending(false);\n       pv->SetExternal(segment.offset, segment.length);\n@@ -113,9 +99,7 @@ class TieredStorage::ShardOpManager : public tiering::OpManager {\n     if (!value.empty())\n       pv->SetString(value);\n \n-    RecordDeleted(db_slice_->MutableStats(dbid), *pv, segment);\n-\n-    (value.empty() ? stats_.total_deletes : stats_.total_fetches)++;\n+    RecordDeleted(db_slice_->MutableStats(dbid), *pv, segment.length);\n   }\n \n   // Find entry by key and store it's up-to-date value in place of external segment.\n@@ -129,25 +113,7 @@ class TieredStorage::ShardOpManager : public tiering::OpManager {\n   }\n \n   // Load all values from bin by their hashes\n-  void Defragment(tiering::DiskSegment segment, string_view value) {\n-    // Note: Bin could've already been deleted, in that case DeleteBin returns an empty list\n-    for (auto [dbid, hash, sub_segment] : ts_->bins_->DeleteBin(segment, value)) {\n-      // Search for key with the same hash and value pointing to the same segment.\n-      // If it still exists, it must correspond to the value stored in this bin\n-      auto predicate = [sub_segment = sub_segment](const PrimeKey& key, const PrimeValue& probe) {\n-        return probe.IsExternal() && tiering::DiskSegment{probe.GetExternalSlice()} == sub_segment;\n-      };\n-      auto it = db_slice_->GetDBTable(dbid)->prime.FindFirst(hash, predicate);\n-      if (!IsValid(it))\n-        continue;\n-\n-      stats_.total_defrags++;\n-\n-      // Cut out relevant part of value and restore it to memory\n-      string_view sub_value = value.substr(sub_segment.offset - segment.offset, sub_segment.length);\n-      SetInMemory(&it->second, dbid, sub_value, sub_segment);\n-    }\n-  }\n+  void Defragment(tiering::DiskSegment segment, string_view value);\n \n   void ReportStashed(EntryId id, tiering::DiskSegment segment, error_code ec) override {\n     if (ec) {\n@@ -159,21 +125,7 @@ class TieredStorage::ShardOpManager : public tiering::OpManager {\n   }\n \n   bool ReportFetched(EntryId id, string_view value, tiering::DiskSegment segment,\n-                     bool modified) override {\n-    if (id == EntryId{kFragmentedBin}) {  // Generally we read whole bins only for defrag\n-      Defragment(segment, value);\n-      return true;  // delete\n-    }\n-\n-    if (!modified && !cache_fetched_)\n-      return false;\n-\n-    if (SliceSnapshot::IsSnaphotInProgress())\n-      return false;\n-\n-    SetInMemory(get<OpManager::KeyRef>(id), value, segment);\n-    return true;\n-  }\n+                     bool modified) override;\n \n   bool ReportDelete(tiering::DiskSegment segment) override {\n     if (OccupiesWholePages(segment.length))\n@@ -203,17 +155,70 @@ class TieredStorage::ShardOpManager : public tiering::OpManager {\n     return IsValid(it) ? &it->second : nullptr;\n   }\n \n+  // Called before overriding value with segment\n+  void RecordAdded(DbTableStats* stats, const PrimeValue& pv, size_t tiered_len) {\n+    stats->AddTypeMemoryUsage(pv.ObjType(), -pv.MallocUsed());\n+    stats->tiered_entries++;\n+    stats->tiered_used_bytes += tiered_len;\n+  }\n+\n+  // Called after setting new value in place of previous segment\n+  void RecordDeleted(DbTableStats* stats, const PrimeValue& pv, size_t tiered_len) {\n+    stats->AddTypeMemoryUsage(pv.ObjType(), pv.MallocUsed());\n+    stats->tiered_entries--;\n+    stats->tiered_used_bytes -= tiered_len;\n+  }\n+\n   bool cache_fetched_ = false;\n \n   struct {\n-    size_t total_stashes = 0, total_fetches = 0, total_cancels = 0, total_deletes = 0;\n-    size_t total_defrags = 0;  // included in total_fetches\n+    size_t total_stashes = 0, total_cancels = 0, total_fetches = 0;\n+    size_t total_defrags = 0;\n   } stats_;\n \n   TieredStorage* ts_;\n   DbSlice* db_slice_;\n };\n \n+void TieredStorage::ShardOpManager::Defragment(tiering::DiskSegment segment, string_view value) {\n+  // Note: Bin could've already been deleted, in that case DeleteBin returns an empty list\n+  for (auto [dbid, hash, sub_segment] : ts_->bins_->DeleteBin(segment, value)) {\n+    // Search for key with the same hash and value pointing to the same segment.\n+    // If it still exists, it must correspond to the value stored in this bin\n+    auto predicate = [sub_segment = sub_segment](const PrimeKey& key, const PrimeValue& probe) {\n+      return probe.IsExternal() && tiering::DiskSegment{probe.GetExternalSlice()} == sub_segment;\n+    };\n+    auto it = db_slice_->GetDBTable(dbid)->prime.FindFirst(hash, predicate);\n+    if (!IsValid(it))\n+      continue;\n+\n+    stats_.total_defrags++;\n+\n+    // Cut out relevant part of value and restore it to memory\n+    string_view sub_value = value.substr(sub_segment.offset - segment.offset, sub_segment.length);\n+    SetInMemory(&it->second, dbid, sub_value, sub_segment);\n+  }\n+}\n+\n+bool TieredStorage::ShardOpManager::ReportFetched(EntryId id, string_view value,\n+                                                  tiering::DiskSegment segment, bool modified) {\n+  ++stats_.total_fetches;\n+\n+  if (id == EntryId{kFragmentedBin}) {  // Generally we read whole bins only for defrag\n+    Defragment(segment, value);\n+    return true;  // delete\n+  }\n+\n+  if (!modified && !cache_fetched_)\n+    return false;\n+\n+  if (SliceSnapshot::IsSnaphotInProgress())\n+    return false;\n+\n+  SetInMemory(get<OpManager::KeyRef>(id), value, segment);\n+  return true;\n+}\n+\n TieredStorage::TieredStorage(DbSlice* db_slice, size_t max_size)\n     : op_manager_{make_unique<ShardOpManager>(this, db_slice, max_size)},\n       bins_{make_unique<tiering::SmallBins>()} {\n@@ -276,7 +281,7 @@ bool TieredStorage::TryStash(DbIndex dbid, string_view key, PrimeValue* value) {\n     return false;\n \n   // This invariant should always hold because ShouldStash tests for IoPending flag.\n-  CHECK(!bins_->IsPending(dbid, key));\n+  DCHECK(!bins_->IsPending(dbid, key));\n \n   // TODO: When we are low on memory we should introduce a back-pressure, to avoid OOMs\n   // with a lot of underutilized disk space.\n@@ -310,9 +315,11 @@ bool TieredStorage::TryStash(DbIndex dbid, string_view key, PrimeValue* value) {\n \n void TieredStorage::Delete(DbIndex dbid, PrimeValue* value) {\n   DCHECK(value->IsExternal());\n+  ++stats_.total_deletes;\n+\n   tiering::DiskSegment segment = value->GetExternalSlice();\n-  op_manager_->Delete(segment);\n-  op_manager_->SetInMemory(value, dbid, \"\", segment);\n+  op_manager_->DeleteOffloaded(segment);\n+  op_manager_->SetInMemory(value, dbid, string_view{}, segment);\n }\n \n void TieredStorage::CancelStash(DbIndex dbid, std::string_view key, PrimeValue* value) {\ndiff --git a/src/server/tiered_storage.h b/src/server/tiered_storage.h\nindex 31daa41a07e9..c0c00a26afb9 100644\n--- a/src/server/tiered_storage.h\n+++ b/src/server/tiered_storage.h\n@@ -85,6 +85,7 @@ class TieredStorage {\n   unsigned write_depth_limit_ = 10;\n   struct {\n     uint64_t stash_overflow_cnt = 0;\n+    uint64_t total_deletes = 0;\n   } stats_;\n };\n \ndiff --git a/src/server/tiering/op_manager.cc b/src/server/tiering/op_manager.cc\nindex 808368134a20..0c8785bbb832 100644\n--- a/src/server/tiering/op_manager.cc\n+++ b/src/server/tiering/op_manager.cc\n@@ -59,15 +59,16 @@ void OpManager::Delete(EntryId id) {\n   pending_stash_ver_.erase(ToOwned(id));\n }\n \n-void OpManager::Delete(DiskSegment segment) {\n-  EntryOps* pending_op = nullptr;\n+void OpManager::DeleteOffloaded(DiskSegment segment) {\n+  EntryOps* pending_read = nullptr;\n \n   auto base_it = pending_reads_.find(segment.ContainingPages().offset);\n   if (base_it != pending_reads_.end())\n-    pending_op = base_it->second.Find(segment);\n+    pending_read = base_it->second.Find(segment);\n \n-  if (pending_op) {\n-    pending_op->deleting = true;\n+  if (pending_read) {\n+    // Mark that the read operation must finilize with deletion.\n+    pending_read->deleting = true;\n   } else if (ReportDelete(segment) && base_it == pending_reads_.end()) {\n     storage_.MarkAsFree(segment.ContainingPages());\n   }\ndiff --git a/src/server/tiering/op_manager.h b/src/server/tiering/op_manager.h\nindex 3a5cc1ca1c6a..853e05bc749f 100644\n--- a/src/server/tiering/op_manager.h\n+++ b/src/server/tiering/op_manager.h\n@@ -54,8 +54,8 @@ class OpManager {\n   // Delete entry with pending io\n   void Delete(EntryId id);\n \n-  // Delete offloaded entry\n-  void Delete(DiskSegment segment);\n+  // Delete offloaded entry located at the segment.\n+  void DeleteOffloaded(DiskSegment segment);\n \n   // Stash value to be offloaded\n   std::error_code Stash(EntryId id, std::string_view value);\n",
  "test_patch": "diff --git a/src/server/dragonfly_test.cc b/src/server/dragonfly_test.cc\nindex aa15589359f5..a96be4b28e89 100644\n--- a/src/server/dragonfly_test.cc\n+++ b/src/server/dragonfly_test.cc\n@@ -362,6 +362,13 @@ TEST_F(DflyEngineTest, MemcacheFlags) {\n   ASSERT_EQ(resp, \"OK\");\n   MCResponse resp2 = RunMC(MP::GET, \"key\");\n   EXPECT_THAT(resp2, ElementsAre(\"VALUE key 42 3\", \"bar\", \"END\"));\n+\n+  ASSERT_EQ(Run(\"resp\", {\"flushdb\"}), \"OK\");\n+  pp_->AwaitFiberOnAll([](auto*) {\n+    if (auto* shard = EngineShard::tlocal(); shard) {\n+      EXPECT_EQ(shard->db_slice().GetDBTable(0)->mcflag.size(), 0u);\n+    }\n+  });\n }\n \n TEST_F(DflyEngineTest, LimitMemory) {\ndiff --git a/src/server/tiered_storage_test.cc b/src/server/tiered_storage_test.cc\nindex 909f053c3792..f97f097d89d2 100644\n--- a/src/server/tiered_storage_test.cc\n+++ b/src/server/tiered_storage_test.cc\n@@ -30,6 +30,13 @@ ABSL_DECLARE_FLAG(unsigned, tiered_storage_write_depth);\n \n namespace dfly {\n \n+using absl::GetFlag;\n+using absl::SetFlag;\n+\n+string BuildString(size_t len, char c = 'A') {\n+  return string(len, c);\n+}\n+\n class TieredStorageTest : public BaseFamilyTest {\n  protected:\n   TieredStorageTest() {\n@@ -37,15 +44,17 @@ class TieredStorageTest : public BaseFamilyTest {\n   }\n \n   void SetUp() override {\n-    if (absl::GetFlag(FLAGS_force_epoll)) {\n+    if (GetFlag(FLAGS_force_epoll)) {\n       LOG(WARNING) << \"Can't run tiered tests on EPOLL\";\n       exit(0);\n     }\n \n-    absl::SetFlag(&FLAGS_tiered_storage_write_depth, 15000);\n-    absl::SetFlag(&FLAGS_tiered_prefix, \"/tmp/tiered_storage_test\");\n-    absl::SetFlag(&FLAGS_tiered_storage_cache_fetched, true);\n-    absl::SetFlag(&FLAGS_backing_file_direct, true);\n+    SetFlag(&FLAGS_tiered_storage_write_depth, 15000);\n+    if (GetFlag(FLAGS_tiered_prefix).empty()) {\n+      SetFlag(&FLAGS_tiered_prefix, \"/tmp/tiered_storage_test\");\n+    }\n+    SetFlag(&FLAGS_tiered_storage_cache_fetched, true);\n+    SetFlag(&FLAGS_backing_file_direct, true);\n \n     BaseFamilyTest::SetUp();\n   }\n@@ -54,13 +63,13 @@ class TieredStorageTest : public BaseFamilyTest {\n // Perform simple series of SET, GETSET and GET\n TEST_F(TieredStorageTest, SimpleGetSet) {\n   absl::FlagSaver saver;\n-  absl::SetFlag(&FLAGS_tiered_offload_threshold, 1.1f);  // disable offloading\n+  SetFlag(&FLAGS_tiered_offload_threshold, 1.1f);  // disable offloading\n   const int kMin = 256;\n   const int kMax = tiering::kPageSize + 10;\n \n   // Perform SETs\n   for (size_t i = kMin; i < kMax; i++) {\n-    Run({\"SET\", absl::StrCat(\"k\", i), string(i, 'A')});\n+    Run({\"SET\", absl::StrCat(\"k\", i), BuildString(i)});\n   }\n \n   // Make sure all entries were stashed, except the one not filling a small page\n@@ -113,18 +122,18 @@ TEST_F(TieredStorageTest, SimpleAppend) {\n   // TODO: use pipelines to issue APPEND/GET/APPEND sequence,\n   // currently it's covered only for op_manager_test\n   for (size_t sleep : {0, 100, 500, 1000}) {\n-    Run({\"SET\", \"k0\", string(3000, 'A')});\n+    Run({\"SET\", \"k0\", BuildString(3000)});\n     if (sleep)\n       util::ThisFiber::SleepFor(sleep * 1us);\n     EXPECT_THAT(Run({\"APPEND\", \"k0\", \"B\"}), IntArg(3001));\n-    EXPECT_EQ(Run({\"GET\", \"k0\"}), string(3000, 'A') + 'B');\n+    EXPECT_EQ(Run({\"GET\", \"k0\"}), BuildString(3000) + 'B');\n   }\n }\n \n TEST_F(TieredStorageTest, MultiDb) {\n   for (size_t i = 0; i < 10; i++) {\n     Run({\"SELECT\", absl::StrCat(i)});\n-    Run({\"SET\", absl::StrCat(\"k\", i), string(3000, char('A' + i))});\n+    Run({\"SET\", absl::StrCat(\"k\", i), BuildString(3000, char('A' + i))});\n   }\n \n   ExpectConditionWithinTimeout([this] { return GetMetrics().tiered_stats.total_stashes >= 10; });\n@@ -132,7 +141,7 @@ TEST_F(TieredStorageTest, MultiDb) {\n   for (size_t i = 0; i < 10; i++) {\n     Run({\"SELECT\", absl::StrCat(i)});\n     EXPECT_EQ(GetMetrics().db_stats[i].tiered_entries, 1);\n-    EXPECT_EQ(Run({\"GET\", absl::StrCat(\"k\", i)}), string(3000, char('A' + i)));\n+    EXPECT_EQ(Run({\"GET\", absl::StrCat(\"k\", i)}), BuildString(3000, char('A' + i)));\n     EXPECT_EQ(GetMetrics().db_stats[i].tiered_entries, 0);\n   }\n }\n@@ -168,7 +177,7 @@ TEST_F(TieredStorageTest, Defrag) {\n \n TEST_F(TieredStorageTest, BackgroundOffloading) {\n   absl::FlagSaver saver;\n-  absl::SetFlag(&FLAGS_tiered_offload_threshold, 0.0f);  // offload all values\n+  SetFlag(&FLAGS_tiered_offload_threshold, 0.0f);  // offload all values\n \n   const int kNum = 500;\n \n@@ -177,7 +186,7 @@ TEST_F(TieredStorageTest, BackgroundOffloading) {\n \n   // Stash all values\n   for (size_t i = 0; i < kNum; i++) {\n-    Run({\"SET\", absl::StrCat(\"k\", i), string(3000, 'A')});\n+    Run({\"SET\", absl::StrCat(\"k\", i), BuildString(3000)});\n   }\n \n   ExpectConditionWithinTimeout([&] { return GetMetrics().db_stats[0].tiered_entries == kNum; });\n@@ -200,11 +209,11 @@ TEST_F(TieredStorageTest, BackgroundOffloading) {\n \n TEST_F(TieredStorageTest, FlushAll) {\n   absl::FlagSaver saver;\n-  absl::SetFlag(&FLAGS_tiered_offload_threshold, 0.0f);  // offload all values\n+  SetFlag(&FLAGS_tiered_offload_threshold, 0.0f);  // offload all values\n \n   const int kNum = 500;\n   for (size_t i = 0; i < kNum; i++) {\n-    Run({\"SET\", absl::StrCat(\"k\", i), string(3000, 'A')});\n+    Run({\"SET\", absl::StrCat(\"k\", i), BuildString(3000)});\n   }\n   ExpectConditionWithinTimeout([&] { return GetMetrics().db_stats[0].tiered_entries == kNum; });\n \n@@ -228,4 +237,18 @@ TEST_F(TieredStorageTest, FlushAll) {\n   EXPECT_GT(metrics.tiered_stats.total_fetches, 2u);\n }\n \n+TEST_F(TieredStorageTest, FlushPending) {\n+  absl::FlagSaver saver;\n+  SetFlag(&FLAGS_tiered_offload_threshold, 0.0f);  // offload all values\n+\n+  const int kNum = 10;\n+  for (size_t i = 0; i < kNum; i++) {\n+    Run({\"SET\", absl::StrCat(\"k\", i), BuildString(256)});\n+  }\n+  ExpectConditionWithinTimeout(\n+      [&] { return GetMetrics().tiered_stats.small_bins_filling_bytes > 0; });\n+  Run({\"FLUSHALL\"});\n+  EXPECT_EQ(GetMetrics().tiered_stats.small_bins_filling_bytes, 0u);\n+}\n+\n }  // namespace dfly\ndiff --git a/src/server/tiering/op_manager_test.cc b/src/server/tiering/op_manager_test.cc\nindex c8aed570ff47..ec589e21a920 100644\n--- a/src/server/tiering/op_manager_test.cc\n+++ b/src/server/tiering/op_manager_test.cc\n@@ -100,7 +100,7 @@ TEST_F(OpManagerTest, DeleteAfterReads) {\n     std::vector<util::fb2::Future<std::string>> reads;\n     for (unsigned i = 0; i < 100; i++)\n       reads.emplace_back(Read(0u, stashed_[0u]));\n-    Delete(stashed_[0u]);\n+    DeleteOffloaded(stashed_[0u]);\n \n     for (auto& fut : reads)\n       EXPECT_EQ(fut.Get(), \"DATA\");\ndiff --git a/tests/dragonfly/conftest.py b/tests/dragonfly/conftest.py\nindex 926dd50734ab..49e06f134df7 100644\n--- a/tests/dragonfly/conftest.py\n+++ b/tests/dragonfly/conftest.py\n@@ -19,7 +19,7 @@\n from copy import deepcopy\n \n from pathlib import Path\n-from tempfile import TemporaryDirectory, gettempdir\n+from tempfile import gettempdir, mkdtemp\n \n from .instance import DflyInstance, DflyParams, DflyInstanceFactory, RedisServer\n from . import PortPicker, dfly_args\n@@ -37,9 +37,12 @@ def tmp_dir():\n     where the Dragonfly executable will be run and where all test data\n     should be stored. The directory will be cleaned up at the end of a session\n     \"\"\"\n-    tmp = TemporaryDirectory()\n-    yield Path(tmp.name)\n-    tmp.cleanup()\n+    tmp_name = mkdtemp()\n+    yield Path(tmp_name)\n+    if os.environ.get(\"DRAGONFLY_KEEP_TMP\"):\n+        logging.info(f\"Keeping tmp dir {tmp_name}\")\n+        return\n+    shutil.rmtree(tmp_name, ignore_errors=True)\n \n \n @pytest.fixture(scope=\"session\")\ndiff --git a/tests/dragonfly/instance.py b/tests/dragonfly/instance.py\nindex ed102c0c9c66..50ce582080b0 100644\n--- a/tests/dragonfly/instance.py\n+++ b/tests/dragonfly/instance.py\n@@ -215,6 +215,8 @@ def _check_status(self):\n         if not self.params.existing_port:\n             return_code = self.proc.poll()\n             if return_code is not None:\n+                # log stdout of the failed process\n+                logging.error(\"Dragonfly process error:\\n%s\", self.proc.stdout.read().decode())\n                 self.proc = None\n                 raise DflyStartException(f\"Failed to start instance, return code {return_code}\")\n \n",
  "problem_statement": "check-fail in tiering\n```\r\n38685\u279c Check failure stack trace: ***\r\n38685\u279c    @     0x556a9fa3585f  google::LogMessageFatal::~LogMessageFatal()\r\n38685\u279c    @     0x556a9f85e5b8  dfly::tiering::SmallBins::Stash()\r\n38685\u279c    @     0x556a9f7192a6  dfly::TieredStorage::Stash()\r\n38685\u279c    @     0x556a9f719846  dfly::TieredStorage::RunOffloading()\r\n38685\u279c    @     0x556a9f76dd90  dfly::EngineShard::Heartbeat()\r\n38685\u279c    @     0x556a9f76ee60  dfly::EngineShard::RunPeriodic()\r\n```\r\nsource https://github.com/dragonflydb/dragonfly/actions/runs/9745085780/job/26892171819#step:12:658\n",
  "hints_text": "Already pinged @dranikpg internally :) \nHe reassigned to me \ud83e\udd37\ud83c\udffc \r\n",
  "created_at": "2024-07-07T14:41:48Z",
  "modified_files": [
    "src/server/db_slice.cc",
    "src/server/db_slice.h",
    "src/server/tiered_storage.cc",
    "src/server/tiered_storage.h",
    "src/server/tiering/op_manager.cc",
    "src/server/tiering/op_manager.h"
  ],
  "modified_test_files": [
    "src/server/dragonfly_test.cc",
    "src/server/tiered_storage_test.cc",
    "src/server/tiering/op_manager_test.cc",
    "tests/dragonfly/conftest.py",
    "tests/dragonfly/instance.py"
  ]
}