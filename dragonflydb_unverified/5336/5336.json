{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 5336,
  "instance_id": "dragonflydb__dragonfly-5336",
  "issue_numbers": [
    "4965"
  ],
  "base_commit": "f32d18ef2b4e5b81de46a1ce7370cbbeb6c077b1",
  "patch": "diff --git a/src/server/snapshot.cc b/src/server/snapshot.cc\nindex b809f8364ea7..732b27badd49 100644\n--- a/src/server/snapshot.cc\n+++ b/src/server/snapshot.cc\n@@ -104,6 +104,7 @@ void SliceSnapshot::Start(bool stream_journal, SnapshotFlush allow_flush) {\n }\n \n void SliceSnapshot::StartIncremental(LSN start_lsn) {\n+  VLOG(1) << \"StartIncremental: \" << start_lsn;\n   serializer_ = std::make_unique<RdbSerializer>(compression_mode_);\n \n   snapshot_fb_ = fb2::Fiber(\"incremental_snapshot\",\n@@ -319,7 +320,6 @@ size_t SliceSnapshot::FlushSerialized(SerializerBase::FlushState flush_state) {\n   uint64_t running_cycles = ThisFiber::GetRunningTimeCycles();\n \n   fb2::NoOpLock lk;\n-\n   // We create a critical section here that ensures that records are pushed in sequential order.\n   // As a result, it is not possible for two fiber producers to push concurrently.\n   // If A.id = 5, and then B.id = 6, and both are blocked here, it means that last_pushed_id_ < 4.\n@@ -356,7 +356,10 @@ bool SliceSnapshot::PushSerialized(bool force) {\n     // Async bucket serialization might have accumulated some delayed values.\n     // Because we can finally block in this function, we'll await and serialize them\n     do {\n-      auto& entry = delayed_entries_.back();\n+      // We may call PushSerialized from multiple fibers concurrently, so we need to\n+      // ensure that we are not serializing the same entry concurrently.\n+      DelayedEntry entry = std::move(delayed_entries_.back());\n+      delayed_entries_.pop_back();\n \n       // TODO: https://github.com/dragonflydb/dragonfly/issues/4654\n       // there are a few problems with how we serialize external values.\n@@ -367,7 +370,6 @@ bool SliceSnapshot::PushSerialized(bool force) {\n \n       // TODO: to introduce RdbSerializer::SaveString that can accept a string value directly.\n       serializer_->SaveEntry(entry.key, pv, entry.expire, entry.mc_flags, entry.dbid);\n-      delayed_entries_.pop_back();\n     } while (!delayed_entries_.empty());\n \n     // blocking point.\ndiff --git a/src/server/snapshot.h b/src/server/snapshot.h\nindex c49ae9f5f7ed..b84e757f7e9d 100644\n--- a/src/server/snapshot.h\n+++ b/src/server/snapshot.h\n@@ -68,7 +68,7 @@ class SliceSnapshot : public journal::JournalConsumerInterface {\n \n   // Initialize snapshot, start bucket iteration fiber, register listeners.\n   // In journal streaming mode it needs to be stopped by either Stop or Cancel.\n-  enum class SnapshotFlush { kAllow, kDisallow };\n+  enum class SnapshotFlush : uint8_t { kAllow, kDisallow };\n \n   void Start(bool stream_journal, SnapshotFlush allow_flush = SnapshotFlush::kDisallow);\n \n",
  "test_patch": "diff --git a/tests/dragonfly/replication_test.py b/tests/dragonfly/replication_test.py\nindex 265f0ea96050..7b9196158e5d 100644\n--- a/tests/dragonfly/replication_test.py\n+++ b/tests/dragonfly/replication_test.py\n@@ -26,14 +26,6 @@\n M_NOT_EPOLL = [pytest.mark.exclude_epoll]\n \n \n-async def wait_for_replicas_state(*clients, state=\"online\", node_role=\"slave\", timeout=0.05):\n-    \"\"\"Wait until all clients (replicas) reach passed state\"\"\"\n-    while len(clients) > 0:\n-        await asyncio.sleep(timeout)\n-        roles = await asyncio.gather(*(c.role() for c in clients))\n-        clients = [c for c, role in zip(clients, roles) if role[0] != node_role or role[3] != state]\n-\n-\n \"\"\"\n Test full replication pipeline. Test full sync with streaming changes and stable state streaming.\n \"\"\"\ndiff --git a/tests/dragonfly/tiering_test.py b/tests/dragonfly/tiering_test.py\nindex 45b74ec17daf..2384d20a86b8 100644\n--- a/tests/dragonfly/tiering_test.py\n+++ b/tests/dragonfly/tiering_test.py\n@@ -1,14 +1,15 @@\n import async_timeout\n import asyncio\n import itertools\n+import logging\n import pytest\n import random\n import redis.asyncio as aioredis\n \n from . import dfly_args\n from .seeder import DebugPopulateSeeder\n-from .utility import info_tick_timer\n-\n+from .utility import info_tick_timer, wait_for_replicas_state\n+from .instance import DflyInstanceFactory\n \n BASIC_ARGS = {\"port\": 6379, \"proactor_threads\": 4, \"tiered_prefix\": \"/tmp/tiered/backing\"}\n \n@@ -87,3 +88,35 @@ async def run(sub_ops):\n     res = await p.execute()\n \n     assert res == [10 * k for k in key_range]\n+\n+\n+@pytest.mark.exclude_epoll\n+@pytest.mark.opt_only\n+@dfly_args(\n+    {\n+        \"proactor_threads\": 2,\n+        \"tiered_prefix\": \"/tmp/tiered/backing_master\",\n+        \"maxmemory\": \"4G\",\n+        \"cache_mode\": True,\n+        \"tiered_offload_threshold\": \"0.2\",\n+        \"tiered_storage_write_depth\": 100,\n+    }\n+)\n+async def test_full_sync(async_client: aioredis.Redis, df_factory: DflyInstanceFactory):\n+    replica = df_factory.create(\n+        proactor_threads=2,\n+        cache_mode=True,\n+        maxmemory=\"4G\",\n+        tiered_prefix=\"/tmp/tiered/backing_replica\",\n+        tiered_offload_threshold=\"0.2\",\n+        tiered_storage_write_depth=1000,\n+    )\n+    replica.start()\n+    replica_client = replica.client()\n+    await async_client.execute_command(\"debug\", \"populate\", \"3000000\", \"key\", \"2000\")\n+    await replica_client.replicaof(\n+        \"localhost\", async_client.connection_pool.connection_kwargs[\"port\"]\n+    )\n+    logging.info(\"Waiting for replica to sync\")\n+    async with async_timeout.timeout(120):\n+        await wait_for_replicas_state(replica_client)\ndiff --git a/tests/dragonfly/utility.py b/tests/dragonfly/utility.py\nindex 5e6fe38c6d01..36f28c763af6 100644\n--- a/tests/dragonfly/utility.py\n+++ b/tests/dragonfly/utility.py\n@@ -848,3 +848,11 @@ def extract_int_after_prefix(prefix, line):\n     match = re.search(prefix + \"(\\\\d+)\", line)\n     assert match\n     return int(match.group(1))\n+\n+\n+async def wait_for_replicas_state(*clients, state=\"online\", node_role=\"slave\", timeout=0.05):\n+    \"\"\"Wait until all clients (replicas) reach passed state\"\"\"\n+    while len(clients) > 0:\n+        await asyncio.sleep(timeout)\n+        roles = await asyncio.gather(*(c.role() for c in clients))\n+        clients = [c for c, role in zip(clients, roles) if role[0] != node_role or role[3] != state]\n",
  "problem_statement": "SSD-tier: full sync replication crash or hang master\n**Describe the bug**\nmake full sync replication from a ssd-tier master node (no replica) will crash or hang it.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. run a single master node (no replica) with ssd-tier opening.\n2. write 20,000,000 ~ 30,000,000 to master, string type kv, key len 32 and value 2KB about that will consume 36G ssd storage.\n3. start another node (same config as master).\n4. replicaof (master ip:port).\n5. run not long time, the master will crash or hang.\n6. if hang, operation on the replica will output: `(error) LOADING Dragonfly is loading the dataset in memory`\n7. if crash, the master stderr output: \n> *** SIGFPE received at time=1745151458 on cpu 2 ***\nPC: @     0x5b216091c605  (unknown)  mi_free_generic_mt\nor\n>  *** SIGSEGV received at time=1745156745 on cpu 1 ***\nPC: @     0x569e90c3df04  (unknown)  util::fb2::EventCount::await<>()\n\n**Expected behavior**\nfull sync normally then replica transfer to partial replication and working well\n\n**Screenshots**\n\n**Environment (please complete the following information):**\n - OS: [ubuntu 24.04]\n - hang Kernel: `Linux ubuntu2404152192 6.8.0-57-generic #59-Ubuntu SMP PREEMPT_DYNAMIC Sat Mar 15 17:40:59 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux`  \n - crash Kernal: `6.6.72+ #1 SMP PREEMPT_DYNAMIC Sun Mar 30 09:01:26 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux`\n - Containerized?: [hang testing in IDC Metal's QEMU, crash testing in google cloud vm ]\n - Dragonfly Version: [1.28.2]\n\n**Reproducible Code Snippet**\n```\n# Minimal code snippet to reproduce this bug\n```\n\n**Additional context**\n\n[flags-6679.txt](https://github.com/user-attachments/files/19825644/flags-6679.txt)\n\n",
  "hints_text": "@zhyhang \nThanks for reporting the issue.\nI reproduced it locally with master in debug or opt mode. easily reproduces. \n\n```sh\nmkdir /tmp/logmaster /tmp/logslave\n```\n\n## Master\n\nconfig:\n```\n# Dragonfly Configuration File\n\n--port=6380\n--maxmemory=10G\n# Enable cache mode (evict keys when near maxmemory)\n--cache_mode=true\n--proactor_threads=4\n\n# Directory to store log files\n--log_dir=/tmp/logmaster\n--dbfilename=\n--tiered_prefix=/mnt/vol/tiered_master\n--tiered_offload_threshold=0.2\n--cluster_mode=emulated\n```\n\nand then `debug POPULATE 5000000 key 4096`\n\n## Slave\nslave config:\n```\n--port=6381\n--maxmemory=10G\n--cache_mode=true\n--proactor_threads=4\n\n# Directory to store log files\n--log_dir=/tmp/logslave\n--dbfilename=\n--tiered_prefix=/mnt/vol/tiered_slave\n--tiered_offload_threshold=0.2\n--cluster_mode=emulated\n```\n\nRun it via:  `taskset -c 4-8 ./dragonfly --flagfile ~/slave.conf` \n\nand then `slaveof localhost 6380`\n\n\nOnce the CPU goes down - `info replication` on slave shows its still in full sync.\n`info replication` on master - is stuck.\n\n`debug stacktrace` dumps the following (pasted only interesting bits):\n\n```\nI20250620 08:59:20.315248 51328 scheduler.cc:487] ------------ Fiber Dispatched (suspended:523947ms) ------------\n0xb2ed88e95c10  std::function<>::operator()()\n0xb2ed88e9362c  util::fb2::detail::FiberInterface::ExecuteOnFiberStack()::{lambda()#1}::operator()()\n0xb2ed88e948b4  boost::context::detail::fiber_ontop<>()\n0xb2ed88e941f8  boost::context::fiber::resume_with<>()\n0xb2ed88e92e6c  util::fb2::detail::FiberInterface::SwitchTo()\n0xb2ed88e883b8  util::fb2::detail::Scheduler::Preempt()\n0xb2ed88166eb8  util::fb2::detail::FiberInterface::Suspend()\n0xb2ed881675a8  util::fb2::EventCount::wait()\n0xb2ed887a202c  util::fb2::EventCount::await<>()\n0xb2ed887a17e8  util::fb2::SharedMutex::lock()\n0xb2ed88a8d434  std::lock_guard<>::lock_guard()\n0xb2ed88a8b58c  dfly::journal::JournalSlice::UnregisterOnChange()\n0xb2ed88a7af6c  dfly::journal::Journal::UnregisterOnChange()\n0xb2ed8836f6d0  dfly::SliceSnapshot::FinalizeJournalStream()\n0xb2ed88340cac  dfly::RdbSaver::Impl::StopSnapshotting()\n\nI20250620 08:59:20.261518 51328 scheduler.cc:487] ------------ Fiber shard_handler_periodic1 (suspended:523892ms) ---\n---------\n0xb2ed88e95c10  std::function<>::operator()()\n0xb2ed88e9362c  util::fb2::detail::FiberInterface::ExecuteOnFiberStack()::{lambda()#1}::operator()()\n0xb2ed88e948b4  boost::context::detail::fiber_ontop<>()\n0xb2ed88e941f8  boost::context::fiber::resume_with<>()\n0xb2ed88e92e6c  util::fb2::detail::FiberInterface::SwitchTo()\n0xb2ed88e883b8  util::fb2::detail::Scheduler::Preempt()\n0xb2ed88166eb8  util::fb2::detail::FiberInterface::Suspend()\n0xb2ed881675a8  util::fb2::EventCount::wait()\n0xb2ed887a2150  util::fb2::EventCount::await<>()\n0xb2ed887a18b8  util::fb2::SharedMutex::lock_shared()\n0xb2ed8888f03c  dfly::SharedLock<>::SharedLock()\n0xb2ed88870548  dfly::DflyCmd::BreakStalledFlowsInShard()\n0xb2ed8825cd08  dfly::Service::Init()::{lambda()#1}::operator()()\n0xb2ed8827d948  std::__invoke_impl<>()\n0xb2ed882795a0  std::__invoke_r<>()\n\n\nI20250620 08:59:20.261500 51328 scheduler.cc:487] ------------ Fiber heartbeat_periodic1 (suspended:532681ms) -------\n-----\n0xb2ed88e95c10  std::function<>::operator()()\n0xb2ed88e9362c  util::fb2::detail::FiberInterface::ExecuteOnFiberStack()::{lambda()#1}::operator()()\n0xb2ed88e948b4  boost::context::detail::fiber_ontop<>()\n0xb2ed88e941f8  boost::context::fiber::resume_with<>()\n0xb2ed88e92e6c  util::fb2::detail::FiberInterface::SwitchTo()\n0xb2ed88e883b8  util::fb2::detail::Scheduler::Preempt()\n0xb2ed88166eb8  util::fb2::detail::FiberInterface::Suspend()\n0xb2ed881675a8  util::fb2::EventCount::wait()\n0xb2ed8821b81c  util::fb2::EventCount::await<>()\n0xb2ed88215c30  util::fb2::Future<>::Get()\n0xb2ed88371914  dfly::SliceSnapshot::PushSerialized()\n0xb2ed883720e0  dfly::SliceSnapshot::ThrottleIfNeeded()\n0xb2ed88a8ab78  dfly::journal::JournalSlice::SetFlushMode()\n0xb2ed88a7b11c  dfly::journal::Journal::SetFlushMode()\n0xb2ed882120bc  dfly::journal::JournalFlushGuard::~JournalFlushGuard()\n\nI20250620 08:59:20.244653 51329 scheduler.cc:487] ------------ Fiber Dispatched (suspended:523877ms) ------------\n0xb2ed88e95c10  std::function<>::operator()()\n0xb2ed88e9362c  util::fb2::detail::FiberInterface::ExecuteOnFiberStack()::{lambda()#1}::operator()()\n0xb2ed88e948b4  boost::context::detail::fiber_ontop<>()\n0xb2ed88e941f8  boost::context::fiber::resume_with<>()\n0xb2ed88e92e6c  util::fb2::detail::FiberInterface::SwitchTo()\n0xb2ed88e883b8  util::fb2::detail::Scheduler::Preempt()\n0xb2ed88166eb8  util::fb2::detail::FiberInterface::Suspend()\n0xb2ed881675a8  util::fb2::EventCount::wait()\n0xb2ed887a202c  util::fb2::EventCount::await<>()\n0xb2ed887a17e8  util::fb2::SharedMutex::lock()\n0xb2ed88a8d434  std::lock_guard<>::lock_guard()\n0xb2ed88a8b58c  dfly::journal::JournalSlice::UnregisterOnChange()\n0xb2ed88a7af6c  dfly::journal::Journal::UnregisterOnChange()\n0xb2ed8836f6d0  dfly::SliceSnapshot::FinalizeJournalStream()\n0xb2ed88340cac  dfly::RdbSaver::Impl::StopSnapshotting()\n\nI20250620 08:59:20.108386 51327 scheduler.cc:487] ------------ Fiber heartbeat_periodic0 (suspended:531536ms) -------\n-----\n0xb2ed88e95c10  std::function<>::operator()()\n0xb2ed88e9362c  util::fb2::detail::FiberInterface::ExecuteOnFiberStack()::{lambda()#1}::operator()()\n0xb2ed88e948b4  boost::context::detail::fiber_ontop<>()\n0xb2ed88e941f8  boost::context::fiber::resume_with<>()\n0xb2ed88e92e6c  util::fb2::detail::FiberInterface::SwitchTo()\n0xb2ed88e883b8  util::fb2::detail::Scheduler::Preempt()\n0xb2ed88166eb8  util::fb2::detail::FiberInterface::Suspend()\n0xb2ed881675a8  util::fb2::EventCount::wait()\n0xb2ed8821b81c  util::fb2::EventCount::await<>()\n0xb2ed88215c30  util::fb2::Future<>::Get()\n0xb2ed88371914  dfly::SliceSnapshot::PushSerialized()\n0xb2ed883720e0  dfly::SliceSnapshot::ThrottleIfNeeded()\n0xb2ed88a8ab78  dfly::journal::JournalSlice::SetFlushMode()\n0xb2ed88a7b11c  dfly::journal::Journal::SetFlushMode()\n0xb2ed882120bc  dfly::journal::JournalFlushGuard::~JournalFlushGuard()\n```\n\n",
  "created_at": "2025-06-21T09:38:22Z",
  "modified_files": [
    "src/server/snapshot.cc",
    "src/server/snapshot.h"
  ],
  "modified_test_files": [
    "tests/dragonfly/replication_test.py",
    "tests/dragonfly/tiering_test.py",
    "tests/dragonfly/utility.py"
  ]
}