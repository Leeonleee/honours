{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 926,
  "instance_id": "dragonflydb__dragonfly-926",
  "issue_numbers": [
    "924"
  ],
  "base_commit": "5c57e4efe2eead02b5409f6d1988c5952ffcd4d4",
  "patch": "diff --git a/src/facade/dragonfly_connection.cc b/src/facade/dragonfly_connection.cc\nindex 969862b4275b..eff1079cdbbd 100644\n--- a/src/facade/dragonfly_connection.cc\n+++ b/src/facade/dragonfly_connection.cc\n@@ -87,7 +87,7 @@ constexpr size_t kMaxReadSize = 32_KB;\n struct PubMsgRecord {\n   Connection::PubMessage pub_msg;\n \n-  PubMsgRecord(const Connection::PubMessage& pmsg) : pub_msg(pmsg) {\n+  PubMsgRecord(Connection::PubMessage pmsg) : pub_msg(move(pmsg)) {\n   }\n };\n \n@@ -149,7 +149,7 @@ class Connection::Request {\n   static RequestPtr New(mi_heap_t* heap, const RespVec& args, size_t capacity);\n \n   // Overload to create a new pubsub message\n-  static RequestPtr New(const PubMessage& pub_msg);\n+  static RequestPtr New(PubMessage pub_msg);\n \n   // Overload to create a new the monitor message\n   static RequestPtr New(MonitorMessage msg);\n@@ -221,13 +221,13 @@ void Connection::Request::SetArgs(const RespVec& args) {\n   }\n }\n \n-Connection::RequestPtr Connection::Request::New(const PubMessage& pub_msg) {\n+Connection::RequestPtr Connection::Request::New(PubMessage pub_msg) {\n   // This will generate a new request for pubsub message\n   // Please note that unlike the above case, we don't need to \"protect\", the internals here\n   // since we are currently using a borrow token for it - i.e. the BlockingCounter will\n   // ensure that the message is not deleted until we are finish sending it at the other\n   // side of the queue\n-  PubMsgRecord new_msg{pub_msg};\n+  PubMsgRecord new_msg{move(pub_msg)};\n   void* ptr = mi_malloc(sizeof(Request));\n   Request* req = new (ptr) Request(std::move(new_msg));\n   return Connection::RequestPtr{req, Connection::RequestDeleter{}};\n@@ -275,17 +275,26 @@ void Connection::DispatchOperations::operator()(const PubMsgRecord& msg) {\n   ++stats->async_writes_cnt;\n   const PubMessage& pub_msg = msg.pub_msg;\n   string_view arr[4];\n-  if (pub_msg.pattern.empty()) {\n-    arr[0] = \"message\";\n-    arr[1] = pub_msg.channel;\n-    arr[2] = *pub_msg.message;\n-    rbuilder->SendStringArr(absl::Span<string_view>{arr, 3});\n+  if (pub_msg.type == PubMessage::kPublish) {\n+    if (pub_msg.pattern.empty()) {\n+      DVLOG(1) << \"Sending message, from channel: \" << *pub_msg.channel << \" \" << *pub_msg.message;\n+      arr[0] = \"message\";\n+      arr[1] = *pub_msg.channel;\n+      arr[2] = *pub_msg.message;\n+      rbuilder->SendStringArr(absl::Span<string_view>{arr, 3});\n+    } else {\n+      arr[0] = \"pmessage\";\n+      arr[1] = pub_msg.pattern;\n+      arr[2] = *pub_msg.channel;\n+      arr[3] = *pub_msg.message;\n+      rbuilder->SendStringArr(absl::Span<string_view>{arr, 4});\n+    }\n   } else {\n-    arr[0] = \"pmessage\";\n-    arr[1] = pub_msg.pattern;\n-    arr[2] = pub_msg.channel;\n-    arr[3] = *pub_msg.message;\n-    rbuilder->SendStringArr(absl::Span<string_view>{arr, 4});\n+    const char* action[2] = {\"unsubscribe\", \"subscribe\"};\n+    rbuilder->StartArray(3);\n+    rbuilder->SendBulkString(action[pub_msg.type == PubMessage::kSubscribe]);\n+    rbuilder->SendBulkString(*pub_msg.channel);\n+    rbuilder->SendLong(pub_msg.channel_cnt);\n   }\n }\n \n@@ -445,13 +454,14 @@ void Connection::RegisterOnBreak(BreakerCb breaker_cb) {\n   breaker_cb_ = breaker_cb;\n }\n \n-void Connection::SendMsgVecAsync(const PubMessage& pub_msg) {\n+void Connection::SendMsgVecAsync(PubMessage pub_msg) {\n   DCHECK(cc_);\n \n   if (cc_->conn_closing) {\n     return;\n   }\n-  RequestPtr req = Request::New(pub_msg);  // new (ptr) Request(0, 0);\n+\n+  RequestPtr req = Request::New(move(pub_msg));\n   dispatch_q_.push_back(std::move(req));\n   if (dispatch_q_.size() == 1) {\n     evc_.notify();\ndiff --git a/src/facade/dragonfly_connection.h b/src/facade/dragonfly_connection.h\nindex 2fcfd55e6995..2836a6d0c0b2 100644\n--- a/src/facade/dragonfly_connection.h\n+++ b/src/facade/dragonfly_connection.h\n@@ -54,14 +54,21 @@ class Connection : public util::Connection {\n \n   struct PubMessage {\n     // if empty - means its a regular message, otherwise it's pmessage.\n-    std::string_view pattern;\n-    std::string_view channel;\n-    std::shared_ptr<const std::string> message;  // ensure that this message would out live passing\n-                                                 // between different threads/fibers\n+    std::string pattern;\n+    std::shared_ptr<std::string> channel;\n+    std::shared_ptr<std::string> message;  // ensure that this message would out live passing\n+                                           // between different threads/fibers\n+    enum Type { kSubscribe, kUnsubscribe, kPublish } type;\n+    uint32_t channel_cnt;  // relevant only for kSubscribe and kUnsubscribe\n+\n+    PubMessage() = default;\n+    PubMessage(const PubMessage&) = delete;\n+    PubMessage& operator=(const PubMessage&) = delete;\n+    PubMessage(PubMessage&&) = default;\n   };\n \n   // this function is overriden at test_utils TestConnection\n-  virtual void SendMsgVecAsync(const PubMessage& pub_msg);\n+  virtual void SendMsgVecAsync(PubMessage pub_msg);\n \n   // Note that this is accepted by value because the message is processed asynchronously.\n   void SendMonitorMsg(std::string monitor_msg);\ndiff --git a/src/server/conn_context.cc b/src/server/conn_context.cc\nindex 6d54f80e1073..474d1e47ad0d 100644\n--- a/src/server/conn_context.cc\n+++ b/src/server/conn_context.cc\n@@ -135,16 +135,13 @@ void ConnectionContext::ChangeSubscription(bool to_add, bool to_reply, CmdArgLis\n   }\n \n   if (to_reply) {\n-    const char* action[2] = {\"unsubscribe\", \"subscribe\"};\n-\n+    using PubMessage = facade::Connection::PubMessage;\n     for (size_t i = 0; i < result.size(); ++i) {\n-      (*this)->StartArray(3);\n-      (*this)->SendBulkString(action[to_add]);\n-      (*this)->SendBulkString(ArgS(args, i));  // channel\n-\n-      // number of subscribed channels for this connection *right after*\n-      // we subscribe.\n-      (*this)->SendLong(result[i]);\n+      PubMessage msg;\n+      msg.type = to_add ? PubMessage::kSubscribe : PubMessage::kUnsubscribe;\n+      msg.channel = make_shared<string>(ArgS(args, i));\n+      msg.channel_cnt = result[i];\n+      owner()->SendMsgVecAsync(move(msg));\n     }\n   }\n }\ndiff --git a/src/server/main_service.cc b/src/server/main_service.cc\nindex 2d6b7203c5e1..7dcf9e39acc5 100644\n--- a/src/server/main_service.cc\n+++ b/src/server/main_service.cc\n@@ -1366,9 +1366,6 @@ void Service::Exec(CmdArgList args, ConnectionContext* cntx) {\n void Service::Publish(CmdArgList args, ConnectionContext* cntx) {\n   string_view channel = ArgS(args, 1);\n \n-  // shared_ptr ensures that the message lives until it's been sent to all subscribers and handled\n-  // by DispatchOperations.\n-  std::shared_ptr<const std::string> message = std::make_shared<const std::string>(ArgS(args, 2));\n   ShardId sid = Shard(channel, shard_count());\n \n   auto cb = [&] { return EngineShard::tlocal()->channel_slice().FetchSubscribers(channel); };\n@@ -1390,12 +1387,18 @@ void Service::Publish(CmdArgList args, ConnectionContext* cntx) {\n       }\n     }\n \n+    // shared_ptr ensures that the message lives until it's been sent to all subscribers and handled\n+    // by DispatchOperations.\n+    shared_ptr<string> msg_ptr = make_shared<string>(ArgS(args, 2));\n+    shared_ptr<string> channel_ptr = make_shared<string>(channel);\n+    using PubMessage = facade::Connection::PubMessage;\n+\n     // We run publish_cb in each subscriber's thread.\n     auto publish_cb = [&](unsigned idx, util::ProactorBase*) mutable {\n       unsigned start = slices[idx];\n \n       for (unsigned i = start; i < subscriber_arr.size(); ++i) {\n-        const ChannelSlice::Subscriber& subscriber = subscriber_arr[i];\n+        ChannelSlice::Subscriber& subscriber = subscriber_arr[i];\n         if (subscriber.thread_id != idx)\n           break;\n \n@@ -1403,11 +1406,14 @@ void Service::Publish(CmdArgList args, ConnectionContext* cntx) {\n \n         facade::Connection* conn = subscriber_arr[i].conn_cntx->owner();\n         DCHECK(conn);\n-        facade::Connection::PubMessage pmsg;\n-        pmsg.channel = channel;\n-        pmsg.message = message;\n-        pmsg.pattern = subscriber.pattern;\n-        conn->SendMsgVecAsync(pmsg);\n+\n+        PubMessage pmsg;\n+        pmsg.channel = channel_ptr;\n+        pmsg.message = msg_ptr;\n+        pmsg.pattern = move(subscriber.pattern);\n+        pmsg.type = PubMessage::kPublish;\n+\n+        conn->SendMsgVecAsync(move(pmsg));\n       }\n     };\n \n",
  "test_patch": "diff --git a/src/server/dragonfly_test.cc b/src/server/dragonfly_test.cc\nindex ca350d99819e..ffcd2d60e3a5 100644\n--- a/src/server/dragonfly_test.cc\n+++ b/src/server/dragonfly_test.cc\n@@ -398,9 +398,9 @@ TEST_F(DflyEngineTest, PSubscribe) {\n \n   ASSERT_EQ(1, SubscriberMessagesLen(\"IO1\"));\n \n-  facade::Connection::PubMessage msg = GetPublishedMessage(\"IO1\", 0);\n+  const facade::Connection::PubMessage& msg = GetPublishedMessage(\"IO1\", 0);\n   EXPECT_EQ(\"foo\", *msg.message);\n-  EXPECT_EQ(\"ab\", msg.channel);\n+  EXPECT_EQ(\"ab\", *msg.channel);\n   EXPECT_EQ(\"a*\", msg.pattern);\n }\n \ndiff --git a/src/server/test_utils.cc b/src/server/test_utils.cc\nindex 59b36856b1f7..da688c0a9470 100644\n--- a/src/server/test_utils.cc\n+++ b/src/server/test_utils.cc\n@@ -58,23 +58,21 @@ static vector<string> SplitLines(const std::string& src) {\n   return res;\n }\n \n-TestConnection::TestConnection(Protocol protocol)\n-    : facade::Connection(protocol, nullptr, nullptr, nullptr) {\n+TestConnection::TestConnection(Protocol protocol, io::StringSink* sink)\n+    : facade::Connection(protocol, nullptr, nullptr, nullptr), sink_(sink) {\n }\n \n-void TestConnection::SendMsgVecAsync(const PubMessage& pmsg) {\n-  backing_str_.emplace_back(new string(pmsg.channel));\n-  PubMessage dest;\n-  dest.channel = *backing_str_.back();\n-\n-  backing_str_.emplace_back(new string(*pmsg.message));\n-  dest.message = pmsg.message;\n-\n-  if (!pmsg.pattern.empty()) {\n-    backing_str_.emplace_back(new string(pmsg.pattern));\n-    dest.pattern = *backing_str_.back();\n+void TestConnection::SendMsgVecAsync(PubMessage pmsg) {\n+  if (pmsg.type == PubMessage::kPublish) {\n+    messages.push_back(move(pmsg));\n+  } else {\n+    RedisReplyBuilder builder(sink_);\n+    const char* action[2] = {\"unsubscribe\", \"subscribe\"};\n+    builder.StartArray(3);\n+    builder.SendBulkString(action[pmsg.type == PubMessage::kSubscribe]);\n+    builder.SendBulkString(*pmsg.channel);\n+    builder.SendLong(pmsg.channel_cnt);\n   }\n-  messages.push_back(dest);\n }\n \n class BaseFamilyTest::TestConnWrapper {\n@@ -87,7 +85,7 @@ class BaseFamilyTest::TestConnWrapper {\n   RespVec ParseResponse(bool fully_consumed);\n \n   // returns: type(pmessage), pattern, channel, message.\n-  facade::Connection::PubMessage GetPubMessage(size_t index) const;\n+  const facade::Connection::PubMessage& GetPubMessage(size_t index) const;\n \n   ConnectionContext* cmd_cntx() {\n     return &cmd_cntx_;\n@@ -117,7 +115,7 @@ class BaseFamilyTest::TestConnWrapper {\n };\n \n BaseFamilyTest::TestConnWrapper::TestConnWrapper(Protocol proto)\n-    : dummy_conn_(new TestConnection(proto)), cmd_cntx_(&sink_, dummy_conn_.get()) {\n+    : dummy_conn_(new TestConnection(proto, &sink_)), cmd_cntx_(&sink_, dummy_conn_.get()) {\n }\n \n BaseFamilyTest::TestConnWrapper::~TestConnWrapper() {\n@@ -359,7 +357,8 @@ RespVec BaseFamilyTest::TestConnWrapper::ParseResponse(bool fully_consumed) {\n   return res;\n }\n \n-facade::Connection::PubMessage BaseFamilyTest::TestConnWrapper::GetPubMessage(size_t index) const {\n+const facade::Connection::PubMessage& BaseFamilyTest::TestConnWrapper::GetPubMessage(\n+    size_t index) const {\n   CHECK_LT(index, dummy_conn_->messages.size());\n   return dummy_conn_->messages[index];\n }\n@@ -389,13 +388,10 @@ size_t BaseFamilyTest::SubscriberMessagesLen(string_view conn_id) const {\n   return it->second->conn()->messages.size();\n }\n \n-facade::Connection::PubMessage BaseFamilyTest::GetPublishedMessage(string_view conn_id,\n-                                                                   size_t index) const {\n-  facade::Connection::PubMessage res;\n-\n+const facade::Connection::PubMessage& BaseFamilyTest::GetPublishedMessage(string_view conn_id,\n+                                                                          size_t index) const {\n   auto it = connections_.find(conn_id);\n-  if (it == connections_.end())\n-    return res;\n+  CHECK(it != connections_.end());\n \n   return it->second->GetPubMessage(index);\n }\ndiff --git a/src/server/test_utils.h b/src/server/test_utils.h\nindex e7d2db24cee2..e2634db1556f 100644\n--- a/src/server/test_utils.h\n+++ b/src/server/test_utils.h\n@@ -19,14 +19,14 @@ using namespace facade;\n \n class TestConnection : public facade::Connection {\n  public:\n-  TestConnection(Protocol protocol);\n+  TestConnection(Protocol protocol, io::StringSink* sink);\n \n-  void SendMsgVecAsync(const PubMessage& pmsg) final;\n+  void SendMsgVecAsync(PubMessage pmsg) final;\n \n   std::vector<PubMessage> messages;\n \n  private:\n-  std::vector<std::unique_ptr<std::string>> backing_str_;\n+  io::StringSink* sink_;\n };\n \n class BaseFamilyTest : public ::testing::Test {\n@@ -87,9 +87,8 @@ class BaseFamilyTest : public ::testing::Test {\n   std::string GetId() const;\n   size_t SubscriberMessagesLen(std::string_view conn_id) const;\n \n-  // Returns message parts as returned by RESP:\n-  // pmessage, pattern, channel, message\n-  facade::Connection::PubMessage GetPublishedMessage(std::string_view conn_id, size_t index) const;\n+  const facade::Connection::PubMessage& GetPublishedMessage(std::string_view conn_id,\n+                                                            size_t index) const;\n \n   std::unique_ptr<util::ProactorPool> pp_;\n   std::unique_ptr<Service> service_;\ndiff --git a/tests/README.md b/tests/README.md\nindex e3db30f6e80e..dbb8af28f669 100644\n--- a/tests/README.md\n+++ b/tests/README.md\n@@ -38,6 +38,10 @@ pip install -r dragonfly/requirements.txt\n to run pytest, run:\n `pytest -xv dragonfly`\n \n+to run selectively, use:\n+`pytest -xv dragonfly -k <substring>`\n+For more pytest flags [check here](https://fig.io/manual/pytest).\n+\n ## Writing tests\n The [Getting Started](https://docs.pytest.org/en/7.1.x/getting-started.html) guide is a great resource to become familiar with writing pytest test cases.\n \ndiff --git a/tests/dragonfly/connection_test.py b/tests/dragonfly/connection_test.py\nindex 58008012abfd..ffc014e4a01e 100644\n--- a/tests/dragonfly/connection_test.py\n+++ b/tests/dragonfly/connection_test.py\n@@ -4,6 +4,8 @@\n import aioredis\n import async_timeout\n \n+from . import DflyInstance\n+\n \n async def run_monitor_eval(monitor, expected):\n     async with monitor as mon:\n@@ -278,6 +280,42 @@ def generate(max):\n     assert state, message\n \n \n+@pytest.mark.asyncio\n+async def test_subsribers_with_active_publisher(df_server: DflyInstance, max_connections=100):\n+    # TODO: I am not how to customize the max connections for the pool.\n+    async_pool = aioredis.ConnectionPool(host=\"localhost\", port=df_server.port,\n+                                         db=0, decode_responses=True, max_connections=max_connections)\n+\n+    async def publish_worker():\n+        client = aioredis.Redis(connection_pool=async_pool)\n+        for i in range(0, 2000):\n+            await client.publish(\"channel\", f\"message-{i}\")\n+        await client.close()\n+\n+    async def channel_reader(channel: aioredis.client.PubSub):\n+        for i in range(0, 150):\n+            try:\n+                async with async_timeout.timeout(1):\n+                    message = await channel.get_message(ignore_subscribe_messages=True)\n+            except asyncio.TimeoutError:\n+                break\n+\n+    async def subscribe_worker():\n+        client = aioredis.Redis(connection_pool=async_pool)\n+        pubsub = client.pubsub()\n+        async with pubsub as p:\n+            await pubsub.subscribe(\"channel\")\n+            await channel_reader(pubsub)\n+            await pubsub.unsubscribe(\"channel\")\n+\n+    # Create a publisher that sends constantly messages to the channel\n+    # Then create subscribers that will subscribe to already active channel\n+    pub_task = asyncio.create_task(publish_worker())\n+    await asyncio.gather(*(subscribe_worker() for _ in range(max_connections - 10)))\n+    await pub_task\n+    await async_pool.disconnect()\n+\n+\n @pytest.mark.asyncio\n async def test_big_command(df_server, size=8 * 1024):\n     reader, writer = await asyncio.open_connection('127.0.0.1', df_server.port)\n",
  "problem_statement": "Response Race On Subscription\nResponses from DragonflyDB corrupt sometimes when subscribing to a channel that encounters a publish at the same time.\r\n\r\nThe following line dump shows how a subscription count is lost or overwritten by the message (array) that follows.\r\n\r\n[lost-subs-count.txt](https://github.com/dragonflydb/dragonfly/files/10944753/lost-subs-count.txt)\r\n\r\nSpecifically, the response was:\r\n\r\n```resp\r\n*3\r\n$9\r\nsubscribe\r\n$28\r\nchannel-18126908833894131297\r\n*3\r\n$7\r\nmessage\r\n\u2026\r\n```\r\n\r\n\u2026 while it should have been:\r\n\r\n```resp\r\n*3\r\n$9\r\nsubscribe\r\n$28\r\nchannel-18126908833894131297\r\n:1\r\n*3\r\n$7\r\nmessage\r\n\u2026\r\n```\r\n\r\n\u2026, with \":1\" included.\r\n\r\nThis is not the only scenario. I just got lucky while recording. Sometimes a redundant \"*3\" appears before another message array, as in \"*3\\r\\n*3\\r\\n$7message\u2026\". Followup on #316.\n",
  "hints_text": "Can reproduce with the following patch to github.com/pascaldekloe/redis.\r\n\r\n```diff\r\ndiff --git a/client.go b/client.go\r\nindex 6207e01..ee5f694 100644\r\n--- a/client.go\r\n+++ b/client.go\r\n@@ -2,6 +2,8 @@ package redis\r\n \r\n import (\r\n        \"bufio\"\r\n+\"os\"\r\n+\"io\"\r\n        \"errors\"\r\n        \"fmt\"\r\n        \"net\"\r\n@@ -470,7 +472,12 @@ func (c *ClientConfig) connect(readBufferSize int) (net.Conn, *bufio.Reader, err\r\n                tcp.SetNoDelay(false)\r\n                tcp.SetLinger(0)\r\n        }\r\n-       reader := bufio.NewReaderSize(conn, readBufferSize)\r\n+\r\n+f, err := os.OpenFile(fmt.Sprintf(\"linedump-%x\", &network), os.O_WRONLY|os.O_CREATE|os.O_EXCL|os.O_APPEND|os.O_SYNC, 0o600)\r\n+if err != nil {\r\n+       panic(err)\r\n+}\r\n+       reader := bufio.NewReaderSize(io.TeeReader(conn, f), readBufferSize)\r\n \r\n        // apply sticky settings\r\n        if c.Password != nil {\r\n```\r\n\r\n\u2026 and then repeat `go test -run 'Subscribe$' && rm linedump-*` until failure. Needs about a dozen tries.\nThe redundant array length just happend again on v0.17.0: `redis: protocol violation; received \"*3\\r\\n*3\\r\\n$7\\r\\nmess\"`",
  "created_at": "2023-03-11T16:36:25Z",
  "modified_files": [
    "src/facade/dragonfly_connection.cc",
    "src/facade/dragonfly_connection.h",
    "src/server/conn_context.cc",
    "src/server/main_service.cc"
  ],
  "modified_test_files": [
    "src/server/dragonfly_test.cc",
    "src/server/test_utils.cc",
    "src/server/test_utils.h",
    "tests/README.md",
    "tests/dragonfly/connection_test.py"
  ]
}