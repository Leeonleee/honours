{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 2531,
  "instance_id": "dragonflydb__dragonfly-2531",
  "issue_numbers": [
    "2519"
  ],
  "base_commit": "562aa151a7b1f8c1903637c90de1ab6aae5052ee",
  "patch": "diff --git a/src/server/journal/tx_executor.cc b/src/server/journal/tx_executor.cc\nindex 58f92e1edc24..521873975f60 100644\n--- a/src/server/journal/tx_executor.cc\n+++ b/src/server/journal/tx_executor.cc\n@@ -119,7 +119,7 @@ std::optional<TransactionData> TransactionReader::NextTxData(JournalReader* read\n \n     // Otherwise, continue building multi command.\n     DCHECK(res->opcode == journal::Op::MULTI_COMMAND || res->opcode == journal::Op::EXEC);\n-    DCHECK(res->txid > 0);\n+    DCHECK(res->txid > 0 || res->shard_cnt == 1);\n \n     auto txid = res->txid;\n     auto& txdata = current_[txid];\ndiff --git a/src/server/main_service.cc b/src/server/main_service.cc\nindex bd1f3b3c1a86..268e81dbec84 100644\n--- a/src/server/main_service.cc\n+++ b/src/server/main_service.cc\n@@ -1850,6 +1850,7 @@ void Service::EvalInternal(CmdArgList args, const EvalArgs& eval_args, Interpret\n       cntx->transaction = stub_tx.get();\n \n       result = interpreter->RunFunction(eval_args.sha, &error);\n+      cntx->transaction->FIX_ConcludeJournalExec();  // flush journal\n \n       cntx->transaction = tx;\n       return OpStatus::OK;\ndiff --git a/src/server/transaction.cc b/src/server/transaction.cc\nindex eff17ce1e80a..2ff3c4b5bc51 100644\n--- a/src/server/transaction.cc\n+++ b/src/server/transaction.cc\n@@ -188,7 +188,9 @@ Transaction::Transaction(const Transaction* parent, ShardId shard_id, std::optio\n     // Use squashing mechanism for inline execution of single-shard EVAL\n     multi_->mode = LOCK_AHEAD;\n   }\n+\n   multi_->role = SQUASHED_STUB;\n+  multi_->shard_journal_write.resize(1);\n \n   time_now_ms_ = parent->time_now_ms_;\n \n@@ -966,6 +968,16 @@ const absl::flat_hash_set<std::string_view>& Transaction::GetMultiKeys() const {\n   return multi_->frozen_keys_set;\n }\n \n+void Transaction::FIX_ConcludeJournalExec() {\n+  if (!multi_->shard_journal_write.front())\n+    return;\n+\n+  if (auto journal = EngineShard::tlocal()->journal(); journal != nullptr) {\n+    journal->RecordEntry(txid_, journal::Op::EXEC, db_index_, 1,\n+                         unique_slot_checker_.GetUniqueSlotId(), {}, false);\n+  }\n+}\n+\n void Transaction::EnableShard(ShardId sid) {\n   unique_shard_cnt_ = 1;\n   unique_shard_id_ = sid;\n@@ -1464,8 +1476,13 @@ void Transaction::LogJournalOnShard(EngineShard* shard, journal::Entry::Payload&\n                                     bool allow_await) const {\n   auto journal = shard->journal();\n   CHECK(journal);\n-  if (multi_ && multi_->role != SQUASHED_STUB)\n-    multi_->shard_journal_write[shard->shard_id()] = true;\n+\n+  if (multi_) {\n+    if (multi_->role != SQUASHED_STUB)\n+      multi_->shard_journal_write[shard->shard_id()] = true;\n+    else\n+      multi_->shard_journal_write[0] = true;\n+  }\n \n   bool is_multi = multi_commands || IsAtomicMulti();\n \n@@ -1486,9 +1503,8 @@ void Transaction::FinishLogJournalOnShard(EngineShard* shard, uint32_t shard_cnt\n \n void Transaction::CancelBlocking(std::function<OpStatus(ArgSlice)> status_cb) {\n   // We're on the owning thread of this transaction, so we can safely access it's data below.\n-  // We still need to claim the blocking barrier, but as this function is often called blindly, we\n-  // want to check first if it makes sense to even proceed.\n-  if (blocking_barrier_.IsClaimed())\n+  // First, check if it makes sense to proceed.\n+  if (blocking_barrier_.IsClaimed() || cid_ == nullptr || (cid_->opt_mask() & CO::BLOCKING) == 0)\n     return;\n \n   OpStatus status = OpStatus::CANCELLED;\ndiff --git a/src/server/transaction.h b/src/server/transaction.h\nindex 15710d1c7f06..0fd0fd3e46d7 100644\n--- a/src/server/transaction.h\n+++ b/src/server/transaction.h\n@@ -343,6 +343,9 @@ class Transaction {\n   // Get keys multi transaction was initialized with, normalized and unique\n   const absl::flat_hash_set<std::string_view>& GetMultiKeys() const;\n \n+  // Send journal EXEC opcode after a series of MULTI commands on the currently active shard\n+  void FIX_ConcludeJournalExec();\n+\n  private:\n   // Holds number of locks for each IntentLock::Mode: shared and exlusive.\n   struct LockCnt {\n",
  "test_patch": "diff --git a/tests/dragonfly/replication_test.py b/tests/dragonfly/replication_test.py\nindex 5e9740291d5e..c97168d5b9b9 100644\n--- a/tests/dragonfly/replication_test.py\n+++ b/tests/dragonfly/replication_test.py\n@@ -769,6 +769,38 @@ async def test_expiry(df_local_factory: DflyInstanceFactory, n_keys=1000):\n     assert all(v is None for v in res)\n \n \n+@dfly_args({\"proactor_threads\": 4})\n+async def test_simple_scripts(df_local_factory: DflyInstanceFactory):\n+    master = df_local_factory.create()\n+    replicas = [df_local_factory.create() for _ in range(2)]\n+    df_local_factory.start_all([master] + replicas)\n+\n+    c_replicas = [replica.client() for replica in replicas]\n+    c_master = master.client()\n+\n+    # Connect replicas and wait for sync to finish\n+    for c_replica in c_replicas:\n+        await c_replica.execute_command(f\"REPLICAOF localhost {master.port}\")\n+    await check_all_replicas_finished([c_replica], c_master)\n+\n+    # Generate some scripts and run them\n+    keys = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n+    for i in range(len(keys) + 1):\n+        script = \"\"\n+        subkeys = keys[:i]\n+        for key in subkeys:\n+            script += f\"redis.call('INCR', '{key}')\"\n+            script += f\"redis.call('INCR', '{key}')\"\n+\n+        await c_master.eval(script, len(subkeys), *subkeys)\n+\n+    # Wait for replicas\n+    await check_all_replicas_finished([c_replica], c_master)\n+\n+    for c_replica in c_replicas:\n+        assert (await c_replica.mget(keys)) == [\"10\", \"8\", \"6\", \"4\", \"2\"]\n+\n+\n \"\"\"\n Test script replication.\n \n",
  "problem_statement": "Eval on a single shard does not log exec to journal\nWhen running eval on a single shard we have an optimisation for a single hop\r\ninside Service::EvalInternal  if (CanRunSingleShardMulti(sid, *params, *tx)\r\nWhen this optimisation is taking place we log to journal all the executed commands in the script with MULTI opcode but when finishing running the script we do not journal EXEC opcode.\r\nThis bug is causing replica to get stuck on execution and memory continues to grow untill oom occurs.\r\n\r\n\r\n\n",
  "hints_text": "Will follow up with a separate quick PR, but generally #2376 will be a proper solution\n@dranikpg  how long do you think it will take to implement #2376? Maybe it's worth just wait for this PR to land? \r\n\nOkay, let's merge it before the remaining refactoring, it doesn't rely much on the parts left. I'll polish it and it's ready for review\n@romange PTAL at the PR if you want this issue closed \ud83d\ude44 ",
  "created_at": "2024-02-04T07:57:35Z",
  "modified_files": [
    "src/server/journal/tx_executor.cc",
    "src/server/main_service.cc",
    "src/server/transaction.cc",
    "src/server/transaction.h"
  ],
  "modified_test_files": [
    "tests/dragonfly/replication_test.py"
  ]
}