diff --git a/tests/dragonfly/cluster_test.py b/tests/dragonfly/cluster_test.py
index bcf0b67cd541..a7f42dc5fdf3 100644
--- a/tests/dragonfly/cluster_test.py
+++ b/tests/dragonfly/cluster_test.py
@@ -14,7 +14,7 @@
 from redis.cluster import RedisCluster
 from redis.cluster import ClusterNode
 from .proxy import Proxy
-from .seeder import Seeder, SeederBase, StaticSeeder
+from .seeder import Seeder, SeederBase, DebugPopulateSeeder
 
 from . import dfly_args
 
@@ -1485,8 +1485,8 @@ async def test_network_disconnect_during_migration(df_factory):
 
     await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])
 
-    await StaticSeeder(key_target=100000).run(nodes[0].client)
-    start_capture = await StaticSeeder.capture(nodes[0].client)
+    await DebugPopulateSeeder(key_target=100000).run(nodes[0].client)
+    start_capture = await DebugPopulateSeeder.capture(nodes[0].client)
 
     proxy = Proxy("127.0.0.1", next(next_port), "127.0.0.1", nodes[1].instance.admin_port)
     await proxy.start()
@@ -1519,7 +1519,7 @@ async def test_network_disconnect_during_migration(df_factory):
         logging.debug("remove finished migrations")
         await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])
 
-        assert (await StaticSeeder.capture(nodes[1].client)) == start_capture
+        assert (await DebugPopulateSeeder.capture(nodes[1].client)) == start_capture
     finally:
         await proxy.close(task)
 
@@ -1902,7 +1902,7 @@ async def test_keys_expiration_during_migration(df_factory: DflyInstanceFactory)
     logging.debug("Start seeder")
     await nodes[0].client.execute_command("debug", "populate", "100", "foo", "100", "RAND")
 
-    capture_before = await StaticSeeder.capture(nodes[0].client)
+    capture_before = await DebugPopulateSeeder.capture(nodes[0].client)
 
     seeder = ExpirySeeder(timeout=4)
     seeder_task = asyncio.create_task(seeder.run(nodes[0].client))
@@ -1928,7 +1928,7 @@ async def test_keys_expiration_during_migration(df_factory: DflyInstanceFactory)
     # wait to expire all keys
     await asyncio.sleep(5)
 
-    assert await StaticSeeder.capture(nodes[1].client) == capture_before
+    assert await DebugPopulateSeeder.capture(nodes[1].client) == capture_before
 
     stats = await nodes[1].client.info("STATS")
     assert stats["expired_keys"] > 0
@@ -2020,7 +2020,7 @@ async def start_save():
     )
 
     # TODO: We can't compare the post-loaded data as is, because it might have changed by now.
-    # We can try to use FakeRedis with the StaticSeeder comparison here.
+    # We can try to use FakeRedis with the DebugPopulateSeeder comparison here.
 
 
 @pytest.mark.exclude_epoll
@@ -2103,7 +2103,7 @@ async def test_cluster_migration_huge_container(df_factory: DflyInstanceFactory)
     await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])
 
     logging.debug("Generating huge containers")
-    seeder = StaticSeeder(
+    seeder = DebugPopulateSeeder(
         key_target=100,
         data_size=10_000_000,
         collection_size=10_000,
@@ -2112,7 +2112,7 @@ async def test_cluster_migration_huge_container(df_factory: DflyInstanceFactory)
         types=["LIST", "HASH", "SET", "ZSET", "STRING"],
     )
     await seeder.run(nodes[0].client)
-    source_data = await StaticSeeder.capture(nodes[0].client)
+    source_data = await DebugPopulateSeeder.capture(nodes[0].client)
 
     mem_before = await get_memory(nodes[0].client, "used_memory_rss")
 
@@ -2125,7 +2125,7 @@ async def test_cluster_migration_huge_container(df_factory: DflyInstanceFactory)
     logging.debug("Waiting for migration to finish")
     await wait_for_status(nodes[0].admin_client, nodes[1].id, "FINISHED", timeout=300)
 
-    target_data = await StaticSeeder.capture(nodes[1].client)
+    target_data = await DebugPopulateSeeder.capture(nodes[1].client)
     assert source_data == target_data
 
     # Get peak memory, because migration removes the data
@@ -2740,8 +2740,8 @@ async def test_migration_timeout_on_sync(df_factory: DflyInstanceFactory, df_see
 
     logging.debug("source node DEBUG POPULATE")
 
-    await StaticSeeder(key_target=300000, data_size=1000).run(nodes[0].client)
-    start_capture = await StaticSeeder.capture(nodes[0].client)
+    await DebugPopulateSeeder(key_target=300000, data_size=1000).run(nodes[0].client)
+    start_capture = await DebugPopulateSeeder.capture(nodes[0].client)
 
     logging.debug("Start migration")
     nodes[0].migrations.append(
@@ -2775,7 +2775,7 @@ async def test_migration_timeout_on_sync(df_factory: DflyInstanceFactory, df_see
 
     await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])
 
-    assert (await StaticSeeder.capture(nodes[1].client)) == start_capture
+    assert (await DebugPopulateSeeder.capture(nodes[1].client)) == start_capture
 
 
 """
@@ -2807,7 +2807,7 @@ async def test_migration_one_after_another(df_factory: DflyInstanceFactory, df_s
 
     logging.debug("DEBUG POPULATE first node")
     key_num = 100000
-    await StaticSeeder(key_target=key_num, data_size=100).run(nodes[0].client)
+    await DebugPopulateSeeder(key_target=key_num, data_size=100).run(nodes[0].client)
     dbsize_node0 = await nodes[0].client.dbsize()
     assert dbsize_node0 > (key_num * 0.95)
 
@@ -2900,7 +2900,7 @@ def create_random_ranges():
 
     key_num = 100000
     logging.debug(f"DEBUG POPULATE first node with number of keys: {key_num}")
-    await StaticSeeder(key_target=key_num, data_size=100).run(nodes[0].client)
+    await DebugPopulateSeeder(key_target=key_num, data_size=100).run(nodes[0].client)
     dbsize_node0 = await nodes[0].client.dbsize()
     assert dbsize_node0 > (key_num * 0.95)
 
diff --git a/tests/dragonfly/generic_test.py b/tests/dragonfly/generic_test.py
index 0f463efe6caf..466a3cb2d49a 100644
--- a/tests/dragonfly/generic_test.py
+++ b/tests/dragonfly/generic_test.py
@@ -7,7 +7,7 @@
 from . import dfly_multi_test_args, dfly_args
 from .instance import DflyInstance, DflyStartException
 from .utility import batch_fill_data, gen_test_data, EnvironCntx
-from .seeder import StaticSeeder
+from .seeder import DebugPopulateSeeder
 
 
 @dfly_multi_test_args({"keys_output_limit": 512}, {"keys_output_limit": 1024})
@@ -114,7 +114,6 @@ async def block(id):
             tasks.append(block(i))
         await asyncio.gather(*tasks)
 
-
     # produce is constantly waking up consumers. It is used to trigger the
     # flow that creates wake ups on a differrent database in the
     # middle of continuation transaction.
@@ -122,11 +121,12 @@ async def tasks_produce(num, iters):
         LPUSH_SCRIPT = """
             redis.call('LPUSH', KEYS[1], "val")
         """
+
         async def produce(id):
             c = df_server.client(db=1)  # important to be on a different db
             for i in range(iters):
                 # Must be a lua script and not multi-exec for some reason.
-                await c.eval(LPUSH_SCRIPT, 1,  f"list{{{id}}}")
+                await c.eval(LPUSH_SCRIPT, 1, f"list{{{id}}}")
 
         tasks = []
         for i in range(num):
@@ -151,7 +151,6 @@ async def drain(id, iters):
         await asyncio.gather(*tasks)
         logging.info("Finished consuming")
 
-
     num_keys = 32
     num_iters = 200
     async_task1 = asyncio.create_task(blmove_task_loose(num_keys))
@@ -264,7 +263,7 @@ async def test_rename_huge_values(df_factory, type):
     client = df_server.client()
 
     logging.debug(f"Generating huge {type}")
-    seeder = StaticSeeder(
+    seeder = DebugPopulateSeeder(
         key_target=1,
         data_size=10_000_000,
         collection_size=10_000,
@@ -273,7 +272,7 @@ async def test_rename_huge_values(df_factory, type):
         types=[type],
     )
     await seeder.run(client)
-    source_data = await StaticSeeder.capture(client)
+    source_data = await DebugPopulateSeeder.capture(client)
     logging.debug(f"src {source_data}")
 
     # Rename multiple times to make sure the key moves between shards
@@ -285,6 +284,6 @@ async def test_rename_huge_values(df_factory, type):
         await client.execute_command(f"rename {old_name} {new_name}")
         old_name = new_name
     await client.execute_command(f"rename {new_name} {orig_name}")
-    target_data = await StaticSeeder.capture(client)
+    target_data = await DebugPopulateSeeder.capture(client)
 
     assert source_data == target_data
diff --git a/tests/dragonfly/replication_test.py b/tests/dragonfly/replication_test.py
index 9e78df5f43c0..a1692af3a15f 100644
--- a/tests/dragonfly/replication_test.py
+++ b/tests/dragonfly/replication_test.py
@@ -17,7 +17,7 @@
 from .seeder import Seeder as SeederV2
 from . import dfly_args
 from .proxy import Proxy
-from .seeder import StaticSeeder
+from .seeder import DebugPopulateSeeder
 
 ADMIN_PORT = 1211
 
@@ -1132,7 +1132,7 @@ async def test_flushall_in_full_sync(df_factory):
     c_replica = replica.client()
 
     # Fill master with test data
-    seeder = StaticSeeder(key_target=100_000)
+    seeder = DebugPopulateSeeder(key_target=100_000)
     await seeder.run(c_master)
 
     # Start replication and wait for full sync
@@ -2754,8 +2754,8 @@ async def get_memory(client, field):
     assert replica_peak_memory < 1.1 * replica_used_memory
 
     # Check replica data consistent
-    replica_data = await StaticSeeder.capture(c_replica)
-    master_data = await StaticSeeder.capture(c_master)
+    replica_data = await DebugPopulateSeeder.capture(c_replica)
+    master_data = await DebugPopulateSeeder.capture(c_master)
     assert master_data == replica_data
 
 
@@ -2774,7 +2774,7 @@ async def test_big_containers(df_factory, element_size, elements_number):
     c_replica = replica.client()
 
     logging.debug("Fill master with test data")
-    seeder = StaticSeeder(
+    seeder = DebugPopulateSeeder(
         key_target=50,
         data_size=element_size * elements_number,
         collection_size=elements_number,
@@ -2809,8 +2809,8 @@ async def get_memory(client, field):
     assert replica_peak_memory < 1.1 * replica_used_memory
 
     # Check replica data consistent
-    replica_data = await StaticSeeder.capture(c_replica)
-    master_data = await StaticSeeder.capture(c_master)
+    replica_data = await DebugPopulateSeeder.capture(c_replica)
+    master_data = await DebugPopulateSeeder.capture(c_master)
     assert master_data == replica_data
 
 
@@ -2860,8 +2860,8 @@ async def test_stream_approximate_trimming(df_factory):
     await asyncio.sleep(1)
 
     # Check replica data consistent
-    master_data = await StaticSeeder.capture(c_master)
-    replica_data = await StaticSeeder.capture(c_replica)
+    master_data = await DebugPopulateSeeder.capture(c_master)
+    replica_data = await DebugPopulateSeeder.capture(c_replica)
     assert master_data == replica_data
 
     # Step 3: Trim all streams to 0
@@ -2870,8 +2870,8 @@ async def test_stream_approximate_trimming(df_factory):
         await c_master.execute_command("XTRIM", stream_name, "MAXLEN", "0")
 
     # Check replica data consistent
-    master_data = await StaticSeeder.capture(c_master)
-    replica_data = await StaticSeeder.capture(c_replica)
+    master_data = await DebugPopulateSeeder.capture(c_master)
+    replica_data = await DebugPopulateSeeder.capture(c_replica)
     assert master_data == replica_data
 
 
@@ -2945,7 +2945,7 @@ async def test_preempt_in_atomic_section_of_heartbeat(df_factory: DflyInstanceFa
         rand = random.randint(1, 10)
         await c_master.execute_command(f"EXPIRE tmp:{i} {rand} NX")
 
-    seeder = StaticSeeder(key_target=10000)
+    seeder = SeederV2(key_target=10_000)
     fill_task = asyncio.create_task(seeder.run(master.client()))
 
     for replica in c_replicas:
@@ -2957,7 +2957,6 @@ async def test_preempt_in_atomic_section_of_heartbeat(df_factory: DflyInstanceFa
     await fill_task
 
 
-@pytest.mark.skip(reason="Temporary skip it. We have a bug around memory tracking")
 async def test_bug_in_json_memory_tracking(df_factory: DflyInstanceFactory):
     """
     This test reproduces a bug in the JSON memory tracking.
@@ -2978,7 +2977,7 @@ async def test_bug_in_json_memory_tracking(df_factory: DflyInstanceFactory):
         rand = random.randint(1, 4)
         await c_master.execute_command(f"EXPIRE tmp:{i} {rand} NX")
 
-    seeder = StaticSeeder(key_target=100_000)
+    seeder = SeederV2(key_target=100_000)
     fill_task = asyncio.create_task(seeder.run(master.client()))
 
     for replica in c_replicas:
diff --git a/tests/dragonfly/seeder/README.md b/tests/dragonfly/seeder/README.md
index 56475f410ee7..596ffb3e8d75 100644
--- a/tests/dragonfly/seeder/README.md
+++ b/tests/dragonfly/seeder/README.md
@@ -4,20 +4,20 @@ Please use the testing frameworks factories to obtain proper seeder instances!
 
 ### 1. Static seeder
 
-The StaticSeeder is a thin wrapper around `DEBUG POPULATE` with a little bit of fuzziness for collection sizes. It should be preffered for generating "static" data for snapshotting, memory consumption tests, etc.
+The DebugPopulateSeeder is a thin wrapper around `DEBUG POPULATE` with a little bit of fuzziness for collection sizes. It should be preffered for generating "static" data for snapshotting, memory consumption tests, etc.
 
 ```python
-s = StaticSeeder(key_target=10_000)
+s = DebugPopulateSeeder(key_target=10_000)
 await s.run(client) # Creates around 10k keys
 ```
 
 ### 2. Checking consistency
 
-Use `SeederBase.capture()` (accessed via `StaticSeeder` or `Seeder`) to calculate a "state hashes" based on all the data inside an instance. Equal data produces equal hashes (equal hashes don't guarantee equal data but what are the odds...).
+Use `SeederBase.capture()` (accessed via `DebugPopulateSeeder` or `Seeder`) to calculate a "state hashes" based on all the data inside an instance. Equal data produces equal hashes (equal hashes don't guarantee equal data but what are the odds...).
 
 ```python
 # Fill master with ~10k keys
-s = StaticSeeder(key_target=10_000)
+s = DebugPopulateSeeder(key_target=10_000)
 await seeder.run(master)
 
 # "Replicate" or other operations
@@ -25,8 +25,8 @@ replicate(master, replica)
 
 # Ensure master and replica have same state hashes
 master_hashes, replica_hashes = await asyncio.gather(
-    StaticSeeder.capture(master), # note it's a static method
-    StaticSeeder.capture(replica)
+    DebugPopulateSeeder.capture(master), # note it's a static method
+    DebugPopulateSeeder.capture(replica)
 )
 assert master_hashes == replica_hashes
 ```
diff --git a/tests/dragonfly/seeder/__init__.py b/tests/dragonfly/seeder/__init__.py
index a10e1a980906..edbf9956a562 100644
--- a/tests/dragonfly/seeder/__init__.py
+++ b/tests/dragonfly/seeder/__init__.py
@@ -70,7 +70,7 @@ def _load_script(clz, fname):
         return script
 
 
-class StaticSeeder(SeederBase):
+class DebugPopulateSeeder(SeederBase):
     """Wrapper around DEBUG POPULATE with fuzzy key sizes and a balanced type mix"""
 
     def __init__(
diff --git a/tests/dragonfly/seeder_test.py b/tests/dragonfly/seeder_test.py
index ed8456d8ea23..c3a5cb59aec4 100644
--- a/tests/dragonfly/seeder_test.py
+++ b/tests/dragonfly/seeder_test.py
@@ -3,14 +3,14 @@
 import string
 from redis import asyncio as aioredis
 from . import dfly_args
-from .seeder import Seeder, StaticSeeder
+from .seeder import Seeder, DebugPopulateSeeder
 from .instance import DflyInstanceFactory, DflyInstance
 from .utility import *
 
 
 @dfly_args({"proactor_threads": 4})
 async def test_static_seeder(async_client: aioredis.Redis):
-    s = StaticSeeder(key_target=10_000, data_size=100)
+    s = DebugPopulateSeeder(key_target=10_000, data_size=100)
     await s.run(async_client)
 
     assert abs(await async_client.dbsize() - 10_000) <= 70
@@ -24,7 +24,7 @@ async def check_list():
             assert await async_client.llen(key) == 1
             assert len(await async_client.lpop(key)) == 10_000
 
-    s = StaticSeeder(
+    s = DebugPopulateSeeder(
         key_target=10, data_size=10_000, variance=1, samples=1, collection_size=1, types=["LIST"]
     )
     await s.run(async_client)
diff --git a/tests/dragonfly/snapshot_test.py b/tests/dragonfly/snapshot_test.py
index f6dc2201feb8..7f34979fb1a6 100644
--- a/tests/dragonfly/snapshot_test.py
+++ b/tests/dragonfly/snapshot_test.py
@@ -17,7 +17,7 @@
 from . import dfly_args
 from .utility import wait_available_async, is_saving, tmp_file_name
 
-from .seeder import StaticSeeder
+from .seeder import DebugPopulateSeeder
 
 BASIC_ARGS = {"dir": "{DRAGONFLY_TMP}/", "proactor_threads": 4}
 FILE_FORMATS = ["RDB", "DF"]
@@ -61,9 +61,9 @@ async def test_consistency(df_factory, format: str, seeder_opts: dict):
     instance = df_factory.create(dbfilename=dbfilename)
     instance.start()
     async_client = instance.client()
-    await StaticSeeder(**seeder_opts).run(async_client)
+    await DebugPopulateSeeder(**seeder_opts).run(async_client)
 
-    start_capture = await StaticSeeder.capture(async_client)
+    start_capture = await DebugPopulateSeeder.capture(async_client)
 
     # save + flush + load
     await async_client.execute_command("SAVE", format)
@@ -74,7 +74,7 @@ async def test_consistency(df_factory, format: str, seeder_opts: dict):
         f"{dbfilename}.rdb" if format == "RDB" else f"{dbfilename}-summary.dfs",
     )
 
-    assert (await StaticSeeder.capture(async_client)) == start_capture
+    assert (await DebugPopulateSeeder.capture(async_client)) == start_capture
 
 
 @pytest.mark.parametrize("format", FILE_FORMATS)
@@ -90,8 +90,8 @@ async def test_multidb(df_factory, format: str):
     start_captures = []
     for dbid in range(10):
         db_client = instance.client(db=dbid)
-        await StaticSeeder(key_target=1000).run(db_client)
-        start_captures.append(await StaticSeeder.capture(db_client))
+        await DebugPopulateSeeder(key_target=1000).run(db_client)
+        start_captures.append(await DebugPopulateSeeder.capture(db_client))
 
     # save + flush + load
     await async_client.execute_command("SAVE", format)
@@ -104,7 +104,7 @@ async def test_multidb(df_factory, format: str):
 
     for dbid in range(10):
         db_client = instance.client(db=dbid)
-        assert (await StaticSeeder.capture(db_client)) == start_captures[dbid]
+        assert (await DebugPopulateSeeder.capture(db_client)) == start_captures[dbid]
 
 
 @pytest.mark.asyncio
@@ -134,8 +134,8 @@ async def test_dbfilenames(
             await wait_available_async(client)
 
             # We use the seeder just to check we don't loose any files (and thus keys)
-            await StaticSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(client)
-            start_capture = await StaticSeeder.capture(client)
+            await DebugPopulateSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(client)
+            start_capture = await DebugPopulateSeeder.capture(client)
 
             await client.execute_command("SAVE " + save_type)
 
@@ -146,7 +146,7 @@ async def test_dbfilenames(
     with df_factory.create(**df_args) as df_server:
         async with df_server.client() as client:
             await wait_available_async(client)
-            assert await StaticSeeder.capture(client) == start_capture
+            assert await DebugPopulateSeeder.capture(client) == start_capture
 
 
 @pytest.mark.asyncio
@@ -163,7 +163,7 @@ async def test_redis_load_snapshot(
     """
     Test redis server loading dragonfly snapshot rdb format
     """
-    await StaticSeeder(
+    await DebugPopulateSeeder(
         **LIGHTWEIGHT_SEEDER_ARGS, types=["STRING", "LIST", "SET", "HASH", "ZSET"]
     ).run(async_client)
 
@@ -186,7 +186,7 @@ async def test_redis_load_snapshot(
 @pytest.mark.slow
 @dfly_args({**BASIC_ARGS, "dbfilename": "test-cron", "snapshot_cron": "* * * * *"})
 async def test_cron_snapshot(tmp_dir: Path, async_client: aioredis.Redis):
-    await StaticSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(async_client)
+    await DebugPopulateSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(async_client)
 
     file = None
     async with timeout(65):
@@ -201,7 +201,7 @@ async def test_cron_snapshot(tmp_dir: Path, async_client: aioredis.Redis):
 @pytest.mark.slow
 @dfly_args({**BASIC_ARGS, "dbfilename": "test-failed-saving", "snapshot_cron": "* * * * *"})
 async def test_cron_snapshot_failed_saving(df_server, tmp_dir: Path, async_client: aioredis.Redis):
-    await StaticSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(async_client)
+    await DebugPopulateSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(async_client)
 
     backups_total = await get_metric_value(df_server, "dragonfly_backups")
     failed_backups_total = await get_metric_value(df_server, "dragonfly_failed_backups")
@@ -240,7 +240,7 @@ async def test_cron_snapshot_failed_saving(df_server, tmp_dir: Path, async_clien
 @pytest.mark.slow
 @dfly_args({**BASIC_ARGS, "dbfilename": "test-cron-set"})
 async def test_set_cron_snapshot(tmp_dir: Path, async_client: aioredis.Redis):
-    await StaticSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(async_client)
+    await DebugPopulateSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(async_client)
 
     await async_client.config_set("snapshot_cron", "* * * * *")
 
@@ -260,8 +260,8 @@ async def test_shutdown_save_with_rename(df_server):
     """Checks that on shutdown we save snapshot"""
     client = df_server.client()
 
-    await StaticSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(client)
-    start_capture = await StaticSeeder.capture(client)
+    await DebugPopulateSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(client)
+    start_capture = await DebugPopulateSeeder.capture(client)
 
     await client.connection_pool.disconnect()
     df_server.stop()
@@ -269,7 +269,7 @@ async def test_shutdown_save_with_rename(df_server):
     client = df_server.client()
 
     await wait_available_async(client)
-    assert await StaticSeeder.capture(client) == start_capture
+    assert await DebugPopulateSeeder.capture(client) == start_capture
 
     await client.connection_pool.disconnect()
 
@@ -304,7 +304,7 @@ async def test_path_escapes(df_factory):
 async def test_info_persistence_field(async_client):
     """Test is_loading field on INFO PERSISTENCE during snapshot loading"""
 
-    await StaticSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(async_client)
+    await DebugPopulateSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(async_client)
 
     # Wait for snapshot to finish loading and try INFO PERSISTENCE
     await wait_available_async(async_client)
@@ -334,10 +334,10 @@ def delete_s3_objects(bucket, prefix):
 )
 @dfly_args({**BASIC_ARGS, "dir": "s3://{DRAGONFLY_S3_BUCKET}{DRAGONFLY_TMP}", "dbfilename": ""})
 async def test_s3_snapshot(async_client, tmp_dir):
-    seeder = StaticSeeder(key_target=10_000)
+    seeder = DebugPopulateSeeder(key_target=10_000)
     await seeder.run(async_client)
 
-    start_capture = await StaticSeeder.capture(async_client)
+    start_capture = await DebugPopulateSeeder.capture(async_client)
 
     try:
         # save + flush + load
@@ -350,7 +350,7 @@ async def test_s3_snapshot(async_client, tmp_dir):
             + "/snapshot-summary.dfs"
         )
 
-        assert await StaticSeeder.capture(async_client) == start_capture
+        assert await DebugPopulateSeeder.capture(async_client) == start_capture
 
     finally:
         delete_s3_objects(
@@ -367,7 +367,7 @@ async def test_s3_snapshot(async_client, tmp_dir):
 )
 @dfly_args({**BASIC_ARGS})
 async def test_s3_save_local_dir(async_client, tmp_dir):
-    seeder = StaticSeeder(key_target=10_000)
+    seeder = DebugPopulateSeeder(key_target=10_000)
     await seeder.run(async_client)
 
     try:
@@ -412,7 +412,7 @@ async def test_memory_counters(self, async_client: aioredis.Redis):
         memory_counters = await self._get_info_memory_fields(async_client)
         assert memory_counters == {"object_used_memory": 0}
 
-        seeder = StaticSeeder(**self.SEEDER_ARGS)
+        seeder = DebugPopulateSeeder(**self.SEEDER_ARGS)
         await seeder.run(async_client)
 
         memory_counters = await self._get_info_memory_fields(async_client)
@@ -430,9 +430,9 @@ async def test_snapshot(self, df_server, async_client):
         3. Memory counters after deleting all keys loaded by snapshot - this validates the memory
            counting when loading from snapshot."""
 
-        seeder = StaticSeeder(**self.SEEDER_ARGS)
+        seeder = DebugPopulateSeeder(**self.SEEDER_ARGS)
         await seeder.run(async_client)
-        start_capture = await StaticSeeder.capture(async_client)
+        start_capture = await DebugPopulateSeeder.capture(async_client)
 
         memory_before = await self._get_info_memory_fields(async_client)
 
@@ -443,7 +443,7 @@ async def test_snapshot(self, df_server, async_client):
         async_client = df_server.client()
         await wait_available_async(async_client)
 
-        assert await StaticSeeder.capture(async_client) == start_capture
+        assert await DebugPopulateSeeder.capture(async_client) == start_capture
 
         memory_after = await self._get_info_memory_fields(async_client)
         for counter, value in memory_before.items():
@@ -514,12 +514,12 @@ async def test_tiered_entries(async_client: aioredis.Redis):
     """This test makes sure tieried entries are correctly persisted"""
 
     # With variance 4: 512 - 8192 we include small and large values
-    await StaticSeeder(key_target=5000, data_size=1024, variance=4, types=["STRING"]).run(
+    await DebugPopulateSeeder(key_target=5000, data_size=1024, variance=4, types=["STRING"]).run(
         async_client
     )
 
     # Compute the capture, this brings all items back to memory... so we'll wait for offloading
-    start_capture = await StaticSeeder.capture(async_client)
+    start_capture = await DebugPopulateSeeder.capture(async_client)
 
     # Wait until the total_stashes counter stops increasing, meaning offloading finished
     last_writes, current_writes = 0, -1
@@ -538,7 +538,7 @@ async def test_tiered_entries(async_client: aioredis.Redis):
     )
 
     # Compare captures
-    assert await StaticSeeder.capture(async_client) == start_capture
+    assert await DebugPopulateSeeder.capture(async_client) == start_capture
 
 
 @pytest.mark.skip("Too heavy")
@@ -556,12 +556,12 @@ async def test_tiered_entries(async_client: aioredis.Redis):
 )
 async def test_tiered_entries_throttle(async_client: aioredis.Redis):
     """This test makes sure tieried entries are correctly persisted"""
-    await StaticSeeder(key_target=600_000, data_size=4096, variance=1, types=["STRING"]).run(
+    await DebugPopulateSeeder(key_target=600_000, data_size=4096, variance=1, types=["STRING"]).run(
         async_client
     )
 
     # Compute the capture, this brings all items back to memory... so we'll wait for offloading
-    start_capture = await StaticSeeder.capture(async_client)
+    start_capture = await DebugPopulateSeeder.capture(async_client)
 
     # Save + flush + load
     await async_client.execute_command("SAVE", "DF")
@@ -582,7 +582,7 @@ async def test_tiered_entries_throttle(async_client: aioredis.Redis):
         await asyncio.sleep(0.05)
 
     await load_task
-    assert await StaticSeeder.capture(async_client) == start_capture
+    assert await DebugPopulateSeeder.capture(async_client) == start_capture
 
 
 @dfly_args({"serialization_max_chunk_size": 4096, "proactor_threads": 1})
diff --git a/tests/dragonfly/tiering_test.py b/tests/dragonfly/tiering_test.py
index 7d77a001d1c5..45b74ec17daf 100644
--- a/tests/dragonfly/tiering_test.py
+++ b/tests/dragonfly/tiering_test.py
@@ -6,7 +6,7 @@
 import redis.asyncio as aioredis
 
 from . import dfly_args
-from .seeder import StaticSeeder
+from .seeder import DebugPopulateSeeder
 from .utility import info_tick_timer
 
 
@@ -21,7 +21,7 @@ async def test_basic_memory_usage(async_client: aioredis.Redis):
     Loading 1GB of mixed size strings (256b-16kb) will keep most of them on disk and thus RAM remains almost unused
     """
 
-    seeder = StaticSeeder(
+    seeder = DebugPopulateSeeder(
         key_target=200_000, data_size=2048, variance=8, samples=100, types=["STRING"]
     )
     await seeder.run(async_client)
