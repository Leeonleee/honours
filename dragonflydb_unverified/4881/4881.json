{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4881,
  "instance_id": "dragonflydb__dragonfly-4881",
  "issue_numbers": [
    "4048"
  ],
  "base_commit": "460690855e27051b0cb595a1afdbecdb9f76fd39",
  "patch": "diff --git a/src/server/debugcmd.cc b/src/server/debugcmd.cc\nindex 9268f5a830ad..435a0de5057e 100644\n--- a/src/server/debugcmd.cc\n+++ b/src/server/debugcmd.cc\n@@ -783,6 +783,8 @@ void DebugCmd::Populate(CmdArgList args, facade::SinkReplyBuilder* builder) {\n   if (!options.has_value()) {\n     return;\n   }\n+  DCHECK(sf_.AreAllReplicasInStableSync());\n+\n   ProactorPool& pp = sf_.service().proactor_pool();\n   size_t runners_count = pp.size();\n   vector<pair<uint64_t, uint64_t>> ranges(runners_count - 1);\n@@ -808,6 +810,8 @@ void DebugCmd::Populate(CmdArgList args, facade::SinkReplyBuilder* builder) {\n     fb.Join();\n \n   builder->SendOk();\n+\n+  DCHECK(sf_.AreAllReplicasInStableSync());\n }\n \n void DebugCmd::PopulateRangeFiber(uint64_t from, uint64_t num_of_keys,\ndiff --git a/src/server/dflycmd.cc b/src/server/dflycmd.cc\nindex 57f092124e8d..139b7dd966ba 100644\n--- a/src/server/dflycmd.cc\n+++ b/src/server/dflycmd.cc\n@@ -44,20 +44,6 @@ using namespace util;\n using std::string;\n using util::ProactorBase;\n \n-namespace {\n-const char kBadMasterId[] = \"bad master id\";\n-const char kIdNotFound[] = \"syncid not found\";\n-const char kInvalidSyncId[] = \"bad sync id\";\n-const char kInvalidState[] = \"invalid state\";\n-\n-bool ToSyncId(string_view str, uint32_t* num) {\n-  if (!absl::StartsWith(str, \"SYNC\"))\n-    return false;\n-  str.remove_prefix(4);\n-\n-  return absl::SimpleAtoi(str, num);\n-}\n-\n std::string_view SyncStateName(DflyCmd::SyncState sync_state) {\n   switch (sync_state) {\n     case DflyCmd::SyncState::PREPARATION:\n@@ -73,6 +59,20 @@ std::string_view SyncStateName(DflyCmd::SyncState sync_state) {\n   return \"unsupported\";\n }\n \n+namespace {\n+const char kBadMasterId[] = \"bad master id\";\n+const char kIdNotFound[] = \"syncid not found\";\n+const char kInvalidSyncId[] = \"bad sync id\";\n+const char kInvalidState[] = \"invalid state\";\n+\n+bool ToSyncId(string_view str, uint32_t* num) {\n+  if (!absl::StartsWith(str, \"SYNC\"))\n+    return false;\n+  str.remove_prefix(4);\n+\n+  return absl::SimpleAtoi(str, num);\n+}\n+\n bool WaitReplicaFlowToCatchup(absl::Time end_time, const DflyCmd::ReplicaInfo* replica,\n                               EngineShard* shard) {\n   // We don't want any writes to the journal after we send the `PING`,\ndiff --git a/src/server/dflycmd.h b/src/server/dflycmd.h\nindex 8230c0ca67e2..c4715bef637a 100644\n--- a/src/server/dflycmd.h\n+++ b/src/server/dflycmd.h\n@@ -143,7 +143,7 @@ class DflyCmd {\n   // Create new sync session. Returns (session_id, number of flows)\n   std::pair<uint32_t, unsigned> CreateSyncSession(ConnectionState* state) ABSL_LOCKS_EXCLUDED(mu_);\n \n-  // Master side acces method to replication info of that connection.\n+  // Master side access method to replication info of that connection.\n   std::shared_ptr<ReplicaInfo> GetReplicaInfoFromConnection(ConnectionState* state);\n \n   // Master-side command. Provides Replica info.\n@@ -235,4 +235,6 @@ class DflyCmd {\n   mutable util::fb2::Mutex mu_;  // Guard global operations. See header top for locking levels.\n };\n \n+std::string_view SyncStateName(DflyCmd::SyncState sync_state);\n+\n }  // namespace dfly\ndiff --git a/src/server/multi_command_squasher.cc b/src/server/multi_command_squasher.cc\nindex df109fecaef3..604340a65bbd 100644\n--- a/src/server/multi_command_squasher.cc\n+++ b/src/server/multi_command_squasher.cc\n@@ -168,8 +168,7 @@ bool MultiCommandSquasher::ExecuteStandalone(facade::RedisReplyBuilder* rb, Stor\n   return true;\n }\n \n-OpStatus MultiCommandSquasher::SquashedHopCb(Transaction* parent_tx, EngineShard* es,\n-                                             RespVersion resp_v) {\n+OpStatus MultiCommandSquasher::SquashedHopCb(EngineShard* es, RespVersion resp_v) {\n   auto& sinfo = sharded_[es->shard_id()];\n   DCHECK(!sinfo.cmds.empty());\n \n@@ -242,14 +241,13 @@ bool MultiCommandSquasher::ExecuteSquashed(facade::RedisReplyBuilder* rb) {\n     auto cb = [this](ShardId sid) { return !sharded_[sid].cmds.empty(); };\n     tx->PrepareSquashedMultiHop(base_cid_, cb);\n     tx->ScheduleSingleHop(\n-        [this, rb](auto* tx, auto* es) { return SquashedHopCb(tx, es, rb->GetRespVersion()); });\n+        [this, rb](auto* tx, auto* es) { return SquashedHopCb(es, rb->GetRespVersion()); });\n   } else {\n-#if 1\n     fb2::BlockingCounter bc(num_shards);\n     DVLOG(1) << \"Squashing \" << num_shards << \" \" << tx->DebugId();\n \n     auto cb = [this, tx, bc, rb]() mutable {\n-      this->SquashedHopCb(tx, EngineShard::tlocal(), rb->GetRespVersion());\n+      this->SquashedHopCb(EngineShard::tlocal(), rb->GetRespVersion());\n       bc->Dec();\n     };\n \n@@ -258,11 +256,6 @@ bool MultiCommandSquasher::ExecuteSquashed(facade::RedisReplyBuilder* rb) {\n         shard_set->AddL2(i, cb);\n     }\n     bc->Wait();\n-#else\n-    shard_set->RunBlockingInParallel(\n-        [this, tx, rb](auto* es) { SquashedHopCb(tx, es, rb->GetRespVersion()); },\n-        [this](auto sid) { return !sharded_[sid].cmds.empty(); });\n-#endif\n   }\n \n   uint64_t after_hop = proactor->GetMonotonicTimeNs();\ndiff --git a/src/server/multi_command_squasher.h b/src/server/multi_command_squasher.h\nindex 3230b66e460b..dc4158dd00b5 100644\n--- a/src/server/multi_command_squasher.h\n+++ b/src/server/multi_command_squasher.h\n@@ -61,8 +61,7 @@ class MultiCommandSquasher {\n   bool ExecuteStandalone(facade::RedisReplyBuilder* rb, StoredCmd* cmd);\n \n   // Callback that runs on shards during squashed hop.\n-  facade::OpStatus SquashedHopCb(Transaction* parent_tx, EngineShard* es,\n-                                 facade::RespVersion resp_v);\n+  facade::OpStatus SquashedHopCb(EngineShard* es, facade::RespVersion resp_v);\n \n   // Execute all currently squashed commands. Return false if aborting on error.\n   bool ExecuteSquashed(facade::RedisReplyBuilder* rb);\ndiff --git a/src/server/server_family.h b/src/server/server_family.h\nindex 092622701c35..5e43ed9c5db9 100644\n--- a/src/server/server_family.h\n+++ b/src/server/server_family.h\n@@ -266,6 +266,18 @@ class ServerFamily {\n \n   void UpdateMemoryGlobalStats();\n \n+  // Return true if no replicas are registered or if all replicas reached stable sync\n+  // Used in debug populate to DCHECK insocsistent flows that violate transaction gurantees\n+  bool AreAllReplicasInStableSync() const {\n+    auto roles = dfly_cmd_->GetReplicasRoleInfo();\n+    if (roles.empty()) {\n+      return true;\n+    }\n+    auto match = SyncStateName(DflyCmd::SyncState::STABLE_SYNC);\n+    return std::all_of(roles.begin(), roles.end(),\n+                       [&match](auto& elem) { return elem.state == match; });\n+  }\n+\n  private:\n   bool HasPrivilegedInterface();\n   void JoinSnapshotSchedule();\ndiff --git a/src/server/transaction.cc b/src/server/transaction.cc\nindex cb631e3780fc..a956ae962c90 100644\n--- a/src/server/transaction.cc\n+++ b/src/server/transaction.cc\n@@ -1114,11 +1114,11 @@ bool Transaction::ScheduleInShard(EngineShard* shard, bool execute_optimistic) {\n   // Acquire intent locks. Intent locks are always acquired, even if already locked by others.\n   if (!IsGlobal()) {\n     lock_args = GetLockArgs(shard->shard_id());\n-    bool shard_unlocked = shard->shard_lock()->Check(mode);\n+    const bool shard_unlocked = shard->shard_lock()->Check(mode);\n \n     // We need to acquire the fp locks because the executing callback\n     // within RunCallback below might preempt.\n-    bool keys_unlocked = GetDbSlice(shard->shard_id()).Acquire(mode, lock_args);\n+    const bool keys_unlocked = GetDbSlice(shard->shard_id()).Acquire(mode, lock_args);\n     lock_granted = shard_unlocked && keys_unlocked;\n \n     sd.local_mask |= KEYLOCK_ACQUIRED;\n@@ -1129,7 +1129,7 @@ bool Transaction::ScheduleInShard(EngineShard* shard, bool execute_optimistic) {\n     DVLOG(3) << \"Lock granted \" << lock_granted << \" for trans \" << DebugId();\n \n     // Check if we can run immediately\n-    if (shard_unlocked && execute_optimistic && lock_granted) {\n+    if (lock_granted && execute_optimistic) {\n       sd.local_mask |= OPTIMISTIC_EXECUTION;\n       shard->stats().tx_optimistic_total++;\n \n",
  "test_patch": "diff --git a/tests/dragonfly/cluster_test.py b/tests/dragonfly/cluster_test.py\nindex bcf0b67cd541..a7f42dc5fdf3 100644\n--- a/tests/dragonfly/cluster_test.py\n+++ b/tests/dragonfly/cluster_test.py\n@@ -14,7 +14,7 @@\n from redis.cluster import RedisCluster\n from redis.cluster import ClusterNode\n from .proxy import Proxy\n-from .seeder import Seeder, SeederBase, StaticSeeder\n+from .seeder import Seeder, SeederBase, DebugPopulateSeeder\n \n from . import dfly_args\n \n@@ -1485,8 +1485,8 @@ async def test_network_disconnect_during_migration(df_factory):\n \n     await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])\n \n-    await StaticSeeder(key_target=100000).run(nodes[0].client)\n-    start_capture = await StaticSeeder.capture(nodes[0].client)\n+    await DebugPopulateSeeder(key_target=100000).run(nodes[0].client)\n+    start_capture = await DebugPopulateSeeder.capture(nodes[0].client)\n \n     proxy = Proxy(\"127.0.0.1\", next(next_port), \"127.0.0.1\", nodes[1].instance.admin_port)\n     await proxy.start()\n@@ -1519,7 +1519,7 @@ async def test_network_disconnect_during_migration(df_factory):\n         logging.debug(\"remove finished migrations\")\n         await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])\n \n-        assert (await StaticSeeder.capture(nodes[1].client)) == start_capture\n+        assert (await DebugPopulateSeeder.capture(nodes[1].client)) == start_capture\n     finally:\n         await proxy.close(task)\n \n@@ -1902,7 +1902,7 @@ async def test_keys_expiration_during_migration(df_factory: DflyInstanceFactory)\n     logging.debug(\"Start seeder\")\n     await nodes[0].client.execute_command(\"debug\", \"populate\", \"100\", \"foo\", \"100\", \"RAND\")\n \n-    capture_before = await StaticSeeder.capture(nodes[0].client)\n+    capture_before = await DebugPopulateSeeder.capture(nodes[0].client)\n \n     seeder = ExpirySeeder(timeout=4)\n     seeder_task = asyncio.create_task(seeder.run(nodes[0].client))\n@@ -1928,7 +1928,7 @@ async def test_keys_expiration_during_migration(df_factory: DflyInstanceFactory)\n     # wait to expire all keys\n     await asyncio.sleep(5)\n \n-    assert await StaticSeeder.capture(nodes[1].client) == capture_before\n+    assert await DebugPopulateSeeder.capture(nodes[1].client) == capture_before\n \n     stats = await nodes[1].client.info(\"STATS\")\n     assert stats[\"expired_keys\"] > 0\n@@ -2020,7 +2020,7 @@ async def start_save():\n     )\n \n     # TODO: We can't compare the post-loaded data as is, because it might have changed by now.\n-    # We can try to use FakeRedis with the StaticSeeder comparison here.\n+    # We can try to use FakeRedis with the DebugPopulateSeeder comparison here.\n \n \n @pytest.mark.exclude_epoll\n@@ -2103,7 +2103,7 @@ async def test_cluster_migration_huge_container(df_factory: DflyInstanceFactory)\n     await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])\n \n     logging.debug(\"Generating huge containers\")\n-    seeder = StaticSeeder(\n+    seeder = DebugPopulateSeeder(\n         key_target=100,\n         data_size=10_000_000,\n         collection_size=10_000,\n@@ -2112,7 +2112,7 @@ async def test_cluster_migration_huge_container(df_factory: DflyInstanceFactory)\n         types=[\"LIST\", \"HASH\", \"SET\", \"ZSET\", \"STRING\"],\n     )\n     await seeder.run(nodes[0].client)\n-    source_data = await StaticSeeder.capture(nodes[0].client)\n+    source_data = await DebugPopulateSeeder.capture(nodes[0].client)\n \n     mem_before = await get_memory(nodes[0].client, \"used_memory_rss\")\n \n@@ -2125,7 +2125,7 @@ async def test_cluster_migration_huge_container(df_factory: DflyInstanceFactory)\n     logging.debug(\"Waiting for migration to finish\")\n     await wait_for_status(nodes[0].admin_client, nodes[1].id, \"FINISHED\", timeout=300)\n \n-    target_data = await StaticSeeder.capture(nodes[1].client)\n+    target_data = await DebugPopulateSeeder.capture(nodes[1].client)\n     assert source_data == target_data\n \n     # Get peak memory, because migration removes the data\n@@ -2740,8 +2740,8 @@ async def test_migration_timeout_on_sync(df_factory: DflyInstanceFactory, df_see\n \n     logging.debug(\"source node DEBUG POPULATE\")\n \n-    await StaticSeeder(key_target=300000, data_size=1000).run(nodes[0].client)\n-    start_capture = await StaticSeeder.capture(nodes[0].client)\n+    await DebugPopulateSeeder(key_target=300000, data_size=1000).run(nodes[0].client)\n+    start_capture = await DebugPopulateSeeder.capture(nodes[0].client)\n \n     logging.debug(\"Start migration\")\n     nodes[0].migrations.append(\n@@ -2775,7 +2775,7 @@ async def test_migration_timeout_on_sync(df_factory: DflyInstanceFactory, df_see\n \n     await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])\n \n-    assert (await StaticSeeder.capture(nodes[1].client)) == start_capture\n+    assert (await DebugPopulateSeeder.capture(nodes[1].client)) == start_capture\n \n \n \"\"\"\n@@ -2807,7 +2807,7 @@ async def test_migration_one_after_another(df_factory: DflyInstanceFactory, df_s\n \n     logging.debug(\"DEBUG POPULATE first node\")\n     key_num = 100000\n-    await StaticSeeder(key_target=key_num, data_size=100).run(nodes[0].client)\n+    await DebugPopulateSeeder(key_target=key_num, data_size=100).run(nodes[0].client)\n     dbsize_node0 = await nodes[0].client.dbsize()\n     assert dbsize_node0 > (key_num * 0.95)\n \n@@ -2900,7 +2900,7 @@ def create_random_ranges():\n \n     key_num = 100000\n     logging.debug(f\"DEBUG POPULATE first node with number of keys: {key_num}\")\n-    await StaticSeeder(key_target=key_num, data_size=100).run(nodes[0].client)\n+    await DebugPopulateSeeder(key_target=key_num, data_size=100).run(nodes[0].client)\n     dbsize_node0 = await nodes[0].client.dbsize()\n     assert dbsize_node0 > (key_num * 0.95)\n \ndiff --git a/tests/dragonfly/generic_test.py b/tests/dragonfly/generic_test.py\nindex 0f463efe6caf..466a3cb2d49a 100644\n--- a/tests/dragonfly/generic_test.py\n+++ b/tests/dragonfly/generic_test.py\n@@ -7,7 +7,7 @@\n from . import dfly_multi_test_args, dfly_args\n from .instance import DflyInstance, DflyStartException\n from .utility import batch_fill_data, gen_test_data, EnvironCntx\n-from .seeder import StaticSeeder\n+from .seeder import DebugPopulateSeeder\n \n \n @dfly_multi_test_args({\"keys_output_limit\": 512}, {\"keys_output_limit\": 1024})\n@@ -114,7 +114,6 @@ async def block(id):\n             tasks.append(block(i))\n         await asyncio.gather(*tasks)\n \n-\n     # produce is constantly waking up consumers. It is used to trigger the\n     # flow that creates wake ups on a differrent database in the\n     # middle of continuation transaction.\n@@ -122,11 +121,12 @@ async def tasks_produce(num, iters):\n         LPUSH_SCRIPT = \"\"\"\n             redis.call('LPUSH', KEYS[1], \"val\")\n         \"\"\"\n+\n         async def produce(id):\n             c = df_server.client(db=1)  # important to be on a different db\n             for i in range(iters):\n                 # Must be a lua script and not multi-exec for some reason.\n-                await c.eval(LPUSH_SCRIPT, 1,  f\"list{{{id}}}\")\n+                await c.eval(LPUSH_SCRIPT, 1, f\"list{{{id}}}\")\n \n         tasks = []\n         for i in range(num):\n@@ -151,7 +151,6 @@ async def drain(id, iters):\n         await asyncio.gather(*tasks)\n         logging.info(\"Finished consuming\")\n \n-\n     num_keys = 32\n     num_iters = 200\n     async_task1 = asyncio.create_task(blmove_task_loose(num_keys))\n@@ -264,7 +263,7 @@ async def test_rename_huge_values(df_factory, type):\n     client = df_server.client()\n \n     logging.debug(f\"Generating huge {type}\")\n-    seeder = StaticSeeder(\n+    seeder = DebugPopulateSeeder(\n         key_target=1,\n         data_size=10_000_000,\n         collection_size=10_000,\n@@ -273,7 +272,7 @@ async def test_rename_huge_values(df_factory, type):\n         types=[type],\n     )\n     await seeder.run(client)\n-    source_data = await StaticSeeder.capture(client)\n+    source_data = await DebugPopulateSeeder.capture(client)\n     logging.debug(f\"src {source_data}\")\n \n     # Rename multiple times to make sure the key moves between shards\n@@ -285,6 +284,6 @@ async def test_rename_huge_values(df_factory, type):\n         await client.execute_command(f\"rename {old_name} {new_name}\")\n         old_name = new_name\n     await client.execute_command(f\"rename {new_name} {orig_name}\")\n-    target_data = await StaticSeeder.capture(client)\n+    target_data = await DebugPopulateSeeder.capture(client)\n \n     assert source_data == target_data\ndiff --git a/tests/dragonfly/replication_test.py b/tests/dragonfly/replication_test.py\nindex 9e78df5f43c0..a1692af3a15f 100644\n--- a/tests/dragonfly/replication_test.py\n+++ b/tests/dragonfly/replication_test.py\n@@ -17,7 +17,7 @@\n from .seeder import Seeder as SeederV2\n from . import dfly_args\n from .proxy import Proxy\n-from .seeder import StaticSeeder\n+from .seeder import DebugPopulateSeeder\n \n ADMIN_PORT = 1211\n \n@@ -1132,7 +1132,7 @@ async def test_flushall_in_full_sync(df_factory):\n     c_replica = replica.client()\n \n     # Fill master with test data\n-    seeder = StaticSeeder(key_target=100_000)\n+    seeder = DebugPopulateSeeder(key_target=100_000)\n     await seeder.run(c_master)\n \n     # Start replication and wait for full sync\n@@ -2754,8 +2754,8 @@ async def get_memory(client, field):\n     assert replica_peak_memory < 1.1 * replica_used_memory\n \n     # Check replica data consistent\n-    replica_data = await StaticSeeder.capture(c_replica)\n-    master_data = await StaticSeeder.capture(c_master)\n+    replica_data = await DebugPopulateSeeder.capture(c_replica)\n+    master_data = await DebugPopulateSeeder.capture(c_master)\n     assert master_data == replica_data\n \n \n@@ -2774,7 +2774,7 @@ async def test_big_containers(df_factory, element_size, elements_number):\n     c_replica = replica.client()\n \n     logging.debug(\"Fill master with test data\")\n-    seeder = StaticSeeder(\n+    seeder = DebugPopulateSeeder(\n         key_target=50,\n         data_size=element_size * elements_number,\n         collection_size=elements_number,\n@@ -2809,8 +2809,8 @@ async def get_memory(client, field):\n     assert replica_peak_memory < 1.1 * replica_used_memory\n \n     # Check replica data consistent\n-    replica_data = await StaticSeeder.capture(c_replica)\n-    master_data = await StaticSeeder.capture(c_master)\n+    replica_data = await DebugPopulateSeeder.capture(c_replica)\n+    master_data = await DebugPopulateSeeder.capture(c_master)\n     assert master_data == replica_data\n \n \n@@ -2860,8 +2860,8 @@ async def test_stream_approximate_trimming(df_factory):\n     await asyncio.sleep(1)\n \n     # Check replica data consistent\n-    master_data = await StaticSeeder.capture(c_master)\n-    replica_data = await StaticSeeder.capture(c_replica)\n+    master_data = await DebugPopulateSeeder.capture(c_master)\n+    replica_data = await DebugPopulateSeeder.capture(c_replica)\n     assert master_data == replica_data\n \n     # Step 3: Trim all streams to 0\n@@ -2870,8 +2870,8 @@ async def test_stream_approximate_trimming(df_factory):\n         await c_master.execute_command(\"XTRIM\", stream_name, \"MAXLEN\", \"0\")\n \n     # Check replica data consistent\n-    master_data = await StaticSeeder.capture(c_master)\n-    replica_data = await StaticSeeder.capture(c_replica)\n+    master_data = await DebugPopulateSeeder.capture(c_master)\n+    replica_data = await DebugPopulateSeeder.capture(c_replica)\n     assert master_data == replica_data\n \n \n@@ -2945,7 +2945,7 @@ async def test_preempt_in_atomic_section_of_heartbeat(df_factory: DflyInstanceFa\n         rand = random.randint(1, 10)\n         await c_master.execute_command(f\"EXPIRE tmp:{i} {rand} NX\")\n \n-    seeder = StaticSeeder(key_target=10000)\n+    seeder = SeederV2(key_target=10_000)\n     fill_task = asyncio.create_task(seeder.run(master.client()))\n \n     for replica in c_replicas:\n@@ -2957,7 +2957,6 @@ async def test_preempt_in_atomic_section_of_heartbeat(df_factory: DflyInstanceFa\n     await fill_task\n \n \n-@pytest.mark.skip(reason=\"Temporary skip it. We have a bug around memory tracking\")\n async def test_bug_in_json_memory_tracking(df_factory: DflyInstanceFactory):\n     \"\"\"\n     This test reproduces a bug in the JSON memory tracking.\n@@ -2978,7 +2977,7 @@ async def test_bug_in_json_memory_tracking(df_factory: DflyInstanceFactory):\n         rand = random.randint(1, 4)\n         await c_master.execute_command(f\"EXPIRE tmp:{i} {rand} NX\")\n \n-    seeder = StaticSeeder(key_target=100_000)\n+    seeder = SeederV2(key_target=100_000)\n     fill_task = asyncio.create_task(seeder.run(master.client()))\n \n     for replica in c_replicas:\ndiff --git a/tests/dragonfly/seeder/README.md b/tests/dragonfly/seeder/README.md\nindex 56475f410ee7..596ffb3e8d75 100644\n--- a/tests/dragonfly/seeder/README.md\n+++ b/tests/dragonfly/seeder/README.md\n@@ -4,20 +4,20 @@ Please use the testing frameworks factories to obtain proper seeder instances!\n \n ### 1. Static seeder\n \n-The StaticSeeder is a thin wrapper around `DEBUG POPULATE` with a little bit of fuzziness for collection sizes. It should be preffered for generating \"static\" data for snapshotting, memory consumption tests, etc.\n+The DebugPopulateSeeder is a thin wrapper around `DEBUG POPULATE` with a little bit of fuzziness for collection sizes. It should be preffered for generating \"static\" data for snapshotting, memory consumption tests, etc.\n \n ```python\n-s = StaticSeeder(key_target=10_000)\n+s = DebugPopulateSeeder(key_target=10_000)\n await s.run(client) # Creates around 10k keys\n ```\n \n ### 2. Checking consistency\n \n-Use `SeederBase.capture()` (accessed via `StaticSeeder` or `Seeder`) to calculate a \"state hashes\" based on all the data inside an instance. Equal data produces equal hashes (equal hashes don't guarantee equal data but what are the odds...).\n+Use `SeederBase.capture()` (accessed via `DebugPopulateSeeder` or `Seeder`) to calculate a \"state hashes\" based on all the data inside an instance. Equal data produces equal hashes (equal hashes don't guarantee equal data but what are the odds...).\n \n ```python\n # Fill master with ~10k keys\n-s = StaticSeeder(key_target=10_000)\n+s = DebugPopulateSeeder(key_target=10_000)\n await seeder.run(master)\n \n # \"Replicate\" or other operations\n@@ -25,8 +25,8 @@ replicate(master, replica)\n \n # Ensure master and replica have same state hashes\n master_hashes, replica_hashes = await asyncio.gather(\n-    StaticSeeder.capture(master), # note it's a static method\n-    StaticSeeder.capture(replica)\n+    DebugPopulateSeeder.capture(master), # note it's a static method\n+    DebugPopulateSeeder.capture(replica)\n )\n assert master_hashes == replica_hashes\n ```\ndiff --git a/tests/dragonfly/seeder/__init__.py b/tests/dragonfly/seeder/__init__.py\nindex a10e1a980906..edbf9956a562 100644\n--- a/tests/dragonfly/seeder/__init__.py\n+++ b/tests/dragonfly/seeder/__init__.py\n@@ -70,7 +70,7 @@ def _load_script(clz, fname):\n         return script\n \n \n-class StaticSeeder(SeederBase):\n+class DebugPopulateSeeder(SeederBase):\n     \"\"\"Wrapper around DEBUG POPULATE with fuzzy key sizes and a balanced type mix\"\"\"\n \n     def __init__(\ndiff --git a/tests/dragonfly/seeder_test.py b/tests/dragonfly/seeder_test.py\nindex ed8456d8ea23..c3a5cb59aec4 100644\n--- a/tests/dragonfly/seeder_test.py\n+++ b/tests/dragonfly/seeder_test.py\n@@ -3,14 +3,14 @@\n import string\n from redis import asyncio as aioredis\n from . import dfly_args\n-from .seeder import Seeder, StaticSeeder\n+from .seeder import Seeder, DebugPopulateSeeder\n from .instance import DflyInstanceFactory, DflyInstance\n from .utility import *\n \n \n @dfly_args({\"proactor_threads\": 4})\n async def test_static_seeder(async_client: aioredis.Redis):\n-    s = StaticSeeder(key_target=10_000, data_size=100)\n+    s = DebugPopulateSeeder(key_target=10_000, data_size=100)\n     await s.run(async_client)\n \n     assert abs(await async_client.dbsize() - 10_000) <= 70\n@@ -24,7 +24,7 @@ async def check_list():\n             assert await async_client.llen(key) == 1\n             assert len(await async_client.lpop(key)) == 10_000\n \n-    s = StaticSeeder(\n+    s = DebugPopulateSeeder(\n         key_target=10, data_size=10_000, variance=1, samples=1, collection_size=1, types=[\"LIST\"]\n     )\n     await s.run(async_client)\ndiff --git a/tests/dragonfly/snapshot_test.py b/tests/dragonfly/snapshot_test.py\nindex f6dc2201feb8..7f34979fb1a6 100644\n--- a/tests/dragonfly/snapshot_test.py\n+++ b/tests/dragonfly/snapshot_test.py\n@@ -17,7 +17,7 @@\n from . import dfly_args\n from .utility import wait_available_async, is_saving, tmp_file_name\n \n-from .seeder import StaticSeeder\n+from .seeder import DebugPopulateSeeder\n \n BASIC_ARGS = {\"dir\": \"{DRAGONFLY_TMP}/\", \"proactor_threads\": 4}\n FILE_FORMATS = [\"RDB\", \"DF\"]\n@@ -61,9 +61,9 @@ async def test_consistency(df_factory, format: str, seeder_opts: dict):\n     instance = df_factory.create(dbfilename=dbfilename)\n     instance.start()\n     async_client = instance.client()\n-    await StaticSeeder(**seeder_opts).run(async_client)\n+    await DebugPopulateSeeder(**seeder_opts).run(async_client)\n \n-    start_capture = await StaticSeeder.capture(async_client)\n+    start_capture = await DebugPopulateSeeder.capture(async_client)\n \n     # save + flush + load\n     await async_client.execute_command(\"SAVE\", format)\n@@ -74,7 +74,7 @@ async def test_consistency(df_factory, format: str, seeder_opts: dict):\n         f\"{dbfilename}.rdb\" if format == \"RDB\" else f\"{dbfilename}-summary.dfs\",\n     )\n \n-    assert (await StaticSeeder.capture(async_client)) == start_capture\n+    assert (await DebugPopulateSeeder.capture(async_client)) == start_capture\n \n \n @pytest.mark.parametrize(\"format\", FILE_FORMATS)\n@@ -90,8 +90,8 @@ async def test_multidb(df_factory, format: str):\n     start_captures = []\n     for dbid in range(10):\n         db_client = instance.client(db=dbid)\n-        await StaticSeeder(key_target=1000).run(db_client)\n-        start_captures.append(await StaticSeeder.capture(db_client))\n+        await DebugPopulateSeeder(key_target=1000).run(db_client)\n+        start_captures.append(await DebugPopulateSeeder.capture(db_client))\n \n     # save + flush + load\n     await async_client.execute_command(\"SAVE\", format)\n@@ -104,7 +104,7 @@ async def test_multidb(df_factory, format: str):\n \n     for dbid in range(10):\n         db_client = instance.client(db=dbid)\n-        assert (await StaticSeeder.capture(db_client)) == start_captures[dbid]\n+        assert (await DebugPopulateSeeder.capture(db_client)) == start_captures[dbid]\n \n \n @pytest.mark.asyncio\n@@ -134,8 +134,8 @@ async def test_dbfilenames(\n             await wait_available_async(client)\n \n             # We use the seeder just to check we don't loose any files (and thus keys)\n-            await StaticSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(client)\n-            start_capture = await StaticSeeder.capture(client)\n+            await DebugPopulateSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(client)\n+            start_capture = await DebugPopulateSeeder.capture(client)\n \n             await client.execute_command(\"SAVE \" + save_type)\n \n@@ -146,7 +146,7 @@ async def test_dbfilenames(\n     with df_factory.create(**df_args) as df_server:\n         async with df_server.client() as client:\n             await wait_available_async(client)\n-            assert await StaticSeeder.capture(client) == start_capture\n+            assert await DebugPopulateSeeder.capture(client) == start_capture\n \n \n @pytest.mark.asyncio\n@@ -163,7 +163,7 @@ async def test_redis_load_snapshot(\n     \"\"\"\n     Test redis server loading dragonfly snapshot rdb format\n     \"\"\"\n-    await StaticSeeder(\n+    await DebugPopulateSeeder(\n         **LIGHTWEIGHT_SEEDER_ARGS, types=[\"STRING\", \"LIST\", \"SET\", \"HASH\", \"ZSET\"]\n     ).run(async_client)\n \n@@ -186,7 +186,7 @@ async def test_redis_load_snapshot(\n @pytest.mark.slow\n @dfly_args({**BASIC_ARGS, \"dbfilename\": \"test-cron\", \"snapshot_cron\": \"* * * * *\"})\n async def test_cron_snapshot(tmp_dir: Path, async_client: aioredis.Redis):\n-    await StaticSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(async_client)\n+    await DebugPopulateSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(async_client)\n \n     file = None\n     async with timeout(65):\n@@ -201,7 +201,7 @@ async def test_cron_snapshot(tmp_dir: Path, async_client: aioredis.Redis):\n @pytest.mark.slow\n @dfly_args({**BASIC_ARGS, \"dbfilename\": \"test-failed-saving\", \"snapshot_cron\": \"* * * * *\"})\n async def test_cron_snapshot_failed_saving(df_server, tmp_dir: Path, async_client: aioredis.Redis):\n-    await StaticSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(async_client)\n+    await DebugPopulateSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(async_client)\n \n     backups_total = await get_metric_value(df_server, \"dragonfly_backups\")\n     failed_backups_total = await get_metric_value(df_server, \"dragonfly_failed_backups\")\n@@ -240,7 +240,7 @@ async def test_cron_snapshot_failed_saving(df_server, tmp_dir: Path, async_clien\n @pytest.mark.slow\n @dfly_args({**BASIC_ARGS, \"dbfilename\": \"test-cron-set\"})\n async def test_set_cron_snapshot(tmp_dir: Path, async_client: aioredis.Redis):\n-    await StaticSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(async_client)\n+    await DebugPopulateSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(async_client)\n \n     await async_client.config_set(\"snapshot_cron\", \"* * * * *\")\n \n@@ -260,8 +260,8 @@ async def test_shutdown_save_with_rename(df_server):\n     \"\"\"Checks that on shutdown we save snapshot\"\"\"\n     client = df_server.client()\n \n-    await StaticSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(client)\n-    start_capture = await StaticSeeder.capture(client)\n+    await DebugPopulateSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(client)\n+    start_capture = await DebugPopulateSeeder.capture(client)\n \n     await client.connection_pool.disconnect()\n     df_server.stop()\n@@ -269,7 +269,7 @@ async def test_shutdown_save_with_rename(df_server):\n     client = df_server.client()\n \n     await wait_available_async(client)\n-    assert await StaticSeeder.capture(client) == start_capture\n+    assert await DebugPopulateSeeder.capture(client) == start_capture\n \n     await client.connection_pool.disconnect()\n \n@@ -304,7 +304,7 @@ async def test_path_escapes(df_factory):\n async def test_info_persistence_field(async_client):\n     \"\"\"Test is_loading field on INFO PERSISTENCE during snapshot loading\"\"\"\n \n-    await StaticSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(async_client)\n+    await DebugPopulateSeeder(**LIGHTWEIGHT_SEEDER_ARGS).run(async_client)\n \n     # Wait for snapshot to finish loading and try INFO PERSISTENCE\n     await wait_available_async(async_client)\n@@ -334,10 +334,10 @@ def delete_s3_objects(bucket, prefix):\n )\n @dfly_args({**BASIC_ARGS, \"dir\": \"s3://{DRAGONFLY_S3_BUCKET}{DRAGONFLY_TMP}\", \"dbfilename\": \"\"})\n async def test_s3_snapshot(async_client, tmp_dir):\n-    seeder = StaticSeeder(key_target=10_000)\n+    seeder = DebugPopulateSeeder(key_target=10_000)\n     await seeder.run(async_client)\n \n-    start_capture = await StaticSeeder.capture(async_client)\n+    start_capture = await DebugPopulateSeeder.capture(async_client)\n \n     try:\n         # save + flush + load\n@@ -350,7 +350,7 @@ async def test_s3_snapshot(async_client, tmp_dir):\n             + \"/snapshot-summary.dfs\"\n         )\n \n-        assert await StaticSeeder.capture(async_client) == start_capture\n+        assert await DebugPopulateSeeder.capture(async_client) == start_capture\n \n     finally:\n         delete_s3_objects(\n@@ -367,7 +367,7 @@ async def test_s3_snapshot(async_client, tmp_dir):\n )\n @dfly_args({**BASIC_ARGS})\n async def test_s3_save_local_dir(async_client, tmp_dir):\n-    seeder = StaticSeeder(key_target=10_000)\n+    seeder = DebugPopulateSeeder(key_target=10_000)\n     await seeder.run(async_client)\n \n     try:\n@@ -412,7 +412,7 @@ async def test_memory_counters(self, async_client: aioredis.Redis):\n         memory_counters = await self._get_info_memory_fields(async_client)\n         assert memory_counters == {\"object_used_memory\": 0}\n \n-        seeder = StaticSeeder(**self.SEEDER_ARGS)\n+        seeder = DebugPopulateSeeder(**self.SEEDER_ARGS)\n         await seeder.run(async_client)\n \n         memory_counters = await self._get_info_memory_fields(async_client)\n@@ -430,9 +430,9 @@ async def test_snapshot(self, df_server, async_client):\n         3. Memory counters after deleting all keys loaded by snapshot - this validates the memory\n            counting when loading from snapshot.\"\"\"\n \n-        seeder = StaticSeeder(**self.SEEDER_ARGS)\n+        seeder = DebugPopulateSeeder(**self.SEEDER_ARGS)\n         await seeder.run(async_client)\n-        start_capture = await StaticSeeder.capture(async_client)\n+        start_capture = await DebugPopulateSeeder.capture(async_client)\n \n         memory_before = await self._get_info_memory_fields(async_client)\n \n@@ -443,7 +443,7 @@ async def test_snapshot(self, df_server, async_client):\n         async_client = df_server.client()\n         await wait_available_async(async_client)\n \n-        assert await StaticSeeder.capture(async_client) == start_capture\n+        assert await DebugPopulateSeeder.capture(async_client) == start_capture\n \n         memory_after = await self._get_info_memory_fields(async_client)\n         for counter, value in memory_before.items():\n@@ -514,12 +514,12 @@ async def test_tiered_entries(async_client: aioredis.Redis):\n     \"\"\"This test makes sure tieried entries are correctly persisted\"\"\"\n \n     # With variance 4: 512 - 8192 we include small and large values\n-    await StaticSeeder(key_target=5000, data_size=1024, variance=4, types=[\"STRING\"]).run(\n+    await DebugPopulateSeeder(key_target=5000, data_size=1024, variance=4, types=[\"STRING\"]).run(\n         async_client\n     )\n \n     # Compute the capture, this brings all items back to memory... so we'll wait for offloading\n-    start_capture = await StaticSeeder.capture(async_client)\n+    start_capture = await DebugPopulateSeeder.capture(async_client)\n \n     # Wait until the total_stashes counter stops increasing, meaning offloading finished\n     last_writes, current_writes = 0, -1\n@@ -538,7 +538,7 @@ async def test_tiered_entries(async_client: aioredis.Redis):\n     )\n \n     # Compare captures\n-    assert await StaticSeeder.capture(async_client) == start_capture\n+    assert await DebugPopulateSeeder.capture(async_client) == start_capture\n \n \n @pytest.mark.skip(\"Too heavy\")\n@@ -556,12 +556,12 @@ async def test_tiered_entries(async_client: aioredis.Redis):\n )\n async def test_tiered_entries_throttle(async_client: aioredis.Redis):\n     \"\"\"This test makes sure tieried entries are correctly persisted\"\"\"\n-    await StaticSeeder(key_target=600_000, data_size=4096, variance=1, types=[\"STRING\"]).run(\n+    await DebugPopulateSeeder(key_target=600_000, data_size=4096, variance=1, types=[\"STRING\"]).run(\n         async_client\n     )\n \n     # Compute the capture, this brings all items back to memory... so we'll wait for offloading\n-    start_capture = await StaticSeeder.capture(async_client)\n+    start_capture = await DebugPopulateSeeder.capture(async_client)\n \n     # Save + flush + load\n     await async_client.execute_command(\"SAVE\", \"DF\")\n@@ -582,7 +582,7 @@ async def test_tiered_entries_throttle(async_client: aioredis.Redis):\n         await asyncio.sleep(0.05)\n \n     await load_task\n-    assert await StaticSeeder.capture(async_client) == start_capture\n+    assert await DebugPopulateSeeder.capture(async_client) == start_capture\n \n \n @dfly_args({\"serialization_max_chunk_size\": 4096, \"proactor_threads\": 1})\ndiff --git a/tests/dragonfly/tiering_test.py b/tests/dragonfly/tiering_test.py\nindex 7d77a001d1c5..45b74ec17daf 100644\n--- a/tests/dragonfly/tiering_test.py\n+++ b/tests/dragonfly/tiering_test.py\n@@ -6,7 +6,7 @@\n import redis.asyncio as aioredis\n \n from . import dfly_args\n-from .seeder import StaticSeeder\n+from .seeder import DebugPopulateSeeder\n from .utility import info_tick_timer\n \n \n@@ -21,7 +21,7 @@ async def test_basic_memory_usage(async_client: aioredis.Redis):\n     Loading 1GB of mixed size strings (256b-16kb) will keep most of them on disk and thus RAM remains almost unused\n     \"\"\"\n \n-    seeder = StaticSeeder(\n+    seeder = DebugPopulateSeeder(\n         key_target=200_000, data_size=2048, variance=8, samples=100, types=[\"STRING\"]\n     )\n     await seeder.run(async_client)\n",
  "problem_statement": "LSN mismatch during replication\nhttps://github.com/dragonflydb/dragonfly/actions/runs/11663767594/job/32472896823\r\n\r\n```\r\n\r\n.39993\u279c    @          0x13cd72a         64  absl::lts_20240116::WriteFailureInfo()\r\n\r\n39993\u279c    @          0x13cd99c        112  absl::lts_20240116::AbslFailureSignalHandler()\r\n\r\n39993\u279c    @     0x7f9c03319420       2208  (unknown)\r\n\r\n39993\u279c    @          0x1349819         16  google::LogMessage::Fail()\r\n\r\n.39993\u279c    @          0x1349760        160  google::LogMessage::SendToLog()\r\n\r\n39993\u279c    @          0x1348f55         80  google::LogMessage::Flush()\r\n\r\n39993\u279c    @          0x134cc6c         32  google::LogMessageFatal::~LogMessageFatal()\r\n.\r\n39993\u279c    @           0xba3e79        592  dfly::TransactionReader::NextTxData()\r\n\r\n.39993\u279c    @           0xb07f57        480  dfly::DflyShardReplica::StableSyncDflyReadFb()\r\n\r\n39993\u279c    @           0xb2a475         64  std::__invoke_impl<>()\r\n\r\n39993\u279c    @           0xb291f2         64  std::__invoke<>()\r\n\r\n39993\u279c    @           0xb280c9         48  std::__apply_impl<>()\r\n.\r\n39993\u279c    @           0xb2810e         48  std::apply<>()\r\n\r\n39993\u279c    @           0xb281a9        112  util::fb2::detail::WorkerFiberImpl<>::run_()\r\n\r\n39993\u279c    @           0xb2706f         80  util::fb2::detail::WorkerFiberImpl<>::WorkerFiberImpl<>()::{lambda()#1}::operator()()\r\n\r\n39993\u279c    @           0xb2c2b6         80  std::__invoke_impl<>()\r\n\r\n.39993\u279c    @           0xb2be6b         80  std::__invoke<>()\r\n\r\n39993\u279c    @           0xb2b847         80  std::invoke<>()\r\n\r\n.39993\u279c    @           0xb2b0c2         80  boost::context::detail::fiber_record<>::run()\r\n\r\n39993\u279c    @           0xb2a594         48  boost::context::detail::fiber_entry<>()\r\n\r\n```\n",
  "hints_text": "I reproduced this locally :) \nanother one: https://github.com/dragonflydb/dragonfly/actions/runs/11773257262/job/32789895747?pr=4109#step:12:1747\nanother one [test_disconnect_replica](https://github.com/dragonflydb/dragonfly/actions/runs/12877735747/job/35902645609)\nhttps://github.com/dragonflydb/dragonfly/actions/runs/13501126544/job/37719578153#step:6:3224\nalso this failure related https://github.com/dragonflydb/dragonfly/actions/runs/14026506582/job/39266040955\nIt more and more becomes an issue @adiholden \n@romange on it",
  "created_at": "2025-04-03T12:10:23Z",
  "modified_files": [
    "src/server/debugcmd.cc",
    "src/server/dflycmd.cc",
    "src/server/dflycmd.h",
    "src/server/multi_command_squasher.cc",
    "src/server/multi_command_squasher.h",
    "src/server/server_family.h",
    "src/server/transaction.cc"
  ],
  "modified_test_files": [
    "tests/dragonfly/cluster_test.py",
    "tests/dragonfly/generic_test.py",
    "tests/dragonfly/replication_test.py",
    "tests/dragonfly/seeder/README.md",
    "tests/dragonfly/seeder/__init__.py",
    "tests/dragonfly/seeder_test.py",
    "tests/dragonfly/snapshot_test.py",
    "tests/dragonfly/tiering_test.py"
  ]
}