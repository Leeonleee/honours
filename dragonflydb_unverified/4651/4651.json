{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4651,
  "instance_id": "dragonflydb__dragonfly-4651",
  "issue_numbers": [
    "4244"
  ],
  "base_commit": "91a3dd828350f5b25d603f13829aededbf40156c",
  "patch": "diff --git a/helio b/helio\nindex fd878df26816..875cedd2a0cd 160000\n--- a/helio\n+++ b/helio\n@@ -1,1 +1,1 @@\n-Subproject commit fd878df26816efc6ead76105b2f2d51e9a828b60\n+Subproject commit 875cedd2a0cd084fd15a3d5dbfe20150e19ffcef\ndiff --git a/src/core/compact_object.h b/src/core/compact_object.h\nindex 1d9187a0799f..fe1a3b336e09 100644\n--- a/src/core/compact_object.h\n+++ b/src/core/compact_object.h\n@@ -355,12 +355,16 @@ class CompactObj {\n     return taglen_ == EXTERNAL_TAG;\n   }\n \n+  // returns true if the value is stored in the cooling storage. Cooling storage has an item both\n+  // on disk and in memory.\n   bool IsCool() const {\n     assert(IsExternal());\n     return u_.ext_ptr.is_cool;\n   }\n \n   void SetExternal(size_t offset, uint32_t sz);\n+\n+  // Assigns a cooling record to the object together with its external slice.\n   void SetCool(size_t offset, uint32_t serialized_size, detail::TieredColdRecord* record);\n \n   struct CoolItem {\n@@ -368,6 +372,9 @@ class CompactObj {\n     size_t serialized_size;\n     detail::TieredColdRecord* record;\n   };\n+\n+  // Prerequisite: IsCool() is true.\n+  // Returns the external data of the object incuding its ColdRecord.\n   CoolItem GetCool() const;\n \n   void ImportExternal(const CompactObj& src);\ndiff --git a/src/server/tiered_storage.cc b/src/server/tiered_storage.cc\nindex 181320825057..2ecbc586f2d9 100644\n--- a/src/server/tiered_storage.cc\n+++ b/src/server/tiered_storage.cc\n@@ -407,10 +407,10 @@ bool TieredStorage::TryStash(DbIndex dbid, string_view key, PrimeValue* value) {\n   error_code ec;\n   if (OccupiesWholePages(value->Size())) {  // large enough for own page\n     id = KeyRef(dbid, key);\n-    ec = op_manager_->Stash(id, raw_string.view(), {});\n-  } else if (auto bin = bins_->Stash(dbid, key, raw_string.view(), {}); bin) {\n+    ec = op_manager_->Stash(id, raw_string.view());\n+  } else if (auto bin = bins_->Stash(dbid, key, raw_string.view()); bin) {\n     id = bin->first;\n-    ec = op_manager_->Stash(id, bin->second, {});\n+    ec = op_manager_->Stash(id, bin->second);\n   }\n \n   if (ec) {\ndiff --git a/src/server/tiering/disk_storage.cc b/src/server/tiering/disk_storage.cc\nindex 265050e2fc95..51f0336e9460 100644\n--- a/src/server/tiering/disk_storage.cc\n+++ b/src/server/tiering/disk_storage.cc\n@@ -136,10 +136,10 @@ void DiskStorage::MarkAsFree(DiskSegment segment) {\n   alloc_.Free(segment.offset, segment.length);\n }\n \n-std::error_code DiskStorage::Stash(io::Bytes bytes, io::Bytes footer, StashCb cb) {\n+std::error_code DiskStorage::Stash(io::Bytes bytes, StashCb cb) {\n   DCHECK_GT(bytes.length(), 0u);\n \n-  size_t len = bytes.size() + footer.size();\n+  size_t len = bytes.size();\n   int64_t offset = alloc_.Malloc(len);\n \n   // If we've run out of space, block and grow as much as needed\n@@ -158,8 +158,6 @@ std::error_code DiskStorage::Stash(io::Bytes bytes, io::Bytes footer, StashCb cb\n \n   UringBuf buf = PrepareBuf(len);\n   memcpy(buf.bytes.data(), bytes.data(), bytes.length());\n-  if (!footer.empty())\n-    memcpy(buf.bytes.data() + bytes.length(), footer.data(), footer.length());\n \n   auto io_cb = [this, cb, offset, buf, len](int io_res) {\n     if (io_res < 0) {\ndiff --git a/src/server/tiering/disk_storage.h b/src/server/tiering/disk_storage.h\nindex 8d47e37e0a05..768b708d676f 100644\n--- a/src/server/tiering/disk_storage.h\n+++ b/src/server/tiering/disk_storage.h\n@@ -15,6 +15,8 @@\n namespace dfly::tiering {\n \n // Disk storage controlled by asynchronous operations.\n+// Provides Random Access Read/Stash asynchronous interface around low level linux file.\n+// Handles ranges management and file growth via underlying ExternalAllocator.\n class DiskStorage {\n  public:\n   struct Stats {\n@@ -44,7 +46,7 @@ class DiskStorage {\n   // grow backing file. Returns error code if operation failed  immediately (most likely it failed\n   // to grow the backing file) or passes an empty segment if the final write operation failed.\n   // Bytes are copied and can be dropped before cb is resolved\n-  std::error_code Stash(io::Bytes bytes, io::Bytes footer, StashCb cb);\n+  std::error_code Stash(io::Bytes bytes, StashCb cb);\n \n   Stats GetStats() const;\n \ndiff --git a/src/server/tiering/op_manager.cc b/src/server/tiering/op_manager.cc\nindex 5e727d67cf0b..ee3f740746b1 100644\n--- a/src/server/tiering/op_manager.cc\n+++ b/src/server/tiering/op_manager.cc\n@@ -86,7 +86,7 @@ void OpManager::DeleteOffloaded(DiskSegment segment) {\n   }\n }\n \n-std::error_code OpManager::Stash(EntryId id_ref, std::string_view value, io::Bytes footer) {\n+std::error_code OpManager::Stash(EntryId id_ref, std::string_view value) {\n   auto id = ToOwned(id_ref);\n   unsigned version = pending_stash_ver_[id] = ++pending_stash_counter_;\n \n@@ -96,7 +96,7 @@ std::error_code OpManager::Stash(EntryId id_ref, std::string_view value, io::Byt\n   };\n \n   // May block due to blocking call to Grow.\n-  auto ec = storage_.Stash(buf_view, footer, std::move(io_cb));\n+  auto ec = storage_.Stash(buf_view, std::move(io_cb));\n   if (ec)\n     pending_stash_ver_.erase(ToOwned(id_ref));\n   return ec;\ndiff --git a/src/server/tiering/op_manager.h b/src/server/tiering/op_manager.h\nindex f78f14b898ec..9b43fc3f13a3 100644\n--- a/src/server/tiering/op_manager.h\n+++ b/src/server/tiering/op_manager.h\n@@ -66,8 +66,8 @@ class OpManager {\n   // Delete offloaded entry located at the segment.\n   void DeleteOffloaded(DiskSegment segment);\n \n-  // Stash (value, footer) to be offloaded. Both arguments are opaque to OpManager.\n-  std::error_code Stash(EntryId id, std::string_view value, io::Bytes footer);\n+  // Stash value to be offloaded. It is opaque to OpManager.\n+  std::error_code Stash(EntryId id, std::string_view value);\n \n   Stats GetStats() const;\n \ndiff --git a/src/server/tiering/small_bins.cc b/src/server/tiering/small_bins.cc\nindex 18009cbf1544..50a800186985 100644\n--- a/src/server/tiering/small_bins.cc\n+++ b/src/server/tiering/small_bins.cc\n@@ -13,7 +13,6 @@\n #include \"core/compact_object.h\"\n #include \"server/tiering/common.h\"\n #include \"server/tiering/disk_storage.h\"\n-#include \"server/tx_base.h\"\n \n namespace dfly::tiering {\n using namespace std;\n@@ -28,10 +27,10 @@ size_t StashedValueSize(string_view value) {\n }  // namespace\n \n std::optional<SmallBins::FilledBin> SmallBins::Stash(DbIndex dbid, std::string_view key,\n-                                                     std::string_view value, io::Bytes footer) {\n+                                                     std::string_view value) {\n   DCHECK_LT(value.size(), 2_KB);\n \n-  size_t value_bytes = StashedValueSize(value) + footer.size();\n+  size_t value_bytes = StashedValueSize(value);\n \n   std::optional<FilledBin> filled_bin;\n   if (2 /* num entries */ + current_bin_bytes_ + value_bytes >= kPageSize) {\n@@ -39,11 +38,7 @@ std::optional<SmallBins::FilledBin> SmallBins::Stash(DbIndex dbid, std::string_v\n   }\n \n   current_bin_bytes_ += value_bytes;\n-  string blob;\n-  blob.reserve(value.size() + footer.size());\n-  blob.append(value);\n-  blob.append(io::View(footer));\n-  auto [it, inserted] = current_bin_.emplace(std::make_pair(dbid, key), std::move(blob));\n+  auto [it, inserted] = current_bin_.emplace(std::make_pair(dbid, key), string(value));\n   CHECK(inserted);\n \n   DVLOG(2) << \"current_bin_bytes: \" << current_bin_bytes_\ndiff --git a/src/server/tiering/small_bins.h b/src/server/tiering/small_bins.h\nindex 1cdfb84b8471..d2f198f6e491 100644\n--- a/src/server/tiering/small_bins.h\n+++ b/src/server/tiering/small_bins.h\n@@ -8,14 +8,14 @@\n \n #include <optional>\n #include <string>\n-#include <unordered_map>\n #include <vector>\n \n #include \"server/tiering/disk_storage.h\"\n-#include \"server/tx_base.h\"\n \n namespace dfly::tiering {\n \n+using DbIndex = uint16_t;\n+\n // Small bins accumulate small values into larger bins that fill up 4kb pages.\n // SIMPLEST VERSION for now.\n class SmallBins {\n@@ -49,8 +49,7 @@ class SmallBins {\n   }\n \n   // Enqueue key/value pair for stash. Returns page to be stashed if it filled up.\n-  std::optional<FilledBin> Stash(DbIndex dbid, std::string_view key, std::string_view value,\n-                                 io::Bytes footer);\n+  std::optional<FilledBin> Stash(DbIndex dbid, std::string_view key, std::string_view value);\n \n   // Report that a stash succeeeded. Returns list of stored keys with calculated value locations.\n   KeySegmentList ReportStashed(BinId id, DiskSegment segment);\n",
  "test_patch": "diff --git a/src/server/tiering/disk_storage_test.cc b/src/server/tiering/disk_storage_test.cc\nindex 7fd2ca9d8bc4..89dd39f022ee 100644\n--- a/src/server/tiering/disk_storage_test.cc\n+++ b/src/server/tiering/disk_storage_test.cc\n@@ -37,7 +37,7 @@ struct DiskStorageTest : public PoolTestBase {\n   void Stash(size_t index, string value) {\n     pending_ops_++;\n     auto buf = make_shared<string>(value);\n-    storage_->Stash(io::Buffer(*buf), {}, [this, index, buf](io::Result<DiskSegment> segment) {\n+    storage_->Stash(io::Buffer(*buf), [this, index, buf](io::Result<DiskSegment> segment) {\n       EXPECT_TRUE(segment);\n       EXPECT_GT(segment->length, 0u);\n       segments_[index] = *segment;\ndiff --git a/src/server/tiering/op_manager_test.cc b/src/server/tiering/op_manager_test.cc\nindex dc3f00d50fe9..7299fc79e7d4 100644\n--- a/src/server/tiering/op_manager_test.cc\n+++ b/src/server/tiering/op_manager_test.cc\n@@ -79,9 +79,9 @@ TEST_F(OpManagerTest, SimpleStashesWithReads) {\n     Open();\n \n     for (unsigned i = 0; i < 100; i++) {\n-      EXPECT_FALSE(Stash(i, absl::StrCat(\"VALUE\", i, \"cancelled\"), {}));\n-      EXPECT_FALSE(Stash(i, absl::StrCat(\"VALUE\", i, \"cancelled\"), {}));\n-      EXPECT_FALSE(Stash(i, absl::StrCat(\"VALUE\", i, \"real\"), {}));\n+      EXPECT_FALSE(Stash(i, absl::StrCat(\"VALUE\", i, \"cancelled\")));\n+      EXPECT_FALSE(Stash(i, absl::StrCat(\"VALUE\", i, \"cancelled\")));\n+      EXPECT_FALSE(Stash(i, absl::StrCat(\"VALUE\", i, \"real\")));\n     }\n \n     EXPECT_EQ(GetStats().pending_stash_cnt, 100);\n@@ -106,7 +106,7 @@ TEST_F(OpManagerTest, DeleteAfterReads) {\n   pp_->at(0)->Await([this] {\n     Open();\n \n-    EXPECT_FALSE(Stash(0u, absl::StrCat(\"DATA\"), {}));\n+    EXPECT_FALSE(Stash(0u, absl::StrCat(\"DATA\")));\n     while (stashed_.empty())\n       util::ThisFiber::SleepFor(1ms);\n \n@@ -135,7 +135,7 @@ TEST_F(OpManagerTest, ReadSamePageDifferentOffsets) {\n       numbers += number;\n     }\n \n-    EXPECT_FALSE(Stash(0u, numbers, {}));\n+    EXPECT_FALSE(Stash(0u, numbers));\n     while (stashed_.empty())\n       util::ThisFiber::SleepFor(1ms);\n \n@@ -157,7 +157,7 @@ TEST_F(OpManagerTest, Modify) {\n   pp_->at(0)->Await([this] {\n     Open();\n \n-    Stash(0u, \"D\", {});\n+    Stash(0u, \"D\");\n     while (stashed_.empty())\n       util::ThisFiber::SleepFor(1ms);\n \ndiff --git a/src/server/tiering/small_bins_test.cc b/src/server/tiering/small_bins_test.cc\nindex 037e4180b1c1..e1f3c051e907 100644\n--- a/src/server/tiering/small_bins_test.cc\n+++ b/src/server/tiering/small_bins_test.cc\n@@ -30,7 +30,7 @@ TEST_F(SmallBinsTest, SimpleStashRead) {\n   // Fill single bin\n   std::optional<SmallBins::FilledBin> bin;\n   for (unsigned i = 0; !bin; i++)\n-    bin = bins_.Stash(0, absl::StrCat(\"k\", i), absl::StrCat(\"v\", i), {});\n+    bin = bins_.Stash(0, absl::StrCat(\"k\", i), absl::StrCat(\"v\", i));\n \n   // Verify cut locations point to correct values\n   auto segments = bins_.ReportStashed(bin->first, DiskSegment{0, 4_KB});\n@@ -47,7 +47,7 @@ TEST_F(SmallBinsTest, SimpleDeleteAbort) {\n   std::optional<SmallBins::FilledBin> bin;\n   unsigned i = 0;\n   for (; !bin; i++)\n-    bin = bins_.Stash(0, absl::StrCat(\"k\", i), absl::StrCat(\"v\", i), {});\n+    bin = bins_.Stash(0, absl::StrCat(\"k\", i), absl::StrCat(\"v\", i));\n \n   // Delete all even values\n   for (unsigned j = 0; j <= i; j += 2)\n@@ -69,7 +69,7 @@ TEST_F(SmallBinsTest, PartialStashDelete) {\n   std::optional<SmallBins::FilledBin> bin;\n   unsigned i = 0;\n   for (; !bin; i++)\n-    bin = bins_.Stash(0, absl::StrCat(\"k\", i), absl::StrCat(\"v\", i), {});\n+    bin = bins_.Stash(0, absl::StrCat(\"k\", i), absl::StrCat(\"v\", i));\n \n   // Delete all even values\n   for (unsigned j = 0; j <= i; j += 2)\n@@ -103,7 +103,7 @@ TEST_F(SmallBinsTest, PartialStashDelete) {\n TEST_F(SmallBinsTest, UpdateStatsAfterDelete) {\n   // caused https://github.com/dragonflydb/dragonfly/issues/3240\n   for (unsigned i = 0; i < 10; i++) {\n-    auto spilled_bin = bins_.Stash(0, absl::StrCat(\"k\", i), SmallString(128), {});\n+    auto spilled_bin = bins_.Stash(0, absl::StrCat(\"k\", i), SmallString(128));\n     ASSERT_FALSE(spilled_bin);\n   }\n \n",
  "problem_statement": "Deadlock inside Dns resolve function\n**Describe the bug**\r\nFirst of all: I am not entirely sure if this is a bug in our java redis cluster client library or in dragonfly itself.\r\nHowever since i have already once created a ticket here where indeed dragonfly was not handling things truly like redis did, i want to create the issue here and see where it leads us.\r\n\r\nWe are seeing rising connections on our dragonfly instance after some time.\r\nI suppose that the lattice library is refreshing its internal representation of the cluster topology, as we do get these entries (filtered for these) via the `client list` command:\r\n\r\n\r\n<details>\r\n  <summary>client list output</summary>\r\n\r\n```\r\nid=105831 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=12 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67701 idle=67701 db=0 flags=t phase=process lib-name= lib-ver=\r\nid=105832 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=13 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67701 idle=67701 db=0 flags=t phase=process lib-name= lib-ver=\r\nid=105835 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=15 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67693 idle=67693 db=0 flags=t phase=process lib-name= lib-ver=\r\nid=105836 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=16 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67691 idle=67691 db=0 flags=t phase=process lib-name= lib-ver=\r\nid=105837 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=19 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67691 idle=67691 db=0 flags=t phase=process lib-name= lib-ver=\r\nid=105838 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=20 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67690 idle=67690 db=0 flags=t phase=process lib-name= lib-ver=\r\nid=105839 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=21 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67690 idle=67690 db=0 flags=t phase=process lib-name= lib-ver=\r\nid=105842 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=22 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67690 idle=67690 db=0 flags=t phase=process lib-name= lib-ver=\r\nid=105849 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=24 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67657 idle=67657 db=0 flags=t phase=process lib-name= lib-ver=\r\nid=105854 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=27 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67636 idle=67636 db=0 flags=t phase=process lib-name= lib-ver=\r\nid=105857 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=31 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67624 idle=67624 db=0 flags=t phase=process lib-name= lib-ver=\r\nid=105870 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=37 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67563 idle=67503 db=0 flags=at phase=shutting_down lib-name= lib-ver=\r\nid=105889 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=38 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67477 idle=67477 db=0 flags=t phase=process lib-name= lib-ver=\r\nid=105894 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=39 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67458 idle=67458 db=0 flags=t phase=process lib-name= lib-ver=\r\nid=105899 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=40 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67438 idle=67438 db=0 flags=t phase=process lib-name= lib-ver=\r\nid=105900 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=41 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67432 idle=67432 db=0 flags=t phase=process lib-name= lib-ver=\r\nid=105905 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=42 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67418 idle=67418 db=0 flags=t phase=process lib-name= lib-ver=\r\nid=105906 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=43 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67418 idle=67418 db=0 flags=t phase=process lib-name= lib-ver=\r\nid=105907 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=44 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67416 idle=67416 db=0 flags=t phase=process lib-name= lib-ver=\r\nid=105910 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=45 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67401 idle=67401 db=0 flags=t phase=process lib-name= lib-ver=\r\nid=105911 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=46 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67401 idle=67401 db=0 flags=t phase=process lib-name= lib-ver=\r\nid=105914 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=47 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67393 idle=67393 db=0 flags=t phase=process lib-name= lib-ver=\r\nid=105915 addr=0.0.0.0:0 laddr=10.137.60.6:6379 fd=48 name=lettuce#ClusterTopologyRefresh tid=0 irqmatch=0 age=67391 idle=67331 db=0 flags=at phase=shutting_down lib-name= lib-ver=\r\n```\r\n</details>\r\n\r\nThey did have this issue in the past and say that it is fixed, so my hope is that this is just another small implementation detail like last time.\r\n\r\nrefs: https://github.com/redis/lettuce/issues/1736\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Create a dragonfly cluster in emulated mode with multiple replicas\r\n2. Run a java application which is using the lattice library `io.lettuce:lettuce-core:6.5.0.RELEASE`\r\n3. delete one pod after the other and wait for them to come back before proceeding with the next\r\n4. See connection count rising\r\n\r\n**Expected behavior**\r\nConnection count should not increase with old aged connections.\r\n\r\n**Screenshots**\r\n-\r\n\r\n**Environment (please complete the following information):**\r\n - OS: bottlerocket 1.26.1\r\n - Kernel: # Command: 6.1.112\r\n - Containerized?: Kubernetes\r\n - Dragonfly Version: 1.25.2\r\n\r\n**Reproducible Code Snippet**\r\n-\r\n\r\n**Additional context**\r\n-\n",
  "hints_text": "I just checked the docs and would've expected that due to the default value of `--tcp_keepalive` (300) the connections should've been dropped. Am I wrong?\nHi @applike-ss \r\nIf it's easy to reproduce on your side, will you be able to record the traffic (like with `tcpdump` or wireshark) and see if the connections are really closed or not?\r\nIf they remain open, then it's a client-side issue. If they are closed, Dragonfly should remove them (and not list them).\nI did in fact not write a demo to test it and our traffic is way too big to monitor/dump that.\r\n\r\nIf the connections were closed (which dragonfly should do upon tcp_keepalive timeout(?), then they should also not be listed. Correct?\nIf the connections are closed, indeed Dragonfly should not list them.\r\ntcp_keepalive is one way of determining that a connection was closed, but usually that is not needed because the client simply closes the connection (like with `FIN`/`RST`). When this doesn't happen (like the entire machine is suddenly offline or crashed), then tcp_keepalive is useful.\nYou can see in the spoiler that the connections are more than half a day old. So i believe this (the tcp_keepalive connection active detection) might not work as expected.\nPerhaps I'm missing something, but could it be that the client is still running, thus keeping the connection alive?\nThat's actually not the case, as we re-deployed the flink application which completely removes all taskmanagers (those are doing the redis stuff).\r\n\r\nI have additional context: Our dragonfly's were not rescheduled, but they restarted the container due to OOM (due to the many connections)\nDo you guys use SSL/TLS when connecting to Dragonfly?\r\nCould you also provide `ss -e | grep redis` output on your Dragonfly node?\nAnd also the output of `client list` at the same time so we could match the states from both outputs.\nAnd also please execute \"DEBUG STACKTRACE\" via redis-cli. The output will be sent to the info log of dragonfly.\r\nit has lines containing `scheduler.cc` followed up by stack traces. we need all of that, please \ud83d\ude4f\ud83c\udffc \n@chakaz  the fact that \"client list\" has clients in the state of \"process\"  **might** indicate a bug in Dragonfly, a deadlock. we will see.\n> Do you guys use SSL/TLS when connecting to Dragonfly? Could you also provide `ss -e | grep redis` output on your Dragonfly node?\r\n\r\nDoing this on the k8s host where the pod/container in doubt is running, it returns with no output. Same if a grep for dragonfly. Same if I grep for the pods IP.\r\n\r\n> And also the output of `client list` at the same time so we could match the states from both outputs.\r\n\r\nSeems there is nothing to match when the `ss` command did not return anything related to dragonfly. The total amount of lines returned by `ss -e` without any grep are 263, so rather normal.\r\n\r\n> And also please execute \"DEBUG STACKTRACE\" via redis-cli. The output will be sent to the info log of dragonfly. it has lines containing `scheduler.cc` followed up by stack traces. we need all of that, please \ud83d\ude4f\ud83c\udffc\r\n\r\nThis one will take some time reading \ud83d\ude48 It is the complete log from the start ~21 hours ago until now. Please also note how the snapshotting seems to have stopped at some point. Not critical for us, but it would definitely be for some people that rely on all data being persisted. We only use it as a cache, so it \"just\" results in a slightly bigger bill from our cloud provider. \r\n[dragonfly-0.log](https://github.com/user-attachments/files/18003790/dragonfly-0.log)\r\n\r\nLooking forward to hear back from you guys and whenever you need more details, let me know!\nYeah, this helped. It's a deadlock. \r\nCan you please run `redis-cli LASTSAVE` and tell me whether it's stuck?\nI suspect that this fiber is the root cause of deadlock:\r\n```\r\n------------ Fiber shard_stable_sync_read (suspended) ------------\r\n0xaaaab45df584  boost::context::detail::fiber_ontop<>()\r\n0xaaaab45e0198  util::fb2::detail::FiberInterface::SwitchTo()\r\n0xaaaab45dcdb0  util::fb2::detail::Scheduler::Preempt()\r\n0xaaaab3e3c1a8  util::fb2::EventCount::await<>()\r\n0xaaaab4167690  dfly::Transaction::Execute()\r\n0xaaaab4167788  dfly::Transaction::ScheduleSingleHop()\r\n0xaaaab3e75a54  dfly::GenericFamily::Del()\r\n0xaaaab414a82c  dfly::CommandId::Invoke()\r\n0xaaaab3e9bf90  dfly::Service::InvokeCmd()\r\n0xaaaab3e9faf0  dfly::Service::DispatchCommand()\r\n0xaaaab41124fc  dfly::DflyShardReplica::ExecuteTx()\r\n```\r\n\r\n@applike-ss  can you please run `DEBUG STACKTRACE` again and send me the logs, so I could compare the fiber states?\n> Yeah, this helped. It's a deadlock. Can you please run `redis-cli LASTSAVE` and tell me whether it's stuck?\r\n\r\nI can do once the issue arises again. I had to re-create the dragonfly cluster to make it work again as this is happening on a production system. I did however see commands like INFO stuck in the past, if that helps already.\r\n\r\n\r\n\r\n> I suspect that this fiber is the root cause of deadlock:\r\n> \r\n> ```\r\n> ------------ Fiber shard_stable_sync_read (suspended) ------------\r\n> 0xaaaab45df584  boost::context::detail::fiber_ontop<>()\r\n> 0xaaaab45e0198  util::fb2::detail::FiberInterface::SwitchTo()\r\n> 0xaaaab45dcdb0  util::fb2::detail::Scheduler::Preempt()\r\n> 0xaaaab3e3c1a8  util::fb2::EventCount::await<>()\r\n> 0xaaaab4167690  dfly::Transaction::Execute()\r\n> 0xaaaab4167788  dfly::Transaction::ScheduleSingleHop()\r\n> 0xaaaab3e75a54  dfly::GenericFamily::Del()\r\n> 0xaaaab414a82c  dfly::CommandId::Invoke()\r\n> 0xaaaab3e9bf90  dfly::Service::InvokeCmd()\r\n> 0xaaaab3e9faf0  dfly::Service::DispatchCommand()\r\n> 0xaaaab41124fc  dfly::DflyShardReplica::ExecuteTx()\r\n> ```\r\n> \r\n> @applike-ss can you please run `DEBUG STACKTRACE` again and send me the logs, so I could compare the fiber states?\r\n\r\nIf it helps to get it from another hanging instance, then I can sent it to you once it occurs again.\ndoes it happen frequently?  when it happens again, please run `DEBUG STACKTRACE` 2-3 times with  a few seconds delay between invocations.  This will help me to understand what are the fibers that are getting stuck. \r\n\r\nIn addition (and I do not know if it's related) there is a problem in dns resolve code and that's why your periodic snapshotting stops - it's just being stuck there. For  me to understand what happens, can you please run dragonfly process with: `--vmodue=dns_resolve=1` ? it will print bunch of logs that may help identifying the issue.\n> does it happen frequently?\r\n\r\nIt happens to us multiple times a week, sometimes daily.\r\n\r\n> when it happens again, please run DEBUG STACKTRACE 2-3 times with a few seconds delay between invocations.\r\n\r\nWill do and come back to you once i have the logs ready.\r\n\r\n> In addition (and I do not know if it's related) there is a problem in dns resolve code and that's why your periodic snapshotting stops\r\n\r\nInteresting! I'll enable that option soon.\nI am still checking our systems daily multiple times, but of course now that we want to track it down it doesn't happen \ud83d\ude48\r\n\r\nI will continue checking and report back once it occurs again, likely today evening or monday.\nThe issue happened again, however this time i did see in the metrics + logs that the snapshotting stopped 2 hours after the issue occured.\r\nI wasn't able to fire the commands you told me to fire, as someone from a different team saw things going down over the weekend and already fixed it without notifying my team in the chat via a mention.\r\nUsual logs (with the `--vmodue=dns_resolve=1` setting) seem to be:\r\n```\r\nDnsResolveStart\r\nsfd: 8 1/0\r\nsfd: 8 0/0\r\n```\r\nWhen the issue occurs, then it turns out to be:\r\n```\r\nDnsResolveStart\r\nsfd: 8 1/0\r\n```\r\nSo the last line is missing.\r\n\r\nNot sure if this is actually an issue or whether it's a follow up problem occuring due to the initial problem 2 hours ago.\r\n\r\n~3:15 is when thing stopped working, ~5:15 is when snapshotting stopped working:\r\n![image](https://github.com/user-attachments/assets/b5af92d2-219b-461c-adf6-378f2c893374)\r\n\r\nI think, I will disable snapshots for the moment and see if that helps us until the next release.\r\nAm I right to assume that you tracked down the dead-lock already?\nLet me add some metrics for you to check out as well.\r\nI am a bit surprised by some execution times (multiple seconds), but maybe this is just grafanas metrics overview doing stupid stuff as it almost looks like it is showing counter values instead of gauge values for seconds (or is it the metric itself exposing its value incorrect? \ud83e\udd14 ).\r\n![image](https://github.com/user-attachments/assets/2ec8b545-3d8c-4afc-b04b-30f5fe38d39d)\r\n![image](https://github.com/user-attachments/assets/00c0a312-8af9-4fe2-bd28-82b54aace5cc)\r\n\n@applike-ss do you still have this log? I would like to check it out \nYes, we keep them for some time. Hope it helps.\r\n\r\n[dragonfly-logs.csv](https://github.com/user-attachments/files/18073155/dragonfly-logs.csv)\r\n\n@applike-ss  we released 1.25.5 While it does not solve this issue, it does provide more logs to understand why DnsResolve does not complete in your environment. Can you please upgrade and run it with `--vmodue=dns_resolve=2` ? \r\n\r\nThanks\nUpgraded already, will observe and report back once we had another occurence\nSo far the issue has not happened again. Current cluster uptime is 5 days.\r\n\r\nDid you already include potential fixes for the detected deadlock?\nNot really. Let's wait more if it reproduces.\n> And also please execute \"DEBUG STACKTRACE\" via redis-cli. The output will be sent to the info log of dragonfly. it has lines containing `scheduler.cc` followed up by stack traces. we need all of that, please \ud83d\ude4f\ud83c\udffc\r\n\r\nWe again see idle connections with way too old age. Here's some more Stacktraces:\r\n* [Stacktrace 1.csv](https://github.com/user-attachments/files/18165752/Untitled.discover.search.4.csv)\r\n* [Stacktrace 2.csv](https://github.com/user-attachments/files/18165832/Stacktrace.2.csv)\r\n* [Stacktrace 3.csv](https://github.com/user-attachments/files/18165835/Stacktrace.3.csv)\r\n\r\n\r\n> Can you please run redis-cli LASTSAVE and tell me whether it's stuck?\r\n\r\nCurrently not.\r\n\r\nHope this helps debugging it further.\nFor the moment we stop using dragonfly as it does not work the way we need it and due to our current use-case being simple enough that we can use an in-memory kvstore.\r\nThat means I will not be able to provide new data from the same dragonfly clusters mentioned in this issue from now on.\r\nWe will re-evaluate dragonfly once we need a redis alternative again.\nI am closing the issue as non-actionable\nWe started using evaluating dragonfly again and we are facing the issue of the dragonfly process stalling again.\n\nHere's the last logs @romange \n\n```\n2025-02-11T20:15:00.001+01:00I20250211 19:15:00.001545    11 dns_resolve.cc:165] DnsResolveStart\n2025-02-11T20:15:00.001+01:00I20250211 19:15:00.001762    11 dns_resolve.cc:53] sfd: 11 1/0\n2025-02-11T20:15:00.007+01:00I20250211 19:15:00.007042    11 dns_resolve.cc:53] sfd: 11 0/0\n2025-02-11T20:15:00.065+01:00I20250211 19:15:00.065469    11 dns_resolve.cc:165] DnsResolveStart\n2025-02-11T20:15:00.065+01:00I20250211 19:15:00.065647    11 dns_resolve.cc:53] sfd: 11 1/0\n2025-02-11T20:15:00.068+01:00I20250211 19:15:00.068156    11 dns_resolve.cc:53] sfd: 11 0/0\n2025-02-11T20:15:00.118+01:00I20250211 19:15:00.118497    11 dns_resolve.cc:165] DnsResolveStart\n2025-02-11T20:15:00.118+01:00I20250211 19:15:00.118682    11 dns_resolve.cc:53] sfd: 11 1/0\n2025-02-11T20:15:00.123+01:00I20250211 19:15:00.123313    11 dns_resolve.cc:53] sfd: 11 0/0\n2025-02-11T20:15:00.184+01:00I20250211 19:15:00.184723    11 dns_resolve.cc:165] DnsResolveStart\n2025-02-11T20:15:00.184+01:00I20250211 19:15:00.184906    11 dns_resolve.cc:53] sfd: 11 1/0\n2025-02-11T20:15:00.186+01:00I20250211 19:15:00.186897    11 dns_resolve.cc:53] sfd: 11 0/0\n2025-02-11T20:15:00.292+01:00I20250211 19:15:00.292305    11 dns_resolve.cc:165] DnsResolveStart\n2025-02-11T20:15:00.292+01:00I20250211 19:15:00.292495    11 dns_resolve.cc:53] sfd: 11 1/0\n2025-02-11T20:15:00.294+01:00I20250211 19:15:00.294160    11 dns_resolve.cc:53] sfd: 11 0/0\n2025-02-11T20:15:00.418+01:00I20250211 19:15:00.418362    11 dns_resolve.cc:165] DnsResolveStart\n2025-02-11T20:15:00.418+01:00I20250211 19:15:00.418558    11 dns_resolve.cc:53] sfd: 11 1/0\n2025-02-11T20:15:00.423+01:00I20250211 19:15:00.423133    11 dns_resolve.cc:53] sfd: 11 0/0\n2025-02-11T20:15:00.524+01:00I20250211 19:15:00.524103    11 save_stages_controller.cc:337] Saving \"s3://my-bucket/dump-summary.dfs\" finished after 1 s\n2025-02-11T20:20:00.001+01:00I20250211 19:20:00.001576    11 dns_resolve.cc:165] DnsResolveStart\n2025-02-11T20:20:00.001+01:00I20250211 19:20:00.001780    11 dns_resolve.cc:53] sfd: 11 1/0\n2025-02-11T20:20:00.008+01:00I20250211 19:20:00.008801    11 dns_resolve.cc:53] sfd: 11 0/0\n2025-02-11T20:20:00.055+01:00I20250211 19:20:00.055711    11 dns_resolve.cc:165] DnsResolveStart\n2025-02-11T20:20:00.055+01:00I20250211 19:20:00.055929    11 dns_resolve.cc:53] sfd: 11 1/0\n2025-02-12T04:54:45.434+01:00I20250212 03:54:45.434288    11 server_family.cc:2769] Replicating 10.11.12.13:9999\n```\n\nThe redis-cli commands (cluster nodes) are stalling upon execution.\n\nI have not yet restarted the pod where the process is stalling, should i execute something?\nAnything else is stalling? Can you perform \"redis-cli get foo\" ? \"redis-cli info all\" ?\n> redis-cli get foo\n\nBoth of these are also stalling. The cluster is currently on v1.26.2.\nis there anything in the logs? does `redis-cli PING` succeed?\n`redis-cli DEBUG STACKTRACE` ?\nPing succeeds, Stacktrace as well. I will collect the logs and post them here soon.\nThe logs don't indicate any errors, panics, exceptions as far as i can see.\nIn the provided file they start at yesterday 19:10UTC and are not filtered, only obfuscation for names/ips was done and retrieved from our logs frontend.\nThe logs posted in the code block earlier are directly grabbed from the pod via lens and use the local UTC+1 timestamp.\nWe did get a target down metric today at 3:57UTC from dragonfly, so much later than the last s3 save.\n\n[dragonfly-1-logs-fixed.csv](https://github.com/user-attachments/files/18763738/dragonfly-1-logs-fixed.csv)\n\nSomething interesting in the metrics: CPU/Memory don't change a lot, but network metrics do.\nThe first arrow is pointing at 19:17UTC and the second one is pointing at 3:57UTC.\n<img width=\"1709\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/399e53e8-2a42-40ff-b467-e2868c1b8212\" />\nThe way you extracted the logs, it's not possible to separate one log from another. Do you have an access to the INFO log on that pod? it has all the log entry prefixed with something like that `I20250212 09:12:18.385618 235838 scheduler.cc:460]`\ni.e. i need the timestamp, thread id, file/lineno prefix to be able to separate entries. if you prefer you can DM me with the log\nI can add more columns to the export, if that is what you need. Also the log level.\n> i.e. i need the timestamp, thread id, file/lineno prefix to be able to separate entries. if you prefer you can DM me with the log\n\nHow can I reach you best?\nroman  at dragonflydb dot io or romange at [discord ](https://discord.gg/HsPjXGVH85)\nWrote you on discord \ud83d\udc4d \nI indeed see that the issue is with dns resolve. Really weird.\n```\nI20250211 19:20:00.055711    11 dns_resolve.cc:165] DnsResolveStart\nI20250211 19:20:00.055929    11 dns_resolve.cc:53] sfd: 11 1/0\n```\nand then the stack trace:\n```\nI20250212 07:12:57.717233    11 scheduler.cc:460] ------------ Fiber  (suspended) ------------\n0xaaaaab4794c4  boost::context::detail::fiber_ontop<>()\n0xaaaaab47a0d8  util::fb2::detail::FiberInterface::SwitchTo()\n0xaaaaab476d40  util::fb2::detail::Scheduler::Preempt()\n0xaaaaab4b4300  util::fb2::DnsResolve()\n0xaaaaab119f28  util::aws::HttpClient::Resolve()\n0xaaaaab11a29c  util::aws::HttpClient::Connect()\n0xaaaaab11aaf4  util::aws::HttpClient::MakeRequest()\n0xaaaaab27f2d4  std::_Function_handler<>::_M_invoke()\n0xaaaaab2dc400  smithy::components::tracing::TracingUtils::MakeCallWithTiming<>()\n0xaaaaab2ae614  Aws::Client::AWSClient::AttemptOneRequest()\n0xaaaaab2b9a6c  Aws::Client::AWSClient::AttemptExhaustively()\n0xaaaaab2bc248  Aws::Client::AWSXMLClient::MakeRequest()\n0xaaaaab2bc758  Aws::Client::AWSXMLClient::MakeRequest()\n0xaaaaab1b7bd8  Aws::S3::S3Client::CreateMultipartUpload()::{lambda()#1}::operator()()\n0xaaaaab1b7cd0  std::_Function_handler<>::_M_invoke()\n0xaaaaab230598  smithy::components::tracing::TracingUtils::MakeCallWithTiming<>()\n0xaaaaab1d4d34  Aws::S3::S3Client::CreateMultipartUpload()\n0xaaaaab1138f0  util::aws::S3WriteFile::Open()\n```\n@applike-ss  answered  you there :)\nThe problem was identified and the fix will be pushed into 1.28\nThank you so much for hunting this one down!",
  "created_at": "2025-02-24T10:44:05Z",
  "modified_files": [
    "helio",
    "src/core/compact_object.h",
    "src/server/tiered_storage.cc",
    "src/server/tiering/disk_storage.cc",
    "src/server/tiering/disk_storage.h",
    "src/server/tiering/op_manager.cc",
    "src/server/tiering/op_manager.h",
    "src/server/tiering/small_bins.cc",
    "src/server/tiering/small_bins.h"
  ],
  "modified_test_files": [
    "src/server/tiering/disk_storage_test.cc",
    "src/server/tiering/op_manager_test.cc",
    "src/server/tiering/small_bins_test.cc"
  ]
}