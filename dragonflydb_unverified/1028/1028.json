{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 1028,
  "instance_id": "dragonflydb__dragonfly-1028",
  "issue_numbers": [
    "970"
  ],
  "base_commit": "fc66dbb2cf76c14d2b9c317a02531441e9133178",
  "patch": "diff --git a/src/server/server_family.cc b/src/server/server_family.cc\nindex 1ebd36aa04af..cf1aad96aec4 100644\n--- a/src/server/server_family.cc\n+++ b/src/server/server_family.cc\n@@ -50,7 +50,7 @@ extern \"C\" {\n using namespace std;\n \n ABSL_FLAG(string, dir, \"\", \"working directory\");\n-ABSL_FLAG(string, dbfilename, \"dump\", \"the filename to save/load the DB\");\n+ABSL_FLAG(string, dbfilename, \"dump-{timestamp}\", \"the filename to save/load the DB\");\n ABSL_FLAG(string, requirepass, \"\",\n           \"password for AUTH authentication. \"\n           \"If empty can also be set with DFLY_PASSWORD environment variable.\");\n@@ -109,6 +109,10 @@ string UnknownCmd(string cmd, CmdArgList args) {\n                       StrJoin(args.begin(), args.end(), \", \", CmdArgListFormatter()));\n }\n \n+void SubstituteFilenameTsPlaceholder(fs::path* filename, std::string_view replacement) {\n+  *filename = absl::StrReplaceAll(filename->string(), {{\"{timestamp}\", replacement}});\n+}\n+\n string InferLoadFile(fs::path data_dir) {\n   const auto& dbname = GetFlag(FLAGS_dbfilename);\n   if (dbname.empty())\n@@ -118,19 +122,22 @@ string InferLoadFile(fs::path data_dir) {\n   if (fs::exists(fl_path))\n     return fl_path.generic_string();\n \n+  SubstituteFilenameTsPlaceholder(&fl_path, \"*\");\n   if (!fl_path.has_extension()) {\n-    std::string glob = absl::StrCat(fl_path.generic_string(), \"*\");\n-    io::Result<io::StatShortVec> short_vec = io::StatFiles(glob);\n-\n-    if (short_vec) {\n-      auto it = std::find_if(short_vec->rbegin(), short_vec->rend(), [](const auto& stat) {\n-        return absl::EndsWith(stat.name, \".rdb\") || absl::EndsWith(stat.name, \"summary.dfs\");\n-      });\n-      if (it != short_vec->rend())\n-        return it->name;\n-    } else {\n-      LOG(WARNING) << \"Could not stat \" << glob << \", error \" << short_vec.error().message();\n-    }\n+    fl_path += \"*\";\n+  }\n+  io::Result<io::StatShortVec> short_vec = io::StatFiles(fl_path.generic_string());\n+\n+  if (short_vec) {\n+    // io::StatFiles returns a list of sorted files. Because our timestamp format has the same\n+    // time order and lexicographic order we iterate from the end to find the latest snapshot.\n+    auto it = std::find_if(short_vec->rbegin(), short_vec->rend(), [](const auto& stat) {\n+      return absl::EndsWith(stat.name, \".rdb\") || absl::EndsWith(stat.name, \"summary.dfs\");\n+    });\n+    if (it != short_vec->rend())\n+      return it->name;\n+  } else {\n+    LOG(WARNING) << \"Could not stat \" << fl_path << \", error \" << short_vec.error().message();\n   }\n   return string{};\n }\n@@ -258,20 +265,36 @@ string FormatTs(absl::Time now) {\n   return absl::FormatTime(\"%Y-%m-%dT%H:%M:%S\", now, absl::LocalTimeZone());\n }\n \n-void ExtendFilename(absl::Time now, absl::AlphaNum postfix, fs::path* filename) {\n+void ExtendDfsFilename(absl::AlphaNum postfix, fs::path* filename) {\n   filename->replace_extension();  // clear if exists\n-  *filename += StrCat(\"-\", FormatTs(now), \"-\", postfix, \".dfs\");\n+  *filename += StrCat(\"-\", postfix, \".dfs\");\n+}\n+\n+void ExtendDfsFilenameWithShard(int shard, fs::path* filename) {\n+  // dragonfly snapshot.\n+  ExtendDfsFilename(absl::Dec(shard, absl::kZeroPad4), filename);\n }\n \n-void ExtendFilenameWithShard(absl::Time now, int shard, fs::path* filename) {\n-  if (shard < 0) {\n-    if (!filename->has_extension()) {\n-      *filename += StrCat(\"-\", FormatTs(now), \".rdb\");\n+GenericError ValidateFilenameExtension(const fs::path& filename, bool new_version) {\n+  if (!filename.has_extension()) {\n+    return {};\n+  }\n+\n+  if (new_version) {\n+    if (absl::EqualsIgnoreCase(filename.extension().c_str(), \".rdb\")) {\n+      return {absl::StrCat(\n+          \"DF snapshot format is used but '.rdb' extension was given. Use --nodf_snapshot_format \"\n+          \"or remove the filename extension.\")};\n+    } else {\n+      return {absl::StrCat(\"DF snapshot format requires no filename extension. Got \\\"\",\n+                           filename.extension().c_str(), \"\\\"\")};\n     }\n-  } else {\n-    // dragonfly snapshot.\n-    ExtendFilename(now, absl::Dec(shard, absl::kZeroPad4), filename);\n   }\n+  if (!new_version && !absl::EqualsIgnoreCase(filename.extension().c_str(), \".rdb\")) {\n+    return {absl::StrCat(\"Bad filename extension \\\"\", filename.extension().c_str(),\n+                         \"\\\" for SAVE with type RDB\")};\n+  }\n+  return {};\n }\n \n void SlowLog(CmdArgList args, ConnectionContext* cntx) {\n@@ -369,7 +392,15 @@ ServerFamily::ServerFamily(Service* service) : service_(*service) {\n   } else if (cluster_mode == \"emulated\") {\n     is_emulated_cluster_ = true;\n   } else {\n-    LOG(FATAL) << \"invalid cluster_mode. Exiting...\";\n+    LOG(ERROR) << \"invalid cluster_mode. Exiting...\";\n+    exit(1);\n+  }\n+\n+  if (auto ec =\n+          ValidateFilenameExtension(GetFlag(FLAGS_dbfilename), GetFlag(FLAGS_df_snapshot_format));\n+      ec) {\n+    LOG(ERROR) << ec.Format();\n+    exit(1);\n   }\n }\n \n@@ -470,7 +501,12 @@ void ServerFamily::Shutdown() {\n // It starts one more fiber that waits for all load fibers to finish and returns the first\n // error (if any occured) with a future.\n Future<std::error_code> ServerFamily::Load(const std::string& load_path) {\n-  CHECK(absl::EndsWith(load_path, \".rdb\") || absl::EndsWith(load_path, \"summary.dfs\"));\n+  if (!(absl::EndsWith(load_path, \".rdb\") || absl::EndsWith(load_path, \"summary.dfs\"))) {\n+    LOG(ERROR) << \"Bad filename extension \\\"\" << load_path << \"\\\"\";\n+    Promise<std::error_code> ec_promise;\n+    ec_promise.set_value(make_error_code(errc::invalid_argument));\n+    return ec_promise.get_future();\n+  }\n \n   vector<std::string> paths{{load_path}};\n \n@@ -854,20 +890,19 @@ static void RunStage(bool new_version, std::function<void(unsigned)> cb) {\n   }\n };\n \n-using PartialSaveOpts =\n-    tuple<const fs::path& /*filename*/, const fs::path& /*path*/, absl::Time /*start*/>;\n+using PartialSaveOpts = tuple<const fs::path& /*filename*/, const fs::path& /*path*/>;\n \n // Start saving a single snapshot of a multi-file dfly snapshot.\n // If shard is null, then this is the summary file.\n error_code DoPartialSave(PartialSaveOpts opts, const dfly::StringVec& scripts,\n                          RdbSnapshot* snapshot, EngineShard* shard) {\n-  auto [filename, path, now] = opts;\n+  auto [filename, path] = opts;\n   // Construct resulting filename.\n   fs::path full_filename = filename;\n   if (shard == nullptr) {\n-    ExtendFilename(now, \"summary\", &full_filename);\n+    ExtendDfsFilename(\"summary\", &full_filename);\n   } else {\n-    ExtendFilenameWithShard(now, shard->shard_id(), &full_filename);\n+    ExtendDfsFilenameWithShard(shard->shard_id(), &full_filename);\n   }\n   fs::path full_path = path / full_filename;  // use / operator to concatenate paths.\n \n@@ -903,11 +938,15 @@ GenericError ServerFamily::DoSave(bool new_version, Transaction* trans) {\n     service_.SwitchState(GlobalState::SAVING, GlobalState::ACTIVE);\n   };\n \n-  const auto& dbfilename = GetFlag(FLAGS_dbfilename);\n-  fs::path filename = dbfilename.empty() ? \"dump\" : dbfilename;\n+  absl::Time start = absl::Now();\n+\n+  fs::path filename = GetFlag(FLAGS_dbfilename);\n+  if (auto ec = ValidateFilenameExtension(filename, new_version); ec) {\n+    return ec;\n+  }\n+  SubstituteFilenameTsPlaceholder(&filename, FormatTs(start));\n   fs::path path = dir_path;\n \n-  absl::Time start = absl::Now();\n   shared_ptr<LastSaveInfo> save_info;\n \n   vector<unique_ptr<RdbSnapshot>> snapshots;\n@@ -944,7 +983,7 @@ GenericError ServerFamily::DoSave(bool new_version, Transaction* trans) {\n \n   // Start snapshots.\n   if (new_version) {\n-    auto file_opts = make_tuple(cref(filename), cref(path), start);\n+    auto file_opts = make_tuple(cref(filename), cref(path));\n \n     // In the new version (.dfs) we store a file for every shard and one more summary file.\n     // Summary file is always last in snapshots array.\n@@ -977,7 +1016,9 @@ GenericError ServerFamily::DoSave(bool new_version, Transaction* trans) {\n   } else {\n     snapshots.resize(1);\n \n-    ExtendFilenameWithShard(start, -1, &filename);\n+    if (!filename.has_extension()) {\n+      filename += \".rdb\";\n+    }\n     path /= filename;  // use / operator to concatenate paths.\n \n     snapshots[0].reset(new RdbSnapshot(fq_threadpool_.get()));\n@@ -1008,7 +1049,7 @@ GenericError ServerFamily::DoSave(bool new_version, Transaction* trans) {\n   RunStage(new_version, close_cb);\n \n   if (new_version) {\n-    ExtendFilename(start, \"summary\", &filename);\n+    ExtendDfsFilename(\"summary\", &filename);\n     path /= filename;\n   }\n \n",
  "test_patch": "diff --git a/tests/dragonfly/snapshot_test.py b/tests/dragonfly/snapshot_test.py\nindex 3307da5b8eb8..3a4dbe877c26 100644\n--- a/tests/dragonfly/snapshot_test.py\n+++ b/tests/dragonfly/snapshot_test.py\n@@ -2,6 +2,7 @@\n import pytest\n import os\n import glob\n+import aioredis\n from pathlib import Path\n \n from . import dfly_args\n@@ -16,13 +17,15 @@ class SnapshotTestBase:\n     def setup(self, tmp_dir: Path):\n         self.tmp_dir = tmp_dir\n \n-    def get_main_file(self, suffix):\n-        def is_main(f): return \"summary\" in f if suffix == \"dfs\" else True\n-        files = glob.glob(str(self.tmp_dir.absolute()) + '/test-*.'+suffix)\n-        return next(f for f in sorted(files) if is_main(f))\n+    def get_main_file(self, pattern):\n+        def is_main(f): return \"summary\" in f if pattern.endswith(\"dfs\") else True\n+        files = glob.glob(str(self.tmp_dir.absolute()) + '/' + pattern)\n+        possible_mains = list(filter(is_main, files))\n+        assert len(possible_mains) == 1, possible_mains\n+        return possible_mains[0]\n \n \n-@dfly_args({**BASIC_ARGS, \"dbfilename\": \"test-rdb\"})\n+@dfly_args({**BASIC_ARGS, \"dbfilename\": \"test-rdb-{{timestamp}}\"})\n class TestRdbSnapshot(SnapshotTestBase):\n     \"\"\"Test single file rdb snapshot\"\"\"\n     @pytest.fixture(autouse=True)\n@@ -39,7 +42,30 @@ async def test_snapshot(self, df_seeder_factory, async_client, df_server):\n         # save + flush + load\n         await async_client.execute_command(\"SAVE RDB\")\n         assert await async_client.flushall()\n-        await async_client.execute_command(\"DEBUG LOAD \" + super().get_main_file(\"rdb\"))\n+        await async_client.execute_command(\"DEBUG LOAD \" + super().get_main_file(\"test-rdb-*.rdb\"))\n+\n+        assert await seeder.compare(start_capture)\n+\n+\n+@dfly_args({**BASIC_ARGS, \"dbfilename\": \"test-rdbexact.rdb\", \"nodf_snapshot_format\": \"\"})\n+class TestRdbSnapshotExactFilename(SnapshotTestBase):\n+    \"\"\"Test single file rdb snapshot without a timestamp\"\"\"\n+    @pytest.fixture(autouse=True)\n+    def setup(self, tmp_dir: Path):\n+        super().setup(tmp_dir)\n+\n+    @pytest.mark.asyncio\n+    async def test_snapshot(self, df_seeder_factory, async_client, df_server):\n+        seeder = df_seeder_factory.create(port=df_server.port, **SEEDER_ARGS)\n+        await seeder.run(target_deviation=0.1)\n+\n+        start_capture = await seeder.capture()\n+\n+        # save + flush + load\n+        await async_client.execute_command(\"SAVE RDB\")\n+        assert await async_client.flushall()\n+        main_file = super().get_main_file(\"test-rdbexact.rdb\")\n+        await async_client.execute_command(\"DEBUG LOAD \" + main_file)\n \n         assert await seeder.compare(start_capture)\n \n@@ -61,12 +87,48 @@ async def test_snapshot(self, df_seeder_factory, async_client, df_server):\n         # save + flush + load\n         await async_client.execute_command(\"SAVE DF\")\n         assert await async_client.flushall()\n-        await async_client.execute_command(\"DEBUG LOAD \" + super().get_main_file(\"dfs\"))\n+        await async_client.execute_command(\"DEBUG LOAD \" + super().get_main_file(\"test-dfs-summary.dfs\"))\n \n         assert await seeder.compare(start_capture)\n \n \n-@dfly_args({**BASIC_ARGS, \"dbfilename\": \"test-periodic.dfs\", \"save_schedule\": \"*:*\"})\n+class TestDflyAutoLoadSnapshot(SnapshotTestBase):\n+    \"\"\"Test automatic loading of dump files on startup with timestamp\"\"\"\n+    @pytest.fixture(autouse=True)\n+    def setup(self, tmp_dir: Path):\n+        self.tmp_dir = tmp_dir\n+\n+    cases = [\n+        (\"rdb\", \"test-autoload1-{{timestamp}}\"),\n+        (\"df\", \"test-autoload2-{{timestamp}}\"),\n+        (\"rdb\", \"test-autoload3-{{timestamp}}.rdb\"),\n+        (\"rdb\", \"test-autoload4\"),\n+        (\"df\", \"test-autoload5\"),\n+        (\"rdb\", \"test-autoload6.rdb\"),\n+    ]\n+\n+    @pytest.mark.asyncio\n+    @pytest.mark.parametrize(\"save_type, dbfilename\", cases)\n+    async def test_snapshot(self, df_local_factory, save_type, dbfilename):\n+        df_args = {\"dbfilename\": dbfilename, **BASIC_ARGS}\n+        if save_type == 'rdb':\n+            df_args['nodf_snapshot_format'] = \"\"\n+        df_server = df_local_factory.create(**df_args)\n+        df_server.start()\n+\n+        client = aioredis.Redis(port=df_server.port)\n+        await client.set(\"TEST\", hash(dbfilename))\n+        await client.execute_command(\"SAVE \" + save_type)\n+        df_server.stop()\n+\n+        df_server2 = df_local_factory.create(**df_args)\n+        df_server2.start()\n+        client = aioredis.Redis(port=df_server.port)\n+        response = await client.get(\"TEST\")\n+        assert response.decode(\"utf-8\") == str(hash(dbfilename))\n+\n+\n+@dfly_args({**BASIC_ARGS, \"dbfilename\": \"test-periodic\", \"save_schedule\": \"*:*\"})\n class TestPeriodicSnapshot(SnapshotTestBase):\n     \"\"\"Test periodic snapshotting\"\"\"\n     @pytest.fixture(autouse=True)\n@@ -80,5 +142,4 @@ async def test_snapshot(self, df_seeder_factory, df_server):\n \n         time.sleep(60)\n \n-        files = [f for f in os.listdir(self.tmp_dir) if f.startswith('test-periodic')]\n-        assert len(files) > 0 and any(f.endswith('summary.dfs') for f in files)\n+        assert super().get_main_file(\"test-periodic-summary.dfs\")\n",
  "problem_statement": "dragonfly with df_snapshot_format=true only saves snapshots with timestamp in a file\nit seems that `ExtendFilenameWithShard` always forces file with timestamp even when extension is provided.\r\n\r\nI do not remember why it was decided.\n",
  "hints_text": "",
  "created_at": "2023-04-02T14:44:33Z",
  "modified_files": [
    "src/server/server_family.cc"
  ],
  "modified_test_files": [
    "tests/dragonfly/snapshot_test.py"
  ]
}