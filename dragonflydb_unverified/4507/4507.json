{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4507,
  "instance_id": "dragonflydb__dragonfly-4507",
  "issue_numbers": [
    "4497"
  ],
  "base_commit": "23af41ca070386c0fe55b070cca36fb32ed889eb",
  "patch": "diff --git a/src/server/db_slice.cc b/src/server/db_slice.cc\nindex f67cf5cda3d3..29089a5be8e8 100644\n--- a/src/server/db_slice.cc\n+++ b/src/server/db_slice.cc\n@@ -270,11 +270,12 @@ SliceEvents& SliceEvents::operator+=(const SliceEvents& o) {\n \n #undef ADD\n \n-DbSlice::DbSlice(uint32_t index, bool caching_mode, EngineShard* owner)\n+DbSlice::DbSlice(uint32_t index, bool cache_mode, EngineShard* owner)\n     : shard_id_(index),\n-      caching_mode_(caching_mode),\n+      cache_mode_(cache_mode),\n       owner_(owner),\n       client_tracking_map_(owner->memory_resource()) {\n+  load_in_progress_ = false;\n   db_arr_.emplace_back();\n   CreateDb(0);\n   expire_base_[0] = expire_base_[1] = 0;\n@@ -471,7 +472,7 @@ OpResult<DbSlice::PrimeItAndExp> DbSlice::FindInternal(const Context& cntx, std:\n     }\n   }\n \n-  if (caching_mode_ && IsValid(res.it)) {\n+  if (IsCacheMode() && IsValid(res.it)) {\n     if (!change_cb_.empty()) {\n       FetchedItemsRestorer fetched_restorer(&fetched_items_);\n       auto bump_cb = [&](PrimeTable::bucket_iterator bit) {\n@@ -600,14 +601,14 @@ OpResult<DbSlice::AddOrFindResult> DbSlice::AddOrFindInternal(const Context& cnt\n       !owner_->IsReplica() && !(ServerState::tlocal()->gstate() == GlobalState::LOADING);\n \n   // If we are over limit in non-cache scenario, just be conservative and throw.\n-  if (apply_memory_limit && !caching_mode_ && memory_budget_ + memory_offset < 0) {\n+  if (apply_memory_limit && !IsCacheMode() && memory_budget_ + memory_offset < 0) {\n     LOG_EVERY_T(WARNING, 1) << \"AddOrFind: over limit, budget: \" << memory_budget_\n                             << \" reclaimed: \" << reclaimed << \" offset: \" << memory_offset;\n     events_.insertion_rejections++;\n     return OpStatus::OUT_OF_MEMORY;\n   }\n \n-  PrimeEvictionPolicy evp{cntx,          (bool(caching_mode_) && !owner_->IsReplica()),\n+  PrimeEvictionPolicy evp{cntx,          (IsCacheMode() && !owner_->IsReplica()),\n                           memory_offset, ssize_t(soft_budget_limit_),\n                           this,          apply_memory_limit};\n \n@@ -1272,7 +1273,7 @@ pair<uint64_t, size_t> DbSlice::FreeMemWithEvictionStep(DbIndex db_ind, size_t s\n       return {0, evicted_bytes};\n   }\n \n-  if ((!caching_mode_) || !expire_allowed_)\n+  if ((!IsCacheMode()) || !expire_allowed_)\n     return {0, 0};\n \n   auto max_eviction_per_hb = GetFlag(FLAGS_max_eviction_per_heartbeat);\ndiff --git a/src/server/db_slice.h b/src/server/db_slice.h\nindex b6457bb8c7e7..55458cfe1c3e 100644\n--- a/src/server/db_slice.h\n+++ b/src/server/db_slice.h\n@@ -229,7 +229,7 @@ class DbSlice {\n     int32_t expire_options = 0;  // ExpireFlags\n   };\n \n-  DbSlice(uint32_t index, bool caching_mode, EngineShard* owner);\n+  DbSlice(uint32_t index, bool cache_mode, EngineShard* owner);\n   ~DbSlice();\n \n   // Activates `db_ind` database if it does not exist (see ActivateDb below).\n@@ -468,7 +468,16 @@ class DbSlice {\n   }\n \n   void TEST_EnableCacheMode() {\n-    caching_mode_ = 1;\n+    cache_mode_ = 1;\n+  }\n+\n+  bool IsCacheMode() const {\n+    // During loading time we never bump elements.\n+    return cache_mode_ && !load_in_progress_;\n+  }\n+\n+  void SetLoadInProgress(bool in_progress) {\n+    load_in_progress_ = in_progress;\n   }\n \n   // Test hook to inspect last locked keys.\n@@ -575,7 +584,8 @@ class DbSlice {\n   mutable LocalBlockingCounter block_counter_;\n \n   ShardId shard_id_;\n-  uint8_t caching_mode_ : 1;\n+  uint8_t cache_mode_ : 1;\n+  uint8_t load_in_progress_ : 1;\n \n   EngineShard* owner_;\n \ndiff --git a/src/server/rdb_load.cc b/src/server/rdb_load.cc\nindex d49555009b57..192df90b041f 100644\n--- a/src/server/rdb_load.cc\n+++ b/src/server/rdb_load.cc\n@@ -2034,6 +2034,9 @@ error_code RdbLoader::Load(io::Source* src) {\n \n   auto cleanup = absl::Cleanup([&] { FinishLoad(start, &keys_loaded); });\n \n+  shard_set->AwaitRunningOnShardQueue([](EngineShard* es) {\n+    namespaces->GetDefaultNamespace().GetCurrentDbSlice().SetLoadInProgress(true);\n+  });\n   while (!stop_early_.load(memory_order_relaxed)) {\n     if (pause_) {\n       ThisFiber::SleepFor(100ms);\n@@ -2223,7 +2226,10 @@ void RdbLoader::FinishLoad(absl::Time start_time, size_t* keys_loaded) {\n     FlushShardAsync(i);\n \n     // Send sentinel callbacks to ensure that all previous messages have been processed.\n-    shard_set->Add(i, [bc]() mutable { bc->Dec(); });\n+    shard_set->Add(i, [bc]() mutable {\n+      namespaces->GetDefaultNamespace().GetCurrentDbSlice().SetLoadInProgress(false);\n+      bc->Dec();\n+    });\n   }\n   bc->Wait();  // wait for sentinels to report.\n \n",
  "test_patch": "diff --git a/src/server/rdb_test.cc b/src/server/rdb_test.cc\nindex 6a312357878d..74f3dc8b1fb2 100644\n--- a/src/server/rdb_test.cc\n+++ b/src/server/rdb_test.cc\n@@ -722,4 +722,16 @@ TEST_F(RdbTest, SnapshotTooBig) {\n   ASSERT_THAT(resp, ErrArg(\"Out of memory\"));\n }\n \n+TEST_F(RdbTest, HugeKeyIssue4497) {\n+  SetTestFlag(\"cache_mode\", \"true\");\n+  ResetService();\n+\n+  EXPECT_EQ(Run({\"flushall\"}), \"OK\");\n+  EXPECT_EQ(Run({\"debug\", \"populate\", \"1\", \"k\", \"1000\", \"rand\", \"type\", \"set\", \"elements\", \"5000\"}),\n+            \"OK\");\n+  EXPECT_EQ(Run({\"save\", \"rdb\", \"hugekey.rdb\"}), \"OK\");\n+  EXPECT_EQ(Run({\"dfly\", \"load\", \"hugekey.rdb\"}), \"OK\");\n+  EXPECT_EQ(Run({\"flushall\"}), \"OK\");\n+}\n+\n }  // namespace dfly\n",
  "problem_statement": "Update from 1.19.2 to 1.26.1 leads to SIGSEGV\n**Describe the bug**\nWe tried upgrading a dragonfly cluster (two members) from v1.19.2 to v1.26.1\nWe restarted the replica instance, and here is the log after reboot : \n\n```\n2025-01-22T15:15:21+01:00 I20250122 14:15:21.933689     1 init.cc:78] dragonfly running in opt mode.\n2025-01-22T15:15:21+01:00 I20250122 14:15:21.933818     1 dfly_main.cc:691] Starting dragonfly df-v1.26.1-c2b95415241debb99bf969ebbc8465ce0bfe206f\n2025-01-22T15:15:21+01:00 I20250122 14:15:21.934052     1 dfly_main.cc:735] maxmemory has not been specified. Deciding myself....\n2025-01-22T15:15:21+01:00 I20250122 14:15:21.934062     1 dfly_main.cc:744] Found 3.00GiB available memory. Setting maxmemory to 2.40GiB\n2025-01-22T15:15:21+01:00 W20250122 14:15:21.934125     1 dfly_main.cc:368] Weird error 1 switching to epoll\n2025-01-22T15:15:22+01:00 I20250122 14:15:22.014271     1 proactor_pool.cc:147] Running 1 io threads\n2025-01-22T15:15:22+01:00 I20250122 14:15:22.017419     1 dfly_main.cc:272] Listening on admin socket any:9999\n2025-01-22T15:15:22+01:00 I20250122 14:15:22.018528     1 server_family.cc:835] Host OS: Linux 5.14.0-480.el9.x86_64 x86_64 with 1 threads\n2025-01-22T15:15:22+01:00 I20250122 14:15:22.022305     1 snapshot_storage.cc:185] Load snapshot: Searching for snapshot in directory: \"/dragonfly/snapshots\"\n2025-01-22T15:15:22+01:00 I20250122 14:15:22.025554     1 server_family.cc:1114] Loading /dragonfly/snapshots/dump-summary.dfs\n2025-01-22T15:15:22+01:00 I20250122 14:15:22.034510     6 listener_interface.cc:101] sock[5] AcceptServer - listening on port 9999\n2025-01-22T15:15:22+01:00 I20250122 14:15:22.034538     6 listener_interface.cc:101] sock[6] AcceptServer - listening on port 6379\n2025-01-22T15:15:22+01:00 I20250122 14:15:22.034548     6 listener_interface.cc:101] sock[7] AcceptServer - listening on port 11211\n2025-01-22T15:15:23+01:00 I20250122 14:15:23.489850     6 server_family.cc:1154] Load finished, num keys read: 323\n2025-01-22T15:15:34+01:00 I20250122 14:15:34.925325     6 server_family.cc:2769] Replicating 10.233.130.142:9999\n2025-01-22T15:15:34+01:00 F20250122 14:15:34.925524     6 db_slice.cc:783] Check failed: fetched_items_.empty() \n2025-01-22T15:15:34+01:00 *** Check failure stack trace: ***\n2025-01-22T15:15:34+01:00     @     0x55c033cde923  google::LogMessage::SendToLog()\n2025-01-22T15:15:34+01:00     @     0x55c033cd70e7  google::LogMessage::Flush()\n2025-01-22T15:15:34+01:00     @     0x55c033cd8a6f  google::LogMessageFatal::~LogMessageFatal()\n2025-01-22T15:15:34+01:00     @     0x55c03351770f  dfly::DbSlice::FlushDbIndexes()\n2025-01-22T15:15:34+01:00     @     0x55c0335178a2  dfly::DbSlice::FlushDb()\n2025-01-22T15:15:34+01:00     @     0x55c033296f2a  _ZN4absl12lts_2024011619functional_internal12InvokeObjectIZN4dfly12ServerFamily8DrakarysEPNS3_11TransactionEtEUlS6_PNS3_11EngineShardEE_NS5_14RunnableResultEJS6_S8_EEET0_NS1_7VoidPtrEDpNS1_8ForwardTIT1_E4typeE\n2025-01-22T15:15:34+01:00     @     0x55c0335577b9  dfly::Transaction::RunCallback()\n2025-01-22T15:15:34+01:00     @     0x55c03355a59b  dfly::Transaction::RunInShard()\n2025-01-22T15:15:34+01:00     @     0x55c033498790  dfly::EngineShard::PollExecution()\n2025-01-22T15:15:34+01:00     @     0x55d7633db3a1  _ZNSt17_Function_handlerIFvvEZN4dfly11Transaction11DispatchHopEvEUlvE1_E9_M_invokeERKSt9_Any_data\n2025-01-22T15:15:34+01:00     @     0x55d76394cf55  util::fb2::FiberQueue::Run()\n2025-01-22T15:15:34+01:00     @     0x55d76342d730  _ZN5boost7context6detail11fiber_entryINS1_12fiber_recordINS0_5fiberEN4util3fb219FixedStackAllocatorEZNS6_6detail15WorkerFiberImplIZN4dfly9TaskQueue5StartESt17basic_string_viewIcSt11char_traitsIcEEEUlvE_JEEC4IS7_EESF_RKNS0_12preallocatedEOT_OSG_EUlOS4_E_EEEEvNS1_10transfer_tE\n2025-01-22T15:15:34+01:00     @     0x55d76396c25f  make_fcontext\n2025-01-22T15:15:34+01:00     *** SIGABRT received at time=1737556484 on cpu 0 ***\n2025-01-22T15:15:34+01:00     PC: @     0x7f1faca299fc  (unknown)  pthread_kill\n2025-01-22T15:15:34+01:00     [failure_signal_handler.cc : 345] RAW: Signal 11 raised at PC=0x7f1fac9bb898 while already in AbslFailureSignalHandler()\n2025-01-22T15:15:34+01:00     *** SIGSEGV received at time=1737556484 on cpu 0 ***\n2025-01-22T15:15:34+01:00     PC: @     0x7f1fac9bb898  (unknown)  abort\n```\n\nThis happens in a loop.\n\nIf we delete snapshot, a full sync happens and member starts correctly : \n```\nI20250122 14:18:26.534972     1 dfly_main.cc:691] Starting dragonfly df-v1.26.1-c2b95415241debb99bf969ebbc8465ce0bfe206f\nI20250122 14:18:26.535194     1 dfly_main.cc:735] maxmemory has not been specified. Deciding myself....\nI20250122 14:18:26.535209     1 dfly_main.cc:744] Found 3.00GiB available memory. Setting maxmemory to 2.40GiB\nW20250122 14:18:26.535251     1 dfly_main.cc:368] Weird error 1 switching to epoll\nI20250122 14:18:26.612828     1 proactor_pool.cc:147] Running 1 io threads\nI20250122 14:18:26.615834     1 dfly_main.cc:272] Listening on admin socket any:9999\nI20250122 14:18:26.616796     1 server_family.cc:835] Host OS: Linux 5.14.0-480.el9.x86_64 x86_64 with 1 threads\nI20250122 14:18:26.620615     1 snapshot_storage.cc:185] Load snapshot: Searching for snapshot in directory: \"/dragonfly/snapshots\"\nW20250122 14:18:26.623203     1 server_family.cc:950] Load snapshot: No snapshot found\nI20250122 14:18:26.630414     6 listener_interface.cc:101] sock[5] AcceptServer - listening on port 9999\nI20250122 14:18:26.630442     6 listener_interface.cc:101] sock[6] AcceptServer - listening on port 6379\nI20250122 14:18:26.630453     6 listener_interface.cc:101] sock[7] AcceptServer - listening on port 11211\nI20250122 14:18:46.526578     6 server_family.cc:2769] Replicating 10.233.130.142:9999\nI20250122 14:18:47.943701     6 replica.cc:569] Started full sync with 10.233.130.142:9999\nI20250122 14:18:49.127570     6 replica.cc:589] full sync finished in 2.6 s\nI20250122 14:18:49.127673     6 replica.cc:679] Transitioned into stable sync\n```\n\nOn another cluster, even deleting snapshot and full resync failed (100% failure after 10+ restarts) : \n```\nI20250122 14:36:19.137816     1 server_family.cc:835] Host OS: Linux 5.14.0-480.el9.x86_64 x86_64 with 1 threads\nI20250122 14:36:19.149236     1 snapshot_storage.cc:185] Load snapshot: Searching for snapshot in directory: \"/dragonfly/snapshots\"\nW20250122 14:36:19.152247     1 server_family.cc:950] Load snapshot: No snapshot found\nI20250122 14:36:19.160511     6 listener_interface.cc:101] sock[5] AcceptServer - listening on port 9999\nI20250122 14:36:19.160552     6 listener_interface.cc:101] sock[6] AcceptServer - listening on port 6379\nI20250122 14:36:19.160574     6 listener_interface.cc:101] sock[7] AcceptServer - listening on port 11211\nI20250122 14:36:37.548327     6 server_family.cc:2769] Replicating 10.233.131.239:9999\nI20250122 14:36:37.560622     6 replica.cc:569] Started full sync with 10.233.131.239:9999\nI20250122 14:36:37.593991     6 server_family.cc:2769] Replicating 10.233.131.239:9999\nW20250122 14:36:37.594259     6 replica.cc:246] Error syncing with 10.233.131.239:9999 generic:125 Operation canceled\nI20250122 14:36:37.660379     6 replica.cc:569] Started full sync with 10.233.131.239:9999\nI20250122 14:36:39.770289     6 server_family.cc:2769] Replicating 10.233.131.239:9999\nW20250122 14:36:39.770624     6 replica.cc:246] Error syncing with 10.233.131.239:9999 generic:125 Operation canceled\nF20250122 14:36:39.775619     6 db_slice.cc:783] Check failed: fetched_items_.empty() \n*** Check failure stack trace: ***\n    @     0x55681cb35923  google::LogMessage::SendToLog()\n    @     0x55681cb2e0e7  google::LogMessage::Flush()\n    @     0x55681cb2fa6f  google::LogMessageFatal::~LogMessageFatal()\n    @     0x55681c36e70f  dfly::DbSlice::FlushDbIndexes()\n    @     0x55681c36e8a2  dfly::DbSlice::FlushDb()\n    @     0x55681c0edf2a  _ZN4absl12lts_2024011619functional_internal12InvokeObjectIZN4dfly12ServerFamily8DrakarysEPNS3_11TransactionEtEUlS6_PNS3_11EngineShardEE_NS5_14RunnableResultEJS6_S8_EEET0_NS1_7VoidPtrEDpNS1_8ForwardTIT1_E4typeE\n    @     0x55681c3ae7b9  dfly::Transaction::RunCallback()\n    @     0x55681c3b159b  dfly::Transaction::RunInShard()\n    @     0x55681c2ef790  dfly::EngineShard::PollExecution()\n    @     0x55681c3aa3a1  _ZNSt17_Function_handlerIFvvEZN4dfly11Transaction11DispatchHopEvEUlvE1_E9_M_invokeERKSt9_Any_data\n    @     0x55681c91bf55  util::fb2::FiberQueue::Run()\n    @     0x55681c3fc730  _ZN5boost7context6detail11fiber_entryINS1_12fiber_recordINS0_5fiberEN4util3fb219FixedStackAllocatorEZNS6_6detail15WorkerFiberImplIZN4dfly9TaskQueue5StartESt17basic_string_viewIcSt11char_traitsIcEEEUlvE_JEEC4IS7_EESF_RKNS0_12preallocatedEOT_OSG_EUlOS4_E_EEEEvNS1_10transfer_tE\n    @     0x55681c93b25f  make_fcontext\n*** SIGABRT received at time=1737556599 on cpu 0 ***\nPC: @     0x7f4768cc29fc  (unknown)  pthread_kill\n[failure_signal_handler.cc : 345] RAW: Signal 11 raised at PC=0x7f4768c54898 while already in AbslFailureSignalHandler()\n\n```\n\non master instance, at the same moment : \n```\nI20250122 14:36:37.550398     6 dflycmd.cc:651] Registered replica 10.233.130.163:6379\nI20250122 14:36:37.559907     6 dflycmd.cc:345] Started sync with replica 10.233.130.163:6379\nI20250122 14:36:37.648327     6 dflycmd.cc:110] Disconnecting from replica 10.233.130.163:6379\nI20250122 14:36:37.648401     6 rdb_save.cc:1247] Error writing to rdb sink Broken pipe\nI20250122 14:36:37.648496     6 dflycmd.cc:641] Replication error: Operation canceled: Context cancelled\nI20250122 14:36:37.650457     6 dflycmd.cc:651] Registered replica 10.233.130.163:6379\nI20250122 14:36:37.659725     6 dflycmd.cc:345] Started sync with replica 10.233.130.163:6379\nI20250122 14:36:39.784649     6 rdb_save.cc:1247] Error writing to rdb sink Broken pipe\nI20250122 14:36:39.784729     6 dflycmd.cc:641] Replication error: Broken pipe\nI20250122 14:36:39.784785     6 dflycmd.cc:110] Disconnecting from replica 10.233.130.163:6379\n```\n\nIn this case we were forced to start over and setup a new empty cluster.\n\n\n**To Reproduce**\nThis was reproduced in FOUR different Dragonfly cluster, with same update (same version source/destination).\nEvery time but one, deleting dfs file makes it work.\n\n\n**Expected behavior**\nWorking update, or at least a full resync from master without crash.\n\n**Environment (please complete the following information):**\n - Dragonfly official image docker.dragonflydb.io/dragonflydb/dragonfly\n - Run in Kubernetes 1.30.4\n\n",
  "hints_text": "Hi @odoucet . Thanks for reporting this.\nWe were able to reproduce it on a new instance that started crashing after a few hours after migration.\nStacktrace is the same : \n```\nF20250122 16:47:58.648206     7 db_slice.cc:783] Check failed: fetched_items_.empty()\n*** Check failure stack trace: ***\n    @     0x56088fa69923  google::LogMessage::SendToLog()\n    @     0x56088fa620e7  google::LogMessage::Flush()\n    @     0x56088fa63a6f  google::LogMessageFatal::~LogMessageFatal()\n    @     0x56088f2a270f  dfly::DbSlice::FlushDbIndexes()\n    @     0x56088f2a28a2  dfly::DbSlice::FlushDb()\n    @     0x56088f021f2a  _ZN4absl12lts_2024011619functional_internal12InvokeObjectIZN4dfly12ServerFamily8DrakarysEPNS3_11TransactionEtEUlS6_PNS3_11EngineShardEE_NS5_14RunnableResultEJS6_S8_EEET0_NS1_7VoidPtrEDpNS1_8ForwardTIT1_E4typeE\n    @     0x56088f2e27b9  dfly::Transaction::RunCallback()\n    @     0x56088f2e559b  dfly::Transaction::RunInShard()\n    @     0x56088f223790  dfly::EngineShard::PollExecution()\n    @     0x56088f2de3a1  _ZNSt17_Function_handlerIFvvEZN4dfly11Transaction11DispatchHopEvEUlvE1_E9_M_invokeERKSt9_Any_data\n    @     0x56088f84ff55  util::fb2::FiberQueue::Run()\n    @     0x56088f330730  _ZN5boost7context6detail11fiber_entryINS1_12fiber_recordINS0_5fiberEN4util3fb219FixedStackAllocatorEZNS6_6detail15WorkerFiberImplIZN4dfly9TaskQueue5StartESt17basic_string_viewIcSt11char_traitsIcEEEUlvE_JEEC4IS7_EESF_RKNS0_12preallocatedEOT_OSG_EUlOS4_E_EEEEvNS1_10transfer_tE\n    @     0x56088f86f25f  make_fcontext\n*** SIGABRT received at time=1737564478 on cpu 1 ***\nPC: @     0x7fcdf39739fc  (unknown)  pthread_kill\n[failure_signal_handler.cc : 345] RAW: Signal 11 raised at PC=0x7fcdf3905898 while already in AbslFailureSignalHandler()\n*** SIGSEGV received at time=1737564478 on cpu 1 ***\nPC: @     0x7fcdf3905898  (unknown)  abort\n```\n\nWe rolled back to 1.26.0 and I will update this ticket if the bug stops happening.\nSame bug on 1.26.0 : \n```\nI20250122 19:23:05.273779     6 server_family.cc:1154] Load finished, num keys read: 193674\nI20250122 19:23:05.452224     7 version_monitor.cc:174] Your current version '1.26.0' is not the latest version. A newer version '1.26.1' is now available. Please consider an update.\nI20250122 19:23:24.684263     7 server_family.cc:2765] Replicating 10.233.137.104:9999\nF20250122 19:23:24.684446     7 db_slice.cc:783] Check failed: fetched_items_.empty() \n*** Check failure stack trace: ***\n    @     0x55a599c55923  google::LogMessage::SendToLog()\n    @     0x55a599c4e0e7  google::LogMessage::Flush()\n    @     0x55a599c4fa6f  google::LogMessageFatal::~LogMessageFatal()\n    @     0x55a5994900cf  dfly::DbSlice::FlushDbIndexes()\n    @     0x55a599490262  dfly::DbSlice::FlushDb()\n    @     0x55a59920fefa  _ZN4absl12lts_2024011619functional_internal12InvokeObjectIZN4dfly12ServerFamily8DrakarysEPNS3_11TransactionEtEUlS6_PNS3_11EngineShardEE_NS5_14RunnableResultEJS6_S8_EEET0_NS1_7VoidPtrEDpNS1_8ForwardTIT1_E4typeE\n    @     0x55a5994d00e9  dfly::Transaction::RunCallback()\n    @     0x55a5994d2ecb  dfly::Transaction::RunInShard()\n    @     0x55a599411160  dfly::EngineShard::PollExecution()\n    @     0x55a5994cbcd1  _ZNSt17_Function_handlerIFvvEZN4dfly11Transaction11DispatchHopEvEUlvE1_E9_M_invokeERKSt9_Any_data\n    @     0x55a599a3ba75  util::fb2::FiberQueue::Run()\n    @     0x55a59951dbf0  _ZN5boost7context6detail11fiber_entryINS1_12fiber_recordINS0_5fiberEN4util3fb219FixedStackAllocatorEZNS6_6detail15WorkerFiberImplIZN4dfly9TaskQueue5StartESt17basic_string_viewIcSt11char_traitsIcEEEUlvE_JEEC4IS7_EESF_RKNS0_12preallocatedEOT_OSG_EUlOS4_E_EEEEvNS1_10transfer_tE\n    @     0x55a599a5ad7f  make_fcontext\n*** SIGABRT received at time=1737573804 on cpu 1 ***\nPC: @     0x7f3f62d2b9fc  (unknown)  pthread_kill\n[failure_signal_handler.cc : 345] RAW: Signal 11 raised at PC=0x7f3f62cbd898 while already in AbslFailureSignalHandler()\n*** SIGSEGV received at time=1737573804 on cpu 1 ***\nPC: @     0x7f3f62cbd898  (unknown)  abort\n```\n\nWe need 1.26.0 because of Prometheus3 compatibility.\nIf it helps, the bug is only happening on replica servers so far. \nyeah, I think it's helpful. if you can, please paste `info all` output from the replica\nHere is the output less than one second before crash. What's strange is \"role:master\". I guess the node does not know yet there is another master and crashes when it connects to it.\n\n```\n# Server\nredis_version:7.2.0\ndragonfly_version:df-v1.26.1\nredis_mode:standalone\narch_bits:64\nos:Linux 5.14.0-480.el9.x86_64 x86_64\nthread_count:1\nmultiplexing_api:epoll\ntcp_port:6379\nuptime_in_seconds:17\nuptime_in_days:0\n\n# Clients\nconnected_clients:1\nmax_clients:64000\nclient_read_buffer_bytes:256\nblocked_clients:0\npipeline_queue_length:0\nsend_delay_ms:0\n\n# Memory\nused_memory:2160514320\nused_memory_human:2.01GiB\nused_memory_peak:2160514320\nused_memory_peak_human:2.01GiB\nfibers_stack_vms:425776\nfibers_count:13\nused_memory_rss:2260889600\nused_memory_rss_human:2.11GiB\nused_memory_peak_rss:2431455232\nmaxmemory:2576980377\nmaxmemory_human:2.40GiB\nused_memory_lua:0\nobject_used_memory:36549296\ntype_used_memory_string:36549296\ntable_used_memory:281904\nnum_buckets:480\nnum_entries:719\ninline_keys:3\nlistpack_blobs:0\nlistpack_bytes:0\nsmall_string_bytes:45872\npipeline_cache_bytes:0\ndispatch_queue_bytes:0\ndispatch_queue_subscriber_bytes:0\ndispatch_queue_peak_bytes:0\nclient_read_buffer_peak_bytes:256\ntls_bytes:5664\nsnapshot_serialization_bytes:0\ncommands_squashing_replies_bytes:0\ncache_mode:cache\nmaxmemory_policy:eviction\nreplication_streaming_buffer_bytes:0\nreplication_full_sync_buffer_bytes:0\n\n# Stats\ntotal_connections_received:16\ntotal_commands_processed:30\ninstantaneous_ops_per_sec:3\ntotal_pipelined_commands:0\ntotal_pipelined_squashed_commands:0\npipeline_throttle_total:0\npipelined_latency_usec:0\ntotal_net_input_bytes:1232\nconnection_migrations:0\ntotal_net_output_bytes:57687\nrdb_save_usec:0\nrdb_save_count:0\nbig_value_preemptions:0\ncompressed_blobs:0\ninstantaneous_input_kbps:-1\ninstantaneous_output_kbps:-1\nrejected_connections:-1\nexpired_keys:0\nevicted_keys:0\nhard_evictions:0\ngarbage_checked:0\ngarbage_collected:0\nbump_ups:21\nstash_unloaded:0\noom_rejections:0\ntraverse_ttl_sec:628\ndelete_ttl_sec:0\nkeyspace_hits:0\nkeyspace_misses:0\nkeyspace_mutations:964\ntotal_reads_processed:28\ntotal_writes_processed:29\ndefrag_attempt_total:0\ndefrag_realloc_total:0\ndefrag_task_invocation_total:0\nreply_count:29\nreply_latency_usec:0\nblocked_on_interpreter:0\nlua_interpreter_cnt:0\nlua_blocked_total:0\n\n# Tiered\ntiered_entries:0\ntiered_entries_bytes:0\ntiered_total_stashes:0\ntiered_total_fetches:0\ntiered_total_cancels:0\ntiered_total_deletes:0\ntiered_total_uploads:0\ntiered_total_stash_overflows:0\ntiered_heap_buf_allocations:0\ntiered_registered_buf_allocations:0\ntiered_allocated_bytes:0\ntiered_capacity_bytes:0\ntiered_pending_read_cnt:0\ntiered_pending_stash_cnt:0\ntiered_small_bins_cnt:0\ntiered_small_bins_entries_cnt:0\ntiered_small_bins_filling_bytes:0\ntiered_cold_storage_bytes:0\ntiered_offloading_steps:0\ntiered_offloading_stashes:0\ntiered_ram_hits:0\ntiered_ram_cool_hits:0\ntiered_ram_misses:0\n\n# Persistence\ncurrent_snapshot_perc:0\ncurrent_save_keys_processed:0\ncurrent_save_keys_total:0\nlast_success_save:1737623316\nlast_saved_file:\nlast_success_save_duration_sec:0\nloading:0\nsaving:0\ncurrent_save_duration_sec:0\nrdb_changes_since_last_success_save:964\nlast_failed_save:0\nlast_error:\nlast_failed_save_duration_sec:0\n\n# Transaction\ntx_shard_polls:0\ntx_shard_optimistic_total:0\ntx_shard_ooo_total:0\ntx_global_total:0\ntx_normal_total:0\ntx_inline_runs_total:0\ntx_schedule_cancel_total:0\ntx_batch_scheduled_items_total:0\ntx_batch_schedule_calls_total:0\ntx_with_freq:0\ntx_queue_len:0\neval_io_coordination_total:0\neval_shardlocal_coordination_total:0\neval_squashed_flushes:0\nmulti_squash_execution_total:0\nmulti_squash_execution_hop_usec:0\nmulti_squash_execution_reply_usec:0\n\n# Replication\nrole:master\nconnected_slaves:0\nmaster_replid:985f47f8b8ca3cd5f6f8a75bd48f0e2b874227b2\n\n# Commandstats\ncmdstat_auth:calls=14,usec=831,usec_per_call=59.3571\ncmdstat_info:calls=13,usec=2752,usec_per_call=211.692\ncmdstat_ping:calls=2,usec=28,usec_per_call=14\n\n# Modules\nmodule:name=ReJSON,ver=20000,api=1,filters=0,usedby=[search],using=[],options=[handle-io-errors]\nmodule:name=search,ver=20000,api=1,filters=0,usedby=[],using=[ReJSON],options=[handle-io-errors]\n\n# Search\nsearch_memory:0\nsearch_num_indices:0\nsearch_num_entries:0\n\n# Errorstats\n\n# Keyspace\ndb0:keys=719,expires=350,avg_ttl=-1\n\n# Cpu\nused_cpu_sys:1.584828\nused_cpu_user:1.978862\nused_cpu_sys_children:0.2876\nused_cpu_user_children:0.3150\nused_cpu_sys_main_thread:1.381782\nused_cpu_user_main_thread:1.889157\n\n# Cluster\ncluster_enabled:0\n```\nDragonfly command line used : \n```\n--alsologtostderr --primary_port_http_enabled=false --admin_port=9999 --admin_nopass --dbfilename=dump --cache_mode=true --memcached_port=11211 --dir=/dragonfly/snapshots --snapshot_cron=*/5 * * * *\n```\nYes, I agree, that's strange. It aligns with the stack trace we see, though. The crash occurs within the Flush code, which runs when the replica attempts to resynchronize with the master. Is the network connection between your nodes unreliable, cross-region, or over the public internet?\n> Yes, I agree, that's strange. It aligns with the stack trace we see, though. The crash occurs within the Flush code, which runs when the replica attempts to resynchronize with the master. Is the network connection between your nodes unreliable, cross-region, or over the public internet?\n\nNot at all, they are on a private local high speed network (< 5ms latency).\nso there are few issues here:\n1. severe - we crash upon reload\n2. less severe but still troubling - replica disconnects during the replication. \n\nwhile we are trying to solve (1), I suggest that you run your servers with additional argument `--vmodule=streamer=1,dflycmd=1,replica=1`\n\nAlso, if you can, please attach the INFO log from your master.\n@odoucet \nDid you see the replica reconnecting only when replicating from master version v1.19 or also when replicating from master version v1.26?\nThis errors are related to reconnecting \nW20250122 14:36:37.594259     6 replica.cc:246] Error syncing with 10.233.131.239:9999 generic:125 Operation canceled\n\nHi @odoucet, do you know if the replica received incoming traffic (like `GET` commands etc) during the time it started to replicate?\nWhat I mean by that is in parallel to handling requests it also tried to replicate a master\n> Hi [@odoucet](https://github.com/odoucet), do you know if the replica received incoming traffic (like `GET` commands etc) during the time it started to replicate? What I mean by that is in parallel to handling requests it also tried to replicate a master\n\nYou can see it in \"info all\" that there is only auth/info commands executed (my tests), and 2 \"ping\" (from dragonfly operator I guess).\nI am able to reproduce this issue, with a big caveat.\nIf the RDB file has recurring key (i.e. key that appears twice), then this will put an entry in `fetched_items_`, which will never be cleared because it is called outside of a command context (we clear `fetched_items_` in `DbSlice::OnCbFinish`).\n\nSo the instructions to reproduce are:\n1. Create RDB with the same key twice (not trivial, I manually modified the file)\n2. Load it via `--dbfilename`, such that it is loaded outside the context of a command\n3. Call `FLUSHALL` (and likely also `REPLICAOF`) immediately after\n\nI remember that we had some issue a while back that could cause the same key to be saved multiple times into an RDB file. I don't know if it was in 1.19 or not. But in any case this is not our case, as the warning `W20250123 14:19:17.709569 463838 rdb_load.cc:2620] RDB has duplicated key 'x123456' in DB 0` is not printed in @odoucet 's case.\n\nI think that this is a good direction though (I've been looking into it since this morning). We need to figure out in which case loading and RDB could keep an entry in `fetched_items_` even without it appearing twice. Still looking, but if @adiholden / @romange have other ideas lmk\nTo clarify, this happens only with `--cache_mode` in the line:\n\nhttps://github.com/dragonflydb/dragonfly/blob/69ef9979f050694de2874b4e016ff02552f16abe/src/server/db_slice.cc#L484\n\nSo in the scenario I think about, we call `FindInternal()` in the context of load, and the key already exists.\nit's a good direction @chakaz .  i know that folks that have multiple snapshots on their disk succeed loading files from multiple files sets, i.e. dragonfly can load an dfs file from another snapshot set. this may happen when a process had  differrent thread configurations and differrent arities of files were created, if I remember correctly.\nwhy do we BumpUp during the rdb load?\n(I think we should not bump up during the load)\nWe bump up just because we use the standard API, `FindOrUpdate()` etc, during the load process. It's not explicit...\n@odoucet can you share the command you use to set up the replication? `REPLICAOF <host> <port>` or something else?\n(Specifically I want to see that you're not using `ADDREPLICAOF`)\n@chakaz  they use dragonfly operator, and it does not use `ADDREPLICAOF`\n> [@odoucet](https://github.com/odoucet) Did you see the replica reconnecting only when replicating from master version v1.19 or also when replicating from master version v1.26? This errors are related to reconnecting W20250122 14:36:37.594259 6 replica.cc:246] Error syncing with 10.233.131.239:9999 generic:125 Operation canceled\n\nThe bug is actually triggering between two 1.26.1 instances (master/replica), but the dfs file was generated on 1.19 though.\nLast crash : \n\non replica : \n```\n2025-01-23T11:38:10+01:00 I20250123 10:38:10.886876     6 replica.cc:679] Transitioned into stable sync\n2025-01-23T11:38:12+01:00 I20250123 10:38:12.898600     6 server_family.cc:2769] Replicating 10.233.130.20:9999\n2025-01-23T11:38:12+01:00 I20250123 10:38:12.898733     6 replica.cc:703] Exit stable sync\n2025-01-23T11:38:12+01:00 F20250123 10:38:12.899036     6 db_slice.cc:783] Check failed: fetched_items_.empty() \n```\n\non master, here are the events:\n```\n2025-01-23T11:35:00+01:00 I20250123 10:35:00.777082     6 save_stages_controller.cc:337] Saving \"/dragonfly/snapshots/dump-summary.dfs\" finished after 1 s\n2025-01-23T11:38:10+01:00 I20250123 10:38:10.558867     6 dflycmd.cc:651] Registered replica 10.233.131.97:6379\n2025-01-23T11:38:10+01:00 I20250123 10:38:10.723584     6 dflycmd.cc:345] Started sync with replica 10.233.131.97:6379\n2025-01-23T11:38:10+01:00 I20250123 10:38:10.886333     6 dflycmd.cc:385] Transitioned into stable sync with replica 10.233.131.97:6379\n2025-01-23T11:38:12+01:00 I20250123 10:38:12.899302     6 dflycmd.cc:110] Disconnecting from replica 10.233.131.97:6379\n2025-01-23T11:38:12+01:00 I20250123 10:38:12.899538     6 dflycmd.cc:641] Replication error: Operation canceled: Context cancelled\n2025-01-23T11:40:00+01:00 I20250123 10:40:00.937175     6 save_stages_controller.cc:337] Saving \"/dragonfly/snapshots/dump-summary.dfs\" finished after 0 us\n```\n> To clarify, this happens only with `--cache_mode` in the line:\n> \n> [dragonfly/src/server/db_slice.cc](https://github.com/dragonflydb/dragonfly/blob/69ef9979f050694de2874b4e016ff02552f16abe/src/server/db_slice.cc#L484)\n> Line 484 in [69ef997](/dragonflydb/dragonfly/commit/69ef9979f050694de2874b4e016ff02552f16abe)\n\n\nAh interesting ! Part of the update from 1.19 to 1.26 was to explicit this flag. I think it was not set before.\n> while we are trying to solve (1), I suggest that you run your servers with additional argument `--vmodule=streamer=1,dflycmd=1,replica=1`\n\nCan I add this argument on all instances (I'm using dragonfly operator) ? Can you tell me what they do ? Dragonfly documentation is a bit lacky (or I'm not reading the correct page :p).\n\n> > while we are trying to solve (1), I suggest that you run your servers with additional argument `--vmodule=streamer=1,dflycmd=1,replica=1`\n> \n> Can I add this argument on all instances (I'm using dragonfly operator) ? Can you tell me what they do ? Dragonfly documentation is a bit lacky (or I'm not reading the correct page :p).\n\nthey just control the verbosity level of relevant modules in dragonfly. you should see more logs. Yes, you can add them to all instances.\nWeird, output seems the same : \n```\n2025-01-24T09:48:54+01:00 I20250124 08:48:54.929366     1 server_family.cc:835] Host OS: Linux 5.14.0-480.el9.x86_64 x86_64 with 1 threads\n2025-01-24T09:48:54+01:00 I20250124 08:48:54.930619     1 snapshot_storage.cc:185] Load snapshot: Searching for snapshot in directory: \"/dragonfly/snapshots\"\n2025-01-24T09:48:54+01:00 I20250124 08:48:54.934929     1 server_family.cc:1114] Loading /dragonfly/snapshots/dump-summary.dfs\n2025-01-24T09:48:54+01:00 I20250124 08:48:54.955077     6 listener_interface.cc:101] sock[5] AcceptServer - listening on port 9999\n2025-01-24T09:48:54+01:00 I20250124 08:48:54.955127     6 listener_interface.cc:101] sock[6] AcceptServer - listening on port 6379\n2025-01-24T09:48:54+01:00 I20250124 08:48:54.955137     6 listener_interface.cc:101] sock[7] AcceptServer - listening on port 11211\n2025-01-24T09:48:55+01:00 I20250124 08:48:55.459303     6 server_family.cc:1154] Load finished, num keys read: 293\n2025-01-24T09:49:10+01:00 I20250124 08:49:10.197369     6 server_family.cc:2769] Replicating 10.233.130.20:9999\n2025-01-24T09:49:10+01:00 F20250124 08:49:10.197620     6 db_slice.cc:783] Check failed: fetched_items_.empty() \n2025-01-24T09:49:10+01:00 *** Check failure stack trace: ***\n2025-01-24T09:49:10+01:00     @     0x55ac0126d923  google::LogMessage::SendToLog()\n2025-01-24T09:49:10+01:00     @     0x55ac012660e7  google::LogMessage::Flush()\n2025-01-24T09:49:10+01:00     @     0x55ac01267a6f  google::LogMessageFatal::~LogMessageFatal()\n2025-01-24T09:49:10+01:00     @     0x55ac00aa670f  dfly::DbSlice::FlushDbIndexes()\n2025-01-24T09:49:10+01:00     @     0x55ac00aa68a2  dfly::DbSlice::FlushDb()\n2025-01-24T09:49:10+01:00     @     0x55ac00825f2a  _ZN4absl12lts_2024011619functional_internal12InvokeObjectIZN4dfly12ServerFamily8DrakarysEPNS3_11TransactionEtEUlS6_PNS3_11EngineShardEE_NS5_14RunnableResultEJS6_S8_EEET0_NS1_7VoidPtrEDpNS1_8ForwardTIT1_E4typeE\n2025-01-24T09:49:10+01:00     @     0x55ac00ae67b9  dfly::Transaction::RunCallback()\n2025-01-24T09:49:10+01:00     @     0x55ac00ae959b  dfly::Transaction::RunInShard()\n2025-01-24T09:49:10+01:00     @     0x55ac00a27790  dfly::EngineShard::PollExecution()\n2025-01-24T09:49:10+01:00     @     0x55ac00ae23a1  _ZNSt17_Function_handlerIFvvEZN4dfly11Transaction11DispatchHopEvEUlvE1_E9_M_invokeERKSt9_Any_data\n2025-01-24T09:49:10+01:00     @     0x55ac01053f55  util::fb2::FiberQueue::Run()\n2025-01-24T09:49:10+01:00     @     0x55ac00b34730  _ZN5boost7context6detail11fiber_entryINS1_12fiber_recordINS0_5fiberEN4util3fb219FixedStackAllocatorEZNS6_6detail15WorkerFiberImplIZN4dfly9TaskQueue5StartESt17basic_string_viewIcSt11char_traitsIcEEEUlvE_JEEC4IS7_EESF_RKNS0_12preallocatedEOT_OSG_EUlOS4_E_EEEEvNS1_10transfer_tE\n2025-01-24T09:49:10+01:00     @     0x55ac0107325f  make_fcontext\n2025-01-24T09:49:10+01:00 *** SIGABRT received at time=1737708550 on cpu 0 ***\n2025-01-24T09:49:10+01:00 PC: @     0x7f60dd4899fc  (unknown)  pthread_kill\n2025-01-24T09:49:10+01:00 [failure_signal_handler.cc : 345] RAW: Signal 11 raised at PC=0x7f60dd41b898 while already in AbslFailureSignalHandler()\n2025-01-24T09:49:10+01:00 *** SIGSEGV received at time=1737708550 on cpu 0 ***\n2025-01-24T09:49:10+01:00 PC: @     0x7f60dd41b898  (unknown)  abort\n```\nComplete command line : \n```\n--alsologtostderr --primary_port_http_enabled=false --admin_port=9999 --admin_nopass --dbfilename=dump --cache_mode=true --vmodule=streamer=1,dflycmd=1,replica=1 --memcached_port=11211 --dir=/dragonfly/snapshots --snapshot_cron=*/5 * * * *\n```\n@odoucet  can you please remove/move files from `/dragonfly/snapshots/` so that the replica won't load anything before replicating?\nSure!\n```\nI20250124 09:29:26.222805     1 dfly_main.cc:691] Starting dragonfly df-v1.26.1-c2b95415241debb99bf969ebbc8465ce0bfe206f\nI20250124 09:29:26.223179     1 dfly_main.cc:735] maxmemory has not been specified. Deciding myself....\nI20250124 09:29:26.223196     1 dfly_main.cc:744] Found 3.00GiB available memory. Setting maxmemory to 2.40GiB\nW20250124 09:29:26.223292     1 dfly_main.cc:368] Weird error 1 switching to epoll\nI20250124 09:29:26.300966     1 proactor_pool.cc:147] Running 1 io threads\nI20250124 09:29:26.305312     1 dfly_main.cc:272] Listening on admin socket any:9999\nI20250124 09:29:26.306802     1 server_family.cc:835] Host OS: Linux 5.14.0-480.el9.x86_64 x86_64 with 1 threads\nI20250124 09:29:26.312215     1 snapshot_storage.cc:185] Load snapshot: Searching for snapshot in directory: \"/dragonfly/snapshots\"\nW20250124 09:29:26.313297     1 server_family.cc:950] Load snapshot: No snapshot found\nI20250124 09:29:26.323559     6 listener_interface.cc:101] sock[5] AcceptServer - listening on port 9999\nI20250124 09:29:26.323609     6 listener_interface.cc:101] sock[6] AcceptServer - listening on port 6379\nI20250124 09:29:26.323621     6 listener_interface.cc:101] sock[7] AcceptServer - listening on port 11211\nI20250124 09:29:44.922405     6 server_family.cc:2769] Replicating 10.233.131.245:9999\nI20250124 09:29:44.922955     6 replica.cc:94] Starting replication\nI20250124 09:29:44.922989     6 replica.cc:116] Resolving master DNS\nI20250124 09:29:44.923134     6 replica.cc:121] Connecting to master\nI20250124 09:29:44.923444     6 replica.cc:126] Greeting\nI20250124 09:29:44.923466     6 replica.cc:281] greeting message handling\nI20250124 09:29:44.924607     6 replica.cc:360] Master id: 067593dfb823f2f32ae3dbfee7a5fff0db3ada78, sync id: SYNC8, num journals: 1, version: 3\nI20250124 09:29:44.925041     6 replica.cc:193] Main replication fiber started\nI20250124 09:29:44.925344     6 replica.cc:741] Sending on flow 067593dfb823f2f32ae3dbfee7a5fff0db3ada78 SYNC8 0\nI20250124 09:29:44.925938     6 replica.cc:724] Sending: DFLY SYNC SYNC8\nI20250124 09:29:45.128309     6 replica.cc:569] Started full sync with 10.233.131.245:9999\nI20250124 09:29:45.128377     6 replica.cc:573] Waiting for all full sync cut confirmations\nI20250124 09:29:45.575390     6 replica.cc:724] Sending: DFLY STARTSTABLE SYNC8\nI20250124 09:29:45.576601     6 replica.cc:843] FullSyncDflyFb finished after reading 29948311 bytes\nI20250124 09:29:45.584250     6 replica.cc:589] full sync finished in 659 ms\nI20250124 09:29:45.584337     6 replica.cc:679] Transitioned into stable sync\nI20250124 09:29:45.588070     6 replica.cc:921] Sending an ACK with offset=1 forced=0\nI20250124 09:29:46.588424     6 replica.cc:921] Sending an ACK with offset=1 forced=0\nI20250124 09:29:47.588763     6 replica.cc:921] Sending an ACK with offset=1 forced=0\nI20250124 09:29:48.235881     6 server_family.cc:2769] Replicating 10.233.131.245:9999\nI20250124 09:29:48.235949     6 replica.cc:148] Stopping replication\nI20250124 09:29:48.236061     6 replica.cc:703] Exit stable sync\nI20250124 09:29:48.236079     6 replica.cc:276] Main replication fiber finished\nF20250124 09:29:48.236179     6 db_slice.cc:783] Check failed: fetched_items_.empty() \n*** Check failure stack trace: ***\n    @     0x5557ab158923  google::LogMessage::SendToLog()\n    @     0x5557ab1510e7  google::LogMessage::Flush()\n    @     0x5557ab152a6f  google::LogMessageFatal::~LogMessageFatal()\n    @     0x5557aa99170f  dfly::DbSlice::FlushDbIndexes()\n    @     0x5557aa9918a2  dfly::DbSlice::FlushDb()\n    @     0x5557aa710f2a  _ZN4absl12lts_2024011619functional_internal12InvokeObjectIZN4dfly12ServerFamily8DrakarysEPNS3_11TransactionEtEUlS6_PNS3_11EngineShardEE_NS5_14RunnableResultEJS6_S8_EEET0_NS1_7VoidPtrEDpNS1_8ForwardTIT1_E4typeE\n    @     0x5557aa9d17b9  dfly::Transaction::RunCallback()\n    @     0x5557aa9d459b  dfly::Transaction::RunInShard()\n    @     0x5557aa912790  dfly::EngineShard::PollExecution()\n    @     0x5557aa9cd3a1  _ZNSt17_Function_handlerIFvvEZN4dfly11Transaction11DispatchHopEvEUlvE1_E9_M_invokeERKSt9_Any_data\n    @     0x5557aaf3ef55  util::fb2::FiberQueue::Run()\n    @     0x5557aaa1f730  _ZN5boost7context6detail11fiber_entryINS1_12fiber_recordINS0_5fiberEN4util3fb219FixedStackAllocatorEZNS6_6detail15WorkerFiberImplIZN4dfly9TaskQueue5StartESt17basic_string_viewIcSt11char_traitsIcEEEUlvE_JEEC4IS7_EESF_RKNS0_12preallocatedEOT_OSG_EUlOS4_E_EEEEvNS1_10transfer_tE\n    @     0x5557aaf5e25f  make_fcontext\n*** SIGABRT received at time=1737710988 on cpu 0 ***\nPC: @     0x7f1dc9c039fc  (unknown)  pthread_kill\n*** SIGSEGV received at time=1737710988 on cpu 0 ***\n[failure_signal_handler.cc : 345] RAW: Signal 11 raised at PC=0x7f1dc9b95898 while already in AbslFailureSignalHandler()\nPC: @     0x7f1dc9b95898  (unknown)  abort\n```\n@odoucet this is very helpful. Can you please provide the full log ? Attach the INFO file if possible.\n\nAlso, I am going to build a special docker image that will print some additional logs so we could understand the core reason for this. Once it's ready, I will let you know what image to use.\nlast comment modified with full log, on latest crash (7th time). \nand \"info all\" just a few milliseconds before restart : \n```\n# Server\nredis_version:7.2.0\ndragonfly_version:df-v1.26.1\nredis_mode:standalone\narch_bits:64\nos:Linux 5.14.0-480.el9.x86_64 x86_64\nthread_count:1\nmultiplexing_api:epoll\ntcp_port:6379\nuptime_in_seconds:14\nuptime_in_days:0\n\n# Clients\nconnected_clients:1\nmax_clients:64000\nclient_read_buffer_bytes:256\nblocked_clients:0\npipeline_queue_length:0\nsend_delay_ms:0\n\n# Memory\nused_memory:217091344\nused_memory_human:207.03MiB\nused_memory_peak:217091344\nused_memory_peak_human:207.03MiB\nfibers_stack_vms:524032\nfibers_count:16\nused_memory_rss:277245952\nused_memory_rss_human:264.40MiB\nused_memory_peak_rss:424484864\nmaxmemory:2576980377\nmaxmemory_human:2.40GiB\nused_memory_lua:0\nobject_used_memory:19556416\ntype_used_memory_string:19556416\ntable_used_memory:281904\nnum_buckets:480\nnum_entries:304\ninline_keys:3\nlistpack_blobs:0\nlistpack_bytes:0\nsmall_string_bytes:14848\npipeline_cache_bytes:0\ndispatch_queue_bytes:0\ndispatch_queue_subscriber_bytes:0\ndispatch_queue_peak_bytes:0\nclient_read_buffer_peak_bytes:256\ntls_bytes:5664\nsnapshot_serialization_bytes:0\ncommands_squashing_replies_bytes:0\ncache_mode:cache\nmaxmemory_policy:eviction\n\n# Stats\ntotal_connections_received:24\ntotal_commands_processed:47\ninstantaneous_ops_per_sec:4\ntotal_pipelined_commands:0\ntotal_pipelined_squashed_commands:0\npipeline_throttle_total:0\npipelined_latency_usec:0\ntotal_net_input_bytes:1840\nconnection_migrations:0\ntotal_net_output_bytes:84946\nrdb_save_usec:0\nrdb_save_count:0\nbig_value_preemptions:0\ncompressed_blobs:0\ninstantaneous_input_kbps:-1\ninstantaneous_output_kbps:-1\nrejected_connections:-1\nexpired_keys:0\nevicted_keys:0\nhard_evictions:0\ngarbage_checked:0\ngarbage_collected:0\nbump_ups:1\nstash_unloaded:0\noom_rejections:0\ntraverse_ttl_sec:0\ndelete_ttl_sec:0\nkeyspace_hits:0\nkeyspace_misses:0\nkeyspace_mutations:329\ntotal_reads_processed:42\ntotal_writes_processed:45\ndefrag_attempt_total:0\ndefrag_realloc_total:0\ndefrag_task_invocation_total:0\nreply_count:45\nreply_latency_usec:0\nblocked_on_interpreter:0\nlua_interpreter_cnt:0\nlua_blocked_total:0\n\n# Tiered\ntiered_entries:0\ntiered_entries_bytes:0\ntiered_total_stashes:0\ntiered_total_fetches:0\ntiered_total_cancels:0\ntiered_total_deletes:0\ntiered_total_uploads:0\ntiered_total_stash_overflows:0\ntiered_heap_buf_allocations:0\ntiered_registered_buf_allocations:0\ntiered_allocated_bytes:0\ntiered_capacity_bytes:0\ntiered_pending_read_cnt:0\ntiered_pending_stash_cnt:0\ntiered_small_bins_cnt:0\ntiered_small_bins_entries_cnt:0\ntiered_small_bins_filling_bytes:0\ntiered_cold_storage_bytes:0\ntiered_offloading_steps:0\ntiered_offloading_stashes:0\ntiered_ram_hits:0\ntiered_ram_cool_hits:0\ntiered_ram_misses:0\n\n# Persistence\ncurrent_snapshot_perc:0\ncurrent_save_keys_processed:0\ncurrent_save_keys_total:0\nlast_success_save:1737711489\nlast_saved_file:\nlast_success_save_duration_sec:0\nloading:0\nsaving:0\ncurrent_save_duration_sec:0\nrdb_changes_since_last_success_save:329\nlast_failed_save:0\nlast_error:\nlast_failed_save_duration_sec:0\n\n# Transaction\ntx_shard_polls:2\ntx_shard_optimistic_total:0\ntx_shard_ooo_total:0\ntx_global_total:2\ntx_normal_total:0\ntx_inline_runs_total:0\ntx_schedule_cancel_total:0\ntx_batch_scheduled_items_total:2\ntx_batch_schedule_calls_total:2\ntx_with_freq:2\ntx_queue_len:0\neval_io_coordination_total:0\neval_shardlocal_coordination_total:0\neval_squashed_flushes:0\nmulti_squash_execution_total:0\nmulti_squash_execution_hop_usec:0\nmulti_squash_execution_reply_usec:0\n\n# Replication\nrole:slave\nmaster_host:10.233.131.245\nmaster_port:9999\nmaster_link_status:up\nmaster_last_io_seconds_ago:0\nmaster_sync_in_progress:0\nmaster_replid:067593dfb823f2f32ae3dbfee7a5fff0db3ada78\nslave_repl_offset:40\nslave_priority:100\nslave_read_only:1\n\n# Commandstats\ncmdstat_auth:calls=20,usec=835,usec_per_call=41.75\ncmdstat_flushall:calls=1,usec=204,usec_per_call=204\ncmdstat_hello:calls=2,usec=127,usec_per_call=63.5\ncmdstat_info:calls=20,usec=2814,usec_per_call=140.7\ncmdstat_ping:calls=2,usec=42,usec_per_call=21\ncmdstat_slaveof:calls=1,usec=2124,usec_per_call=2124\n\n# Modules\nmodule:name=ReJSON,ver=20000,api=1,filters=0,usedby=[search],using=[],options=[handle-io-errors]\nmodule:name=search,ver=20000,api=1,filters=0,usedby=[],using=[ReJSON],options=[handle-io-errors]\n\n# Search\nsearch_memory:0\nsearch_num_indices:0\nsearch_num_entries:0\n\n# Errorstats\n\n# Keyspace\ndb0:keys=304,expires=107,avg_ttl=-1\n\n# Cpu\nused_cpu_sys:0.168811\nused_cpu_user:0.304481\nused_cpu_sys_children:0.2283\nused_cpu_user_children:0.2291\nused_cpu_sys_main_thread:0.149975\nused_cpu_user_main_thread:0.212442\n\n# Cluster\ncluster_enabled:0\n```\nWhat's interesting is that we can see new commands ! I guess I intercepted this a few seconds later the previous \"info all\" reported earlier, when sync is performed with master.\nwe understand what happens, and this info log only confirms our assumption.\n\nSomewhere in the code we perform a lookup for an existing key. We see it via: `bump_ups:1`. \nBut `commandstats` do not have any indication of a command that performs the lookup. So we do it somewhere else. \n\nSo we are trying to understand where this lookup is coming from. I kicked off a special build that should help us to solve the riddle:\nhttps://github.com/dragonflydb/dragonfly/actions/runs/12947318140\n\nThis lookup is related to the crash. Once we understand where it is called, we will be able to solve the problem.\nSeems build has failed :( \nhttps://github.com/dragonflydb/dragonfly/actions/runs/12948512596\n\nsorry, failed again.\nFind new stacktrace with new image `ghcr.io/dragonflydb/dragonfly-weekly:3b54da2199e5979cf8d989623356c12ee1daeb98`\n```\n./dragonfly.*\n* For the available flags type dragonfly [--help | --helpfull]\n* Documentation can be found at: https://www.dragonflydb.io/docs\nI20250124 14:29:15.257787     1 dfly_main.cc:728] Starting dragonfly df-HEAD-HASH-NOTFOUND-GITDIR-NOTFOUND\nI20250124 14:29:15.258239     1 dfly_main.cc:772] maxmemory has not been specified. Deciding myself....\nI20250124 14:29:15.258260     1 dfly_main.cc:781] Found 3.00GiB available memory. Setting maxmemory to 2.40GiB\nW20250124 14:29:15.258329     1 dfly_main.cc:376] Weird error 1 switching to epoll\nI20250124 14:29:15.336063     1 proactor_pool.cc:149] Running 1 io threads\nI20250124 14:29:15.339635     1 dfly_main.cc:280] Listening on admin socket any:9999\nI20250124 14:29:15.340895     1 server_family.cc:835] Host OS: Linux 5.14.0-480.el9.x86_64 x86_64 with 1 threads\nI20250124 14:29:15.346280     1 snapshot_storage.cc:185] Load snapshot: Searching for snapshot in directory: \"/dragonfly/snapshots\"\nI20250124 14:29:15.348778     1 server_family.cc:1114] Loading /dragonfly/snapshots/dump-summary.dfs\nI20250124 14:29:15.350069     6 listener_interface.cc:101] sock[5] AcceptServer - listening on port 9999\nI20250124 14:29:15.350138     6 listener_interface.cc:101] sock[6] AcceptServer - listening on port 6379\nI20250124 14:29:15.350153     6 listener_interface.cc:101] sock[7] AcceptServer - listening on port 11211\nI20250124 14:29:15.900583     6 server_family.cc:1154] Load finished, num keys read: 289\nI20250124 14:29:30.835438     6 server_family.cc:2777] Replicating 10.233.131.245:9999\nF20250124 14:29:30.835582     6 db_slice.cc:799] Check failed: fetched_items_.size() == 0u (8 vs. 0) \n*** Check failure stack trace: ***\n    @     0x55e9354cfe63  google::LogMessage::SendToLog()\n    @     0x55e9354c8307  google::LogMessage::Flush()\n    @     0x55e9354c9c8f  google::LogMessageFatal::~LogMessageFatal()\n    @     0x55e934cfd7f8  dfly::DbSlice::FlushDbIndexes()\n    @     0x55e934cfde96  dfly::DbSlice::FlushDb()\n    @     0x55e934a7b9ea  _ZN4absl12lts_2024072219functional_internal12InvokeObjectIZN4dfly12ServerFamily8DrakarysEPNS3_11TransactionEtEUlS6_PNS3_11EngineShardEE_NS5_14RunnableResultEJS6_S8_EEET0_NS1_7VoidPtrEDpNS1_8ForwardTIT1_E4typeE\n    @     0x55e934d4244a  dfly::Transaction::RunCallback()\n    @     0x55e934d4524b  dfly::Transaction::RunInShard()\n    @     0x55e934c7e1b0  dfly::EngineShard::PollExecution()\n    @     0x55e934d3e0b1  _ZNSt17_Function_handlerIFvvEZN4dfly11Transaction11DispatchHopEvEUlvE1_E9_M_invokeERKSt9_Any_data\n    @     0x55e9352b3a74  util::fb2::FiberQueue::Run()\n    @     0x55e934d91cd0  _ZN5boost7context6detail11fiber_entryINS1_12fiber_recordINS0_5fiberEN4util3fb219FixedStackAllocatorEZNS6_6detail15WorkerFiberImplIZN4dfly9TaskQueue5StartESt17basic_string_viewIcSt11char_traitsIcEEEUlvE_JEEC4IS7_EESF_RKNS0_12preallocatedEOT_OSG_EUlOS4_E_EEEEvNS1_10transfer_tE\n    @     0x55e9352d541f  make_fcontext\n*** SIGABRT received at time=1737728970 on cpu 0 ***\nPC: @     0x7fe1930009fc  (unknown)  pthread_kill\n[failure_signal_handler.cc : 345] RAW: Signal 11 raised at PC=0x7fe192f92898 while already in AbslFailureSignalHandler()\n*** SIGSEGV received at time=1737728970 on cpu 0 ***\nPC: @     0x7fe192f92898  (unknown)  abort\n```\nPlease run it with '--vmodule=db_slice=1'\r\n\r\nOn Fri, Jan 24, 2025, 4:31\u202fPM Olivier Doucet ***@***.***>\r\nwrote:\r\n\r\n> Find new stacktrace with new image\r\n> ghcr.io/dragonflydb/dragonfly-weekly:3b54da2199e5979cf8d989623356c12ee1daeb98\r\n>\r\n> ./dragonfly.*\r\n> * For the available flags type dragonfly [--help | --helpfull]\r\n> * Documentation can be found at: https://www.dragonflydb.io/docs\r\n> I20250124 <https://www.dragonflydb.io/docsI20250124> 14:29:15.257787     1 dfly_main.cc:728] Starting dragonfly df-HEAD-HASH-NOTFOUND-GITDIR-NOTFOUND\r\n> I20250124 14:29:15.258239     1 dfly_main.cc:772] maxmemory has not been specified. Deciding myself....\r\n> I20250124 14:29:15.258260     1 dfly_main.cc:781] Found 3.00GiB available memory. Setting maxmemory to 2.40GiB\r\n> W20250124 14:29:15.258329     1 dfly_main.cc:376] Weird error 1 switching to epoll\r\n> I20250124 14:29:15.336063     1 proactor_pool.cc:149] Running 1 io threads\r\n> I20250124 14:29:15.339635     1 dfly_main.cc:280] Listening on admin socket any:9999\r\n> I20250124 14:29:15.340895     1 server_family.cc:835] Host OS: Linux 5.14.0-480.el9.x86_64 x86_64 with 1 threads\r\n> I20250124 14:29:15.346280     1 snapshot_storage.cc:185] Load snapshot: Searching for snapshot in directory: \"/dragonfly/snapshots\"\r\n> I20250124 14:29:15.348778     1 server_family.cc:1114] Loading /dragonfly/snapshots/dump-summary.dfs\r\n> I20250124 14:29:15.350069     6 listener_interface.cc:101] sock[5] AcceptServer - listening on port 9999\r\n> I20250124 14:29:15.350138     6 listener_interface.cc:101] sock[6] AcceptServer - listening on port 6379\r\n> I20250124 14:29:15.350153     6 listener_interface.cc:101] sock[7] AcceptServer - listening on port 11211\r\n> I20250124 14:29:15.900583     6 server_family.cc:1154] Load finished, num keys read: 289\r\n> I20250124 14:29:30.835438     6 server_family.cc:2777] Replicating 10.233.131.245:9999\r\n> F20250124 14:29:30.835582     6 db_slice.cc:799] Check failed: fetched_items_.size() == 0u (8 vs. 0)\r\n> *** Check failure stack trace: ***\r\n>     @     0x55e9354cfe63  google::LogMessage::SendToLog()\r\n>     @     0x55e9354c8307  google::LogMessage::Flush()\r\n>     @     0x55e9354c9c8f  google::LogMessageFatal::~LogMessageFatal()\r\n>     @     0x55e934cfd7f8  dfly::DbSlice::FlushDbIndexes()\r\n>     @     0x55e934cfde96  dfly::DbSlice::FlushDb()\r\n>     @     0x55e934a7b9ea  _ZN4absl12lts_2024072219functional_internal12InvokeObjectIZN4dfly12ServerFamily8DrakarysEPNS3_11TransactionEtEUlS6_PNS3_11EngineShardEE_NS5_14RunnableResultEJS6_S8_EEET0_NS1_7VoidPtrEDpNS1_8ForwardTIT1_E4typeE\r\n>     @     0x55e934d4244a  dfly::Transaction::RunCallback()\r\n>     @     0x55e934d4524b  dfly::Transaction::RunInShard()\r\n>     @     0x55e934c7e1b0  dfly::EngineShard::PollExecution()\r\n>     @     0x55e934d3e0b1  _ZNSt17_Function_handlerIFvvEZN4dfly11Transaction11DispatchHopEvEUlvE1_E9_M_invokeERKSt9_Any_data\r\n>     @     0x55e9352b3a74  util::fb2::FiberQueue::Run()\r\n>     @     0x55e934d91cd0  _ZN5boost7context6detail11fiber_entryINS1_12fiber_recordINS0_5fiberEN4util3fb219FixedStackAllocatorEZNS6_6detail15WorkerFiberImplIZN4dfly9TaskQueue5StartESt17basic_string_viewIcSt11char_traitsIcEEEUlvE_JEEC4IS7_EESF_RKNS0_12preallocatedEOT_OSG_EUlOS4_E_EEEEvNS1_10transfer_tE\r\n>     @     0x55e9352d541f  make_fcontext\r\n> *** SIGABRT received at time=1737728970 on cpu 0 ***\r\n> PC: @     0x7fe1930009fc  (unknown)  pthread_kill\r\n> [failure_signal_handler.cc : 345] RAW: Signal 11 raised at PC=0x7fe192f92898 while already in AbslFailureSignalHandler()\r\n> *** SIGSEGV received at time=1737728970 on cpu 0 ***\r\n> PC: @     0x7fe192f92898  (unknown)  abort\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/dragonflydb/dragonfly/issues/4497#issuecomment-2612668074>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AA4BFCCBUBKMY7TA6OPIOBT2MJFDFAVCNFSM6AAAAABVVDTQLOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDMMJSGY3DQMBXGQ>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n\n```\nI20250124 17:17:12.477321     1 dfly_main.cc:728] Starting dragonfly df-HEAD-HASH-NOTFOUND-GITDIR-NOTFOUND\nI20250124 17:17:12.477640     1 dfly_main.cc:772] maxmemory has not been specified. Deciding myself....\nI20250124 17:17:12.477725     1 dfly_main.cc:781] Found 3.00GiB available memory. Setting maxmemory to 2.40GiB\nW20250124 17:17:12.477789     1 dfly_main.cc:376] Weird error 1 switching to epoll\nI20250124 17:17:12.555739     1 proactor_pool.cc:149] Running 1 io threads\nI20250124 17:17:12.559964     1 dfly_main.cc:280] Listening on admin socket any:9999\nI20250124 17:17:12.561813     1 server_family.cc:835] Host OS: Linux 5.14.0-503.21.1.el9_5.x86_64 x86_64 with 1 threads\nI20250124 17:17:12.568647     1 snapshot_storage.cc:185] Load snapshot: Searching for snapshot in directory: \"/dragonfly/snapshots\"\nI20250124 17:17:12.572139     1 server_family.cc:1114] Loading /dragonfly/snapshots/dump-summary.dfs\nI20250124 17:17:12.573837     6 listener_interface.cc:101] sock[5] AcceptServer - listening on port 9999\nI20250124 17:17:12.573879     6 listener_interface.cc:101] sock[6] AcceptServer - listening on port 6379\nI20250124 17:17:12.573890     6 listener_interface.cc:101] sock[7] AcceptServer - listening on port 11211\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.641593     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:446a8438-1797-46c1-94ca-5672064139e6 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.648752     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:446a8438-1797-46c1-94ca-5672064139e6 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.717072     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:e80e21d3-f36d-4195-ae32-b4baeb438458 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.721078     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:e80e21d3-f36d-4195-ae32-b4baeb438458 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.727167     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:e80e21d3-f36d-4195-ae32-b4baeb438458 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.749312     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:1479812d-b82d-47d6-9e69-85015c977d0e 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.750306     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:1479812d-b82d-47d6-9e69-85015c977d0e 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.751591     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:1479812d-b82d-47d6-9e69-85015c977d0e 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.752851     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:1479812d-b82d-47d6-9e69-85015c977d0e 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.753947     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:1479812d-b82d-47d6-9e69-85015c977d0e 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.754925     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:1479812d-b82d-47d6-9e69-85015c977d0e 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.797360     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:4798d037-d766-4ecc-a9c0-87661be7c64c 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.802446     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:4798d037-d766-4ecc-a9c0-87661be7c64c 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.805863     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:4798d037-d766-4ecc-a9c0-87661be7c64c 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.833777     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:841577b9-426c-4961-b748-9724e51e7aa0 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.837213     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:841577b9-426c-4961-b748-9724e51e7aa0 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.874264     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:09cf6636-a42c-4a30-bcb2-1b15933f8879 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.930166     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:bda57ea5-fb07-48e2-9bf4-a49ec1d59683 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.934319     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:bda57ea5-fb07-48e2-9bf4-a49ec1d59683 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.936893     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:bda57ea5-fb07-48e2-9bf4-a49ec1d59683 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.938997     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:bda57ea5-fb07-48e2-9bf4-a49ec1d59683 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.941053     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:bda57ea5-fb07-48e2-9bf4-a49ec1d59683 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:12.944351     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:bda57ea5-fb07-48e2-9bf4-a49ec1d59683 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:13.044169     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:b6f1ea9b-8762-4913-870e-133974be0519 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:13.049753     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:b6f1ea9b-8762-4913-870e-133974be0519 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:13.054833     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:b6f1ea9b-8762-4913-870e-133974be0519 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:13.060192     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:b6f1ea9b-8762-4913-870e-133974be0519 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:13.065511     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:b6f1ea9b-8762-4913-870e-133974be0519 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:13.070454     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:b6f1ea9b-8762-4913-870e-133974be0519 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:13.074831     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:b6f1ea9b-8762-4913-870e-133974be0519 0x5615654114c9  dfly::DbSlice::FindMutable()\n0x561565154a34  dfly::RdbLoader::LoadItemsBuffer()\n0x56156515573e  std::_Function_handler<>::_M_invoke()\n0x5615659c0a74  util::fb2::FiberQueue::Run()\nI20250124 17:17:13.080152     6 db_slice.cc:491] BumpUp 9C4Fr-WoX5:ev:b6f1ea9b-8762-4913-870e-133974be0519 0x5615654114c9  dfly::DbSlice::FindMutable()\nI20250124 17:17:13.092437     6 server_family.cc:1154] Load finished, num keys read: 283\nI20250124 17:17:32.131767     6 server_family.cc:2777] Replicating 10.233.130.54:9999\nF20250124 17:17:32.131942     6 db_slice.cc:799] Check failed: fetched_items_.size() == 0u (8 vs. 0) \n*** Check failure stack trace: ***\n    @     0x561565bdce63  google::LogMessage::SendToLog()\n    @     0x561565bd5307  google::LogMessage::Flush()\n    @     0x561565bd6c8f  google::LogMessageFatal::~LogMessageFatal()\n    @     0x56156540a7f8  dfly::DbSlice::FlushDbIndexes()\n    @     0x56156540ae96  dfly::DbSlice::FlushDb()\n    @     0x5615651889ea  _ZN4absl12lts_2024072219functional_internal12InvokeObjectIZN4dfly12ServerFamily8DrakarysEPNS3_11TransactionEtEUlS6_PNS3_11EngineShardEE_NS5_14RunnableResultEJS6_S8_EEET0_NS1_7VoidPtrEDpNS1_8ForwardTIT1_E4typeE\n    @     0x56156544f44a  dfly::Transaction::RunCallback()\n    @     0x56156545224b  dfly::Transaction::RunInShard()\n    @     0x56156538b1b0  dfly::EngineShard::PollExecution()\n    @     0x56156544b0b1  _ZNSt17_Function_handlerIFvvEZN4dfly11Transaction11DispatchHopEvEUlvE1_E9_M_invokeERKSt9_Any_data\n    @     0x5615659c0a74  util::fb2::FiberQueue::Run()\n    @     0x56156549ecd0  _ZN5boost7context6detail11fiber_entryINS1_12fiber_recordINS0_5fiberEN4util3fb219FixedStackAllocatorEZNS6_6detail15WorkerFiberImplIZN4dfly9TaskQueue5StartESt17basic_string_viewIcSt11char_traitsIcEEEUlvE_JEEC4IS7_EESF_RKNS0_12preallocatedEOT_OSG_EUlOS4_E_EEEEvNS1_10transfer_tE\n    @     0x5615659e241f  make_fcontext\n*** SIGABRT received at time=1737739052 on cpu 0 ***\nPC: @     0x7f3fd1c9f9fc  (unknown)  pthread_kill\n*** SIGSEGV received at time=1737739052 on cpu 0 ***\n[failure_signal_handler.cc : 345] RAW: Signal 11 raised at PC=0x7f3fd1c31898 while already in AbslFailureSignalHandler()\nPC: @     0x7f3fd1c31898  (unknown)  abort\n```\nwe will provide the fix early next week. Could you please reach out to me on discord?\ninvite requested on https://www.dragonflydb.io/community, waiting for the invitation link ;)\nSorry, I mean here: https://discord.com/invite/HsPjXGVH85\n\nalso can you please provide the list of all files under `/dragonfly/snapshots/`\n```\n-rw-------  1 systemd-coredump ssh_keys  29M Jan 24 20:15 dump-0000.dfs\n-rw-------  1 systemd-coredump ssh_keys 4.0K Jan 24 20:15 dump-summary.dfs\n```\nThanks, @odoucet  can you please reach out to me on Discord? If it is possible I woud like to receive these snapshot files.\nHi @odoucet, I was able to reproduce the issue, thanks to your tremendous help above here, with all our questions that you've answered. Thanks a lot, again, for filing this issue and helping us get to the bottom of it.\nI'll now work on a fix, which should be ready soon. We no longer require the dfs files generated by your instance.",
  "created_at": "2025-01-26T09:18:33Z",
  "modified_files": [
    "src/server/db_slice.cc",
    "src/server/db_slice.h",
    "src/server/rdb_load.cc"
  ],
  "modified_test_files": [
    "src/server/rdb_test.cc"
  ]
}