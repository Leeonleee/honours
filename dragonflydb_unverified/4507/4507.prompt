You will be provided with a partial code base and an issue statement explaining a problem to resolve.

<issue>
Update from 1.19.2 to 1.26.1 leads to SIGSEGV
**Describe the bug**
We tried upgrading a dragonfly cluster (two members) from v1.19.2 to v1.26.1
We restarted the replica instance, and here is the log after reboot : 

```
2025-01-22T15:15:21+01:00 I20250122 14:15:21.933689     1 init.cc:78] dragonfly running in opt mode.
2025-01-22T15:15:21+01:00 I20250122 14:15:21.933818     1 dfly_main.cc:691] Starting dragonfly df-v1.26.1-c2b95415241debb99bf969ebbc8465ce0bfe206f
2025-01-22T15:15:21+01:00 I20250122 14:15:21.934052     1 dfly_main.cc:735] maxmemory has not been specified. Deciding myself....
2025-01-22T15:15:21+01:00 I20250122 14:15:21.934062     1 dfly_main.cc:744] Found 3.00GiB available memory. Setting maxmemory to 2.40GiB
2025-01-22T15:15:21+01:00 W20250122 14:15:21.934125     1 dfly_main.cc:368] Weird error 1 switching to epoll
2025-01-22T15:15:22+01:00 I20250122 14:15:22.014271     1 proactor_pool.cc:147] Running 1 io threads
2025-01-22T15:15:22+01:00 I20250122 14:15:22.017419     1 dfly_main.cc:272] Listening on admin socket any:9999
2025-01-22T15:15:22+01:00 I20250122 14:15:22.018528     1 server_family.cc:835] Host OS: Linux 5.14.0-480.el9.x86_64 x86_64 with 1 threads
2025-01-22T15:15:22+01:00 I20250122 14:15:22.022305     1 snapshot_storage.cc:185] Load snapshot: Searching for snapshot in directory: "/dragonfly/snapshots"
2025-01-22T15:15:22+01:00 I20250122 14:15:22.025554     1 server_family.cc:1114] Loading /dragonfly/snapshots/dump-summary.dfs
2025-01-22T15:15:22+01:00 I20250122 14:15:22.034510     6 listener_interface.cc:101] sock[5] AcceptServer - listening on port 9999
2025-01-22T15:15:22+01:00 I20250122 14:15:22.034538     6 listener_interface.cc:101] sock[6] AcceptServer - listening on port 6379
2025-01-22T15:15:22+01:00 I20250122 14:15:22.034548     6 listener_interface.cc:101] sock[7] AcceptServer - listening on port 11211
2025-01-22T15:15:23+01:00 I20250122 14:15:23.489850     6 server_family.cc:1154] Load finished, num keys read: 323
2025-01-22T15:15:34+01:00 I20250122 14:15:34.925325     6 server_family.cc:2769] Replicating 10.233.130.142:9999
2025-01-22T15:15:34+01:00 F20250122 14:15:34.925524     6 db_slice.cc:783] Check failed: fetched_items_.empty() 
2025-01-22T15:15:34+01:00 *** Check failure stack trace: ***
2025-01-22T15:15:34+01:00     @     0x55c033cde923  google::LogMessage::SendToLog()
2025-01-22T15:15:34+01:00     @     0x55c033cd70e7  google::LogMessage::Flush()
2025-01-22T15:15:34+01:00     @     0x55c033cd8a6f  google::LogMessageFatal::~LogMessageFatal()
2025-01-22T15:15:34+01:00     @     0x55c03351770f  dfly::DbSlice::FlushDbIndexes()
2025-01-22T15:15:34+01:00     @     0x55c0335178a2  dfly::DbSlice::FlushDb()
2025-01-22T15:15:34+01:00     @     0x55c033296f2a  _ZN4absl12lts_2024011619functional_internal12InvokeObjectIZN4dfly12ServerFamily8DrakarysEPNS3_11TransactionEtEUlS6_PNS3_11EngineShardEE_NS5_14RunnableResultEJS6_S8_EEET0_NS1_7VoidPtrEDpNS1_8ForwardTIT1_E4typeE
2025-01-22T15:15:34+01:00     @     0x55c0335577b9  dfly::Transaction::RunCallback()
2025-01-22T15:15:34+01:00     @     0x55c03355a59b  dfly::Transaction::RunInShard()
2025-01-22T15:15:34+01:00     @     0x55c033498790  dfly::EngineShard::PollExecution()
2025-01-22T15:15:34+01:00     @     0x55d7633db3a1  _ZNSt17_Function_handlerIFvvEZN4dfly11Transaction11DispatchHopEvEUlvE1_E9_M_invokeERKSt9_Any_data
2025-01-22T15:15:34+01:00     @     0x55d76394cf55  util::fb2::FiberQueue::Run()
2025-01-22T15:15:34+01:00     @     0x55d76342d730  _ZN5boost7context6detail11fiber_entryINS1_12fiber_recordINS0_5fiberEN4util3fb219FixedStackAllocatorEZNS6_6detail15WorkerFiberImplIZN4dfly9TaskQueue5StartESt17basic_string_viewIcSt11char_traitsIcEEEUlvE_JEEC4IS7_EESF_RKNS0_12preallocatedEOT_OSG_EUlOS4_E_EEEEvNS1_10transfer_tE
2025-01-22T15:15:34+01:00     @     0x55d76396c25f  make_fcontext
2025-01-22T15:15:34+01:00     *** SIGABRT received at time=1737556484 on cpu 0 ***
2025-01-22T15:15:34+01:00     PC: @     0x7f1faca299fc  (unknown)  pthread_kill
2025-01-22T15:15:34+01:00     [failure_signal_handler.cc : 345] RAW: Signal 11 raised at PC=0x7f1fac9bb898 while already in AbslFailureSignalHandler()
2025-01-22T15:15:34+01:00     *** SIGSEGV received at time=1737556484 on cpu 0 ***
2025-01-22T15:15:34+01:00     PC: @     0x7f1fac9bb898  (unknown)  abort
```

This happens in a loop.

If we delete snapshot, a full sync happens and member starts correctly : 
```
I20250122 14:18:26.534972     1 dfly_main.cc:691] Starting dragonfly df-v1.26.1-c2b95415241debb99bf969ebbc8465ce0bfe206f
I20250122 14:18:26.535194     1 dfly_main.cc:735] maxmemory has not been specified. Deciding myself....
I20250122 14:18:26.535209     1 dfly_main.cc:744] Found 3.00GiB available memory. Setting maxmemory to 2.40GiB
W20250122 14:18:26.535251     1 dfly_main.cc:368] Weird error 1 switching to epoll
I20250122 14:18:26.612828     1 proactor_pool.cc:147] Running 1 io threads
I20250122 14:18:26.615834     1 dfly_main.cc:272] Listening on admin socket any:9999
I20250122 14:18:26.616796     1 server_family.cc:835] Host OS: Linux 5.14.0-480.el9.x86_64 x86_64 with 1 threads
I20250122 14:18:26.620615     1 snapshot_storage.cc:185] Load snapshot: Searching for snapshot in directory: "/dragonfly/snapshots"
W20250122 14:18:26.623203     1 server_family.cc:950] Load snapshot: No snapshot found
I20250122 14:18:26.630414     6 listener_interface.cc:101] sock[5] AcceptServer - listening on port 9999
I20250122 14:18:26.630442     6 listener_interface.cc:101] sock[6] AcceptServer - listening on port 6379
I20250122 14:18:26.630453     6 listener_interface.cc:101] sock[7] AcceptServer - listening on port 11211
I20250122 14:18:46.526578     6 server_family.cc:2769] Replicating 10.233.130.142:9999
I20250122 14:18:47.943701     6 replica.cc:569] Started full sync with 10.233.130.142:9999
I20250122 14:18:49.127570     6 replica.cc:589] full sync finished in 2.6 s
I20250122 14:18:49.127673     6 replica.cc:679] Transitioned into stable sync
```

On another cluster, even deleting snapshot and full resync failed (100% failure after 10+ restarts) : 
```
I20250122 14:36:19.137816     1 server_family.cc:835] Host OS: Linux 5.14.0-480.el9.x86_64 x86_64 with 1 threads
I20250122 14:36:19.149236     1 snapshot_storage.cc:185] Load snapshot: Searching for snapshot in directory: "/dragonfly/snapshots"
W20250122 14:36:19.152247     1 server_family.cc:950] Load snapshot: No snapshot found
I20250122 14:36:19.160511     6 listener_interface.cc:101] sock[5] AcceptServer - listening on port 9999
I20250122 14:36:19.160552     6 listener_interface.cc:101] sock[6] AcceptServer - listening on port 6379
I20250122 14:36:19.160574     6 listener_interface.cc:101] sock[7] AcceptServer - listening on port 11211
I20250122 14:36:37.548327     6 server_family.cc:2769] Replicating 10.233.131.239:9999
I20250122 14:36:37.560622     6 replica.cc:569] Started full sync with 10.233.131.239:9999
I20250122 14:36:37.593991     6 server_family.cc:2769] Replicating 10.233.131.239:9999
W20250122 14:36:37.594259     6 replica.cc:246] Error syncing with 10.233.131.239:9999 generic:125 Operation canceled
I20250122 14:36:37.660379     6 replica.cc:569] Started full sync with 10.233.131.239:9999
I20250122 14:36:39.770289     6 server_family.cc:2769] Replicating 10.233.131.239:9999
W20250122 14:36:39.770624     6 replica.cc:246] Error syncing with 10.233.131.239:9999 generic:125 Operation canceled
F20250122 14:36:39.775619     6 db_slice.cc:783] Check failed: fetched_items_.empty() 
*** Check failure stack trace: ***
    @     0x55681cb35923  google::LogMessage::SendToLog()
    @     0x55681cb2e0e7  google::LogMessage::Flush()
    @     0x55681cb2fa6f  google::LogMessageFatal::~LogMessageFatal()
    @     0x55681c36e70f  dfly::DbSlice::FlushDbIndexes()
    @     0x55681c36e8a2  dfly::DbSlice::FlushDb()
    @     0x55681c0edf2a  _ZN4absl12lts_2024011619functional_internal12InvokeObjectIZN4dfly12ServerFamily8DrakarysEPNS3_11TransactionEtEUlS6_PNS3_11EngineShardEE_NS5_14RunnableResultEJS6_S8_EEET0_NS1_7VoidPtrEDpNS1_8ForwardTIT1_E4typeE
    @     0x55681c3ae7b9  dfly::Transaction::RunCallback()
    @     0x55681c3b159b  dfly::Transaction::RunInShard()
    @     0x55681c2ef790  dfly::EngineShard::PollExecution()
    @     0x55681c3aa3a1  _ZNSt17_Function_handlerIFvvEZN4dfly11Transaction11DispatchHopEvEUlvE1_E9_M_invokeERKSt9_Any_data
    @     0x55681c91bf55  util::fb2::FiberQueue::Run()
    @     0x55681c3fc730  _ZN5boost7context6detail11fiber_entryINS1_12fiber_recordINS0_5fiberEN4util3fb219FixedStackAllocatorEZNS6_6detail15WorkerFiberImplIZN4dfly9TaskQueue5StartESt17basic_string_viewIcSt11char_traitsIcEEEUlvE_JEEC4IS7_EESF_RKNS0_12preallocatedEOT_OSG_EUlOS4_E_EEEEvNS1_10transfer_tE
    @     0x55681c93b25f  make_fcontext
*** SIGABRT received at time=1737556599 on cpu 0 ***
PC: @     0x7f4768cc29fc  (unknown)  pthread_kill
[failure_signal_handler.cc : 345] RAW: Signal 11 raised at PC=0x7f4768c54898 while already in AbslFailureSignalHandler()

```

on master instance, at the same moment : 
```
I20250122 14:36:37.550398     6 dflycmd.cc:651] Registered replica 10.233.130.163:6379
I20250122 14:36:37.559907     6 dflycmd.cc:345] Started sync with replica 10.233.130.163:6379
I20250122 14:36:37.648327     6 dflycmd.cc:110] Disconnecting from replica 10.233.130.163:6379
I20250122 14:36:37.648401     6 rdb_save.cc:1247] Error writing to rdb sink Broken pipe
I20250122 14:36:37.648496     6 dflycmd.cc:641] Replication error: Operation canceled: Context cancelled
I20250122 14:36:37.650457     6 dflycmd.cc:651] Registered replica 10.233.130.163:6379
I20250122 14:36:37.659725     6 dflycmd.cc:345] Started sync with replica 10.233.130.163:6379
I20250122 14:36:39.784649     6 rdb_save.cc:1247] Error writing to rdb sink Broken pipe
I20250122 14:36:39.784729     6 dflycmd.cc:641] Replication error: Broken pipe
I20250122 14:36:39.784785     6 dflycmd.cc:110] Disconnecting from replica 10.233.130.163:6379
```

In this case we were forced to start over and setup a new empty cluster.


**To Reproduce**
This was reproduced in FOUR different Dragonfly cluster, with same update (same version source/destination).
Every time but one, deleting dfs file makes it work.


**Expected behavior**
Working update, or at least a full resync from master without crash.

**Environment (please complete the following information):**
 - Dragonfly official image docker.dragonflydb.io/dragonflydb/dragonfly
 - Run in Kubernetes 1.30.4
</issue>

I need you to solve the provided issue by generating a code fix that can be applied directly to the repository

Respond below:
