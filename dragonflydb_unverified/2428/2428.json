{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 2428,
  "instance_id": "dragonflydb__dragonfly-2428",
  "issue_numbers": [
    "2381"
  ],
  "base_commit": "b66db852f9095511106bfac23e3a62570d57c53b",
  "patch": "diff --git a/src/server/string_family.cc b/src/server/string_family.cc\nindex 6b1f91e72b1b..257443b4bc87 100644\n--- a/src/server/string_family.cc\n+++ b/src/server/string_family.cc\n@@ -323,21 +323,38 @@ int64_t AbsExpiryToTtl(int64_t abs_expiry_time, bool as_milli) {\n }\n \n // Returns true if keys were set, false otherwise.\n-OpStatus OpMSet(const OpArgs& op_args, ArgSlice args) {\n+void OpMSet(const OpArgs& op_args, ArgSlice args, atomic_bool* success) {\n   DCHECK(!args.empty() && args.size() % 2 == 0);\n \n   SetCmd::SetParams params;\n   SetCmd sg(op_args, false);\n \n-  for (size_t i = 0; i < args.size(); i += 2) {\n+  size_t i = 0;\n+  for (; i < args.size(); i += 2) {\n     DVLOG(1) << \"MSet \" << args[i] << \":\" << args[i + 1];\n     OpResult<optional<string>> res = sg.Set(params, args[i], args[i + 1]);\n     if (res.status() != OpStatus::OK) {  // OOM for example.\n-      return res.status();\n+      success->store(false);\n+      break;\n     }\n   }\n \n-  return OpStatus::OK;\n+  if (auto journal = op_args.shard->journal(); journal) {\n+    // We write a custom journal because an OOM in the above loop could lead to partial success, so\n+    // we replicate only what was changed.\n+    string_view cmd;\n+    ArgSlice cmd_args;\n+    if (i == 0) {\n+      // All shards must record the tx was executed for the replica to execute it, so we send a PING\n+      // in case nothing was changed\n+      cmd = \"PING\";\n+    } else {\n+      // journal [0, i)\n+      cmd = \"MSET\";\n+      cmd_args = ArgSlice(&args[0], i);\n+    }\n+    RecordJournal(op_args, cmd, cmd_args, op_args.tx->GetUniqueShardCnt());\n+  }\n }\n \n // See comment for SetCmd::Set() for when and how OpResult's value (i.e. optional<string>) is set.\n@@ -1227,13 +1244,15 @@ void StringFamily::MSet(CmdArgList args, ConnectionContext* cntx) {\n     LOG(INFO) << \"MSET/\" << transaction->GetUniqueShardCnt() << str;\n   }\n \n+  atomic_bool success = true;\n   auto cb = [&](Transaction* t, EngineShard* shard) {\n     auto args = t->GetShardArgs(shard->shard_id());\n-    return OpMSet(t->GetOpArgs(shard), args);\n+    OpMSet(t->GetOpArgs(shard), args, &success);\n+    return OpStatus::OK;\n   };\n \n   OpStatus status = transaction->ScheduleSingleHop(std::move(cb));\n-  if (status == OpStatus::OK) {\n+  if (success.load()) {\n     cntx->SendOk();\n   } else {\n     cntx->SendError(status);\n@@ -1261,19 +1280,20 @@ void StringFamily::MSetNx(CmdArgList args, ConnectionContext* cntx) {\n   };\n \n   transaction->Execute(std::move(cb), false);\n-  bool to_skip = exists.load(memory_order_relaxed) == true;\n+  const bool to_skip = exists.load(memory_order_relaxed);\n \n+  atomic_bool success = true;\n   auto epilog_cb = [&](Transaction* t, EngineShard* shard) {\n     if (to_skip)\n       return OpStatus::OK;\n \n     auto args = t->GetShardArgs(shard->shard_id());\n-    return OpMSet(t->GetOpArgs(shard), std::move(args));\n+    OpMSet(t->GetOpArgs(shard), std::move(args), &success);\n+    return OpStatus::OK;\n   };\n-\n   transaction->Execute(std::move(epilog_cb), true);\n \n-  cntx->SendLong(to_skip ? 0 : 1);\n+  cntx->SendLong(to_skip || !success.load() ? 0 : 1);\n }\n \n void StringFamily::StrLen(CmdArgList args, ConnectionContext* cntx) {\n@@ -1500,6 +1520,9 @@ constexpr uint32_t kClThrottle = THROTTLE;\n }  // namespace acl\n \n void StringFamily::Register(CommandRegistry* registry) {\n+  constexpr uint32_t kMSetMask =\n+      CO::WRITE | CO::DENYOOM | CO::INTERLEAVED_KEYS | CO::NO_AUTOJOURNAL;\n+\n   registry->StartFamily();\n   *registry\n       << CI{\"SET\", CO::WRITE | CO::DENYOOM | CO::NO_AUTOJOURNAL, -3, 1, 1, acl::kSet}.HFUNC(Set)\n@@ -1522,10 +1545,8 @@ void StringFamily::Register(CommandRegistry* registry) {\n       << CI{\"GETSET\", CO::WRITE | CO::DENYOOM | CO::FAST, 3, 1, 1, acl::kGetSet}.HFUNC(GetSet)\n       << CI{\"MGET\", CO::READONLY | CO::FAST | CO::REVERSE_MAPPING, -2, 1, -1, acl::kMGet}.HFUNC(\n              MGet)\n-      << CI{\"MSET\", CO::WRITE | CO::DENYOOM | CO::INTERLEAVED_KEYS, -3, 1, -1, acl::kMSet}.HFUNC(\n-             MSet)\n-      << CI{\"MSETNX\", CO::WRITE | CO::DENYOOM | CO::INTERLEAVED_KEYS, -3, 1, -1, acl::kMSetNx}\n-             .HFUNC(MSetNx)\n+      << CI{\"MSET\", kMSetMask, -3, 1, -1, acl::kMSet}.HFUNC(MSet)\n+      << CI{\"MSETNX\", kMSetMask, -3, 1, -1, acl::kMSetNx}.HFUNC(MSetNx)\n       << CI{\"STRLEN\", CO::READONLY | CO::FAST, 2, 1, 1, acl::kStrLen}.HFUNC(StrLen)\n       << CI{\"GETRANGE\", CO::READONLY | CO::FAST, 4, 1, 1, acl::kGetRange}.HFUNC(GetRange)\n       << CI{\"SUBSTR\", CO::READONLY | CO::FAST, 4, 1, 1, acl::kSubStr}.HFUNC(\ndiff --git a/src/server/transaction.cc b/src/server/transaction.cc\nindex 8d74505683ea..9bf1dc5a82a1 100644\n--- a/src/server/transaction.cc\n+++ b/src/server/transaction.cc\n@@ -555,7 +555,7 @@ bool Transaction::RunInShard(EngineShard* shard, bool txq_ooo) {\n \n   // Log to jounrnal only once the command finished running\n   if (is_concluding || (multi_ && multi_->concluding))\n-    LogAutoJournalOnShard(shard);\n+    LogAutoJournalOnShard(shard, result);\n \n   // If we're the head of tx queue (txq_ooo is false), we remove ourselves upon first invocation\n   // and successive hops are run by continuation_trans_ in engine shard.\n@@ -1047,6 +1047,10 @@ ShardId Transaction::GetUniqueShard() const {\n   return unique_shard_id_;\n }\n \n+optional<SlotId> Transaction::GetUniqueSlotId() const {\n+  return unique_slot_checker_.GetUniqueSlotId();\n+}\n+\n KeyLockArgs Transaction::GetLockArgs(ShardId sid) const {\n   KeyLockArgs res;\n   res.db_index = db_index_;\n@@ -1089,7 +1093,7 @@ bool Transaction::ScheduleUniqueShard(EngineShard* shard) {\n       DCHECK_EQ(sd.is_armed, false);\n       unlocked_keys = false;\n     } else {\n-      LogAutoJournalOnShard(shard);\n+      LogAutoJournalOnShard(shard, result);\n     }\n   }\n \n@@ -1327,7 +1331,7 @@ OpStatus Transaction::RunSquashedMultiCb(RunnableType cb) {\n   auto* shard = EngineShard::tlocal();\n   auto result = cb(this, shard);\n   shard->db_slice().OnCbFinish();\n-  LogAutoJournalOnShard(shard);\n+  LogAutoJournalOnShard(shard, result);\n \n   DCHECK_EQ(result.flags, 0);  // if it's sophisticated, we shouldn't squash it\n   return result;\n@@ -1454,7 +1458,7 @@ optional<string_view> Transaction::GetWakeKey(ShardId sid) const {\n   return GetShardArgs(sid).at(sd.wake_key_pos);\n }\n \n-void Transaction::LogAutoJournalOnShard(EngineShard* shard) {\n+void Transaction::LogAutoJournalOnShard(EngineShard* shard, RunnableResult result) {\n   // TODO: For now, we ignore non shard coordination.\n   if (shard == nullptr)\n     return;\n@@ -1467,14 +1471,22 @@ void Transaction::LogAutoJournalOnShard(EngineShard* shard) {\n   if (cid_->IsWriteOnly() == 0 && (cid_->opt_mask() & CO::NO_KEY_TRANSACTIONAL) == 0)\n     return;\n \n-  // If autojournaling was disabled and not re-enabled, skip it\n-  if ((cid_->opt_mask() & CO::NO_AUTOJOURNAL) && !renabled_auto_journal_.load(memory_order_relaxed))\n-    return;\n-\n   auto journal = shard->journal();\n   if (journal == nullptr)\n     return;\n \n+  if (result.status != OpStatus::OK) {\n+    // We log NOOP even for NO_AUTOJOURNAL commands because the non-success status could have been\n+    // due to OOM in a single shard, while other shards succeeded\n+    journal->RecordEntry(txid_, journal::Op::NOOP, db_index_, unique_shard_cnt_,\n+                         unique_slot_checker_.GetUniqueSlotId(), journal::Entry::Payload{}, true);\n+    return;\n+  }\n+\n+  // If autojournaling was disabled and not re-enabled, skip it\n+  if ((cid_->opt_mask() & CO::NO_AUTOJOURNAL) && !renabled_auto_journal_.load(memory_order_relaxed))\n+    return;\n+\n   // TODO: Handle complex commands like LMPOP correctly once they are implemented.\n   journal::Entry::Payload entry_payload;\n \ndiff --git a/src/server/transaction.h b/src/server/transaction.h\nindex c6ecb518f85b..18d16c1ce030 100644\n--- a/src/server/transaction.h\n+++ b/src/server/transaction.h\n@@ -295,6 +295,8 @@ class Transaction {\n   // This method is meaningless if GetUniqueShardCnt() != 1.\n   ShardId GetUniqueShard() const;\n \n+  std::optional<SlotId> GetUniqueSlotId() const;\n+\n   bool IsMulti() const {\n     return bool(multi_);\n   }\n@@ -518,7 +520,7 @@ class Transaction {\n \n   // Log command in shard's journal, if this is a write command with auto-journaling enabled.\n   // Should be called immediately after the last phase (hop).\n-  void LogAutoJournalOnShard(EngineShard* shard);\n+  void LogAutoJournalOnShard(EngineShard* shard, RunnableResult shard_result);\n \n   // Returns the previous value of run count.\n   uint32_t DecreaseRunCnt();\n",
  "test_patch": "diff --git a/tests/dragonfly/replication_test.py b/tests/dragonfly/replication_test.py\nindex 539c7c5cf29a..acac258124b8 100644\n--- a/tests/dragonfly/replication_test.py\n+++ b/tests/dragonfly/replication_test.py\n@@ -1838,12 +1838,8 @@ async def test_replicaof_reject_on_load(df_local_factory, df_seeder_factory):\n     replica.stop()\n \n \n-# note: please be careful if you want to change any of the parameters used in this test.\n-# changing parameters without extensive testing may easily lead to weak testing case assertion\n-# which means eviction may not get triggered.\n @pytest.mark.asyncio\n-@pytest.mark.skip(reason=\"Failing due to bug in replication on command errors\")\n-async def test_policy_based_eviction_propagation(df_local_factory, df_seeder_factory):\n+async def test_heartbeat_eviction_propagation(df_local_factory):\n     master = df_local_factory.create(\n         proactor_threads=1, cache_mode=\"true\", maxmemory=\"256mb\", enable_heartbeat_eviction=\"false\"\n     )\n@@ -1853,69 +1849,65 @@ async def test_policy_based_eviction_propagation(df_local_factory, df_seeder_fac\n     c_master = master.client()\n     c_replica = replica.client()\n \n-    await c_master.execute_command(\"DEBUG POPULATE 6000 size 44000\")\n-\n+    # fill the master to use about 233mb > 256mb * 0.9, which will trigger heartbeat eviction.\n+    await c_master.execute_command(\"DEBUG POPULATE 233 size 1048576\")\n     await c_replica.execute_command(f\"REPLICAOF localhost {master.port}\")\n     await wait_available_async(c_replica)\n \n-    seeder = df_seeder_factory.create(\n-        port=master.port,\n-        keys=500,\n-        val_size=200,\n-        stop_on_failure=False,\n-        unsupported_types=[\n-            ValueType.JSON,\n-            ValueType.LIST,\n-            ValueType.SET,\n-            ValueType.HSET,\n-            ValueType.ZSET,\n-        ],\n-    )\n-    await seeder.run(target_deviation=0.1)\n+    # now enable heart beat eviction\n+    await c_master.execute_command(\"CONFIG SET enable_heartbeat_eviction true\")\n \n-    info = await c_master.info(\"stats\")\n-    assert info[\"evicted_keys\"] > 0, \"Weak testcase: policy based eviction was not triggered.\"\n+    while True:\n+        info = await c_master.info(\"stats\")\n+        evicted_1 = info[\"evicted_keys\"]\n+        time.sleep(2)\n+        info = await c_master.info(\"stats\")\n+        evicted_2 = info[\"evicted_keys\"]\n+        if evicted_2 == evicted_1:\n+            break\n+        else:\n+            print(\"waiting for eviction to finish...\", end=\"\\r\", flush=True)\n \n     await check_all_replicas_finished([c_replica], c_master)\n     keys_master = await c_master.execute_command(\"keys *\")\n     keys_replica = await c_replica.execute_command(\"keys *\")\n-\n     assert set(keys_master) == set(keys_replica)\n     await disconnect_clients(c_master, *[c_replica])\n \n \n @pytest.mark.asyncio\n-async def test_heartbeat_eviction_propagation(df_local_factory):\n+async def test_policy_based_eviction_propagation(df_local_factory, df_seeder_factory):\n     master = df_local_factory.create(\n-        proactor_threads=1, cache_mode=\"true\", maxmemory=\"256mb\", enable_heartbeat_eviction=\"false\"\n+        proactor_threads=2,\n+        cache_mode=\"true\",\n+        maxmemory=\"512mb\",\n+        logtostdout=\"true\",\n+        enable_heartbeat_eviction=\"false\",\n     )\n-    replica = df_local_factory.create(proactor_threads=1)\n+    replica = df_local_factory.create(proactor_threads=2)\n     df_local_factory.start_all([master, replica])\n \n     c_master = master.client()\n     c_replica = replica.client()\n \n-    # fill the master to use about 233mb > 256mb * 0.9, which will trigger heartbeat eviction.\n-    await c_master.execute_command(\"DEBUG POPULATE 233 size 1048576\")\n+    await c_master.execute_command(\"DEBUG POPULATE 6000 size 88000\")\n+\n     await c_replica.execute_command(f\"REPLICAOF localhost {master.port}\")\n     await wait_available_async(c_replica)\n \n-    # now enable heart beat eviction\n-    await c_master.execute_command(\"CONFIG SET enable_heartbeat_eviction true\")\n+    seeder = df_seeder_factory.create(\n+        port=master.port, keys=500, val_size=1000, stop_on_failure=False\n+    )\n+    await seeder.run(target_deviation=0.1)\n \n-    while True:\n-        info = await c_master.info(\"stats\")\n-        evicted_1 = info[\"evicted_keys\"]\n-        time.sleep(2)\n-        info = await c_master.info(\"stats\")\n-        evicted_2 = info[\"evicted_keys\"]\n-        if evicted_2 == evicted_1:\n-            break\n-        else:\n-            print(\"waiting for eviction to finish...\", end=\"\\r\", flush=True)\n+    info = await c_master.info(\"stats\")\n+    assert info[\"evicted_keys\"] > 0, \"Weak testcase: policy based eviction was not triggered.\"\n \n     await check_all_replicas_finished([c_replica], c_master)\n-    keys_master = await c_master.execute_command(\"keys *\")\n-    keys_replica = await c_replica.execute_command(\"keys *\")\n+    keys_master = await c_master.execute_command(\"keys k*\")\n+    keys_replica = await c_replica.execute_command(\"keys k*\")\n+\n+    assert len(keys_master) == len(keys_replica)\n     assert set(keys_master) == set(keys_replica)\n+\n     await disconnect_clients(c_master, *[c_replica])\n",
  "problem_statement": "Dragonfly replica inconsistent with master data\n**Describe the bug**\r\nWhen master executes a write command the command is send to replica (journal change). If the callback on master side returns error we still send it to replica which can result in inconsistency between master and replica. f.e if master returned with oom when executing the command replica will still execute the command. \r\n\r\n**To Reproduce**\r\n```python\r\n@pytest.mark.asyncio\r\nasync def test_policy_based_eviction_propagation(df_local_factory, df_seeder_factory):\r\n    master = df_local_factory.create(\r\n        proactor_threads=1,\r\n        cache_mode=\"true\",\r\n        maxmemory=\"256mb\",\r\n        enable_heartbeat_eviction=\"false\"\r\n    )\r\n    replica = df_local_factory.create(proactor_threads=1)\r\n    df_local_factory.start_all([master, replica])\r\n    df_local_factory.start_all([master])\r\n\r\n    c_master = master.client()\r\n    c_replica = replica.client()\r\n\r\n    await c_master.execute_command(\"DEBUG POPULATE 6000 size 44000\")\r\n\r\n    await c_replica.execute_command(f\"REPLICAOF localhost {master.port}\")\r\n    await wait_available_async(c_replica)\r\n\r\n    seeder = df_seeder_factory.create(port=master.port, keys=500, val_size=1000, stop_on_failure=False)\r\n    await seeder.run(target_deviation=0.1)\r\n\r\n    info = await c_master.info(\"stats\")\r\n    assert info[\"evicted_keys\"] > 0, \"Weak testcase: policy based eviction was not triggered.\"\r\n\r\n    await check_all_replicas_finished([c_replica], c_master)\r\n    keys_master = await c_master.execute_command(\"keys *\")\r\n    keys_replica = await c_replica.execute_command(\"keys *\")\r\n    \r\n    assert set(keys_master) != set(keys_replica):\r\n    await disconnect_clients(c_master, *[c_replica])\r\n```\r\n\r\n**Expected behavior**\r\nmaster will not replicate a command that returned error on callback execution\r\n\r\n\n",
  "hints_text": "I looked into this issue for most of today.\r\nIt's quite easy to add an early return inside `LogAutoJournalOnShard()` in case `local_result_` is not `OK`. After some debugging I figured out a place to patch to fix a bug with that, so that's pretty easy, and it solves the cases where the master sends to the replica commands that it did not execute.\r\nThere is a challenge, though: partial success.\r\nFor example, think of the case of `MSET k1 ... k2 ... k3 ...`, where the OOM was thrown for k3, i.e. k1 and k2 *were* set. In this case (assume all are in the same shard), what do we send to the replica? We need some intimate details of the command and what exactly failed within it.\r\nIt's a much more rare failure, but I can catch it with Adi's test every few (many) runs.\r\nUnfortunately, I'm not sure how to address it. I thought that, in the case of an error, we can simply serialize all relevant keys, maybe in the format of a `RESTORE` command like we do for slot migration, unless anyone else can think of a better idea?\nWe are not a database. Redis, for example, crashes in some cases of OOM.\r\nI think we should do a reasonable effort for replicating correct results but in case of OOM we do not guarantee atomicity of MULTI/EXEC transactions, lua scripts and we can not guarantee correctness of replication.\r\nFor this we would need a transaction log and being able to rollback. We can not afford it for in-memory store.\nSo you don't even want the `RESTORE` solution?\r\nNote that it's not about `MULTI`/`EXEC` atomicity. Even in the case of a simple `MSET` with 2 values, that atomicity is not guaranteed (not in the master / standalone, nor in the replica)\nIn case of OOM we do not guarantee the atomicity of multi transactions and lua scripts, but with shahar fix we should be able to replicate only the commands that where executed successfully and we will be able to guarantee correctness of replication.\r\nFor MSET command (or other write multi key commands if there are any) we can do the following change which can ensure the execution of mset command for all keys in shard which will guarantee correctness of replication:\r\nToday in mset we do a for loop on all keys insert key and value one by one, to make sure we dont get bad_alloc for one of the keys in the mset we can do first insert all keys, only if all keys are inserted we can do the value setting. Note that this flow is not so trivial because once we insert a key it can invalidate the iterators of the other inserted keys.\nI sent https://github.com/dragonflydb/dragonfly/pull/2416 for review, which fixes command failures except for partial failures.\r\n\r\n> For MSET command (or other write multi key commands if there are any) we can do the following (...)\r\n\r\nThere are a few downsides to this approach:\r\n1. It requires changes to all mutating multi-key commands, present and future\r\n2. As you said, implementing this is not trivial (and I'd even say that it's not guaranteed to work, for example pushing an item to an existing list can also OOM no?)\r\n\r\nThe proposal of automatically sending a `RESTORE` command for all relevant keys is a one place change, which doesn't sound too hard to implement, and is also future proof..\n@adiholden and I discussed this and came to the following conclusions:\r\n1. The only command that can create multiple keys is `MSET` (note that other commands that \"touch\" multiple keys, like `LMOVE` or `SUNIONSTORE` can only create at most 1 key)\r\n2. For all commands _except for `MSET`_, we'll simply send a `NOOP` opcode on shard callback failure. This will be sent also for non auto journal commands.\r\n3. For `MSET` (and potentially future commands which we do not currently support) we will use non-auto journal, where the callback will always return `Status::OK` (to force not sending `NOOP` from transaction, see (2) above), and we will custom handle sending the subset of the command that succeeded:\r\n\r\n```cpp\r\natomic_bool success = true;\r\n\r\nScheduleSingleHop([&]() {\r\n  int i = 0;\r\n  for (i = 0; i < args.count(); ++i) {\r\n    if (!sg.Set(...)) {\r\n      success = false;\r\n      break;\r\n    }\r\n  }\r\n   \r\n  if (i == 0) {\r\n    journal no-op\r\n  } else {\r\n    journal: MSET [0, i)\r\n  }\r\n\r\n  return OK;\r\n});\r\n\r\nif (!success) {\r\n  send error\r\n}\r\n```",
  "created_at": "2024-01-17T06:59:18Z",
  "modified_files": [
    "src/server/string_family.cc",
    "src/server/transaction.cc",
    "src/server/transaction.h"
  ],
  "modified_test_files": [
    "tests/dragonfly/replication_test.py"
  ]
}