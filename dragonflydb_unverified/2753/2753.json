{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 2753,
  "instance_id": "dragonflydb__dragonfly-2753",
  "issue_numbers": [
    "2636"
  ],
  "base_commit": "31fabf21826d72007f89f8745743310134a28783",
  "patch": "diff --git a/src/server/replica.cc b/src/server/replica.cc\nindex a5051a81739b..0b3474dda783 100644\n--- a/src/server/replica.cc\n+++ b/src/server/replica.cc\n@@ -41,6 +41,8 @@ ABSL_FLAG(int, master_reconnect_timeout_ms, 1000,\n           \"Timeout for re-establishing connection to a replication master\");\n ABSL_FLAG(bool, replica_partial_sync, true,\n           \"Use partial sync to reconnect when a replica connection is interrupted.\");\n+ABSL_FLAG(bool, replica_reconnect_on_master_restart, false,\n+          \"When in replica mode, and master restarts, break replication from master.\");\n ABSL_DECLARE_FLAG(int32_t, port);\n \n namespace dfly {\n@@ -303,10 +305,18 @@ std::error_code Replica::HandleCapaDflyResp() {\n   }\n \n   // If we're syncing a different replication ID, drop the saved LSNs.\n-  if (master_context_.master_repl_id != ToSV(LastResponseArgs()[0].GetBuf())) {\n+  string_view master_repl_id = ToSV(LastResponseArgs()[0].GetBuf());\n+  if (master_context_.master_repl_id != master_repl_id) {\n+    if (absl::GetFlag(FLAGS_replica_reconnect_on_master_restart) &&\n+        !master_context_.master_repl_id.empty()) {\n+      LOG(ERROR) << \"Encountered different master repl id (\" << master_repl_id << \" vs \"\n+                 << master_context_.master_repl_id << \")\";\n+      state_mask_.store(0);\n+      return make_error_code(errc::connection_aborted);\n+    }\n     last_journal_LSNs_.reset();\n   }\n-  master_context_.master_repl_id = ToSV(LastResponseArgs()[0].GetBuf());\n+  master_context_.master_repl_id = master_repl_id;\n   master_context_.dfly_session_id = ToSV(LastResponseArgs()[1].GetBuf());\n   num_df_flows_ = param_num_flows;\n \n",
  "test_patch": "diff --git a/tests/dragonfly/proxy.py b/tests/dragonfly/proxy.py\nindex a0bf06575bec..4e96b4d293f1 100644\n--- a/tests/dragonfly/proxy.py\n+++ b/tests/dragonfly/proxy.py\n@@ -68,9 +68,14 @@ def drop_connection(self):\n             self.stop_connections.remove(cb)\n             cb()\n \n-    def close(self):\n+    async def close(self, task=None):\n         if self.server is not None:\n             self.server.close()\n             self.server = None\n         for cb in self.stop_connections:\n             cb()\n+        if not task == None:\n+            try:\n+                await task\n+            except asyncio.exceptions.CancelledError:\n+                pass\ndiff --git a/tests/dragonfly/replication_test.py b/tests/dragonfly/replication_test.py\nindex aec109d0da98..053732a6b91e 100644\n--- a/tests/dragonfly/replication_test.py\n+++ b/tests/dragonfly/replication_test.py\n@@ -1386,6 +1386,12 @@ async def test_tls_replication(\n     db_size = await c_master.execute_command(\"DBSIZE\")\n     assert 100 == db_size\n \n+    proxy = Proxy(\n+        \"127.0.0.1\", 1114, \"127.0.0.1\", master.port if not test_admin_port else master.admin_port\n+    )\n+    await proxy.start()\n+    proxy_task = asyncio.create_task(proxy.serve())\n+\n     # 2. Spin up a replica and initiate a REPLICAOF\n     replica = df_local_factory.create(\n         tls_replication=\"true\",\n@@ -1394,8 +1400,7 @@ async def test_tls_replication(\n     )\n     replica.start()\n     c_replica = replica.client(**with_ca_tls_client_args)\n-    port = master.port if not test_admin_port else master.admin_port\n-    res = await c_replica.execute_command(\"REPLICAOF localhost \" + str(port))\n+    res = await c_replica.execute_command(\"REPLICAOF localhost \" + str(proxy.port))\n     assert \"OK\" == res\n     await check_all_replicas_finished([c_replica], c_master)\n \n@@ -1403,22 +1408,24 @@ async def test_tls_replication(\n     db_size = await c_replica.execute_command(\"DBSIZE\")\n     assert 100 == db_size\n \n-    # 4. Kill master, spin it up and see if replica reconnects\n-    master.stop(kill=True)\n+    # 4. Break the connection between master and replica\n+    await proxy.close(proxy_task)\n     await asyncio.sleep(3)\n-    master.start()\n-    c_master = master.client(**with_ca_tls_client_args)\n-    # Master doesn't load the snapshot, therefore dbsize should be 0\n+    await proxy.start()\n+    proxy_task = asyncio.create_task(proxy.serve())\n+\n+    # Check replica gets new keys\n     await c_master.execute_command(\"SET MY_KEY 1\")\n     db_size = await c_master.execute_command(\"DBSIZE\")\n-    assert 1 == db_size\n+    assert 101 == db_size\n \n     await check_all_replicas_finished([c_replica], c_master)\n     db_size = await c_replica.execute_command(\"DBSIZE\")\n-    assert 1 == db_size\n+    assert 101 == db_size\n \n     await c_replica.close()\n     await c_master.close()\n+    await proxy.close(proxy_task)\n \n \n # busy wait for 'replica' instance to have replication status 'status'\n@@ -1641,11 +1648,7 @@ async def test_network_disconnect(df_local_factory, df_seeder_factory):\n             capture = await seeder.capture()\n             assert await seeder.compare(capture, replica.port)\n         finally:\n-            proxy.close()\n-            try:\n-                await task\n-            except asyncio.exceptions.CancelledError:\n-                pass\n+            await proxy.close(task)\n \n     master.stop()\n     replica.stop()\n@@ -1688,11 +1691,7 @@ async def test_network_disconnect_active_stream(df_local_factory, df_seeder_fact\n             capture = await seeder.capture()\n             assert await seeder.compare(capture, replica.port)\n         finally:\n-            proxy.close()\n-            try:\n-                await task\n-            except asyncio.exceptions.CancelledError:\n-                pass\n+            await proxy.close(task)\n \n     master.stop()\n     replica.stop()\n@@ -1739,11 +1738,7 @@ async def test_network_disconnect_small_buffer(df_local_factory, df_seeder_facto\n             capture = await seeder.capture()\n             assert await seeder.compare(capture, replica.port)\n         finally:\n-            proxy.close()\n-            try:\n-                await task\n-            except asyncio.exceptions.CancelledError:\n-                pass\n+            await proxy.close(task)\n \n     master.stop()\n     replica.stop()\n@@ -2060,6 +2055,13 @@ async def save_replica():\n     await disconnect_clients(c_master, *[c_replica])\n \n \n+async def is_replicaiton_conn_down(conn):\n+    role = await conn.execute_command(\"INFO REPLICATION\")\n+    # fancy of way of extracting the field master_link_status\n+    is_down = role.split(\"\\r\\n\")[4].split(\":\")[1]\n+    return is_down == \"down\"\n+\n+\n @pytest.mark.asyncio\n async def test_user_acl_replication(df_local_factory):\n     master = df_local_factory.create(proactor_threads=4)\n@@ -2083,10 +2085,7 @@ async def test_user_acl_replication(df_local_factory):\n     await c_master.execute_command(\"ACL SETUSER tmp -replconf\")\n     async with async_timeout.timeout(5):\n         while True:\n-            role = await c_replica.execute_command(\"INFO REPLICATION\")\n-            # fancy of way of extracting the field master_link_status\n-            is_down = role.split(\"\\r\\n\")[4].split(\":\")[1]\n-            if is_down == \"down\":\n+            if await is_replicaiton_conn_down(c_replica):\n                 break\n             await asyncio.sleep(1)\n \n@@ -2096,3 +2095,61 @@ async def test_user_acl_replication(df_local_factory):\n     await c_master.execute_command(\"ACL SETUSER tmp +replconf\")\n     await check_all_replicas_finished([c_replica], c_master, 5)\n     assert 2 == await c_replica.execute_command(\"DBSIZE\")\n+\n+\n+@pytest.mark.parametrize(\"break_conn\", [False, True])\n+@pytest.mark.asyncio\n+async def test_replica_reconnect(df_local_factory, break_conn):\n+    \"\"\"\n+    Test replica does not connect to master if master restarted\n+    step1: create master and replica\n+    step2: stop master and start again with the same port\n+    step3: check replica is not replicating the restarted master\n+    step4: issue new replicaof command\n+    step5: check replica replicates master\n+    \"\"\"\n+    # Connect replica to master\n+    master = df_local_factory.create(proactor_threads=1)\n+    replica = df_local_factory.create(\n+        proactor_threads=1, replica_reconnect_on_master_restart=break_conn\n+    )\n+    df_local_factory.start_all([master, replica])\n+\n+    c_master = master.client()\n+    c_replica = replica.client()\n+\n+    await c_master.execute_command(\"set k 12345\")\n+    await c_replica.execute_command(f\"REPLICAOF localhost {master.port}\")\n+    await wait_available_async(c_replica)\n+\n+    assert not await is_replicaiton_conn_down(c_replica)\n+\n+    # kill existing master, create master with different repl_id but same port\n+    master_port = master.port\n+    master.stop()\n+    assert await is_replicaiton_conn_down(c_replica)\n+\n+    master = df_local_factory.create(proactor_threads=1, port=master_port)\n+    df_local_factory.start_all([master])\n+    await asyncio.sleep(1)  # We sleep for 0.5s in replica.cc before reconnecting\n+\n+    # Assert that replica did not reconnected to master with different repl_id\n+    if break_conn:\n+        assert await c_master.execute_command(\"get k\") == None\n+        assert await c_replica.execute_command(\"get k\") == \"12345\"\n+        assert await c_master.execute_command(\"set k 6789\")\n+        assert await c_replica.execute_command(\"get k\") == \"12345\"\n+        assert await is_replicaiton_conn_down(c_replica)\n+    else:\n+        assert await c_master.execute_command(\"get k\") == None\n+        assert await c_replica.execute_command(\"get k\") == None\n+        assert await c_master.execute_command(\"set k 6789\")\n+        assert await c_replica.execute_command(\"get k\") == \"6789\"\n+        assert not await is_replicaiton_conn_down(c_replica)\n+\n+    # Force re-replication, assert that it worked\n+    await c_replica.execute_command(f\"REPLICAOF localhost {master.port}\")\n+    await wait_available_async(c_replica)\n+    assert await c_replica.execute_command(\"get k\") == \"6789\"\n+\n+    await disconnect_clients(c_master, c_replica)\n",
  "problem_statement": "master_id (runid) should protect from accidental master restarts\nCurrently we use master_id for two purposes: cluster node id generation and as \"master id\" during the replication.\r\n\r\n1. Similarly to Redis, we do not protect slave from accidental data flushes during full sync (see http://antirez.com/news/80 for context)\r\n2. Unlike Redis we currently do not employ partial sync where such protection exists\r\n\r\nNot directly related to this issue but important for providing additional context - using master_id as nodeid for cluster management is cumbersome and confusing.\r\n\r\nI suggest we implement master_id protection so that replica that had been synced already and reached SSR with the master id A, won't reconnect automatically with master under the same address with master id B.  Specifically, one would need to reissue \"replicaof ..\" command again to bootstrap the replication again.\r\n\r\nThis behavior change on replica side should be under flag with default to preserve the current behaviour. \n",
  "hints_text": "Also consider the case where master is restarted during takeover ",
  "created_at": "2024-03-20T12:31:23Z",
  "modified_files": [
    "src/server/replica.cc"
  ],
  "modified_test_files": [
    "tests/dragonfly/proxy.py",
    "tests/dragonfly/replication_test.py"
  ]
}