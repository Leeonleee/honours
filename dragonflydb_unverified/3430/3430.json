{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 3430,
  "instance_id": "dragonflydb__dragonfly-3430",
  "issue_numbers": [
    "3414"
  ],
  "base_commit": "41be819dbbff0b8dc5ece26ba9f75ce20f23b2b3",
  "patch": "diff --git a/src/server/common.h b/src/server/common.h\nindex 33efbcb35e73..c3fbca79a57a 100644\n--- a/src/server/common.h\n+++ b/src/server/common.h\n@@ -361,11 +361,6 @@ class UniquePicksGenerator : public PicksGenerator {\n   absl::BitGen bitgen_{};\n };\n \n-struct ConditionFlag {\n-  util::fb2::CondVarAny cond_var;\n-  bool flag = false;\n-};\n-\n // Helper class used to guarantee atomicity between serialization of buckets\n class ThreadLocalMutex {\n  public:\ndiff --git a/src/server/db_slice.cc b/src/server/db_slice.cc\nindex 2992e6d2fc33..2d7e13c5f678 100644\n--- a/src/server/db_slice.cc\n+++ b/src/server/db_slice.cc\n@@ -208,7 +208,8 @@ unsigned PrimeEvictionPolicy::Evict(const PrimeTable::HotspotBuckets& eb, PrimeT\n     if (auto journal = db_slice_->shard_owner()->journal(); journal) {\n       RecordExpiry(cntx_.db_index, key);\n     }\n-\n+    // Safe we already acquired std::unique_lock lk(db_slice_->GetSerializationMutex());\n+    // on the flows that call this function\n     db_slice_->PerformDeletion(DbSlice::Iterator(last_slot_it, StringOrView::FromView(key)), table);\n \n     ++evicted_;\n@@ -481,6 +482,8 @@ OpResult<DbSlice::PrimeItAndExp> DbSlice::FindInternal(const Context& cntx, std:\n \n   if (caching_mode_ && IsValid(res.it)) {\n     if (!change_cb_.empty()) {\n+      FetchedItemsRestorer fetched_restorer(&fetched_items_);\n+      std::unique_lock lk(local_mu_);\n       auto bump_cb = [&](PrimeTable::bucket_iterator bit) {\n         CallChangeCallbacks(cntx.db_index, key, bit);\n       };\n@@ -572,6 +575,9 @@ OpResult<DbSlice::AddOrFindResult> DbSlice::AddOrFindInternal(const Context& cnt\n   auto status = res.status();\n   CHECK(status == OpStatus::KEY_NOTFOUND || status == OpStatus::OUT_OF_MEMORY) << status;\n \n+  FetchedItemsRestorer fetched_restorer(&fetched_items_);\n+  std::unique_lock lk(local_mu_);\n+\n   // It's a new entry.\n   CallChangeCallbacks(cntx.db_index, key, {key});\n \n@@ -679,6 +685,8 @@ void DbSlice::ActivateDb(DbIndex db_ind) {\n }\n \n bool DbSlice::Del(Context cntx, Iterator it) {\n+  std::unique_lock lk(local_mu_);\n+\n   if (!IsValid(it)) {\n     return false;\n   }\n@@ -798,6 +806,10 @@ void DbSlice::FlushDbIndexes(const std::vector<DbIndex>& indexes) {\n }\n \n void DbSlice::FlushDb(DbIndex db_ind) {\n+  // We should not flush if serialization of a big value is in progress because this\n+  // could lead to UB or assertion failures (while DashTable::Traverse is iterating over\n+  // a logical bucket).\n+  std::unique_lock lk(local_mu_);\n   // clear client tracking map.\n   client_tracking_map_.clear();\n \n@@ -819,6 +831,7 @@ void DbSlice::FlushDb(DbIndex db_ind) {\n }\n \n void DbSlice::AddExpire(DbIndex db_ind, Iterator main_it, uint64_t at) {\n+  std::unique_lock lk(local_mu_);\n   uint64_t delta = at - expire_base_[0];  // TODO: employ multigen expire updates.\n   auto& db = *db_arr_[db_ind];\n   size_t table_before = db.expire.mem_usage();\n@@ -828,6 +841,7 @@ void DbSlice::AddExpire(DbIndex db_ind, Iterator main_it, uint64_t at) {\n }\n \n bool DbSlice::RemoveExpire(DbIndex db_ind, Iterator main_it) {\n+  std::unique_lock lk(local_mu_);\n   if (main_it->second.HasExpire()) {\n     auto& db = *db_arr_[db_ind];\n     size_t table_before = db.expire.mem_usage();\n@@ -1049,6 +1063,8 @@ bool DbSlice::CheckLock(IntentLock::Mode mode, DbIndex dbid, uint64_t fp) const\n }\n \n void DbSlice::PreUpdate(DbIndex db_ind, Iterator it, std::string_view key) {\n+  FetchedItemsRestorer fetched_restorer(&fetched_items_);\n+  std::unique_lock lk(local_mu_);\n   CallChangeCallbacks(db_ind, key, ChangeReq{it.GetInnerIt()});\n   it.GetInnerIt().SetVersion(NextVersion());\n }\n@@ -1216,13 +1232,13 @@ auto DbSlice::DeleteExpiredStep(const Context& cntx, unsigned count) -> DeleteEx\n \n   unsigned i = 0;\n   for (; i < count / 3; ++i) {\n-    db.expire_cursor = Traverse(&db.expire, db.expire_cursor, cb);\n+    db.expire_cursor = db.expire.Traverse(db.expire_cursor, cb);\n   }\n \n   // continue traversing only if we had strong deletion rate based on the first sample.\n   if (result.deleted * 4 > result.traversed) {\n     for (; i < count; ++i) {\n-      db.expire_cursor = Traverse(&db.expire, db.expire_cursor, cb);\n+      db.expire_cursor = db.expire.Traverse(db.expire_cursor, cb);\n     }\n   }\n \n@@ -1341,10 +1357,14 @@ void DbSlice::CreateDb(DbIndex db_ind) {\n \n void DbSlice::RegisterWatchedKey(DbIndex db_indx, std::string_view key,\n                                  ConnectionState::ExecInfo* exec_info) {\n+  // Because we might insert while another fiber is preempted\n+  std::unique_lock lk(local_mu_);\n   db_arr_[db_indx]->watched_keys[key].push_back(exec_info);\n }\n \n void DbSlice::UnregisterConnectionWatches(const ConnectionState::ExecInfo* exec_info) {\n+  // Because we might remove while another fiber is preempted and miss a notification\n+  std::unique_lock lk(local_mu_);\n   for (const auto& [db_indx, key] : exec_info->watched_keys) {\n     auto& watched_keys = db_arr_[db_indx]->watched_keys;\n     if (auto it = watched_keys.find(key); it != watched_keys.end()) {\n@@ -1388,7 +1408,7 @@ void DbSlice::ClearOffloadedEntries(absl::Span<const DbIndex> indices, const DbT\n     // Delete all tiered entries\n     PrimeTable::Cursor cursor;\n     do {\n-      cursor = Traverse(&db_ptr->prime, cursor, [&](PrimeIterator it) {\n+      cursor = db_ptr->prime.Traverse(cursor, [&](PrimeIterator it) {\n         if (it->second.IsExternal()) {\n           tiered_storage->Delete(index, &it->second);\n         } else if (it->second.HasStashPending()) {\n@@ -1514,8 +1534,6 @@ void DbSlice::CallChangeCallbacks(DbIndex id, std::string_view key, const Change\n     return;\n \n   DVLOG(2) << \"Running callbacks for key \" << key << \" in dbid \" << id;\n-  FetchedItemsRestorer fetched_restorer(&fetched_items_);\n-  std::unique_lock lk(local_mu_);\n \n   const size_t limit = change_cb_.size();\n   auto ccb = change_cb_.begin();\ndiff --git a/src/server/db_slice.h b/src/server/db_slice.h\nindex 034d76d0dcc4..0c6f84c539b7 100644\n--- a/src/server/db_slice.h\n+++ b/src/server/db_slice.h\n@@ -305,33 +305,34 @@ class DbSlice {\n     AddOrFindResult& operator=(ItAndUpdater&& o);\n   };\n \n-  OpResult<AddOrFindResult> AddOrFind(const Context& cntx, std::string_view key);\n+  OpResult<AddOrFindResult> AddOrFind(const Context& cntx, std::string_view key)\n+      ABSL_LOCKS_EXCLUDED(local_mu_);\n \n   // Same as AddOrSkip, but overwrites in case entry exists.\n   OpResult<AddOrFindResult> AddOrUpdate(const Context& cntx, std::string_view key, PrimeValue obj,\n-                                        uint64_t expire_at_ms);\n+                                        uint64_t expire_at_ms) ABSL_LOCKS_EXCLUDED(local_mu_);\n \n   // Adds a new entry. Requires: key does not exist in this slice.\n   // Returns the iterator to the newly added entry.\n   // Returns OpStatus::OUT_OF_MEMORY if bad_alloc is thrown\n   OpResult<ItAndUpdater> AddNew(const Context& cntx, std::string_view key, PrimeValue obj,\n-                                uint64_t expire_at_ms);\n+                                uint64_t expire_at_ms) ABSL_LOCKS_EXCLUDED(local_mu_);\n \n   // Update entry expiration. Return epxiration timepoint in abs milliseconds, or -1 if the entry\n   // already expired and was deleted;\n   facade::OpResult<int64_t> UpdateExpire(const Context& cntx, Iterator prime_it, ExpIterator exp_it,\n-                                         const ExpireParams& params);\n+                                         const ExpireParams& params) ABSL_LOCKS_EXCLUDED(local_mu_);\n \n   // Adds expiry information.\n-  void AddExpire(DbIndex db_ind, Iterator main_it, uint64_t at);\n+  void AddExpire(DbIndex db_ind, Iterator main_it, uint64_t at) ABSL_LOCKS_EXCLUDED(local_mu_);\n \n   // Removes the corresponing expiry information if exists.\n   // Returns true if expiry existed (and removed).\n-  bool RemoveExpire(DbIndex db_ind, Iterator main_it);\n+  bool RemoveExpire(DbIndex db_ind, Iterator main_it) ABSL_LOCKS_EXCLUDED(local_mu_);\n \n   // Either adds or removes (if at == 0) expiry. Returns true if a change was made.\n   // Does not change expiry if at != 0 and expiry already exists.\n-  bool UpdateExpire(DbIndex db_ind, Iterator main_it, uint64_t at);\n+  bool UpdateExpire(DbIndex db_ind, Iterator main_it, uint64_t at) ABSL_LOCKS_EXCLUDED(local_mu_);\n \n   void SetMCFlag(DbIndex db_ind, PrimeKey key, uint32_t flag);\n   uint32_t GetMCFlag(DbIndex db_ind, const PrimeKey& key) const;\n@@ -339,12 +340,15 @@ class DbSlice {\n   // Creates a database with index `db_ind`. If such database exists does nothing.\n   void ActivateDb(DbIndex db_ind);\n \n-  bool Del(Context cntx, Iterator it);\n+  // Delete a key referred by its iterator.\n+  void PerformDeletion(Iterator del_it, DbTable* table);\n+\n+  bool Del(Context cntx, Iterator it) ABSL_LOCKS_EXCLUDED(local_mu_);\n \n   constexpr static DbIndex kDbAll = 0xFFFF;\n \n   // Flushes db_ind or all databases if kDbAll is passed\n-  void FlushDb(DbIndex db_ind);\n+  void FlushDb(DbIndex db_ind) ABSL_LOCKS_EXCLUDED(local_mu_);\n \n   // Flushes the data of given slot ranges.\n   void FlushSlots(cluster::SlotRanges slot_ranges);\n@@ -435,7 +439,7 @@ class DbSlice {\n   void FlushChangeToEarlierCallbacks(DbIndex db_ind, Iterator it, uint64_t upper_bound);\n \n   //! Unregisters the callback.\n-  void UnregisterOnChange(uint64_t id);\n+  void UnregisterOnChange(uint64_t id) ABSL_LOCKS_EXCLUDED(local_mu_);\n \n   struct DeleteExpiredStats {\n     uint32_t deleted = 0;         // number of deleted items due to expiry (less than traversed).\n@@ -451,7 +455,6 @@ class DbSlice {\n   // Returnes number of (elements,bytes) freed due to evictions.\n   std::pair<uint64_t, size_t> FreeMemWithEvictionStep(DbIndex db_indx, size_t starting_segment_id,\n                                                       size_t increase_goal_bytes);\n-  void ScheduleForOffloadStep(DbIndex db_indx, size_t increase_goal_bytes);\n \n   int32_t GetNextSegmentForEviction(int32_t segment_id, DbIndex db_ind) const;\n \n@@ -493,20 +496,17 @@ class DbSlice {\n     client_tracking_map_[key].insert(conn_ref);\n   }\n \n-  // Delete a key referred by its iterator.\n-  void PerformDeletion(Iterator del_it, DbTable* table);\n-  void PerformDeletion(PrimeIterator del_it, DbTable* table);\n-\n   // Provides access to the internal lock of db_slice for flows that serialize\n   // entries with preemption and need to synchronize with Traverse below which\n   // acquires the same lock.\n-  ThreadLocalMutex* GetSerializationMutex() {\n-    return &local_mu_;\n+  ThreadLocalMutex& GetSerializationMutex() {\n+    return local_mu_;\n   }\n \n   // Wrapper around DashTable::Traverse that allows preemptions\n   template <typename Cb, typename DashTable>\n-  PrimeTable::Cursor Traverse(DashTable* pt, PrimeTable::Cursor cursor, Cb&& cb) {\n+  PrimeTable::Cursor Traverse(DashTable* pt, PrimeTable::Cursor cursor, Cb&& cb)\n+      ABSL_LOCKS_EXCLUDED(local_mu_) {\n     std::unique_lock lk(local_mu_);\n     return pt->Traverse(cursor, std::forward<Cb>(cb));\n   }\n@@ -532,6 +532,7 @@ class DbSlice {\n   void ClearOffloadedEntries(absl::Span<const DbIndex> indices, const DbTableArray& db_arr);\n \n   void PerformDeletion(Iterator del_it, ExpIterator exp_it, DbTable* table);\n+  void PerformDeletion(PrimeIterator del_it, DbTable* table);\n \n   // Send invalidation message to the clients that are tracking the change to a key.\n   void SendInvalidationTrackingMessage(std::string_view key);\n@@ -562,7 +563,8 @@ class DbSlice {\n     return version_++;\n   }\n \n-  void CallChangeCallbacks(DbIndex id, std::string_view key, const ChangeReq& cr) const;\n+  void CallChangeCallbacks(DbIndex id, std::string_view key, const ChangeReq& cr) const\n+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(local_mu_);\n \n   // Used to provide exclusive access while Traversing segments\n   mutable ThreadLocalMutex local_mu_;\ndiff --git a/src/server/engine_shard_set.cc b/src/server/engine_shard_set.cc\nindex 9512c80bb824..45496ed661c2 100644\n--- a/src/server/engine_shard_set.cc\n+++ b/src/server/engine_shard_set.cc\n@@ -607,6 +607,9 @@ void EngineShard::Heartbeat() {\n     RetireExpiredAndEvict();\n   }\n \n+  // TODO: iterate over all namespaces\n+  DbSlice& db_slice = namespaces.GetDefaultNamespace().GetDbSlice(shard_id());\n+\n   // Offset CoolMemoryUsage when consider background offloading.\n   // TODO: Another approach could be is to align the approach  similarly to how we do with\n   // FreeMemWithEvictionStep, i.e. if memory_budget is below the limit.\n@@ -621,7 +624,6 @@ void EngineShard::Heartbeat() {\n             << \" tiering_threshold: \" << tiering_offload_threshold\n             << \", cool memory: \" << tiered_storage_->CoolMemoryUsage();\n \n-    DbSlice& db_slice = namespaces.GetDefaultNamespace().GetDbSlice(shard_id());\n     for (unsigned i = 0; i < db_slice.db_array_size(); ++i) {\n       if (!db_slice.IsDbValid(i))\n         continue;\n@@ -631,6 +633,10 @@ void EngineShard::Heartbeat() {\n }\n \n void EngineShard::RetireExpiredAndEvict() {\n+  // TODO: iterate over all namespaces\n+  DbSlice& db_slice = namespaces.GetDefaultNamespace().GetDbSlice(shard_id());\n+  // Some of the functions below might acquire the lock again so we need to unlock it\n+  std::unique_lock lk(db_slice.GetSerializationMutex());\n   constexpr double kTtlDeleteLimit = 200;\n   constexpr double kRedLimitFactor = 0.1;\n \n@@ -651,8 +657,6 @@ void EngineShard::RetireExpiredAndEvict() {\n   DbContext db_cntx;\n   db_cntx.time_now_ms = GetCurrentTimeMs();\n \n-  // TODO: iterate over all namespaces\n-  DbSlice& db_slice = namespaces.GetDefaultNamespace().GetDbSlice(shard_id());\n   for (unsigned i = 0; i < db_slice.db_array_size(); ++i) {\n     if (!db_slice.IsDbValid(i))\n       continue;\n@@ -674,6 +678,8 @@ void EngineShard::RetireExpiredAndEvict() {\n     }\n   }\n \n+  // Because TriggerOnJournalWriteToSink will lock the same lock leading to a deadlock.\n+  lk.unlock();\n   // Journal entries for expired entries are not writen to socket in the loop above.\n   // Trigger write to socket when loop finishes.\n   if (auto journal = EngineShard::tlocal()->journal(); journal) {\ndiff --git a/src/server/snapshot.cc b/src/server/snapshot.cc\nindex d78b331de3c4..feb1fded2ab1 100644\n--- a/src/server/snapshot.cc\n+++ b/src/server/snapshot.cc\n@@ -389,7 +389,7 @@ void SliceSnapshot::OnJournalEntry(const journal::JournalItem& item, bool await)\n   // To enable journal flushing to sync after non auto journal command is executed we call\n   // TriggerJournalWriteToSink. This call uses the NOOP opcode with await=true. Since there is no\n   // additional journal change to serialize, it simply invokes PushSerializedToChannel.\n-  std::unique_lock lk(*db_slice_->GetSerializationMutex());\n+  std::unique_lock lk(db_slice_->GetSerializationMutex());\n   if (item.opcode != journal::Op::NOOP) {\n     serializer_->WriteJournalEntry(item.data);\n   }\n@@ -402,7 +402,7 @@ void SliceSnapshot::OnJournalEntry(const journal::JournalItem& item, bool await)\n }\n \n void SliceSnapshot::CloseRecordChannel() {\n-  std::unique_lock lk(*db_slice_->GetSerializationMutex());\n+  std::unique_lock lk(db_slice_->GetSerializationMutex());\n \n   CHECK(!serialize_bucket_running_);\n   // Make sure we close the channel only once with a CAS check.\n",
  "test_patch": "diff --git a/tests/dragonfly/instance.py b/tests/dragonfly/instance.py\nindex 8aaa5a37130c..a92534b6ab6d 100644\n--- a/tests/dragonfly/instance.py\n+++ b/tests/dragonfly/instance.py\n@@ -88,7 +88,7 @@ def __init__(self, params: DflyParams, args):\n                 self.args[\"num_shards\"] = threads - 1\n \n         # Add 1 byte limit for big values\n-        # self.args[\"serialization_max_chunk_size\"] = 1\n+        self.args[\"serialization_max_chunk_size\"] = 1\n \n     def __del__(self):\n         assert self.proc == None\ndiff --git a/tests/dragonfly/seeder/__init__.py b/tests/dragonfly/seeder/__init__.py\nindex dbfbf820f860..231afe9edf44 100644\n--- a/tests/dragonfly/seeder/__init__.py\n+++ b/tests/dragonfly/seeder/__init__.py\n@@ -155,7 +155,7 @@ async def run(self, client: aioredis.Redis, target_ops=None, target_deviation=No\n         )\n \n     async def stop(self, client: aioredis.Redis):\n-        \"\"\"Reqeust seeder seeder if it's running without a target, future returned from start() must still be awaited\"\"\"\n+        \"\"\"Request seeder seeder if it's running without a target, future returned from start() must still be awaited\"\"\"\n \n         await asyncio.gather(*(client.set(unit.stop_key, \"X\") for unit in self.units))\n \ndiff --git a/tests/dragonfly/utility.py b/tests/dragonfly/utility.py\nindex b4bf8df038d2..bbc6ba9458a1 100644\n--- a/tests/dragonfly/utility.py\n+++ b/tests/dragonfly/utility.py\n@@ -91,7 +91,7 @@ async def info_tick_timer(client: aioredis.Redis, section=None, **kwargs):\n         yield x\n \n \n-async def wait_available_async(client: aioredis.Redis, timeout=10):\n+async def wait_available_async(client: aioredis.Redis, timeout=120):\n     \"\"\"Block until instance exits loading phase\"\"\"\n     its = 0\n     start = time.time()\n",
  "problem_statement": "`dragonfly/replication_test.py::test_replication_all` fails sporadically\n[Example](https://github.com/dragonflydb/dragonfly/actions/runs/10129412325)\n",
  "hints_text": "@chakaz what is this for ? The failure we see is from the flag I introduced \n@kostasrim which flag?\nI think I am confused. The CI failure you linked, happened on Sunday and the failures that we see are because of `--serialization_max_chunk_size=1` which you reverted the same day. I relater introduced it on Monday with the necessary fixes so what we see above is already resolved no ?\r\n\r\nWe still got a sporadic failure which triggers an assertion and I am working on it so I am also confused why this was assigned to Vlad. \r\n\r\nWhat am I missing?\r\n\r\nThis is when you reverted it https://github.com/dragonflydb/dragonfly/commit/20bda84317a21336bcd7ae2ea52f2ee1bab0ab94 \r\n\r\nAnd this where I reintroduced it: https://github.com/dragonflydb/dragonfly/pull/3406\nSorry, I forgot that you are working on this. I'll un-assign Vlad then.\r\n\r\nWe still see failures here, right? Perhaps I mistakenly linked to one that's related to the serialization that you already fixed.\nyes, there is one corner case in some flow + another flow which I already have a fix but I will push it together with this one",
  "created_at": "2024-08-02T07:25:42Z",
  "modified_files": [
    "src/server/common.h",
    "src/server/db_slice.cc",
    "src/server/db_slice.h",
    "src/server/engine_shard_set.cc",
    "src/server/snapshot.cc"
  ],
  "modified_test_files": [
    "tests/dragonfly/instance.py",
    "tests/dragonfly/seeder/__init__.py",
    "tests/dragonfly/utility.py"
  ]
}