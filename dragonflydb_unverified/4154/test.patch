diff --git a/tests/dragonfly/cluster_test.py b/tests/dragonfly/cluster_test.py
index 66f70856e236..83acd11f71b3 100644
--- a/tests/dragonfly/cluster_test.py
+++ b/tests/dragonfly/cluster_test.py
@@ -13,7 +13,7 @@
 from redis.cluster import RedisCluster
 from redis.cluster import ClusterNode
 from .proxy import Proxy
-from .seeder import SeederBase
+from .seeder import StaticSeeder
 
 from . import dfly_args
 
@@ -1773,6 +1773,45 @@ async def node1size0():
         assert str(i) == await nodes[1].client.get(f"{{key50}}:{i}")
 
 
+@dfly_args({"proactor_threads": 2, "cluster_mode": "yes"})
+@pytest.mark.asyncio
+async def test_cluster_migration_huge_container(df_factory: DflyInstanceFactory):
+    instances = [
+        df_factory.create(port=BASE_PORT + i, admin_port=BASE_PORT + i + 1000) for i in range(2)
+    ]
+    df_factory.start_all(instances)
+
+    nodes = [await create_node_info(instance) for instance in instances]
+    nodes[0].slots = [(0, 16383)]
+    nodes[1].slots = []
+
+    await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])
+
+    logging.debug("Generating huge containers")
+    seeder = StaticSeeder(
+        key_target=10,
+        data_size=10_000_000,
+        collection_size=10_000,
+        variance=1,
+        samples=1,
+        types=["LIST", "HASH", "SET", "ZSET", "STRING"],
+    )
+    await seeder.run(nodes[0].client)
+    source_data = await StaticSeeder.capture(nodes[0].client)
+
+    nodes[0].migrations = [
+        MigrationInfo("127.0.0.1", instances[1].admin_port, [(0, 16383)], nodes[1].id)
+    ]
+    logging.debug("Migrating slots")
+    await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])
+
+    logging.debug("Waiting for migration to finish")
+    await wait_for_status(nodes[0].admin_client, nodes[1].id, "FINISHED")
+
+    target_data = await StaticSeeder.capture(nodes[1].client)
+    assert source_data == target_data
+
+
 def parse_lag(replication_info: str):
     lags = re.findall("lag=([0-9]+)\r
", replication_info)
     assert len(lags) == 1
diff --git a/tests/dragonfly/generic_test.py b/tests/dragonfly/generic_test.py
index 020d364a9c23..c4ceef3d6398 100644
--- a/tests/dragonfly/generic_test.py
+++ b/tests/dragonfly/generic_test.py
@@ -1,4 +1,5 @@
 import os
+import logging
 import pytest
 import redis
 import asyncio
@@ -7,6 +8,7 @@
 from . import dfly_multi_test_args, dfly_args
 from .instance import DflyStartException
 from .utility import batch_fill_data, gen_test_data, EnvironCntx
+from .seeder import StaticSeeder
 
 
 @dfly_multi_test_args({"keys_output_limit": 512}, {"keys_output_limit": 1024})
@@ -168,3 +170,38 @@ async def test_denyoom_commands(df_factory):
 
     # mget should not be rejected
     await client.execute_command("mget x")
+
+
+@pytest.mark.parametrize("type", ["LIST", "HASH", "SET", "ZSET", "STRING"])
+@dfly_args({"proactor_threads": 4})
+@pytest.mark.asyncio
+async def test_rename_huge_values(df_factory, type):
+    df_server = df_factory.create()
+    df_server.start()
+    client = df_server.client()
+
+    logging.debug(f"Generating huge {type}")
+    seeder = StaticSeeder(
+        key_target=1,
+        data_size=10_000_000,
+        collection_size=10_000,
+        variance=1,
+        samples=1,
+        types=[type],
+    )
+    await seeder.run(client)
+    source_data = await StaticSeeder.capture(client)
+    logging.debug(f"src {source_data}")
+
+    # Rename multiple times to make sure the key moves between shards
+    orig_name = (await client.execute_command("keys *"))[0]
+    old_name = orig_name
+    new_name = ""
+    for i in range(10):
+        new_name = f"new:{i}"
+        await client.execute_command(f"rename {old_name} {new_name}")
+        old_name = new_name
+    await client.execute_command(f"rename {new_name} {orig_name}")
+    target_data = await StaticSeeder.capture(client)
+
+    assert source_data == target_data
