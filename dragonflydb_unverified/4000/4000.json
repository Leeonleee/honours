{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4000,
  "instance_id": "dragonflydb__dragonfly-4000",
  "issue_numbers": [
    "3999"
  ],
  "base_commit": "851e43211e29220bc6f08c9828a28a17c1977091",
  "patch": "diff --git a/src/facade/dragonfly_connection.cc b/src/facade/dragonfly_connection.cc\nindex e4fd48a46e4e..1cdab6cb2873 100644\n--- a/src/facade/dragonfly_connection.cc\n+++ b/src/facade/dragonfly_connection.cc\n@@ -52,8 +52,18 @@ ABSL_FLAG(string, admin_bind, \"\",\n ABSL_FLAG(uint64_t, request_cache_limit, 64_MB,\n           \"Amount of memory to use for request cache in bytes - per IO thread.\");\n \n-ABSL_FLAG(uint64_t, pipeline_buffer_limit, 8_MB,\n-          \"Amount of memory to use for parsing pipeline requests - per IO thread.\");\n+ABSL_FLAG(uint64_t, pipeline_buffer_limit, 128_MB,\n+          \"Amount of memory to use for storing pipeline requests - per IO thread.\"\n+          \"Please note that clients that send excecissively huge pipelines, \"\n+          \"may deadlock themselves. See https://github.com/dragonflydb/dragonfly/discussions/3997\"\n+          \"for details.\");\n+\n+ABSL_FLAG(uint32_t, pipeline_queue_limit, 10000,\n+          \"Pipeline queue max length, the server will stop reading from the client socket\"\n+          \" once its pipeline queue crosses this limit, and will resume once it processes \"\n+          \"excessive requests. This is to prevent OOM states. Users of huge pipelines sizes \"\n+          \"may require increasing this limit to prevent the risk of deadlocking.\"\n+          \"See https://github.com/dragonflydb/dragonfly/discussions/3997 for details\");\n \n ABSL_FLAG(uint64_t, publish_buffer_limit, 128_MB,\n           \"Amount of memory to use for storing pub commands in bytes - per IO thread\");\n@@ -63,10 +73,6 @@ ABSL_FLAG(bool, no_tls_on_admin_port, false, \"Allow non-tls connections on admin\n ABSL_FLAG(uint32_t, pipeline_squash, 10,\n           \"Number of queued pipelined commands above which squashing is enabled, 0 means disabled\");\n \n-ABSL_FLAG(uint32_t, pipeline_queue_limit, 1000,\n-          \"Pipeline queue max length, the server will stop reading from the client socket\"\n-          \" once the pipeline reaches this limit\");\n-\n // When changing this constant, also update `test_large_cmd` test in connection_test.py.\n ABSL_FLAG(uint32_t, max_multi_bulk_len, 1u << 16,\n           \"Maximum multi-bulk (array) length that is \"\n@@ -1020,6 +1026,10 @@ void Connection::DispatchSingle(bool has_more, absl::FunctionRef<void()> invoke_\n \n   if (optimize_for_async && queue_backpressure_->IsPipelineBufferOverLimit(\n                                 stats_->dispatch_queue_bytes, dispatch_q_.size())) {\n+    stats_->pipeline_throttle_count++;\n+    LOG_EVERY_T(WARNING, 10) << \"Pipeline buffer over limit: pipeline_bytes \"\n+                             << stats_->dispatch_queue_bytes << \" queue_size \" << dispatch_q_.size()\n+                             << \", consider increasing pipeline_buffer_limit/pipeline_queue_limit\";\n     fb2::NoOpLock noop;\n     queue_backpressure_->pipeline_cnd.wait(noop, [this] {\n       bool over_limits = queue_backpressure_->IsPipelineBufferOverLimit(\n@@ -1826,6 +1836,12 @@ void Connection::BreakOnce(uint32_t ev_mask) {\n \n void Connection::SetMaxQueueLenThreadLocal(uint32_t val) {\n   tl_queue_backpressure_.pipeline_queue_max_len = val;\n+  tl_queue_backpressure_.pipeline_cnd.notify_all();\n+}\n+\n+void Connection::SetPipelineBufferLimit(size_t val) {\n+  tl_queue_backpressure_.pipeline_buffer_limit = val;\n+  tl_queue_backpressure_.pipeline_cnd.notify_all();\n }\n \n void Connection::GetRequestSizeHistogramThreadLocal(std::string* hist) {\ndiff --git a/src/facade/dragonfly_connection.h b/src/facade/dragonfly_connection.h\nindex 762581e6c42a..bc1af66c30b2 100644\n--- a/src/facade/dragonfly_connection.h\n+++ b/src/facade/dragonfly_connection.h\n@@ -312,6 +312,7 @@ class Connection : public util::Connection {\n \n   // Sets max queue length locally in the calling thread.\n   static void SetMaxQueueLenThreadLocal(uint32_t val);\n+  static void SetPipelineBufferLimit(size_t val);\n   static void GetRequestSizeHistogramThreadLocal(std::string* hist);\n   static void TrackRequestSize(bool enable);\n \ndiff --git a/src/facade/facade.cc b/src/facade/facade.cc\nindex 698a690d6445..52a6d7cea5e9 100644\n--- a/src/facade/facade.cc\n+++ b/src/facade/facade.cc\n@@ -20,7 +20,7 @@ constexpr size_t kSizeConnStats = sizeof(ConnectionStats);\n \n ConnectionStats& ConnectionStats::operator+=(const ConnectionStats& o) {\n   // To break this code deliberately if we add/remove a field to this struct.\n-  static_assert(kSizeConnStats == 112u);\n+  static_assert(kSizeConnStats == 120u);\n \n   ADD(read_buf_capacity);\n   ADD(dispatch_queue_entries);\n@@ -37,6 +37,7 @@ ConnectionStats& ConnectionStats::operator+=(const ConnectionStats& o) {\n   ADD(num_replicas);\n   ADD(num_blocked_clients);\n   ADD(num_migrations);\n+  ADD(pipeline_throttle_count);\n \n   return *this;\n }\ndiff --git a/src/facade/facade_types.h b/src/facade/facade_types.h\nindex acf8c166f296..1dbcc9087161 100644\n--- a/src/facade/facade_types.h\n+++ b/src/facade/facade_types.h\n@@ -109,6 +109,9 @@ struct ConnectionStats {\n   uint32_t num_blocked_clients = 0;\n   uint64_t num_migrations = 0;\n \n+  // Number of events when the pipeline queue was over the limit and was throttled.\n+  uint64_t pipeline_throttle_count = 0;\n+\n   ConnectionStats& operator+=(const ConnectionStats& o);\n };\n \ndiff --git a/src/facade/reply_builder.cc b/src/facade/reply_builder.cc\nindex fea16d66b97d..68186af5afb4 100644\n--- a/src/facade/reply_builder.cc\n+++ b/src/facade/reply_builder.cc\n@@ -126,7 +126,7 @@ void SinkReplyBuilder::Send(const iovec* v, uint32_t len) {\n   send_active_ = true;\n   tl_facade_stats->reply_stats.io_write_cnt++;\n   tl_facade_stats->reply_stats.io_write_bytes += bsize;\n-  DVLOG(2) << \"Writing \" << bsize << \" bytes of len \" << len;\n+  DVLOG(2) << \"Writing \" << bsize + batch_.size() << \" bytes of len \" << len;\n \n   if (batch_.empty()) {\n     ec = sink_->Write(v, len);\ndiff --git a/src/server/acl/acl_family.cc b/src/server/acl/acl_family.cc\nindex 06e47ece82bc..fe1281fb59f2 100644\n--- a/src/server/acl/acl_family.cc\n+++ b/src/server/acl/acl_family.cc\n@@ -706,16 +706,6 @@ void AclFamily::Init(facade::Listener* main_listener, UserRegistry* registry) {\n     return;\n   }\n   registry_->Init(&CategoryToIdx(), &reverse_cat_table_, &CategoryToCommandsIndex());\n-  config_registry.RegisterMutable(\"aclfile\");\n-  config_registry.RegisterMutable(\"acllog_max_len\", [this](const absl::CommandLineFlag& flag) {\n-    auto res = flag.TryGet<size_t>();\n-    if (res.has_value()) {\n-      pool_->AwaitFiberOnAll([&res](auto index, auto* context) {\n-        ServerState::tlocal()->acl_log.SetTotalEntries(res.value());\n-      });\n-    }\n-    return res.has_value();\n-  });\n }\n \n std::string AclFamily::AclCatToString(uint32_t acl_category, User::Sign sign) const {\ndiff --git a/src/server/acl/acl_log.cc b/src/server/acl/acl_log.cc\nindex 7ab4ca75effe..05641cb640df 100644\n--- a/src/server/acl/acl_log.cc\n+++ b/src/server/acl/acl_log.cc\n@@ -12,7 +12,7 @@\n #include \"facade/dragonfly_connection.h\"\n #include \"server/conn_context.h\"\n \n-ABSL_FLAG(size_t, acllog_max_len, 32,\n+ABSL_FLAG(uint32_t, acllog_max_len, 32,\n           \"Specify the number of log entries. Logs are kept locally for each thread \"\n           \"and therefore the total number of entries are acllog_max_len * threads\");\n \ndiff --git a/src/server/config_registry.h b/src/server/config_registry.h\nindex eccd3c987de8..12c01b94bbf2 100644\n--- a/src/server/config_registry.h\n+++ b/src/server/config_registry.h\n@@ -26,6 +26,18 @@ class ConfigRegistry {\n     return *this;\n   }\n \n+  template <typename T>\n+  ConfigRegistry& RegisterSetter(std::string_view name, std::function<void(const T&)> f) {\n+    return RegisterMutable(name, [f](const absl::CommandLineFlag& flag) {\n+      auto res = flag.TryGet<T>();\n+      if (res.has_value()) {\n+        f(*res);\n+        return true;\n+      }\n+      return false;\n+    });\n+  }\n+\n   enum class SetResult : uint8_t {\n     OK,\n     UNKNOWN,\ndiff --git a/src/server/main_service.cc b/src/server/main_service.cc\nindex fdb0ede99f54..69ec162c52a1 100644\n--- a/src/server/main_service.cc\n+++ b/src/server/main_service.cc\n@@ -883,14 +883,8 @@ Service::~Service() {\n void Service::Init(util::AcceptServer* acceptor, std::vector<facade::Listener*> listeners) {\n   InitRedisTables();\n \n-  config_registry.RegisterMutable(\"maxmemory\", [](const absl::CommandLineFlag& flag) {\n-    auto res = flag.TryGet<MemoryBytesFlag>();\n-    if (!res)\n-      return false;\n-\n-    max_memory_limit = res->value;\n-    return true;\n-  });\n+  config_registry.RegisterSetter<MemoryBytesFlag>(\n+      \"maxmemory\", [](const MemoryBytesFlag& flag) { max_memory_limit = flag.value; });\n \n   config_registry.RegisterMutable(\"dbfilename\");\n   config_registry.Register(\"dbnum\");  // equivalent to databases in redis.\n@@ -901,32 +895,24 @@ void Service::Init(util::AcceptServer* acceptor, std::vector<facade::Listener*>\n   config_registry.RegisterMutable(\"max_eviction_per_heartbeat\");\n   config_registry.RegisterMutable(\"max_segment_to_consider\");\n \n-  config_registry.RegisterMutable(\"oom_deny_ratio\", [](const absl::CommandLineFlag& flag) {\n-    auto res = flag.TryGet<double>();\n-    if (res.has_value()) {\n-      SetOomDenyRatioOnAllThreads(*res);\n-    }\n-    return res.has_value();\n+  config_registry.RegisterSetter<double>(\"oom_deny_ratio\",\n+                                         [](double val) { SetOomDenyRatioOnAllThreads(val); });\n+\n+  config_registry.RegisterSetter<double>(\"rss_oom_deny_ratio\",\n+                                         [](double val) { SetRssOomDenyRatioOnAllThreads(val); });\n+\n+  config_registry.RegisterMutable(\"pipeline_squash\");\n+\n+  config_registry.RegisterSetter<uint32_t>(\"pipeline_queue_limit\", [](uint32_t val) {\n+    shard_set->pool()->AwaitBrief(\n+        [val](unsigned, auto*) { facade::Connection::SetMaxQueueLenThreadLocal(val); });\n   });\n \n-  config_registry.RegisterMutable(\"rss_oom_deny_ratio\", [](const absl::CommandLineFlag& flag) {\n-    auto res = flag.TryGet<double>();\n-    if (res.has_value()) {\n-      SetRssOomDenyRatioOnAllThreads(*res);\n-    }\n-    return res.has_value();\n+  config_registry.RegisterSetter<size_t>(\"pipeline_buffer_limit\", [](size_t val) {\n+    shard_set->pool()->AwaitBrief(\n+        [val](unsigned, auto*) { facade::Connection::SetPipelineBufferLimit(val); });\n   });\n-  config_registry.RegisterMutable(\"pipeline_squash\");\n-  config_registry.RegisterMutable(\"pipeline_queue_limit\",\n-                                  [pool = &pp_](const absl::CommandLineFlag& flag) {\n-                                    auto res = flag.TryGet<uint32_t>();\n-                                    if (res.has_value()) {\n-                                      pool->AwaitBrief([val = *res](unsigned, auto*) {\n-                                        facade::Connection::SetMaxQueueLenThreadLocal(val);\n-                                      });\n-                                    }\n-                                    return res.has_value();\n-                                  });\n+\n   config_registry.RegisterMutable(\"replica_partial_sync\");\n   config_registry.RegisterMutable(\"replication_timeout\");\n   config_registry.RegisterMutable(\"table_growth_margin\");\n@@ -951,6 +937,12 @@ void Service::Init(util::AcceptServer* acceptor, std::vector<facade::Listener*>\n         return true;\n       });\n \n+  config_registry.RegisterMutable(\"aclfile\");\n+  config_registry.RegisterSetter<uint32_t>(\"acllog_max_len\", [](uint32_t val) {\n+    shard_set->pool()->AwaitFiberOnAll(\n+        [val](auto index, auto* context) { ServerState::tlocal()->acl_log.SetTotalEntries(val); });\n+  });\n+\n   serialization_max_chunk_size = GetFlag(FLAGS_serialization_max_chunk_size);\n   uint32_t shard_num = GetFlag(FLAGS_num_shards);\n   if (shard_num == 0 || shard_num > pp_.size()) {\ndiff --git a/src/server/server_family.cc b/src/server/server_family.cc\nindex f39503cdc961..76b6e0a2dd22 100644\n--- a/src/server/server_family.cc\n+++ b/src/server/server_family.cc\n@@ -816,12 +816,8 @@ void ServerFamily::Init(util::AcceptServer* acceptor, std::vector<facade::Listen\n   LOG_FIRST_N(INFO, 1) << \"Host OS: \" << os_string << \" with \" << shard_set->pool()->size()\n                        << \" threads\";\n   SetMaxClients(listeners_, absl::GetFlag(FLAGS_maxclients));\n-  config_registry.RegisterMutable(\"maxclients\", [this](const absl::CommandLineFlag& flag) {\n-    auto res = flag.TryGet<uint32_t>();\n-    if (res.has_value())\n-      SetMaxClients(listeners_, res.value());\n-    return res.has_value();\n-  });\n+  config_registry.RegisterSetter<uint32_t>(\n+      \"maxclients\", [this](uint32_t val) { SetMaxClients(listeners_, val); });\n \n   SetSlowLogThreshold(service_.proactor_pool(), absl::GetFlag(FLAGS_slowlog_log_slower_than));\n   config_registry.RegisterMutable(\"slowlog_log_slower_than\",\n@@ -832,12 +828,8 @@ void ServerFamily::Init(util::AcceptServer* acceptor, std::vector<facade::Listen\n                                     return res.has_value();\n                                   });\n   SetSlowLogMaxLen(service_.proactor_pool(), absl::GetFlag(FLAGS_slowlog_max_len));\n-  config_registry.RegisterMutable(\"slowlog_max_len\", [this](const absl::CommandLineFlag& flag) {\n-    auto res = flag.TryGet<uint32_t>();\n-    if (res.has_value())\n-      SetSlowLogMaxLen(service_.proactor_pool(), res.value());\n-    return res.has_value();\n-  });\n+  config_registry.RegisterSetter<uint32_t>(\n+      \"slowlog_max_len\", [this](uint32_t val) { SetSlowLogMaxLen(service_.proactor_pool(), val); });\n \n   // We only reconfigure TLS when the 'tls' config key changes. Therefore to\n   // update TLS certs, first update tls_cert_file, then set 'tls true'.\n@@ -1280,6 +1272,8 @@ void PrintPrometheusMetrics(const Metrics& m, DflyCmd* dfly_cmd, StringResponse*\n                             MetricType::GAUGE, &resp->body());\n   AppendMetricWithoutLabels(\"pipeline_queue_length\", \"\", conn_stats.dispatch_queue_entries,\n                             MetricType::GAUGE, &resp->body());\n+  AppendMetricWithoutLabels(\"pipeline_throttle_total\", \"\", conn_stats.pipeline_throttle_count,\n+                            MetricType::COUNTER, &resp->body());\n   AppendMetricWithoutLabels(\"pipeline_cmd_cache_bytes\", \"\", conn_stats.pipeline_cmd_cache_bytes,\n                             MetricType::GAUGE, &resp->body());\n   AppendMetricWithoutLabels(\"pipeline_commands_total\", \"\", conn_stats.pipelined_cmd_cnt,\n@@ -2298,6 +2292,7 @@ void ServerFamily::Info(CmdArgList args, ConnectionContext* cntx) {\n     append(\"instantaneous_ops_per_sec\", m.qps);\n     append(\"total_pipelined_commands\", conn_stats.pipelined_cmd_cnt);\n     append(\"total_pipelined_squashed_commands\", m.coordinator_stats.squashed_commands);\n+    append(\"pipeline_throttle_total\", conn_stats.pipeline_throttle_count);\n     append(\"pipelined_latency_usec\", conn_stats.pipelined_cmd_latency);\n     append(\"total_net_input_bytes\", conn_stats.io_read_bytes);\n     append(\"connection_migrations\", conn_stats.num_migrations);\n@@ -2327,9 +2322,13 @@ void ServerFamily::Info(CmdArgList args, ConnectionContext* cntx) {\n     append(\"defrag_task_invocation_total\", m.shard_stats.defrag_task_invocation_total);\n     append(\"reply_count\", reply_stats.send_stats.count);\n     append(\"reply_latency_usec\", reply_stats.send_stats.total_duration);\n+\n+    // Number of connections that are currently blocked on grabbing interpreter.\n     append(\"blocked_on_interpreter\", m.coordinator_stats.blocked_on_interpreter);\n     append(\"lua_interpreter_cnt\", m.lua_stats.interpreter_cnt);\n-    append(\"lua_blocked\", m.lua_stats.blocked_cnt);\n+\n+    // Total number of events of when a connection was blocked on grabbing interpreter.\n+    append(\"lua_blocked_total\", m.lua_stats.blocked_cnt);\n   }\n \n   if (should_enter(\"TIERED\", true)) {\n",
  "test_patch": "diff --git a/src/server/test_utils.cc b/src/server/test_utils.cc\nindex b24b4f7ef2c0..c3a02285bb90 100644\n--- a/src/server/test_utils.cc\n+++ b/src/server/test_utils.cc\n@@ -31,7 +31,7 @@ ABSL_DECLARE_FLAG(string, dbfilename);\n ABSL_DECLARE_FLAG(double, rss_oom_deny_ratio);\n ABSL_DECLARE_FLAG(uint32_t, num_shards);\n ABSL_FLAG(bool, force_epoll, false, \"If true, uses epoll api instead iouring to run tests\");\n-ABSL_DECLARE_FLAG(size_t, acllog_max_len);\n+ABSL_DECLARE_FLAG(uint32_t, acllog_max_len);\n namespace dfly {\n \n std::ostream& operator<<(std::ostream& os, const DbStats& stats) {\n",
  "problem_statement": "Document `pipeline_buffer_limit`/`pipeline_queue_limit` and, optionally, add log or raise an error when a pipeline hits the limit\n**Is your feature request related to a problem? Please describe.**\r\n\r\nThis is a follow-up for https://github.com/dragonflydb/dragonfly/discussions/3997\r\n\r\nCurrently, it can be tricky for a user to see why executing large pipelines stopped working after `v1.20.0`.\r\n\r\n**Describe the solution you'd like**\r\n\r\n- [ ] Mention `pipeline_queue_limit` and `pipeline_buffer_limit` on the https://www.dragonflydb.io/docs/managing-dragonfly/known-limitations webpage.\r\n\r\n\r\n- [ ] Optionally, one of the following:\r\n   - Return an error to the client.\r\n   - Write ERROR or WARNING logs.\r\n\r\n\r\n**Describe alternatives you've considered**\r\n\r\nA user can limit the pipeline size, but a user will not know about this problem until their code hangs forever, and then they find this issue. \n",
  "hints_text": "",
  "created_at": "2024-10-26T16:56:31Z",
  "modified_files": [
    "src/facade/dragonfly_connection.cc",
    "src/facade/dragonfly_connection.h",
    "src/facade/facade.cc",
    "src/facade/facade_types.h",
    "src/facade/reply_builder.cc",
    "src/server/acl/acl_family.cc",
    "src/server/acl/acl_log.cc",
    "src/server/config_registry.h",
    "src/server/main_service.cc",
    "src/server/server_family.cc"
  ],
  "modified_test_files": [
    "src/server/test_utils.cc"
  ]
}