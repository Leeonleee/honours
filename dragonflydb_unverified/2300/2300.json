{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 2300,
  "instance_id": "dragonflydb__dragonfly-2300",
  "issue_numbers": [
    "2276"
  ],
  "base_commit": "6c32c8004dc93dc6b724344422bff51cde475d9d",
  "patch": "diff --git a/src/server/db_slice.cc b/src/server/db_slice.cc\nindex 81e79f4d8cbc..6dae685e4ec3 100644\n--- a/src/server/db_slice.cc\n+++ b/src/server/db_slice.cc\n@@ -166,10 +166,16 @@ class PrimeEvictionPolicy {\n \n class PrimeBumpPolicy {\n  public:\n+  PrimeBumpPolicy(const absl::flat_hash_set<CompactObjectView, PrimeHasher>& bumped_items)\n+      : bumped_items_(bumped_items) {\n+  }\n   // returns true if key can be made less important for eviction (opposite of bump up)\n-  bool CanBumpDown(const CompactObj& key) const {\n-    return !key.IsSticky();\n+  bool CanBumpDown(const CompactObj& obj) const {\n+    return !obj.IsSticky() && !bumped_items_.contains(obj);\n   }\n+\n+ private:\n+  const absl::flat_hash_set<CompactObjectView, PrimeHasher>& bumped_items_;\n };\n \n bool PrimeEvictionPolicy::CanGrow(const PrimeTable& tbl) const {\n@@ -468,12 +474,11 @@ DbSlice::ItAndExp DbSlice::FindInternal(const Context& cntx, std::string_view ke\n           ccb.second(cntx.db_index, bit);\n         }\n       };\n-\n       db.prime.CVCUponBump(change_cb_.back().first, res.it, bump_cb);\n     }\n-\n-    res.it = db.prime.BumpUp(res.it, PrimeBumpPolicy{});\n+    res.it = db.prime.BumpUp(res.it, PrimeBumpPolicy{bumped_items_});\n     ++events_.bumpups;\n+    bumped_items_.insert(res.it->first.AsRef());\n   }\n \n   db.top_keys.Touch(key);\n@@ -625,7 +630,7 @@ bool DbSlice::Del(DbIndex db_ind, PrimeIterator it) {\n     DbContext cntx{db_ind, GetCurrentTimeMs()};\n     doc_del_cb_(key, cntx, it->second);\n   }\n-\n+  bumped_items_.erase(it->first.AsRef());\n   PerformDeletion(it, shard_owner(), db.get());\n   deletion_count_++;\n \n@@ -686,7 +691,7 @@ void DbSlice::FlushDbIndexes(const std::vector<DbIndex>& indexes) {\n       tiered->CancelAllIos(index);\n     }\n   }\n-\n+  CHECK(bumped_items_.empty());\n   auto cb = [this, flush_db_arr = std::move(flush_db_arr)]() mutable {\n     for (auto& db_ptr : flush_db_arr) {\n       if (db_ptr && db_ptr->stats.tiered_entries > 0) {\n@@ -1410,4 +1415,8 @@ void DbSlice::TrackKeys(const facade::Connection::WeakRef& conn, const ArgSlice&\n   }\n }\n \n+void DbSlice::OnCbFinish() {\n+  bumped_items_.clear();\n+}\n+\n }  // namespace dfly\ndiff --git a/src/server/db_slice.h b/src/server/db_slice.h\nindex 73fa93c0ea6d..89a953a0ad8e 100644\n--- a/src/server/db_slice.h\n+++ b/src/server/db_slice.h\n@@ -276,6 +276,8 @@ class DbSlice {\n     return shard_id_;\n   }\n \n+  void OnCbFinish();\n+\n   bool Acquire(IntentLock::Mode m, const KeyLockArgs& lock_args);\n \n   void Release(IntentLock::Mode m, const KeyLockArgs& lock_args);\n@@ -445,6 +447,9 @@ class DbSlice {\n   // ordered from the smallest to largest version.\n   std::vector<std::pair<uint64_t, ChangeCallback>> change_cb_;\n \n+  // Used in temporary computations in Find item and CbFinish\n+  mutable absl::flat_hash_set<CompactObjectView, PrimeHasher> bumped_items_;\n+\n   // Registered by shard indices on when first document index is created.\n   DocDeletionCallback doc_del_cb_;\n \ndiff --git a/src/server/table.h b/src/server/table.h\nindex 34c556b9f8ce..337059ec080d 100644\n--- a/src/server/table.h\n+++ b/src/server/table.h\n@@ -47,6 +47,12 @@ inline bool IsValid(ExpireConstIterator it) {\n   return !it.is_done();\n }\n \n+struct PrimeHasher {\n+  size_t operator()(const PrimeKey& o) const {\n+    return o.HashCode();\n+  }\n+};\n+\n struct SlotStats {\n   uint64_t key_count = 0;\n   uint64_t total_reads = 0;\ndiff --git a/src/server/tiered_storage.cc b/src/server/tiered_storage.cc\nindex 2375321f1fa6..75f5c491a30c 100644\n--- a/src/server/tiered_storage.cc\n+++ b/src/server/tiered_storage.cc\n@@ -100,12 +100,6 @@ static size_t ExternalizeEntry(size_t item_offset, DbTableStats* stats, PrimeVal\n   return item_size;\n }\n \n-struct PrimeHasher {\n-  size_t operator()(const PrimeKey& o) const {\n-    return o.HashCode();\n-  }\n-};\n-\n struct SingleRequest {\n   SingleRequest(size_t blob_len, int64 offset, string key)\n       : blob_len(blob_len), offset(offset), key(std::move(key)) {\n@@ -422,7 +416,7 @@ error_code TieredStorage::ScheduleOffload(DbIndex db_index, PrimeIterator it) {\n   CHECK_LT(bin_record.pending_entries.size(), max_entries);\n \n   VLOG(2) << \"ScheduleOffload:\" << it->first.ToString();\n-  bin_record.pending_entries.insert(it->first);\n+  bin_record.pending_entries.insert(it->first.AsRef());\n   it->second.SetIoPending(true);\n \n   if (bin_record.pending_entries.size() < max_entries)\ndiff --git a/src/server/transaction.cc b/src/server/transaction.cc\nindex 8cd30b0692ba..7e795825738c 100644\n--- a/src/server/transaction.cc\n+++ b/src/server/transaction.cc\n@@ -497,6 +497,7 @@ bool Transaction::RunInShard(EngineShard* shard, bool txq_ooo) {\n   if (is_concluding)  // Check last hop\n     LogAutoJournalOnShard(shard);\n \n+  shard->db_slice().OnCbFinish();\n   // at least the coordinator thread owns the reference.\n   DCHECK_GE(GetUseCount(), 1u);\n \n@@ -950,7 +951,7 @@ void Transaction::RunQuickie(EngineShard* shard) {\n   } catch (std::exception& e) {\n     LOG(FATAL) << \"Unexpected exception \" << e.what();\n   }\n-\n+  shard->db_slice().OnCbFinish();\n   LogAutoJournalOnShard(shard);\n \n   sd.is_armed.store(false, memory_order_relaxed);\n@@ -1238,6 +1239,7 @@ OpStatus Transaction::RunSquashedMultiCb(RunnableType cb) {\n   DCHECK_EQ(unique_shard_cnt_, 1u);\n   auto* shard = EngineShard::tlocal();\n   auto status = cb(this, shard);\n+  shard->db_slice().OnCbFinish();\n   LogAutoJournalOnShard(shard);\n   return status;\n }\n",
  "test_patch": "diff --git a/src/server/string_family_test.cc b/src/server/string_family_test.cc\nindex 50d447dac2e3..32a0dde86d5a 100644\n--- a/src/server/string_family_test.cc\n+++ b/src/server/string_family_test.cc\n@@ -228,6 +228,48 @@ TEST_F(StringFamilyTest, MGetSet) {\n   set_fb.Join();\n }\n \n+TEST_F(StringFamilyTest, MGetCachingModeBug2276) {\n+  absl::FlagSaver fs;\n+  SetTestFlag(\"cache_mode\", \"true\");\n+  ResetService();\n+  Run({\"debug\", \"populate\", \"100000\", \"key\", \"32\", \"RAND\"});\n+\n+  // Scan starts traversing the database, because we populated the database with lots of items we\n+  // assume that scan will return items from the same bucket that reside next to each other.\n+  auto resp = Run({\"scan\", \"0\"});\n+  ASSERT_THAT(resp, ArrLen(2));\n+  StringVec vec = StrArray(resp.GetVec()[1]);\n+  ASSERT_GE(vec.size(), 10);\n+\n+  auto get_bump_ups = [](const string& str) -> size_t {\n+    const string matcher = \"bump_ups:\";\n+    const auto pos = str.find(matcher) + matcher.size();\n+    const auto sub = str.substr(pos, 1);\n+    return atoi(sub.c_str());\n+  };\n+\n+  resp = Run({\"info\", \"stats\"});\n+  EXPECT_EQ(get_bump_ups(resp.GetString()), 0);\n+\n+  auto mget_resp = StrArray(Run(\n+      {\"mget\", vec[0], vec[1], vec[2], vec[3], vec[4], vec[5], vec[6], vec[7], vec[8], vec[9]}));\n+\n+  resp = Run({\"info\", \"stats\"});\n+  size_t bumps1 = get_bump_ups(resp.GetString());\n+  EXPECT_GT(bumps1, 0);\n+  EXPECT_LT(bumps1, 10);  // we assume that some bumps are blocked because items reside next to each\n+                          // other in the slot.\n+\n+  for (int i = 0; i < 10; ++i) {\n+    auto get_resp = Run({\"get\", vec[i]});\n+    EXPECT_EQ(get_resp, mget_resp[i]);\n+  }\n+\n+  resp = Run({\"info\", \"stats\"});\n+  size_t bumps2 = get_bump_ups(resp.GetString());\n+  EXPECT_GT(bumps2, bumps1);\n+}\n+\n TEST_F(StringFamilyTest, MSetGet) {\n   Run({\"mset\", \"x\", \"0\", \"y\", \"0\", \"a\", \"0\", \"b\", \"0\"});\n   ASSERT_EQ(2, GetDebugInfo().shards_count);\n",
  "problem_statement": "Bug UB on access invalid prime table iterator\nThis bug can happen is a very rare case when source and dest key of command belong to the same bucket of dash table.\r\nOn commands that have a source and dest key ,we call dash table Find for both source and dest key. On Find we do bumpup therefore we can invalidate the iterator to the one we called Find first if both source and dest keys are on the same bucket.\n",
  "hints_text": "oh wow! Did you encounter this? Since both are bumped up, i guess it can only happen if they reside next to each other. \r\n`|s|d|` . In fact they can even reside in a different buckets. For example, `s` in its home bucket, slot 13 (last one). \r\n`d` is in the stack bucket, slot 0. `d` searched first, it has been promoted to home bucket, slot 13. `s` was demoted to stash bucket. `s` now is searched as well. it's been promoted back to home bucket, demoting `d` and invalidating `it_d`.\r\n\r\nAlso, the case can apply to any multi-key operation that finds multiple keys before processing them. For example,  `OpMGet` first finds all the keys in the shard and then processes them.\nMaybe it's a hammer looking for a nail, but will adding some checks during the destruction of `AutoUpdater` help?\r\n\r\nhttps://github.com/dragonflydb/dragonfly/pull/2268\n@romange I did not encounter this, but while reviewing the PR of autoupdater and our flow related to tiering I just realised we have a bug here\nthis bug needs to be fixed. please tell me if you are gonna look into it or should I\n@romange I want to suggest fixing this by using the same mechanism Shahar intruduced with AutoUpdater . We will set the items to sticky after calling db.Find() and setting the sticky back to the original value when the AutoUpdater is destructed. Because sticky items does not allow bumpdown this should fix the problem. What do you think about this solution?",
  "created_at": "2023-12-13T12:54:09Z",
  "modified_files": [
    "src/server/db_slice.cc",
    "src/server/db_slice.h",
    "src/server/table.h",
    "src/server/tiered_storage.cc",
    "src/server/transaction.cc"
  ],
  "modified_test_files": [
    "src/server/string_family_test.cc"
  ]
}