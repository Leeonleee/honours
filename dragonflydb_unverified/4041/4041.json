{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4041,
  "instance_id": "dragonflydb__dragonfly-4041",
  "issue_numbers": [
    "4040"
  ],
  "base_commit": "7df8c268d874c1f34d4158be1e41b0e2e6c4601f",
  "patch": "diff --git a/src/server/detail/save_stages_controller.cc b/src/server/detail/save_stages_controller.cc\nindex 8b4c21d73b48..ddfa4eb37fd5 100644\n--- a/src/server/detail/save_stages_controller.cc\n+++ b/src/server/detail/save_stages_controller.cc\n@@ -124,7 +124,11 @@ GenericError RdbSnapshot::Start(SaveMode save_mode, const std::string& path,\n }\n \n error_code RdbSnapshot::SaveBody() {\n-  return saver_->SaveBody(&cntx_, &freq_map_);\n+  return saver_->SaveBody(&cntx_);\n+}\n+\n+error_code RdbSnapshot::WaitSnapshotInShard(EngineShard* shard) {\n+  return saver_->WaitSnapshotInShard(shard);\n }\n \n size_t RdbSnapshot::GetSaveBuffersSize() {\n@@ -132,6 +136,10 @@ size_t RdbSnapshot::GetSaveBuffersSize() {\n   return saver_->GetTotalBuffersSize();\n }\n \n+void RdbSnapshot::FillFreqMap() {\n+  saver_->FillFreqMap(&freq_map_);\n+}\n+\n RdbSaver::SnapshotStats RdbSnapshot::GetCurrentSnapshotProgress() const {\n   CHECK(saver_);\n   return saver_->GetCurrentSnapshotProgress();\n@@ -147,7 +155,7 @@ error_code RdbSnapshot::Close() {\n }\n \n void RdbSnapshot::StartInShard(EngineShard* shard) {\n-  saver_->StartSnapshotInShard(false, cntx_.GetCancellation(), shard);\n+  saver_->StartSnapshotInShard(false, &cntx_, shard);\n   started_shards_.fetch_add(1, memory_order_relaxed);\n }\n \n@@ -176,7 +184,12 @@ std::optional<SaveInfo> SaveStagesController::InitResourcesAndStart() {\n }\n \n void SaveStagesController::WaitAllSnapshots() {\n-  RunStage(&SaveStagesController::SaveCb);\n+  if (use_dfs_format_) {\n+    shard_set->RunBlockingInParallel([&](EngineShard* shard) { WaitSnapshotInShard(shard); });\n+    SaveBody(shard_set->size());\n+  } else {\n+    SaveBody(0);\n+  }\n }\n \n SaveInfo SaveStagesController::Finalize() {\n@@ -395,13 +408,22 @@ GenericError SaveStagesController::BuildFullPath() {\n   return {};\n }\n \n-void SaveStagesController::SaveCb(unsigned index) {\n-  if (auto& snapshot = snapshots_[index].first; snapshot && snapshot->HasStarted())\n+void SaveStagesController::SaveBody(unsigned index) {\n+  CHECK(!use_dfs_format_ || index == shard_set->size());  // used in rdb and df summary file\n+  if (auto& snapshot = snapshots_[index].first; snapshot && snapshot->HasStarted()) {\n     shared_err_ = snapshot->SaveBody();\n+  }\n+}\n+\n+void SaveStagesController::WaitSnapshotInShard(EngineShard* shard) {\n+  if (auto& snapshot = snapshots_[shard->shard_id()].first; snapshot && snapshot->HasStarted()) {\n+    shared_err_ = snapshot->WaitSnapshotInShard(shard);\n+  }\n }\n \n void SaveStagesController::CloseCb(unsigned index) {\n   if (auto& snapshot = snapshots_[index].first; snapshot) {\n+    snapshot->FillFreqMap();\n     shared_err_ = snapshot->Close();\n \n     unique_lock lk{rdb_name_map_mu_};\ndiff --git a/src/server/detail/save_stages_controller.h b/src/server/detail/save_stages_controller.h\nindex 0b9db29c46dc..7446dffc3423 100644\n--- a/src/server/detail/save_stages_controller.h\n+++ b/src/server/detail/save_stages_controller.h\n@@ -46,6 +46,8 @@ class RdbSnapshot {\n   void StartInShard(EngineShard* shard);\n \n   error_code SaveBody();\n+  error_code WaitSnapshotInShard(EngineShard* shard);\n+  void FillFreqMap();\n   error_code Close();\n   size_t GetSaveBuffersSize();\n \n@@ -101,6 +103,8 @@ struct SaveStagesController : public SaveStagesInputs {\n \n   // Start saving a dfs file on shard\n   void SaveDfsSingle(EngineShard* shard);\n+  void SaveSnashot(EngineShard* shard);\n+  void WaitSnapshotInShard(EngineShard* shard);\n \n   // Save a single rdb file\n   void SaveRdb();\n@@ -115,7 +119,7 @@ struct SaveStagesController : public SaveStagesInputs {\n   // Build full path: get dir, try creating dirs, get filename with placeholder\n   GenericError BuildFullPath();\n \n-  void SaveCb(unsigned index);\n+  void SaveBody(unsigned index);\n \n   void CloseCb(unsigned index);\n \ndiff --git a/src/server/dflycmd.cc b/src/server/dflycmd.cc\nindex f76bd84139b4..0d6251d3ce23 100644\n--- a/src/server/dflycmd.cc\n+++ b/src/server/dflycmd.cc\n@@ -121,7 +121,6 @@ void DflyCmd::ReplicaInfo::Cancel() {\n       flow->cleanup();\n     }\n     VLOG(2) << \"After flow cleanup \" << shard->shard_id();\n-    flow->full_sync_fb.JoinIfNeeded();\n     flow->conn = nullptr;\n   });\n   // Wait for error handler to quit.\n@@ -371,7 +370,7 @@ void DflyCmd::StartStable(CmdArgList args, Transaction* tx, RedisReplyBuilder* r\n     auto cb = [this, &status, replica_ptr = replica_ptr](EngineShard* shard) {\n       FlowInfo* flow = &replica_ptr->flows[shard->shard_id()];\n \n-      StopFullSyncInThread(flow, shard);\n+      StopFullSyncInThread(flow, &replica_ptr->cntx, shard);\n       status = StartStableSyncInThread(flow, &replica_ptr->cntx, shard);\n     };\n     shard_set->RunBlockingInParallel(std::move(cb));\n@@ -551,7 +550,6 @@ void DflyCmd::Load(CmdArgList args, RedisReplyBuilder* rb, ConnectionContext* cn\n }\n \n OpStatus DflyCmd::StartFullSyncInThread(FlowInfo* flow, Context* cntx, EngineShard* shard) {\n-  DCHECK(!flow->full_sync_fb.IsJoinable());\n   DCHECK(shard);\n   DCHECK(flow->conn);\n \n@@ -569,7 +567,6 @@ OpStatus DflyCmd::StartFullSyncInThread(FlowInfo* flow, Context* cntx, EngineSha\n     // callbacks are blocked on trying to insert to channel.\n     flow->TryShutdownSocket();\n     flow->saver->CancelInShard(shard);  // stops writing to journal stream to channel\n-    flow->full_sync_fb.JoinIfNeeded();  // finishes poping data from channel\n     flow->saver.reset();\n   };\n \n@@ -588,18 +585,24 @@ OpStatus DflyCmd::StartFullSyncInThread(FlowInfo* flow, Context* cntx, EngineSha\n   if (flow->start_partial_sync_at.has_value())\n     saver->StartIncrementalSnapshotInShard(cntx, shard, *flow->start_partial_sync_at);\n   else\n-    saver->StartSnapshotInShard(true, cntx->GetCancellation(), shard);\n+    saver->StartSnapshotInShard(true, cntx, shard);\n \n-  flow->full_sync_fb = fb2::Fiber(\"full_sync\", &DflyCmd::FullSyncFb, this, flow, cntx);\n   return OpStatus::OK;\n }\n \n-void DflyCmd::StopFullSyncInThread(FlowInfo* flow, EngineShard* shard) {\n+void DflyCmd::StopFullSyncInThread(FlowInfo* flow, Context* cntx, EngineShard* shard) {\n   DCHECK(shard);\n-  flow->saver->StopFullSyncInShard(shard);\n+  error_code ec = flow->saver->StopFullSyncInShard(shard);\n+  if (ec) {\n+    cntx->ReportError(ec);\n+    return;\n+  }\n \n-  // Wait for full sync to finish.\n-  flow->full_sync_fb.JoinIfNeeded();\n+  ec = flow->conn->socket()->Write(io::Buffer(flow->eof_token));\n+  if (ec) {\n+    cntx->ReportError(ec);\n+    return;\n+  }\n \n   // Reset cleanup and saver\n   flow->cleanup = []() {};\n@@ -626,23 +629,6 @@ OpStatus DflyCmd::StartStableSyncInThread(FlowInfo* flow, Context* cntx, EngineS\n   return OpStatus::OK;\n }\n \n-void DflyCmd::FullSyncFb(FlowInfo* flow, Context* cntx) {\n-  error_code ec;\n-\n-  if (ec = flow->saver->SaveBody(cntx, nullptr); ec) {\n-    if (!flow->conn->socket()->IsOpen())\n-      ec = make_error_code(errc::operation_canceled);  // we cancelled the operation.\n-    cntx->ReportError(ec);\n-    return;\n-  }\n-\n-  ec = flow->conn->socket()->Write(io::Buffer(flow->eof_token));\n-  if (ec) {\n-    cntx->ReportError(ec);\n-    return;\n-  }\n-}\n-\n auto DflyCmd::CreateSyncSession(ConnectionState* state) -> std::pair<uint32_t, unsigned> {\n   util::fb2::LockGuard lk(mu_);\n   unsigned sync_id = next_sync_id_++;\ndiff --git a/src/server/dflycmd.h b/src/server/dflycmd.h\nindex d3bf07aa3314..f67d7fee1abc 100644\n--- a/src/server/dflycmd.h\n+++ b/src/server/dflycmd.h\n@@ -39,7 +39,6 @@ struct FlowInfo {\n \n   facade::Connection* conn = nullptr;\n \n-  util::fb2::Fiber full_sync_fb;              // Full sync fiber.\n   std::unique_ptr<RdbSaver> saver;            // Saver for full sync phase.\n   std::unique_ptr<JournalStreamer> streamer;  // Streamer for stable sync phase\n   std::string eof_token;\n@@ -210,14 +209,11 @@ class DflyCmd {\n   facade::OpStatus StartFullSyncInThread(FlowInfo* flow, Context* cntx, EngineShard* shard);\n \n   // Stop full sync in thread. Run state switch cleanup.\n-  void StopFullSyncInThread(FlowInfo* flow, EngineShard* shard);\n+  void StopFullSyncInThread(FlowInfo* flow, Context* cntx, EngineShard* shard);\n \n   // Start stable sync in thread. Called for each flow.\n   facade::OpStatus StartStableSyncInThread(FlowInfo* flow, Context* cntx, EngineShard* shard);\n \n-  // Fiber that runs full sync for each flow.\n-  void FullSyncFb(FlowInfo* flow, Context* cntx);\n-\n   // Get ReplicaInfo by sync_id.\n   std::shared_ptr<ReplicaInfo> GetReplicaInfo(uint32_t sync_id) ABSL_LOCKS_EXCLUDED(mu_);\n \ndiff --git a/src/server/rdb_save.cc b/src/server/rdb_save.cc\nindex 8718020b5cdf..32257d621c0d 100644\n--- a/src/server/rdb_save.cc\n+++ b/src/server/rdb_save.cc\n@@ -29,6 +29,7 @@ extern \"C\" {\n #include \"base/logging.h\"\n #include \"core/bloom.h\"\n #include \"core/json/json_object.h\"\n+#include \"core/size_tracking_channel.h\"\n #include \"core/sorted_map.h\"\n #include \"core/string_map.h\"\n #include \"core/string_set.h\"\n@@ -834,6 +835,20 @@ error_code RdbSerializer::SaveStreamConsumers(streamCG* cg) {\n   return error_code{};\n }\n \n+error_code RdbSerializer::SendEofAndChecksum() {\n+  VLOG(2) << \"SendEof\";\n+  /* EOF opcode */\n+  RETURN_ON_ERR(WriteOpcode(RDB_OPCODE_EOF));\n+\n+  /* CRC64 checksum. It will be zero if checksum computation is disabled, the\n+   * loading code skips the check in this case. */\n+  uint8_t buf[8];\n+  uint64_t chksum = 0;\n+\n+  absl::little_endian::Store64(buf, chksum);\n+  return WriteRaw(buf);\n+}\n+\n error_code RdbSerializer::SendJournalOffset(uint64_t journal_offset) {\n   VLOG(2) << \"SendJournalOffset\";\n   RETURN_ON_ERR(WriteOpcode(RDB_OPCODE_JOURNAL_OFFSET));\n@@ -1107,11 +1122,13 @@ class RdbSaver::Impl {\n \n   ~Impl();\n \n-  void StartSnapshotting(bool stream_journal, const Cancellation* cll, EngineShard* shard);\n+  void StartSnapshotting(bool stream_journal, Context* cntx, EngineShard* shard);\n   void StartIncrementalSnapshotting(Context* cntx, EngineShard* shard, LSN start_lsn);\n \n   void StopSnapshotting(EngineShard* shard);\n+  void WaitForSnapshottingFinish(EngineShard* shard);\n \n+  // used only for legacy rdb save flows.\n   error_code ConsumeChannel(const Cancellation* cll);\n \n   void FillFreqMap(RdbTypeFreqMap* dest) const;\n@@ -1143,6 +1160,8 @@ class RdbSaver::Impl {\n   }\n \n  private:\n+  void PushSnapshotData(Context* cntx, string record);\n+  void FinalizeSnapshotWriting();\n   error_code WriteRecord(io::Bytes src);\n \n   unique_ptr<SliceSnapshot>& GetSnapshot(EngineShard* shard);\n@@ -1152,7 +1171,8 @@ class RdbSaver::Impl {\n   vector<unique_ptr<SliceSnapshot>> shard_snapshots_;\n   // used for serializing non-body components in the calling fiber.\n   RdbSerializer meta_serializer_;\n-  SliceSnapshot::RecordChannel channel_;\n+  using RecordChannel = SizeTrackingChannel<string, base::mpmc_bounded_queue<string>>;\n+  std::optional<RecordChannel> channel_;\n   std::optional<AlignedBuffer> aligned_buf_;\n \n   // Single entry compression is compatible with redis rdb snapshot\n@@ -1170,14 +1190,14 @@ RdbSaver::Impl::Impl(bool align_writes, unsigned producers_len, CompressionMode\n       shard_snapshots_(producers_len),\n       meta_serializer_(CompressionMode::NONE),  // Note: I think there is not need for compression\n                                                 // at all in meta serializer\n-      channel_{kChannelLen, producers_len},\n       compression_mode_(compression_mode) {\n   if (align_writes) {\n     aligned_buf_.emplace(kBufLen, sink);\n     sink_ = &aligned_buf_.value();\n   }\n-\n-  DCHECK(producers_len > 0 || channel_.IsClosing());\n+  if (sm == SaveMode::RDB) {\n+    channel_.emplace(kChannelLen, producers_len);\n+  }\n   save_mode_ = sm;\n }\n \n@@ -1213,13 +1233,13 @@ error_code RdbSaver::Impl::SaveAuxFieldStrStr(string_view key, string_view val)\n \n error_code RdbSaver::Impl::ConsumeChannel(const Cancellation* cll) {\n   error_code io_error;\n-  SliceSnapshot::DbRecord record;\n+  string record;\n \n   auto& stats = ServerState::tlocal()->stats;\n-\n+  DCHECK(channel_.has_value());\n   // we can not exit on io-error since we spawn fibers that push data.\n   // TODO: we may signal them to stop processing and exit asap in case of the error.\n-  while (channel_.Pop(record)) {\n+  while (channel_->Pop(record)) {\n     if (io_error || cll->IsCancelled())\n       continue;\n \n@@ -1227,9 +1247,8 @@ error_code RdbSaver::Impl::ConsumeChannel(const Cancellation* cll) {\n       if (cll->IsCancelled())\n         continue;\n \n-      DVLOG(2) << \"Pulled \" << record.id;\n       auto start = absl::GetCurrentTimeNanos();\n-      io_error = WriteRecord(io::Buffer(record.value));\n+      io_error = WriteRecord(io::Buffer(record));\n       if (io_error) {\n         break;  // from the inner TryPop loop.\n       }\n@@ -1237,15 +1256,15 @@ error_code RdbSaver::Impl::ConsumeChannel(const Cancellation* cll) {\n       auto delta_usec = (absl::GetCurrentTimeNanos() - start) / 1'000;\n       stats.rdb_save_usec += delta_usec;\n       stats.rdb_save_count++;\n-    } while ((channel_.TryPop(record)));\n+    } while ((channel_->TryPop(record)));\n   }  // while (channel_.Pop())\n \n   for (auto& ptr : shard_snapshots_) {\n-    ptr->Join();\n+    ptr->WaitSnapshotting();\n   }\n   VLOG(1) << \"ConsumeChannel finished \" << io_error;\n \n-  DCHECK(!channel_.TryPop(record));\n+  DCHECK(!channel_->TryPop(record));\n \n   return io_error;\n }\n@@ -1278,32 +1297,70 @@ error_code RdbSaver::Impl::WriteRecord(io::Bytes src) {\n   return ec;\n }\n \n-void RdbSaver::Impl::StartSnapshotting(bool stream_journal, const Cancellation* cll,\n-                                       EngineShard* shard) {\n+void RdbSaver::Impl::PushSnapshotData(Context* cntx, string record) {\n+  if (cntx->IsCancelled()) {\n+    return;\n+  }\n+  if (channel_) {  // Rdb write to channel\n+    channel_->Push(record);\n+  } else {  // Write directly to socket\n+    auto ec = WriteRecord(io::Buffer(record));\n+    if (ec) {\n+      cntx->ReportError(ec);\n+    }\n+  }\n+}\n+\n+void RdbSaver::Impl::FinalizeSnapshotWriting() {\n+  if (channel_) {\n+    channel_->StartClosing();\n+  }\n+}\n+\n+void RdbSaver::Impl::StartSnapshotting(bool stream_journal, Context* cntx, EngineShard* shard) {\n   auto& s = GetSnapshot(shard);\n   auto& db_slice = namespaces.GetDefaultNamespace().GetDbSlice(shard->shard_id());\n-  s = std::make_unique<SliceSnapshot>(&db_slice, &channel_, compression_mode_);\n+  auto on_snapshot_finish = std::bind(&RdbSaver::Impl::FinalizeSnapshotWriting, this);\n+  auto push_cb = std::bind(&RdbSaver::Impl::PushSnapshotData, this, cntx, std::placeholders::_1);\n+\n+  s = std::make_unique<SliceSnapshot>(&db_slice, compression_mode_, push_cb, on_snapshot_finish);\n \n   const auto allow_flush = (save_mode_ != SaveMode::RDB) ? SliceSnapshot::SnapshotFlush::kAllow\n                                                          : SliceSnapshot::SnapshotFlush::kDisallow;\n-  s->Start(stream_journal, cll, allow_flush);\n+\n+  s->Start(stream_journal, cntx->GetCancellation(), allow_flush);\n }\n \n void RdbSaver::Impl::StartIncrementalSnapshotting(Context* cntx, EngineShard* shard,\n                                                   LSN start_lsn) {\n   auto& db_slice = namespaces.GetDefaultNamespace().GetDbSlice(shard->shard_id());\n   auto& s = GetSnapshot(shard);\n-  s = std::make_unique<SliceSnapshot>(&db_slice, &channel_, compression_mode_);\n+  auto on_finalize_cb = std::bind(&RdbSaver::Impl::FinalizeSnapshotWriting, this);\n+  auto push_cb = std::bind(&RdbSaver::Impl::PushSnapshotData, this, cntx, std::placeholders::_1);\n+  s = std::make_unique<SliceSnapshot>(&db_slice, compression_mode_, push_cb, on_finalize_cb);\n \n   s->StartIncremental(cntx, start_lsn);\n }\n \n+// called on save flow\n+void RdbSaver::Impl::WaitForSnapshottingFinish(EngineShard* shard) {\n+  auto& snapshot = GetSnapshot(shard);\n+  CHECK(snapshot);\n+  snapshot->WaitSnapshotting();\n+}\n+\n+// called from replication flow\n void RdbSaver::Impl::StopSnapshotting(EngineShard* shard) {\n-  GetSnapshot(shard)->FinalizeJournalStream(false);\n+  auto& snapshot = GetSnapshot(shard);\n+  CHECK(snapshot);\n+  snapshot->FinalizeJournalStream(false);\n }\n \n void RdbSaver::Impl::CancelInShard(EngineShard* shard) {\n-  GetSnapshot(shard)->FinalizeJournalStream(true);\n+  auto& snapshot = GetSnapshot(shard);\n+  if (snapshot) {  // Cancel can be called before snapshotting started.\n+    snapshot->FinalizeJournalStream(true);\n+  }\n }\n \n // This function is called from connection thread when info command is invoked.\n@@ -1314,7 +1371,8 @@ size_t RdbSaver::Impl::GetTotalBuffersSize() const {\n \n   auto cb = [this, &channel_bytes, &serializer_bytes](ShardId sid) {\n     auto& snapshot = shard_snapshots_[sid];\n-    channel_bytes.fetch_add(snapshot->GetTotalChannelCapacity(), memory_order_relaxed);\n+    if (channel_.has_value())\n+      channel_bytes.fetch_add(channel_->GetSize(), memory_order_relaxed);\n     serializer_bytes.store(snapshot->GetBufferCapacity() + snapshot->GetTempBuffersSize(),\n                            memory_order_relaxed);\n   };\n@@ -1437,17 +1495,22 @@ RdbSaver::~RdbSaver() {\n   tlocal->DecommitMemory(ServerState::kAllMemory);\n }\n \n-void RdbSaver::StartSnapshotInShard(bool stream_journal, const Cancellation* cll,\n-                                    EngineShard* shard) {\n-  impl_->StartSnapshotting(stream_journal, cll, shard);\n+void RdbSaver::StartSnapshotInShard(bool stream_journal, Context* cntx, EngineShard* shard) {\n+  impl_->StartSnapshotting(stream_journal, cntx, shard);\n }\n \n void RdbSaver::StartIncrementalSnapshotInShard(Context* cntx, EngineShard* shard, LSN start_lsn) {\n   impl_->StartIncrementalSnapshotting(cntx, shard, start_lsn);\n }\n \n-void RdbSaver::StopFullSyncInShard(EngineShard* shard) {\n+error_code RdbSaver::WaitSnapshotInShard(EngineShard* shard) {\n+  impl_->WaitForSnapshottingFinish(shard);\n+  return SaveEpilog();\n+}\n+\n+error_code RdbSaver::StopFullSyncInShard(EngineShard* shard) {\n   impl_->StopSnapshotting(shard);\n+  return SaveEpilog();\n }\n \n error_code RdbSaver::SaveHeader(const GlobalData& glob_state) {\n@@ -1459,16 +1522,14 @@ error_code RdbSaver::SaveHeader(const GlobalData& glob_state) {\n \n   RETURN_ON_ERR(impl_->serializer()->WriteRaw(Bytes{reinterpret_cast<uint8_t*>(magic), sz}));\n   RETURN_ON_ERR(SaveAux(std::move(glob_state)));\n-\n+  RETURN_ON_ERR(impl_->FlushSerializer());\n   return error_code{};\n }\n \n-error_code RdbSaver::SaveBody(Context* cntx, RdbTypeFreqMap* freq_map) {\n+error_code RdbSaver::SaveBody(Context* cntx) {\n   RETURN_ON_ERR(impl_->FlushSerializer());\n \n-  if (save_mode_ == SaveMode::SUMMARY) {\n-    impl_->serializer()->SendFullSyncCut();\n-  } else {\n+  if (save_mode_ == SaveMode::RDB) {\n     VLOG(1) << \"SaveBody , snapshots count: \" << impl_->Size();\n     error_code io_error = impl_->ConsumeChannel(cntx->GetCancellation());\n     if (io_error) {\n@@ -1477,16 +1538,16 @@ error_code RdbSaver::SaveBody(Context* cntx, RdbTypeFreqMap* freq_map) {\n     if (cntx->GetError()) {\n       return cntx->GetError();\n     }\n+  } else {\n+    DCHECK(save_mode_ == SaveMode::SUMMARY);\n   }\n \n-  RETURN_ON_ERR(SaveEpilog());\n-\n-  if (freq_map) {\n-    freq_map->clear();\n-    impl_->FillFreqMap(freq_map);\n-  }\n+  return SaveEpilog();\n+}\n \n-  return error_code{};\n+void RdbSaver::FillFreqMap(RdbTypeFreqMap* freq_map) {\n+  freq_map->clear();\n+  impl_->FillFreqMap(freq_map);\n }\n \n error_code RdbSaver::SaveAux(const GlobalData& glob_state) {\n@@ -1523,20 +1584,7 @@ error_code RdbSaver::SaveAux(const GlobalData& glob_state) {\n }\n \n error_code RdbSaver::SaveEpilog() {\n-  uint8_t buf[8];\n-  uint64_t chksum;\n-\n-  auto& ser = *impl_->serializer();\n-\n-  /* EOF opcode */\n-  RETURN_ON_ERR(ser.WriteOpcode(RDB_OPCODE_EOF));\n-\n-  /* CRC64 checksum. It will be zero if checksum computation is disabled, the\n-   * loading code skips the check in this case. */\n-  chksum = 0;\n-\n-  absl::little_endian::Store64(buf, chksum);\n-  RETURN_ON_ERR(ser.WriteRaw(buf));\n+  RETURN_ON_ERR(impl_->serializer()->SendEofAndChecksum());\n \n   RETURN_ON_ERR(impl_->FlushSerializer());\n \ndiff --git a/src/server/rdb_save.h b/src/server/rdb_save.h\nindex 90ca435c720f..5d889d21f1e1 100644\n--- a/src/server/rdb_save.h\n+++ b/src/server/rdb_save.h\n@@ -89,21 +89,26 @@ class RdbSaver {\n \n   // Initiates the serialization in the shard's thread.\n   // cll allows breaking in the middle.\n-  void StartSnapshotInShard(bool stream_journal, const Cancellation* cll, EngineShard* shard);\n+  void StartSnapshotInShard(bool stream_journal, Context* cntx, EngineShard* shard);\n \n   // Send only the incremental snapshot since start_lsn.\n   void StartIncrementalSnapshotInShard(Context* cntx, EngineShard* shard, LSN start_lsn);\n \n   // Stops full-sync serialization for replication in the shard's thread.\n-  void StopFullSyncInShard(EngineShard* shard);\n+  std::error_code StopFullSyncInShard(EngineShard* shard);\n+\n+  // Wait for snapshotting finish in shard thread. Called from save flows in shard thread.\n+  std::error_code WaitSnapshotInShard(EngineShard* shard);\n \n   // Stores auxiliary (meta) values and header_info\n   std::error_code SaveHeader(const GlobalData& header_info);\n \n   // Writes the RDB file into sink. Waits for the serialization to finish.\n+  // Called only for save rdb flow and save df on summary file.\n+  std::error_code SaveBody(Context* cntx);\n+\n   // Fills freq_map with the histogram of rdb types.\n-  // freq_map can optionally be null.\n-  std::error_code SaveBody(Context* cntx, RdbTypeFreqMap* freq_map);\n+  void FillFreqMap(RdbTypeFreqMap* freq_map);\n \n   void CancelInShard(EngineShard* shard);\n \n@@ -232,6 +237,7 @@ class RdbSerializer : public SerializerBase {\n   std::error_code SendJournalOffset(uint64_t journal_offset);\n \n   size_t GetTempBufferSize() const override;\n+  std::error_code SendEofAndChecksum();\n \n  private:\n   // Might preempt if flush_fun_ is used\ndiff --git a/src/server/snapshot.cc b/src/server/snapshot.cc\nindex 810cf69479f5..745233552526 100644\n--- a/src/server/snapshot.cc\n+++ b/src/server/snapshot.cc\n@@ -32,16 +32,17 @@ using facade::operator\"\"_KB;\n namespace {\n thread_local absl::flat_hash_set<SliceSnapshot*> tl_slice_snapshots;\n \n-constexpr size_t kMinChannelBlobSize = 32_KB;\n+constexpr size_t kMinBlobSize = 32_KB;\n \n }  // namespace\n \n-size_t SliceSnapshot::DbRecord::size() const {\n-  return HeapSize(value);\n-}\n-\n-SliceSnapshot::SliceSnapshot(DbSlice* slice, RecordChannel* dest, CompressionMode compression_mode)\n-    : db_slice_(slice), dest_(dest), compression_mode_(compression_mode) {\n+SliceSnapshot::SliceSnapshot(DbSlice* slice, CompressionMode compression_mode,\n+                             std::function<void(std::string)> on_push_record,\n+                             std::function<void()> on_snapshot_finish)\n+    : db_slice_(slice),\n+      compression_mode_(compression_mode),\n+      on_push_(on_push_record),\n+      on_snapshot_finish_(on_snapshot_finish) {\n   db_array_ = slice->databases();\n   tl_slice_snapshots.insert(this);\n }\n@@ -53,7 +54,7 @@ SliceSnapshot::~SliceSnapshot() {\n size_t SliceSnapshot::GetThreadLocalMemoryUsage() {\n   size_t mem = 0;\n   for (SliceSnapshot* snapshot : tl_slice_snapshots) {\n-    mem += snapshot->GetBufferCapacity() + snapshot->GetTotalChannelCapacity();\n+    mem += snapshot->GetBufferCapacity();\n   }\n   return mem;\n }\n@@ -81,8 +82,8 @@ void SliceSnapshot::Start(bool stream_journal, const Cancellation* cll, Snapshot\n     flush_fun = [this, flush_threshold](size_t bytes_serialized,\n                                         RdbSerializer::FlushState flush_state) {\n       if (bytes_serialized > flush_threshold) {\n-        size_t serialized = FlushChannelRecord(flush_state);\n-        VLOG(2) << \"FlushedToChannel \" << serialized << \" bytes\";\n+        size_t serialized = FlushSerialized(flush_state);\n+        VLOG(2) << \"FlushSerialized \" << serialized << \" bytes\";\n       }\n     };\n   }\n@@ -93,11 +94,7 @@ void SliceSnapshot::Start(bool stream_journal, const Cancellation* cll, Snapshot\n   snapshot_fb_ = fb2::Fiber(\"snapshot\", [this, stream_journal, cll] {\n     IterateBucketsFb(cll, stream_journal);\n     db_slice_->UnregisterOnChange(snapshot_version_);\n-    // We stop the channel if we are performing backups (non-streaming).\n-    // We keep the channel for replication in order to send jounal changes until we finalize.\n-    if (!stream_journal) {\n-      CloseRecordChannel();\n-    }\n+    on_snapshot_finish_();\n   });\n }\n \n@@ -127,10 +124,8 @@ void SliceSnapshot::FinalizeJournalStream(bool cancel) {\n   journal->UnregisterOnChange(cb_id);\n   if (!cancel) {\n     serializer_->SendJournalOffset(journal->GetLsn());\n-    PushSerializedToChannel(true);\n+    PushSerialized(true);\n   }\n-\n-  CloseRecordChannel();\n }\n \n // The algorithm is to go over all the buckets and serialize those with\n@@ -175,7 +170,7 @@ void SliceSnapshot::IterateBucketsFb(const Cancellation* cll, bool send_full_syn\n       PrimeTable::Cursor next =\n           db_slice_->Traverse(pt, cursor, absl::bind_front(&SliceSnapshot::BucketSaveCb, this));\n       cursor = next;\n-      PushSerializedToChannel(false);\n+      PushSerialized(false);\n \n       if (stats_.loop_serialized >= last_yield + 100) {\n         DVLOG(2) << \"Before sleep \" << ThisFiber::GetName();\n@@ -185,18 +180,18 @@ void SliceSnapshot::IterateBucketsFb(const Cancellation* cll, bool send_full_syn\n         last_yield = stats_.loop_serialized;\n         // Push in case other fibers (writes commands that pushed previous values)\n         // filled the buffer.\n-        PushSerializedToChannel(false);\n+        PushSerialized(false);\n       }\n     } while (cursor);\n \n     DVLOG(2) << \"after loop \" << ThisFiber::GetName();\n-    PushSerializedToChannel(true);\n+    PushSerialized(true);\n   }  // for (dbindex)\n \n   CHECK(!serialize_bucket_running_);\n   if (send_full_sync_cut) {\n     CHECK(!serializer_->SendFullSyncCut());\n-    PushSerializedToChannel(true);\n+    PushSerialized(true);\n   }\n \n   // serialized + side_saved must be equal to the total saved.\n@@ -214,7 +209,7 @@ void SliceSnapshot::SwitchIncrementalFb(Context* cntx, LSN lsn) {\n   // The replica sends the LSN of the next entry is wants to receive.\n   while (!cntx->IsCancelled() && journal->IsLSNInBuffer(lsn)) {\n     serializer_->WriteJournalEntry(journal->GetEntry(lsn));\n-    PushSerializedToChannel(false);\n+    PushSerialized(false);\n     lsn++;\n   }\n \n@@ -236,7 +231,7 @@ void SliceSnapshot::SwitchIncrementalFb(Context* cntx, LSN lsn) {\n     }\n     auto journal_cb = absl::bind_front(&SliceSnapshot::OnJournalEntry, this);\n     journal_cb_id_ = journal->RegisterOnChange(std::move(journal_cb));\n-    PushSerializedToChannel(true);\n+    PushSerialized(true);\n   } else {\n     // We stopped but we didn't manage to send the whole stream.\n     cntx->ReportError(\n@@ -321,7 +316,7 @@ void SliceSnapshot::SerializeEntry(DbIndex db_indx, const PrimeKey& pk, const Pr\n   }\n }\n \n-size_t SliceSnapshot::FlushChannelRecord(SerializerBase::FlushState flush_state) {\n+size_t SliceSnapshot::FlushSerialized(SerializerBase::FlushState flush_state) {\n   io::StringFile sfile;\n   serializer_->FlushToSink(&sfile, flush_state);\n \n@@ -331,34 +326,34 @@ size_t SliceSnapshot::FlushChannelRecord(SerializerBase::FlushState flush_state)\n \n   uint64_t id = rec_id_++;\n   DVLOG(2) << \"Pushing \" << id;\n-  DbRecord db_rec{.id = id, .value = std::move(sfile.val)};\n+\n   fb2::NoOpLock lk;\n \n   // We create a critical section here that ensures that records are pushed in sequential order.\n-  // As a result, it is not possible for two fiber producers to push into channel concurrently.\n+  // As a result, it is not possible for two fiber producers to push concurrently.\n   // If A.id = 5, and then B.id = 6, and both are blocked here, it means that last_pushed_id_ < 4.\n   // Once last_pushed_id_ = 4, A will be unblocked, while B will wait until A finishes pushing and\n   // update last_pushed_id_ to 5.\n   seq_cond_.wait(lk, [&] { return id == this->last_pushed_id_ + 1; });\n \n   // Blocking point.\n-  size_t channel_usage = dest_->Push(std::move(db_rec));\n+  on_push_(std::move(sfile.val));\n+\n   DCHECK_EQ(last_pushed_id_ + 1, id);\n   last_pushed_id_ = id;\n   seq_cond_.notify_all();\n \n-  VLOG(2) << \"Pushed with Serialize() \" << serialized\n-          << \" bytes, channel total usage: \" << channel_usage;\n+  VLOG(2) << \"Pushed with Serialize() \" << serialized;\n \n   return serialized;\n }\n \n-bool SliceSnapshot::PushSerializedToChannel(bool force) {\n-  if (!force && serializer_->SerializedLen() < kMinChannelBlobSize)\n+bool SliceSnapshot::PushSerialized(bool force) {\n+  if (!force && serializer_->SerializedLen() < kMinBlobSize)\n     return false;\n \n   // Flush any of the leftovers to avoid interleavings\n-  size_t serialized = FlushChannelRecord(FlushState::kFlushMidEntry);\n+  size_t serialized = FlushSerialized(FlushState::kFlushMidEntry);\n \n   if (!delayed_entries_.empty()) {\n     // Async bucket serialization might have accumulated some delayed values.\n@@ -371,7 +366,7 @@ bool SliceSnapshot::PushSerializedToChannel(bool force) {\n     } while (!delayed_entries_.empty());\n \n     // blocking point.\n-    serialized += FlushChannelRecord(FlushState::kFlushMidEntry);\n+    serialized += FlushSerialized(FlushState::kFlushMidEntry);\n   }\n   return serialized > 0;\n }\n@@ -400,7 +395,7 @@ void SliceSnapshot::OnDbChange(DbIndex db_index, const DbSlice::ChangeReq& req)\n void SliceSnapshot::OnJournalEntry(const journal::JournalItem& item, bool await) {\n   // To enable journal flushing to sync after non auto journal command is executed we call\n   // TriggerJournalWriteToSink. This call uses the NOOP opcode with await=true. Since there is no\n-  // additional journal change to serialize, it simply invokes PushSerializedToChannel.\n+  // additional journal change to serialize, it simply invokes PushSerialized.\n   std::unique_lock lk(db_slice_->GetSerializationMutex());\n   if (item.opcode != journal::Op::NOOP) {\n     serializer_->WriteJournalEntry(item.data);\n@@ -409,19 +404,7 @@ void SliceSnapshot::OnJournalEntry(const journal::JournalItem& item, bool await)\n   if (await) {\n     // This is the only place that flushes in streaming mode\n     // once the iterate buckets fiber finished.\n-    PushSerializedToChannel(false);\n-  }\n-}\n-\n-void SliceSnapshot::CloseRecordChannel() {\n-  DCHECK(db_slice_->shard_owner()->IsMyThread());\n-  std::unique_lock lk(db_slice_->GetSerializationMutex());\n-\n-  CHECK(!serialize_bucket_running_);\n-  // Make sure we close the channel only once with a CAS check.\n-  bool expected = false;\n-  if (closed_chan_.compare_exchange_strong(expected, true)) {\n-    dest_->StartClosing();\n+    PushSerialized(false);\n   }\n }\n \n@@ -433,10 +416,6 @@ size_t SliceSnapshot::GetBufferCapacity() const {\n   return serializer_->GetBufferCapacity();\n }\n \n-size_t SliceSnapshot::GetTotalChannelCapacity() const {\n-  return dest_->GetSize();\n-}\n-\n size_t SliceSnapshot::GetTempBuffersSize() const {\n   if (serializer_ == nullptr) {\n     return 0;\ndiff --git a/src/server/snapshot.h b/src/server/snapshot.h\nindex 9c5fd976c88e..38ad86c889ad 100644\n--- a/src/server/snapshot.h\n+++ b/src/server/snapshot.h\n@@ -8,7 +8,6 @@\n #include <bitset>\n \n #include \"base/pod_array.h\"\n-#include \"core/size_tracking_channel.h\"\n #include \"io/file.h\"\n #include \"server/common.h\"\n #include \"server/db_slice.h\"\n@@ -31,35 +30,27 @@ struct Entry;\n // \u2502     SerializeBucket      \u2502        Both might fall back to a temporary serializer\n // \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        if default is used on another db index\n //              \u2502\n-//              |                      Channel is left open in journal streaming mode\n+//              |                      Socket is left open in journal streaming mode\n //              \u25bc\n // \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n // \u2502     SerializeEntry       \u2502 \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524     OnJournalEntry      \u2502\n // \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n //               \u2502\n-//         PushBytesToChannel        Default buffer gets flushed on iteration,\n-//               \u2502                   temporary on destruction\n+//         PushBytes                  Default buffer gets flushed on iteration,\n+//               \u2502                    temporary on destruction\n //               \u25bc\n // \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n-// \u2502     dest->Push(buffer)       \u2502\n+// \u2502     push_cb(buffer)       \u2502\n // \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n \n // SliceSnapshot is used for iterating over a shard at a specified point-in-time\n-// and submitting all values to an output channel.\n+// and submitting all values to an output sink.\n // In journal streaming mode, the snapshot continues submitting changes\n-// over the channel until explicitly stopped.\n+// over the sink until explicitly stopped.\n class SliceSnapshot {\n  public:\n-  struct DbRecord {\n-    uint64_t id;\n-    std::string value;\n-\n-    size_t size() const;\n-  };\n-\n-  using RecordChannel = SizeTrackingChannel<DbRecord, base::mpmc_bounded_queue<DbRecord>>;\n-\n-  SliceSnapshot(DbSlice* slice, RecordChannel* dest, CompressionMode compression_mode);\n+  SliceSnapshot(DbSlice* slice, CompressionMode compression_mode,\n+                std::function<void(std::string)> on_push, std::function<void()> on_snapshot_finish);\n   ~SliceSnapshot();\n \n   static size_t GetThreadLocalMemoryUsage();\n@@ -85,7 +76,7 @@ class SliceSnapshot {\n \n   // Waits for a regular, non journal snapshot to finish.\n   // Called only for non-replication, backups usecases.\n-  void Join() {\n+  void WaitSnapshotting() {\n     snapshot_fb_.JoinIfNeeded();\n   }\n \n@@ -114,18 +105,15 @@ class SliceSnapshot {\n   // Journal listener\n   void OnJournalEntry(const journal::JournalItem& item, bool unused_await_arg);\n \n-  // Close dest channel if not closed yet.\n-  void CloseRecordChannel();\n-\n-  // Push serializer's internal buffer to channel.\n+  // Push serializer's internal buffer.\n   // Push regardless of buffer size if force is true.\n   // Return true if pushed. Can block. Is called from the snapshot thread.\n-  bool PushSerializedToChannel(bool force);\n+  bool PushSerialized(bool force);\n \n   // Helper function that flushes the serialized items into the RecordStream.\n-  // Can block on the channel.\n+  // Can block.\n   using FlushState = SerializerBase::FlushState;\n-  size_t FlushChannelRecord(FlushState flush_state);\n+  size_t FlushSerialized(FlushState flush_state);\n \n  public:\n   uint64_t snapshot_version() const {\n@@ -138,7 +126,6 @@ class SliceSnapshot {\n \n   // Get different sizes, in bytes. All disjoint.\n   size_t GetBufferCapacity() const;\n-  size_t GetTotalChannelCapacity() const;\n   size_t GetTempBuffersSize() const;\n \n   RdbSaver::SnapshotStats GetCurrentSnapshotProgress() const;\n@@ -156,9 +143,6 @@ class SliceSnapshot {\n   DbSlice* db_slice_;\n   DbTableArray db_array_;\n \n-  RecordChannel* dest_;\n-  std::atomic_bool closed_chan_{false};  // true if dest_->StartClosing was already called\n-\n   DbIndex current_db_;\n \n   std::unique_ptr<RdbSerializer> serializer_;\n@@ -184,6 +168,9 @@ class SliceSnapshot {\n     size_t savecb_calls = 0;\n     size_t keys_total = 0;\n   } stats_;\n+\n+  std::function<void(std::string)> on_push_;\n+  std::function<void()> on_snapshot_finish_;\n };\n \n }  // namespace dfly\n",
  "test_patch": "diff --git a/tests/dragonfly/snapshot_test.py b/tests/dragonfly/snapshot_test.py\nindex e7062f48dbd9..622b07215be8 100644\n--- a/tests/dragonfly/snapshot_test.py\n+++ b/tests/dragonfly/snapshot_test.py\n@@ -439,7 +439,7 @@ async def test_snapshot(self, df_server, async_client):\n \n @pytest.mark.parametrize(\"format\", FILE_FORMATS)\n @dfly_args({**BASIC_ARGS, \"dbfilename\": \"info-while-snapshot\"})\n-async def test_infomemory_while_snapshoting(df_factory, format: str):\n+async def test_infomemory_while_snapshotting(df_factory, format: str):\n     instance = df_factory.create(dbfilename=f\"dump_{tmp_file_name()}\")\n     instance.start()\n     async_client = instance.client()\n",
  "problem_statement": "Pass io::Sink to snapshot in case we perform DFS/Replication snapshotting, and give up on the channel.\n\n",
  "hints_text": "",
  "created_at": "2024-11-03T10:48:16Z",
  "modified_files": [
    "src/server/detail/save_stages_controller.cc",
    "src/server/detail/save_stages_controller.h",
    "src/server/dflycmd.cc",
    "src/server/dflycmd.h",
    "src/server/rdb_save.cc",
    "src/server/rdb_save.h",
    "src/server/snapshot.cc",
    "src/server/snapshot.h"
  ],
  "modified_test_files": [
    "tests/dragonfly/snapshot_test.py"
  ]
}