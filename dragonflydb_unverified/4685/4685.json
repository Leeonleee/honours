{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4685,
  "instance_id": "dragonflydb__dragonfly-4685",
  "issue_numbers": [
    "4663"
  ],
  "base_commit": "77b9a8f6999c120b7a8857f39c19967f521c276e",
  "patch": "diff --git a/src/server/main_service.cc b/src/server/main_service.cc\nindex 272d805b985c..12dfb5196789 100644\n--- a/src/server/main_service.cc\n+++ b/src/server/main_service.cc\n@@ -621,6 +621,7 @@ Transaction::MultiMode DeduceExecMode(ExecScriptUse state,\n                                       const ScriptMgr& script_mgr) {\n   // Check if script most LIKELY has global eval transactions\n   bool contains_global = false;\n+  bool contains_admin_cmd = false;\n   Transaction::MultiMode multi_mode = Transaction::LOCK_AHEAD;\n \n   if (state == ExecScriptUse::SCRIPT_RUN) {\n@@ -641,6 +642,7 @@ Transaction::MultiMode DeduceExecMode(ExecScriptUse state,\n         transactional |= scmd.Cid()->IsTransactional();\n       }\n       contains_global |= scmd.Cid()->opt_mask() & CO::GLOBAL_TRANS;\n+      contains_admin_cmd |= scmd.Cid()->opt_mask() & CO::ADMIN;\n \n       // We can't run no-key-transactional commands in lock-ahead mode currently,\n       // because it means we have to schedule on all shards\n@@ -656,9 +658,13 @@ Transaction::MultiMode DeduceExecMode(ExecScriptUse state,\n   if (!transactional && exec_info.watched_keys.empty())\n     return Transaction::NOT_DETERMINED;\n \n+  if (contains_admin_cmd) {\n+    multi_mode = Transaction::NON_ATOMIC;\n+  }\n   // Atomic modes fall back to GLOBAL if they contain global commands.\n-  if (contains_global && multi_mode == Transaction::LOCK_AHEAD)\n+  else if (contains_global && multi_mode == Transaction::LOCK_AHEAD) {\n     multi_mode = Transaction::GLOBAL;\n+  }\n \n   return multi_mode;\n }\n",
  "test_patch": "diff --git a/tests/dragonfly/replication_test.py b/tests/dragonfly/replication_test.py\nindex 80aa07356a8a..7b8b7b2adb0c 100644\n--- a/tests/dragonfly/replication_test.py\n+++ b/tests/dragonfly/replication_test.py\n@@ -2870,3 +2870,35 @@ async def test_replicaof_does_not_flush_if_it_fails_to_connect(df_factory):\n         await c_replica.execute_command(f\"REPLICAOF localhost {replica.port}\")\n     res = await c_replica.execute_command(\"dbsize\")\n     assert res == 1\n+\n+\n+@dfly_args({\"proactor_threads\": 2})\n+async def test_replicaof_inside_multi(df_factory):\n+    master = df_factory.create()\n+    replica = df_factory.create()\n+    df_factory.start_all([master, replica])\n+\n+    async def replicate_inside_multi():\n+        try:\n+            c_master = master.client()\n+            p = c_master.pipeline(transaction=True)\n+            for i in range(5):\n+                p.execute_command(\"dbsize\")\n+            p.execute_command(f\"replicaof localhost {replica.port}\")\n+            await p.execute()\n+            return True\n+        except redis.exceptions.ResponseError:\n+            return False\n+\n+    MULTI_COMMANDS_TO_ISSUE = 30\n+    replication_commands = [\n+        asyncio.create_task(replicate_inside_multi()) for _ in range(MULTI_COMMANDS_TO_ISSUE)\n+    ]\n+\n+    num_successes = 0\n+    for result in asyncio.as_completed(replication_commands, timeout=80):\n+        num_successes += await result\n+\n+    logging.info(f\"succeses: {num_successes}\")\n+    assert MULTI_COMMANDS_TO_ISSUE > num_successes, \"At least one REPLICAOF must be cancelled\"\n+    assert num_successes > 0, \"At least one REPLICAOF must success\"\n",
  "problem_statement": "Hung replica\nHello, I have been having the problem for many months where the dragonfly process randomly hangs completely. The process does not die but it is impossible to connect to that instance.\nI have installed the \"debug\" version and I have managed to capture a strace of the process in this case. I attach the content of the strace.\n\nIn this situation you cannot connect to the node and it starts generating dumps in the directory that is not configured for that purpose, it is as if it had started with a configuration that is not the one specified in the original startup.\n\nIn the logs (which I also attach) there is nothing that indicates an error.\n\nHmmm, has the ability to attach files been disabled?\n\n",
  "hints_text": "hello Fernano! What version you are talking about?\nDo you perform periodic snapshots directly to cloud storage?\nMy current version is 1.27.1. But this problem has always existed.\nAnother replica has just stopped, I'm going to capture the strace and you can tell me how you want me to send it to you.\nif you can run \"DEBUG STACKTRACE\" it will help more that strace\nI connect with telnet and I execute DEBUG STACKTRACE. \nPlease remember me where is located the debug file.....\nYes I have configure snapshot with flag file key:\n--snapshot_cron=0 */1 * * *\nthe output goes to dragonfly.INFO that is usually located under /var/log/dragonfly.\n\nWhat's the destination of the snapshot? is it cloud s3:// or gcs:// bucket or a local path?\nHow do I send you the log?\n[Archive.zip](https://github.com/user-attachments/files/18985523/Archive.zip)\nWe will follow up on this, thanks Fernando and sorry for this trouble.\nThank you for always being attentive\nAnother node just stopped, I'm sending you the logs in case they are helpful.\n\n[Archive_other_hung.zip](https://github.com/user-attachments/files/18985963/Archive_other_hung.zip)\n@adiholden  writing my thoughts here.\n\n1. clear that shard queues are stuck - there are multiple single key commands with traces like these:\n```\n0x5645da76c52c  util::fb2::detail::FiberInterface::SwitchTo()\n0x5645da768e15  util::fb2::detail::Scheduler::Preempt()\n0x5645d9ea4d2d  util::fb2::EventCount::await<>()\n0x5645da236b02  dfly::Transaction::Execute()\n0x5645da236bca  dfly::Transaction::ScheduleSingleHop()\n0x5645d9ed8108  dfly::GenericFamily::Del()\n0x5645da214aaf  dfly::CommandId::Invoke()\n0x5645d9f0edf3  dfly::Service::InvokeCmd()\n0x5645d9f15a81  dfly::Service::DispatchCommand()\n0x5645da3090b9  facade::Connection::DispatchSingle()\n0x5645da30960f  facade::Connection::ParseRedis()\n0x5645da30b83c  facade::Connection::IoLoop()\n0x5645da30becd  facade::Connection::ConnectionFlow()\n0x5645da30cfff  facade::Connection::HandleRequests()\n```\n2. My immediate suspect is this guy:\n```\n0x5645da76c52c  util::fb2::detail::FiberInterface::SwitchTo()\n0x5645da768e15  util::fb2::detail::Scheduler::Preempt()\n0x5645d9ea4d2d  util::fb2::EventCount::await<>()\n0x5645da236b02  dfly::Transaction::Execute()\n0x5645d9f77b5d  dfly::ServerFamily::Drakarys()\n0x5645d9f8d153  dfly::ServerFamily::ReplicaOfInternal()\n0x5645da214aaf  dfly::CommandId::Invoke()\n0x5645d9f0edf3  dfly::Service::InvokeCmd()\n0x5645da04dc83  dfly::MultiCommandSquasher::ExecuteStandalone()\n0x5645da04e6ae  dfly::MultiCommandSquasher::Run()\n0x5645d9f1dc08  dfly::MultiCommandSquasher::Execute()\n0x5645d9f1487c  dfly::Service::Exec()\n0x5645da214aaf  dfly::CommandId::Invoke()\n0x5645d9f0edf3  dfly::Service::InvokeCmd()\n0x5645d9f15a81  dfly::Service::DispatchCommand()\n0x5645d9f1643a  dfly::Service::DispatchManyCommands()\n0x5645da309f93  facade::Connection::SquashPipeline()\n0x5645da30aebf  facade::Connection::AsyncFiber()\n```\n\nsomeone seems like \"REPLICAOF  \" has been called within a transaction. Now  ReplicaOfInternal grabs a replica mutex, which may explain why we see other stacks trying to grab the mutex and being stuck.\nI do not know yet why Drakarys is stuck - have not went beyond this but I guess  \"replicaof\" inside transaction is not something we covered\n\nThis command is probably run by sentinel if it detects a problem, the sentinel log is in the file, in case it helps....\nThe question is why does sentinel detect that a node is not available....\nCorrelating, from the last file that I sent you, the sentinel.log with the INFO, the replica command is executed when sentinel detects that the node from which I have taken the log is \"dead\"\n\n**sentinel.log**\n`2179:X 26 Feb 2025 15:42:13.115 # -sdown slave 172.16.0.25:6392 172.16.0.25 6392 @ ovh-innova 172.16.0.24 6392\n2179:X 26 Feb 2025 15:42:41.669 # +sdown master ovh-innova 172.16.0.24 6392\n2179:X 26 Feb 2025 15:42:43.215 * Sentinel new configuration saved on disk\n2179:X 26 Feb 2025 15:42:43.215 # +new-epoch 482\n2179:X 26 Feb 2025 15:42:43.217 * Sentinel new configuration saved on disk\n2179:X 26 Feb 2025 15:42:43.217 # +vote-for-leader 48b3e88d373c0ab3dd8c048a6b35b5fea263e9b3 482\n2179:X 26 Feb 2025 15:42:43.382 # -sdown master ovh-innova 172.16.0.24 6392\n2179:X 26 Feb 2025 15:42:44.214 # +config-update-from sentinel 48b3e88d373c0ab3dd8c048a6b35b5fea263e9b3 172.16.0.23 26392 @ ovh-innova 172.16.0.24 6392\n`\n\n\n**dragonfly.ovh-innova.dfly.log.INFO.20250226-082351.2182.log**\n`I20250226 15:42:22.154214  5696 server_family.cc:2216] GetMetrics took 75 ms, out of which callback took 75 ms\nI20250226 15:42:23.080855  5652 server_family.cc:2216] GetMetrics took 79 ms, out of which callback took 79 ms\nI20250226 15:42:23.145159  5653 dflycmd.cc:647] Registered replica 172.16.0.25:6392\nI20250226 15:42:24.071637  5653 dflycmd.cc:345] Started sync with replica 172.16.0.25:6392\nI20250226 15:42:24.079833  5652 server_family.cc:2216] GetMetrics took 79 ms, out of which callback took 79 ms\nI20250226 15:42:24.940400  5651 server_family.cc:2216] GetMetrics took 50 ms, out of which callback took 50 ms\nI20250226 15:42:25.079320  5652 server_family.cc:2216] GetMetrics took 78 ms, out of which callback took 78 ms\n`\n\nThen it doesn't seem like that's the problem, you see a change in the replica because sentinel detects that the node is not responding\n@romange I think I reproduced. \n\nStart 2 instances of dragonfly manually. Then use this:\n\n```\nasync def test_bug():\n    async def push_pipeline(size=1):\n        bad_actor_client = aioredis.Redis()\n        p = bad_actor_client.pipeline(transaction=True)\n        for i in range(size):\n            p.execute_command(\"get x\")\n        p.execute_command(\"replicaof localhost 6380\")\n        await p.execute()\n\n    lst = [] \n    for i in range(0, 30): \n       lst.append(asyncio.create_task(push_pipeline(5)))\n\n    for i in lst: \n        await i\n~                \n```\n\nIt will get stack. Interestingly enough, `debug stacktrace` does not print anything. Connecting with redis-cli works but when I try to run a command it says that we are loading a dataset in memory. I suspect it's the deadlock\n\nMy two cents are `stub transactions` (pipelines) mess with Drakarrays which starts a transaction on its own under the stub. I can take a look and fix this but I got limited bandwidth for the rest of the day.\n\nAlso dragonfly won't shut down meaning we deadlocked. \n\nI will probably be able to supply a fix tomorrow.\nI do not know if this is it but it's indeed buggy. and you can even relax pipeline with as non-transactional:\n```py\nasync def test_bug():\n    async def push_pipeline(size=1):\n        bad_actor_client = aioredis.Redis()\n        p = bad_actor_client.pipeline(transaction=True)\n        for i in range(size):\n            p.execute_command(\"get x\")\n        p.execute_command(\"replicaof localhost 6380\")\n        await p.execute()\n\n    lst = [] \n    for i in range(0, 30): \n       lst.append(asyncio.create_task(push_pipeline(5)))\n\n    for i in lst: \n        await i\n```\n\nit crashed for me in debug:\n```\nF20250226 20:18:45.836943 40318 fibers.cc:19] Check failed: !IsJoinable() \n*** Check failure stack trace: ***\n    @     0x555556ea2613  google::LogMessage::Fail()\n    @     0x555556ea2559  google::LogMessage::SendToLog()\n    @     0x555556ea1d0c  google::LogMessage::Flush()\n    @     0x555556ea5e04  google::LogMessageFatal::~LogMessageFatal()\n    @     0x555556d93153  util::fb2::Fiber::operator=()\n    @     0x5555565f5015  dfly::Replica::StartMainReplicationFiber()\n    @     0x55555668b223  dfly::ServerFamily::ReplicaOfInternal()\n    @     0x55555668b481  dfly::ServerFamily::ReplicaOf()\n    @     0x55555666ed8c  _ZZN4dfly12_GLOBAL__N_111HandlerFuncEPNS_12ServerFamilyEMS1_FvN4absl12lts_202407224SpanIKSt17basic_string_viewIcSt11char_traitsIcEEEERKNS_14CommandContextEEENKUlSB_SE_E_clESB_SE_\n    @     0x5555566bde79  _ZN3fu27abi_4006detail10invocation6invokeIRKZN4dfly12_GLOBAL__N_111HandlerFuncEPNS4_12ServerFamilyEMS6_FvN4absl12lts_202407224SpanIKSt17basic_string_viewIcSt11char_traitsIcEEEERKNS4_14CommandContextEEEUlSG_SJ_E_JSG_SJ_EEEDTclcl7forwardIT_Efp_Espcl7forwardIT0_Efp0_EEEOSP_DpOSQ_\n    @     0x5555566b5df5  _ZN3fu27abi_4006detail12type_erasure16invocation_table14function_traitIKFvN4absl12lts_202407224SpanIKSt17basic_string_viewIcSt11char_traitsIcEEEERKN4dfly14CommandContextEEE16internal_invokerINS2_3boxILb1EZNSE_12_GLOBAL__N_111HandlerFuncEPNSE_12ServerFamilyEMSN_FvSD_SH_EEUlSD_SH_E_SaISR_EEELb0EE6invokeEPKNS2_13data_accessorEmSD_SH_\n    @     0x5555569f46d5  _ZNK3fu27abi_4006detail12type_erasure6tables6vtableINS1_8propertyILb0ELb0EJKFvN4absl12lts_202407224SpanIKSt17basic_string_viewIcSt11char_traitsIcEEEERKN4dfly14CommandContextEEEEEE6invokeILm0EJPKNS2_13data_accessorERKmSE_SI_EEEDcDpOT0_\n    @     0x5555569f476e  _ZN3fu27abi_4006detail12type_erasure7erasureILb1ENS1_6configILb1ELb1ENS_16capacity_defaultEEENS1_8propertyILb0ELb0EJKFvN4absl12lts_202407224SpanIKSt17basic_string_viewIcSt11char_traitsIcEEEERKN4dfly14CommandContextEEEEEE6invokeILm0ERKSN_JSG_SK_EEEDcOT0_DpOT1_\n    @     0x5555569f47ec  fu2::abi_400::detail::type_erasure::invocation_table::operator_impl<>::operator()()\n    @     0x5555569f125f  dfly::CommandId::Invoke()\n    @     0x55555650e2ec  dfly::Service::InvokeCmd()\n    @     0x55555650d2cf  dfly::Service::DispatchCommand()\n    @     0x555556c1a4ac  facade::Connection::AsyncOperations::operator()()\n    @     0x555556c3dc04  facade::Connection::AsyncOperations::operator()<>()\n\n```\nin other occurrance it crashed for me in this stack:\n```\nF20250226 20:14:27.895853 37879 fiber_interface.cc:425] Fibers belong to different schedulers\n*** Check failure stack trace: ***\n    @     0x5a1ac1251613  google::LogMessage::Fail()\n    @     0x5a1ac1251559  google::LogMessage::SendToLog()\n    @     0x5a1ac1250d0c  google::LogMessage::Flush()\n    @     0x5a1ac1250a87  google::LogMessage::~LogMessage()\n    @     0x5a1ac1176648  util::fb2::detail::ActivateSameThread()\n    @     0x5a1ac11f1159  _ZZN4util3fb29FiberCallC4EPNS0_13UringProactorEjENKUlPNS0_6detail14FiberInterfaceEijjE_clES6_ijj\n    @     0x5a1ac11f4916  _ZN3fu27abi_4006detail10invocation6invokeIRZN4util3fb29FiberCallC4EPNS5_13UringProactorEjEUlPNS5_6detail14FiberInterfaceEijjE_JSB_ijjEEEDTclcl7forwardIT_Efp_Espcl7forwardIT0_Efp0_EEEOSE_DpOSF_\n    @     0x5a1ac11f4538  _ZN3fu27abi_4006detail12type_erasure16invocation_table14function_traitIFvPN4util3fb26detail14FiberInterfaceEijjEE16internal_invokerINS2_3boxILb0EZNS6_9FiberCallC4EPNS6_13UringProactorEjEUlS9_ijjE_SaISH_EEELb1EE6invokeEPNS2_13data_accessorEmS9_ijj\n    @     0x5a1ac11f5c51  _ZNK3fu27abi_4006detail12type_erasure6tables6vtableINS1_8propertyILb0ELb0EJFvPN4util3fb26detail14FiberInterfaceEijjEEEEE6invokeILm0EJPNS2_13data_accessorERKmSA_ijjEEEDcDpOT0_\n    @     0x5a1ac11f5d1e  _ZN3fu27abi_4006detail12type_erasure7erasureILb1ENS1_6configILb1ELb0ENS_14capacity_fixedILm16ELm8EEEEENS1_8propertyILb0ELb0EJFvPN4util3fb26detail14FiberInterfaceEijjEEEEE6invokeILm0ERSG_JSD_ijjEEEDcOT0_DpOT1_\n    @     0x5a1ac11f5dc2  fu2::abi_400::detail::type_erasure::invocation_table::operator_impl<>::operator()()\n    @     0x5a1ac11e999a  util::fb2::UringProactor::ProcessCqeBatch()\n    @     0x5a1ac11e9faf  util::fb2::UringProactor::ReapCompletions()\n    @     0x5a1ac11ee738  util::fb2::UringProactor::MainLoop()\n    @     0x5a1ac114577d  util::fb2::ProactorDispatcher::Run()\n    @     0x5a1ac1169a19  util::fb2::detail::(anonymous namespace)::DispatcherImpl::Run()\n    @     0x5a1ac1169545  _ZZN4util3fb26detail12_GLOBAL__N_114DispatcherImplC4ERKN5boost7context12preallocatedEONS5_21basic_fixedsize_stackINS5_12stack_traitsEEEPNS1_9SchedulerEENKUlONS5_5fiberEE_clESG_\n    @     0x5a1ac1170158  _ZSt13__invoke_implIN5boost7context5fiberERZN4util3fb26detail12_GLOBAL__N_114DispatcherImplC4ERKNS1_12preallocatedEONS1_21basic_fixedsize_stackINS1_12stack_traitsEEEPNS5_9SchedulerEEUlOS2_E_JS2_EET_St14__invoke_otherOT0_DpOT1_\n    @     0x5a1ac117007a  _ZSt8__invokeIRZN4util3fb26detail12_GLOBAL__N_114DispatcherImplC4ERKN5boost7context12preallocatedEONS6_21basic_fixedsize_stackINS6_12stack_traitsEEEPNS2_9SchedulerEEUlONS6_5fiberEE_JSG_EENSt15__invoke_resultIT_JDpT0_EE4typeEOSL_DpOSM_\n    @     0x5a1ac116febb  _ZSt6invokeIRZN4util3fb26detail12_GLOBAL__N_114DispatcherImplC4ERKN5boost7context12preallocatedEONS6_21basic_fixedsize_stackINS6_12stack_traitsEEEPNS2_9SchedulerEEUlONS6_5fiberEE_JSG_EENSt13invoke_resultIT_JDpT0_EE4typeEOSL_DpOSM_\n    @     0x5a1ac116fc02  _ZN5boost7context6detail12fiber_recordINS0_5fiberERNS0_21basic_fixedsize_stackINS0_12stack_traitsEEEZN4util3fb26detail12_GLOBAL__N_114DispatcherImplC4ERKNS0_12preallocatedEOS6_PNSA_9SchedulerEEUlOS3_E_E3runEPv\n    @     0x5a1ac116f824  _ZN5boost7context6detail11fiber_entryINS1_12fiber_recordINS0_5fiberERNS0_21basic_fixedsize_stackINS0_12stack_traitsEEEZN4util3fb26detail12_GLOBAL__N_114DispatcherImplC4ERKNS0_12preallocatedEOS7_PNSB_9SchedulerEEUlOS4_E_EEEEvNS1_10transfer_tE\n    @     0x7d338acd81f7  make_fcontext\n*** SIGABRT received at time=1740593667 on cpu 2 ***\nPC: @     0x7d3389e9eb1c  (unknown)  pthread_kill\n    @     0x5a1ac12d9492         64  absl::lts_20240722::WriteFailureInfo()\n    @     0x5a1ac12d970e         96  absl::lts_20240722::AbslFailureSignalHandler()\n    @     0x7d3389e45320       3152  (unknown)\n    @     0x7d3389e4526e         32  raise\n    @     0x7d3389e288ff        192  abort\n    @     0x5a1ac125c43e        176  google::DumpStackTraceAndExit()\n    @     0x5a1ac1251613         16  google::LogMessage::Fail()\n    @     0x5a1ac1251559        160  google::LogMessage::SendToLog()\n    @     0x5a1ac1250d0c         80  google::LogMessage::Flush()\n    @     0x5a1ac1250a87         48  google::LogMessage::~LogMessage()\n    @     0x5a1ac1176648        160  util::fb2::detail::ActivateSameThread()\n    @     0x5a1ac11f1159        192  util::fb2::FiberCall::FiberCall()::{lambda()#1}::operator()()\n    @     0x5a1ac11f4916         96  fu2::abi_400::detail::invocation::invoke<>()\n    @     0x5a1ac11f4538        112  fu2::abi_400::detail::type_erasure::invocation_table::function_trait<>::internal_invoker<>::invoke()\n    @     0x5a1ac11f5c51        144  fu2::abi_400::detail::type_erasure::tables::vtable<>::invoke<>()\n    @     0x5a1ac11f5d1e        160  fu2::abi_400::detail::type_erasure::erasure<>::invoke<>()\n    @     0x5a1ac11f5dc2         96  fu2::abi_400::detail::type_erasure::invocation_table::operator_impl<>::operator()()\n    @     0x5a1ac11e999a        288  util::fb2::UringProactor::ProcessCqeBatch()\n    @     0x5a1ac11e9faf         96  util::fb2::UringProactor::ReapCompletions()\n    @     0x5a1ac11ee738       1664  util::fb2::UringProactor::MainLoop()\n\n```\nGood job finding a bug @kostasrim !\nI created a branch and added the assertion that catches that we write into a socket from a foreign thread.\nhttps://github.com/dragonflydb/dragonfly/tree/Issue4663\nIn my case only once has redis-cli worked for me and I get the same message \"dataset is loading...\" in the rest of the cases I have had to connect with telnet\n@romange This code (replicaof command) changed today as there was a bug introduced 2 days ago. This crashes you see are related to this bug. Please dont investigate them, please take latest main version.\nI checked sentinel code and they do use multi exec to run replicaof command but the commands in the multi exec are not transactional and therefor the flow there is ok.\nWe do need to fix the multi exec with the get commands but I dont think this is the issue here.\n\n> I checked sentinel code and they do use multi exec to run replicaof command but the commands in the multi exec are not transactional and therefor the flow there is ok. We do need to fix the multi exec with the get commands but I dont think this is the issue here.\n\nreplace `get` with `dbsize` which is not transactional and you will get the same deadlock. The problem is not running transactional commands (get) and replicaof within multi. My suspicion is drakarrays + stubs that is causing this. \n\n+1 for verifying the multi/exec of sentinel\noh also possible related -- I just saw Roman's branch.   `DCHECK(sock_->proactor() == ProactorBase::me());` \ud83e\udd14 \nI think the most puzzling is that we have ScanGeneric commands being stuck - they are not transactional so should not deadlock. Maybe they are not stuck and we just have the snapshot of them being in the middle. I will think if we can improve our stacktrace output to provide the idle information as well.\n\n@fernandomacho  please try runnning your servers with `--pipeline_squash=0 --multi_exec_squash=false` and tell us if deadlocks reproduce.\n\nIn case they do, please also run `redis-cli -3 DEBUG TX` and send us the output. \n\nthis is the minimal example that causes a deadlock for me - do not know if it's related to this issue or not.\nfyi, we prohibit flushdb inside multi but try to execute it as part of `replicaof` anyways.\n\n```py\nasync def test_issue4663():\n    async def push_pipeline(size):\n        bad_actor_client = aioredis.Redis()\n        p = bad_actor_client.pipeline(transaction=True)\n        for i in range(size):\n            p.execute_command(\"type\", \"foo\")\n        p.execute_command(\"replicaof localhost 6380\")\n        await p.execute()\n\n    lst = [] \n    for i in range(0, 5): \n       lst.append(asyncio.create_task(push_pipeline(5)))\n\n    for i in lst: \n        await i\n```\n> --pipeline_squash=0 --multi_exec_squash=false\n\nHi, I config this params only on two servers (now are replica servers). Unable stop current master now.\n> In case they do, please also run `redis-cli -3 DEBUG TX` and send us the output.\n\nremember that when server is hung unable use redis-cli only I can use telnet to execute commands.\nY try to connect with RESP3 protocol if hung... else I try to execute this command via telnet\nyou can use `HELLO 3` via telnet. it's also interesting that `redis-cli` does not work, it sends `COMMAND DOCS` but it should not hung.\n> > --pipeline_squash=0 --multi_exec_squash=false\n> \n> Hi, I config this params only on two servers (now are replica servers). Unable stop current master now.\n\nthat's fine\nHi, one server started with --pipeline_squash=0 --multi_exec_squash=false is hung\n\nthe output for **DEBUG TX** is:\n\ndebug tx\n$3303\nshard0:\n  tx armed 4, total: 5,global:0,runnable:4\n  locks total:4,contended:0\n  max contention score: 0,lock_name:140221823877920\nshard1:\n  tx armed 4, total: 5,global:0,runnable:2\n  locks total:3,contended:1\n  max contention score: 257,lock_name:3197088069226786117\nshard2:\n  tx armed 1, total: 2,global:0,runnable:1\n  locks total:1,contended:0\n  max contention score: 0,lock_name:140221555442464\nshard3:\n  tx armed 3, total: 4,global:0,runnable:3\n  locks total:3,contended:0\n  max contention score: 0,lock_name:140221622551328\nshard4:\n  tx armed 8, total: 9,global:0,runnable:8\n  locks total:7,contended:0\n  max contention score: 0,lock_name:140221421224736\nshard5:\n  tx armed 4, total: 5,global:0,runnable:4\n  locks total:4,contended:0\n  max contention score: 0,lock_name:140221287007008\nshard6:\n  tx armed 4, total: 5,global:0,runnable:4\n  locks total:4,contended:0\n  max contention score: 0,lock_name:140221488333600\nshard7:\n  tx armed 5, total: 6,global:0,runnable:5\n  locks total:5,contended:0\n  max contention score: 0,lock_name:140221219898144\nshard8:\n  tx armed 3, total: 4,global:0,runnable:3\n  locks total:3,contended:0\n  max contention score: 0,lock_name:140221287040544\nshard9:\n  tx armed 3, total: 4,global:0,runnable:3\n  locks total:3,contended:0\n  max contention score: 0,lock_name:93988859086624\nshard10:\n  tx armed 2, total: 3,global:0,runnable:2\n  locks total:2,contended:0\n  max contention score: 0,lock_name:140221823911456\nshard11:\n  tx armed 4, total: 5,global:0,runnable:4\n  locks total:4,contended:0\n  max contention score: 0,lock_name:140221219931680\nshard12:\n  tx armed 1, total: 2,global:0,runnable:1\n  locks total:1,contended:0\n  max contention score: 0,lock_name:140221287074080\nshard13:\n  tx armed 1, total: 2,global:0,runnable:1\n  locks total:1,contended:0\n  max contention score: 0,lock_name:140221421258272\nshard14:\n  tx armed 3, total: 4,global:0,runnable:3\n  locks total:3,contended:0\n  max contention score: 0,lock_name:140221488367136\nshard15:\n  tx armed 5, total: 6,global:0,runnable:2\n  locks total:3,contended:1\n  max contention score: 768,lock_name:7574730457940617528\nshard16:\n  tx armed 1, total: 2,global:0,runnable:1\n  locks total:1,contended:0\n  max contention score: 0,lock_name:93988859122208\nshard17:\n  tx armed 2, total: 3,global:0,runnable:2\n  locks total:2,contended:0\n  max contention score: 0,lock_name:140221689693728\nshard18:\n  tx armed 13, total: 14,global:0,runnable:13\n  locks total:1,contended:0\n  max contention score: 0,lock_name:140221555476000\nshard19:\n  tx armed 4, total: 5,global:0,runnable:4\n  locks total:4,contended:0\n  max contention score: 0,lock_name:140221219965216\nshard20:\n  tx armed 3, total: 4,global:0,runnable:3\n  locks total:3,contended:0\n  max contention score: 0,lock_name:140221488400672\nshard21:\n  tx armed 0, total: 1,global:0,runnable:0\n  locks total:0,contended:0\n  max contention score: 0,lock_name:140221555509536\nshard22:\n  tx armed 3, total: 4,global:0,runnable:3\n  locks total:3,contended:0\n  max contention score: 0,lock_name:140221689727264\nshard23:\n  tx armed 5, total: 6,global:0,runnable:5\n  locks total:5,contended:0\n  max contention score: 0,lock_name:140221421291808\nshard24:\n  tx armed 1, total: 2,global:0,runnable:1\n  locks total:1,contended:0\n  max contention score: 0,lock_name:140221622618400\n\nNeed DEBUG STACKTRACE??\nno, thanks. we will need to improve some of our debugging code and release a new binary I think unless @adiholden  will have more ideas. Please remind me do you use containers or native binaries?\nNative binaries\n@fernandomacho  how many sentinels do you use for managing your cluster?\nUpdate regarding Kostas example (yet to see if it's the same bug but I suspect it is):\n1. We send several concurrent \"REPLICAOF\" commands wrapped within a transaction.\n2. The first transaction is executing, runs REPLICAOF which spawns `MainReplicationFb` fiber\n3. The first transaction ends, MainReplicationFb progresses.\n4. Meanwhile more global transactions are queued up in the transaction queue. The next one calls REPLICAOF that tries to Stop the current replication.\n5. The first MainReplicationFb calls Drakarys, that creates a FLUSHALL global transaction that is queued last\n6. It waits for it to end but before it we have other global transactions waiting for the transaction lock held by MainReplicationFb that is stuck on Drakarys. \n\nI must say that this scenario is VERY close to what we see in @fernandomacho stacktraces.\n> [@fernandomacho](https://github.com/fernandomacho) how many sentinels do you use for managing your cluster?\n\n3 one instance on each node.\nSentinel need update local config for instance....\nIf it helps, when the node hangs it starts saving the snapshots out of place, in my case in /root and they are empty...\n> Update regarding Kostas example (yet to see if it's the same bug but I suspect it is):\n> \n> 1. We send several concurrent \"REPLICAOF\" commands wrapped within a transaction.\n> 2. The first transaction is executing, runs REPLICAOF which spawns `MainReplicationFb` fiber\n> 3. The first transaction ends, MainReplicationFb progresses.\n> 4. Meanwhile more global transactions are queued up in the transaction queue. The next one calls REPLICAOF that tries to Stop the current replication.\n> 5. The first MainReplicationFb calls Drakarys, that creates a FLUSHALL global transaction that is queued last\n> 6. It waits for it to end but before it we have other global transactions waiting for the transaction lock held by MainReplicationFb that is stuck on Drakarys.\n> \n> I must say that this scenario is VERY close to what we see in [@fernandomacho](https://github.com/fernandomacho) stacktraces.\n\nI wrote the code based on the stack traces and that's why I said I think it's related. I have now a little bit free bandwidth so I can actually look on this\n1. REPLICA OF finishes, spawns `MainReplicationFb`\n2. Next `replicaof` command calls `replica_->Stop();` that waits for the previous MainReplicationFb to finish.\n   This replicaof command is at the head of the TX queue.\n3. MainReplicationFb from (1) calls FlushALL that is scheduled after (2).\n\nAnd we have a deadlock. I do not see a clean fix here, only workarounds. \nWe will discuss our options next week.\nHello, let's see if I understood correctly.\nThe problem is that a REPLICAOF is received and it starts to execute, but before it finishes it receives another REPLICAOF (with the same parameters as the one in the main thread), as the one in the main thread executes a FLUSH ALL at the end it cannot execute it because the second REPLICAOF is at the first in the queue.\n\nA simple solution would not be: if I have a REPLICAOF in the main thread (with its host and port or NO ONE parameters) and without it having finished executing it receives exactly the same command as the one being executed, ignore this second REPLICAOF?\nThis does not seem to be a problem because if there were no blocks preventing execution, the result of the REPLICAOF that is being executed will be exactly the same as that of the second REPLICAOF.\n@fernandomacho  it's a hacky fix but I think it's a good short term solution. We will probably do something along these lines if we won't think of anything better before 1.28. \nNice ;)",
  "created_at": "2025-03-03T12:04:37Z",
  "modified_files": [
    "src/server/main_service.cc"
  ],
  "modified_test_files": [
    "tests/dragonfly/replication_test.py"
  ]
}