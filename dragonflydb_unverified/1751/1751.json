{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 1751,
  "instance_id": "dragonflydb__dragonfly-1751",
  "issue_numbers": [
    "1700"
  ],
  "base_commit": "343b3ef8003a95cb74089b2535977c39622bde3d",
  "patch": "diff --git a/src/facade/error.h b/src/facade/error.h\nindex 1b34f44b95cb..7f863687a5c7 100644\n--- a/src/facade/error.h\n+++ b/src/facade/error.h\n@@ -27,11 +27,13 @@ extern const char kInvalidDbIndErr[];\n extern const char kScriptNotFound[];\n extern const char kAuthRejected[];\n extern const char kExpiryOutOfRange[];\n-extern const char kSyntaxErrType[];\n-extern const char kScriptErrType[];\n extern const char kIndexOutOfRange[];\n extern const char kOutOfMemory[];\n extern const char kInvalidNumericResult[];\n extern const char kClusterNotConfigured[];\n \n+extern const char kSyntaxErrType[];\n+extern const char kScriptErrType[];\n+extern const char kConfigErrType[];\n+\n }  // namespace facade\ndiff --git a/src/facade/facade.cc b/src/facade/facade.cc\nindex 5f62bc1e7f1d..16651eae6525 100644\n--- a/src/facade/facade.cc\n+++ b/src/facade/facade.cc\n@@ -79,13 +79,15 @@ const char kInvalidDbIndErr[] = \"invalid DB index\";\n const char kScriptNotFound[] = \"-NOSCRIPT No matching script. Please use EVAL.\";\n const char kAuthRejected[] = \"-WRONGPASS invalid username-password pair or user is disabled.\";\n const char kExpiryOutOfRange[] = \"expiry is out of range\";\n-const char kSyntaxErrType[] = \"syntax_error\";\n-const char kScriptErrType[] = \"script_error\";\n const char kIndexOutOfRange[] = \"index out of range\";\n const char kOutOfMemory[] = \"Out of memory\";\n const char kInvalidNumericResult[] = \"result is not a number\";\n const char kClusterNotConfigured[] = \"Cluster is not yet configured\";\n \n+const char kSyntaxErrType[] = \"syntax_error\";\n+const char kScriptErrType[] = \"script_error\";\n+const char kConfigErrType[] = \"config_error\";\n+\n const char* RespExpr::TypeName(Type t) {\n   switch (t) {\n     case STRING:\ndiff --git a/src/server/config_registry.cc b/src/server/config_registry.cc\nindex a7d65bd3b701..49ab7e8acbef 100644\n--- a/src/server/config_registry.cc\n+++ b/src/server/config_registry.cc\n@@ -7,40 +7,34 @@\n \n #include \"base/logging.h\"\n \n+extern \"C\" {\n+#include \"redis/util.h\"\n+}\n+\n namespace dfly {\n \n using namespace std;\n \n-ConfigRegistry& ConfigRegistry::Register(std::string_view name, WriteCb cb) {\n-  absl::CommandLineFlag* flag = absl::FindCommandLineFlag(name);\n-  CHECK(flag) << \"Unknown config name: \" << name;\n-\n-  unique_lock lk(mu_);\n-  auto [it, inserted] = registry_.emplace(name, std::move(cb));\n-  CHECK(inserted) << \"Duplicate config name: \" << name;\n-  return *this;\n-}\n-\n-ConfigRegistry& ConfigRegistry::Register(std::string_view name) {\n-  return Register(name, [](const absl::CommandLineFlag& flag) { return true; });\n-}\n-\n // Returns true if the value was updated.\n-bool ConfigRegistry::Set(std::string_view config_name, std::string_view value) {\n+auto ConfigRegistry::Set(std::string_view config_name, std::string_view value) -> SetResult {\n   unique_lock lk(mu_);\n   auto it = registry_.find(config_name);\n   if (it == registry_.end())\n-    return false;\n-  auto cb = it->second;\n+    return SetResult::UNKNOWN;\n+  if (!it->second.is_mutable)\n+    return SetResult::READONLY;\n+\n+  auto cb = it->second.cb;\n   lk.unlock();\n \n   absl::CommandLineFlag* flag = absl::FindCommandLineFlag(config_name);\n   CHECK(flag);\n   string error;\n   if (!flag->ParseFrom(value, &error))\n-    return false;\n+    return SetResult::INVALID;\n \n-  return cb(*flag);\n+  bool success = !cb || cb(*flag);\n+  return success ? SetResult::OK : SetResult::INVALID;\n }\n \n std::optional<std::string> ConfigRegistry::Get(std::string_view config_name) {\n@@ -59,6 +53,25 @@ void ConfigRegistry::Reset() {\n   registry_.clear();\n }\n \n+vector<string> ConfigRegistry::List(string_view glob) const {\n+  vector<string> res;\n+  unique_lock lk(mu_);\n+  for (const auto& [name, _] : registry_) {\n+    if (stringmatchlen(glob.data(), glob.size(), name.data(), name.size(), 1))\n+      res.push_back(name);\n+  }\n+  return res;\n+}\n+\n+void ConfigRegistry::RegisterInternal(std::string_view name, bool is_mutable, WriteCb cb) {\n+  absl::CommandLineFlag* flag = absl::FindCommandLineFlag(name);\n+  CHECK(flag) << \"Unknown config name: \" << name;\n+\n+  unique_lock lk(mu_);\n+  auto [it, inserted] = registry_.emplace(name, Entry{std::move(cb), is_mutable});\n+  CHECK(inserted) << \"Duplicate config name: \" << name;\n+}\n+\n ConfigRegistry config_registry;\n \n }  // namespace dfly\ndiff --git a/src/server/config_registry.h b/src/server/config_registry.h\nindex ab72eee0eb13..d017bf72ffcd 100644\n--- a/src/server/config_registry.h\n+++ b/src/server/config_registry.h\n@@ -14,19 +14,43 @@ class ConfigRegistry {\n   // Accepts the new value as argument. Return true if config was successfully updated.\n   using WriteCb = std::function<bool(const absl::CommandLineFlag&)>;\n \n-  ConfigRegistry& Register(std::string_view name, WriteCb cb);\n-  ConfigRegistry& Register(std::string_view name);\n+  ConfigRegistry& Register(std::string_view name) {\n+    RegisterInternal(name, false, {});\n+    return *this;\n+  }\n+\n+  ConfigRegistry& RegisterMutable(std::string_view name, WriteCb cb = {}) {\n+    RegisterInternal(name, true, std::move(cb));\n+    return *this;\n+  }\n+\n+  enum class SetResult : uint8_t {\n+    OK,\n+    UNKNOWN,\n+    READONLY,\n+    INVALID,\n+  };\n \n   // Returns true if the value was updated.\n-  bool Set(std::string_view config_name, std::string_view value);\n+  SetResult Set(std::string_view config_name, std::string_view value);\n \n   std::optional<std::string> Get(std::string_view config_name);\n \n   void Reset();\n \n+  std::vector<std::string> List(std::string_view glob) const;\n+\n  private:\n-  util::fb2::Mutex mu_;\n-  absl::flat_hash_map<std::string, WriteCb> registry_ ABSL_GUARDED_BY(mu_);\n+  void RegisterInternal(std::string_view name, bool is_mutable, WriteCb cb);\n+\n+  mutable util::fb2::Mutex mu_;\n+\n+  struct Entry {\n+    WriteCb cb;\n+    bool is_mutable;\n+  };\n+\n+  absl::flat_hash_map<std::string, Entry> registry_ ABSL_GUARDED_BY(mu_);\n };\n \n extern ConfigRegistry config_registry;\ndiff --git a/src/server/main_service.cc b/src/server/main_service.cc\nindex ec3af2b910b1..1b83f4af21fb 100644\n--- a/src/server/main_service.cc\n+++ b/src/server/main_service.cc\n@@ -637,7 +637,7 @@ void Service::Init(util::AcceptServer* acceptor, std::vector<facade::Listener*>\n                    const InitOpts& opts) {\n   InitRedisTables();\n \n-  config_registry.Register(\"maxmemory\", [](const absl::CommandLineFlag& flag) {\n+  config_registry.RegisterMutable(\"maxmemory\", [](const absl::CommandLineFlag& flag) {\n     auto res = flag.TryGet<MaxMemoryFlag>();\n     if (!res)\n       return false;\n@@ -646,10 +646,12 @@ void Service::Init(util::AcceptServer* acceptor, std::vector<facade::Listener*>\n     return true;\n   });\n \n-  config_registry.Register(\"dir\");\n-  config_registry.Register(\"requirepass\");\n-  config_registry.Register(\"masterauth\");\n-  config_registry.Register(\"tcp_keepalive\");\n+  config_registry.Register(\"dbnum\");       // equivalent to databases in redis.\n+  config_registry.RegisterMutable(\"dir\");  // TODO: to add validation for dir\n+  config_registry.RegisterMutable(\"requirepass\");\n+  config_registry.RegisterMutable(\"masterauth\");\n+  config_registry.RegisterMutable(\"tcp_keepalive\");\n+\n   acl::UserRegistry* reg = &user_registry_;\n   pp_.Await([reg](uint32_t index, ProactorBase* pb) { ServerState::Init(index, reg); });\n \ndiff --git a/src/server/server_family.cc b/src/server/server_family.cc\nindex 584659e30963..73e3cc9b8441 100644\n--- a/src/server/server_family.cc\n+++ b/src/server/server_family.cc\n@@ -915,7 +915,7 @@ void ServerFamily::Init(util::AcceptServer* acceptor, std::vector<facade::Listen\n   dfly_cmd_ = make_unique<DflyCmd>(this);\n \n   SetMaxClients(listeners_, absl::GetFlag(FLAGS_maxclients));\n-  config_registry.Register(\"maxclients\", [this](const absl::CommandLineFlag& flag) {\n+  config_registry.RegisterMutable(\"maxclients\", [this](const absl::CommandLineFlag& flag) {\n     auto res = flag.TryGet<uint32_t>();\n     if (res.has_value())\n       SetMaxClients(listeners_, res.value());\n@@ -1624,29 +1624,47 @@ void ServerFamily::Config(CmdArgList args, ConnectionContext* cntx) {\n     }\n \n     ToLower(&args[1]);\n-    string_view config_name = ArgS(args, 1);\n-    bool success = config_registry.Set(config_name, ArgS(args, 2));\n-    if (success) {\n-      return (*cntx)->SendOk();\n-    } else {\n-      return (*cntx)->SendError(ConfigSetFailed(config_name), kSyntaxErrType);\n+    string_view param = ArgS(args, 1);\n+\n+    ConfigRegistry::SetResult result = config_registry.Set(param, ArgS(args, 2));\n+\n+    const char kErrPrefix[] = \"CONFIG SET failed (possibly related to argument '\";\n+    switch (result) {\n+      case ConfigRegistry::SetResult::OK:\n+        return (*cntx)->SendOk();\n+      case ConfigRegistry::SetResult::UNKNOWN:\n+        return (*cntx)->SendError(\n+            absl::StrCat(\"Unknown option or number of arguments for CONFIG SET - '\", param, \"'\"),\n+            kConfigErrType);\n+\n+      case ConfigRegistry::SetResult::READONLY:\n+        return (*cntx)->SendError(\n+            absl::StrCat(kErrPrefix, param, \"') - can't set immutable config\"), kConfigErrType);\n+\n+      case ConfigRegistry::SetResult::INVALID:\n+        return (*cntx)->SendError(absl::StrCat(kErrPrefix, param, \"') - argument can not be set\"),\n+                                  kConfigErrType);\n     }\n-  } else if (sub_cmd == \"GET\" && args.size() == 2) {\n+    ABSL_UNREACHABLE();\n+  }\n+\n+  if (sub_cmd == \"GET\" && args.size() == 2) {\n     // Send empty response, like Redis does, unless the param is supported\n-    std::vector<std::string> res;\n \n     string_view param = ArgS(args, 1);\n-    if (param == \"databases\") {\n-      res.emplace_back(param);\n-      res.push_back(absl::StrCat(absl::GetFlag(FLAGS_dbnum)));\n-    } else if (auto value_from_registry = config_registry.Get(param);\n-               value_from_registry.has_value()) {\n-      res.emplace_back(param);\n-      res.push_back(*value_from_registry);\n+    vector<string> names = config_registry.List(param);\n+    vector<string> res;\n+\n+    for (const auto& name : names) {\n+      absl::CommandLineFlag* flag = CHECK_NOTNULL(absl::FindCommandLineFlag(name));\n+      res.push_back(name);\n+      res.push_back(flag->CurrentValue());\n     }\n \n     return (*cntx)->SendStringArr(res, RedisReplyBuilder::MAP);\n-  } else if (sub_cmd == \"RESETSTAT\") {\n+  }\n+\n+  if (sub_cmd == \"RESETSTAT\") {\n     shard_set->pool()->Await([registry = service_.mutable_registry()](unsigned index, auto*) {\n       registry->ResetCallStats(index);\n       auto& sstate = *ServerState::tlocal();\ndiff --git a/src/server/server_family.cc.orig b/src/server/server_family.cc.orig\nnew file mode 100644\nindex 000000000000..5a77424862a3\n--- /dev/null\n+++ b/src/server/server_family.cc.orig\n@@ -0,0 +1,2560 @@\n+// Copyright 2022, DragonflyDB authors.  All rights reserved.\n+// See LICENSE for licensing terms.\n+//\n+\n+#include \"server/server_family.h\"\n+\n+#include <absl/cleanup/cleanup.h>\n+#include <absl/random/random.h>  // for master_id_ generation.\n+#include <absl/strings/match.h>\n+#include <absl/strings/str_join.h>\n+#include <absl/strings/str_replace.h>\n+#include <absl/strings/strip.h>\n+#include <sys/resource.h>\n+\n+#include <algorithm>\n+#include <chrono>\n+#include <filesystem>\n+#include <optional>\n+\n+extern \"C\" {\n+#include \"redis/redis_aux.h\"\n+}\n+\n+#include \"base/flags.h\"\n+#include \"base/logging.h\"\n+#include \"croncpp.h\"  // cron::cronexpr\n+#include \"facade/dragonfly_connection.h\"\n+#include \"facade/reply_builder.h\"\n+#include \"io/file_util.h\"\n+#include \"io/proc_reader.h\"\n+#include \"search/doc_index.h\"\n+#include \"server/acl/acl_commands_def.h\"\n+#include \"server/command_registry.h\"\n+#include \"server/conn_context.h\"\n+#include \"server/debugcmd.h\"\n+#include \"server/dflycmd.h\"\n+#include \"server/engine_shard_set.h\"\n+#include \"server/error.h\"\n+#include \"server/generic_family.h\"\n+#include \"server/journal/journal.h\"\n+#include \"server/main_service.h\"\n+#include \"server/memory_cmd.h\"\n+#include \"server/protocol_client.h\"\n+#include \"server/rdb_load.h\"\n+#include \"server/rdb_save.h\"\n+#include \"server/script_mgr.h\"\n+#include \"server/server_state.h\"\n+#include \"server/tiered_storage.h\"\n+#include \"server/transaction.h\"\n+#include \"server/version.h\"\n+#include \"strings/human_readable.h\"\n+#include \"util/accept_server.h\"\n+#include \"util/cloud/aws.h\"\n+#include \"util/cloud/s3.h\"\n+#include \"util/fibers/fiber_file.h\"\n+#include \"util/uring/uring_file.h\"\n+\n+using namespace std;\n+\n+struct ReplicaOfFlag {\n+  string host;\n+  string port;\n+\n+  bool has_value() const {\n+    return !host.empty() && !port.empty();\n+  }\n+};\n+\n+static bool AbslParseFlag(std::string_view in, ReplicaOfFlag* flag, std::string* err);\n+static std::string AbslUnparseFlag(const ReplicaOfFlag& flag);\n+\n+ABSL_FLAG(string, dir, \"\", \"working directory\");\n+ABSL_FLAG(string, dbfilename, \"dump-{timestamp}\", \"the filename to save/load the DB\");\n+ABSL_FLAG(string, requirepass, \"\",\n+          \"password for AUTH authentication. \"\n+          \"If empty can also be set with DFLY_PASSWORD environment variable.\");\n+ABSL_FLAG(uint32_t, maxclients, 64000, \"Maximum number of concurrent clients allowed.\");\n+\n+ABSL_FLAG(string, save_schedule, \"\",\n+          \"glob spec for the UTC time to save a snapshot which matches HH:MM 24h time\");\n+ABSL_FLAG(string, snapshot_cron, \"\",\n+          \"cron expression for the time to save a snapshot, crontab style\");\n+ABSL_FLAG(bool, df_snapshot_format, true,\n+          \"if true, save in dragonfly-specific snapshotting format\");\n+ABSL_FLAG(int, epoll_file_threads, 0,\n+          \"thread size for file workers when running in epoll mode, default is hardware concurrent \"\n+          \"threads\");\n+ABSL_FLAG(ReplicaOfFlag, replicaof, ReplicaOfFlag{},\n+          \"Specifies a host and port which point to a target master \"\n+          \"to replicate. \"\n+          \"Format should be <IPv4>:<PORT> or host:<PORT> or [<IPv6>]:<PORT>\");\n+\n+ABSL_DECLARE_FLAG(uint32_t, port);\n+ABSL_DECLARE_FLAG(bool, cache_mode);\n+ABSL_DECLARE_FLAG(uint32_t, hz);\n+ABSL_DECLARE_FLAG(bool, tls);\n+ABSL_DECLARE_FLAG(string, tls_ca_cert_file);\n+ABSL_DECLARE_FLAG(string, tls_ca_cert_dir);\n+\n+bool AbslParseFlag(std::string_view in, ReplicaOfFlag* flag, std::string* err) {\n+#define RETURN_ON_ERROR(cond, m)                                           \\\n+  do {                                                                     \\\n+    if ((cond)) {                                                          \\\n+      *err = m;                                                            \\\n+      LOG(WARNING) << \"Error in parsing arguments for --replicaof: \" << m; \\\n+      return false;                                                        \\\n+    }                                                                      \\\n+  } while (0)\n+\n+  if (in.empty()) {  // on empty flag \"parse\" nothing. If we return false then DF exists.\n+    *flag = ReplicaOfFlag{};\n+    return true;\n+  }\n+\n+  auto pos = in.find_last_of(':');\n+  RETURN_ON_ERROR(pos == string::npos, \"missing ':'.\");\n+\n+  string_view ip = in.substr(0, pos);\n+  flag->port = in.substr(pos + 1);\n+\n+  RETURN_ON_ERROR(ip.empty() || flag->port.empty(), \"IP/host or port are empty.\");\n+\n+  // For IPv6: ip1.front == '[' AND ip1.back == ']'\n+  // For IPv4: ip1.front != '[' AND ip1.back != ']'\n+  // Together, this ip1.front == '[' iff ip1.back == ']', which can be implemented as XNOR (NOT XOR)\n+  RETURN_ON_ERROR(((ip.front() == '[') ^ (ip.back() == ']')), \"unclosed brackets.\");\n+\n+  if (ip.front() == '[') {\n+    // shortest possible IPv6 is '::1' (loopback)\n+    RETURN_ON_ERROR(ip.length() <= 2, \"IPv6 host name is too short\");\n+\n+    flag->host = ip.substr(1, ip.length() - 2);\n+    VLOG(1) << \"received IP of type IPv6: \" << flag->host;\n+  } else {\n+    flag->host = ip;\n+    VLOG(1) << \"received IP of type IPv4 (or a host): \" << flag->host;\n+  }\n+\n+  VLOG(1) << \"--replicaof: Received \" << flag->host << \" :  \" << flag->port;\n+  return true;\n+#undef RETURN_ON_ERROR\n+}\n+\n+std::string AbslUnparseFlag(const ReplicaOfFlag& flag) {\n+  return (flag.has_value()) ? absl::StrCat(flag.host, \":\", flag.port) : \"\";\n+}\n+\n+namespace dfly {\n+\n+namespace fs = std::filesystem;\n+\n+using absl::GetFlag;\n+using absl::StrCat;\n+using namespace facade;\n+using namespace util;\n+using http::StringResponse;\n+using strings::HumanReadableNumBytes;\n+\n+namespace {\n+\n+const auto kRedisVersion = \"6.2.11\";\n+constexpr string_view kS3Prefix = \"s3://\"sv;\n+\n+const auto kRdbWriteFlags = O_CREAT | O_WRONLY | O_TRUNC | O_CLOEXEC | O_DIRECT;\n+const size_t kBucketConnectMs = 2000;\n+\n+using EngineFunc = void (ServerFamily::*)(CmdArgList args, ConnectionContext* cntx);\n+\n+inline CommandId::Handler HandlerFunc(ServerFamily* se, EngineFunc f) {\n+  return [=](CmdArgList args, ConnectionContext* cntx) { return (se->*f)(args, cntx); };\n+}\n+\n+using CI = CommandId;\n+\n+// Create a direc\n+error_code CreateDirs(fs::path dir_path) {\n+  error_code ec;\n+  fs::file_status dir_status = fs::status(dir_path, ec);\n+  if (ec == errc::no_such_file_or_directory) {\n+    fs::create_directories(dir_path, ec);\n+    if (!ec)\n+      dir_status = fs::status(dir_path, ec);\n+  }\n+  return ec;\n+}\n+\n+string UnknownCmd(string cmd, CmdArgList args) {\n+  return absl::StrCat(\"unknown command '\", cmd, \"' with args beginning with: \",\n+                      StrJoin(args.begin(), args.end(), \", \", CmdArgListFormatter()));\n+}\n+\n+void SubstituteFilenameTsPlaceholder(fs::path* filename, std::string_view replacement) {\n+  *filename = absl::StrReplaceAll(filename->string(), {{\"{timestamp}\", replacement}});\n+}\n+\n+bool IsCloudPath(string_view path) {\n+  return absl::StartsWith(path, kS3Prefix);\n+}\n+\n+// Returns bucket_name, obj_path for an s3 path.\n+optional<pair<string, string>> GetBucketPath(string_view path) {\n+  string_view clean = absl::StripPrefix(path, kS3Prefix);\n+\n+  size_t pos = clean.find('/');\n+  if (pos == string_view::npos)\n+    return nullopt;\n+\n+  string bucket_name{clean.substr(0, pos)};\n+  string obj_path{clean.substr(pos + 1)};\n+  return make_pair(move(bucket_name), move(obj_path));\n+}\n+\n+string InferLoadFile(string_view dir, cloud::AWS* aws) {\n+  fs::path data_folder;\n+  string bucket_name, obj_path;\n+\n+  if (dir.empty()) {\n+    data_folder = fs::current_path();\n+  } else {\n+    if (IsCloudPath(dir)) {\n+      CHECK(aws);\n+      auto res = GetBucketPath(dir);\n+      if (!res) {\n+        LOG(ERROR) << \"Invalid S3 path: \" << dir;\n+        return {};\n+      }\n+      data_folder = dir;\n+      bucket_name = res->first;\n+      obj_path = res->second;\n+    } else {\n+      error_code file_ec;\n+      data_folder = fs::canonical(dir, file_ec);\n+      if (file_ec) {\n+        LOG(ERROR) << \"Data directory error: \" << file_ec.message() << \" for dir \" << dir;\n+        return {};\n+      }\n+    }\n+  }\n+\n+  LOG(INFO) << \"Data directory is \" << data_folder;\n+\n+  const auto& dbname = GetFlag(FLAGS_dbfilename);\n+  if (dbname.empty())\n+    return string{};\n+\n+  if (IsCloudPath(dir)) {\n+    cloud::S3Bucket bucket(*aws, bucket_name);\n+    ProactorBase* proactor = shard_set->pool()->GetNextProactor();\n+    auto ec = proactor->Await([&] { return bucket.Connect(kBucketConnectMs); });\n+    if (ec) {\n+      LOG(ERROR) << \"Couldn't connect to S3 bucket: \" << ec.message();\n+      return {};\n+    }\n+\n+    fs::path fl_path{obj_path};\n+    fl_path.append(dbname);\n+\n+    LOG(INFO) << \"Loading from s3 path s3://\" << bucket_name << \"/\" << fl_path;\n+    // TODO: to load from S3 file.\n+    return {};\n+  }\n+\n+  fs::path fl_path = data_folder.append(dbname);\n+  if (fs::exists(fl_path))\n+    return fl_path.generic_string();\n+\n+  SubstituteFilenameTsPlaceholder(&fl_path, \"*\");\n+  if (!fl_path.has_extension()) {\n+    fl_path += \"*\";\n+  }\n+  io::Result<io::StatShortVec> short_vec = io::StatFiles(fl_path.generic_string());\n+\n+  if (short_vec) {\n+    // io::StatFiles returns a list of sorted files. Because our timestamp format has the same\n+    // time order and lexicographic order we iterate from the end to find the latest snapshot.\n+    auto it = std::find_if(short_vec->rbegin(), short_vec->rend(), [](const auto& stat) {\n+      return absl::EndsWith(stat.name, \".rdb\") || absl::EndsWith(stat.name, \"summary.dfs\");\n+    });\n+    if (it != short_vec->rend())\n+      return it->name;\n+  } else {\n+    LOG(WARNING) << \"Could not stat \" << fl_path << \", error \" << short_vec.error().message();\n+  }\n+  return string{};\n+}\n+\n+bool IsValidSaveScheduleNibble(string_view time, unsigned int max) {\n+  /*\n+   * a nibble is valid iff there exists one time that matches the pattern\n+   * and that time is <= max. For any wildcard the minimum value is 0.\n+   * Therefore the minimum time the pattern can match is the time with\n+   * all *s replaced with 0s. If this time is > max all other times that\n+   * match the pattern are > max and the pattern is invalid. Otherwise\n+   * there exists at least one valid nibble specified by this pattern\n+   *\n+   * Note the edge case of \"*\" is equivalent to \"**\". While using this\n+   * approach \"*\" and \"**\" both map to 0.\n+   */\n+  unsigned int min_match = 0;\n+  for (size_t i = 0; i < time.size(); ++i) {\n+    // check for valid characters\n+    if (time[i] != '*' && (time[i] < '0' || time[i] > '9')) {\n+      return false;\n+    }\n+    min_match *= 10;\n+    min_match += time[i] == '*' ? 0 : time[i] - '0';\n+  }\n+\n+  return min_match <= max;\n+}\n+\n+// takes ownership over the file.\n+class LinuxWriteWrapper : public io::Sink {\n+ public:\n+  LinuxWriteWrapper(LinuxFile* lf) : lf_(lf) {\n+  }\n+\n+  io::Result<size_t> WriteSome(const iovec* v, uint32_t len) final;\n+\n+  error_code Close() {\n+    return lf_->Close();\n+  }\n+\n+ private:\n+  unique_ptr<LinuxFile> lf_;\n+  off_t offset_ = 0;\n+};\n+\n+class RdbSnapshot {\n+ public:\n+  RdbSnapshot(FiberQueueThreadPool* fq_tp, cloud::AWS* aws) : fq_tp_{fq_tp}, aws_{aws} {\n+  }\n+\n+  GenericError Start(SaveMode save_mode, const string& path, const RdbSaver::GlobalData& glob_data);\n+  void StartInShard(EngineShard* shard);\n+\n+  error_code SaveBody();\n+  error_code Close();\n+\n+  const RdbTypeFreqMap freq_map() const {\n+    return freq_map_;\n+  }\n+\n+  bool HasStarted() const {\n+    return started_ || (saver_ && saver_->Mode() == SaveMode::SUMMARY);\n+  }\n+\n+ private:\n+  bool started_ = false;\n+  bool is_linux_file_ = false;\n+  FiberQueueThreadPool* fq_tp_ = nullptr;\n+  cloud::AWS* aws_ = nullptr;\n+\n+  unique_ptr<io::Sink> io_sink_;\n+  unique_ptr<RdbSaver> saver_;\n+  RdbTypeFreqMap freq_map_;\n+\n+  Cancellation cll_{};\n+};\n+\n+io::Result<size_t> LinuxWriteWrapper::WriteSome(const iovec* v, uint32_t len) {\n+  io::Result<size_t> res = lf_->WriteSome(v, len, offset_, 0);\n+  if (res) {\n+    offset_ += *res;\n+  }\n+\n+  return res;\n+}\n+\n+GenericError RdbSnapshot::Start(SaveMode save_mode, const std::string& path,\n+                                const RdbSaver::GlobalData& glob_data) {\n+  bool is_direct = false;\n+  VLOG(1) << \"Saving RDB \" << path;\n+\n+  if (IsCloudPath(path)) {\n+    DCHECK(aws_);\n+\n+    optional<pair<string, string>> bucket_path = GetBucketPath(path);\n+    if (!bucket_path) {\n+      return GenericError(\"Invalid S3 path\");\n+    }\n+    auto [bucket_name, obj_path] = *bucket_path;\n+\n+    cloud::S3Bucket bucket(*aws_, bucket_name);\n+    error_code ec = bucket.Connect(kBucketConnectMs);\n+    if (ec) {\n+      return GenericError(ec, \"Couldn't connect to S3 bucket\");\n+    }\n+    auto res = bucket.OpenWriteFile(obj_path);\n+    if (!res) {\n+      return GenericError(res.error(), \"Couldn't open file for writing\");\n+    }\n+    io_sink_.reset(*res);\n+  } else {\n+    if (fq_tp_) {  // EPOLL\n+      auto res = util::OpenFiberWriteFile(path, fq_tp_);\n+      if (!res)\n+        return GenericError(res.error(), \"Couldn't open file for writing\");\n+      io_sink_.reset(*res);\n+    } else {\n+      auto res = OpenLinux(path, kRdbWriteFlags, 0666);\n+      if (!res) {\n+        return GenericError(\n+            res.error(),\n+            \"Couldn't open file for writing (is direct I/O supported by the file system?)\");\n+      }\n+      is_linux_file_ = true;\n+      io_sink_.reset(new LinuxWriteWrapper(res->release()));\n+      is_direct = kRdbWriteFlags & O_DIRECT;\n+    }\n+  }\n+\n+  saver_.reset(new RdbSaver(io_sink_.get(), save_mode, is_direct));\n+\n+  return saver_->SaveHeader(move(glob_data));\n+}\n+\n+error_code RdbSnapshot::SaveBody() {\n+  return saver_->SaveBody(&cll_, &freq_map_);\n+}\n+\n+error_code RdbSnapshot::Close() {\n+  if (is_linux_file_) {\n+    return static_cast<LinuxWriteWrapper*>(io_sink_.get())->Close();\n+  }\n+  return static_cast<io::WriteFile*>(io_sink_.get())->Close();\n+}\n+\n+void RdbSnapshot::StartInShard(EngineShard* shard) {\n+  saver_->StartSnapshotInShard(false, &cll_, shard);\n+  started_ = true;\n+}\n+\n+string FormatTs(absl::Time now) {\n+  return absl::FormatTime(\"%Y-%m-%dT%H:%M:%S\", now, absl::LocalTimeZone());\n+}\n+\n+// modifies 'filename' to be \"filename-postfix.extension\"\n+void SetExtension(absl::AlphaNum postfix, string_view extension, fs::path* filename) {\n+  filename->replace_extension();  // clear if exists\n+  *filename += StrCat(\"-\", postfix, extension);\n+}\n+\n+void ExtendDfsFilenameWithShard(int shard, string_view extension, fs::path* filename) {\n+  // dragonfly snapshot.\n+  SetExtension(absl::Dec(shard, absl::kZeroPad4), extension, filename);\n+}\n+\n+GenericError ValidateFilename(const fs::path& filename, bool new_version) {\n+  bool is_cloud_path = IsCloudPath(filename.string());\n+\n+  if (!filename.parent_path().empty() && !is_cloud_path) {\n+    return {absl::StrCat(\"filename may not contain directory separators (Got \\\"\", filename.c_str(),\n+                         \"\\\"). dbfilename should specify the filename without the directory\")};\n+  }\n+\n+  if (!filename.has_extension()) {\n+    return {};\n+  }\n+\n+  if (new_version) {\n+    if (absl::EqualsIgnoreCase(filename.extension().c_str(), \".rdb\")) {\n+      return {absl::StrCat(\n+          \"DF snapshot format is used but '.rdb' extension was given. Use --nodf_snapshot_format \"\n+          \"or remove the filename extension.\")};\n+    } else {\n+      return {absl::StrCat(\"DF snapshot format requires no filename extension. Got \\\"\",\n+                           filename.extension().c_str(), \"\\\"\")};\n+    }\n+  }\n+  if (!new_version && !absl::EqualsIgnoreCase(filename.extension().c_str(), \".rdb\")) {\n+    return {absl::StrCat(\"Bad filename extension \\\"\", filename.extension().c_str(),\n+                         \"\\\" for SAVE with type RDB\")};\n+  }\n+  return {};\n+}\n+\n+void SlowLog(CmdArgList args, ConnectionContext* cntx) {\n+  ToUpper(&args[0]);\n+  string_view sub_cmd = ArgS(args, 0);\n+\n+  if (sub_cmd == \"LEN\") {\n+    return (*cntx)->SendLong(0);\n+  }\n+\n+  if (sub_cmd == \"GET\") {\n+    return (*cntx)->SendEmptyArray();\n+  }\n+\n+  (*cntx)->SendError(UnknownSubCmd(sub_cmd, \"SLOWLOG\"), kSyntaxErrType);\n+}\n+\n+// Check that if TLS is used at least one form of client authentication is\n+// enabled. That means either using a password or giving a root\n+// certificate for authenticating client certificates which will\n+// be required.\n+void ValidateServerTlsFlags() {\n+  if (!absl::GetFlag(FLAGS_tls)) {\n+    return;\n+  }\n+\n+  bool has_auth = false;\n+\n+  if (!dfly::GetPassword().empty()) {\n+    has_auth = true;\n+  }\n+\n+  if (!(absl::GetFlag(FLAGS_tls_ca_cert_file).empty() &&\n+        absl::GetFlag(FLAGS_tls_ca_cert_dir).empty())) {\n+    has_auth = true;\n+  }\n+\n+  if (!has_auth) {\n+    LOG(ERROR) << \"TLS configured but no authentication method is used!\";\n+    exit(1);\n+  }\n+}\n+\n+bool IsReplicatingNoOne(string_view host, string_view port) {\n+  return absl::EqualsIgnoreCase(host, \"no\") && absl::EqualsIgnoreCase(port, \"one\");\n+}\n+\n+struct SaveStagesInputs {\n+  bool use_dfs_format_;\n+  string_view basename_;\n+  Transaction* trans_;\n+  Service* service_;\n+  atomic_bool* is_saving_;\n+  FiberQueueThreadPool* fq_threadpool_;\n+  shared_ptr<LastSaveInfo>* last_save_info_;\n+  Mutex* save_mu_;\n+  unique_ptr<cloud::AWS>* aws_;\n+};\n+\n+struct SaveStagesController : public SaveStagesInputs {\n+  SaveStagesController(SaveStagesInputs&& inputs) : SaveStagesInputs{move(inputs)} {\n+    start_time_ = absl::Now();\n+  }\n+\n+  ~SaveStagesController() {\n+    service_->SwitchState(GlobalState::SAVING, GlobalState::ACTIVE);\n+  }\n+\n+  GenericError Save() {\n+    if (auto err = BuildFullPath(); err)\n+      return err;\n+\n+    if (auto err = SwitchState(); err)\n+      return err;\n+\n+    if (auto err = InitResources(); err)\n+      return err;\n+\n+    // The stages below report errors to shared_err_\n+    if (use_dfs_format_)\n+      SaveDfs();\n+    else\n+      SaveRdb();\n+\n+    is_saving_->store(true, memory_order_relaxed);\n+    RunStage(&SaveStagesController::SaveCb);\n+    is_saving_->store(false, memory_order_relaxed);\n+\n+    RunStage(&SaveStagesController::CloseCb);\n+\n+    FinalizeFileMovement();\n+\n+    if (!shared_err_)\n+      UpdateSaveInfo();\n+\n+    return *shared_err_;\n+  }\n+\n+ private:\n+  // In the new version (.dfs) we store a file for every shard and one more summary file.\n+  // Summary file is always last in snapshots array.\n+  void SaveDfs() {\n+    // Extend all filenames with -{sid} or -summary and append .dfs.tmp\n+    const string_view ext = is_cloud_ ? \".dfs\" : \".dfs.tmp\";\n+    ShardId sid = 0;\n+    for (auto& [_, filename] : snapshots_) {\n+      filename = full_path_;\n+      if (sid < shard_set->size())\n+        ExtendDfsFilenameWithShard(sid++, ext, &filename);\n+      else\n+        SetExtension(\"summary\", ext, &filename);\n+    }\n+\n+    // Save summary file.\n+    SaveDfsSingle(nullptr);\n+\n+    // Save shard files.\n+    auto cb = [this](Transaction* t, EngineShard* shard) {\n+      SaveDfsSingle(shard);\n+      return OpStatus::OK;\n+    };\n+    trans_->ScheduleSingleHop(std::move(cb));\n+  }\n+\n+  // Start saving a dfs file on shard\n+  void SaveDfsSingle(EngineShard* shard) {\n+    // for summary file, shard=null and index=shard_set->size(), see SaveDfs() above\n+    auto& [snapshot, filename] = snapshots_[shard ? shard->shard_id() : shard_set->size()];\n+\n+    SaveMode mode = shard == nullptr ? SaveMode::SUMMARY : SaveMode::SINGLE_SHARD;\n+    auto glob_data = shard == nullptr ? GetGlobalData() : RdbSaver::GlobalData{};\n+\n+    if (auto err = snapshot->Start(mode, filename, glob_data); err) {\n+      shared_err_ = err;\n+      snapshot.reset();\n+      return;\n+    }\n+\n+    if (mode == SaveMode::SINGLE_SHARD)\n+      snapshot->StartInShard(shard);\n+  }\n+\n+  // Save a single rdb file\n+  void SaveRdb() {\n+    auto& [snapshot, filename] = snapshots_.front();\n+\n+    filename = full_path_;\n+    if (!filename.has_extension())\n+      filename += \".rdb\";\n+    if (!is_cloud_)\n+      filename += \".tmp\";\n+\n+    if (auto err = snapshot->Start(SaveMode::RDB, filename, GetGlobalData()); err) {\n+      snapshot.reset();\n+      return;\n+    }\n+\n+    auto cb = [snapshot = snapshot.get()](Transaction* t, EngineShard* shard) {\n+      snapshot->StartInShard(shard);\n+      return OpStatus::OK;\n+    };\n+    trans_->ScheduleSingleHop(std::move(cb));\n+  }\n+\n+  void UpdateSaveInfo() {\n+    fs::path resulting_path = full_path_;\n+    if (use_dfs_format_)\n+      SetExtension(\"summary\", \".dfs\", &resulting_path);\n+    else\n+      resulting_path.replace_extension();  // remove .tmp\n+\n+    double seconds = double(absl::ToInt64Milliseconds(absl::Now() - start_time_)) / 1000;\n+    LOG(INFO) << \"Saving \" << resulting_path << \" finished after \"\n+              << strings::HumanReadableElapsedTime(seconds);\n+\n+    auto save_info = make_shared<LastSaveInfo>();\n+    for (const auto& k_v : rdb_name_map_) {\n+      save_info->freq_map.emplace_back(k_v);\n+    }\n+    save_info->save_time = absl::ToUnixSeconds(start_time_);\n+    save_info->file_name = resulting_path.generic_string();\n+    save_info->duration_sec = uint32_t(seconds);\n+\n+    lock_guard lk{*save_mu_};\n+    last_save_info_->swap(save_info);  // swap - to deallocate the old version outstide of the lock.\n+  }\n+\n+  GenericError InitResources() {\n+    if (is_cloud_ && !aws_) {\n+      *aws_ = make_unique<cloud::AWS>(\"s3\");\n+      if (auto ec = aws_->get()->Init(); ec) {\n+        aws_->reset();\n+        return {ec, \"Couldn't initialize AWS\"};\n+      }\n+    }\n+\n+    snapshots_.resize(use_dfs_format_ ? shard_set->size() + 1 : 1);\n+    for (auto& [snapshot, _] : snapshots_)\n+      snapshot = make_unique<RdbSnapshot>(fq_threadpool_, aws_->get());\n+    return {};\n+  }\n+\n+  // Remove .tmp extension or delete files in case of error\n+  void FinalizeFileMovement() {\n+    if (is_cloud_)\n+      return;\n+\n+    // If the shared_err is set, the snapshot saving failed\n+    bool has_error = bool(shared_err_);\n+\n+    for (const auto& [_, filename] : snapshots_) {\n+      if (has_error)\n+        filesystem::remove(filename);\n+      else\n+        filesystem::rename(filename, fs::path{filename}.replace_extension(\"\"));\n+    }\n+  }\n+\n+  // Build full path: get dir, try creating dirs, get filename with placeholder\n+  GenericError BuildFullPath() {\n+    fs::path dir_path = GetFlag(FLAGS_dir);\n+    if (!dir_path.empty()) {\n+      if (auto ec = CreateDirs(dir_path); ec)\n+        return {ec, \"Failed to create directories\"};\n+    }\n+\n+    fs::path filename = basename_.empty() ? GetFlag(FLAGS_dbfilename) : basename_;\n+    if (auto err = ValidateFilename(filename, use_dfs_format_); err)\n+      return err;\n+\n+    SubstituteFilenameTsPlaceholder(&filename, FormatTs(start_time_));\n+    full_path_ = dir_path / filename;\n+    is_cloud_ = IsCloudPath(full_path_.string());\n+    return {};\n+  }\n+\n+  // Switch to saving state if in active state\n+  GenericError SwitchState() {\n+    GlobalState new_state = service_->SwitchState(GlobalState::ACTIVE, GlobalState::SAVING);\n+    if (new_state != GlobalState::SAVING && new_state != GlobalState::TAKEN_OVER)\n+      return {make_error_code(errc::operation_in_progress),\n+              StrCat(GlobalStateName(new_state), \" - can not save database\")};\n+    return {};\n+  }\n+\n+  void SaveCb(unsigned index) {\n+    if (auto& snapshot = snapshots_[index].first; snapshot && snapshot->HasStarted())\n+      shared_err_ = snapshot->SaveBody();\n+  }\n+\n+  void CloseCb(unsigned index) {\n+    if (auto& snapshot = snapshots_[index].first; snapshot) {\n+      shared_err_ = snapshot->Close();\n+\n+      lock_guard lk{rdb_name_map_mu_};\n+      for (const auto& k_v : snapshot->freq_map())\n+        rdb_name_map_[RdbTypeName(k_v.first)] += k_v.second;\n+    }\n+\n+    if (auto* es = EngineShard::tlocal(); use_dfs_format_ && es)\n+      es->db_slice().ResetUpdateEvents();\n+  }\n+\n+  void RunStage(void (SaveStagesController::*cb)(unsigned)) {\n+    if (use_dfs_format_) {\n+      shard_set->RunBlockingInParallel([&](EngineShard* es) { (this->*cb)(es->shard_id()); });\n+      (this->*cb)(shard_set->size());\n+    } else {\n+      (this->*cb)(0);\n+    }\n+  }\n+\n+  RdbSaver::GlobalData GetGlobalData() const {\n+    StringVec script_bodies, search_indices;\n+\n+    {\n+      auto scripts = service_->script_mgr()->GetAll();\n+      script_bodies.reserve(scripts.size());\n+      for (auto& [sha, data] : scripts)\n+        script_bodies.push_back(move(data.body));\n+    }\n+\n+    {\n+      shard_set->Await(0, [&] {\n+        auto* indices = EngineShard::tlocal()->search_indices();\n+        for (auto index_name : indices->GetIndexNames()) {\n+          auto index_info = indices->GetIndex(index_name)->GetInfo();\n+          search_indices.emplace_back(\n+              absl::StrCat(index_name, \" \", index_info.BuildRestoreCommand()));\n+        }\n+      });\n+    }\n+\n+    return RdbSaver::GlobalData{move(script_bodies), move(search_indices)};\n+  }\n+\n+ private:\n+  absl::Time start_time_;\n+  fs::path full_path_;\n+  bool is_cloud_;\n+\n+  AggregateGenericError shared_err_;\n+  vector<pair<unique_ptr<RdbSnapshot>, fs::path>> snapshots_;\n+\n+  absl::flat_hash_map<string_view, size_t> rdb_name_map_;\n+  Mutex rdb_name_map_mu_;\n+};\n+\n+void RebuildAllSearchIndices(Service* service) {\n+  boost::intrusive_ptr<Transaction> trans{new Transaction{service->FindCmd(\"FT.CREATE\")}};\n+  trans->InitByArgs(0, {});\n+  trans->ScheduleSingleHop([](auto* trans, auto* es) {\n+    es->search_indices()->RebuildAllIndices(trans->GetOpArgs(es));\n+    return OpStatus::OK;\n+  });\n+}\n+\n+}  // namespace\n+\n+std::optional<SnapshotSpec> ParseSaveSchedule(string_view time) {\n+  if (time.length() < 3 || time.length() > 5) {\n+    return std::nullopt;\n+  }\n+\n+  size_t separator_idx = time.find(':');\n+  // the time cannot start with ':' and it must be present in the first 3 characters of any time\n+  if (separator_idx == 0 || separator_idx >= 3) {\n+    return std::nullopt;\n+  }\n+\n+  SnapshotSpec spec{string(time.substr(0, separator_idx)), string(time.substr(separator_idx + 1))};\n+  // a minute should be 2 digits as it is zero padded, unless it is a '*' in which case this\n+  // greedily can make up both digits\n+  if (spec.minute_spec != \"*\" && spec.minute_spec.length() != 2) {\n+    return std::nullopt;\n+  }\n+\n+  return IsValidSaveScheduleNibble(spec.hour_spec, 23) &&\n+                 IsValidSaveScheduleNibble(spec.minute_spec, 59)\n+             ? std::optional<SnapshotSpec>(spec)\n+             : std::nullopt;\n+}\n+\n+bool DoesTimeNibbleMatchSpecifier(string_view time_spec, unsigned int current_time) {\n+  // single greedy wildcard matches everything\n+  if (time_spec == \"*\") {\n+    return true;\n+  }\n+\n+  for (int i = time_spec.length() - 1; i >= 0; --i) {\n+    // if the current digit is not a wildcard and it does not match the digit in the current time it\n+    // does not match\n+    if (time_spec[i] != '*' && int(current_time % 10) != (time_spec[i] - '0')) {\n+      return false;\n+    }\n+    current_time /= 10;\n+  }\n+\n+  return current_time == 0;\n+}\n+\n+bool DoesTimeMatchSpecifier(const SnapshotSpec& spec, time_t now) {\n+  unsigned hour = (now / 3600) % 24;\n+  unsigned min = (now / 60) % 60;\n+  return DoesTimeNibbleMatchSpecifier(spec.hour_spec, hour) &&\n+         DoesTimeNibbleMatchSpecifier(spec.minute_spec, min);\n+}\n+\n+std::optional<cron::cronexpr> InferSnapshotCronExpr() {\n+  string save_time = GetFlag(FLAGS_save_schedule);\n+  string snapshot_cron_exp = GetFlag(FLAGS_snapshot_cron);\n+\n+  if (!snapshot_cron_exp.empty() && !save_time.empty()) {\n+    LOG(ERROR) << \"snapshot_cron and save_schedule flags should not be set simultaneously\";\n+    quick_exit(1);\n+  }\n+\n+  string raw_cron_expr;\n+  if (!save_time.empty()) {\n+    std::optional<SnapshotSpec> spec = ParseSaveSchedule(save_time);\n+\n+    if (spec) {\n+      // Setting snapshot to HH:mm everyday, as specified by `save_schedule` flag\n+      raw_cron_expr = \"0 \" + spec.value().minute_spec + \" \" + spec.value().hour_spec + \" * * *\";\n+    } else {\n+      LOG(WARNING) << \"Invalid snapshot time specifier \" << save_time;\n+    }\n+  } else if (!snapshot_cron_exp.empty()) {\n+    raw_cron_expr = \"0 \" + snapshot_cron_exp;\n+  }\n+\n+  if (!raw_cron_expr.empty()) {\n+    try {\n+      return std::optional<cron::cronexpr>(cron::make_cron(raw_cron_expr));\n+    } catch (const cron::bad_cronexpr& ex) {\n+      LOG(WARNING) << \"Invalid cron expression: \" << ex.what();\n+    }\n+  }\n+  return std::nullopt;\n+}\n+\n+ServerFamily::ServerFamily(Service* service) : service_(*service) {\n+  start_time_ = time(NULL);\n+  last_save_info_ = make_shared<LastSaveInfo>();\n+  last_save_info_->save_time = start_time_;\n+  script_mgr_.reset(new ScriptMgr());\n+  journal_.reset(new journal::Journal());\n+\n+  {\n+    absl::InsecureBitGen eng;\n+    master_id_ = GetRandomHex(eng, CONFIG_RUN_ID_SIZE);\n+    DCHECK_EQ(CONFIG_RUN_ID_SIZE, master_id_.size());\n+  }\n+\n+  if (auto ec = ValidateFilename(GetFlag(FLAGS_dbfilename), GetFlag(FLAGS_df_snapshot_format));\n+      ec) {\n+    LOG(ERROR) << ec.Format();\n+    exit(1);\n+  }\n+\n+  ValidateServerTlsFlags();\n+  ValidateClientTlsFlags();\n+}\n+\n+ServerFamily::~ServerFamily() {\n+}\n+\n+void SetMaxClients(std::vector<facade::Listener*>& listeners, uint32_t maxclients) {\n+  for (auto* listener : listeners) {\n+    if (!listener->IsAdminInterface()) {\n+      listener->SetMaxClients(maxclients);\n+    }\n+  }\n+}\n+\n+void ServerFamily::Init(util::AcceptServer* acceptor, std::vector<facade::Listener*> listeners) {\n+  CHECK(acceptor_ == nullptr);\n+  acceptor_ = acceptor;\n+  listeners_ = std::move(listeners);\n+  dfly_cmd_ = make_unique<DflyCmd>(this);\n+\n+  SetMaxClients(listeners_, absl::GetFlag(FLAGS_maxclients));\n+  config_registry.Register(\"maxclients\", [this](const absl::CommandLineFlag& flag) {\n+    auto res = flag.TryGet<uint32_t>();\n+    if (res.has_value())\n+      SetMaxClients(listeners_, res.value());\n+    return res.has_value();\n+  });\n+\n+  pb_task_ = shard_set->pool()->GetNextProactor();\n+  if (pb_task_->GetKind() == ProactorBase::EPOLL) {\n+    fq_threadpool_.reset(new FiberQueueThreadPool(absl::GetFlag(FLAGS_epoll_file_threads)));\n+  }\n+\n+  // Unlike EngineShard::Heartbeat that runs independently in each shard thread,\n+  // this callback runs in a single thread and it aggregates globally stats from all the shards.\n+  auto cache_cb = [] {\n+    uint64_t sum = 0;\n+    const auto& stats = EngineShardSet::GetCachedStats();\n+    for (const auto& s : stats)\n+      sum += s.used_memory.load(memory_order_relaxed);\n+\n+    used_mem_current.store(sum, memory_order_relaxed);\n+\n+    // Single writer, so no races.\n+    if (sum > used_mem_peak.load(memory_order_relaxed))\n+      used_mem_peak.store(sum, memory_order_relaxed);\n+  };\n+\n+  uint32_t cache_hz = max(GetFlag(FLAGS_hz) / 10, 1u);\n+  uint32_t period_ms = max(1u, 1000 / cache_hz);\n+  stats_caching_task_ =\n+      pb_task_->AwaitBrief([&] { return pb_task_->AddPeriodic(period_ms, cache_cb); });\n+\n+  // check for '--replicaof' before loading anything\n+  if (ReplicaOfFlag flag = GetFlag(FLAGS_replicaof); flag.has_value()) {\n+    service_.proactor_pool().GetNextProactor()->Await(\n+        [this, &flag]() { this->Replicate(flag.host, flag.port); });\n+    return;  // DONT load any snapshots\n+  }\n+\n+  string flag_dir = GetFlag(FLAGS_dir);\n+  if (IsCloudPath(flag_dir)) {\n+    aws_ = make_unique<cloud::AWS>(\"s3\");\n+    if (auto ec = aws_->Init(); ec) {\n+      LOG(FATAL) << \"Failed to initialize AWS \" << ec;\n+    }\n+  }\n+\n+  string load_path = InferLoadFile(flag_dir, aws_.get());\n+  if (!load_path.empty()) {\n+    load_result_ = Load(load_path);\n+  }\n+\n+  snapshot_schedule_fb_ =\n+      service_.proactor_pool().GetNextProactor()->LaunchFiber([this] { SnapshotScheduling(); });\n+}\n+\n+void ServerFamily::Shutdown() {\n+  VLOG(1) << \"ServerFamily::Shutdown\";\n+\n+  if (load_result_.valid())\n+    load_result_.wait();\n+\n+  schedule_done_.Notify();\n+  if (snapshot_schedule_fb_.IsJoinable()) {\n+    snapshot_schedule_fb_.Join();\n+  }\n+\n+  if (save_on_shutdown_ && !absl::GetFlag(FLAGS_dbfilename).empty()) {\n+    shard_set->pool()->GetNextProactor()->Await([this] {\n+      GenericError ec = DoSave();\n+      if (ec) {\n+        LOG(WARNING) << \"Failed to perform snapshot \" << ec.Format();\n+      }\n+    });\n+  }\n+\n+  pb_task_->Await([this] {\n+    pb_task_->CancelPeriodic(stats_caching_task_);\n+    stats_caching_task_ = 0;\n+\n+    if (journal_->EnterLameDuck()) {\n+      auto ec = journal_->Close();\n+      LOG_IF(ERROR, ec) << \"Error closing journal \" << ec;\n+    }\n+\n+    unique_lock lk(replicaof_mu_);\n+    if (replica_) {\n+      replica_->Stop();\n+    }\n+\n+    dfly_cmd_->Shutdown();\n+  });\n+}\n+\n+struct AggregateLoadResult {\n+  AggregateError first_error;\n+  std::atomic<size_t> keys_read;\n+};\n+\n+// Load starts as many fibers as there are files to load each one separately.\n+// It starts one more fiber that waits for all load fibers to finish and returns the first\n+// error (if any occured) with a future.\n+Future<std::error_code> ServerFamily::Load(const std::string& load_path) {\n+  if (!(absl::EndsWith(load_path, \".rdb\") || absl::EndsWith(load_path, \"summary.dfs\"))) {\n+    LOG(ERROR) << \"Bad filename extension \\\"\" << load_path << \"\\\"\";\n+    Promise<std::error_code> ec_promise;\n+    ec_promise.set_value(make_error_code(errc::invalid_argument));\n+    return ec_promise.get_future();\n+  }\n+\n+  vector<std::string> paths{{load_path}};\n+\n+  // Collect all other files in case we're loading dfs.\n+  if (absl::EndsWith(load_path, \"summary.dfs\")) {\n+    std::string glob = absl::StrReplaceAll(load_path, {{\"summary\", \"????\"}});\n+    io::Result<io::StatShortVec> files = io::StatFiles(glob);\n+\n+    if (files && files->size() == 0) {\n+      Promise<std::error_code> ec_promise;\n+      ec_promise.set_value(make_error_code(errc::no_such_file_or_directory));\n+      return ec_promise.get_future();\n+    }\n+\n+    for (auto& fstat : *files) {\n+      paths.push_back(std::move(fstat.name));\n+    }\n+  }\n+\n+  // Check all paths are valid.\n+  for (const auto& path : paths) {\n+    error_code ec;\n+    (void)fs::canonical(path, ec);\n+    if (ec) {\n+      LOG(ERROR) << \"Error loading \" << load_path << \" \" << ec.message();\n+      Promise<std::error_code> ec_promise;\n+      ec_promise.set_value(ec);\n+      return ec_promise.get_future();\n+    }\n+  }\n+\n+  LOG(INFO) << \"Loading \" << load_path;\n+\n+  GlobalState new_state = service_.SwitchState(GlobalState::ACTIVE, GlobalState::LOADING);\n+  if (new_state != GlobalState::LOADING) {\n+    LOG(WARNING) << GlobalStateName(new_state) << \" in progress, ignored\";\n+    return {};\n+  }\n+\n+  auto& pool = service_.proactor_pool();\n+\n+  vector<Fiber> load_fibers;\n+  load_fibers.reserve(paths.size());\n+\n+  auto aggregated_result = std::make_shared<AggregateLoadResult>();\n+\n+  for (auto& path : paths) {\n+    // For single file, choose thread that does not handle shards if possible.\n+    // This will balance out the CPU during the load.\n+    ProactorBase* proactor;\n+    if (paths.size() == 1 && shard_count() < pool.size()) {\n+      proactor = pool.at(shard_count());\n+    } else {\n+      proactor = pool.GetNextProactor();\n+    }\n+\n+    auto load_fiber = [this, aggregated_result, path = std::move(path)]() {\n+      auto load_result = LoadRdb(path);\n+      if (load_result.has_value())\n+        aggregated_result->keys_read.fetch_add(*load_result);\n+      else\n+        aggregated_result->first_error = load_result.error();\n+    };\n+    load_fibers.push_back(proactor->LaunchFiber(std::move(load_fiber)));\n+  }\n+\n+  Promise<std::error_code> ec_promise;\n+  Future<std::error_code> ec_future = ec_promise.get_future();\n+\n+  // Run fiber that empties the channel and sets ec_promise.\n+  auto load_join_fiber = [this, aggregated_result, load_fibers = std::move(load_fibers),\n+                          ec_promise = std::move(ec_promise)]() mutable {\n+    for (auto& fiber : load_fibers) {\n+      fiber.Join();\n+    }\n+\n+    RebuildAllSearchIndices(&service_);\n+\n+    LOG(INFO) << \"Load finished, num keys read: \" << aggregated_result->keys_read;\n+    service_.SwitchState(GlobalState::LOADING, GlobalState::ACTIVE);\n+    ec_promise.set_value(*(aggregated_result->first_error));\n+  };\n+  pool.GetNextProactor()->Dispatch(std::move(load_join_fiber));\n+\n+  return ec_future;\n+}\n+\n+void ServerFamily::SnapshotScheduling() {\n+  const std::optional<cron::cronexpr> cron_expr = InferSnapshotCronExpr();\n+  if (!cron_expr) {\n+    return;\n+  }\n+\n+  const auto loading_check_interval = std::chrono::seconds(10);\n+  while (service_.GetGlobalState() == GlobalState::LOADING) {\n+    schedule_done_.WaitFor(loading_check_interval);\n+  }\n+\n+  while (true) {\n+    const std::chrono::time_point now = std::chrono::system_clock::now();\n+    const std::chrono::time_point next = cron::cron_next(cron_expr.value(), now);\n+\n+    if (schedule_done_.WaitFor(next - now)) {\n+      break;\n+    };\n+\n+    GenericError ec = DoSave();\n+    if (ec) {\n+      LOG(WARNING) << \"Failed to perform snapshot \" << ec.Format();\n+    }\n+  }\n+}\n+\n+io::Result<size_t> ServerFamily::LoadRdb(const std::string& rdb_file) {\n+  error_code ec;\n+  io::ReadonlyFileOrError res;\n+\n+  if (fq_threadpool_) {\n+    res = util::OpenFiberReadFile(rdb_file, fq_threadpool_.get());\n+  } else {\n+    res = OpenRead(rdb_file);\n+  }\n+\n+  if (res) {\n+    io::FileSource fs(*res);\n+\n+    RdbLoader loader{&service_};\n+    ec = loader.Load(&fs);\n+    if (!ec) {\n+      VLOG(1) << \"Done loading RDB from \" << rdb_file << \", keys loaded: \" << loader.keys_loaded();\n+      VLOG(1) << \"Loading finished after \" << strings::HumanReadableElapsedTime(loader.load_time());\n+      return loader.keys_loaded();\n+    }\n+  } else {\n+    ec = res.error();\n+  }\n+  return nonstd::make_unexpected(ec);\n+}\n+\n+enum MetricType { COUNTER, GAUGE, SUMMARY, HISTOGRAM };\n+\n+const char* MetricTypeName(MetricType type) {\n+  switch (type) {\n+    case MetricType::COUNTER:\n+      return \"counter\";\n+    case MetricType::GAUGE:\n+      return \"gauge\";\n+    case MetricType::SUMMARY:\n+      return \"summary\";\n+    case MetricType::HISTOGRAM:\n+      return \"histogram\";\n+  }\n+  return \"unknown\";\n+}\n+\n+inline string GetMetricFullName(string_view metric_name) {\n+  return StrCat(\"dragonfly_\", metric_name);\n+}\n+\n+void AppendMetricHeader(string_view metric_name, string_view metric_help, MetricType type,\n+                        string* dest) {\n+  const auto full_metric_name = GetMetricFullName(metric_name);\n+  absl::StrAppend(dest, \"# HELP \", full_metric_name, \" \", metric_help, \"\\n\");\n+  absl::StrAppend(dest, \"# TYPE \", full_metric_name, \" \", MetricTypeName(type), \"\\n\");\n+}\n+\n+void AppendLabelTupple(absl::Span<const string_view> label_names,\n+                       absl::Span<const string_view> label_values, string* dest) {\n+  if (label_names.empty())\n+    return;\n+\n+  absl::StrAppend(dest, \"{\");\n+  for (size_t i = 0; i < label_names.size(); ++i) {\n+    if (i > 0) {\n+      absl::StrAppend(dest, \", \");\n+    }\n+    absl::StrAppend(dest, label_names[i], \"=\\\"\", label_values[i], \"\\\"\");\n+  }\n+\n+  absl::StrAppend(dest, \"}\");\n+}\n+\n+void AppendMetricValue(string_view metric_name, const absl::AlphaNum& value,\n+                       absl::Span<const string_view> label_names,\n+                       absl::Span<const string_view> label_values, string* dest) {\n+  absl::StrAppend(dest, GetMetricFullName(metric_name));\n+  AppendLabelTupple(label_names, label_values, dest);\n+  absl::StrAppend(dest, \" \", value, \"\\n\");\n+}\n+\n+void AppendMetricWithoutLabels(string_view name, string_view help, const absl::AlphaNum& value,\n+                               MetricType type, string* dest) {\n+  AppendMetricHeader(name, help, type, dest);\n+  AppendMetricValue(name, value, {}, {}, dest);\n+}\n+\n+void PrintPrometheusMetrics(const Metrics& m, StringResponse* resp) {\n+  // Server metrics\n+  AppendMetricHeader(\"version\", \"\", MetricType::GAUGE, &resp->body());\n+  AppendMetricValue(\"version\", 1, {\"version\"}, {GetVersion()}, &resp->body());\n+  AppendMetricHeader(\"role\", \"\", MetricType::GAUGE, &resp->body());\n+  AppendMetricValue(\"role\", 1, {\"role\"}, {m.is_master ? \"master\" : \"replica\"}, &resp->body());\n+  AppendMetricWithoutLabels(\"uptime_in_seconds\", \"\", m.uptime, MetricType::GAUGE, &resp->body());\n+\n+  // Clients metrics\n+  AppendMetricWithoutLabels(\"connected_clients\", \"\", m.conn_stats.num_conns, MetricType::GAUGE,\n+                            &resp->body());\n+  AppendMetricWithoutLabels(\"client_read_buffer_bytes\", \"\", m.conn_stats.read_buf_capacity,\n+                            MetricType::GAUGE, &resp->body());\n+  AppendMetricWithoutLabels(\"blocked_clients\", \"\", m.conn_stats.num_blocked_clients,\n+                            MetricType::GAUGE, &resp->body());\n+\n+  // Memory metrics\n+  auto sdata_res = io::ReadStatusInfo();\n+  AppendMetricWithoutLabels(\"memory_used_bytes\", \"\", m.heap_used_bytes, MetricType::GAUGE,\n+                            &resp->body());\n+  AppendMetricWithoutLabels(\"memory_used_peak_bytes\", \"\", used_mem_peak.load(memory_order_relaxed),\n+                            MetricType::GAUGE, &resp->body());\n+  AppendMetricWithoutLabels(\"comitted_memory\", \"\", GetMallocCurrentCommitted(), MetricType::GAUGE,\n+                            &resp->body());\n+  AppendMetricWithoutLabels(\"memory_max_bytes\", \"\", max_memory_limit, MetricType::GAUGE,\n+                            &resp->body());\n+  if (sdata_res.has_value()) {\n+    AppendMetricWithoutLabels(\"used_memory_rss_bytes\", \"\", sdata_res->vm_rss, MetricType::GAUGE,\n+                              &resp->body());\n+  } else {\n+    LOG_FIRST_N(ERROR, 10) << \"Error fetching /proc/self/status stats. error \"\n+                           << sdata_res.error().message();\n+  }\n+\n+  // Stats metrics\n+  AppendMetricWithoutLabels(\"connections_received_total\", \"\", m.conn_stats.conn_received_cnt,\n+                            MetricType::COUNTER, &resp->body());\n+\n+  AppendMetricWithoutLabels(\"commands_processed_total\", \"\", m.conn_stats.command_cnt,\n+                            MetricType::COUNTER, &resp->body());\n+  AppendMetricWithoutLabels(\"keyspace_hits_total\", \"\", m.events.hits, MetricType::COUNTER,\n+                            &resp->body());\n+  AppendMetricWithoutLabels(\"keyspace_misses_total\", \"\", m.events.misses, MetricType::COUNTER,\n+                            &resp->body());\n+\n+  // Net metrics\n+  AppendMetricWithoutLabels(\"net_input_bytes_total\", \"\", m.conn_stats.io_read_bytes,\n+                            MetricType::COUNTER, &resp->body());\n+  AppendMetricWithoutLabels(\"net_output_bytes_total\", \"\", m.conn_stats.io_write_bytes,\n+                            MetricType::COUNTER, &resp->body());\n+\n+  // DB stats\n+  AppendMetricWithoutLabels(\"expired_keys_total\", \"\", m.events.expired_keys, MetricType::COUNTER,\n+                            &resp->body());\n+  AppendMetricWithoutLabels(\"evicted_keys_total\", \"\", m.events.evicted_keys, MetricType::COUNTER,\n+                            &resp->body());\n+\n+  string db_key_metrics;\n+  string db_key_expire_metrics;\n+\n+  AppendMetricHeader(\"db_keys\", \"Total number of keys by DB\", MetricType::GAUGE, &db_key_metrics);\n+  AppendMetricHeader(\"db_keys_expiring\", \"Total number of expiring keys by DB\", MetricType::GAUGE,\n+                     &db_key_expire_metrics);\n+\n+  for (size_t i = 0; i < m.db.size(); ++i) {\n+    AppendMetricValue(\"db_keys\", m.db[i].key_count, {\"db\"}, {StrCat(\"db\", i)}, &db_key_metrics);\n+    AppendMetricValue(\"db_keys_expiring\", m.db[i].expire_count, {\"db\"}, {StrCat(\"db\", i)},\n+                      &db_key_expire_metrics);\n+  }\n+\n+  // Command stats\n+  {\n+    string command_metrics;\n+\n+    AppendMetricHeader(\"commands\", \"Metrics for all commands ran\", MetricType::COUNTER,\n+                       &command_metrics);\n+    for (const auto& [name, stat] : m.cmd_stats_map) {\n+      const auto calls = stat.first;\n+      const auto duration_seconds = stat.second * 0.001;\n+      AppendMetricValue(\"commands_total\", calls, {\"cmd\"}, {name}, &command_metrics);\n+      AppendMetricValue(\"commands_duration_seconds_total\", duration_seconds, {\"cmd\"}, {name},\n+                        &command_metrics);\n+    }\n+    absl::StrAppend(&resp->body(), command_metrics);\n+  }\n+\n+  if (!m.replication_metrics.empty()) {\n+    string replication_lag_metrics;\n+    AppendMetricHeader(\"connected_replica_lag_records\", \"Lag in records of a connected replica.\",\n+                       MetricType::GAUGE, &replication_lag_metrics);\n+    for (const auto& replica : m.replication_metrics) {\n+      AppendMetricValue(\"connected_replica_lag_records\", replica.lsn_lag,\n+                        {\"replica_ip\", \"replica_port\", \"replica_state\"},\n+                        {replica.address, absl::StrCat(replica.listening_port), replica.state},\n+                        &replication_lag_metrics);\n+    }\n+    absl::StrAppend(&resp->body(), replication_lag_metrics);\n+  }\n+\n+  absl::StrAppend(&resp->body(), db_key_metrics);\n+  absl::StrAppend(&resp->body(), db_key_expire_metrics);\n+}\n+\n+void ServerFamily::ConfigureMetrics(util::HttpListenerBase* http_base) {\n+  // The naming of the metrics should be compatible with redis_exporter, see\n+  // https://github.com/oliver006/redis_exporter/blob/master/exporter/exporter.go#L111\n+\n+  auto cb = [this](const util::http::QueryArgs& args, util::HttpContext* send) {\n+    StringResponse resp = util::http::MakeStringResponse(boost::beast::http::status::ok);\n+    PrintPrometheusMetrics(this->GetMetrics(), &resp);\n+\n+    return send->Invoke(std::move(resp));\n+  };\n+\n+  http_base->RegisterCb(\"/metrics\", cb);\n+}\n+\n+void ServerFamily::PauseReplication(bool pause) {\n+  unique_lock lk(replicaof_mu_);\n+\n+  // Switch to primary mode.\n+  if (!ServerState::tlocal()->is_master) {\n+    auto repl_ptr = replica_;\n+    CHECK(repl_ptr);\n+    repl_ptr->Pause(pause);\n+  }\n+}\n+\n+std::optional<ReplicaOffsetInfo> ServerFamily::GetReplicaOffsetInfo() {\n+  unique_lock lk(replicaof_mu_);\n+\n+  // Switch to primary mode.\n+  if (!ServerState::tlocal()->is_master) {\n+    auto repl_ptr = replica_;\n+    CHECK(repl_ptr);\n+    return ReplicaOffsetInfo{repl_ptr->GetSyncId(), repl_ptr->GetReplicaOffset()};\n+  }\n+  return nullopt;\n+}\n+\n+bool ServerFamily::HasReplica() const {\n+  unique_lock lk(replicaof_mu_);\n+  return replica_ != nullptr;\n+}\n+\n+optional<Replica::Info> ServerFamily::GetReplicaInfo() const {\n+  unique_lock lk(replicaof_mu_);\n+  if (replica_ == nullptr) {\n+    return nullopt;\n+  } else {\n+    return replica_->GetInfo();\n+  }\n+}\n+\n+string ServerFamily::GetReplicaMasterId() const {\n+  unique_lock lk(replicaof_mu_);\n+  return string(replica_->MasterId());\n+}\n+\n+void ServerFamily::OnClose(ConnectionContext* cntx) {\n+  dfly_cmd_->OnClose(cntx);\n+}\n+\n+void ServerFamily::StatsMC(std::string_view section, facade::ConnectionContext* cntx) {\n+  if (!section.empty()) {\n+    return cntx->reply_builder()->SendError(\"\");\n+  }\n+  string info;\n+\n+#define ADD_LINE(name, val) absl::StrAppend(&info, \"STAT \" #name \" \", val, \"\\r\\n\")\n+\n+  time_t now = time(NULL);\n+  struct rusage ru;\n+  getrusage(RUSAGE_SELF, &ru);\n+\n+  auto dbl_time = [](const timeval& tv) -> double {\n+    return tv.tv_sec + double(tv.tv_usec) / 1000000.0;\n+  };\n+\n+  double utime = dbl_time(ru.ru_utime);\n+  double systime = dbl_time(ru.ru_stime);\n+\n+  Metrics m = GetMetrics();\n+\n+  ADD_LINE(pid, getpid());\n+  ADD_LINE(uptime, m.uptime);\n+  ADD_LINE(time, now);\n+  ADD_LINE(version, kGitTag);\n+  ADD_LINE(libevent, \"iouring\");\n+  ADD_LINE(pointer_size, sizeof(void*));\n+  ADD_LINE(rusage_user, utime);\n+  ADD_LINE(rusage_system, systime);\n+  ADD_LINE(max_connections, -1);\n+  ADD_LINE(curr_connections, m.conn_stats.num_conns);\n+  ADD_LINE(total_connections, -1);\n+  ADD_LINE(rejected_connections, -1);\n+  ADD_LINE(bytes_read, m.conn_stats.io_read_bytes);\n+  ADD_LINE(bytes_written, m.conn_stats.io_write_bytes);\n+  ADD_LINE(limit_maxbytes, -1);\n+\n+  absl::StrAppend(&info, \"END\\r\\n\");\n+\n+  MCReplyBuilder* builder = static_cast<MCReplyBuilder*>(cntx->reply_builder());\n+  builder->SendRaw(info);\n+\n+#undef ADD_LINE\n+}\n+\n+GenericError ServerFamily::DoSave() {\n+  const CommandId* cid = service().FindCmd(\"SAVE\");\n+  CHECK_NOTNULL(cid);\n+  boost::intrusive_ptr<Transaction> trans(new Transaction{cid});\n+  trans->InitByArgs(0, {});\n+  return DoSave(absl::GetFlag(FLAGS_df_snapshot_format), {}, trans.get());\n+}\n+\n+GenericError ServerFamily::DoSave(bool new_version, string_view basename, Transaction* trans) {\n+  SaveStagesController sc{SaveStagesInputs{new_version, basename, trans, &service_, &is_saving_,\n+                                           fq_threadpool_.get(), &last_save_info_, &save_mu_,\n+                                           &aws_}};\n+  return sc.Save();\n+}\n+\n+error_code ServerFamily::Drakarys(Transaction* transaction, DbIndex db_ind) {\n+  VLOG(1) << \"Drakarys\";\n+\n+  transaction->Schedule();  // TODO: to convert to ScheduleSingleHop ?\n+\n+  transaction->Execute(\n+      [db_ind](Transaction* t, EngineShard* shard) {\n+        shard->db_slice().FlushDb(db_ind);\n+        return OpStatus::OK;\n+      },\n+      true);\n+\n+  return error_code{};\n+}\n+\n+shared_ptr<const LastSaveInfo> ServerFamily::GetLastSaveInfo() const {\n+  lock_guard lk(save_mu_);\n+  return last_save_info_;\n+}\n+\n+void ServerFamily::DbSize(CmdArgList args, ConnectionContext* cntx) {\n+  atomic_ulong num_keys{0};\n+\n+  shard_set->RunBriefInParallel(\n+      [&](EngineShard* shard) {\n+        auto db_size = shard->db_slice().DbSize(cntx->conn_state.db_index);\n+        num_keys.fetch_add(db_size, memory_order_relaxed);\n+      },\n+      [](ShardId) { return true; });\n+\n+  return (*cntx)->SendLong(num_keys.load(memory_order_relaxed));\n+}\n+\n+void ServerFamily::BreakOnShutdown() {\n+  dfly_cmd_->BreakOnShutdown();\n+}\n+\n+void ServerFamily::CancelBlockingCommands() {\n+  auto cb = [](unsigned thread_index, util::Connection* conn) {\n+    facade::ConnectionContext* fc = static_cast<facade::Connection*>(conn)->cntx();\n+    if (fc) {\n+      ConnectionContext* cntx = static_cast<ConnectionContext*>(fc);\n+      cntx->CancelBlocking();\n+    }\n+  };\n+  for (auto* listener : listeners_) {\n+    listener->TraverseConnections(cb);\n+  }\n+}\n+\n+bool ServerFamily::AwaitDispatches(absl::Duration timeout,\n+                                   const std::function<bool(util::Connection*)>& filter) {\n+  auto start = absl::Now();\n+  for (auto* listener : listeners_) {\n+    absl::Duration remaining_time = timeout - (absl::Now() - start);\n+    if (remaining_time < absl::Nanoseconds(0) ||\n+        !listener->AwaitDispatches(remaining_time, filter)) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+string GetPassword() {\n+  string flag = GetFlag(FLAGS_requirepass);\n+  if (!flag.empty()) {\n+    return flag;\n+  }\n+\n+  const char* env_var = getenv(\"DFLY_PASSWORD\");\n+  if (env_var) {\n+    return env_var;\n+  }\n+\n+  return \"\";\n+}\n+\n+void ServerFamily::FlushDb(CmdArgList args, ConnectionContext* cntx) {\n+  DCHECK(cntx->transaction);\n+  Drakarys(cntx->transaction, cntx->transaction->GetDbIndex());\n+\n+  cntx->reply_builder()->SendOk();\n+}\n+\n+void ServerFamily::FlushAll(CmdArgList args, ConnectionContext* cntx) {\n+  if (args.size() > 1) {\n+    (*cntx)->SendError(kSyntaxErr);\n+    return;\n+  }\n+\n+  DCHECK(cntx->transaction);\n+  Drakarys(cntx->transaction, DbSlice::kDbAll);\n+  (*cntx)->SendOk();\n+}\n+\n+void ServerFamily::Auth(CmdArgList args, ConnectionContext* cntx) {\n+  if (args.size() > 2) {\n+    return (*cntx)->SendError(kSyntaxErr);\n+  }\n+\n+  if (args.size() == 2) {\n+    const auto* registry = ServerState::tlocal()->user_registry;\n+    std::string_view username = facade::ToSV(args[0]);\n+    std::string_view password = facade::ToSV(args[1]);\n+    auto is_authorized = registry->AuthUser(username, password);\n+    if (is_authorized) {\n+      cntx->authed_username = username;\n+      return (*cntx)->SendOk();\n+    }\n+    return (*cntx)->SendError(absl::StrCat(\"Could not authorize user: \", username));\n+  }\n+\n+  if (!cntx->req_auth) {\n+    return (*cntx)->SendError(\n+        \"AUTH <password> called without any password configured for the \"\n+        \"default user. Are you sure your configuration is correct?\");\n+  }\n+\n+  string_view pass = ArgS(args, 0);\n+  if (pass == GetPassword()) {\n+    cntx->authenticated = true;\n+    (*cntx)->SendOk();\n+  } else {\n+    (*cntx)->SendError(facade::kAuthRejected);\n+  }\n+}\n+\n+void ServerFamily::Client(CmdArgList args, ConnectionContext* cntx) {\n+  ToUpper(&args[0]);\n+  string_view sub_cmd = ArgS(args, 0);\n+\n+  if (sub_cmd == \"SETNAME\" && args.size() == 2) {\n+    cntx->owner()->SetName(string{ArgS(args, 1)});\n+    return (*cntx)->SendOk();\n+  }\n+\n+  if (sub_cmd == \"GETNAME\") {\n+    auto name = cntx->owner()->GetName();\n+    if (!name.empty()) {\n+      return (*cntx)->SendBulkString(name);\n+    } else {\n+      return (*cntx)->SendNull();\n+    }\n+  }\n+\n+  if (sub_cmd == \"LIST\") {\n+    vector<string> client_info;\n+    absl::base_internal::SpinLock mu;\n+\n+    // we can not preempt the connection traversal, so we need to use a spinlock.\n+    // alternatively we could lock when mutating the connection list, but it seems not important.\n+    auto cb = [&](unsigned thread_index, util::Connection* conn) {\n+      facade::Connection* dcon = static_cast<facade::Connection*>(conn);\n+      string info = dcon->GetClientInfo(thread_index);\n+      absl::base_internal::SpinLockHolder l(&mu);\n+      client_info.push_back(move(info));\n+    };\n+\n+    for (auto* listener : listeners_) {\n+      listener->TraverseConnections(cb);\n+    }\n+\n+    string result = absl::StrJoin(move(client_info), \"\\n\");\n+    result.append(\"\\n\");\n+    return (*cntx)->SendBulkString(result);\n+  }\n+\n+  LOG_FIRST_N(ERROR, 10) << \"Subcommand \" << sub_cmd << \" not supported\";\n+  return (*cntx)->SendError(UnknownSubCmd(sub_cmd, \"CLIENT\"), kSyntaxErrType);\n+}\n+\n+void ServerFamily::Config(CmdArgList args, ConnectionContext* cntx) {\n+  ToUpper(&args[0]);\n+  string_view sub_cmd = ArgS(args, 0);\n+\n+  if (sub_cmd == \"SET\") {\n+    if (args.size() < 3) {\n+      return (*cntx)->SendError(WrongNumArgsError(\"config|set\"));\n+    }\n+\n+    ToLower(&args[1]);\n+    string_view param = ArgS(args, 1);\n+\n+    ConfigRegistry::SetResult result = config_registry.Set(param, ArgS(args, 2));\n+\n+    const char kErrPrefix[] = \"CONFIG SET failed (possibly related to argument '\";\n+    switch (result) {\n+      case ConfigRegistry::SetResult::OK:\n+        return (*cntx)->SendOk();\n+      case ConfigRegistry::SetResult::UNKNOWN:\n+        return (*cntx)->SendError(\n+            absl::StrCat(\"Unknown option or number of arguments for CONFIG SET - '\", param, \"'\"),\n+            kConfigErrType);\n+\n+      case ConfigRegistry::SetResult::READONLY:\n+        return (*cntx)->SendError(\n+            absl::StrCat(kErrPrefix, param, \"') - can't set immutable config\"), kConfigErrType);\n+\n+      case ConfigRegistry::SetResult::INVALID:\n+        return (*cntx)->SendError(absl::StrCat(kErrPrefix, param, \"') - argument can not be set\"),\n+                                  kConfigErrType);\n+    }\n+    ABSL_UNREACHABLE();\n+  }\n+\n+  if (sub_cmd == \"GET\" && args.size() == 2) {\n+    // Send empty response, like Redis does, unless the param is supported\n+\n+    string_view param = ArgS(args, 1);\n+<<<<<<< HEAD\n+    if (param == \"databases\") {\n+      res.emplace_back(param);\n+      res.push_back(absl::StrCat(absl::GetFlag(FLAGS_dbnum)));\n+    } else if (auto value_from_registry = config_registry.Get(param);\n+               value_from_registry.has_value()) {\n+      res.emplace_back(param);\n+      res.push_back(*value_from_registry);\n+=======\n+    vector<string> names = config_registry.List(param);\n+    vector<string> res;\n+\n+    for (const auto& name : names) {\n+      absl::CommandLineFlag* flag = CHECK_NOTNULL(absl::FindCommandLineFlag(name));\n+      res.push_back(name);\n+      res.push_back(flag->CurrentValue());\n+>>>>>>> 660af341 (feat: implement CONFIG GET command)\n+    }\n+\n+    return (*cntx)->SendStringArr(res, RedisReplyBuilder::MAP);\n+  }\n+\n+  if (sub_cmd == \"RESETSTAT\") {\n+    shard_set->pool()->Await([registry = service_.mutable_registry()](unsigned index, auto*) {\n+      registry->ResetCallStats(index);\n+      auto& sstate = *ServerState::tlocal();\n+      auto& stats = sstate.connection_stats;\n+      stats.err_count_map.clear();\n+      stats.command_cnt = 0;\n+      stats.async_writes_cnt = 0;\n+    });\n+\n+    return (*cntx)->SendOk();\n+  } else {\n+    return (*cntx)->SendError(UnknownSubCmd(sub_cmd, \"CONFIG\"), kSyntaxErrType);\n+  }\n+}\n+\n+void ServerFamily::Debug(CmdArgList args, ConnectionContext* cntx) {\n+  ToUpper(&args[0]);\n+\n+  DebugCmd dbg_cmd{this, cntx};\n+\n+  return dbg_cmd.Run(args);\n+}\n+\n+void ServerFamily::Memory(CmdArgList args, ConnectionContext* cntx) {\n+  ToUpper(&args[0]);\n+\n+  MemoryCmd mem_cmd{this, cntx};\n+\n+  return mem_cmd.Run(args);\n+}\n+\n+void ServerFamily::Save(CmdArgList args, ConnectionContext* cntx) {\n+  string err_detail;\n+  bool new_version = absl::GetFlag(FLAGS_df_snapshot_format);\n+  if (args.size() > 2) {\n+    return (*cntx)->SendError(kSyntaxErr);\n+  }\n+\n+  if (args.size() >= 1) {\n+    ToUpper(&args[0]);\n+    string_view sub_cmd = ArgS(args, 0);\n+    if (sub_cmd == \"DF\") {\n+      new_version = true;\n+    } else if (sub_cmd == \"RDB\") {\n+      new_version = false;\n+    } else {\n+      return (*cntx)->SendError(UnknownSubCmd(sub_cmd, \"SAVE\"), kSyntaxErrType);\n+    }\n+  }\n+\n+  string_view basename;\n+  if (args.size() == 2) {\n+    basename = ArgS(args, 1);\n+  }\n+\n+  GenericError ec = DoSave(new_version, basename, cntx->transaction);\n+  if (ec) {\n+    (*cntx)->SendError(ec.Format());\n+  } else {\n+    (*cntx)->SendOk();\n+  }\n+}\n+\n+static void MergeInto(const DbSlice::Stats& src, Metrics* dest) {\n+  if (src.db_stats.size() > dest->db.size())\n+    dest->db.resize(src.db_stats.size());\n+  for (size_t i = 0; i < src.db_stats.size(); ++i) {\n+    dest->db[i] += src.db_stats[i];\n+  }\n+\n+  dest->events += src.events;\n+  dest->small_string_bytes += src.small_string_bytes;\n+}\n+\n+Metrics ServerFamily::GetMetrics() const {\n+  Metrics result;\n+\n+  Mutex mu;\n+\n+  auto cb = [&](unsigned index, ProactorBase* pb) {\n+    EngineShard* shard = EngineShard::tlocal();\n+    ServerState* ss = ServerState::tlocal();\n+\n+    lock_guard lk(mu);\n+\n+    result.uptime = time(NULL) - this->start_time_;\n+    result.conn_stats += ss->connection_stats;\n+    result.qps += uint64_t(ss->MovingSum6());\n+    result.ooo_tx_transaction_cnt += ss->stats.ooo_tx_cnt;\n+\n+    service_.mutable_registry()->MergeCallStats(\n+        index, [&dest_map = result.cmd_stats_map](string_view name, const CmdCallStats& src) {\n+          auto& ent = dest_map[string{name}];\n+          ent.first += src.first;\n+          ent.second += src.second;\n+        });\n+\n+    if (shard) {\n+      MergeInto(shard->db_slice().GetStats(), &result);\n+\n+      result.heap_used_bytes += shard->UsedMemory();\n+      if (shard->tiered_storage()) {\n+        result.tiered_stats += shard->tiered_storage()->GetStats();\n+      }\n+      result.shard_stats += shard->stats();\n+      result.traverse_ttl_per_sec += shard->GetMovingSum6(EngineShard::TTL_TRAVERSE);\n+      result.delete_ttl_per_sec += shard->GetMovingSum6(EngineShard::TTL_DELETE);\n+    }\n+  };\n+\n+  service_.proactor_pool().AwaitFiberOnAll(std::move(cb));\n+  result.qps /= 6;  // normalize moving average stats\n+  result.traverse_ttl_per_sec /= 6;\n+  result.delete_ttl_per_sec /= 6;\n+\n+  result.is_master = false;\n+  if (ServerState::tlocal() && ServerState::tlocal()->is_master) {\n+    result.is_master = true;\n+    result.replication_metrics = dfly_cmd_->GetReplicasRoleInfo();\n+  }\n+\n+  return result;\n+}\n+\n+void ServerFamily::Info(CmdArgList args, ConnectionContext* cntx) {\n+  if (args.size() > 1) {\n+    return (*cntx)->SendError(kSyntaxErr);\n+  }\n+\n+  string_view section;\n+\n+  if (args.size() == 1) {\n+    ToUpper(&args[0]);\n+    section = ArgS(args, 0);\n+  }\n+\n+  string info;\n+\n+  auto should_enter = [&](string_view name, bool hidden = false) {\n+    bool res = (!hidden && section.empty()) || section == \"ALL\" || section == name;\n+    if (res && !info.empty())\n+      info.append(\"\\r\\n\");\n+\n+    return res;\n+  };\n+\n+  auto append = [&info](absl::AlphaNum a1, absl::AlphaNum a2) {\n+    absl::StrAppend(&info, a1, \":\", a2, \"\\r\\n\");\n+  };\n+\n+#define ADD_HEADER(x) absl::StrAppend(&info, x \"\\r\\n\")\n+  Metrics m = GetMetrics();\n+\n+  if (should_enter(\"SERVER\")) {\n+    auto kind = ProactorBase::me()->GetKind();\n+    const char* multiplex_api = (kind == ProactorBase::IOURING) ? \"iouring\" : \"epoll\";\n+\n+    ADD_HEADER(\"# Server\");\n+\n+    append(\"redis_version\", kRedisVersion);\n+    append(\"dragonfly_version\", GetVersion());\n+    append(\"redis_mode\", \"standalone\");\n+    append(\"arch_bits\", 64);\n+    append(\"multiplexing_api\", multiplex_api);\n+    append(\"tcp_port\", GetFlag(FLAGS_port));\n+\n+    size_t uptime = m.uptime;\n+    append(\"uptime_in_seconds\", uptime);\n+    append(\"uptime_in_days\", uptime / (3600 * 24));\n+  }\n+\n+  auto sdata_res = io::ReadStatusInfo();\n+\n+  DbStats total;\n+  for (const auto& db_stats : m.db) {\n+    total += db_stats;\n+  }\n+\n+  if (should_enter(\"CLIENTS\")) {\n+    ADD_HEADER(\"# Clients\");\n+    append(\"connected_clients\", m.conn_stats.num_conns);\n+    append(\"client_read_buffer_bytes\", m.conn_stats.read_buf_capacity);\n+    append(\"blocked_clients\", m.conn_stats.num_blocked_clients);\n+  }\n+\n+  if (should_enter(\"MEMORY\")) {\n+    ADD_HEADER(\"# Memory\");\n+\n+    append(\"used_memory\", m.heap_used_bytes);\n+    append(\"used_memory_human\", HumanReadableNumBytes(m.heap_used_bytes));\n+    append(\"used_memory_peak\", used_mem_peak.load(memory_order_relaxed));\n+\n+    append(\"comitted_memory\", GetMallocCurrentCommitted());\n+\n+    if (sdata_res.has_value()) {\n+      append(\"used_memory_rss\", sdata_res->vm_rss);\n+      append(\"used_memory_rss_human\", HumanReadableNumBytes(sdata_res->vm_rss));\n+    } else {\n+      LOG_FIRST_N(ERROR, 10) << \"Error fetching /proc/self/status stats. error \"\n+                             << sdata_res.error().message();\n+    }\n+\n+    // Blob - all these cases where the key/objects are represented by a single blob allocated on\n+    // heap. For example, strings or intsets. members of lists, sets, zsets etc\n+    // are not accounted for to avoid complex computations. In some cases, when number of members\n+    // is known we approximate their allocations by taking 16 bytes per member.\n+    append(\"object_used_memory\", total.obj_memory_usage);\n+    append(\"table_used_memory\", total.table_mem_usage);\n+    append(\"num_buckets\", total.bucket_count);\n+    append(\"num_entries\", total.key_count);\n+    append(\"inline_keys\", total.inline_keys);\n+    append(\"strval_bytes\", total.strval_memory_usage);\n+    append(\"updateval_amount\", total.update_value_amount);\n+    append(\"listpack_blobs\", total.listpack_blob_cnt);\n+    append(\"listpack_bytes\", total.listpack_bytes);\n+    append(\"small_string_bytes\", m.small_string_bytes);\n+    append(\"pipeline_cache_bytes\", m.conn_stats.pipeline_cache_capacity);\n+    append(\"maxmemory\", max_memory_limit);\n+    append(\"maxmemory_human\", HumanReadableNumBytes(max_memory_limit));\n+    if (GetFlag(FLAGS_cache_mode)) {\n+      append(\"cache_mode\", \"cache\");\n+    } else {\n+      append(\"cache_mode\", \"store\");\n+      // Compatible with redis based frameworks.\n+      append(\"maxmemory_policy\", \"noeviction\");\n+    }\n+  }\n+\n+  if (should_enter(\"STATS\")) {\n+    ADD_HEADER(\"# Stats\");\n+\n+    append(\"total_connections_received\", m.conn_stats.conn_received_cnt);\n+    append(\"total_commands_processed\", m.conn_stats.command_cnt);\n+    append(\"instantaneous_ops_per_sec\", m.qps);\n+    append(\"total_pipelined_commands\", m.conn_stats.pipelined_cmd_cnt);\n+    append(\"total_net_input_bytes\", m.conn_stats.io_read_bytes);\n+    append(\"total_net_output_bytes\", m.conn_stats.io_write_bytes);\n+    append(\"instantaneous_input_kbps\", -1);\n+    append(\"instantaneous_output_kbps\", -1);\n+    append(\"rejected_connections\", -1);\n+    append(\"expired_keys\", m.events.expired_keys);\n+    append(\"evicted_keys\", m.events.evicted_keys);\n+    append(\"hard_evictions\", m.events.hard_evictions);\n+    append(\"garbage_checked\", m.events.garbage_checked);\n+    append(\"garbage_collected\", m.events.garbage_collected);\n+    append(\"bump_ups\", m.events.bumpups);\n+    append(\"stash_unloaded\", m.events.stash_unloaded);\n+    append(\"oom_rejections\", m.events.insertion_rejections);\n+    append(\"traverse_ttl_sec\", m.traverse_ttl_per_sec);\n+    append(\"delete_ttl_sec\", m.delete_ttl_per_sec);\n+    append(\"keyspace_hits\", m.events.hits);\n+    append(\"keyspace_misses\", m.events.misses);\n+    append(\"total_reads_processed\", m.conn_stats.io_read_cnt);\n+    append(\"total_writes_processed\", m.conn_stats.io_write_cnt);\n+    append(\"async_writes_count\", m.conn_stats.async_writes_cnt);\n+    append(\"defrag_attempt_total\", m.shard_stats.defrag_attempt_total);\n+    append(\"defrag_realloc_total\", m.shard_stats.defrag_realloc_total);\n+    append(\"defrag_task_invocation_total\", m.shard_stats.defrag_task_invocation_total);\n+  }\n+\n+  if (should_enter(\"TIERED\", true)) {\n+    ADD_HEADER(\"# TIERED\");\n+    append(\"tiered_entries\", total.tiered_entries);\n+    append(\"tiered_bytes\", total.tiered_size);\n+    append(\"tiered_reads\", m.tiered_stats.tiered_reads);\n+    append(\"tiered_writes\", m.tiered_stats.tiered_writes);\n+    append(\"tiered_reserved\", m.tiered_stats.storage_reserved);\n+    append(\"tiered_capacity\", m.tiered_stats.storage_capacity);\n+    append(\"tiered_aborted_write_total\", m.tiered_stats.aborted_write_cnt);\n+    append(\"tiered_flush_skip_total\", m.tiered_stats.flush_skip_cnt);\n+  }\n+\n+  if (should_enter(\"PERSISTENCE\", true)) {\n+    ADD_HEADER(\"# PERSISTENCE\");\n+    decltype(last_save_info_) save_info;\n+    {\n+      lock_guard lk(save_mu_);\n+      save_info = last_save_info_;\n+    }\n+    // when when last save\n+    append(\"last_save\", save_info->save_time);\n+    append(\"last_save_duration_sec\", save_info->duration_sec);\n+    append(\"last_save_file\", save_info->file_name);\n+    size_t is_loading = service_.GetGlobalState() == GlobalState::LOADING;\n+    append(\"loading\", is_loading);\n+\n+    for (const auto& k_v : save_info->freq_map) {\n+      append(StrCat(\"rdb_\", k_v.first), k_v.second);\n+    }\n+    append(\"rdb_changes_since_last_save\", m.events.update);\n+  }\n+\n+  if (should_enter(\"REPLICATION\")) {\n+    ADD_HEADER(\"# Replication\");\n+\n+    ServerState& etl = *ServerState::tlocal();\n+\n+    if (etl.is_master) {\n+      append(\"role\", \"master\");\n+      append(\"connected_slaves\", m.conn_stats.num_replicas);\n+      const auto& replicas = m.replication_metrics;\n+      for (size_t i = 0; i < replicas.size(); i++) {\n+        auto& r = replicas[i];\n+        // e.g. slave0:ip=172.19.0.3,port=6379,state=full_sync\n+        append(StrCat(\"slave\", i), StrCat(\"ip=\", r.address, \",port=\", r.listening_port,\n+                                          \",state=\", r.state, \",lag=\", r.lsn_lag));\n+      }\n+      append(\"master_replid\", master_id_);\n+    } else {\n+      append(\"role\", \"replica\");\n+\n+      // it's safe to access replica_ because replica_ is created before etl.is_master set to\n+      // false and cleared after etl.is_master is set to true. And since the code here that checks\n+      // for is_master and copies shared_ptr is atomic, it1 should be correct.\n+      auto replica_ptr = replica_;\n+      Replica::Info rinfo = replica_ptr->GetInfo();\n+      append(\"master_host\", rinfo.host);\n+      append(\"master_port\", rinfo.port);\n+\n+      const char* link = rinfo.master_link_established ? \"up\" : \"down\";\n+      append(\"master_link_status\", link);\n+      append(\"master_last_io_seconds_ago\", rinfo.master_last_io_sec);\n+      append(\"master_sync_in_progress\", rinfo.full_sync_in_progress);\n+    }\n+  }\n+\n+  if (should_enter(\"COMMANDSTATS\", true)) {\n+    ADD_HEADER(\"# Commandstats\");\n+\n+    auto unknown_cmd = service_.UknownCmdMap();\n+\n+    auto append_sorted = [&append](string_view prefix, auto display) {\n+      sort(display.begin(), display.end());\n+\n+      for (const auto& k_v : display) {\n+        append(StrCat(prefix, k_v.first), k_v.second);\n+      }\n+    };\n+\n+    vector<pair<string_view, string>> commands;\n+\n+    for (const auto& [name, stats] : m.cmd_stats_map) {\n+      const auto calls = stats.first, sum = stats.second;\n+      commands.push_back(\n+          {name, absl::StrJoin({absl::StrCat(\"calls=\", calls), absl::StrCat(\"usec=\", sum),\n+                                absl::StrCat(\"usec_per_call=\", static_cast<double>(sum) / calls)},\n+                               \",\")});\n+    }\n+\n+    append_sorted(\"cmdstat_\", move(commands));\n+    append_sorted(\"unknown_\",\n+                  vector<pair<string_view, uint64_t>>(unknown_cmd.cbegin(), unknown_cmd.cend()));\n+  }\n+\n+  if (should_enter(\"ERRORSTATS\", true)) {\n+    ADD_HEADER(\"# Errorstats\");\n+    for (const auto& k_v : m.conn_stats.err_count_map) {\n+      append(k_v.first, k_v.second);\n+    }\n+  }\n+\n+  if (should_enter(\"KEYSPACE\")) {\n+    ADD_HEADER(\"# Keyspace\");\n+    for (size_t i = 0; i < m.db.size(); ++i) {\n+      const auto& stats = m.db[i];\n+      bool show = (i == 0) || (stats.key_count > 0);\n+      if (show) {\n+        string val = StrCat(\"keys=\", stats.key_count, \",expires=\", stats.expire_count,\n+                            \",avg_ttl=-1\");  // TODO\n+        append(StrCat(\"db\", i), val);\n+      }\n+    }\n+  }\n+\n+  if (should_enter(\"CPU\")) {\n+    ADD_HEADER(\"# CPU\");\n+    struct rusage ru, cu, tu;\n+    getrusage(RUSAGE_SELF, &ru);\n+    getrusage(RUSAGE_CHILDREN, &cu);\n+    getrusage(RUSAGE_THREAD, &tu);\n+    append(\"used_cpu_sys\", StrCat(ru.ru_stime.tv_sec, \".\", ru.ru_stime.tv_usec));\n+    append(\"used_cpu_user\", StrCat(ru.ru_utime.tv_sec, \".\", ru.ru_utime.tv_usec));\n+    append(\"used_cpu_sys_children\", StrCat(cu.ru_stime.tv_sec, \".\", cu.ru_stime.tv_usec));\n+    append(\"used_cpu_user_children\", StrCat(cu.ru_utime.tv_sec, \".\", cu.ru_utime.tv_usec));\n+    append(\"used_cpu_sys_main_thread\", StrCat(tu.ru_stime.tv_sec, \".\", tu.ru_stime.tv_usec));\n+    append(\"used_cpu_user_main_thread\", StrCat(tu.ru_utime.tv_sec, \".\", tu.ru_utime.tv_usec));\n+  }\n+\n+  if (should_enter(\"CLUSTER\")) {\n+    ADD_HEADER(\"# Cluster\");\n+    append(\"cluster_enabled\", ClusterConfig::IsEnabledOrEmulated());\n+  }\n+\n+  (*cntx)->SendBulkString(info);\n+}\n+\n+void ServerFamily::Hello(CmdArgList args, ConnectionContext* cntx) {\n+  // If no arguments are provided default to RESP2.\n+  bool is_resp3 = false;\n+  bool has_auth = false;\n+  bool has_setname = false;\n+  string_view username;\n+  string_view password;\n+  string_view clientname;\n+\n+  if (args.size() > 0) {\n+    string_view proto_version = ArgS(args, 0);\n+    is_resp3 = proto_version == \"3\";\n+    bool valid_proto_version = proto_version == \"2\" || is_resp3;\n+    if (!valid_proto_version) {\n+      (*cntx)->SendError(UnknownCmd(\"HELLO\", args));\n+      return;\n+    }\n+\n+    for (uint32_t i = 1; i < args.size(); i++) {\n+      auto sub_cmd = ArgS(args, i);\n+      auto moreargs = args.size() - 1 - i;\n+      if (absl::EqualsIgnoreCase(sub_cmd, \"AUTH\") && moreargs >= 2) {\n+        has_auth = true;\n+        username = ArgS(args, i + 1);\n+        password = ArgS(args, i + 2);\n+        i += 2;\n+      } else if (absl::EqualsIgnoreCase(sub_cmd, \"SETNAME\") && moreargs > 0) {\n+        has_setname = true;\n+        clientname = ArgS(args, i + 1);\n+        i += 1;\n+      } else {\n+        (*cntx)->SendError(kSyntaxErr);\n+        return;\n+      }\n+    }\n+  }\n+\n+  if (has_auth) {\n+    if (username == \"default\" && password == GetPassword()) {\n+      cntx->authenticated = true;\n+    } else {\n+      (*cntx)->SendError(facade::kAuthRejected);\n+      return;\n+    }\n+  }\n+\n+  if (cntx->req_auth && !cntx->authenticated) {\n+    (*cntx)->SendError(\n+        \"-NOAUTH HELLO must be called with the client already \"\n+        \"authenticated, otherwise the HELLO <proto> AUTH <user> <pass> \"\n+        \"option can be used to authenticate the client and \"\n+        \"select the RESP protocol version at the same time\");\n+    return;\n+  }\n+\n+  if (has_setname) {\n+    cntx->owner()->SetName(string{clientname});\n+  }\n+\n+  int proto_version = 2;\n+  if (is_resp3) {\n+    proto_version = 3;\n+    (*cntx)->SetResp3(true);\n+  } else {\n+    // Issuing hello 2 again is valid and should switch back to RESP2\n+    (*cntx)->SetResp3(false);\n+  }\n+\n+  (*cntx)->StartCollection(7, RedisReplyBuilder::MAP);\n+  (*cntx)->SendBulkString(\"server\");\n+  (*cntx)->SendBulkString(\"redis\");\n+  (*cntx)->SendBulkString(\"version\");\n+  (*cntx)->SendBulkString(kRedisVersion);\n+  (*cntx)->SendBulkString(\"dragonfly_version\");\n+  (*cntx)->SendBulkString(GetVersion());\n+  (*cntx)->SendBulkString(\"proto\");\n+  (*cntx)->SendLong(proto_version);\n+  (*cntx)->SendBulkString(\"id\");\n+  (*cntx)->SendLong(cntx->owner()->GetClientId());\n+  (*cntx)->SendBulkString(\"mode\");\n+  (*cntx)->SendBulkString(\"standalone\");\n+  (*cntx)->SendBulkString(\"role\");\n+  (*cntx)->SendBulkString((*ServerState::tlocal()).is_master ? \"master\" : \"slave\");\n+}\n+\n+void ServerFamily::ReplicaOfInternal(string_view host, string_view port_sv, ConnectionContext* cntx,\n+                                     ActionOnConnectionFail on_err) {\n+  auto& pool = service_.proactor_pool();\n+  LOG(INFO) << \"Replicating \" << host << \":\" << port_sv;\n+\n+  // We lock to protect global state changes that we perform during the replication setup:\n+  // The replica_ pointer, GlobalState, and the DB itself (we do a flushall txn before syncing).\n+  // The lock is only released during replica_->Start because we want to allow cancellation during\n+  // the connection. If another replication command is received during Start() of an old\n+  // replication, it will acquire the lock, call Stop() on the old replica_ and wait for Stop() to\n+  // complete. So Replica::Stop() must\n+  // 1. Be very responsive, as it is called while holding the lock.\n+  // 2. Leave the DB in a consistent state after it is done.\n+  // We have a relatively involved state machine inside Replica itself which handels cancellation\n+  // with those requirements.\n+  VLOG(1) << \"Acquire replica lock\";\n+  unique_lock lk(replicaof_mu_);\n+\n+  if (IsReplicatingNoOne(host, port_sv)) {\n+    if (!ServerState::tlocal()->is_master) {\n+      auto repl_ptr = replica_;\n+      CHECK(repl_ptr);\n+\n+      pool.AwaitFiberOnAll(\n+          [&](util::ProactorBase* pb) { ServerState::tlocal()->is_master = true; });\n+      replica_->Stop();\n+      replica_.reset();\n+    }\n+\n+    CHECK(service_.SwitchState(GlobalState::LOADING, GlobalState::ACTIVE) == GlobalState::ACTIVE)\n+        << \"Server is set to replica no one, yet state is not active!\";\n+\n+    return (*cntx)->SendOk();\n+  }\n+\n+  uint32_t port;\n+\n+  if (!absl::SimpleAtoi(port_sv, &port) || port < 1 || port > 65535) {\n+    (*cntx)->SendError(kInvalidIntErr);\n+    return;\n+  }\n+\n+  auto new_replica = make_shared<Replica>(string(host), port, &service_, master_id());\n+\n+  if (replica_) {\n+    replica_->Stop();  // NOTE: consider introducing update API flow.\n+  } else {\n+    // TODO: to disconnect all the blocked clients (pubsub, blpop etc)\n+\n+    pool.AwaitFiberOnAll([&](util::ProactorBase* pb) { ServerState::tlocal()->is_master = false; });\n+  }\n+  replica_ = new_replica;\n+\n+  GlobalState new_state = service_.SwitchState(GlobalState::ACTIVE, GlobalState::LOADING);\n+  if (new_state != GlobalState::LOADING) {\n+    LOG(WARNING) << GlobalStateName(new_state) << \" in progress, ignored\";\n+    return;\n+  }\n+\n+  // Replica sends response in either case. No need to send response in this function.\n+  // It's a bit confusing but simpler.\n+  lk.unlock();\n+  error_code ec{};\n+\n+  switch (on_err) {\n+    case ActionOnConnectionFail::kReturnOnError:\n+      ec = new_replica->Start(cntx);\n+      break;\n+    case ActionOnConnectionFail::kContinueReplication:  // set DF to replicate, and forget about it\n+      new_replica->EnableReplication(cntx);\n+      break;\n+  };\n+\n+  VLOG(1) << \"Acquire replica lock\";\n+  lk.lock();\n+\n+  // Since we released the replication lock during Start(..), we need to check if this still the\n+  // last replicaof command we got. If it's not, then we were cancelled and just exit.\n+  if (replica_ == new_replica) {\n+    if (ec) {\n+      service_.SwitchState(GlobalState::LOADING, GlobalState::ACTIVE);\n+      replica_->Stop();\n+      replica_.reset();\n+    }\n+    bool is_master = !replica_;\n+    pool.AwaitFiberOnAll(\n+        [&](util::ProactorBase* pb) { ServerState::tlocal()->is_master = is_master; });\n+  } else {\n+    new_replica->Stop();\n+  }\n+}\n+\n+void ServerFamily::ReplicaOf(CmdArgList args, ConnectionContext* cntx) {\n+  string_view host = ArgS(args, 0);\n+  string_view port = ArgS(args, 1);\n+\n+  // don't flush if input is NO ONE\n+  if (!IsReplicatingNoOne(host, port))\n+    Drakarys(cntx->transaction, DbSlice::kDbAll);\n+\n+  ReplicaOfInternal(host, port, cntx, ActionOnConnectionFail::kReturnOnError);\n+}\n+\n+void ServerFamily::Replicate(string_view host, string_view port) {\n+  io::NullSink sink;\n+  ConnectionContext ctxt{&sink, nullptr};\n+\n+  // we don't flush the database as the context is null\n+  // (and also because there is nothing to flush)\n+\n+  ReplicaOfInternal(host, port, &ctxt, ActionOnConnectionFail::kContinueReplication);\n+}\n+\n+void ServerFamily::ReplTakeOver(CmdArgList args, ConnectionContext* cntx) {\n+  VLOG(1) << \"Starting take over\";\n+  VLOG(1) << \"Acquire replica lock\";\n+  unique_lock lk(replicaof_mu_);\n+\n+  float_t timeout_sec;\n+  if (!absl::SimpleAtof(ArgS(args, 0), &timeout_sec)) {\n+    return (*cntx)->SendError(kInvalidIntErr);\n+  }\n+  if (timeout_sec < 0) {\n+    return (*cntx)->SendError(\"timeout is negative\");\n+  }\n+\n+  if (ServerState::tlocal()->is_master)\n+    return (*cntx)->SendError(\"Already a master instance\");\n+  auto repl_ptr = replica_;\n+  CHECK(repl_ptr);\n+\n+  auto info = replica_->GetInfo();\n+  if (!info.full_sync_done) {\n+    return (*cntx)->SendError(\"Full sync not done\");\n+  }\n+\n+  std::error_code ec = replica_->TakeOver(ArgS(args, 0));\n+  if (ec)\n+    return (*cntx)->SendError(\"Couldn't execute takeover\");\n+\n+  LOG(INFO) << \"Takeover successful, promoting this instance to master.\";\n+  service_.proactor_pool().AwaitFiberOnAll(\n+      [&](util::ProactorBase* pb) { ServerState::tlocal()->is_master = true; });\n+  replica_->Stop();\n+  replica_.reset();\n+  return (*cntx)->SendOk();\n+}\n+\n+void ServerFamily::ReplConf(CmdArgList args, ConnectionContext* cntx) {\n+  if (args.size() % 2 == 1)\n+    goto err;\n+  for (unsigned i = 0; i < args.size(); i += 2) {\n+    DCHECK_LT(i + 1, args.size());\n+    ToUpper(&args[i]);\n+\n+    std::string_view cmd = ArgS(args, i);\n+    std::string_view arg = ArgS(args, i + 1);\n+    if (cmd == \"CAPA\") {\n+      if (arg == \"dragonfly\" && args.size() == 2 && i == 0) {\n+        auto [sid, replica_info] = dfly_cmd_->CreateSyncSession(cntx);\n+        cntx->owner()->SetName(absl::StrCat(\"repl_ctrl_\", sid));\n+\n+        string sync_id = absl::StrCat(\"SYNC\", sid);\n+        cntx->conn_state.replication_info.repl_session_id = sid;\n+\n+        if (!cntx->replica_conn) {\n+          ServerState::tl_connection_stats()->num_replicas += 1;\n+        }\n+        cntx->replica_conn = true;\n+\n+        // The response for 'capa dragonfly' is: <masterid> <syncid> <numthreads> <version>\n+        (*cntx)->StartArray(4);\n+        (*cntx)->SendSimpleString(master_id_);\n+        (*cntx)->SendSimpleString(sync_id);\n+        (*cntx)->SendLong(replica_info->flows.size());\n+        (*cntx)->SendLong(unsigned(DflyVersion::CURRENT_VER));\n+        return;\n+      }\n+    } else if (cmd == \"LISTENING-PORT\") {\n+      uint32_t replica_listening_port;\n+      if (!absl::SimpleAtoi(arg, &replica_listening_port)) {\n+        (*cntx)->SendError(kInvalidIntErr);\n+        return;\n+      }\n+      cntx->conn_state.replication_info.repl_listening_port = replica_listening_port;\n+    } else if (cmd == \"CLIENT-ID\" && args.size() == 2) {\n+      std::string client_id{arg};\n+      auto& pool = service_.proactor_pool();\n+      pool.AwaitFiberOnAll(\n+          [&](util::ProactorBase* pb) { ServerState::tlocal()->remote_client_id_ = arg; });\n+    } else if (cmd == \"CLIENT-VERSION\" && args.size() == 2) {\n+      unsigned version;\n+      if (!absl::SimpleAtoi(arg, &version)) {\n+        return (*cntx)->SendError(kInvalidIntErr);\n+      }\n+      VLOG(1) << \"Client version for session_id=\"\n+              << cntx->conn_state.replication_info.repl_session_id << \" is \" << version;\n+      cntx->conn_state.replication_info.repl_version = DflyVersion(version);\n+    } else if (cmd == \"ACK\" && args.size() == 2) {\n+      // Don't send error/Ok back through the socket, because we don't want to interleave with\n+      // the journal writes that we write into the same socket.\n+\n+      if (!cntx->replication_flow) {\n+        LOG(ERROR) << \"No replication flow assigned\";\n+        return;\n+      }\n+\n+      uint64_t ack;\n+      if (!absl::SimpleAtoi(arg, &ack)) {\n+        LOG(ERROR) << \"Bad int in REPLCONF ACK command! arg=\" << arg;\n+        return;\n+      }\n+      VLOG(1) << \"Received client ACK=\" << ack;\n+      cntx->replication_flow->last_acked_lsn = ack;\n+      return;\n+    } else {\n+      VLOG(1) << cmd << \" \" << arg << \" \" << args.size();\n+      goto err;\n+    }\n+  }\n+\n+  (*cntx)->SendOk();\n+  return;\n+\n+err:\n+  LOG(ERROR) << \"Error in receiving command: \" << args;\n+  (*cntx)->SendError(kSyntaxErr);\n+}\n+\n+void ServerFamily::Role(CmdArgList args, ConnectionContext* cntx) {\n+  ServerState& etl = *ServerState::tlocal();\n+  if (etl.is_master) {\n+    (*cntx)->StartArray(2);\n+    (*cntx)->SendBulkString(\"master\");\n+    auto vec = dfly_cmd_->GetReplicasRoleInfo();\n+    (*cntx)->StartArray(vec.size());\n+    for (auto& data : vec) {\n+      (*cntx)->StartArray(3);\n+      (*cntx)->SendBulkString(data.address);\n+      (*cntx)->SendBulkString(absl::StrCat(data.listening_port));\n+      (*cntx)->SendBulkString(data.state);\n+    }\n+\n+  } else {\n+    auto replica_ptr = replica_;\n+    CHECK(replica_ptr);\n+    Replica::Info rinfo = replica_ptr->GetInfo();\n+    (*cntx)->StartArray(4);\n+    (*cntx)->SendBulkString(\"replica\");\n+    (*cntx)->SendBulkString(rinfo.host);\n+    (*cntx)->SendBulkString(absl::StrCat(rinfo.port));\n+    if (rinfo.full_sync_done) {\n+      (*cntx)->SendBulkString(\"stable_sync\");\n+    } else if (rinfo.full_sync_in_progress) {\n+      (*cntx)->SendBulkString(\"full_sync\");\n+    } else if (rinfo.master_link_established) {\n+      (*cntx)->SendBulkString(\"preparation\");\n+    } else {\n+      (*cntx)->SendBulkString(\"connecting\");\n+    }\n+  }\n+}\n+\n+void ServerFamily::Script(CmdArgList args, ConnectionContext* cntx) {\n+  ToUpper(&args.front());\n+\n+  script_mgr_->Run(std::move(args), cntx);\n+}\n+\n+void ServerFamily::Sync(CmdArgList args, ConnectionContext* cntx) {\n+  SyncGeneric(\"\", 0, cntx);\n+}\n+\n+void ServerFamily::Psync(CmdArgList args, ConnectionContext* cntx) {\n+  SyncGeneric(\"?\", 0, cntx);  // full sync, ignore the request.\n+}\n+\n+void ServerFamily::LastSave(CmdArgList args, ConnectionContext* cntx) {\n+  time_t save_time;\n+  {\n+    lock_guard lk(save_mu_);\n+    save_time = last_save_info_->save_time;\n+  }\n+  (*cntx)->SendLong(save_time);\n+}\n+\n+void ServerFamily::Latency(CmdArgList args, ConnectionContext* cntx) {\n+  ToUpper(&args[0]);\n+  string_view sub_cmd = ArgS(args, 0);\n+\n+  if (sub_cmd == \"LATEST\") {\n+    return (*cntx)->SendEmptyArray();\n+  }\n+\n+  LOG_FIRST_N(ERROR, 10) << \"Subcommand \" << sub_cmd << \" not supported\";\n+  (*cntx)->SendError(kSyntaxErr);\n+}\n+\n+void ServerFamily::ShutdownCmd(CmdArgList args, ConnectionContext* cntx) {\n+  if (args.size() > 1) {\n+    (*cntx)->SendError(kSyntaxErr);\n+    return;\n+  }\n+\n+  if (args.size() == 1) {\n+    auto sub_cmd = ArgS(args, 0);\n+    if (absl::EqualsIgnoreCase(sub_cmd, \"SAVE\")) {\n+    } else if (absl::EqualsIgnoreCase(sub_cmd, \"NOSAVE\")) {\n+      save_on_shutdown_ = false;\n+    } else {\n+      (*cntx)->SendError(kSyntaxErr);\n+      return;\n+    }\n+  }\n+\n+  service_.proactor_pool().AwaitFiberOnAll(\n+      [](ProactorBase* pb) { ServerState::tlocal()->EnterLameDuck(); });\n+\n+  CHECK_NOTNULL(acceptor_)->Stop();\n+  (*cntx)->SendOk();\n+}\n+\n+void ServerFamily::SyncGeneric(std::string_view repl_master_id, uint64_t offs,\n+                               ConnectionContext* cntx) {\n+  if (cntx->async_dispatch) {\n+    // SYNC is a special command that should not be sent in batch with other commands.\n+    // It should be the last command since afterwards the server just dumps the replication data.\n+    (*cntx)->SendError(\"Can not sync in pipeline mode\");\n+    return;\n+  }\n+\n+  cntx->replica_conn = true;\n+  ServerState::tl_connection_stats()->num_replicas += 1;\n+  // TBD.\n+}\n+\n+void ServerFamily::Dfly(CmdArgList args, ConnectionContext* cntx) {\n+  dfly_cmd_->Run(args, cntx);\n+}\n+\n+#define HFUNC(x) SetHandler(HandlerFunc(this, &ServerFamily::x))\n+\n+namespace acl {\n+constexpr uint32_t kAuth = FAST | CONNECTION;\n+constexpr uint32_t kBGSave = ADMIN | SLOW | DANGEROUS;\n+constexpr uint32_t kClient = SLOW | CONNECTION;\n+constexpr uint32_t kConfig = ADMIN | SLOW | DANGEROUS;\n+constexpr uint32_t kDbSize = KEYSPACE | READ | FAST;\n+constexpr uint32_t kDebug = ADMIN | SLOW | DANGEROUS;\n+constexpr uint32_t kFlushDB = KEYSPACE | WRITE | SLOW | DANGEROUS;\n+constexpr uint32_t kFlushAll = KEYSPACE | WRITE | SLOW | DANGEROUS;\n+constexpr uint32_t kInfo = SLOW | DANGEROUS;\n+constexpr uint32_t kHello = FAST | CONNECTION;\n+constexpr uint32_t kLastSave = ADMIN | FAST | DANGEROUS;\n+constexpr uint32_t kLatency = ADMIN | SLOW | DANGEROUS;\n+constexpr uint32_t kMemory = READ | SLOW;\n+constexpr uint32_t kSave = ADMIN | SLOW | DANGEROUS;\n+constexpr uint32_t kShutDown = ADMIN | SLOW | DANGEROUS;\n+constexpr uint32_t kSlaveOf = ADMIN | SLOW | DANGEROUS;\n+constexpr uint32_t kReplicaOf = ADMIN | SLOW | DANGEROUS;\n+constexpr uint32_t kReplTakeOver = DANGEROUS;\n+constexpr uint32_t kReplConf = ADMIN | SLOW | DANGEROUS;\n+constexpr uint32_t kRole = ADMIN | FAST | DANGEROUS;\n+constexpr uint32_t kSlowLog = ADMIN | SLOW | DANGEROUS;\n+constexpr uint32_t kScript = SLOW | SCRIPTING;\n+constexpr uint32_t kDfly = ADMIN;\n+}  // namespace acl\n+\n+void ServerFamily::Register(CommandRegistry* registry) {\n+  constexpr auto kReplicaOpts = CO::LOADING | CO::ADMIN | CO::GLOBAL_TRANS;\n+  constexpr auto kMemOpts = CO::LOADING | CO::READONLY | CO::FAST | CO::NOSCRIPT;\n+\n+  *registry\n+      << CI{\"AUTH\", CO::NOSCRIPT | CO::FAST | CO::LOADING, -2, 0, 0, 0, acl::kAuth}.HFUNC(Auth)\n+      << CI{\"BGSAVE\", CO::ADMIN | CO::GLOBAL_TRANS, 1, 0, 0, 0, acl::kBGSave}.HFUNC(Save)\n+      << CI{\"CLIENT\", CO::NOSCRIPT | CO::LOADING, -2, 0, 0, 0, acl::kClient}.HFUNC(Client)\n+      << CI{\"CONFIG\", CO::ADMIN, -2, 0, 0, 0, acl::kConfig}.HFUNC(Config)\n+      << CI{\"DBSIZE\", CO::READONLY | CO::FAST | CO::LOADING, 1, 0, 0, 0, acl::kDbSize}.HFUNC(DbSize)\n+      << CI{\"DEBUG\", CO::ADMIN | CO::LOADING, -2, 0, 0, 0, acl::kDebug}.HFUNC(Debug)\n+      << CI{\"FLUSHDB\", CO::WRITE | CO::GLOBAL_TRANS, 1, 0, 0, 0, acl::kFlushDB}.HFUNC(FlushDb)\n+      << CI{\"FLUSHALL\", CO::WRITE | CO::GLOBAL_TRANS, -1, 0, 0, 0, acl::kFlushAll}.HFUNC(FlushAll)\n+      << CI{\"INFO\", CO::LOADING, -1, 0, 0, 0, acl::kInfo}.HFUNC(Info)\n+      << CI{\"HELLO\", CO::LOADING, -1, 0, 0, 0, acl::kHello}.HFUNC(Hello)\n+      << CI{\"LASTSAVE\", CO::LOADING | CO::FAST, 1, 0, 0, 0, acl::kLastSave}.HFUNC(LastSave)\n+      << CI{\"LATENCY\", CO::NOSCRIPT | CO::LOADING | CO::FAST, -2, 0, 0, 0, acl::kLatency}.HFUNC(\n+             Latency)\n+      << CI{\"MEMORY\", kMemOpts, -2, 0, 0, 0, acl::kMemory}.HFUNC(Memory)\n+      << CI{\"SAVE\", CO::ADMIN | CO::GLOBAL_TRANS, -1, 0, 0, 0, acl::kSave}.HFUNC(Save)\n+      << CI{\"SHUTDOWN\", CO::ADMIN | CO::NOSCRIPT | CO::LOADING, -1, 0, 0, 0, acl::kShutDown}.HFUNC(\n+             ShutdownCmd)\n+      << CI{\"SLAVEOF\", kReplicaOpts, 3, 0, 0, 0, acl::kSlaveOf}.HFUNC(ReplicaOf)\n+      << CI{\"REPLICAOF\", kReplicaOpts, 3, 0, 0, 0, acl::kReplicaOf}.HFUNC(ReplicaOf)\n+      << CI{\"REPLTAKEOVER\", CO::ADMIN | CO::GLOBAL_TRANS, 2, 0, 0, 0, acl::kReplTakeOver}.HFUNC(\n+             ReplTakeOver)\n+      << CI{\"REPLCONF\", CO::ADMIN | CO::LOADING, -1, 0, 0, 0, acl::kReplConf}.HFUNC(ReplConf)\n+      << CI{\"ROLE\", CO::LOADING | CO::FAST | CO::NOSCRIPT, 1, 0, 0, 0, acl::kRole}.HFUNC(Role)\n+      << CI{\"SLOWLOG\", CO::ADMIN | CO::FAST, -2, 0, 0, 0, acl::kSlowLog}.SetHandler(SlowLog)\n+      << CI{\"SCRIPT\", CO::NOSCRIPT | CO::NO_KEY_JOURNAL, -2, 0, 0, 0, acl::kScript}.HFUNC(Script)\n+      << CI{\"DFLY\", CO::ADMIN | CO::GLOBAL_TRANS | CO::HIDDEN, -2, 0, 0, 0, acl::kDfly}.HFUNC(Dfly);\n+}\n+\n+}  // namespace dfly\n",
  "test_patch": "diff --git a/tests/dragonfly/management_test.py b/tests/dragonfly/management_test.py\nnew file mode 100644\nindex 000000000000..6eeed3248cc8\n--- /dev/null\n+++ b/tests/dragonfly/management_test.py\n@@ -0,0 +1,14 @@\n+import pytest\n+import asyncio\n+from redis import asyncio as aioredis\n+from redis.exceptions import ResponseError\n+\n+\n+@pytest.mark.asyncio\n+async def test_config_cmd(async_client: aioredis.Redis):\n+    with pytest.raises(ResponseError):\n+        await async_client.config_set(\"foo\", \"bar\")\n+    await async_client.config_set(\"requirepass\", \"foobar\") == \"OK\"\n+    res = await async_client.config_get(\"*\")\n+    assert len(res) > 0\n+    assert res[\"requirepass\"] == \"foobar\"\n",
  "problem_statement": "config get command support\nDue to the difference from the underlying architecture of redis, some previous config commands may not be fully supported, but can you have a complete display of the configurable items of dragonfly, similar to the function of `config get *`. From the user's point of view, I just don't know which configs can be configured and which ones can't. After all, I have transitioned from redis before, and some usage habits need to be adapted. \r\n\r\nI hope there will be a clear display of our configurable config items.\r\n<img width=\"530\" alt=\"image\" src=\"https://github.com/dragonflydb/dragonfly/assets/8543659/69a69fc8-518c-4e5a-9bae-949c8e1d6ff2\">\n",
  "hints_text": "@boomballa  why did you close it?\n@romange  Thanks for reopening.\r\nI heard it said yesterday that for the monitoring level of dragonfly, the monitoring items should be as streamlined as possible, and some important indicators should be kept. I don't know if there is something wrong with my understanding\r\n,I thought we had no plans to build this, so i closed it first.  \ud83d\ude2c\r\n\r\nIn fact, our current requirement is that if we want to use dragonfly, we need to modify it according to the python script monitored by redis before. If there are some important indicators in config get, it may be relatively troublesome to modify. So if we don't have a plan, we plan to try the grafana way first. The reason why we do not give priority to using grafana to collect monitoring information is that we have our own monitoring system. If we use grafana, there will be two sets of monitoring and alarm systems. The monitoring data collection caliber is not uniform, which will cause this problem.",
  "created_at": "2023-08-27T17:16:42Z",
  "modified_files": [
    "src/facade/error.h",
    "src/facade/facade.cc",
    "src/server/config_registry.cc",
    "src/server/config_registry.h",
    "src/server/main_service.cc",
    "src/server/server_family.cc",
    "b/src/server/server_family.cc.orig"
  ],
  "modified_test_files": [
    "b/tests/dragonfly/management_test.py"
  ]
}