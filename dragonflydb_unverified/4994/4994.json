{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4994,
  "instance_id": "dragonflydb__dragonfly-4994",
  "issue_numbers": [
    "4998"
  ],
  "base_commit": "d5c375235f2bc29ca8973fa4628a895b7efe390c",
  "patch": "diff --git a/src/facade/dragonfly_connection.cc b/src/facade/dragonfly_connection.cc\nindex 4c325e092d8b..8221193038ba 100644\n--- a/src/facade/dragonfly_connection.cc\n+++ b/src/facade/dragonfly_connection.cc\n@@ -13,6 +13,7 @@\n #include <numeric>\n #include <variant>\n \n+#include \"base/cycle_clock.h\"\n #include \"base/flags.h\"\n #include \"base/histogram.h\"\n #include \"base/io_buf.h\"\n@@ -23,7 +24,9 @@\n #include \"facade/memcache_parser.h\"\n #include \"facade/redis_parser.h\"\n #include \"facade/service_interface.h\"\n+#include \"glog/logging.h\"\n #include \"io/file.h\"\n+#include \"util/fibers/fibers.h\"\n #include \"util/fibers/proactor_base.h\"\n \n #ifdef DFLY_USE_SSL\n@@ -93,6 +96,10 @@ ABSL_FLAG(bool, migrate_connections, true,\n           \"they operate. Currently this is only supported for Lua script invocations, and can \"\n           \"happen at most once per connection.\");\n \n+ABSL_FLAG(uint32_t, max_busy_read_usec, 100,\n+          \"Maximum time we read and parse from \"\n+          \"a socket without yielding. In microseconds.\");\n+\n using namespace util;\n using namespace std;\n using absl::GetFlag;\n@@ -146,7 +153,7 @@ struct TrafficLogger {\n \n void TrafficLogger::ResetLocked() {\n   if (log_file) {\n-    log_file->Close();\n+    std::ignore = log_file->Close();\n     log_file.reset();\n   }\n }\n@@ -196,7 +203,7 @@ void OpenTrafficLogger(string_view base_path) {\n \n   // Write version, incremental numbering :)\n   uint8_t version[1] = {2};\n-  tl_traffic_logger.log_file->Write(version);\n+  std::ignore = tl_traffic_logger.log_file->Write(version);\n }\n \n void LogTraffic(uint32_t id, bool has_more, absl::Span<RespExpr> resp,\n@@ -876,6 +883,7 @@ pair<string, string> Connection::GetClientInfoBeforeAfterTid() const {\n   absl::StrAppend(&after, \" irqmatch=\", int(cpu == my_cpu_id));\n   if (dispatch_q_.size()) {\n     absl::StrAppend(&after, \" pipeline=\", dispatch_q_.size());\n+    absl::StrAppend(&after, \" pbuf=\", pending_pipeline_bytes_);\n   }\n   absl::StrAppend(&after, \" age=\", now - creation_time_, \" idle=\", now - last_interaction_);\n   string_view phase_name = PHASE_NAMES[phase_];\n@@ -1028,7 +1036,7 @@ void Connection::ConnectionFlow() {\n   if (io_buf_.InputLen() > 0) {\n     phase_ = PROCESS;\n     if (redis_parser_) {\n-      parse_status = ParseRedis();\n+      parse_status = ParseRedis(10000);\n     } else {\n       DCHECK(memcache_parser_);\n       parse_status = ParseMemcache();\n@@ -1136,19 +1144,6 @@ void Connection::DispatchSingle(bool has_more, absl::FunctionRef<void()> invoke_\n   // Dispatch async if we're handling a pipeline or if we can't dispatch sync.\n   if (optimize_for_async || !can_dispatch_sync) {\n     SendAsync(cmd_msg_cb());\n-\n-    auto epoch = fb2::FiberSwitchEpoch();\n-\n-    if (async_fiber_epoch_ == epoch) {\n-      // If we pushed too many items without context switching - yield\n-      if (++async_streak_len_ >= 10 && !cc_->async_dispatch) {\n-        async_streak_len_ = 0;\n-        ThisFiber::Yield();\n-      }\n-    } else {\n-      async_streak_len_ = 0;\n-      async_fiber_epoch_ = epoch;\n-    }\n   } else {\n     ShrinkPipelinePool();  // Gradually release pipeline request pool.\n     {\n@@ -1164,20 +1159,17 @@ void Connection::DispatchSingle(bool has_more, absl::FunctionRef<void()> invoke_\n   }\n }\n \n-Connection::ParserStatus Connection::ParseRedis() {\n+Connection::ParserStatus Connection::ParseRedis(unsigned max_busy_cycles) {\n   uint32_t consumed = 0;\n   RedisParser::Result result = RedisParser::OK;\n \n-  // Re-use connection local resources to reduce allocations\n-  RespVec& parse_args = tmp_parse_args_;\n-  CmdArgVec& cmd_vec = tmp_cmd_vec_;\n-\n-  auto dispatch_sync = [this, &parse_args, &cmd_vec] {\n-    RespExpr::VecToArgList(parse_args, &cmd_vec);\n-    service_->DispatchCommand(absl::MakeSpan(cmd_vec), reply_builder_.get(), cc_.get());\n+  auto dispatch_sync = [this] {\n+    RespExpr::VecToArgList(tmp_parse_args_, &tmp_cmd_vec_);\n+    service_->DispatchCommand(absl::MakeSpan(tmp_cmd_vec_), reply_builder_.get(), cc_.get());\n   };\n-  auto dispatch_async = [this, &parse_args, tlh = mi_heap_get_backing()]() -> MessageHandle {\n-    return {FromArgs(std::move(parse_args), tlh)};\n+\n+  auto dispatch_async = [this, tlh = mi_heap_get_backing()]() -> MessageHandle {\n+    return {FromArgs(std::move(tmp_parse_args_), tlh)};\n   };\n \n   ReadBuffer read_buffer = GetReadBuffer();\n@@ -1186,10 +1178,10 @@ Connection::ParserStatus Connection::ParseRedis() {\n     if (read_buffer.ShouldAdvance()) {  // can happen only with io_uring/bundles\n       read_buffer.slice = NextBundleBuffer(read_buffer.available_bytes);\n     }\n-    result = redis_parser_->Parse(read_buffer.slice, &consumed, &parse_args);\n+    result = redis_parser_->Parse(read_buffer.slice, &consumed, &tmp_parse_args_);\n     request_consumed_bytes_ += consumed;\n-    if (result == RedisParser::OK && !parse_args.empty()) {\n-      if (RespExpr& first = parse_args.front(); first.type == RespExpr::STRING)\n+    if (result == RedisParser::OK && !tmp_parse_args_.empty()) {\n+      if (RespExpr& first = tmp_parse_args_.front(); first.type == RespExpr::STRING)\n         DVLOG(2) << \"Got Args with first token \" << ToSV(first.GetBuf());\n \n       if (io_req_size_hist)\n@@ -1198,12 +1190,20 @@ Connection::ParserStatus Connection::ParseRedis() {\n       bool has_more = consumed < read_buffer.available_bytes;\n \n       if (tl_traffic_logger.log_file && IsMain() /* log only on the main interface */) {\n-        LogTraffic(id_, has_more, absl::MakeSpan(parse_args), service_->GetContextInfo(cc_.get()));\n+        LogTraffic(id_, has_more, absl::MakeSpan(tmp_parse_args_),\n+                   service_->GetContextInfo(cc_.get()));\n       }\n \n       DispatchSingle(has_more, dispatch_sync, dispatch_async);\n     }\n     read_buffer.Consume(consumed);\n+\n+    // We must yield from time to time to allow other fibers to run.\n+    // Specifically, if a client sends a huge chunk of data resulting in a very long pipeline,\n+    // we want to yield to allow AsyncFiber to actually execute on the pending pipeline.\n+    if (ThisFiber::GetRunningTimeCycles() > max_busy_cycles) {\n+      ThisFiber::Yield();\n+    }\n   } while (RedisParser::OK == result && read_buffer.available_bytes > 0 &&\n            !reply_builder_->GetError());\n \n@@ -1390,6 +1390,9 @@ auto Connection::IoLoop() -> variant<error_code, ParserStatus> {\n   ParserStatus parse_status = OK;\n \n   size_t max_iobfuf_len = GetFlag(FLAGS_max_client_iobuf_len);\n+  unsigned max_busy_read_cycles =\n+      (base::CycleClock::Frequency() * GetFlag(FLAGS_max_busy_read_usec)) / 1000000U;\n+\n   auto* peer = socket_.get();\n   recv_buf_.res_len = 0;\n \n@@ -1404,12 +1407,16 @@ auto Connection::IoLoop() -> variant<error_code, ParserStatus> {\n     bool is_iobuf_full = io_buf_.AppendLen() == 0;\n \n     if (redis_parser_) {\n-      parse_status = ParseRedis();\n+      parse_status = ParseRedis(max_busy_read_cycles);\n     } else {\n       DCHECK(memcache_parser_);\n       parse_status = ParseMemcache();\n     }\n \n+    if (reply_builder_->GetError()) {\n+      return reply_builder_->GetError();\n+    }\n+\n     if (parse_status == NEED_MORE) {\n       parse_status = OK;\n \n@@ -1429,11 +1436,9 @@ auto Connection::IoLoop() -> variant<error_code, ParserStatus> {\n                               [&]() { io_buf_.Reserve(std::min(max_iobfuf_len, parser_hint)); });\n         }\n \n-        // If we got a partial request and we couldn't parse the length, just\n-        // double the capacity.\n         // If we got a partial request because iobuf was full, grow it up to\n         // a reasonable limit to save on Recv() calls.\n-        if (io_buf_.AppendLen() < 64u || (is_iobuf_full && capacity < 4096)) {\n+        if (is_iobuf_full && capacity < max_iobfuf_len / 2) {\n           // Last io used most of the io_buf to the end.\n           UpdateIoBufCapacity(io_buf_, stats_, [&]() {\n             io_buf_.Reserve(capacity * 2);  // Valid growth range.\n@@ -1441,21 +1446,11 @@ auto Connection::IoLoop() -> variant<error_code, ParserStatus> {\n         }\n \n         DCHECK_GT(io_buf_.AppendLen(), 0U);\n-      } else if (io_buf_.AppendLen() == 0) {\n-        // We have a full buffer and we can not progress with parsing.\n-        // This means that we have request too large.\n-        LOG(ERROR) << \"Request is too large, closing connection\";\n-        parse_status = ERROR;\n-        break;\n       }\n     } else if (parse_status != OK) {\n       break;\n     }\n-    ec = reply_builder_->GetError();\n-  } while (peer->IsOpen() && !ec);\n-\n-  if (ec)\n-    return ec;\n+  } while (peer->IsOpen());\n \n   return parse_status;\n }\n@@ -1833,6 +1828,7 @@ void Connection::SendAsync(MessageHandle msg) {\n   // Squashing is only applied to redis commands\n   if (std::holds_alternative<PipelineMessagePtr>(msg.handle)) {\n     pending_pipeline_cmd_cnt_++;\n+    pending_pipeline_bytes_ += used_mem;\n   }\n \n   if (msg.IsControl()) {\n@@ -1869,7 +1865,10 @@ void Connection::RecycleMessage(MessageHandle msg) {\n \n   // Retain pipeline message in pool.\n   if (auto* pipe = get_if<PipelineMessagePtr>(&msg.handle); pipe) {\n+    DCHECK_GE(pending_pipeline_bytes_, used_mem);\n+    DCHECK_GE(pending_pipeline_cmd_cnt_, 1u);\n     pending_pipeline_cmd_cnt_--;\n+    pending_pipeline_bytes_ -= used_mem;\n     if (stats_->pipeline_cmd_cache_bytes < qbp.pipeline_cache_limit) {\n       stats_->pipeline_cmd_cache_bytes += (*pipe)->StorageCapacity();\n       pipeline_req_pool_.push_back(std::move(*pipe));\ndiff --git a/src/facade/dragonfly_connection.h b/src/facade/dragonfly_connection.h\nindex 3b3095a1ee53..5e1e1be97f3a 100644\n--- a/src/facade/dragonfly_connection.h\n+++ b/src/facade/dragonfly_connection.h\n@@ -367,7 +367,7 @@ class Connection : public util::Connection {\n   // Create new pipeline request, re-use from pool when possible.\n   PipelineMessagePtr FromArgs(RespVec args, mi_heap_t* heap);\n \n-  ParserStatus ParseRedis();\n+  ParserStatus ParseRedis(unsigned max_busy_cycles);\n   ParserStatus ParseMemcache();\n \n   void OnBreakCb(int32_t mask);\n@@ -427,6 +427,7 @@ class Connection : public util::Connection {\n   util::fb2::Fiber async_fb_;             // async fiber (if started)\n \n   uint64_t pending_pipeline_cmd_cnt_ = 0;  // how many queued Redis async commands in dispatch_q\n+  size_t pending_pipeline_bytes_ = 0;      // how many bytes of the queued Redis async commands\n \n   // how many bytes of the current request have been consumed\n   size_t request_consumed_bytes_ = 0;\n@@ -455,10 +456,6 @@ class Connection : public util::Connection {\n \n   unsigned parser_error_ = 0;\n \n-  // amount of times we enqued requests asynchronously during the same async_fiber_epoch_.\n-  unsigned async_streak_len_ = 0;\n-  uint64_t async_fiber_epoch_ = 0;\n-\n   BreakerCb breaker_cb_;\n \n   // Used by redis parser to avoid allocations\ndiff --git a/src/server/main_service.cc b/src/server/main_service.cc\nindex 91f003b5a466..bf5590b11923 100644\n--- a/src/server/main_service.cc\n+++ b/src/server/main_service.cc\n@@ -116,8 +116,8 @@ ABSL_FLAG(size_t, serialization_max_chunk_size, 64_KB,\n           \"Maximum size of a value that may be serialized at once during snapshotting or full \"\n           \"sync. Values bigger than this threshold will be serialized using streaming \"\n           \"serialization. 0 - to disable streaming mode\");\n-ABSL_FLAG(uint32_t, max_squashed_cmd_num, 32,\n-          \"Max number of commands squashed in command squash optimizaiton\");\n+ABSL_FLAG(uint32_t, max_squashed_cmd_num, 100,\n+          \"Max number of commands squashed in a single shard during squash optimizaiton\");\n \n namespace dfly {\n \ndiff --git a/src/server/server_family.cc b/src/server/server_family.cc\nindex 40c4243fda80..155577c2bd3f 100644\n--- a/src/server/server_family.cc\n+++ b/src/server/server_family.cc\n@@ -1314,6 +1314,20 @@ void PrintPrometheusMetrics(uint64_t uptime, const Metrics& m, DflyCmd* dfly_cmd\n   AppendMetricWithoutLabels(\"pipeline_commands_duration_seconds\", \"\",\n                             conn_stats.pipelined_cmd_latency * 1e-6, MetricType::COUNTER,\n                             &resp->body());\n+\n+  AppendMetricWithoutLabels(\"cmd_squash_hop_total\", \"\", m.coordinator_stats.multi_squash_executions,\n+                            MetricType::COUNTER, &resp->body());\n+\n+  AppendMetricWithoutLabels(\"cmd_squash_commands_total\", \"\", m.coordinator_stats.squashed_commands,\n+                            MetricType::COUNTER, &resp->body());\n+\n+  AppendMetricWithoutLabels(\"cmd_squash_hop_duration_seconds\", \"\",\n+                            m.coordinator_stats.multi_squash_exec_hop_usec * 1e-6,\n+                            MetricType::COUNTER, &resp->body());\n+  AppendMetricWithoutLabels(\"cmd_squash_hop_reply_seconds\", \"\",\n+                            m.coordinator_stats.multi_squash_exec_reply_usec * 1e-6,\n+                            MetricType::COUNTER, &resp->body());\n+\n   AppendMetricWithoutLabels(\"commands_squashing_replies_bytes\", \"\",\n                             MultiCommandSquasher::GetRepliesMemSize(), MetricType::GAUGE,\n                             &resp->body());\n@@ -2486,7 +2500,6 @@ string ServerFamily::FormatInfoMetrics(const Metrics& m, std::string_view sectio\n     append(\"total_commands_processed\", conn_stats.command_cnt_main + conn_stats.command_cnt_other);\n     append(\"instantaneous_ops_per_sec\", m.qps);\n     append(\"total_pipelined_commands\", conn_stats.pipelined_cmd_cnt);\n-    append(\"total_pipelined_squashed_commands\", m.coordinator_stats.squashed_commands);\n     append(\"pipeline_throttle_total\", conn_stats.pipeline_throttle_count);\n     append(\"pipelined_latency_usec\", conn_stats.pipelined_cmd_latency);\n     append(\"total_net_input_bytes\", conn_stats.io_read_bytes);\n@@ -2628,9 +2641,6 @@ string ServerFamily::FormatInfoMetrics(const Metrics& m, std::string_view sectio\n     append(\"eval_shardlocal_coordination_total\",\n            m.coordinator_stats.eval_shardlocal_coordination_cnt);\n     append(\"eval_squashed_flushes\", m.coordinator_stats.eval_squashed_flushes);\n-    append(\"multi_squash_execution_total\", m.coordinator_stats.multi_squash_executions);\n-    append(\"multi_squash_execution_hop_usec\", m.coordinator_stats.multi_squash_exec_hop_usec);\n-    append(\"multi_squash_execution_reply_usec\", m.coordinator_stats.multi_squash_exec_reply_usec);\n   };\n \n   auto add_repl_info = [&] {\ndiff --git a/tools/local/monitoring/grafana/provisioning/dashboards/dragonfly.json b/tools/local/monitoring/grafana/provisioning/dashboards/dragonfly.json\nindex 88e0476eaae1..85f1cb9d3c45 100644\n--- a/tools/local/monitoring/grafana/provisioning/dashboards/dragonfly.json\n+++ b/tools/local/monitoring/grafana/provisioning/dashboards/dragonfly.json\n@@ -1301,7 +1301,8 @@\n                 \"value\": 80\n               }\n             ]\n-          }\n+          },\n+          \"unit\": \"s\"\n         },\n         \"overrides\": []\n       },\n@@ -1311,7 +1312,7 @@\n         \"x\": 12,\n         \"y\": 29\n       },\n-      \"id\": 13,\n+      \"id\": 27,\n       \"options\": {\n         \"alertThreshold\": true,\n         \"legend\": {\n@@ -1335,15 +1336,14 @@\n           \"editorMode\": \"code\",\n           \"exemplar\": true,\n           \"expr\":\n-              \"sum (dragonfly_db_keys{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}) - sum (dragonfly_db_keys_expiring{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}) \",\n+              \"irate(dragonfly_pipeline_commands_duration_seconds{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}[$__rate_interval])/irate(dragonfly_pipeline_commands_total{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}[$__rate_interval])\",\n           \"format\": \"time_series\",\n           \"interval\": \"\",\n           \"intervalFactor\": 2,\n-          \"legendFormat\": \"not expiring\",\n+          \"legendFormat\": \"pipeline\",\n           \"range\": true,\n           \"refId\": \"A\",\n-          \"step\": 240,\n-          \"target\": \"\"\n+          \"step\": 240\n         },\n         {\n           \"datasource\": {\n@@ -1352,18 +1352,37 @@\n           },\n           \"editorMode\": \"code\",\n           \"exemplar\": true,\n-          \"expr\": \"sum (dragonfly_db_keys_expiring{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"})\",\n+          \"expr\":\n+              \"irate(dragonfly_cmd_squash_hop_duration_seconds{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}[$__rate_interval])/irate(dragonfly_cmd_squash_hop_total{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}[$__rate_interval])\",\n           \"format\": \"time_series\",\n+          \"hide\": false,\n           \"interval\": \"\",\n           \"intervalFactor\": 2,\n-          \"legendFormat\": \"expiring\",\n-          \"metric\": \"\",\n+          \"legendFormat\": \"execute_hop\",\n           \"range\": true,\n           \"refId\": \"B\",\n           \"step\": 240\n+        },\n+        {\n+          \"datasource\": {\n+            \"type\": \"prometheus\",\n+            \"uid\": \"${DS_PROMETHEUS}\"\n+          },\n+          \"editorMode\": \"code\",\n+          \"exemplar\": true,\n+          \"expr\":\n+              \"irate(dragonfly_cmd_squash_hop_reply_seconds{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}[$__rate_interval])/irate(dragonfly_cmd_squash_hop_total{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}[$__rate_interval])\",\n+          \"format\": \"time_series\",\n+          \"hide\": false,\n+          \"interval\": \"\",\n+          \"intervalFactor\": 2,\n+          \"legendFormat\": \"reply\",\n+          \"range\": true,\n+          \"refId\": \"C\",\n+          \"step\": 240\n         }\n       ],\n-      \"title\": \"Expiring vs Not-Expiring Keys\",\n+      \"title\": \"Pipeline Latency\",\n       \"type\": \"timeseries\"\n     },\n     {\n@@ -1428,7 +1447,7 @@\n       \"gridPos\": {\n         \"h\": 7,\n         \"w\": 12,\n-        \"x\": 12,\n+        \"x\": 0,\n         \"y\": 36\n       },\n       \"id\": 16,\n@@ -1466,19 +1485,6 @@\n       \"title\": \"Dragonfly connected clients\",\n       \"type\": \"timeseries\"\n     },\n-    {\n-      \"collapsed\": false,\n-      \"gridPos\": {\n-        \"h\": 1,\n-        \"w\": 24,\n-        \"x\": 0,\n-        \"y\": 43\n-      },\n-      \"id\": 19,\n-      \"panels\": [],\n-      \"title\": \"Advanced metrics\",\n-      \"type\": \"row\"\n-    },\n     {\n       \"datasource\": {\n         \"type\": \"prometheus\",\n@@ -1520,6 +1526,7 @@\n               \"mode\": \"off\"\n             }\n           },\n+          \"links\": [],\n           \"mappings\": [],\n           \"thresholds\": {\n             \"mode\": \"absolute\",\n@@ -1533,19 +1540,19 @@\n                 \"value\": 80\n               }\n             ]\n-          },\n-          \"unit\": \"s\"\n+          }\n         },\n         \"overrides\": []\n       },\n       \"gridPos\": {\n-        \"h\": 8,\n+        \"h\": 7,\n         \"w\": 12,\n-        \"x\": 0,\n-        \"y\": 44\n+        \"x\": 12,\n+        \"y\": 36\n       },\n-      \"id\": 18,\n+      \"id\": 13,\n       \"options\": {\n+        \"alertThreshold\": true,\n         \"legend\": {\n           \"calcs\": [],\n           \"displayMode\": \"list\",\n@@ -1564,17 +1571,18 @@\n             \"type\": \"prometheus\",\n             \"uid\": \"${DS_PROMETHEUS}\"\n           },\n-          \"disableTextWrap\": false,\n           \"editorMode\": \"code\",\n+          \"exemplar\": true,\n           \"expr\":\n-              \"irate(dragonfly_fiber_switch_delay_seconds_total{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}[$__rate_interval])/rate(dragonfly_fiber_switch_total{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}[$__rate_interval])\",\n-          \"fullMetaSearch\": false,\n-          \"includeNullMetadata\": false,\n-          \"instant\": false,\n-          \"legendFormat\": \"switch\",\n+              \"sum (dragonfly_db_keys{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}) - sum (dragonfly_db_keys_expiring{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}) \",\n+          \"format\": \"time_series\",\n+          \"interval\": \"\",\n+          \"intervalFactor\": 2,\n+          \"legendFormat\": \"not expiring\",\n           \"range\": true,\n           \"refId\": \"A\",\n-          \"useBackend\": false\n+          \"step\": 240,\n+          \"target\": \"\"\n         },\n         {\n           \"datasource\": {\n@@ -1582,16 +1590,19 @@\n             \"uid\": \"${DS_PROMETHEUS}\"\n           },\n           \"editorMode\": \"code\",\n-          \"expr\":\n-              \"irate(dragonfly_fiber_longrun_seconds{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}[$__rate_interval])/irate(dragonfly_fiber_longrun_total{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}[$__rate_interval])\",\n-          \"hide\": false,\n-          \"instant\": false,\n-          \"legendFormat\": \"longrun\",\n+          \"exemplar\": true,\n+          \"expr\": \"sum (dragonfly_db_keys_expiring{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"})\",\n+          \"format\": \"time_series\",\n+          \"interval\": \"\",\n+          \"intervalFactor\": 2,\n+          \"legendFormat\": \"expiring\",\n+          \"metric\": \"\",\n           \"range\": true,\n-          \"refId\": \"B\"\n+          \"refId\": \"B\",\n+          \"step\": 240\n         }\n       ],\n-      \"title\": \"FiberSwitchDelay\",\n+      \"title\": \"Expiring vs Not-Expiring Keys\",\n       \"type\": \"timeseries\"\n     },\n     {\n@@ -1655,8 +1666,8 @@\n       \"gridPos\": {\n         \"h\": 8,\n         \"w\": 12,\n-        \"x\": 12,\n-        \"y\": 44\n+        \"x\": 0,\n+        \"y\": 43\n       },\n       \"id\": 22,\n       \"options\": {\n@@ -1694,6 +1705,7 @@\n         \"type\": \"prometheus\",\n         \"uid\": \"${DS_PROMETHEUS}\"\n       },\n+      \"description\": \"\",\n       \"fieldConfig\": {\n         \"defaults\": {\n           \"color\": {\n@@ -1747,13 +1759,123 @@\n         },\n         \"overrides\": []\n       },\n+      \"gridPos\": {\n+        \"h\": 8,\n+        \"w\": 12,\n+        \"x\": 12,\n+        \"y\": 43\n+      },\n+      \"id\": 28,\n+      \"options\": {\n+        \"legend\": {\n+          \"calcs\": [],\n+          \"displayMode\": \"list\",\n+          \"placement\": \"bottom\",\n+          \"showLegend\": true\n+        },\n+        \"tooltip\": {\n+          \"mode\": \"single\",\n+          \"sort\": \"none\"\n+        }\n+      },\n+      \"pluginVersion\": \"10.1.10\",\n+      \"targets\": [\n+        {\n+          \"datasource\": {\n+            \"type\": \"prometheus\",\n+            \"uid\": \"${DS_PROMETHEUS}\"\n+          },\n+          \"editorMode\": \"code\",\n+          \"expr\":\n+              \"irate(dragonfly_cmd_squash_commands_total\\n{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}[$__rate_interval])/irate(dragonfly_cmd_squash_hop_total{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}[$__rate_interval])\",\n+          \"instant\": false,\n+          \"legendFormat\": \"__auto\",\n+          \"range\": true,\n+          \"refId\": \"A\"\n+        }\n+      ],\n+      \"title\": \"Average Squashing Length\",\n+      \"type\": \"timeseries\"\n+    },\n+    {\n+      \"collapsed\": false,\n+      \"gridPos\": {\n+        \"h\": 1,\n+        \"w\": 24,\n+        \"x\": 0,\n+        \"y\": 51\n+      },\n+      \"id\": 19,\n+      \"panels\": [],\n+      \"title\": \"Advanced metrics\",\n+      \"type\": \"row\"\n+    },\n+    {\n+      \"datasource\": {\n+        \"type\": \"prometheus\",\n+        \"uid\": \"${DS_PROMETHEUS}\"\n+      },\n+      \"fieldConfig\": {\n+        \"defaults\": {\n+          \"color\": {\n+            \"mode\": \"palette-classic\"\n+          },\n+          \"custom\": {\n+            \"axisCenteredZero\": false,\n+            \"axisColorMode\": \"text\",\n+            \"axisLabel\": \"\",\n+            \"axisPlacement\": \"auto\",\n+            \"barAlignment\": 0,\n+            \"drawStyle\": \"line\",\n+            \"fillOpacity\": 0,\n+            \"gradientMode\": \"none\",\n+            \"hideFrom\": {\n+              \"legend\": false,\n+              \"tooltip\": false,\n+              \"viz\": false\n+            },\n+            \"insertNulls\": false,\n+            \"lineInterpolation\": \"linear\",\n+            \"lineWidth\": 1,\n+            \"pointSize\": 5,\n+            \"scaleDistribution\": {\n+              \"type\": \"linear\"\n+            },\n+            \"showPoints\": \"auto\",\n+            \"spanNulls\": false,\n+            \"stacking\": {\n+              \"group\": \"A\",\n+              \"mode\": \"none\"\n+            },\n+            \"thresholdsStyle\": {\n+              \"mode\": \"off\"\n+            }\n+          },\n+          \"mappings\": [],\n+          \"thresholds\": {\n+            \"mode\": \"absolute\",\n+            \"steps\": [\n+              {\n+                \"color\": \"green\",\n+                \"value\": null\n+              },\n+              {\n+                \"color\": \"red\",\n+                \"value\": 80\n+              }\n+            ]\n+          },\n+          \"unit\": \"s\"\n+        },\n+        \"overrides\": []\n+      },\n       \"gridPos\": {\n         \"h\": 8,\n         \"w\": 12,\n         \"x\": 0,\n         \"y\": 52\n       },\n-      \"id\": 21,\n+      \"id\": 18,\n       \"options\": {\n         \"legend\": {\n           \"calcs\": [],\n@@ -1773,13 +1895,17 @@\n             \"type\": \"prometheus\",\n             \"uid\": \"${DS_PROMETHEUS}\"\n           },\n+          \"disableTextWrap\": false,\n           \"editorMode\": \"code\",\n           \"expr\":\n-              \"dragonfly_replication_full_sync_bytes{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}\",\n+              \"irate(dragonfly_fiber_switch_delay_seconds_total{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}[$__rate_interval])/rate(dragonfly_fiber_switch_total{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}[$__rate_interval])\",\n+          \"fullMetaSearch\": false,\n+          \"includeNullMetadata\": false,\n           \"instant\": false,\n-          \"legendFormat\": \"fullsync\",\n+          \"legendFormat\": \"switch\",\n           \"range\": true,\n-          \"refId\": \"A\"\n+          \"refId\": \"A\",\n+          \"useBackend\": false\n         },\n         {\n           \"datasource\": {\n@@ -1788,15 +1914,15 @@\n           },\n           \"editorMode\": \"code\",\n           \"expr\":\n-              \"dragonfly_replication_streaming_bytes{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}\",\n+              \"irate(dragonfly_fiber_longrun_seconds{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}[$__rate_interval])/irate(dragonfly_fiber_longrun_total{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}[$__rate_interval])\",\n           \"hide\": false,\n           \"instant\": false,\n-          \"legendFormat\": \"stable_sync\",\n+          \"legendFormat\": \"longrun\",\n           \"range\": true,\n           \"refId\": \"B\"\n         }\n       ],\n-      \"title\": \"Master Replication memory\",\n+      \"title\": \"FiberSwitchDelay\",\n       \"type\": \"timeseries\"\n     },\n     {\n@@ -1853,8 +1979,7 @@\n                 \"value\": 80\n               }\n             ]\n-          },\n-          \"unit\": \"s\"\n+          }\n         },\n         \"overrides\": []\n       },\n@@ -1864,7 +1989,7 @@\n         \"x\": 12,\n         \"y\": 52\n       },\n-      \"id\": 23,\n+      \"id\": 21,\n       \"options\": {\n         \"legend\": {\n           \"calcs\": [],\n@@ -1886,14 +2011,28 @@\n           },\n           \"editorMode\": \"code\",\n           \"expr\":\n-              \"irate(dragonfly_pipeline_commands_duration_seconds{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}[$__rate_interval])/irate(dragonfly_pipeline_commands_total{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}[$__rate_interval])\",\n+              \"dragonfly_replication_full_sync_bytes{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}\",\n           \"instant\": false,\n-          \"legendFormat\": \"{{pod}}\",\n+          \"legendFormat\": \"fullsync\",\n           \"range\": true,\n           \"refId\": \"A\"\n+        },\n+        {\n+          \"datasource\": {\n+            \"type\": \"prometheus\",\n+            \"uid\": \"${DS_PROMETHEUS}\"\n+          },\n+          \"editorMode\": \"code\",\n+          \"expr\":\n+              \"dragonfly_replication_streaming_bytes{namespace=\\\"$namespace\\\",pod=~\\\"$pod_name\\\"}\",\n+          \"hide\": false,\n+          \"instant\": false,\n+          \"legendFormat\": \"stable_sync\",\n+          \"range\": true,\n+          \"refId\": \"B\"\n         }\n       ],\n-      \"title\": \"Pipeline latency\",\n+      \"title\": \"Master Replication memory\",\n       \"type\": \"timeseries\"\n     }\n   ],\n",
  "test_patch": "diff --git a/tests/dragonfly/connection_test.py b/tests/dragonfly/connection_test.py\nindex 3af54832a757..7721555381d7 100755\n--- a/tests/dragonfly/connection_test.py\n+++ b/tests/dragonfly/connection_test.py\n@@ -561,13 +561,16 @@ async def measure(aw):\n     e = async_client.pipeline(transaction=True)\n     for _ in range(100):\n         e.incr(\"num-1\")\n-    assert await measure(e.execute()) == 2  # OK + Response\n+\n+    # one - for MULTI-OK, one for the rest. Depends on the squashing efficiency,\n+    # can be either 1 or 2 replies.\n+    assert await measure(e.execute()) <= 2\n \n     # Just pipeline\n     p = async_client.pipeline(transaction=False)\n     for _ in range(100):\n         p.incr(\"num-1\")\n-    assert await measure(p.execute()) == 1\n+    assert await measure(p.execute()) <= 2\n \n     # Script result\n     assert await measure(async_client.eval('return {1,2,{3,4},5,6,7,8,\"nine\"}', 0)) == 1\n@@ -1118,14 +1121,14 @@ async def wait_for_stuck_on_send():\n \n \n # Test that the cache pipeline does not grow or shrink under constant pipeline load.\n-@dfly_args({\"proactor_threads\": 1, \"pipeline_squash\": 9})\n+@dfly_args({\"proactor_threads\": 1, \"pipeline_squash\": 9, \"max_busy_read_usec\": 1000})\n async def test_pipeline_cache_only_async_squashed_dispatches(df_factory):\n     server = df_factory.create()\n     server.start()\n \n     client = server.client()\n \n-    async def push_pipeline(size=1):\n+    async def push_pipeline(size):\n         p = client.pipeline(transaction=True)\n         for i in range(size):\n             p.info()\n@@ -1136,14 +1139,15 @@ async def push_pipeline(size=1):\n     # should be zero because:\n     # We always dispatch the items that will be squashed, so when `INFO` gets called\n     # the cache is empty because the pipeline consumed it throughout its execution\n-    for i in range(0, 30):\n+    # high max_busy_read_usec ensures that the connection fiber has enough time to push\n+    # all the commands to reach the squashing limit.\n+    for i in range(0, 10):\n         # it's actually 11 commands. 8 INFO + 2 from the MULTI/EXEC block that is injected\n-        # by the client. Connection fiber yields to dispatch/async fiber when\n-        # ++async_streak_len_ >= 10. The minimum to squash is 9 so it will squash the pipeline\n+        # by the client. The minimum to squash is 9 so it will squash the pipeline\n         # and INFO ALL should return zero for all the squashed commands in the pipeline\n         res = await push_pipeline(8)\n-        for i in range(1):\n-            assert res[i][\"pipeline_cache_bytes\"] == 0\n+        for r in res:\n+            assert r[\"pipeline_cache_bytes\"] == 0\n \n     # Non zero because we reclaimed/recycled the messages back to the cache\n     info = await client.info()\n",
  "problem_statement": "Suboptimal P99 latency under pipelining workloads\nWe have not really focused on combination of P99 latency and pipelining, especially in the context \nof uncoordinated omission access patterns.  \n\nWhen investigating the elevated latency phenomena even under small CPU load I noticed the following:\n\n\n1. The connection fiber yields after reading 11 requests from the socket. This limits how much optimization we can do further down the road when processing batches of requests.\n2. When reading from the socket we limit the socket buffer to 4KB, unless a single request requires more. Which means that when reading series of small requests from the socket we can read at most K request that fit into 4KB. This causes a similar effect of the pipelining efficiency.\n3. For cluster mode, we shard our request by tags. Valkey Cluster spec allows processing a multi-key command that touches multiple tags that belong to the same slot if, i.e. `mget {foo}aaa {bar}aaa` is allowed as long as both `{foo}` and `{bar}` belong to the same shard. Dragonfly processes such requests correctly but this means these commands run on multiple shards. Unfortunately pipelining optimizations do not work well with multi-shard commands and loose this efficiency (can be fixed in the future)\n",
  "hints_text": "Regarding (1): we should still yield in the connnection fiber to let the AsyncFiber to unload requests - otherwise we will keep reading from the socket until all the data is read or we reached the pipelining limit. It is sub-optimal because of the lost opportunity to kick off the pipeline running in parallel to reading from the socket.\n\nThere are two possible solutions:\n1. Simple: To add a configurable counter limit and yield like we do today (default of 11 is still probably too low).\n2. Add a more sophisticated self tuning heuristic. For that we could take into account:\n    a. Hardcoded uper limit (1000?)\n    b. dispatch queue memory usage: after a certain threshold - yield \n    c. How many commands we could squash in one step: there is no point adding much more than this - if we will end up dividing the dispatch stream into a smaller batches for squashing.\nFinally whatever we do - we should comment why we do it and even link this issue for more details. \nHere is the general overview of the flow + the bugs we found\n\n![Image](https://github.com/user-attachments/assets/9554c936-8f17-48f9-b763-31de6792c133)",
  "created_at": "2025-04-24T15:44:25Z",
  "modified_files": [
    "src/facade/dragonfly_connection.cc",
    "src/facade/dragonfly_connection.h",
    "src/server/main_service.cc",
    "src/server/server_family.cc",
    "tools/local/monitoring/grafana/provisioning/dashboards/dragonfly.json"
  ],
  "modified_test_files": [
    "tests/dragonfly/connection_test.py"
  ]
}