{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 674,
  "instance_id": "dragonflydb__dragonfly-674",
  "issue_numbers": [
    "587"
  ],
  "base_commit": "50e14db6c749bc04432284cc5b2005269169b8ee",
  "patch": "diff --git a/src/redis/rdb.h b/src/redis/rdb.h\nindex c9d58eb36689..8f2284cb3dd3 100644\n--- a/src/redis/rdb.h\n+++ b/src/redis/rdb.h\n@@ -101,7 +101,7 @@\n /* NOTE: WHEN ADDING NEW RDB TYPE, UPDATE rdbIsObjectType() BELOW */\n \n /* Test if a type is an object type. */\n-#define rdbIsObjectType(t) ((t >= 0 && t <= 7) || (t >= 9 && t <= 18))\n+#define __rdbIsObjectType(t) ((t >= 0 && t <= 7) || (t >= 9 && t <= 18))\n \n /* Range 200-240 is used by Dragonfly specific opcodes */\n \ndiff --git a/src/server/error.h b/src/server/error.h\nindex 98b7187b45d5..f6f87c1e6112 100644\n--- a/src/server/error.h\n+++ b/src/server/error.h\n@@ -45,6 +45,7 @@ enum errc {\n   invalid_encoding = 9,\n   empty_key = 10,\n   out_of_memory = 11,\n+  bad_json_string = 12,\n };\n \n }  // namespace rdb\ndiff --git a/src/server/generic_family.cc b/src/server/generic_family.cc\nindex b288f3213a8a..925e2bc21148 100644\n--- a/src/server/generic_family.cc\n+++ b/src/server/generic_family.cc\n@@ -19,6 +19,7 @@ extern \"C\" {\n #include \"server/container_utils.h\"\n #include \"server/engine_shard_set.h\"\n #include \"server/error.h\"\n+#include \"server/rdb_extensions.h\"\n #include \"server/rdb_load.h\"\n #include \"server/rdb_save.h\"\n #include \"server/transaction.h\"\n@@ -134,7 +135,7 @@ class RdbRestoreValue : protected RdbLoaderBase {\n std::optional<RdbLoaderBase::OpaqueObj> RdbRestoreValue::Parse(std::string_view payload) {\n   InMemSource source(payload);\n   src_ = &source;\n-  if (io::Result<uint8_t> type_id = FetchType(); type_id && rdbIsObjectType(type_id.value())) {\n+  if (io::Result<uint8_t> type_id = FetchType(); type_id && rdbIsObjectTypeDF(type_id.value())) {\n     OpaqueObj obj;\n     error_code ec = ReadObj(type_id.value(), &obj);  // load the type from the input stream\n     if (ec) {\ndiff --git a/src/server/rdb_extensions.h b/src/server/rdb_extensions.h\nindex a4b37abb3dd3..7870d3ffb526 100644\n--- a/src/server/rdb_extensions.h\n+++ b/src/server/rdb_extensions.h\n@@ -4,13 +4,26 @@\n \n #pragma once\n \n-// Range 200-240 is used by DF extensions.\n+extern \"C\" {\n+#include \"redis/rdb.h\"\n+}\n+\n+//  Custom types: Range 20-25 is used by DF RDB types.\n+const uint8_t RDB_TYPE_JSON = 20;\n+\n+constexpr bool rdbIsObjectTypeDF(uint8_t type) {\n+  return __rdbIsObjectType(type) || (type == RDB_TYPE_JSON);\n+}\n+\n+//  Opcodes: Range 200-240 is used by DF extensions.\n \n // This opcode is sent by the master Dragonfly instance to a replica\n // to notify that it finished streaming static data and is ready\n // to switch to the stable state replication phase.\n const uint8_t RDB_OPCODE_FULLSYNC_END = 200;\n+\n const uint8_t RDB_OPCODE_COMPRESSED_ZSTD_BLOB_START = 201;\n const uint8_t RDB_OPCODE_COMPRESSED_LZ4_BLOB_START = 202;\n const uint8_t RDB_OPCODE_COMPRESSED_BLOB_END = 203;\n+\n const uint8_t RDB_OPCODE_JOURNAL_BLOB = 210;\ndiff --git a/src/server/rdb_load.cc b/src/server/rdb_load.cc\nindex abdb02dd2ee7..552beeeb6d80 100644\n--- a/src/server/rdb_load.cc\n+++ b/src/server/rdb_load.cc\n@@ -21,9 +21,12 @@ extern \"C\" {\n #include <lz4frame.h>\n #include <zstd.h>\n \n+#include <jsoncons/json.hpp>\n+\n #include \"base/endian.h\"\n #include \"base/flags.h\"\n #include \"base/logging.h\"\n+#include \"core/json_object.h\"\n #include \"core/string_map.h\"\n #include \"core/string_set.h\"\n #include \"server/engine_shard_set.h\"\n@@ -353,9 +356,9 @@ class RdbLoaderBase::OpaqueObjLoader {\n   }\n \n   void operator()(const base::PODArray<char>& str);\n-\n   void operator()(const LzfString& lzfstr);\n   void operator()(const unique_ptr<LoadTrace>& ptr);\n+  void operator()(const JsonType& jt);\n \n   std::error_code ec() const {\n     return ec_;\n@@ -426,6 +429,10 @@ void RdbLoaderBase::OpaqueObjLoader::operator()(const unique_ptr<LoadTrace>& ptr\n   }\n }\n \n+void RdbLoaderBase::OpaqueObjLoader::operator()(const JsonType& json) {\n+  pv_->SetJson(JsonType{json});\n+}\n+\n void RdbLoaderBase::OpaqueObjLoader::CreateSet(const LoadTrace* ltrace) {\n   size_t len = ltrace->arr.size();\n \n@@ -1209,6 +1216,8 @@ error_code RdbLoaderBase::ReadObj(int rdbtype, OpaqueObj* dest) {\n     case RDB_TYPE_STREAM_LISTPACKS:\n       iores = ReadStreams();\n       break;\n+    case RDB_TYPE_JSON:\n+      iores = ReadJson();\n       break;\n     default:\n       LOG(ERROR) << \"Unsupported rdb type \" << rdbtype;\n@@ -1597,6 +1606,17 @@ auto RdbLoaderBase::ReadStreams() -> io::Result<OpaqueObj> {\n   return OpaqueObj{std::move(load_trace), RDB_TYPE_STREAM_LISTPACKS};\n }\n \n+auto RdbLoaderBase::ReadJson() -> io::Result<OpaqueObj> {\n+  string json_str;\n+  SET_OR_UNEXPECT(FetchGenericString(), json_str);\n+\n+  auto json = JsonFromString(json_str);\n+  if (!json)\n+    return Unexpected(errc::bad_json_string);\n+\n+  return OpaqueObj{std::move(*json), RDB_TYPE_JSON};\n+}\n+\n template <typename T> io::Result<T> RdbLoaderBase::FetchInt() {\n   auto ec = EnsureRead(sizeof(T));\n   if (ec)\n@@ -1804,7 +1824,7 @@ error_code RdbLoader::Load(io::Source* src) {\n       continue;\n     }\n \n-    if (!rdbIsObjectType(type)) {\n+    if (!rdbIsObjectTypeDF(type)) {\n       return RdbError(errc::invalid_rdb_type);\n     }\n \ndiff --git a/src/server/rdb_load.h b/src/server/rdb_load.h\nindex cfd763c89c96..bf9d46583587 100644\n--- a/src/server/rdb_load.h\n+++ b/src/server/rdb_load.h\n@@ -4,6 +4,7 @@\n #pragma once\n \n #include <boost/fiber/mutex.hpp>\n+#include <jsoncons/json.hpp>\n #include <system_error>\n \n extern \"C\" {\n@@ -12,6 +13,7 @@ extern \"C\" {\n \n #include \"base/io_buf.h\"\n #include \"base/pod_array.h\"\n+#include \"core/json_object.h\"\n #include \"core/mpsc_intrusive_queue.h\"\n #include \"io/io.h\"\n #include \"server/common.h\"\n@@ -39,8 +41,8 @@ class RdbLoaderBase {\n     uint64_t uncompressed_len;\n   };\n \n-  using RdbVariant =\n-      std::variant<long long, base::PODArray<char>, LzfString, std::unique_ptr<LoadTrace>>;\n+  using RdbVariant = std::variant<long long, base::PODArray<char>, LzfString,\n+                                  std::unique_ptr<LoadTrace>, JsonType>;\n \n   struct OpaqueObj {\n     RdbVariant obj;\n@@ -121,6 +123,7 @@ class RdbLoaderBase {\n   ::io::Result<OpaqueObj> ReadZSetZL();\n   ::io::Result<OpaqueObj> ReadListQuicklist(int rdbtype);\n   ::io::Result<OpaqueObj> ReadStreams();\n+  ::io::Result<OpaqueObj> ReadJson();\n \n   std::error_code HandleCompressedBlob(int op_type);\n   std::error_code HandleCompressedBlobFinish();\ndiff --git a/src/server/rdb_save.cc b/src/server/rdb_save.cc\nindex 704d6fab68ae..242b0fd924f0 100644\n--- a/src/server/rdb_save.cc\n+++ b/src/server/rdb_save.cc\n@@ -10,6 +10,8 @@\n #include <lz4frame.h>\n #include <zstd.h>\n \n+#include <jsoncons/json.hpp>\n+\n extern \"C\" {\n #include \"redis/intset.h\"\n #include \"redis/listpack.h\"\n@@ -23,6 +25,7 @@ extern \"C\" {\n \n #include \"base/flags.h\"\n #include \"base/logging.h\"\n+#include \"core/json_object.h\"\n #include \"core/string_map.h\"\n #include \"core/string_set.h\"\n #include \"server/engine_shard_set.h\"\n@@ -132,6 +135,8 @@ uint8_t RdbObjectType(unsigned type, unsigned compact_enc) {\n       return RDB_TYPE_STREAM_LISTPACKS;\n     case OBJ_MODULE:\n       return RDB_TYPE_MODULE_2;\n+    case OBJ_JSON:\n+      return RDB_TYPE_JSON;\n   }\n   LOG(FATAL) << \"Unknown encoding \" << compact_enc << \" for type \" << type;\n   return 0; /* avoid warning */\n@@ -284,9 +289,11 @@ io::Result<uint8_t> RdbSerializer::SaveEntry(const PrimeKey& pk, const PrimeValu\n   ec = SaveString(key);\n   if (ec)\n     return make_unexpected(ec);\n+\n   ec = SaveValue(pv);\n   if (ec)\n     return make_unexpected(ec);\n+\n   return rdb_type;\n }\n \n@@ -314,6 +321,10 @@ error_code RdbSerializer::SaveObject(const PrimeValue& pv) {\n     return SaveStreamObject(pv.AsRObj());\n   }\n \n+  if (obj_type == OBJ_JSON) {\n+    return SaveJsonObject(pv);\n+  }\n+\n   LOG(ERROR) << \"Not implemented \" << obj_type;\n   return make_error_code(errc::function_not_supported);\n }\n@@ -529,6 +540,11 @@ error_code RdbSerializer::SaveStreamObject(const robj* obj) {\n   return error_code{};\n }\n \n+error_code RdbSerializer::SaveJsonObject(const PrimeValue& pv) {\n+  auto json_string = pv.GetJson()->to_string();\n+  return SaveString(json_string);\n+}\n+\n /* Save a long long value as either an encoded string or a string. */\n error_code RdbSerializer::SaveLongLongAsString(int64_t value) {\n   uint8_t buf[32];\ndiff --git a/src/server/rdb_save.h b/src/server/rdb_save.h\nindex cbc4f6ca52ba..081618ec4e9a 100644\n--- a/src/server/rdb_save.h\n+++ b/src/server/rdb_save.h\n@@ -165,6 +165,8 @@ class RdbSerializer {\n   std::error_code SaveHSetObject(const PrimeValue& pv);\n   std::error_code SaveZSetObject(const robj* obj);\n   std::error_code SaveStreamObject(const robj* obj);\n+  std::error_code SaveJsonObject(const PrimeValue& pv);\n+\n   std::error_code SaveLongLongAsString(int64_t value);\n   std::error_code SaveBinaryDouble(double val);\n   std::error_code SaveListPackAsZiplist(uint8_t* lp);\n",
  "test_patch": "diff --git a/tests/dragonfly/utility.py b/tests/dragonfly/utility.py\nindex f26604fad1c9..641efffc6108 100644\n--- a/tests/dragonfly/utility.py\n+++ b/tests/dragonfly/utility.py\n@@ -7,6 +7,7 @@\n import itertools\n import time\n import difflib\n+import json\n from enum import Enum\n \n \n@@ -64,6 +65,7 @@ class ValueType(Enum):\n     SET = 2\n     HSET = 3\n     ZSET = 4\n+    JSON = 5\n \n     @staticmethod\n     def randomize():\n@@ -141,23 +143,35 @@ def rand_str(k=3, s=''):\n \n         if t == ValueType.STRING:\n             # Random string for MSET\n-            return rand_str(self.val_size)\n+            return (rand_str(self.val_size),)\n         elif t == ValueType.LIST:\n             # Random sequence k-letter elements for LPUSH\n-            return ' '.join(rand_str() for _ in range(self.val_size//4))\n+            return tuple(rand_str() for _ in range(self.val_size//4))\n         elif t == ValueType.SET:\n             # Random sequence of k-letter elements for SADD\n-            return ' '.join(rand_str() for _ in range(self.val_size//4))\n+            return tuple(rand_str() for _ in range(self.val_size//4))\n         elif t == ValueType.HSET:\n             # Random sequence of k-letter keys + int and two start values for HSET\n-            return 'v0 0 v1 0 ' + ' '.join(\n-                rand_str() + ' ' + str(random.randint(0, self.val_size))\n-                for _ in range(self.val_size//5)\n-            )\n-        else:\n+            elements = ((rand_str(), random.randint(0, self.val_size))\n+                        for _ in range(self.val_size//5))\n+            return ('v0', 0.0, 'v1', 0.0) + tuple(itertools.chain(*elements))\n+        elif t == ValueType.ZSET:\n             # Random sequnce of k-letter keys and int score for ZSET\n-            return ' '.join(str(random.randint(0, self.val_size)) + ' ' + rand_str()\n-                            for _ in range(self.val_size//4))\n+            elements = ((random.randint(0, self.val_size), rand_str())\n+                        for _ in range(self.val_size//4))\n+            return tuple(itertools.chain(*elements))\n+\n+        elif t == ValueType.JSON:\n+            # Json object with keys:\n+            # - arr (array of random strings)\n+            # - ints (array of objects {i:random integer})\n+            # - i (random integer)\n+            ints = [{\"i\": random.randint(0, 100)}\n+                    for i in range(self.val_size//6)]\n+            strs = [rand_str() for _ in range(self.val_size//6)]\n+            return \"$\", json.dumps({\"arr\": strs, \"ints\": ints, \"i\": random.randint(0, 100)})\n+        else:\n+            assert False, \"Invalid ValueType\"\n \n     def gen_shrink_cmd(self):\n         \"\"\"\n@@ -176,12 +190,15 @@ def gen_shrink_cmd(self):\n         ('SETRANGE {k} 10 {val}', ValueType.STRING),\n         ('LPUSH {k} {val}', ValueType.LIST),\n         ('LPOP {k}', ValueType.LIST),\n-        #('SADD {k} {val}', ValueType.SET),\n-        #('SPOP {k}', ValueType.SET),\n-        #('HSETNX {k} v0 {val}', ValueType.HSET),\n-        #('HINCRBY {k} v1 1', ValueType.HSET),\n-        #('ZPOPMIN {k} 1', ValueType.ZSET),\n-        #('ZADD {k} 0 {val}', ValueType.ZSET)\n+        # ('SADD {k} {val}', ValueType.SET),\n+        # ('SPOP {k}', ValueType.SET),\n+        # ('HSETNX {k} v0 {val}', ValueType.HSET),\n+        # ('HINCRBY {k} v1 1', ValueType.HSET),\n+        # ('ZPOPMIN {k} 1', ValueType.ZSET),\n+        # ('ZADD {k} 0 {val}', ValueType.ZSET)\n+        ('JSON.NUMINCRBY {k} $..i 1', ValueType.JSON),\n+        ('JSON.ARRPOP {k} $.arr', ValueType.JSON),\n+        ('JSON.ARRAPPEND {k} $.arr \"{val}\"', ValueType.JSON)\n     ]\n \n     def gen_update_cmd(self):\n@@ -190,7 +207,7 @@ def gen_update_cmd(self):\n         \"\"\"\n         cmd, t = random.choice(self.UPDATE_ACTIONS)\n         k, _ = self.randomize_key(t)\n-        val = ''.join(random.choices(string.ascii_letters, k=4))\n+        val = ''.join(random.choices(string.ascii_letters, k=3))\n         return cmd.format(k=f\"k{k}\", val=val) if k is not None else None, 0\n \n     GROW_ACTINONS = {\n@@ -198,7 +215,8 @@ def gen_update_cmd(self):\n         ValueType.LIST: 'LPUSH',\n         ValueType.SET: 'SADD',\n         ValueType.HSET: 'HMSET',\n-        ValueType.ZSET: 'ZADD'\n+        ValueType.ZSET: 'ZADD',\n+        ValueType.JSON: 'JSON.SET'\n     }\n \n     def gen_grow_cmd(self):\n@@ -213,8 +231,11 @@ def gen_grow_cmd(self):\n             count = 1\n \n         keys = (self.add_key(t) for _ in range(count))\n-        payload = \" \".join(f\"k{k}\" + \" \" + self.generate_val(t) for k in keys)\n-        return self.GROW_ACTINONS[t] + \" \" + payload, count\n+        payload = itertools.chain(\n+            *((f\"k{k}\",) + self.generate_val(t) for k in keys))\n+        filtered_payload = filter(lambda p: p is not None, payload)\n+\n+        return (self.GROW_ACTINONS[t],) + tuple(filtered_payload), count\n \n     def make(self, action):\n         \"\"\" Create command for action and return it together with number of keys added (removed)\"\"\"\n@@ -313,7 +334,7 @@ class DflySeeder:\n         assert await seeder.compare(capture, port=1112)\n     \"\"\"\n \n-    def __init__(self, port=6379, keys=1000, val_size=50, batch_size=100, max_multikey=5, dbcount=1, multi_transaction_probability=0.3 , log_file=None):\n+    def __init__(self, port=6379, keys=1000, val_size=50, batch_size=100, max_multikey=5, dbcount=1, multi_transaction_probability=0.3, log_file=None):\n         self.gen = CommandGenerator(\n             keys, val_size, batch_size, max_multikey\n         )\n@@ -407,6 +428,12 @@ def should_run():\n                 return False\n             return True\n \n+        def stringify_cmd(cmd):\n+            if isinstance(cmd, tuple):\n+                return \" \".join(str(c) for c in cmd)\n+            else:\n+                return str(cmd)\n+\n         while should_run():\n             start_time = time.time()\n             blob, deviation = self.gen.generate()\n@@ -421,7 +448,7 @@ def should_run():\n             if file is not None:\n                 if is_multi_transaction:\n                     file.write('MULTI\\n')\n-                file.write('\\n'.join(blob))\n+                file.write('\\n'.join(stringify_cmd(cmd) for cmd in blob))\n                 file.write('\\n')\n                 if is_multi_transaction:\n                     file.write('EXEC\\n')\n@@ -429,7 +456,8 @@ def should_run():\n             print('.', end='', flush=True)\n             await asyncio.sleep(0.0)\n \n-        print(\"\\ncpu time\", cpu_time, \"batches\", batches, \"commands\", submitted)\n+        print(\"\\ncpu time\", cpu_time, \"batches\",\n+              batches, \"commands\", submitted)\n \n         await asyncio.gather(*(q.put(None) for q in queues))\n         for q in queues:\n@@ -438,7 +466,7 @@ def should_run():\n         if file is not None:\n             file.flush()\n \n-        return submitted * self.gen.batch_size\n+        return submitted\n \n     async def _executor_task(self, db, queue):\n         client = aioredis.Redis(port=self.port, db=db)\n@@ -451,9 +479,16 @@ async def _executor_task(self, db, queue):\n \n             pipe = client.pipeline(transaction=tx_data[1])\n             for cmd in tx_data[0]:\n-                pipe.execute_command(cmd)\n+                if isinstance(cmd, str):\n+                    pipe.execute_command(cmd)\n+                else:\n+                    pipe.execute_command(*cmd)\n+\n+            try:\n+                await pipe.execute()\n+            except Exception as e:\n+                raise SystemExit(e)\n \n-            await pipe.execute()\n             queue.task_done()\n         await client.connection_pool.disconnect()\n \n@@ -463,7 +498,9 @@ async def _executor_task(self, db, queue):\n         ValueType.SET: lambda pipe, k: pipe.smembers(k),\n         ValueType.HSET: lambda pipe, k: pipe.hgetall(k),\n         ValueType.ZSET: lambda pipe, k: pipe.zrange(\n-            k, start=0, end=-1, withscores=True)\n+            k, start=0, end=-1, withscores=True),\n+        ValueType.JSON: lambda pipe, k: pipe.execute_command(\n+            \"JSON.GET\", k, \"$\")\n     }\n \n     CAPTURE_EXTRACTORS = {\n@@ -472,7 +509,8 @@ async def _executor_task(self, db, queue):\n         ValueType.SET: lambda res, tostr: sorted(tostr(s) for s in res),\n         ValueType.HSET: lambda res, tostr: sorted(tostr(k)+\"=\"+tostr(v) for k, v in res.items()),\n         ValueType.ZSET: lambda res, tostr: (\n-            tostr(s)+\"-\"+str(f) for (s, f) in res)\n+            tostr(s)+\"-\"+str(f) for (s, f) in res),\n+        ValueType.JSON: lambda res, tostr: (tostr(res),)\n     }\n \n     async def _capture_entries(self, client, keys):\n",
  "problem_statement": "JSON type should be supported for replication and saves.\n**Did you search GitHub Issues and GitHub Discussions First?**\r\nThis become an issue after #238 that added the new type JSON.\r\n\r\n**Is your feature request related to a problem? Please describe.**\r\nThis can become an issue in case a save or replication is made when the dataset contain JSON type.\r\n\r\n**Describe the solution you'd like**\r\nBeing able to save/replication dataset that contain JSON entries.\r\nFor example:\r\nRunning the following\r\n```\r\nJSON.SET doc $ '{\"a\":2}'\r\n```\r\nwill add a new entry for key \"doc\" with the value type being JSON.\r\nThis then cannot be replication or save since this type is not part of the RDB type system.\r\nIt is not expected that this would become part of RDB types, and would be compatible with REDIS.\r\n\r\n**Describe alternatives you've considered**\r\nSince the JSON type is not part of the RDB data types, the only way would be to save this as a string and somehow know that it should then read back as JSON, but this would have the issue that this is not supported as well.\r\n\r\n**Additional context**\r\nThis would required it own formatting as JSON has more internal types (such as bool, float, string, arrays, decimal and so on) than RDB, and it is not either subset or super set of it.\r\n\n",
  "hints_text": "Also for dump, restore ",
  "created_at": "2023-01-13T16:42:04Z",
  "modified_files": [
    "src/redis/rdb.h",
    "src/server/error.h",
    "src/server/generic_family.cc",
    "src/server/rdb_extensions.h",
    "src/server/rdb_load.cc",
    "src/server/rdb_load.h",
    "src/server/rdb_save.cc",
    "src/server/rdb_save.h"
  ],
  "modified_test_files": [
    "tests/dragonfly/utility.py"
  ]
}