{
  "repo": "dragonflydb/dragonfly",
  "pull_number": 4495,
  "instance_id": "dragonflydb__dragonfly-4495",
  "issue_numbers": [
    "4455"
  ],
  "base_commit": "4a2f2e3d04a3643a00acfa80584d1144e6ed4192",
  "patch": "diff --git a/src/core/dash.h b/src/core/dash.h\nindex 05a99560f21a..de7a025d44d3 100644\n--- a/src/core/dash.h\n+++ b/src/core/dash.h\n@@ -409,6 +409,13 @@ class DashTable<_Key, _Value, Policy>::Iterator {\n     return *this;\n   }\n \n+  Iterator& AdvanceIfNotOccupied() {\n+    if (!IsOccupied()) {\n+      this->operator++();\n+    }\n+    return *this;\n+  }\n+\n   IteratorPairType operator->() const {\n     auto* seg = owner_->segment_[seg_id_];\n     return {seg->Key(bucket_id_, slot_id_), seg->Value(bucket_id_, slot_id_)};\ndiff --git a/src/server/journal/streamer.cc b/src/server/journal/streamer.cc\nindex 7cc3e038e2e1..9dd2d9472d6c 100644\n--- a/src/server/journal/streamer.cc\n+++ b/src/server/journal/streamer.cc\n@@ -231,7 +231,7 @@ void RestoreStreamer::Run() {\n       ThisFiber::Yield();\n       last_yield = 0;\n     }\n-  } while (cursor);\n+  } while (cursor && !fiber_cancelled_);\n \n   VLOG(1) << \"RestoreStreamer finished loop of \" << my_slots_.ToSlotRanges().ToString()\n           << \", shard \" << db_slice_->shard_id() << \". Buckets looped \" << stats_.buckets_loop;\n@@ -302,7 +302,7 @@ bool RestoreStreamer::WriteBucket(PrimeTable::bucket_iterator it) {\n \n     it.SetVersion(snapshot_version_);\n     string key_buffer;  // we can reuse it\n-    for (; !it.is_done(); ++it) {\n+    for (it.AdvanceIfNotOccupied(); !it.is_done(); ++it) {\n       const auto& pv = it->second;\n       string_view key = it->first.GetSlice(&key_buffer);\n       if (ShouldWrite(key)) {\ndiff --git a/src/server/snapshot.cc b/src/server/snapshot.cc\nindex 4c2abbe51a23..c6c64261abb9 100644\n--- a/src/server/snapshot.cc\n+++ b/src/server/snapshot.cc\n@@ -291,11 +291,10 @@ unsigned SliceSnapshot::SerializeBucket(DbIndex db_index, PrimeTable::bucket_ite\n   it.SetVersion(snapshot_version_);\n   unsigned result = 0;\n \n-  while (!it.is_done()) {\n+  for (it.AdvanceIfNotOccupied(); !it.is_done(); ++it) {\n     ++result;\n     // might preempt due to big value serialization.\n     SerializeEntry(db_index, it->first, it->second);\n-    ++it;\n   }\n   serialize_bucket_running_ = false;\n   return result;\n",
  "test_patch": "diff --git a/tests/dragonfly/cluster_test.py b/tests/dragonfly/cluster_test.py\nindex 9f0cbbd46ec2..76de4918364a 100644\n--- a/tests/dragonfly/cluster_test.py\n+++ b/tests/dragonfly/cluster_test.py\n@@ -2685,3 +2685,80 @@ async def test_migration_timeout_on_sync(df_factory: DflyInstanceFactory, df_see\n     await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])\n \n     assert (await StaticSeeder.capture(nodes[1].client)) == start_capture\n+\n+\n+\"\"\"\n+Test cluster node distributing its slots into 2 other nodes.\n+In this test we start migrating to the second node only after the first one finished to\n+reproduce the bug found in issue #4455\n+\"\"\"\n+\n+\n+@pytest.mark.asyncio\n+@dfly_args({\"proactor_threads\": 4, \"cluster_mode\": \"yes\"})\n+async def test_migration_one_after_another(df_factory: DflyInstanceFactory, df_seeder_factory):\n+    # 1. Create cluster of 3 nodes with all slots allocated to first node.\n+    instances = [\n+        df_factory.create(\n+            port=next(next_port),\n+            admin_port=next(next_port),\n+            vmodule=\"outgoing_slot_migration=2,cluster_family=2,incoming_slot_migration=2,streamer=2\",\n+        )\n+        for i in range(3)\n+    ]\n+    df_factory.start_all(instances)\n+\n+    nodes = [(await create_node_info(instance)) for instance in instances]\n+    nodes[0].slots = [(0, 16383)]\n+    nodes[1].slots = []\n+    nodes[2].slots = []\n+    await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])\n+\n+    logging.debug(\"DEBUG POPULATE first node\")\n+    key_num = 100000\n+    await StaticSeeder(key_target=key_num, data_size=100).run(nodes[0].client)\n+    dbsize_node0 = await nodes[0].client.dbsize()\n+    assert dbsize_node0 > (key_num * 0.95)\n+\n+    # 2. Start migrating part of the slots from first node to second\n+    logging.debug(\"Start first migration\")\n+    nodes[0].migrations.append(\n+        MigrationInfo(\"127.0.0.1\", nodes[1].instance.admin_port, [(0, 16300)], nodes[1].id)\n+    )\n+    await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])\n+\n+    # 3. Wait for migratin finish\n+    await wait_for_status(nodes[0].admin_client, nodes[1].id, \"FINISHED\", timeout=50)\n+    await wait_for_status(nodes[1].admin_client, nodes[0].id, \"FINISHED\", timeout=50)\n+\n+    nodes[0].migrations = []\n+    nodes[0].slots = [(16301, 16383)]\n+    nodes[1].slots = [(0, 16300)]\n+    nodes[2].slots = []\n+    await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])\n+\n+    # 4. Start migrating remaind slots from first node to third node\n+    logging.debug(\"Start second migration\")\n+    nodes[0].migrations.append(\n+        MigrationInfo(\"127.0.0.1\", nodes[2].instance.admin_port, [(16301, 16383)], nodes[2].id)\n+    )\n+    await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])\n+\n+    # 5. Wait for migratin finish\n+    await wait_for_status(nodes[0].admin_client, nodes[2].id, \"FINISHED\", timeout=10)\n+    await wait_for_status(nodes[2].admin_client, nodes[0].id, \"FINISHED\", timeout=10)\n+\n+    nodes[0].migrations = []\n+    nodes[0].slots = []\n+    nodes[1].slots = [(0, 16300)]\n+    nodes[2].slots = [(16301, 16383)]\n+    await push_config(json.dumps(generate_config(nodes)), [node.admin_client for node in nodes])\n+\n+    # 6. Check all data was migrated\n+    # Using dbsize to check all the data was migrated to the other nodes.\n+    # Note: we can not use the seeder capture as we migrate the data to 2 different nodes.\n+    # TODO: improve the migration conrrectness by running the seeder capture on slot range (requiers changes in capture script).\n+    dbsize_node1 = await nodes[1].client.dbsize()\n+    dbsize_node2 = await nodes[2].client.dbsize()\n+    assert dbsize_node1 + dbsize_node2 == dbsize_node0\n+    assert dbsize_node2 > 0 and dbsize_node1 > 0\n",
  "problem_statement": "Dragonfly crashes during migrations\nDragonfly crashes during migrations, with stack trace:\n```\n#0  __pthread_kill_implementation (threadid=281474836474368, signo=signo@entry=11, no_tid=no_tid@entry=0) at ./nptl/pthread_kill.c:44\n#1  0x0000fffff7d97690 in __pthread_kill_internal (signo=11, threadid=<optimized out>) at ./nptl/pthread_kill.c:78\n#2  0x0000fffff7d4cb3c in __GI_raise (sig=11) at ../sysdeps/posix/raise.c:26\n#3  <signal handler called>\n#4  0x0000aaaaaaf2e4fc in dfly::DashTable<dfly::CompactObj, dfly::CompactObj, dfly::detail::PrimeTablePolicy>::Iterator<false, true>::GetVersion<true> (this=<optimized out>)\n    at /var/lib/dragonfly/dragonfly/dragonfly/src/core/dash.h:430\n#5  dfly::RestoreStreamer::WriteBucket (this=this@entry=0x3bd5e0c1598, it=...) at /var/lib/dragonfly/dragonfly/dragonfly/src/server/journal/streamer.cc:300\n#6  0x0000aaaaaaf2f178 in dfly::RestoreStreamer::OnDbChange (this=0x3bd5e0c1598, db_index=<optimized out>, req=...) at /var/lib/dragonfly/dragonfly/dragonfly/src/server/journal/streamer.cc:337\n#7  0x0000aaaaaaee31cc in std::function<void (unsigned short, dfly::DbSlice::ChangeReq const&)>::operator()(unsigned short, dfly::DbSlice::ChangeReq const&) const (__args#1=..., __args#0=<optimized out>, \n    this=0x3bd5e0e5558) at /usr/include/c++/13/bits/std_function.h:591\n#8  dfly::DbSlice::FlushChangeToEarlierCallbacks (this=this@entry=0x3bd5e0f0440, db_ind=db_ind@entry=0, it=..., upper_bound=3889649) at /var/lib/dragonfly/dragonfly/dragonfly/src/server/db_slice.cc:1196\n#9  0x0000aaaaaaf2eaf8 in operator() (it=..., __closure=<optimized out>) at /usr/include/c++/13/bits/allocator.h:184\n#10 dfly::DashTable<dfly::CompactObj, dfly::CompactObj, dfly::detail::PrimeTablePolicy>::TraverseBuckets<dfly::RestoreStreamer::Run()::<lambda(dfly::DashTable<dfly::CompactObj, dfly::CompactObj, dfly::detail::PrimeTablePolicy>::bucket_iterator)> > (cb=..., cursor=..., this=<optimized out>) at /var/lib/dragonfly/dragonfly/dragonfly/src/core/dash.h:1003\n#11 dfly::RestoreStreamer::Run (this=0x3bd5e0c1e98) at /var/lib/dragonfly/dragonfly/dragonfly/src/server/journal/streamer.cc:211\n#12 0x0000aaaaaae00224 in std::function<void (std::unique_ptr<dfly::cluster::OutgoingMigration::SliceSlotMigration, std::default_delete<dfly::cluster::OutgoingMigration::SliceSlotMigration> >&)>::operator()(std::unique_ptr<dfly::cluster::OutgoingMigration::SliceSlotMigration, std::default_delete<dfly::cluster::OutgoingMigration::SliceSlotMigration> >&) const (__args#0=..., this=<optimized out>)\n    at /usr/include/c++/13/bits/std_function.h:591\n#13 operator() (__closure=<synthetic pointer>, pb=<optimized out>) at /var/lib/dragonfly/dragonfly/dragonfly/src/server/cluster/outgoing_slot_migration.cc:137\n#14 operator() (context=<optimized out>, __closure=<synthetic pointer>) at /var/lib/dragonfly/dragonfly/dragonfly/helio/util/proactor_pool.h:194\n#15 std::__invoke_impl<void, util::ProactorPool::AwaitFiberOnAll<dfly::cluster::OutgoingMigration::OnAllShards(std::function<void(std::unique_ptr<SliceSlotMigration>&)>)::<lambda(util::fb2::ProactorBase*)> >(dfly::cluster::OutgoingMigration::OnAllShards(std::function<void(std::unique_ptr<SliceSlotMigration>&)>)::<lambda(util::fb2::ProactorBase*)>&&)::<lambda(util::ProactorPool::ProactorBase*)>, util::fb2::ProactorBase*> (\n    __f=<synthetic pointer>) at /usr/include/c++/13/bits/invoke.h:61\n#16 std::__invoke<util::ProactorPool::AwaitFiberOnAll<dfly::cluster::OutgoingMigration::OnAllShards(std::function<void(std::unique_ptr<SliceSlotMigration>&)>)::<lambda(util::fb2::ProactorBase*)> >(dfly::cluster::OutgoingMigration::OnAllShards(std::function<void(std::unique_ptr<SliceSlotMigration>&)>)::<lambda(util::fb2::ProactorBase*)>&&)::<lambda(util::ProactorPool::ProactorBase*)>, util::fb2::ProactorBase*> (\n    __fn=<synthetic pointer>) at /usr/include/c++/13/bits/invoke.h:96\n#17 std::__apply_impl<util::ProactorPool::AwaitFiberOnAll<dfly::cluster::OutgoingMigration::OnAllShards(std::function<void(std::unique_ptr<SliceSlotMigration>&)>)::<lambda(util::fb2::ProactorBase*)> >(dfly::cluster::OutgoingMigration::OnAllShards(std::function<void(std::unique_ptr<SliceSlotMigration>&)>)::<lambda(util::fb2::ProactorBase*)>&&)::<lambda(util::ProactorPool::ProactorBase*)>, std::tuple<util::fb2::ProactorBase*>, 0> (__t=<synthetic pointer>, __f=<synthetic pointer>) at /usr/include/c++/13/tuple:2302\n#18 std::apply<util::ProactorPool::AwaitFiberOnAll<dfly::cluster::OutgoingMigration::OnAllShards(std::function<void(std::unique_ptr<SliceSlotMigration>&)>)::<lambda(util::fb2::ProactorBase*)> >(dfly::cluster::OutgoingMigration::OnAllShards(std::function<void(std::unique_ptr<SliceSlotMigration>&)>)::<lambda(util::fb2::ProactorBase*)>&&)::<lambda(util::ProactorPool::ProactorBase*)>, std::tuple<util::fb2::ProactorBase*> > (\n    __t=<synthetic pointer>, __f=<synthetic pointer>) at /usr/include/c++/13/tuple:2313\n#19 util::fb2::detail::WorkerFiberImpl<const util::ProactorPool::AwaitFiberOnAll<dfly::cluster::OutgoingMigration::OnAllShards(std::function<void(std::unique_ptr<SliceSlotMigration>&)>)::<lambda(util::fb2::ProactorBase*)> >(dfly::cluster::OutgoingMigration::OnAllShards(std::function<void(std::unique_ptr<SliceSlotMigration>&)>)::<lambda(util::fb2::ProactorBase*)>&&)::<lambda(util::ProactorPool::ProactorBase*)>, util::fb2::ProactorBase*&>::run_ (c=..., this=0xffffc4040e00) at /var/lib/dragonfly/dragonfly/dragonfly/helio/util/fibers/detail/fiber_interface.h:304\n\n```\n\nRunning Dragonfly v1.26.1\n\nThis is running 100x100GB shards, each populated to 75% memory. I'll add more information below...\n",
  "hints_text": "an 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaabe8d0b8         16  google::LogMessage::Fail()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaabe8cfc0        144  google::LogMessage::SendToLog()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaabe8c7c0         80  google::LogMessage::Flush()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaabe905a0         32  google::LogMessageFatal::~LogMessageFatal()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaaac69344        176  __assert_fail\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaaac93b34         16  dfly::detail::Segment<>::Key()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaaade71f0         80  dfly::DashTable<>::Iterator<>::operator->()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab50ff6c        176  dfly::RestoreStreamer::WriteBucket()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab50f11c        176  dfly::RestoreStreamer::Run()::{lambda()#1}::operator()()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab511278        112  dfly::DashTable<>::TraverseBuckets<>()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab50f384        240  dfly::RestoreStreamer::Run()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e8650         48  dfly::cluster::OutgoingMigration::SliceSlotMigration::R>\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1de248         32  dfly::cluster::OutgoingMigration::SyncFb()::{lambda()#4>\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e44c8         32  std::__invoke_impl<>()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e3494         64  std::__invoke_r<>()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e2390         48  std::_Function_handler<>::_M_invoke()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e8cc8         48  std::function<>::operator()()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1dd8ac         48  dfly::cluster::OutgoingMigration::OnAllShards()::{lambd>\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e1208         64  util::ProactorPool::AwaitFiberOnAll<>()::{lambda()#1}::>\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e6540         32  std::__invoke_impl<>()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e61d0         64  std::__invoke<>()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e5cac         48  std::__apply_impl<>()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e5cf4         64  std::apply<>()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e5d84        112  util::fb2::detail::WorkerFiberImpl<>::run_()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e5724         64  util::fb2::detail::WorkerFiberImpl<>::WorkerFiberImpl<>>\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @     0xaaaaab1e7810         80  std::__invoke_impl<>()\nJan 20 14:58:53 ip-10-5-15-59 dragonfly[1896]:     @ ... and at least 4 more frames\nJan 20 15:01:06 ip-10-5-15-59 systemd[1]: dragonfly.service: Main process exited, code=dumped, status=6/ABRT\nJan 20 15:01:06 ip-10-5-15-59 systemd[1]: dragonfly.service: Failed with result 'core-dump'.\nAssumption for a flow which can lead to this crash\nA Cluster node gets new cluster config after migration was done leads to flushslots callback register, this callback deletes entries from table.\nAnother migration start from this cluster node starting RestoreStreamer::Run \nWhen calling the callback passed to TraverseBuckets we first call FlushChangeToEarlierCallbacks which will delete entries on this bucket and than will call the WriteBucket which will fail on the assert as the iterator to the entry in dash table points to an entry which is not longer occupied (GetBusy on the slot is false).\n\nHow to fix - fix the for loop in RestoreStreamer::WriteBucket to access only valid entries.\nTo test - create a test with 2 outgoing migrations from a cluster node. start the second migration after the first migration is finished\nI was able to reproduce this crash with my test",
  "created_at": "2025-01-22T10:27:37Z",
  "modified_files": [
    "src/core/dash.h",
    "src/server/journal/streamer.cc",
    "src/server/snapshot.cc"
  ],
  "modified_test_files": [
    "tests/dragonfly/cluster_test.py"
  ]
}