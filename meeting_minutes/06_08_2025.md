# Meeting Updates

## Questions

- The testing process takes very long time, each iteration with only 7 problems takes almost 2 hours with k=5, not sure if there's anything I can do about that
  - Right now I'm just running everything on AWS since it takes forever and I have email notifications whenever it finishes running
- What should I benchmark for my final data? so that I can get started on making sure it all works because I don't want to have to run the benchmark when I'm running out of time
- Anthropic seems to still have very low rate limits. The Aider website also mentions some alternatives ways to access Anthropic models that we could check out 
- Do you think its worth still checking out 1-2 more repos? DuckDB has a bunch of easy/hard issues with varying success rates, but having more will be good variety. Just not sure if I have the time to go through and select/verify PRs from other repo

- While testing problem 12982, I found that Aider tried to install Playwright for better web scraping of the github pages, which fails, and it also hit a rate limit in tokens per minute

litellm.RateLimitError: RateLimitError: OpenAIException - Request too large for 
o3 in organization org-Ecd8nzvL53f1xVu44Ttzj0Du on tokens per min (TPM): Limit 
800000, Requested 8033938. The input or output tokens must be reduced in order to
run successfully. Visit https://platform.openai.com/account/rate-limits to learn 
more.

  - Details are in log file 2025-07-31_15:13:58.log at around line 468357
  - One potential solution is to do something like SWE-Bench Lite, which is subset of SWE-Bench on more self-contained problems
    - Issue is what can be considered "self-contained"? And does it end up making the benchmark too easy to pass
  - Or maybe see if Aider can set max context limit? Then we choose the context limit to be the minimum of the models that I want to test
  - Another issue I found for this issue is that it tries to scrape the website included in the example code in the github issue description, which is a lichess.org and it fails a few times

- Also, since Aider already has RAG, do I need the BM25? So in this case its a combo of Oracle and RAG since we provide the file to be changed, and then Aider RAGs the rest of the context

- Since Aider does a lot of the RAG, creating context, am I doing too little work?
  - Also are we just testing the LLMs ability? Or also Aiders in generating a repo map and selecting relevant context
  - But if we use Aider for every model then maybe the only difference is the LLMs themselves? So maybe that does show the difference in capabilities?
    - Aider uses different edit formats (e.g. whole file, diff) depending on what they consider "best" for each model

- I tried gpt-4 to try an older model that definitely should have worse results, but hit token limit issues on many of the problems

## Completed

- Benchmark script with Aider is basically done

## In Progress

- Implement BM25 for finding the most appropriate files

## Meeting Notes