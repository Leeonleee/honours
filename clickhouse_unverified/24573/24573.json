{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 24573,
  "instance_id": "ClickHouse__ClickHouse-24573",
  "issue_numbers": [
    "1821"
  ],
  "base_commit": "2b0cd9cc2979ffa994c57bb5a13b483c4b246ed3",
  "patch": "diff --git a/programs/client/Suggest.cpp b/programs/client/Suggest.cpp\nindex 029388a0bdad..d4db49a15988 100644\n--- a/programs/client/Suggest.cpp\n+++ b/programs/client/Suggest.cpp\n@@ -6,6 +6,7 @@\n #include <IO/WriteBufferFromString.h>\n #include <IO/Operators.h>\n \n+\n namespace DB\n {\n namespace ErrorCodes\ndiff --git a/src/Common/Throttler.cpp b/src/Common/Throttler.cpp\nnew file mode 100644\nindex 000000000000..3462abfeb54d\n--- /dev/null\n+++ b/src/Common/Throttler.cpp\n@@ -0,0 +1,108 @@\n+#include <Common/Throttler.h>\n+#include <Common/ProfileEvents.h>\n+#include <Common/Exception.h>\n+#include <Common/Stopwatch.h>\n+#include <IO/WriteHelpers.h>\n+#include <cmath>\n+\n+namespace ProfileEvents\n+{\n+    extern const Event ThrottlerSleepMicroseconds;\n+}\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int LIMIT_EXCEEDED;\n+}\n+\n+/// Just 10^9.\n+static constexpr auto NS = 1000000000UL;\n+\n+/// Tracking window. Actually the size is not really important. We just want to avoid\n+/// throttles when there are no actions for a long period time.\n+static const double window_ns = 7UL * NS;\n+\n+void Throttler::add(size_t amount)\n+{\n+    size_t new_count;\n+    /// This outer variable is always equal to smoothed_speed.\n+    /// We use to avoid race condition.\n+    double current_speed = 0;\n+\n+    {\n+        std::lock_guard lock(mutex);\n+\n+        auto now = clock_gettime_ns();\n+        /// If prev_ns is equal to zero (first `add` call) we known nothing about speed\n+        /// and don't track anything.\n+        if (max_speed && prev_ns != 0)\n+        {\n+            /// Time spent to process the amount of bytes\n+            double time_spent = now - prev_ns;\n+\n+            /// The speed in bytes per second is equal to amount / time_spent in seconds\n+            auto new_speed = amount / (time_spent / NS);\n+\n+            /// We want to make old values of speed less important for our smoothed value\n+            /// so we decay it's value with coef.\n+            auto decay_coeff = std::pow(0.5, time_spent / window_ns);\n+\n+            /// Weighted average between previous and new speed\n+            smoothed_speed = smoothed_speed * decay_coeff + (1 - decay_coeff) * new_speed;\n+            current_speed = smoothed_speed;\n+        }\n+\n+        count += amount;\n+        new_count = count;\n+        prev_ns = now;\n+    }\n+\n+    if (limit && new_count > limit)\n+        throw Exception(limit_exceeded_exception_message + std::string(\" Maximum: \") + toString(limit), ErrorCodes::LIMIT_EXCEEDED);\n+\n+    if (max_speed && current_speed > max_speed)\n+    {\n+        /// If we was too fast then we have to sleep until our smoothed speed became <= max_speed\n+        int64_t sleep_time = -window_ns * std::log2(max_speed / current_speed);\n+\n+        if (sleep_time > 0)\n+        {\n+            accumulated_sleep += sleep_time;\n+\n+            sleepForNanoseconds(sleep_time);\n+\n+            accumulated_sleep -= sleep_time;\n+\n+            ProfileEvents::increment(ProfileEvents::ThrottlerSleepMicroseconds, sleep_time / 1000UL);\n+        }\n+    }\n+\n+    if (parent)\n+        parent->add(amount);\n+}\n+\n+void Throttler::reset()\n+{\n+    std::lock_guard lock(mutex);\n+\n+    count = 0;\n+    accumulated_sleep = 0;\n+    smoothed_speed = 0;\n+    prev_ns = 0;\n+}\n+\n+bool Throttler::isThrottling() const\n+{\n+    if (accumulated_sleep != 0)\n+        return true;\n+\n+    if (parent)\n+        return parent->isThrottling();\n+\n+    return false;\n+}\n+\n+}\ndiff --git a/src/Common/Throttler.h b/src/Common/Throttler.h\nindex 172cce783d84..a30df74d510a 100644\n--- a/src/Common/Throttler.h\n+++ b/src/Common/Throttler.h\n@@ -2,32 +2,16 @@\n \n #include <mutex>\n #include <memory>\n-#include <Common/Stopwatch.h>\n-#include <Common/Exception.h>\n-#include <Common/ProfileEvents.h>\n #include <common/sleep.h>\n-#include <IO/WriteHelpers.h>\n-\n-\n-namespace ProfileEvents\n-{\n-    extern const Event ThrottlerSleepMicroseconds;\n-}\n-\n+#include <atomic>\n \n namespace DB\n {\n \n-namespace ErrorCodes\n-{\n-    extern const int LIMIT_EXCEEDED;\n-}\n-\n-\n /** Allows you to limit the speed of something (in entities per second) using sleep.\n   * Specifics of work:\n-  * - only the average speed is considered, from the moment of the first call of `add` function;\n-  *   if there were periods with low speed, then during some time after them, the speed will be higher;\n+  *  Tracks exponentially (pow of 1/2) smoothed speed with hardcoded window.\n+  *  See more comments in .cpp file.\n   *\n   * Also allows you to set a limit on the maximum number of entities. If exceeded, an exception will be thrown.\n   */\n@@ -41,49 +25,9 @@ class Throttler\n               const std::shared_ptr<Throttler> & parent_ = nullptr)\n         : max_speed(max_speed_), limit(limit_), limit_exceeded_exception_message(limit_exceeded_exception_message_), parent(parent_) {}\n \n-    void add(const size_t amount)\n-    {\n-        size_t new_count;\n-        UInt64 elapsed_ns = 0;\n-\n-        {\n-            std::lock_guard lock(mutex);\n-\n-            if (max_speed)\n-            {\n-                if (0 == count)\n-                {\n-                    watch.start();\n-                    elapsed_ns = 0;\n-                }\n-                else\n-                    elapsed_ns = watch.elapsed();\n-            }\n-\n-            count += amount;\n-            new_count = count;\n-        }\n-\n-        if (limit && new_count > limit)\n-            throw Exception(limit_exceeded_exception_message + std::string(\" Maximum: \") + toString(limit), ErrorCodes::LIMIT_EXCEEDED);\n-\n-        if (max_speed)\n-        {\n-            /// How much time to wait for the average speed to become `max_speed`.\n-            UInt64 desired_ns = new_count * 1000000000 / max_speed;\n-\n-            if (desired_ns > elapsed_ns)\n-            {\n-                UInt64 sleep_ns = desired_ns - elapsed_ns;\n-                sleepForNanoseconds(sleep_ns);\n-\n-                ProfileEvents::increment(ProfileEvents::ThrottlerSleepMicroseconds, sleep_ns / 1000UL);\n-            }\n-        }\n-\n-        if (parent)\n-            parent->add(amount);\n-    }\n+    /// Calculates the smoothed speed, sleeps if required and throws exception on\n+    /// limit overflow.\n+    void add(size_t amount);\n \n     /// Not thread safe\n     void setParent(const std::shared_ptr<Throttler> & parent_)\n@@ -91,21 +35,23 @@ class Throttler\n         parent = parent_;\n     }\n \n-    void reset()\n-    {\n-        std::lock_guard lock(mutex);\n+    /// Reset all throttlers internal stats\n+    void reset();\n \n-        count = 0;\n-        watch.reset();\n-    }\n+    /// Is throttler already accumulated some sleep time and throttling.\n+    bool isThrottling() const;\n \n private:\n-    size_t count = 0;\n-    const size_t max_speed = 0;\n-    const UInt64 limit = 0;        /// 0 - not limited.\n+    size_t count{0};\n+    const size_t max_speed{0};\n+    const uint64_t limit{0};        /// 0 - not limited.\n     const char * limit_exceeded_exception_message = nullptr;\n-    Stopwatch watch {CLOCK_MONOTONIC_COARSE};\n     std::mutex mutex;\n+    std::atomic<uint64_t> accumulated_sleep{0};\n+    /// Smoothed value of current speed. Updated in `add` method.\n+    double smoothed_speed{0};\n+    /// previous `add` call time (in nanoseconds)\n+    uint64_t prev_ns{0};\n \n     /// Used to implement a hierarchy of throttlers\n     std::shared_ptr<Throttler> parent;\ndiff --git a/src/Common/ya.make b/src/Common/ya.make\nindex 57b60e9cce50..2da45e02b41d 100644\n--- a/src/Common/ya.make\n+++ b/src/Common/ya.make\n@@ -80,6 +80,7 @@ SRCS(\n     ThreadPool.cpp\n     ThreadProfileEvents.cpp\n     ThreadStatus.cpp\n+    Throttler.cpp\n     TimerDescriptor.cpp\n     TraceCollector.cpp\n     UTF8Helpers.cpp\ndiff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex bc308b56599e..dfcb7d11acee 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -83,6 +83,8 @@ class IColumn;\n     M(UInt64, background_schedule_pool_size, 16, \"Number of threads performing background tasks for replicated tables, dns cache updates. Only has meaning at server startup.\", 0) \\\n     M(UInt64, background_message_broker_schedule_pool_size, 16, \"Number of threads performing background tasks for message streaming. Only has meaning at server startup.\", 0) \\\n     M(UInt64, background_distributed_schedule_pool_size, 16, \"Number of threads performing background tasks for distributed sends. Only has meaning at server startup.\", 0) \\\n+    M(UInt64, max_replicated_fetches_network_bandwidth_for_server, 0, \"The maximum speed of data exchange over the network in bytes per second for replicated fetches. Zero means unlimited. Only has meaning at server startup.\", 0) \\\n+    M(UInt64, max_replicated_sends_network_bandwidth_for_server, 0, \"The maximum speed of data exchange over the network in bytes per second for replicated sends. Zero means unlimited. Only has meaning at server startup.\", 0) \\\n     \\\n     M(Milliseconds, distributed_directory_monitor_sleep_time_ms, 100, \"Sleep time for StorageDistributed DirectoryMonitors, in case of any errors delay grows exponentially.\", 0) \\\n     M(Milliseconds, distributed_directory_monitor_max_sleep_time_ms, 30000, \"Maximum sleep time for StorageDistributed DirectoryMonitors, it limits exponential growth too.\", 0) \\\ndiff --git a/src/IO/copyData.cpp b/src/IO/copyData.cpp\nindex c653af761d4d..8a044b50de94 100644\n--- a/src/IO/copyData.cpp\n+++ b/src/IO/copyData.cpp\n@@ -1,4 +1,5 @@\n #include <Common/Exception.h>\n+#include <Common/Throttler.h>\n #include <IO/ReadBuffer.h>\n #include <IO/WriteBuffer.h>\n #include <IO/copyData.h>\n@@ -14,7 +15,7 @@ namespace ErrorCodes\n namespace\n {\n \n-void copyDataImpl(ReadBuffer & from, WriteBuffer & to, bool check_bytes, size_t bytes, const std::atomic<int> * is_cancelled)\n+void copyDataImpl(ReadBuffer & from, WriteBuffer & to, bool check_bytes, size_t bytes, const std::atomic<int> * is_cancelled, ThrottlerPtr throttler)\n {\n     /// If read to the end of the buffer, eof() either fills the buffer with new data and moves the cursor to the beginning, or returns false.\n     while (bytes > 0 && !from.eof())\n@@ -27,13 +28,16 @@ void copyDataImpl(ReadBuffer & from, WriteBuffer & to, bool check_bytes, size_t\n         to.write(from.position(), count);\n         from.position() += count;\n         bytes -= count;\n+\n+        if (throttler)\n+            throttler->add(count);\n     }\n \n     if (check_bytes && bytes > 0)\n         throw Exception(\"Attempt to read after EOF.\", ErrorCodes::ATTEMPT_TO_READ_AFTER_EOF);\n }\n \n-void copyDataImpl(ReadBuffer & from, WriteBuffer & to, bool check_bytes, size_t bytes, std::function<void()> cancellation_hook)\n+void copyDataImpl(ReadBuffer & from, WriteBuffer & to, bool check_bytes, size_t bytes, std::function<void()> cancellation_hook, ThrottlerPtr throttler)\n {\n     /// If read to the end of the buffer, eof() either fills the buffer with new data and moves the cursor to the beginning, or returns false.\n     while (bytes > 0 && !from.eof())\n@@ -46,6 +50,9 @@ void copyDataImpl(ReadBuffer & from, WriteBuffer & to, bool check_bytes, size_t\n         to.write(from.position(), count);\n         from.position() += count;\n         bytes -= count;\n+\n+        if (throttler)\n+            throttler->add(count);\n     }\n \n     if (check_bytes && bytes > 0)\n@@ -56,32 +63,42 @@ void copyDataImpl(ReadBuffer & from, WriteBuffer & to, bool check_bytes, size_t\n \n void copyData(ReadBuffer & from, WriteBuffer & to)\n {\n-    copyDataImpl(from, to, false, std::numeric_limits<size_t>::max(), nullptr);\n+    copyDataImpl(from, to, false, std::numeric_limits<size_t>::max(), nullptr, nullptr);\n }\n \n void copyData(ReadBuffer & from, WriteBuffer & to, const std::atomic<int> & is_cancelled)\n {\n-    copyDataImpl(from, to, false, std::numeric_limits<size_t>::max(), &is_cancelled);\n+    copyDataImpl(from, to, false, std::numeric_limits<size_t>::max(), &is_cancelled, nullptr);\n }\n \n void copyData(ReadBuffer & from, WriteBuffer & to, std::function<void()> cancellation_hook)\n {\n-    copyDataImpl(from, to, false, std::numeric_limits<size_t>::max(), cancellation_hook);\n+    copyDataImpl(from, to, false, std::numeric_limits<size_t>::max(), cancellation_hook, nullptr);\n }\n \n void copyData(ReadBuffer & from, WriteBuffer & to, size_t bytes)\n {\n-    copyDataImpl(from, to, true, bytes, nullptr);\n+    copyDataImpl(from, to, true, bytes, nullptr, nullptr);\n }\n \n void copyData(ReadBuffer & from, WriteBuffer & to, size_t bytes, const std::atomic<int> & is_cancelled)\n {\n-    copyDataImpl(from, to, true, bytes, &is_cancelled);\n+    copyDataImpl(from, to, true, bytes, &is_cancelled, nullptr);\n }\n \n void copyData(ReadBuffer & from, WriteBuffer & to, size_t bytes, std::function<void()> cancellation_hook)\n {\n-    copyDataImpl(from, to, true, bytes, cancellation_hook);\n+    copyDataImpl(from, to, true, bytes, cancellation_hook, nullptr);\n+}\n+\n+void copyDataWithThrottler(ReadBuffer & from, WriteBuffer & to, const std::atomic<int> & is_cancelled, ThrottlerPtr throttler)\n+{\n+    copyDataImpl(from, to, false, std::numeric_limits<size_t>::max(), &is_cancelled, throttler);\n+}\n+\n+void copyDataWithThrottler(ReadBuffer & from, WriteBuffer & to, size_t bytes, const std::atomic<int> & is_cancelled, ThrottlerPtr throttler)\n+{\n+    copyDataImpl(from, to, true, bytes, &is_cancelled, throttler);\n }\n \n }\ndiff --git a/src/IO/copyData.h b/src/IO/copyData.h\nindex f888a039e9e4..2202f36f79e5 100644\n--- a/src/IO/copyData.h\n+++ b/src/IO/copyData.h\n@@ -9,22 +9,26 @@ namespace DB\n \n class ReadBuffer;\n class WriteBuffer;\n+class Throttler;\n \n+using ThrottlerPtr = std::shared_ptr<Throttler>;\n \n-/** Copies data from ReadBuffer to WriteBuffer, all that is.\n-  */\n+\n+/// Copies data from ReadBuffer to WriteBuffer, all that is.\n void copyData(ReadBuffer & from, WriteBuffer & to);\n \n-/** Copies `bytes` bytes from ReadBuffer to WriteBuffer. If there are no `bytes` bytes, then throws an exception.\n-  */\n+/// Copies `bytes` bytes from ReadBuffer to WriteBuffer. If there are no `bytes` bytes, then throws an exception.\n void copyData(ReadBuffer & from, WriteBuffer & to, size_t bytes);\n \n-/** The same, with the condition to cancel.\n-  */\n+/// The same, with the condition to cancel.\n void copyData(ReadBuffer & from, WriteBuffer & to, const std::atomic<int> & is_cancelled);\n void copyData(ReadBuffer & from, WriteBuffer & to, size_t bytes, const std::atomic<int> & is_cancelled);\n \n void copyData(ReadBuffer & from, WriteBuffer & to, std::function<void()> cancellation_hook);\n void copyData(ReadBuffer & from, WriteBuffer & to, size_t bytes, std::function<void()> cancellation_hook);\n \n+/// Same as above but also use throttler to limit maximum speed\n+void copyDataWithThrottler(ReadBuffer & from, WriteBuffer & to, const std::atomic<int> & is_cancelled, ThrottlerPtr throttler);\n+void copyDataWithThrottler(ReadBuffer & from, WriteBuffer & to, size_t bytes, const std::atomic<int> & is_cancelled, ThrottlerPtr throttler);\n+\n }\ndiff --git a/src/Interpreters/Context.cpp b/src/Interpreters/Context.cpp\nindex a920a26e4c40..0c9b6ae473c8 100644\n--- a/src/Interpreters/Context.cpp\n+++ b/src/Interpreters/Context.cpp\n@@ -11,6 +11,7 @@\n #include <Common/setThreadName.h>\n #include <Common/Stopwatch.h>\n #include <Common/formatReadable.h>\n+#include <Common/Throttler.h>\n #include <Common/thread_local_rng.h>\n #include <Coordination/KeeperStorageDispatcher.h>\n #include <Compression/ICompressionCodec.h>\n@@ -363,6 +364,10 @@ struct ContextSharedPart\n     mutable std::optional<BackgroundSchedulePool> schedule_pool;    /// A thread pool that can run different jobs in background (used in replicated tables)\n     mutable std::optional<BackgroundSchedulePool> distributed_schedule_pool; /// A thread pool that can run different jobs in background (used for distributed sends)\n     mutable std::optional<BackgroundSchedulePool> message_broker_schedule_pool; /// A thread pool that can run different jobs in background (used for message brokers, like RabbitMQ and Kafka)\n+\n+    mutable ThrottlerPtr replicated_fetches_throttler; /// A server-wide throttler for replicated fetches\n+    mutable ThrottlerPtr replicated_sends_throttler; /// A server-wide throttler for replicated sends\n+\n     MultiVersion<Macros> macros;                            /// Substitutions extracted from config.\n     std::unique_ptr<DDLWorker> ddl_worker;                  /// Process ddl commands from zk.\n     /// Rules for selecting the compression settings, depending on the size of the part.\n@@ -1628,6 +1633,26 @@ BackgroundSchedulePool & Context::getMessageBrokerSchedulePool() const\n     return *shared->message_broker_schedule_pool;\n }\n \n+ThrottlerPtr Context::getReplicatedFetchesThrottler() const\n+{\n+    auto lock = getLock();\n+    if (!shared->replicated_fetches_throttler)\n+        shared->replicated_fetches_throttler = std::make_shared<Throttler>(\n+            settings.max_replicated_fetches_network_bandwidth_for_server);\n+\n+    return shared->replicated_fetches_throttler;\n+}\n+\n+ThrottlerPtr Context::getReplicatedSendsThrottler() const\n+{\n+    auto lock = getLock();\n+    if (!shared->replicated_sends_throttler)\n+        shared->replicated_sends_throttler = std::make_shared<Throttler>(\n+            settings.max_replicated_sends_network_bandwidth_for_server);\n+\n+    return shared->replicated_sends_throttler;\n+}\n+\n bool Context::hasDistributedDDL() const\n {\n     return getConfigRef().has(\"distributed_ddl\");\ndiff --git a/src/Interpreters/Context.h b/src/Interpreters/Context.h\nindex 97cb1b980f1f..2be34bb3cb12 100644\n--- a/src/Interpreters/Context.h\n+++ b/src/Interpreters/Context.h\n@@ -113,6 +113,9 @@ using VolumePtr = std::shared_ptr<IVolume>;\n struct NamedSession;\n struct BackgroundTaskSchedulingSettings;\n \n+class Throttler;\n+using ThrottlerPtr = std::shared_ptr<Throttler>;\n+\n class ZooKeeperMetadataTransaction;\n using ZooKeeperMetadataTransactionPtr = std::shared_ptr<ZooKeeperMetadataTransaction>;\n \n@@ -657,6 +660,9 @@ class Context: public std::enable_shared_from_this<Context>\n     BackgroundSchedulePool & getMessageBrokerSchedulePool() const;\n     BackgroundSchedulePool & getDistributedSchedulePool() const;\n \n+    ThrottlerPtr getReplicatedFetchesThrottler() const;\n+    ThrottlerPtr getReplicatedSendsThrottler() const;\n+\n     /// Has distributed_ddl configuration or not.\n     bool hasDistributedDDL() const;\n     void setDDLWorker(std::unique_ptr<DDLWorker> ddl_worker);\ndiff --git a/src/Storages/MergeTree/DataPartsExchange.cpp b/src/Storages/MergeTree/DataPartsExchange.cpp\nindex 529fc9f54b8f..47f446a0b8b6 100644\n--- a/src/Storages/MergeTree/DataPartsExchange.cpp\n+++ b/src/Storages/MergeTree/DataPartsExchange.cpp\n@@ -9,6 +9,7 @@\n #include <Storages/MergeTree/MergeTreeDataPartInMemory.h>\n #include <Storages/MergeTree/MergedBlockOutputStream.h>\n #include <Storages/MergeTree/ReplicatedFetchList.h>\n+#include <Storages/StorageReplicatedMergeTree.h>\n #include <Common/CurrentMetrics.h>\n #include <Common/NetException.h>\n #include <IO/createReadBufferFromFileBase.h>\n@@ -86,6 +87,10 @@ struct ReplicatedFetchReadCallback\n \n }\n \n+\n+Service::Service(StorageReplicatedMergeTree & data_) :\n+    data(data_), log(&Poco::Logger::get(data.getLogName() + \" (Replicated PartsService)\")) {}\n+\n std::string Service::getId(const std::string & node_id) const\n {\n     return getEndpointId(node_id);\n@@ -243,6 +248,8 @@ void Service::sendPartFromMemory(\n     NativeBlockOutputStream block_out(out, 0, metadata_snapshot->getSampleBlock());\n     part->checksums.write(out);\n     block_out.write(part_in_memory->block);\n+\n+    data.getSendsThrottler()->add(part_in_memory->block.bytes());\n }\n \n MergeTreeData::DataPart::Checksums Service::sendPartFromDisk(\n@@ -298,7 +305,7 @@ MergeTreeData::DataPart::Checksums Service::sendPartFromDisk(\n \n         auto file_in = disk->readFile(path);\n         HashingWriteBuffer hashing_out(out);\n-        copyData(*file_in, hashing_out, blocker.getCounter());\n+        copyDataWithThrottler(*file_in, hashing_out, blocker.getCounter(), data.getSendsThrottler());\n \n         if (blocker.isCancelled())\n             throw Exception(\"Transferring part to replica was cancelled\", ErrorCodes::ABORTED);\n@@ -354,7 +361,7 @@ void Service::sendPartS3Metadata(const MergeTreeData::DataPartPtr & part, WriteB\n \n         auto file_in = createReadBufferFromFileBase(metadata_file, 0, 0, 0, nullptr, DBMS_DEFAULT_BUFFER_SIZE);\n         HashingWriteBuffer hashing_out(out);\n-        copyData(*file_in, hashing_out, blocker.getCounter());\n+        copyDataWithThrottler(*file_in, hashing_out, blocker.getCounter(), data.getSendsThrottler());\n         if (blocker.isCancelled())\n             throw Exception(\"Transferring part to replica was cancelled\", ErrorCodes::ABORTED);\n \n@@ -388,6 +395,7 @@ MergeTreeData::MutableDataPartPtr Fetcher::fetchPart(\n     const String & user,\n     const String & password,\n     const String & interserver_scheme,\n+    ThrottlerPtr throttler,\n     bool to_detached,\n     const String & tmp_prefix_,\n     std::optional<CurrentlySubmergingEmergingTagger> * tagger_ptr,\n@@ -514,7 +522,7 @@ MergeTreeData::MutableDataPartPtr Fetcher::fetchPart(\n \n         try\n         {\n-            return downloadPartToS3(part_name, replica_path, to_detached, tmp_prefix_, std::move(disks_s3), in);\n+            return downloadPartToS3(part_name, replica_path, to_detached, tmp_prefix_, std::move(disks_s3), in, throttler);\n         }\n         catch (const Exception & e)\n         {\n@@ -522,7 +530,7 @@ MergeTreeData::MutableDataPartPtr Fetcher::fetchPart(\n                 throw;\n             /// Try again but without S3 copy\n             return fetchPart(metadata_snapshot, context, part_name, replica_path, host, port, timeouts,\n-                user, password, interserver_scheme, to_detached, tmp_prefix_, nullptr, false);\n+                user, password, interserver_scheme, throttler, to_detached, tmp_prefix_, nullptr, false);\n         }\n     }\n \n@@ -585,8 +593,8 @@ MergeTreeData::MutableDataPartPtr Fetcher::fetchPart(\n \n     MergeTreeData::DataPart::Checksums checksums;\n     return part_type == \"InMemory\"\n-        ? downloadPartToMemory(part_name, part_uuid, metadata_snapshot, context, std::move(reservation), in, projections)\n-        : downloadPartToDisk(part_name, replica_path, to_detached, tmp_prefix_, sync, reservation->getDisk(), in, projections, checksums);\n+        ? downloadPartToMemory(part_name, part_uuid, metadata_snapshot, context, std::move(reservation), in, projections, throttler)\n+        : downloadPartToDisk(part_name, replica_path, to_detached, tmp_prefix_, sync, reservation->getDisk(), in, projections, checksums, throttler);\n }\n \n MergeTreeData::MutableDataPartPtr Fetcher::downloadPartToMemory(\n@@ -596,7 +604,8 @@ MergeTreeData::MutableDataPartPtr Fetcher::downloadPartToMemory(\n     ContextPtr context,\n     ReservationPtr reservation,\n     PooledReadWriteBufferFromHTTP & in,\n-    size_t projections)\n+    size_t projections,\n+    ThrottlerPtr throttler)\n {\n     auto volume = std::make_shared<SingleDiskVolume>(\"volume_\" + part_name, reservation->getDisk(), 0);\n     MergeTreeData::MutableDataPartPtr new_data_part =\n@@ -612,6 +621,7 @@ MergeTreeData::MutableDataPartPtr Fetcher::downloadPartToMemory(\n \n         NativeBlockInputStream block_in(in, 0);\n         auto block = block_in.read();\n+        throttler->add(block.bytes());\n \n         MergeTreePartInfo new_part_info(\"all\", 0, 0, 0);\n         MergeTreeData::MutableDataPartPtr new_projection_part =\n@@ -643,6 +653,7 @@ MergeTreeData::MutableDataPartPtr Fetcher::downloadPartToMemory(\n \n     NativeBlockInputStream block_in(in, 0);\n     auto block = block_in.read();\n+    throttler->add(block.bytes());\n \n     new_data_part->uuid = part_uuid;\n     new_data_part->is_temp = true;\n@@ -666,7 +677,8 @@ void Fetcher::downloadBaseOrProjectionPartToDisk(\n     bool sync,\n     DiskPtr disk,\n     PooledReadWriteBufferFromHTTP & in,\n-    MergeTreeData::DataPart::Checksums & checksums) const\n+    MergeTreeData::DataPart::Checksums & checksums,\n+    ThrottlerPtr throttler) const\n {\n     size_t files;\n     readBinary(files, in);\n@@ -689,7 +701,7 @@ void Fetcher::downloadBaseOrProjectionPartToDisk(\n \n         auto file_out = disk->writeFile(fs::path(part_download_path) / file_name);\n         HashingWriteBuffer hashing_out(*file_out);\n-        copyData(in, hashing_out, file_size, blocker.getCounter());\n+        copyDataWithThrottler(in, hashing_out, file_size, blocker.getCounter(), throttler);\n \n         if (blocker.isCancelled())\n         {\n@@ -726,7 +738,8 @@ MergeTreeData::MutableDataPartPtr Fetcher::downloadPartToDisk(\n     DiskPtr disk,\n     PooledReadWriteBufferFromHTTP & in,\n     size_t projections,\n-    MergeTreeData::DataPart::Checksums & checksums)\n+    MergeTreeData::DataPart::Checksums & checksums,\n+    ThrottlerPtr throttler)\n {\n     static const String TMP_PREFIX = \"tmp_fetch_\";\n     String tmp_prefix = tmp_prefix_.empty() ? TMP_PREFIX : tmp_prefix_;\n@@ -763,13 +776,13 @@ MergeTreeData::MutableDataPartPtr Fetcher::downloadPartToDisk(\n         MergeTreeData::DataPart::Checksums projection_checksum;\n         disk->createDirectories(part_download_path + projection_name + \".proj/\");\n         downloadBaseOrProjectionPartToDisk(\n-            replica_path, part_download_path + projection_name + \".proj/\", sync, disk, in, projection_checksum);\n+            replica_path, part_download_path + projection_name + \".proj/\", sync, disk, in, projection_checksum, throttler);\n         checksums.addFile(\n             projection_name + \".proj\", projection_checksum.getTotalSizeOnDisk(), projection_checksum.getTotalChecksumUInt128());\n     }\n \n     // Download the base part\n-    downloadBaseOrProjectionPartToDisk(replica_path, part_download_path, sync, disk, in, checksums);\n+    downloadBaseOrProjectionPartToDisk(replica_path, part_download_path, sync, disk, in, checksums, throttler);\n \n     assertEOF(in);\n     auto volume = std::make_shared<SingleDiskVolume>(\"volume_\" + part_name, disk, 0);\n@@ -787,8 +800,8 @@ MergeTreeData::MutableDataPartPtr Fetcher::downloadPartToS3(\n     bool to_detached,\n     const String & tmp_prefix_,\n     const Disks & disks_s3,\n-    PooledReadWriteBufferFromHTTP & in\n-    )\n+    PooledReadWriteBufferFromHTTP & in,\n+    ThrottlerPtr throttler)\n {\n     if (disks_s3.empty())\n         throw Exception(\"No S3 disks anymore\", ErrorCodes::LOGICAL_ERROR);\n@@ -841,7 +854,7 @@ MergeTreeData::MutableDataPartPtr Fetcher::downloadPartToS3(\n \n             HashingWriteBuffer hashing_out(*file_out);\n \n-            copyData(in, hashing_out, file_size, blocker.getCounter());\n+            copyDataWithThrottler(in, hashing_out, file_size, blocker.getCounter(), throttler);\n \n             if (blocker.isCancelled())\n             {\ndiff --git a/src/Storages/MergeTree/DataPartsExchange.h b/src/Storages/MergeTree/DataPartsExchange.h\nindex f59942ef7f46..eb776c33f0fb 100644\n--- a/src/Storages/MergeTree/DataPartsExchange.h\n+++ b/src/Storages/MergeTree/DataPartsExchange.h\n@@ -7,6 +7,7 @@\n #include <IO/copyData.h>\n #include <IO/ConnectionTimeouts.h>\n #include <IO/ReadWriteBufferFromHTTP.h>\n+#include <Common/Throttler.h>\n \n \n namespace zkutil\n@@ -18,15 +19,17 @@ namespace zkutil\n namespace DB\n {\n \n+class StorageReplicatedMergeTree;\n+\n namespace DataPartsExchange\n {\n \n-/** Service for sending parts from the table *MergeTree.\n+/** Service for sending parts from the table *ReplicatedMergeTree.\n   */\n class Service final : public InterserverIOEndpoint\n {\n public:\n-    explicit Service(MergeTreeData & data_) : data(data_), log(&Poco::Logger::get(data.getLogName() + \" (Replicated PartsService)\")) {}\n+    explicit Service(StorageReplicatedMergeTree & data_);\n \n     Service(const Service &) = delete;\n     Service & operator=(const Service &) = delete;\n@@ -51,7 +54,7 @@ class Service final : public InterserverIOEndpoint\n \n     /// StorageReplicatedMergeTree::shutdown() waits for all parts exchange handlers to finish,\n     /// so Service will never access dangling reference to storage\n-    MergeTreeData & data;\n+    StorageReplicatedMergeTree & data;\n     Poco::Logger * log;\n };\n \n@@ -74,6 +77,7 @@ class Fetcher final : private boost::noncopyable\n         const String & user,\n         const String & password,\n         const String & interserver_scheme,\n+        ThrottlerPtr throttler,\n         bool to_detached = false,\n         const String & tmp_prefix_ = \"\",\n         std::optional<CurrentlySubmergingEmergingTagger> * tagger_ptr = nullptr,\n@@ -90,7 +94,9 @@ class Fetcher final : private boost::noncopyable\n             bool sync,\n             DiskPtr disk,\n             PooledReadWriteBufferFromHTTP & in,\n-            MergeTreeData::DataPart::Checksums & checksums) const;\n+            MergeTreeData::DataPart::Checksums & checksums,\n+            ThrottlerPtr throttler) const;\n+\n \n     MergeTreeData::MutableDataPartPtr downloadPartToDisk(\n             const String & part_name,\n@@ -101,7 +107,8 @@ class Fetcher final : private boost::noncopyable\n             DiskPtr disk,\n             PooledReadWriteBufferFromHTTP & in,\n             size_t projections,\n-            MergeTreeData::DataPart::Checksums & checksums);\n+            MergeTreeData::DataPart::Checksums & checksums,\n+            ThrottlerPtr throttler);\n \n     MergeTreeData::MutableDataPartPtr downloadPartToMemory(\n             const String & part_name,\n@@ -110,7 +117,8 @@ class Fetcher final : private boost::noncopyable\n             ContextPtr context,\n             ReservationPtr reservation,\n             PooledReadWriteBufferFromHTTP & in,\n-            size_t projections);\n+            size_t projections,\n+            ThrottlerPtr throttler);\n \n     MergeTreeData::MutableDataPartPtr downloadPartToS3(\n             const String & part_name,\n@@ -118,7 +126,8 @@ class Fetcher final : private boost::noncopyable\n             bool to_detached,\n             const String & tmp_prefix_,\n             const Disks & disks_s3,\n-            PooledReadWriteBufferFromHTTP & in);\n+            PooledReadWriteBufferFromHTTP & in,\n+            ThrottlerPtr throttler);\n \n     MergeTreeData & data;\n     Poco::Logger * log;\ndiff --git a/src/Storages/MergeTree/MergeTreeSettings.h b/src/Storages/MergeTree/MergeTreeSettings.h\nindex 673105b3ed46..888ca80e0153 100644\n--- a/src/Storages/MergeTree/MergeTreeSettings.h\n+++ b/src/Storages/MergeTree/MergeTreeSettings.h\n@@ -92,6 +92,8 @@ struct Settings;\n     M(Bool, replicated_can_become_leader, true, \"If true, Replicated tables replicas on this node will try to acquire leadership.\", 0) \\\n     M(Seconds, zookeeper_session_expiration_check_period, 60, \"ZooKeeper session expiration check period, in seconds.\", 0) \\\n     M(Bool, detach_old_local_parts_when_cloning_replica, 1, \"Do not remove old local parts when repairing lost replica.\", 0) \\\n+    M(UInt64, max_replicated_fetches_network_bandwidth, 0, \"The maximum speed of data exchange over the network in bytes per second for replicated fetches. Zero means unlimited.\", 0) \\\n+    M(UInt64, max_replicated_sends_network_bandwidth, 0, \"The maximum speed of data exchange over the network in bytes per second for replicated sends. Zero means unlimited.\", 0) \\\n     \\\n     /** Check delay of replicas settings. */ \\\n     M(UInt64, min_relative_delay_to_measure, 120, \"Calculate relative replica delay only if absolute delay is not less that this value.\", 0) \\\ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex cb48d08b64ec..305d48e28e94 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -290,6 +290,8 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(\n     , part_moves_between_shards_orchestrator(*this)\n     , allow_renaming(allow_renaming_)\n     , replicated_fetches_pool_size(getContext()->getSettingsRef().background_fetches_pool_size)\n+    , replicated_fetches_throttler(std::make_shared<Throttler>(getSettings()->max_replicated_fetches_network_bandwidth, getContext()->getReplicatedFetchesThrottler()))\n+    , replicated_sends_throttler(std::make_shared<Throttler>(getSettings()->max_replicated_sends_network_bandwidth, getContext()->getReplicatedSendsThrottler()))\n {\n     queue_updating_task = getContext()->getSchedulePool().createTask(\n         getStorageID().getFullTableName() + \" (StorageReplicatedMergeTree::queueUpdatingTask)\", [this]{ queueUpdatingTask(); });\n@@ -2507,7 +2509,8 @@ bool StorageReplicatedMergeTree::executeReplaceRange(const LogEntry & entry)\n \n             part_desc->res_part = fetcher.fetchPart(\n                 metadata_snapshot, getContext(), part_desc->found_new_part_name, source_replica_path,\n-                address.host, address.replication_port, timeouts, credentials->getUser(), credentials->getPassword(), interserver_scheme, false, TMP_PREFIX + \"fetch_\");\n+                address.host, address.replication_port, timeouts, credentials->getUser(), credentials->getPassword(),\n+                interserver_scheme, replicated_fetches_throttler, false, TMP_PREFIX + \"fetch_\");\n \n             /// TODO: check columns_version of fetched part\n \n@@ -2624,7 +2627,8 @@ void StorageReplicatedMergeTree::executeClonePartFromShard(const LogEntry & entr\n             return fetcher.fetchPart(\n                 metadata_snapshot, getContext(), entry.new_part_name, source_replica_path,\n                 address.host, address.replication_port,\n-                timeouts, credentials->getUser(), credentials->getPassword(), interserver_scheme, true);\n+                timeouts, credentials->getUser(), credentials->getPassword(), interserver_scheme,\n+                replicated_fetches_throttler, true);\n         };\n \n         part = get_part();\n@@ -3163,6 +3167,13 @@ bool StorageReplicatedMergeTree::canExecuteFetch(const ReplicatedMergeTreeLogEnt\n         return false;\n     }\n \n+    if (replicated_fetches_throttler->isThrottling())\n+    {\n+        disable_reason = fmt::format(\"Not executing fetch of part {} because fetches have already throttled by network settings \"\n+                                     \"<max_replicated_fetches_network_bandwidth> or <max_replicated_fetches_network_bandwidth_for_server>.\", entry.new_part_name);\n+        return false;\n+    }\n+\n     return true;\n }\n \n@@ -4031,6 +4042,7 @@ bool StorageReplicatedMergeTree::fetchPart(const String & part_name, const Stora\n                 credentials->getUser(),\n                 credentials->getPassword(),\n                 interserver_scheme,\n+                replicated_fetches_throttler,\n                 to_detached,\n                 \"\",\n                 &tagger_ptr,\n@@ -4180,7 +4192,8 @@ bool StorageReplicatedMergeTree::fetchExistsPart(const String & part_name, const\n         return fetcher.fetchPart(\n             metadata_snapshot, getContext(), part_name, source_replica_path,\n             address.host, address.replication_port,\n-            timeouts, credentials->getUser(), credentials->getPassword(), interserver_scheme, false, \"\", nullptr, true,\n+            timeouts, credentials->getUser(), credentials->getPassword(),\n+            interserver_scheme, replicated_fetches_throttler, false, \"\", nullptr, true,\n             replaced_disk);\n     };\n \ndiff --git a/src/Storages/StorageReplicatedMergeTree.h b/src/Storages/StorageReplicatedMergeTree.h\nindex ec542acc2755..8ffb4974cb3d 100644\n--- a/src/Storages/StorageReplicatedMergeTree.h\n+++ b/src/Storages/StorageReplicatedMergeTree.h\n@@ -26,6 +26,7 @@\n #include <Interpreters/PartLog.h>\n #include <Common/randomSeed.h>\n #include <Common/ZooKeeper/ZooKeeper.h>\n+#include <Common/Throttler.h>\n #include <Core/BackgroundSchedulePool.h>\n #include <Processors/Pipe.h>\n #include <Storages/MergeTree/BackgroundJobsExecutor.h>\n@@ -239,6 +240,18 @@ class StorageReplicatedMergeTree final : public ext::shared_ptr_helper<StorageRe\n     /// Get best replica having this partition on S3\n     String getSharedDataReplica(const IMergeTreeDataPart & part) const;\n \n+    /// Get throttler for replicated fetches\n+    ThrottlerPtr getFetchesThrottler() const\n+    {\n+        return replicated_fetches_throttler;\n+    }\n+\n+    /// Get throttler for replicated sends\n+    ThrottlerPtr getSendsThrottler() const\n+    {\n+        return replicated_sends_throttler;\n+    }\n+\n private:\n     /// Get a sequential consistent view of current parts.\n     ReplicatedMergeTreeQuorumAddedParts::PartitionIdToMaxBlock getMaxAddedBlocks() const;\n@@ -363,6 +376,11 @@ class StorageReplicatedMergeTree final : public ext::shared_ptr_helper<StorageRe\n \n     const size_t replicated_fetches_pool_size;\n \n+    /// Throttlers used in DataPartsExchange to lower maximum fetch/sends\n+    /// speed.\n+    ThrottlerPtr replicated_fetches_throttler;\n+    ThrottlerPtr replicated_sends_throttler;\n+\n     template <class Func>\n     void foreachCommittedParts(Func && func, bool select_sequential_consistency) const;\n \n",
  "test_patch": "diff --git a/tests/integration/helpers/network.py b/tests/integration/helpers/network.py\nindex aa697c63d727..4a64ccee809a 100644\n--- a/tests/integration/helpers/network.py\n+++ b/tests/integration/helpers/network.py\n@@ -219,3 +219,61 @@ def _exec_run(self, cmd, **kwargs):\n             raise subprocess.CalledProcessError(exit_code, cmd)\n \n         return output\n+\n+# Approximately mesure network I/O speed for interface\n+class NetThroughput(object):\n+    def __init__(self, node, interface=\"eth0\"):\n+        self.interface = interface\n+        self.node = node\n+        try:\n+            check = subprocess.check_output(f'grep \"^ *{self.interface}:\" /proc/net/dev', shell=True)\n+            if not check:\n+                raise Exception(f\"No such interface {self.interface} found in /proc/net/dev\")\n+        except:\n+            raise Exception(f\"No such interface {self.interface} found in /proc/net/dev\")\n+\n+        self.current_in = self._get_in_bytes()\n+        self.current_out = self._get_out_bytes()\n+        self.measure_time = time.time()\n+\n+    def _get_in_bytes(self):\n+        try:\n+            result = self.node.exec_in_container(['bash', '-c', f'awk \"/^ *{self.interface}:/\"\\' {{ if ($1 ~ /.*:[0-9][0-9]*/) {{ sub(/^.*:/, \"\") ; print $1 }} else {{ print $2 }} }}\\' /proc/net/dev'])\n+        except:\n+            raise Exception(f\"Cannot receive in bytes from /proc/net/dev for interface {self.interface}\")\n+\n+        try:\n+            return int(result)\n+        except:\n+            raise Exception(f\"Got non-numeric in bytes '{result}' from /proc/net/dev for interface {self.interface}\")\n+\n+    def _get_out_bytes(self):\n+        try:\n+            result = self.node.exec_in_container(['bash', '-c', f'awk \"/^ *{self.interface}:/\"\\' {{ if ($1 ~ /.*:[0-9][0-9]*/) {{ print $9 }} else {{ print $10 }} }}\\' /proc/net/dev'])\n+        except:\n+            raise Exception(f\"Cannot receive out bytes from /proc/net/dev for interface {self.interface}\")\n+\n+        try:\n+            return int(result)\n+        except:\n+            raise Exception(f\"Got non-numeric out bytes '{result}' from /proc/net/dev for interface {self.interface}\")\n+\n+    def measure_speed(self, measure='bytes'):\n+        new_in = self._get_in_bytes()\n+        new_out = self._get_out_bytes()\n+        current_time = time.time()\n+        in_speed = (new_in - self.current_in) / (current_time - self.measure_time)\n+        out_speed = (new_out - self.current_out) / (current_time - self.measure_time)\n+\n+        self.current_out = new_out\n+        self.current_in = new_in\n+        self.measure_time = current_time\n+\n+        if measure == 'bytes':\n+            return in_speed, out_speed\n+        elif measure == 'kilobytes':\n+            return in_speed / 1024., out_speed / 1024.\n+        elif measure == 'megabytes':\n+            return in_speed / (1024 * 1024), out_speed / (1024 * 1024)\n+        else:\n+            raise Exception(f\"Unknown measure {measure}\")\ndiff --git a/tests/integration/test_replicated_fetches_bandwidth/__init__.py b/tests/integration/test_replicated_fetches_bandwidth/__init__.py\nnew file mode 100644\nindex 000000000000..e5a0d9b4834e\n--- /dev/null\n+++ b/tests/integration/test_replicated_fetches_bandwidth/__init__.py\n@@ -0,0 +1,1 @@\n+#!/usr/bin/env python3\ndiff --git a/tests/integration/test_replicated_fetches_bandwidth/configs/limit_replication_config.xml b/tests/integration/test_replicated_fetches_bandwidth/configs/limit_replication_config.xml\nnew file mode 100644\nindex 000000000000..566c7c07f5c2\n--- /dev/null\n+++ b/tests/integration/test_replicated_fetches_bandwidth/configs/limit_replication_config.xml\n@@ -0,0 +1,9 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+    <profiles>\n+        <default>\n+          <max_replicated_fetches_network_bandwidth_for_server>5242880</max_replicated_fetches_network_bandwidth_for_server>\n+          <max_replicated_sends_network_bandwidth_for_server>10485760</max_replicated_sends_network_bandwidth_for_server>\n+        </default>\n+    </profiles>\n+</yandex>\ndiff --git a/tests/integration/test_replicated_fetches_bandwidth/test.py b/tests/integration/test_replicated_fetches_bandwidth/test.py\nnew file mode 100644\nindex 000000000000..5b8332cda160\n--- /dev/null\n+++ b/tests/integration/test_replicated_fetches_bandwidth/test.py\n@@ -0,0 +1,205 @@\n+#!/usr/bin/env python3\n+from helpers.cluster import ClickHouseCluster\n+import pytest\n+import random\n+import string\n+from helpers.network import NetThroughput\n+import subprocess\n+import time\n+import statistics\n+\n+cluster = ClickHouseCluster(__file__)\n+node1 = cluster.add_instance('node1', with_zookeeper=True)\n+node2 = cluster.add_instance('node2', with_zookeeper=True)\n+node3 = cluster.add_instance('node3', user_configs=['configs/limit_replication_config.xml'], with_zookeeper=True)\n+\n+@pytest.fixture(scope=\"module\")\n+def start_cluster():\n+    try:\n+        cluster.start()\n+\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+def get_random_string(length):\n+    return ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(length))\n+\n+def test_limited_fetch_single_table(start_cluster):\n+    print(\"Limited fetches single table\")\n+    try:\n+        for i, node in enumerate([node1, node2]):\n+            node.query(f\"CREATE TABLE limited_fetch_table(key UInt64, data String) ENGINE = ReplicatedMergeTree('/clickhouse/tables/limited_fetch_table', '{i}') ORDER BY tuple() PARTITION BY key SETTINGS max_replicated_fetches_network_bandwidth=10485760\")\n+\n+        node2.query(\"SYSTEM STOP FETCHES limited_fetch_table\")\n+\n+        for i in range(5):\n+            node1.query(\"INSERT INTO limited_fetch_table SELECT {}, '{}' FROM numbers(300)\".format(i, get_random_string(104857)))\n+\n+        n1_net = NetThroughput(node1)\n+        n2_net = NetThroughput(node2)\n+\n+        node2.query(\"SYSTEM START FETCHES limited_fetch_table\")\n+        n2_fetch_speed = []\n+        for i in range(10):\n+            n1_in, n1_out = n1_net.measure_speed('megabytes')\n+            n2_in, n2_out = n2_net.measure_speed('megabytes')\n+            print(\"[N1] input:\", n1_in, 'MB/s', \"output:\", n1_out, \"MB/s\")\n+            print(\"[N2] input:\", n2_in, 'MB/s', \"output:\", n2_out, \"MB/s\")\n+            n2_fetch_speed.append(n2_in)\n+            time.sleep(0.5)\n+\n+        median_speed = statistics.median(n2_fetch_speed)\n+        # approximate border. Without limit we will have more than 100 MB/s for very slow builds.\n+        assert median_speed <= 15, \"We exceeded max fetch speed for more than 10MB/s. Must be around 10 (+- 5), got \" + str(median_speed)\n+\n+    finally:\n+        for node in [node1, node2]:\n+            node.query(\"DROP TABLE IF EXISTS limited_fetch_table SYNC\")\n+\n+def test_limited_send_single_table(start_cluster):\n+    print(\"Limited sends single table\")\n+    try:\n+        for i, node in enumerate([node1, node2]):\n+            node.query(f\"CREATE TABLE limited_send_table(key UInt64, data String) ENGINE = ReplicatedMergeTree('/clickhouse/tables/limited_fetch_table', '{i}') ORDER BY tuple() PARTITION BY key SETTINGS max_replicated_sends_network_bandwidth=5242880\")\n+\n+        node2.query(\"SYSTEM STOP FETCHES limited_send_table\")\n+\n+        for i in range(5):\n+            node1.query(\"INSERT INTO limited_send_table SELECT {}, '{}' FROM numbers(150)\".format(i, get_random_string(104857)))\n+\n+        n1_net = NetThroughput(node1)\n+        n2_net = NetThroughput(node2)\n+\n+        node2.query(\"SYSTEM START FETCHES limited_send_table\")\n+        n1_sends_speed = []\n+        for i in range(10):\n+            n1_in, n1_out = n1_net.measure_speed('megabytes')\n+            n2_in, n2_out = n2_net.measure_speed('megabytes')\n+            print(\"[N1] input:\", n1_in, 'MB/s', \"output:\", n1_out, \"MB/s\")\n+            print(\"[N2] input:\", n2_in, 'MB/s', \"output:\", n2_out, \"MB/s\")\n+            n1_sends_speed.append(n1_out)\n+            time.sleep(0.5)\n+\n+        median_speed = statistics.median(n1_sends_speed)\n+        # approximate border. Without limit we will have more than 100 MB/s for very slow builds.\n+        assert median_speed <= 10, \"We exceeded max send speed for more than 5MB/s. Must be around 5 (+- 5), got \" + str(median_speed)\n+\n+    finally:\n+        for node in [node1, node2]:\n+            node.query(\"DROP TABLE IF EXISTS limited_send_table SYNC\")\n+\n+\n+def test_limited_fetches_for_server(start_cluster):\n+    print(\"Limited fetches for server\")\n+    try:\n+        for i, node in enumerate([node1, node3]):\n+            for j in range(5):\n+                node.query(f\"CREATE TABLE limited_fetches{j}(key UInt64, data String) ENGINE = ReplicatedMergeTree('/clickhouse/tables/limited_fetches{j}', '{i}') ORDER BY tuple() PARTITION BY key\")\n+\n+        for j in range(5):\n+            node3.query(f\"SYSTEM STOP FETCHES limited_fetches{j}\")\n+            for i in range(5):\n+                node1.query(\"INSERT INTO limited_fetches{} SELECT {}, '{}' FROM numbers(50)\".format(j, i, get_random_string(104857)))\n+\n+        n1_net = NetThroughput(node1)\n+        n3_net = NetThroughput(node3)\n+\n+        for j in range(5):\n+            node3.query(f\"SYSTEM START FETCHES limited_fetches{j}\")\n+\n+        n3_fetches_speed = []\n+        for i in range(5):\n+            n1_in, n1_out = n1_net.measure_speed('megabytes')\n+            n3_in, n3_out = n3_net.measure_speed('megabytes')\n+            print(\"[N1] input:\", n1_in, 'MB/s', \"output:\", n1_out, \"MB/s\")\n+            print(\"[N3] input:\", n3_in, 'MB/s', \"output:\", n3_out, \"MB/s\")\n+            n3_fetches_speed.append(n3_in)\n+            time.sleep(0.5)\n+\n+        median_speed = statistics.median(n3_fetches_speed)\n+        # approximate border. Without limit we will have more than 100 MB/s for very slow builds.\n+        assert median_speed <= 15, \"We exceeded max fetch speed for more than 15MB/s. Must be around 5 (+- 10), got \" + str(median_speed)\n+\n+    finally:\n+        for node in [node1, node3]:\n+            for j in range(5):\n+                node.query(f\"DROP TABLE IF EXISTS limited_fetches{j} SYNC\")\n+\n+\n+def test_limited_sends_for_server(start_cluster):\n+    print(\"Limited sends for server\")\n+    try:\n+        for i, node in enumerate([node1, node3]):\n+            for j in range(5):\n+                node.query(f\"CREATE TABLE limited_sends{j}(key UInt64, data String) ENGINE = ReplicatedMergeTree('/clickhouse/tables/limited_sends{j}', '{i}') ORDER BY tuple() PARTITION BY key\")\n+\n+        for j in range(5):\n+            node1.query(f\"SYSTEM STOP FETCHES limited_sends{j}\")\n+            for i in range(5):\n+                node3.query(\"INSERT INTO limited_sends{} SELECT {}, '{}' FROM numbers(50)\".format(j, i, get_random_string(104857)))\n+\n+        n1_net = NetThroughput(node1)\n+        n3_net = NetThroughput(node3)\n+\n+        for j in range(5):\n+            node1.query(f\"SYSTEM START FETCHES limited_sends{j}\")\n+\n+        n3_sends_speed = []\n+        for i in range(5):\n+            n1_in, n1_out = n1_net.measure_speed('megabytes')\n+            n3_in, n3_out = n3_net.measure_speed('megabytes')\n+            print(\"[N1] input:\", n1_in, 'MB/s', \"output:\", n1_out, \"MB/s\")\n+            print(\"[N3] input:\", n3_in, 'MB/s', \"output:\", n3_out, \"MB/s\")\n+            n3_sends_speed.append(n3_out)\n+            time.sleep(0.5)\n+\n+        median_speed = statistics.median(n3_sends_speed)\n+        # approximate border. Without limit we will have more than 100 MB/s for very slow builds.\n+        assert median_speed <= 20, \"We exceeded max send speed for more than 20MB/s. Must be around 5 (+- 10), got \" + str(median_speed)\n+\n+    finally:\n+        for node in [node1, node3]:\n+            for j in range(5):\n+                node.query(f\"DROP TABLE IF EXISTS limited_sends{j} SYNC\")\n+\n+\n+def test_should_execute_fetch(start_cluster):\n+    print(\"Should execute fetch\")\n+    try:\n+        for i, node in enumerate([node1, node2]):\n+            node.query(f\"CREATE TABLE should_execute_table(key UInt64, data String) ENGINE = ReplicatedMergeTree('/clickhouse/tables/should_execute_table', '{i}') ORDER BY tuple() PARTITION BY key SETTINGS max_replicated_fetches_network_bandwidth=3505253\")\n+\n+        node2.query(\"SYSTEM STOP FETCHES should_execute_table\")\n+\n+        for i in range(3):\n+            node1.query(\"INSERT INTO should_execute_table SELECT {}, '{}' FROM numbers(200)\".format(i, get_random_string(104857)))\n+\n+        n1_net = NetThroughput(node1)\n+        n2_net = NetThroughput(node2)\n+\n+        node2.query(\"SYSTEM START FETCHES should_execute_table\")\n+\n+        for i in range(10):\n+            node1.query(\"INSERT INTO should_execute_table SELECT {}, '{}' FROM numbers(3)\".format(i, get_random_string(104857)))\n+\n+        n2_fetch_speed = []\n+        replication_queue_data = []\n+        for i in range(10):\n+            n1_in, n1_out = n1_net.measure_speed('megabytes')\n+            n2_in, n2_out = n2_net.measure_speed('megabytes')\n+            fetches_count = node2.query(\"SELECT count() FROM system.replicated_fetches\")\n+            if fetches_count == \"0\\n\":\n+                break\n+\n+            print(\"Fetches count\", fetches_count)\n+            replication_queue_data.append(node2.query(\"SELECT count() FROM system.replication_queue WHERE postpone_reason like '%fetches have already throttled%'\"))\n+            n2_fetch_speed.append(n2_in)\n+            time.sleep(0.5)\n+\n+        node2.query(\"SYSTEM SYNC REPLICA should_execute_table\")\n+        assert any(int(f.strip()) != 0 for f in replication_queue_data)\n+        assert node2.query(\"SELECT COUNT() FROM should_execute_table\") == \"630\\n\"\n+    finally:\n+        for node in [node1, node2]:\n+            node.query(\"DROP TABLE IF EXISTS should_execute_table SYNC\")\n",
  "problem_statement": "Throttle recovery speed\nWe're in a process of migration from older hardware under ClickHouse to the newer generation. \r\n\r\nOlder machines have 12x6T disks, 128GB RAM and 2x10G NICs, newer machines have 12x10T disks, 256GB RAM and 2x25G NICs. Dataset per replica is around 35TiB. Each shard is 3 replicas.\r\n\r\nOur process is:\r\n\r\n1. Stop one replica from shard.\r\n2. Clear it from zookeeper.\r\n3. Remove it from cluster topology (znode update for `remote_servers`).\r\n4. Add new replica to cluster topology.\r\n5. Start new replica and let it replicate all the data from peers.\r\n\r\nThe issue we're seeing is that source replicas saturate disks, starving user queries and merges.\r\n\r\nIt takes ~7h to replicate full dataset, below are the graphs for 12h around that time:\r\n\r\n![image](https://user-images.githubusercontent.com/89186/35352138-d88ca43c-00f7-11e8-8aa9-fd755fb6e4a7.png)\r\n\r\n![image](https://user-images.githubusercontent.com/89186/35352198-11003a04-00f8-11e8-9d19-eb94db646119.png)\r\n\r\n![image](https://user-images.githubusercontent.com/89186/35352253-3a13a5c0-00f8-11e8-941e-371a251b1a7d.png)\r\n\r\n![image](https://user-images.githubusercontent.com/89186/35352271-481fe08e-00f8-11e8-8f95-c271c43bc415.png)\r\n\r\n![image](https://user-images.githubusercontent.com/89186/35352296-62049efe-00f8-11e8-83d5-548e833ef99f.png)\r\n\r\nSource peer:\r\n\r\n![image](https://user-images.githubusercontent.com/89186/35352345-82c44928-00f8-11e8-96b9-a63022a35a04.png)\r\n\r\n![image](https://user-images.githubusercontent.com/89186/35352369-9019f596-00f8-11e8-8b5f-586e37ae4b93.png)\r\n\r\nTarget peer:\r\n\r\n![image](https://user-images.githubusercontent.com/89186/35352422-b394c9f6-00f8-11e8-98e2-d589cee15439.png)\r\n\r\n![image](https://user-images.githubusercontent.com/89186/35352437-c5fade50-00f8-11e8-8590-0168e801ef2c.png)\r\n\r\nNaturally, source peers are not great at IO in the first place (that's why we're upgrading), but having 7h of degraded service is not great. It'd be nice to be able to set recovery speed, so it doesn't starve  other activities like user queries and merges.\r\n\r\nMoreover, max number of parts in partition quickly reaches the max (we set to 1000) on the target replica, where inserts are throttled, which doesn't make things any better. With throttled recovery at lower speed but with higher duration, this will probably be even longer period. Maybe we should split threads that do merges and threads that do replication, it seems like whole pool is busy just replicating.\r\n\r\nIt is also possible that we're just doing it wrong, then it'd be great to have a guide describing the process.\r\n\r\ncc @vavrusa, @dqminh, @bocharov\n",
  "hints_text": "If anyone stumbles on this issue, we worked around it by throttling 50G interface to 4G-8G with `comcast` tool, depending on disk performance:\r\n\r\n```\r\n./bin/comcast --device vlanXYZ --stop; ./bin/comcast --device vlanXYZ --packet-loss=0% --target-bw $((4 * 1000 * 1000)\r\n```\r\n\r\n* https://github.com/tylertreat/comcast\r\n\r\nLinux is still not great at throttling ingress, so this has to be run on source replicas.\nLooks like you have successfully went through this.\nLimit on number of connections and throttling of recovery speed is actual task.\nRelated: https://github.com/yandex/ClickHouse/issues/520\n+1, running into similar issues when adding/replacing nodes during our tests, any ETA on the fix?\nSurprisingly, it affects our hypervisors during deploy of new nodes as well... Would like to see either speed or threads limitation",
  "created_at": "2021-05-27T13:31:00Z"
}