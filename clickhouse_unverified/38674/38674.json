{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 38674,
  "instance_id": "ClickHouse__ClickHouse-38674",
  "issue_numbers": [
    "38618"
  ],
  "base_commit": "6384fe23c336b40c832cef928b0ec44f2947f438",
  "patch": "diff --git a/src/Access/ContextAccess.cpp b/src/Access/ContextAccess.cpp\nindex 221113cb4256..995a46d07caa 100644\n--- a/src/Access/ContextAccess.cpp\n+++ b/src/Access/ContextAccess.cpp\n@@ -412,15 +412,18 @@ bool ContextAccess::checkAccessImplHelper(AccessFlags flags, const Args &... arg\n         return false;\n     };\n \n+    if (is_full_access)\n+        return access_granted();\n+\n+    if (user_was_dropped)\n+        return access_denied(\"User has been dropped\", ErrorCodes::UNKNOWN_USER);\n+\n     if (flags & AccessType::CLUSTER && !access_control->doesOnClusterQueriesRequireClusterGrant())\n         flags &= ~AccessType::CLUSTER;\n \n-    if (!flags || is_full_access)\n+    if (!flags)\n         return access_granted();\n \n-    if (!tryGetUser())\n-        return access_denied(\"User has been dropped\", ErrorCodes::UNKNOWN_USER);\n-\n     /// Access to temporary tables is controlled in an unusual way, not like normal tables.\n     /// Creating of temporary tables is controlled by AccessType::CREATE_TEMPORARY_TABLES grant,\n     /// and other grants are considered as always given.\n@@ -600,9 +603,6 @@ void ContextAccess::checkGrantOption(const AccessRightsElements & elements) cons\n template <bool throw_if_denied, typename Container, typename GetNameFunction>\n bool ContextAccess::checkAdminOptionImplHelper(const Container & role_ids, const GetNameFunction & get_name_function) const\n {\n-    if (!std::size(role_ids) || is_full_access)\n-        return true;\n-\n     auto show_error = [this](const String & msg, int error_code [[maybe_unused]])\n     {\n         UNUSED(this);\n@@ -610,12 +610,18 @@ bool ContextAccess::checkAdminOptionImplHelper(const Container & role_ids, const\n             throw Exception(getUserName() + \": \" + msg, error_code);\n     };\n \n-    if (!tryGetUser())\n+    if (is_full_access)\n+        return true;\n+\n+    if (user_was_dropped)\n     {\n         show_error(\"User has been dropped\", ErrorCodes::UNKNOWN_USER);\n         return false;\n     }\n \n+    if (!std::size(role_ids))\n+        return true;\n+\n     if (isGranted(AccessType::ROLE_ADMIN))\n         return true;\n \n",
  "test_patch": "diff --git a/tests/integration/test_access_control_on_cluster/test.py b/tests/integration/test_access_control_on_cluster/test.py\nindex 6c2331178e09..db76233a35f3 100644\n--- a/tests/integration/test_access_control_on_cluster/test.py\n+++ b/tests/integration/test_access_control_on_cluster/test.py\n@@ -49,3 +49,13 @@ def test_access_control_on_cluster():\n     assert \"There is no user `Alex`\" in ch1.query_and_get_error(\"SHOW CREATE USER Alex\")\n     assert \"There is no user `Alex`\" in ch2.query_and_get_error(\"SHOW CREATE USER Alex\")\n     assert \"There is no user `Alex`\" in ch3.query_and_get_error(\"SHOW CREATE USER Alex\")\n+\n+\n+def test_grant_all_on_cluster():\n+    ch1.query(\"CREATE USER IF NOT EXISTS Alex ON CLUSTER 'cluster'\")\n+    ch1.query(\"GRANT ALL ON *.* TO Alex ON CLUSTER 'cluster'\")\n+\n+    assert ch1.query(\"SHOW GRANTS FOR Alex\") == \"GRANT ALL ON *.* TO Alex\\n\"\n+    assert ch2.query(\"SHOW GRANTS FOR Alex\") == \"GRANT ALL ON *.* TO Alex\\n\"\n+\n+    ch1.query(\"DROP USER Alex ON CLUSTER 'cluster'\")\n",
  "problem_statement": "GRANT/REVOKE with ON CLUSTER causes Segmentation fault\nWhen you run GRANT with ON CLUSTER, every server in the cluster will crash.\r\n\r\nVersion 22.6.2.12. \r\n\r\nI used the official docker image to create a small cluster. You can see the config here and try it out with docker-compose: https://github.com/maybedino/test-clickhouse\r\n\r\nThen log in with the default user and try to add another user:\r\n\r\n```sql\r\ncreate user testuser on cluster testcluster identified with sha256_password by 'test'\r\n```\r\n\r\n```sql\r\ngrant on cluster testcluster all on *.* to testuser with grant option\r\n```\r\n\r\nGRANT will crash every server in the cluster:\r\n\r\n```\r\n2022.06.30 08:18:38.865561 [ 273 ] {} <Fatal> BaseDaemon: ########################################\r\n2022.06.30 08:18:38.865712 [ 273 ] {} <Fatal> BaseDaemon: (version 22.6.2.12 (official build), build id: 52AFD84A0FEDD1BA) (from thread 262) (query_id: 6469e16f-b37f-4565-a9f7-e8e023c1632e) (query: /* ddl_entry=query-0000000001 */ GRANT ALL ON *.* TO testuser WITH GRANT OPTION) Received signal Segmentation fault (11)\r\n2022.06.30 08:18:38.865769 [ 273 ] {} <Fatal> BaseDaemon: Address: 0x16b Access: read. Address not mapped to object.\r\n2022.06.30 08:18:38.865809 [ 273 ] {} <Fatal> BaseDaemon: Stack trace: 0x1541b8dd 0x1659fa4b 0x16566556 0x1656a5c4 0x15cdcb49 0x15cdb557 0x15cd94d3 0x15cd3405 0x15ce6ff6 0xb94d077 0xb95049d 0x7f0096c6a609 0x7f0096b8f133\r\n2022.06.30 08:18:38.865902 [ 273 ] {} <Fatal> BaseDaemon: 2. bool DB::ContextAccess::checkAccessImplHelper<true, true>(DB::AccessFlags) const @ 0x1541b8dd in /usr/bin/clickhouse\r\n2022.06.30 08:18:38.866029 [ 273 ] {} <Fatal> BaseDaemon: 3. DB::InterpreterGrantQuery::execute() @ 0x1659fa4b in /usr/bin/clickhouse\r\n2022.06.30 08:18:38.866053 [ 273 ] {} <Fatal> BaseDaemon: 4. ? @ 0x16566556 in /usr/bin/clickhouse\r\n2022.06.30 08:18:38.866088 [ 273 ] {} <Fatal> BaseDaemon: 5. DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, std::__1::shared_ptr<DB::Context>, std::__1::function<void (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)>, std::__1::optional<DB::FormatSettings> const&) @ 0x1656a5c4 in /usr/bin/clickhouse\r\n2022.06.30 08:18:38.866120 [ 273 ] {} <Fatal> BaseDaemon: 6. DB::DDLWorker::tryExecuteQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::DDLTaskBase&, std::__1::shared_ptr<zkutil::ZooKeeper> const&) @ 0x15cdcb49 in /usr/bin/clickhouse\r\n2022.06.30 08:18:38.866140 [ 273 ] {} <Fatal> BaseDaemon: 7. DB::DDLWorker::processTask(DB::DDLTaskBase&, std::__1::shared_ptr<zkutil::ZooKeeper> const&) @ 0x15cdb557 in /usr/bin/clickhouse\r\n2022.06.30 08:18:38.866155 [ 273 ] {} <Fatal> BaseDaemon: 8. DB::DDLWorker::scheduleTasks(bool) @ 0x15cd94d3 in /usr/bin/clickhouse\r\n2022.06.30 08:18:38.866170 [ 273 ] {} <Fatal> BaseDaemon: 9. DB::DDLWorker::runMainThread() @ 0x15cd3405 in /usr/bin/clickhouse\r\n2022.06.30 08:18:38.866190 [ 273 ] {} <Fatal> BaseDaemon: 10. ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::DDLWorker::*)(), DB::DDLWorker*>(void (DB::DDLWorker::*&&)(), DB::DDLWorker*&&)::'lambda'()::operator()() @ 0x15ce6ff6 in /usr/bin/clickhouse\r\n2022.06.30 08:18:38.866209 [ 273 ] {} <Fatal> BaseDaemon: 11. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0xb94d077 in /usr/bin/clickhouse\r\n2022.06.30 08:18:38.866223 [ 273 ] {} <Fatal> BaseDaemon: 12. ? @ 0xb95049d in /usr/bin/clickhouse\r\n2022.06.30 08:18:38.866245 [ 273 ] {} <Fatal> BaseDaemon: 13. ? @ 0x7f0096c6a609 in ?\r\n2022.06.30 08:18:38.866259 [ 273 ] {} <Fatal> BaseDaemon: 14. clone @ 0x7f0096b8f133 in ?\r\n2022.06.30 08:18:38.993700 [ 273 ] {} <Fatal> BaseDaemon: Integrity check of the executable successfully passed (checksum: 894C4AAB85FCB9AAC26136BC446CC5AF)\r\n/entrypoint.sh: line 155:    43 Segmentation fault      (core dumped) /usr/bin/clickhouse su \"${USER}:${GROUP}\" /usr/bin/clickhouse-server --config-file=\"$CLICKHOUSE_CONFIG\" \"$@\"\r\n```\r\n\r\nAnd then you can't start the cluster anymore because it stays in the task queue and keeps crashing the server. You need to manually stop the server and remove the tasks from clickhouse-keeper. (Or just delete the volumes in this test cluster.)\r\n\r\nThe same thing happens with REVOKE ... ON CLUSTER ....\r\n\r\nRunning GRANT without ON CLUSTER on every server individually works fine.\r\n\r\nI ran it once with crash reports enabled, not sure if it worked.\r\n\n",
  "hints_text": "",
  "created_at": "2022-07-01T10:19:22Z"
}