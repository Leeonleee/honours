{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 17310,
  "instance_id": "ClickHouse__ClickHouse-17310",
  "issue_numbers": [
    "18826"
  ],
  "base_commit": "015e392bb7fa43989f4d58592759d94e9410e05d",
  "patch": "diff --git a/src/Columns/ColumnMap.h b/src/Columns/ColumnMap.h\nindex 9f1410eefe4d..c1948491db59 100644\n--- a/src/Columns/ColumnMap.h\n+++ b/src/Columns/ColumnMap.h\n@@ -86,6 +86,9 @@ class ColumnMap final : public COWHelper<IColumn, ColumnMap>\n     const ColumnArray & getNestedColumn() const { return assert_cast<const ColumnArray &>(*nested); }\n     ColumnArray & getNestedColumn() { return assert_cast<ColumnArray &>(*nested); }\n \n+    const ColumnPtr & getNestedColumnPtr() const { return nested; }\n+    ColumnPtr & getNestedColumnPtr() { return nested; }\n+\n     const ColumnTuple & getNestedData() const { return assert_cast<const ColumnTuple &>(getNestedColumn().getData()); }\n     ColumnTuple & getNestedData() { return assert_cast<ColumnTuple &>(getNestedColumn().getData()); }\n };\ndiff --git a/src/Columns/ColumnNullable.h b/src/Columns/ColumnNullable.h\nindex 8a17f6573409..ade2c1066275 100644\n--- a/src/Columns/ColumnNullable.h\n+++ b/src/Columns/ColumnNullable.h\n@@ -143,9 +143,11 @@ class ColumnNullable final : public COWHelper<IColumn, ColumnNullable>\n     const IColumn & getNestedColumn() const { return *nested_column; }\n \n     const ColumnPtr & getNestedColumnPtr() const { return nested_column; }\n+    ColumnPtr & getNestedColumnPtr() { return nested_column; }\n \n     /// Return the column that represents the byte map.\n     const ColumnPtr & getNullMapColumnPtr() const { return null_map; }\n+    ColumnPtr & getNullMapColumnPtr() { return null_map; }\n \n     ColumnUInt8 & getNullMapColumn() { return assert_cast<ColumnUInt8 &>(*null_map); }\n     const ColumnUInt8 & getNullMapColumn() const { return assert_cast<const ColumnUInt8 &>(*null_map); }\ndiff --git a/src/Columns/ColumnTuple.h b/src/Columns/ColumnTuple.h\nindex 68b502f97052..f763ca3fcba9 100644\n--- a/src/Columns/ColumnTuple.h\n+++ b/src/Columns/ColumnTuple.h\n@@ -99,6 +99,7 @@ class ColumnTuple final : public COWHelper<IColumn, ColumnTuple>\n     Columns getColumnsCopy() const { return {columns.begin(), columns.end()}; }\n \n     const ColumnPtr & getColumnPtr(size_t idx) const { return columns[idx]; }\n+    ColumnPtr & getColumnPtr(size_t idx) { return columns[idx]; }\n \n private:\n     int compareAtImpl(size_t n, size_t m, const IColumn & rhs, int nan_direction_hint, const Collator * collator=nullptr) const;\ndiff --git a/src/Core/NamesAndTypes.cpp b/src/Core/NamesAndTypes.cpp\nindex 3a55a4328a74..e96ce1824d2e 100644\n--- a/src/Core/NamesAndTypes.cpp\n+++ b/src/Core/NamesAndTypes.cpp\n@@ -17,6 +17,29 @@ namespace ErrorCodes\n     extern const int THERE_IS_NO_COLUMN;\n }\n \n+NameAndTypePair::NameAndTypePair(\n+    const String & name_in_storage_, const String & subcolumn_name_,\n+    const DataTypePtr & type_in_storage_, const DataTypePtr & subcolumn_type_)\n+    : name(name_in_storage_ + (subcolumn_name_.empty() ? \"\" : \".\" + subcolumn_name_))\n+    , type(subcolumn_type_)\n+    , type_in_storage(type_in_storage_)\n+    , subcolumn_delimiter_position(name_in_storage_.size()) {}\n+\n+String NameAndTypePair::getNameInStorage() const\n+{\n+    if (!subcolumn_delimiter_position)\n+        return name;\n+\n+    return name.substr(0, *subcolumn_delimiter_position);\n+}\n+\n+String NameAndTypePair::getSubcolumnName() const\n+{\n+    if (!subcolumn_delimiter_position)\n+        return \"\";\n+\n+    return name.substr(*subcolumn_delimiter_position + 1, name.size() - *subcolumn_delimiter_position);\n+}\n \n void NamesAndTypesList::readText(ReadBuffer & buf)\n {\n@@ -137,25 +160,20 @@ NamesAndTypesList NamesAndTypesList::filter(const Names & names) const\n \n NamesAndTypesList NamesAndTypesList::addTypes(const Names & names) const\n {\n-    /// NOTE: It's better to make a map in `IStorage` than to create it here every time again.\n-#if !defined(ARCADIA_BUILD)\n-    google::dense_hash_map<StringRef, const DataTypePtr *, StringRefHash> types;\n-#else\n-    google::sparsehash::dense_hash_map<StringRef, const DataTypePtr *, StringRefHash> types;\n-#endif\n-    types.set_empty_key(StringRef());\n+    std::unordered_map<std::string_view, const NameAndTypePair *> self_columns;\n \n-    for (const NameAndTypePair & column : *this)\n-        types[column.name] = &column.type;\n+    for (const auto & column : *this)\n+        self_columns[column.name] = &column;\n \n     NamesAndTypesList res;\n     for (const String & name : names)\n     {\n-        auto it = types.find(name);\n-        if (it == types.end())\n+        auto it = self_columns.find(name);\n+        if (it == self_columns.end())\n             throw Exception(\"No column \" + name, ErrorCodes::THERE_IS_NO_COLUMN);\n-        res.emplace_back(name, *it->second);\n+        res.emplace_back(*it->second);\n     }\n+\n     return res;\n }\n \ndiff --git a/src/Core/NamesAndTypes.h b/src/Core/NamesAndTypes.h\nindex 28567fed3e30..dad031a543c0 100644\n--- a/src/Core/NamesAndTypes.h\n+++ b/src/Core/NamesAndTypes.h\n@@ -15,11 +15,19 @@ namespace DB\n \n struct NameAndTypePair\n {\n-    String name;\n-    DataTypePtr type;\n+public:\n+    NameAndTypePair() = default;\n+    NameAndTypePair(const String & name_, const DataTypePtr & type_)\n+        : name(name_), type(type_), type_in_storage(type_) {}\n+\n+    NameAndTypePair(const String & name_in_storage_, const String & subcolumn_name_,\n+        const DataTypePtr & type_in_storage_, const DataTypePtr & subcolumn_type_);\n \n-    NameAndTypePair() {}\n-    NameAndTypePair(const String & name_, const DataTypePtr & type_) : name(name_), type(type_) {}\n+    String getNameInStorage() const;\n+    String getSubcolumnName() const;\n+\n+    bool isSubcolumn() const { return subcolumn_delimiter_position != std::nullopt; }\n+    DataTypePtr getTypeInStorage() const { return type_in_storage; }\n \n     bool operator<(const NameAndTypePair & rhs) const\n     {\n@@ -30,8 +38,26 @@ struct NameAndTypePair\n     {\n         return name == rhs.name && type->equals(*rhs.type);\n     }\n+\n+    String name;\n+    DataTypePtr type;\n+\n+private:\n+    DataTypePtr type_in_storage;\n+    std::optional<size_t> subcolumn_delimiter_position;\n };\n \n+/// This needed to use structured bindings for NameAndTypePair\n+/// const auto & [name, type] = name_and_type\n+template <int I>\n+decltype(auto) get(const NameAndTypePair & name_and_type)\n+{\n+    if constexpr (I == 0)\n+        return name_and_type.name;\n+    else if constexpr (I == 1)\n+        return name_and_type.type;\n+}\n+\n using NamesAndTypes = std::vector<NameAndTypePair>;\n \n class NamesAndTypesList : public std::list<NameAndTypePair>\n@@ -81,3 +107,10 @@ class NamesAndTypesList : public std::list<NameAndTypePair>\n };\n \n }\n+\n+namespace std\n+{\n+    template <> struct tuple_size<DB::NameAndTypePair> : std::integral_constant<size_t, 2> {};\n+    template <> struct tuple_element<0, DB::NameAndTypePair> { using type = DB::String; };\n+    template <> struct tuple_element<1, DB::NameAndTypePair> { using type = DB::DataTypePtr; };\n+}\ndiff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex 5368b9fb2106..1e4b07997ab5 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -405,6 +405,7 @@ class IColumn;\n     M(Bool, allow_non_metadata_alters, true, \"Allow to execute alters which affects not only tables metadata, but also data on disk\", 0) \\\n     M(Bool, enable_global_with_statement, false, \"Propagate WITH statements to UNION queries and all subqueries\", 0) \\\n     M(Bool, aggregate_functions_null_for_empty, false, \"Rewrite all aggregate functions in a query, adding -OrNull suffix to them\", 0) \\\n+    M(Bool, flatten_nested, true, \"If true, columns of type Nested will be flatten to separate array columns instead of one array of tuples\", 0) \\\n     M(Bool, asterisk_include_materialized_columns, false, \"Include MATERIALIZED columns for wildcard query\", 0) \\\n     M(Bool, asterisk_include_alias_columns, false, \"Include ALIAS columns for wildcard query\", 0) \\\n     M(Bool, optimize_skip_merged_partitions, false, \"Skip partitions with one part with level > 0 in optimize final\", 0) \\\ndiff --git a/src/DataStreams/NativeBlockInputStream.cpp b/src/DataStreams/NativeBlockInputStream.cpp\nindex b182d5e05882..377f44514195 100644\n--- a/src/DataStreams/NativeBlockInputStream.cpp\n+++ b/src/DataStreams/NativeBlockInputStream.cpp\n@@ -71,7 +71,7 @@ void NativeBlockInputStream::resetParser()\n     is_killed.store(false);\n }\n \n-void NativeBlockInputStream::readData(const IDataType & type, IColumn & column, ReadBuffer & istr, size_t rows, double avg_value_size_hint)\n+void NativeBlockInputStream::readData(const IDataType & type, ColumnPtr & column, ReadBuffer & istr, size_t rows, double avg_value_size_hint)\n {\n     IDataType::DeserializeBinaryBulkSettings settings;\n     settings.getter = [&](IDataType::SubstreamPath) -> ReadBuffer * { return &istr; };\n@@ -82,8 +82,8 @@ void NativeBlockInputStream::readData(const IDataType & type, IColumn & column,\n     type.deserializeBinaryBulkStatePrefix(settings, state);\n     type.deserializeBinaryBulkWithMultipleStreams(column, rows, settings, state);\n \n-    if (column.size() != rows)\n-        throw Exception(\"Cannot read all data in NativeBlockInputStream. Rows read: \" + toString(column.size()) + \". Rows expected: \" + toString(rows) + \".\",\n+    if (column->size() != rows)\n+        throw Exception(\"Cannot read all data in NativeBlockInputStream. Rows read: \" + toString(column->size()) + \". Rows expected: \" + toString(rows) + \".\",\n             ErrorCodes::CANNOT_READ_ALL_DATA);\n }\n \n@@ -158,11 +158,11 @@ Block NativeBlockInputStream::readImpl()\n         }\n \n         /// Data\n-        MutableColumnPtr read_column = column.type->createColumn();\n+        ColumnPtr read_column = column.type->createColumn();\n \n         double avg_value_size_hint = avg_value_size_hints.empty() ? 0 : avg_value_size_hints[i];\n         if (rows)    /// If no rows, nothing to read.\n-            readData(*column.type, *read_column, istr, rows, avg_value_size_hint);\n+            readData(*column.type, read_column, istr, rows, avg_value_size_hint);\n \n         column.column = std::move(read_column);\n \ndiff --git a/src/DataStreams/NativeBlockInputStream.h b/src/DataStreams/NativeBlockInputStream.h\nindex 774a1cfa1cd7..8f3d2843e0f9 100644\n--- a/src/DataStreams/NativeBlockInputStream.h\n+++ b/src/DataStreams/NativeBlockInputStream.h\n@@ -74,7 +74,7 @@ class NativeBlockInputStream : public IBlockInputStream\n \n     String getName() const override { return \"Native\"; }\n \n-    static void readData(const IDataType & type, IColumn & column, ReadBuffer & istr, size_t rows, double avg_value_size_hint);\n+    static void readData(const IDataType & type, ColumnPtr & column, ReadBuffer & istr, size_t rows, double avg_value_size_hint);\n \n     Block getHeader() const override;\n \ndiff --git a/src/DataTypes/DataTypeArray.cpp b/src/DataTypes/DataTypeArray.cpp\nindex 9cd56d0e2b5d..3ad84a8fcd77 100644\n--- a/src/DataTypes/DataTypeArray.cpp\n+++ b/src/DataTypes/DataTypeArray.cpp\n@@ -10,12 +10,15 @@\n #include <DataTypes/DataTypesNumber.h>\n #include <DataTypes/DataTypeArray.h>\n #include <DataTypes/DataTypeFactory.h>\n+#include <DataTypes/DataTypeOneElementTuple.h>\n \n #include <Parsers/IAST.h>\n \n #include <Common/typeid_cast.h>\n #include <Common/assert_cast.h>\n \n+#include <Core/NamesAndTypes.h>\n+\n \n namespace DB\n {\n@@ -145,10 +148,57 @@ namespace\n \n         offset_values.resize(i);\n     }\n+\n+    ColumnPtr arrayOffsetsToSizes(const IColumn & column)\n+    {\n+        const auto & column_offsets = assert_cast<const ColumnArray::ColumnOffsets &>(column);\n+        MutableColumnPtr column_sizes = column_offsets.cloneEmpty();\n+\n+        if (column_offsets.empty())\n+            return column_sizes;\n+\n+        const auto & offsets_data = column_offsets.getData();\n+        auto & sizes_data = assert_cast<ColumnArray::ColumnOffsets &>(*column_sizes).getData();\n+\n+        sizes_data.resize(offsets_data.size());\n+\n+        IColumn::Offset prev_offset = 0;\n+        for (size_t i = 0, size = offsets_data.size(); i < size; ++i)\n+        {\n+            auto current_offset = offsets_data[i];\n+            sizes_data[i] = current_offset - prev_offset;\n+            prev_offset =  current_offset;\n+        }\n+\n+        return column_sizes;\n+    }\n+\n+    ColumnPtr arraySizesToOffsets(const IColumn & column)\n+    {\n+        const auto & column_sizes = assert_cast<const ColumnArray::ColumnOffsets &>(column);\n+        MutableColumnPtr column_offsets = column_sizes.cloneEmpty();\n+\n+        if (column_sizes.empty())\n+            return column_offsets;\n+\n+        const auto & sizes_data = column_sizes.getData();\n+        auto & offsets_data = assert_cast<ColumnArray::ColumnOffsets &>(*column_offsets).getData();\n+\n+        offsets_data.resize(sizes_data.size());\n+\n+        IColumn::Offset prev_offset = 0;\n+        for (size_t i = 0, size = sizes_data.size(); i < size; ++i)\n+        {\n+            prev_offset += sizes_data[i];\n+            offsets_data[i] = prev_offset;\n+        }\n+\n+        return column_offsets;\n+    }\n }\n \n \n-void DataTypeArray::enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const\n+void DataTypeArray::enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const\n {\n     path.push_back(Substream::ArraySizes);\n     callback(path, *this);\n@@ -158,7 +208,7 @@ void DataTypeArray::enumerateStreams(const StreamCallback & callback, SubstreamP\n }\n \n \n-void DataTypeArray::serializeBinaryBulkStatePrefix(\n+void DataTypeArray::serializeBinaryBulkStatePrefixImpl(\n     SerializeBinaryBulkSettings & settings,\n     SerializeBinaryBulkStatePtr & state) const\n {\n@@ -168,7 +218,7 @@ void DataTypeArray::serializeBinaryBulkStatePrefix(\n }\n \n \n-void DataTypeArray::serializeBinaryBulkStateSuffix(\n+void DataTypeArray::serializeBinaryBulkStateSuffixImpl(\n     SerializeBinaryBulkSettings & settings,\n     SerializeBinaryBulkStatePtr & state) const\n {\n@@ -178,7 +228,7 @@ void DataTypeArray::serializeBinaryBulkStateSuffix(\n }\n \n \n-void DataTypeArray::deserializeBinaryBulkStatePrefix(\n+void DataTypeArray::deserializeBinaryBulkStatePrefixImpl(\n     DeserializeBinaryBulkSettings & settings,\n     DeserializeBinaryBulkStatePtr & state) const\n {\n@@ -188,7 +238,7 @@ void DataTypeArray::deserializeBinaryBulkStatePrefix(\n }\n \n \n-void DataTypeArray::serializeBinaryBulkWithMultipleStreams(\n+void DataTypeArray::serializeBinaryBulkWithMultipleStreamsImpl(\n     const IColumn & column,\n     size_t offset,\n     size_t limit,\n@@ -235,44 +285,52 @@ void DataTypeArray::serializeBinaryBulkWithMultipleStreams(\n }\n \n \n-void DataTypeArray::deserializeBinaryBulkWithMultipleStreams(\n+void DataTypeArray::deserializeBinaryBulkWithMultipleStreamsImpl(\n     IColumn & column,\n     size_t limit,\n     DeserializeBinaryBulkSettings & settings,\n-    DeserializeBinaryBulkStatePtr & state) const\n+    DeserializeBinaryBulkStatePtr & state,\n+    SubstreamsCache * cache) const\n {\n     ColumnArray & column_array = typeid_cast<ColumnArray &>(column);\n-\n     settings.path.push_back(Substream::ArraySizes);\n-    if (auto * stream = settings.getter(settings.path))\n+\n+    if (auto cached_column = getFromSubstreamsCache(cache, settings.path))\n+    {\n+        column_array.getOffsetsPtr() = arraySizesToOffsets(*cached_column);\n+    }\n+    else if (auto * stream = settings.getter(settings.path))\n     {\n         if (settings.position_independent_encoding)\n             deserializeArraySizesPositionIndependent(column, *stream, limit);\n         else\n             DataTypeNumber<ColumnArray::Offset>().deserializeBinaryBulk(column_array.getOffsetsColumn(), *stream, limit, 0);\n+\n+        addToSubstreamsCache(cache, settings.path, arrayOffsetsToSizes(column_array.getOffsetsColumn()));\n     }\n \n     settings.path.back() = Substream::ArrayElements;\n \n     ColumnArray::Offsets & offset_values = column_array.getOffsets();\n-    IColumn & nested_column = column_array.getData();\n+    ColumnPtr & nested_column = column_array.getDataPtr();\n \n     /// Number of values corresponding with `offset_values` must be read.\n     size_t last_offset = offset_values.back();\n-    if (last_offset < nested_column.size())\n+    if (last_offset < nested_column->size())\n         throw Exception(\"Nested column is longer than last offset\", ErrorCodes::LOGICAL_ERROR);\n-    size_t nested_limit = last_offset - nested_column.size();\n+    size_t nested_limit = last_offset - nested_column->size();\n \n     /// Adjust value size hint. Divide it to the average array size.\n     settings.avg_value_size_hint = nested_limit ? settings.avg_value_size_hint / nested_limit * offset_values.size() : 0;\n \n-    nested->deserializeBinaryBulkWithMultipleStreams(nested_column, nested_limit, settings, state);\n+    nested->deserializeBinaryBulkWithMultipleStreams(nested_column, nested_limit, settings, state, cache);\n+\n     settings.path.pop_back();\n \n     /// Check consistency between offsets and elements subcolumns.\n     /// But if elements column is empty - it's ok for columns of Nested types that was added by ALTER.\n-    if (!nested_column.empty() && nested_column.size() != last_offset)\n-        throw ParsingException(\"Cannot read all array values: read just \" + toString(nested_column.size()) + \" of \" + toString(last_offset),\n+    if (!nested_column->empty() && nested_column->size() != last_offset)\n+        throw ParsingException(\"Cannot read all array values: read just \" + toString(nested_column->size()) + \" of \" + toString(last_offset),\n             ErrorCodes::CANNOT_READ_ALL_DATA);\n }\n \n@@ -530,6 +588,44 @@ bool DataTypeArray::equals(const IDataType & rhs) const\n     return typeid(rhs) == typeid(*this) && nested->equals(*static_cast<const DataTypeArray &>(rhs).nested);\n }\n \n+DataTypePtr DataTypeArray::tryGetSubcolumnType(const String & subcolumn_name) const\n+{\n+    return tryGetSubcolumnTypeImpl(subcolumn_name, 0);\n+}\n+\n+DataTypePtr DataTypeArray::tryGetSubcolumnTypeImpl(const String & subcolumn_name, size_t level) const\n+{\n+    if (subcolumn_name == \"size\" + std::to_string(level))\n+        return createOneElementTuple(std::make_shared<DataTypeUInt64>(), subcolumn_name, false);\n+\n+    DataTypePtr subcolumn;\n+    if (const auto * nested_array = typeid_cast<const DataTypeArray *>(nested.get()))\n+        subcolumn = nested_array->tryGetSubcolumnTypeImpl(subcolumn_name, level + 1);\n+    else\n+        subcolumn = nested->tryGetSubcolumnType(subcolumn_name);\n+\n+    return (subcolumn ? std::make_shared<DataTypeArray>(std::move(subcolumn)) : subcolumn);\n+}\n+\n+ColumnPtr DataTypeArray::getSubcolumn(const String & subcolumn_name, const IColumn & column) const\n+{\n+    return getSubcolumnImpl(subcolumn_name, column, 0);\n+}\n+\n+ColumnPtr DataTypeArray::getSubcolumnImpl(const String & subcolumn_name, const IColumn & column, size_t level) const\n+{\n+    const auto & column_array = assert_cast<const ColumnArray &>(column);\n+    if (subcolumn_name == \"size\" + std::to_string(level))\n+        return arrayOffsetsToSizes(column_array.getOffsetsColumn());\n+\n+    ColumnPtr subcolumn;\n+    if (const auto * nested_array = typeid_cast<const DataTypeArray *>(nested.get()))\n+        subcolumn = nested_array->getSubcolumnImpl(subcolumn_name, column_array.getData(), level + 1);\n+    else\n+        subcolumn = nested->getSubcolumn(subcolumn_name, column_array.getData());\n+\n+    return ColumnArray::create(subcolumn, column_array.getOffsetsPtr());\n+}\n \n size_t DataTypeArray::getNumberOfDimensions() const\n {\ndiff --git a/src/DataTypes/DataTypeArray.h b/src/DataTypes/DataTypeArray.h\nindex 1451f27dfbe5..ba19ad021be3 100644\n--- a/src/DataTypes/DataTypeArray.h\n+++ b/src/DataTypes/DataTypeArray.h\n@@ -57,32 +57,33 @@ class DataTypeArray final : public DataTypeWithSimpleSerialization\n       * This is necessary, because when implementing nested structures, several arrays can have common sizes.\n       */\n \n-    void enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const override;\n+    void enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const override;\n \n-    void serializeBinaryBulkStatePrefix(\n+    void serializeBinaryBulkStatePrefixImpl(\n             SerializeBinaryBulkSettings & settings,\n             SerializeBinaryBulkStatePtr & state) const override;\n \n-    void serializeBinaryBulkStateSuffix(\n+    void serializeBinaryBulkStateSuffixImpl(\n             SerializeBinaryBulkSettings & settings,\n             SerializeBinaryBulkStatePtr & state) const override;\n \n-    void deserializeBinaryBulkStatePrefix(\n+    void deserializeBinaryBulkStatePrefixImpl(\n             DeserializeBinaryBulkSettings & settings,\n             DeserializeBinaryBulkStatePtr & state) const override;\n \n-    void serializeBinaryBulkWithMultipleStreams(\n+    void serializeBinaryBulkWithMultipleStreamsImpl(\n             const IColumn & column,\n             size_t offset,\n             size_t limit,\n             SerializeBinaryBulkSettings & settings,\n             SerializeBinaryBulkStatePtr & state) const override;\n \n-    void deserializeBinaryBulkWithMultipleStreams(\n+    void deserializeBinaryBulkWithMultipleStreamsImpl(\n             IColumn & column,\n             size_t limit,\n             DeserializeBinaryBulkSettings & settings,\n-            DeserializeBinaryBulkStatePtr & state) const override;\n+            DeserializeBinaryBulkStatePtr & state,\n+            SubstreamsCache * cache) const override;\n \n     void serializeProtobuf(const IColumn & column,\n                            size_t row_num,\n@@ -111,10 +112,17 @@ class DataTypeArray final : public DataTypeWithSimpleSerialization\n         return nested->isValueUnambiguouslyRepresentedInFixedSizeContiguousMemoryRegion();\n     }\n \n+    DataTypePtr tryGetSubcolumnType(const String & subcolumn_name) const override;\n+    ColumnPtr getSubcolumn(const String & subcolumn_name, const IColumn & column) const override;\n+\n     const DataTypePtr & getNestedType() const { return nested; }\n \n     /// 1 for plain array, 2 for array of arrays and so on.\n     size_t getNumberOfDimensions() const;\n+\n+private:\n+    ColumnPtr getSubcolumnImpl(const String & subcolumn_name, const IColumn & column, size_t level) const;\n+    DataTypePtr tryGetSubcolumnTypeImpl(const String & subcolumn_name, size_t level) const;\n };\n \n }\ndiff --git a/src/DataTypes/DataTypeCustom.h b/src/DataTypes/DataTypeCustom.h\nindex c4f846d02593..0fa2e365990d 100644\n--- a/src/DataTypes/DataTypeCustom.h\n+++ b/src/DataTypes/DataTypeCustom.h\n@@ -3,6 +3,7 @@\n #include <memory>\n #include <cstddef>\n #include <Core/Types.h>\n+#include <DataTypes/IDataType.h>\n \n namespace DB\n {\n@@ -62,8 +63,51 @@ class IDataTypeCustomTextSerialization\n     virtual void serializeTextXML(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const = 0;\n };\n \n+/** Allows to customize an existing data type by representation with custom substreams.\n+  * Customized data type will be serialized/deserialized to files with different names than base type,\n+  * but binary and text representation will be unchanged.\n+  * E.g it can be used for reading single subcolumns of complex types.\n+  */\n+class IDataTypeCustomStreams\n+{\n+public:\n+    virtual ~IDataTypeCustomStreams() = default;\n+\n+    virtual void enumerateStreams(\n+        const IDataType::StreamCallback & callback,\n+        IDataType::SubstreamPath & path) const = 0;\n+\n+    virtual void serializeBinaryBulkStatePrefix(\n+        IDataType::SerializeBinaryBulkSettings & settings,\n+        IDataType::SerializeBinaryBulkStatePtr & state) const = 0;\n+\n+    virtual void serializeBinaryBulkStateSuffix(\n+        IDataType::SerializeBinaryBulkSettings & settings,\n+        IDataType::SerializeBinaryBulkStatePtr & state) const = 0;\n+\n+    virtual void deserializeBinaryBulkStatePrefix(\n+        IDataType::DeserializeBinaryBulkSettings & settings,\n+        IDataType::DeserializeBinaryBulkStatePtr & state) const = 0;\n+\n+    virtual void serializeBinaryBulkWithMultipleStreams(\n+        const IColumn & column,\n+        size_t offset,\n+        size_t limit,\n+        IDataType::SerializeBinaryBulkSettings & settings,\n+        IDataType::SerializeBinaryBulkStatePtr & state) const = 0;\n+\n+    virtual void deserializeBinaryBulkWithMultipleStreams(\n+        ColumnPtr & column,\n+        size_t limit,\n+        IDataType::DeserializeBinaryBulkSettings & settings,\n+        IDataType::DeserializeBinaryBulkStatePtr & state,\n+        IDataType::SubstreamsCache * cache) const = 0;\n+};\n+\n using DataTypeCustomNamePtr = std::unique_ptr<const IDataTypeCustomName>;\n using DataTypeCustomTextSerializationPtr = std::unique_ptr<const IDataTypeCustomTextSerialization>;\n+using DataTypeCustomStreamsPtr = std::unique_ptr<const IDataTypeCustomStreams>;\n+\n \n /** Describe a data type customization\n  */\n@@ -71,9 +115,15 @@ struct DataTypeCustomDesc\n {\n     DataTypeCustomNamePtr name;\n     DataTypeCustomTextSerializationPtr text_serialization;\n-\n-    DataTypeCustomDesc(DataTypeCustomNamePtr name_, DataTypeCustomTextSerializationPtr text_serialization_)\n-            : name(std::move(name_)), text_serialization(std::move(text_serialization_)) {}\n+    DataTypeCustomStreamsPtr streams;\n+\n+    DataTypeCustomDesc(\n+        DataTypeCustomNamePtr name_,\n+        DataTypeCustomTextSerializationPtr text_serialization_ = nullptr,\n+        DataTypeCustomStreamsPtr streams_ = nullptr)\n+    : name(std::move(name_))\n+    , text_serialization(std::move(text_serialization_))\n+    , streams(std::move(streams_)) {}\n };\n \n using DataTypeCustomDescPtr = std::unique_ptr<DataTypeCustomDesc>;\ndiff --git a/src/DataTypes/DataTypeCustom_fwd.h b/src/DataTypes/DataTypeCustom_fwd.h\nnew file mode 100644\nindex 000000000000..99c8eee9748b\n--- /dev/null\n+++ b/src/DataTypes/DataTypeCustom_fwd.h\n@@ -0,0 +1,18 @@\n+#pragma once\n+\n+#include <memory>\n+\n+namespace DB\n+{\n+\n+class IDataTypeCustomName;\n+class IDataTypeCustomTextSerialization;\n+class IDataTypeCustomStreams;\n+struct DataTypeCustomDesc;\n+\n+using DataTypeCustomNamePtr = std::unique_ptr<const IDataTypeCustomName>;\n+using DataTypeCustomTextSerializationPtr = std::unique_ptr<const IDataTypeCustomTextSerialization>;\n+using DataTypeCustomStreamsPtr = std::unique_ptr<const IDataTypeCustomStreams>;\n+using DataTypeCustomDescPtr = std::unique_ptr<DataTypeCustomDesc>;\n+\n+}\ndiff --git a/src/DataTypes/DataTypeFactory.cpp b/src/DataTypes/DataTypeFactory.cpp\nindex 1ff00d97e84c..1a1f51178ade 100644\n--- a/src/DataTypes/DataTypeFactory.cpp\n+++ b/src/DataTypes/DataTypeFactory.cpp\n@@ -79,6 +79,16 @@ DataTypePtr DataTypeFactory::get(const String & family_name_param, const ASTPtr\n     return findCreatorByName(family_name)(parameters);\n }\n \n+DataTypePtr DataTypeFactory::getCustom(DataTypeCustomDescPtr customization) const\n+{\n+    if (!customization->name)\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Cannot create custom type without name\");\n+\n+    auto type = get(customization->name->getName());\n+    type->setCustomization(std::move(customization));\n+    return type;\n+}\n+\n \n void DataTypeFactory::registerDataType(const String & family_name, Value creator, CaseSensitiveness case_sensitiveness)\n {\ndiff --git a/src/DataTypes/DataTypeFactory.h b/src/DataTypes/DataTypeFactory.h\nindex 192b1beae5da..618c1f510679 100644\n--- a/src/DataTypes/DataTypeFactory.h\n+++ b/src/DataTypes/DataTypeFactory.h\n@@ -3,6 +3,7 @@\n #include <DataTypes/IDataType.h>\n #include <Parsers/IAST_fwd.h>\n #include <Common/IFactoryWithAliases.h>\n+#include <DataTypes/DataTypeCustom_fwd.h>\n \n \n #include <functional>\n@@ -33,6 +34,7 @@ class DataTypeFactory final : private boost::noncopyable, public IFactoryWithAli\n     DataTypePtr get(const String & full_name) const;\n     DataTypePtr get(const String & family_name, const ASTPtr & parameters) const;\n     DataTypePtr get(const ASTPtr & ast) const;\n+    DataTypePtr getCustom(DataTypeCustomDescPtr customization) const;\n \n     /// Register a type family by its name.\n     void registerDataType(const String & family_name, Value creator, CaseSensitiveness case_sensitiveness = CaseSensitive);\n@@ -84,5 +86,6 @@ void registerDataTypeLowCardinality(DataTypeFactory & factory);\n void registerDataTypeDomainIPv4AndIPv6(DataTypeFactory & factory);\n void registerDataTypeDomainSimpleAggregateFunction(DataTypeFactory & factory);\n void registerDataTypeDomainGeo(DataTypeFactory & factory);\n+void registerDataTypeOneElementTuple(DataTypeFactory & factory);\n \n }\ndiff --git a/src/DataTypes/DataTypeLowCardinality.cpp b/src/DataTypes/DataTypeLowCardinality.cpp\nindex 8f4b2bf76366..a433d39c5611 100644\n--- a/src/DataTypes/DataTypeLowCardinality.cpp\n+++ b/src/DataTypes/DataTypeLowCardinality.cpp\n@@ -50,7 +50,7 @@ DataTypeLowCardinality::DataTypeLowCardinality(DataTypePtr dictionary_type_)\n                         + dictionary_type->getName(), ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT);\n }\n \n-void DataTypeLowCardinality::enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const\n+void DataTypeLowCardinality::enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const\n {\n     path.push_back(Substream::DictionaryKeys);\n     dictionary_type->enumerateStreams(callback, path);\n@@ -243,7 +243,7 @@ static DeserializeStateLowCardinality * checkAndGetLowCardinalityDeserializeStat\n     return low_cardinality_state;\n }\n \n-void DataTypeLowCardinality::serializeBinaryBulkStatePrefix(\n+void DataTypeLowCardinality::serializeBinaryBulkStatePrefixImpl(\n     SerializeBinaryBulkSettings & settings,\n     SerializeBinaryBulkStatePtr & state) const\n {\n@@ -263,7 +263,7 @@ void DataTypeLowCardinality::serializeBinaryBulkStatePrefix(\n     state = std::make_shared<SerializeStateLowCardinality>(key_version);\n }\n \n-void DataTypeLowCardinality::serializeBinaryBulkStateSuffix(\n+void DataTypeLowCardinality::serializeBinaryBulkStateSuffixImpl(\n     SerializeBinaryBulkSettings & settings,\n     SerializeBinaryBulkStatePtr & state) const\n {\n@@ -289,7 +289,7 @@ void DataTypeLowCardinality::serializeBinaryBulkStateSuffix(\n     }\n }\n \n-void DataTypeLowCardinality::deserializeBinaryBulkStatePrefix(\n+void DataTypeLowCardinality::deserializeBinaryBulkStatePrefixImpl(\n     DeserializeBinaryBulkSettings & settings,\n     DeserializeBinaryBulkStatePtr & state) const\n {\n@@ -482,7 +482,7 @@ namespace\n     }\n }\n \n-void DataTypeLowCardinality::serializeBinaryBulkWithMultipleStreams(\n+void DataTypeLowCardinality::serializeBinaryBulkWithMultipleStreamsImpl(\n     const IColumn & column,\n     size_t offset,\n     size_t limit,\n@@ -579,11 +579,12 @@ void DataTypeLowCardinality::serializeBinaryBulkWithMultipleStreams(\n     index_version.getDataType()->serializeBinaryBulk(*positions, *indexes_stream, 0, num_rows);\n }\n \n-void DataTypeLowCardinality::deserializeBinaryBulkWithMultipleStreams(\n+void DataTypeLowCardinality::deserializeBinaryBulkWithMultipleStreamsImpl(\n     IColumn & column,\n     size_t limit,\n     DeserializeBinaryBulkSettings & settings,\n-    DeserializeBinaryBulkStatePtr & state) const\n+    DeserializeBinaryBulkStatePtr & state,\n+    SubstreamsCache * /* cache */) const\n {\n     ColumnLowCardinality & low_cardinality_column = typeid_cast<ColumnLowCardinality &>(column);\n \ndiff --git a/src/DataTypes/DataTypeLowCardinality.h b/src/DataTypes/DataTypeLowCardinality.h\nindex f8c314909b85..6ed2b792ce3e 100644\n--- a/src/DataTypes/DataTypeLowCardinality.h\n+++ b/src/DataTypes/DataTypeLowCardinality.h\n@@ -22,32 +22,33 @@ class DataTypeLowCardinality : public IDataType\n     const char * getFamilyName() const override { return \"LowCardinality\"; }\n     TypeIndex getTypeId() const override { return TypeIndex::LowCardinality; }\n \n-    void enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const override;\n+    void enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const override;\n \n-    void serializeBinaryBulkStatePrefix(\n+    void serializeBinaryBulkStatePrefixImpl(\n             SerializeBinaryBulkSettings & settings,\n             SerializeBinaryBulkStatePtr & state) const override;\n \n-    void serializeBinaryBulkStateSuffix(\n+    void serializeBinaryBulkStateSuffixImpl(\n             SerializeBinaryBulkSettings & settings,\n             SerializeBinaryBulkStatePtr & state) const override;\n \n-    void deserializeBinaryBulkStatePrefix(\n+    void deserializeBinaryBulkStatePrefixImpl(\n             DeserializeBinaryBulkSettings & settings,\n             DeserializeBinaryBulkStatePtr & state) const override;\n \n-    void serializeBinaryBulkWithMultipleStreams(\n+    void serializeBinaryBulkWithMultipleStreamsImpl(\n             const IColumn & column,\n             size_t offset,\n             size_t limit,\n             SerializeBinaryBulkSettings & settings,\n             SerializeBinaryBulkStatePtr & state) const override;\n \n-    void deserializeBinaryBulkWithMultipleStreams(\n+    void deserializeBinaryBulkWithMultipleStreamsImpl(\n             IColumn & column,\n             size_t limit,\n             DeserializeBinaryBulkSettings & settings,\n-            DeserializeBinaryBulkStatePtr & state) const override;\n+            DeserializeBinaryBulkStatePtr & state,\n+            SubstreamsCache * cache) const override;\n \n     void serializeBinary(const Field & field, WriteBuffer & ostr) const override;\n     void deserializeBinary(Field & field, ReadBuffer & istr) const override;\ndiff --git a/src/DataTypes/DataTypeMap.cpp b/src/DataTypes/DataTypeMap.cpp\nindex b207702d9b7f..3f59e1d36549 100644\n--- a/src/DataTypes/DataTypeMap.cpp\n+++ b/src/DataTypes/DataTypeMap.cpp\n@@ -278,34 +278,34 @@ void DataTypeMap::deserializeTextCSV(IColumn & column, ReadBuffer & istr, const\n }\n \n \n-void DataTypeMap::enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const\n+void DataTypeMap::enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const\n {\n     nested->enumerateStreams(callback, path);\n }\n \n-void DataTypeMap::serializeBinaryBulkStatePrefix(\n+void DataTypeMap::serializeBinaryBulkStatePrefixImpl(\n     SerializeBinaryBulkSettings & settings,\n     SerializeBinaryBulkStatePtr & state) const\n {\n     nested->serializeBinaryBulkStatePrefix(settings, state);\n }\n \n-void DataTypeMap::serializeBinaryBulkStateSuffix(\n+void DataTypeMap::serializeBinaryBulkStateSuffixImpl(\n     SerializeBinaryBulkSettings & settings,\n     SerializeBinaryBulkStatePtr & state) const\n {\n     nested->serializeBinaryBulkStateSuffix(settings, state);\n }\n \n-void DataTypeMap::deserializeBinaryBulkStatePrefix(\n-        DeserializeBinaryBulkSettings & settings,\n-        DeserializeBinaryBulkStatePtr & state) const\n+void DataTypeMap::deserializeBinaryBulkStatePrefixImpl(\n+    DeserializeBinaryBulkSettings & settings,\n+    DeserializeBinaryBulkStatePtr & state) const\n {\n     nested->deserializeBinaryBulkStatePrefix(settings, state);\n }\n \n \n-void DataTypeMap::serializeBinaryBulkWithMultipleStreams(\n+void DataTypeMap::serializeBinaryBulkWithMultipleStreamsImpl(\n     const IColumn & column,\n     size_t offset,\n     size_t limit,\n@@ -315,13 +315,15 @@ void DataTypeMap::serializeBinaryBulkWithMultipleStreams(\n     nested->serializeBinaryBulkWithMultipleStreams(extractNestedColumn(column), offset, limit, settings, state);\n }\n \n-void DataTypeMap::deserializeBinaryBulkWithMultipleStreams(\n+void DataTypeMap::deserializeBinaryBulkWithMultipleStreamsImpl(\n     IColumn & column,\n     size_t limit,\n     DeserializeBinaryBulkSettings & settings,\n-    DeserializeBinaryBulkStatePtr & state) const\n+    DeserializeBinaryBulkStatePtr & state,\n+    SubstreamsCache * cache) const\n {\n-    nested->deserializeBinaryBulkWithMultipleStreams(extractNestedColumn(column), limit, settings, state);\n+    auto & column_map = assert_cast<ColumnMap &>(column);\n+    nested->deserializeBinaryBulkWithMultipleStreams(column_map.getNestedColumnPtr(), limit, settings, state, cache);\n }\n \n void DataTypeMap::serializeProtobuf(const IColumn & column, size_t row_num, ProtobufWriter & protobuf, size_t & value_index) const\ndiff --git a/src/DataTypes/DataTypeMap.h b/src/DataTypes/DataTypeMap.h\nindex a52969428f41..7c51ac2f5795 100644\n--- a/src/DataTypes/DataTypeMap.h\n+++ b/src/DataTypes/DataTypeMap.h\n@@ -46,34 +46,33 @@ class DataTypeMap final : public DataTypeWithSimpleSerialization\n     void deserializeTextCSV(IColumn & column, ReadBuffer & istr, const FormatSettings &) const override;\n \n \n-    /** Each sub-column in a map is serialized in separate stream.\n-      */\n-    void enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const override;\n+    void enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const override;\n \n-    void serializeBinaryBulkStatePrefix(\n+    void serializeBinaryBulkStatePrefixImpl(\n            SerializeBinaryBulkSettings & settings,\n            SerializeBinaryBulkStatePtr & state) const override;\n \n-    void serializeBinaryBulkStateSuffix(\n+    void serializeBinaryBulkStateSuffixImpl(\n            SerializeBinaryBulkSettings & settings,\n            SerializeBinaryBulkStatePtr & state) const override;\n \n-    void deserializeBinaryBulkStatePrefix(\n+    void deserializeBinaryBulkStatePrefixImpl(\n            DeserializeBinaryBulkSettings & settings,\n            DeserializeBinaryBulkStatePtr & state) const override;\n \n-    void serializeBinaryBulkWithMultipleStreams(\n+    void serializeBinaryBulkWithMultipleStreamsImpl(\n            const IColumn & column,\n            size_t offset,\n            size_t limit,\n            SerializeBinaryBulkSettings & settings,\n            SerializeBinaryBulkStatePtr & state) const override;\n \n-    void deserializeBinaryBulkWithMultipleStreams(\n+    void deserializeBinaryBulkWithMultipleStreamsImpl(\n            IColumn & column,\n            size_t limit,\n            DeserializeBinaryBulkSettings & settings,\n-           DeserializeBinaryBulkStatePtr & state) const override;\n+           DeserializeBinaryBulkStatePtr & state,\n+           SubstreamsCache * cache) const override;\n \n     void serializeProtobuf(const IColumn & column, size_t row_num, ProtobufWriter & protobuf, size_t & value_index) const override;\n     void deserializeProtobuf(IColumn & column, ProtobufReader & protobuf, bool allow_add_row, bool & row_added) const override;\ndiff --git a/src/DataTypes/DataTypeNested.cpp b/src/DataTypes/DataTypeNested.cpp\nnew file mode 100644\nindex 000000000000..cfbfb4c17504\n--- /dev/null\n+++ b/src/DataTypes/DataTypeNested.cpp\n@@ -0,0 +1,76 @@\n+#include <DataTypes/DataTypeNested.h>\n+#include <DataTypes/DataTypeFactory.h>\n+#include <DataTypes/DataTypeArray.h>\n+#include <DataTypes/DataTypeTuple.h>\n+#include <IO/WriteBufferFromString.h>\n+#include <IO/Operators.h>\n+#include <Common/quoteString.h>\n+#include <Parsers/ASTNameTypePair.h>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int EMPTY_DATA_PASSED;\n+    extern const int BAD_ARGUMENTS;\n+}\n+\n+String DataTypeNestedCustomName::getName() const\n+{\n+    WriteBufferFromOwnString s;\n+    s << \"Nested(\";\n+    for (size_t i = 0; i < elems.size(); ++i)\n+    {\n+        if (i != 0)\n+            s << \", \";\n+\n+        s << backQuoteIfNeed(names[i]) << ' ';\n+        s << elems[i]->getName();\n+    }\n+    s << \")\";\n+\n+    return s.str();\n+}\n+\n+static std::pair<DataTypePtr, DataTypeCustomDescPtr> create(const ASTPtr & arguments)\n+{\n+    if (!arguments || arguments->children.empty())\n+        throw Exception(\"Nested cannot be empty\", ErrorCodes::EMPTY_DATA_PASSED);\n+\n+    DataTypes nested_types;\n+    Strings nested_names;\n+    nested_types.reserve(arguments->children.size());\n+    nested_names.reserve(arguments->children.size());\n+\n+    for (const auto & child : arguments->children)\n+    {\n+        const auto * name_type = child->as<ASTNameTypePair>();\n+        if (!name_type)\n+            throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Data type Nested accepts only pairs with name and type\");\n+\n+        auto nested_type = DataTypeFactory::instance().get(name_type->type);\n+        nested_types.push_back(std::move(nested_type));\n+        nested_names.push_back(name_type->name);\n+    }\n+\n+    auto data_type = std::make_shared<DataTypeArray>(std::make_shared<DataTypeTuple>(nested_types, nested_names));\n+    auto custom_name = std::make_unique<DataTypeNestedCustomName>(nested_types, nested_names);\n+\n+    return std::make_pair(std::move(data_type), std::make_unique<DataTypeCustomDesc>(std::move(custom_name), nullptr));\n+}\n+\n+void registerDataTypeNested(DataTypeFactory & factory)\n+{\n+    return factory.registerDataTypeCustom(\"Nested\", create);\n+}\n+\n+DataTypePtr createNested(const DataTypes & types, const Names & names)\n+{\n+    auto custom_desc = std::make_unique<DataTypeCustomDesc>(\n+        std::make_unique<DataTypeNestedCustomName>(types, names));\n+\n+    return DataTypeFactory::instance().getCustom(std::move(custom_desc));\n+}\n+\n+}\ndiff --git a/src/DataTypes/DataTypeNested.h b/src/DataTypes/DataTypeNested.h\nnew file mode 100644\nindex 000000000000..9fb12ad49247\n--- /dev/null\n+++ b/src/DataTypes/DataTypeNested.h\n@@ -0,0 +1,34 @@\n+#pragma once\n+\n+#include <DataTypes/DataTypeWithSimpleSerialization.h>\n+#include <DataTypes/DataTypeCustom.h>\n+\n+\n+namespace DB\n+{\n+\n+class DataTypeNestedCustomName final : public IDataTypeCustomName\n+{\n+private:\n+    DataTypes elems;\n+    Strings names;\n+\n+public:\n+    DataTypeNestedCustomName(const DataTypes & elems_, const Strings & names_)\n+        : elems(elems_), names(names_)\n+    {\n+    }\n+\n+    String getName() const override;\n+};\n+\n+DataTypePtr createNested(const DataTypes & types, const Names & names);\n+\n+template <typename DataType>\n+inline bool isNested(const DataType & data_type)\n+{\n+    return typeid_cast<const DataTypeNestedCustomName *>(data_type->getCustomName()) != nullptr;\n+}\n+\n+}\n+\ndiff --git a/src/DataTypes/DataTypeNullable.cpp b/src/DataTypes/DataTypeNullable.cpp\nindex a0fc8baaf7e4..64b060e521bc 100644\n--- a/src/DataTypes/DataTypeNullable.cpp\n+++ b/src/DataTypes/DataTypeNullable.cpp\n@@ -2,6 +2,7 @@\n #include <DataTypes/DataTypeNothing.h>\n #include <DataTypes/DataTypesNumber.h>\n #include <DataTypes/DataTypeFactory.h>\n+#include <DataTypes/DataTypeOneElementTuple.h>\n #include <Columns/ColumnNullable.h>\n #include <Core/Field.h>\n #include <IO/ReadBuffer.h>\n@@ -41,7 +42,7 @@ bool DataTypeNullable::onlyNull() const\n }\n \n \n-void DataTypeNullable::enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const\n+void DataTypeNullable::enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const\n {\n     path.push_back(Substream::NullMap);\n     callback(path, *this);\n@@ -51,7 +52,7 @@ void DataTypeNullable::enumerateStreams(const StreamCallback & callback, Substre\n }\n \n \n-void DataTypeNullable::serializeBinaryBulkStatePrefix(\n+void DataTypeNullable::serializeBinaryBulkStatePrefixImpl(\n         SerializeBinaryBulkSettings & settings,\n         SerializeBinaryBulkStatePtr & state) const\n {\n@@ -61,7 +62,7 @@ void DataTypeNullable::serializeBinaryBulkStatePrefix(\n }\n \n \n-void DataTypeNullable::serializeBinaryBulkStateSuffix(\n+void DataTypeNullable::serializeBinaryBulkStateSuffixImpl(\n     SerializeBinaryBulkSettings & settings,\n     SerializeBinaryBulkStatePtr & state) const\n {\n@@ -71,7 +72,7 @@ void DataTypeNullable::serializeBinaryBulkStateSuffix(\n }\n \n \n-void DataTypeNullable::deserializeBinaryBulkStatePrefix(\n+void DataTypeNullable::deserializeBinaryBulkStatePrefixImpl(\n     DeserializeBinaryBulkSettings & settings,\n     DeserializeBinaryBulkStatePtr & state) const\n {\n@@ -81,7 +82,7 @@ void DataTypeNullable::deserializeBinaryBulkStatePrefix(\n }\n \n \n-void DataTypeNullable::serializeBinaryBulkWithMultipleStreams(\n+void DataTypeNullable::serializeBinaryBulkWithMultipleStreamsImpl(\n     const IColumn & column,\n     size_t offset,\n     size_t limit,\n@@ -103,20 +104,28 @@ void DataTypeNullable::serializeBinaryBulkWithMultipleStreams(\n }\n \n \n-void DataTypeNullable::deserializeBinaryBulkWithMultipleStreams(\n+void DataTypeNullable::deserializeBinaryBulkWithMultipleStreamsImpl(\n     IColumn & column,\n     size_t limit,\n     DeserializeBinaryBulkSettings & settings,\n-    DeserializeBinaryBulkStatePtr & state) const\n+    DeserializeBinaryBulkStatePtr & state,\n+    SubstreamsCache * cache) const\n {\n     ColumnNullable & col = assert_cast<ColumnNullable &>(column);\n \n     settings.path.push_back(Substream::NullMap);\n-    if (auto * stream = settings.getter(settings.path))\n+    if (auto cached_column = getFromSubstreamsCache(cache, settings.path))\n+    {\n+        col.getNullMapColumnPtr() = cached_column;\n+    }\n+    else if (auto * stream = settings.getter(settings.path))\n+    {\n         DataTypeUInt8().deserializeBinaryBulk(col.getNullMapColumn(), *stream, limit, 0);\n+        addToSubstreamsCache(cache, settings.path, col.getNullMapColumnPtr());\n+    }\n \n     settings.path.back() = Substream::NullableElements;\n-    nested_data_type->deserializeBinaryBulkWithMultipleStreams(col.getNestedColumn(), limit, settings, state);\n+    nested_data_type->deserializeBinaryBulkWithMultipleStreams(col.getNestedColumnPtr(), limit, settings, state, cache);\n     settings.path.pop_back();\n }\n \n@@ -525,6 +534,23 @@ bool DataTypeNullable::equals(const IDataType & rhs) const\n     return rhs.isNullable() && nested_data_type->equals(*static_cast<const DataTypeNullable &>(rhs).nested_data_type);\n }\n \n+DataTypePtr DataTypeNullable::tryGetSubcolumnType(const String & subcolumn_name) const\n+{\n+    if (subcolumn_name == \"null\")\n+        return createOneElementTuple(std::make_shared<DataTypeUInt8>(), subcolumn_name, false);\n+\n+    return nested_data_type->tryGetSubcolumnType(subcolumn_name);\n+}\n+\n+ColumnPtr DataTypeNullable::getSubcolumn(const String & subcolumn_name, const IColumn & column) const\n+{\n+    const auto & column_nullable = assert_cast<const ColumnNullable &>(column);\n+    if (subcolumn_name == \"null\")\n+        return column_nullable.getNullMapColumnPtr()->assumeMutable();\n+\n+    return nested_data_type->getSubcolumn(subcolumn_name, column_nullable.getNestedColumn());\n+}\n+\n \n static DataTypePtr create(const ASTPtr & arguments)\n {\ndiff --git a/src/DataTypes/DataTypeNullable.h b/src/DataTypes/DataTypeNullable.h\nindex 587eecdf32e9..db641faf0afb 100644\n--- a/src/DataTypes/DataTypeNullable.h\n+++ b/src/DataTypes/DataTypeNullable.h\n@@ -18,32 +18,33 @@ class DataTypeNullable final : public IDataType\n     const char * getFamilyName() const override { return \"Nullable\"; }\n     TypeIndex getTypeId() const override { return TypeIndex::Nullable; }\n \n-    void enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const override;\n+    void enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const override;\n \n-    void serializeBinaryBulkStatePrefix(\n+    void serializeBinaryBulkStatePrefixImpl(\n             SerializeBinaryBulkSettings & settings,\n             SerializeBinaryBulkStatePtr & state) const override;\n \n-    void serializeBinaryBulkStateSuffix(\n+    void serializeBinaryBulkStateSuffixImpl(\n             SerializeBinaryBulkSettings & settings,\n             SerializeBinaryBulkStatePtr & state) const override;\n \n-    void deserializeBinaryBulkStatePrefix(\n+    void deserializeBinaryBulkStatePrefixImpl(\n             DeserializeBinaryBulkSettings & settings,\n             DeserializeBinaryBulkStatePtr & state) const override;\n \n-    void serializeBinaryBulkWithMultipleStreams(\n+    void serializeBinaryBulkWithMultipleStreamsImpl(\n             const IColumn & column,\n             size_t offset,\n             size_t limit,\n             SerializeBinaryBulkSettings & settings,\n             SerializeBinaryBulkStatePtr & state) const override;\n \n-    void deserializeBinaryBulkWithMultipleStreams(\n+    void deserializeBinaryBulkWithMultipleStreamsImpl(\n             IColumn & column,\n             size_t limit,\n             DeserializeBinaryBulkSettings & settings,\n-            DeserializeBinaryBulkStatePtr & state) const override;\n+            DeserializeBinaryBulkStatePtr & state,\n+            SubstreamsCache * cache) const override;\n \n     void serializeBinary(const Field & field, WriteBuffer & ostr) const override;\n     void deserializeBinary(Field & field, ReadBuffer & istr) const override;\n@@ -97,6 +98,8 @@ class DataTypeNullable final : public IDataType\n     size_t getSizeOfValueInMemory() const override;\n     bool onlyNull() const override;\n     bool canBeInsideLowCardinality() const override { return nested_data_type->canBeInsideLowCardinality(); }\n+    DataTypePtr tryGetSubcolumnType(const String & subcolumn_name) const override;\n+    ColumnPtr getSubcolumn(const String & subcolumn_name, const IColumn & column) const override;\n \n     const DataTypePtr & getNestedType() const { return nested_data_type; }\n \ndiff --git a/src/DataTypes/DataTypeOneElementTuple.cpp b/src/DataTypes/DataTypeOneElementTuple.cpp\nnew file mode 100644\nindex 000000000000..a41692203623\n--- /dev/null\n+++ b/src/DataTypes/DataTypeOneElementTuple.cpp\n@@ -0,0 +1,112 @@\n+#include <DataTypes/DataTypeOneElementTuple.h>\n+#include <DataTypes/DataTypeFactory.h>\n+#include <DataTypes/DataTypeCustom.h>\n+#include <IO/WriteBufferFromString.h>\n+#include <IO/Operators.h>\n+#include <Common/quoteString.h>\n+#include <Parsers/ASTNameTypePair.h>\n+#include <Columns/IColumn.h>\n+\n+\n+namespace DB\n+{\n+\n+namespace\n+{\n+\n+/** Custom substreams representation for single subcolumn.\n+  * It serializes/deserializes column as a nested type, but in that way\n+  * if it was a named tuple with one element and a given name.\n+  */\n+class DataTypeOneElementTupleStreams : public IDataTypeCustomStreams\n+{\n+private:\n+    DataTypePtr nested;\n+    String name;\n+    bool escape_delimiter;\n+\n+public:\n+    DataTypeOneElementTupleStreams(const DataTypePtr & nested_, const String & name_, bool escape_delimiter_)\n+        : nested(nested_), name(name_), escape_delimiter(escape_delimiter_) {}\n+\n+    void enumerateStreams(\n+        const IDataType::StreamCallback & callback,\n+        IDataType::SubstreamPath & path) const override\n+    {\n+        addToPath(path);\n+        nested->enumerateStreams(callback, path);\n+        path.pop_back();\n+    }\n+\n+    void serializeBinaryBulkStatePrefix(\n+        IDataType:: SerializeBinaryBulkSettings & settings,\n+        IDataType::SerializeBinaryBulkStatePtr & state) const override\n+    {\n+        addToPath(settings.path);\n+        nested->serializeBinaryBulkStatePrefix(settings, state);\n+        settings.path.pop_back();\n+    }\n+\n+    void serializeBinaryBulkStateSuffix(\n+        IDataType::SerializeBinaryBulkSettings & settings,\n+        IDataType::SerializeBinaryBulkStatePtr & state) const override\n+    {\n+        addToPath(settings.path);\n+        nested->serializeBinaryBulkStateSuffix(settings, state);\n+        settings.path.pop_back();\n+    }\n+\n+    void deserializeBinaryBulkStatePrefix(\n+        IDataType::DeserializeBinaryBulkSettings & settings,\n+        IDataType::DeserializeBinaryBulkStatePtr & state) const override\n+    {\n+        addToPath(settings.path);\n+        nested->deserializeBinaryBulkStatePrefix(settings, state);\n+        settings.path.pop_back();\n+    }\n+\n+    void serializeBinaryBulkWithMultipleStreams(\n+        const IColumn & column,\n+        size_t offset,\n+        size_t limit,\n+        IDataType::SerializeBinaryBulkSettings & settings,\n+        IDataType::SerializeBinaryBulkStatePtr & state) const override\n+    {\n+        addToPath(settings.path);\n+        nested->serializeBinaryBulkWithMultipleStreams(column, offset, limit, settings, state);\n+        settings.path.pop_back();\n+    }\n+\n+    void deserializeBinaryBulkWithMultipleStreams(\n+        ColumnPtr & column,\n+        size_t limit,\n+        IDataType::DeserializeBinaryBulkSettings & settings,\n+        IDataType::DeserializeBinaryBulkStatePtr & state,\n+        IDataType::SubstreamsCache * cache) const override\n+    {\n+        addToPath(settings.path);\n+        nested->deserializeBinaryBulkWithMultipleStreams(column, limit, settings, state, cache);\n+        settings.path.pop_back();\n+    }\n+\n+private:\n+    void addToPath(IDataType::SubstreamPath & path) const\n+    {\n+        path.push_back(IDataType::Substream::TupleElement);\n+        path.back().tuple_element_name = name;\n+        path.back().escape_tuple_delimiter = escape_delimiter;\n+    }\n+};\n+\n+}\n+\n+DataTypePtr createOneElementTuple(const DataTypePtr & type, const String & name, bool escape_delimiter)\n+{\n+    auto custom_desc = std::make_unique<DataTypeCustomDesc>(\n+        std::make_unique<DataTypeCustomFixedName>(type->getName()),nullptr,\n+        std::make_unique<DataTypeOneElementTupleStreams>(type, name, escape_delimiter));\n+\n+    return DataTypeFactory::instance().getCustom(std::move(custom_desc));\n+}\n+\n+}\ndiff --git a/src/DataTypes/DataTypeOneElementTuple.h b/src/DataTypes/DataTypeOneElementTuple.h\nnew file mode 100644\nindex 000000000000..03b0511ef4ab\n--- /dev/null\n+++ b/src/DataTypes/DataTypeOneElementTuple.h\n@@ -0,0 +1,10 @@\n+#pragma once\n+\n+#include <DataTypes/IDataType.h>\n+\n+namespace DB\n+{\n+\n+DataTypePtr createOneElementTuple(const DataTypePtr & type, const String & name, bool escape_delimiter = true);\n+\n+}\ndiff --git a/src/DataTypes/DataTypeTuple.cpp b/src/DataTypes/DataTypeTuple.cpp\nindex 02fc49f7e9aa..c62aa1c1187a 100644\n--- a/src/DataTypes/DataTypeTuple.cpp\n+++ b/src/DataTypes/DataTypeTuple.cpp\n@@ -5,6 +5,7 @@\n #include <DataTypes/DataTypeTuple.h>\n #include <DataTypes/DataTypeArray.h>\n #include <DataTypes/DataTypeFactory.h>\n+#include <DataTypes/DataTypeOneElementTuple.h>\n #include <Parsers/IAST.h>\n #include <Parsers/ASTNameTypePair.h>\n #include <Common/typeid_cast.h>\n@@ -30,6 +31,7 @@ namespace ErrorCodes\n     extern const int EMPTY_DATA_PASSED;\n     extern const int LOGICAL_ERROR;\n     extern const int NOT_FOUND_COLUMN_IN_BLOCK;\n+    extern const int ILLEGAL_COLUMN;\n     extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;\n     extern const int SIZES_OF_COLUMNS_IN_TUPLE_DOESNT_MATCH;\n }\n@@ -357,7 +359,7 @@ void DataTypeTuple::deserializeTextCSV(IColumn & column, ReadBuffer & istr, cons\n     });\n }\n \n-void DataTypeTuple::enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const\n+void DataTypeTuple::enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const\n {\n     path.push_back(Substream::TupleElement);\n     for (const auto i : ext::range(0, ext::size(elems)))\n@@ -412,7 +414,7 @@ static DeserializeBinaryBulkStateTuple * checkAndGetTupleDeserializeState(IDataT\n     return tuple_state;\n }\n \n-void DataTypeTuple::serializeBinaryBulkStatePrefix(\n+void DataTypeTuple::serializeBinaryBulkStatePrefixImpl(\n     SerializeBinaryBulkSettings & settings,\n     SerializeBinaryBulkStatePtr & state) const\n {\n@@ -430,7 +432,7 @@ void DataTypeTuple::serializeBinaryBulkStatePrefix(\n     state = std::move(tuple_state);\n }\n \n-void DataTypeTuple::serializeBinaryBulkStateSuffix(\n+void DataTypeTuple::serializeBinaryBulkStateSuffixImpl(\n     SerializeBinaryBulkSettings & settings,\n     SerializeBinaryBulkStatePtr & state) const\n {\n@@ -445,7 +447,7 @@ void DataTypeTuple::serializeBinaryBulkStateSuffix(\n     settings.path.pop_back();\n }\n \n-void DataTypeTuple::deserializeBinaryBulkStatePrefix(\n+void DataTypeTuple::deserializeBinaryBulkStatePrefixImpl(\n         DeserializeBinaryBulkSettings & settings,\n         DeserializeBinaryBulkStatePtr & state) const\n {\n@@ -463,7 +465,7 @@ void DataTypeTuple::deserializeBinaryBulkStatePrefix(\n     state = std::move(tuple_state);\n }\n \n-void DataTypeTuple::serializeBinaryBulkWithMultipleStreams(\n+void DataTypeTuple::serializeBinaryBulkWithMultipleStreamsImpl(\n     const IColumn & column,\n     size_t offset,\n     size_t limit,\n@@ -482,21 +484,22 @@ void DataTypeTuple::serializeBinaryBulkWithMultipleStreams(\n     settings.path.pop_back();\n }\n \n-void DataTypeTuple::deserializeBinaryBulkWithMultipleStreams(\n+void DataTypeTuple::deserializeBinaryBulkWithMultipleStreamsImpl(\n     IColumn & column,\n     size_t limit,\n     DeserializeBinaryBulkSettings & settings,\n-    DeserializeBinaryBulkStatePtr & state) const\n+    DeserializeBinaryBulkStatePtr & state,\n+    SubstreamsCache * cache) const\n {\n     auto * tuple_state = checkAndGetTupleDeserializeState(state);\n+    auto & column_tuple = assert_cast<ColumnTuple &>(column);\n \n     settings.path.push_back(Substream::TupleElement);\n     settings.avg_value_size_hint = 0;\n     for (const auto i : ext::range(0, ext::size(elems)))\n     {\n         settings.path.back().tuple_element_name = names[i];\n-        auto & element_col = extractElementColumn(column, i);\n-        elems[i]->deserializeBinaryBulkWithMultipleStreams(element_col, limit, settings, tuple_state->states[i]);\n+        elems[i]->deserializeBinaryBulkWithMultipleStreams(column_tuple.getColumnPtr(i), limit, settings, tuple_state->states[i], cache);\n     }\n     settings.path.pop_back();\n }\n@@ -611,6 +614,47 @@ size_t DataTypeTuple::getSizeOfValueInMemory() const\n     return res;\n }\n \n+DataTypePtr DataTypeTuple::tryGetSubcolumnType(const String & subcolumn_name) const\n+{\n+    for (size_t i = 0; i < names.size(); ++i)\n+    {\n+        if (startsWith(subcolumn_name, names[i]))\n+        {\n+            size_t name_length = names[i].size();\n+            DataTypePtr subcolumn_type;\n+            if (subcolumn_name.size() == name_length)\n+                subcolumn_type = elems[i];\n+            else if (subcolumn_name[name_length] == '.')\n+                subcolumn_type = elems[i]->tryGetSubcolumnType(subcolumn_name.substr(name_length + 1));\n+\n+            if (subcolumn_type)\n+                return createOneElementTuple(std::move(subcolumn_type), names[i]);\n+        }\n+    }\n+\n+    return nullptr;\n+}\n+\n+ColumnPtr DataTypeTuple::getSubcolumn(const String & subcolumn_name, const IColumn & column) const\n+{\n+    for (size_t i = 0; i < names.size(); ++i)\n+    {\n+        if (startsWith(subcolumn_name, names[i]))\n+        {\n+            size_t name_length = names[i].size();\n+            const auto & subcolumn = extractElementColumn(column, i);\n+\n+            if (subcolumn_name.size() == name_length)\n+                return subcolumn.assumeMutable();\n+\n+            if (subcolumn_name[name_length] == '.')\n+                return elems[i]->getSubcolumn(subcolumn_name.substr(name_length + 1), subcolumn);\n+        }\n+    }\n+\n+    throw Exception(ErrorCodes::ILLEGAL_COLUMN, \"There is no subcolumn {} in type {}\", subcolumn_name, getName());\n+}\n+\n \n static DataTypePtr create(const ASTPtr & arguments)\n {\n@@ -648,13 +692,4 @@ void registerDataTypeTuple(DataTypeFactory & factory)\n     factory.registerDataType(\"Tuple\", create);\n }\n \n-void registerDataTypeNested(DataTypeFactory & factory)\n-{\n-    /// Nested(...) data type is just a sugar for Array(Tuple(...))\n-    factory.registerDataType(\"Nested\", [&factory](const ASTPtr & arguments)\n-    {\n-        return std::make_shared<DataTypeArray>(factory.get(\"Tuple\", arguments));\n-    });\n-}\n-\n }\ndiff --git a/src/DataTypes/DataTypeTuple.h b/src/DataTypes/DataTypeTuple.h\nindex 7e4e68651f1b..0b28ebe5a635 100644\n--- a/src/DataTypes/DataTypeTuple.h\n+++ b/src/DataTypes/DataTypeTuple.h\n@@ -53,32 +53,33 @@ class DataTypeTuple final : public DataTypeWithSimpleSerialization\n \n     /** Each sub-column in a tuple is serialized in separate stream.\n       */\n-    void enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const override;\n+    void enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const override;\n \n-    void serializeBinaryBulkStatePrefix(\n+    void serializeBinaryBulkStatePrefixImpl(\n             SerializeBinaryBulkSettings & settings,\n             SerializeBinaryBulkStatePtr & state) const override;\n \n-    void serializeBinaryBulkStateSuffix(\n+    void serializeBinaryBulkStateSuffixImpl(\n             SerializeBinaryBulkSettings & settings,\n             SerializeBinaryBulkStatePtr & state) const override;\n \n-    void deserializeBinaryBulkStatePrefix(\n+    void deserializeBinaryBulkStatePrefixImpl(\n             DeserializeBinaryBulkSettings & settings,\n             DeserializeBinaryBulkStatePtr & state) const override;\n \n-    void serializeBinaryBulkWithMultipleStreams(\n+    void serializeBinaryBulkWithMultipleStreamsImpl(\n             const IColumn & column,\n             size_t offset,\n             size_t limit,\n             SerializeBinaryBulkSettings & settings,\n             SerializeBinaryBulkStatePtr & state) const override;\n \n-    void deserializeBinaryBulkWithMultipleStreams(\n+    void deserializeBinaryBulkWithMultipleStreamsImpl(\n             IColumn & column,\n             size_t limit,\n             DeserializeBinaryBulkSettings & settings,\n-            DeserializeBinaryBulkStatePtr & state) const override;\n+            DeserializeBinaryBulkStatePtr & state,\n+            SubstreamsCache * cache) const override;\n \n     void serializeProtobuf(const IColumn & column, size_t row_num, ProtobufWriter & protobuf, size_t & value_index) const override;\n     void deserializeProtobuf(IColumn & column, ProtobufReader & protobuf, bool allow_add_row, bool & row_added) const override;\n@@ -98,6 +99,9 @@ class DataTypeTuple final : public DataTypeWithSimpleSerialization\n     size_t getMaximumSizeOfValueInMemory() const override;\n     size_t getSizeOfValueInMemory() const override;\n \n+    DataTypePtr tryGetSubcolumnType(const String & subcolumn_name) const override;\n+    ColumnPtr getSubcolumn(const String & subcolumn_name, const IColumn & column) const override;\n+\n     const DataTypes & getElements() const { return elems; }\n     const Strings & getElementNames() const { return names; }\n \ndiff --git a/src/DataTypes/IDataType.cpp b/src/DataTypes/IDataType.cpp\nindex d1c9f1bde778..5582a8698e08 100644\n--- a/src/DataTypes/IDataType.cpp\n+++ b/src/DataTypes/IDataType.cpp\n@@ -3,8 +3,10 @@\n \n #include <Common/Exception.h>\n #include <Common/escapeForFileName.h>\n+#include <Common/SipHash.h>\n \n #include <IO/WriteHelpers.h>\n+#include <IO/Operators.h>\n \n #include <DataTypes/IDataType.h>\n #include <DataTypes/DataTypeCustom.h>\n@@ -19,9 +21,48 @@ namespace ErrorCodes\n     extern const int MULTIPLE_STREAMS_REQUIRED;\n     extern const int LOGICAL_ERROR;\n     extern const int DATA_TYPE_CANNOT_BE_PROMOTED;\n+    extern const int ILLEGAL_COLUMN;\n }\n \n-IDataType::IDataType() : custom_name(nullptr), custom_text_serialization(nullptr)\n+String IDataType::Substream::toString() const\n+{\n+    switch (type)\n+    {\n+        case ArrayElements:\n+            return \"ArrayElements\";\n+        case ArraySizes:\n+            return \"ArraySizes\";\n+        case NullableElements:\n+            return \"NullableElements\";\n+        case NullMap:\n+            return \"NullMap\";\n+        case TupleElement:\n+            return \"TupleElement(\" + tuple_element_name + \", \"\n+                + std::to_string(escape_tuple_delimiter) + \")\";\n+        case DictionaryKeys:\n+            return \"DictionaryKeys\";\n+        case DictionaryIndexes:\n+            return \"DictionaryIndexes\";\n+    }\n+\n+    __builtin_unreachable();\n+}\n+\n+String IDataType::SubstreamPath::toString() const\n+{\n+    WriteBufferFromOwnString wb;\n+    wb << \"{\";\n+    for (size_t i = 0; i < size(); ++i)\n+    {\n+        if (i != 0)\n+            wb << \", \";\n+        wb << at(i).toString();\n+    }\n+    wb << \"}\";\n+    return wb.str();\n+}\n+\n+IDataType::IDataType() : custom_name(nullptr), custom_text_serialization(nullptr), custom_streams(nullptr)\n {\n }\n \n@@ -93,42 +134,89 @@ size_t IDataType::getSizeOfValueInMemory() const\n     throw Exception(\"Value of type \" + getName() + \" in memory is not of fixed size.\", ErrorCodes::LOGICAL_ERROR);\n }\n \n+DataTypePtr IDataType::getSubcolumnType(const String & subcolumn_name) const\n+{\n+    auto subcolumn_type = tryGetSubcolumnType(subcolumn_name);\n+    if (subcolumn_type)\n+        return subcolumn_type;\n+\n+    throw Exception(ErrorCodes::ILLEGAL_COLUMN, \"There is no subcolumn {} in type {}\", subcolumn_name, getName());\n+}\n+\n+ColumnPtr IDataType::getSubcolumn(const String & subcolumn_name, const IColumn &) const\n+{\n+    throw Exception(ErrorCodes::ILLEGAL_COLUMN, \"There is no subcolumn {} in type {}\", subcolumn_name, getName());\n+}\n \n-String IDataType::getFileNameForStream(const String & column_name, const IDataType::SubstreamPath & path)\n+Names IDataType::getSubcolumnNames() const\n {\n-    /// Sizes of arrays (elements of Nested type) are shared (all reside in single file).\n-    String nested_table_name = Nested::extractTableName(column_name);\n+    NameSet res;\n+    enumerateStreams([&res, this](const SubstreamPath & substream_path, const IDataType & /* substream_type */)\n+    {\n+        SubstreamPath new_path;\n+        /// Iterate over path to try to get intermediate subcolumns for complex nested types.\n+        for (const auto & elem : substream_path)\n+        {\n+            new_path.push_back(elem);\n+            auto subcolumn_name = getSubcolumnNameForStream(new_path);\n+            if (!subcolumn_name.empty() && tryGetSubcolumnType(subcolumn_name))\n+                res.insert(subcolumn_name);\n+        }\n+    });\n \n-    bool is_sizes_of_nested_type =\n-        path.size() == 1    /// Nested structure may have arrays as nested elements (so effectively we have multidimensional arrays).\n-                            /// Sizes of arrays are shared only at first level.\n-        && path[0].type == IDataType::Substream::ArraySizes\n-        && nested_table_name != column_name;\n+    return Names(std::make_move_iterator(res.begin()), std::make_move_iterator(res.end()));\n+}\n \n+static String getNameForSubstreamPath(\n+    String stream_name,\n+    const IDataType::SubstreamPath & path,\n+    bool escape_tuple_delimiter)\n+{\n     size_t array_level = 0;\n-    String stream_name = escapeForFileName(is_sizes_of_nested_type ? nested_table_name : column_name);\n-    for (const Substream & elem : path)\n+    for (const auto & elem : path)\n     {\n-        if (elem.type == Substream::NullMap)\n+        if (elem.type == IDataType::Substream::NullMap)\n             stream_name += \".null\";\n-        else if (elem.type == Substream::ArraySizes)\n+        else if (elem.type == IDataType::Substream::ArraySizes)\n             stream_name += \".size\" + toString(array_level);\n-        else if (elem.type == Substream::ArrayElements)\n+        else if (elem.type == IDataType::Substream::ArrayElements)\n             ++array_level;\n-        else if (elem.type == Substream::TupleElement)\n+        else if (elem.type == IDataType::Substream::DictionaryKeys)\n+            stream_name += \".dict\";\n+        else if (elem.type == IDataType::Substream::TupleElement)\n         {\n-            /// For compatibility reasons, we use %2E instead of dot.\n+            /// For compatibility reasons, we use %2E (escaped dot) instead of dot.\n             /// Because nested data may be represented not by Array of Tuple,\n             ///  but by separate Array columns with names in a form of a.b,\n             ///  and name is encoded as a whole.\n-            stream_name += \"%2E\" + escapeForFileName(elem.tuple_element_name);\n+            stream_name += (escape_tuple_delimiter && elem.escape_tuple_delimiter ?\n+                escapeForFileName(\".\") : \".\") + escapeForFileName(elem.tuple_element_name);\n         }\n-        else if (elem.type == Substream::DictionaryKeys)\n-            stream_name += \".dict\";\n     }\n+\n     return stream_name;\n }\n \n+String IDataType::getFileNameForStream(const NameAndTypePair & column, const SubstreamPath & path)\n+{\n+    auto name_in_storage = column.getNameInStorage();\n+    auto nested_storage_name = Nested::extractTableName(name_in_storage);\n+\n+    if (name_in_storage != nested_storage_name && (path.size() == 1 && path[0].type == IDataType::Substream::ArraySizes))\n+        name_in_storage = nested_storage_name;\n+\n+    auto stream_name = escapeForFileName(name_in_storage);\n+    return getNameForSubstreamPath(std::move(stream_name), path, true);\n+}\n+\n+String IDataType::getSubcolumnNameForStream(const SubstreamPath & path)\n+{\n+    auto subcolumn_name = getNameForSubstreamPath(\"\", path, false);\n+    if (!subcolumn_name.empty())\n+        subcolumn_name = subcolumn_name.substr(1); // It starts with a dot.\n+\n+    return subcolumn_name;\n+}\n \n bool IDataType::isSpecialCompressionAllowed(const SubstreamPath & path)\n {\n@@ -147,6 +235,102 @@ void IDataType::insertDefaultInto(IColumn & column) const\n     column.insertDefault();\n }\n \n+void IDataType::enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const\n+{\n+    if (custom_streams)\n+        custom_streams->enumerateStreams(callback, path);\n+    else\n+        enumerateStreamsImpl(callback, path);\n+}\n+\n+void IDataType::serializeBinaryBulkStatePrefix(\n+    SerializeBinaryBulkSettings & settings,\n+    SerializeBinaryBulkStatePtr & state) const\n+{\n+    if (custom_streams)\n+        custom_streams->serializeBinaryBulkStatePrefix(settings, state);\n+    else\n+        serializeBinaryBulkStatePrefixImpl(settings, state);\n+}\n+\n+void IDataType::serializeBinaryBulkStateSuffix(\n+    SerializeBinaryBulkSettings & settings,\n+    SerializeBinaryBulkStatePtr & state) const\n+{\n+    if (custom_streams)\n+        custom_streams->serializeBinaryBulkStateSuffix(settings, state);\n+    else\n+        serializeBinaryBulkStateSuffixImpl(settings, state);\n+}\n+\n+void IDataType::deserializeBinaryBulkStatePrefix(\n+    DeserializeBinaryBulkSettings & settings,\n+    DeserializeBinaryBulkStatePtr & state) const\n+{\n+    if (custom_streams)\n+        custom_streams->deserializeBinaryBulkStatePrefix(settings, state);\n+    else\n+        deserializeBinaryBulkStatePrefixImpl(settings, state);\n+}\n+\n+void IDataType::serializeBinaryBulkWithMultipleStreams(\n+    const IColumn & column,\n+    size_t offset,\n+    size_t limit,\n+    SerializeBinaryBulkSettings & settings,\n+    SerializeBinaryBulkStatePtr & state) const\n+{\n+    if (custom_streams)\n+        custom_streams->serializeBinaryBulkWithMultipleStreams(column, offset, limit, settings, state);\n+    else\n+        serializeBinaryBulkWithMultipleStreamsImpl(column, offset, limit, settings, state);\n+}\n+\n+void IDataType::deserializeBinaryBulkWithMultipleStreamsImpl(\n+    IColumn & column,\n+    size_t limit,\n+    DeserializeBinaryBulkSettings & settings,\n+    DeserializeBinaryBulkStatePtr & /* state */,\n+    SubstreamsCache * /* cache */) const\n+{\n+    if (ReadBuffer * stream = settings.getter(settings.path))\n+        deserializeBinaryBulk(column, *stream, limit, settings.avg_value_size_hint);\n+}\n+\n+\n+void IDataType::deserializeBinaryBulkWithMultipleStreams(\n+    ColumnPtr & column,\n+    size_t limit,\n+    DeserializeBinaryBulkSettings & settings,\n+    DeserializeBinaryBulkStatePtr & state,\n+    SubstreamsCache * cache) const\n+{\n+    if (custom_streams)\n+    {\n+        custom_streams->deserializeBinaryBulkWithMultipleStreams(column, limit, settings, state, cache);\n+        return;\n+    }\n+\n+    /// Do not cache complex type, because they can be constructed\n+    /// from their subcolumns, which are in cache.\n+    if (!haveSubtypes())\n+    {\n+        auto cached_column = getFromSubstreamsCache(cache, settings.path);\n+        if (cached_column)\n+        {\n+            column = cached_column;\n+            return;\n+        }\n+    }\n+\n+    auto mutable_column = column->assumeMutable();\n+    deserializeBinaryBulkWithMultipleStreamsImpl(*mutable_column, limit, settings, state, cache);\n+    column = std::move(mutable_column);\n+\n+    if (!haveSubtypes())\n+        addToSubstreamsCache(cache, settings.path, column);\n+}\n+\n void IDataType::serializeAsTextEscaped(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings & settings) const\n {\n     if (custom_text_serialization)\n@@ -243,6 +427,27 @@ void IDataType::setCustomization(DataTypeCustomDescPtr custom_desc_) const\n \n     if (custom_desc_->text_serialization)\n         custom_text_serialization = std::move(custom_desc_->text_serialization);\n+\n+    if (custom_desc_->streams)\n+        custom_streams = std::move(custom_desc_->streams);\n+}\n+\n+void IDataType::addToSubstreamsCache(SubstreamsCache * cache, const SubstreamPath & path, ColumnPtr column)\n+{\n+    if (cache && !path.empty())\n+        cache->emplace(getSubcolumnNameForStream(path), column);\n+}\n+\n+ColumnPtr IDataType::getFromSubstreamsCache(SubstreamsCache * cache, const SubstreamPath & path)\n+{\n+    if (!cache || path.empty())\n+        return nullptr;\n+\n+    auto it = cache->find(getSubcolumnNameForStream(path));\n+    if (it == cache->end())\n+        return nullptr;\n+\n+    return it->second;\n }\n \n }\ndiff --git a/src/DataTypes/IDataType.h b/src/DataTypes/IDataType.h\nindex b51722ed96d5..b67c5ee18462 100644\n--- a/src/DataTypes/IDataType.h\n+++ b/src/DataTypes/IDataType.h\n@@ -3,7 +3,9 @@\n #include <memory>\n #include <Common/COW.h>\n #include <boost/noncopyable.hpp>\n-#include <DataTypes/DataTypeCustom.h>\n+#include <Core/Names.h>\n+#include <Core/Types.h>\n+#include <DataTypes/DataTypeCustom_fwd.h>\n \n \n namespace DB\n@@ -27,6 +29,8 @@ using DataTypes = std::vector<DataTypePtr>;\n class ProtobufReader;\n class ProtobufWriter;\n \n+struct NameAndTypePair;\n+\n \n /** Properties of data type.\n   * Contains methods for serialization/deserialization.\n@@ -91,30 +95,42 @@ class IDataType : private boost::noncopyable\n \n             TupleElement,\n \n-            MapElement,\n-\n             DictionaryKeys,\n             DictionaryIndexes,\n         };\n         Type type;\n \n-        /// Index of tuple element, starting at 1.\n+        /// Index of tuple element, starting at 1 or name.\n         String tuple_element_name;\n \n+        /// Do we need to escape a dot in filenames for tuple elements.\n+        bool escape_tuple_delimiter = true;\n+\n         Substream(Type type_) : type(type_) {}\n+\n+        String toString() const;\n+    };\n+\n+    struct SubstreamPath : public std::vector<Substream>\n+    {\n+        String toString() const;\n     };\n \n-    using SubstreamPath = std::vector<Substream>;\n+    /// Cache for common substreams of one type, but possible different its subcolumns.\n+    /// E.g. sizes of arrays of Nested data type.\n+    using SubstreamsCache = std::unordered_map<String, ColumnPtr>;\n \n     using StreamCallback = std::function<void(const SubstreamPath &, const IDataType &)>;\n \n-    virtual void enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const\n-    {\n-        callback(path, *this);\n-    }\n+    void enumerateStreams(const StreamCallback & callback, SubstreamPath & path) const;\n     void enumerateStreams(const StreamCallback & callback, SubstreamPath && path) const { enumerateStreams(callback, path); }\n     void enumerateStreams(const StreamCallback & callback) const { enumerateStreams(callback, {}); }\n \n+    virtual DataTypePtr tryGetSubcolumnType(const String & /* subcolumn_name */) const { return nullptr; }\n+    DataTypePtr getSubcolumnType(const String & subcolumn_name) const;\n+    virtual ColumnPtr getSubcolumn(const String & subcolumn_name, const IColumn & column) const;\n+    Names getSubcolumnNames() const;\n+\n     using OutputStreamGetter = std::function<WriteBuffer*(const SubstreamPath &)>;\n     using InputStreamGetter = std::function<ReadBuffer*(const SubstreamPath &)>;\n \n@@ -155,19 +171,19 @@ class IDataType : private boost::noncopyable\n     };\n \n     /// Call before serializeBinaryBulkWithMultipleStreams chain to write something before first mark.\n-    virtual void serializeBinaryBulkStatePrefix(\n-            SerializeBinaryBulkSettings & /*settings*/,\n-            SerializeBinaryBulkStatePtr & /*state*/) const {}\n+    void serializeBinaryBulkStatePrefix(\n+        SerializeBinaryBulkSettings & settings,\n+        SerializeBinaryBulkStatePtr & state) const;\n \n     /// Call after serializeBinaryBulkWithMultipleStreams chain to finish serialization.\n-    virtual void serializeBinaryBulkStateSuffix(\n-        SerializeBinaryBulkSettings & /*settings*/,\n-        SerializeBinaryBulkStatePtr & /*state*/) const {}\n+    void serializeBinaryBulkStateSuffix(\n+        SerializeBinaryBulkSettings & settings,\n+        SerializeBinaryBulkStatePtr & state) const;\n \n     /// Call before before deserializeBinaryBulkWithMultipleStreams chain to get DeserializeBinaryBulkStatePtr.\n-    virtual void deserializeBinaryBulkStatePrefix(\n-        DeserializeBinaryBulkSettings & /*settings*/,\n-        DeserializeBinaryBulkStatePtr & /*state*/) const {}\n+    void deserializeBinaryBulkStatePrefix(\n+        DeserializeBinaryBulkSettings & settings,\n+        DeserializeBinaryBulkStatePtr & state) const;\n \n     /** 'offset' and 'limit' are used to specify range.\n       * limit = 0 - means no limit.\n@@ -175,27 +191,20 @@ class IDataType : private boost::noncopyable\n       * offset + limit could be greater than size of column\n       *  - in that case, column is serialized till the end.\n       */\n-    virtual void serializeBinaryBulkWithMultipleStreams(\n+    void serializeBinaryBulkWithMultipleStreams(\n         const IColumn & column,\n         size_t offset,\n         size_t limit,\n         SerializeBinaryBulkSettings & settings,\n-        SerializeBinaryBulkStatePtr & /*state*/) const\n-    {\n-        if (WriteBuffer * stream = settings.getter(settings.path))\n-            serializeBinaryBulk(column, *stream, offset, limit);\n-    }\n+        SerializeBinaryBulkStatePtr & state) const;\n \n     /// Read no more than limit values and append them into column.\n-    virtual void deserializeBinaryBulkWithMultipleStreams(\n-        IColumn & column,\n+    void deserializeBinaryBulkWithMultipleStreams(\n+        ColumnPtr & column,\n         size_t limit,\n         DeserializeBinaryBulkSettings & settings,\n-        DeserializeBinaryBulkStatePtr & /*state*/) const\n-    {\n-        if (ReadBuffer * stream = settings.getter(settings.path))\n-            deserializeBinaryBulk(column, *stream, limit, settings.avg_value_size_hint);\n-    }\n+        DeserializeBinaryBulkStatePtr & state,\n+        SubstreamsCache * cache = nullptr) const;\n \n     /** Override these methods for data types that require just single stream (most of data types).\n       */\n@@ -268,6 +277,41 @@ class IDataType : private boost::noncopyable\n protected:\n     virtual String doGetName() const;\n \n+    virtual void enumerateStreamsImpl(const StreamCallback & callback, SubstreamPath & path) const\n+    {\n+        callback(path, *this);\n+    }\n+\n+    virtual void serializeBinaryBulkStatePrefixImpl(\n+        SerializeBinaryBulkSettings & /*settings*/,\n+        SerializeBinaryBulkStatePtr & /*state*/) const {}\n+\n+    virtual void serializeBinaryBulkStateSuffixImpl(\n+        SerializeBinaryBulkSettings & /*settings*/,\n+        SerializeBinaryBulkStatePtr & /*state*/) const {}\n+\n+    virtual void deserializeBinaryBulkStatePrefixImpl(\n+        DeserializeBinaryBulkSettings & /*settings*/,\n+        DeserializeBinaryBulkStatePtr & /*state*/) const {}\n+\n+    virtual void serializeBinaryBulkWithMultipleStreamsImpl(\n+        const IColumn & column,\n+        size_t offset,\n+        size_t limit,\n+        SerializeBinaryBulkSettings & settings,\n+        SerializeBinaryBulkStatePtr & /*state*/) const\n+    {\n+        if (WriteBuffer * stream = settings.getter(settings.path))\n+            serializeBinaryBulk(column, *stream, offset, limit);\n+    }\n+\n+    virtual void deserializeBinaryBulkWithMultipleStreamsImpl(\n+        IColumn & column,\n+        size_t limit,\n+        DeserializeBinaryBulkSettings & settings,\n+        DeserializeBinaryBulkStatePtr & state,\n+        SubstreamsCache * cache) const;\n+\n     /// Default implementations of text serialization in case of 'custom_text_serialization' is not set.\n \n     virtual void serializeTextEscaped(const IColumn & column, size_t row_num, WriteBuffer & ostr, const FormatSettings &) const = 0;\n@@ -286,6 +330,9 @@ class IDataType : private boost::noncopyable\n     }\n \n public:\n+    static void addToSubstreamsCache(SubstreamsCache * cache, const SubstreamPath & path, ColumnPtr column);\n+    static ColumnPtr getFromSubstreamsCache(SubstreamsCache * cache, const SubstreamPath & path);\n+\n     /** Create empty column for corresponding type.\n       */\n     virtual MutableColumnPtr createColumn() const = 0;\n@@ -443,7 +490,8 @@ class IDataType : private boost::noncopyable\n     /// Updates avg_value_size_hint for newly read column. Uses to optimize deserialization. Zero expected for first column.\n     static void updateAvgValueSizeHint(const IColumn & column, double & avg_value_size_hint);\n \n-    static String getFileNameForStream(const String & column_name, const SubstreamPath & path);\n+    static String getFileNameForStream(const NameAndTypePair & column, const SubstreamPath & path);\n+    static String getSubcolumnNameForStream(const SubstreamPath & path);\n \n     /// Substream path supports special compression methods like codec Delta.\n     /// For all other substreams (like ArraySizes, NullMasks, etc.) we use only\n@@ -458,9 +506,11 @@ class IDataType : private boost::noncopyable\n     /// This is mutable to allow setting custom name and serialization on `const IDataType` post construction.\n     mutable DataTypeCustomNamePtr custom_name;\n     mutable DataTypeCustomTextSerializationPtr custom_text_serialization;\n+    mutable DataTypeCustomStreamsPtr custom_streams;\n \n public:\n     const IDataTypeCustomName * getCustomName() const { return custom_name.get(); }\n+    const IDataTypeCustomStreams * getCustomStreams() const { return custom_streams.get(); }\n };\n \n \ndiff --git a/src/DataTypes/NestedUtils.cpp b/src/DataTypes/NestedUtils.cpp\nindex 0537fa5cdc19..6c13eea0a1b7 100644\n--- a/src/DataTypes/NestedUtils.cpp\n+++ b/src/DataTypes/NestedUtils.cpp\n@@ -7,6 +7,7 @@\n #include <DataTypes/DataTypeArray.h>\n #include <DataTypes/DataTypeTuple.h>\n #include <DataTypes/NestedUtils.h>\n+#include <DataTypes/DataTypeNested.h>\n \n #include <Columns/ColumnArray.h>\n #include <Columns/ColumnTuple.h>\n@@ -84,7 +85,8 @@ Block flatten(const Block & block)\n \n     for (const auto & elem : block)\n     {\n-        if (const DataTypeArray * type_arr = typeid_cast<const DataTypeArray *>(elem.type.get()))\n+        const DataTypeArray * type_arr = typeid_cast<const DataTypeArray *>(elem.type.get());\n+        if (type_arr)\n         {\n             const DataTypeTuple * type_tuple = typeid_cast<const DataTypeTuple *>(type_arr->getNestedType().get());\n             if (type_tuple && type_tuple->haveExplicitNames())\n@@ -128,32 +130,67 @@ Block flatten(const Block & block)\n     return res;\n }\n \n-\n-NamesAndTypesList collect(const NamesAndTypesList & names_and_types)\n+namespace\n {\n-    NamesAndTypesList res;\n \n-    std::map<std::string, NamesAndTypesList> nested;\n+using NameToDataType = std::map<String, DataTypePtr>;\n+\n+NameToDataType getSubcolumnsOfNested(const NamesAndTypesList & names_and_types)\n+{\n+    std::unordered_map<String, NamesAndTypesList> nested;\n     for (const auto & name_type : names_and_types)\n     {\n-        bool collected = false;\n-        if (const DataTypeArray * type_arr = typeid_cast<const DataTypeArray *>(name_type.type.get()))\n+        const DataTypeArray * type_arr = typeid_cast<const DataTypeArray *>(name_type.type.get());\n+\n+        /// Ignore true Nested type, but try to unite flatten arrays to Nested type.\n+        if (!isNested(name_type.type) && type_arr)\n         {\n             auto split = splitName(name_type.name);\n             if (!split.second.empty())\n-            {\n                 nested[split.first].emplace_back(split.second, type_arr->getNestedType());\n-                collected = true;\n-            }\n         }\n+    }\n+\n+    std::map<String, DataTypePtr> nested_types;\n+\n+    for (const auto & [name, elems] : nested)\n+        nested_types.emplace(name, createNested(elems.getTypes(), elems.getNames()));\n+\n+    return nested_types;\n+}\n+\n+}\n \n-        if (!collected)\n+NamesAndTypesList collect(const NamesAndTypesList & names_and_types)\n+{\n+    NamesAndTypesList res;\n+    auto nested_types = getSubcolumnsOfNested(names_and_types);\n+\n+    for (const auto & name_type : names_and_types)\n+        if (!nested_types.count(splitName(name_type.name).first))\n             res.push_back(name_type);\n-    }\n \n-    for (const auto & name_elems : nested)\n-        res.emplace_back(name_elems.first, std::make_shared<DataTypeArray>(\n-            std::make_shared<DataTypeTuple>(name_elems.second.getTypes(), name_elems.second.getNames())));\n+    for (const auto & name_type : nested_types)\n+        res.emplace_back(name_type.first, name_type.second);\n+\n+    return res;\n+}\n+\n+NamesAndTypesList convertToSubcolumns(const NamesAndTypesList & names_and_types)\n+{\n+    auto nested_types = getSubcolumnsOfNested(names_and_types);\n+    auto res = names_and_types;\n+\n+    for (auto & name_type : res)\n+    {\n+        auto split = splitName(name_type.name);\n+        if (name_type.isSubcolumn() || split.second.empty())\n+            continue;\n+\n+        auto it = nested_types.find(split.first);\n+        if (it != nested_types.end())\n+            name_type = NameAndTypePair{split.first, split.second, it->second, it->second->getSubcolumnType(split.second)};\n+    }\n \n     return res;\n }\ndiff --git a/src/DataTypes/NestedUtils.h b/src/DataTypes/NestedUtils.h\nindex 3039fd7f118a..b8428b96d3e9 100644\n--- a/src/DataTypes/NestedUtils.h\n+++ b/src/DataTypes/NestedUtils.h\n@@ -23,6 +23,9 @@ namespace Nested\n     /// Collect Array columns in a form of `column_name.element_name` to single Array(Tuple(...)) column.\n     NamesAndTypesList collect(const NamesAndTypesList & names_and_types);\n \n+    /// Convert old-style nested (single arrays with same prefix, `n.a`, `n.b`...) to subcolumns of data type Nested.\n+    NamesAndTypesList convertToSubcolumns(const NamesAndTypesList & names_and_types);\n+\n     /// Check that sizes of arrays - elements of nested data structures - are equal.\n     void validateArraySizes(const Block & block);\n }\ndiff --git a/src/DataTypes/ya.make b/src/DataTypes/ya.make\nindex db6793db6ca9..356424af8ddc 100644\n--- a/src/DataTypes/ya.make\n+++ b/src/DataTypes/ya.make\n@@ -28,9 +28,11 @@ SRCS(\n     DataTypeLowCardinality.cpp\n     DataTypeLowCardinalityHelpers.cpp\n     DataTypeMap.cpp\n+    DataTypeNested.cpp\n     DataTypeNothing.cpp\n     DataTypeNullable.cpp\n     DataTypeNumberBase.cpp\n+    DataTypeOneElementTuple.cpp\n     DataTypeString.cpp\n     DataTypeTuple.cpp\n     DataTypeUUID.cpp\ndiff --git a/src/Interpreters/InterpreterCreateQuery.cpp b/src/Interpreters/InterpreterCreateQuery.cpp\nindex 9b087b3d2e5a..e9a11b9eb0df 100644\n--- a/src/Interpreters/InterpreterCreateQuery.cpp\n+++ b/src/Interpreters/InterpreterCreateQuery.cpp\n@@ -465,7 +465,8 @@ ColumnsDescription InterpreterCreateQuery::getColumnsDescription(\n         res.add(std::move(column));\n     }\n \n-    res.flattenNested();\n+    if (context.getSettingsRef().flatten_nested)\n+        res.flattenNested();\n \n     if (res.getAllPhysical().empty())\n         throw Exception{\"Cannot CREATE table without physical columns\", ErrorCodes::EMPTY_LIST_OF_COLUMNS_PASSED};\ndiff --git a/src/Interpreters/TreeRewriter.cpp b/src/Interpreters/TreeRewriter.cpp\nindex 554fedeed64e..321bbf988198 100644\n--- a/src/Interpreters/TreeRewriter.cpp\n+++ b/src/Interpreters/TreeRewriter.cpp\n@@ -526,7 +526,12 @@ void TreeRewriterResult::collectSourceColumns(bool add_special)\n     {\n         const ColumnsDescription & columns = metadata_snapshot->getColumns();\n \n-        auto columns_from_storage = add_special ? columns.getAll() : columns.getAllPhysical();\n+        NamesAndTypesList columns_from_storage;\n+        if (storage->supportsSubcolumns())\n+            columns_from_storage = add_special ? columns.getAllWithSubcolumns() : columns.getAllPhysicalWithSubcolumns();\n+        else\n+            columns_from_storage = add_special ? columns.getAll() : columns.getAllPhysical();\n+\n         if (source_columns.empty())\n             source_columns.swap(columns_from_storage);\n         else\ndiff --git a/src/Storages/AlterCommands.cpp b/src/Storages/AlterCommands.cpp\nindex 59b40ffdfbb7..7043a32760b3 100644\n--- a/src/Storages/AlterCommands.cpp\n+++ b/src/Storages/AlterCommands.cpp\n@@ -320,7 +320,8 @@ void AlterCommand::apply(StorageInMemoryMetadata & metadata, const Context & con\n         metadata.columns.add(column, after_column, first);\n \n         /// Slow, because each time a list is copied\n-        metadata.columns.flattenNested();\n+        if (context.getSettingsRef().flatten_nested)\n+            metadata.columns.flattenNested();\n     }\n     else if (type == DROP_COLUMN)\n     {\ndiff --git a/src/Storages/ColumnsDescription.cpp b/src/Storages/ColumnsDescription.cpp\nindex b43aab71af2c..b4ddde7c0f31 100644\n--- a/src/Storages/ColumnsDescription.cpp\n+++ b/src/Storages/ColumnsDescription.cpp\n@@ -19,6 +19,7 @@\n #include <DataTypes/NestedUtils.h>\n #include <DataTypes/DataTypeArray.h>\n #include <DataTypes/DataTypeTuple.h>\n+#include <DataTypes/DataTypeNested.h>\n #include <Common/Exception.h>\n #include <Interpreters/Context.h>\n #include <Storages/IStorage.h>\n@@ -184,6 +185,7 @@ void ColumnsDescription::add(ColumnDescription column, const String & after_colu\n         insert_it = range.second;\n     }\n \n+    addSubcolumns(column.name, column.type);\n     columns.get<0>().insert(insert_it, std::move(column));\n }\n \n@@ -195,7 +197,10 @@ void ColumnsDescription::remove(const String & column_name)\n             ErrorCodes::NO_SUCH_COLUMN_IN_TABLE);\n \n     for (auto list_it = range.first; list_it != range.second;)\n+    {\n+        removeSubcolumns(list_it->name, list_it->type);\n         list_it = columns.get<0>().erase(list_it);\n+    }\n }\n \n void ColumnsDescription::rename(const String & column_from, const String & column_to)\n@@ -268,6 +273,7 @@ void ColumnsDescription::flattenNested()\n         }\n \n         ColumnDescription column = std::move(*it);\n+        removeSubcolumns(column.name, column.type);\n         it = columns.get<0>().erase(it);\n \n         const DataTypes & elements = type_tuple->getElements();\n@@ -281,6 +287,7 @@ void ColumnsDescription::flattenNested()\n             nested_column.name = Nested::concatenateName(column.name, names[i]);\n             nested_column.type = std::make_shared<DataTypeArray>(elements[i]);\n \n+            addSubcolumns(nested_column.name, nested_column.type);\n             columns.get<0>().insert(it, std::move(nested_column));\n         }\n     }\n@@ -322,10 +329,10 @@ NamesAndTypesList ColumnsDescription::getAll() const\n     return ret;\n }\n \n-\n bool ColumnsDescription::has(const String & column_name) const\n {\n-    return columns.get<1>().find(column_name) != columns.get<1>().end();\n+    return columns.get<1>().find(column_name) != columns.get<1>().end()\n+        || subcolumns.find(column_name) != subcolumns.end();\n }\n \n bool ColumnsDescription::hasNested(const String & column_name) const\n@@ -371,12 +378,56 @@ NameAndTypePair ColumnsDescription::getPhysical(const String & column_name) cons\n     return NameAndTypePair(it->name, it->type);\n }\n \n+NameAndTypePair ColumnsDescription::getPhysicalOrSubcolumn(const String & column_name) const\n+{\n+    if (auto it = columns.get<1>().find(column_name); it != columns.get<1>().end()\n+        && it->default_desc.kind != ColumnDefaultKind::Alias)\n+    {\n+        return NameAndTypePair(it->name, it->type);\n+    }\n+\n+    if (auto it = subcolumns.find(column_name); it != subcolumns.end())\n+    {\n+        return it->second;\n+    }\n+\n+    throw Exception(ErrorCodes::NO_SUCH_COLUMN_IN_TABLE,\n+        \"There is no physical column or subcolumn {} in table.\", column_name);\n+}\n+\n bool ColumnsDescription::hasPhysical(const String & column_name) const\n {\n     auto it = columns.get<1>().find(column_name);\n     return it != columns.get<1>().end() && it->default_desc.kind != ColumnDefaultKind::Alias;\n }\n \n+bool ColumnsDescription::hasPhysicalOrSubcolumn(const String & column_name) const\n+{\n+    return hasPhysical(column_name) || subcolumns.find(column_name) != subcolumns.end();\n+}\n+\n+static NamesAndTypesList getWithSubcolumns(NamesAndTypesList && source_list)\n+{\n+    NamesAndTypesList ret;\n+    for (const auto & col : source_list)\n+    {\n+        ret.emplace_back(col.name, col.type);\n+        for (const auto & subcolumn : col.type->getSubcolumnNames())\n+            ret.emplace_back(col.name, subcolumn, col.type, col.type->getSubcolumnType(subcolumn));\n+    }\n+\n+    return ret;\n+}\n+\n+NamesAndTypesList ColumnsDescription::getAllWithSubcolumns() const\n+{\n+    return getWithSubcolumns(getAll());\n+}\n+\n+NamesAndTypesList ColumnsDescription::getAllPhysicalWithSubcolumns() const\n+{\n+    return getWithSubcolumns(getAllPhysical());\n+}\n \n bool ColumnsDescription::hasDefaults() const\n {\n@@ -483,13 +534,33 @@ ColumnsDescription ColumnsDescription::parse(const String & str)\n         ColumnDescription column;\n         column.readText(buf);\n         buf.ignore(1); /// ignore new line\n-        result.add(std::move(column));\n+        result.add(column);\n     }\n \n     assertEOF(buf);\n     return result;\n }\n \n+void ColumnsDescription::addSubcolumns(const String & name_in_storage, const DataTypePtr & type_in_storage)\n+{\n+    for (const auto & subcolumn_name : type_in_storage->getSubcolumnNames())\n+    {\n+        auto subcolumn = NameAndTypePair(name_in_storage, subcolumn_name,\n+            type_in_storage, type_in_storage->getSubcolumnType(subcolumn_name));\n+\n+        if (has(subcolumn.name))\n+            throw Exception(ErrorCodes::ILLEGAL_COLUMN,\n+                \"Cannot add subcolumn {}: column with this name already exists\", subcolumn.name);\n+\n+        subcolumns[subcolumn.name] = subcolumn;\n+    }\n+}\n+\n+void ColumnsDescription::removeSubcolumns(const String & name_in_storage, const DataTypePtr & type_in_storage)\n+{\n+    for (const auto & subcolumn_name : type_in_storage->getSubcolumnNames())\n+        subcolumns.erase(name_in_storage + \".\" + subcolumn_name);\n+}\n \n Block validateColumnsDefaultsAndGetSampleBlock(ASTPtr default_expr_list, const NamesAndTypesList & all_columns, const Context & context)\n {\ndiff --git a/src/Storages/ColumnsDescription.h b/src/Storages/ColumnsDescription.h\nindex 1df6c6ae67b6..1990c565b656 100644\n--- a/src/Storages/ColumnsDescription.h\n+++ b/src/Storages/ColumnsDescription.h\n@@ -77,6 +77,8 @@ class ColumnsDescription\n     NamesAndTypesList getAliases() const;\n     NamesAndTypesList getAllPhysical() const; /// ordinary + materialized.\n     NamesAndTypesList getAll() const; /// ordinary + materialized + aliases\n+    NamesAndTypesList getAllWithSubcolumns() const;\n+    NamesAndTypesList getAllPhysicalWithSubcolumns() const;\n \n     using ColumnTTLs = std::unordered_map<String, ASTPtr>;\n     ColumnTTLs getColumnTTLs() const;\n@@ -105,7 +107,9 @@ class ColumnsDescription\n \n     Names getNamesOfPhysical() const;\n     bool hasPhysical(const String & column_name) const;\n+    bool hasPhysicalOrSubcolumn(const String & column_name) const;\n     NameAndTypePair getPhysical(const String & column_name) const;\n+    NameAndTypePair getPhysicalOrSubcolumn(const String & column_name) const;\n \n     ColumnDefaults getDefaults() const; /// TODO: remove\n     bool hasDefault(const String & column_name) const;\n@@ -141,7 +145,12 @@ class ColumnsDescription\n private:\n     Container columns;\n \n+    using SubcolumnsContainer = std::unordered_map<String, NameAndTypePair>;\n+    SubcolumnsContainer subcolumns;\n+\n     void modifyColumnOrder(const String & column_name, const String & after_column, bool first);\n+    void addSubcolumns(const String & name_in_storage, const DataTypePtr & type_in_storage);\n+    void removeSubcolumns(const String & name_in_storage, const DataTypePtr & type_in_storage);\n };\n \n /// Validate default expressions and corresponding types compatibility, i.e.\ndiff --git a/src/Storages/IStorage.h b/src/Storages/IStorage.h\nindex cfb4c4e9646d..031b960fac11 100644\n--- a/src/Storages/IStorage.h\n+++ b/src/Storages/IStorage.h\n@@ -128,6 +128,9 @@ class IStorage : public std::enable_shared_from_this<IStorage>, public TypePromo\n     /// Example is StorageSystemNumbers.\n     virtual bool hasEvenlyDistributedRead() const { return false; }\n \n+    /// Returns true if the storage supports reading of subcolumns of complex types.\n+    virtual bool supportsSubcolumns() const { return false; }\n+\n \n     /// Optional size information of each physical column.\n     /// Currently it's only used by the MergeTree family for query optimizations.\ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.cpp b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\nindex 9aefe69fce71..5a2c17a7b98c 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n@@ -17,6 +17,7 @@\n #include <common/logger_useful.h>\n #include <Compression/getCompressionCodecForFile.h>\n #include <Parsers/queryToString.h>\n+#include <DataTypes/NestedUtils.h>\n \n \n namespace CurrentMetrics\n@@ -321,7 +322,12 @@ void IMergeTreeDataPart::setColumns(const NamesAndTypesList & new_columns)\n     column_name_to_position.reserve(new_columns.size());\n     size_t pos = 0;\n     for (const auto & column : columns)\n-        column_name_to_position.emplace(column.name, pos++);\n+    {\n+        column_name_to_position.emplace(column.name, pos);\n+        for (const auto & subcolumn : column.type->getSubcolumnNames())\n+            column_name_to_position.emplace(Nested::concatenateName(column.name, subcolumn), pos);\n+        ++pos;\n+    }\n }\n \n void IMergeTreeDataPart::removeIfNeeded()\n@@ -454,7 +460,7 @@ String IMergeTreeDataPart::getColumnNameWithMinimumCompressedSize(const StorageM\n         if (alter_conversions.isColumnRenamed(column.name))\n             column_name = alter_conversions.getColumnOldName(column.name);\n \n-        if (!hasColumnFiles(column_name, *column_type))\n+        if (!hasColumnFiles(column))\n             continue;\n \n         const auto size = getColumnSize(column_name, *column_type).data_compressed;\n@@ -640,7 +646,7 @@ CompressionCodecPtr IMergeTreeDataPart::detectDefaultCompressionCodec() const\n             {\n                 if (path_to_data_file.empty())\n                 {\n-                    String candidate_path = getFullRelativePath() + IDataType::getFileNameForStream(part_column.name, substream_path) + \".bin\";\n+                    String candidate_path = getFullRelativePath() + IDataType::getFileNameForStream(part_column, substream_path) + \".bin\";\n \n                     /// We can have existing, but empty .bin files. Example: LowCardinality(Nullable(...)) columns and column_name.dict.null.bin file.\n                     if (volume->getDisk()->exists(candidate_path) && volume->getDisk()->getFileSize(candidate_path) != 0)\ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.h b/src/Storages/MergeTree/IMergeTreeDataPart.h\nindex 9c8247ba9bdd..2f531bd83911 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.h\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.h\n@@ -330,7 +330,7 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n     /// NOTE: Doesn't take column renames into account, if some column renames\n     /// take place, you must take original name of column for this part from\n     /// storage and pass it to this method.\n-    virtual bool hasColumnFiles(const String & /* column */, const IDataType & /* type */) const { return false; }\n+    virtual bool hasColumnFiles(const NameAndTypePair & /* column */) const { return false; }\n \n     /// Returns true if this part shall participate in merges according to\n     /// settings of given storage policy.\ndiff --git a/src/Storages/MergeTree/IMergeTreeReader.cpp b/src/Storages/MergeTree/IMergeTreeReader.cpp\nindex 4379ac09af01..0140b32e12cf 100644\n--- a/src/Storages/MergeTree/IMergeTreeReader.cpp\n+++ b/src/Storages/MergeTree/IMergeTreeReader.cpp\n@@ -42,7 +42,14 @@ IMergeTreeReader::IMergeTreeReader(\n     , all_mark_ranges(all_mark_ranges_)\n     , alter_conversions(storage.getAlterConversionsForPart(data_part))\n {\n-    for (const NameAndTypePair & column_from_part : data_part->getColumns())\n+    auto part_columns = data_part->getColumns();\n+    if (settings.convert_nested_to_subcolumns)\n+    {\n+        columns = Nested::convertToSubcolumns(columns);\n+        part_columns = Nested::collect(part_columns);\n+    }\n+\n+    for (const NameAndTypePair & column_from_part : part_columns)\n         columns_from_part[column_from_part.name] = column_from_part.type;\n }\n \n@@ -74,7 +81,6 @@ static bool arrayHasNoElementsRead(const IColumn & column)\n     return last_offset != 0;\n }\n \n-\n void IMergeTreeReader::fillMissingColumns(Columns & res_columns, bool & should_evaluate_missing_defaults, size_t num_rows)\n {\n     try\n@@ -197,19 +203,33 @@ void IMergeTreeReader::evaluateMissingDefaults(Block additional_columns, Columns\n \n NameAndTypePair IMergeTreeReader::getColumnFromPart(const NameAndTypePair & required_column) const\n {\n-    if (alter_conversions.isColumnRenamed(required_column.name))\n+    auto name_in_storage = required_column.getNameInStorage();\n+\n+    decltype(columns_from_part.begin()) it;\n+    if (alter_conversions.isColumnRenamed(name_in_storage))\n     {\n-        String old_name = alter_conversions.getColumnOldName(required_column.name);\n-        auto it = columns_from_part.find(old_name);\n-        if (it != columns_from_part.end())\n-            return {it->first, it->second};\n+        String old_name = alter_conversions.getColumnOldName(name_in_storage);\n+        it = columns_from_part.find(old_name);\n     }\n-    else if (auto it = columns_from_part.find(required_column.name); it != columns_from_part.end())\n+    else\n     {\n-        return {it->first, it->second};\n+        it = columns_from_part.find(name_in_storage);\n+    }\n+\n+    if (it == columns_from_part.end())\n+        return required_column;\n+\n+    if (required_column.isSubcolumn())\n+    {\n+        auto subcolumn_name = required_column.getSubcolumnName();\n+        auto subcolumn_type = it->second->tryGetSubcolumnType(subcolumn_name);\n+        if (!subcolumn_type)\n+            subcolumn_type = required_column.type;\n+\n+        return {it->first, subcolumn_name, it->second, subcolumn_type};\n     }\n \n-    return required_column;\n+    return {it->first, it->second};\n }\n \n void IMergeTreeReader::performRequiredConversions(Columns & res_columns)\ndiff --git a/src/Storages/MergeTree/IMergedBlockOutputStream.cpp b/src/Storages/MergeTree/IMergedBlockOutputStream.cpp\nindex 1ea74d9fb27f..7e562ae03d63 100644\n--- a/src/Storages/MergeTree/IMergedBlockOutputStream.cpp\n+++ b/src/Storages/MergeTree/IMergedBlockOutputStream.cpp\n@@ -33,7 +33,7 @@ NameSet IMergedBlockOutputStream::removeEmptyColumnsFromPart(\n         column.type->enumerateStreams(\n             [&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_path */)\n             {\n-                ++stream_counts[IDataType::getFileNameForStream(column.name, substream_path)];\n+                ++stream_counts[IDataType::getFileNameForStream(column, substream_path)];\n             },\n             {});\n     }\n@@ -42,9 +42,13 @@ NameSet IMergedBlockOutputStream::removeEmptyColumnsFromPart(\n     const String mrk_extension = data_part->getMarksFileExtension();\n     for (const auto & column_name : empty_columns)\n     {\n+        auto column_with_type = columns.tryGetByName(column_name);\n+        if (!column_with_type)\n+           continue;\n+\n         IDataType::StreamCallback callback = [&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_path */)\n         {\n-            String stream_name = IDataType::getFileNameForStream(column_name, substream_path);\n+            String stream_name = IDataType::getFileNameForStream(*column_with_type, substream_path);\n             /// Delete files if they are no longer shared with another column.\n             if (--stream_counts[stream_name] == 0)\n             {\n@@ -52,10 +56,9 @@ NameSet IMergedBlockOutputStream::removeEmptyColumnsFromPart(\n                 remove_files.emplace(stream_name + mrk_extension);\n             }\n         };\n+\n         IDataType::SubstreamPath stream_path;\n-        auto column_with_type = columns.tryGetByName(column_name);\n-        if (column_with_type)\n-            column_with_type->type->enumerateStreams(callback, stream_path);\n+        column_with_type->type->enumerateStreams(callback, stream_path);\n     }\n \n     /// Remove files on disk and checksums\ndiff --git a/src/Storages/MergeTree/MergeTreeBlockReadUtils.cpp b/src/Storages/MergeTree/MergeTreeBlockReadUtils.cpp\nindex ad10a437b1ec..f8b5e0a9c0a0 100644\n--- a/src/Storages/MergeTree/MergeTreeBlockReadUtils.cpp\n+++ b/src/Storages/MergeTree/MergeTreeBlockReadUtils.cpp\n@@ -1,5 +1,6 @@\n #include <Storages/MergeTree/MergeTreeBlockReadUtils.h>\n #include <Storages/MergeTree/MergeTreeData.h>\n+#include <Core/NamesAndTypes.h>\n #include <Common/checkStackSize.h>\n #include <Common/typeid_cast.h>\n #include <Columns/ColumnConst.h>\n@@ -33,21 +34,30 @@ bool injectRequiredColumnsRecursively(\n     /// huge AST which for some reason was not validated on parsing/interpreter\n     /// stages.\n     checkStackSize();\n-    String column_name_in_part = column_name;\n-    if (alter_conversions.isColumnRenamed(column_name_in_part))\n-        column_name_in_part = alter_conversions.getColumnOldName(column_name_in_part);\n \n-    /// column has files and hence does not require evaluation\n-    if (storage_columns.hasPhysical(column_name) && part->hasColumnFiles(column_name_in_part, *storage_columns.getPhysical(column_name).type))\n+    if (storage_columns.hasPhysicalOrSubcolumn(column_name))\n     {\n-        /// ensure each column is added only once\n-        if (required_columns.count(column_name) == 0)\n+        auto column_in_storage = storage_columns.getPhysicalOrSubcolumn(column_name);\n+        auto column_name_in_part = column_in_storage.getNameInStorage();\n+        if (alter_conversions.isColumnRenamed(column_name_in_part))\n+            column_name_in_part = alter_conversions.getColumnOldName(column_name_in_part);\n+\n+        auto column_in_part = NameAndTypePair(\n+            column_name_in_part, column_in_storage.getSubcolumnName(),\n+            column_in_storage.getTypeInStorage(), column_in_storage.type);\n+\n+        /// column has files and hence does not require evaluation\n+        if (part->hasColumnFiles(column_in_part))\n         {\n-            columns.emplace_back(column_name);\n-            required_columns.emplace(column_name);\n-            injected_columns.emplace(column_name);\n+            /// ensure each column is added only once\n+            if (required_columns.count(column_name) == 0)\n+            {\n+                columns.emplace_back(column_name);\n+                required_columns.emplace(column_name);\n+                injected_columns.emplace(column_name);\n+            }\n+            return true;\n         }\n-        return true;\n     }\n \n     /// Column doesn't have default value and don't exist in part\n@@ -81,8 +91,8 @@ NameSet injectRequiredColumns(const MergeTreeData & storage, const StorageMetada\n     for (size_t i = 0; i < columns.size(); ++i)\n     {\n         /// We are going to fetch only physical columns\n-        if (!storage_columns.hasPhysical(columns[i]))\n-            throw Exception(\"There is no physical column \" + columns[i] + \" in table.\", ErrorCodes::NO_SUCH_COLUMN_IN_TABLE);\n+        if (!storage_columns.hasPhysicalOrSubcolumn(columns[i]))\n+            throw Exception(\"There is no physical column or subcolumn \" + columns[i] + \" in table.\", ErrorCodes::NO_SUCH_COLUMN_IN_TABLE);\n \n         have_at_least_one_physical_column |= injectRequiredColumnsRecursively(\n             columns[i], storage_columns, alter_conversions,\n@@ -285,7 +295,7 @@ MergeTreeReadTaskColumns getReadTaskColumns(\n \n     if (check_columns)\n     {\n-        const NamesAndTypesList & physical_columns = metadata_snapshot->getColumns().getAllPhysical();\n+        const NamesAndTypesList & physical_columns = metadata_snapshot->getColumns().getAllWithSubcolumns();\n         result.pre_columns = physical_columns.addTypes(pre_column_names);\n         result.columns = physical_columns.addTypes(column_names);\n     }\ndiff --git a/src/Storages/MergeTree/MergeTreeData.h b/src/Storages/MergeTree/MergeTreeData.h\nindex 6098cd22c989..9d0218158887 100644\n--- a/src/Storages/MergeTree/MergeTreeData.h\n+++ b/src/Storages/MergeTree/MergeTreeData.h\n@@ -357,6 +357,8 @@ class MergeTreeData : public IStorage\n             || merging_params.mode == MergingParams::VersionedCollapsing;\n     }\n \n+    bool supportsSubcolumns() const override { return true; }\n+\n     NamesAndTypesList getVirtuals() const override;\n \n     bool mayBenefitFromIndexForIn(const ASTPtr & left_in_operand, const Context &, const StorageMetadataPtr & metadata_snapshot) const override;\ndiff --git a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\nindex 0704ad484356..f999aa67bbe8 100644\n--- a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\n@@ -1493,7 +1493,7 @@ NameToNameVector MergeTreeDataMergerMutator::collectFilesForRenames(\n         column.type->enumerateStreams(\n             [&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n             {\n-                ++stream_counts[IDataType::getFileNameForStream(column.name, substream_path)];\n+                ++stream_counts[IDataType::getFileNameForStream(column, substream_path)];\n             },\n             {});\n     }\n@@ -1511,7 +1511,7 @@ NameToNameVector MergeTreeDataMergerMutator::collectFilesForRenames(\n         {\n             IDataType::StreamCallback callback = [&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n             {\n-                String stream_name = IDataType::getFileNameForStream(command.column_name, substream_path);\n+                String stream_name = IDataType::getFileNameForStream({command.column_name, command.data_type}, substream_path);\n                 /// Delete files if they are no longer shared with another column.\n                 if (--stream_counts[stream_name] == 0)\n                 {\n@@ -1532,7 +1532,7 @@ NameToNameVector MergeTreeDataMergerMutator::collectFilesForRenames(\n \n             IDataType::StreamCallback callback = [&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n             {\n-                String stream_from = IDataType::getFileNameForStream(command.column_name, substream_path);\n+                String stream_from = IDataType::getFileNameForStream({command.column_name, command.data_type}, substream_path);\n \n                 String stream_to = boost::replace_first_copy(stream_from, escaped_name_from, escaped_name_to);\n \n@@ -1565,7 +1565,7 @@ NameSet MergeTreeDataMergerMutator::collectFilesToSkip(\n     {\n         IDataType::StreamCallback callback = [&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n         {\n-            String stream_name = IDataType::getFileNameForStream(entry.name, substream_path);\n+            String stream_name = IDataType::getFileNameForStream({entry.name, entry.type}, substream_path);\n             files_to_skip.insert(stream_name + \".bin\");\n             files_to_skip.insert(stream_name + mrk_extension);\n         };\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartCompact.cpp b/src/Storages/MergeTree/MergeTreeDataPartCompact.cpp\nindex c42caa2d4d4a..32f54e3b7829 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartCompact.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartCompact.cpp\n@@ -1,4 +1,5 @@\n #include \"MergeTreeDataPartCompact.h\"\n+#include <DataTypes/NestedUtils.h>\n #include <Storages/MergeTree/MergeTreeReaderCompact.h>\n #include <Storages/MergeTree/MergeTreeDataPartWriterCompact.h>\n #include <Poco/File.h>\n@@ -121,9 +122,9 @@ void MergeTreeDataPartCompact::loadIndexGranularity()\n     index_granularity.setInitialized();\n }\n \n-bool MergeTreeDataPartCompact::hasColumnFiles(const String & column_name, const IDataType &) const\n+bool MergeTreeDataPartCompact::hasColumnFiles(const NameAndTypePair & column) const\n {\n-    if (!getColumnPosition(column_name))\n+    if (!getColumnPosition(column.name))\n         return false;\n \n     auto bin_checksum = checksums.files.find(DATA_FILE_NAME_WITH_EXTENSION);\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartCompact.h b/src/Storages/MergeTree/MergeTreeDataPartCompact.h\nindex 2f2a2f537aac..2c0c4020bb08 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartCompact.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartCompact.h\n@@ -55,7 +55,7 @@ class MergeTreeDataPartCompact : public IMergeTreeDataPart\n \n     bool isStoredOnDisk() const override { return true; }\n \n-    bool hasColumnFiles(const String & column_name, const IDataType & type) const override;\n+    bool hasColumnFiles(const NameAndTypePair & column) const override;\n \n     String getFileNameForColumn(const NameAndTypePair & /* column */) const override { return DATA_FILE_NAME; }\n \ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp b/src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp\nindex 1d70ff9d6c4d..96fa411339c7 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartInMemory.cpp\n@@ -3,6 +3,7 @@\n #include <Storages/MergeTree/MergedBlockOutputStream.h>\n #include <Storages/MergeTree/MergeTreeDataPartWriterInMemory.h>\n #include <Storages/MergeTree/IMergeTreeReader.h>\n+#include <DataTypes/NestedUtils.h>\n #include <Interpreters/Context.h>\n #include <Poco/File.h>\n #include <Poco/Logger.h>\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartInMemory.h b/src/Storages/MergeTree/MergeTreeDataPartInMemory.h\nindex 1fceb47cba87..397d3d2036c3 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartInMemory.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartInMemory.h\n@@ -32,6 +32,7 @@ class MergeTreeDataPartInMemory : public IMergeTreeDataPart\n         const MergeTreeReaderSettings & reader_settings_,\n         const ValueSizeMap & avg_value_size_hints,\n         const ReadBufferFromFileBase::ProfileCallback & profile_callback) const override;\n+\n     MergeTreeWriterPtr getWriter(\n         const NamesAndTypesList & columns_list,\n         const StorageMetadataPtr & metadata_snapshot,\n@@ -41,7 +42,7 @@ class MergeTreeDataPartInMemory : public IMergeTreeDataPart\n         const MergeTreeIndexGranularity & computed_index_granularity) const override;\n \n     bool isStoredOnDisk() const override { return false; }\n-    bool hasColumnFiles(const String & column_name, const IDataType & /* type */) const override { return !!getColumnPosition(column_name); }\n+    bool hasColumnFiles(const NameAndTypePair & column) const override { return !!getColumnPosition(column.name); }\n     String getFileNameForColumn(const NameAndTypePair & /* column */) const override { return \"\"; }\n     void renameTo(const String & new_relative_path, bool remove_new_dir_if_exists) const override;\n     void makeCloneInDetached(const String & prefix, const StorageMetadataPtr & metadata_snapshot) const override;\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWide.cpp b/src/Storages/MergeTree/MergeTreeDataPartWide.cpp\nindex 39898a7cc57a..5dd8c26f224e 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWide.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWide.cpp\n@@ -3,6 +3,8 @@\n #include <Storages/MergeTree/MergeTreeReaderWide.h>\n #include <Storages/MergeTree/MergeTreeDataPartWriterWide.h>\n #include <Storages/MergeTree/IMergeTreeDataPartWriter.h>\n+#include <DataTypes/NestedUtils.h>\n+#include <Core/NamesAndTypes.h>\n \n \n namespace DB\n@@ -46,10 +48,13 @@ IMergeTreeDataPart::MergeTreeReaderPtr MergeTreeDataPartWide::getReader(\n     const ValueSizeMap & avg_value_size_hints,\n     const ReadBufferFromFileBase::ProfileCallback & profile_callback) const\n {\n+    auto new_settings = reader_settings;\n+    new_settings.convert_nested_to_subcolumns = true;\n+\n     auto ptr = std::static_pointer_cast<const MergeTreeDataPartWide>(shared_from_this());\n     return std::make_unique<MergeTreeReaderWide>(\n         ptr, columns_to_read, metadata_snapshot, uncompressed_cache,\n-        mark_cache, mark_ranges, reader_settings,\n+        mark_cache, mark_ranges, new_settings,\n         avg_value_size_hints, profile_callback);\n }\n \n@@ -71,15 +76,15 @@ IMergeTreeDataPart::MergeTreeWriterPtr MergeTreeDataPartWide::getWriter(\n /// Takes into account the fact that several columns can e.g. share their .size substreams.\n /// When calculating totals these should be counted only once.\n ColumnSize MergeTreeDataPartWide::getColumnSizeImpl(\n-    const String & column_name, const IDataType & type, std::unordered_set<String> * processed_substreams) const\n+    const NameAndTypePair & column, std::unordered_set<String> * processed_substreams) const\n {\n     ColumnSize size;\n     if (checksums.empty())\n         return size;\n \n-    type.enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n+    column.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n     {\n-        String file_name = IDataType::getFileNameForStream(column_name, substream_path);\n+        String file_name = IDataType::getFileNameForStream(column, substream_path);\n \n         if (processed_substreams && !processed_substreams->insert(file_name).second)\n             return;\n@@ -157,7 +162,7 @@ void MergeTreeDataPartWide::checkConsistency(bool require_part_metadata) const\n                 IDataType::SubstreamPath stream_path;\n                 name_type.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n                 {\n-                    String file_name = IDataType::getFileNameForStream(name_type.name, substream_path);\n+                    String file_name = IDataType::getFileNameForStream(name_type, substream_path);\n                     String mrk_file_name = file_name + index_granularity_info.marks_file_extension;\n                     String bin_file_name = file_name + \".bin\";\n                     if (!checksums.files.count(mrk_file_name))\n@@ -179,7 +184,7 @@ void MergeTreeDataPartWide::checkConsistency(bool require_part_metadata) const\n         {\n             name_type.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n             {\n-                auto file_path = path + IDataType::getFileNameForStream(name_type.name, substream_path) + index_granularity_info.marks_file_extension;\n+                auto file_path = path + IDataType::getFileNameForStream(name_type, substream_path) + index_granularity_info.marks_file_extension;\n \n                 /// Missing file is Ok for case when new column was added.\n                 if (volume->getDisk()->exists(file_path))\n@@ -201,13 +206,13 @@ void MergeTreeDataPartWide::checkConsistency(bool require_part_metadata) const\n     }\n }\n \n-bool MergeTreeDataPartWide::hasColumnFiles(const String & column_name, const IDataType & type) const\n+bool MergeTreeDataPartWide::hasColumnFiles(const NameAndTypePair & column) const\n {\n     bool res = true;\n \n-    type.enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n+    column.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n     {\n-        String file_name = IDataType::getFileNameForStream(column_name, substream_path);\n+        String file_name = IDataType::getFileNameForStream(column, substream_path);\n \n         auto bin_checksum = checksums.files.find(file_name + \".bin\");\n         auto mrk_checksum = checksums.files.find(file_name + index_granularity_info.marks_file_extension);\n@@ -225,7 +230,7 @@ String MergeTreeDataPartWide::getFileNameForColumn(const NameAndTypePair & colum\n     column.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n     {\n         if (filename.empty())\n-            filename = IDataType::getFileNameForStream(column.name, substream_path);\n+            filename = IDataType::getFileNameForStream(column, substream_path);\n     });\n     return filename;\n }\n@@ -235,7 +240,7 @@ void MergeTreeDataPartWide::calculateEachColumnSizes(ColumnSizeByName & each_col\n     std::unordered_set<String> processed_substreams;\n     for (const NameAndTypePair & column : columns)\n     {\n-        ColumnSize size = getColumnSizeImpl(column.name, *column.type, &processed_substreams);\n+        ColumnSize size = getColumnSizeImpl(column, &processed_substreams);\n         each_columns_size[column.name] = size;\n         total_size.add(size);\n \ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWide.h b/src/Storages/MergeTree/MergeTreeDataPartWide.h\nindex aa8c3aedea7e..30d3021d0033 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWide.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWide.h\n@@ -54,7 +54,7 @@ class MergeTreeDataPartWide : public IMergeTreeDataPart\n \n     ~MergeTreeDataPartWide() override;\n \n-    bool hasColumnFiles(const String & column, const IDataType & type) const override;\n+    bool hasColumnFiles(const NameAndTypePair & column) const override;\n \n private:\n     void checkConsistency(bool require_part_metadata) const override;\n@@ -62,7 +62,7 @@ class MergeTreeDataPartWide : public IMergeTreeDataPart\n     /// Loads marks index granularity into memory\n     void loadIndexGranularity() override;\n \n-    ColumnSize getColumnSizeImpl(const String & name, const IDataType & type, std::unordered_set<String> * processed_substreams) const;\n+    ColumnSize getColumnSizeImpl(const NameAndTypePair & column, std::unordered_set<String> * processed_substreams) const;\n \n     void calculateEachColumnSizes(ColumnSizeByName & each_columns_size, ColumnSize & total_size) const override;\n };\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\nindex ef3b5eb7d24e..7ec5cf8920c7 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\n@@ -34,14 +34,14 @@ MergeTreeDataPartWriterCompact::MergeTreeDataPartWriterCompact(\n {\n     const auto & storage_columns = metadata_snapshot->getColumns();\n     for (const auto & column : columns_list)\n-        addStreams(column.name, *column.type, storage_columns.getCodecDescOrDefault(column.name, default_codec));\n+        addStreams(column, storage_columns.getCodecDescOrDefault(column.name, default_codec));\n }\n \n-void MergeTreeDataPartWriterCompact::addStreams(const String & name, const IDataType & type, const ASTPtr & effective_codec_desc)\n+void MergeTreeDataPartWriterCompact::addStreams(const NameAndTypePair & column, const ASTPtr & effective_codec_desc)\n {\n     IDataType::StreamCallback callback = [&] (const IDataType::SubstreamPath & substream_path, const IDataType & substream_type)\n     {\n-        String stream_name = IDataType::getFileNameForStream(name, substream_path);\n+        String stream_name = IDataType::getFileNameForStream(column, substream_path);\n \n         /// Shared offsets for Nested type.\n         if (compressed_streams.count(stream_name))\n@@ -64,7 +64,7 @@ void MergeTreeDataPartWriterCompact::addStreams(const String & name, const IData\n     };\n \n     IDataType::SubstreamPath stream_path;\n-    type.enumerateStreams(callback, stream_path);\n+    column.type->enumerateStreams(callback, stream_path);\n }\n \n namespace\n@@ -183,7 +183,7 @@ void MergeTreeDataPartWriterCompact::writeDataBlock(const Block & block, const G\n             CompressedStreamPtr prev_stream;\n             auto stream_getter = [&, this](const IDataType::SubstreamPath & substream_path) -> WriteBuffer *\n             {\n-                String stream_name = IDataType::getFileNameForStream(name_and_type->name, substream_path);\n+                String stream_name = IDataType::getFileNameForStream(*name_and_type, substream_path);\n \n                 auto & result_stream = compressed_streams[stream_name];\n                 /// Write one compressed block per column in granule for more optimal reading.\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h\nindex 2913960fd352..8b86a9701c9c 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h\n@@ -37,7 +37,7 @@ class MergeTreeDataPartWriterCompact : public MergeTreeDataPartWriterOnDisk\n \n     void addToChecksums(MergeTreeDataPartChecksums & checksums);\n \n-    void addStreams(const String & name, const IDataType & type, const ASTPtr & effective_codec_desc);\n+    void addStreams(const NameAndTypePair & column, const ASTPtr & effective_codec_desc);\n \n     Block header;\n \ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\nindex 982345f8240f..81a6539780c2 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.cpp\n@@ -80,17 +80,17 @@ MergeTreeDataPartWriterWide::MergeTreeDataPartWriterWide(\n {\n     const auto & columns = metadata_snapshot->getColumns();\n     for (const auto & it : columns_list)\n-        addStreams(it.name, *it.type, columns.getCodecDescOrDefault(it.name, default_codec));\n+        addStreams(it, columns.getCodecDescOrDefault(it.name, default_codec));\n }\n \n+\n void MergeTreeDataPartWriterWide::addStreams(\n-    const String & name,\n-    const IDataType & type,\n+    const NameAndTypePair & column,\n     const ASTPtr & effective_codec_desc)\n {\n     IDataType::StreamCallback callback = [&] (const IDataType::SubstreamPath & substream_path, const IDataType & substream_type)\n     {\n-        String stream_name = IDataType::getFileNameForStream(name, substream_path);\n+        String stream_name = IDataType::getFileNameForStream(column, substream_path);\n         /// Shared offsets for Nested type.\n         if (column_streams.count(stream_name))\n             return;\n@@ -112,18 +112,18 @@ void MergeTreeDataPartWriterWide::addStreams(\n     };\n \n     IDataType::SubstreamPath stream_path;\n-    type.enumerateStreams(callback, stream_path);\n+    column.type->enumerateStreams(callback, stream_path);\n }\n \n \n IDataType::OutputStreamGetter MergeTreeDataPartWriterWide::createStreamGetter(\n-        const String & name, WrittenOffsetColumns & offset_columns) const\n+        const NameAndTypePair & column, WrittenOffsetColumns & offset_columns) const\n {\n     return [&, this] (const IDataType::SubstreamPath & substream_path) -> WriteBuffer *\n     {\n         bool is_offsets = !substream_path.empty() && substream_path.back().type == IDataType::Substream::ArraySizes;\n \n-        String stream_name = IDataType::getFileNameForStream(name, substream_path);\n+        String stream_name = IDataType::getFileNameForStream(column, substream_path);\n \n         /// Don't write offsets more than one time for Nested type.\n         if (is_offsets && offset_columns.count(stream_name))\n@@ -210,23 +210,23 @@ void MergeTreeDataPartWriterWide::write(const Block & block, const IColumn::Perm\n             if (primary_key_block.has(it->name))\n             {\n                 const auto & primary_column = *primary_key_block.getByName(it->name).column;\n-                writeColumn(column.name, *column.type, primary_column, offset_columns, granules_to_write);\n+                writeColumn(*it, primary_column, offset_columns, granules_to_write);\n             }\n             else if (skip_indexes_block.has(it->name))\n             {\n                 const auto & index_column = *skip_indexes_block.getByName(it->name).column;\n-                writeColumn(column.name, *column.type, index_column, offset_columns, granules_to_write);\n+                writeColumn(*it, index_column, offset_columns, granules_to_write);\n             }\n             else\n             {\n                 /// We rearrange the columns that are not included in the primary key here; Then the result is released - to save RAM.\n                 ColumnPtr permuted_column = column.column->permute(*permutation, 0);\n-                writeColumn(column.name, *column.type, *permuted_column, offset_columns, granules_to_write);\n+                writeColumn(*it, *permuted_column, offset_columns, granules_to_write);\n             }\n         }\n         else\n         {\n-            writeColumn(column.name, *column.type, *column.column, offset_columns, granules_to_write);\n+            writeColumn(*it, *column.column, offset_columns, granules_to_write);\n         }\n     }\n \n@@ -239,13 +239,12 @@ void MergeTreeDataPartWriterWide::write(const Block & block, const IColumn::Perm\n }\n \n void MergeTreeDataPartWriterWide::writeSingleMark(\n-    const String & name,\n-    const IDataType & type,\n+    const NameAndTypePair & column,\n     WrittenOffsetColumns & offset_columns,\n     size_t number_of_rows,\n     DB::IDataType::SubstreamPath & path)\n {\n-    StreamsWithMarks marks = getCurrentMarksForColumn(name, type, offset_columns, path);\n+    StreamsWithMarks marks = getCurrentMarksForColumn(column, offset_columns, path);\n     for (const auto & mark : marks)\n         flushMarkToFile(mark, number_of_rows);\n }\n@@ -260,17 +259,16 @@ void MergeTreeDataPartWriterWide::flushMarkToFile(const StreamNameAndMark & stre\n }\n \n StreamsWithMarks MergeTreeDataPartWriterWide::getCurrentMarksForColumn(\n-    const String & name,\n-    const IDataType & type,\n+    const NameAndTypePair & column,\n     WrittenOffsetColumns & offset_columns,\n     DB::IDataType::SubstreamPath & path)\n {\n     StreamsWithMarks result;\n-    type.enumerateStreams([&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n+    column.type->enumerateStreams([&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n     {\n         bool is_offsets = !substream_path.empty() && substream_path.back().type == IDataType::Substream::ArraySizes;\n \n-        String stream_name = IDataType::getFileNameForStream(name, substream_path);\n+        String stream_name = IDataType::getFileNameForStream(column, substream_path);\n \n         /// Don't write offsets more than one time for Nested type.\n         if (is_offsets && offset_columns.count(stream_name))\n@@ -294,22 +292,21 @@ StreamsWithMarks MergeTreeDataPartWriterWide::getCurrentMarksForColumn(\n }\n \n void MergeTreeDataPartWriterWide::writeSingleGranule(\n-    const String & name,\n-    const IDataType & type,\n+    const NameAndTypePair & name_and_type,\n     const IColumn & column,\n     WrittenOffsetColumns & offset_columns,\n     IDataType::SerializeBinaryBulkStatePtr & serialization_state,\n     IDataType::SerializeBinaryBulkSettings & serialize_settings,\n     const Granule & granule)\n {\n-    type.serializeBinaryBulkWithMultipleStreams(column, granule.start_row, granule.rows_to_write, serialize_settings, serialization_state);\n+    name_and_type.type->serializeBinaryBulkWithMultipleStreams(column, granule.start_row, granule.rows_to_write, serialize_settings, serialization_state);\n \n     /// So that instead of the marks pointing to the end of the compressed block, there were marks pointing to the beginning of the next one.\n-    type.enumerateStreams([&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n+    name_and_type.type->enumerateStreams([&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n     {\n         bool is_offsets = !substream_path.empty() && substream_path.back().type == IDataType::Substream::ArraySizes;\n \n-        String stream_name = IDataType::getFileNameForStream(name, substream_path);\n+        String stream_name = IDataType::getFileNameForStream(name_and_type, substream_path);\n \n         /// Don't write offsets more than one time for Nested type.\n         if (is_offsets && offset_columns.count(stream_name))\n@@ -321,27 +318,27 @@ void MergeTreeDataPartWriterWide::writeSingleGranule(\n \n /// Column must not be empty. (column.size() !== 0)\n void MergeTreeDataPartWriterWide::writeColumn(\n-    const String & name,\n-    const IDataType & type,\n+    const NameAndTypePair & name_and_type,\n     const IColumn & column,\n     WrittenOffsetColumns & offset_columns,\n     const Granules & granules)\n {\n     if (granules.empty())\n-        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Empty granules for column {}, current mark {}\", backQuoteIfNeed(name), getCurrentMark());\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Empty granules for column {}, current mark {}\", backQuoteIfNeed(name_and_type.name), getCurrentMark());\n \n+    const auto & [name, type] = name_and_type;\n     auto [it, inserted] = serialization_states.emplace(name, nullptr);\n \n     if (inserted)\n     {\n         IDataType::SerializeBinaryBulkSettings serialize_settings;\n-        serialize_settings.getter = createStreamGetter(name, offset_columns);\n-        type.serializeBinaryBulkStatePrefix(serialize_settings, it->second);\n+        serialize_settings.getter = createStreamGetter(name_and_type, offset_columns);\n+        type->serializeBinaryBulkStatePrefix(serialize_settings, it->second);\n     }\n \n     const auto & global_settings = storage.global_context.getSettingsRef();\n     IDataType::SerializeBinaryBulkSettings serialize_settings;\n-    serialize_settings.getter = createStreamGetter(name, offset_columns);\n+    serialize_settings.getter = createStreamGetter(name_and_type, offset_columns);\n     serialize_settings.low_cardinality_max_dictionary_size = global_settings.low_cardinality_max_dictionary_size;\n     serialize_settings.low_cardinality_use_single_dictionary_for_part = global_settings.low_cardinality_use_single_dictionary_for_part != 0;\n \n@@ -353,12 +350,11 @@ void MergeTreeDataPartWriterWide::writeColumn(\n         {\n             if (last_non_written_marks.count(name))\n                 throw Exception(ErrorCodes::LOGICAL_ERROR, \"We have to add new mark for column, but already have non written mark. Current mark {}, total marks {}, offset {}\", getCurrentMark(), index_granularity.getMarksCount(), rows_written_in_last_mark);\n-            last_non_written_marks[name] = getCurrentMarksForColumn(name, type, offset_columns, serialize_settings.path);\n+            last_non_written_marks[name] = getCurrentMarksForColumn(name_and_type, offset_columns, serialize_settings.path);\n         }\n \n         writeSingleGranule(\n-           name,\n-           type,\n+           name_and_type,\n            column,\n            offset_columns,\n            it->second,\n@@ -378,12 +374,12 @@ void MergeTreeDataPartWriterWide::writeColumn(\n         }\n     }\n \n-    type.enumerateStreams([&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n+    name_and_type.type->enumerateStreams([&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n     {\n         bool is_offsets = !substream_path.empty() && substream_path.back().type == IDataType::Substream::ArraySizes;\n         if (is_offsets)\n         {\n-            String stream_name = IDataType::getFileNameForStream(name, substream_path);\n+            String stream_name = IDataType::getFileNameForStream(name_and_type, substream_path);\n             offset_columns.insert(stream_name);\n         }\n     }, serialize_settings.path);\n@@ -526,12 +522,12 @@ void MergeTreeDataPartWriterWide::finishDataSerialization(IMergeTreeDataPart::Ch\n         {\n             if (!serialization_states.empty())\n             {\n-                serialize_settings.getter = createStreamGetter(it->name, written_offset_columns ? *written_offset_columns : offset_columns);\n+                serialize_settings.getter = createStreamGetter(*it, written_offset_columns ? *written_offset_columns : offset_columns);\n                 it->type->serializeBinaryBulkStateSuffix(serialize_settings, serialization_states[it->name]);\n             }\n \n             if (write_final_mark)\n-                writeFinalMark(it->name, it->type, offset_columns, serialize_settings.path);\n+                writeFinalMark(*it, offset_columns, serialize_settings.path);\n         }\n     }\n     for (auto & stream : column_streams)\n@@ -567,19 +563,18 @@ void MergeTreeDataPartWriterWide::finish(IMergeTreeDataPart::Checksums & checksu\n }\n \n void MergeTreeDataPartWriterWide::writeFinalMark(\n-    const std::string & column_name,\n-    const DataTypePtr column_type,\n+    const NameAndTypePair & column,\n     WrittenOffsetColumns & offset_columns,\n     DB::IDataType::SubstreamPath & path)\n {\n-    writeSingleMark(column_name, *column_type, offset_columns, 0, path);\n+    writeSingleMark(column, offset_columns, 0, path);\n     /// Memoize information about offsets\n-    column_type->enumerateStreams([&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n+    column.type->enumerateStreams([&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n     {\n         bool is_offsets = !substream_path.empty() && substream_path.back().type == IDataType::Substream::ArraySizes;\n         if (is_offsets)\n         {\n-            String stream_name = IDataType::getFileNameForStream(column_name, substream_path);\n+            String stream_name = IDataType::getFileNameForStream(column, substream_path);\n             offset_columns.insert(stream_name);\n         }\n     }, path);\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h\nindex d897503a0330..e6f96f3f146f 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterWide.h\n@@ -40,16 +40,14 @@ class MergeTreeDataPartWriterWide : public MergeTreeDataPartWriterOnDisk\n     /// Return how many marks were written and\n     /// how many rows were written for last mark\n     void writeColumn(\n-        const String & name,\n-        const IDataType & type,\n+        const NameAndTypePair & name_and_type,\n         const IColumn & column,\n         WrittenOffsetColumns & offset_columns,\n         const Granules & granules);\n \n     /// Write single granule of one column.\n     void writeSingleGranule(\n-        const String & name,\n-        const IDataType & type,\n+        const NameAndTypePair & name_and_type,\n         const IColumn & column,\n         WrittenOffsetColumns & offset_columns,\n         IDataType::SerializeBinaryBulkStatePtr & serialization_state,\n@@ -58,8 +56,7 @@ class MergeTreeDataPartWriterWide : public MergeTreeDataPartWriterOnDisk\n \n     /// Take offsets from column and return as MarkInCompressed file with stream name\n     StreamsWithMarks getCurrentMarksForColumn(\n-        const String & name,\n-        const IDataType & type,\n+        const NameAndTypePair & column,\n         WrittenOffsetColumns & offset_columns,\n         DB::IDataType::SubstreamPath & path);\n \n@@ -70,21 +67,18 @@ class MergeTreeDataPartWriterWide : public MergeTreeDataPartWriterOnDisk\n \n     /// Write mark for column taking offsets from column stream\n     void writeSingleMark(\n-        const String & name,\n-        const IDataType & type,\n+        const NameAndTypePair & column,\n         WrittenOffsetColumns & offset_columns,\n         size_t number_of_rows,\n         DB::IDataType::SubstreamPath & path);\n \n     void writeFinalMark(\n-        const std::string & column_name,\n-        const DataTypePtr column_type,\n+        const NameAndTypePair & column,\n         WrittenOffsetColumns & offset_columns,\n         DB::IDataType::SubstreamPath & path);\n \n     void addStreams(\n-        const String & name,\n-        const IDataType & type,\n+        const NameAndTypePair & column,\n         const ASTPtr & effective_codec_desc);\n \n     /// Method for self check (used in debug-build only). Checks that written\n@@ -106,7 +100,7 @@ class MergeTreeDataPartWriterWide : public MergeTreeDataPartWriterOnDisk\n     /// Also useful to have exact amount of rows in last (non-final) mark.\n     void adjustLastMarkIfNeedAndFlushToDisk(size_t new_rows_in_last_mark);\n \n-    IDataType::OutputStreamGetter createStreamGetter(const String & name, WrittenOffsetColumns & offset_columns) const;\n+    IDataType::OutputStreamGetter createStreamGetter(const NameAndTypePair & column, WrittenOffsetColumns & offset_columns) const;\n \n     using SerializationState = IDataType::SerializeBinaryBulkStatePtr;\n     using SerializationStates = std::unordered_map<String, SerializationState>;\ndiff --git a/src/Storages/MergeTree/MergeTreeIOSettings.h b/src/Storages/MergeTree/MergeTreeIOSettings.h\nindex 9e315c086813..d82aa7dd7c2c 100644\n--- a/src/Storages/MergeTree/MergeTreeIOSettings.h\n+++ b/src/Storages/MergeTree/MergeTreeIOSettings.h\n@@ -14,6 +14,8 @@ struct MergeTreeReaderSettings\n     /// If save_marks_in_cache is false, then, if marks are not in cache,\n     ///  we will load them but won't save in the cache, to avoid evicting other data.\n     bool save_marks_in_cache = false;\n+    /// Convert old-style nested (single arrays with same prefix, `n.a`, `n.b`...) to subcolumns of data type Nested.\n+    bool convert_nested_to_subcolumns = false;\n };\n \n struct MergeTreeWriterSettings\ndiff --git a/src/Storages/MergeTree/MergeTreeIndexSet.cpp b/src/Storages/MergeTree/MergeTreeIndexSet.cpp\nindex 9aaf894a0cb6..b6706367bfa0 100644\n--- a/src/Storages/MergeTree/MergeTreeIndexSet.cpp\n+++ b/src/Storages/MergeTree/MergeTreeIndexSet.cpp\n@@ -93,7 +93,7 @@ void MergeTreeIndexGranuleSet::deserializeBinary(ReadBuffer & istr)\n     {\n         const auto & column = index_sample_block.getByPosition(i);\n         const auto & type = column.type;\n-        auto new_column = type->createColumn();\n+        ColumnPtr new_column = type->createColumn();\n \n         IDataType::DeserializeBinaryBulkSettings settings;\n         settings.getter = [&](IDataType::SubstreamPath) -> ReadBuffer * { return &istr; };\n@@ -101,9 +101,9 @@ void MergeTreeIndexGranuleSet::deserializeBinary(ReadBuffer & istr)\n \n         IDataType::DeserializeBinaryBulkStatePtr state;\n         type->deserializeBinaryBulkStatePrefix(settings, state);\n-        type->deserializeBinaryBulkWithMultipleStreams(*new_column, rows_to_read, settings, state);\n+        type->deserializeBinaryBulkWithMultipleStreams(new_column, rows_to_read, settings, state);\n \n-        block.insert(ColumnWithTypeAndName(new_column->getPtr(), type, column.name));\n+        block.insert(ColumnWithTypeAndName(new_column, type, column.name));\n     }\n }\n \ndiff --git a/src/Storages/MergeTree/MergeTreeReaderCompact.cpp b/src/Storages/MergeTree/MergeTreeReaderCompact.cpp\nindex 87b3f0a43298..635c59cf19a2 100644\n--- a/src/Storages/MergeTree/MergeTreeReaderCompact.cpp\n+++ b/src/Storages/MergeTree/MergeTreeReaderCompact.cpp\n@@ -53,14 +53,14 @@ MergeTreeReaderCompact::MergeTreeReaderCompact(\n         auto name_and_type = columns.begin();\n         for (size_t i = 0; i < columns_num; ++i, ++name_and_type)\n         {\n-            const auto & [name, type] = getColumnFromPart(*name_and_type);\n-            auto position = data_part->getColumnPosition(name);\n+            auto column_from_part = getColumnFromPart(*name_and_type);\n \n-            if (!position && typeid_cast<const DataTypeArray *>(type.get()))\n+            auto position = data_part->getColumnPosition(column_from_part.name);\n+            if (!position && typeid_cast<const DataTypeArray *>(column_from_part.type.get()))\n             {\n                 /// If array of Nested column is missing in part,\n                 /// we have to read its offsets if they exist.\n-                position = findColumnForOffsets(name);\n+                position = findColumnForOffsets(column_from_part.name);\n                 read_only_offsets[i] = (position != std::nullopt);\n             }\n \n@@ -133,10 +133,8 @@ size_t MergeTreeReaderCompact::readRows(size_t from_mark, bool continue_reading,\n         if (!column_positions[i])\n             continue;\n \n-        bool append = res_columns[i] != nullptr;\n-        if (!append)\n+        if (res_columns[i] == nullptr)\n             res_columns[i] = getColumnFromPart(*column_it).type->createColumn();\n-        mutable_columns[i] = res_columns[i]->assumeMutable();\n     }\n \n     while (read_rows < max_rows_to_read)\n@@ -146,20 +144,18 @@ size_t MergeTreeReaderCompact::readRows(size_t from_mark, bool continue_reading,\n         auto name_and_type = columns.begin();\n         for (size_t pos = 0; pos < num_columns; ++pos, ++name_and_type)\n         {\n+            auto column_from_part = getColumnFromPart(*name_and_type);\n             if (!res_columns[pos])\n                 continue;\n \n-            auto [name, type] = getColumnFromPart(*name_and_type);\n-            auto & column = mutable_columns[pos];\n-\n             try\n             {\n+                auto & column = res_columns[pos];\n                 size_t column_size_before_reading = column->size();\n \n-                readData(name, *column, *type, from_mark, *column_positions[pos], rows_to_read, read_only_offsets[pos]);\n+                readData(column_from_part, column, from_mark, *column_positions[pos], rows_to_read, read_only_offsets[pos]);\n \n                 size_t read_rows_in_column = column->size() - column_size_before_reading;\n-\n                 if (read_rows_in_column < rows_to_read)\n                     throw Exception(\"Cannot read all data in MergeTreeReaderCompact. Rows read: \" + toString(read_rows_in_column) +\n                         \". Rows expected: \" + toString(rows_to_read) + \".\", ErrorCodes::CANNOT_READ_ALL_DATA);\n@@ -170,7 +166,7 @@ size_t MergeTreeReaderCompact::readRows(size_t from_mark, bool continue_reading,\n                     storage.reportBrokenPart(data_part->name);\n \n                 /// Better diagnostics.\n-                e.addMessage(\"(while reading column \" + name + \")\");\n+                e.addMessage(\"(while reading column \" + column_from_part.name + \")\");\n                 throw;\n             }\n             catch (...)\n@@ -184,24 +180,17 @@ size_t MergeTreeReaderCompact::readRows(size_t from_mark, bool continue_reading,\n         read_rows += rows_to_read;\n     }\n \n-    for (size_t i = 0; i < num_columns; ++i)\n-    {\n-        auto & column = mutable_columns[i];\n-        if (column && !column->empty())\n-            res_columns[i] = std::move(column);\n-        else\n-            res_columns[i] = nullptr;\n-    }\n-\n     next_mark = from_mark;\n \n     return read_rows;\n }\n \n void MergeTreeReaderCompact::readData(\n-    const String & name, IColumn & column, const IDataType & type,\n+    const NameAndTypePair & name_and_type, ColumnPtr & column,\n     size_t from_mark, size_t column_position, size_t rows_to_read, bool only_offsets)\n {\n+    const auto & [name, type] = name_and_type;\n+\n     if (!isContinuousReading(from_mark, column_position))\n         seekToMark(from_mark, column_position);\n \n@@ -213,14 +202,25 @@ void MergeTreeReaderCompact::readData(\n         return data_buffer;\n     };\n \n+    IDataType::DeserializeBinaryBulkStatePtr state;\n     IDataType::DeserializeBinaryBulkSettings deserialize_settings;\n     deserialize_settings.getter = buffer_getter;\n     deserialize_settings.avg_value_size_hint = avg_value_size_hints[name];\n-    deserialize_settings.position_independent_encoding = true;\n \n-    IDataType::DeserializeBinaryBulkStatePtr state;\n-    type.deserializeBinaryBulkStatePrefix(deserialize_settings, state);\n-    type.deserializeBinaryBulkWithMultipleStreams(column, rows_to_read, deserialize_settings, state);\n+    if (name_and_type.isSubcolumn())\n+    {\n+        auto type_in_storage = name_and_type.getTypeInStorage();\n+        ColumnPtr temp_column = type_in_storage->createColumn();\n+\n+        type_in_storage->deserializeBinaryBulkStatePrefix(deserialize_settings, state);\n+        type_in_storage->deserializeBinaryBulkWithMultipleStreams(temp_column, rows_to_read, deserialize_settings, state);\n+        column = type_in_storage->getSubcolumn(name_and_type.getSubcolumnName(), *temp_column);\n+    }\n+    else\n+    {\n+        type->deserializeBinaryBulkStatePrefix(deserialize_settings, state);\n+        type->deserializeBinaryBulkWithMultipleStreams(column, rows_to_read, deserialize_settings, state);\n+    }\n \n     /// The buffer is left in inconsistent state after reading single offsets\n     if (only_offsets)\ndiff --git a/src/Storages/MergeTree/MergeTreeReaderCompact.h b/src/Storages/MergeTree/MergeTreeReaderCompact.h\nindex 9ef887165791..dbfaa7868fac 100644\n--- a/src/Storages/MergeTree/MergeTreeReaderCompact.h\n+++ b/src/Storages/MergeTree/MergeTreeReaderCompact.h\n@@ -56,8 +56,8 @@ class MergeTreeReaderCompact : public IMergeTreeReader\n \n     void seekToMark(size_t row_index, size_t column_index);\n \n-    void readData(const String & name, IColumn & column, const IDataType & type,\n-        size_t from_mark, size_t column_position, size_t rows_to_read, bool only_offsets = false);\n+    void readData(const NameAndTypePair & name_and_type, ColumnPtr & column, size_t from_mark,\n+        size_t column_position, size_t rows_to_read, bool only_offsets);\n \n     /// Returns maximal value of granule size in compressed file from @mark_ranges.\n     /// This value is used as size of read buffer.\ndiff --git a/src/Storages/MergeTree/MergeTreeReaderInMemory.cpp b/src/Storages/MergeTree/MergeTreeReaderInMemory.cpp\nindex e684205658a3..5ee4aa555e6c 100644\n--- a/src/Storages/MergeTree/MergeTreeReaderInMemory.cpp\n+++ b/src/Storages/MergeTree/MergeTreeReaderInMemory.cpp\n@@ -12,6 +12,7 @@ namespace ErrorCodes\n {\n     extern const int CANNOT_READ_ALL_DATA;\n     extern const int ARGUMENT_OUT_OF_BOUND;\n+    extern const int LOGICAL_ERROR;\n }\n \n \n@@ -38,6 +39,19 @@ MergeTreeReaderInMemory::MergeTreeReaderInMemory(\n     }\n }\n \n+static ColumnPtr getColumnFromBlock(const Block & block, const NameAndTypePair & name_and_type)\n+{\n+    auto storage_name = name_and_type.getNameInStorage();\n+    if (!block.has(storage_name))\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Not found column '{}' in block\", storage_name);\n+\n+    const auto & column = block.getByName(storage_name).column;\n+    if (name_and_type.isSubcolumn())\n+        return name_and_type.getTypeInStorage()->getSubcolumn(name_and_type.getSubcolumnName(), *column);\n+\n+    return column;\n+}\n+\n size_t MergeTreeReaderInMemory::readRows(size_t from_mark, bool continue_reading, size_t max_rows_to_read, Columns & res_columns)\n {\n     if (!continue_reading)\n@@ -60,17 +74,17 @@ size_t MergeTreeReaderInMemory::readRows(size_t from_mark, bool continue_reading\n     auto column_it = columns.begin();\n     for (size_t i = 0; i < num_columns; ++i, ++column_it)\n     {\n-        auto [name, type] = getColumnFromPart(*column_it);\n+        auto name_type = getColumnFromPart(*column_it);\n \n         /// Copy offsets, if array of Nested column is missing in part.\n-        auto offsets_it = positions_for_offsets.find(name);\n-        if (offsets_it != positions_for_offsets.end())\n+        auto offsets_it = positions_for_offsets.find(name_type.name);\n+        if (offsets_it != positions_for_offsets.end() && !name_type.isSubcolumn())\n         {\n             const auto & source_offsets = assert_cast<const ColumnArray &>(\n                 *part_in_memory->block.getByPosition(offsets_it->second).column).getOffsets();\n \n             if (res_columns[i] == nullptr)\n-                res_columns[i] = type->createColumn();\n+                res_columns[i] = name_type.type->createColumn();\n \n             auto mutable_column = res_columns[i]->assumeMutable();\n             auto & res_offstes = assert_cast<ColumnArray &>(*mutable_column).getOffsets();\n@@ -80,9 +94,9 @@ size_t MergeTreeReaderInMemory::readRows(size_t from_mark, bool continue_reading\n \n             res_columns[i] = std::move(mutable_column);\n         }\n-        else if (part_in_memory->block.has(name))\n+        else if (part_in_memory->hasColumnFiles(name_type))\n         {\n-            const auto & block_column = part_in_memory->block.getByName(name).column;\n+            auto block_column = getColumnFromBlock(part_in_memory->block, name_type);\n             if (rows_to_read == part_rows)\n             {\n                 res_columns[i] = block_column;\n@@ -90,7 +104,7 @@ size_t MergeTreeReaderInMemory::readRows(size_t from_mark, bool continue_reading\n             else\n             {\n                 if (res_columns[i] == nullptr)\n-                    res_columns[i] = type->createColumn();\n+                    res_columns[i] = name_type.type->createColumn();\n \n                 auto mutable_column = res_columns[i]->assumeMutable();\n                 mutable_column->insertRangeFrom(*block_column, total_rows_read, rows_to_read);\ndiff --git a/src/Storages/MergeTree/MergeTreeReaderWide.cpp b/src/Storages/MergeTree/MergeTreeReaderWide.cpp\nindex 1dacdacbae09..30db54fc8e06 100644\n--- a/src/Storages/MergeTree/MergeTreeReaderWide.cpp\n+++ b/src/Storages/MergeTree/MergeTreeReaderWide.cpp\n@@ -9,7 +9,6 @@\n #include <Common/escapeForFileName.h>\n #include <Common/typeid_cast.h>\n \n-\n namespace DB\n {\n \n@@ -50,7 +49,7 @@ MergeTreeReaderWide::MergeTreeReaderWide(\n         for (const NameAndTypePair & column : columns)\n         {\n             auto column_from_part = getColumnFromPart(column);\n-            addStreams(column_from_part.name, *column_from_part.type, profile_callback_, clock_type_);\n+            addStreams(column_from_part, profile_callback_, clock_type_);\n         }\n     }\n     catch (...)\n@@ -73,48 +72,26 @@ size_t MergeTreeReaderWide::readRows(size_t from_mark, bool continue_reading, si\n         /// If append is true, then the value will be equal to nullptr and will be used only to\n         /// check that the offsets column has been already read.\n         OffsetColumns offset_columns;\n+        std::unordered_map<String, IDataType::SubstreamsCache> caches;\n \n         auto name_and_type = columns.begin();\n         for (size_t pos = 0; pos < num_columns; ++pos, ++name_and_type)\n         {\n-            auto [name, type] = getColumnFromPart(*name_and_type);\n+            auto column_from_part = getColumnFromPart(*name_and_type);\n+            const auto & [name, type] = column_from_part;\n \n             /// The column is already present in the block so we will append the values to the end.\n             bool append = res_columns[pos] != nullptr;\n             if (!append)\n                 res_columns[pos] = type->createColumn();\n \n-            /// To keep offsets shared. TODO Very dangerous. Get rid of this.\n-            MutableColumnPtr column = res_columns[pos]->assumeMutable();\n-\n-            bool read_offsets = true;\n-\n-            /// For nested data structures collect pointers to offset columns.\n-            if (const auto * type_arr = typeid_cast<const DataTypeArray *>(type.get()))\n-            {\n-                String table_name = Nested::extractTableName(name);\n-\n-                auto it_inserted = offset_columns.emplace(table_name, nullptr);\n-\n-                /// offsets have already been read on the previous iteration and we don't need to read it again\n-                if (!it_inserted.second)\n-                    read_offsets = false;\n-\n-                /// need to create new offsets\n-                if (it_inserted.second && !append)\n-                    it_inserted.first->second = ColumnArray::ColumnOffsets::create();\n-\n-                /// share offsets in all elements of nested structure\n-                if (!append)\n-                    column = ColumnArray::create(type_arr->getNestedType()->createColumn(),\n-                                                 it_inserted.first->second)->assumeMutable();\n-            }\n-\n+            auto & column = res_columns[pos];\n             try\n             {\n                 size_t column_size_before_reading = column->size();\n+                auto & cache = caches[column_from_part.getNameInStorage()];\n \n-                readData(name, *type, *column, from_mark, continue_reading, max_rows_to_read, read_offsets);\n+                readData(column_from_part, column, from_mark, continue_reading, max_rows_to_read, cache);\n \n                 /// For elements of Nested, column_size_before_reading may be greater than column size\n                 ///  if offsets are not empty and were already read, but elements are empty.\n@@ -130,8 +107,6 @@ size_t MergeTreeReaderWide::readRows(size_t from_mark, bool continue_reading, si\n \n             if (column->empty())\n                 res_columns[pos] = nullptr;\n-            else\n-                res_columns[pos] = std::move(column);\n         }\n \n         /// NOTE: positions for all streams must be kept in sync.\n@@ -159,12 +134,12 @@ size_t MergeTreeReaderWide::readRows(size_t from_mark, bool continue_reading, si\n     return read_rows;\n }\n \n-void MergeTreeReaderWide::addStreams(const String & name, const IDataType & type,\n+void MergeTreeReaderWide::addStreams(const NameAndTypePair & name_and_type,\n     const ReadBufferFromFileBase::ProfileCallback & profile_callback, clockid_t clock_type)\n {\n     IDataType::StreamCallback callback = [&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n     {\n-        String stream_name = IDataType::getFileNameForStream(name, substream_path);\n+        String stream_name = IDataType::getFileNameForStream(name_and_type, substream_path);\n \n         if (streams.count(stream_name))\n             return;\n@@ -186,24 +161,24 @@ void MergeTreeReaderWide::addStreams(const String & name, const IDataType & type\n     };\n \n     IDataType::SubstreamPath substream_path;\n-    type.enumerateStreams(callback, substream_path);\n+    name_and_type.type->enumerateStreams(callback, substream_path);\n }\n \n \n void MergeTreeReaderWide::readData(\n-    const String & name, const IDataType & type, IColumn & column,\n+    const NameAndTypePair & name_and_type, ColumnPtr & column,\n     size_t from_mark, bool continue_reading, size_t max_rows_to_read,\n-    bool with_offsets)\n+    IDataType::SubstreamsCache & cache)\n {\n     auto get_stream_getter = [&](bool stream_for_prefix) -> IDataType::InputStreamGetter\n     {\n         return [&, stream_for_prefix](const IDataType::SubstreamPath & substream_path) -> ReadBuffer *\n         {\n-            /// If offsets for arrays have already been read.\n-            if (!with_offsets && substream_path.size() == 1 && substream_path[0].type == IDataType::Substream::ArraySizes)\n+            /// If substream have already been read.\n+            if (cache.count(IDataType::getSubcolumnNameForStream(substream_path)))\n                 return nullptr;\n \n-            String stream_name = IDataType::getFileNameForStream(name, substream_path);\n+            String stream_name = IDataType::getFileNameForStream(name_and_type, substream_path);\n \n             auto it = streams.find(stream_name);\n             if (it == streams.end())\n@@ -223,21 +198,21 @@ void MergeTreeReaderWide::readData(\n         };\n     };\n \n-    double & avg_value_size_hint = avg_value_size_hints[name];\n+    double & avg_value_size_hint = avg_value_size_hints[name_and_type.name];\n     IDataType::DeserializeBinaryBulkSettings deserialize_settings;\n     deserialize_settings.avg_value_size_hint = avg_value_size_hint;\n \n-    if (deserialize_binary_bulk_state_map.count(name) == 0)\n+    if (deserialize_binary_bulk_state_map.count(name_and_type.name) == 0)\n     {\n         deserialize_settings.getter = get_stream_getter(true);\n-        type.deserializeBinaryBulkStatePrefix(deserialize_settings, deserialize_binary_bulk_state_map[name]);\n+        name_and_type.type->deserializeBinaryBulkStatePrefix(deserialize_settings, deserialize_binary_bulk_state_map[name_and_type.name]);\n     }\n \n     deserialize_settings.getter = get_stream_getter(false);\n     deserialize_settings.continuous_reading = continue_reading;\n-    auto & deserialize_state = deserialize_binary_bulk_state_map[name];\n-    type.deserializeBinaryBulkWithMultipleStreams(column, max_rows_to_read, deserialize_settings, deserialize_state);\n-    IDataType::updateAvgValueSizeHint(column, avg_value_size_hint);\n+    auto & deserialize_state = deserialize_binary_bulk_state_map[name_and_type.name];\n+    name_and_type.type->deserializeBinaryBulkWithMultipleStreams(column, max_rows_to_read, deserialize_settings, deserialize_state, &cache);\n+    IDataType::updateAvgValueSizeHint(*column, avg_value_size_hint);\n }\n \n }\ndiff --git a/src/Storages/MergeTree/MergeTreeReaderWide.h b/src/Storages/MergeTree/MergeTreeReaderWide.h\nindex 69652d1e954c..bf9e97035d02 100644\n--- a/src/Storages/MergeTree/MergeTreeReaderWide.h\n+++ b/src/Storages/MergeTree/MergeTreeReaderWide.h\n@@ -37,13 +37,13 @@ class MergeTreeReaderWide : public IMergeTreeReader\n \n     FileStreams streams;\n \n-    void addStreams(const String & name, const IDataType & type,\n+    void addStreams(const NameAndTypePair & name_and_type,\n         const ReadBufferFromFileBase::ProfileCallback & profile_callback, clockid_t clock_type);\n \n     void readData(\n-        const String & name, const IDataType & type, IColumn & column,\n+        const NameAndTypePair & name_and_type, ColumnPtr & column,\n         size_t from_mark, bool continue_reading, size_t max_rows_to_read,\n-        bool with_offsets = true);\n+        IDataType::SubstreamsCache & cache);\n };\n \n }\ndiff --git a/src/Storages/MergeTree/checkDataPart.cpp b/src/Storages/MergeTree/checkDataPart.cpp\nindex 2838c8eb8812..c9da156dc975 100644\n--- a/src/Storages/MergeTree/checkDataPart.cpp\n+++ b/src/Storages/MergeTree/checkDataPart.cpp\n@@ -122,7 +122,7 @@ IMergeTreeDataPart::Checksums checkDataPart(\n         {\n             column.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n             {\n-                String file_name = IDataType::getFileNameForStream(column.name, substream_path) + \".bin\";\n+                String file_name = IDataType::getFileNameForStream(column, substream_path) + \".bin\";\n                 checksums_data.files[file_name] = checksum_compressed_file(disk, path + file_name);\n             }, {});\n         }\ndiff --git a/src/Storages/StorageBuffer.cpp b/src/Storages/StorageBuffer.cpp\nindex 3bc89053c741..e0657f31de5c 100644\n--- a/src/Storages/StorageBuffer.cpp\n+++ b/src/Storages/StorageBuffer.cpp\n@@ -95,7 +95,7 @@ class BufferSource : public SourceWithProgress\n     BufferSource(const Names & column_names_, StorageBuffer::Buffer & buffer_, const StorageBuffer & storage, const StorageMetadataPtr & metadata_snapshot)\n         : SourceWithProgress(\n             metadata_snapshot->getSampleBlockForColumns(column_names_, storage.getVirtuals(), storage.getStorageID()))\n-        , column_names(column_names_.begin(), column_names_.end())\n+        , column_names_and_types(metadata_snapshot->getColumns().getAllWithSubcolumns().addTypes(column_names_))\n         , buffer(buffer_) {}\n \n     String getName() const override { return \"Buffer\"; }\n@@ -115,10 +115,16 @@ class BufferSource : public SourceWithProgress\n             return res;\n \n         Columns columns;\n-        columns.reserve(column_names.size());\n+        columns.reserve(column_names_and_types.size());\n \n-        for (const auto & name : column_names)\n-            columns.push_back(buffer.data.getByName(name).column);\n+        for (const auto & elem : column_names_and_types)\n+        {\n+            const auto & current_column = buffer.data.getByName(elem.getNameInStorage()).column;\n+            if (elem.isSubcolumn())\n+                columns.emplace_back(elem.getTypeInStorage()->getSubcolumn(elem.getSubcolumnName(), *current_column));\n+            else\n+                columns.emplace_back(std::move(current_column));\n+        }\n \n         UInt64 size = columns.at(0)->size();\n         res.setColumns(std::move(columns), size);\n@@ -127,7 +133,7 @@ class BufferSource : public SourceWithProgress\n     }\n \n private:\n-    Names column_names;\n+    NamesAndTypesList column_names_and_types;\n     StorageBuffer::Buffer & buffer;\n     bool has_been_read = false;\n };\n@@ -188,8 +194,8 @@ void StorageBuffer::read(\n         {\n             const auto & dest_columns = destination_metadata_snapshot->getColumns();\n             const auto & our_columns = metadata_snapshot->getColumns();\n-            return dest_columns.hasPhysical(column_name) &&\n-                   dest_columns.get(column_name).type->equals(*our_columns.get(column_name).type);\n+            return dest_columns.hasPhysicalOrSubcolumn(column_name) &&\n+                   dest_columns.getPhysicalOrSubcolumn(column_name).type->equals(*our_columns.getPhysicalOrSubcolumn(column_name).type);\n         });\n \n         if (dst_has_same_structure)\ndiff --git a/src/Storages/StorageBuffer.h b/src/Storages/StorageBuffer.h\nindex ed8a405298b1..9656c78637bc 100644\n--- a/src/Storages/StorageBuffer.h\n+++ b/src/Storages/StorageBuffer.h\n@@ -76,6 +76,8 @@ friend class BufferBlockOutputStream;\n \n     bool supportsParallelInsert() const override { return true; }\n \n+    bool supportsSubcolumns() const override { return true; }\n+\n     BlockOutputStreamPtr write(const ASTPtr & query, const StorageMetadataPtr & /*metadata_snapshot*/, const Context & context) override;\n \n     void startup() override;\ndiff --git a/src/Storages/StorageInMemoryMetadata.cpp b/src/Storages/StorageInMemoryMetadata.cpp\nindex a4500e2aa7bc..2f488ce36c62 100644\n--- a/src/Storages/StorageInMemoryMetadata.cpp\n+++ b/src/Storages/StorageInMemoryMetadata.cpp\n@@ -3,7 +3,10 @@\n #include <sparsehash/dense_hash_map>\n #include <sparsehash/dense_hash_set>\n #include <Common/quoteString.h>\n+#include <Common/StringUtils/StringUtils.h>\n #include <Core/ColumnWithTypeAndName.h>\n+#include <IO/ReadBufferFromString.h>\n+#include <IO/ReadHelpers.h>\n #include <IO/Operators.h>\n \n \n@@ -270,7 +273,7 @@ Block StorageInMemoryMetadata::getSampleBlockForColumns(\n \n     std::unordered_map<String, DataTypePtr> columns_map;\n \n-    NamesAndTypesList all_columns = getColumns().getAll();\n+    auto all_columns = getColumns().getAllWithSubcolumns();\n     for (const auto & elem : all_columns)\n         columns_map.emplace(elem.name, elem.type);\n \n@@ -459,7 +462,7 @@ namespace\n \n void StorageInMemoryMetadata::check(const Names & column_names, const NamesAndTypesList & virtuals, const StorageID & storage_id) const\n {\n-    NamesAndTypesList available_columns = getColumns().getAllPhysical();\n+    NamesAndTypesList available_columns = getColumns().getAllPhysicalWithSubcolumns();\n     available_columns.insert(available_columns.end(), virtuals.begin(), virtuals.end());\n \n     const String list_of_columns = listOfColumns(available_columns);\ndiff --git a/src/Storages/StorageLog.cpp b/src/Storages/StorageLog.cpp\nindex 06e9bb8a2d6d..02172517eb1d 100644\n--- a/src/Storages/StorageLog.cpp\n+++ b/src/Storages/StorageLog.cpp\n@@ -47,7 +47,6 @@ namespace ErrorCodes\n     extern const int INCORRECT_FILE_NAME;\n }\n \n-\n class LogSource final : public SourceWithProgress\n {\n public:\n@@ -58,7 +57,7 @@ class LogSource final : public SourceWithProgress\n         for (const auto & name_type : columns)\n             res.insert({ name_type.type->createColumn(), name_type.type, name_type.name });\n \n-        return Nested::flatten(res);\n+        return res;\n     }\n \n     LogSource(\n@@ -91,8 +90,8 @@ class LogSource final : public SourceWithProgress\n     struct Stream\n     {\n         Stream(const DiskPtr & disk, const String & data_path, size_t offset, size_t max_read_buffer_size_)\n-            : plain(disk->readFile(data_path, std::min(max_read_buffer_size_, disk->getFileSize(data_path)))),\n-            compressed(*plain)\n+            : plain(disk->readFile(data_path, std::min(max_read_buffer_size_, disk->getFileSize(data_path))))\n+            , compressed(*plain)\n         {\n             if (offset)\n                 plain->seek(offset, SEEK_SET);\n@@ -109,7 +108,7 @@ class LogSource final : public SourceWithProgress\n     using DeserializeStates = std::map<String, DeserializeState>;\n     DeserializeStates deserialize_states;\n \n-    void readData(const String & name, const IDataType & type, IColumn & column, size_t max_rows_to_read);\n+    void readData(const NameAndTypePair & name_and_type, ColumnPtr & column, size_t max_rows_to_read, IDataType::SubstreamsCache & cache);\n };\n \n \n@@ -125,14 +124,15 @@ Chunk LogSource::generate()\n \n     /// How many rows to read for the next block.\n     size_t max_rows_to_read = std::min(block_size, rows_limit - rows_read);\n+    std::unordered_map<String, IDataType::SubstreamsCache> caches;\n \n     for (const auto & name_type : columns)\n     {\n-        MutableColumnPtr column = name_type.type->createColumn();\n-\n+        ColumnPtr column;\n         try\n         {\n-            readData(name_type.name, *name_type.type, *column, max_rows_to_read);\n+            column = name_type.type->createColumn();\n+            readData(name_type, column, max_rows_to_read, caches[name_type.getNameInStorage()]);\n         }\n         catch (Exception & e)\n         {\n@@ -156,22 +156,25 @@ Chunk LogSource::generate()\n         streams.clear();\n     }\n \n-    res = Nested::flatten(res);\n     UInt64 num_rows = res.rows();\n     return Chunk(res.getColumns(), num_rows);\n }\n \n \n-void LogSource::readData(const String & name, const IDataType & type, IColumn & column, size_t max_rows_to_read)\n+void LogSource::readData(const NameAndTypePair & name_and_type, ColumnPtr & column,\n+    size_t max_rows_to_read, IDataType::SubstreamsCache & cache)\n {\n     IDataType::DeserializeBinaryBulkSettings settings; /// TODO Use avg_value_size_hint.\n+    const auto & [name, type] = name_and_type;\n \n-    auto create_string_getter = [&](bool stream_for_prefix)\n+    auto create_stream_getter = [&](bool stream_for_prefix)\n     {\n         return [&, stream_for_prefix] (const IDataType::SubstreamPath & path) -> ReadBuffer *\n         {\n-            String stream_name = IDataType::getFileNameForStream(name, path);\n+            if (cache.count(IDataType::getSubcolumnNameForStream(path)))\n+                return nullptr;\n \n+            String stream_name = IDataType::getFileNameForStream(name_and_type, path);\n             const auto & file_it = storage.files.find(stream_name);\n             if (storage.files.end() == file_it)\n                 throw Exception(\"Logical error: no information about file \" + stream_name + \" in StorageLog\", ErrorCodes::LOGICAL_ERROR);\n@@ -182,18 +185,19 @@ void LogSource::readData(const String & name, const IDataType & type, IColumn &\n \n             auto & data_file_path = file_it->second.data_file_path;\n             auto it = streams.try_emplace(stream_name, storage.disk, data_file_path, offset, max_read_buffer_size).first;\n+\n             return &it->second.compressed;\n         };\n     };\n \n     if (deserialize_states.count(name) == 0)\n     {\n-        settings.getter = create_string_getter(true);\n-        type.deserializeBinaryBulkStatePrefix(settings, deserialize_states[name]);\n+        settings.getter = create_stream_getter(true);\n+        type->deserializeBinaryBulkStatePrefix(settings, deserialize_states[name]);\n     }\n \n-    settings.getter = create_string_getter(false);\n-    type.deserializeBinaryBulkWithMultipleStreams(column, max_rows_to_read, settings, deserialize_states[name]);\n+    settings.getter = create_stream_getter(false);\n+    type->deserializeBinaryBulkWithMultipleStreams(column, max_rows_to_read, settings, deserialize_states[name], &cache);\n }\n \n \n@@ -282,9 +286,11 @@ class LogBlockOutputStream final : public IBlockOutputStream\n     using SerializeStates = std::map<String, SerializeState>;\n     SerializeStates serialize_states;\n \n-    IDataType::OutputStreamGetter createStreamGetter(const String & name, WrittenStreams & written_streams);\n+    IDataType::OutputStreamGetter createStreamGetter(const NameAndTypePair & name_and_type, WrittenStreams & written_streams);\n \n-    void writeData(const String & name, const IDataType & type, const IColumn & column,\n+    void writeData(\n+        const NameAndTypePair & name_and_type,\n+        const IColumn & column,\n         MarksForColumns & out_marks,\n         WrittenStreams & written_streams);\n \n@@ -305,7 +311,7 @@ void LogBlockOutputStream::write(const Block & block)\n     for (size_t i = 0; i < block.columns(); ++i)\n     {\n         const ColumnWithTypeAndName & column = block.safeGetByPosition(i);\n-        writeData(column.name, *column.type, *column.column, marks, written_streams);\n+        writeData(NameAndTypePair(column.name, column.type), *column.column, marks, written_streams);\n     }\n \n     writeMarks(std::move(marks));\n@@ -324,7 +330,7 @@ void LogBlockOutputStream::writeSuffix()\n         auto it = serialize_states.find(column.name);\n         if (it != serialize_states.end())\n         {\n-            settings.getter = createStreamGetter(column.name, written_streams);\n+            settings.getter = createStreamGetter(NameAndTypePair(column.name, column.type), written_streams);\n             column.type->serializeBinaryBulkStateSuffix(settings, it->second);\n         }\n     }\n@@ -350,12 +356,12 @@ void LogBlockOutputStream::writeSuffix()\n }\n \n \n-IDataType::OutputStreamGetter LogBlockOutputStream::createStreamGetter(const String & name,\n+IDataType::OutputStreamGetter LogBlockOutputStream::createStreamGetter(const NameAndTypePair & name_and_type,\n                                                                        WrittenStreams & written_streams)\n {\n     return [&] (const IDataType::SubstreamPath & path) -> WriteBuffer *\n     {\n-        String stream_name = IDataType::getFileNameForStream(name, path);\n+        String stream_name = IDataType::getFileNameForStream(name_and_type, path);\n         if (written_streams.count(stream_name))\n             return nullptr;\n \n@@ -368,14 +374,15 @@ IDataType::OutputStreamGetter LogBlockOutputStream::createStreamGetter(const Str\n }\n \n \n-void LogBlockOutputStream::writeData(const String & name, const IDataType & type, const IColumn & column,\n+void LogBlockOutputStream::writeData(const NameAndTypePair & name_and_type, const IColumn & column,\n     MarksForColumns & out_marks, WrittenStreams & written_streams)\n {\n     IDataType::SerializeBinaryBulkSettings settings;\n+    const auto & [name, type] = name_and_type;\n \n-    type.enumerateStreams([&] (const IDataType::SubstreamPath & path, const IDataType & /* substream_type */)\n+    type->enumerateStreams([&] (const IDataType::SubstreamPath & path, const IDataType & /* substream_type */)\n     {\n-        String stream_name = IDataType::getFileNameForStream(name, path);\n+        String stream_name = IDataType::getFileNameForStream(name_and_type, path);\n         if (written_streams.count(stream_name))\n             return;\n \n@@ -384,18 +391,18 @@ void LogBlockOutputStream::writeData(const String & name, const IDataType & type\n             stream_name,\n             storage.disk,\n             storage.files[stream_name].data_file_path,\n-            columns.getCodecOrDefault(name),\n+            columns.getCodecOrDefault(name_and_type.name),\n             storage.max_compress_block_size);\n     }, settings.path);\n \n-    settings.getter = createStreamGetter(name, written_streams);\n+    settings.getter = createStreamGetter(name_and_type, written_streams);\n \n     if (serialize_states.count(name) == 0)\n-         type.serializeBinaryBulkStatePrefix(settings, serialize_states[name]);\n+         type->serializeBinaryBulkStatePrefix(settings, serialize_states[name]);\n \n-    type.enumerateStreams([&] (const IDataType::SubstreamPath & path, const IDataType & /* substream_type */)\n+    type->enumerateStreams([&] (const IDataType::SubstreamPath & path, const IDataType & /* substream_type */)\n     {\n-        String stream_name = IDataType::getFileNameForStream(name, path);\n+        String stream_name = IDataType::getFileNameForStream(name_and_type, path);\n         if (written_streams.count(stream_name))\n             return;\n \n@@ -409,11 +416,11 @@ void LogBlockOutputStream::writeData(const String & name, const IDataType & type\n         out_marks.emplace_back(file.column_index, mark);\n     }, settings.path);\n \n-    type.serializeBinaryBulkWithMultipleStreams(column, 0, 0, settings, serialize_states[name]);\n+    type->serializeBinaryBulkWithMultipleStreams(column, 0, 0, settings, serialize_states[name]);\n \n-    type.enumerateStreams([&] (const IDataType::SubstreamPath & path, const IDataType & /* substream_type */)\n+    type->enumerateStreams([&] (const IDataType::SubstreamPath & path, const IDataType & /* substream_type */)\n     {\n-        String stream_name = IDataType::getFileNameForStream(name, path);\n+        String stream_name = IDataType::getFileNameForStream(name_and_type, path);\n         if (!written_streams.emplace(stream_name).second)\n             return;\n \n@@ -482,21 +489,21 @@ StorageLog::StorageLog(\n     }\n \n     for (const auto & column : storage_metadata.getColumns().getAllPhysical())\n-        addFiles(column.name, *column.type);\n+        addFiles(column);\n \n     marks_file_path = table_path + DBMS_STORAGE_LOG_MARKS_FILE_NAME;\n }\n \n \n-void StorageLog::addFiles(const String & column_name, const IDataType & type)\n+void StorageLog::addFiles(const NameAndTypePair & column)\n {\n-    if (files.end() != files.find(column_name))\n-        throw Exception(\"Duplicate column with name \" + column_name + \" in constructor of StorageLog.\",\n+    if (files.end() != files.find(column.name))\n+        throw Exception(\"Duplicate column with name \" + column.name + \" in constructor of StorageLog.\",\n             ErrorCodes::DUPLICATE_COLUMN);\n \n     IDataType::StreamCallback stream_callback = [&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n     {\n-        String stream_name = IDataType::getFileNameForStream(column_name, substream_path);\n+        String stream_name = IDataType::getFileNameForStream(column, substream_path);\n \n         if (!files.count(stream_name))\n         {\n@@ -510,7 +517,7 @@ void StorageLog::addFiles(const String & column_name, const IDataType & type)\n     };\n \n     IDataType::SubstreamPath substream_path;\n-    type.enumerateStreams(stream_callback, substream_path);\n+    column.type->enumerateStreams(stream_callback, substream_path);\n }\n \n \n@@ -583,7 +590,7 @@ void StorageLog::truncate(const ASTPtr &, const StorageMetadataPtr & metadata_sn\n     disk->clearDirectory(table_path);\n \n     for (const auto & column : metadata_snapshot->getColumns().getAllPhysical())\n-        addFiles(column.name, *column.type);\n+        addFiles(column);\n \n     file_checker = FileChecker{disk, table_path + \"sizes.json\"};\n     marks_file_path = table_path + DBMS_STORAGE_LOG_MARKS_FILE_NAME;\n@@ -593,8 +600,7 @@ void StorageLog::truncate(const ASTPtr &, const StorageMetadataPtr & metadata_sn\n const StorageLog::Marks & StorageLog::getMarksWithRealRowCount(const StorageMetadataPtr & metadata_snapshot) const\n {\n     /// There should be at least one physical column\n-    const String column_name = metadata_snapshot->getColumns().getAllPhysical().begin()->name;\n-    const auto column_type = metadata_snapshot->getColumns().getAllPhysical().begin()->type;\n+    auto column = *metadata_snapshot->getColumns().getAllPhysical().begin();\n     String filename;\n \n     /** We take marks from first column.\n@@ -602,10 +608,10 @@ const StorageLog::Marks & StorageLog::getMarksWithRealRowCount(const StorageMeta\n       * (Example: for Array data type, first stream is array sizes; and number of array sizes is the number of arrays).\n       */\n     IDataType::SubstreamPath substream_root_path;\n-    column_type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n+    column.type->enumerateStreams([&](const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n     {\n         if (filename.empty())\n-            filename = IDataType::getFileNameForStream(column_name, substream_path);\n+            filename = IDataType::getFileNameForStream(column, substream_path);\n     }, substream_root_path);\n \n     Files::const_iterator it = files.find(filename);\n@@ -640,7 +646,8 @@ Pipe StorageLog::read(\n     auto lock_timeout = getLockTimeout(context);\n     loadMarks(lock_timeout);\n \n-    NamesAndTypesList all_columns = Nested::collect(metadata_snapshot->getColumns().getAllPhysical().addTypes(column_names));\n+    auto all_columns = metadata_snapshot->getColumns().getAllWithSubcolumns().addTypes(column_names);\n+    all_columns = Nested::convertToSubcolumns(all_columns);\n \n     std::shared_lock lock(rwlock, lock_timeout);\n     if (!lock)\ndiff --git a/src/Storages/StorageLog.h b/src/Storages/StorageLog.h\nindex a88b6dfb6ff6..acb03658182e 100644\n--- a/src/Storages/StorageLog.h\n+++ b/src/Storages/StorageLog.h\n@@ -8,6 +8,7 @@\n #include <Storages/IStorage.h>\n #include <Common/FileChecker.h>\n #include <Common/escapeForFileName.h>\n+#include <Core/NamesAndTypes.h>\n \n \n namespace DB\n@@ -43,6 +44,7 @@ class StorageLog final : public ext::shared_ptr_helper<StorageLog>, public IStor\n \n     bool storesDataOnDisk() const override { return true; }\n     Strings getDataPaths() const override { return {DB::fullPath(disk, table_path)}; }\n+    bool supportsSubcolumns() const override { return true; }\n \n protected:\n     /** Attach the table with the appropriate name, along the appropriate path (with / at the end),\n@@ -93,7 +95,7 @@ class StorageLog final : public ext::shared_ptr_helper<StorageLog>, public IStor\n     String marks_file_path;\n \n     /// The order of adding files should not change: it corresponds to the order of the columns in the marks file.\n-    void addFiles(const String & column_name, const IDataType & type);\n+    void addFiles(const NameAndTypePair & column);\n \n     bool loaded_marks = false;\n \ndiff --git a/src/Storages/StorageMaterializedView.h b/src/Storages/StorageMaterializedView.h\nindex e1dd73e85806..fab9e28afe34 100644\n--- a/src/Storages/StorageMaterializedView.h\n+++ b/src/Storages/StorageMaterializedView.h\n@@ -26,6 +26,7 @@ class StorageMaterializedView final : public ext::shared_ptr_helper<StorageMater\n     bool supportsFinal() const override { return getTargetTable()->supportsFinal(); }\n     bool supportsIndexForIn() const override { return getTargetTable()->supportsIndexForIn(); }\n     bool supportsParallelInsert() const override { return getTargetTable()->supportsParallelInsert(); }\n+    bool supportsSubcolumns() const override { return getTargetTable()->supportsSubcolumns(); }\n     bool mayBenefitFromIndexForIn(const ASTPtr & left_in_operand, const Context & query_context, const StorageMetadataPtr & /* metadata_snapshot */) const override\n     {\n         auto target_table = getTargetTable();\ndiff --git a/src/Storages/StorageMemory.cpp b/src/Storages/StorageMemory.cpp\nindex 8651caecdfac..1474fbcee029 100644\n--- a/src/Storages/StorageMemory.cpp\n+++ b/src/Storages/StorageMemory.cpp\n@@ -38,7 +38,7 @@ class MemorySource : public SourceWithProgress\n         std::shared_ptr<std::atomic<size_t>> parallel_execution_index_,\n         InitializerFunc initializer_func_ = {})\n         : SourceWithProgress(metadata_snapshot->getSampleBlockForColumns(column_names_, storage.getVirtuals(), storage.getStorageID()))\n-        , column_names(std::move(column_names_))\n+        , column_names_and_types(metadata_snapshot->getColumns().getAllWithSubcolumns().addTypes(std::move(column_names_)))\n         , data(data_)\n         , parallel_execution_index(parallel_execution_index_)\n         , initializer_func(std::move(initializer_func_))\n@@ -65,11 +65,17 @@ class MemorySource : public SourceWithProgress\n \n         const Block & src = (*data)[current_index];\n         Columns columns;\n-        columns.reserve(column_names.size());\n+        columns.reserve(columns.size());\n \n         /// Add only required columns to `res`.\n-        for (const auto & name : column_names)\n-            columns.push_back(src.getByName(name).column);\n+        for (const auto & elem : column_names_and_types)\n+        {\n+            auto current_column = src.getByName(elem.getNameInStorage()).column;\n+            if (elem.isSubcolumn())\n+                columns.emplace_back(elem.getTypeInStorage()->getSubcolumn(elem.getSubcolumnName(), *current_column));\n+            else\n+                columns.emplace_back(std::move(current_column));\n+        }\n \n         return Chunk(std::move(columns), src.rows());\n     }\n@@ -87,7 +93,7 @@ class MemorySource : public SourceWithProgress\n         }\n     }\n \n-    const Names column_names;\n+    const NamesAndTypesList column_names_and_types;\n     size_t execution_index = 0;\n     std::shared_ptr<const Blocks> data;\n     std::shared_ptr<std::atomic<size_t>> parallel_execution_index;\ndiff --git a/src/Storages/StorageMemory.h b/src/Storages/StorageMemory.h\nindex 6453e6a53e2e..702cb265ea92 100644\n--- a/src/Storages/StorageMemory.h\n+++ b/src/Storages/StorageMemory.h\n@@ -41,6 +41,8 @@ friend struct ext::shared_ptr_helper<StorageMemory>;\n \n     bool supportsParallelInsert() const override { return true; }\n \n+    bool supportsSubcolumns() const override { return true; }\n+\n     BlockOutputStreamPtr write(const ASTPtr & query, const StorageMetadataPtr & metadata_snapshot, const Context & context) override;\n \n     void drop() override;\ndiff --git a/src/Storages/StorageMerge.h b/src/Storages/StorageMerge.h\nindex 223383a6975a..3ac251fbe521 100644\n--- a/src/Storages/StorageMerge.h\n+++ b/src/Storages/StorageMerge.h\n@@ -25,6 +25,7 @@ class StorageMerge final : public ext::shared_ptr_helper<StorageMerge>, public I\n     bool supportsPrewhere() const override { return true; }\n     bool supportsFinal() const override { return true; }\n     bool supportsIndexForIn() const override { return true; }\n+    bool supportsSubcolumns() const override { return true; }\n \n     QueryProcessingStage::Enum getQueryProcessingStage(const Context &, QueryProcessingStage::Enum /*to_stage*/, SelectQueryInfo &) const override;\n \ndiff --git a/src/Storages/StorageTinyLog.cpp b/src/Storages/StorageTinyLog.cpp\nindex 6e3e9c612bb6..06e2c21b1a8c 100644\n--- a/src/Storages/StorageTinyLog.cpp\n+++ b/src/Storages/StorageTinyLog.cpp\n@@ -64,7 +64,7 @@ class TinyLogSource final : public SourceWithProgress\n         for (const auto & name_type : columns)\n             res.insert({ name_type.type->createColumn(), name_type.type, name_type.name });\n \n-        return Nested::flatten(res);\n+        return res;\n     }\n \n     TinyLogSource(\n@@ -113,7 +113,7 @@ class TinyLogSource final : public SourceWithProgress\n     using DeserializeStates = std::map<String, DeserializeState>;\n     DeserializeStates deserialize_states;\n \n-    void readData(const String & name, const IDataType & type, IColumn & column, UInt64 limit);\n+    void readData(const NameAndTypePair & name_and_type, ColumnPtr & column, UInt64 limit, IDataType::SubstreamsCache & cache);\n };\n \n \n@@ -132,13 +132,14 @@ Chunk TinyLogSource::generate()\n         return {};\n     }\n \n+    std::unordered_map<String, IDataType::SubstreamsCache> caches;\n     for (const auto & name_type : columns)\n     {\n-        MutableColumnPtr column = name_type.type->createColumn();\n-\n+        ColumnPtr column;\n         try\n         {\n-            readData(name_type.name, *name_type.type, *column, block_size);\n+            column = name_type.type->createColumn();\n+            readData(name_type, column, block_size, caches[name_type.getNameInStorage()]);\n         }\n         catch (Exception & e)\n         {\n@@ -156,32 +157,36 @@ Chunk TinyLogSource::generate()\n         streams.clear();\n     }\n \n-    auto flatten = Nested::flatten(res);\n-    return Chunk(flatten.getColumns(), flatten.rows());\n+    return Chunk(res.getColumns(), res.rows());\n }\n \n \n-void TinyLogSource::readData(const String & name, const IDataType & type, IColumn & column, UInt64 limit)\n+void TinyLogSource::readData(const NameAndTypePair & name_and_type,\n+    ColumnPtr & column, UInt64 limit, IDataType::SubstreamsCache & cache)\n {\n     IDataType::DeserializeBinaryBulkSettings settings; /// TODO Use avg_value_size_hint.\n+    const auto & [name, type] = name_and_type;\n     settings.getter = [&] (const IDataType::SubstreamPath & path) -> ReadBuffer *\n     {\n-        String stream_name = IDataType::getFileNameForStream(name, path);\n+        if (cache.count(IDataType::getSubcolumnNameForStream(path)))\n+            return nullptr;\n \n-        if (!streams.count(stream_name))\n+        String stream_name = IDataType::getFileNameForStream(name_and_type, path);\n+        auto & stream = streams[stream_name];\n+        if (!stream)\n         {\n             String file_path = storage.files[stream_name].data_file_path;\n-            streams[stream_name] = std::make_unique<Stream>(\n+            stream = std::make_unique<Stream>(\n                 storage.disk, file_path, max_read_buffer_size, file_sizes[fileName(file_path)]);\n         }\n \n-        return &streams[stream_name]->compressed;\n+        return &stream->compressed;\n     };\n \n     if (deserialize_states.count(name) == 0)\n-        type.deserializeBinaryBulkStatePrefix(settings, deserialize_states[name]);\n+         type->deserializeBinaryBulkStatePrefix(settings, deserialize_states[name]);\n \n-    type.deserializeBinaryBulkWithMultipleStreams(column, limit, settings, deserialize_states[name]);\n+    type->deserializeBinaryBulkWithMultipleStreams(column, limit, settings, deserialize_states[name], &cache);\n }\n \n \n@@ -262,18 +267,18 @@ class TinyLogBlockOutputStream final : public IBlockOutputStream\n \n     using WrittenStreams = std::set<String>;\n \n-    IDataType::OutputStreamGetter createStreamGetter(const String & name, WrittenStreams & written_streams);\n-    void writeData(const String & name, const IDataType & type, const IColumn & column, WrittenStreams & written_streams);\n+    IDataType::OutputStreamGetter createStreamGetter(const NameAndTypePair & column, WrittenStreams & written_streams);\n+    void writeData(const NameAndTypePair & name_and_type, const IColumn & column, WrittenStreams & written_streams);\n };\n \n \n IDataType::OutputStreamGetter TinyLogBlockOutputStream::createStreamGetter(\n-    const String & name,\n+    const NameAndTypePair & column,\n     WrittenStreams & written_streams)\n {\n     return [&] (const IDataType::SubstreamPath & path) -> WriteBuffer *\n     {\n-        String stream_name = IDataType::getFileNameForStream(name, path);\n+        String stream_name = IDataType::getFileNameForStream(column, path);\n \n         if (!written_streams.insert(stream_name).second)\n             return nullptr;\n@@ -283,7 +288,7 @@ IDataType::OutputStreamGetter TinyLogBlockOutputStream::createStreamGetter(\n             streams[stream_name] = std::make_unique<Stream>(\n                 storage.disk,\n                 storage.files[stream_name].data_file_path,\n-                columns.getCodecOrDefault(name),\n+                columns.getCodecOrDefault(column.name),\n                 storage.max_compress_block_size);\n \n         return &streams[stream_name]->compressed;\n@@ -291,21 +296,22 @@ IDataType::OutputStreamGetter TinyLogBlockOutputStream::createStreamGetter(\n }\n \n \n-void TinyLogBlockOutputStream::writeData(const String & name, const IDataType & type, const IColumn & column, WrittenStreams & written_streams)\n+void TinyLogBlockOutputStream::writeData(const NameAndTypePair & name_and_type, const IColumn & column, WrittenStreams & written_streams)\n {\n     IDataType::SerializeBinaryBulkSettings settings;\n+    const auto & [name, type] = name_and_type;\n \n     if (serialize_states.count(name) == 0)\n     {\n         /// Some stream getters may be called form `serializeBinaryBulkStatePrefix`.\n         /// Use different WrittenStreams set, or we get nullptr for them in `serializeBinaryBulkWithMultipleStreams`\n         WrittenStreams prefix_written_streams;\n-        settings.getter = createStreamGetter(name, prefix_written_streams);\n-        type.serializeBinaryBulkStatePrefix(settings, serialize_states[name]);\n+        settings.getter = createStreamGetter(name_and_type, prefix_written_streams);\n+        type->serializeBinaryBulkStatePrefix(settings, serialize_states[name]);\n     }\n \n-    settings.getter = createStreamGetter(name, written_streams);\n-    type.serializeBinaryBulkWithMultipleStreams(column, 0, 0, settings, serialize_states[name]);\n+    settings.getter = createStreamGetter(name_and_type, written_streams);\n+    type->serializeBinaryBulkWithMultipleStreams(column, 0, 0, settings, serialize_states[name]);\n }\n \n \n@@ -328,7 +334,7 @@ void TinyLogBlockOutputStream::writeSuffix()\n         auto it = serialize_states.find(column.name);\n         if (it != serialize_states.end())\n         {\n-            settings.getter = createStreamGetter(column.name, written_streams);\n+            settings.getter = createStreamGetter(NameAndTypePair(column.name, column.type), written_streams);\n             column.type->serializeBinaryBulkStateSuffix(settings, it->second);\n         }\n     }\n@@ -360,7 +366,7 @@ void TinyLogBlockOutputStream::write(const Block & block)\n     for (size_t i = 0; i < block.columns(); ++i)\n     {\n         const ColumnWithTypeAndName & column = block.safeGetByPosition(i);\n-        writeData(column.name, *column.type, *column.column, written_streams);\n+        writeData(NameAndTypePair(column.name, column.type), *column.column, written_streams);\n     }\n }\n \n@@ -406,19 +412,20 @@ StorageTinyLog::StorageTinyLog(\n     }\n \n     for (const auto & col : storage_metadata.getColumns().getAllPhysical())\n-        addFiles(col.name, *col.type);\n+        addFiles(col);\n }\n \n \n-void StorageTinyLog::addFiles(const String & column_name, const IDataType & type)\n+void StorageTinyLog::addFiles(const NameAndTypePair & column)\n {\n-    if (files.end() != files.find(column_name))\n-        throw Exception(\"Duplicate column with name \" + column_name + \" in constructor of StorageTinyLog.\",\n+    const auto & [name, type] = column;\n+    if (files.end() != files.find(name))\n+        throw Exception(\"Duplicate column with name \" + name + \" in constructor of StorageTinyLog.\",\n             ErrorCodes::DUPLICATE_COLUMN);\n \n     IDataType::StreamCallback stream_callback = [&] (const IDataType::SubstreamPath & substream_path, const IDataType & /* substream_type */)\n     {\n-        String stream_name = IDataType::getFileNameForStream(column_name, substream_path);\n+        String stream_name = IDataType::getFileNameForStream(column, substream_path);\n         if (!files.count(stream_name))\n         {\n             ColumnData column_data;\n@@ -428,7 +435,7 @@ void StorageTinyLog::addFiles(const String & column_name, const IDataType & type\n     };\n \n     IDataType::SubstreamPath substream_path;\n-    type.enumerateStreams(stream_callback, substream_path);\n+    type->enumerateStreams(stream_callback, substream_path);\n }\n \n \n@@ -469,6 +476,8 @@ Pipe StorageTinyLog::read(\n {\n     metadata_snapshot->check(column_names, getVirtuals(), getStorageID());\n \n+    auto all_columns = metadata_snapshot->getColumns().getAllWithSubcolumns().addTypes(column_names);\n+\n     // When reading, we lock the entire storage, because we only have one file\n     // per column and can't modify it concurrently.\n     const Settings & settings = context.getSettingsRef();\n@@ -480,7 +489,7 @@ Pipe StorageTinyLog::read(\n     /// No need to hold lock while reading because we read fixed range of data that does not change while appending more data.\n     return Pipe(std::make_shared<TinyLogSource>(\n         max_block_size,\n-        Nested::collect(metadata_snapshot->getColumns().getAllPhysical().addTypes(column_names)),\n+        Nested::convertToSubcolumns(all_columns),\n         *this,\n         settings.max_read_buffer_size,\n         file_checker.getFileSizes()));\n@@ -511,7 +520,7 @@ void StorageTinyLog::truncate(\n     file_checker = FileChecker{disk, table_path + \"sizes.json\"};\n \n     for (const auto & column : metadata_snapshot->getColumns().getAllPhysical())\n-        addFiles(column.name, *column.type);\n+        addFiles(column);\n }\n \n \ndiff --git a/src/Storages/StorageTinyLog.h b/src/Storages/StorageTinyLog.h\nindex 1398af24f828..b76e8e34dfb8 100644\n--- a/src/Storages/StorageTinyLog.h\n+++ b/src/Storages/StorageTinyLog.h\n@@ -41,6 +41,7 @@ class StorageTinyLog final : public ext::shared_ptr_helper<StorageTinyLog>, publ\n \n     bool storesDataOnDisk() const override { return true; }\n     Strings getDataPaths() const override { return {DB::fullPath(disk, table_path)}; }\n+    bool supportsSubcolumns() const override { return true; }\n \n     void truncate(const ASTPtr &, const StorageMetadataPtr & metadata_snapshot, const Context &, TableExclusiveLockHolder &) override;\n \n@@ -73,7 +74,7 @@ class StorageTinyLog final : public ext::shared_ptr_helper<StorageTinyLog>, publ\n \n     Poco::Logger * log;\n \n-    void addFiles(const String & column_name, const IDataType & type);\n+    void addFiles(const NameAndTypePair & column);\n };\n \n }\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01475_read_subcolumns.reference b/tests/queries/0_stateless/01475_read_subcolumns.reference\nnew file mode 100644\nindex 000000000000..336c11fe775c\n--- /dev/null\n+++ b/tests/queries/0_stateless/01475_read_subcolumns.reference\n@@ -0,0 +1,21 @@\n+====array====\n+1\n+0\n+3\n+2\n+2\n+====tuple====\n+foo\n+bar\n+baz\n+1\n+2\n+42\n+2\n+2\n+====nullable====\n+0\n+1\n+0\n+1\n+2\ndiff --git a/tests/queries/0_stateless/01475_read_subcolumns.sql b/tests/queries/0_stateless/01475_read_subcolumns.sql\nnew file mode 100644\nindex 000000000000..06b2d3795def\n--- /dev/null\n+++ b/tests/queries/0_stateless/01475_read_subcolumns.sql\n@@ -0,0 +1,44 @@\n+SELECT '====array====';\n+DROP TABLE IF EXISTS t_arr;\n+CREATE TABLE t_arr (a Array(UInt32)) ENGINE = MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part = 0;\n+INSERT INTO t_arr VALUES ([1]) ([]) ([1, 2, 3]) ([1, 2]);\n+\n+SYSTEM DROP MARK CACHE;\n+SELECT a.size0 FROM t_arr;\n+\n+SYSTEM FLUSH LOGS;\n+SELECT ProfileEvents.Values[indexOf(ProfileEvents.Names, 'FileOpen')]\n+FROM system.query_log\n+WHERE (type = 'QueryFinish') AND (lower(query) LIKE lower('SELECT a.size0 FROM %t_arr%'))\n+    AND event_time > now() - INTERVAL 10 SECOND AND current_database = currentDatabase();\n+\n+SELECT '====tuple====';\n+DROP TABLE IF EXISTS t_tup;\n+CREATE TABLE t_tup (t Tuple(s String, u UInt32)) ENGINE = MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part = 0;\n+INSERT INTO t_tup VALUES (('foo', 1)) (('bar', 2)) (('baz', 42));\n+\n+SYSTEM DROP MARK CACHE;\n+SELECT t.s FROM t_tup;\n+\n+SYSTEM DROP MARK CACHE;\n+SELECT t.u FROM t_tup;\n+\n+SYSTEM FLUSH LOGS;\n+SELECT ProfileEvents.Values[indexOf(ProfileEvents.Names, 'FileOpen')]\n+FROM system.query_log\n+WHERE (type = 'QueryFinish') AND (lower(query) LIKE lower('SELECT t._ FROM %t_tup%'))\n+    AND event_time > now() - INTERVAL 10 SECOND AND current_database = currentDatabase();\n+\n+SELECT '====nullable====';\n+DROP TABLE IF EXISTS t_nul;\n+CREATE TABLE t_nul (n Nullable(UInt32)) ENGINE = MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part = 0;\n+INSERT INTO t_nul VALUES (1) (NULL) (2) (NULL);\n+\n+SYSTEM DROP MARK CACHE;\n+SELECT n.null FROM t_nul;\n+\n+SYSTEM FLUSH LOGS;\n+SELECT ProfileEvents.Values[indexOf(ProfileEvents.Names, 'FileOpen')]\n+FROM system.query_log\n+WHERE (type = 'QueryFinish') AND (lower(query) LIKE lower('SELECT n.null FROM %t_nul%'))\n+    AND event_time > now() - INTERVAL 10 SECOND AND current_database = currentDatabase();\ndiff --git a/tests/queries/0_stateless/01475_read_subcolumns_2.reference b/tests/queries/0_stateless/01475_read_subcolumns_2.reference\nnew file mode 100644\nindex 000000000000..31c929a91330\n--- /dev/null\n+++ b/tests/queries/0_stateless/01475_read_subcolumns_2.reference\n@@ -0,0 +1,10 @@\n+([1,NULL],2,'a')\t['foo',NULL,'bar']\t[['123'],['456','789']]\tqqqq\t['zzz','xxx']\t[42,43]\n+[1,NULL]\t2\ta\t['zzz','xxx']\t[42,43]\tqqqq\n+2\t[0,1]\t2\ta\t0\n+1\t3\n+[['123'],['456','789']]\t2\t[1,2]\t[[0],[0,0]]\n+([1,NULL],2,'a')\t['foo',NULL,'bar']\t[['123'],['456','789']]\tqqqq\t['zzz','xxx']\t[42,43]\n+[1,NULL]\t2\ta\t['zzz','xxx']\t[42,43]\tqqqq\n+2\t[0,1]\t2\ta\t0\n+1\t3\n+[['123'],['456','789']]\t2\t[1,2]\t[[0],[0,0]]\ndiff --git a/tests/queries/0_stateless/01475_read_subcolumns_2.sql b/tests/queries/0_stateless/01475_read_subcolumns_2.sql\nnew file mode 100644\nindex 000000000000..b8959cf27f7a\n--- /dev/null\n+++ b/tests/queries/0_stateless/01475_read_subcolumns_2.sql\n@@ -0,0 +1,49 @@\n+DROP TABLE IF EXISTS subcolumns;\n+\n+CREATE TABLE subcolumns\n+(\n+    t Tuple\n+    (\n+        a Array(Nullable(UInt32)),\n+        u UInt32,\n+        s Nullable(String)\n+    ),\n+    arr Array(Nullable(String)),\n+    arr2 Array(Array(Nullable(String))),\n+    lc LowCardinality(String),\n+    nested Nested(col1 String, col2 Nullable(UInt32))\n+) \n+ENGINE = MergeTree order by tuple() SETTINGS min_bytes_for_wide_part = '10M';\n+\n+INSERT INTO subcolumns VALUES (([1, NULL], 2, 'a'), ['foo', NULL, 'bar'], [['123'], ['456', '789']], 'qqqq', ['zzz', 'xxx'], [42, 43]);\n+SELECT * FROM subcolumns;\n+SELECT t.a, t.u, t.s, nested.col1, nested.col2, lc FROM subcolumns;\n+SELECT t.a.size0, t.a.null, t.u, t.s, t.s.null FROM subcolumns;\n+SELECT sumArray(arr.null), sum(arr.size0) FROM subcolumns;\n+SELECT arr2, arr2.size0, arr2.size1, arr2.null FROM subcolumns;\n+-- SELECT nested.col1, nested.col2, nested.col1.size0, nested.col2.size0, nested.col2.null FROM subcolumns;\n+\n+DROP TABLE IF EXISTS subcolumns;\n+\n+CREATE TABLE subcolumns\n+(\n+    t Tuple\n+    (\n+        a Array(Nullable(UInt32)),\n+        u UInt32,\n+        s Nullable(String)\n+    ),\n+    arr Array(Nullable(String)),\n+    arr2 Array(Array(Nullable(String))),\n+    lc LowCardinality(String),\n+    nested Nested(col1 String, col2 Nullable(UInt32))\n+) \n+ENGINE = MergeTree order by tuple() SETTINGS min_bytes_for_wide_part = 0;\n+\n+INSERT INTO subcolumns VALUES (([1, NULL], 2, 'a'), ['foo', NULL, 'bar'], [['123'], ['456', '789']], 'qqqq', ['zzz', 'xxx'], [42, 43]);\n+SELECT * FROM subcolumns;\n+SELECT t.a, t.u, t.s, nested.col1, nested.col2, lc FROM subcolumns;\n+SELECT t.a.size0, t.a.null, t.u, t.s, t.s.null FROM subcolumns;\n+SELECT sumArray(arr.null), sum(arr.size0) FROM subcolumns;\n+SELECT arr2, arr2.size0, arr2.size1, arr2.null FROM subcolumns;\n+-- SELECT nested.col1, nested.col2, nested.size0, nested.size0, nested.col2.null FROM subcolumns;\ndiff --git a/tests/queries/0_stateless/01475_read_subcolumns_storages.reference b/tests/queries/0_stateless/01475_read_subcolumns_storages.reference\nnew file mode 100644\nindex 000000000000..f848977a55d2\n--- /dev/null\n+++ b/tests/queries/0_stateless/01475_read_subcolumns_storages.reference\n@@ -0,0 +1,18 @@\n+Log\n+100\t[1,2,3]\t[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]\t[1,NULL,2]\t('foo',200)\n+100\t0\t[1,2,3]\t3\t[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]\t3\t[3,2,1]\t[[2,0,1],[2,2],[0]]\t[1,NULL,2]\t3\t[0,1,0]\t('foo',200)\tfoo\t200\n+TinyLog\n+100\t[1,2,3]\t[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]\t[1,NULL,2]\t('foo',200)\n+100\t0\t[1,2,3]\t3\t[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]\t3\t[3,2,1]\t[[2,0,1],[2,2],[0]]\t[1,NULL,2]\t3\t[0,1,0]\t('foo',200)\tfoo\t200\n+Memory\n+100\t[1,2,3]\t[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]\t[1,NULL,2]\t('foo',200)\n+100\t0\t[1,2,3]\t3\t[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]\t3\t[3,2,1]\t[[2,0,1],[2,2],[0]]\t[1,NULL,2]\t3\t[0,1,0]\t('foo',200)\tfoo\t200\n+MergeTree ORDER BY tuple() SETTINGS min_bytes_for_compact_part='10M'\n+100\t[1,2,3]\t[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]\t[1,NULL,2]\t('foo',200)\n+100\t0\t[1,2,3]\t3\t[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]\t3\t[3,2,1]\t[[2,0,1],[2,2],[0]]\t[1,NULL,2]\t3\t[0,1,0]\t('foo',200)\tfoo\t200\n+MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part='10M'\n+100\t[1,2,3]\t[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]\t[1,NULL,2]\t('foo',200)\n+100\t0\t[1,2,3]\t3\t[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]\t3\t[3,2,1]\t[[2,0,1],[2,2],[0]]\t[1,NULL,2]\t3\t[0,1,0]\t('foo',200)\tfoo\t200\n+MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part=0\n+100\t[1,2,3]\t[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]\t[1,NULL,2]\t('foo',200)\n+100\t0\t[1,2,3]\t3\t[[[1,2],[],[4]],[[5,6],[7,8]],[[]]]\t3\t[3,2,1]\t[[2,0,1],[2,2],[0]]\t[1,NULL,2]\t3\t[0,1,0]\t('foo',200)\tfoo\t200\ndiff --git a/tests/queries/0_stateless/01475_read_subcolumns_storages.sh b/tests/queries/0_stateless/01475_read_subcolumns_storages.sh\nnew file mode 100755\nindex 000000000000..54cd0c073b27\n--- /dev/null\n+++ b/tests/queries/0_stateless/01475_read_subcolumns_storages.sh\n@@ -0,0 +1,25 @@\n+#!/usr/bin/env bash\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+set -e\n+\n+create_query=\"CREATE TABLE subcolumns(n Nullable(UInt32), a1 Array(UInt32),\\\n+    a2 Array(Array(Array(UInt32))), a3 Array(Nullable(UInt32)), t Tuple(s String, v UInt32))\"\n+\n+# \"StripeLog\"\n+declare -a ENGINES=(\"Log\" \"TinyLog\" \"Memory\" \\\n+    \"MergeTree ORDER BY tuple() SETTINGS min_bytes_for_compact_part='10M'\" \\\n+    \"MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part='10M'\" \\\n+    \"MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part=0\")\n+\n+for engine in \"${ENGINES[@]}\"; do\n+    echo $engine\n+    $CLICKHOUSE_CLIENT --query \"DROP TABLE IF EXISTS subcolumns\"\n+    $CLICKHOUSE_CLIENT --query \"$create_query ENGINE = $engine\"\n+    $CLICKHOUSE_CLIENT --query \"INSERT INTO subcolumns VALUES (100, [1, 2, 3], [[[1, 2], [], [4]], [[5, 6], [7, 8]], [[]]], [1, NULL, 2], ('foo', 200))\"\n+    $CLICKHOUSE_CLIENT --query \"SELECT * FROM subcolumns\"\n+    $CLICKHOUSE_CLIENT --query \"SELECT n, n.null, a1, a1.size0, a2, a2.size0, a2.size1, a2.size2, a3, a3.size0, a3.null, t, t.s, t.v FROM subcolumns\"\n+done\ndiff --git a/tests/queries/0_stateless/01533_multiple_nested.reference b/tests/queries/0_stateless/01533_multiple_nested.reference\nnew file mode 100644\nindex 000000000000..ba37ce1c32ce\n--- /dev/null\n+++ b/tests/queries/0_stateless/01533_multiple_nested.reference\n@@ -0,0 +1,40 @@\n+all\n+[(1,'q'),(2,'w'),(3,'e')]\t[(4,[('a',5),('s',6),('d',7)])]\t[([(8,9),(10,11)],[('z','x'),('c','v')])]\n+[(12,'qq')]\t[(4,[]),(5,[('b',6),('n',7)])]\t[([],[]),([(44,55),(66,77)],[])]\n+col1\n+[1,2,3]\t['q','w','e']\n+[12]\t['qq']\n+col2\n+[4]\t[[('a',5),('s',6),('d',7)]]\t[['a','s','d']]\t[[5,6,7]]\n+[4,5]\t[[],[('b',6),('n',7)]]\t[[],['b','n']]\t[[],[6,7]]\n+col3\n+[[(8,9),(10,11)]]\t[[('z','x'),('c','v')]]\t[[8,10]]\t[[9,11]]\t[['z','c']]\t[['x','v']]\n+[[],[(44,55),(66,77)]]\t[[],[]]\t[[],[44,66]]\t[[],[55,77]]\t[[],[]]\t[[],[]]\n+read files\n+4\n+6\n+0\t899984\t7199412\n+1\t899987\t7199877\n+2\t899990\t7200255\n+3\t899993\t7199883\n+4\t899996\t7199798\n+5\t899999\t7200306\n+6\t900002\t7200064\n+7\t900005\t7199429\n+8\t900008\t7200067\n+9\t899992\t7199993\n+0\t[]\n+0\t[0]\n+1\t[0,2]\n+3\t[0,2,8]\n+6\t[0,2,8,18]\n+2\t3\n+0\t2\n+2\t3\n+4\t4\n+0\t0\n+0\t1\n+0\t2\n+0\t3\n+0\t1\n+1\t2\ndiff --git a/tests/queries/0_stateless/01533_multiple_nested.sql b/tests/queries/0_stateless/01533_multiple_nested.sql\nnew file mode 100644\nindex 000000000000..6374d6fca21d\n--- /dev/null\n+++ b/tests/queries/0_stateless/01533_multiple_nested.sql\n@@ -0,0 +1,66 @@\n+DROP TABLE IF EXISTS nested;\n+\n+SET flatten_nested = 0;\n+\n+CREATE TABLE nested\n+(\n+    col1 Nested(a UInt32, s String),\n+    col2 Nested(a UInt32, n Nested(s String, b UInt32)),\n+    col3 Nested(n1 Nested(a UInt32, b UInt32), n2 Nested(s String, t String))\n+)\n+ENGINE = MergeTree \n+ORDER BY tuple()\n+SETTINGS min_bytes_for_wide_part = 0;\n+\n+INSERT INTO nested VALUES ([(1, 'q'), (2, 'w'), (3, 'e')], [(4, [('a', 5), ('s', 6), ('d', 7)])], [([(8, 9), (10, 11)], [('z', 'x'), ('c', 'v')])]);\n+INSERT INTO nested VALUES ([(12, 'qq')], [(4, []), (5, [('b', 6), ('n', 7)])], [([], []), ([(44, 55), (66, 77)], [])]);\n+\n+OPTIMIZE TABLE nested FINAL;\n+\n+SELECT 'all';\n+SELECT * FROM nested;\n+SELECT 'col1';\n+SELECT col1.a, col1.s FROM nested;\n+SELECT 'col2';\n+SELECT col2.a, col2.n, col2.n.s, col2.n.b FROM nested;\n+SELECT 'col3';\n+SELECT col3.n1, col3.n2, col3.n1.a, col3.n1.b, col3.n2.s, col3.n2.t FROM nested;\n+\n+SELECT 'read files';\n+\n+SYSTEM DROP MARK CACHE;\n+SELECT col1.a FROM nested FORMAT Null;\n+\n+-- 4 files: (col1.size0, col1.a) x2\n+SYSTEM FLUSH LOGS;\n+SELECT ProfileEvents.Values[indexOf(ProfileEvents.Names, 'FileOpen')]\n+FROM system.query_log\n+WHERE (type = 'QueryFinish') AND (lower(query) LIKE lower('SELECT col1.a FROM %nested%'))\n+    AND event_time > now() - INTERVAL 10 SECOND AND current_database = currentDatabase();\n+\n+SYSTEM DROP MARK CACHE;\n+SELECT col3.n2.s FROM nested FORMAT Null;\n+\n+-- 6 files: (col3.size0, col3.n2.size1, col3.n2.s) x2\n+SYSTEM FLUSH LOGS;\n+SELECT ProfileEvents.Values[indexOf(ProfileEvents.Names, 'FileOpen')]\n+FROM system.query_log\n+WHERE (type = 'QueryFinish') AND (lower(query) LIKE lower('SELECT col3.n2.s FROM %nested%'))\n+    AND event_time > now() - INTERVAL 10 SECOND AND current_database = currentDatabase();\n+\n+DROP TABLE nested;\n+\n+CREATE TABLE nested\n+(\n+    id UInt32,\n+    col1 Nested(a UInt32, n Nested(s String, b UInt32))\n+)\n+ENGINE = MergeTree \n+ORDER BY id\n+SETTINGS min_bytes_for_wide_part = 0;\n+\n+INSERT INTO nested SELECT number, arrayMap(x -> (x, arrayMap(y -> (toString(y * x), y + x), range(number % 17))), range(number % 19)) FROM numbers(1000000);\n+SELECT id % 10, sum(length(col1)), sumArray(arrayMap(x -> length(x), col1.n.b)) FROM nested GROUP BY id % 10;\n+\n+SELECT arraySum(col1.a), arrayMap(x -> x * x * 2, col1.a) FROM nested ORDER BY id LIMIT 5;\n+SELECT untuple(arrayJoin(arrayJoin(col1.n))) FROM nested ORDER BY id LIMIT 10 OFFSET 10;\ndiff --git a/tests/queries/skip_list.json b/tests/queries/skip_list.json\nindex cfbac4639329..c2e8d1263f19 100644\n--- a/tests/queries/skip_list.json\n+++ b/tests/queries/skip_list.json\n@@ -309,7 +309,8 @@\n         \"01632_max_partitions_to_read\",\n         \"01638_div_mod_ambiguities\",\n         \"01642_if_nullable_regression\",\n-        \"01643_system_suspend\"\n+        \"01643_system_suspend\",\n+        \"01475_read_subcolumns_storages\"\n     ],\n     \"parallel\":\n     [\n",
  "problem_statement": "Cannot access named tuple element by its name using dot\n**Describe the bug**\r\nConsider a column `a` of type `Tuple(s String, i Int64)`. Currently we can access certain field by using any of the following expressions:\r\n\r\n```\r\ntupleElement(a, 1)\r\na.1\r\ntupleElement(a, 's')\r\n```\r\n\r\nbut what seems to be the most convenient way for named tuples does not work:\r\n\r\n```\r\na.s\r\n```\r\n\r\nIt would be nice to support accessing tuple elements by their names.\r\n\r\nReproduction on current Arcadia version from revision r7736838 (which is similar to github master around Jan 7th):\r\n\r\n```\r\nmax42-dev.sas.yp-c.yandex.net :) create table t(a Tuple(s String, i Int8)) engine = Log()\r\n\r\nCREATE TABLE t\r\n(\r\n    `a` Tuple(s String, i Int8)\r\n)\r\nENGINE = Log\r\n\r\nQuery id: f9cbbcf2-0db4-4312-bd7c-ee3e901bfdd7\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.004 sec.\r\n\r\nmax42-dev.sas.yp-c.yandex.net :) insert into t values (('x', 42))\r\n\r\nINSERT INTO t VALUES\r\n\r\nQuery id: acd443b2-1daa-43c3-b201-8713b0a2250a\r\n\r\nOk.\r\n\r\n1 rows in set. Elapsed: 0.008 sec.\r\n\r\nmax42-dev.sas.yp-c.yandex.net :) select a from t\r\n\r\nSELECT a\r\nFROM t\r\n\r\nQuery id: bb38d164-29b6-49f4-a35c-df9f5b0b7d42\r\n\r\n\u250c\u2500a\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 ('x',42) \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.005 sec.\r\n\r\nmax42-dev.sas.yp-c.yandex.net :) select a.s from t\r\n\r\nSELECT a.s\r\nFROM t\r\n\r\nQuery id: de107034-0831-409d-a7a3-81a60c81a360\r\n\r\n\r\nReceived exception from server (version 20.13.1):\r\nCode: 47. DB::Exception: Received from localhost:9000. DB::Exception: Missing columns: 'a.s' while processing query: 'SELECT a.s FROM t', required columns: 'a.s', maybe you meant: .\r\n\r\n0 rows in set. Elapsed: 0.003 sec.\r\n\r\nmax42-dev.sas.yp-c.yandex.net :) select tupleElement(a, 's') from t\r\n\r\nSELECT tupleElement(a, 's')\r\nFROM t\r\n\r\nQuery id: b5454a30-1c16-4b8f-a4e1-9d1fde5bdf9b\r\n\r\n\u250c\u2500tupleElement(a, 's')\u2500\u2510\r\n\u2502 x                    \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.006 sec.\r\n\r\nmax42-dev.sas.yp-c.yandex.net :) select a.1 from t\r\n\r\nSELECT a.1\r\nFROM t\r\n\r\nQuery id: 29442b78-4235-410e-bfb1-6b6c0f624fa5\r\n\r\n\u250c\u2500tupleElement(a, 1)\u2500\u2510\r\n\u2502 x                  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.006 sec.\r\n```\r\n\r\n\n",
  "hints_text": "",
  "created_at": "2020-11-23T14:25:27Z"
}