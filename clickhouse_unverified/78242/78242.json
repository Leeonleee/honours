{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 78242,
  "instance_id": "ClickHouse__ClickHouse-78242",
  "issue_numbers": [
    "77638"
  ],
  "base_commit": "3dabb67667e4061975dc4f9964cca1cc53bfc4fa",
  "patch": "diff --git a/src/Common/ProfileEvents.cpp b/src/Common/ProfileEvents.cpp\nindex bafc4846c2cb..8baaca95b898 100644\n--- a/src/Common/ProfileEvents.cpp\n+++ b/src/Common/ProfileEvents.cpp\n@@ -220,6 +220,7 @@\n     \\\n     M(IcebergPartitionPrunnedFiles, \"Number of skipped files during Iceberg partition pruning\", ValueType::Number) \\\n     M(IcebergTrivialCountOptimizationApplied, \"Trivial count optimization applied while reading from Iceberg\", ValueType::Number) \\\n+    M(IcebergMinMaxIndexPrunnedFiles, \"Number of skipped files by using MinMax index in Iceberg\", ValueType::Number) \\\n     M(JoinBuildTableRowCount, \"Total number of rows in the build table for a JOIN operation.\", ValueType::Number) \\\n     M(JoinProbeTableRowCount, \"Total number of rows in the probe table for a JOIN operation.\", ValueType::Number) \\\n     M(JoinResultRowCount, \"Total number of rows in the result of a JOIN operation.\", ValueType::Number) \\\ndiff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp b/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp\nindex e5f45afc3530..aedba7369ecd 100644\n--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp\n+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp\n@@ -18,7 +18,7 @@\n #include <Storages/ObjectStorage/DataLakes/Iceberg/Utils.h>\n #include <Storages/ObjectStorage/DataLakes/Iceberg/AvroForIcebergDeserializer.h>\n #include <Storages/ObjectStorage/DataLakes/Iceberg/Snapshot.h>\n-#include <Storages/ObjectStorage/DataLakes/Iceberg/PartitionPruning.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFilesPruning.h>\n #include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h>\n \n #include <Common/logger_useful.h>\n@@ -26,7 +26,6 @@\n \n namespace ProfileEvents\n {\n-    extern const Event IcebergPartitionPrunnedFiles;\n     extern const Event IcebergTrivialCountOptimizationApplied;\n }\n \n@@ -569,17 +568,13 @@ Strings IcebergMetadata::getDataFilesImpl(const ActionsDAG * filter_dag) const\n     Strings data_files;\n     for (const auto & manifest_file_ptr : *(relevant_snapshot->manifest_list))\n     {\n-        PartitionPruner pruner(schema_processor, relevant_snapshot_schema_id, filter_dag, *manifest_file_ptr, getContext());\n+        ManifestFilesPruner pruner(schema_processor, relevant_snapshot_schema_id, filter_dag, *manifest_file_ptr, getContext());\n         const auto & data_files_in_manifest = manifest_file_ptr->getFiles();\n         for (const auto & manifest_file_entry : data_files_in_manifest)\n         {\n             if (manifest_file_entry.status != ManifestEntryStatus::DELETED)\n             {\n-                if (pruner.canBePruned(manifest_file_entry))\n-                {\n-                    ProfileEvents::increment(ProfileEvents::IcebergPartitionPrunnedFiles);\n-                }\n-                else\n+                if (!pruner.canBePruned(manifest_file_entry))\n                 {\n                     if (std::holds_alternative<DataFileEntry>(manifest_file_entry.file))\n                         data_files.push_back(std::get<DataFileEntry>(manifest_file_entry.file).file_name);\ndiff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp\nindex b9a968f06db6..650a5c4af033 100644\n--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp\n+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp\n@@ -4,14 +4,16 @@\n \n #include <Storages/ObjectStorage/DataLakes/Iceberg/Utils.h>\n #include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h>\n-#include <Storages/ObjectStorage/DataLakes/Iceberg/PartitionPruning.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFilesPruning.h>\n \n+#include <DataTypes/DataTypesDecimal.h>\n #include <Poco/JSON/Parser.h>\n #include <Storages/ColumnsDescription.h>\n #include <Parsers/ASTFunction.h>\n #include <Common/quoteString.h>\n #include <DataTypes/DataTypeNullable.h>\n-#include <Common/logger_useful.h>\n+#include <IO/ReadBufferFromString.h>\n+#include <IO/ReadHelpers.h>\n \n namespace DB::ErrorCodes\n {\n@@ -23,6 +25,86 @@ namespace DB::ErrorCodes\n namespace Iceberg\n {\n \n+namespace\n+{\n+    /// Iceberg stores lower_bounds and upper_bounds serialized with some custom deserialization as bytes array\n+    /// https://iceberg.apache.org/spec/#appendix-d-single-value-serialization\n+    std::optional<DB::Field> deserializeFieldFromBinaryRepr(std::string str, DB::DataTypePtr expected_type, bool lower_bound)\n+    {\n+        auto non_nullable_type = DB::removeNullable(expected_type);\n+        auto column = non_nullable_type->createColumn();\n+        if (DB::WhichDataType(non_nullable_type).isDecimal())\n+        {\n+            /// Iceberg store decimal values as unscaled value with two\u2019s-complement big-endian binary\n+            /// using the minimum number of bytes for the value\n+            /// Our decimal binary representation is little endian\n+            /// so we cannot reuse our default code for parsing it.\n+            int64_t unscaled_value = 0;\n+\n+            // Convert from big-endian to signed int\n+            for (const auto byte : str)\n+                unscaled_value = (unscaled_value << 8) | static_cast<uint8_t>(byte);\n+\n+            /// Add sign\n+            if (str[0] & 0x80)\n+            {\n+                int64_t sign_extension = -1;\n+                sign_extension <<= (str.size() * 8);\n+                unscaled_value |= sign_extension;\n+            }\n+\n+            /// NOTE: It's very weird, but Decimal values for lower bound and upper bound\n+            /// are stored rounded, without fractional part. What is more strange\n+            /// the integer part is rounded mathematically correctly according to fractional part.\n+            /// Example: 17.22 -> 17, 8888.999 -> 8889, 1423.77 -> 1424.\n+            /// I've checked two implementations: Spark and Amazon Athena and both of them\n+            /// do this.\n+            ///\n+            /// The problem is -- we cannot use rounded values for lower bounds and upper bounds.\n+            /// Example: upper_bound(x) = 17.22, but it's rounded 17.00, now condition WHERE x >= 17.21 will\n+            /// check rounded value and say: \"Oh largest value is 17, so values bigger than 17.21 cannot be in this file,\n+            /// let's skip it\". But it will produce incorrect result since actual value (17.22 >= 17.21) is stored in this file.\n+            ///\n+            /// To handle this issue we subtract 1 from the integral part for lower_bound and add 1 to integral\n+            /// part of upper_bound. This produces: 17.22 -> [16.0, 18.0]. So this is more rough boundary,\n+            /// but at least it doesn't lead to incorrect results.\n+            {\n+                int64_t scaler = lower_bound ? -10 : 10;\n+                int32_t scale = DB::getDecimalScale(*non_nullable_type);\n+                while (--scale)\n+                    scaler *= 10;\n+\n+                unscaled_value += scaler;\n+            }\n+\n+            if (const auto * decimal_type = DB::checkDecimal<DB::Decimal32>(*non_nullable_type))\n+            {\n+                DB::DecimalField<DB::Decimal32> result(unscaled_value, decimal_type->getScale());\n+                return result;\n+            }\n+            if (const auto * decimal_type = DB::checkDecimal<DB::Decimal64>(*non_nullable_type))\n+            {\n+                DB::DecimalField<DB::Decimal64> result(unscaled_value, decimal_type->getScale());\n+                return result;\n+            }\n+            else\n+            {\n+                return std::nullopt;\n+            }\n+        }\n+        else\n+        {\n+            /// For all other types except decimal binary representation\n+            /// matches our internal representation\n+            column->insertData(str.data(), str.length());\n+            DB::Field result;\n+            column->get(0, result);\n+            return result;\n+        }\n+    }\n+\n+}\n+\n constexpr const char * COLUMN_STATUS_NAME = \"status\";\n constexpr const char * COLUMN_TUPLE_DATA_FILE_NAME = \"data_file\";\n constexpr const char * COLUMN_SEQ_NUMBER_NAME = \"sequence_number\";\n@@ -104,10 +186,9 @@ ManifestFileContent::ManifestFileContent(\n \n         partition_key_ast->arguments->children.emplace_back(std::move(partition_ast));\n         partition_columns_description.emplace_back(numeric_column_name, removeNullable(manifest_file_column_characteristics.type));\n-        this->partition_column_ids.push_back(source_id);\n     }\n \n-    if (!partition_column_ids.empty())\n+    if (!partition_columns_description.empty())\n         this->partition_key_description.emplace(DB::KeyDescription::getKeyFromAST(std::move(partition_key_ast), ColumnsDescription(partition_columns_description), context));\n \n     for (size_t i = 0; i < manifest_file_deserializer.rows(); ++i)\n@@ -165,6 +246,7 @@ ManifestFileContent::ManifestFileContent(\n             }\n         }\n \n+        std::unordered_map<Int32, std::pair<Field, Field>> value_for_bounds;\n         for (const auto & path : {SUBCOLUMN_LOWER_BOUNDS_NAME, SUBCOLUMN_UPPER_BOUNDS_NAME})\n         {\n             if (manifest_file_deserializer.hasPath(path))\n@@ -175,14 +257,28 @@ ManifestFileContent::ManifestFileContent(\n                     const auto & column_number_and_bound = column_stats.safeGet<Tuple>();\n                     Int32 number = column_number_and_bound[0].safeGet<Int32>();\n                     const Field & bound_value = column_number_and_bound[1];\n+\n                     if (path == SUBCOLUMN_LOWER_BOUNDS_NAME)\n-                        columns_infos[number].lower_bound = bound_value;\n+                        value_for_bounds[number].first = bound_value;\n                     else\n-                        columns_infos[number].upper_bound = bound_value;\n+                        value_for_bounds[number].second = bound_value;\n+\n+                    column_ids_which_have_bounds.insert(number);\n                 }\n             }\n         }\n \n+        for (const auto & [column_id, bounds] : value_for_bounds)\n+        {\n+            DB::NameAndTypePair name_and_type = schema_processor.getFieldCharacteristics(schema_id, column_id);\n+            auto left = deserializeFieldFromBinaryRepr(bounds.first.safeGet<std::string>(), name_and_type.type, true);\n+            auto right = deserializeFieldFromBinaryRepr(bounds.second.safeGet<std::string>(), name_and_type.type, false);\n+            if (!left || !right)\n+                continue;\n+\n+            columns_infos[column_id].hyperrectangle.emplace(*left, true, *right, true);\n+        }\n+\n         FileEntry file = FileEntry{DataFileEntry{file_path}};\n \n         Int64 added_sequence_number = 0;\n@@ -215,7 +311,7 @@ ManifestFileContent::ManifestFileContent(\n \n bool ManifestFileContent::hasPartitionKey() const\n {\n-    return !partition_column_ids.empty();\n+    return partition_key_description.has_value();\n }\n \n const DB::KeyDescription & ManifestFileContent::getPartitionKeyDescription() const\n@@ -225,9 +321,14 @@ const DB::KeyDescription & ManifestFileContent::getPartitionKeyDescription() con\n     return *(partition_key_description);\n }\n \n-const std::vector<Int32> & ManifestFileContent::getPartitionKeyColumnIDs() const\n+bool ManifestFileContent::hasBoundsInfoInManifests() const\n+{\n+    return !column_ids_which_have_bounds.empty();\n+}\n+\n+const std::set<Int32> & ManifestFileContent::getColumnsIDsWithBounds() const\n {\n-    return partition_column_ids;\n+    return column_ids_which_have_bounds;\n }\n \n std::optional<Int64> ManifestFileContent::getRowsCountInAllDataFilesExcludingDeleted() const\ndiff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h\nindex 46d4fae5bc77..83b531400326 100644\n--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h\n+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h\n@@ -7,6 +7,7 @@\n #include <Storages/ObjectStorage/DataLakes/Iceberg/SchemaProcessor.h>\n #include <Storages/ObjectStorage/DataLakes/Iceberg/AvroForIcebergDeserializer.h>\n #include <Storages/KeyDescription.h>\n+#include <Storages/MergeTree/KeyCondition.h>\n #include <Core/Field.h>\n \n #include <cstdint>\n@@ -41,8 +42,7 @@ struct ColumnInfo\n     std::optional<Int64> rows_count;\n     std::optional<Int64> bytes_size;\n     std::optional<Int64> nulls_count;\n-    std::optional<DB::Field> lower_bound;\n-    std::optional<DB::Field> upper_bound;\n+    std::optional<DB::Range> hyperrectangle;\n };\n \n using FileEntry = std::variant<DataFileEntry>; // In the future we will add PositionalDeleteFileEntry and EqualityDeleteFileEntry here\n@@ -102,21 +102,24 @@ class ManifestFileContent\n \n     bool hasPartitionKey() const;\n     const DB::KeyDescription & getPartitionKeyDescription() const;\n-    const std::vector<Int32> & getPartitionKeyColumnIDs() const;\n \n     /// Fields with rows count in manifest files are optional\n     /// they can be absent.\n     std::optional<Int64> getRowsCountInAllDataFilesExcludingDeleted() const;\n     std::optional<Int64> getBytesCountInAllDataFiles() const;\n+\n+    bool hasBoundsInfoInManifests() const;\n+    const std::set<Int32> & getColumnsIDsWithBounds() const;\n private:\n \n     Int32 schema_id;\n \n     std::optional<DB::KeyDescription> partition_key_description;\n-    std::vector<Int32> partition_column_ids;\n     // Size - number of files\n     std::vector<ManifestFileEntry> files;\n \n+    std::set<Int32> column_ids_which_have_bounds;\n+\n };\n \n /// Once manifest file is constructed. It's unchangeable.\ndiff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFilesPruning.cpp b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFilesPruning.cpp\nnew file mode 100644\nindex 000000000000..246af3f2d457\n--- /dev/null\n+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFilesPruning.cpp\n@@ -0,0 +1,217 @@\n+#include \"config.h\"\n+\n+#if USE_AVRO\n+\n+#include <Columns/ColumnNullable.h>\n+#include <Columns/ColumnsDateTime.h>\n+#include <Common/DateLUTImpl.h>\n+#include <DataTypes/DataTypeNullable.h>\n+#include <Common/logger_useful.h>\n+#include <Parsers/ASTFunction.h>\n+#include <Parsers/ASTIdentifier.h>\n+#include <Parsers/ASTExpressionList.h>\n+#include <Parsers/ASTLiteral.h>\n+#include <IO/ReadHelpers.h>\n+#include <Common/quoteString.h>\n+#include <fmt/ranges.h>\n+\n+#include <Interpreters/ExpressionActions.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFilesPruning.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h>\n+\n+using namespace DB;\n+\n+namespace ProfileEvents\n+{\n+    extern const Event IcebergPartitionPrunnedFiles;\n+    extern const Event IcebergMinMaxIndexPrunnedFiles;\n+}\n+\n+\n+namespace Iceberg\n+{\n+\n+DB::ASTPtr getASTFromTransform(const String & transform_name_src, const String & column_name)\n+{\n+    std::string transform_name = Poco::toLower(transform_name_src);\n+\n+    if (transform_name == \"year\" || transform_name == \"years\")\n+        return makeASTFunction(\"toYearNumSinceEpoch\", std::make_shared<DB::ASTIdentifier>(column_name));\n+\n+    if (transform_name == \"month\" || transform_name == \"months\")\n+        return makeASTFunction(\"toMonthNumSinceEpoch\", std::make_shared<DB::ASTIdentifier>(column_name));\n+\n+    if (transform_name == \"day\" || transform_name == \"date\" || transform_name == \"days\" || transform_name == \"dates\")\n+        return makeASTFunction(\"toRelativeDayNum\", std::make_shared<DB::ASTIdentifier>(column_name));\n+\n+    if (transform_name == \"hour\" || transform_name == \"hours\")\n+        return makeASTFunction(\"toRelativeHourNum\", std::make_shared<DB::ASTIdentifier>(column_name));\n+\n+    if (transform_name == \"identity\")\n+        return std::make_shared<ASTIdentifier>(column_name);\n+\n+    if (transform_name == \"void\")\n+        return makeASTFunction(\"tuple\");\n+\n+    if (transform_name.starts_with(\"truncate\"))\n+    {\n+        /// should look like transform[N]\n+\n+        if (transform_name.back() != ']')\n+            return nullptr;\n+\n+        auto argument_start = transform_name.find('[');\n+\n+        if (argument_start == std::string::npos)\n+            return nullptr;\n+\n+        auto argument_width = transform_name.length() - 2 - argument_start;\n+        std::string width = transform_name.substr(argument_start + 1, argument_width);\n+        size_t truncate_width;\n+        bool parsed = DB::tryParse<size_t>(truncate_width, width);\n+\n+        if (!parsed)\n+            return nullptr;\n+\n+        return makeASTFunction(\"icebergTruncate\", std::make_shared<DB::ASTLiteral>(truncate_width), std::make_shared<DB::ASTIdentifier>(column_name));\n+    }\n+    else\n+    {\n+        return nullptr;\n+    }\n+}\n+\n+std::unique_ptr<DB::ActionsDAG> ManifestFilesPruner::transformFilterDagForManifest(const DB::ActionsDAG * source_dag, std::vector<Int32> & used_columns_in_filter) const\n+{\n+    if (source_dag == nullptr)\n+        return nullptr;\n+\n+    const auto & inputs = source_dag->getInputs();\n+\n+    for (const auto & input : inputs)\n+    {\n+        if (input->type == ActionsDAG::ActionType::INPUT)\n+        {\n+            std::string input_name = input->result_name;\n+            std::optional<Int32> input_id = schema_processor.tryGetColumnIDByName(current_schema_id, input_name);\n+            if (input_id)\n+                used_columns_in_filter.push_back(*input_id);\n+        }\n+    }\n+\n+    ActionsDAG dag_with_renames;\n+    for (const auto column_id : used_columns_in_filter)\n+    {\n+        auto column = schema_processor.tryGetFieldCharacteristics(current_schema_id, column_id);\n+\n+        /// Columns which we dropped and don't exist in current schema\n+        /// cannot be queried in WHERE expression.\n+        if (!column.has_value())\n+            continue;\n+\n+        /// We take data type from manifest schema, not latest type\n+        auto column_from_manifest = schema_processor.tryGetFieldCharacteristics(manifest_schema_id, column_id);\n+        if (!column_from_manifest.has_value())\n+            continue;\n+\n+        auto numeric_column_name = DB::backQuote(DB::toString(column_id));\n+        const auto * node = &dag_with_renames.addInput(numeric_column_name, column_from_manifest->type);\n+        node = &dag_with_renames.addAlias(*node, column->name);\n+        dag_with_renames.getOutputs().push_back(node);\n+    }\n+    auto result = std::make_unique<DB::ActionsDAG>(DB::ActionsDAG::merge(std::move(dag_with_renames), source_dag->clone()));\n+    result->removeUnusedActions();\n+    return result;\n+\n+}\n+\n+\n+ManifestFilesPruner::ManifestFilesPruner(\n+    const DB::IcebergSchemaProcessor & schema_processor_,\n+    Int32 current_schema_id_,\n+    const DB::ActionsDAG * filter_dag,\n+    const ManifestFileContent & manifest_file,\n+    DB::ContextPtr context)\n+    : schema_processor(schema_processor_)\n+    , current_schema_id(current_schema_id_)\n+    , manifest_schema_id(manifest_file.getSchemaId())\n+{\n+    std::unique_ptr<ActionsDAG> transformed_dag;\n+    std::vector<Int32> used_columns_in_filter;\n+    if (manifest_file.hasPartitionKey() || manifest_file.hasBoundsInfoInManifests())\n+        transformed_dag = transformFilterDagForManifest(filter_dag, used_columns_in_filter);\n+\n+    if (manifest_file.hasPartitionKey())\n+    {\n+        partition_key = &manifest_file.getPartitionKeyDescription();\n+        if (transformed_dag != nullptr)\n+            partition_key_condition.emplace(transformed_dag.get(), context, partition_key->column_names, partition_key->expression, true /* single_point */);\n+    }\n+\n+    if (manifest_file.hasBoundsInfoInManifests() && transformed_dag != nullptr)\n+    {\n+        {\n+            const auto & bounded_colums = manifest_file.getColumnsIDsWithBounds();\n+            for (Int32 used_column_id : used_columns_in_filter)\n+            {\n+                if (!bounded_colums.contains(used_column_id))\n+                    continue;\n+\n+                NameAndTypePair name_and_type = schema_processor.getFieldCharacteristics(manifest_schema_id, used_column_id);\n+                name_and_type.name = DB::backQuote(DB::toString(used_column_id));\n+\n+                ExpressionActionsPtr expression = std::make_shared<ExpressionActions>(\n+                    ActionsDAG({name_and_type}), ExpressionActionsSettings(context));\n+\n+                min_max_key_conditions.emplace(used_column_id, KeyCondition(transformed_dag.get(), context, {name_and_type.name}, expression));\n+            }\n+        }\n+    }\n+}\n+\n+bool ManifestFilesPruner::canBePruned(const ManifestFileEntry & entry) const\n+{\n+    if (partition_key_condition.has_value())\n+    {\n+        const auto & partition_value = entry.partition_key_value;\n+        std::vector<FieldRef> index_value(partition_value.begin(), partition_value.end());\n+        for (auto & field : index_value)\n+        {\n+            // NULL_LAST\n+            if (field.isNull())\n+                field = POSITIVE_INFINITY;\n+        }\n+\n+        bool can_be_true = partition_key_condition->mayBeTrueInRange(\n+            partition_value.size(), index_value.data(), index_value.data(), partition_key->data_types);\n+\n+        if (!can_be_true)\n+        {\n+            ProfileEvents::increment(ProfileEvents::IcebergPartitionPrunnedFiles);\n+            return true;\n+        }\n+    }\n+\n+    for (const auto & [column_id, key_condition] : min_max_key_conditions)\n+    {\n+        std::optional<NameAndTypePair> name_and_type = schema_processor.tryGetFieldCharacteristics(manifest_schema_id, column_id);\n+\n+        /// There is no such column in this manifest file\n+        if (!name_and_type.has_value())\n+            continue;\n+\n+        auto hyperrectangle = entry.columns_infos.at(column_id).hyperrectangle;\n+        if (hyperrectangle.has_value() && !key_condition.mayBeTrueInRange(1, &hyperrectangle->left, &hyperrectangle->right, {name_and_type->type}))\n+        {\n+            ProfileEvents::increment(ProfileEvents::IcebergMinMaxIndexPrunnedFiles);\n+            return true;\n+        }\n+    }\n+\n+    return false;\n+}\n+\n+\n+}\n+\n+#endif\ndiff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/PartitionPruning.h b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFilesPruning.h\nsimilarity index 72%\nrename from src/Storages/ObjectStorage/DataLakes/Iceberg/PartitionPruning.h\nrename to src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFilesPruning.h\nindex 8b9fd80c612f..89a761265985 100644\n--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/PartitionPruning.h\n+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFilesPruning.h\n@@ -20,20 +20,23 @@ class ManifestFileContent;\n DB::ASTPtr getASTFromTransform(const String & transform_name_src, const String & column_name);\n \n /// Prune specific data files based on manifest content\n-class PartitionPruner\n+class ManifestFilesPruner\n {\n private:\n     const DB::IcebergSchemaProcessor & schema_processor;\n     Int32 current_schema_id;\n+    Int32 manifest_schema_id;\n     const DB::KeyDescription * partition_key;\n+    std::optional<DB::KeyCondition> partition_key_condition;\n \n-    std::optional<DB::KeyCondition> key_condition;\n-    /// NOTE: tricky part to support RENAME column in partition key.\n+    std::unordered_map<Int32, DB::KeyCondition> min_max_key_conditions;\n+    /// NOTE: tricky part to support RENAME column.\n     /// Takes ActionDAG representation of user's WHERE expression and\n-    /// rename columns to the their origina numeric id's in iceberg\n-    std::unique_ptr<DB::ActionsDAG> transformFilterDagForManifest(const DB::ActionsDAG * source_dag, Int32 manifest_schema_id, const std::vector<Int32> & partition_column_ids) const;\n+    /// rename columns to the their origina numeric ID's in iceberg\n+    std::unique_ptr<DB::ActionsDAG> transformFilterDagForManifest(const DB::ActionsDAG * source_dag, std::vector<Int32> & used_columns_in_filter) const;\n public:\n-    PartitionPruner(\n+\n+    ManifestFilesPruner(\n         const DB::IcebergSchemaProcessor & schema_processor_,\n         Int32 current_schema_id_,\n         const DB::ActionsDAG * filter_dag,\ndiff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/PartitionPruning.cpp b/src/Storages/ObjectStorage/DataLakes/Iceberg/PartitionPruning.cpp\ndeleted file mode 100644\nindex 9e24494e071e..000000000000\n--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/PartitionPruning.cpp\n+++ /dev/null\n@@ -1,144 +0,0 @@\n-#include \"config.h\"\n-\n-#if USE_AVRO\n-\n-#include <Columns/ColumnNullable.h>\n-#include <Columns/ColumnsDateTime.h>\n-#include <Common/DateLUTImpl.h>\n-#include <DataTypes/DataTypeNullable.h>\n-#include <Common/logger_useful.h>\n-#include <Parsers/ASTFunction.h>\n-#include <Parsers/ASTIdentifier.h>\n-#include <Parsers/ASTExpressionList.h>\n-#include <Parsers/ASTLiteral.h>\n-#include <IO/ReadHelpers.h>\n-#include <Common/quoteString.h>\n-\n-#include <Storages/ObjectStorage/DataLakes/Iceberg/PartitionPruning.h>\n-#include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h>\n-\n-using namespace DB;\n-\n-namespace Iceberg\n-{\n-\n-DB::ASTPtr getASTFromTransform(const String & transform_name_src, const String & column_name)\n-{\n-    std::string transform_name = Poco::toLower(transform_name_src);\n-\n-    if (transform_name == \"year\" || transform_name == \"years\")\n-        return makeASTFunction(\"toYearNumSinceEpoch\", std::make_shared<DB::ASTIdentifier>(column_name));\n-\n-    if (transform_name == \"month\" || transform_name == \"months\")\n-        return makeASTFunction(\"toMonthNumSinceEpoch\", std::make_shared<DB::ASTIdentifier>(column_name));\n-\n-    if (transform_name == \"day\" || transform_name == \"date\" || transform_name == \"days\" || transform_name == \"dates\")\n-        return makeASTFunction(\"toRelativeDayNum\", std::make_shared<DB::ASTIdentifier>(column_name));\n-\n-    if (transform_name == \"hour\" || transform_name == \"hours\")\n-        return makeASTFunction(\"toRelativeHourNum\", std::make_shared<DB::ASTIdentifier>(column_name));\n-\n-    if (transform_name == \"identity\")\n-        return std::make_shared<ASTIdentifier>(column_name);\n-\n-    if (transform_name == \"void\")\n-        return makeASTFunction(\"tuple\");\n-\n-    if (transform_name.starts_with(\"truncate\"))\n-    {\n-        /// should look like transform[N]\n-\n-        if (transform_name.back() != ']')\n-            return nullptr;\n-\n-        auto argument_start = transform_name.find('[');\n-\n-        if (argument_start == std::string::npos)\n-            return nullptr;\n-\n-        auto argument_width = transform_name.length() - 2 - argument_start;\n-        std::string width = transform_name.substr(argument_start + 1, argument_width);\n-        size_t truncate_width;\n-        bool parsed = DB::tryParse<size_t>(truncate_width, width);\n-\n-        if (!parsed)\n-            return nullptr;\n-\n-        return makeASTFunction(\"icebergTruncate\", std::make_shared<DB::ASTLiteral>(truncate_width), std::make_shared<DB::ASTIdentifier>(column_name));\n-    }\n-    else\n-    {\n-        return nullptr;\n-    }\n-}\n-\n-std::unique_ptr<DB::ActionsDAG> PartitionPruner::transformFilterDagForManifest(const DB::ActionsDAG * source_dag, Int32 manifest_schema_id, const std::vector<Int32> & partition_column_ids) const\n-{\n-    if (source_dag == nullptr)\n-        return nullptr;\n-\n-    ActionsDAG dag_with_renames;\n-    for (const auto column_id : partition_column_ids)\n-    {\n-        auto column = schema_processor.tryGetFieldCharacteristics(current_schema_id, column_id);\n-\n-        /// Columns which we dropped and doesn't exist in current schema\n-        /// cannot be queried in WHERE expression.\n-        if (!column.has_value())\n-            continue;\n-\n-        /// We take data type from manifest schema, not latest type\n-        auto column_type = schema_processor.getFieldCharacteristics(manifest_schema_id, column_id).type;\n-        auto numeric_column_name = DB::backQuote(DB::toString(column_id));\n-        const auto * node = &dag_with_renames.addInput(numeric_column_name, column_type);\n-        node = &dag_with_renames.addAlias(*node, column->name);\n-        dag_with_renames.getOutputs().push_back(node);\n-    }\n-    auto result = std::make_unique<DB::ActionsDAG>(DB::ActionsDAG::merge(std::move(dag_with_renames), source_dag->clone()));\n-    result->removeUnusedActions();\n-    return result;\n-\n-}\n-\n-PartitionPruner::PartitionPruner(\n-    const DB::IcebergSchemaProcessor & schema_processor_,\n-    Int32 current_schema_id_,\n-    const DB::ActionsDAG * filter_dag,\n-    const ManifestFileContent & manifest_file,\n-    DB::ContextPtr context)\n-    : schema_processor(schema_processor_)\n-    , current_schema_id(current_schema_id_)\n-{\n-    if (manifest_file.hasPartitionKey())\n-    {\n-        partition_key = &manifest_file.getPartitionKeyDescription();\n-        auto transformed_dag = transformFilterDagForManifest(filter_dag, manifest_file.getSchemaId(), manifest_file.getPartitionKeyColumnIDs());\n-        if (transformed_dag != nullptr)\n-            key_condition.emplace(transformed_dag.get(), context, partition_key->column_names, partition_key->expression, true /* single_point */);\n-    }\n-}\n-\n-bool PartitionPruner::canBePruned(const ManifestFileEntry & entry) const\n-{\n-    if (!key_condition.has_value())\n-        return false;\n-\n-    const auto & partition_value = entry.partition_key_value;\n-    std::vector<FieldRef> index_value(partition_value.begin(), partition_value.end());\n-    for (auto & field : index_value)\n-    {\n-        // NULL_LAST\n-        if (field.isNull())\n-            field = POSITIVE_INFINITY;\n-    }\n-\n-    bool can_be_true = key_condition->mayBeTrueInRange(\n-        partition_value.size(), index_value.data(), index_value.data(), partition_key->data_types);\n-\n-    return !can_be_true;\n-}\n-\n-\n-}\n-\n-#endif\ndiff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/SchemaProcessor.cpp b/src/Storages/ObjectStorage/DataLakes/Iceberg/SchemaProcessor.cpp\nindex 961d129f71ca..acd057ff4a97 100644\n--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/SchemaProcessor.cpp\n+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/SchemaProcessor.cpp\n@@ -117,6 +117,7 @@ void IcebergSchemaProcessor::addIcebergTableSchema(Poco::JSON::Object::Ptr schem\n             auto type = getFieldType(field, \"type\", required, current_full_name, true);\n             clickhouse_schema->push_back(NameAndTypePair{name, type});\n             clickhouse_types_by_source_ids[{schema_id, field->getValue<Int32>(\"id\")}] = NameAndTypePair{current_full_name, type};\n+            clickhouse_ids_by_source_names[{schema_id, current_full_name}] = field->getValue<Int32>(\"id\");\n         }\n         clickhouse_table_schemas_by_ids[schema_id] = clickhouse_schema;\n     }\n@@ -139,6 +140,14 @@ std::optional<NameAndTypePair> IcebergSchemaProcessor::tryGetFieldCharacteristic\n     return it->second;\n }\n \n+std::optional<Int32> IcebergSchemaProcessor::tryGetColumnIDByName(Int32 schema_id, const std::string & name) const\n+{\n+    auto it = clickhouse_ids_by_source_names.find({schema_id, name});\n+    if (it == clickhouse_ids_by_source_names.end())\n+        return {};\n+    return it->second;\n+}\n+\n NamesAndTypesList IcebergSchemaProcessor::tryGetFieldsCharacteristics(Int32 schema_id, const std::vector<Int32> & source_ids) const\n {\n     NamesAndTypesList fields;\n@@ -232,6 +241,8 @@ IcebergSchemaProcessor::getComplexTypeFromObject(const Poco::JSON::Object::Ptr &\n                 element_types.push_back(getFieldType(field, \"type\", required, current_full_name, true));\n                 clickhouse_types_by_source_ids[{current_schema_id.value(), field->getValue<Int32>(\"id\")}]\n                     = NameAndTypePair{current_full_name, element_types.back()};\n+\n+                clickhouse_ids_by_source_names[{current_schema_id.value(), current_full_name}] = field->getValue<Int32>(\"id\");\n             }\n             else\n             {\ndiff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/SchemaProcessor.h b/src/Storages/ObjectStorage/DataLakes/Iceberg/SchemaProcessor.h\nindex de003456cef2..c0e9b57909b8 100644\n--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/SchemaProcessor.h\n+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/SchemaProcessor.h\n@@ -82,6 +82,7 @@ class IcebergSchemaProcessor\n     NameAndTypePair getFieldCharacteristics(Int32 schema_version, Int32 source_id) const;\n     std::optional<NameAndTypePair> tryGetFieldCharacteristics(Int32 schema_version, Int32 source_id) const;\n     NamesAndTypesList tryGetFieldsCharacteristics(Int32 schema_id, const std::vector<Int32> & source_ids) const;\n+    std::optional<Int32> tryGetColumnIDByName(Int32 schema_id, const std::string & name) const;\n \n     bool hasClickhouseTableSchemaById(Int32 id) const;\n \n@@ -91,6 +92,7 @@ class IcebergSchemaProcessor\n     std::unordered_map<Int32, std::shared_ptr<NamesAndTypesList>> clickhouse_table_schemas_by_ids;\n     std::map<std::pair<Int32, Int32>, std::shared_ptr<ActionsDAG>> transform_dags_by_ids;\n     mutable std::map<std::pair<Int32, Int32>, NameAndTypePair> clickhouse_types_by_source_ids;\n+    mutable std::map<std::pair<Int32, std::string>, Int32> clickhouse_ids_by_source_names;\n \n     NamesAndTypesList getSchemaType(const Poco::JSON::Object::Ptr & schema);\n     DataTypePtr getComplexTypeFromObject(const Poco::JSON::Object::Ptr & type, String & current_full_name, bool is_subfield_of_root);\n",
  "test_patch": "diff --git a/tests/integration/test_storage_iceberg/test.py b/tests/integration/test_storage_iceberg/test.py\nindex bc4348609a3a..efb69c74a423 100644\n--- a/tests/integration/test_storage_iceberg/test.py\n+++ b/tests/integration/test_storage_iceberg/test.py\n@@ -2445,3 +2445,251 @@ def test_iceberg_snapshot_reads(started_cluster, format_version, storage_type):\n         == instance.query(\"SELECT number, toString(number + 1) FROM numbers(300)\")\n     )\n \n+@pytest.mark.parametrize(\"storage_type\", [\"s3\", \"azure\", \"local\"])\n+def test_minmax_pruning(started_cluster, storage_type):\n+    instance = started_cluster.instances[\"node1\"]\n+    spark = started_cluster.spark_session\n+    TABLE_NAME = \"test_minmax_pruning_\" + storage_type + \"_\" + get_uuid_str()\n+\n+    def execute_spark_query(query: str):\n+        spark.sql(query)\n+        default_upload_directory(\n+            started_cluster,\n+            storage_type,\n+            f\"/iceberg_data/default/{TABLE_NAME}/\",\n+            f\"/iceberg_data/default/{TABLE_NAME}/\",\n+        )\n+        return\n+\n+    execute_spark_query(\n+        f\"\"\"\n+            CREATE TABLE {TABLE_NAME} (\n+                tag INT,\n+                date DATE,\n+                ts TIMESTAMP,\n+                time_struct struct<a : DATE, b : TIMESTAMP>,\n+                name VARCHAR(50),\n+                number BIGINT\n+            )\n+            USING iceberg\n+            OPTIONS('format-version'='2')\n+        \"\"\"\n+    )\n+\n+    execute_spark_query(\n+        f\"\"\"\n+        INSERT INTO {TABLE_NAME} VALUES\n+        (1, DATE '2024-01-20',\n+        TIMESTAMP '2024-02-20 10:00:00', named_struct('a', DATE '2024-01-20', 'b', TIMESTAMP '2024-02-20 10:00:00'), 'vasya', 5)\n+    \"\"\"\n+    )\n+\n+    execute_spark_query(\n+        f\"\"\"\n+        INSERT INTO {TABLE_NAME} VALUES\n+        (2, DATE '2024-02-20',\n+        TIMESTAMP '2024-03-20 15:00:00', named_struct('a', DATE '2024-02-20', 'b', TIMESTAMP '2024-03-20 14:00:00'), 'vasilisa', 6)\n+    \"\"\"\n+    )\n+\n+    execute_spark_query(\n+        f\"\"\"\n+        INSERT INTO {TABLE_NAME} VALUES\n+        (3, DATE '2025-03-20',\n+        TIMESTAMP '2024-04-30 14:00:00', named_struct('a', DATE '2024-03-20', 'b', TIMESTAMP '2024-04-30 14:00:00'), 'icebreaker', 7)\n+    \"\"\"\n+    )\n+    execute_spark_query(\n+        f\"\"\"\n+        INSERT INTO {TABLE_NAME} VALUES\n+        (4, DATE '2025-04-20',\n+        TIMESTAMP '2024-05-30 14:00:00', named_struct('a', DATE '2024-04-20', 'b', TIMESTAMP '2024-05-30 14:00:00'), 'iceberg', 8)\n+    \"\"\"\n+    )\n+\n+    creation_expression = get_creation_expression(\n+        storage_type, TABLE_NAME, started_cluster, table_function=True\n+    )\n+\n+    def check_validity_and_get_prunned_files(select_expression):\n+        query_id1 = f\"{TABLE_NAME}-{uuid.uuid4()}\"\n+        query_id2 = f\"{TABLE_NAME}-{uuid.uuid4()}\"\n+\n+        data1 = instance.query(\n+            select_expression,\n+            query_id=query_id1,\n+            settings={\"use_iceberg_partition_pruning\": 0, \"input_format_parquet_bloom_filter_push_down\": 0, \"input_format_parquet_filter_push_down\": 0},\n+        )\n+        data1 = list(\n+            map(\n+                lambda x: x.split(\"\\t\"),\n+                filter(lambda x: len(x) > 0, data1.strip().split(\"\\n\")),\n+            )\n+        )\n+\n+        data2 = instance.query(\n+            select_expression,\n+            query_id=query_id2,\n+            settings={\"use_iceberg_partition_pruning\": 1, \"input_format_parquet_bloom_filter_push_down\": 0, \"input_format_parquet_filter_push_down\": 0},\n+        )\n+        data2 = list(\n+            map(\n+                lambda x: x.split(\"\\t\"),\n+                filter(lambda x: len(x) > 0, data2.strip().split(\"\\n\")),\n+            )\n+        )\n+\n+        assert data1 == data2\n+\n+        instance.query(\"SYSTEM FLUSH LOGS\")\n+\n+        print(\n+            \"Unprunned: \",\n+            instance.query(\n+                f\"SELECT ProfileEvents['IcebergMinMaxIndexPrunnedFiles'] FROM system.query_log WHERE query_id = '{query_id1}' AND type = 'QueryFinish'\"\n+            ),\n+        )\n+        print(\n+            \"Prunned: \",\n+            instance.query(\n+                f\"SELECT ProfileEvents['IcebergMinMaxIndexPrunnedFiles'] FROM system.query_log WHERE query_id = '{query_id2}' AND type = 'QueryFinish'\"\n+            ),\n+        )\n+\n+        assert 0 == int(\n+            instance.query(\n+                f\"SELECT ProfileEvents['IcebergMinMaxIndexPrunnedFiles'] FROM system.query_log WHERE query_id = '{query_id1}' AND type = 'QueryFinish'\"\n+            )\n+        )\n+        return int(\n+            instance.query(\n+                f\"SELECT ProfileEvents['IcebergMinMaxIndexPrunnedFiles'] FROM system.query_log WHERE query_id = '{query_id2}' AND type = 'QueryFinish'\"\n+            )\n+        )\n+\n+    assert (\n+        check_validity_and_get_prunned_files(\n+            f\"SELECT * FROM {creation_expression} ORDER BY ALL\"\n+        )\n+        == 0\n+    )\n+    assert (\n+        check_validity_and_get_prunned_files(\n+            f\"SELECT * FROM {creation_expression} WHERE date <= '2024-01-25' ORDER BY ALL\"\n+        )\n+        == 3\n+    )\n+    assert (\n+        check_validity_and_get_prunned_files(\n+            f\"SELECT * FROM {creation_expression} WHERE ts <= timestamp('2024-03-20 14:00:00.000000') ORDER BY ALL\"\n+        )\n+        == 3\n+    )\n+\n+    assert (\n+        check_validity_and_get_prunned_files(\n+            f\"SELECT * FROM {creation_expression} WHERE tag == 1 ORDER BY ALL\"\n+        )\n+        == 3\n+    )\n+\n+    assert (\n+        check_validity_and_get_prunned_files(\n+            f\"SELECT * FROM {creation_expression} WHERE tag <= 1 ORDER BY ALL\"\n+        )\n+        == 3\n+    )\n+\n+    assert (\n+        check_validity_and_get_prunned_files(\n+            f\"SELECT * FROM {creation_expression} WHERE name == 'vasilisa' ORDER BY ALL\"\n+        )\n+        == 3\n+    )\n+\n+    assert (\n+        check_validity_and_get_prunned_files(\n+            f\"SELECT * FROM {creation_expression} WHERE name < 'kek' ORDER BY ALL\"\n+        )\n+        == 2\n+    )\n+\n+    assert (\n+        check_validity_and_get_prunned_files(\n+            f\"SELECT * FROM {creation_expression} WHERE number == 8 ORDER BY ALL\"\n+        )\n+        == 3\n+    )\n+\n+    assert (\n+        check_validity_and_get_prunned_files(\n+            f\"SELECT * FROM {creation_expression} WHERE number <= 5 ORDER BY ALL\"\n+        )\n+        == 3\n+    )\n+\n+    execute_spark_query(f\"ALTER TABLE {TABLE_NAME} RENAME COLUMN date TO date3\")\n+\n+    assert (\n+        check_validity_and_get_prunned_files(\n+            f\"SELECT * FROM {creation_expression} WHERE date3 <= '2024-01-25' ORDER BY ALL\"\n+        )\n+        == 3\n+    )\n+\n+    execute_spark_query(f\"ALTER TABLE {TABLE_NAME} ALTER COLUMN tag TYPE BIGINT\")\n+\n+    assert (\n+        check_validity_and_get_prunned_files(\n+            f\"SELECT * FROM {creation_expression} WHERE tag <= 1 ORDER BY ALL\"\n+        )\n+        == 3\n+    )\n+\n+    assert (\n+        check_validity_and_get_prunned_files(\n+            f\"SELECT * FROM {creation_expression} WHERE time_struct.a <= '2024-02-01' ORDER BY ALL\"\n+        )\n+        == 3\n+    )\n+\n+    execute_spark_query(\n+        f\"INSERT INTO {TABLE_NAME} VALUES (1, DATE '2024-01-20', TIMESTAMP '2024-02-20 10:00:00', named_struct('a', DATE '2024-03-15', 'b', TIMESTAMP '2024-02-20 10:00:00'), 'kek', 10)\"\n+    )\n+\n+    assert (\n+        check_validity_and_get_prunned_files(\n+            f\"SELECT * FROM {creation_expression} WHERE time_struct.a <= '2024-02-01' ORDER BY ALL\"\n+        )\n+        == 4\n+    )\n+\n+    execute_spark_query(f\"ALTER TABLE {TABLE_NAME} ADD COLUMNS (ddd decimal(10, 3))\")\n+\n+    execute_spark_query(\n+        f\"INSERT INTO {TABLE_NAME} VALUES (1, DATE '2024-01-20', TIMESTAMP '2024-02-20 10:00:00', named_struct('a', DATE '2024-03-15', 'b', TIMESTAMP '2024-02-20 10:00:00'), 'kek', 30, decimal(17.22))\"\n+    )\n+\n+    execute_spark_query(\n+        f\"INSERT INTO {TABLE_NAME} VALUES (1, DATE '2024-01-20', TIMESTAMP '2024-02-20 10:00:00', named_struct('a', DATE '2024-03-15', 'b', TIMESTAMP '2024-02-20 10:00:00'), 'kek', 10, decimal(14311.772))\"\n+    )\n+\n+    execute_spark_query(\n+        f\"INSERT INTO {TABLE_NAME} VALUES (1, DATE '2024-01-20', TIMESTAMP '2024-02-20 10:00:00', named_struct('a', DATE '2024-03-15', 'b', TIMESTAMP '2024-02-20 10:00:00'), 'kek', 10, decimal(-8888.999))\"\n+    )\n+\n+\n+    assert (\n+        check_validity_and_get_prunned_files(\n+            f\"SELECT * FROM {creation_expression} WHERE ddd >= 100 ORDER BY ALL\"\n+        )\n+        == 2\n+    )\n+    # Spark store rounded values of decimals, this query checks that we work it around.\n+    # Please check the code where we parse lower bounds and upper bounds\n+    assert (\n+        check_validity_and_get_prunned_files(\n+            f\"SELECT * FROM {creation_expression} WHERE ddd >= toDecimal64('17.21', 3) ORDER BY ALL\"\n+        )\n+        == 1\n+    )\n",
  "problem_statement": "Support MinMax index for iceberg\nIn Manifest list files according to standard Iceberg writers may store min/max values for each column in each data file: https://iceberg.apache.org/spec/#manifest-lists. \n\n![Image](https://github.com/user-attachments/assets/014e89ec-4f0c-410f-aefa-2ec0fd19eb52)\n\nFor example Spark store them by default, and this information can be very useful to speedup wide range of queries. This is how it looks like:\n```\n    \"lower_bounds\": {\n      \"array\": [\n        {\n          \"key\": 1,\n          \"value\": \"\\u0001\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\"\n        },\n        {\n          \"key\": 2,\n          \"value\": \"vasya\"\n        },\n        {\n          \"key\": 3,\n          \"value\": \"\u00d2\\u0002<U+0096>I\\u0000\\u0000\\u0000\\u0000\"\n        },\n        {\n          \"key\": 4,\n          \"value\": \"\u00bdN\\u0000\\u0000\"\n        },\n        {\n          \"key\": 5,\n          \"value\": \"'B\"\n        }\n      ]\n    },\n    \"upper_bounds\": {\n      \"array\": [\n        {\n          \"key\": 1,\n          \"value\": \"\\u0001\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\"\n        },\n        {\n          \"key\": 2,\n          \"value\": \"vasya\"\n        },\n        {\n          \"key\": 3,\n          \"value\": \"\u00d2\\u0002<U+0096>I\\u0000\\u0000\\u0000\\u0000\"\n        },\n        {\n          \"key\": 4,\n          \"value\": \"\u00bdN\\u0000\\u0000\"\n        },\n        {\n          \"key\": 5,\n          \"value\": \"'B\"\n        }\n      ]\n    },\n```\nMerge Tree MinMax indices can be reused to implement this feature https://github.com/ClickHouse/ClickHouse/blob/master/src/Storages/MergeTree/MergeTreeIndexMinMax.h.\n",
  "hints_text": "",
  "created_at": "2025-03-25T13:43:21Z"
}