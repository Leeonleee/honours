diff --git a/src/Common/ProfileEvents.cpp b/src/Common/ProfileEvents.cpp
index bafc4846c2cb..8baaca95b898 100644
--- a/src/Common/ProfileEvents.cpp
+++ b/src/Common/ProfileEvents.cpp
@@ -220,6 +220,7 @@
     \
     M(IcebergPartitionPrunnedFiles, "Number of skipped files during Iceberg partition pruning", ValueType::Number) \
     M(IcebergTrivialCountOptimizationApplied, "Trivial count optimization applied while reading from Iceberg", ValueType::Number) \
+    M(IcebergMinMaxIndexPrunnedFiles, "Number of skipped files by using MinMax index in Iceberg", ValueType::Number) \
     M(JoinBuildTableRowCount, "Total number of rows in the build table for a JOIN operation.", ValueType::Number) \
     M(JoinProbeTableRowCount, "Total number of rows in the probe table for a JOIN operation.", ValueType::Number) \
     M(JoinResultRowCount, "Total number of rows in the result of a JOIN operation.", ValueType::Number) \
diff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp b/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp
index e5f45afc3530..aedba7369ecd 100644
--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp
+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp
@@ -18,7 +18,7 @@
 #include <Storages/ObjectStorage/DataLakes/Iceberg/Utils.h>
 #include <Storages/ObjectStorage/DataLakes/Iceberg/AvroForIcebergDeserializer.h>
 #include <Storages/ObjectStorage/DataLakes/Iceberg/Snapshot.h>
-#include <Storages/ObjectStorage/DataLakes/Iceberg/PartitionPruning.h>
+#include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFilesPruning.h>
 #include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h>
 
 #include <Common/logger_useful.h>
@@ -26,7 +26,6 @@
 
 namespace ProfileEvents
 {
-    extern const Event IcebergPartitionPrunnedFiles;
     extern const Event IcebergTrivialCountOptimizationApplied;
 }
 
@@ -569,17 +568,13 @@ Strings IcebergMetadata::getDataFilesImpl(const ActionsDAG * filter_dag) const
     Strings data_files;
     for (const auto & manifest_file_ptr : *(relevant_snapshot->manifest_list))
     {
-        PartitionPruner pruner(schema_processor, relevant_snapshot_schema_id, filter_dag, *manifest_file_ptr, getContext());
+        ManifestFilesPruner pruner(schema_processor, relevant_snapshot_schema_id, filter_dag, *manifest_file_ptr, getContext());
         const auto & data_files_in_manifest = manifest_file_ptr->getFiles();
         for (const auto & manifest_file_entry : data_files_in_manifest)
         {
             if (manifest_file_entry.status != ManifestEntryStatus::DELETED)
             {
-                if (pruner.canBePruned(manifest_file_entry))
-                {
-                    ProfileEvents::increment(ProfileEvents::IcebergPartitionPrunnedFiles);
-                }
-                else
+                if (!pruner.canBePruned(manifest_file_entry))
                 {
                     if (std::holds_alternative<DataFileEntry>(manifest_file_entry.file))
                         data_files.push_back(std::get<DataFileEntry>(manifest_file_entry.file).file_name);
diff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp
index b9a968f06db6..650a5c4af033 100644
--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp
+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp
@@ -4,14 +4,16 @@
 
 #include <Storages/ObjectStorage/DataLakes/Iceberg/Utils.h>
 #include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h>
-#include <Storages/ObjectStorage/DataLakes/Iceberg/PartitionPruning.h>
+#include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFilesPruning.h>
 
+#include <DataTypes/DataTypesDecimal.h>
 #include <Poco/JSON/Parser.h>
 #include <Storages/ColumnsDescription.h>
 #include <Parsers/ASTFunction.h>
 #include <Common/quoteString.h>
 #include <DataTypes/DataTypeNullable.h>
-#include <Common/logger_useful.h>
+#include <IO/ReadBufferFromString.h>
+#include <IO/ReadHelpers.h>
 
 namespace DB::ErrorCodes
 {
@@ -23,6 +25,86 @@ namespace DB::ErrorCodes
 namespace Iceberg
 {
 
+namespace
+{
+    /// Iceberg stores lower_bounds and upper_bounds serialized with some custom deserialization as bytes array
+    /// https://iceberg.apache.org/spec/#appendix-d-single-value-serialization
+    std::optional<DB::Field> deserializeFieldFromBinaryRepr(std::string str, DB::DataTypePtr expected_type, bool lower_bound)
+    {
+        auto non_nullable_type = DB::removeNullable(expected_type);
+        auto column = non_nullable_type->createColumn();
+        if (DB::WhichDataType(non_nullable_type).isDecimal())
+        {
+            /// Iceberg store decimal values as unscaled value with twoâ€™s-complement big-endian binary
+            /// using the minimum number of bytes for the value
+            /// Our decimal binary representation is little endian
+            /// so we cannot reuse our default code for parsing it.
+            int64_t unscaled_value = 0;
+
+            // Convert from big-endian to signed int
+            for (const auto byte : str)
+                unscaled_value = (unscaled_value << 8) | static_cast<uint8_t>(byte);
+
+            /// Add sign
+            if (str[0] & 0x80)
+            {
+                int64_t sign_extension = -1;
+                sign_extension <<= (str.size() * 8);
+                unscaled_value |= sign_extension;
+            }
+
+            /// NOTE: It's very weird, but Decimal values for lower bound and upper bound
+            /// are stored rounded, without fractional part. What is more strange
+            /// the integer part is rounded mathematically correctly according to fractional part.
+            /// Example: 17.22 -> 17, 8888.999 -> 8889, 1423.77 -> 1424.
+            /// I've checked two implementations: Spark and Amazon Athena and both of them
+            /// do this.
+            ///
+            /// The problem is -- we cannot use rounded values for lower bounds and upper bounds.
+            /// Example: upper_bound(x) = 17.22, but it's rounded 17.00, now condition WHERE x >= 17.21 will
+            /// check rounded value and say: "Oh largest value is 17, so values bigger than 17.21 cannot be in this file,
+            /// let's skip it". But it will produce incorrect result since actual value (17.22 >= 17.21) is stored in this file.
+            ///
+            /// To handle this issue we subtract 1 from the integral part for lower_bound and add 1 to integral
+            /// part of upper_bound. This produces: 17.22 -> [16.0, 18.0]. So this is more rough boundary,
+            /// but at least it doesn't lead to incorrect results.
+            {
+                int64_t scaler = lower_bound ? -10 : 10;
+                int32_t scale = DB::getDecimalScale(*non_nullable_type);
+                while (--scale)
+                    scaler *= 10;
+
+                unscaled_value += scaler;
+            }
+
+            if (const auto * decimal_type = DB::checkDecimal<DB::Decimal32>(*non_nullable_type))
+            {
+                DB::DecimalField<DB::Decimal32> result(unscaled_value, decimal_type->getScale());
+                return result;
+            }
+            if (const auto * decimal_type = DB::checkDecimal<DB::Decimal64>(*non_nullable_type))
+            {
+                DB::DecimalField<DB::Decimal64> result(unscaled_value, decimal_type->getScale());
+                return result;
+            }
+            else
+            {
+                return std::nullopt;
+            }
+        }
+        else
+        {
+            /// For all other types except decimal binary representation
+            /// matches our internal representation
+            column->insertData(str.data(), str.length());
+            DB::Field result;
+            column->get(0, result);
+            return result;
+        }
+    }
+
+}
+
 constexpr const char * COLUMN_STATUS_NAME = "status";
 constexpr const char * COLUMN_TUPLE_DATA_FILE_NAME = "data_file";
 constexpr const char * COLUMN_SEQ_NUMBER_NAME = "sequence_number";
@@ -104,10 +186,9 @@ ManifestFileContent::ManifestFileContent(
 
         partition_key_ast->arguments->children.emplace_back(std::move(partition_ast));
         partition_columns_description.emplace_back(numeric_column_name, removeNullable(manifest_file_column_characteristics.type));
-        this->partition_column_ids.push_back(source_id);
     }
 
-    if (!partition_column_ids.empty())
+    if (!partition_columns_description.empty())
         this->partition_key_description.emplace(DB::KeyDescription::getKeyFromAST(std::move(partition_key_ast), ColumnsDescription(partition_columns_description), context));
 
     for (size_t i = 0; i < manifest_file_deserializer.rows(); ++i)
@@ -165,6 +246,7 @@ ManifestFileContent::ManifestFileContent(
             }
         }
 
+        std::unordered_map<Int32, std::pair<Field, Field>> value_for_bounds;
         for (const auto & path : {SUBCOLUMN_LOWER_BOUNDS_NAME, SUBCOLUMN_UPPER_BOUNDS_NAME})
         {
             if (manifest_file_deserializer.hasPath(path))
@@ -175,14 +257,28 @@ ManifestFileContent::ManifestFileContent(
                     const auto & column_number_and_bound = column_stats.safeGet<Tuple>();
                     Int32 number = column_number_and_bound[0].safeGet<Int32>();
                     const Field & bound_value = column_number_and_bound[1];
+
                     if (path == SUBCOLUMN_LOWER_BOUNDS_NAME)
-                        columns_infos[number].lower_bound = bound_value;
+                        value_for_bounds[number].first = bound_value;
                     else
-                        columns_infos[number].upper_bound = bound_value;
+                        value_for_bounds[number].second = bound_value;
+
+                    column_ids_which_have_bounds.insert(number);
                 }
             }
         }
 
+        for (const auto & [column_id, bounds] : value_for_bounds)
+        {
+            DB::NameAndTypePair name_and_type = schema_processor.getFieldCharacteristics(schema_id, column_id);
+            auto left = deserializeFieldFromBinaryRepr(bounds.first.safeGet<std::string>(), name_and_type.type, true);
+            auto right = deserializeFieldFromBinaryRepr(bounds.second.safeGet<std::string>(), name_and_type.type, false);
+            if (!left || !right)
+                continue;
+
+            columns_infos[column_id].hyperrectangle.emplace(*left, true, *right, true);
+        }
+
         FileEntry file = FileEntry{DataFileEntry{file_path}};
 
         Int64 added_sequence_number = 0;
@@ -215,7 +311,7 @@ ManifestFileContent::ManifestFileContent(
 
 bool ManifestFileContent::hasPartitionKey() const
 {
-    return !partition_column_ids.empty();
+    return partition_key_description.has_value();
 }
 
 const DB::KeyDescription & ManifestFileContent::getPartitionKeyDescription() const
@@ -225,9 +321,14 @@ const DB::KeyDescription & ManifestFileContent::getPartitionKeyDescription() con
     return *(partition_key_description);
 }
 
-const std::vector<Int32> & ManifestFileContent::getPartitionKeyColumnIDs() const
+bool ManifestFileContent::hasBoundsInfoInManifests() const
+{
+    return !column_ids_which_have_bounds.empty();
+}
+
+const std::set<Int32> & ManifestFileContent::getColumnsIDsWithBounds() const
 {
-    return partition_column_ids;
+    return column_ids_which_have_bounds;
 }
 
 std::optional<Int64> ManifestFileContent::getRowsCountInAllDataFilesExcludingDeleted() const
diff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h
index 46d4fae5bc77..83b531400326 100644
--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h
+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h
@@ -7,6 +7,7 @@
 #include <Storages/ObjectStorage/DataLakes/Iceberg/SchemaProcessor.h>
 #include <Storages/ObjectStorage/DataLakes/Iceberg/AvroForIcebergDeserializer.h>
 #include <Storages/KeyDescription.h>
+#include <Storages/MergeTree/KeyCondition.h>
 #include <Core/Field.h>
 
 #include <cstdint>
@@ -41,8 +42,7 @@ struct ColumnInfo
     std::optional<Int64> rows_count;
     std::optional<Int64> bytes_size;
     std::optional<Int64> nulls_count;
-    std::optional<DB::Field> lower_bound;
-    std::optional<DB::Field> upper_bound;
+    std::optional<DB::Range> hyperrectangle;
 };
 
 using FileEntry = std::variant<DataFileEntry>; // In the future we will add PositionalDeleteFileEntry and EqualityDeleteFileEntry here
@@ -102,21 +102,24 @@ class ManifestFileContent
 
     bool hasPartitionKey() const;
     const DB::KeyDescription & getPartitionKeyDescription() const;
-    const std::vector<Int32> & getPartitionKeyColumnIDs() const;
 
     /// Fields with rows count in manifest files are optional
     /// they can be absent.
     std::optional<Int64> getRowsCountInAllDataFilesExcludingDeleted() const;
     std::optional<Int64> getBytesCountInAllDataFiles() const;
+
+    bool hasBoundsInfoInManifests() const;
+    const std::set<Int32> & getColumnsIDsWithBounds() const;
 private:
 
     Int32 schema_id;
 
     std::optional<DB::KeyDescription> partition_key_description;
-    std::vector<Int32> partition_column_ids;
     // Size - number of files
     std::vector<ManifestFileEntry> files;
 
+    std::set<Int32> column_ids_which_have_bounds;
+
 };
 
 /// Once manifest file is constructed. It's unchangeable.
diff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFilesPruning.cpp b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFilesPruning.cpp
new file mode 100644
index 000000000000..246af3f2d457
--- /dev/null
+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFilesPruning.cpp
@@ -0,0 +1,217 @@
+#include "config.h"
+
+#if USE_AVRO
+
+#include <Columns/ColumnNullable.h>
+#include <Columns/ColumnsDateTime.h>
+#include <Common/DateLUTImpl.h>
+#include <DataTypes/DataTypeNullable.h>
+#include <Common/logger_useful.h>
+#include <Parsers/ASTFunction.h>
+#include <Parsers/ASTIdentifier.h>
+#include <Parsers/ASTExpressionList.h>
+#include <Parsers/ASTLiteral.h>
+#include <IO/ReadHelpers.h>
+#include <Common/quoteString.h>
+#include <fmt/ranges.h>
+
+#include <Interpreters/ExpressionActions.h>
+#include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFilesPruning.h>
+#include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h>
+
+using namespace DB;
+
+namespace ProfileEvents
+{
+    extern const Event IcebergPartitionPrunnedFiles;
+    extern const Event IcebergMinMaxIndexPrunnedFiles;
+}
+
+
+namespace Iceberg
+{
+
+DB::ASTPtr getASTFromTransform(const String & transform_name_src, const String & column_name)
+{
+    std::string transform_name = Poco::toLower(transform_name_src);
+
+    if (transform_name == "year" || transform_name == "years")
+        return makeASTFunction("toYearNumSinceEpoch", std::make_shared<DB::ASTIdentifier>(column_name));
+
+    if (transform_name == "month" || transform_name == "months")
+        return makeASTFunction("toMonthNumSinceEpoch", std::make_shared<DB::ASTIdentifier>(column_name));
+
+    if (transform_name == "day" || transform_name == "date" || transform_name == "days" || transform_name == "dates")
+        return makeASTFunction("toRelativeDayNum", std::make_shared<DB::ASTIdentifier>(column_name));
+
+    if (transform_name == "hour" || transform_name == "hours")
+        return makeASTFunction("toRelativeHourNum", std::make_shared<DB::ASTIdentifier>(column_name));
+
+    if (transform_name == "identity")
+        return std::make_shared<ASTIdentifier>(column_name);
+
+    if (transform_name == "void")
+        return makeASTFunction("tuple");
+
+    if (transform_name.starts_with("truncate"))
+    {
+        /// should look like transform[N]
+
+        if (transform_name.back() != ']')
+            return nullptr;
+
+        auto argument_start = transform_name.find('[');
+
+        if (argument_start == std::string::npos)
+            return nullptr;
+
+        auto argument_width = transform_name.length() - 2 - argument_start;
+        std::string width = transform_name.substr(argument_start + 1, argument_width);
+        size_t truncate_width;
+        bool parsed = DB::tryParse<size_t>(truncate_width, width);
+
+        if (!parsed)
+            return nullptr;
+
+        return makeASTFunction("icebergTruncate", std::make_shared<DB::ASTLiteral>(truncate_width), std::make_shared<DB::ASTIdentifier>(column_name));
+    }
+    else
+    {
+        return nullptr;
+    }
+}
+
+std::unique_ptr<DB::ActionsDAG> ManifestFilesPruner::transformFilterDagForManifest(const DB::ActionsDAG * source_dag, std::vector<Int32> & used_columns_in_filter) const
+{
+    if (source_dag == nullptr)
+        return nullptr;
+
+    const auto & inputs = source_dag->getInputs();
+
+    for (const auto & input : inputs)
+    {
+        if (input->type == ActionsDAG::ActionType::INPUT)
+        {
+            std::string input_name = input->result_name;
+            std::optional<Int32> input_id = schema_processor.tryGetColumnIDByName(current_schema_id, input_name);
+            if (input_id)
+                used_columns_in_filter.push_back(*input_id);
+        }
+    }
+
+    ActionsDAG dag_with_renames;
+    for (const auto column_id : used_columns_in_filter)
+    {
+        auto column = schema_processor.tryGetFieldCharacteristics(current_schema_id, column_id);
+
+        /// Columns which we dropped and don't exist in current schema
+        /// cannot be queried in WHERE expression.
+        if (!column.has_value())
+            continue;
+
+        /// We take data type from manifest schema, not latest type
+        auto column_from_manifest = schema_processor.tryGetFieldCharacteristics(manifest_schema_id, column_id);
+        if (!column_from_manifest.has_value())
+            continue;
+
+        auto numeric_column_name = DB::backQuote(DB::toString(column_id));
+        const auto * node = &dag_with_renames.addInput(numeric_column_name, column_from_manifest->type);
+        node = &dag_with_renames.addAlias(*node, column->name);
+        dag_with_renames.getOutputs().push_back(node);
+    }
+    auto result = std::make_unique<DB::ActionsDAG>(DB::ActionsDAG::merge(std::move(dag_with_renames), source_dag->clone()));
+    result->removeUnusedActions();
+    return result;
+
+}
+
+
+ManifestFilesPruner::ManifestFilesPruner(
+    const DB::IcebergSchemaProcessor & schema_processor_,
+    Int32 current_schema_id_,
+    const DB::ActionsDAG * filter_dag,
+    const ManifestFileContent & manifest_file,
+    DB::ContextPtr context)
+    : schema_processor(schema_processor_)
+    , current_schema_id(current_schema_id_)
+    , manifest_schema_id(manifest_file.getSchemaId())
+{
+    std::unique_ptr<ActionsDAG> transformed_dag;
+    std::vector<Int32> used_columns_in_filter;
+    if (manifest_file.hasPartitionKey() || manifest_file.hasBoundsInfoInManifests())
+        transformed_dag = transformFilterDagForManifest(filter_dag, used_columns_in_filter);
+
+    if (manifest_file.hasPartitionKey())
+    {
+        partition_key = &manifest_file.getPartitionKeyDescription();
+        if (transformed_dag != nullptr)
+            partition_key_condition.emplace(transformed_dag.get(), context, partition_key->column_names, partition_key->expression, true /* single_point */);
+    }
+
+    if (manifest_file.hasBoundsInfoInManifests() && transformed_dag != nullptr)
+    {
+        {
+            const auto & bounded_colums = manifest_file.getColumnsIDsWithBounds();
+            for (Int32 used_column_id : used_columns_in_filter)
+            {
+                if (!bounded_colums.contains(used_column_id))
+                    continue;
+
+                NameAndTypePair name_and_type = schema_processor.getFieldCharacteristics(manifest_schema_id, used_column_id);
+                name_and_type.name = DB::backQuote(DB::toString(used_column_id));
+
+                ExpressionActionsPtr expression = std::make_shared<ExpressionActions>(
+                    ActionsDAG({name_and_type}), ExpressionActionsSettings(context));
+
+                min_max_key_conditions.emplace(used_column_id, KeyCondition(transformed_dag.get(), context, {name_and_type.name}, expression));
+            }
+        }
+    }
+}
+
+bool ManifestFilesPruner::canBePruned(const ManifestFileEntry & entry) const
+{
+    if (partition_key_condition.has_value())
+    {
+        const auto & partition_value = entry.partition_key_value;
+        std::vector<FieldRef> index_value(partition_value.begin(), partition_value.end());
+        for (auto & field : index_value)
+        {
+            // NULL_LAST
+            if (field.isNull())
+                field = POSITIVE_INFINITY;
+        }
+
+        bool can_be_true = partition_key_condition->mayBeTrueInRange(
+            partition_value.size(), index_value.data(), index_value.data(), partition_key->data_types);
+
+        if (!can_be_true)
+        {
+            ProfileEvents::increment(ProfileEvents::IcebergPartitionPrunnedFiles);
+            return true;
+        }
+    }
+
+    for (const auto & [column_id, key_condition] : min_max_key_conditions)
+    {
+        std::optional<NameAndTypePair> name_and_type = schema_processor.tryGetFieldCharacteristics(manifest_schema_id, column_id);
+
+        /// There is no such column in this manifest file
+        if (!name_and_type.has_value())
+            continue;
+
+        auto hyperrectangle = entry.columns_infos.at(column_id).hyperrectangle;
+        if (hyperrectangle.has_value() && !key_condition.mayBeTrueInRange(1, &hyperrectangle->left, &hyperrectangle->right, {name_and_type->type}))
+        {
+            ProfileEvents::increment(ProfileEvents::IcebergMinMaxIndexPrunnedFiles);
+            return true;
+        }
+    }
+
+    return false;
+}
+
+
+}
+
+#endif
diff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/PartitionPruning.h b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFilesPruning.h
similarity index 72%
rename from src/Storages/ObjectStorage/DataLakes/Iceberg/PartitionPruning.h
rename to src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFilesPruning.h
index 8b9fd80c612f..89a761265985 100644
--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/PartitionPruning.h
+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFilesPruning.h
@@ -20,20 +20,23 @@ class ManifestFileContent;
 DB::ASTPtr getASTFromTransform(const String & transform_name_src, const String & column_name);
 
 /// Prune specific data files based on manifest content
-class PartitionPruner
+class ManifestFilesPruner
 {
 private:
     const DB::IcebergSchemaProcessor & schema_processor;
     Int32 current_schema_id;
+    Int32 manifest_schema_id;
     const DB::KeyDescription * partition_key;
+    std::optional<DB::KeyCondition> partition_key_condition;
 
-    std::optional<DB::KeyCondition> key_condition;
-    /// NOTE: tricky part to support RENAME column in partition key.
+    std::unordered_map<Int32, DB::KeyCondition> min_max_key_conditions;
+    /// NOTE: tricky part to support RENAME column.
     /// Takes ActionDAG representation of user's WHERE expression and
-    /// rename columns to the their origina numeric id's in iceberg
-    std::unique_ptr<DB::ActionsDAG> transformFilterDagForManifest(const DB::ActionsDAG * source_dag, Int32 manifest_schema_id, const std::vector<Int32> & partition_column_ids) const;
+    /// rename columns to the their origina numeric ID's in iceberg
+    std::unique_ptr<DB::ActionsDAG> transformFilterDagForManifest(const DB::ActionsDAG * source_dag, std::vector<Int32> & used_columns_in_filter) const;
 public:
-    PartitionPruner(
+
+    ManifestFilesPruner(
         const DB::IcebergSchemaProcessor & schema_processor_,
         Int32 current_schema_id_,
         const DB::ActionsDAG * filter_dag,
diff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/PartitionPruning.cpp b/src/Storages/ObjectStorage/DataLakes/Iceberg/PartitionPruning.cpp
deleted file mode 100644
index 9e24494e071e..000000000000
--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/PartitionPruning.cpp
+++ /dev/null
@@ -1,144 +0,0 @@
-#include "config.h"
-
-#if USE_AVRO
-
-#include <Columns/ColumnNullable.h>
-#include <Columns/ColumnsDateTime.h>
-#include <Common/DateLUTImpl.h>
-#include <DataTypes/DataTypeNullable.h>
-#include <Common/logger_useful.h>
-#include <Parsers/ASTFunction.h>
-#include <Parsers/ASTIdentifier.h>
-#include <Parsers/ASTExpressionList.h>
-#include <Parsers/ASTLiteral.h>
-#include <IO/ReadHelpers.h>
-#include <Common/quoteString.h>
-
-#include <Storages/ObjectStorage/DataLakes/Iceberg/PartitionPruning.h>
-#include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h>
-
-using namespace DB;
-
-namespace Iceberg
-{
-
-DB::ASTPtr getASTFromTransform(const String & transform_name_src, const String & column_name)
-{
-    std::string transform_name = Poco::toLower(transform_name_src);
-
-    if (transform_name == "year" || transform_name == "years")
-        return makeASTFunction("toYearNumSinceEpoch", std::make_shared<DB::ASTIdentifier>(column_name));
-
-    if (transform_name == "month" || transform_name == "months")
-        return makeASTFunction("toMonthNumSinceEpoch", std::make_shared<DB::ASTIdentifier>(column_name));
-
-    if (transform_name == "day" || transform_name == "date" || transform_name == "days" || transform_name == "dates")
-        return makeASTFunction("toRelativeDayNum", std::make_shared<DB::ASTIdentifier>(column_name));
-
-    if (transform_name == "hour" || transform_name == "hours")
-        return makeASTFunction("toRelativeHourNum", std::make_shared<DB::ASTIdentifier>(column_name));
-
-    if (transform_name == "identity")
-        return std::make_shared<ASTIdentifier>(column_name);
-
-    if (transform_name == "void")
-        return makeASTFunction("tuple");
-
-    if (transform_name.starts_with("truncate"))
-    {
-        /// should look like transform[N]
-
-        if (transform_name.back() != ']')
-            return nullptr;
-
-        auto argument_start = transform_name.find('[');
-
-        if (argument_start == std::string::npos)
-            return nullptr;
-
-        auto argument_width = transform_name.length() - 2 - argument_start;
-        std::string width = transform_name.substr(argument_start + 1, argument_width);
-        size_t truncate_width;
-        bool parsed = DB::tryParse<size_t>(truncate_width, width);
-
-        if (!parsed)
-            return nullptr;
-
-        return makeASTFunction("icebergTruncate", std::make_shared<DB::ASTLiteral>(truncate_width), std::make_shared<DB::ASTIdentifier>(column_name));
-    }
-    else
-    {
-        return nullptr;
-    }
-}
-
-std::unique_ptr<DB::ActionsDAG> PartitionPruner::transformFilterDagForManifest(const DB::ActionsDAG * source_dag, Int32 manifest_schema_id, const std::vector<Int32> & partition_column_ids) const
-{
-    if (source_dag == nullptr)
-        return nullptr;
-
-    ActionsDAG dag_with_renames;
-    for (const auto column_id : partition_column_ids)
-    {
-        auto column = schema_processor.tryGetFieldCharacteristics(current_schema_id, column_id);
-
-        /// Columns which we dropped and doesn't exist in current schema
-        /// cannot be queried in WHERE expression.
-        if (!column.has_value())
-            continue;
-
-        /// We take data type from manifest schema, not latest type
-        auto column_type = schema_processor.getFieldCharacteristics(manifest_schema_id, column_id).type;
-        auto numeric_column_name = DB::backQuote(DB::toString(column_id));
-        const auto * node = &dag_with_renames.addInput(numeric_column_name, column_type);
-        node = &dag_with_renames.addAlias(*node, column->name);
-        dag_with_renames.getOutputs().push_back(node);
-    }
-    auto result = std::make_unique<DB::ActionsDAG>(DB::ActionsDAG::merge(std::move(dag_with_renames), source_dag->clone()));
-    result->removeUnusedActions();
-    return result;
-
-}
-
-PartitionPruner::PartitionPruner(
-    const DB::IcebergSchemaProcessor & schema_processor_,
-    Int32 current_schema_id_,
-    const DB::ActionsDAG * filter_dag,
-    const ManifestFileContent & manifest_file,
-    DB::ContextPtr context)
-    : schema_processor(schema_processor_)
-    , current_schema_id(current_schema_id_)
-{
-    if (manifest_file.hasPartitionKey())
-    {
-        partition_key = &manifest_file.getPartitionKeyDescription();
-        auto transformed_dag = transformFilterDagForManifest(filter_dag, manifest_file.getSchemaId(), manifest_file.getPartitionKeyColumnIDs());
-        if (transformed_dag != nullptr)
-            key_condition.emplace(transformed_dag.get(), context, partition_key->column_names, partition_key->expression, true /* single_point */);
-    }
-}
-
-bool PartitionPruner::canBePruned(const ManifestFileEntry & entry) const
-{
-    if (!key_condition.has_value())
-        return false;
-
-    const auto & partition_value = entry.partition_key_value;
-    std::vector<FieldRef> index_value(partition_value.begin(), partition_value.end());
-    for (auto & field : index_value)
-    {
-        // NULL_LAST
-        if (field.isNull())
-            field = POSITIVE_INFINITY;
-    }
-
-    bool can_be_true = key_condition->mayBeTrueInRange(
-        partition_value.size(), index_value.data(), index_value.data(), partition_key->data_types);
-
-    return !can_be_true;
-}
-
-
-}
-
-#endif
diff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/SchemaProcessor.cpp b/src/Storages/ObjectStorage/DataLakes/Iceberg/SchemaProcessor.cpp
index 961d129f71ca..acd057ff4a97 100644
--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/SchemaProcessor.cpp
+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/SchemaProcessor.cpp
@@ -117,6 +117,7 @@ void IcebergSchemaProcessor::addIcebergTableSchema(Poco::JSON::Object::Ptr schem
             auto type = getFieldType(field, "type", required, current_full_name, true);
             clickhouse_schema->push_back(NameAndTypePair{name, type});
             clickhouse_types_by_source_ids[{schema_id, field->getValue<Int32>("id")}] = NameAndTypePair{current_full_name, type};
+            clickhouse_ids_by_source_names[{schema_id, current_full_name}] = field->getValue<Int32>("id");
         }
         clickhouse_table_schemas_by_ids[schema_id] = clickhouse_schema;
     }
@@ -139,6 +140,14 @@ std::optional<NameAndTypePair> IcebergSchemaProcessor::tryGetFieldCharacteristic
     return it->second;
 }
 
+std::optional<Int32> IcebergSchemaProcessor::tryGetColumnIDByName(Int32 schema_id, const std::string & name) const
+{
+    auto it = clickhouse_ids_by_source_names.find({schema_id, name});
+    if (it == clickhouse_ids_by_source_names.end())
+        return {};
+    return it->second;
+}
+
 NamesAndTypesList IcebergSchemaProcessor::tryGetFieldsCharacteristics(Int32 schema_id, const std::vector<Int32> & source_ids) const
 {
     NamesAndTypesList fields;
@@ -232,6 +241,8 @@ IcebergSchemaProcessor::getComplexTypeFromObject(const Poco::JSON::Object::Ptr &
                 element_types.push_back(getFieldType(field, "type", required, current_full_name, true));
                 clickhouse_types_by_source_ids[{current_schema_id.value(), field->getValue<Int32>("id")}]
                     = NameAndTypePair{current_full_name, element_types.back()};
+
+                clickhouse_ids_by_source_names[{current_schema_id.value(), current_full_name}] = field->getValue<Int32>("id");
             }
             else
             {
diff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/SchemaProcessor.h b/src/Storages/ObjectStorage/DataLakes/Iceberg/SchemaProcessor.h
index de003456cef2..c0e9b57909b8 100644
--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/SchemaProcessor.h
+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/SchemaProcessor.h
@@ -82,6 +82,7 @@ class IcebergSchemaProcessor
     NameAndTypePair getFieldCharacteristics(Int32 schema_version, Int32 source_id) const;
     std::optional<NameAndTypePair> tryGetFieldCharacteristics(Int32 schema_version, Int32 source_id) const;
     NamesAndTypesList tryGetFieldsCharacteristics(Int32 schema_id, const std::vector<Int32> & source_ids) const;
+    std::optional<Int32> tryGetColumnIDByName(Int32 schema_id, const std::string & name) const;
 
     bool hasClickhouseTableSchemaById(Int32 id) const;
 
@@ -91,6 +92,7 @@ class IcebergSchemaProcessor
     std::unordered_map<Int32, std::shared_ptr<NamesAndTypesList>> clickhouse_table_schemas_by_ids;
     std::map<std::pair<Int32, Int32>, std::shared_ptr<ActionsDAG>> transform_dags_by_ids;
     mutable std::map<std::pair<Int32, Int32>, NameAndTypePair> clickhouse_types_by_source_ids;
+    mutable std::map<std::pair<Int32, std::string>, Int32> clickhouse_ids_by_source_names;
 
     NamesAndTypesList getSchemaType(const Poco::JSON::Object::Ptr & schema);
     DataTypePtr getComplexTypeFromObject(const Poco::JSON::Object::Ptr & type, String & current_full_name, bool is_subfield_of_root);
