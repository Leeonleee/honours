{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 11283,
  "instance_id": "ClickHouse__ClickHouse-11283",
  "issue_numbers": [
    "9337"
  ],
  "base_commit": "9a34310ead9df490212d2b3d9253c2d79f133e6e",
  "patch": "diff --git a/src/Storages/Kafka/KafkaBlockInputStream.cpp b/src/Storages/Kafka/KafkaBlockInputStream.cpp\nindex a2403e66c504..3e4533f8bb28 100644\n--- a/src/Storages/Kafka/KafkaBlockInputStream.cpp\n+++ b/src/Storages/Kafka/KafkaBlockInputStream.cpp\n@@ -19,9 +19,8 @@ KafkaBlockInputStream::KafkaBlockInputStream(\n     , column_names(columns)\n     , max_block_size(max_block_size_)\n     , commit_in_suffix(commit_in_suffix_)\n-    , non_virtual_header(storage.getSampleBlockNonMaterialized()) /// FIXME: add materialized columns support\n-    , virtual_header(storage.getSampleBlockForColumns({\"_topic\", \"_key\", \"_offset\", \"_partition\", \"_timestamp\"}))\n-\n+    , non_virtual_header(storage.getSampleBlockNonMaterialized())\n+    , virtual_header(storage.getSampleBlockForColumns({\"_topic\", \"_key\", \"_offset\", \"_partition\", \"_timestamp\",\"_timestamp_ms\",\"_headers.name\",\"_headers.value\"}))\n {\n     context.setSetting(\"input_format_skip_unknown_fields\", 1u); // Always skip unknown fields regardless of the context (JSON or TSKV)\n     context.setSetting(\"input_format_allow_errors_ratio\", 0.);\n@@ -141,8 +140,22 @@ Block KafkaBlockInputStream::readImpl()\n         auto offset        = buffer->currentOffset();\n         auto partition     = buffer->currentPartition();\n         auto timestamp_raw = buffer->currentTimestamp();\n-        auto timestamp     = timestamp_raw ? std::chrono::duration_cast<std::chrono::seconds>(timestamp_raw->get_timestamp()).count()\n-                                                : 0;\n+        auto header_list   = buffer->currentHeaderList();\n+\n+        Array headers_names;\n+        Array headers_values;\n+\n+        if (!header_list.empty())\n+        {\n+            headers_names.reserve(header_list.size());\n+            headers_values.reserve(header_list.size());\n+            for (const auto & header : header_list)\n+            {\n+                headers_names.emplace_back(header.get_name());\n+                headers_values.emplace_back(static_cast<std::string>(header.get_value()));\n+            }\n+        }\n+\n         for (size_t i = 0; i < new_rows; ++i)\n         {\n             virtual_columns[0]->insert(topic);\n@@ -151,12 +164,17 @@ Block KafkaBlockInputStream::readImpl()\n             virtual_columns[3]->insert(partition);\n             if (timestamp_raw)\n             {\n-                virtual_columns[4]->insert(timestamp);\n+                auto ts = timestamp_raw->get_timestamp();\n+                virtual_columns[4]->insert(std::chrono::duration_cast<std::chrono::seconds>(ts).count());\n+                virtual_columns[5]->insert(DecimalField<Decimal64>(std::chrono::duration_cast<std::chrono::milliseconds>(ts).count(),3));\n             }\n             else\n             {\n                 virtual_columns[4]->insertDefault();\n+                virtual_columns[5]->insertDefault();\n             }\n+            virtual_columns[6]->insert(headers_names);\n+            virtual_columns[7]->insert(headers_values);\n         }\n \n         total_rows = total_rows + new_rows;\ndiff --git a/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h b/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h\nindex e90e3b488816..7449f58c838f 100644\n--- a/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h\n+++ b/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h\n@@ -49,6 +49,7 @@ class ReadBufferFromKafkaConsumer : public ReadBuffer\n     auto currentOffset() const { return current[-1].get_offset(); }\n     auto currentPartition() const { return current[-1].get_partition(); }\n     auto currentTimestamp() const { return current[-1].get_timestamp(); }\n+    const auto & currentHeaderList() const { return current[-1].get_header_list(); }\n \n private:\n     using Messages = std::vector<cppkafka::Message>;\ndiff --git a/src/Storages/Kafka/StorageKafka.cpp b/src/Storages/Kafka/StorageKafka.cpp\nindex 7731cf3c06a9..d1014fdb0f83 100644\n--- a/src/Storages/Kafka/StorageKafka.cpp\n+++ b/src/Storages/Kafka/StorageKafka.cpp\n@@ -6,9 +6,11 @@\n #include <DataStreams/UnionBlockInputStream.h>\n #include <DataStreams/copyData.h>\n #include <DataTypes/DataTypeDateTime.h>\n+#include <DataTypes/DataTypeDateTime64.h>\n #include <DataTypes/DataTypeNullable.h>\n #include <DataTypes/DataTypesNumber.h>\n #include <DataTypes/DataTypeString.h>\n+#include <DataTypes/DataTypeArray.h>\n #include <Interpreters/InterpreterInsertQuery.h>\n #include <Interpreters/evaluateConstantExpression.h>\n #include <Parsers/ASTCreateQuery.h>\n@@ -724,7 +726,10 @@ NamesAndTypesList StorageKafka::getVirtuals() const\n         {\"_key\", std::make_shared<DataTypeString>()},\n         {\"_offset\", std::make_shared<DataTypeUInt64>()},\n         {\"_partition\", std::make_shared<DataTypeUInt64>()},\n-        {\"_timestamp\", std::make_shared<DataTypeNullable>(std::make_shared<DataTypeDateTime>())}\n+        {\"_timestamp\", std::make_shared<DataTypeNullable>(std::make_shared<DataTypeDateTime>())},\n+        {\"_timestamp_ms\", std::make_shared<DataTypeNullable>(std::make_shared<DataTypeDateTime64>(3))},\n+        {\"_headers.name\", std::make_shared<DataTypeArray>(std::make_shared<DataTypeString>())},\n+        {\"_headers.value\", std::make_shared<DataTypeArray>(std::make_shared<DataTypeString>())}\n     };\n }\n \n",
  "test_patch": "diff --git a/tests/integration/test_storage_kafka/test.py b/tests/integration/test_storage_kafka/test.py\nindex 13577864870f..82b409aa85e8 100644\n--- a/tests/integration/test_storage_kafka/test.py\n+++ b/tests/integration/test_storage_kafka/test.py\n@@ -840,28 +840,28 @@ def test_kafka_virtual_columns2(kafka_cluster):\n                      kafka_format = 'JSONEachRow';\n \n         CREATE MATERIALIZED VIEW test.view Engine=Log AS\n-        SELECT value, _key, _topic, _partition, _offset, toUnixTimestamp(_timestamp) FROM test.kafka;\n+        SELECT value, _key, _topic, _partition, _offset, toUnixTimestamp(_timestamp), toUnixTimestamp64Milli(_timestamp_ms), _headers.name, _headers.value FROM test.kafka;\n         ''')\n \n     producer = KafkaProducer(bootstrap_servers=\"localhost:9092\")\n \n-    producer.send(topic='virt2_0', value=json.dumps({'value': 1}), partition=0, key='k1', timestamp_ms=1577836801000)\n-    producer.send(topic='virt2_0', value=json.dumps({'value': 2}), partition=0, key='k2', timestamp_ms=1577836802000)\n+    producer.send(topic='virt2_0', value=json.dumps({'value': 1}), partition=0, key='k1', timestamp_ms=1577836801001, headers=[('content-encoding', b'base64')])\n+    producer.send(topic='virt2_0', value=json.dumps({'value': 2}), partition=0, key='k2', timestamp_ms=1577836802002, headers=[('empty_value', ''),('', 'empty name'), ('',''), ('repetition', '1'), ('repetition', '2')])\n     producer.flush()\n     time.sleep(1)\n \n-    producer.send(topic='virt2_0', value=json.dumps({'value': 3}), partition=1, key='k3', timestamp_ms=1577836803000)\n-    producer.send(topic='virt2_0', value=json.dumps({'value': 4}), partition=1, key='k4', timestamp_ms=1577836804000)\n+    producer.send(topic='virt2_0', value=json.dumps({'value': 3}), partition=1, key='k3', timestamp_ms=1577836803003, headers=[('b', 'b'),('a', 'a')])\n+    producer.send(topic='virt2_0', value=json.dumps({'value': 4}), partition=1, key='k4', timestamp_ms=1577836804004, headers=[('a', 'a'),('b', 'b')])\n     producer.flush()\n     time.sleep(1)\n \n-    producer.send(topic='virt2_1', value=json.dumps({'value': 5}), partition=0, key='k5', timestamp_ms=1577836805000)\n-    producer.send(topic='virt2_1', value=json.dumps({'value': 6}), partition=0, key='k6', timestamp_ms=1577836806000)\n+    producer.send(topic='virt2_1', value=json.dumps({'value': 5}), partition=0, key='k5', timestamp_ms=1577836805005)\n+    producer.send(topic='virt2_1', value=json.dumps({'value': 6}), partition=0, key='k6', timestamp_ms=1577836806006)\n     producer.flush()\n     time.sleep(1)\n \n-    producer.send(topic='virt2_1', value=json.dumps({'value': 7}), partition=1, key='k7', timestamp_ms=1577836807000)\n-    producer.send(topic='virt2_1', value=json.dumps({'value': 8}), partition=1, key='k8', timestamp_ms=1577836808000)\n+    producer.send(topic='virt2_1', value=json.dumps({'value': 7}), partition=1, key='k7', timestamp_ms=1577836807007)\n+    producer.send(topic='virt2_1', value=json.dumps({'value': 8}), partition=1, key='k8', timestamp_ms=1577836808008)\n     producer.flush()\n \n     time.sleep(10)\n@@ -869,14 +869,14 @@ def test_kafka_virtual_columns2(kafka_cluster):\n     result = instance.query(\"SELECT * FROM test.view ORDER BY value\", ignore_error=True)\n \n     expected = '''\\\n-1\tk1\tvirt2_0\t0\t0\t1577836801\n-2\tk2\tvirt2_0\t0\t1\t1577836802\n-3\tk3\tvirt2_0\t1\t0\t1577836803\n-4\tk4\tvirt2_0\t1\t1\t1577836804\n-5\tk5\tvirt2_1\t0\t0\t1577836805\n-6\tk6\tvirt2_1\t0\t1\t1577836806\n-7\tk7\tvirt2_1\t1\t0\t1577836807\n-8\tk8\tvirt2_1\t1\t1\t1577836808\n+1\tk1\tvirt2_0\t0\t0\t1577836801\t1577836801001\t['content-encoding']\t['base64']\n+2\tk2\tvirt2_0\t0\t1\t1577836802\t1577836802002\t['empty_value','','','repetition','repetition']\t['','empty name','','1','2']\n+3\tk3\tvirt2_0\t1\t0\t1577836803\t1577836803003\t['b','a']\t['b','a']\n+4\tk4\tvirt2_0\t1\t1\t1577836804\t1577836804004\t['a','b']\t['a','b']\n+5\tk5\tvirt2_1\t0\t0\t1577836805\t1577836805005\t[]\t[]\n+6\tk6\tvirt2_1\t0\t1\t1577836806\t1577836806006\t[]\t[]\n+7\tk7\tvirt2_1\t1\t0\t1577836807\t1577836807007\t[]\t[]\n+8\tk8\tvirt2_1\t1\t1\t1577836808\t1577836808008\t[]\t[]\n '''\n \n     assert TSV(result) == TSV(expected)\n",
  "problem_statement": "Kafka headers\nHi! We intend to use the Kafka Engine (https://clickhouse.tech/docs/ru/operations/table_engines/kafka/) for storing events in Clickhouse.\r\nAlso, some of the necessary information is stored in the Kafka's headers (https://issues.apache.org/jira/browse/KAFKA-4208) and we would like to store them too.\r\nIs there a way to achive this ?\n",
  "hints_text": "Right now it's not supported.\r\n\r\nWe can add support for that in future version in pair of virtual columns like\r\n```\r\n_header_names Array(String),\r\n_header_values Array(String)\r\n```\r\nWould such a solution satisfy your needs?\nWe are willing to have this feature too. Would be very helpful.\nIt's ok for us to use 2 arrays if this approach guarantees the order. \r\nI mean we'll be able to get header's value somehow like this: \r\n`arrayElement(header_values, indexOf(header_names, 'header_name')) as header_name`\n@filimonov It would be great to have it indeed, thanks a lot!",
  "created_at": "2020-05-29T09:05:50Z",
  "modified_files": [
    "src/Storages/Kafka/KafkaBlockInputStream.cpp",
    "src/Storages/Kafka/ReadBufferFromKafkaConsumer.h",
    "src/Storages/Kafka/StorageKafka.cpp"
  ],
  "modified_test_files": [
    "tests/integration/test_storage_kafka/test.py"
  ]
}