{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 23051,
  "instance_id": "ClickHouse__ClickHouse-23051",
  "issue_numbers": [
    "15171"
  ],
  "base_commit": "355cc09004fa31937b2f2e65458126b5d0bcade9",
  "patch": "diff --git a/src/Common/isValidUTF8.cpp b/src/Common/isValidUTF8.cpp\nnew file mode 100644\nindex 000000000000..f9589e3730b0\n--- /dev/null\n+++ b/src/Common/isValidUTF8.cpp\n@@ -0,0 +1,131 @@\n+#include <Common/isValidUTF8.h>\n+#include <cstring>\n+\n+/// inspired by https://github.com/cyb70289/utf8/\n+\n+/*\n+MIT License\n+\n+Copyright (c) 2019 Yibo Cai\n+\n+Permission is hereby granted, free of charge, to any person obtaining a copy\n+of this software and associated documentation files (the \"Software\"), to deal\n+in the Software without restriction, including without limitation the rights\n+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+copies of the Software, and to permit persons to whom the Software is\n+furnished to do so, subject to the following conditions:\n+\n+The above copyright notice and this permission notice shall be included in all\n+copies or substantial portions of the Software.\n+\n+THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n+SOFTWARE.\n+*/\n+\n+/*\n+* http://www.unicode.org/versions/Unicode6.0.0/ch03.pdf - page 94\n+*\n+* Table 3-7. Well-Formed UTF-8 Byte Sequences\n+*\n+* +--------------------+------------+-------------+------------+-------------+\n+* | Code Points        | First Byte | Second Byte | Third Byte | Fourth Byte |\n+* +--------------------+------------+-------------+------------+-------------+\n+* | U+0000..U+007F     | 00..7F     |             |            |             |\n+* +--------------------+------------+-------------+------------+-------------+\n+* | U+0080..U+07FF     | C2..DF     | 80..BF      |            |             |\n+* +--------------------+------------+-------------+------------+-------------+\n+* | U+0800..U+0FFF     | E0         | A0..BF      | 80..BF     |             |\n+* +--------------------+------------+-------------+------------+-------------+\n+* | U+1000..U+CFFF     | E1..EC     | 80..BF      | 80..BF     |             |\n+* +--------------------+------------+-------------+------------+-------------+\n+* | U+D000..U+D7FF     | ED         | 80..9F      | 80..BF     |             |\n+* +--------------------+------------+-------------+------------+-------------+\n+* | U+E000..U+FFFF     | EE..EF     | 80..BF      | 80..BF     |             |\n+* +--------------------+------------+-------------+------------+-------------+\n+* | U+10000..U+3FFFF   | F0         | 90..BF      | 80..BF     | 80..BF      |\n+* +--------------------+------------+-------------+------------+-------------+\n+* | U+40000..U+FFFFF   | F1..F3     | 80..BF      | 80..BF     | 80..BF      |\n+* +--------------------+------------+-------------+------------+-------------+\n+* | U+100000..U+10FFFF | F4         | 80..8F      | 80..BF     | 80..BF      |\n+* +--------------------+------------+-------------+------------+-------------+\n+*/\n+namespace DB\n+{\n+\n+namespace UTF8\n+{\n+\n+UInt8 isValidUTF8(const UInt8 * data, UInt64 len)\n+{\n+    while (len)\n+    {\n+        int bytes;\n+        const UInt8 byte1 = data[0];\n+        /* 00..7F */\n+        if (byte1 <= 0x7F)\n+        {\n+            bytes = 1;\n+        }\n+        /* C2..DF, 80..BF */\n+        else if (len >= 2 && byte1 >= 0xC2 && byte1 <= 0xDF && static_cast<Int8>(data[1]) <= static_cast<Int8>(0xBF))\n+        {\n+            bytes = 2;\n+        }\n+        else if (len >= 3)\n+        {\n+            const UInt8 byte2 = data[1];\n+            bool byte2_ok = static_cast<Int8>(byte2) <= static_cast<Int8>(0xBF);\n+            bool byte3_ok = static_cast<Int8>(data[2]) <= static_cast<Int8>(0xBF);\n+\n+            if (byte2_ok && byte3_ok &&\n+                /* E0, A0..BF, 80..BF */\n+                ((byte1 == 0xE0 && byte2 >= 0xA0) ||\n+                 /* E1..EC, 80..BF, 80..BF */\n+                 (byte1 >= 0xE1 && byte1 <= 0xEC) ||\n+                 /* ED, 80..9F, 80..BF */\n+                 (byte1 == 0xED && byte2 <= 0x9F) ||\n+                 /* EE..EF, 80..BF, 80..BF */\n+                 (byte1 >= 0xEE && byte1 <= 0xEF)))\n+            {\n+                bytes = 3;\n+            }\n+            else if (len >= 4)\n+            {\n+                bool byte4_ok = static_cast<Int8>(data[3]) <= static_cast<Int8>(0xBF);\n+                if (byte2_ok && byte3_ok && byte4_ok &&\n+                    /* F0, 90..BF, 80..BF, 80..BF */\n+                    ((byte1 == 0xF0 && byte2 >= 0x90) ||\n+                     /* F1..F3, 80..BF, 80..BF, 80..BF */\n+                     (byte1 >= 0xF1 && byte1 <= 0xF3) ||\n+                     /* F4, 80..8F, 80..BF, 80..BF */\n+                     (byte1 == 0xF4 && byte2 <= 0x8F)))\n+                {\n+                    bytes = 4;\n+                }\n+                else\n+                {\n+                    return false;\n+                }\n+            }\n+            else\n+            {\n+                return false;\n+            }\n+        }\n+        else\n+        {\n+            return false;\n+        }\n+        len -= bytes;\n+        data += bytes;\n+    }\n+    return true;\n+}\n+\n+}\n+}\ndiff --git a/src/Common/isValidUTF8.h b/src/Common/isValidUTF8.h\nnew file mode 100644\nindex 000000000000..e2396852845f\n--- /dev/null\n+++ b/src/Common/isValidUTF8.h\n@@ -0,0 +1,10 @@\n+#pragma once\n+\n+#include <common/types.h>\n+\n+namespace DB::UTF8\n+{\n+\n+UInt8 isValidUTF8(const UInt8 * data, UInt64 len);\n+\n+}\ndiff --git a/src/Common/ya.make b/src/Common/ya.make\nindex 82962123e56d..c3be16c6e076 100644\n--- a/src/Common/ya.make\n+++ b/src/Common/ya.make\n@@ -116,6 +116,7 @@ SRCS(\n     hasLinuxCapability.cpp\n     hex.cpp\n     isLocalAddress.cpp\n+    isValidUTF8.cpp\n     malloc.cpp\n     memory.cpp\n     new_delete.cpp\ndiff --git a/src/Functions/isValidUTF8.cpp b/src/Functions/isValidUTF8.cpp\nindex abdda53990d6..0168a36b5b52 100644\n--- a/src/Functions/isValidUTF8.cpp\n+++ b/src/Functions/isValidUTF8.cpp\n@@ -1,14 +1,7 @@\n #include <DataTypes/DataTypeString.h>\n #include <Functions/FunctionFactory.h>\n #include <Functions/FunctionStringOrArrayToT.h>\n-\n-#include <cstring>\n-\n-#ifdef __SSE4_1__\n-#    include <emmintrin.h>\n-#    include <smmintrin.h>\n-#    include <tmmintrin.h>\n-#endif\n+#include <Common/isValidUTF8.h>\n \n namespace DB\n {\n@@ -71,75 +64,8 @@ SOFTWARE.\n  * +--------------------+------------+-------------+------------+-------------+\n  */\n \n-    static inline UInt8 isValidUTF8Naive(const UInt8 * data, UInt64 len)\n-    {\n-        while (len)\n-        {\n-            int bytes;\n-            const UInt8 byte1 = data[0];\n-            /* 00..7F */\n-            if (byte1 <= 0x7F)\n-            {\n-                bytes = 1;\n-            }\n-            /* C2..DF, 80..BF */\n-            else if (len >= 2 && byte1 >= 0xC2 && byte1 <= 0xDF && static_cast<Int8>(data[1]) <= static_cast<Int8>(0xBF))\n-            {\n-                bytes = 2;\n-            }\n-            else if (len >= 3)\n-            {\n-                const UInt8 byte2 = data[1];\n-                bool byte2_ok = static_cast<Int8>(byte2) <= static_cast<Int8>(0xBF);\n-                bool byte3_ok = static_cast<Int8>(data[2]) <= static_cast<Int8>(0xBF);\n-\n-                if (byte2_ok && byte3_ok &&\n-                    /* E0, A0..BF, 80..BF */\n-                    ((byte1 == 0xE0 && byte2 >= 0xA0) ||\n-                     /* E1..EC, 80..BF, 80..BF */\n-                     (byte1 >= 0xE1 && byte1 <= 0xEC) ||\n-                     /* ED, 80..9F, 80..BF */\n-                     (byte1 == 0xED && byte2 <= 0x9F) ||\n-                     /* EE..EF, 80..BF, 80..BF */\n-                     (byte1 >= 0xEE && byte1 <= 0xEF)))\n-                {\n-                    bytes = 3;\n-                }\n-                else if (len >= 4)\n-                {\n-                    bool byte4_ok = static_cast<Int8>(data[3]) <= static_cast<Int8>(0xBF);\n-                    if (byte2_ok && byte3_ok && byte4_ok &&\n-                        /* F0, 90..BF, 80..BF, 80..BF */\n-                        ((byte1 == 0xF0 && byte2 >= 0x90) ||\n-                         /* F1..F3, 80..BF, 80..BF, 80..BF */\n-                         (byte1 >= 0xF1 && byte1 <= 0xF3) ||\n-                         /* F4, 80..8F, 80..BF, 80..BF */\n-                         (byte1 == 0xF4 && byte2 <= 0x8F)))\n-                    {\n-                        bytes = 4;\n-                    }\n-                    else\n-                    {\n-                        return false;\n-                    }\n-                }\n-                else\n-                {\n-                    return false;\n-                }\n-            }\n-            else\n-            {\n-                return false;\n-            }\n-            len -= bytes;\n-            data += bytes;\n-        }\n-        return true;\n-    }\n-\n #ifndef __SSE4_1__\n-    static inline UInt8 isValidUTF8(const UInt8 * data, UInt64 len) { return isValidUTF8Naive(data, len); }\n+    static inline UInt8 isValidUTF8(const UInt8 * data, UInt64 len) { return DB::UTF8::isValidUTF8(data, len); }\n #else\n     static inline UInt8 isValidUTF8(const UInt8 * data, UInt64 len)\n     {\ndiff --git a/src/IO/S3Common.cpp b/src/IO/S3Common.cpp\nindex 74c328661c48..ff06d6777e74 100644\n--- a/src/IO/S3Common.cpp\n+++ b/src/IO/S3Common.cpp\n@@ -2,9 +2,10 @@\n \n #if USE_AWS_S3\n \n+#    include <IO/S3Common.h>\n+\n #    include <Common/quoteString.h>\n \n-#    include <IO/S3Common.h>\n #    include <IO/WriteBufferFromString.h>\n #    include <Storages/StorageS3Settings.h>\n \n@@ -617,7 +618,7 @@ namespace S3\n         storage_name = S3;\n \n         if (uri.getHost().empty())\n-            throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Host is empty in S3 URI: {}\", uri.toString());\n+            throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Host is empty in S3 URI.\");\n \n         String name;\n         String endpoint_authority_from_uri;\n@@ -626,12 +627,7 @@ namespace S3\n         {\n             is_virtual_hosted_style = true;\n             endpoint = uri.getScheme() + \"://\" + name + endpoint_authority_from_uri;\n-\n-            /// S3 specification requires at least 3 and at most 63 characters in bucket name.\n-            /// https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-s3-bucket-naming-requirements.html\n-            if (bucket.length() < 3 || bucket.length() > 63)\n-                throw Exception(ErrorCodes::BAD_ARGUMENTS,\n-                    \"Bucket name length is out of bounds in virtual hosted style S3 URI: {} ({})\", quoteString(bucket), uri.toString());\n+            validateBucket(bucket, uri);\n \n             if (!uri.getPath().empty())\n             {\n@@ -642,7 +638,7 @@ namespace S3\n             boost::to_upper(name);\n             if (name != S3 && name != COS)\n             {\n-                throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Object storage system name is unrecognized in virtual hosted style S3 URI: {} ({})\", quoteString(name), uri.toString());\n+                throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Object storage system name is unrecognized in virtual hosted style S3 URI: {}\", quoteString(name));\n             }\n             if (name == S3)\n             {\n@@ -657,14 +653,19 @@ namespace S3\n         {\n             is_virtual_hosted_style = false;\n             endpoint = uri.getScheme() + \"://\" + uri.getAuthority();\n-\n-            /// S3 specification requires at least 3 and at most 63 characters in bucket name.\n-            /// https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-s3-bucket-naming-requirements.html\n-            if (bucket.length() < 3 || bucket.length() > 63)\n-                throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Bucket name length is out of bounds in virtual hosted style S3 URI: {} ({})\", quoteString(bucket), uri.toString());\n+            validateBucket(bucket, uri);\n         }\n         else\n-            throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Bucket or key name are invalid in S3 URI: {}\", uri.toString());\n+            throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Bucket or key name are invalid in S3 URI.\");\n+    }\n+\n+    void URI::validateBucket(const String & bucket, const Poco::URI & uri)\n+    {\n+        /// S3 specification requires at least 3 and at most 63 characters in bucket name.\n+        /// https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-s3-bucket-naming-requirements.html\n+        if (bucket.length() < 3 || bucket.length() > 63)\n+            throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Bucket name length is out of bounds in virtual hosted style S3 URI:     {}{}\",\n+                            quoteString(bucket), !uri.empty() ? \" (\" + uri.toString() + \")\" : \"\");\n     }\n }\n \ndiff --git a/src/IO/S3Common.h b/src/IO/S3Common.h\nindex 886230e36c66..20ec982138a5 100644\n--- a/src/IO/S3Common.h\n+++ b/src/IO/S3Common.h\n@@ -74,6 +74,8 @@ struct URI\n     bool is_virtual_hosted_style;\n \n     explicit URI(const Poco::URI & uri_);\n+\n+    static void validateBucket(const String & bucket, const Poco::URI & uri);\n };\n \n }\ndiff --git a/src/Interpreters/InterpreterInsertQuery.cpp b/src/Interpreters/InterpreterInsertQuery.cpp\nindex 3589176f231b..6742b93fc713 100644\n--- a/src/Interpreters/InterpreterInsertQuery.cpp\n+++ b/src/Interpreters/InterpreterInsertQuery.cpp\n@@ -37,6 +37,7 @@ namespace DB\n \n namespace ErrorCodes\n {\n+    extern const int NOT_IMPLEMENTED;\n     extern const int NO_SUCH_COLUMN_IN_TABLE;\n     extern const int ILLEGAL_COLUMN;\n     extern const int DUPLICATE_COLUMN;\n@@ -155,6 +156,9 @@ BlockIO InterpreterInsertQuery::execute()\n     BlockIO res;\n \n     StoragePtr table = getTable(query);\n+    if (query.partition_by && !table->supportsPartitionBy())\n+        throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"PARTITION BY clause is not supported by storage\");\n+\n     auto table_lock = table->lockForShare(getContext()->getInitialQueryId(), settings.lock_acquire_timeout);\n     auto metadata_snapshot = table->getInMemoryMetadataPtr();\n \ndiff --git a/src/Parsers/ASTInsertQuery.cpp b/src/Parsers/ASTInsertQuery.cpp\nindex 39ae5f2a58aa..745585ae175b 100644\n--- a/src/Parsers/ASTInsertQuery.cpp\n+++ b/src/Parsers/ASTInsertQuery.cpp\n@@ -25,6 +25,11 @@ void ASTInsertQuery::formatImpl(const FormatSettings & settings, FormatState & s\n     {\n         settings.ostr << (settings.hilite ? hilite_keyword : \"\") << \"FUNCTION \";\n         table_function->formatImpl(settings, state, frame);\n+        if (partition_by)\n+        {\n+            settings.ostr << \" PARTITION BY \";\n+            partition_by->formatImpl(settings, state, frame);\n+        }\n     }\n     else\n         settings.ostr << (settings.hilite ? hilite_none : \"\")\ndiff --git a/src/Parsers/ASTInsertQuery.h b/src/Parsers/ASTInsertQuery.h\nindex e98fe79dedbd..6eab3e7acac4 100644\n--- a/src/Parsers/ASTInsertQuery.h\n+++ b/src/Parsers/ASTInsertQuery.h\n@@ -20,6 +20,7 @@ class ASTInsertQuery : public IAST\n     ASTPtr infile;\n     ASTPtr watch;\n     ASTPtr table_function;\n+    ASTPtr partition_by;\n     ASTPtr settings_ast;\n \n     /// Data to insert\n@@ -44,6 +45,7 @@ class ASTInsertQuery : public IAST\n         if (select) { res->select = select->clone(); res->children.push_back(res->select); }\n         if (watch) { res->watch = watch->clone(); res->children.push_back(res->watch); }\n         if (table_function) { res->table_function = table_function->clone(); res->children.push_back(res->table_function); }\n+        if (partition_by) { res->partition_by = partition_by->clone(); res->children.push_back(res->partition_by); }\n         if (settings_ast) { res->settings_ast = settings_ast->clone(); res->children.push_back(res->settings_ast); }\n \n         return res;\ndiff --git a/src/Parsers/ParserInsertQuery.cpp b/src/Parsers/ParserInsertQuery.cpp\nindex d597e572437d..19457f027bf7 100644\n--- a/src/Parsers/ParserInsertQuery.cpp\n+++ b/src/Parsers/ParserInsertQuery.cpp\n@@ -35,6 +35,7 @@ bool ParserInsertQuery::parseImpl(Pos & pos, ASTPtr & node, Expected & expected)\n     ParserKeyword s_settings(\"SETTINGS\");\n     ParserKeyword s_select(\"SELECT\");\n     ParserKeyword s_watch(\"WATCH\");\n+    ParserKeyword s_partition_by(\"PARTITION BY\");\n     ParserKeyword s_with(\"WITH\");\n     ParserToken s_lparen(TokenType::OpeningRoundBracket);\n     ParserToken s_rparen(TokenType::ClosingRoundBracket);\n@@ -42,6 +43,7 @@ bool ParserInsertQuery::parseImpl(Pos & pos, ASTPtr & node, Expected & expected)\n     ParserList columns_p(std::make_unique<ParserInsertElement>(), std::make_unique<ParserToken>(TokenType::Comma), false);\n     ParserFunction table_function_p{false};\n     ParserStringLiteral infile_name_p;\n+    ParserExpressionWithOptionalAlias exp_elem_p(false);\n \n     ASTPtr database;\n     ASTPtr table;\n@@ -52,6 +54,8 @@ bool ParserInsertQuery::parseImpl(Pos & pos, ASTPtr & node, Expected & expected)\n     ASTPtr watch;\n     ASTPtr table_function;\n     ASTPtr settings_ast;\n+    ASTPtr partition_by_expr;\n+\n     /// Insertion data\n     const char * data = nullptr;\n \n@@ -64,6 +68,12 @@ bool ParserInsertQuery::parseImpl(Pos & pos, ASTPtr & node, Expected & expected)\n     {\n         if (!table_function_p.parse(pos, table_function, expected))\n             return false;\n+\n+        if (s_partition_by.ignore(pos, expected))\n+        {\n+            if (!exp_elem_p.parse(pos, partition_by_expr, expected))\n+                return false;\n+        }\n     }\n     else\n     {\n@@ -183,6 +193,7 @@ bool ParserInsertQuery::parseImpl(Pos & pos, ASTPtr & node, Expected & expected)\n     if (table_function)\n     {\n         query->table_function = table_function;\n+        query->partition_by = partition_by_expr;\n     }\n     else\n     {\ndiff --git a/src/Storages/IStorage.h b/src/Storages/IStorage.h\nindex b1d8fab57508..0432fee85f96 100644\n--- a/src/Storages/IStorage.h\n+++ b/src/Storages/IStorage.h\n@@ -127,6 +127,9 @@ class IStorage : public std::enable_shared_from_this<IStorage>, public TypePromo\n     /// Returns true if the storage supports queries with the FINAL section.\n     virtual bool supportsFinal() const { return false; }\n \n+    /// Returns true if the storage supports insert queries with the PARTITION BY section.\n+    virtual bool supportsPartitionBy() const { return false; }\n+\n     /// Returns true if the storage supports queries with the PREWHERE section.\n     virtual bool supportsPrewhere() const { return false; }\n \ndiff --git a/src/Storages/StorageS3.cpp b/src/Storages/StorageS3.cpp\nindex fc3ce3a10ed5..0e84b7a26e71 100644\n--- a/src/Storages/StorageS3.cpp\n+++ b/src/Storages/StorageS3.cpp\n@@ -2,15 +2,26 @@\n \n #if USE_AWS_S3\n \n+#include <Columns/ColumnString.h>\n+#include <Common/isValidUTF8.h>\n+\n+#include <Functions/FunctionsConversion.h>\n+\n #include <IO/S3Common.h>\n-#include <Storages/StorageFactory.h>\n-#include <Storages/StorageS3.h>\n-#include <Storages/StorageS3Settings.h>\n \n #include <Interpreters/Context.h>\n+#include <Interpreters/ExpressionAnalyzer.h>\n+#include <Interpreters/TreeRewriter.h>\n #include <Interpreters/evaluateConstantExpression.h>\n+\n+#include <Parsers/ASTFunction.h>\n+#include <Parsers/ASTInsertQuery.h>\n #include <Parsers/ASTLiteral.h>\n \n+#include <Storages/StorageFactory.h>\n+#include <Storages/StorageS3.h>\n+#include <Storages/StorageS3Settings.h>\n+\n #include <IO/ReadBufferFromS3.h>\n #include <IO/ReadHelpers.h>\n #include <IO/WriteBufferFromS3.h>\n@@ -46,13 +57,21 @@\n \n namespace fs = std::filesystem;\n \n+#include <boost/algorithm/string.hpp>\n+\n+\n+static const String PARTITION_ID_WILDCARD = \"{_partition_id}\";\n+\n namespace DB\n {\n+\n namespace ErrorCodes\n {\n+    extern const int CANNOT_PARSE_TEXT;\n+    extern const int BAD_ARGUMENTS;\n     extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;\n-    extern const int UNEXPECTED_EXPRESSION;\n     extern const int S3_ERROR;\n+    extern const int UNEXPECTED_EXPRESSION;\n }\n class StorageS3Source::DisclosedGlobIterator::Impl\n {\n@@ -299,11 +318,6 @@ class StorageS3Sink : public SinkToStorage\n         writer->write(getPort().getHeader().cloneWithColumns(chunk.detachColumns()));\n     }\n \n-    // void flush() override\n-    // {\n-    //     writer->flush();\n-    // }\n-\n     void onFinish() override\n     {\n         try\n@@ -328,6 +342,185 @@ class StorageS3Sink : public SinkToStorage\n };\n \n \n+class PartitionedStorageS3Sink : public SinkToStorage\n+{\n+public:\n+    PartitionedStorageS3Sink(\n+        const ASTPtr & partition_by,\n+        const String & format_,\n+        const Block & sample_block_,\n+        ContextPtr context_,\n+        const CompressionMethod compression_method_,\n+        const std::shared_ptr<Aws::S3::S3Client> & client_,\n+        const String & bucket_,\n+        const String & key_,\n+        size_t min_upload_part_size_,\n+        size_t max_single_part_upload_size_)\n+        : SinkToStorage(sample_block_)\n+        , format(format_)\n+        , sample_block(sample_block_)\n+        , context(context_)\n+        , compression_method(compression_method_)\n+        , client(client_)\n+        , bucket(bucket_)\n+        , key(key_)\n+        , min_upload_part_size(min_upload_part_size_)\n+        , max_single_part_upload_size(max_single_part_upload_size_)\n+\n+    {\n+        std::vector<ASTPtr> arguments(1, partition_by);\n+        ASTPtr partition_by_string = makeASTFunction(FunctionToString::name, std::move(arguments));\n+\n+        auto syntax_result = TreeRewriter(context).analyze(partition_by_string, sample_block.getNamesAndTypesList());\n+        partition_by_expr = ExpressionAnalyzer(partition_by_string, syntax_result, context).getActions(false);\n+        partition_by_column_name = partition_by_string->getColumnName();\n+    }\n+\n+    String getName() const override { return \"PartitionedStorageS3Sink\"; }\n+\n+    void consume(Chunk chunk) override\n+    {\n+        const auto & columns = chunk.getColumns();\n+\n+        Block block_with_partition_by_expr = sample_block.cloneWithoutColumns();\n+        block_with_partition_by_expr.setColumns(columns);\n+        partition_by_expr->execute(block_with_partition_by_expr);\n+\n+        const auto * column = block_with_partition_by_expr.getByName(partition_by_column_name).column.get();\n+\n+        std::unordered_map<String, size_t> sub_chunks_indices;\n+        IColumn::Selector selector;\n+        for (size_t row = 0; row < chunk.getNumRows(); ++row)\n+        {\n+            auto value = column->getDataAt(row);\n+            auto [it, inserted] = sub_chunks_indices.emplace(value, sub_chunks_indices.size());\n+            selector.push_back(it->second);\n+        }\n+\n+        Chunks sub_chunks;\n+        sub_chunks.reserve(sub_chunks_indices.size());\n+        for (size_t column_index = 0; column_index < columns.size(); ++column_index)\n+        {\n+            MutableColumns column_sub_chunks = columns[column_index]->scatter(sub_chunks_indices.size(), selector);\n+            if (column_index == 0) /// Set sizes for sub-chunks.\n+            {\n+                for (const auto & column_sub_chunk : column_sub_chunks)\n+                {\n+                    sub_chunks.emplace_back(Columns(), column_sub_chunk->size());\n+                }\n+            }\n+            for (size_t sub_chunk_index = 0; sub_chunk_index < column_sub_chunks.size(); ++sub_chunk_index)\n+            {\n+                sub_chunks[sub_chunk_index].addColumn(std::move(column_sub_chunks[sub_chunk_index]));\n+            }\n+        }\n+\n+        for (const auto & [partition_id, sub_chunk_index] : sub_chunks_indices)\n+        {\n+            getSinkForPartition(partition_id)->consume(std::move(sub_chunks[sub_chunk_index]));\n+        }\n+    }\n+\n+    void onFinish() override\n+    {\n+        for (auto & [partition_id, sink] : sinks)\n+        {\n+            sink->onFinish();\n+        }\n+    }\n+\n+private:\n+    using SinkPtr = std::shared_ptr<StorageS3Sink>;\n+\n+    const String format;\n+    const Block sample_block;\n+    ContextPtr context;\n+    const CompressionMethod compression_method;\n+    std::shared_ptr<Aws::S3::S3Client> client;\n+    const String bucket;\n+    const String key;\n+    size_t min_upload_part_size;\n+    size_t max_single_part_upload_size;\n+\n+    ExpressionActionsPtr partition_by_expr;\n+    String partition_by_column_name;\n+\n+    std::unordered_map<String, SinkPtr> sinks;\n+\n+    static String replaceWildcards(const String & haystack, const String & partition_id)\n+    {\n+        return boost::replace_all_copy(haystack, PARTITION_ID_WILDCARD, partition_id);\n+    }\n+\n+    SinkPtr getSinkForPartition(const String & partition_id)\n+    {\n+        auto it = sinks.find(partition_id);\n+        if (it == sinks.end())\n+        {\n+            auto partition_bucket = replaceWildcards(bucket, partition_id);\n+            validateBucket(partition_bucket);\n+\n+            auto partition_key = replaceWildcards(key, partition_id);\n+            validateKey(partition_key);\n+\n+            std::tie(it, std::ignore) = sinks.emplace(partition_id, std::make_shared<StorageS3Sink>(\n+                format,\n+                sample_block,\n+                context,\n+                compression_method,\n+                client,\n+                partition_bucket,\n+                partition_key,\n+                min_upload_part_size,\n+                max_single_part_upload_size\n+            ));\n+        }\n+\n+        return it->second;\n+    }\n+\n+    static void validateBucket(const String & str)\n+    {\n+        S3::URI::validateBucket(str, {});\n+\n+        if (!DB::UTF8::isValidUTF8(reinterpret_cast<const UInt8 *>(str.data()), str.size()))\n+            throw Exception(ErrorCodes::CANNOT_PARSE_TEXT, \"Incorrect non-UTF8 sequence in bucket name\");\n+\n+        validatePartitionKey(str, false);\n+    }\n+\n+    static void validateKey(const String & str)\n+    {\n+        /// See:\n+        /// - https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\n+        /// - https://cloud.ibm.com/apidocs/cos/cos-compatibility#putobject\n+\n+        if (str.empty() || str.size() > 1024)\n+            throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Incorrect key length (not empty, max 1023 characters), got: {}\", str.size());\n+\n+        if (!DB::UTF8::isValidUTF8(reinterpret_cast<const UInt8 *>(str.data()), str.size()))\n+            throw Exception(ErrorCodes::CANNOT_PARSE_TEXT, \"Incorrect non-UTF8 sequence in key\");\n+\n+        validatePartitionKey(str, true);\n+    }\n+\n+    static void validatePartitionKey(const StringRef & str, bool allow_slash)\n+    {\n+        for (const char * i = str.data; i != str.data + str.size; ++i)\n+        {\n+            if (static_cast<UInt8>(*i) < 0x20 || *i == '{' || *i == '}' || *i == '*' || *i == '?' || (!allow_slash && *i == '/'))\n+            {\n+                /// Need to convert to UInt32 because UInt8 can't be passed to format due to \"mixing character types is disallowed\".\n+                UInt32 invalid_char_byte = static_cast<UInt32>(static_cast<UInt8>(*i));\n+                throw DB::Exception(\n+                    ErrorCodes::CANNOT_PARSE_TEXT, \"Illegal character '\\\\x{:02x}' in partition id starting with '{}'\",\n+                    invalid_char_byte, StringRef(str.data, i - str.data));\n+            }\n+        }\n+    }\n+};\n+\n+\n StorageS3::StorageS3(\n     const S3::URI & uri_,\n     const String & access_key_id_,\n@@ -427,19 +620,44 @@ Pipe StorageS3::read(\n     return pipe;\n }\n \n-SinkToStoragePtr StorageS3::write(const ASTPtr & /*query*/, const StorageMetadataPtr & metadata_snapshot, ContextPtr local_context)\n+SinkToStoragePtr StorageS3::write(const ASTPtr & query, const StorageMetadataPtr & metadata_snapshot, ContextPtr local_context)\n {\n     updateClientAndAuthSettings(local_context, client_auth);\n-    return std::make_shared<StorageS3Sink>(\n-        format_name,\n-        metadata_snapshot->getSampleBlock(),\n-        local_context,\n-        chooseCompressionMethod(client_auth.uri.key, compression_method),\n-        client_auth.client,\n-        client_auth.uri.bucket,\n-        client_auth.uri.key,\n-        min_upload_part_size,\n-        max_single_part_upload_size);\n+\n+    auto sample_block = metadata_snapshot->getSampleBlock();\n+    auto chosen_compression_method = chooseCompressionMethod(client_auth.uri.key, compression_method);\n+    bool has_wildcards = client_auth.uri.bucket.find(PARTITION_ID_WILDCARD) != String::npos || client_auth.uri.key.find(PARTITION_ID_WILDCARD) != String::npos;\n+    auto insert_query = std::dynamic_pointer_cast<ASTInsertQuery>(query);\n+\n+    bool is_partitioned_implementation = insert_query && insert_query->partition_by && has_wildcards;\n+\n+    if (is_partitioned_implementation)\n+    {\n+        return std::make_shared<PartitionedStorageS3Sink>(\n+            insert_query->partition_by,\n+            format_name,\n+            sample_block,\n+            local_context,\n+            chosen_compression_method,\n+            client_auth.client,\n+            client_auth.uri.bucket,\n+            client_auth.uri.key,\n+            min_upload_part_size,\n+            max_single_part_upload_size);\n+    }\n+    else\n+    {\n+        return std::make_shared<StorageS3Sink>(\n+            format_name,\n+            sample_block,\n+            local_context,\n+            chosen_compression_method,\n+            client_auth.client,\n+            client_auth.uri.bucket,\n+            client_auth.uri.key,\n+            min_upload_part_size,\n+            max_single_part_upload_size);\n+    }\n }\n \n \n@@ -583,6 +801,11 @@ NamesAndTypesList StorageS3::getVirtuals() const\n     };\n }\n \n+bool StorageS3::supportsPartitionBy() const\n+{\n+    return true;\n+}\n+\n }\n \n #endif\ndiff --git a/src/Storages/StorageS3.h b/src/Storages/StorageS3.h\nindex a00895789478..df4112cbfdd1 100644\n--- a/src/Storages/StorageS3.h\n+++ b/src/Storages/StorageS3.h\n@@ -136,6 +136,8 @@ class StorageS3 : public shared_ptr_helper<StorageS3>, public IStorage, WithCont\n \n     NamesAndTypesList getVirtuals() const override;\n \n+    bool supportsPartitionBy() const override;\n+\n private:\n \n     friend class StorageS3Cluster;\n",
  "test_patch": "diff --git a/docker/test/fasttest/run.sh b/docker/test/fasttest/run.sh\nindex 43c1add74ccf..c4493de477c8 100755\n--- a/docker/test/fasttest/run.sh\n+++ b/docker/test/fasttest/run.sh\n@@ -393,6 +393,9 @@ function run_tests\n         01853_s2_cells_intersect\n         01854_s2_cap_contains\n         01854_s2_cap_union\n+\n+        # needs s3\n+        01944_insert_partition_by\n     )\n \n     time clickhouse-test --hung-check -j 8 --order=random --use-skip-list \\\ndiff --git a/tests/integration/test_storage_s3/test.py b/tests/integration/test_storage_s3/test.py\nindex 5908def82979..87f9fec57b59 100644\n--- a/tests/integration/test_storage_s3/test.py\n+++ b/tests/integration/test_storage_s3/test.py\n@@ -146,6 +146,59 @@ def test_put(started_cluster, maybe_auth, positive, compression):\n         assert values_csv == get_s3_file_content(started_cluster, bucket, filename)\n \n \n+def test_partition_by(started_cluster):\n+    bucket = started_cluster.minio_bucket\n+    instance = started_cluster.instances[\"dummy\"]  # type: ClickHouseInstance\n+    table_format = \"column1 UInt32, column2 UInt32, column3 UInt32\"\n+    partition_by = \"column3\"\n+    values = \"(1, 2, 3), (3, 2, 1), (78, 43, 45)\"\n+    filename = \"test_{_partition_id}.csv\"\n+    put_query = f\"\"\"INSERT INTO TABLE FUNCTION\n+        s3('http://{started_cluster.minio_host}:{started_cluster.minio_port}/{bucket}/{filename}', 'CSV', '{table_format}')\n+        PARTITION BY {partition_by} VALUES {values}\"\"\"\n+\n+    run_query(instance, put_query)\n+\n+    assert \"1,2,3\\n\" == get_s3_file_content(started_cluster, bucket, \"test_3.csv\")\n+    assert \"3,2,1\\n\" == get_s3_file_content(started_cluster, bucket, \"test_1.csv\")\n+    assert \"78,43,45\\n\" == get_s3_file_content(started_cluster, bucket, \"test_45.csv\")\n+\n+\n+def test_partition_by_string_column(started_cluster):\n+    bucket = started_cluster.minio_bucket\n+    instance = started_cluster.instances[\"dummy\"]  # type: ClickHouseInstance\n+    table_format = \"col_num UInt32, col_str String\"\n+    partition_by = \"col_str\"\n+    values = \"(1, 'foo/bar'), (3, '\u0439\u0446\u0443\u043a'), (78, '\u4f60\u597d')\"\n+    filename = \"test_{_partition_id}.csv\"\n+    put_query = f\"\"\"INSERT INTO TABLE FUNCTION\n+        s3('http://{started_cluster.minio_host}:{started_cluster.minio_port}/{bucket}/{filename}', 'CSV', '{table_format}')\n+        PARTITION BY {partition_by} VALUES {values}\"\"\"\n+\n+    run_query(instance, put_query)\n+\n+    assert '1,\"foo/bar\"\\n' == get_s3_file_content(started_cluster, bucket, \"test_foo/bar.csv\")\n+    assert '3,\"\u0439\u0446\u0443\u043a\"\\n' == get_s3_file_content(started_cluster, bucket, \"test_\u0439\u0446\u0443\u043a.csv\")\n+    assert '78,\"\u4f60\u597d\"\\n' == get_s3_file_content(started_cluster, bucket, \"test_\u4f60\u597d.csv\")\n+\n+\n+def test_partition_by_const_column(started_cluster):\n+    bucket = started_cluster.minio_bucket\n+    instance = started_cluster.instances[\"dummy\"]  # type: ClickHouseInstance\n+    table_format = \"column1 UInt32, column2 UInt32, column3 UInt32\"\n+    values = \"(1, 2, 3), (3, 2, 1), (78, 43, 45)\"\n+    partition_by = \"'88'\"\n+    values_csv = \"1,2,3\\n3,2,1\\n78,43,45\\n\"\n+    filename = \"test_{_partition_id}.csv\"\n+    put_query = f\"\"\"INSERT INTO TABLE FUNCTION\n+        s3('http://{started_cluster.minio_host}:{started_cluster.minio_port}/{bucket}/{filename}', 'CSV', '{table_format}')\n+        PARTITION BY {partition_by} VALUES {values}\"\"\"\n+\n+    run_query(instance, put_query)\n+\n+    assert values_csv == get_s3_file_content(started_cluster, bucket, \"test_88.csv\")\n+\n+\n @pytest.mark.parametrize(\"special\", [\n     \"space\",\n     \"plus\"\ndiff --git a/tests/queries/0_stateless/01944_insert_partition_by.reference b/tests/queries/0_stateless/01944_insert_partition_by.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/01944_insert_partition_by.sql b/tests/queries/0_stateless/01944_insert_partition_by.sql\nnew file mode 100644\nindex 000000000000..1c7d96d77620\n--- /dev/null\n+++ b/tests/queries/0_stateless/01944_insert_partition_by.sql\n@@ -0,0 +1,9 @@\n+INSERT INTO TABLE FUNCTION file('foo.csv', 'CSV', 'id Int32, val Int32') PARTITION BY val VALUES (1, 1), (2, 2); -- { serverError NOT_IMPLEMENTED }\n+INSERT INTO TABLE FUNCTION s3('http://localhost:9001/foo/test_{_partition_id}.csv', 'admin', 'admin', 'CSV', 'id Int32, val String') PARTITION BY val VALUES (1, '\\r\\n'); -- { serverError CANNOT_PARSE_TEXT }\n+INSERT INTO TABLE FUNCTION s3('http://localhost:9001/foo/test_{_partition_id}.csv', 'admin', 'admin', 'CSV', 'id Int32, val String') PARTITION BY val VALUES (1, 'abc\\x00abc'); -- { serverError CANNOT_PARSE_TEXT }\n+INSERT INTO TABLE FUNCTION s3('http://localhost:9001/foo/test_{_partition_id}.csv', 'admin', 'admin', 'CSV', 'id Int32, val String') PARTITION BY val VALUES (1, 'abc\\xc3\\x28abc'); -- { serverError CANNOT_PARSE_TEXT }\n+INSERT INTO TABLE FUNCTION s3('http://localhost:9001/foo/test_{_partition_id}.csv', 'admin', 'admin', 'CSV', 'id Int32, val String') PARTITION BY val VALUES (1, 'abc}{abc'); -- { serverError CANNOT_PARSE_TEXT }\n+INSERT INTO TABLE FUNCTION s3('http://localhost:9001/foo/test_{_partition_id}.csv', 'admin', 'admin', 'CSV', 'id Int32, val String') PARTITION BY val VALUES (1, 'abc*abc'); -- { serverError CANNOT_PARSE_TEXT }\n+INSERT INTO TABLE FUNCTION s3('http://localhost:9001/foo/{_partition_id}', 'admin', 'admin', 'CSV', 'id Int32, val String') PARTITION BY val VALUES (1, ''); -- { serverError BAD_ARGUMENTS }\n+INSERT INTO TABLE FUNCTION s3('http://localhost:9001/{_partition_id}/key.csv', 'admin', 'admin', 'CSV', 'id Int32, val String') PARTITION BY val VALUES (1, ''); -- { serverError BAD_ARGUMENTS }\n+INSERT INTO TABLE FUNCTION s3('http://localhost:9001/{_partition_id}/key.csv', 'admin', 'admin', 'CSV', 'id Int32, val String') PARTITION BY val VALUES (1, 'aa/bb'); -- { serverError CANNOT_PARSE_TEXT }\ndiff --git a/tests/queries/0_stateless/arcadia_skip_list.txt b/tests/queries/0_stateless/arcadia_skip_list.txt\nindex 606015c369ff..8b97f4dcfa42 100644\n--- a/tests/queries/0_stateless/arcadia_skip_list.txt\n+++ b/tests/queries/0_stateless/arcadia_skip_list.txt\n@@ -267,3 +267,4 @@\n 01428_h3_range_check\n 01442_h3kring_range_check\n 01906_h3_to_geo\n+01944_insert_partition_by\n",
  "problem_statement": "Partitioned write into s3 table function / S3 table\nAllow to specify PARTITION BY expression for S3 table engine.\r\nWhen writing to this table, multiple files will be created, with names prefixed (?) by partition key.\r\n\r\n`PARTITION BY rand() % N` will be also applicable for creation of multiple files instead of one.\r\n\r\nMotivation:\r\n- more efficient to read;\r\n- easier manipulation by other data processing systems.\r\n\r\nDetails of implementation:\r\n- if s3 path contains wildcard (`*`), we will substitute this wildcard by the value of partition key; it can be located in file name or in part of path (in the latter case multiple directories will be created).\n",
  "hints_text": "@alexey-milovidov @filimonov We can make syntax like python f-strings: `s3('http://bucket.amazonaws.com/key/{_partition_id}.csv', ...)` . What do you think about it?",
  "created_at": "2021-04-13T21:09:06Z"
}