{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 10095,
  "instance_id": "ClickHouse__ClickHouse-10095",
  "issue_numbers": [
    "10037"
  ],
  "base_commit": "385e8839dc82417780353c61e78cee0e3f3fd642",
  "patch": "diff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex 7107328e4ff8..31456c8d1f17 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -256,6 +256,15 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(\n     }\n     else\n     {\n+        /// In old tables this node may missing or be empty\n+        String replica_metadata;\n+        bool replica_metadata_exists = current_zookeeper->tryGet(replica_path + \"/metadata\", replica_metadata);\n+        if (!replica_metadata_exists || replica_metadata.empty())\n+        {\n+            ReplicatedMergeTreeTableMetadata current_metadata(*this);\n+            current_zookeeper->createOrUpdate(replica_path + \"/metadata\", current_metadata.toString(), zkutil::CreateMode::Persistent);\n+        }\n+\n         checkTableStructure(replica_path);\n         checkParts(skip_sanity_checks);\n \n@@ -263,8 +272,13 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(\n         {\n             metadata_version = parse<int>(current_zookeeper->get(replica_path + \"/metadata_version\"));\n         }\n-        else /// This replica was created on old version, so we have to take version of global node\n+        else\n         {\n+            /// This replica was created with old clickhouse version, so we have\n+            /// to take version of global node. If somebody will alter our\n+            /// table, then we will fill /metadata_version node in zookeeper.\n+            /// Otherwise on the next restart we can again use version from\n+            /// shared metadata node because it was not changed.\n             Coordination::Stat metadata_stat;\n             current_zookeeper->get(zookeeper_path + \"/metadata\", &metadata_stat);\n             metadata_version = metadata_stat.version;\n",
  "test_patch": "diff --git a/tests/integration/test_no_local_metadata_node/__init__.py b/tests/integration/test_no_local_metadata_node/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_no_local_metadata_node/test.py b/tests/integration/test_no_local_metadata_node/test.py\nnew file mode 100644\nindex 000000000000..ef240cd710ce\n--- /dev/null\n+++ b/tests/integration/test_no_local_metadata_node/test.py\n@@ -0,0 +1,54 @@\n+import time\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+\n+cluster = ClickHouseCluster(__file__)\n+node1 = cluster.add_instance('node1', with_zookeeper=True)\n+\n+@pytest.fixture(scope=\"module\")\n+def start_cluster():\n+    try:\n+        cluster.start()\n+\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_table_start_without_metadata(start_cluster):\n+    node1.query(\"\"\"\n+        CREATE TABLE test (date Date)\n+        ENGINE = ReplicatedMergeTree('/clickhouse/table/test_table', '1')\n+        ORDER BY tuple()\n+    \"\"\")\n+\n+    node1.query(\"INSERT INTO test VALUES(toDate('2019-12-01'))\")\n+\n+    assert node1.query(\"SELECT date FROM test\") == \"2019-12-01\\n\"\n+\n+    # some fake alter\n+    node1.query(\"ALTER TABLE test MODIFY COLUMN date Date DEFAULT toDate('2019-10-01')\")\n+\n+    assert node1.query(\"SELECT date FROM test\") == \"2019-12-01\\n\"\n+\n+    node1.query(\"DETACH TABLE test\")\n+    zk_cli = cluster.get_kazoo_client('zoo1')\n+\n+    # simulate update from old version\n+    zk_cli.delete(\"/clickhouse/table/test_table/replicas/1/metadata\")\n+    zk_cli.delete(\"/clickhouse/table/test_table/replicas/1/metadata_version\")\n+\n+    node1.query(\"ATTACH TABLE test\")\n+\n+    assert node1.query(\"SELECT date FROM test\") == \"2019-12-01\\n\"\n+\n+    node1.query(\"ALTER TABLE test MODIFY COLUMN date Date DEFAULT toDate('2019-09-01')\")\n+\n+    node1.query(\"DETACH TABLE test\")\n+\n+    zk_cli.set(\"/clickhouse/table/test_table/replicas/1/metadata\", \"\")\n+\n+    node1.query(\"ATTACH TABLE test\")\n+\n+    assert node1.query(\"SELECT date FROM test\") == \"2019-12-01\\n\"\n",
  "problem_statement": "Can't start Clickhouse due to zookeeper metadata node of some replicated table doesn't exist.\n**Description**\r\n\r\nAfter I upgrade my Clickhouse to version `20.3.5.21`, I can't start the clickhouse-server. Logs in `clickhouse-server.log` are shown as below:\r\n\r\n```\r\n[ 38541 ] {} <Error> Application: Coordination::Exception: Can't get data for node /clickhouse/tables/shard_name/table_name/replicas/replica_name/metadata: node doesn't exist (No node): Cannot attach table `db`.`table` from metadata file.\r\n[ 38541 ] {} <Information> Application: shutting down\r\n[ 38541 ] {} <Debug> Application: Uninitializing subsystem: Logging Subsystem\r\n[ 38545 ] {} <Information> BaseDaemon: Stop SignalListener thread\r\n```\r\n\r\nI've checked zookeeper, and `/clickhouse/tables/shard_name/table_name/replicas/replica_name` does exist but there is no `metadata` node in it. Nodes in `replica_name` are show below:\r\n\r\n```\r\n ['queue', 'host', 'flags', 'columns', 'parts', 'max_processed_insert_time', 'log_pointer', 'min_unprocessed_insert_time']\r\n```\r\n\r\nIf I set the shard and replica in macros to empty stirng, Clickhouse would start with some warning logs, just says `No metadata in ZooKeeper: table will be in readonly mode.`\r\n\r\nPrevious version (1.1.54343) works well when restart. And it didn't need `metadata` node in zookeeper.\r\n\r\nSo, is this a bug? And how to handle this with new version of clickhous? Thanks~\n",
  "hints_text": "Seems like you have updated from the extremely old version. `replica_name/metadata` node was introduced about two years ago. Now you can just copy `/table/metadata` node to `/table/replicas/replica_name/metadata` with `zkCli.sh` util.\n@alesapin Thanks~, I've solved this problem by copying `metadata` node. Considering that other people who want to upgrade clickhouse will also encounter this  problem, I'll give the `python code` I've  used. Also, `zkCli.sh` maybe work, but I  don't think it's convenient to handle lots of  replicated tables.\r\n\r\nIn python, I suggest to install `kazoo` [package](https://kazoo.readthedocs.io/en/latest/basic_usage.html) using  `pip install kazoo` . To `copy` node, we should get the node data first, and create new  node with these data. Given the structure `/clickhouse/tables/{shard}/{table_name}/replicas/{replica_name}`, we should copy `/clickhouse/tables/{shard}/{table_name}/metadata` to `/clickhouse/tables/{shard}/{table_name}/replicas/{replica_name}/metadata`. Code shown as below:\r\n\r\n```python\r\nfrom kazoo.client import KazooClient\r\n\r\nzk = KazooClient(hosts='ip:2181')\r\nzk.start()\r\nprint(zk.server_version())\r\n\r\nMETADATA = \"metadata\"\r\nREPLICAS = \"replicas\"\r\nroot_node = \"/clickhouse/tables\"\r\nif zk.exists(root_node):\r\n    shards = zk.get_children(root_node)\r\n    for shard in shards:\r\n        shard_node = \"/\".join([root_node, shard])\r\n        tables = zk.get_children(shard_node)\r\n        for table in tables:\r\n            table_meta_node = \"/\".join([shard_node, table, METADATA])\r\n            if zk.exists(table_meta_node):\r\n                data, stat = zk.get(table_meta_node)\r\n                replica_node = \"/\".join([shard_node, table, REPLICAS])\r\n                if zk.exists(replica_node):\r\n                    replicas = zk.get_children(replica_node)\r\n                    for replica in replicas:\r\n                        replica_meta_node = \"/\".join(\r\n                            [replica_node, replica, METADATA])\r\n                        if zk.exists(replica_meta_node):\r\n                            print(\"replica meta node %s already exist\" %\r\n                                  (replica_meta_node))\r\n                        else:\r\n                            zk.create(replica_meta_node, data)\r\n                            if zk.exists(replica_meta_node):\r\n                                print(\"create meta node %s successfully\" %\r\n                                      (replica_meta_node))\r\n                            else:\r\n                                print(\"create meta node %s failed\" %\r\n                                      (replica_meta_node))\r\n                else:\r\n                    print(\"replica node %s doesn't exsit\" % (replica_node))\r\n            else:\r\n                print(\"table metadata node %s doesn't exist\" %\r\n                      (table_meta_node))\r\nelse:\r\n    print(\"root node %s doesn't exist\" % (root_node))\r\n```",
  "created_at": "2020-04-07T15:31:40Z",
  "modified_files": [
    "src/Storages/StorageReplicatedMergeTree.cpp"
  ],
  "modified_test_files": [
    "b/tests/integration/test_no_local_metadata_node/test.py"
  ]
}