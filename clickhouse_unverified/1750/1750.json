{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 1750,
  "instance_id": "ClickHouse__ClickHouse-1750",
  "issue_numbers": [
    "1560"
  ],
  "base_commit": "288c6c8406fba0edc503630490364ab1c1f4036d",
  "patch": "diff --git a/dbms/src/Client/ConnectionPoolWithFailover.cpp b/dbms/src/Client/ConnectionPoolWithFailover.cpp\nindex acba75086b28..ee8c3607c43e 100644\n--- a/dbms/src/Client/ConnectionPoolWithFailover.cpp\n+++ b/dbms/src/Client/ConnectionPoolWithFailover.cpp\n@@ -4,6 +4,7 @@\n #include <Poco/Net/DNS.h>\n \n #include <Common/getFQDNOrHostName.h>\n+#include <Common/isLocalAddress.h>\n #include <Common/ProfileEvents.h>\n #include <Interpreters/Settings.h>\n \n@@ -39,14 +40,7 @@ ConnectionPoolWithFailover::ConnectionPoolWithFailover(\n     for (size_t i = 0; i < nested_pools.size(); ++i)\n     {\n         ConnectionPool & connection_pool = dynamic_cast<ConnectionPool &>(*nested_pools[i]);\n-        const std::string & host = connection_pool.getHost();\n-\n-        size_t hostname_difference = 0;\n-        for (size_t i = 0; i < std::min(local_hostname.length(), host.length()); ++i)\n-            if (local_hostname[i] != host[i])\n-                ++hostname_difference;\n-\n-        hostname_differences[i] = hostname_difference;\n+        hostname_differences[i] = getHostNameDifference(local_hostname, connection_pool.getHost());\n     }\n }\n \ndiff --git a/dbms/src/Common/ZooKeeper/Types.h b/dbms/src/Common/ZooKeeper/Types.h\nindex dd6e6c51068c..4097627f395a 100644\n--- a/dbms/src/Common/ZooKeeper/Types.h\n+++ b/dbms/src/Common/ZooKeeper/Types.h\n@@ -138,11 +138,12 @@ struct Op::Check : public Op\n \n struct OpResult : public zoo_op_result_t\n {\n-    /// \u0423\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0438 \u0432 \u044d\u0442\u043e\u0439 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0435 \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0442 \u043d\u0430 \u043f\u043e\u043b\u044f \u0432 \u043a\u043b\u0430\u0441\u0441\u0435 Op.\n-    /// \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u0434\u0435\u0441\u0442\u0440\u0443\u043a\u0442\u043e\u0440 \u043d\u0435 \u043d\u0443\u0436\u0435\u043d\n+    /// Pointers in this class point to fields of class Op.\n+    /// Op instances have the same (or longer lifetime), therefore destructor is not required.\n };\n \n-using Ops = std::vector<std::unique_ptr<Op>>;\n+using OpPtr = std::unique_ptr<Op>;\n+using Ops = std::vector<OpPtr>;\n using OpResults = std::vector<OpResult>;\n using OpResultsPtr = std::shared_ptr<OpResults>;\n using Strings = std::vector<std::string>;\ndiff --git a/dbms/src/Common/getFQDNOrHostName.h b/dbms/src/Common/getFQDNOrHostName.h\nindex a4367a726221..fe164a642046 100644\n--- a/dbms/src/Common/getFQDNOrHostName.h\n+++ b/dbms/src/Common/getFQDNOrHostName.h\n@@ -2,6 +2,7 @@\n \n #include <string>\n \n+\n /** Get the FQDN for the local server by resolving DNS hostname - similar to calling the 'hostname' tool with the -f flag.\n   * If it does not work, return hostname - similar to calling 'hostname' without flags or 'uname -n'.\n   */\ndiff --git a/dbms/src/Common/isLocalAddress.cpp b/dbms/src/Common/isLocalAddress.cpp\nindex 25644400bc54..742eac967ba8 100644\n--- a/dbms/src/Common/isLocalAddress.cpp\n+++ b/dbms/src/Common/isLocalAddress.cpp\n@@ -10,25 +10,35 @@\n namespace DB\n {\n \n-bool isLocalAddress(const Poco::Net::SocketAddress & address, UInt16 clickhouse_port)\n+bool isLocalAddress(const Poco::Net::SocketAddress & address)\n {\n     static auto interfaces = Poco::Net::NetworkInterface::list();\n \n-    if (clickhouse_port == address.port())\n-    {\n-        return interfaces.end() != std::find_if(interfaces.begin(), interfaces.end(),\n-            [&] (const Poco::Net::NetworkInterface & interface)\n-            {\n-                /** Compare the addresses without taking into account `scope`.\n-                  * Theoretically, this may not be correct - depends on `route` setting\n-                  *  - through which interface we will actually access the specified address.\n-                  */\n-                return interface.address().length() == address.host().length()\n-                    && 0 == memcmp(interface.address().addr(), address.host().addr(), address.host().length());\n-            });\n-    }\n-\n-    return false;\n+    return interfaces.end() != std::find_if(interfaces.begin(), interfaces.end(),\n+                [&] (const Poco::Net::NetworkInterface & interface)\n+                {\n+                    /** Compare the addresses without taking into account `scope`.\n+                      * Theoretically, this may not be correct - depends on `route` setting\n+                      *  - through which interface we will actually access the specified address.\n+                      */\n+                    return interface.address().length() == address.host().length()\n+                        && 0 == memcmp(interface.address().addr(), address.host().addr(), address.host().length());\n+                });\n+}\n+\n+bool isLocalAddress(const Poco::Net::SocketAddress & address, UInt16 clickhouse_port)\n+{\n+    return clickhouse_port == address.port() && isLocalAddress(address);\n+}\n+\n+\n+size_t getHostNameDifference(const std::string & local_hostname, const std::string & host)\n+{\n+    size_t hostname_difference = 0;\n+    for (size_t i = 0; i < std::min(local_hostname.length(), host.length()); ++i)\n+        if (local_hostname[i] != host[i])\n+            ++hostname_difference;\n+    return hostname_difference;\n }\n \n }\ndiff --git a/dbms/src/Common/isLocalAddress.h b/dbms/src/Common/isLocalAddress.h\nindex e6d85432ce80..d90d7aef63aa 100644\n--- a/dbms/src/Common/isLocalAddress.h\n+++ b/dbms/src/Common/isLocalAddress.h\n@@ -23,4 +23,8 @@ namespace DB\n      */\n     bool isLocalAddress(const Poco::Net::SocketAddress & address, UInt16 clickhouse_port);\n \n+    bool isLocalAddress(const Poco::Net::SocketAddress & address);\n+\n+    /// Returns number of different bytes in hostnames, used for load balancing\n+    size_t getHostNameDifference(const std::string & local_hostname, const std::string & host);\n }\ndiff --git a/dbms/src/DataStreams/SquashingBlockInputStream.cpp b/dbms/src/DataStreams/SquashingBlockInputStream.cpp\nindex 3759ba1d1a3a..61af8e1a0f0e 100644\n--- a/dbms/src/DataStreams/SquashingBlockInputStream.cpp\n+++ b/dbms/src/DataStreams/SquashingBlockInputStream.cpp\n@@ -4,7 +4,8 @@\n namespace DB\n {\n \n-SquashingBlockInputStream::SquashingBlockInputStream(BlockInputStreamPtr & src, size_t min_block_size_rows, size_t min_block_size_bytes)\n+SquashingBlockInputStream::SquashingBlockInputStream(const BlockInputStreamPtr & src,\n+                                                     size_t min_block_size_rows, size_t min_block_size_bytes)\n     : transform(min_block_size_rows, min_block_size_bytes)\n {\n     children.emplace_back(src);\ndiff --git a/dbms/src/DataStreams/SquashingBlockInputStream.h b/dbms/src/DataStreams/SquashingBlockInputStream.h\nindex 7d2583d35bac..3da819d21677 100644\n--- a/dbms/src/DataStreams/SquashingBlockInputStream.h\n+++ b/dbms/src/DataStreams/SquashingBlockInputStream.h\n@@ -12,7 +12,7 @@ namespace DB\n class SquashingBlockInputStream : public IProfilingBlockInputStream\n {\n public:\n-    SquashingBlockInputStream(BlockInputStreamPtr & src, size_t min_block_size_rows, size_t min_block_size_bytes);\n+    SquashingBlockInputStream(const BlockInputStreamPtr & src, size_t min_block_size_rows, size_t min_block_size_bytes);\n \n     String getName() const override { return \"Squashing\"; }\n \ndiff --git a/dbms/src/DataStreams/copyData.cpp b/dbms/src/DataStreams/copyData.cpp\nindex dcf908b1f9a0..cf015c38b790 100644\n--- a/dbms/src/DataStreams/copyData.cpp\n+++ b/dbms/src/DataStreams/copyData.cpp\n@@ -16,20 +16,21 @@ bool isAtomicSet(std::atomic<bool> * val)\n \n }\n \n-void copyData(IBlockInputStream & from, IBlockOutputStream & to, std::atomic<bool> * is_cancelled)\n+template <typename Pred>\n+void copyDataImpl(IBlockInputStream & from, IBlockOutputStream & to, Pred && is_cancelled)\n {\n     from.readPrefix();\n     to.writePrefix();\n \n     while (Block block = from.read())\n     {\n-        if (isAtomicSet(is_cancelled))\n+        if (is_cancelled())\n             break;\n \n         to.write(block);\n     }\n \n-    if (isAtomicSet(is_cancelled))\n+    if (is_cancelled())\n         return;\n \n     /// For outputting additional information in some formats.\n@@ -42,11 +43,28 @@ void copyData(IBlockInputStream & from, IBlockOutputStream & to, std::atomic<boo\n         to.setExtremes(input->getExtremes());\n     }\n \n-    if (isAtomicSet(is_cancelled))\n+    if (is_cancelled())\n         return;\n \n     from.readSuffix();\n     to.writeSuffix();\n }\n \n+\n+void copyData(IBlockInputStream & from, IBlockOutputStream & to, std::atomic<bool> * is_cancelled)\n+{\n+    auto is_cancelled_pred = [is_cancelled] ()\n+    {\n+        return isAtomicSet(is_cancelled);\n+    };\n+\n+    copyDataImpl(from, to, is_cancelled_pred);\n+}\n+\n+\n+void copyData(IBlockInputStream & from, IBlockOutputStream & to, const std::function<bool()> & is_cancelled)\n+{\n+    copyDataImpl(from, to, is_cancelled);\n+}\n+\n }\ndiff --git a/dbms/src/DataStreams/copyData.h b/dbms/src/DataStreams/copyData.h\nindex 2a42ef191cb1..6e8d54806c42 100644\n--- a/dbms/src/DataStreams/copyData.h\n+++ b/dbms/src/DataStreams/copyData.h\n@@ -1,6 +1,7 @@\n #pragma once\n \n #include <atomic>\n+#include <functional>\n \n \n namespace DB\n@@ -14,4 +15,6 @@ class IBlockOutputStream;\n   */\n void copyData(IBlockInputStream & from, IBlockOutputStream & to, std::atomic<bool> * is_cancelled = nullptr);\n \n+void copyData(IBlockInputStream & from, IBlockOutputStream & to, const std::function<bool()> & is_cancelled);\n+\n }\ndiff --git a/dbms/src/Databases/DatabaseDictionary.cpp b/dbms/src/Databases/DatabaseDictionary.cpp\nindex 43c1c1247aa6..69890ba075cc 100644\n--- a/dbms/src/Databases/DatabaseDictionary.cpp\n+++ b/dbms/src/Databases/DatabaseDictionary.cpp\n@@ -173,4 +173,9 @@ void DatabaseDictionary::drop()\n     /// Additional actions to delete database are not required.\n }\n \n+String DatabaseDictionary::getDataPath(const Context &) const\n+{\n+    return {};\n+}\n+\n }\ndiff --git a/dbms/src/Databases/DatabaseDictionary.h b/dbms/src/Databases/DatabaseDictionary.h\nindex 0aeba8db629e..d6a8944fa0dd 100644\n--- a/dbms/src/Databases/DatabaseDictionary.h\n+++ b/dbms/src/Databases/DatabaseDictionary.h\n@@ -93,6 +93,8 @@ class DatabaseDictionary : public IDatabase\n         const Context & context,\n         const String & table_name) const override;\n \n+    String getDataPath(const Context & context) const override;\n+\n     void shutdown() override;\n     void drop() override;\n };\ndiff --git a/dbms/src/Databases/DatabaseFactory.cpp b/dbms/src/Databases/DatabaseFactory.cpp\nindex c9259642e03e..f9976de9029e 100644\n--- a/dbms/src/Databases/DatabaseFactory.cpp\n+++ b/dbms/src/Databases/DatabaseFactory.cpp\n@@ -15,11 +15,11 @@ namespace ErrorCodes\n DatabasePtr DatabaseFactory::get(\n     const String & engine_name,\n     const String & database_name,\n-    const String & path,\n+    const String & metadata_path,\n     Context & context)\n {\n     if (engine_name == \"Ordinary\")\n-        return std::make_shared<DatabaseOrdinary>(database_name, path);\n+        return std::make_shared<DatabaseOrdinary>(database_name, metadata_path, context);\n     else if (engine_name == \"Memory\")\n         return std::make_shared<DatabaseMemory>(database_name);\n     else if (engine_name == \"Dictionary\")\ndiff --git a/dbms/src/Databases/DatabaseFactory.h b/dbms/src/Databases/DatabaseFactory.h\nindex 5e8d02ed1380..00265a2454b2 100644\n--- a/dbms/src/Databases/DatabaseFactory.h\n+++ b/dbms/src/Databases/DatabaseFactory.h\n@@ -13,7 +13,7 @@ class DatabaseFactory\n     static DatabasePtr get(\n         const String & engine_name,\n         const String & database_name,\n-        const String & path,\n+        const String & metadata_path,\n         Context & context);\n };\n \ndiff --git a/dbms/src/Databases/DatabaseMemory.cpp b/dbms/src/Databases/DatabaseMemory.cpp\nindex cdbfedb6326e..33a12947ca77 100644\n--- a/dbms/src/Databases/DatabaseMemory.cpp\n+++ b/dbms/src/Databases/DatabaseMemory.cpp\n@@ -152,4 +152,9 @@ void DatabaseMemory::drop()\n     /// Additional actions to delete database are not required.\n }\n \n+String DatabaseMemory::getDataPath(const Context &) const\n+{\n+    return {};\n+}\n+\n }\ndiff --git a/dbms/src/Databases/DatabaseMemory.h b/dbms/src/Databases/DatabaseMemory.h\nindex 662d8d0b6ae0..ef8207c86bf0 100644\n--- a/dbms/src/Databases/DatabaseMemory.h\n+++ b/dbms/src/Databases/DatabaseMemory.h\n@@ -84,6 +84,8 @@ class DatabaseMemory : public IDatabase\n         const Context & context,\n         const String & table_name) const override;\n \n+    String getDataPath(const Context & context) const override;\n+\n     void shutdown() override;\n     void drop() override;\n };\ndiff --git a/dbms/src/Databases/DatabaseOrdinary.cpp b/dbms/src/Databases/DatabaseOrdinary.cpp\nindex 3509a9356ebb..37ae8024ee03 100644\n--- a/dbms/src/Databases/DatabaseOrdinary.cpp\n+++ b/dbms/src/Databases/DatabaseOrdinary.cpp\n@@ -90,10 +90,10 @@ static void loadTable(\n }\n \n \n-DatabaseOrdinary::DatabaseOrdinary(\n-    const String & name_, const String & path_)\n-    : DatabaseMemory(name_), path(path_)\n+DatabaseOrdinary::DatabaseOrdinary(const String & name_, const String & metadata_path_, const Context & context)\n+    : DatabaseMemory(name_), metadata_path(metadata_path_), data_path(context.getPath() + \"data/\" + escapeForFileName(name_) + \"/\")\n {\n+    Poco::File(data_path).createDirectory();\n }\n \n \n@@ -108,7 +108,7 @@ void DatabaseOrdinary::loadTables(\n     FileNames file_names;\n \n     Poco::DirectoryIterator dir_end;\n-    for (Poco::DirectoryIterator dir_it(path); dir_it != dir_end; ++dir_it)\n+    for (Poco::DirectoryIterator dir_it(metadata_path); dir_it != dir_end; ++dir_it)\n     {\n         /// For '.svn', '.gitignore' directory and similar.\n         if (dir_it.name().at(0) == '.')\n@@ -130,7 +130,7 @@ void DatabaseOrdinary::loadTables(\n         if (endsWith(dir_it.name(), \".sql\"))\n             file_names.push_back(dir_it.name());\n         else\n-            throw Exception(\"Incorrect file extension: \" + dir_it.name() + \" in metadata directory \" + path,\n+            throw Exception(\"Incorrect file extension: \" + dir_it.name() + \" in metadata directory \" + metadata_path,\n                 ErrorCodes::INCORRECT_FILE_NAME);\n     }\n \n@@ -162,7 +162,7 @@ void DatabaseOrdinary::loadTables(\n                 watch.restart();\n             }\n \n-            loadTable(context, path, *this, name, data_path, table, has_force_restore_data_flag);\n+            loadTable(context, metadata_path, *this, name, data_path, table, has_force_restore_data_flag);\n         }\n     };\n \n@@ -269,7 +269,7 @@ void DatabaseOrdinary::createTable(\n             throw Exception(\"Table \" + name + \".\" + table_name + \" already exists.\", ErrorCodes::TABLE_ALREADY_EXISTS);\n     }\n \n-    String table_metadata_path = getTableMetadataPath(path, table_name);\n+    String table_metadata_path = getTableMetadataPath(metadata_path, table_name);\n     String table_metadata_tmp_path = table_metadata_path + \".tmp\";\n     String statement;\n \n@@ -312,7 +312,7 @@ void DatabaseOrdinary::removeTable(\n {\n     StoragePtr res = detachTable(table_name);\n \n-    String table_metadata_path = getTableMetadataPath(path, table_name);\n+    String table_metadata_path = getTableMetadataPath(metadata_path, table_name);\n \n     try\n     {\n@@ -374,7 +374,7 @@ void DatabaseOrdinary::renameTable(\n         throw Exception{e};\n     }\n \n-    ASTPtr ast = getCreateQueryImpl(path, table_name);\n+    ASTPtr ast = getCreateQueryImpl(metadata_path, table_name);\n     ASTCreateQuery & ast_create_query = typeid_cast<ASTCreateQuery &>(*ast);\n     ast_create_query.table = to_table_name;\n \n@@ -388,7 +388,7 @@ time_t DatabaseOrdinary::getTableMetadataModificationTime(\n     const Context & /*context*/,\n     const String & table_name)\n {\n-    String table_metadata_path = getTableMetadataPath(path, table_name);\n+    String table_metadata_path = getTableMetadataPath(metadata_path, table_name);\n     Poco::File meta_file(table_metadata_path);\n \n     if (meta_file.exists())\n@@ -406,7 +406,7 @@ ASTPtr DatabaseOrdinary::getCreateQuery(\n     const Context & /*context*/,\n     const String & table_name) const\n {\n-    ASTPtr ast = getCreateQueryImpl(path, table_name);\n+    ASTPtr ast = getCreateQueryImpl(metadata_path, table_name);\n \n     ASTCreateQuery & ast_create_query = typeid_cast<ASTCreateQuery &>(*ast);\n     ast_create_query.attach = false;\n@@ -455,8 +455,8 @@ void DatabaseOrdinary::alterTable(\n     /// Read the definition of the table and replace the necessary parts with new ones.\n \n     String table_name_escaped = escapeForFileName(name);\n-    String table_metadata_tmp_path = path + \"/\" + table_name_escaped + \".sql.tmp\";\n-    String table_metadata_path = path + \"/\" + table_name_escaped + \".sql\";\n+    String table_metadata_tmp_path = metadata_path + \"/\" + table_name_escaped + \".sql.tmp\";\n+    String table_metadata_path = metadata_path + \"/\" + table_name_escaped + \".sql\";\n     String statement;\n \n     {\n@@ -499,4 +499,9 @@ void DatabaseOrdinary::alterTable(\n     }\n }\n \n+String DatabaseOrdinary::getDataPath(const Context &) const\n+{\n+    return data_path;\n+}\n+\n }\ndiff --git a/dbms/src/Databases/DatabaseOrdinary.h b/dbms/src/Databases/DatabaseOrdinary.h\nindex 323e012b2691..efaabc590334 100644\n--- a/dbms/src/Databases/DatabaseOrdinary.h\n+++ b/dbms/src/Databases/DatabaseOrdinary.h\n@@ -13,10 +13,11 @@ namespace DB\n class DatabaseOrdinary : public DatabaseMemory\n {\n protected:\n-    const String path;\n+    const String metadata_path;\n+    const String data_path;\n \n public:\n-    DatabaseOrdinary(const String & name_, const String & path_);\n+    DatabaseOrdinary(const String & name_, const String & metadata_path_, const Context & context);\n \n     String getEngineName() const override { return \"Ordinary\"; }\n \n@@ -58,6 +59,8 @@ class DatabaseOrdinary : public DatabaseMemory\n         const Context & context,\n         const String & table_name) const override;\n \n+    String getDataPath(const Context & context) const override;\n+\n     void shutdown() override;\n     void drop() override;\n \ndiff --git a/dbms/src/Databases/IDatabase.h b/dbms/src/Databases/IDatabase.h\nindex f1c3b90f6aa2..3f30e83dfcbc 100644\n--- a/dbms/src/Databases/IDatabase.h\n+++ b/dbms/src/Databases/IDatabase.h\n@@ -129,6 +129,9 @@ class IDatabase : public std::enable_shared_from_this<IDatabase>\n         const Context & context,\n         const String & name) const = 0;\n \n+    /// Returns path for persistent data storage if the database supports it, empty string otherwise\n+    virtual String getDataPath(const Context & context) const = 0;\n+\n     /// Ask all tables to complete the background threads they are using and delete all table objects.\n     virtual void shutdown() = 0;\n \ndiff --git a/dbms/src/Interpreters/Cluster.cpp b/dbms/src/Interpreters/Cluster.cpp\nindex 81954444c750..7b4857431b62 100644\n--- a/dbms/src/Interpreters/Cluster.cpp\n+++ b/dbms/src/Interpreters/Cluster.cpp\n@@ -132,19 +132,26 @@ Clusters::Clusters(Poco::Util::AbstractConfiguration & config, const Settings &\n \n ClusterPtr Clusters::getCluster(const std::string & cluster_name) const\n {\n-    std::lock_guard<std::mutex> lock(mutex);\n+    std::lock_guard lock(mutex);\n \n     auto it = impl.find(cluster_name);\n     return (it != impl.end()) ? it->second : nullptr;\n }\n \n \n+void Clusters::setCluster(const String & cluster_name, const std::shared_ptr<Cluster> & cluster)\n+{\n+    std::lock_guard lock(mutex);\n+    impl[cluster_name] = cluster;\n+}\n+\n+\n void Clusters::updateClusters(Poco::Util::AbstractConfiguration & config, const Settings & settings, const String & config_name)\n {\n     Poco::Util::AbstractConfiguration::Keys config_keys;\n     config.keys(config_name, config_keys);\n \n-    std::lock_guard<std::mutex> lock(mutex);\n+    std::lock_guard lock(mutex);\n \n     for (const auto & key : config_keys)\n     {\n@@ -163,11 +170,12 @@ void Clusters::updateClusters(Poco::Util::AbstractConfiguration & config, const\n \n Clusters::Impl Clusters::getContainer() const\n {\n-    std::lock_guard<std::mutex> lock(mutex);\n+    std::lock_guard lock(mutex);\n     /// The following line copies container of shared_ptrs to return value under lock\n     return impl;\n }\n \n+\n /// Implementation of `Cluster` class\n \n Cluster::Cluster(Poco::Util::AbstractConfiguration & config, const Settings & settings, const String & cluster_name)\ndiff --git a/dbms/src/Interpreters/Cluster.h b/dbms/src/Interpreters/Cluster.h\nindex 18a418fc47b0..658ffc08a137 100644\n--- a/dbms/src/Interpreters/Cluster.h\n+++ b/dbms/src/Interpreters/Cluster.h\n@@ -97,6 +97,7 @@ class Cluster\n         UInt32 shard_num;\n         UInt32 weight;\n         Addresses local_addresses;\n+        /// nullptr if there are no remote addresses\n         ConnectionPoolWithFailoverPtr pool;\n         bool has_internal_replication;\n     };\n@@ -168,8 +169,9 @@ class Clusters\n     Clusters & operator=(const Clusters &) = delete;\n \n     ClusterPtr getCluster(const std::string & cluster_name) const;\n+    void setCluster(const String & cluster_name, const ClusterPtr & cluster);\n \n-    void updateClusters(Poco::Util::AbstractConfiguration & config, const Settings & settings, const String & config_name = \"remote_servers\");\n+    void updateClusters(Poco::Util::AbstractConfiguration & config, const Settings & settings, const String & config_name);\n \n public:\n     using Impl = std::map<String, ClusterPtr>;\ndiff --git a/dbms/src/Interpreters/Context.cpp b/dbms/src/Interpreters/Context.cpp\nindex edb3acdd45ac..c3bc60dadd54 100644\n--- a/dbms/src/Interpreters/Context.cpp\n+++ b/dbms/src/Interpreters/Context.cpp\n@@ -1370,13 +1370,27 @@ Clusters & Context::getClusters() const\n \n \n /// On repeating calls updates existing clusters and adds new clusters, doesn't delete old clusters\n-void Context::setClustersConfig(const ConfigurationPtr & config)\n+void Context::setClustersConfig(const ConfigurationPtr & config, const String & config_name)\n {\n     std::lock_guard<std::mutex> lock(shared->clusters_mutex);\n \n     shared->clusters_config = config;\n-    if (shared->clusters)\n-        shared->clusters->updateClusters(*shared->clusters_config, settings);\n+\n+    if (!shared->clusters)\n+        shared->clusters = std::make_unique<Clusters>(*shared->clusters_config, settings, config_name);\n+    else\n+        shared->clusters->updateClusters(*shared->clusters_config, settings, config_name);\n+}\n+\n+\n+void Context::setCluster(const String & cluster_name, const std::shared_ptr<Cluster> & cluster)\n+{\n+    std::lock_guard<std::mutex> lock(shared->clusters_mutex);\n+\n+    if (!shared->clusters)\n+        throw Exception(\"Clusters are not set\", ErrorCodes::LOGICAL_ERROR);\n+\n+    shared->clusters->setCluster(cluster_name, cluster);\n }\n \n \ndiff --git a/dbms/src/Interpreters/Context.h b/dbms/src/Interpreters/Context.h\nindex 374c48b1fd66..7e7e3622e7b1 100644\n--- a/dbms/src/Interpreters/Context.h\n+++ b/dbms/src/Interpreters/Context.h\n@@ -318,8 +318,10 @@ class Context\n     Clusters & getClusters() const;\n     std::shared_ptr<Cluster> getCluster(const std::string & cluster_name) const;\n     std::shared_ptr<Cluster> tryGetCluster(const std::string & cluster_name) const;\n+    void setClustersConfig(const ConfigurationPtr & config, const String & config_name = \"remote_servers\");\n+    /// Sets custom cluster, but doesn't update configuration\n+    void setCluster(const String & cluster_name, const std::shared_ptr<Cluster> & cluster);\n     void reloadClusterConfig();\n-    void setClustersConfig(const ConfigurationPtr & config);\n \n     Compiler & getCompiler();\n     QueryLog & getQueryLog();\ndiff --git a/dbms/src/Interpreters/InterpreterCheckQuery.h b/dbms/src/Interpreters/InterpreterCheckQuery.h\nindex 95a09e2bff79..dc2e9cc8bb1d 100644\n--- a/dbms/src/Interpreters/InterpreterCheckQuery.h\n+++ b/dbms/src/Interpreters/InterpreterCheckQuery.h\n@@ -7,11 +7,13 @@ namespace DB\n {\n \n class Context;\n+class Cluster;\n \n class InterpreterCheckQuery : public IInterpreter\n {\n public:\n     InterpreterCheckQuery(const ASTPtr & query_ptr_, const Context & context_);\n+\n     BlockIO execute() override;\n \n private:\n@@ -19,6 +21,7 @@ class InterpreterCheckQuery : public IInterpreter\n \n private:\n     ASTPtr query_ptr;\n+\n     const Context & context;\n     Block result;\n };\ndiff --git a/dbms/src/Interpreters/InterpreterCreateQuery.cpp b/dbms/src/Interpreters/InterpreterCreateQuery.cpp\nindex 4558741cdcaf..8e803a39f7de 100644\n--- a/dbms/src/Interpreters/InterpreterCreateQuery.cpp\n+++ b/dbms/src/Interpreters/InterpreterCreateQuery.cpp\n@@ -103,13 +103,10 @@ BlockIO InterpreterCreateQuery::createDatabase(ASTCreateQuery & create)\n \n     String database_name_escaped = escapeForFileName(database_name);\n \n-    /// Create directories for tables data and metadata.\n+    /// Create directories for tables metadata.\n     String path = context.getPath();\n-    String data_path = path + \"data/\" + database_name_escaped + \"/\";\n     String metadata_path = path + \"metadata/\" + database_name_escaped + \"/\";\n-\n     Poco::File(metadata_path).createDirectory();\n-    Poco::File(data_path).createDirectory();\n \n     DatabasePtr database = DatabaseFactory::get(database_engine_name, database_name, metadata_path, context);\n \n@@ -458,13 +455,9 @@ BlockIO InterpreterCreateQuery::createTable(ASTCreateQuery & create)\n     String current_database = context.getCurrentDatabase();\n \n     String database_name = create.database.empty() ? current_database : create.database;\n-    String database_name_escaped = escapeForFileName(database_name);\n     String table_name = create.table;\n     String table_name_escaped = escapeForFileName(table_name);\n \n-    String data_path = path + \"data/\" + database_name_escaped + \"/\";\n-    String metadata_path = path + \"metadata/\" + database_name_escaped + \"/\" + table_name_escaped + \".sql\";\n-\n     // If this is a stub ATTACH query, read the query definition from the database\n     if (create.attach && !create.storage && !create.columns)\n     {\n@@ -511,9 +504,13 @@ BlockIO InterpreterCreateQuery::createTable(ASTCreateQuery & create)\n     {\n         std::unique_ptr<DDLGuard> guard;\n \n+        String data_path;\n+        DatabasePtr database;\n+\n         if (!create.is_temporary)\n         {\n-            context.assertDatabaseExists(database_name);\n+            database = context.getDatabase(database_name);\n+            data_path = database->getDataPath(context);\n \n             /** If the table already exists, and the request specifies IF NOT EXISTS,\n               *  then we allow concurrent CREATE queries (which do nothing).\n@@ -548,7 +545,7 @@ BlockIO InterpreterCreateQuery::createTable(ASTCreateQuery & create)\n         if (create.is_temporary)\n             context.getSessionContext().addExternalTable(table_name, res);\n         else\n-            context.getDatabase(database_name)->createTable(context, table_name, res, query_ptr);\n+            database->createTable(context, table_name, res, query_ptr);\n     }\n \n     res->startup();\ndiff --git a/dbms/src/Interpreters/InterpreterDropQuery.cpp b/dbms/src/Interpreters/InterpreterDropQuery.cpp\nindex 053f0e24d54f..303081a54842 100644\n--- a/dbms/src/Interpreters/InterpreterDropQuery.cpp\n+++ b/dbms/src/Interpreters/InterpreterDropQuery.cpp\n@@ -66,6 +66,7 @@ BlockIO InterpreterDropQuery::execute()\n \n     String data_path = path + \"data/\" + database_name_escaped + \"/\";\n     String metadata_path = path + \"metadata/\" + database_name_escaped + \"/\";\n+    String database_metadata_path = path + \"metadata/\" + database_name_escaped + \".sql\";\n \n     auto database = context.tryGetDatabase(database_name);\n     if (!database && !drop.if_exists)\n@@ -163,6 +164,11 @@ BlockIO InterpreterDropQuery::execute()\n \n         Poco::File(data_path).remove(false);\n         Poco::File(metadata_path).remove(false);\n+\n+        /// Old ClickHouse versions did not store database.sql files\n+        Poco::File database_metadata_file(database_metadata_path);\n+        if (database_metadata_file.exists())\n+            database_metadata_file.remove(false);\n     }\n \n     return {};\ndiff --git a/dbms/src/Interpreters/loadMetadata.cpp b/dbms/src/Interpreters/loadMetadata.cpp\nindex 3f4d6d78b43f..efe8698d70fa 100644\n--- a/dbms/src/Interpreters/loadMetadata.cpp\n+++ b/dbms/src/Interpreters/loadMetadata.cpp\n@@ -136,7 +136,7 @@ void loadMetadataSystem(Context & context)\n         Poco::File(global_path + \"data/\" SYSTEM_DATABASE).createDirectories();\n         Poco::File(global_path + \"metadata/\" SYSTEM_DATABASE).createDirectories();\n \n-        auto system_database = std::make_shared<DatabaseOrdinary>(SYSTEM_DATABASE, global_path + \"metadata/\" SYSTEM_DATABASE);\n+        auto system_database = std::make_shared<DatabaseOrdinary>(SYSTEM_DATABASE, global_path + \"metadata/\" SYSTEM_DATABASE, context);\n         context.addDatabase(SYSTEM_DATABASE, system_database);\n     }\n \ndiff --git a/dbms/src/Parsers/ExpressionListParsers.cpp b/dbms/src/Parsers/ExpressionListParsers.cpp\nindex 5476d0cc3180..50f5ce81e513 100644\n--- a/dbms/src/Parsers/ExpressionListParsers.cpp\n+++ b/dbms/src/Parsers/ExpressionListParsers.cpp\n@@ -82,7 +82,7 @@ bool ParserList::parseImpl(Pos & pos, ASTPtr & node, Expected & expected)\n     auto list = std::make_shared<ASTExpressionList>();\n     node = list;\n \n-    while (1)\n+    while (true)\n     {\n         if (first)\n         {\ndiff --git a/dbms/src/Parsers/parseQuery.cpp b/dbms/src/Parsers/parseQuery.cpp\nindex 6a886b9b266e..56af3c5d991c 100644\n--- a/dbms/src/Parsers/parseQuery.cpp\n+++ b/dbms/src/Parsers/parseQuery.cpp\n@@ -317,6 +317,21 @@ ASTPtr parseQuery(\n }\n \n \n+ASTPtr parseQuery(\n+    IParser & parser,\n+    const std::string & query,\n+    const std::string & query_description)\n+{\n+    return parseQuery(parser, query.data(), query.data() + query.size(), query_description);\n+}\n+\n+\n+ASTPtr parseQuery(IParser & parser, const std::string & query)\n+{\n+    return parseQuery(parser, query.data(), query.data() + query.size(), parser.getName());\n+}\n+\n+\n std::pair<const char *, bool> splitMultipartQuery(const std::string & queries, std::vector<std::string> & queries_list)\n {\n     ASTPtr ast;\n@@ -357,4 +372,5 @@ std::pair<const char *, bool> splitMultipartQuery(const std::string & queries, s\n     return std::make_pair(begin, pos == end);\n }\n \n+\n }\ndiff --git a/dbms/src/Parsers/parseQuery.h b/dbms/src/Parsers/parseQuery.h\nindex e13f2993f9ce..727d38ceb2fd 100644\n--- a/dbms/src/Parsers/parseQuery.h\n+++ b/dbms/src/Parsers/parseQuery.h\n@@ -32,6 +32,15 @@ ASTPtr parseQuery(\n     const char * end,\n     const std::string & description);\n \n+ASTPtr parseQuery(\n+    IParser & parser,\n+    const std::string & query,\n+    const std::string & query_description);\n+\n+ASTPtr parseQuery(\n+    IParser & parser,\n+    const std::string & query);\n+\n \n /** Split queries separated by ; on to list of single queries\n   * Returns pointer to the end of last sucessfuly parsed query (first), and true if all queries are sucessfuly parsed (second)\ndiff --git a/dbms/src/Server/CMakeLists.txt b/dbms/src/Server/CMakeLists.txt\nindex d40683855055..18440450b176 100644\n--- a/dbms/src/Server/CMakeLists.txt\n+++ b/dbms/src/Server/CMakeLists.txt\n@@ -44,6 +44,9 @@ target_link_libraries (clickhouse-compressor-lib clickhouse_common_io ${Boost_PR\n add_library (clickhouse-format-lib ${SPLIT_SHARED} Format.cpp)\n target_link_libraries (clickhouse-format-lib clickhouse_common_io ${Boost_PROGRAM_OPTIONS_LIBRARY})\n \n+add_library (clickhouse-cluster-copier-lib ClusterCopier.cpp)\n+target_link_libraries (clickhouse-cluster-copier-lib clickhouse-server-lib clickhouse_functions clickhouse_aggregate_functions clickhouse_table_functions)\n+\n if (USE_EMBEDDED_COMPILER)\n     link_directories (${LLVM_LIBRARY_DIRS})\n     add_subdirectory (\"Compiler-${LLVM_VERSION}\")\n@@ -67,8 +70,11 @@ if (CLICKHOUSE_SPLIT_BINARY)\n     target_link_libraries (clickhouse-compressor clickhouse-compressor-lib)\n     add_executable (clickhouse-format clickhouse-format.cpp)\n     target_link_libraries (clickhouse-format clickhouse-format-lib dbms)\n+    add_executable (clickhouse-cluster-copier clickhouse-cluster-copier.cpp)\n+    target_link_libraries (clickhouse-cluster-copier clickhouse-cluster-copier-lib)\n \n-    set (CLICKHOUSE_ALL_TARGETS clickhouse-server clickhouse-client clickhouse-local clickhouse-benchmark clickhouse-performance-test clickhouse-extract-from-config clickhouse-format)\n+    set (CLICKHOUSE_ALL_TARGETS clickhouse-server clickhouse-client clickhouse-local clickhouse-benchmark clickhouse-performance-test\n+            clickhouse-extract-from-config clickhouse-format clickhouse-cluster-copier)\n \n     if (USE_EMBEDDED_COMPILER)\n         add_executable (clickhouse-clang clickhouse-clang.cpp)\n@@ -100,6 +106,7 @@ else ()\n         clickhouse-extract-from-config-lib\n         clickhouse-compressor-lib\n         clickhouse-format-lib\n+        clickhouse-cluster-copier-lib\n     )\n \n     add_custom_target (clickhouse-server ALL COMMAND ${CMAKE_COMMAND} -E create_symlink clickhouse clickhouse-server DEPENDS clickhouse)\n@@ -110,6 +117,7 @@ else ()\n     add_custom_target (clickhouse-extract-from-config ALL COMMAND ${CMAKE_COMMAND} -E create_symlink clickhouse clickhouse-extract-from-config DEPENDS clickhouse)\n     add_custom_target (clickhouse-compressor ALL COMMAND ${CMAKE_COMMAND} -E create_symlink clickhouse clickhouse-compressor DEPENDS clickhouse)\n     add_custom_target (clickhouse-format ALL COMMAND ${CMAKE_COMMAND} -E create_symlink clickhouse clickhouse-format DEPENDS clickhouse)\n+    add_custom_target (clickhouse-cluster-copier ALL COMMAND ${CMAKE_COMMAND} -E create_symlink clickhouse clickhouse-cluster-copier DEPENDS clickhouse)\n     # install always because depian package want this files:\n     add_custom_target (clickhouse-clang ALL COMMAND ${CMAKE_COMMAND} -E create_symlink clickhouse clickhouse-clang DEPENDS clickhouse)\n     add_custom_target (clickhouse-lld ALL COMMAND ${CMAKE_COMMAND} -E create_symlink clickhouse clickhouse-lld DEPENDS clickhouse)\n@@ -124,6 +132,7 @@ else ()\n        ${CMAKE_CURRENT_BINARY_DIR}/clickhouse-extract-from-config\n        ${CMAKE_CURRENT_BINARY_DIR}/clickhouse-compressor\n        ${CMAKE_CURRENT_BINARY_DIR}/clickhouse-format\n+       ${CMAKE_CURRENT_BINARY_DIR}/clickhouse-cluster-copier\n        ${CMAKE_CURRENT_BINARY_DIR}/clickhouse-clang\n        ${CMAKE_CURRENT_BINARY_DIR}/clickhouse-lld\n        DESTINATION ${CMAKE_INSTALL_BINDIR} COMPONENT clickhouse)\ndiff --git a/dbms/src/Server/ClusterCopier.cpp b/dbms/src/Server/ClusterCopier.cpp\nnew file mode 100644\nindex 000000000000..8af339d1a730\n--- /dev/null\n+++ b/dbms/src/Server/ClusterCopier.cpp\n@@ -0,0 +1,1641 @@\n+#include \"ClusterCopier.h\"\n+\n+#include <chrono>\n+\n+#include <Poco/Util/XMLConfiguration.h>\n+#include <Poco/Logger.h>\n+#include <Poco/ConsoleChannel.h>\n+#include <Poco/FormattingChannel.h>\n+#include <Poco/PatternFormatter.h>\n+#include <Poco/UUIDGenerator.h>\n+#include <Poco/File.h>\n+#include <Poco/Process.h>\n+#include <Poco/FileChannel.h>\n+#include <Poco/SplitterChannel.h>\n+#include <Poco/Util/HelpFormatter.h>\n+\n+#include <boost/algorithm/string.hpp>\n+#include <pcg_random.hpp>\n+\n+#include <Common/Exception.h>\n+#include <Common/ZooKeeper/ZooKeeper.h>\n+#include <Common/getFQDNOrHostName.h>\n+#include <Client/Connection.h>\n+#include <Interpreters/Context.h>\n+#include <Interpreters/Cluster.h>\n+#include <Interpreters/InterpreterSelectQuery.h>\n+#include <Interpreters/InterpreterInsertQuery.h>\n+#include <Interpreters/InterpreterFactory.h>\n+#include <Interpreters/InterpreterDropQuery.h>\n+#include <Interpreters/InterpreterCreateQuery.h>\n+\n+#include <common/logger_useful.h>\n+#include <common/ThreadPool.h>\n+#include <Common/typeid_cast.h>\n+#include <Common/ClickHouseRevision.h>\n+#include <Common/escapeForFileName.h>\n+#include <Columns/ColumnString.h>\n+#include <Columns/ColumnsNumber.h>\n+#include <Storages/StorageDistributed.h>\n+#include <Parsers/ParserCreateQuery.h>\n+#include <Parsers/parseQuery.h>\n+#include <Parsers/ParserQuery.h>\n+#include <Parsers/ASTCreateQuery.h>\n+#include <Parsers/queryToString.h>\n+#include <Parsers/ASTDropQuery.h>\n+#include <Parsers/ASTLiteral.h>\n+#include <Databases/DatabaseMemory.h>\n+#include <DataStreams/RemoteBlockInputStream.h>\n+#include <DataStreams/SquashingBlockInputStream.h>\n+#include <Common/isLocalAddress.h>\n+#include <DataStreams/copyData.h>\n+#include <DataStreams/NullBlockOutputStream.h>\n+#include <IO/Operators.h>\n+#include <IO/ReadBufferFromString.h>\n+#include <Functions/registerFunctions.h>\n+#include <TableFunctions/registerTableFunctions.h>\n+#include <AggregateFunctions/registerAggregateFunctions.h>\n+#include <Server/StatusFile.h>\n+#include <Storages/registerStorages.h>\n+\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int NO_ZOOKEEPER;\n+    extern const int BAD_ARGUMENTS;\n+    extern const int UNKNOWN_TABLE;\n+    extern const int UNFINISHED;\n+    extern const int UNKNOWN_ELEMENT_IN_CONFIG;\n+}\n+\n+\n+using ConfigurationPtr = Poco::AutoPtr<Poco::Util::AbstractConfiguration>;\n+\n+static ConfigurationPtr getConfigurationFromXMLString(const std::string & xml_data)\n+{\n+    std::stringstream ss(xml_data);\n+    Poco::XML::InputSource input_source{ss};\n+    return {new Poco::Util::XMLConfiguration{&input_source}};\n+}\n+\n+namespace\n+{\n+\n+using DatabaseAndTableName = std::pair<String, String>;\n+\n+\n+enum class TaskState\n+{\n+    Started = 0,\n+    Finished,\n+    Unknown\n+};\n+\n+\n+/// Used to mark status of shard partition tasks\n+struct TaskStateWithOwner\n+{\n+    TaskStateWithOwner() = default;\n+    TaskStateWithOwner(TaskState state, const String & owner) : state(state), owner(owner) {}\n+\n+    TaskState state{TaskState::Unknown};\n+    String owner;\n+\n+    static String getData(TaskState state, const String & owner)\n+    {\n+        return TaskStateWithOwner(state, owner).toString();\n+    }\n+\n+    String toString()\n+    {\n+        WriteBufferFromOwnString wb;\n+        wb << static_cast<UInt32>(state) << \"\\n\" << escape << owner;\n+        return wb.str();\n+    }\n+\n+    static TaskStateWithOwner fromString(const String & data)\n+    {\n+        ReadBufferFromString rb(data);\n+        TaskStateWithOwner res;\n+        UInt32 state;\n+\n+        rb >> state >> \"\\n\" >> escape >> res.owner;\n+\n+        if (state >= static_cast<int>(TaskState::Unknown))\n+            throw Exception(\"Unknown state \" + data, ErrorCodes::LOGICAL_ERROR);\n+\n+        res.state = static_cast<TaskState>(state);\n+        return res;\n+    }\n+};\n+\n+\n+/// Hierarchical description of the tasks\n+struct TaskPartition;\n+struct TaskShard;\n+struct TaskTable;\n+struct TaskCluster;\n+\n+using TasksPartition = std::map<String, TaskPartition>;\n+using ShardInfo = Cluster::ShardInfo;\n+using TaskShardPtr = std::shared_ptr<TaskShard>;\n+using TasksShard = std::vector<TaskShardPtr>;\n+using TasksTable = std::list<TaskTable>;\n+using PartitionToShards = std::map<String, TasksShard>;\n+\n+struct TaskPartition\n+{\n+    TaskPartition(TaskShard & parent, const String & name_) : task_shard(parent), name(name_) {}\n+\n+    String getPartitionPath() const;\n+    String getCommonPartitionIsDirtyPath() const;\n+    String getPartitionActiveWorkersPath() const;\n+    String getActiveWorkerPath() const;\n+    String getPartitionShardsPath() const;\n+    String getShardStatusPath() const;\n+\n+    TaskShard & task_shard;\n+    String name;\n+};\n+\n+\n+struct ShardPriority\n+{\n+    UInt8 is_remote = 1;\n+    size_t hostname_difference = 0;\n+    UInt8 random = 0;\n+\n+    static bool greaterPriority(const ShardPriority & current, const ShardPriority & other)\n+    {\n+        return std::forward_as_tuple(current.is_remote, current.hostname_difference, current.random)\n+               < std::forward_as_tuple(other.is_remote, other.hostname_difference, other.random);\n+    }\n+};\n+\n+\n+struct TaskShard\n+{\n+    TaskShard(TaskTable & parent, const ShardInfo & info_) : task_table(parent), info(info_) {}\n+\n+    TaskTable & task_table;\n+\n+    ShardInfo info;\n+    UInt32 numberInCluster() const { return info.shard_num; }\n+    UInt32 indexInCluster() const { return info.shard_num - 1; }\n+\n+    TasksPartition partitions;\n+\n+    ShardPriority priority;\n+};\n+\n+struct TaskTable\n+{\n+    TaskTable(TaskCluster & parent, const Poco::Util::AbstractConfiguration & config, const String & prefix,\n+                  const String & table_key);\n+\n+    TaskCluster & task_cluster;\n+\n+    String getPartitionPath(const String & partition_name) const;\n+    String getPartitionIsDirtyPath(const String & partition_name) const;\n+\n+    /// Used as task ID\n+    String name_in_config;\n+\n+    /// Source cluster and table\n+    String cluster_pull_name;\n+    DatabaseAndTableName table_pull;\n+\n+    /// Destination cluster and table\n+    String cluster_push_name;\n+    DatabaseAndTableName table_push;\n+\n+    /// Storage of destination table\n+    String engine_push_str;\n+    ASTPtr engine_push_ast;\n+\n+    /// Local Distributed table used to split data\n+    DatabaseAndTableName table_split;\n+    String sharding_key_str;\n+    ASTPtr sharding_key_ast;\n+    ASTPtr engine_split_ast;\n+\n+    /// Additional WHERE expression to filter input data\n+    String where_condition_str;\n+    ASTPtr where_condition_ast;\n+\n+    /// Resolved clusters\n+    ClusterPtr cluster_pull;\n+    ClusterPtr cluster_push;\n+\n+    /// Filter partitions that should be copied\n+    bool has_enabled_partitions = false;\n+    NameSet enabled_partitions;\n+\n+    /// Prioritized list of shards\n+    TasksShard all_shards;\n+    TasksShard local_shards;\n+\n+    PartitionToShards partition_to_shards;\n+\n+    template <typename RandomEngine>\n+    void initShards(RandomEngine && random_engine);\n+};\n+\n+struct TaskCluster\n+{\n+    TaskCluster(const String & task_zookeeper_path_, const Poco::Util::AbstractConfiguration & config, const String & base_key, const String & default_local_database_);\n+\n+    /// Base node for all tasks. Its structure:\n+    ///  workers/ - directory with active workers (amount of them is less or equal max_workers)\n+    ///  description - node with task configuration\n+    ///  table_table1/ - directories with per-partition copying status\n+    String task_zookeeper_path;\n+\n+    /// Limits number of simultaneous workers\n+    size_t max_workers = 0;\n+\n+    /// Settings used to fetch data\n+    Settings settings_pull;\n+    /// Settings used to insert data\n+    Settings settings_push;\n+\n+    /// Subtasks\n+    TasksTable table_tasks;\n+\n+    /// Database used to create temporary Distributed tables\n+    String default_local_database;\n+\n+    /// Path to remote_servers in task config\n+    String clusters_prefix;\n+\n+    std::random_device random_device;\n+    pcg64 random_engine;\n+};\n+\n+\n+String getDatabaseDotTable(const String & database, const String & table)\n+{\n+    return backQuoteIfNeed(database) + \".\" + backQuoteIfNeed(table);\n+}\n+\n+String getDatabaseDotTable(const DatabaseAndTableName & db_and_table)\n+{\n+    return getDatabaseDotTable(db_and_table.first, db_and_table.second);\n+}\n+\n+\n+/// Atomically checks that is_dirty node is not exists, and made the remaining op\n+/// Returns relative number of failed operation in the second field (the passed op has 0 index)\n+static void checkNoNodeAndCommit(\n+    const zkutil::ZooKeeperPtr & zookeeper,\n+    const String & checking_node_path,\n+    zkutil::OpPtr && op,\n+    zkutil::MultiTransactionInfo & info)\n+{\n+    zkutil::Ops ops;\n+    ops.emplace_back(std::make_unique<zkutil::Op::Create>(checking_node_path, \"\", zookeeper->getDefaultACL(), zkutil::CreateMode::Persistent));\n+    ops.emplace_back(std::make_unique<zkutil::Op::Remove>(checking_node_path, -1));\n+    ops.emplace_back(std::move(op));\n+\n+    zookeeper->tryMultiUnsafe(ops, info);\n+    if (info.code != ZOK && !zkutil::isUserError(info.code))\n+        throw info.getException();\n+}\n+\n+\n+// Creates AST representing 'ENGINE = Distributed(cluster, db, table, [sharding_key])\n+std::shared_ptr<ASTStorage> createASTStorageDistributed(\n+    const String & cluster_name, const String & database, const String & table, const ASTPtr & sharding_key_ast = nullptr)\n+{\n+    auto args = std::make_shared<ASTExpressionList>();\n+    args->children.emplace_back(std::make_shared<ASTLiteral>(StringRange(nullptr, nullptr), cluster_name));\n+    args->children.emplace_back(std::make_shared<ASTIdentifier>(StringRange(nullptr, nullptr), database));\n+    args->children.emplace_back(std::make_shared<ASTIdentifier>(StringRange(nullptr, nullptr), table));\n+    if (sharding_key_ast)\n+        args->children.emplace_back(sharding_key_ast);\n+\n+    auto engine = std::make_shared<ASTFunction>();\n+    engine->name = \"Distributed\";\n+    engine->arguments = args;\n+\n+    auto storage = std::make_shared<ASTStorage>();\n+    storage->set(storage->engine, engine);\n+\n+    return storage;\n+}\n+\n+\n+BlockInputStreamPtr squashStreamIntoOneBlock(const BlockInputStreamPtr & stream)\n+{\n+    return std::make_shared<SquashingBlockInputStream>(\n+        stream,\n+        std::numeric_limits<size_t>::max(),\n+        std::numeric_limits<size_t>::max()\n+    );\n+}\n+\n+Block getBlockWithAllStreamData(const BlockInputStreamPtr & stream)\n+{\n+    return squashStreamIntoOneBlock(stream)->read();\n+}\n+\n+// Path getters\n+\n+String TaskTable::getPartitionPath(const String & partition_name) const\n+{\n+    return task_cluster.task_zookeeper_path                     // root\n+           + \"/tables/\" + escapeForFileName(name_in_config)     // tables/table_hits\n+           + \"/\" + partition_name;                              // 201701\n+}\n+\n+String TaskPartition::getPartitionPath() const\n+{\n+    return task_shard.task_table.getPartitionPath(name);\n+}\n+\n+String TaskPartition::getShardStatusPath() const\n+{\n+    // /root/table_test.hits/201701/1\n+    return getPartitionPath() + \"/shards/\" + toString(task_shard.numberInCluster());\n+}\n+\n+String TaskPartition::getPartitionShardsPath() const\n+{\n+    return getPartitionPath() + \"/shards\";\n+}\n+\n+String TaskPartition::getPartitionActiveWorkersPath() const\n+{\n+    return getPartitionPath() + \"/partition_active_workers\";\n+}\n+\n+String TaskPartition::getActiveWorkerPath() const\n+{\n+    return getPartitionActiveWorkersPath() + \"/\" + toString(task_shard.numberInCluster());\n+}\n+\n+String TaskPartition::getCommonPartitionIsDirtyPath() const\n+{\n+    return getPartitionPath() + \"/is_dirty\";\n+}\n+\n+String TaskTable::getPartitionIsDirtyPath(const String & partition_name) const\n+{\n+    return getPartitionPath(partition_name) + \"/is_dirty\";\n+}\n+\n+\n+TaskTable::TaskTable(TaskCluster & parent, const Poco::Util::AbstractConfiguration & config, const String & prefix_,\n+                     const String & table_key)\n+: task_cluster(parent)\n+{\n+    String table_prefix = prefix_ + \".\" + table_key + \".\";\n+\n+    name_in_config = table_key;\n+\n+    cluster_pull_name = config.getString(table_prefix + \"cluster_pull\");\n+    cluster_push_name = config.getString(table_prefix + \"cluster_push\");\n+\n+    table_pull.first = config.getString(table_prefix + \"database_pull\");\n+    table_pull.second = config.getString(table_prefix + \"table_pull\");\n+\n+    table_push.first = config.getString(table_prefix + \"database_push\");\n+    table_push.second = config.getString(table_prefix + \"table_push\");\n+\n+    engine_push_str = config.getString(table_prefix + \"engine\");\n+    {\n+        ParserStorage parser_storage;\n+        engine_push_ast = parseQuery(parser_storage, engine_push_str);\n+    }\n+\n+    sharding_key_str = config.getString(table_prefix + \"sharding_key\");\n+    {\n+        ParserExpressionWithOptionalAlias parser_expression(false);\n+        sharding_key_ast = parseQuery(parser_expression, sharding_key_str);\n+        engine_split_ast = createASTStorageDistributed(cluster_push_name, table_push.first, table_push.second, sharding_key_ast);\n+\n+        table_split = DatabaseAndTableName(task_cluster.default_local_database, \".split.\" + name_in_config);\n+    }\n+\n+    where_condition_str = config.getString(table_prefix + \"where_condition\", \"\");\n+    if (!where_condition_str.empty())\n+    {\n+        ParserExpressionWithOptionalAlias parser_expression(false);\n+        where_condition_ast = parseQuery(parser_expression, where_condition_str);\n+\n+        // Will use canonical expression form\n+        where_condition_str = queryToString(where_condition_ast);\n+    }\n+\n+    String enabled_partitions_prefix = table_prefix + \"enabled_partitions\";\n+    has_enabled_partitions = config.has(enabled_partitions_prefix);\n+\n+    if (has_enabled_partitions)\n+    {\n+        Strings keys;\n+        config.keys(enabled_partitions_prefix, keys);\n+\n+        Strings partitions;\n+        if (keys.empty())\n+        {\n+            /// Parse list of partition from space-separated string\n+            String partitions_str = config.getString(table_prefix + \"enabled_partitions\");\n+            boost::trim_if(partitions_str, isWhitespaceASCII);\n+            boost::split(partitions, partitions_str, isWhitespaceASCII, boost::token_compress_on);\n+        }\n+        else\n+        {\n+            /// Parse sequence of <partition>...</partition>\n+            for (const String & key : keys)\n+            {\n+                if (!startsWith(key, \"partition\"))\n+                    throw Exception(\"Unknown key \" + key + \" in \" + enabled_partitions_prefix, ErrorCodes::UNKNOWN_ELEMENT_IN_CONFIG);\n+\n+                partitions.emplace_back(config.getString(enabled_partitions_prefix + \".\" + key));\n+            }\n+        }\n+\n+        std::copy(partitions.begin(), partitions.end(), std::inserter(enabled_partitions, enabled_partitions.begin()));\n+    }\n+\n+}\n+\n+\n+static ShardPriority getReplicasPriority(const Cluster::Addresses & replicas, const std::string & local_hostname, UInt8 random)\n+{\n+    ShardPriority res;\n+\n+    if (replicas.empty())\n+        return res;\n+\n+    res.is_remote = 1;\n+    for (auto & replica : replicas)\n+    {\n+        if (isLocalAddress(replica.resolved_address))\n+        {\n+            res.is_remote = 0;\n+            break;\n+        }\n+    }\n+\n+    res.hostname_difference = std::numeric_limits<size_t>::max();\n+    for (auto & replica : replicas)\n+    {\n+        size_t difference = getHostNameDifference(local_hostname, replica.host_name);\n+        res.hostname_difference = std::min(difference, res.hostname_difference);\n+    }\n+\n+    res.random = random;\n+    return res;\n+}\n+\n+template<typename RandomEngine>\n+void TaskTable::initShards(RandomEngine && random_engine)\n+{\n+    const String & fqdn_name = getFQDNOrHostName();\n+    std::uniform_int_distribution<UInt8> get_urand(0, std::numeric_limits<UInt8>::max());\n+\n+    // Compute the priority\n+    for (auto & shard_info : cluster_pull->getShardsInfo())\n+    {\n+        TaskShardPtr task_shard = std::make_shared<TaskShard>(*this, shard_info);\n+        const auto & replicas = cluster_pull->getShardsAddresses().at(task_shard->indexInCluster());\n+        task_shard->priority = getReplicasPriority(replicas, fqdn_name, get_urand(random_engine));\n+\n+        all_shards.emplace_back(task_shard);\n+    }\n+\n+    // Sort by priority\n+    std::sort(all_shards.begin(), all_shards.end(),\n+        [] (const TaskShardPtr & lhs, const TaskShardPtr & rhs)\n+        {\n+            return ShardPriority::greaterPriority(lhs->priority, rhs->priority);\n+        });\n+\n+    // Cut local shards\n+    auto it_first_remote = std::lower_bound(all_shards.begin(), all_shards.end(), 1,\n+        [] (const TaskShardPtr & lhs, UInt8 is_remote)\n+        {\n+            return lhs->priority.is_remote < is_remote;\n+        });\n+\n+    local_shards.assign(all_shards.begin(), it_first_remote);\n+}\n+\n+TaskCluster::TaskCluster(const String & task_zookeeper_path_, const Poco::Util::AbstractConfiguration & config, const String & base_key,\n+                         const String & default_local_database_)\n+{\n+    String prefix = base_key.empty() ? \"\" : base_key + \".\";\n+\n+    task_zookeeper_path = task_zookeeper_path_;\n+\n+    default_local_database = default_local_database_;\n+\n+    max_workers = config.getUInt64(prefix + \"max_workers\");\n+\n+    if (config.has(prefix + \"settings\"))\n+    {\n+        settings_pull.loadSettingsFromConfig(prefix + \"settings\", config);\n+        settings_push.loadSettingsFromConfig(prefix + \"settings\", config);\n+    }\n+\n+    if (config.has(prefix + \"settings_pull\"))\n+        settings_pull.loadSettingsFromConfig(prefix + \"settings_pull\", config);\n+\n+    if (config.has(prefix + \"settings_push\"))\n+        settings_push.loadSettingsFromConfig(prefix + \"settings_push\", config);\n+\n+    clusters_prefix = prefix + \"remote_servers\";\n+\n+    if (!config.has(clusters_prefix))\n+        throw Exception(\"You should specify list of clusters in \" + clusters_prefix, ErrorCodes::BAD_ARGUMENTS);\n+\n+    Poco::Util::AbstractConfiguration::Keys tables_keys;\n+    config.keys(prefix + \"tables\", tables_keys);\n+\n+    for (const auto & table_key : tables_keys)\n+    {\n+        table_tasks.emplace_back(*this, config, prefix + \"tables\", table_key);\n+    }\n+}\n+\n+} // end of an anonymous namespace\n+\n+\n+class ClusterCopier\n+{\n+public:\n+\n+    ClusterCopier(const ConfigurationPtr & zookeeper_config_,\n+                  const String & task_path_,\n+                  const String & host_id_,\n+                  const String & proxy_database_name_,\n+                  Context & context_)\n+    :\n+        zookeeper_config(zookeeper_config_),\n+        task_zookeeper_path(task_path_),\n+        host_id(host_id_),\n+        working_database_name(proxy_database_name_),\n+        context(context_),\n+        log(&Poco::Logger::get(\"ClusterCopier\"))\n+    {\n+        initZooKeeper();\n+    }\n+\n+    void init()\n+    {\n+        String description_path = task_zookeeper_path + \"/description\";\n+        String task_config_str = getZooKeeper()->get(description_path);\n+\n+        task_cluster_config = getConfigurationFromXMLString(task_config_str);\n+        task_cluster = std::make_unique<TaskCluster>(task_zookeeper_path, *task_cluster_config, \"\", working_database_name);\n+\n+        /// Override important settings\n+        Settings & settings_pull = task_cluster->settings_pull;\n+        settings_pull.load_balancing = LoadBalancing::NEAREST_HOSTNAME;\n+        settings_pull.limits.readonly = 1;\n+        settings_pull.max_threads = 1;\n+        settings_pull.max_block_size = std::min(8192UL, settings_pull.max_block_size.value);\n+        settings_pull.preferred_block_size_bytes = 0;\n+\n+        Settings & settings_push = task_cluster->settings_push;\n+        settings_push.insert_distributed_timeout = 0;\n+        settings_push.insert_distributed_sync = 1;\n+\n+        /// Set up clusters\n+        context.setClustersConfig(task_cluster_config, task_cluster->clusters_prefix);\n+\n+        /// Set up shards and their priority\n+        task_cluster->random_engine.seed(task_cluster->random_device());\n+        for (auto & task_table : task_cluster->table_tasks)\n+        {\n+            task_table.cluster_pull = context.getCluster(task_table.cluster_pull_name);\n+            task_table.cluster_push = context.getCluster(task_table.cluster_push_name);\n+            task_table.initShards(task_cluster->random_engine);\n+        }\n+\n+        LOG_DEBUG(log, \"Loaded \" << task_cluster->table_tasks.size() << \" table tasks\");\n+\n+        /// Compute set of partitions, set of partitions aren't changed\n+        for (auto & task_table : task_cluster->table_tasks)\n+        {\n+            for (const TaskShardPtr & task_shard : task_table.all_shards)\n+            {\n+                if (task_shard->info.pool == nullptr)\n+                {\n+                    throw Exception(\"It is impossible to have only local shards, at least port number must be different\",\n+                                    ErrorCodes::LOGICAL_ERROR);\n+                }\n+\n+                LOG_DEBUG(log, \"Set up table task \" << task_table.name_in_config << \" (\"\n+                               << \"cluster \" << task_table.cluster_pull_name\n+                               << \", table \" << getDatabaseDotTable(task_table.table_pull)\n+                               << \", shard \" << task_shard->info.shard_num << \")\");\n+\n+                LOG_DEBUG(log, \"There are \"\n+                    << task_table.all_shards.size() << \" shards, \"\n+                    << task_table.local_shards.size() << \" of them are remote ones\");\n+\n+                auto connection_entry = task_shard->info.pool->get(&task_cluster->settings_pull);\n+                LOG_DEBUG(log, \"Will get meta information for shard \" << task_shard->numberInCluster()\n+                               << \" from replica \" << connection_entry->getDescription());\n+\n+                Strings partitions = getRemotePartitions(task_table.table_pull, *connection_entry, &task_cluster->settings_pull);\n+                for (const String & partition_name : partitions)\n+                {\n+                    /// Do not process partition if it is not in enabled_partitions list\n+                    if (task_table.has_enabled_partitions && !task_table.enabled_partitions.count(partition_name))\n+                    {\n+                        LOG_DEBUG(log, \"Will skip partition \" << partition_name);\n+                        continue;\n+                    }\n+\n+                    task_shard->partitions.emplace(partition_name, TaskPartition(*task_shard, partition_name));\n+                    task_table.partition_to_shards[partition_name].emplace_back(task_shard);\n+                }\n+\n+                LOG_DEBUG(log, \"Will fetch \" << task_shard->partitions.size() << \" partitions\");\n+            }\n+        }\n+\n+        auto zookeeper = getZooKeeper();\n+        zookeeper->createAncestors(getWorkersPath() + \"/\");\n+    }\n+\n+    void process()\n+    {\n+        for (TaskTable & task_table : task_cluster->table_tasks)\n+        {\n+            if (task_table.all_shards.empty())\n+                continue;\n+\n+            /// An optimization: first of all, try to process all partitions of the local shards\n+//            for (const TaskShardPtr & shard : task_table.local_shards)\n+//            {\n+//                for (auto & task_partition : shard->partitions)\n+//                {\n+//                    LOG_DEBUG(log, \"Processing partition \" << task_partition.first << \" for local shard \" << shard->numberInCluster());\n+//                    processPartitionTask(task_partition.second);\n+//                }\n+//            }\n+\n+            /// Then check and copy all shards until the whole partition is copied\n+            for (const auto & partition_with_shards : task_table.partition_to_shards)\n+            {\n+                const String & partition_name = partition_with_shards.first;\n+                const TasksShard & shards_with_partition = partition_with_shards.second;\n+                bool is_done;\n+\n+                size_t num_tries = 0;\n+                constexpr size_t max_tries = 1000;\n+\n+                Stopwatch watch;\n+\n+                do\n+                {\n+                    LOG_DEBUG(log, \"Processing partition \" << partition_name << \" for the whole cluster\"\n+                        << \" (\" << shards_with_partition.size() << \" shards)\");\n+\n+                    size_t num_successful_shards = 0;\n+\n+                    for (const TaskShardPtr & shard : shards_with_partition)\n+                    {\n+                        auto it_shard_partition = shard->partitions.find(partition_name);\n+                        if (it_shard_partition == shard->partitions.end())\n+                            throw Exception(\"There are no such partition in a shard. This is a bug.\", ErrorCodes::LOGICAL_ERROR);\n+\n+                        TaskPartition & task_shard_partition = it_shard_partition->second;\n+                        if (processPartitionTask(task_shard_partition))\n+                            ++num_successful_shards;\n+                    }\n+\n+                    try\n+                    {\n+                        is_done = (num_successful_shards == shards_with_partition.size())\n+                            && checkPartitionIsDone(task_table, partition_name, shards_with_partition);\n+                    }\n+                    catch (...)\n+                    {\n+                        tryLogCurrentException(log);\n+                        is_done = false;\n+                    }\n+\n+                    if (!is_done)\n+                        std::this_thread::sleep_for(default_sleep_time);\n+\n+                    ++num_tries;\n+                } while (!is_done && num_tries < max_tries);\n+\n+                if (!is_done)\n+                    throw Exception(\"Too many retries while copying partition\", ErrorCodes::UNFINISHED);\n+                else\n+                    LOG_INFO(log, \"It took \" << watch.elapsedSeconds() << \" seconds to copy partition \" << partition_name);\n+            }\n+        }\n+    }\n+\n+    /// Disables DROP PARTITION commands that used to clear data after errors\n+    void setSafeMode(bool is_safe_mode_ = true)\n+    {\n+        is_safe_mode = is_safe_mode_;\n+    }\n+\n+    void setCopyFaultProbability(double copy_fault_probability_)\n+    {\n+        copy_fault_probability = copy_fault_probability_;\n+    }\n+\n+    /** Checks that the whole partition of a table was copied. We should do it carefully due to dirty lock.\n+     * State of some task could be changed during the processing.\n+     * We have to ensure that all shards have the finished state and there are no dirty flag.\n+     * Moreover, we have to check status twice and check zxid, because state could be changed during the checking.\n+     */\n+    bool checkPartitionIsDone(const TaskTable & task_table, const String & partition_name, const TasksShard & shards_with_partition)\n+    {\n+        LOG_DEBUG(log, \"Check that all shards processed partition \" << partition_name << \" successfully\");\n+\n+        auto zookeeper = getZooKeeper();\n+\n+        Strings status_paths;\n+        for (auto & shard : shards_with_partition)\n+        {\n+            TaskPartition & task_shard_partition = shard->partitions.find(partition_name)->second;\n+            status_paths.emplace_back(task_shard_partition.getShardStatusPath());\n+        }\n+\n+        zkutil::Stat stat;\n+        std::vector<int64_t> zxid1, zxid2;\n+\n+        try\n+        {\n+            // Check that state is Finished and remember zxid\n+            for (const String & path : status_paths)\n+            {\n+                TaskStateWithOwner status = TaskStateWithOwner::fromString(zookeeper->get(path, &stat));\n+                if (status.state != TaskState::Finished)\n+                {\n+                    LOG_INFO(log, \"The task \" << path << \" is being rewritten by \" << status.owner\n+                                               << \". Partition will be rechecked\");\n+                    return false;\n+                }\n+                zxid1.push_back(stat.pzxid);\n+            }\n+\n+            // Check that partition is not dirty\n+            if (zookeeper->exists(task_table.getPartitionIsDirtyPath(partition_name)))\n+            {\n+                LOG_INFO(log, \"Partition \" << partition_name << \" become dirty\");\n+                return false;\n+            }\n+\n+            // Remember zxid of states again\n+            for (const auto & path : status_paths)\n+            {\n+                zookeeper->exists(path, &stat);\n+                zxid2.push_back(stat.pzxid);\n+            }\n+        }\n+        catch (const zkutil::KeeperException & e)\n+        {\n+            LOG_INFO(log, \"A ZooKeeper error occurred while checking partition \" << partition_name\n+                          << \". Will recheck the partition. Error: \" << e.what());\n+            return false;\n+        }\n+\n+        // If all task is finished and zxid is not changed then partition could not become dirty again\n+        for (size_t shard_num = 0; shard_num < status_paths.size(); ++shard_num)\n+        {\n+            if (zxid1[shard_num] != zxid2[shard_num])\n+            {\n+                LOG_INFO(log, \"The task \" << status_paths[shard_num] << \" is being modified now. Partition will be rechecked\");\n+                return false;\n+            }\n+        }\n+\n+        LOG_INFO(log, \"Partition \" << partition_name << \" is copied successfully\");\n+        return true;\n+    }\n+\n+protected:\n+\n+    String getWorkersPath() const\n+    {\n+        return task_cluster->task_zookeeper_path + \"/task_active_workers\";\n+    }\n+\n+    String getCurrentWorkerNodePath() const\n+    {\n+        return getWorkersPath() + \"/\" + host_id;\n+    }\n+\n+    zkutil::EphemeralNodeHolder::Ptr createWorkerNodeAndWaitIfNeed(const zkutil::ZooKeeperPtr & zookeeper, const String & task_description)\n+    {\n+        while (true)\n+        {\n+            zkutil::Stat stat;\n+            zookeeper->get(getWorkersPath(), &stat);\n+\n+            if (static_cast<size_t>(stat.numChildren) >= task_cluster->max_workers)\n+            {\n+                LOG_DEBUG(log, \"Too many workers (\" << stat.numChildren << \", maximum \" << task_cluster->max_workers << \")\"\n+                    << \". Postpone processing \" << task_description);\n+                std::this_thread::sleep_for(default_sleep_time);\n+            }\n+            else\n+            {\n+                return std::make_shared<zkutil::EphemeralNodeHolder>(getCurrentWorkerNodePath(), *zookeeper, true, false, task_description);\n+            }\n+        }\n+    }\n+\n+    std::shared_ptr<ASTCreateQuery> rewriteCreateQueryStorage(const ASTPtr & create_query_pull, const DatabaseAndTableName & new_table,\n+                                     const ASTPtr & new_storage_ast)\n+    {\n+        auto & create = typeid_cast<ASTCreateQuery &>(*create_query_pull);\n+        auto res = std::make_shared<ASTCreateQuery>(create);\n+\n+        if (create.storage == nullptr || new_storage_ast == nullptr)\n+            throw Exception(\"Storage is not specified\", ErrorCodes::LOGICAL_ERROR);\n+\n+        res->database = new_table.first;\n+        res->table = new_table.second;\n+\n+        res->children.clear();\n+        res->set(res->columns, create.columns->clone());\n+        res->set(res->storage, new_storage_ast->clone());\n+\n+        return res;\n+    }\n+\n+    bool tryDropPartition(TaskPartition & task_partition, const zkutil::ZooKeeperPtr & zookeeper)\n+    {\n+        if (is_safe_mode)\n+            throw Exception(\"DROP PARTITION is prohibited in safe mode\", ErrorCodes::NOT_IMPLEMENTED);\n+\n+        TaskTable & task_table = task_partition.task_shard.task_table;\n+\n+        String current_shards_path = task_partition.getPartitionShardsPath();\n+        String current_partition_active_workers_dir = task_partition.getPartitionActiveWorkersPath();\n+        String is_dirty_flag_path = task_partition.getCommonPartitionIsDirtyPath();\n+        String dirt_cleaner_path = is_dirty_flag_path + \"/cleaner\";\n+\n+        zkutil::EphemeralNodeHolder::Ptr cleaner_holder;\n+        try\n+        {\n+            cleaner_holder = zkutil::EphemeralNodeHolder::create(dirt_cleaner_path, *zookeeper, host_id);\n+        }\n+        catch (zkutil::KeeperException & e)\n+        {\n+            if (e.code == ZNODEEXISTS)\n+            {\n+                LOG_DEBUG(log, \"Partition \" << task_partition.name << \" is cleaning now by somebody, sleep\");\n+                std::this_thread::sleep_for(default_sleep_time);\n+                return false;\n+            }\n+\n+            throw;\n+        }\n+\n+        zkutil::Stat stat;\n+        if (zookeeper->exists(current_partition_active_workers_dir, &stat))\n+        {\n+            if (stat.numChildren != 0)\n+            {\n+                LOG_DEBUG(log, \"Partition \" << task_partition.name << \" contains \" << stat.numChildren << \" active workers, sleep\");\n+                std::this_thread::sleep_for(default_sleep_time);\n+                return false;\n+            }\n+        }\n+\n+        /// Remove all status nodes\n+        zookeeper->tryRemoveRecursive(current_shards_path);\n+\n+        String query = \"ALTER TABLE \" + getDatabaseDotTable(task_table.table_push);\n+        query += \" DROP PARTITION \" + task_partition.name + \"\";\n+\n+        /// TODO: use this statement after servers will be updated up to 1.1.54310\n+        // query += \" DROP PARTITION ID '\" + task_partition.name + \"'\";\n+\n+        ClusterPtr & cluster_push = task_table.cluster_push;\n+        Settings settings_push = task_cluster->settings_push;\n+\n+        /// It is important, DROP PARTITION must be done synchronously\n+        settings_push.replication_alter_partitions_sync = 2;\n+\n+        LOG_DEBUG(log, \"Execute distributed DROP PARTITION: \" << query);\n+        /// Limit number of max executing replicas to 1\n+        size_t num_shards = executeQueryOnCluster(cluster_push, query, nullptr, &settings_push, PoolMode::GET_ALL, 1);\n+\n+        if (num_shards < cluster_push->getShardCount())\n+        {\n+            LOG_INFO(log, \"DROP PARTITION wasn't successfully executed on \" << cluster_push->getShardCount() - num_shards << \" shards\");\n+            return false;\n+        }\n+\n+        /// Remove the locking node\n+        cleaner_holder.reset();\n+        zookeeper->remove(is_dirty_flag_path);\n+\n+        LOG_INFO(log, \"Partition \" << task_partition.name << \" was dropped on cluster \" << task_table.cluster_push_name);\n+        return true;\n+    }\n+\n+    bool processPartitionTask(TaskPartition & task_partition)\n+    {\n+        try\n+        {\n+            return processPartitionTaskImpl(task_partition);\n+        }\n+        catch (...)\n+        {\n+            tryLogCurrentException(log, \"An error occurred while processing partition \" + task_partition.name);\n+            return false;\n+        }\n+    }\n+\n+    bool processPartitionTaskImpl(TaskPartition & task_partition)\n+    {\n+        TaskShard & task_shard = task_partition.task_shard;\n+        TaskTable & task_table = task_shard.task_table;\n+\n+        auto zookeeper = getZooKeeper();\n+        auto acl = zookeeper->getDefaultACL();\n+\n+        String is_dirty_flag_path = task_partition.getCommonPartitionIsDirtyPath();\n+        String current_task_is_active_path = task_partition.getActiveWorkerPath();\n+        String current_task_status_path = task_partition.getShardStatusPath();\n+\n+        /// Auxiliary functions:\n+\n+        /// Creates is_dirty node to initialize DROP PARTITION\n+        auto create_is_dirty_node = [&] ()\n+        {\n+            auto code = zookeeper->tryCreate(is_dirty_flag_path, current_task_status_path, zkutil::CreateMode::Persistent);\n+            if (code != ZOK && code != ZNODEEXISTS)\n+                throw zkutil::KeeperException(code, is_dirty_flag_path);\n+        };\n+\n+        /// Returns SELECT query filtering current partition and applying user filter\n+        auto get_select_query = [&] (const DatabaseAndTableName & from_table, const String & fields, String limit = \"\")\n+        {\n+            String query;\n+            query += \"SELECT \" + fields + \" FROM \" + getDatabaseDotTable(from_table);\n+            query += \" WHERE (_part LIKE '\" + task_partition.name + \"%')\";\n+            if (!task_table.where_condition_str.empty())\n+                query += \" AND (\" + task_table.where_condition_str + \")\";\n+            if (!limit.empty())\n+                query += \" LIMIT \" + limit;\n+\n+            ParserQuery p_query(query.data() + query.size());\n+            return parseQuery(p_query, query);\n+        };\n+\n+\n+        /// Load balancing\n+        auto worker_node_holder = createWorkerNodeAndWaitIfNeed(zookeeper, current_task_status_path);\n+\n+        LOG_DEBUG(log, \"Processing \" << current_task_status_path);\n+\n+        /// Do not start if partition is dirty, try to clean it\n+        if (zookeeper->exists(is_dirty_flag_path))\n+        {\n+            LOG_DEBUG(log, \"Partition \" << task_partition.name << \" is dirty, try to drop it\");\n+\n+            try\n+            {\n+                tryDropPartition(task_partition, zookeeper);\n+            }\n+            catch (...)\n+            {\n+                tryLogCurrentException(log, \"An error occurred while clean partition\");\n+            }\n+\n+            return false;\n+        }\n+\n+        /// Create ephemeral node to mark that we are active and process the partition\n+        zookeeper->createAncestors(current_task_is_active_path);\n+        zkutil::EphemeralNodeHolderPtr partition_task_node_holder;\n+        try\n+        {\n+            partition_task_node_holder = zkutil::EphemeralNodeHolder::create(current_task_is_active_path, *zookeeper, host_id);\n+        }\n+        catch (const zkutil::KeeperException & e)\n+        {\n+            if (e.code == ZNODEEXISTS)\n+            {\n+                LOG_DEBUG(log, \"Someone is already processing \" << current_task_is_active_path);\n+                return false;\n+            }\n+\n+            throw;\n+        }\n+\n+        /// Exit if task has been already processed, create blocking node if it is abandoned\n+        {\n+            String status_data;\n+            if (zookeeper->tryGet(current_task_status_path, status_data))\n+            {\n+                TaskStateWithOwner status = TaskStateWithOwner::fromString(status_data);\n+                if (status.state == TaskState::Finished)\n+                {\n+                    LOG_DEBUG(log, \"Task \" << current_task_status_path << \" has been successfully executed by \" << status.owner);\n+                    return true;\n+                }\n+\n+                // Task is abandoned, initialize DROP PARTITION\n+                LOG_DEBUG(log, \"Task \" << current_task_status_path << \" has not been successfully finished by \" << status.owner);\n+\n+                create_is_dirty_node();\n+                return false;\n+            }\n+        }\n+\n+        zookeeper->createAncestors(current_task_status_path);\n+\n+        /// We need to update table definitions for each part, it could be changed after ALTER\n+        ASTPtr create_query_pull_ast;\n+        {\n+            /// Fetch and parse (possibly) new definition\n+            auto connection_entry = task_shard.info.pool->get(&task_cluster->settings_pull);\n+            String create_query_pull_str = getRemoteCreateTable(task_table.table_pull, *connection_entry, &task_cluster->settings_pull);\n+\n+            ParserCreateQuery parser_create_query;\n+            create_query_pull_ast = parseQuery(parser_create_query, create_query_pull_str);\n+        }\n+\n+        /// Create local Distributed tables:\n+        ///  a table fetching data from current shard and a table inserting data to the whole destination cluster\n+        DatabaseAndTableName table_shard(working_database_name, \".read_shard.\" + task_table.name_in_config);\n+        DatabaseAndTableName table_split(working_database_name, \".split.\" + task_table.name_in_config);\n+        {\n+            /// Create special cluster with single shard\n+            String shard_read_cluster_name = \".read_shard.\" + task_table.cluster_pull_name;\n+            ClusterPtr cluster_pull_current_shard = task_table.cluster_pull->getClusterWithSingleShard(task_shard.indexInCluster());\n+            context.setCluster(shard_read_cluster_name, cluster_pull_current_shard);\n+\n+            auto storage_shard_ast = createASTStorageDistributed(shard_read_cluster_name, task_table.table_pull.first, task_table.table_pull.second);\n+            const auto & storage_split_ast = task_table.engine_split_ast;\n+\n+            auto create_table_pull_ast = rewriteCreateQueryStorage(create_query_pull_ast, table_shard, storage_shard_ast);\n+            auto create_table_split_ast = rewriteCreateQueryStorage(create_query_pull_ast, table_split, storage_split_ast);\n+\n+            //LOG_DEBUG(log, \"Create shard reading table. Query: \" << queryToString(create_table_pull_ast));\n+            dropAndCreateLocalTable(create_table_pull_ast);\n+\n+            //LOG_DEBUG(log, \"Create split table. Query: \" << queryToString(create_table_split_ast));\n+            dropAndCreateLocalTable(create_table_split_ast);\n+        }\n+\n+        /// Check that destination partition is empty if we are first worker\n+        /// NOTE: this check is incorrect if pull and push tables have different partition key!\n+        {\n+            ASTPtr query_select_ast = get_select_query(table_split, \"count()\");\n+            UInt64 count;\n+            {\n+                Context local_context = context;\n+                // Use pull (i.e. readonly) settings, but fetch data from destination servers\n+                local_context.getSettingsRef() = task_cluster->settings_pull;\n+                local_context.getSettingsRef().skip_unavailable_shards = true;\n+\n+                InterpreterSelectQuery interperter(query_select_ast, local_context);\n+                BlockIO io = interperter.execute();\n+\n+                Block block = getBlockWithAllStreamData(io.in);\n+                count = (block) ? block.safeGetByPosition(0).column->getUInt(0) : 0;\n+            }\n+\n+            if (count != 0)\n+            {\n+                zkutil::Stat stat_shards;\n+                zookeeper->get(task_partition.getPartitionShardsPath(), &stat_shards);\n+\n+                if (stat_shards.numChildren == 0)\n+                {\n+                    LOG_WARNING(log, \"There are no any workers for partition \" << task_partition.name\n+                                     << \", but destination table contains \" << count << \" rows\"\n+                                     << \". Partition will be dropped and refilled.\");\n+\n+                    create_is_dirty_node();\n+                    return false;\n+                }\n+            }\n+        }\n+\n+        /// Try start processing, create node about it\n+        {\n+            String start_state = TaskStateWithOwner::getData(TaskState::Started, host_id);\n+            auto op_create = std::make_unique<zkutil::Op::Create>(current_task_status_path, start_state, acl, zkutil::CreateMode::Persistent);\n+\n+            zkutil::MultiTransactionInfo info;\n+            checkNoNodeAndCommit(zookeeper, is_dirty_flag_path, std::move(op_create), info);\n+\n+            if (info.code != ZOK)\n+            {\n+                if (info.getFailedOp().getPath() == is_dirty_flag_path)\n+                {\n+                    LOG_INFO(log, \"Partition \" << task_partition.name << \" is dirty and will be dropped and refilled\");\n+                    return false;\n+                }\n+\n+                throw zkutil::KeeperException(info.code, current_task_status_path);\n+            }\n+        }\n+\n+        /// Try create table (if not exists) on each shard\n+        {\n+            auto create_query_push_ast = rewriteCreateQueryStorage(create_query_pull_ast, task_table.table_push, task_table.engine_push_ast);\n+            typeid_cast<ASTCreateQuery &>(*create_query_push_ast).if_not_exists = true;\n+            String query = queryToString(create_query_push_ast);\n+\n+            LOG_DEBUG(log, \"Create remote push tables. Query: \" << query);\n+            executeQueryOnCluster(task_table.cluster_push, query, create_query_push_ast, &task_cluster->settings_push);\n+        }\n+\n+        /// Do the copying\n+        {\n+            bool inject_fault = false;\n+            if (copy_fault_probability > 0)\n+            {\n+                std::uniform_real_distribution<> get_urand(0, 1);\n+                double value = get_urand(task_table.task_cluster.random_engine);\n+                inject_fault = value < copy_fault_probability;\n+            }\n+\n+            // Select all fields\n+            ASTPtr query_select_ast = get_select_query(table_shard, \"*\", inject_fault ? \"1\" : \"\");\n+\n+            LOG_DEBUG(log, \"Executing SELECT query: \" << queryToString(query_select_ast));\n+\n+            ASTPtr query_insert_ast;\n+            {\n+                String query;\n+                query += \"INSERT INTO \" + getDatabaseDotTable(table_split) + \" VALUES \";\n+\n+                ParserQuery p_query(query.data() + query.size());\n+                query_insert_ast = parseQuery(p_query, query);\n+\n+                LOG_DEBUG(log, \"Executing INSERT query: \" << query);\n+            }\n+\n+            try\n+            {\n+                /// Custom INSERT SELECT implementation\n+                Context context_select = context;\n+                context_select.getSettingsRef() = task_cluster->settings_pull;\n+\n+                Context context_insert = context;\n+                context_insert.getSettingsRef() = task_cluster->settings_push;\n+\n+                InterpreterSelectQuery interpreter_select(query_select_ast, context_select);\n+                BlockIO io_select = interpreter_select.execute();\n+\n+                InterpreterInsertQuery interpreter_insert(query_insert_ast, context_insert);\n+                BlockIO io_insert = interpreter_insert.execute();\n+\n+                using ExistsFuture = zkutil::ZooKeeper::ExistsFuture;\n+                auto future_is_dirty_checker = std::make_unique<ExistsFuture>(zookeeper->asyncExists(is_dirty_flag_path));\n+\n+                Stopwatch watch(CLOCK_MONOTONIC_COARSE);\n+                constexpr size_t check_period_milliseconds = 500;\n+\n+                /// Will asynchronously check that ZooKeeper connection and is_dirty flag appearing while copy data\n+                auto cancel_check = [&] ()\n+                {\n+                    if (zookeeper->expired())\n+                        throw Exception(\"ZooKeeper session is expired, cancel INSERT SELECT\", ErrorCodes::UNFINISHED);\n+\n+                    if (future_is_dirty_checker != nullptr)\n+                    {\n+                        zkutil::ZooKeeper::StatAndExists status;\n+                        try\n+                        {\n+                            status = future_is_dirty_checker->get();\n+                            future_is_dirty_checker.reset();\n+                        }\n+                        catch (zkutil::KeeperException & e)\n+                        {\n+                            future_is_dirty_checker.reset();\n+\n+                            if (e.isTemporaryError())\n+                                LOG_INFO(log, \"ZooKeeper is lagging: \" << e.displayText());\n+                            else\n+                                throw;\n+                        }\n+\n+                        if (status.exists)\n+                            throw Exception(\"Partition is dirty, cancel INSERT SELECT\", ErrorCodes::UNFINISHED);\n+                    }\n+\n+                    if (watch.elapsedMilliseconds() >= check_period_milliseconds)\n+                    {\n+                        watch.restart();\n+                        future_is_dirty_checker = std::make_unique<ExistsFuture>(zookeeper->asyncExists(is_dirty_flag_path));\n+                    }\n+\n+                    return false;\n+                };\n+\n+                /// Main work is here\n+                copyData(*io_select.in, *io_insert.out, cancel_check);\n+\n+                // Just in case\n+                if (future_is_dirty_checker != nullptr)\n+                    future_is_dirty_checker.get();\n+\n+                if (inject_fault)\n+                    throw Exception(\"Copy fault injection is activated\", ErrorCodes::UNFINISHED);\n+            }\n+            catch (...)\n+            {\n+                tryLogCurrentException(log, \"An error occurred during copying, partition will be marked as dirty\");\n+                return false;\n+            }\n+        }\n+\n+        /// Finalize the processing, change state of current partition task (and also check is_dirty flag)\n+        {\n+            String state_finished = TaskStateWithOwner::getData(TaskState::Finished, host_id);\n+            auto op_set = std::make_unique<zkutil::Op::SetData>(current_task_status_path, state_finished, 0);\n+            zkutil::MultiTransactionInfo info;\n+            checkNoNodeAndCommit(zookeeper, is_dirty_flag_path, std::move(op_set), info);\n+\n+            if (info.code != ZOK)\n+            {\n+                if (info.getFailedOp().getPath() == is_dirty_flag_path)\n+                    LOG_INFO(log, \"Partition \" << task_partition.name << \" became dirty and will be dropped and refilled\");\n+                else\n+                    LOG_INFO(log, \"Someone made the node abandoned. Will refill partition. \" << zkutil::ZooKeeper::error2string(info.code));\n+\n+                return false;\n+            }\n+        }\n+\n+        LOG_INFO(log, \"Partition \" << task_partition.name << \" copied\");\n+        return true;\n+    }\n+\n+    void dropAndCreateLocalTable(const ASTPtr & create_ast)\n+    {\n+        auto & create = typeid_cast<ASTCreateQuery &>(*create_ast);\n+        dropLocalTableIfExists({create.database, create.table});\n+\n+        InterpreterCreateQuery interpreter(create_ast, context);\n+        interpreter.execute();\n+    }\n+\n+    void dropLocalTableIfExists(const DatabaseAndTableName & table_name) const\n+    {\n+        auto drop_ast = std::make_shared<ASTDropQuery>();\n+        drop_ast->if_exists = true;\n+        drop_ast->database = table_name.first;\n+        drop_ast->table = table_name.second;\n+\n+        InterpreterDropQuery interpreter(drop_ast, context);\n+        interpreter.execute();\n+    }\n+\n+    bool existsRemoteTable(const DatabaseAndTableName & table, Connection & connection)\n+    {\n+        String query = \"EXISTS \" + getDatabaseDotTable(table);\n+        Block block = getBlockWithAllStreamData(std::make_shared<RemoteBlockInputStream>(connection, query, context));\n+        return block.safeGetByPosition(0).column->getUInt(0) != 0;\n+    }\n+\n+    String getRemoteCreateTable(const DatabaseAndTableName & table, Connection & connection, const Settings * settings = nullptr)\n+    {\n+        String query = \"SHOW CREATE TABLE \" + getDatabaseDotTable(table);\n+        Block block = getBlockWithAllStreamData(std::make_shared<RemoteBlockInputStream>(connection, query, context, settings));\n+\n+        return typeid_cast<const ColumnString &>(*block.safeGetByPosition(0).column).getDataAt(0).toString();\n+    }\n+\n+    Strings getRemotePartitions(const DatabaseAndTableName & table, Connection & connection, const Settings * settings = nullptr)\n+    {\n+        Block block;\n+        {\n+            WriteBufferFromOwnString wb;\n+            wb << \"SELECT DISTINCT partition FROM system.parts WHERE\"\n+               << \" database = \" << DB::quote << table.first\n+               << \" AND table = \" << DB::quote << table.second;\n+\n+            block = getBlockWithAllStreamData(std::make_shared<RemoteBlockInputStream>(connection, wb.str(), context, settings));\n+        }\n+\n+        Strings res;\n+        if (block)\n+        {\n+            auto & partition_col = typeid_cast<const ColumnString &>(*block.getByName(\"partition\").column);\n+            for (size_t i = 0; i < partition_col.size(); ++i)\n+                res.push_back(partition_col.getDataAt(i).toString());\n+        }\n+        else\n+        {\n+            if (!existsRemoteTable(table, connection))\n+            {\n+                throw Exception(\"Table \" + getDatabaseDotTable(table) + \" is not exists on server \"\n+                                + connection.getDescription(), ErrorCodes::UNKNOWN_TABLE);\n+            }\n+        }\n+\n+        return res;\n+    }\n+\n+    /** Executes simple query (without output streams, for example DDL queries) on each shard of the cluster\n+     * Returns number of shards for which at least one replica executed query successfully\n+     */\n+    size_t executeQueryOnCluster(\n+        const ClusterPtr & cluster,\n+        const String & query,\n+        const ASTPtr & query_ast_ = nullptr,\n+        const Settings * settings = nullptr,\n+        PoolMode pool_mode = PoolMode::GET_ALL,\n+        size_t max_successful_executions_per_shard = 0) const\n+    {\n+        auto num_shards = cluster->getShardsInfo().size();\n+        std::vector<size_t> per_shard_num_successful_replicas(num_shards, 0);\n+\n+        ASTPtr query_ast;\n+        if (query_ast_ == nullptr)\n+        {\n+            ParserQuery p_query(query.data() + query.size());\n+            query_ast = parseQuery(p_query, query);\n+        }\n+        else\n+            query_ast = query_ast_;\n+\n+\n+        /// We need to execute query on one replica at least\n+        auto do_for_shard = [&] (size_t shard_index)\n+        {\n+            const Cluster::ShardInfo & shard = cluster->getShardsInfo().at(shard_index);\n+            size_t & num_successful_executions = per_shard_num_successful_replicas.at(shard_index);\n+            num_successful_executions = 0;\n+\n+            auto increment_and_check_exit = [&] ()\n+            {\n+                ++num_successful_executions;\n+                return max_successful_executions_per_shard && num_successful_executions >= max_successful_executions_per_shard;\n+            };\n+\n+            /// In that case we don't have local replicas, but do it just in case\n+            for (size_t i = 0; i < shard.getLocalNodeCount(); ++i)\n+            {\n+                auto interpreter = InterpreterFactory::get(query_ast, context);\n+                interpreter->execute();\n+\n+                if (increment_and_check_exit())\n+                    return;\n+            }\n+\n+            /// Will try to make as many as possible queries\n+            if (shard.hasRemoteConnections())\n+            {\n+                std::vector<IConnectionPool::Entry> connections = shard.pool->getMany(settings, pool_mode);\n+\n+                for (auto & connection : connections)\n+                {\n+                    if (!connection.isNull())\n+                    {\n+                        try\n+                        {\n+                            RemoteBlockInputStream stream(*connection, query, context, settings);\n+                            NullBlockOutputStream output;\n+                            copyData(stream, output);\n+\n+                            if (increment_and_check_exit())\n+                                return;\n+                        }\n+                        catch (const Exception & e)\n+                        {\n+                            LOG_INFO(log, getCurrentExceptionMessage(false, true));\n+                        }\n+                    }\n+                }\n+            }\n+        };\n+\n+        {\n+            ThreadPool thread_pool(std::min(num_shards, getNumberOfPhysicalCPUCores()));\n+\n+            for (size_t shard_index = 0; shard_index < num_shards; ++shard_index)\n+                thread_pool.schedule([=] { do_for_shard(shard_index); });\n+\n+            thread_pool.wait();\n+        }\n+\n+        size_t successful_shards = 0;\n+        for (size_t num_replicas : per_shard_num_successful_replicas)\n+            successful_shards += (num_replicas > 0);\n+\n+        return successful_shards;\n+    }\n+\n+    void initZooKeeper()\n+    {\n+        current_zookeeper = std::make_shared<zkutil::ZooKeeper>(*zookeeper_config, \"zookeeper\");\n+    }\n+\n+    const zkutil::ZooKeeperPtr & getZooKeeper()\n+    {\n+        if (!current_zookeeper)\n+            throw Exception(\"Cannot get ZooKeeper\", ErrorCodes::NO_ZOOKEEPER);\n+\n+        return current_zookeeper;\n+    }\n+\n+private:\n+    ConfigurationPtr zookeeper_config;\n+    String task_zookeeper_path;\n+    String host_id;\n+    String working_database_name;\n+\n+    bool is_safe_mode = false;\n+    double copy_fault_probability = 0.0;\n+\n+    ConfigurationPtr task_cluster_config;\n+    std::unique_ptr<TaskCluster> task_cluster;\n+\n+    zkutil::ZooKeeperPtr current_zookeeper;\n+\n+    Context & context;\n+    Poco::Logger * log;\n+\n+    std::chrono::milliseconds default_sleep_time{1000};\n+};\n+\n+\n+/// ClusterCopierApp\n+\n+\n+void ClusterCopierApp::initialize(Poco::Util::Application & self)\n+{\n+    Poco::Util::Application::initialize(self);\n+\n+    is_help = config().has(\"help\");\n+    if (is_help)\n+        return;\n+\n+    config_xml_path = config().getString(\"config-file\");\n+    task_path = config().getString(\"task-path\");\n+    log_level = config().getString(\"log-level\", \"debug\");\n+    is_safe_mode = config().has(\"safe-mode\");\n+    if (config().has(\"copy-fault-probability\"))\n+        copy_fault_probability = std::max(std::min(config().getDouble(\"copy-fault-probability\"), 1.0), 0.0);\n+    base_dir = (config().has(\"base-dir\")) ? config().getString(\"base-dir\") : Poco::Path::current();\n+\n+    // process_id is '<hostname>#<pid>_<start_timestamp>'\n+    process_id = std::to_string(Poco::Process::id()) + \"_\" + std::to_string(Poco::Timestamp().epochTime());\n+    host_id = escapeForFileName(getFQDNOrHostName()) + '#' + process_id;\n+    process_path = Poco::Path(base_dir + \"/clickhouse-copier_\" + process_id).absolute().toString();\n+    Poco::File(process_path).createDirectories();\n+\n+    setupLogging();\n+\n+    std::string stderr_path = process_path + \"/stderr\";\n+    if (!freopen(stderr_path.c_str(), \"a+\", stderr))\n+        throw Poco::OpenFileException(\"Cannot attach stderr to \" + stderr_path);\n+}\n+\n+\n+void ClusterCopierApp::handleHelp(const std::string &, const std::string &)\n+{\n+    Poco::Util::HelpFormatter helpFormatter(options());\n+    helpFormatter.setCommand(commandName());\n+    helpFormatter.setHeader(\"Copies tables from one cluster to another\");\n+    helpFormatter.setUsage(\"--config-file <config-file> --task-path <task-path>\");\n+    helpFormatter.format(std::cerr);\n+\n+    stopOptionsProcessing();\n+}\n+\n+\n+void ClusterCopierApp::defineOptions(Poco::Util::OptionSet & options)\n+{\n+    options.addOption(Poco::Util::Option(\"config-file\", \"c\", \"path to config file with ZooKeeper config\", true)\n+                          .argument(\"config-file\").binding(\"config-file\"));\n+    options.addOption(Poco::Util::Option(\"task-path\", \"\", \"path to task in ZooKeeper\")\n+                          .argument(\"task-path\").binding(\"task-path\"));\n+    options.addOption(Poco::Util::Option(\"safe-mode\", \"\", \"disables ALTER DROP PARTITION in case of errors\")\n+                          .binding(\"safe-mode\"));\n+    options.addOption(Poco::Util::Option(\"copy-fault-probability\", \"\", \"the copying fails with specified probability (used to test partition state recovering)\")\n+                          .argument(\"copy-fault-probability\").binding(\"copy-fault-probability\"));\n+    options.addOption(Poco::Util::Option(\"log-level\", \"\", \"sets log level\")\n+                          .argument(\"log-level\").binding(\"log-level\"));\n+    options.addOption(Poco::Util::Option(\"base-dir\", \"\", \"base directory for copiers, consequitive copier launches will populate /base-dir/launch_id/* directories\")\n+                          .argument(\"base-dir\").binding(\"base-dir\"));\n+\n+    using Me = std::decay_t<decltype(*this)>;\n+    options.addOption(Poco::Util::Option(\"help\", \"\", \"produce this help message\").binding(\"help\")\n+                          .callback(Poco::Util::OptionCallback<Me>(this, &Me::handleHelp)));\n+}\n+\n+\n+void ClusterCopierApp::setupLogging()\n+{\n+    Poco::AutoPtr<Poco::SplitterChannel> split_channel(new Poco::SplitterChannel);\n+\n+    Poco::AutoPtr<Poco::FileChannel> log_file_channel(new Poco::FileChannel);\n+    log_file_channel->setProperty(\"path\", process_path + \"/log.log\");\n+    split_channel->addChannel(log_file_channel);\n+    log_file_channel->open();\n+\n+    if (!config().getBool(\"application.runAsService\", true))\n+    {\n+        Poco::AutoPtr<Poco::ConsoleChannel> console_channel(new Poco::ConsoleChannel);\n+        split_channel->addChannel(console_channel);\n+        console_channel->open();\n+    }\n+\n+    Poco::AutoPtr<Poco::PatternFormatter> formatter(new Poco::PatternFormatter);\n+    formatter->setProperty(\"pattern\", \"%L%Y-%m-%d %H:%M:%S.%i <%p> %s: %t\");\n+    Poco::AutoPtr<Poco::FormattingChannel> formatting_channel(new Poco::FormattingChannel(formatter));\n+    formatting_channel->setChannel(split_channel);\n+    split_channel->open();\n+\n+    Poco::Logger::root().setChannel(formatting_channel);\n+    Poco::Logger::root().setLevel(log_level);\n+}\n+\n+\n+void ClusterCopierApp::mainImpl()\n+{\n+    ConfigurationPtr zookeeper_configuration(new Poco::Util::XMLConfiguration(config_xml_path));\n+    auto log = &logger();\n+\n+    StatusFile status_file(process_path + \"/status\");\n+\n+    LOG_INFO(log, \"Starting clickhouse-copier (\"\n+        << \"id \" << process_id << \", \"\n+        << \"host_id \" << host_id << \", \"\n+        << \"path \" << process_path << \", \"\n+        << \"revision \" << ClickHouseRevision::get() << \")\");\n+\n+    auto context = std::make_unique<Context>(Context::createGlobal());\n+    SCOPE_EXIT(context->shutdown());\n+\n+    context->setGlobalContext(*context);\n+    context->setApplicationType(Context::ApplicationType::LOCAL);\n+    context->setPath(process_path);\n+\n+    registerFunctions();\n+    registerAggregateFunctions();\n+    registerTableFunctions();\n+    registerStorages();\n+\n+    static const std::string default_database = \"_local\";\n+    context->addDatabase(default_database, std::make_shared<DatabaseMemory>(default_database));\n+    context->setCurrentDatabase(default_database);\n+\n+    std::unique_ptr<ClusterCopier> copier(new ClusterCopier(\n+        zookeeper_configuration, task_path, host_id, default_database, *context));\n+\n+    copier->setSafeMode(is_safe_mode);\n+    copier->setCopyFaultProbability(copy_fault_probability);\n+    copier->init();\n+    copier->process();\n+}\n+\n+\n+int ClusterCopierApp::main(const std::vector<std::string> &)\n+{\n+    if (is_help)\n+        return 0;\n+\n+    try\n+    {\n+        mainImpl();\n+    }\n+    catch (...)\n+    {\n+        std::cerr << DB::getCurrentExceptionMessage(true) << \"\\n\";\n+        auto code = getCurrentExceptionCode();\n+\n+        return (code) ? code : -1;\n+    }\n+\n+    return 0;\n+}\n+\n+\n+}\n+\n+\n+int mainEntryClickHouseClusterCopier(int argc, char ** argv)\n+{\n+    try\n+    {\n+        DB::ClusterCopierApp app;\n+        return app.run(argc, argv);\n+    }\n+    catch (...)\n+    {\n+        std::cerr << DB::getCurrentExceptionMessage(true) << \"\\n\";\n+        auto code = DB::getCurrentExceptionCode();\n+\n+        return (code) ? code : -1;\n+    }\n+}\ndiff --git a/dbms/src/Server/ClusterCopier.h b/dbms/src/Server/ClusterCopier.h\nnew file mode 100644\nindex 000000000000..d0fb82ed04be\n--- /dev/null\n+++ b/dbms/src/Server/ClusterCopier.h\n@@ -0,0 +1,205 @@\n+#pragma once\n+#include <Poco/Util/ServerApplication.h>\n+\n+/* = clickhouse-cluster-copier util =\n+ * Copies tables data from one cluster to new tables of other (possibly the same) cluster in distributed fault-tolerant manner.\n+ *\n+ * Configuration of copying tasks is set in special ZooKeeper node (called the description node).\n+ * A ZooKeeper path to the description node is specified via --task-path </task/path> parameter.\n+ * So, node /task/path/description should contain special XML content describing copying tasks.\n+ *\n+ * Simultaneously many clickhouse-cluster-copier processes located on any servers could execute the same task.\n+ * ZooKeeper node /task/path/ is used by the processes to coordinate their work.\n+ * You must not add additional child nodes to /task/path/.\n+ *\n+ * Currently you are responsible for launching cluster-copier processes.\n+ * You can launch as many processes as you want, whenever and wherever you want.\n+ * Each process try to select nearest available shard of source cluster and copy some part of data (partition) from it to the whole\n+ * destination cluster with resharding.\n+ * Therefore it makes sense to launch cluster-copier processes on the source cluster nodes to reduce the network usage.\n+ *\n+ * Since the workers coordinate their work via ZooKeeper, in addition to --task-path </task/path> you have to specify ZooKeeper\n+ * configuration via --config-file <zookeeper.xml> parameter. Example of zookeeper.xml:\n+\n+   <yandex>\n+    <zookeeper>\n+        <node index=\"1\">\n+            <host>127.0.0.1</host>\n+            <port>2181</port>\n+        </node>\n+    </zookeeper>\n+   </yandex>\n+\n+ * When you run clickhouse-cluster-copier --config-file <zookeeper.xml> --task-path </task/path>\n+ * the process connects to ZooKeeper, reads tasks config from /task/path/description and executes them.\n+ *\n+ *\n+ * = Format of task config =\n+\n+<yandex>\n+    <!-- Configuration of clusters as in an ordinary server config -->\n+    <remote_servers>\n+        <source_cluster>\n+            <shard>\n+                <internal_replication>false</internal_replication>\n+                    <replica>\n+                        <host>127.0.0.1</host>\n+                        <port>9000</port>\n+                    </replica>\n+            </shard>\n+            ...\n+        </source_cluster>\n+\n+        <destination_cluster>\n+        ...\n+        </destination_cluster>\n+    </remote_servers>\n+\n+    <!-- How many simultaneously active workers are possible. If you run more workers superfluous workers will sleep. -->\n+    <max_workers>2</max_workers>\n+\n+    <!-- Setting used to fetch (pull) data from source cluster tables -->\n+    <settings_pull>\n+        <readonly>1</readonly>\n+    </settings_pull>\n+\n+    <!-- Setting used to insert (push) data to destination cluster tables -->\n+    <settings_push>\n+        <readonly>0</readonly>\n+    </settings_push>\n+\n+    <!-- Common setting for fetch (pull) and insert (push) operations.\n+         They are overlaid by <settings_pull/> and <settings_push/> respectively -->\n+    <settings>\n+        <insert_distributed_sync>1</insert_distributed_sync>\n+    </settings>\n+\n+    <!-- Copying tasks description.\n+         You could specify several table task in the same task description (in the same ZooKeeper node), they will be performed\n+         sequentially.\n+    -->\n+    <tables>\n+        <!-- Name of the table task, it must be an unique name suitable for ZooKeeper node name -->\n+        <table_hits>\n+            <-- Source cluster name (from <remote_servers/> section) and tables in it that should be copied -->\n+            <cluster_pull>source_cluster</cluster_pull>\n+            <database_pull>test</database_pull>\n+            <table_pull>hits</table_pull>\n+\n+            <-- Destination cluster name and tables in which the data should be inserted -->\n+            <cluster_push>destination_cluster</cluster_push>\n+            <database_push>test</database_push>\n+            <table_push>hits2</table_push>\n+\n+            <!-- Engine of destination tables.\n+                 If destination tables have not be created, workers create them using columns definition from source tables and engine\n+                 definition from here.\n+\n+                 NOTE: If the first worker starts insert data and detects that destination partition is not empty then the partition will\n+                 be dropped and refilled, take it into account if you already have some data in destination tables. You could directly\n+                 specify partitions that should be copied in <enabled_partitions/>.\n+\n+                 NOTE: Currently partition key of source and destination tables should be the same.\n+            -->\n+            <engine>ENGINE = ReplicatedMergeTree('/clickhouse/tables/test/hits2/{shard}/hits2', '{replica}', EventDate, (CounterID, EventDate), 8192)</engine>\n+\n+            <!-- Sharding key used to insert data to destination cluster -->\n+            <sharding_key>intHash32(UserID)</sharding_key>\n+\n+            <!-- Optional expression that filter data while pull them from source servers -->\n+            <where_condition>CounterID != 0</where_condition>\n+\n+            <!-- Optional section, it specifies partitions that should be copied, other partition will be ignored -->\n+            <enabled_partitions>\n+                <partition>201712</partition>\n+                <partition>201801</partition>\n+                ...\n+            </enabled_partitions>\n+        </table_hits>\n+\n+        </table_visits>\n+        ...\n+        </table_visits>\n+        ...\n+    </tables>\n+</yandex>\n+\n+\n+ * = Implementation details =\n+ *\n+ * cluster-copier workers pull each partition of each shard of the source cluster and push it to the destination cluster through\n+ * Distributed table (to preform data resharding). So, worker job is a partition of a source shard.\n+ * A job has three states: Active, Finished and Abandoned. Abandoned means that worker died and did not finish the job.\n+ *\n+ * If an error occurred during the copying (a worker failed or a worker did not finish the INSERT), then the whole partition (on\n+ * all destination servers) should be dropped and refilled. So, copying entity is a partition of all destination shards.\n+ * If a failure is detected a special /is_dirty node is created in ZooKeeper signalling that other workers copying the same partition\n+ * should stop, after a refilling procedure should start.\n+ *\n+ * ZooKeeper task node has the following structure:\n+ *  /task/path_root                     - path passed in --task-path parameter\n+ *      /description                    - contains user-defined XML config of the task\n+ *      /task_active_workers            - contains ephemeral nodes of all currently active workers, used to implement max_workers limitation\n+ *          /server_fqdn#PID_timestamp  - cluster-copier worker ID\n+ *          ...\n+ *      /tables             - directory with table tasks\n+ *      /table_hits         - directory of table_hits task\n+ *          /partition1     - directory for partition1\n+ *              /shards     - directory for source cluster shards\n+ *                  /1      - worker job for the first shard of partition1 of table test.hits\n+ *                            Contains info about current status (Active or Finished) and worker ID.\n+ *                  /2\n+ *                  ...\n+ *              /partition_active_workers\n+ *                  /1      - for each job in /shards a corresponding ephemeral node created in /partition_active_workers\n+ *                            It is used to detect Abandoned jobs (if there is Active node in /shards and there is no node in\n+ *                            /partition_active_workers).\n+ *                            Also, it is used to track active workers in the partition (when we need to refill the partition we do\n+ *                            not DROP PARTITION while there are active workers)\n+ *                  /2\n+ *                  ...\n+ *              /is_dirty   - the node is set if some worker detected that an error occurred (the INSERT is failed or an Abandoned node is\n+ *                            detected). If the node appeared workers in this partition should stop and start cleaning and refilling\n+ *                            partition procedure.\n+ *                            During this procedure a single 'cleaner' worker is selected. The worker waits for stopping all partition\n+ *                            workers, removes /shards node, executes DROP PARTITION on each destination node and removes /is_dirty node.\n+ *                  /cleaner- An ephemeral node used to select 'cleaner' worker. Contains ID of the worker.\n+ *      /test_visits\n+ *          ...\n+ */\n+\n+namespace DB\n+{\n+\n+class ClusterCopierApp : public Poco::Util::ServerApplication\n+{\n+public:\n+\n+    void initialize(Poco::Util::Application & self) override;\n+\n+    void handleHelp(const std::string &, const std::string &);\n+\n+    void defineOptions(Poco::Util::OptionSet & options) override;\n+\n+    int main(const std::vector<std::string> &) override;\n+\n+private:\n+\n+    void mainImpl();\n+\n+    void setupLogging();\n+\n+    std::string config_xml_path;\n+    std::string task_path;\n+    std::string log_level = \"debug\";\n+    bool is_safe_mode = false;\n+    double copy_fault_probability = 0;\n+    bool is_help = false;\n+\n+    std::string base_dir;\n+    std::string process_path;\n+    std::string process_id;\n+    std::string host_id;\n+};\n+\n+}\ndiff --git a/dbms/src/Server/LocalServer.cpp b/dbms/src/Server/LocalServer.cpp\nindex ff8649b3753c..e3dc7c35f206 100644\n--- a/dbms/src/Server/LocalServer.cpp\n+++ b/dbms/src/Server/LocalServer.cpp\n@@ -463,6 +463,14 @@ static const char * minimal_default_user_xml =\n \"</yandex>\";\n \n \n+static ConfigurationPtr getConfigurationFromXMLString(const char * xml_data)\n+{\n+    std::stringstream ss{std::string{xml_data}};\n+    Poco::XML::InputSource input_source{ss};\n+    return {new Poco::Util::XMLConfiguration{&input_source}};\n+}\n+\n+\n void LocalServer::setupUsers()\n {\n     ConfigurationPtr users_config;\n@@ -477,11 +485,7 @@ void LocalServer::setupUsers()\n     }\n     else\n     {\n-        std::stringstream default_user_stream;\n-        default_user_stream << minimal_default_user_xml;\n-\n-        Poco::XML::InputSource default_user_source(default_user_stream);\n-        users_config = ConfigurationPtr(new Poco::Util::XMLConfiguration(&default_user_source));\n+        users_config = getConfigurationFromXMLString(minimal_default_user_xml);\n     }\n \n     if (users_config)\ndiff --git a/dbms/src/Server/clickhouse-cluster-copier.cpp b/dbms/src/Server/clickhouse-cluster-copier.cpp\nnew file mode 100644\nindex 000000000000..653a0128aa4e\n--- /dev/null\n+++ b/dbms/src/Server/clickhouse-cluster-copier.cpp\n@@ -0,0 +1,2 @@\n+int mainEntryClickHouseClusterCopier(int argc, char ** argv);\n+int main(int argc_, char ** argv_) { return mainEntryClickHouseClusterCopier(argc_, argv_); }\ndiff --git a/dbms/src/Server/main.cpp b/dbms/src/Server/main.cpp\nindex f923c75d87e6..d84c0ab732d6 100644\n--- a/dbms/src/Server/main.cpp\n+++ b/dbms/src/Server/main.cpp\n@@ -18,6 +18,7 @@ int mainEntryClickHousePerformanceTest(int argc, char ** argv);\n int mainEntryClickHouseExtractFromConfig(int argc, char ** argv);\n int mainEntryClickHouseCompressor(int argc, char ** argv);\n int mainEntryClickHouseFormat(int argc, char ** argv);\n+int mainEntryClickHouseClusterCopier(int argc, char ** argv);\n \n #if USE_EMBEDDED_COMPILER\n     int mainEntryClickHouseClang(int argc, char ** argv);\n@@ -41,6 +42,7 @@ std::pair<const char *, MainFunc> clickhouse_applications[] =\n     {\"extract-from-config\", mainEntryClickHouseExtractFromConfig},\n     {\"compressor\", mainEntryClickHouseCompressor},\n     {\"format\", mainEntryClickHouseFormat},\n+    {\"copier\", mainEntryClickHouseClusterCopier},\n #if USE_EMBEDDED_COMPILER\n     {\"clang\", mainEntryClickHouseClang},\n     {\"lld\", mainEntryClickHouseLLD},\ndiff --git a/dbms/src/Storages/Distributed/DistributedBlockOutputStream.cpp b/dbms/src/Storages/Distributed/DistributedBlockOutputStream.cpp\nindex 156e016fac3e..2d2a5c454341 100644\n--- a/dbms/src/Storages/Distributed/DistributedBlockOutputStream.cpp\n+++ b/dbms/src/Storages/Distributed/DistributedBlockOutputStream.cpp\n@@ -53,9 +53,10 @@ namespace ErrorCodes\n }\n \n \n-DistributedBlockOutputStream::DistributedBlockOutputStream(StorageDistributed & storage, const ASTPtr & query_ast,\n-                                                           const ClusterPtr & cluster_, bool insert_sync_, UInt64 insert_timeout_)\n-    : storage(storage), query_ast(query_ast), cluster(cluster_), insert_sync(insert_sync_), insert_timeout(insert_timeout_)\n+DistributedBlockOutputStream::DistributedBlockOutputStream(StorageDistributed & storage, const ASTPtr & query_ast, const ClusterPtr & cluster_,\n+                                                           const Settings & settings_, bool insert_sync_, UInt64 insert_timeout_)\n+    : storage(storage), query_ast(query_ast), cluster(cluster_), settings(settings_), insert_sync(insert_sync_),\n+      insert_timeout(insert_timeout_)\n {\n }\n \n@@ -392,9 +393,21 @@ void DistributedBlockOutputStream::writeAsyncImpl(const Block & block, const siz\n \n void DistributedBlockOutputStream::writeToLocal(const Block & block, const size_t repeats)\n {\n-    InterpreterInsertQuery interp{query_ast, storage.context};\n+    std::unique_ptr<Context> local_context;\n+    std::optional<InterpreterInsertQuery> interp;\n \n-    auto block_io = interp.execute();\n+    /// Async insert does not support settings forwarding yet whereas sync one supports\n+    if (insert_sync)\n+        interp.emplace(query_ast, storage.context);\n+    else\n+    {\n+        /// Overwrite global settings by user settings\n+        local_context = std::make_unique<Context>(storage.context);\n+        local_context->setSettings(settings);\n+        interp.emplace(query_ast, *local_context);\n+    }\n+\n+    auto block_io = interp->execute();\n     block_io.out->writePrefix();\n \n     for (size_t i = 0; i < repeats; ++i)\n@@ -410,7 +423,7 @@ void DistributedBlockOutputStream::writeToShardSync(const Block & block, const s\n     auto connection = pool->get();\n \n     const auto & query_string = queryToString(query_ast);\n-    RemoteBlockOutputStream remote{*connection, query_string};\n+    RemoteBlockOutputStream remote{*connection, query_string, &settings};\n \n     CurrentMetrics::Increment metric_increment{CurrentMetrics::DistributedSend};\n \ndiff --git a/dbms/src/Storages/Distributed/DistributedBlockOutputStream.h b/dbms/src/Storages/Distributed/DistributedBlockOutputStream.h\nindex 526ccb92c752..de8dc12649b7 100644\n--- a/dbms/src/Storages/Distributed/DistributedBlockOutputStream.h\n+++ b/dbms/src/Storages/Distributed/DistributedBlockOutputStream.h\n@@ -32,7 +32,8 @@ class StorageDistributed;\n class DistributedBlockOutputStream : public IBlockOutputStream\n {\n public:\n-    DistributedBlockOutputStream(StorageDistributed & storage, const ASTPtr & query_ast, const ClusterPtr & cluster_, bool insert_sync_, UInt64 insert_timeout_);\n+    DistributedBlockOutputStream(StorageDistributed & storage, const ASTPtr & query_ast, const ClusterPtr & cluster_,\n+                                 const Settings & settings_, bool insert_sync_, UInt64 insert_timeout_);\n \n     void write(const Block & block) override;\n \n@@ -88,6 +89,7 @@ class DistributedBlockOutputStream : public IBlockOutputStream\n     StorageDistributed & storage;\n     ASTPtr query_ast;\n     ClusterPtr cluster;\n+    const Settings & settings;\n     bool insert_sync;\n     UInt64 insert_timeout;\n     size_t blocks_inserted = 0;\ndiff --git a/dbms/src/Storages/ITableDeclaration.cpp b/dbms/src/Storages/ITableDeclaration.cpp\nindex b9948c89bfe4..8bc53a8163cf 100644\n--- a/dbms/src/Storages/ITableDeclaration.cpp\n+++ b/dbms/src/Storages/ITableDeclaration.cpp\n@@ -21,6 +21,7 @@ namespace ErrorCodes\n     extern const int TYPE_MISMATCH;\n     extern const int DUPLICATE_COLUMN;\n     extern const int NOT_FOUND_COLUMN_IN_BLOCK;\n+    extern const int EMPTY_LIST_OF_COLUMNS_PASSED;\n }\n \n \n@@ -289,4 +290,15 @@ void ITableDeclaration::check(const Block & block, bool need_all) const\n     }\n }\n \n+ITableDeclaration::ITableDeclaration(const NamesAndTypesList & columns, const NamesAndTypesList & materialized_columns,\n+                                     const NamesAndTypesList & alias_columns, const ColumnDefaults & column_defaults)\n+    : columns{columns},\n+      materialized_columns{materialized_columns},\n+      alias_columns{alias_columns},\n+      column_defaults{column_defaults}\n+{\n+    if (columns.empty())\n+        throw Exception(\"Empty list of columns passed to storage constructor\", ErrorCodes::EMPTY_LIST_OF_COLUMNS_PASSED);\n+}\n+\n }\ndiff --git a/dbms/src/Storages/ITableDeclaration.h b/dbms/src/Storages/ITableDeclaration.h\nindex 4861a36205f4..19c57d45e40e 100644\n--- a/dbms/src/Storages/ITableDeclaration.h\n+++ b/dbms/src/Storages/ITableDeclaration.h\n@@ -84,20 +84,21 @@ class ITableDeclaration\n \n     ITableDeclaration() = default;\n     ITableDeclaration(\n+        const NamesAndTypesList & columns,\n         const NamesAndTypesList & materialized_columns,\n         const NamesAndTypesList & alias_columns,\n-        const ColumnDefaults & column_defaults)\n-        : materialized_columns{materialized_columns},\n-          alias_columns{alias_columns},\n-          column_defaults{column_defaults}\n-    {}\n+        const ColumnDefaults & column_defaults);\n \n+    NamesAndTypesList columns;\n     NamesAndTypesList materialized_columns{};\n     NamesAndTypesList alias_columns{};\n     ColumnDefaults column_defaults{};\n \n private:\n-    virtual const NamesAndTypesList & getColumnsListImpl() const = 0;\n+    virtual const NamesAndTypesList & getColumnsListImpl() const\n+    {\n+        return columns;\n+    }\n \n     using ColumnsListRange = boost::range::joined_range<const NamesAndTypesList, const NamesAndTypesList>;\n     /// Returns a lazily joined range of table's ordinary and materialized columns, without unnecessary copying\ndiff --git a/dbms/src/Storages/MergeTree/MergeTreeData.cpp b/dbms/src/Storages/MergeTree/MergeTreeData.cpp\nindex c3afacf499e7..aa1698778812 100644\n--- a/dbms/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/dbms/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -92,7 +92,8 @@ MergeTreeData::MergeTreeData(\n     bool require_part_metadata_,\n     bool attach,\n     BrokenPartCallback broken_part_callback_)\n-    : ITableDeclaration{materialized_columns_, alias_columns_, column_defaults_}, context(context_),\n+    : ITableDeclaration{columns_, materialized_columns_, alias_columns_, column_defaults_},\n+    context(context_),\n     sampling_expression(sampling_expression_),\n     index_granularity(settings_.index_granularity),\n     merging_params(merging_params_),\n@@ -101,7 +102,7 @@ MergeTreeData::MergeTreeData(\n     partition_expr_ast(partition_expr_ast_),\n     require_part_metadata(require_part_metadata_),\n     database_name(database_), table_name(table_),\n-    full_path(full_path_), columns(columns_),\n+    full_path(full_path_),\n     broken_part_callback(broken_part_callback_),\n     log_name(database_name + \".\" + table_name), log(&Logger::get(log_name + \" (Data)\")),\n     data_parts_by_name(data_parts_indexes.get<TagByName>()),\ndiff --git a/dbms/src/Storages/MergeTree/MergeTreeData.h b/dbms/src/Storages/MergeTree/MergeTreeData.h\nindex 0a468f99f258..b9797dc710ff 100644\n--- a/dbms/src/Storages/MergeTree/MergeTreeData.h\n+++ b/dbms/src/Storages/MergeTree/MergeTreeData.h\n@@ -550,8 +550,6 @@ class MergeTreeData : public ITableDeclaration\n     String table_name;\n     String full_path;\n \n-    NamesAndTypesList columns;\n-\n     /// Current column sizes in compressed and uncompressed form.\n     ColumnSizes column_sizes;\n \ndiff --git a/dbms/src/Storages/StorageBuffer.cpp b/dbms/src/Storages/StorageBuffer.cpp\nindex b932324892df..91e112210848 100644\n--- a/dbms/src/Storages/StorageBuffer.cpp\n+++ b/dbms/src/Storages/StorageBuffer.cpp\n@@ -56,8 +56,8 @@ StorageBuffer::StorageBuffer(const std::string & name_, const NamesAndTypesList\n     Context & context_,\n     size_t num_shards_, const Thresholds & min_thresholds_, const Thresholds & max_thresholds_,\n     const String & destination_database_, const String & destination_table_, bool allow_materialized_)\n-    : IStorage{materialized_columns_, alias_columns_, column_defaults_},\n-    name(name_), columns(columns_), context(context_),\n+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_},\n+    name(name_), context(context_),\n     num_shards(num_shards_), buffers(num_shards_),\n     min_thresholds(min_thresholds_), max_thresholds(max_thresholds_),\n     destination_database(destination_database_), destination_table(destination_table_),\ndiff --git a/dbms/src/Storages/StorageBuffer.h b/dbms/src/Storages/StorageBuffer.h\nindex 98cbcff661c5..9d9f72340c59 100644\n--- a/dbms/src/Storages/StorageBuffer.h\n+++ b/dbms/src/Storages/StorageBuffer.h\n@@ -53,8 +53,6 @@ friend class BufferBlockOutputStream;\n     std::string getName() const override { return \"Buffer\"; }\n     std::string getTableName() const override { return name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     BlockInputStreams read(\n         const Names & column_names,\n         const SelectQueryInfo & query_info,\n@@ -82,7 +80,6 @@ friend class BufferBlockOutputStream;\n \n private:\n     String name;\n-    NamesAndTypesList columns;\n \n     Context & context;\n \ndiff --git a/dbms/src/Storages/StorageCatBoostPool.h b/dbms/src/Storages/StorageCatBoostPool.h\nindex 685b951aaf24..34aed30693c9 100644\n--- a/dbms/src/Storages/StorageCatBoostPool.h\n+++ b/dbms/src/Storages/StorageCatBoostPool.h\n@@ -14,8 +14,6 @@ class StorageCatBoostPool : public ext::shared_ptr_helper<StorageCatBoostPool>,\n \n     std::string getTableName() const override { return table_name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     BlockInputStreams read(const Names & column_names,\n                            const SelectQueryInfo & query_info,\n                            const Context & context,\n@@ -25,7 +23,7 @@ class StorageCatBoostPool : public ext::shared_ptr_helper<StorageCatBoostPool>,\n \n private:\n     String table_name;\n-    NamesAndTypesList columns;\n+\n     String column_description_file_name;\n     String data_description_file_name;\n     Block sample_block;\ndiff --git a/dbms/src/Storages/StorageDictionary.cpp b/dbms/src/Storages/StorageDictionary.cpp\nindex 2eb96faf7331..2bb69748c6d8 100644\n--- a/dbms/src/Storages/StorageDictionary.cpp\n+++ b/dbms/src/Storages/StorageDictionary.cpp\n@@ -30,8 +30,8 @@ StorageDictionary::StorageDictionary(\n     const ColumnDefaults & column_defaults_,\n     const DictionaryStructure & dictionary_structure_,\n     const String & dictionary_name_)\n-    : IStorage{materialized_columns_, alias_columns_, column_defaults_}, table_name(table_name_),\n-    columns(columns_), dictionary_name(dictionary_name_),\n+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_}, table_name(table_name_),\n+    dictionary_name(dictionary_name_),\n     logger(&Poco::Logger::get(\"StorageDictionary\"))\n {\n     checkNamesAndTypesCompatibleWithDictionary(dictionary_structure_);\ndiff --git a/dbms/src/Storages/StorageDictionary.h b/dbms/src/Storages/StorageDictionary.h\nindex 5394e1abfaff..0f21373f546c 100644\n--- a/dbms/src/Storages/StorageDictionary.h\n+++ b/dbms/src/Storages/StorageDictionary.h\n@@ -37,7 +37,6 @@ class StorageDictionary : public ext::shared_ptr_helper<StorageDictionary>, publ\n     using Ptr = MultiVersion<IDictionaryBase>::Version;\n \n     String table_name;\n-    NamesAndTypesList columns;\n     String dictionary_name;\n     Poco::Logger * logger;\n \ndiff --git a/dbms/src/Storages/StorageDistributed.cpp b/dbms/src/Storages/StorageDistributed.cpp\nindex 954b45e269d2..4728463c007f 100644\n--- a/dbms/src/Storages/StorageDistributed.cpp\n+++ b/dbms/src/Storages/StorageDistributed.cpp\n@@ -50,6 +50,8 @@ namespace DB\n namespace ErrorCodes\n {\n     extern const int STORAGE_REQUIRES_PARAMETER;\n+    extern const int BAD_ARGUMENTS;\n+    extern const int READONLY;\n     extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;\n     extern const int INCORRECT_NUMBER_OF_COLUMNS;\n }\n@@ -141,8 +143,8 @@ StorageDistributed::StorageDistributed(\n     const Context & context_,\n     const ASTPtr & sharding_key_,\n     const String & data_path_)\n-    : IStorage{materialized_columns_, alias_columns_, column_defaults_},\n-    name(name_), columns(columns_),\n+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_},\n+    name(name_),\n     remote_database(remote_database_), remote_table(remote_table_),\n     context(context_), cluster_name(context.getMacros().expand(cluster_name_)), has_sharding_key(sharding_key_),\n     sharding_key_expr(sharding_key_ ? ExpressionAnalyzer(sharding_key_, context, nullptr, columns).getActions(false) : nullptr),\n@@ -197,7 +199,7 @@ BlockInputStreams StorageDistributed::read(\n         external_tables = context.getExternalTables();\n \n     ClusterProxy::SelectStreamFactory select_stream_factory(\n-        processed_stage,  QualifiedTableName{remote_database, remote_table}, external_tables);\n+        processed_stage, QualifiedTableName{remote_database, remote_table}, external_tables);\n \n     return ClusterProxy::executeQuery(\n             select_stream_factory, cluster, modified_query_ast, context, settings);\n@@ -206,25 +208,29 @@ BlockInputStreams StorageDistributed::read(\n \n BlockOutputStreamPtr StorageDistributed::write(const ASTPtr & query, const Settings & settings)\n {\n-    auto cluster = owned_cluster ? owned_cluster : context.getCluster(cluster_name);\n+    auto cluster = (owned_cluster) ? owned_cluster : context.getCluster(cluster_name);\n \n-    /// TODO: !path.empty() can be replaced by !owned_cluster or !cluster_name.empty() ?\n-    /// owned_cluster for remote table function use sync insertion => doesn't need a path.\n-    bool write_enabled = (!path.empty() || owned_cluster)\n-                         && (((cluster->getLocalShardCount() + cluster->getRemoteShardCount()) < 2) || has_sharding_key);\n+    /// Ban an attempt to make async insert into the table belonging to DatabaseMemory\n+    if (path.empty() && !owned_cluster && !settings.insert_distributed_sync.value)\n+    {\n+        throw Exception(\"Storage \" + getName() + \" must has own data directory to enable asynchronous inserts\",\n+                        ErrorCodes::BAD_ARGUMENTS);\n+    }\n \n-    if (!write_enabled)\n-        throw Exception{\n-            \"Method write is not supported by storage \" + getName() +\n-            \" with more than one shard and no sharding key provided\",\n-            ErrorCodes::STORAGE_REQUIRES_PARAMETER};\n+    /// If sharding key is not specified, then you can only write to a shard containing only one shard\n+    if (!has_sharding_key && ((cluster->getLocalShardCount() + cluster->getRemoteShardCount()) >= 2))\n+    {\n+        throw Exception(\"Method write is not supported by storage \" + getName() + \" with more than one shard and no sharding key provided\",\n+                        ErrorCodes::STORAGE_REQUIRES_PARAMETER);\n+    }\n \n+    /// Force sync insertion if it is remote() table function\n     bool insert_sync = settings.insert_distributed_sync || owned_cluster;\n     auto timeout = settings.insert_distributed_timeout;\n \n     /// DistributedBlockOutputStream will not own cluster, but will own ConnectionPools of the cluster\n     return std::make_shared<DistributedBlockOutputStream>(\n-        *this, rewriteInsertQuery(query, remote_database, remote_table), cluster, insert_sync, timeout);\n+        *this, rewriteInsertQuery(query, remote_database, remote_table), cluster, settings, insert_sync, timeout);\n }\n \n \ndiff --git a/dbms/src/Storages/StorageDistributed.h b/dbms/src/Storages/StorageDistributed.h\nindex 09c18857f143..744903a077a4 100644\n--- a/dbms/src/Storages/StorageDistributed.h\n+++ b/dbms/src/Storages/StorageDistributed.h\n@@ -94,7 +94,6 @@ class StorageDistributed : public ext::shared_ptr_helper<StorageDistributed>, pu\n \n \n     String name;\n-    NamesAndTypesList columns;\n     String remote_database;\n     String remote_table;\n \ndiff --git a/dbms/src/Storages/StorageFile.cpp b/dbms/src/Storages/StorageFile.cpp\nindex 199e2a972bb2..57d567ff9039 100644\n--- a/dbms/src/Storages/StorageFile.cpp\n+++ b/dbms/src/Storages/StorageFile.cpp\n@@ -31,6 +31,8 @@ namespace ErrorCodes\n     extern const int DATABASE_ACCESS_DENIED;\n     extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;\n     extern const int UNKNOWN_IDENTIFIER;\n+    extern const int INCORRECT_FILE_NAME;\n+    extern const int EMPTY_LIST_OF_COLUMNS_PASSED;\n };\n \n \n@@ -57,8 +59,8 @@ StorageFile::StorageFile(\n         const NamesAndTypesList & alias_columns_,\n         const ColumnDefaults & column_defaults_,\n         Context & context_)\n-    : IStorage(materialized_columns_, alias_columns_, column_defaults_),\n-    table_name(table_name_), format_name(format_name_), columns(columns_), context_global(context_), table_fd(table_fd_)\n+    : IStorage(columns_, materialized_columns_, alias_columns_, column_defaults_),\n+    table_name(table_name_), format_name(format_name_), context_global(context_), table_fd(table_fd_)\n {\n     if (table_fd < 0) /// Will use file\n     {\n@@ -72,6 +74,9 @@ StorageFile::StorageFile(\n         }\n         else /// Is DB's file\n         {\n+            if (db_dir_path.empty())\n+                throw Exception(\"Storage \" + getName() + \" requires data path\", ErrorCodes::INCORRECT_FILE_NAME);\n+\n             path = getTablePath(db_dir_path, table_name, format_name);\n             is_db_table = true;\n             Poco::File(Poco::Path(path).parent()).createDirectories();\ndiff --git a/dbms/src/Storages/StorageFile.h b/dbms/src/Storages/StorageFile.h\nindex 58bb8bc1e9dd..4ab458261b2a 100644\n--- a/dbms/src/Storages/StorageFile.h\n+++ b/dbms/src/Storages/StorageFile.h\n@@ -77,7 +77,6 @@ class StorageFile : public ext::shared_ptr_helper<StorageFile>, public IStorage\n \n     std::string table_name;\n     std::string format_name;\n-    NamesAndTypesList columns;\n     Context & context_global;\n \n     std::string path;\ndiff --git a/dbms/src/Storages/StorageKafka.cpp b/dbms/src/Storages/StorageKafka.cpp\nindex 3d603b53adfb..61236d34c323 100644\n--- a/dbms/src/Storages/StorageKafka.cpp\n+++ b/dbms/src/Storages/StorageKafka.cpp\n@@ -228,9 +228,9 @@ StorageKafka::StorageKafka(\n     const ColumnDefaults & column_defaults_,\n     const String & brokers_, const String & group_, const Names & topics_,\n     const String & format_name_, const String & schema_name_, size_t num_consumers_)\n-    : IStorage{materialized_columns_, alias_columns_, column_defaults_},\n+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_},\n     table_name(table_name_), database_name(database_name_), context(context_),\n-    columns(columns_), topics(topics_), brokers(brokers_), group(group_), format_name(format_name_), schema_name(schema_name_),\n+    topics(topics_), brokers(brokers_), group(group_), format_name(format_name_), schema_name(schema_name_),\n     num_consumers(num_consumers_), log(&Logger::get(\"StorageKafka (\" + table_name_ + \")\")),\n     semaphore(0, num_consumers_), mutex(), consumers(), event_update()\n {\ndiff --git a/dbms/src/Storages/StorageKafka.h b/dbms/src/Storages/StorageKafka.h\nindex a41fee8f7471..39df9c136a14 100644\n--- a/dbms/src/Storages/StorageKafka.h\n+++ b/dbms/src/Storages/StorageKafka.h\n@@ -32,8 +32,6 @@ friend class KafkaBlockOutputStream;\n     std::string getTableName() const override { return table_name; }\n     std::string getDatabaseName() const { return database_name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     void startup() override;\n     void shutdown() override;\n \n@@ -73,7 +71,6 @@ friend class KafkaBlockOutputStream;\n     String table_name;\n     String database_name;\n     Context & context;\n-    NamesAndTypesList columns;\n     Names topics;\n     const String brokers;\n     const String group;\ndiff --git a/dbms/src/Storages/StorageLog.cpp b/dbms/src/Storages/StorageLog.cpp\nindex 1f477aedd199..6fdcaa2ed6ac 100644\n--- a/dbms/src/Storages/StorageLog.cpp\n+++ b/dbms/src/Storages/StorageLog.cpp\n@@ -41,6 +41,7 @@ namespace ErrorCodes\n     extern const int DUPLICATE_COLUMN;\n     extern const int SIZES_OF_MARKS_FILES_ARE_INCONSISTENT;\n     extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;\n+    extern const int INCORRECT_FILE_NAME;\n }\n \n \n@@ -363,15 +364,15 @@ StorageLog::StorageLog(\n     const NamesAndTypesList & alias_columns_,\n     const ColumnDefaults & column_defaults_,\n     size_t max_compress_block_size_)\n-    : IStorage{materialized_columns_, alias_columns_, column_defaults_},\n-    path(path_), name(name_), columns(columns_),\n+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_},\n+    path(path_), name(name_),\n     max_compress_block_size(max_compress_block_size_),\n     file_checker(path + escapeForFileName(name) + '/' + \"sizes.json\")\n {\n-    if (columns.empty())\n-        throw Exception(\"Empty list of columns passed to StorageLog constructor\", ErrorCodes::EMPTY_LIST_OF_COLUMNS_PASSED);\n+    if (path.empty())\n+        throw Exception(\"Storage \" + getName() + \" requires data path\", ErrorCodes::INCORRECT_FILE_NAME);\n \n-    /// create files if they do not exist\n+     /// create files if they do not exist\n     Poco::File(path + escapeForFileName(name) + '/').createDirectories();\n \n     for (const auto & column : getColumnsList())\ndiff --git a/dbms/src/Storages/StorageLog.h b/dbms/src/Storages/StorageLog.h\nindex 740454f6fce3..2d362c09272a 100644\n--- a/dbms/src/Storages/StorageLog.h\n+++ b/dbms/src/Storages/StorageLog.h\n@@ -26,8 +26,6 @@ friend class LogBlockOutputStream;\n     std::string getName() const override { return \"Log\"; }\n     std::string getTableName() const override { return name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     BlockInputStreams read(\n         const Names & column_names,\n         const SelectQueryInfo & query_info,\n@@ -59,7 +57,6 @@ friend class LogBlockOutputStream;\n private:\n     String path;\n     String name;\n-    NamesAndTypesList columns;\n \n     mutable std::shared_mutex rwlock;\n \ndiff --git a/dbms/src/Storages/StorageMaterializedView.cpp b/dbms/src/Storages/StorageMaterializedView.cpp\nindex 101a6566d472..f1326a52933f 100644\n--- a/dbms/src/Storages/StorageMaterializedView.cpp\n+++ b/dbms/src/Storages/StorageMaterializedView.cpp\n@@ -65,8 +65,8 @@ StorageMaterializedView::StorageMaterializedView(\n     const NamesAndTypesList & alias_columns_,\n     const ColumnDefaults & column_defaults_,\n     bool attach_)\n-    : IStorage{materialized_columns_, alias_columns_, column_defaults_}, table_name(table_name_),\n-    database_name(database_name_), global_context(local_context.getGlobalContext()), columns(columns_)\n+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_}, table_name(table_name_),\n+    database_name(database_name_), global_context(local_context.getGlobalContext())\n {\n     if (!query.select)\n         throw Exception(\"SELECT query is not specified for \" + getName(), ErrorCodes::INCORRECT_QUERY);\ndiff --git a/dbms/src/Storages/StorageMaterializedView.h b/dbms/src/Storages/StorageMaterializedView.h\nindex b153a4e864b1..b2a2069166aa 100644\n--- a/dbms/src/Storages/StorageMaterializedView.h\n+++ b/dbms/src/Storages/StorageMaterializedView.h\n@@ -51,7 +51,6 @@ class StorageMaterializedView : public ext::shared_ptr_helper<StorageMaterialize\n     String database_name;\n     ASTPtr inner_query;\n     Context & global_context;\n-    NamesAndTypesList columns;\n     bool has_inner_table = false;\n \n protected:\ndiff --git a/dbms/src/Storages/StorageMemory.cpp b/dbms/src/Storages/StorageMemory.cpp\nindex 75afcdbcc8be..db4be02635cc 100644\n--- a/dbms/src/Storages/StorageMemory.cpp\n+++ b/dbms/src/Storages/StorageMemory.cpp\n@@ -87,8 +87,8 @@ StorageMemory::StorageMemory(\n     const NamesAndTypesList & materialized_columns_,\n     const NamesAndTypesList & alias_columns_,\n     const ColumnDefaults & column_defaults_)\n-    : IStorage{materialized_columns_, alias_columns_, column_defaults_},\n-    name(name_), columns(columns_)\n+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_},\n+    name(name_)\n {\n }\n \ndiff --git a/dbms/src/Storages/StorageMemory.h b/dbms/src/Storages/StorageMemory.h\nindex 255434e9e344..956e4354727b 100644\n--- a/dbms/src/Storages/StorageMemory.h\n+++ b/dbms/src/Storages/StorageMemory.h\n@@ -26,8 +26,6 @@ friend class MemoryBlockOutputStream;\n     std::string getName() const override { return \"Memory\"; }\n     std::string getTableName() const override { return name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     size_t getSize() const { return data.size(); }\n \n     BlockInputStreams read(\n@@ -45,7 +43,6 @@ friend class MemoryBlockOutputStream;\n \n private:\n     String name;\n-    NamesAndTypesList columns;\n \n     /// The data itself. `list` - so that when inserted to the end, the existing iterators are not invalidated.\n     BlocksList data;\ndiff --git a/dbms/src/Storages/StorageMerge.cpp b/dbms/src/Storages/StorageMerge.cpp\nindex 88f646af0bdb..1e9a590714bd 100644\n--- a/dbms/src/Storages/StorageMerge.cpp\n+++ b/dbms/src/Storages/StorageMerge.cpp\n@@ -42,8 +42,8 @@ StorageMerge::StorageMerge(\n     const String & source_database_,\n     const String & table_name_regexp_,\n     const Context & context_)\n-    : IStorage{materialized_columns_, alias_columns_, column_defaults_},\n-    name(name_), columns(columns_), source_database(source_database_),\n+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_},\n+    name(name_), source_database(source_database_),\n     table_name_regexp(table_name_regexp_), context(context_)\n {\n }\ndiff --git a/dbms/src/Storages/StorageMerge.h b/dbms/src/Storages/StorageMerge.h\nindex 9ceae8f92eaf..9bef645c0e00 100644\n--- a/dbms/src/Storages/StorageMerge.h\n+++ b/dbms/src/Storages/StorageMerge.h\n@@ -47,7 +47,6 @@ class StorageMerge : public ext::shared_ptr_helper<StorageMerge>, public IStorag\n \n private:\n     String name;\n-    NamesAndTypesList columns;\n     String source_database;\n     OptimizedRegularExpression table_name_regexp;\n     const Context & context;\ndiff --git a/dbms/src/Storages/StorageMergeTree.cpp b/dbms/src/Storages/StorageMergeTree.cpp\nindex 4fec351d6a12..50e316706d58 100644\n--- a/dbms/src/Storages/StorageMergeTree.cpp\n+++ b/dbms/src/Storages/StorageMergeTree.cpp\n@@ -27,6 +27,7 @@ namespace ErrorCodes\n     extern const int ABORTED;\n     extern const int BAD_ARGUMENTS;\n     extern const int INCORRECT_DATA;\n+    extern const int INCORRECT_FILE_NAME;\n     extern const int CANNOT_ASSIGN_OPTIMIZE;\n }\n \n@@ -48,7 +49,7 @@ StorageMergeTree::StorageMergeTree(\n     const MergeTreeData::MergingParams & merging_params_,\n     const MergeTreeSettings & settings_,\n     bool has_force_restore_data_flag)\n-    : IStorage{materialized_columns_, alias_columns_, column_defaults_},\n+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_},\n     path(path_), database_name(database_name_), table_name(table_name_), full_path(path + escapeForFileName(table_name) + '/'),\n     context(context_), background_pool(context_.getBackgroundPool()),\n     data(database_name, table_name,\n@@ -60,6 +61,9 @@ StorageMergeTree::StorageMergeTree(\n     reader(data), writer(data), merger(data, context.getBackgroundPool()),\n     log(&Logger::get(database_name_ + \".\" + table_name + \" (StorageMergeTree)\"))\n {\n+    if (path_.empty())\n+        throw Exception(\"MergeTree storages require data path\", ErrorCodes::INCORRECT_FILE_NAME);\n+\n     data.loadDataParts(has_force_restore_data_flag);\n \n     if (!attach)\ndiff --git a/dbms/src/Storages/StorageMySQL.cpp b/dbms/src/Storages/StorageMySQL.cpp\nindex 694489ba8959..f3f18518d836 100644\n--- a/dbms/src/Storages/StorageMySQL.cpp\n+++ b/dbms/src/Storages/StorageMySQL.cpp\n@@ -23,11 +23,11 @@ StorageMySQL::StorageMySQL(\n     mysqlxx::Pool && pool,\n     const std::string & remote_database_name,\n     const std::string & remote_table_name,\n-    const NamesAndTypesList & columns)\n-    : name(name)\n+    const NamesAndTypesList & columns_)\n+    : IStorage{columns_, {}, {}, {}}\n+    , name(name)\n     , remote_database_name(remote_database_name)\n     , remote_table_name(remote_table_name)\n-    , columns(columns)\n     , pool(std::move(pool))\n {\n }\ndiff --git a/dbms/src/Storages/StorageMySQL.h b/dbms/src/Storages/StorageMySQL.h\nindex 29d0d35e69d7..9558605ff0e2 100644\n--- a/dbms/src/Storages/StorageMySQL.h\n+++ b/dbms/src/Storages/StorageMySQL.h\n@@ -24,11 +24,10 @@ class StorageMySQL : public ext::shared_ptr_helper<StorageMySQL>, public IStorag\n         mysqlxx::Pool && pool,\n         const std::string & remote_database_name,\n         const std::string & remote_table_name,\n-        const NamesAndTypesList & columns);\n+        const NamesAndTypesList & columns_);\n \n     std::string getName() const override { return \"MySQL\"; }\n     std::string getTableName() const override { return name; }\n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n \n     BlockInputStreams read(\n         const Names & column_names,\n@@ -44,7 +43,7 @@ class StorageMySQL : public ext::shared_ptr_helper<StorageMySQL>, public IStorag\n     std::string remote_database_name;\n     std::string remote_table_name;\n \n-    NamesAndTypesList columns;\n+\n     mysqlxx::Pool pool;\n };\n \ndiff --git a/dbms/src/Storages/StorageNull.h b/dbms/src/Storages/StorageNull.h\nindex 3df037a46bec..b2d3fb557bf2 100644\n--- a/dbms/src/Storages/StorageNull.h\n+++ b/dbms/src/Storages/StorageNull.h\n@@ -20,8 +20,6 @@ class StorageNull : public ext::shared_ptr_helper<StorageNull>, public IStorage\n     std::string getName() const override { return \"Null\"; }\n     std::string getTableName() const override { return name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     BlockInputStreams read(\n         const Names &,\n         const SelectQueryInfo &,\n@@ -47,7 +45,6 @@ class StorageNull : public ext::shared_ptr_helper<StorageNull>, public IStorage\n \n private:\n     String name;\n-    NamesAndTypesList columns;\n \n protected:\n     StorageNull(\n@@ -56,7 +53,7 @@ class StorageNull : public ext::shared_ptr_helper<StorageNull>, public IStorage\n         const NamesAndTypesList & materialized_columns_,\n         const NamesAndTypesList & alias_columns_,\n         const ColumnDefaults & column_defaults_)\n-        : IStorage{materialized_columns_, alias_columns_, column_defaults_}, name(name_), columns(columns_) {}\n+        : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_}, name(name_)  {}\n };\n \n }\ndiff --git a/dbms/src/Storages/StorageODBC.cpp b/dbms/src/Storages/StorageODBC.cpp\nindex 31ee1531dcf9..67362bf2f902 100644\n--- a/dbms/src/Storages/StorageODBC.cpp\n+++ b/dbms/src/Storages/StorageODBC.cpp\n@@ -21,11 +21,11 @@ StorageODBC::StorageODBC(\n     const std::string & connection_string,\n     const std::string & remote_database_name,\n     const std::string & remote_table_name,\n-    const NamesAndTypesList & columns)\n-    : name(name)\n+    const NamesAndTypesList & columns_)\n+    : IStorage{columns_, {}, {}, {}}\n+    , name(name)\n     , remote_database_name(remote_database_name)\n     , remote_table_name(remote_table_name)\n-    , columns(columns)\n {\n     pool = createAndCheckResizePocoSessionPool([&]\n     {\ndiff --git a/dbms/src/Storages/StorageODBC.h b/dbms/src/Storages/StorageODBC.h\nindex d411a45f766e..f582c74a2635 100644\n--- a/dbms/src/Storages/StorageODBC.h\n+++ b/dbms/src/Storages/StorageODBC.h\n@@ -30,11 +30,10 @@ class StorageODBC : public ext::shared_ptr_helper<StorageODBC>, public IStorage\n         const std::string & connection_string,\n         const std::string & remote_database_name,\n         const std::string & remote_table_name,\n-        const NamesAndTypesList & columns);\n+        const NamesAndTypesList & columns_);\n \n     std::string getName() const override { return \"ODBC\"; }\n     std::string getTableName() const override { return name; }\n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n \n     BlockInputStreams read(\n         const Names & column_names,\n@@ -46,12 +45,9 @@ class StorageODBC : public ext::shared_ptr_helper<StorageODBC>, public IStorage\n \n private:\n     std::string name;\n-\n     std::string remote_database_name;\n     std::string remote_table_name;\n \n-    NamesAndTypesList columns;\n-\n     std::shared_ptr<Poco::Data::SessionPool> pool;\n };\n }\ndiff --git a/dbms/src/Storages/StorageReplicatedMergeTree.cpp b/dbms/src/Storages/StorageReplicatedMergeTree.cpp\nindex 308088d5b254..b419545efb54 100644\n--- a/dbms/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/dbms/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -95,6 +95,7 @@ namespace ErrorCodes\n     extern const int TOO_MUCH_FETCHES;\n     extern const int BAD_DATA_PART_NAME;\n     extern const int PART_IS_TEMPORARILY_LOCKED;\n+    extern const int INCORRECT_FILE_NAME;\n     extern const int CANNOT_ASSIGN_OPTIMIZE;\n }\n \n@@ -178,7 +179,7 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(\n     const MergeTreeData::MergingParams & merging_params_,\n     const MergeTreeSettings & settings_,\n     bool has_force_restore_data_flag)\n-    : IStorage{materialized_columns_, alias_columns_, column_defaults_}, context(context_),\n+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_}, context(context_),\n     current_zookeeper(context.getZooKeeper()), database_name(database_name_),\n     table_name(name_), full_path(path_ + escapeForFileName(table_name) + '/'),\n     zookeeper_path(context.getMacros().expand(zookeeper_path_)),\n@@ -195,6 +196,9 @@ StorageReplicatedMergeTree::StorageReplicatedMergeTree(\n     shutdown_event(false), part_check_thread(*this),\n     log(&Logger::get(database_name + \".\" + table_name + \" (StorageReplicatedMergeTree)\"))\n {\n+    if (path_.empty())\n+        throw Exception(\"ReplicatedMergeTree storages require data path\", ErrorCodes::INCORRECT_FILE_NAME);\n+\n     if (!zookeeper_path.empty() && zookeeper_path.back() == '/')\n         zookeeper_path.resize(zookeeper_path.size() - 1);\n     /// If zookeeper chroot prefix is used, path should starts with '/', because chroot concatenates without it.\ndiff --git a/dbms/src/Storages/StorageSet.cpp b/dbms/src/Storages/StorageSet.cpp\nindex 581b28766c13..fbd7f0803f3c 100644\n--- a/dbms/src/Storages/StorageSet.cpp\n+++ b/dbms/src/Storages/StorageSet.cpp\n@@ -21,6 +21,12 @@ namespace ErrorCodes\n }\n \n \n+namespace ErrorCodes\n+{\n+    extern const int INCORRECT_FILE_NAME;\n+}\n+\n+\n class SetOrJoinBlockOutputStream : public IBlockOutputStream\n {\n public:\n@@ -86,9 +92,13 @@ StorageSetOrJoinBase::StorageSetOrJoinBase(\n     const NamesAndTypesList & materialized_columns_,\n     const NamesAndTypesList & alias_columns_,\n     const ColumnDefaults & column_defaults_)\n-    : IStorage{materialized_columns_, alias_columns_, column_defaults_},\n-    path(path_ + escapeForFileName(name_) + '/'), name(name_), columns(columns_)\n+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_},\n+    name(name_)\n {\n+    if (path_.empty())\n+        throw Exception(\"Join and Set storages require data path\", ErrorCodes::INCORRECT_FILE_NAME);\n+\n+    path = path_ + escapeForFileName(name_) + '/';\n }\n \n \ndiff --git a/dbms/src/Storages/StorageSet.h b/dbms/src/Storages/StorageSet.h\nindex bd5045208def..0b08f3af2dbd 100644\n--- a/dbms/src/Storages/StorageSet.h\n+++ b/dbms/src/Storages/StorageSet.h\n@@ -20,7 +20,6 @@ class StorageSetOrJoinBase : public IStorage\n \n public:\n     String getTableName() const override { return name; }\n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n \n     void rename(const String & new_path_to_db, const String & new_database_name, const String & new_table_name) override;\n \n@@ -37,7 +36,6 @@ class StorageSetOrJoinBase : public IStorage\n \n     String path;\n     String name;\n-    NamesAndTypesList columns;\n \n     UInt64 increment = 0;    /// For the backup file names.\n \ndiff --git a/dbms/src/Storages/StorageStripeLog.cpp b/dbms/src/Storages/StorageStripeLog.cpp\nindex 948832f2cac4..62e9c3cc9985 100644\n--- a/dbms/src/Storages/StorageStripeLog.cpp\n+++ b/dbms/src/Storages/StorageStripeLog.cpp\n@@ -39,6 +39,7 @@ namespace ErrorCodes\n     extern const int EMPTY_LIST_OF_COLUMNS_PASSED;\n     extern const int CANNOT_CREATE_DIRECTORY;\n     extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;\n+    extern const int INCORRECT_FILE_NAME;\n }\n \n \n@@ -183,14 +184,14 @@ StorageStripeLog::StorageStripeLog(\n     const ColumnDefaults & column_defaults_,\n     bool attach,\n     size_t max_compress_block_size_)\n-    : IStorage{materialized_columns_, alias_columns_, column_defaults_},\n-    path(path_), name(name_), columns(columns_),\n+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_},\n+    path(path_), name(name_),\n     max_compress_block_size(max_compress_block_size_),\n     file_checker(path + escapeForFileName(name) + '/' + \"sizes.json\"),\n     log(&Logger::get(\"StorageStripeLog\"))\n {\n-    if (columns.empty())\n-        throw Exception(\"Empty list of columns passed to StorageStripeLog constructor\", ErrorCodes::EMPTY_LIST_OF_COLUMNS_PASSED);\n+    if (path.empty())\n+        throw Exception(\"Storage \" + getName() + \" requires data path\", ErrorCodes::INCORRECT_FILE_NAME);\n \n     String full_path = path + escapeForFileName(name) + '/';\n     if (!attach)\ndiff --git a/dbms/src/Storages/StorageStripeLog.h b/dbms/src/Storages/StorageStripeLog.h\nindex f8cad8a4d70c..e1e72873794d 100644\n--- a/dbms/src/Storages/StorageStripeLog.h\n+++ b/dbms/src/Storages/StorageStripeLog.h\n@@ -28,8 +28,6 @@ friend class StripeLogBlockOutputStream;\n     std::string getName() const override { return \"StripeLog\"; }\n     std::string getTableName() const override { return name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     BlockInputStreams read(\n         const Names & column_names,\n         const SelectQueryInfo & query_info,\n@@ -56,7 +54,6 @@ friend class StripeLogBlockOutputStream;\n private:\n     String path;\n     String name;\n-    NamesAndTypesList columns;\n \n     size_t max_compress_block_size;\n \ndiff --git a/dbms/src/Storages/StorageTinyLog.cpp b/dbms/src/Storages/StorageTinyLog.cpp\nindex 986cbcc97a79..23d1e0c44759 100644\n--- a/dbms/src/Storages/StorageTinyLog.cpp\n+++ b/dbms/src/Storages/StorageTinyLog.cpp\n@@ -46,6 +46,7 @@ namespace ErrorCodes\n     extern const int CANNOT_READ_ALL_DATA;\n     extern const int DUPLICATE_COLUMN;\n     extern const int LOGICAL_ERROR;\n+    extern const int INCORRECT_FILE_NAME;\n     extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;\n }\n \n@@ -284,14 +285,14 @@ StorageTinyLog::StorageTinyLog(\n     const ColumnDefaults & column_defaults_,\n     bool attach,\n     size_t max_compress_block_size_)\n-    : IStorage{materialized_columns_, alias_columns_, column_defaults_},\n-    path(path_), name(name_), columns(columns_),\n+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_},\n+    path(path_), name(name_),\n     max_compress_block_size(max_compress_block_size_),\n     file_checker(path + escapeForFileName(name) + '/' + \"sizes.json\"),\n     log(&Logger::get(\"StorageTinyLog\"))\n {\n-    if (columns.empty())\n-        throw Exception(\"Empty list of columns passed to StorageTinyLog constructor\", ErrorCodes::EMPTY_LIST_OF_COLUMNS_PASSED);\n+    if (path.empty())\n+        throw Exception(\"Storage \" + getName() + \" requires data path\", ErrorCodes::INCORRECT_FILE_NAME);\n \n     String full_path = path + escapeForFileName(name) + '/';\n     if (!attach)\ndiff --git a/dbms/src/Storages/StorageTinyLog.h b/dbms/src/Storages/StorageTinyLog.h\nindex 7d240fae0176..84e52afe265d 100644\n--- a/dbms/src/Storages/StorageTinyLog.h\n+++ b/dbms/src/Storages/StorageTinyLog.h\n@@ -27,8 +27,6 @@ friend class TinyLogBlockOutputStream;\n     std::string getName() const override { return \"TinyLog\"; }\n     std::string getTableName() const override { return name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     BlockInputStreams read(\n         const Names & column_names,\n         const SelectQueryInfo & query_info,\n@@ -55,7 +53,6 @@ friend class TinyLogBlockOutputStream;\n private:\n     String path;\n     String name;\n-    NamesAndTypesList columns;\n \n     size_t max_compress_block_size;\n \ndiff --git a/dbms/src/Storages/StorageView.cpp b/dbms/src/Storages/StorageView.cpp\nindex 6920c38fd5ca..3001bc293041 100644\n--- a/dbms/src/Storages/StorageView.cpp\n+++ b/dbms/src/Storages/StorageView.cpp\n@@ -25,8 +25,8 @@ StorageView::StorageView(\n     const NamesAndTypesList & materialized_columns_,\n     const NamesAndTypesList & alias_columns_,\n     const ColumnDefaults & column_defaults_)\n-    : IStorage{materialized_columns_, alias_columns_, column_defaults_}, table_name(table_name_),\n-    database_name(database_name_), columns(columns_)\n+    : IStorage{columns_, materialized_columns_, alias_columns_, column_defaults_}, table_name(table_name_),\n+    database_name(database_name_)\n {\n     if (!query.select)\n         throw Exception(\"SELECT query is not specified for \" + getName(), ErrorCodes::INCORRECT_QUERY);\ndiff --git a/dbms/src/Storages/StorageView.h b/dbms/src/Storages/StorageView.h\nindex 675f92f3cad5..854420206830 100644\n--- a/dbms/src/Storages/StorageView.h\n+++ b/dbms/src/Storages/StorageView.h\n@@ -16,7 +16,6 @@ class StorageView : public ext::shared_ptr_helper<StorageView>, public IStorage\n public:\n     std::string getName() const override { return \"View\"; }\n     std::string getTableName() const override { return table_name; }\n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n \n     /// It is passed inside the query and solved at its level.\n     bool supportsSampling() const override { return true; }\n@@ -36,7 +35,6 @@ class StorageView : public ext::shared_ptr_helper<StorageView>, public IStorage\n     String table_name;\n     String database_name;\n     ASTPtr inner_query;\n-    NamesAndTypesList columns;\n \n protected:\n     StorageView(\ndiff --git a/dbms/src/Storages/System/StorageSystemAsynchronousMetrics.cpp b/dbms/src/Storages/System/StorageSystemAsynchronousMetrics.cpp\nindex 5b35a7ebb679..44f8d933c997 100644\n--- a/dbms/src/Storages/System/StorageSystemAsynchronousMetrics.cpp\n+++ b/dbms/src/Storages/System/StorageSystemAsynchronousMetrics.cpp\n@@ -14,13 +14,12 @@ namespace DB\n \n StorageSystemAsynchronousMetrics::StorageSystemAsynchronousMetrics(const std::string & name_, const AsynchronousMetrics & async_metrics_)\n     : name(name_),\n-    columns\n-    {\n-        {\"metric\", std::make_shared<DataTypeString>()},\n-        {\"value\", std::make_shared<DataTypeFloat64>()},\n-    },\n     async_metrics(async_metrics_)\n {\n+    columns = NamesAndTypesList{\n+        {\"metric\", std::make_shared<DataTypeString>()},\n+        {\"value\", std::make_shared<DataTypeFloat64>()},\n+    };\n }\n \n \ndiff --git a/dbms/src/Storages/System/StorageSystemAsynchronousMetrics.h b/dbms/src/Storages/System/StorageSystemAsynchronousMetrics.h\nindex 560265e1dab9..60e500961435 100644\n--- a/dbms/src/Storages/System/StorageSystemAsynchronousMetrics.h\n+++ b/dbms/src/Storages/System/StorageSystemAsynchronousMetrics.h\n@@ -19,8 +19,6 @@ class StorageSystemAsynchronousMetrics : public ext::shared_ptr_helper<StorageSy\n     std::string getName() const override { return \"SystemAsynchronousMetrics\"; }\n     std::string getTableName() const override { return name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     BlockInputStreams read(\n         const Names & column_names,\n         const SelectQueryInfo & query_info,\n@@ -31,7 +29,6 @@ class StorageSystemAsynchronousMetrics : public ext::shared_ptr_helper<StorageSy\n \n private:\n     const std::string name;\n-    NamesAndTypesList columns;\n     const AsynchronousMetrics & async_metrics;\n \n protected:\ndiff --git a/dbms/src/Storages/System/StorageSystemBuildOptions.cpp b/dbms/src/Storages/System/StorageSystemBuildOptions.cpp\nindex 029620ff4ad7..c8da9f12c404 100644\n--- a/dbms/src/Storages/System/StorageSystemBuildOptions.cpp\n+++ b/dbms/src/Storages/System/StorageSystemBuildOptions.cpp\n@@ -12,12 +12,11 @@ namespace DB\n \n StorageSystemBuildOptions::StorageSystemBuildOptions(const std::string & name_)\n     : name(name_)\n-    , columns\n-    {\n+{\n+    columns = NamesAndTypesList{\n         { \"name\", std::make_shared<DataTypeString>() },\n         { \"value\", std::make_shared<DataTypeString>() },\n-    }\n-{\n+    };\n }\n \n \ndiff --git a/dbms/src/Storages/System/StorageSystemBuildOptions.h b/dbms/src/Storages/System/StorageSystemBuildOptions.h\nindex a1ecae77b3a3..d772b2553830 100644\n--- a/dbms/src/Storages/System/StorageSystemBuildOptions.h\n+++ b/dbms/src/Storages/System/StorageSystemBuildOptions.h\n@@ -18,8 +18,6 @@ class StorageSystemBuildOptions : public ext::shared_ptr_helper<StorageSystemBui\n     std::string getName() const override { return \"SystemBuildOptions\"; }\n     std::string getTableName() const override { return name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     BlockInputStreams read(\n         const Names & column_names,\n         const SelectQueryInfo & query_info,\n@@ -30,7 +28,6 @@ class StorageSystemBuildOptions : public ext::shared_ptr_helper<StorageSystemBui\n \n private:\n     const std::string name;\n-    NamesAndTypesList columns;\n \n protected:\n     StorageSystemBuildOptions(const std::string & name_);\ndiff --git a/dbms/src/Storages/System/StorageSystemClusters.cpp b/dbms/src/Storages/System/StorageSystemClusters.cpp\nindex 5cd79fa03723..084f295de74d 100644\n--- a/dbms/src/Storages/System/StorageSystemClusters.cpp\n+++ b/dbms/src/Storages/System/StorageSystemClusters.cpp\n@@ -13,7 +13,8 @@ namespace DB\n \n StorageSystemClusters::StorageSystemClusters(const std::string & name_)\n     : name(name_)\n-    , columns{\n+{\n+    columns = NamesAndTypesList{\n         { \"cluster\",      std::make_shared<DataTypeString>() },\n         { \"shard_num\",    std::make_shared<DataTypeUInt32>() },\n         { \"shard_weight\", std::make_shared<DataTypeUInt32>() },\n@@ -24,8 +25,7 @@ StorageSystemClusters::StorageSystemClusters(const std::string & name_)\n         { \"is_local\",     std::make_shared<DataTypeUInt8>() },\n         { \"user\",         std::make_shared<DataTypeString>() },\n         { \"default_database\", std::make_shared<DataTypeString>() }\n-    }\n-{\n+    };\n }\n \n \ndiff --git a/dbms/src/Storages/System/StorageSystemClusters.h b/dbms/src/Storages/System/StorageSystemClusters.h\nindex c8113d4d0953..1e36269ded2d 100644\n--- a/dbms/src/Storages/System/StorageSystemClusters.h\n+++ b/dbms/src/Storages/System/StorageSystemClusters.h\n@@ -18,7 +18,6 @@ class StorageSystemClusters : public ext::shared_ptr_helper<StorageSystemCluster\n public:\n     std::string getName() const override { return \"SystemClusters\"; }\n     std::string getTableName() const override { return name; }\n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n \n     BlockInputStreams read(\n         const Names & column_names,\n@@ -30,7 +29,6 @@ class StorageSystemClusters : public ext::shared_ptr_helper<StorageSystemCluster\n \n private:\n     const std::string name;\n-    NamesAndTypesList columns;\n \n protected:\n     StorageSystemClusters(const std::string & name_);\ndiff --git a/dbms/src/Storages/System/StorageSystemColumns.cpp b/dbms/src/Storages/System/StorageSystemColumns.cpp\nindex f7a38cd42955..5f9116b08a59 100644\n--- a/dbms/src/Storages/System/StorageSystemColumns.cpp\n+++ b/dbms/src/Storages/System/StorageSystemColumns.cpp\n@@ -17,7 +17,8 @@ namespace DB\n \n StorageSystemColumns::StorageSystemColumns(const std::string & name_)\n     : name(name_)\n-    , columns{\n+{\n+    columns = NamesAndTypesList{\n         { \"database\",           std::make_shared<DataTypeString>() },\n         { \"table\",              std::make_shared<DataTypeString>() },\n         { \"name\",               std::make_shared<DataTypeString>() },\n@@ -27,8 +28,7 @@ StorageSystemColumns::StorageSystemColumns(const std::string & name_)\n         { \"data_compressed_bytes\",      std::make_shared<DataTypeUInt64>() },\n         { \"data_uncompressed_bytes\",    std::make_shared<DataTypeUInt64>() },\n         { \"marks_bytes\",                std::make_shared<DataTypeUInt64>() },\n-    }\n-{\n+    };\n }\n \n \ndiff --git a/dbms/src/Storages/System/StorageSystemColumns.h b/dbms/src/Storages/System/StorageSystemColumns.h\nindex bd7dfbf21ce1..ba187f7306fa 100644\n--- a/dbms/src/Storages/System/StorageSystemColumns.h\n+++ b/dbms/src/Storages/System/StorageSystemColumns.h\n@@ -16,7 +16,6 @@ class StorageSystemColumns : public ext::shared_ptr_helper<StorageSystemColumns>\n public:\n     std::string getName() const override { return \"SystemColumns\"; }\n     std::string getTableName() const override { return name; }\n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n \n     BlockInputStreams read(\n         const Names & column_names,\n@@ -31,7 +30,6 @@ class StorageSystemColumns : public ext::shared_ptr_helper<StorageSystemColumns>\n \n private:\n     const std::string name;\n-    NamesAndTypesList columns;\n };\n \n }\ndiff --git a/dbms/src/Storages/System/StorageSystemDatabases.cpp b/dbms/src/Storages/System/StorageSystemDatabases.cpp\nindex 34a7d566c827..c4c41f3375da 100644\n--- a/dbms/src/Storages/System/StorageSystemDatabases.cpp\n+++ b/dbms/src/Storages/System/StorageSystemDatabases.cpp\n@@ -11,13 +11,12 @@ namespace DB\n \n \n StorageSystemDatabases::StorageSystemDatabases(const std::string & name_)\n-    : name(name_),\n-    columns\n-    {\n-        {\"name\",     std::make_shared<DataTypeString>()},\n-        {\"engine\",     std::make_shared<DataTypeString>()},\n-    }\n+    : name(name_)\n {\n+    columns = NamesAndTypesList{\n+        {\"name\",    std::make_shared<DataTypeString>()},\n+        {\"engine\",  std::make_shared<DataTypeString>()},\n+    };\n }\n \n \ndiff --git a/dbms/src/Storages/System/StorageSystemDatabases.h b/dbms/src/Storages/System/StorageSystemDatabases.h\nindex c574861a94d1..621e490963aa 100644\n--- a/dbms/src/Storages/System/StorageSystemDatabases.h\n+++ b/dbms/src/Storages/System/StorageSystemDatabases.h\n@@ -18,8 +18,6 @@ class StorageSystemDatabases : public ext::shared_ptr_helper<StorageSystemDataba\n     std::string getName() const override { return \"SystemDatabases\"; }\n     std::string getTableName() const override { return name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     BlockInputStreams read(\n         const Names & column_names,\n         const SelectQueryInfo & query_info,\n@@ -30,7 +28,6 @@ class StorageSystemDatabases : public ext::shared_ptr_helper<StorageSystemDataba\n \n private:\n     const std::string name;\n-    NamesAndTypesList columns;\n \n protected:\n     StorageSystemDatabases(const std::string & name_);\ndiff --git a/dbms/src/Storages/System/StorageSystemDictionaries.cpp b/dbms/src/Storages/System/StorageSystemDictionaries.cpp\nindex 2ef9a8c4db67..93c742ef1a79 100644\n--- a/dbms/src/Storages/System/StorageSystemDictionaries.cpp\n+++ b/dbms/src/Storages/System/StorageSystemDictionaries.cpp\n@@ -19,24 +19,24 @@ namespace DB\n {\n \n StorageSystemDictionaries::StorageSystemDictionaries(const std::string & name)\n-    : name{name},\n-      columns{\n-          { \"name\", std::make_shared<DataTypeString>() },\n-          { \"origin\", std::make_shared<DataTypeString>() },\n-          { \"type\", std::make_shared<DataTypeString>() },\n-          { \"key\", std::make_shared<DataTypeString>() },\n-          { \"attribute.names\", std::make_shared<DataTypeArray>(std::make_shared<DataTypeString>()) },\n-          { \"attribute.types\", std::make_shared<DataTypeArray>(std::make_shared<DataTypeString>()) },\n-          { \"bytes_allocated\", std::make_shared<DataTypeUInt64>() },\n-          { \"query_count\", std::make_shared<DataTypeUInt64>() },\n-          { \"hit_rate\", std::make_shared<DataTypeFloat64>() },\n-          { \"element_count\", std::make_shared<DataTypeUInt64>() },\n-          { \"load_factor\", std::make_shared<DataTypeFloat64>() },\n-          { \"creation_time\", std::make_shared<DataTypeDateTime>() },\n-          { \"source\", std::make_shared<DataTypeString>() },\n-          { \"last_exception\", std::make_shared<DataTypeString>() }\n-    }\n+    : name{name}\n {\n+    columns = NamesAndTypesList{\n+        { \"name\", std::make_shared<DataTypeString>() },\n+        { \"origin\", std::make_shared<DataTypeString>() },\n+        { \"type\", std::make_shared<DataTypeString>() },\n+        { \"key\", std::make_shared<DataTypeString>() },\n+        { \"attribute.names\", std::make_shared<DataTypeArray>(std::make_shared<DataTypeString>()) },\n+        { \"attribute.types\", std::make_shared<DataTypeArray>(std::make_shared<DataTypeString>()) },\n+        { \"bytes_allocated\", std::make_shared<DataTypeUInt64>() },\n+        { \"query_count\", std::make_shared<DataTypeUInt64>() },\n+        { \"hit_rate\", std::make_shared<DataTypeFloat64>() },\n+        { \"element_count\", std::make_shared<DataTypeUInt64>() },\n+        { \"load_factor\", std::make_shared<DataTypeFloat64>() },\n+        { \"creation_time\", std::make_shared<DataTypeDateTime>() },\n+        { \"source\", std::make_shared<DataTypeString>() },\n+        { \"last_exception\", std::make_shared<DataTypeString>() }\n+    };\n }\n \n \ndiff --git a/dbms/src/Storages/System/StorageSystemDictionaries.h b/dbms/src/Storages/System/StorageSystemDictionaries.h\nindex a27bfbd21a4a..57ac9b0b6eb9 100644\n--- a/dbms/src/Storages/System/StorageSystemDictionaries.h\n+++ b/dbms/src/Storages/System/StorageSystemDictionaries.h\n@@ -26,9 +26,6 @@ class StorageSystemDictionaries : public ext::shared_ptr_helper<StorageSystemDic\n \n private:\n     const std::string name;\n-    const NamesAndTypesList columns;\n-\n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n \n protected:\n     StorageSystemDictionaries(const std::string & name);\ndiff --git a/dbms/src/Storages/System/StorageSystemEvents.cpp b/dbms/src/Storages/System/StorageSystemEvents.cpp\nindex 38f134fe356a..96a02d723327 100644\n--- a/dbms/src/Storages/System/StorageSystemEvents.cpp\n+++ b/dbms/src/Storages/System/StorageSystemEvents.cpp\n@@ -11,13 +11,13 @@ namespace DB\n \n \n StorageSystemEvents::StorageSystemEvents(const std::string & name_)\n-    : name(name_),\n-    columns\n+    : name(name_)\n+{\n+    columns = NamesAndTypesList\n     {\n         {\"event\", std::make_shared<DataTypeString>()},\n         {\"value\", std::make_shared<DataTypeUInt64>()}\n-    }\n-{\n+    };\n }\n \n \ndiff --git a/dbms/src/Storages/System/StorageSystemEvents.h b/dbms/src/Storages/System/StorageSystemEvents.h\nindex 4a0f2672cc1b..b987151e4007 100644\n--- a/dbms/src/Storages/System/StorageSystemEvents.h\n+++ b/dbms/src/Storages/System/StorageSystemEvents.h\n@@ -18,8 +18,6 @@ class StorageSystemEvents : public ext::shared_ptr_helper<StorageSystemEvents>,\n     std::string getName() const override { return \"SystemEvents\"; }\n     std::string getTableName() const override { return name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     BlockInputStreams read(\n         const Names & column_names,\n         const SelectQueryInfo & query_info,\n@@ -30,7 +28,6 @@ class StorageSystemEvents : public ext::shared_ptr_helper<StorageSystemEvents>,\n \n private:\n     const std::string name;\n-    NamesAndTypesList columns;\n \n protected:\n     StorageSystemEvents(const std::string & name_);\ndiff --git a/dbms/src/Storages/System/StorageSystemFunctions.cpp b/dbms/src/Storages/System/StorageSystemFunctions.cpp\nindex aa9d7da9b1b3..74e58338d40f 100644\n--- a/dbms/src/Storages/System/StorageSystemFunctions.cpp\n+++ b/dbms/src/Storages/System/StorageSystemFunctions.cpp\n@@ -15,11 +15,11 @@ namespace DB\n \n StorageSystemFunctions::StorageSystemFunctions(const std::string & name_)\n     : name(name_)\n-    , columns{\n+{\n+    columns = NamesAndTypesList{\n         { \"name\",         std::make_shared<DataTypeString>() },\n         { \"is_aggregate\", std::make_shared<DataTypeUInt8>()  }\n-    }\n-{\n+    };\n }\n \n \ndiff --git a/dbms/src/Storages/System/StorageSystemFunctions.h b/dbms/src/Storages/System/StorageSystemFunctions.h\nindex c60515b92f3a..f77b95364538 100644\n--- a/dbms/src/Storages/System/StorageSystemFunctions.h\n+++ b/dbms/src/Storages/System/StorageSystemFunctions.h\n@@ -18,7 +18,6 @@ class StorageSystemFunctions : public ext::shared_ptr_helper<StorageSystemFuncti\n public:\n     std::string getName() const override { return \"SystemFunctions\"; }\n     std::string getTableName() const override { return name; }\n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n \n     BlockInputStreams read(\n         const Names & column_names,\n@@ -33,7 +32,6 @@ class StorageSystemFunctions : public ext::shared_ptr_helper<StorageSystemFuncti\n \n private:\n     const std::string name;\n-    NamesAndTypesList columns;\n };\n \n }\ndiff --git a/dbms/src/Storages/System/StorageSystemGraphite.cpp b/dbms/src/Storages/System/StorageSystemGraphite.cpp\nindex e57d31434997..76f3defe8852 100644\n--- a/dbms/src/Storages/System/StorageSystemGraphite.cpp\n+++ b/dbms/src/Storages/System/StorageSystemGraphite.cpp\n@@ -124,16 +124,16 @@ static Strings getAllGraphiteSections(const AbstractConfiguration & config)\n \n StorageSystemGraphite::StorageSystemGraphite(const std::string & name_)\n     : name(name_)\n-    , columns\n-    {\n+{\n+    columns = NamesAndTypesList{\n         {\"config_name\", std::make_shared<DataTypeString>()},\n         {\"regexp\",      std::make_shared<DataTypeString>()},\n         {\"function\",    std::make_shared<DataTypeString>()},\n         {\"age\",         std::make_shared<DataTypeUInt64>()},\n         {\"precision\",   std::make_shared<DataTypeUInt64>()},\n         {\"priority\",    std::make_shared<DataTypeUInt16>()},\n-        {\"is_default\",  std::make_shared<DataTypeUInt8>()}}\n-{\n+        {\"is_default\",  std::make_shared<DataTypeUInt8>()}\n+    };\n }\n \n \ndiff --git a/dbms/src/Storages/System/StorageSystemGraphite.h b/dbms/src/Storages/System/StorageSystemGraphite.h\nindex 4d4f930679fe..8c7a625de540 100644\n--- a/dbms/src/Storages/System/StorageSystemGraphite.h\n+++ b/dbms/src/Storages/System/StorageSystemGraphite.h\n@@ -12,7 +12,6 @@ class StorageSystemGraphite : public ext::shared_ptr_helper<StorageSystemGraphit\n public:\n     std::string getName() const override { return \"SystemGraphite\"; }\n     std::string getTableName() const override { return name; }\n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n \n     BlockInputStreams read(\n         const Names & column_names,\n@@ -24,7 +23,6 @@ class StorageSystemGraphite : public ext::shared_ptr_helper<StorageSystemGraphit\n \n private:\n     const std::string name;\n-    NamesAndTypesList columns;\n \n protected:\n     StorageSystemGraphite(const std::string & name_);\ndiff --git a/dbms/src/Storages/System/StorageSystemMerges.cpp b/dbms/src/Storages/System/StorageSystemMerges.cpp\nindex 0071f4794ae8..f6667c199081 100644\n--- a/dbms/src/Storages/System/StorageSystemMerges.cpp\n+++ b/dbms/src/Storages/System/StorageSystemMerges.cpp\n@@ -13,7 +13,8 @@ namespace DB\n \n StorageSystemMerges::StorageSystemMerges(const std::string & name)\n     : name{name}\n-    , columns{\n+{\n+    columns = NamesAndTypesList{\n         { \"database\",                    std::make_shared<DataTypeString>() },\n         { \"table\",                       std::make_shared<DataTypeString>() },\n         { \"elapsed\",                     std::make_shared<DataTypeFloat64>() },\n@@ -30,8 +31,7 @@ StorageSystemMerges::StorageSystemMerges(const std::string & name)\n         { \"columns_written\",             std::make_shared<DataTypeUInt64>() },\n         { \"memory_usage\",                std::make_shared<DataTypeUInt64>() },\n         { \"thread_number\",               std::make_shared<DataTypeUInt64>() },\n-    }\n-{\n+    };\n }\n \n \ndiff --git a/dbms/src/Storages/System/StorageSystemMerges.h b/dbms/src/Storages/System/StorageSystemMerges.h\nindex d4925c1e821c..d48c97bfa17f 100644\n--- a/dbms/src/Storages/System/StorageSystemMerges.h\n+++ b/dbms/src/Storages/System/StorageSystemMerges.h\n@@ -16,7 +16,6 @@ class StorageSystemMerges : public ext::shared_ptr_helper<StorageSystemMerges>,\n     std::string getName() const override { return \"SystemMerges\"; }\n     std::string getTableName() const override { return name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n     BlockInputStreams read(\n         const Names & column_names,\n         const SelectQueryInfo & query_info,\n@@ -27,7 +26,6 @@ class StorageSystemMerges : public ext::shared_ptr_helper<StorageSystemMerges>,\n \n private:\n     const std::string name;\n-    NamesAndTypesList columns;\n \n protected:\n     StorageSystemMerges(const std::string & name);\ndiff --git a/dbms/src/Storages/System/StorageSystemMetrics.cpp b/dbms/src/Storages/System/StorageSystemMetrics.cpp\nindex eb3aab1df15c..c60d7f4b4f5d 100644\n--- a/dbms/src/Storages/System/StorageSystemMetrics.cpp\n+++ b/dbms/src/Storages/System/StorageSystemMetrics.cpp\n@@ -12,13 +12,12 @@ namespace DB\n \n \n StorageSystemMetrics::StorageSystemMetrics(const std::string & name_)\n-    : name(name_),\n-    columns\n-    {\n+    : name(name_)\n+{\n+    columns = NamesAndTypesList{\n         {\"metric\", std::make_shared<DataTypeString>()},\n         {\"value\",  std::make_shared<DataTypeInt64>()},\n-    }\n-{\n+    };\n }\n \n \ndiff --git a/dbms/src/Storages/System/StorageSystemMetrics.h b/dbms/src/Storages/System/StorageSystemMetrics.h\nindex 0dac0d56f920..7b6058de9e5b 100644\n--- a/dbms/src/Storages/System/StorageSystemMetrics.h\n+++ b/dbms/src/Storages/System/StorageSystemMetrics.h\n@@ -18,8 +18,6 @@ class StorageSystemMetrics : public ext::shared_ptr_helper<StorageSystemMetrics>\n     std::string getName() const override { return \"SystemMetrics\"; }\n     std::string getTableName() const override { return name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     BlockInputStreams read(\n         const Names & column_names,\n         const SelectQueryInfo & query_info,\n@@ -30,7 +28,6 @@ class StorageSystemMetrics : public ext::shared_ptr_helper<StorageSystemMetrics>\n \n private:\n     const std::string name;\n-    NamesAndTypesList columns;\n \n protected:\n     StorageSystemMetrics(const std::string & name_);\ndiff --git a/dbms/src/Storages/System/StorageSystemModels.cpp b/dbms/src/Storages/System/StorageSystemModels.cpp\nindex e0f1e666ad18..da0249b17059 100644\n--- a/dbms/src/Storages/System/StorageSystemModels.cpp\n+++ b/dbms/src/Storages/System/StorageSystemModels.cpp\n@@ -12,15 +12,15 @@ namespace DB\n {\n \n StorageSystemModels::StorageSystemModels(const std::string & name)\n-    : name{name},\n-    columns{\n+    : name{name}\n+{\n+    columns = NamesAndTypesList{\n         { \"name\", std::make_shared<DataTypeString>() },\n         { \"origin\", std::make_shared<DataTypeString>() },\n         { \"type\", std::make_shared<DataTypeString>() },\n         { \"creation_time\", std::make_shared<DataTypeDateTime>() },\n         { \"last_exception\", std::make_shared<DataTypeString>() }\n-    }\n-{\n+    };\n }\n \n \ndiff --git a/dbms/src/Storages/System/StorageSystemModels.h b/dbms/src/Storages/System/StorageSystemModels.h\nindex 6827043f11c4..b32c5a804ce4 100644\n--- a/dbms/src/Storages/System/StorageSystemModels.h\n+++ b/dbms/src/Storages/System/StorageSystemModels.h\n@@ -26,9 +26,6 @@ class StorageSystemModels : public ext::shared_ptr_helper<StorageSystemModels>,\n \n private:\n     const std::string name;\n-    const NamesAndTypesList columns;\n-\n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n \n protected:\n     StorageSystemModels(const std::string & name);\ndiff --git a/dbms/src/Storages/System/StorageSystemNumbers.cpp b/dbms/src/Storages/System/StorageSystemNumbers.cpp\nindex ff5c4206cd19..4a9e85049e76 100644\n--- a/dbms/src/Storages/System/StorageSystemNumbers.cpp\n+++ b/dbms/src/Storages/System/StorageSystemNumbers.cpp\n@@ -41,8 +41,9 @@ class NumbersBlockInputStream : public IProfilingBlockInputStream\n \n \n StorageSystemNumbers::StorageSystemNumbers(const std::string & name_, bool multithreaded_, size_t limit_)\n-    : name(name_), columns{{\"number\", std::make_shared<DataTypeUInt64>()}}, multithreaded(multithreaded_), limit(limit_)\n+    : name(name_), multithreaded(multithreaded_), limit(limit_)\n {\n+    columns = NamesAndTypesList{{\"number\", std::make_shared<DataTypeUInt64>()}};\n }\n \n \ndiff --git a/dbms/src/Storages/System/StorageSystemNumbers.h b/dbms/src/Storages/System/StorageSystemNumbers.h\nindex 37ef809736f6..e0769fb39682 100644\n--- a/dbms/src/Storages/System/StorageSystemNumbers.h\n+++ b/dbms/src/Storages/System/StorageSystemNumbers.h\n@@ -25,8 +25,6 @@ class StorageSystemNumbers : public ext::shared_ptr_helper<StorageSystemNumbers>\n     std::string getName() const override { return \"SystemNumbers\"; }\n     std::string getTableName() const override { return name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     BlockInputStreams read(\n         const Names & column_names,\n         const SelectQueryInfo & query_info,\n@@ -37,7 +35,6 @@ class StorageSystemNumbers : public ext::shared_ptr_helper<StorageSystemNumbers>\n \n private:\n     const std::string name;\n-    NamesAndTypesList columns;\n     bool multithreaded;\n     size_t limit;\n \ndiff --git a/dbms/src/Storages/System/StorageSystemOne.cpp b/dbms/src/Storages/System/StorageSystemOne.cpp\nindex 58e3e5bfeddd..4ddc64031e3b 100644\n--- a/dbms/src/Storages/System/StorageSystemOne.cpp\n+++ b/dbms/src/Storages/System/StorageSystemOne.cpp\n@@ -11,8 +11,9 @@ namespace DB\n \n \n StorageSystemOne::StorageSystemOne(const std::string & name_)\n-    : name(name_), columns{{\"dummy\", std::make_shared<DataTypeUInt8>()}}\n+    : name(name_)\n {\n+    columns = NamesAndTypesList{{\"dummy\", std::make_shared<DataTypeUInt8>()}};\n }\n \n \ndiff --git a/dbms/src/Storages/System/StorageSystemOne.h b/dbms/src/Storages/System/StorageSystemOne.h\nindex ad3ba3c7a929..346f7a869824 100644\n--- a/dbms/src/Storages/System/StorageSystemOne.h\n+++ b/dbms/src/Storages/System/StorageSystemOne.h\n@@ -21,8 +21,6 @@ class StorageSystemOne : public ext::shared_ptr_helper<StorageSystemOne>, public\n     std::string getName() const override { return \"SystemOne\"; }\n     std::string getTableName() const override { return name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     BlockInputStreams read(\n         const Names & column_names,\n         const SelectQueryInfo & query_info,\n@@ -33,7 +31,6 @@ class StorageSystemOne : public ext::shared_ptr_helper<StorageSystemOne>, public\n \n private:\n     const std::string name;\n-    NamesAndTypesList columns;\n \n protected:\n     StorageSystemOne(const std::string & name_);\ndiff --git a/dbms/src/Storages/System/StorageSystemPartsBase.cpp b/dbms/src/Storages/System/StorageSystemPartsBase.cpp\nindex 30281cc0960b..b185cca8be22 100644\n--- a/dbms/src/Storages/System/StorageSystemPartsBase.cpp\n+++ b/dbms/src/Storages/System/StorageSystemPartsBase.cpp\n@@ -277,4 +277,10 @@ bool StorageSystemPartsBase::hasColumn(const String & column_name) const\n     return ITableDeclaration::hasColumn(column_name);\n }\n \n+StorageSystemPartsBase::StorageSystemPartsBase(std::string name_, NamesAndTypesList && columns_)\n+    : name(std::move(name_))\n+{\n+    columns = columns_;\n+}\n+\n }\ndiff --git a/dbms/src/Storages/System/StorageSystemPartsBase.h b/dbms/src/Storages/System/StorageSystemPartsBase.h\nindex 4079f7a8e634..4ff9c7e48991 100644\n--- a/dbms/src/Storages/System/StorageSystemPartsBase.h\n+++ b/dbms/src/Storages/System/StorageSystemPartsBase.h\n@@ -18,8 +18,6 @@ class StorageSystemPartsBase : public IStorage\n public:\n     std::string getTableName() const override { return name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     NameAndTypePair getColumn(const String & column_name) const override;\n \n     bool hasColumn(const String & column_name) const override;\n@@ -50,12 +48,12 @@ class StorageSystemPartsBase : public IStorage\n \n private:\n     const std::string name;\n-    NamesAndTypesList columns;\n+\n \n     bool hasStateColumn(const Names & column_names);\n \n protected:\n-    StorageSystemPartsBase(std::string name_, NamesAndTypesList && columns) : name(std::move(name_)), columns(columns) {}\n+    StorageSystemPartsBase(std::string name_, NamesAndTypesList && columns_);\n \n     virtual void processNextStorage(MutableColumns & columns, const StoragesInfo & info, bool has_state_column) = 0;\n };\ndiff --git a/dbms/src/Storages/System/StorageSystemProcesses.cpp b/dbms/src/Storages/System/StorageSystemProcesses.cpp\nindex 82a0ef229749..af02417af4f1 100644\n--- a/dbms/src/Storages/System/StorageSystemProcesses.cpp\n+++ b/dbms/src/Storages/System/StorageSystemProcesses.cpp\n@@ -13,7 +13,8 @@ namespace DB\n \n StorageSystemProcesses::StorageSystemProcesses(const std::string & name_)\n     : name(name_)\n-    , columns{\n+{\n+    columns = NamesAndTypesList{\n         { \"is_initial_query\",     std::make_shared<DataTypeUInt8>() },\n \n         { \"user\",                 std::make_shared<DataTypeString>() },\n@@ -48,8 +49,7 @@ StorageSystemProcesses::StorageSystemProcesses(const std::string & name_)\n         { \"written_bytes\",        std::make_shared<DataTypeUInt64>() },\n         { \"memory_usage\",         std::make_shared<DataTypeInt64>() },\n         { \"query\",                std::make_shared<DataTypeString>() }\n-    }\n-{\n+    };\n }\n \n \ndiff --git a/dbms/src/Storages/System/StorageSystemProcesses.h b/dbms/src/Storages/System/StorageSystemProcesses.h\nindex 73da8cfc1705..f8f26d13d358 100644\n--- a/dbms/src/Storages/System/StorageSystemProcesses.h\n+++ b/dbms/src/Storages/System/StorageSystemProcesses.h\n@@ -18,8 +18,6 @@ class StorageSystemProcesses : public ext::shared_ptr_helper<StorageSystemProces\n     std::string getName() const override { return \"SystemProcesses\"; }\n     std::string getTableName() const override { return name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     BlockInputStreams read(\n         const Names & column_names,\n         const SelectQueryInfo & query_info,\n@@ -30,7 +28,6 @@ class StorageSystemProcesses : public ext::shared_ptr_helper<StorageSystemProces\n \n private:\n     const std::string name;\n-    NamesAndTypesList columns;\n \n protected:\n     StorageSystemProcesses(const std::string & name_);\ndiff --git a/dbms/src/Storages/System/StorageSystemReplicas.cpp b/dbms/src/Storages/System/StorageSystemReplicas.cpp\nindex cbfc0a8ef812..6dc11214b7ce 100644\n--- a/dbms/src/Storages/System/StorageSystemReplicas.cpp\n+++ b/dbms/src/Storages/System/StorageSystemReplicas.cpp\n@@ -16,7 +16,8 @@ namespace DB\n \n StorageSystemReplicas::StorageSystemReplicas(const std::string & name_)\n     : name(name_)\n-    , columns{\n+{\n+    columns = NamesAndTypesList{\n         { \"database\",                             std::make_shared<DataTypeString>()   },\n         { \"table\",                                std::make_shared<DataTypeString>()   },\n         { \"engine\",                               std::make_shared<DataTypeString>()   },\n@@ -43,8 +44,7 @@ StorageSystemReplicas::StorageSystemReplicas(const std::string & name_)\n         { \"absolute_delay\",                       std::make_shared<DataTypeUInt64>()   },\n         { \"total_replicas\",                       std::make_shared<DataTypeUInt8>()    },\n         { \"active_replicas\",                      std::make_shared<DataTypeUInt8>()    },\n-    }\n-{\n+    };\n }\n \n \ndiff --git a/dbms/src/Storages/System/StorageSystemReplicas.h b/dbms/src/Storages/System/StorageSystemReplicas.h\nindex 0140e94f9df7..465b6baa5819 100644\n--- a/dbms/src/Storages/System/StorageSystemReplicas.h\n+++ b/dbms/src/Storages/System/StorageSystemReplicas.h\n@@ -18,8 +18,6 @@ class StorageSystemReplicas : public ext::shared_ptr_helper<StorageSystemReplica\n     std::string getName() const override { return \"SystemReplicas\"; }\n     std::string getTableName() const override { return name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     BlockInputStreams read(\n         const Names & column_names,\n         const SelectQueryInfo & query_info,\n@@ -30,7 +28,6 @@ class StorageSystemReplicas : public ext::shared_ptr_helper<StorageSystemReplica\n \n private:\n     const std::string name;\n-    NamesAndTypesList columns;\n \n protected:\n     StorageSystemReplicas(const std::string & name_);\ndiff --git a/dbms/src/Storages/System/StorageSystemReplicationQueue.cpp b/dbms/src/Storages/System/StorageSystemReplicationQueue.cpp\nindex 88683c742af0..a24325250444 100644\n--- a/dbms/src/Storages/System/StorageSystemReplicationQueue.cpp\n+++ b/dbms/src/Storages/System/StorageSystemReplicationQueue.cpp\n@@ -19,7 +19,8 @@ namespace DB\n \n StorageSystemReplicationQueue::StorageSystemReplicationQueue(const std::string & name_)\n     : name(name_)\n-    , columns{\n+{\n+    columns = NamesAndTypesList{\n         /// Table properties.\n         { \"database\",                std::make_shared<DataTypeString>() },\n         { \"table\",                   std::make_shared<DataTypeString>() },\n@@ -42,8 +43,7 @@ StorageSystemReplicationQueue::StorageSystemReplicationQueue(const std::string &\n         { \"num_postponed\",           std::make_shared<DataTypeUInt32>() },\n         { \"postpone_reason\",         std::make_shared<DataTypeString>() },\n         { \"last_postpone_time\",      std::make_shared<DataTypeDateTime>() },\n-    }\n-{\n+    };\n }\n \n \ndiff --git a/dbms/src/Storages/System/StorageSystemReplicationQueue.h b/dbms/src/Storages/System/StorageSystemReplicationQueue.h\nindex 369ace0ec366..8554361e0dfa 100644\n--- a/dbms/src/Storages/System/StorageSystemReplicationQueue.h\n+++ b/dbms/src/Storages/System/StorageSystemReplicationQueue.h\n@@ -18,8 +18,6 @@ class StorageSystemReplicationQueue : public ext::shared_ptr_helper<StorageSyste\n     std::string getName() const override { return \"SystemReplicationQueue\"; }\n     std::string getTableName() const override { return name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     BlockInputStreams read(\n         const Names & column_names,\n         const SelectQueryInfo & query_info,\n@@ -30,7 +28,6 @@ class StorageSystemReplicationQueue : public ext::shared_ptr_helper<StorageSyste\n \n private:\n     const std::string name;\n-    NamesAndTypesList columns;\n \n protected:\n     StorageSystemReplicationQueue(const std::string & name_);\ndiff --git a/dbms/src/Storages/System/StorageSystemSettings.cpp b/dbms/src/Storages/System/StorageSystemSettings.cpp\nindex 699c8983ccab..58c3eb504b9a 100644\n--- a/dbms/src/Storages/System/StorageSystemSettings.cpp\n+++ b/dbms/src/Storages/System/StorageSystemSettings.cpp\n@@ -13,13 +13,13 @@ namespace DB\n \n StorageSystemSettings::StorageSystemSettings(const std::string & name_)\n     : name(name_)\n-    , columns{\n+{\n+    columns = NamesAndTypesList{\n         { \"name\",        std::make_shared<DataTypeString>() },\n         { \"value\",       std::make_shared<DataTypeString>() },\n         { \"changed\",     std::make_shared<DataTypeUInt8>() },\n-        { \"description\", std::make_shared<DataTypeString>() },\n-    }\n-{\n+        { \"description\", std::make_shared<DataTypeString>() }\n+    };\n }\n \n \ndiff --git a/dbms/src/Storages/System/StorageSystemSettings.h b/dbms/src/Storages/System/StorageSystemSettings.h\nindex f48611c2519d..153b9213ef8c 100644\n--- a/dbms/src/Storages/System/StorageSystemSettings.h\n+++ b/dbms/src/Storages/System/StorageSystemSettings.h\n@@ -18,8 +18,6 @@ class StorageSystemSettings : public ext::shared_ptr_helper<StorageSystemSetting\n     std::string getName() const override { return \"SystemSettings\"; }\n     std::string getTableName() const override { return name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     BlockInputStreams read(\n         const Names & column_names,\n         const SelectQueryInfo & query_info,\n@@ -30,7 +28,6 @@ class StorageSystemSettings : public ext::shared_ptr_helper<StorageSystemSetting\n \n private:\n     const std::string name;\n-    NamesAndTypesList columns;\n \n protected:\n     StorageSystemSettings(const std::string & name_);\ndiff --git a/dbms/src/Storages/System/StorageSystemTables.cpp b/dbms/src/Storages/System/StorageSystemTables.cpp\nindex c4eac3d7ec7a..b40cf29498f9 100644\n--- a/dbms/src/Storages/System/StorageSystemTables.cpp\n+++ b/dbms/src/Storages/System/StorageSystemTables.cpp\n@@ -13,15 +13,14 @@ namespace DB\n {\n \n StorageSystemTables::StorageSystemTables(const std::string & name_)\n-    : name(name_),\n-    columns\n-    {\n+    : name(name_)\n+{\n+    columns = NamesAndTypesList{\n         {\"database\", std::make_shared<DataTypeString>()},\n         {\"name\", std::make_shared<DataTypeString>()},\n         {\"engine\", std::make_shared<DataTypeString>()},\n         {\"metadata_modification_time\", std::make_shared<DataTypeDateTime>()}\n-    }\n-{\n+    };\n }\n \n \ndiff --git a/dbms/src/Storages/System/StorageSystemTables.h b/dbms/src/Storages/System/StorageSystemTables.h\nindex 3dd66e0f409c..d66ca54be683 100644\n--- a/dbms/src/Storages/System/StorageSystemTables.h\n+++ b/dbms/src/Storages/System/StorageSystemTables.h\n@@ -18,8 +18,6 @@ class StorageSystemTables : public ext::shared_ptr_helper<StorageSystemTables>,\n     std::string getName() const override { return \"SystemTables\"; }\n     std::string getTableName() const override { return name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     BlockInputStreams read(\n         const Names & column_names,\n         const SelectQueryInfo & query_info,\n@@ -30,7 +28,6 @@ class StorageSystemTables : public ext::shared_ptr_helper<StorageSystemTables>,\n \n private:\n     const std::string name;\n-    NamesAndTypesList columns;\n \n protected:\n     StorageSystemTables(const std::string & name_);\ndiff --git a/dbms/src/Storages/System/StorageSystemZooKeeper.cpp b/dbms/src/Storages/System/StorageSystemZooKeeper.cpp\nindex 7f7dfa1aa422..80ca3bc2f0e5 100644\n--- a/dbms/src/Storages/System/StorageSystemZooKeeper.cpp\n+++ b/dbms/src/Storages/System/StorageSystemZooKeeper.cpp\n@@ -21,7 +21,8 @@ namespace DB\n \n StorageSystemZooKeeper::StorageSystemZooKeeper(const std::string & name_)\n     : name(name_)\n-    , columns{\n+{\n+    columns = NamesAndTypesList{\n         { \"name\",           std::make_shared<DataTypeString>() },\n         { \"value\",          std::make_shared<DataTypeString>() },\n         { \"czxid\",          std::make_shared<DataTypeInt64>() },\n@@ -36,8 +37,7 @@ StorageSystemZooKeeper::StorageSystemZooKeeper(const std::string & name_)\n         { \"numChildren\",    std::make_shared<DataTypeInt32>() },\n         { \"pzxid\",          std::make_shared<DataTypeInt64>() },\n         { \"path\",           std::make_shared<DataTypeString>() },\n-    }\n-{\n+    };\n }\n \n \ndiff --git a/dbms/src/Storages/System/StorageSystemZooKeeper.h b/dbms/src/Storages/System/StorageSystemZooKeeper.h\nindex 72913ab8beb7..45625ebab128 100644\n--- a/dbms/src/Storages/System/StorageSystemZooKeeper.h\n+++ b/dbms/src/Storages/System/StorageSystemZooKeeper.h\n@@ -18,8 +18,6 @@ class StorageSystemZooKeeper : public ext::shared_ptr_helper<StorageSystemZooKee\n     std::string getName() const override { return \"SystemZooKeeper\"; }\n     std::string getTableName() const override { return name; }\n \n-    const NamesAndTypesList & getColumnsListImpl() const override { return columns; }\n-\n     BlockInputStreams read(\n         const Names & column_names,\n         const SelectQueryInfo & query_info,\n@@ -30,7 +28,6 @@ class StorageSystemZooKeeper : public ext::shared_ptr_helper<StorageSystemZooKee\n \n private:\n     const std::string name;\n-    NamesAndTypesList columns;\n \n protected:\n     StorageSystemZooKeeper(const std::string & name_);\n",
  "test_patch": "diff --git a/dbms/tests/integration/README.md b/dbms/tests/integration/README.md\nindex 94cfd29ef407..0f8f615b616d 100644\n--- a/dbms/tests/integration/README.md\n+++ b/dbms/tests/integration/README.md\n@@ -14,7 +14,7 @@ Don't use Docker from your system repository.\n \n * [pip](https://pypi.python.org/pypi/pip). To install: `sudo apt-get install python-pip`\n * [py.test](https://docs.pytest.org/) testing framework. To install: `sudo -H pip install pytest`\n-* [docker-compose](https://docs.docker.com/compose/) and additional python libraries. To install: `sudo -H pip install docker-compose docker dicttoxml`\n+* [docker-compose](https://docs.docker.com/compose/) and additional python libraries. To install: `sudo -H pip install docker-compose docker dicttoxml kazoo`\n \n If you want to run the tests under a non-privileged user, you must add this user to `docker` group: `sudo usermod -aG docker $USER` and re-login.\n (You must close all your sessions (for example, restart your computer))\ndiff --git a/dbms/tests/integration/helpers/cluster.py b/dbms/tests/integration/helpers/cluster.py\nindex b20749a08d3f..c27a0b94f5db 100644\n--- a/dbms/tests/integration/helpers/cluster.py\n+++ b/dbms/tests/integration/helpers/cluster.py\n@@ -10,6 +10,8 @@\n import errno\n from dicttoxml import dicttoxml\n import xml.dom.minidom\n+from kazoo.client import KazooClient\n+from kazoo.exceptions import KazooException\n \n import docker\n from docker.errors import ContainerError\n@@ -46,7 +48,7 @@ def __init__(self, base_path, name=None, base_configs_dir=None, server_bin_path=\n \n         self.base_cmd = ['docker-compose', '--project-directory', self.base_dir, '--project-name', self.project_name]\n         self.base_zookeeper_cmd = None\n-        self.pre_zookkeeper_commands = []\n+        self.pre_zookeeper_commands = []\n         self.instances = {}\n         self.with_zookeeper = False\n \n@@ -91,6 +93,12 @@ def get_instance_docker_id(self, instance_name):\n         return self.project_name + '_' + instance_name + '_1'\n \n \n+    def get_instance_ip(self, instance_name):\n+        docker_id = self.get_instance_docker_id(instance_name)\n+        handle = self.docker_client.containers.get(docker_id)\n+        return handle.attrs['NetworkSettings']['Networks'].values()[0]['IPAddress']\n+\n+\n     def start(self, destroy_dirs=True):\n         if self.is_up:\n             return\n@@ -113,17 +121,18 @@ def start(self, destroy_dirs=True):\n \n         if self.with_zookeeper and self.base_zookeeper_cmd:\n             subprocess.check_call(self.base_zookeeper_cmd + ['up', '-d', '--no-recreate'])\n-            for command in self.pre_zookkeeper_commands:\n-                self.run_zookeeper_client_command(command, repeats=5)\n+            for command in self.pre_zookeeper_commands:\n+                self.run_kazoo_commands_with_retries(command, repeats=5)\n+\n+        # Uncomment for debugging\n+        # print ' '.join(self.base_cmd + ['up', '--no-recreate'])\n \n         subprocess.check_call(self.base_cmd + ['up', '-d', '--no-recreate'])\n \n         start_deadline = time.time() + 20.0 # seconds\n         for instance in self.instances.itervalues():\n             instance.docker_client = self.docker_client\n-\n-            container = self.docker_client.containers.get(instance.docker_id)\n-            instance.ip_address = container.attrs['NetworkSettings']['Networks'].values()[0]['IPAddress']\n+            instance.ip_address = self.get_instance_ip(instance.name)\n \n             instance.wait_for_start(start_deadline)\n \n@@ -146,20 +155,26 @@ def shutdown(self, kill=True):\n             instance.client = None\n \n \n-    def run_zookeeper_client_command(self, command, zoo_node = 'zoo1', repeats=1, sleep_for=1):\n-        cli_cmd = 'zkCli.sh  ' + command\n-        zoo_name = self.get_instance_docker_id(zoo_node)\n-        network_mode = 'container:' + zoo_name\n-        for i in range(0, repeats - 1):\n+    def get_kazoo_client(self, zoo_instance_name):\n+        zk = KazooClient(hosts=self.get_instance_ip(zoo_instance_name))\n+        zk.start()\n+        return zk\n+\n+\n+    def run_kazoo_commands_with_retries(self, kazoo_callback, zoo_instance_name = 'zoo1', repeats=1, sleep_for=1):\n+        for i in range(repeats - 1):\n             try:\n-                return self.docker_client.containers.run('zookeeper', cli_cmd, remove=True, network_mode=network_mode)\n-            except ContainerError:\n+                kazoo_callback(self.get_kazoo_client(zoo_instance_name))\n+                return\n+            except KazooException as e:\n+                print repr(e)\n                 time.sleep(sleep_for)\n \n-        return self.docker_client.containers.run('zookeeper', cli_cmd, remove=True, network_mode=network_mode)\n+        kazoo_callback(self.get_kazoo_client(zoo_instance_name))\n+\n \n     def add_zookeeper_startup_command(self, command):\n-        self.pre_zookkeeper_commands.append(command)\n+        self.pre_zookeeper_commands.append(command)\n \n \n DOCKER_COMPOSE_TEMPLATE = '''\n@@ -224,11 +239,13 @@ def get_query_request(self, *args, **kwargs):\n \n     def exec_in_container(self, cmd, **kwargs):\n         container = self.get_docker_handle()\n-        handle = self.docker_client.api.exec_create(container.id, cmd, **kwargs)\n-        output = self.docker_client.api.exec_start(handle).decode('utf8')\n-        exit_code = self.docker_client.api.exec_inspect(handle)['ExitCode']\n+        exec_id = self.docker_client.api.exec_create(container.id, cmd, **kwargs)\n+        output = self.docker_client.api.exec_start(exec_id, detach=False)\n+\n+        output = output.decode('utf8')\n+        exit_code = self.docker_client.api.exec_inspect(exec_id)['ExitCode']\n         if exit_code:\n-            raise Exception('Cmd {} failed! Return code {}. Output {}'.format(' '.join(cmd), exit_code, output))\n+            raise Exception('Cmd \"{}\" failed! Return code {}. Output: {}'.format(' '.join(cmd), exit_code, output))\n         return output\n \n \ndiff --git a/dbms/tests/integration/test_cluster_copier/__init__.py b/dbms/tests/integration/test_cluster_copier/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/dbms/tests/integration/test_cluster_copier/configs/config.d/clusters.xml b/dbms/tests/integration/test_cluster_copier/configs/config.d/clusters.xml\nnew file mode 100644\nindex 000000000000..d956ae98f5a5\n--- /dev/null\n+++ b/dbms/tests/integration/test_cluster_copier/configs/config.d/clusters.xml\n@@ -0,0 +1,47 @@\n+<yandex>\n+<remote_servers>\n+\n+    <cluster0>\n+        <shard>\n+            <internal_replication>true</internal_replication>\n+            <replica>\n+                <host>s0_0_0</host>\n+                <port>9000</port>\n+            </replica>\n+            <replica>\n+                <host>s0_0_1</host>\n+                <port>9000</port>\n+            </replica>\n+        </shard>\n+        <shard>\n+            <internal_replication>true</internal_replication>\n+            <replica>\n+                <host>s0_1_0</host>\n+                <port>9000</port>\n+            </replica>\n+        </shard>\n+    </cluster0>\n+\n+    <cluster1>\n+        <shard>\n+            <internal_replication>true</internal_replication>\n+            <replica>\n+                <host>s1_0_0</host>\n+                <port>9000</port>\n+            </replica>\n+            <replica>\n+                <host>s1_0_1</host>\n+                <port>9000</port>\n+            </replica>\n+        </shard>\n+        <shard>\n+            <internal_replication>true</internal_replication>\n+            <replica>\n+                <host>s1_1_0</host>\n+                <port>9000</port>\n+            </replica>\n+        </shard>\n+    </cluster1>\n+\n+</remote_servers>\n+</yandex>\n\\ No newline at end of file\ndiff --git a/dbms/tests/integration/test_cluster_copier/configs/config.d/ddl.xml b/dbms/tests/integration/test_cluster_copier/configs/config.d/ddl.xml\nnew file mode 100644\nindex 000000000000..abad0dee450d\n--- /dev/null\n+++ b/dbms/tests/integration/test_cluster_copier/configs/config.d/ddl.xml\n@@ -0,0 +1,5 @@\n+<yandex>\n+    <distributed_ddl>\n+        <path>/clickhouse/task_queue/ddl</path>\n+    </distributed_ddl>\n+</yandex>\n\\ No newline at end of file\ndiff --git a/dbms/tests/integration/test_cluster_copier/configs/config.d/query_log.xml b/dbms/tests/integration/test_cluster_copier/configs/config.d/query_log.xml\nnew file mode 100644\nindex 000000000000..839ef92c6dcb\n--- /dev/null\n+++ b/dbms/tests/integration/test_cluster_copier/configs/config.d/query_log.xml\n@@ -0,0 +1,14 @@\n+<yandex>\n+    <!-- Query log. Used only for queries with setting log_queries = 1. -->\n+    <query_log>\n+        <!-- What table to insert data. If table is not exist, it will be created.\n+             When query log structure is changed after system update,\n+              then old table will be renamed and new table will be created automatically.\n+        -->\n+        <database>system</database>\n+        <table>query_log</table>\n+\n+        <!-- Interval of flushing data. -->\n+        <flush_interval_milliseconds>1000</flush_interval_milliseconds>\n+    </query_log>\n+</yandex>\n\\ No newline at end of file\ndiff --git a/dbms/tests/integration/test_cluster_copier/configs/users.xml b/dbms/tests/integration/test_cluster_copier/configs/users.xml\nnew file mode 100644\nindex 000000000000..3c739cc2cc45\n--- /dev/null\n+++ b/dbms/tests/integration/test_cluster_copier/configs/users.xml\n@@ -0,0 +1,24 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+    <profiles>\n+        <default>\n+            <log_queries>1</log_queries>\n+        </default>\n+    </profiles>\n+\n+    <users>\n+        <default>\n+            <password></password>\n+            <networks incl=\"networks\" replace=\"replace\">\n+                <ip>::/0</ip>\n+            </networks>\n+            <profile>default</profile>\n+            <quota>default</quota>\n+        </default>\n+    </users>\n+\n+    <quotas>\n+        <default>\n+        </default>\n+    </quotas>\n+</yandex>\ndiff --git a/dbms/tests/integration/test_cluster_copier/task0_description.xml b/dbms/tests/integration/test_cluster_copier/task0_description.xml\nnew file mode 100644\nindex 000000000000..b5560eeba959\n--- /dev/null\n+++ b/dbms/tests/integration/test_cluster_copier/task0_description.xml\n@@ -0,0 +1,88 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+\n+    <!-- Configuration of clusters -->\n+    <remote_servers>\n+    <cluster0>\n+        <shard>\n+            <internal_replication>true</internal_replication>\n+            <replica>\n+                <host>s0_0_0</host>\n+                <port>9000</port>\n+            </replica>\n+            <replica>\n+                <host>s0_0_1</host>\n+                <port>9000</port>\n+            </replica>\n+        </shard>\n+        <shard>\n+            <internal_replication>true</internal_replication>\n+            <replica>\n+                <host>s0_1_0</host>\n+                <port>9000</port>\n+            </replica>\n+        </shard>\n+    </cluster0>\n+\n+    <cluster1>\n+        <shard>\n+            <internal_replication>true</internal_replication>\n+            <replica>\n+                <host>s1_0_0</host>\n+                <port>9000</port>\n+            </replica>\n+            <replica>\n+                <host>s1_0_1</host>\n+                <port>9000</port>\n+            </replica>\n+        </shard>\n+        <shard>\n+            <internal_replication>true</internal_replication>\n+            <replica>\n+                <host>s1_1_0</host>\n+                <port>9000</port>\n+            </replica>\n+        </shard>\n+    </cluster1>\n+    </remote_servers>\n+\n+    <!-- How many simualteneous workers are posssible -->\n+    <max_workers>4</max_workers>\n+\n+    <!-- Common setting for pull and push operations -->\n+    <settings>\n+    </settings>\n+\n+    <!-- Setting used to fetch data -->\n+    <settings_pull>\n+    </settings_pull>\n+\n+    <!-- Setting used to insert data -->\n+    <settings_push>\n+    </settings_push>\n+\n+    <!-- Tasks -->\n+    <tables>\n+        <hits>\n+            <cluster_pull>cluster0</cluster_pull>\n+            <database_pull>default</database_pull>\n+            <table_pull>hits</table_pull>\n+\n+            <cluster_push>cluster1</cluster_push>\n+            <database_push>default</database_push>\n+            <table_push>hits</table_push>\n+\n+            <enabled_partitions> 0  1   2</enabled_partitions>\n+\n+            <!-- Engine of destination tables -->\n+            <engine>ENGINE=ReplicatedMergeTree('/clickhouse/tables/cluster{cluster}/{shard}', '{replica}') PARTITION BY d % 3 ORDER BY d SETTINGS index_granularity = 16</engine>\n+\n+            <!-- Which sarding key to use while copying -->\n+            <sharding_key>d + 1</sharding_key>\n+\n+            <!-- Optional expression that filter copying data -->\n+            <where_condition>d - d = 0</where_condition>\n+        </hits>\n+    </tables>\n+\n+</yandex>\n\\ No newline at end of file\ndiff --git a/dbms/tests/integration/test_cluster_copier/test.py b/dbms/tests/integration/test_cluster_copier/test.py\nnew file mode 100644\nindex 000000000000..683c723f83fe\n--- /dev/null\n+++ b/dbms/tests/integration/test_cluster_copier/test.py\n@@ -0,0 +1,145 @@\n+import os\n+import os.path as p\n+import sys\n+import time\n+import datetime\n+import pytest\n+from contextlib import contextmanager\n+import docker\n+from kazoo.client import KazooClient\n+\n+\n+CURRENT_TEST_DIR = os.path.dirname(os.path.abspath(__file__))\n+sys.path.insert(0, os.path.dirname(CURRENT_TEST_DIR))\n+from helpers.cluster import ClickHouseCluster\n+from helpers.test_tools import TSV\n+\n+COPYING_FAIL_PROBABILITY = 0.33\n+cluster = None\n+\n+\n+def check_all_hosts_sucesfully_executed(tsv_content, num_hosts):\n+    M = TSV.toMat(tsv_content)\n+    hosts = [(l[0], l[1]) for l in M] # (host, port)\n+    codes = [l[2] for l in M]\n+    messages = [l[3] for l in M]\n+\n+    assert len(hosts) == num_hosts and len(set(hosts)) == num_hosts, \"\\n\" + tsv_content\n+    assert len(set(codes)) == 1, \"\\n\" + tsv_content\n+    assert codes[0] == \"0\", \"\\n\" + tsv_content\n+\n+\n+def ddl_check_query(instance, query, num_hosts=3):\n+    contents = instance.query(query)\n+    check_all_hosts_sucesfully_executed(contents, num_hosts)\n+    return contents\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    global cluster\n+    try:\n+        clusters_schema = {\n+         \"0\" : {\n+            \"0\" : [\"0\", \"1\"],\n+            \"1\" : [\"0\"]\n+         },\n+         \"1\" : {\n+            \"0\" : [\"0\", \"1\"],\n+            \"1\" : [\"0\"]\n+         }\n+        }\n+\n+        cluster = ClickHouseCluster(__file__)\n+\n+        for cluster_name, shards in clusters_schema.iteritems():\n+            for shard_name, replicas in shards.iteritems():\n+                for replica_name in replicas:\n+                    name = \"s{}_{}_{}\".format(cluster_name, shard_name, replica_name)\n+                    cluster.add_instance(name,\n+                        config_dir=\"configs\",\n+                        macroses={\"cluster\": cluster_name, \"shard\": shard_name, \"replica\": replica_name},\n+                        with_zookeeper=True)\n+\n+        cluster.start()\n+        yield cluster\n+\n+    finally:\n+        pass\n+        cluster.shutdown()\n+\n+\n+def _test_copying(cmd_options):\n+    instance = cluster.instances['s0_0_0']\n+\n+    ddl_check_query(instance, \"DROP TABLE IF EXISTS hits ON CLUSTER cluster0\")\n+    ddl_check_query(instance, \"DROP TABLE IF EXISTS hits ON CLUSTER cluster1\")\n+\n+    ddl_check_query(instance, \"CREATE TABLE hits ON CLUSTER cluster0 (d UInt64) ENGINE=ReplicatedMergeTree('/clickhouse/tables/cluster_{cluster}/{shard}', '{replica}') PARTITION BY d % 3 ORDER BY d SETTINGS index_granularity = 16\")\n+    ddl_check_query(instance, \"CREATE TABLE hits_all ON CLUSTER cluster0 (d UInt64) ENGINE=Distributed(cluster0, default, hits, d)\")\n+    ddl_check_query(instance, \"CREATE TABLE hits_all ON CLUSTER cluster1 (d UInt64) ENGINE=Distributed(cluster1, default, hits, d + 1)\")\n+    instance.query(\"INSERT INTO hits_all SELECT * FROM system.numbers LIMIT 1002\")\n+\n+    zk = cluster.get_kazoo_client('zoo1')\n+    print \"Use ZooKeeper server: {}:{}\".format(zk.hosts[0][0], zk.hosts[0][1])\n+\n+    zk_task_path = \"/clickhouse-copier/task_simple\"\n+    zk.ensure_path(zk_task_path)\n+\n+    copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task0_description.xml'), 'r').read()\n+    zk.create(zk_task_path + \"/description\", copier_task_config)\n+\n+    # Run cluster-copier processes on each node\n+    docker_api = docker.from_env().api\n+    copiers_exec_ids = []\n+\n+    cmd = ['/usr/bin/clickhouse', 'copier',\n+        '--config', '/etc/clickhouse-server/config-preprocessed.xml',\n+        '--task-path', '/clickhouse-copier/task_simple',\n+        '--base-dir', '/var/log/clickhouse-server/copier']\n+    cmd += cmd_options\n+\n+    for instance_name, instance in cluster.instances.iteritems():\n+        container = instance.get_docker_handle()\n+        exec_id = docker_api.exec_create(container.id, cmd, stderr=True)\n+        docker_api.exec_start(exec_id, detach=True)\n+\n+        copiers_exec_ids.append(exec_id)\n+        print \"Copier for {} ({}) has started\".format(instance.name, instance.ip_address)\n+\n+    # Wait for copiers stopping and check their return codes\n+    for exec_id, instance in zip(copiers_exec_ids, cluster.instances.itervalues()):\n+        while True:\n+            res = docker_api.exec_inspect(exec_id)\n+            if not res['Running']:\n+                break\n+            time.sleep(1)\n+\n+        assert res['ExitCode'] == 0, \"Instance: {} ({}). Info: {}\".format(instance.name, instance.ip_address, repr(res))\n+\n+    assert TSV(cluster.instances['s0_0_0'].query(\"SELECT count() FROM hits_all\")) == TSV(\"1002\\n\")\n+    assert TSV(cluster.instances['s1_0_0'].query(\"SELECT count() FROM hits_all\")) == TSV(\"1002\\n\")\n+\n+    assert TSV(cluster.instances['s1_0_0'].query(\"SELECT DISTINCT d % 2 FROM hits\")) == TSV(\"1\\n\")\n+    assert TSV(cluster.instances['s1_1_0'].query(\"SELECT DISTINCT d % 2 FROM hits\")) == TSV(\"0\\n\")\n+\n+    zk.delete(zk_task_path, recursive=True)\n+    ddl_check_query(instance, \"DROP TABLE hits_all ON CLUSTER cluster0\")\n+    ddl_check_query(instance, \"DROP TABLE hits_all ON CLUSTER cluster1\")\n+    ddl_check_query(instance, \"DROP TABLE hits ON CLUSTER cluster0\")\n+    ddl_check_query(instance, \"DROP TABLE hits ON CLUSTER cluster1\")\n+\n+\n+def test_copy_simple(started_cluster):\n+    _test_copying([])\n+\n+\n+def test_copy_with_recovering(started_cluster):\n+    _test_copying(['--copy-fault-probability', str(COPYING_FAIL_PROBABILITY)])\n+\n+\n+if __name__ == '__main__':\n+    with contextmanager(started_cluster)() as cluster:\n+       for name, instance in cluster.instances.items():\n+           print name, instance.ip_address\n+       raw_input(\"Cluster created, press any key to destroy...\")\ndiff --git a/dbms/tests/integration/test_distributed_ddl/test.py b/dbms/tests/integration/test_distributed_ddl/test.py\nindex 09ae04583cd1..8b7e46443d55 100755\n--- a/dbms/tests/integration/test_distributed_ddl/test.py\n+++ b/dbms/tests/integration/test_distributed_ddl/test.py\n@@ -89,7 +89,6 @@ def init_cluster(cluster):\n         # Initialize databases and service tables\n         instance = cluster.instances['ch1']\n \n-        instance.query(\"SELECT 1\")\n         ddl_check_query(instance, \"\"\"\n CREATE TABLE IF NOT EXISTS all_tables ON CLUSTER 'cluster_no_replicas'\n     (database String, name String, engine String, metadata_modification_time DateTime)\n@@ -119,6 +118,8 @@ def started_cluster():\n         for instance in cluster.instances.values():\n             ddl_check_there_are_no_dublicates(instance)\n \n+        cluster.pm_random_drops.heal_all()\n+\n     finally:\n         cluster.shutdown()\n \ndiff --git a/dbms/tests/integration/test_zookeeper_config/test.py b/dbms/tests/integration/test_zookeeper_config/test.py\nindex d3962b147ef4..5d01c92b46a1 100644\n--- a/dbms/tests/integration/test_zookeeper_config/test.py\n+++ b/dbms/tests/integration/test_zookeeper_config/test.py\n@@ -1,3 +1,4 @@\n+from __future__ import print_function\n from helpers.cluster import ClickHouseCluster\n import pytest\n \n@@ -11,8 +12,10 @@ def test_chroot_with_same_root():\n     node2 = cluster_2.add_instance('node2', config_dir='configs', with_zookeeper=True)\n     nodes = [node1, node2]\n \n-    cluster_1.add_zookeeper_startup_command('create /root_a \"\"')\n-    cluster_1.add_zookeeper_startup_command('ls / ')\n+    def create_zk_root(zk):\n+        zk.ensure_path('/root_a')\n+        print(zk.get_children('/'))\n+    cluster_1.add_zookeeper_startup_command(create_zk_root)\n \n     try:\n         cluster_1.start()\n@@ -21,7 +24,7 @@ def test_chroot_with_same_root():\n             cluster_2.start(destroy_dirs=False)\n             for i, node in enumerate(nodes):\n                 node.query('''\n-                CREATE TABLE simple (date Date, id UInt32) \n+                CREATE TABLE simple (date Date, id UInt32)\n                 ENGINE = ReplicatedMergeTree('/clickhouse/tables/0/simple', '{replica}', date, id, 8192);\n                 '''.format(replica=node.name))\n                 node.query(\"INSERT INTO simple VALUES ({0}, {0})\".format(i))\n@@ -45,9 +48,11 @@ def test_chroot_with_different_root():\n     node2 = cluster_2.add_instance('node2', config_dir='configs', with_zookeeper=True)\n     nodes = [node1, node2]\n \n-    cluster_1.add_zookeeper_startup_command('create /root_a \"\"')\n-    cluster_1.add_zookeeper_startup_command('create /root_b \"\"')\n-    cluster_1.add_zookeeper_startup_command('ls / ')\n+    def create_zk_roots(zk):\n+        zk.ensure_path('/root_a')\n+        zk.ensure_path('/root_b')\n+        print(zk.get_children('/'))\n+    cluster_1.add_zookeeper_startup_command(create_zk_roots)\n \n     try:\n         cluster_1.start()\ndiff --git a/dbms/tests/queries/0_stateless/00563_insert_into_remote_and_zookeeper.reference b/dbms/tests/queries/0_stateless/00563_insert_into_remote_and_zookeeper.reference\nnew file mode 100644\nindex 000000000000..2ca3cd52fcf5\n--- /dev/null\n+++ b/dbms/tests/queries/0_stateless/00563_insert_into_remote_and_zookeeper.reference\n@@ -0,0 +1,3 @@\n+1\n+2\n+2\ndiff --git a/dbms/tests/queries/0_stateless/00563_insert_into_remote_and_zookeeper.sql b/dbms/tests/queries/0_stateless/00563_insert_into_remote_and_zookeeper.sql\nnew file mode 100644\nindex 000000000000..f29d897017a7\n--- /dev/null\n+++ b/dbms/tests/queries/0_stateless/00563_insert_into_remote_and_zookeeper.sql\n@@ -0,0 +1,14 @@\n+-- Check that settings are correctly passed through Distributed table\n+DROP TABLE IF EXISTS test.simple;\n+CREATE TABLE test.simple (d Int8) ENGINE = ReplicatedMergeTree('/clickhouse_test/tables/test/simple', '1') ORDER BY d;\n+\n+-- TODO: replace '127.0.0.2' -> '127.0.0.1' after a fix\n+INSERT INTO TABLE FUNCTION remote('127.0.0.2', 'test', 'simple') VALUES (1);\n+INSERT INTO TABLE FUNCTION remote('127.0.0.2', 'test', 'simple') VALUES (1);\n+\n+SET insert_deduplicate=0;\n+INSERT INTO TABLE FUNCTION remote('127.0.0.2', 'test', 'simple') VALUES (2);\n+INSERT INTO TABLE FUNCTION remote('127.0.0.2', 'test', 'simple') VALUES (2);\n+\n+SELECT * FROM remote('127.0.0.2', 'test', 'simple') ORDER BY d;\n+DROP TABLE test.simple;\n\\ No newline at end of file\n",
  "problem_statement": "when dropping a database the metadata file is not deleted\nHello,\r\n\r\nWhen you drop a database the sql file in /var/lib/clickhouse/metadata for that db is not deleted.\r\n\r\nIs this a bug ?\r\n\r\n\n",
  "hints_text": "",
  "created_at": "2018-01-11T20:52:32Z",
  "modified_files": [
    "dbms/src/Client/ConnectionPoolWithFailover.cpp",
    "dbms/src/Common/ZooKeeper/Types.h",
    "dbms/src/Common/getFQDNOrHostName.h",
    "dbms/src/Common/isLocalAddress.cpp",
    "dbms/src/Common/isLocalAddress.h",
    "dbms/src/DataStreams/SquashingBlockInputStream.cpp",
    "dbms/src/DataStreams/SquashingBlockInputStream.h",
    "dbms/src/DataStreams/copyData.cpp",
    "dbms/src/DataStreams/copyData.h",
    "dbms/src/Databases/DatabaseDictionary.cpp",
    "dbms/src/Databases/DatabaseDictionary.h",
    "dbms/src/Databases/DatabaseFactory.cpp",
    "dbms/src/Databases/DatabaseFactory.h",
    "dbms/src/Databases/DatabaseMemory.cpp",
    "dbms/src/Databases/DatabaseMemory.h",
    "dbms/src/Databases/DatabaseOrdinary.cpp",
    "dbms/src/Databases/DatabaseOrdinary.h",
    "dbms/src/Databases/IDatabase.h",
    "dbms/src/Interpreters/Cluster.cpp",
    "dbms/src/Interpreters/Cluster.h",
    "dbms/src/Interpreters/Context.cpp",
    "dbms/src/Interpreters/Context.h",
    "dbms/src/Interpreters/InterpreterCheckQuery.h",
    "dbms/src/Interpreters/InterpreterCreateQuery.cpp",
    "dbms/src/Interpreters/InterpreterDropQuery.cpp",
    "dbms/src/Interpreters/loadMetadata.cpp",
    "dbms/src/Parsers/ExpressionListParsers.cpp",
    "dbms/src/Parsers/parseQuery.cpp",
    "dbms/src/Parsers/parseQuery.h",
    "dbms/src/Server/CMakeLists.txt",
    "b/dbms/src/Server/ClusterCopier.cpp",
    "b/dbms/src/Server/ClusterCopier.h",
    "dbms/src/Server/LocalServer.cpp",
    "b/dbms/src/Server/clickhouse-cluster-copier.cpp",
    "dbms/src/Server/main.cpp",
    "dbms/src/Storages/Distributed/DistributedBlockOutputStream.cpp",
    "dbms/src/Storages/Distributed/DistributedBlockOutputStream.h",
    "dbms/src/Storages/ITableDeclaration.cpp",
    "dbms/src/Storages/ITableDeclaration.h",
    "dbms/src/Storages/MergeTree/MergeTreeData.cpp",
    "dbms/src/Storages/MergeTree/MergeTreeData.h",
    "dbms/src/Storages/StorageBuffer.cpp",
    "dbms/src/Storages/StorageBuffer.h",
    "dbms/src/Storages/StorageCatBoostPool.h",
    "dbms/src/Storages/StorageDictionary.cpp",
    "dbms/src/Storages/StorageDictionary.h",
    "dbms/src/Storages/StorageDistributed.cpp",
    "dbms/src/Storages/StorageDistributed.h",
    "dbms/src/Storages/StorageFile.cpp",
    "dbms/src/Storages/StorageFile.h",
    "dbms/src/Storages/StorageKafka.cpp",
    "dbms/src/Storages/StorageKafka.h",
    "dbms/src/Storages/StorageLog.cpp",
    "dbms/src/Storages/StorageLog.h",
    "dbms/src/Storages/StorageMaterializedView.cpp",
    "dbms/src/Storages/StorageMaterializedView.h",
    "dbms/src/Storages/StorageMemory.cpp",
    "dbms/src/Storages/StorageMemory.h",
    "dbms/src/Storages/StorageMerge.cpp",
    "dbms/src/Storages/StorageMerge.h",
    "dbms/src/Storages/StorageMergeTree.cpp",
    "dbms/src/Storages/StorageMySQL.cpp",
    "dbms/src/Storages/StorageMySQL.h",
    "dbms/src/Storages/StorageNull.h",
    "dbms/src/Storages/StorageODBC.cpp",
    "dbms/src/Storages/StorageODBC.h",
    "dbms/src/Storages/StorageReplicatedMergeTree.cpp",
    "dbms/src/Storages/StorageSet.cpp",
    "dbms/src/Storages/StorageSet.h",
    "dbms/src/Storages/StorageStripeLog.cpp",
    "dbms/src/Storages/StorageStripeLog.h",
    "dbms/src/Storages/StorageTinyLog.cpp",
    "dbms/src/Storages/StorageTinyLog.h",
    "dbms/src/Storages/StorageView.cpp",
    "dbms/src/Storages/StorageView.h",
    "dbms/src/Storages/System/StorageSystemAsynchronousMetrics.cpp",
    "dbms/src/Storages/System/StorageSystemAsynchronousMetrics.h",
    "dbms/src/Storages/System/StorageSystemBuildOptions.cpp",
    "dbms/src/Storages/System/StorageSystemBuildOptions.h",
    "dbms/src/Storages/System/StorageSystemClusters.cpp",
    "dbms/src/Storages/System/StorageSystemClusters.h",
    "dbms/src/Storages/System/StorageSystemColumns.cpp",
    "dbms/src/Storages/System/StorageSystemColumns.h",
    "dbms/src/Storages/System/StorageSystemDatabases.cpp",
    "dbms/src/Storages/System/StorageSystemDatabases.h",
    "dbms/src/Storages/System/StorageSystemDictionaries.cpp",
    "dbms/src/Storages/System/StorageSystemDictionaries.h",
    "dbms/src/Storages/System/StorageSystemEvents.cpp",
    "dbms/src/Storages/System/StorageSystemEvents.h",
    "dbms/src/Storages/System/StorageSystemFunctions.cpp",
    "dbms/src/Storages/System/StorageSystemFunctions.h",
    "dbms/src/Storages/System/StorageSystemGraphite.cpp",
    "dbms/src/Storages/System/StorageSystemGraphite.h",
    "dbms/src/Storages/System/StorageSystemMerges.cpp",
    "dbms/src/Storages/System/StorageSystemMerges.h",
    "dbms/src/Storages/System/StorageSystemMetrics.cpp",
    "dbms/src/Storages/System/StorageSystemMetrics.h",
    "dbms/src/Storages/System/StorageSystemModels.cpp",
    "dbms/src/Storages/System/StorageSystemModels.h",
    "dbms/src/Storages/System/StorageSystemNumbers.cpp",
    "dbms/src/Storages/System/StorageSystemNumbers.h",
    "dbms/src/Storages/System/StorageSystemOne.cpp",
    "dbms/src/Storages/System/StorageSystemOne.h",
    "dbms/src/Storages/System/StorageSystemPartsBase.cpp",
    "dbms/src/Storages/System/StorageSystemPartsBase.h",
    "dbms/src/Storages/System/StorageSystemProcesses.cpp",
    "dbms/src/Storages/System/StorageSystemProcesses.h",
    "dbms/src/Storages/System/StorageSystemReplicas.cpp",
    "dbms/src/Storages/System/StorageSystemReplicas.h",
    "dbms/src/Storages/System/StorageSystemReplicationQueue.cpp",
    "dbms/src/Storages/System/StorageSystemReplicationQueue.h",
    "dbms/src/Storages/System/StorageSystemSettings.cpp",
    "dbms/src/Storages/System/StorageSystemSettings.h",
    "dbms/src/Storages/System/StorageSystemTables.cpp",
    "dbms/src/Storages/System/StorageSystemTables.h",
    "dbms/src/Storages/System/StorageSystemZooKeeper.cpp",
    "dbms/src/Storages/System/StorageSystemZooKeeper.h"
  ],
  "modified_test_files": [
    "dbms/tests/integration/README.md",
    "dbms/tests/integration/helpers/cluster.py",
    "b/dbms/tests/integration/test_cluster_copier/configs/config.d/clusters.xml",
    "b/dbms/tests/integration/test_cluster_copier/configs/config.d/ddl.xml",
    "b/dbms/tests/integration/test_cluster_copier/configs/config.d/query_log.xml",
    "b/dbms/tests/integration/test_cluster_copier/configs/users.xml",
    "b/dbms/tests/integration/test_cluster_copier/task0_description.xml",
    "b/dbms/tests/integration/test_cluster_copier/test.py",
    "dbms/tests/integration/test_distributed_ddl/test.py",
    "dbms/tests/integration/test_zookeeper_config/test.py",
    "b/dbms/tests/queries/0_stateless/00563_insert_into_remote_and_zookeeper.reference",
    "b/dbms/tests/queries/0_stateless/00563_insert_into_remote_and_zookeeper.sql"
  ]
}