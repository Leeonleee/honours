diff --git a/dbms/tests/integration/README.md b/dbms/tests/integration/README.md
index 94cfd29ef407..0f8f615b616d 100644
--- a/dbms/tests/integration/README.md
+++ b/dbms/tests/integration/README.md
@@ -14,7 +14,7 @@ Don't use Docker from your system repository.
 
 * [pip](https://pypi.python.org/pypi/pip). To install: `sudo apt-get install python-pip`
 * [py.test](https://docs.pytest.org/) testing framework. To install: `sudo -H pip install pytest`
-* [docker-compose](https://docs.docker.com/compose/) and additional python libraries. To install: `sudo -H pip install docker-compose docker dicttoxml`
+* [docker-compose](https://docs.docker.com/compose/) and additional python libraries. To install: `sudo -H pip install docker-compose docker dicttoxml kazoo`
 
 If you want to run the tests under a non-privileged user, you must add this user to `docker` group: `sudo usermod -aG docker $USER` and re-login.
 (You must close all your sessions (for example, restart your computer))
diff --git a/dbms/tests/integration/helpers/cluster.py b/dbms/tests/integration/helpers/cluster.py
index b20749a08d3f..c27a0b94f5db 100644
--- a/dbms/tests/integration/helpers/cluster.py
+++ b/dbms/tests/integration/helpers/cluster.py
@@ -10,6 +10,8 @@
 import errno
 from dicttoxml import dicttoxml
 import xml.dom.minidom
+from kazoo.client import KazooClient
+from kazoo.exceptions import KazooException
 
 import docker
 from docker.errors import ContainerError
@@ -46,7 +48,7 @@ def __init__(self, base_path, name=None, base_configs_dir=None, server_bin_path=
 
         self.base_cmd = ['docker-compose', '--project-directory', self.base_dir, '--project-name', self.project_name]
         self.base_zookeeper_cmd = None
-        self.pre_zookkeeper_commands = []
+        self.pre_zookeeper_commands = []
         self.instances = {}
         self.with_zookeeper = False
 
@@ -91,6 +93,12 @@ def get_instance_docker_id(self, instance_name):
         return self.project_name + '_' + instance_name + '_1'
 
 
+    def get_instance_ip(self, instance_name):
+        docker_id = self.get_instance_docker_id(instance_name)
+        handle = self.docker_client.containers.get(docker_id)
+        return handle.attrs['NetworkSettings']['Networks'].values()[0]['IPAddress']
+
+
     def start(self, destroy_dirs=True):
         if self.is_up:
             return
@@ -113,17 +121,18 @@ def start(self, destroy_dirs=True):
 
         if self.with_zookeeper and self.base_zookeeper_cmd:
             subprocess.check_call(self.base_zookeeper_cmd + ['up', '-d', '--no-recreate'])
-            for command in self.pre_zookkeeper_commands:
-                self.run_zookeeper_client_command(command, repeats=5)
+            for command in self.pre_zookeeper_commands:
+                self.run_kazoo_commands_with_retries(command, repeats=5)
+
+        # Uncomment for debugging
+        # print ' '.join(self.base_cmd + ['up', '--no-recreate'])
 
         subprocess.check_call(self.base_cmd + ['up', '-d', '--no-recreate'])
 
         start_deadline = time.time() + 20.0 # seconds
         for instance in self.instances.itervalues():
             instance.docker_client = self.docker_client
-
-            container = self.docker_client.containers.get(instance.docker_id)
-            instance.ip_address = container.attrs['NetworkSettings']['Networks'].values()[0]['IPAddress']
+            instance.ip_address = self.get_instance_ip(instance.name)
 
             instance.wait_for_start(start_deadline)
 
@@ -146,20 +155,26 @@ def shutdown(self, kill=True):
             instance.client = None
 
 
-    def run_zookeeper_client_command(self, command, zoo_node = 'zoo1', repeats=1, sleep_for=1):
-        cli_cmd = 'zkCli.sh  ' + command
-        zoo_name = self.get_instance_docker_id(zoo_node)
-        network_mode = 'container:' + zoo_name
-        for i in range(0, repeats - 1):
+    def get_kazoo_client(self, zoo_instance_name):
+        zk = KazooClient(hosts=self.get_instance_ip(zoo_instance_name))
+        zk.start()
+        return zk
+
+
+    def run_kazoo_commands_with_retries(self, kazoo_callback, zoo_instance_name = 'zoo1', repeats=1, sleep_for=1):
+        for i in range(repeats - 1):
             try:
-                return self.docker_client.containers.run('zookeeper', cli_cmd, remove=True, network_mode=network_mode)
-            except ContainerError:
+                kazoo_callback(self.get_kazoo_client(zoo_instance_name))
+                return
+            except KazooException as e:
+                print repr(e)
                 time.sleep(sleep_for)
 
-        return self.docker_client.containers.run('zookeeper', cli_cmd, remove=True, network_mode=network_mode)
+        kazoo_callback(self.get_kazoo_client(zoo_instance_name))
+
 
     def add_zookeeper_startup_command(self, command):
-        self.pre_zookkeeper_commands.append(command)
+        self.pre_zookeeper_commands.append(command)
 
 
 DOCKER_COMPOSE_TEMPLATE = '''
@@ -224,11 +239,13 @@ def get_query_request(self, *args, **kwargs):
 
     def exec_in_container(self, cmd, **kwargs):
         container = self.get_docker_handle()
-        handle = self.docker_client.api.exec_create(container.id, cmd, **kwargs)
-        output = self.docker_client.api.exec_start(handle).decode('utf8')
-        exit_code = self.docker_client.api.exec_inspect(handle)['ExitCode']
+        exec_id = self.docker_client.api.exec_create(container.id, cmd, **kwargs)
+        output = self.docker_client.api.exec_start(exec_id, detach=False)
+
+        output = output.decode('utf8')
+        exit_code = self.docker_client.api.exec_inspect(exec_id)['ExitCode']
         if exit_code:
-            raise Exception('Cmd {} failed! Return code {}. Output {}'.format(' '.join(cmd), exit_code, output))
+            raise Exception('Cmd "{}" failed! Return code {}. Output: {}'.format(' '.join(cmd), exit_code, output))
         return output
 
 
diff --git a/dbms/tests/integration/test_cluster_copier/__init__.py b/dbms/tests/integration/test_cluster_copier/__init__.py
new file mode 100644
index 000000000000..e69de29bb2d1
diff --git a/dbms/tests/integration/test_cluster_copier/configs/config.d/clusters.xml b/dbms/tests/integration/test_cluster_copier/configs/config.d/clusters.xml
new file mode 100644
index 000000000000..d956ae98f5a5
--- /dev/null
+++ b/dbms/tests/integration/test_cluster_copier/configs/config.d/clusters.xml
@@ -0,0 +1,47 @@
+<yandex>
+<remote_servers>
+
+    <cluster0>
+        <shard>
+            <internal_replication>true</internal_replication>
+            <replica>
+                <host>s0_0_0</host>
+                <port>9000</port>
+            </replica>
+            <replica>
+                <host>s0_0_1</host>
+                <port>9000</port>
+            </replica>
+        </shard>
+        <shard>
+            <internal_replication>true</internal_replication>
+            <replica>
+                <host>s0_1_0</host>
+                <port>9000</port>
+            </replica>
+        </shard>
+    </cluster0>
+
+    <cluster1>
+        <shard>
+            <internal_replication>true</internal_replication>
+            <replica>
+                <host>s1_0_0</host>
+                <port>9000</port>
+            </replica>
+            <replica>
+                <host>s1_0_1</host>
+                <port>9000</port>
+            </replica>
+        </shard>
+        <shard>
+            <internal_replication>true</internal_replication>
+            <replica>
+                <host>s1_1_0</host>
+                <port>9000</port>
+            </replica>
+        </shard>
+    </cluster1>
+
+</remote_servers>
+</yandex>
\ No newline at end of file
diff --git a/dbms/tests/integration/test_cluster_copier/configs/config.d/ddl.xml b/dbms/tests/integration/test_cluster_copier/configs/config.d/ddl.xml
new file mode 100644
index 000000000000..abad0dee450d
--- /dev/null
+++ b/dbms/tests/integration/test_cluster_copier/configs/config.d/ddl.xml
@@ -0,0 +1,5 @@
+<yandex>
+    <distributed_ddl>
+        <path>/clickhouse/task_queue/ddl</path>
+    </distributed_ddl>
+</yandex>
\ No newline at end of file
diff --git a/dbms/tests/integration/test_cluster_copier/configs/config.d/query_log.xml b/dbms/tests/integration/test_cluster_copier/configs/config.d/query_log.xml
new file mode 100644
index 000000000000..839ef92c6dcb
--- /dev/null
+++ b/dbms/tests/integration/test_cluster_copier/configs/config.d/query_log.xml
@@ -0,0 +1,14 @@
+<yandex>
+    <!-- Query log. Used only for queries with setting log_queries = 1. -->
+    <query_log>
+        <!-- What table to insert data. If table is not exist, it will be created.
+             When query log structure is changed after system update,
+              then old table will be renamed and new table will be created automatically.
+        -->
+        <database>system</database>
+        <table>query_log</table>
+
+        <!-- Interval of flushing data. -->
+        <flush_interval_milliseconds>1000</flush_interval_milliseconds>
+    </query_log>
+</yandex>
\ No newline at end of file
diff --git a/dbms/tests/integration/test_cluster_copier/configs/users.xml b/dbms/tests/integration/test_cluster_copier/configs/users.xml
new file mode 100644
index 000000000000..3c739cc2cc45
--- /dev/null
+++ b/dbms/tests/integration/test_cluster_copier/configs/users.xml
@@ -0,0 +1,24 @@
+<?xml version="1.0"?>
+<yandex>
+    <profiles>
+        <default>
+            <log_queries>1</log_queries>
+        </default>
+    </profiles>
+
+    <users>
+        <default>
+            <password></password>
+            <networks incl="networks" replace="replace">
+                <ip>::/0</ip>
+            </networks>
+            <profile>default</profile>
+            <quota>default</quota>
+        </default>
+    </users>
+
+    <quotas>
+        <default>
+        </default>
+    </quotas>
+</yandex>
diff --git a/dbms/tests/integration/test_cluster_copier/task0_description.xml b/dbms/tests/integration/test_cluster_copier/task0_description.xml
new file mode 100644
index 000000000000..b5560eeba959
--- /dev/null
+++ b/dbms/tests/integration/test_cluster_copier/task0_description.xml
@@ -0,0 +1,88 @@
+<?xml version="1.0"?>
+<yandex>
+
+    <!-- Configuration of clusters -->
+    <remote_servers>
+    <cluster0>
+        <shard>
+            <internal_replication>true</internal_replication>
+            <replica>
+                <host>s0_0_0</host>
+                <port>9000</port>
+            </replica>
+            <replica>
+                <host>s0_0_1</host>
+                <port>9000</port>
+            </replica>
+        </shard>
+        <shard>
+            <internal_replication>true</internal_replication>
+            <replica>
+                <host>s0_1_0</host>
+                <port>9000</port>
+            </replica>
+        </shard>
+    </cluster0>
+
+    <cluster1>
+        <shard>
+            <internal_replication>true</internal_replication>
+            <replica>
+                <host>s1_0_0</host>
+                <port>9000</port>
+            </replica>
+            <replica>
+                <host>s1_0_1</host>
+                <port>9000</port>
+            </replica>
+        </shard>
+        <shard>
+            <internal_replication>true</internal_replication>
+            <replica>
+                <host>s1_1_0</host>
+                <port>9000</port>
+            </replica>
+        </shard>
+    </cluster1>
+    </remote_servers>
+
+    <!-- How many simualteneous workers are posssible -->
+    <max_workers>4</max_workers>
+
+    <!-- Common setting for pull and push operations -->
+    <settings>
+    </settings>
+
+    <!-- Setting used to fetch data -->
+    <settings_pull>
+    </settings_pull>
+
+    <!-- Setting used to insert data -->
+    <settings_push>
+    </settings_push>
+
+    <!-- Tasks -->
+    <tables>
+        <hits>
+            <cluster_pull>cluster0</cluster_pull>
+            <database_pull>default</database_pull>
+            <table_pull>hits</table_pull>
+
+            <cluster_push>cluster1</cluster_push>
+            <database_push>default</database_push>
+            <table_push>hits</table_push>
+
+            <enabled_partitions> 0  1   2</enabled_partitions>
+
+            <!-- Engine of destination tables -->
+            <engine>ENGINE=ReplicatedMergeTree('/clickhouse/tables/cluster{cluster}/{shard}', '{replica}') PARTITION BY d % 3 ORDER BY d SETTINGS index_granularity = 16</engine>
+
+            <!-- Which sarding key to use while copying -->
+            <sharding_key>d + 1</sharding_key>
+
+            <!-- Optional expression that filter copying data -->
+            <where_condition>d - d = 0</where_condition>
+        </hits>
+    </tables>
+
+</yandex>
\ No newline at end of file
diff --git a/dbms/tests/integration/test_cluster_copier/test.py b/dbms/tests/integration/test_cluster_copier/test.py
new file mode 100644
index 000000000000..683c723f83fe
--- /dev/null
+++ b/dbms/tests/integration/test_cluster_copier/test.py
@@ -0,0 +1,145 @@
+import os
+import os.path as p
+import sys
+import time
+import datetime
+import pytest
+from contextlib import contextmanager
+import docker
+from kazoo.client import KazooClient
+
+
+CURRENT_TEST_DIR = os.path.dirname(os.path.abspath(__file__))
+sys.path.insert(0, os.path.dirname(CURRENT_TEST_DIR))
+from helpers.cluster import ClickHouseCluster
+from helpers.test_tools import TSV
+
+COPYING_FAIL_PROBABILITY = 0.33
+cluster = None
+
+
+def check_all_hosts_sucesfully_executed(tsv_content, num_hosts):
+    M = TSV.toMat(tsv_content)
+    hosts = [(l[0], l[1]) for l in M] # (host, port)
+    codes = [l[2] for l in M]
+    messages = [l[3] for l in M]
+
+    assert len(hosts) == num_hosts and len(set(hosts)) == num_hosts, "
" + tsv_content
+    assert len(set(codes)) == 1, "
" + tsv_content
+    assert codes[0] == "0", "
" + tsv_content
+
+
+def ddl_check_query(instance, query, num_hosts=3):
+    contents = instance.query(query)
+    check_all_hosts_sucesfully_executed(contents, num_hosts)
+    return contents
+
+
+@pytest.fixture(scope="module")
+def started_cluster():
+    global cluster
+    try:
+        clusters_schema = {
+         "0" : {
+            "0" : ["0", "1"],
+            "1" : ["0"]
+         },
+         "1" : {
+            "0" : ["0", "1"],
+            "1" : ["0"]
+         }
+        }
+
+        cluster = ClickHouseCluster(__file__)
+
+        for cluster_name, shards in clusters_schema.iteritems():
+            for shard_name, replicas in shards.iteritems():
+                for replica_name in replicas:
+                    name = "s{}_{}_{}".format(cluster_name, shard_name, replica_name)
+                    cluster.add_instance(name,
+                        config_dir="configs",
+                        macroses={"cluster": cluster_name, "shard": shard_name, "replica": replica_name},
+                        with_zookeeper=True)
+
+        cluster.start()
+        yield cluster
+
+    finally:
+        pass
+        cluster.shutdown()
+
+
+def _test_copying(cmd_options):
+    instance = cluster.instances['s0_0_0']
+
+    ddl_check_query(instance, "DROP TABLE IF EXISTS hits ON CLUSTER cluster0")
+    ddl_check_query(instance, "DROP TABLE IF EXISTS hits ON CLUSTER cluster1")
+
+    ddl_check_query(instance, "CREATE TABLE hits ON CLUSTER cluster0 (d UInt64) ENGINE=ReplicatedMergeTree('/clickhouse/tables/cluster_{cluster}/{shard}', '{replica}') PARTITION BY d % 3 ORDER BY d SETTINGS index_granularity = 16")
+    ddl_check_query(instance, "CREATE TABLE hits_all ON CLUSTER cluster0 (d UInt64) ENGINE=Distributed(cluster0, default, hits, d)")
+    ddl_check_query(instance, "CREATE TABLE hits_all ON CLUSTER cluster1 (d UInt64) ENGINE=Distributed(cluster1, default, hits, d + 1)")
+    instance.query("INSERT INTO hits_all SELECT * FROM system.numbers LIMIT 1002")
+
+    zk = cluster.get_kazoo_client('zoo1')
+    print "Use ZooKeeper server: {}:{}".format(zk.hosts[0][0], zk.hosts[0][1])
+
+    zk_task_path = "/clickhouse-copier/task_simple"
+    zk.ensure_path(zk_task_path)
+
+    copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task0_description.xml'), 'r').read()
+    zk.create(zk_task_path + "/description", copier_task_config)
+
+    # Run cluster-copier processes on each node
+    docker_api = docker.from_env().api
+    copiers_exec_ids = []
+
+    cmd = ['/usr/bin/clickhouse', 'copier',
+        '--config', '/etc/clickhouse-server/config-preprocessed.xml',
+        '--task-path', '/clickhouse-copier/task_simple',
+        '--base-dir', '/var/log/clickhouse-server/copier']
+    cmd += cmd_options
+
+    for instance_name, instance in cluster.instances.iteritems():
+        container = instance.get_docker_handle()
+        exec_id = docker_api.exec_create(container.id, cmd, stderr=True)
+        docker_api.exec_start(exec_id, detach=True)
+
+        copiers_exec_ids.append(exec_id)
+        print "Copier for {} ({}) has started".format(instance.name, instance.ip_address)
+
+    # Wait for copiers stopping and check their return codes
+    for exec_id, instance in zip(copiers_exec_ids, cluster.instances.itervalues()):
+        while True:
+            res = docker_api.exec_inspect(exec_id)
+            if not res['Running']:
+                break
+            time.sleep(1)
+
+        assert res['ExitCode'] == 0, "Instance: {} ({}). Info: {}".format(instance.name, instance.ip_address, repr(res))
+
+    assert TSV(cluster.instances['s0_0_0'].query("SELECT count() FROM hits_all")) == TSV("1002
")
+    assert TSV(cluster.instances['s1_0_0'].query("SELECT count() FROM hits_all")) == TSV("1002
")
+
+    assert TSV(cluster.instances['s1_0_0'].query("SELECT DISTINCT d % 2 FROM hits")) == TSV("1
")
+    assert TSV(cluster.instances['s1_1_0'].query("SELECT DISTINCT d % 2 FROM hits")) == TSV("0
")
+
+    zk.delete(zk_task_path, recursive=True)
+    ddl_check_query(instance, "DROP TABLE hits_all ON CLUSTER cluster0")
+    ddl_check_query(instance, "DROP TABLE hits_all ON CLUSTER cluster1")
+    ddl_check_query(instance, "DROP TABLE hits ON CLUSTER cluster0")
+    ddl_check_query(instance, "DROP TABLE hits ON CLUSTER cluster1")
+
+
+def test_copy_simple(started_cluster):
+    _test_copying([])
+
+
+def test_copy_with_recovering(started_cluster):
+    _test_copying(['--copy-fault-probability', str(COPYING_FAIL_PROBABILITY)])
+
+
+if __name__ == '__main__':
+    with contextmanager(started_cluster)() as cluster:
+       for name, instance in cluster.instances.items():
+           print name, instance.ip_address
+       raw_input("Cluster created, press any key to destroy...")
diff --git a/dbms/tests/integration/test_distributed_ddl/test.py b/dbms/tests/integration/test_distributed_ddl/test.py
index 09ae04583cd1..8b7e46443d55 100755
--- a/dbms/tests/integration/test_distributed_ddl/test.py
+++ b/dbms/tests/integration/test_distributed_ddl/test.py
@@ -89,7 +89,6 @@ def init_cluster(cluster):
         # Initialize databases and service tables
         instance = cluster.instances['ch1']
 
-        instance.query("SELECT 1")
         ddl_check_query(instance, """
 CREATE TABLE IF NOT EXISTS all_tables ON CLUSTER 'cluster_no_replicas'
     (database String, name String, engine String, metadata_modification_time DateTime)
@@ -119,6 +118,8 @@ def started_cluster():
         for instance in cluster.instances.values():
             ddl_check_there_are_no_dublicates(instance)
 
+        cluster.pm_random_drops.heal_all()
+
     finally:
         cluster.shutdown()
 
diff --git a/dbms/tests/integration/test_zookeeper_config/test.py b/dbms/tests/integration/test_zookeeper_config/test.py
index d3962b147ef4..5d01c92b46a1 100644
--- a/dbms/tests/integration/test_zookeeper_config/test.py
+++ b/dbms/tests/integration/test_zookeeper_config/test.py
@@ -1,3 +1,4 @@
+from __future__ import print_function
 from helpers.cluster import ClickHouseCluster
 import pytest
 
@@ -11,8 +12,10 @@ def test_chroot_with_same_root():
     node2 = cluster_2.add_instance('node2', config_dir='configs', with_zookeeper=True)
     nodes = [node1, node2]
 
-    cluster_1.add_zookeeper_startup_command('create /root_a ""')
-    cluster_1.add_zookeeper_startup_command('ls / ')
+    def create_zk_root(zk):
+        zk.ensure_path('/root_a')
+        print(zk.get_children('/'))
+    cluster_1.add_zookeeper_startup_command(create_zk_root)
 
     try:
         cluster_1.start()
@@ -21,7 +24,7 @@ def test_chroot_with_same_root():
             cluster_2.start(destroy_dirs=False)
             for i, node in enumerate(nodes):
                 node.query('''
-                CREATE TABLE simple (date Date, id UInt32) 
+                CREATE TABLE simple (date Date, id UInt32)
                 ENGINE = ReplicatedMergeTree('/clickhouse/tables/0/simple', '{replica}', date, id, 8192);
                 '''.format(replica=node.name))
                 node.query("INSERT INTO simple VALUES ({0}, {0})".format(i))
@@ -45,9 +48,11 @@ def test_chroot_with_different_root():
     node2 = cluster_2.add_instance('node2', config_dir='configs', with_zookeeper=True)
     nodes = [node1, node2]
 
-    cluster_1.add_zookeeper_startup_command('create /root_a ""')
-    cluster_1.add_zookeeper_startup_command('create /root_b ""')
-    cluster_1.add_zookeeper_startup_command('ls / ')
+    def create_zk_roots(zk):
+        zk.ensure_path('/root_a')
+        zk.ensure_path('/root_b')
+        print(zk.get_children('/'))
+    cluster_1.add_zookeeper_startup_command(create_zk_roots)
 
     try:
         cluster_1.start()
diff --git a/dbms/tests/queries/0_stateless/00563_insert_into_remote_and_zookeeper.reference b/dbms/tests/queries/0_stateless/00563_insert_into_remote_and_zookeeper.reference
new file mode 100644
index 000000000000..2ca3cd52fcf5
--- /dev/null
+++ b/dbms/tests/queries/0_stateless/00563_insert_into_remote_and_zookeeper.reference
@@ -0,0 +1,3 @@
+1
+2
+2
diff --git a/dbms/tests/queries/0_stateless/00563_insert_into_remote_and_zookeeper.sql b/dbms/tests/queries/0_stateless/00563_insert_into_remote_and_zookeeper.sql
new file mode 100644
index 000000000000..f29d897017a7
--- /dev/null
+++ b/dbms/tests/queries/0_stateless/00563_insert_into_remote_and_zookeeper.sql
@@ -0,0 +1,14 @@
+-- Check that settings are correctly passed through Distributed table
+DROP TABLE IF EXISTS test.simple;
+CREATE TABLE test.simple (d Int8) ENGINE = ReplicatedMergeTree('/clickhouse_test/tables/test/simple', '1') ORDER BY d;
+
+-- TODO: replace '127.0.0.2' -> '127.0.0.1' after a fix
+INSERT INTO TABLE FUNCTION remote('127.0.0.2', 'test', 'simple') VALUES (1);
+INSERT INTO TABLE FUNCTION remote('127.0.0.2', 'test', 'simple') VALUES (1);
+
+SET insert_deduplicate=0;
+INSERT INTO TABLE FUNCTION remote('127.0.0.2', 'test', 'simple') VALUES (2);
+INSERT INTO TABLE FUNCTION remote('127.0.0.2', 'test', 'simple') VALUES (2);
+
+SELECT * FROM remote('127.0.0.2', 'test', 'simple') ORDER BY d;
+DROP TABLE test.simple;
\ No newline at end of file
