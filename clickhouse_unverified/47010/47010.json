{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 47010,
  "instance_id": "ClickHouse__ClickHouse-47010",
  "issue_numbers": [
    "46465"
  ],
  "base_commit": "c3788084bb37ea67890aa8df960034546d6dab87",
  "patch": "diff --git a/src/Storages/MergeTree/DataPartsExchange.cpp b/src/Storages/MergeTree/DataPartsExchange.cpp\nindex 7a38033a126d..c6efe9c95894 100644\n--- a/src/Storages/MergeTree/DataPartsExchange.cpp\n+++ b/src/Storages/MergeTree/DataPartsExchange.cpp\n@@ -173,6 +173,7 @@ void Service::processQuery(const HTMLForm & params, ReadBuffer & /*body*/, Write\n             writeUUIDText(part->uuid, out);\n \n         String remote_fs_metadata = parse<String>(params.get(\"remote_fs_metadata\", \"\"));\n+\n         std::regex re(\"\\\\s*,\\\\s*\");\n         Strings capability(\n             std::sregex_token_iterator(remote_fs_metadata.begin(), remote_fs_metadata.end(), re, -1),\n@@ -477,6 +478,22 @@ MergeTreeData::MutableDataPartPtr Fetcher::fetchSelectedPart(\n \n     int server_protocol_version = parse<int>(in->getResponseCookie(\"server_protocol_version\", \"0\"));\n \n+    String remote_fs_metadata = parse<String>(in->getResponseCookie(\"remote_fs_metadata\", \"\"));\n+\n+    DiskPtr preffered_disk = disk;\n+\n+    if (!preffered_disk)\n+    {\n+        for (const auto & disk_candidate : data.getDisks())\n+        {\n+            if (toString(disk_candidate->getDataSourceDescription().type) == remote_fs_metadata)\n+            {\n+                preffered_disk = disk_candidate;\n+                break;\n+            }\n+        }\n+    }\n+\n     ReservationPtr reservation;\n     size_t sum_files_size = 0;\n     if (server_protocol_version >= REPLICATION_PROTOCOL_VERSION_WITH_PARTS_SIZE)\n@@ -496,11 +513,12 @@ MergeTreeData::MutableDataPartPtr Fetcher::fetchSelectedPart(\n                 LOG_TEST(log, \"Disk for fetch is not provided, reserving space using storage balanced reservation\");\n                 reservation\n                     = data.balancedReservation(metadata_snapshot, sum_files_size, 0, part_name, part_info, {}, tagger_ptr, &ttl_infos, true);\n+\n                 if (!reservation)\n                 {\n                     LOG_TEST(log, \"Disk for fetch is not provided, reserving space using TTL rules\");\n                     reservation\n-                        = data.reserveSpacePreferringTTLRules(metadata_snapshot, sum_files_size, ttl_infos, std::time(nullptr), 0, true);\n+                        = data.reserveSpacePreferringTTLRules(metadata_snapshot, sum_files_size, ttl_infos, std::time(nullptr), 0, true, preffered_disk);\n                 }\n             }\n         }\n@@ -525,7 +543,7 @@ MergeTreeData::MutableDataPartPtr Fetcher::fetchSelectedPart(\n     if (!disk)\n     {\n         disk = reservation->getDisk();\n-        LOG_TRACE(log, \"Disk for fetch is not provided, getting disk from reservation {} with type {}\", disk->getName(), toString(disk->getDataSourceDescription().type));\n+        LOG_TRACE(log, \"Disk for fetch is not provided, getting disk from reservation {} with type '{}'\", disk->getName(), toString(disk->getDataSourceDescription().type));\n     }\n     else\n     {\n@@ -552,8 +570,6 @@ MergeTreeData::MutableDataPartPtr Fetcher::fetchSelectedPart(\n     if (server_protocol_version >= REPLICATION_PROTOCOL_VERSION_WITH_PARTS_UUID)\n         readUUIDText(part_uuid, *in);\n \n-    String remote_fs_metadata = parse<String>(in->getResponseCookie(\"remote_fs_metadata\", \"\"));\n-\n     size_t projections = 0;\n     if (server_protocol_version >= REPLICATION_PROTOCOL_VERSION_WITH_PARTS_PROJECTION)\n         readBinary(projections, *in);\n",
  "test_patch": "diff --git a/tests/integration/test_zero_copy_fetch/__init__.py b/tests/integration/test_zero_copy_fetch/__init__.py\nnew file mode 100644\nindex 000000000000..e5a0d9b4834e\n--- /dev/null\n+++ b/tests/integration/test_zero_copy_fetch/__init__.py\n@@ -0,0 +1,1 @@\n+#!/usr/bin/env python3\ndiff --git a/tests/integration/test_zero_copy_fetch/configs/storage_conf.xml b/tests/integration/test_zero_copy_fetch/configs/storage_conf.xml\nnew file mode 100644\nindex 000000000000..257ae0a355cc\n--- /dev/null\n+++ b/tests/integration/test_zero_copy_fetch/configs/storage_conf.xml\n@@ -0,0 +1,30 @@\n+<clickhouse>\n+    <storage_configuration>\n+        <disks>\n+            <s3>\n+                <type>s3</type>\n+                <endpoint>http://minio1:9001/root/data/</endpoint>\n+                <access_key_id>minio</access_key_id>\n+                <secret_access_key>minio123</secret_access_key>\n+            </s3>\n+        </disks>\n+        <policies>\n+            <s3>\n+                <volumes>\n+                    <default>\n+                        <disk>default</disk>\n+                    </default>\n+                    <main>\n+                        <disk>s3</disk>\n+                        <prefer_not_to_merge>False</prefer_not_to_merge>\n+                        <perform_ttl_move_on_insert>True</perform_ttl_move_on_insert>\n+                    </main>\n+                </volumes>\n+            </s3>\n+        </policies>\n+    </storage_configuration>\n+\n+    <merge_tree>\n+        <allow_remote_fs_zero_copy_replication>true</allow_remote_fs_zero_copy_replication>\n+    </merge_tree>\n+</clickhouse>\ndiff --git a/tests/integration/test_zero_copy_fetch/test.py b/tests/integration/test_zero_copy_fetch/test.py\nnew file mode 100644\nindex 000000000000..f13eac5e9d10\n--- /dev/null\n+++ b/tests/integration/test_zero_copy_fetch/test.py\n@@ -0,0 +1,104 @@\n+#!/usr/bin/env python3\n+\n+import logging\n+import random\n+import string\n+import time\n+\n+import pytest\n+from helpers.cluster import ClickHouseCluster\n+\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    try:\n+\n+        cluster.add_instance(\n+            \"node1\",\n+            main_configs=[\"configs/storage_conf.xml\"],\n+            with_minio=True,\n+            with_zookeeper=True,\n+        )\n+        cluster.add_instance(\n+            \"node2\",\n+            main_configs=[\"configs/storage_conf.xml\"],\n+            with_minio=True,\n+            with_zookeeper=True,\n+        )\n+        cluster.start()\n+\n+        yield cluster\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_fetch_correct_volume(started_cluster):\n+    node1 = cluster.instances[\"node1\"]\n+    node2 = cluster.instances[\"node2\"]\n+\n+    node1.query(\n+        \"\"\"\n+CREATE TABLE test1 (EventDate Date, CounterID UInt32)\n+ENGINE = ReplicatedMergeTree('/clickhouse-tables/test1', 'r1')\n+PARTITION BY toMonday(EventDate)\n+ORDER BY (CounterID, EventDate)\n+SETTINGS index_granularity = 8192, storage_policy = 's3'\"\"\"\n+    )\n+\n+    node1.query(\n+        \"INSERT INTO test1 SELECT toDate('2023-01-01') + toIntervalDay(number), number + 1000 from system.numbers limit 20\"\n+    )\n+\n+    def get_part_to_disk(query_result):\n+        part_to_disk = {}\n+        for row in query_result.strip().split(\"\\n\"):\n+            print(row)\n+            disk, part = row.split(\"\\t\")\n+            part_to_disk[part] = disk\n+        return part_to_disk\n+\n+    part_to_disk = get_part_to_disk(\n+        node1.query(\n+            \"SELECT disk_name, name FROM system.parts where table = 'test1' and active\"\n+        )\n+    )\n+    for disk in part_to_disk.values():\n+        assert disk == \"default\"\n+\n+    node1.query(\"ALTER TABLE test1 MOVE PARTITION '2022-12-26' TO DISK 's3'\")\n+    node1.query(\"ALTER TABLE test1 MOVE PARTITION '2023-01-02' TO DISK 's3'\")\n+    node1.query(\"ALTER TABLE test1 MOVE PARTITION '2023-01-09' TO DISK 's3'\")\n+\n+    part_to_disk = get_part_to_disk(\n+        node1.query(\n+            \"SELECT disk_name, name FROM system.parts where table = 'test1' and active\"\n+        )\n+    )\n+    assert part_to_disk[\"20221226_0_0_0\"] == \"s3\"\n+    assert part_to_disk[\"20230102_0_0_0\"] == \"s3\"\n+    assert part_to_disk[\"20230109_0_0_0\"] == \"s3\"\n+    assert part_to_disk[\"20230116_0_0_0\"] == \"default\"\n+\n+    node2.query(\n+        \"\"\"\n+CREATE TABLE test1 (EventDate Date, CounterID UInt32)\n+ENGINE = ReplicatedMergeTree('/clickhouse-tables/test1', 'r2')\n+PARTITION BY toMonday(EventDate)\n+ORDER BY (CounterID, EventDate)\n+SETTINGS index_granularity = 8192, storage_policy = 's3'\"\"\"\n+    )\n+\n+    node2.query(\"SYSTEM SYNC REPLICA test1\")\n+\n+    part_to_disk = get_part_to_disk(\n+        node2.query(\n+            \"SELECT disk_name, name FROM system.parts where table = 'test1' and active\"\n+        )\n+    )\n+    assert part_to_disk[\"20221226_0_0_0\"] == \"s3\"\n+    assert part_to_disk[\"20230102_0_0_0\"] == \"s3\"\n+    assert part_to_disk[\"20230109_0_0_0\"] == \"s3\"\n+    assert part_to_disk[\"20230116_0_0_0\"] == \"default\"\n",
  "problem_statement": "New replica download parts to default disk instead of external disk\n**Describe what's wrong**\r\n\r\nNewly added replica downloads all data to default disk regardless of its location on other replicas.\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nExperienced in our production on version 22.8.11.15\r\n\r\n\r\n**How to reproduce**\r\nConsider two replicas with following config for externa storage:\r\n```\r\n<yandex>\r\n    <merge_tree>\r\n        <allow_remote_fs_zero_copy_replication>true</allow_remote_fs_zero_copy_replication>\r\n    </merge_tree>\r\n    <storage_configuration>\r\n        <disks>\r\n            <s3_debug>\r\n                <type>s3</type>\r\n                <endpoint>https://s3.some-host/test/debug/001/</endpoint>\r\n                <access_key_id>xxxxxxxxxxxxxxxxxxxx</access_key_id>\r\n                <secret_access_key>xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx</secret_access_key>\r\n            </s3_debug>\r\n        </disks>\r\n        <policies>\r\n            <s3_debug>\r\n                <volumes>\r\n                    <default>\r\n                        <disk>default</disk>\r\n                    </default>\r\n                    <external>\r\n                        <disk>s3_debug</disk>\r\n                        <prefer_not_to_merge>False</prefer_not_to_merge>\r\n                        <perform_ttl_move_on_insert>True</perform_ttl_move_on_insert>\r\n                    </external>\r\n                </volumes>\r\n            </s3_debug>\r\n        </policies>\r\n    </storage_configuration>\r\n</yandex>\r\n```\r\n\r\nCretate test table on one replica and insert some data:\r\n```\r\nCREATE DATABASE debug;\r\n\r\nCREATE TABLE debug.test1 (EventDate Date, CounterID UInt32)\r\nENGINE = ReplicatedMergeTree('/clickhouse-tables/{shard}/test1', '{replica}')\r\nPARTITION BY toMonday(EventDate)\r\nORDER BY (CounterID, EventDate)\r\nSAMPLE BY intHash32(CounterID)\r\nSETTINGS index_granularity = 8192, storage_policy = 's3_debug'\r\n\r\nINSERT INTO debug.test1 SELECT toDate('2023-01-01') + toIntervalDay(number), number + 1000 from system.numbers limit 20;\r\n```\r\n\r\nLook at partitions:\r\n```\r\nSELECT\r\n        database,\r\n        table,\r\n        partition,\r\n        disk_name,\r\n        count(*) AS parts,\r\n        sum(rows) AS total_rows,\r\n        sum(bytes_on_disk) AS total_bytes_on_disk,\r\n        min(min_date) AS min_date_in_partition,\r\n        max(max_date) AS max_date_in_partition\r\n    FROM system.parts\r\n    WHERE (database = 'debug') AND (table = 'test1') AND (active = 1)\r\n    GROUP BY\r\n        database,\r\n        table,\r\n        partition,\r\n        disk_name\r\n    ORDER BY partition ASC\r\n\r\n\u250c\u2500database\u2500\u252c\u2500table\u2500\u252c\u2500partition\u2500\u2500\u252c\u2500disk_name\u2500\u252c\u2500parts\u2500\u252c\u2500total_rows\u2500\u252c\u2500total_bytes_on_disk\u2500\u252c\u2500min_date_in_partition\u2500\u252c\u2500max_date_in_partition\u2500\u2510\r\n\u2502 debug    \u2502 test1 \u2502 2022-12-26 \u2502 default   \u2502     1 \u2502          1 \u2502                 173 \u2502            2023-01-01 \u2502            2023-01-01 \u2502\r\n\u2502 debug    \u2502 test1 \u2502 2023-01-02 \u2502 default   \u2502     1 \u2502          7 \u2502                 209 \u2502            2023-01-02 \u2502            2023-01-08 \u2502\r\n\u2502 debug    \u2502 test1 \u2502 2023-01-09 \u2502 default   \u2502     1 \u2502          7 \u2502                 209 \u2502            2023-01-09 \u2502            2023-01-15 \u2502\r\n\u2502 debug    \u2502 test1 \u2502 2023-01-16 \u2502 default   \u2502     1 \u2502          5 \u2502                 197 \u2502            2023-01-16 \u2502            2023-01-20 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nall data is local.\r\n\r\nMove partitions to external storage:\r\n\r\n```\r\nALTER TABLE debug.test1 MOVE PARTITION '2022-12-26' TO DISK 's3_debug';\r\nALTER TABLE debug.test1 MOVE PARTITION '2023-01-02' TO DISK 's3_debug';\r\nALTER TABLE debug.test1 MOVE PARTITION '2023-01-09' TO DISK 's3_debug';\r\n```\r\nAnd look again at partitons:\r\n```\r\n\u250c\u2500database\u2500\u252c\u2500table\u2500\u252c\u2500partition\u2500\u2500\u252c\u2500disk_name\u2500\u252c\u2500parts\u2500\u252c\u2500total_rows\u2500\u252c\u2500total_bytes_on_disk\u2500\u252c\u2500min_date_in_partition\u2500\u252c\u2500max_date_in_partition\u2500\u2510\r\n\u2502 debug    \u2502 test1 \u2502 2022-12-26 \u2502 s3_debug  \u2502     1 \u2502          1 \u2502                 173 \u2502            2023-01-01 \u2502            2023-01-01 \u2502\r\n\u2502 debug    \u2502 test1 \u2502 2023-01-02 \u2502 s3_debug  \u2502     1 \u2502          7 \u2502                 209 \u2502            2023-01-02 \u2502            2023-01-08 \u2502\r\n\u2502 debug    \u2502 test1 \u2502 2023-01-09 \u2502 s3_debug  \u2502     1 \u2502          7 \u2502                 209 \u2502            2023-01-09 \u2502            2023-01-15 \u2502\r\n\u2502 debug    \u2502 test1 \u2502 2023-01-16 \u2502 default   \u2502     1 \u2502          5 \u2502                 197 \u2502            2023-01-16 \u2502            2023-01-20 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nSome data on external storage.\r\n\r\nThen switch to second replica and create table:\r\n\r\n```\r\nCREATE DATABASE debug;\r\n\r\nCREATE TABLE debug.test1 (EventDate Date, CounterID UInt32)\r\nENGINE = ReplicatedMergeTree('/clickhouse-tables/{shard}/test1', '{replica}')\r\nPARTITION BY toMonday(EventDate)\r\nORDER BY (CounterID, EventDate)\r\nSAMPLE BY intHash32(CounterID)\r\nSETTINGS index_granularity = 8192, storage_policy = 's3_debug'\r\n```\r\n\r\nAnd look at partitions of newly created replica\r\n```\r\n\u250c\u2500database\u2500\u252c\u2500table\u2500\u252c\u2500partition\u2500\u2500\u252c\u2500disk_name\u2500\u252c\u2500parts\u2500\u252c\u2500total_rows\u2500\u252c\u2500total_bytes_on_disk\u2500\u252c\u2500min_date_in_partition\u2500\u252c\u2500max_date_in_partition\u2500\u2510\r\n\u2502 debug    \u2502 test1 \u2502 2022-12-26 \u2502 default   \u2502     1 \u2502          1 \u2502                 173 \u2502            2023-01-01 \u2502            2023-01-01 \u2502\r\n\u2502 debug    \u2502 test1 \u2502 2023-01-02 \u2502 default   \u2502     1 \u2502          7 \u2502                 209 \u2502            2023-01-02 \u2502            2023-01-08 \u2502\r\n\u2502 debug    \u2502 test1 \u2502 2023-01-09 \u2502 default   \u2502     1 \u2502          7 \u2502                 209 \u2502            2023-01-09 \u2502            2023-01-15 \u2502\r\n\u2502 debug    \u2502 test1 \u2502 2023-01-16 \u2502 default   \u2502     1 \u2502          5 \u2502                 197 \u2502            2023-01-16 \u2502            2023-01-20 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nThe data is here, but on default disk instead of external storage.\r\n\r\n\r\n\r\n**Expected behavior**\r\n\r\nThe data on newly created replica shoud be on external storage as on first replica.\r\n\r\n\r\n**Additional context**\r\n\r\nThere will be some complications because of possible different storage_policy settings and different storage policies itself.\r\n\r\nBut it is a blocker for scenario of replica redeployment when we need to reinstall operating system from scratch and recreate replicated tables in case of hardware failures. It will be impossible to redeploy if total amount of data is more than local disk size.\r\n\r\nProbably CREATE TABLE should fail in case of different storage_policy settings and differences in storage policies.\r\n\r\nAnd it will be more complicated in case of more than two replicas.\r\n\n",
  "hints_text": "",
  "created_at": "2023-02-28T13:49:45Z",
  "modified_files": [
    "src/Storages/MergeTree/DataPartsExchange.cpp"
  ],
  "modified_test_files": [
    "b/tests/integration/test_zero_copy_fetch/__init__.py",
    "b/tests/integration/test_zero_copy_fetch/configs/storage_conf.xml",
    "b/tests/integration/test_zero_copy_fetch/test.py"
  ]
}