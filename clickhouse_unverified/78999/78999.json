{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 78999,
  "instance_id": "ClickHouse__ClickHouse-78999",
  "issue_numbers": [
    "25581"
  ],
  "base_commit": "4cf7125836d25379f4ac7a313a4e47121362a32a",
  "patch": "diff --git a/src/Planner/Planner.cpp b/src/Planner/Planner.cpp\nindex fdf3d91b9361..0b6d992e2937 100644\n--- a/src/Planner/Planner.cpp\n+++ b/src/Planner/Planner.cpp\n@@ -1062,6 +1062,16 @@ void addPreliminarySortOrDistinctOrLimitStepsIfNeeded(\n         addLimitByStep(query_plan, limit_by_analysis_result, query_node, true /*do_not_skip_offset*/);\n     }\n \n+    /// Do not apply PreLimit at first stage for LIMIT BY and `exact_rows_before_limit`,\n+    /// as it may break `rows_before_limit_at_least` value during the second stage in\n+    /// case it also contains LIMIT BY\n+    const Settings & settings = planner_context->getQueryContext()->getSettingsRef();\n+\n+    if (query_node.hasLimitBy() && settings[Setting::exact_rows_before_limit])\n+    {\n+        return;\n+    }\n+\n     /// WITH TIES simply not supported properly for preliminary steps, so let's disable it.\n     if (query_node.hasLimit() && !query_node.hasLimitByOffset() && !query_node.isLimitWithTies())\n         addPreliminaryLimitStep(query_plan, query_analysis_result, planner_context, true /*do_not_skip_offset*/);\ndiff --git a/src/Processors/Transforms/LimitByTransform.cpp b/src/Processors/Transforms/LimitByTransform.cpp\nindex 9ca84f7a2cc2..80041e2ff2ee 100644\n--- a/src/Processors/Transforms/LimitByTransform.cpp\n+++ b/src/Processors/Transforms/LimitByTransform.cpp\n@@ -64,6 +64,9 @@ void LimitByTransform::transform(Chunk & chunk)\n                 column = column->filter(filter, inserted_count);\n     }\n \n+    if (rows_before_limit_at_least)\n+        rows_before_limit_at_least->add(inserted_count);\n+\n     chunk.setColumns(std::move(columns), inserted_count);\n }\n \ndiff --git a/src/Processors/Transforms/LimitByTransform.h b/src/Processors/Transforms/LimitByTransform.h\nindex 8c2b46af69ef..020bbd93f05a 100644\n--- a/src/Processors/Transforms/LimitByTransform.h\n+++ b/src/Processors/Transforms/LimitByTransform.h\n@@ -1,5 +1,6 @@\n #pragma once\n #include <Processors/ISimpleTransform.h>\n+#include <Processors/RowsBeforeStepCounter.h>\n #include <Common/HashTable/HashMap.h>\n \n \n@@ -14,6 +15,8 @@ class LimitByTransform : public ISimpleTransform\n \n     String getName() const override { return \"LimitByTransform\"; }\n \n+    void setRowsBeforeLimitCounter(RowsBeforeStepCounterPtr counter) override { rows_before_limit_at_least.swap(counter); }\n+\n protected:\n     void transform(Chunk & chunk) override;\n \n@@ -24,6 +27,8 @@ class LimitByTransform : public ISimpleTransform\n     std::vector<size_t> key_positions;\n     const UInt64 group_length;\n     const UInt64 group_offset;\n+\n+    RowsBeforeStepCounterPtr rows_before_limit_at_least;\n };\n \n }\ndiff --git a/src/QueryPipeline/QueryPipeline.cpp b/src/QueryPipeline/QueryPipeline.cpp\nindex 7e1fc90c1dc3..b2f8d9cc6362 100644\n--- a/src/QueryPipeline/QueryPipeline.cpp\n+++ b/src/QueryPipeline/QueryPipeline.cpp\n@@ -22,6 +22,7 @@\n #include <Processors/Transforms/AggregatingTransform.h>\n #include <Processors/Transforms/CountingTransform.h>\n #include <Processors/Transforms/ExpressionTransform.h>\n+#include <Processors/Transforms/LimitByTransform.h>\n #include <Processors/Transforms/LimitsCheckingTransform.h>\n #include <Processors/Transforms/MaterializingTransform.h>\n #include <Processors/Transforms/PartialSortingTransform.h>\n@@ -178,7 +179,8 @@ static void initRowsBeforeLimit(IOutputFormat * output_format)\n         ///   2. Limit ... PartialSorting: Set counter on PartialSorting\n         ///   3. Limit ... TotalsHaving(with filter) ... Remote: Set counter on the input port of Limit\n         ///   4. Limit ... Remote: Set counter on Remote\n-        ///   5. Limit ... : Set counter on the input port of Limit\n+        ///   5. Limit ... LimitBy: Set counter on LimitBy, as it may not be executed on initiator\n+        ///   6. Limit ... : Set counter on the input port of Limit\n \n         /// Case 1.\n         if ((typeid_cast<RemoteSource *>(processor) || typeid_cast<DelayedSource *>(processor)) && !limit_processor)\n@@ -222,6 +224,14 @@ static void initRowsBeforeLimit(IOutputFormat * output_format)\n                 limit_candidates[limit_processor].push_back(limit_input_port);\n                 continue;\n             }\n+\n+            /// Case 5.\n+            if (typeid_cast<LimitByTransform *>(processor))\n+            {\n+                processors.emplace_back(processor);\n+                limit_candidates[limit_processor].push_back(limit_input_port);\n+                continue;\n+            }\n         }\n \n         /// Skip totals and extremes port for output format.\n@@ -256,7 +266,7 @@ static void initRowsBeforeLimit(IOutputFormat * output_format)\n         }\n     }\n \n-    /// Case 5.\n+    /// Case 6.\n     for (auto && [limit, ports] : limit_candidates)\n     {\n         /// If there are some input ports which don't have the counter, add it to LimitTransform.\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/03408_limit_by_rows_before_limit.reference b/tests/queries/0_stateless/03408_limit_by_rows_before_limit.reference\nnew file mode 100644\nindex 000000000000..54c63b6e880f\n--- /dev/null\n+++ b/tests/queries/0_stateless/03408_limit_by_rows_before_limit.reference\n@@ -0,0 +1,161 @@\n+-- Assert total number of groups and records in unsorted\n+10\t50\n+\n+-- Assert rows_before_limit for unsorted ORDER BY + LIMIT BY + LIMIT\n+{\n+\t\"meta\":\n+\t[\n+\t\t{\n+\t\t\t\"name\": \"id\",\n+\t\t\t\"type\": \"Int32\"\n+\t\t},\n+\t\t{\n+\t\t\t\"name\": \"val\",\n+\t\t\t\"type\": \"String\"\n+\t\t}\n+\t],\n+\n+\t\"data\":\n+\t[\n+\t\t[0, \"00\"],\n+\t\t[1, \"01\"],\n+\t\t[2, \"02\"]\n+\t],\n+\n+\t\"rows\": 3,\n+\n+\t\"rows_before_limit_at_least\": 3\n+}\n+\n+-- Assert rows_before_limit for unsorted ORDER BY + LIMIT BY + LIMIT, exact\n+{\n+\t\"meta\":\n+\t[\n+\t\t{\n+\t\t\t\"name\": \"id\",\n+\t\t\t\"type\": \"Int32\"\n+\t\t},\n+\t\t{\n+\t\t\t\"name\": \"val\",\n+\t\t\t\"type\": \"String\"\n+\t\t}\n+\t],\n+\n+\t\"data\":\n+\t[\n+\t\t[0, \"00\"],\n+\t\t[1, \"01\"],\n+\t\t[2, \"02\"]\n+\t],\n+\n+\t\"rows\": 3,\n+\n+\t\"rows_before_limit_at_least\": 10\n+}\n+\n+-- Assert rows_before_limit for unsorted HAVING + ORDER BY + LIMIT BY + LIMIT, exact\n+{\n+\t\"meta\":\n+\t[\n+\t\t{\n+\t\t\t\"name\": \"id\",\n+\t\t\t\"type\": \"Int32\"\n+\t\t},\n+\t\t{\n+\t\t\t\"name\": \"val\",\n+\t\t\t\"type\": \"String\"\n+\t\t}\n+\t],\n+\n+\t\"data\":\n+\t[\n+\t\t[0, \"40\"],\n+\t\t[1, \"41\"],\n+\t\t[2, \"42\"]\n+\t],\n+\n+\t\"rows\": 3,\n+\n+\t\"rows_before_limit_at_least\": 7\n+}\n+\n+-- Assert total number of groups and records in sorted\n+10\t50\n+\n+-- Assert rows_before_limit for sorted ORDER BY + LIMIT BY + LIMIT\n+{\n+\t\"meta\":\n+\t[\n+\t\t{\n+\t\t\t\"name\": \"id\",\n+\t\t\t\"type\": \"Int32\"\n+\t\t},\n+\t\t{\n+\t\t\t\"name\": \"val\",\n+\t\t\t\"type\": \"String\"\n+\t\t}\n+\t],\n+\n+\t\"data\":\n+\t[\n+\t\t[0, \"00\"],\n+\t\t[1, \"01\"],\n+\t\t[2, \"02\"]\n+\t],\n+\n+\t\"rows\": 3,\n+\n+\t\"rows_before_limit_at_least\": 3\n+}\n+\n+-- Assert rows_before_limit for sorted ORDER BY + LIMIT BY + LIMIT, exact\n+{\n+\t\"meta\":\n+\t[\n+\t\t{\n+\t\t\t\"name\": \"id\",\n+\t\t\t\"type\": \"Int32\"\n+\t\t},\n+\t\t{\n+\t\t\t\"name\": \"val\",\n+\t\t\t\"type\": \"String\"\n+\t\t}\n+\t],\n+\n+\t\"data\":\n+\t[\n+\t\t[0, \"00\"],\n+\t\t[1, \"01\"],\n+\t\t[2, \"02\"]\n+\t],\n+\n+\t\"rows\": 3,\n+\n+\t\"rows_before_limit_at_least\": 10\n+}\n+\n+-- Assert rows_before_limit for sorted HAVING + ORDER BY + LIMIT BY + LIMIT, exact\n+{\n+\t\"meta\":\n+\t[\n+\t\t{\n+\t\t\t\"name\": \"id\",\n+\t\t\t\"type\": \"Int32\"\n+\t\t},\n+\t\t{\n+\t\t\t\"name\": \"val\",\n+\t\t\t\"type\": \"String\"\n+\t\t}\n+\t],\n+\n+\t\"data\":\n+\t[\n+\t\t[0, \"40\"],\n+\t\t[1, \"41\"],\n+\t\t[2, \"42\"]\n+\t],\n+\n+\t\"rows\": 3,\n+\n+\t\"rows_before_limit_at_least\": 7\n+}\ndiff --git a/tests/queries/0_stateless/03408_limit_by_rows_before_limit.sql b/tests/queries/0_stateless/03408_limit_by_rows_before_limit.sql\nnew file mode 100644\nindex 000000000000..9a897b5edeff\n--- /dev/null\n+++ b/tests/queries/0_stateless/03408_limit_by_rows_before_limit.sql\n@@ -0,0 +1,60 @@\n+SET output_format_write_statistics = 0;\n+\n+DROP TABLE IF EXISTS 03408_unsorted;\n+\n+CREATE TABLE 03408_unsorted (id Int32, val String) ENGINE = MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part=1\n+AS\n+SELECT number % 10, leftPad(toString(number), 2, '0') FROM numbers(50);\n+\n+SELECT '-- Assert total number of groups and records in unsorted';\n+SELECT uniqExact(id), count() FROM 03408_unsorted;\n+\n+SELECT '';\n+SELECT '-- Assert rows_before_limit for unsorted ORDER BY + LIMIT BY + LIMIT';\n+\n+SELECT id, val FROM 03408_unsorted ORDER BY id, val LIMIT 1 BY id LIMIT 3\n+FORMAT JsonCompact SETTINGS max_block_size=1, exact_rows_before_limit=0;\n+\n+SELECT '';\n+SELECT '-- Assert rows_before_limit for unsorted ORDER BY + LIMIT BY + LIMIT, exact';\n+\n+SELECT id, val FROM 03408_unsorted ORDER BY id, val LIMIT 1 BY id LIMIT 3\n+FORMAT JsonCompact SETTINGS max_block_size=1, exact_rows_before_limit=1;\n+\n+SELECT '';\n+SELECT '-- Assert rows_before_limit for unsorted HAVING + ORDER BY + LIMIT BY + LIMIT, exact';\n+\n+SELECT id, val FROM 03408_unsorted GROUP BY id, val HAVING id < 7 ORDER BY id, val DESC LIMIT 1 BY id LIMIT 3\n+FORMAT JsonCompact SETTINGS max_block_size=1, exact_rows_before_limit=1;\n+\n+DROP TABLE 03408_unsorted;\n+\n+DROP TABLE IF EXISTS 03408_sorted;\n+\n+CREATE TABLE 03408_sorted (id Int32, val String) ENGINE = MergeTree ORDER BY (id, val) SETTINGS min_bytes_for_wide_part=1\n+AS\n+SELECT number % 10, leftPad(toString(number), 2, '0') FROM numbers(50);\n+\n+SELECT '';\n+SELECT '-- Assert total number of groups and records in sorted';\n+SELECT uniqExact(id), count() FROM 03408_sorted;\n+\n+SELECT '';\n+SELECT '-- Assert rows_before_limit for sorted ORDER BY + LIMIT BY + LIMIT';\n+\n+SELECT id, val FROM 03408_sorted ORDER BY id, val LIMIT 1 BY id LIMIT 3\n+FORMAT JsonCompact SETTINGS max_block_size=1, exact_rows_before_limit=0;\n+\n+SELECT '';\n+SELECT '-- Assert rows_before_limit for sorted ORDER BY + LIMIT BY + LIMIT, exact';\n+\n+SELECT id, val FROM 03408_sorted ORDER BY id, val LIMIT 1 BY id LIMIT 3\n+FORMAT JsonCompact SETTINGS max_block_size=1, exact_rows_before_limit=1;\n+\n+SELECT '';\n+SELECT '-- Assert rows_before_limit for sorted HAVING + ORDER BY + LIMIT BY + LIMIT, exact';\n+\n+SELECT id, val FROM 03408_sorted GROUP BY id, val HAVING id < 7 ORDER BY id, val DESC LIMIT 1 BY id LIMIT 3\n+FORMAT JsonCompact SETTINGS max_block_size=1, exact_rows_before_limit=1;\n+\n+DROP TABLE 03408_sorted;\ndiff --git a/tests/queries/0_stateless/03408_limit_by_rows_before_limit_dist.reference b/tests/queries/0_stateless/03408_limit_by_rows_before_limit_dist.reference\nnew file mode 100644\nindex 000000000000..e84fb63fe260\n--- /dev/null\n+++ b/tests/queries/0_stateless/03408_limit_by_rows_before_limit_dist.reference\n@@ -0,0 +1,107 @@\n+-- Assert total number of groups and records in distributed\n+10\t100\n+\n+-- Assert rows_before_limit for distributed ORDER BY + LIMIT BY + LIMIT\n+{\n+\t\"meta\":\n+\t[\n+\t\t{\n+\t\t\t\"name\": \"id\",\n+\t\t\t\"type\": \"Int32\"\n+\t\t},\n+\t\t{\n+\t\t\t\"name\": \"val\",\n+\t\t\t\"type\": \"String\"\n+\t\t}\n+\t],\n+\n+\t\"data\":\n+\t[\n+\t\t[0, \"00\"],\n+\t\t[1, \"01\"],\n+\t\t[2, \"02\"]\n+\t],\n+\n+\t\"rows\": 3,\n+\n+\t\"rows_before_limit_at_least\": 3\n+}\n+\n+-- Assert rows_before_limit for distributed ORDER BY + LIMIT BY + LIMIT, exact\n+{\n+\t\"meta\":\n+\t[\n+\t\t{\n+\t\t\t\"name\": \"id\",\n+\t\t\t\"type\": \"Int32\"\n+\t\t},\n+\t\t{\n+\t\t\t\"name\": \"val\",\n+\t\t\t\"type\": \"String\"\n+\t\t}\n+\t],\n+\n+\t\"data\":\n+\t[\n+\t\t[0, \"00\"],\n+\t\t[1, \"01\"],\n+\t\t[2, \"02\"]\n+\t],\n+\n+\t\"rows\": 3,\n+\n+\t\"rows_before_limit_at_least\": 10\n+}\n+\n+-- Assert rows_before_limit for distributed HAVING + ORDER BY + LIMIT BY + LIMIT, exact\n+{\n+\t\"meta\":\n+\t[\n+\t\t{\n+\t\t\t\"name\": \"id\",\n+\t\t\t\"type\": \"Int32\"\n+\t\t},\n+\t\t{\n+\t\t\t\"name\": \"val\",\n+\t\t\t\"type\": \"String\"\n+\t\t}\n+\t],\n+\n+\t\"data\":\n+\t[\n+\t\t[0, \"40\"],\n+\t\t[1, \"41\"],\n+\t\t[2, \"42\"]\n+\t],\n+\n+\t\"rows\": 3,\n+\n+\t\"rows_before_limit_at_least\": 7\n+}\n+\n+-- Assert rows_before_limit for distributed without LIMIT BY on initiator, exact\n+{\n+\t\"meta\":\n+\t[\n+\t\t{\n+\t\t\t\"name\": \"id\",\n+\t\t\t\"type\": \"Int32\"\n+\t\t},\n+\t\t{\n+\t\t\t\"name\": \"max(val)\",\n+\t\t\t\"type\": \"String\"\n+\t\t}\n+\t],\n+\n+\t\"data\":\n+\t[\n+\t\t[0, \"40\"],\n+\t\t[0, \"40\"],\n+\t\t[1, \"41\"],\n+\t\t[1, \"41\"]\n+\t],\n+\n+\t\"rows\": 4,\n+\n+\t\"rows_before_limit_at_least\": 20\n+}\ndiff --git a/tests/queries/0_stateless/03408_limit_by_rows_before_limit_dist.sql b/tests/queries/0_stateless/03408_limit_by_rows_before_limit_dist.sql\nnew file mode 100644\nindex 000000000000..48cb4a2241b0\n--- /dev/null\n+++ b/tests/queries/0_stateless/03408_limit_by_rows_before_limit_dist.sql\n@@ -0,0 +1,42 @@\n+-- Tags: shard\n+\n+SET output_format_write_statistics = 0;\n+\n+DROP TABLE IF EXISTS 03408_local;\n+DROP TABLE IF EXISTS 03408_dist;\n+\n+CREATE TABLE 03408_local (id Int32, val String) ENGINE = MergeTree ORDER BY tuple() SETTINGS min_bytes_for_wide_part=1\n+AS\n+SELECT number % 10, leftPad(toString(number), 2, '0') FROM numbers(50);\n+\n+CREATE TABLE 03408_dist(id Int32, val String) engine = Distributed(test_cluster_two_shards, currentDatabase(), 03408_local, id);\n+\n+SELECT '-- Assert total number of groups and records in distributed';\n+SELECT uniqExact(id), count() FROM 03408_dist;\n+\n+SELECT '';\n+SELECT '-- Assert rows_before_limit for distributed ORDER BY + LIMIT BY + LIMIT';\n+\n+SELECT id, val FROM 03408_dist ORDER BY id, val LIMIT 1 BY id LIMIT 3\n+FORMAT JsonCompact SETTINGS max_block_size=1, exact_rows_before_limit=0;\n+\n+SELECT '';\n+SELECT '-- Assert rows_before_limit for distributed ORDER BY + LIMIT BY + LIMIT, exact';\n+\n+SELECT id, val FROM 03408_dist ORDER BY id, val LIMIT 1 BY id LIMIT 3\n+FORMAT JsonCompact SETTINGS max_block_size=1, exact_rows_before_limit=1;\n+\n+SELECT '';\n+SELECT '-- Assert rows_before_limit for distributed HAVING + ORDER BY + LIMIT BY + LIMIT, exact';\n+\n+SELECT id, val FROM 03408_dist GROUP BY id, val HAVING id < 7 ORDER BY id, val DESC LIMIT 1 BY id LIMIT 3\n+FORMAT JsonCompact SETTINGS max_block_size=1, exact_rows_before_limit=1;\n+\n+SELECT '';\n+SELECT '-- Assert rows_before_limit for distributed without LIMIT BY on initiator, exact';\n+\n+SELECT id, max(val) FROM 03408_dist GROUP BY id ORDER BY id LIMIT 1 BY id LIMIT 4\n+FORMAT JSONCompact SETTINGS max_block_size=1, exact_rows_before_limit = 1, distributed_group_by_no_merge=2;\n+\n+DROP TABLE 03408_local;\n+DROP TABLE 03408_dist;\ndiff --git a/tests/queries/0_stateless/03408_limit_by_rows_before_limit_mem.reference b/tests/queries/0_stateless/03408_limit_by_rows_before_limit_mem.reference\nnew file mode 100644\nindex 000000000000..0dd922754c02\n--- /dev/null\n+++ b/tests/queries/0_stateless/03408_limit_by_rows_before_limit_mem.reference\n@@ -0,0 +1,54 @@\n+-- Assert total number of groups and records in memory\n+10\t50\n+\n+-- Assert rows_before_limit for memory ORDER BY + LIMIT BY + LIMIT, exact\n+{\n+\t\"meta\":\n+\t[\n+\t\t{\n+\t\t\t\"name\": \"id\",\n+\t\t\t\"type\": \"Int32\"\n+\t\t},\n+\t\t{\n+\t\t\t\"name\": \"val\",\n+\t\t\t\"type\": \"String\"\n+\t\t}\n+\t],\n+\n+\t\"data\":\n+\t[\n+\t\t[0, \"00\"],\n+\t\t[1, \"01\"],\n+\t\t[2, \"02\"]\n+\t],\n+\n+\t\"rows\": 3,\n+\n+\t\"rows_before_limit_at_least\": 10\n+}\n+\n+-- Assert rows_before_limit for memory HAVING + ORDER BY + LIMIT BY + LIMIT, exact\n+{\n+\t\"meta\":\n+\t[\n+\t\t{\n+\t\t\t\"name\": \"id\",\n+\t\t\t\"type\": \"Int32\"\n+\t\t},\n+\t\t{\n+\t\t\t\"name\": \"val\",\n+\t\t\t\"type\": \"String\"\n+\t\t}\n+\t],\n+\n+\t\"data\":\n+\t[\n+\t\t[0, \"40\"],\n+\t\t[1, \"41\"],\n+\t\t[2, \"42\"]\n+\t],\n+\n+\t\"rows\": 3,\n+\n+\t\"rows_before_limit_at_least\": 7\n+}\ndiff --git a/tests/queries/0_stateless/03408_limit_by_rows_before_limit_mem.sql b/tests/queries/0_stateless/03408_limit_by_rows_before_limit_mem.sql\nnew file mode 100644\nindex 000000000000..8c80ff80453b\n--- /dev/null\n+++ b/tests/queries/0_stateless/03408_limit_by_rows_before_limit_mem.sql\n@@ -0,0 +1,24 @@\n+-- Tags: no-parallel-replicas\n+\n+SET output_format_write_statistics = 0;\n+\n+DROP TABLE IF EXISTS 03408_memory;\n+\n+CREATE TABLE 03408_memory (id Int32, val String) ENGINE = Memory\n+AS\n+SELECT number % 10, leftPad(toString(number), 2, '0') FROM numbers(50);\n+\n+SELECT '-- Assert total number of groups and records in memory';\n+SELECT uniqExact(id), count() FROM 03408_memory;\n+\n+SELECT '';\n+SELECT '-- Assert rows_before_limit for memory ORDER BY + LIMIT BY + LIMIT, exact';\n+SELECT id, val FROM 03408_memory ORDER BY id, val LIMIT 1 BY id LIMIT 3\n+FORMAT JsonCompact SETTINGS exact_rows_before_limit=1;\n+\n+SELECT '';\n+SELECT '-- Assert rows_before_limit for memory HAVING + ORDER BY + LIMIT BY + LIMIT, exact';\n+SELECT id, val FROM 03408_memory GROUP BY id, val HAVING id < 7 ORDER BY id, val DESC LIMIT 1 BY id LIMIT 3\n+FORMAT JsonCompact SETTINGS exact_rows_before_limit=1;\n+\n+DROP TABLE 03408_memory;\n",
  "problem_statement": "rows_before_limit_at_least and COUNT(*) OVER() window function do not return expected count with LIMIT BY\nWe want to implement pagination using LIMIT (and OFFSET) and for that we need the total number of rows. We have so far used the rows_before_limit_at_least for this but have experienced that it for some queries do not give the expected result. It gives a number bigger than what is actually true. The issue can be boiled down to the following example:\r\n\r\n```\r\ncreate table if not exists test\r\n(\r\n    id      Int8,\r\n    text    String,\r\n    number  Int8\r\n)\r\n    engine = MergeTree()\r\n        PARTITION BY id % 2\r\n        ORDER BY (id, text);\r\n\r\ninsert into test (id, text, number)\r\nVALUES (1, 'a', 1),\r\n       (1, 'b', 2),\r\n       (2, 'c', 3);\r\n\r\nselect id, text, sum(number), COUNT(*) OVER() as window_function_count\r\nfrom test\r\ngroup by id, text\r\norder by text desc\r\nlimit 1 by id\r\nlimit 5\r\nsettings allow_experimental_window_functions = 1\r\nFORMAT\r\nJSON\r\n```\r\n\r\nwhich gives the following output:\r\n\r\n```\r\n{\r\n\t\"meta\":\r\n\t[\r\n\t\t{\r\n\t\t\t\"name\": \"id\",\r\n\t\t\t\"type\": \"Int8\"\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"name\": \"text\",\r\n\t\t\t\"type\": \"String\"\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"name\": \"sum(number)\",\r\n\t\t\t\"type\": \"Int64\"\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"name\": \"window_function_count\",\r\n\t\t\t\"type\": \"UInt64\"\r\n\t\t}\r\n\t],\r\n\r\n\t\"data\":\r\n\t[\r\n\t\t{\r\n\t\t\t\"id\": 2,\r\n\t\t\t\"text\": \"c\",\r\n\t\t\t\"sum(number)\": \"3\",\r\n\t\t\t\"window_function_count\": \"3\"\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"id\": 1,\r\n\t\t\t\"text\": \"b\",\r\n\t\t\t\"sum(number)\": \"2\",\r\n\t\t\t\"window_function_count\": \"3\"\r\n\t\t}                                                                                                                                                             \r\n\t],\r\n\r\n\t\"rows\": 2,\r\n\r\n\t\"rows_before_limit_at_least\": 3,\r\n\r\n\t\"statistics\":\r\n\t{\r\n\t\t\"elapsed\": 0.009523299,\r\n\t\t\"rows_read\": 3,\r\n\t\t\"bytes_read\": 36\r\n\t}\r\n}\r\n\r\n```\r\n\r\nAs expected the result returns two rows as it is limited to one row for each `id`, but both the count over window function and the `rows_before_limit_at_least` says there are three rows which is clearly not the case for this query. They count in the row which should be excluded by the `LIMIT 1 BY id`.\r\n\r\n**So the question is:** Is this as expected or is it a bug? And in the case that this is expected behavior is there a different way to get the total rows without doing a query with count(*) as well as the \"normal\" query?\n",
  "hints_text": "",
  "created_at": "2025-04-10T17:49:13Z"
}