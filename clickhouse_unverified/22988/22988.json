{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 22988,
  "instance_id": "ClickHouse__ClickHouse-22988",
  "issue_numbers": [
    "21957"
  ],
  "base_commit": "9b546f3b8963f542822d90c8ca9013e617ed772b",
  "patch": "diff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex d31073ae9329..0eebee4e3ae2 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -70,6 +70,7 @@ class IColumn;\n     M(UInt64, connections_with_failover_max_tries, DBMS_CONNECTION_POOL_WITH_FAILOVER_DEFAULT_MAX_TRIES, \"The maximum number of attempts to connect to replicas.\", 0) \\\n     M(UInt64, s3_min_upload_part_size, 512*1024*1024, \"The minimum size of part to upload during multipart upload to S3.\", 0) \\\n     M(UInt64, s3_max_single_part_upload_size, 64*1024*1024, \"The maximum size of object to upload using singlepart upload to S3.\", 0) \\\n+    M(UInt64, s3_max_single_read_retries, 4, \"The maximum number of retries during single S3 read.\", 0) \\\n     M(UInt64, s3_max_redirects, 10, \"Max number of S3 redirects hops allowed.\", 0) \\\n     M(UInt64, s3_max_connections, 1024, \"The maximum number of connections per server.\", 0) \\\n     M(Bool, extremes, false, \"Calculate minimums and maximums of the result columns. They can be output in JSON-formats.\", IMPORTANT) \\\ndiff --git a/src/Disks/S3/DiskS3.cpp b/src/Disks/S3/DiskS3.cpp\nindex 1de4ab843ac5..90c69b7f3b36 100644\n--- a/src/Disks/S3/DiskS3.cpp\n+++ b/src/Disks/S3/DiskS3.cpp\n@@ -249,8 +249,12 @@ class ReadIndirectBufferFromS3 final : public ReadBufferFromFileBase\n {\n public:\n     ReadIndirectBufferFromS3(\n-        std::shared_ptr<Aws::S3::S3Client> client_ptr_, const String & bucket_, DiskS3::Metadata metadata_, size_t buf_size_)\n-        : client_ptr(std::move(client_ptr_)), bucket(bucket_), metadata(std::move(metadata_)), buf_size(buf_size_)\n+        std::shared_ptr<Aws::S3::S3Client> client_ptr_, const String & bucket_, DiskS3::Metadata metadata_, UInt64 s3_max_single_read_retries_, size_t buf_size_)\n+        : client_ptr(std::move(client_ptr_))\n+        , bucket(bucket_)\n+        , metadata(std::move(metadata_))\n+        , s3_max_single_read_retries(s3_max_single_read_retries_)\n+        , buf_size(buf_size_)\n     {\n     }\n \n@@ -306,7 +310,7 @@ class ReadIndirectBufferFromS3 final : public ReadBufferFromFileBase\n             const auto & [path, size] = metadata.s3_objects[i];\n             if (size > offset)\n             {\n-                auto buf = std::make_unique<ReadBufferFromS3>(client_ptr, bucket, metadata.s3_root_path + path, buf_size);\n+                auto buf = std::make_unique<ReadBufferFromS3>(client_ptr, bucket, metadata.s3_root_path + path, s3_max_single_read_retries, buf_size);\n                 buf->seek(offset, SEEK_SET);\n                 return buf;\n             }\n@@ -335,7 +339,7 @@ class ReadIndirectBufferFromS3 final : public ReadBufferFromFileBase\n \n         ++current_buf_idx;\n         const auto & path = metadata.s3_objects[current_buf_idx].first;\n-        current_buf = std::make_unique<ReadBufferFromS3>(client_ptr, bucket, metadata.s3_root_path + path, buf_size);\n+        current_buf = std::make_unique<ReadBufferFromS3>(client_ptr, bucket, metadata.s3_root_path + path, s3_max_single_read_retries, buf_size);\n         current_buf->next();\n         working_buffer = current_buf->buffer();\n         absolute_position += working_buffer.size();\n@@ -346,6 +350,7 @@ class ReadIndirectBufferFromS3 final : public ReadBufferFromFileBase\n     std::shared_ptr<Aws::S3::S3Client> client_ptr;\n     const String & bucket;\n     DiskS3::Metadata metadata;\n+    UInt64 s3_max_single_read_retries;\n     size_t buf_size;\n \n     size_t absolute_position = 0;\n@@ -559,6 +564,7 @@ DiskS3::DiskS3(\n     String bucket_,\n     String s3_root_path_,\n     String metadata_path_,\n+    UInt64 s3_max_single_read_retries_,\n     size_t min_upload_part_size_,\n     size_t max_single_part_upload_size_,\n     size_t min_bytes_for_seek_,\n@@ -572,6 +578,7 @@ DiskS3::DiskS3(\n     , bucket(std::move(bucket_))\n     , s3_root_path(std::move(s3_root_path_))\n     , metadata_path(std::move(metadata_path_))\n+    , s3_max_single_read_retries(s3_max_single_read_retries_)\n     , min_upload_part_size(min_upload_part_size_)\n     , max_single_part_upload_size(max_single_part_upload_size_)\n     , min_bytes_for_seek(min_bytes_for_seek_)\n@@ -678,7 +685,7 @@ std::unique_ptr<ReadBufferFromFileBase> DiskS3::readFile(const String & path, si\n     LOG_DEBUG(&Poco::Logger::get(\"DiskS3\"), \"Read from file by path: {}. Existing S3 objects: {}\",\n         backQuote(metadata_path + path), metadata.s3_objects.size());\n \n-    auto reader = std::make_unique<ReadIndirectBufferFromS3>(client, bucket, metadata, buf_size);\n+    auto reader = std::make_unique<ReadIndirectBufferFromS3>(client, bucket, metadata, s3_max_single_read_retries, buf_size);\n     return std::make_unique<SeekAvoidingReadBuffer>(std::move(reader), min_bytes_for_seek);\n }\n \n@@ -982,7 +989,7 @@ int DiskS3::readSchemaVersion(const String & source_bucket, const String & sourc\n     if (!checkObjectExists(source_bucket, source_path + SCHEMA_VERSION_OBJECT))\n         return version;\n \n-    ReadBufferFromS3 buffer (client, source_bucket, source_path + SCHEMA_VERSION_OBJECT);\n+    ReadBufferFromS3 buffer(client, source_bucket, source_path + SCHEMA_VERSION_OBJECT, s3_max_single_read_retries);\n     readIntText(version, buffer);\n \n     return version;\ndiff --git a/src/Disks/S3/DiskS3.h b/src/Disks/S3/DiskS3.h\nindex 87aab71fc44b..72548addfdf5 100644\n--- a/src/Disks/S3/DiskS3.h\n+++ b/src/Disks/S3/DiskS3.h\n@@ -40,6 +40,7 @@ class DiskS3 : public IDisk\n         String bucket_,\n         String s3_root_path_,\n         String metadata_path_,\n+        UInt64 s3_max_single_read_retries_,\n         size_t min_upload_part_size_,\n         size_t max_single_part_upload_size_,\n         size_t min_bytes_for_seek_,\n@@ -180,6 +181,7 @@ class DiskS3 : public IDisk\n     const String bucket;\n     const String s3_root_path;\n     String metadata_path;\n+    UInt64 s3_max_single_read_retries;\n     size_t min_upload_part_size;\n     size_t max_single_part_upload_size;\n     size_t min_bytes_for_seek;\ndiff --git a/src/Disks/S3/registerDiskS3.cpp b/src/Disks/S3/registerDiskS3.cpp\nindex a15b6bcf8228..2a9dbda930db 100644\n--- a/src/Disks/S3/registerDiskS3.cpp\n+++ b/src/Disks/S3/registerDiskS3.cpp\n@@ -152,6 +152,7 @@ void registerDiskS3(DiskFactory & factory)\n             uri.bucket,\n             uri.key,\n             metadata_path,\n+            context->getSettingsRef().s3_max_single_read_retries,\n             context->getSettingsRef().s3_min_upload_part_size,\n             context->getSettingsRef().s3_max_single_part_upload_size,\n             config.getUInt64(config_prefix + \".min_bytes_for_seek\", 1024 * 1024),\ndiff --git a/src/IO/ReadBufferFromS3.cpp b/src/IO/ReadBufferFromS3.cpp\nindex fd07a7f309ad..1e27b0284b76 100644\n--- a/src/IO/ReadBufferFromS3.cpp\n+++ b/src/IO/ReadBufferFromS3.cpp\n@@ -12,10 +12,12 @@\n \n #    include <utility>\n \n+\n namespace ProfileEvents\n {\n     extern const Event S3ReadMicroseconds;\n     extern const Event S3ReadBytes;\n+    extern const Event S3ReadRequestsErrors;\n }\n \n namespace DB\n@@ -29,26 +31,58 @@ namespace ErrorCodes\n \n \n ReadBufferFromS3::ReadBufferFromS3(\n-    std::shared_ptr<Aws::S3::S3Client> client_ptr_, const String & bucket_, const String & key_, size_t buffer_size_)\n-    : SeekableReadBuffer(nullptr, 0), client_ptr(std::move(client_ptr_)), bucket(bucket_), key(key_), buffer_size(buffer_size_)\n+    std::shared_ptr<Aws::S3::S3Client> client_ptr_, const String & bucket_, const String & key_, UInt64 s3_max_single_read_retries_, size_t buffer_size_)\n+    : SeekableReadBuffer(nullptr, 0)\n+    , client_ptr(std::move(client_ptr_))\n+    , bucket(bucket_)\n+    , key(key_)\n+    , s3_max_single_read_retries(s3_max_single_read_retries_)\n+    , buffer_size(buffer_size_)\n {\n }\n \n-\n bool ReadBufferFromS3::nextImpl()\n {\n-    if (!initialized)\n-    {\n+    /// Restoring valid value of `count()` during `nextImpl()`. See `ReadBuffer::next()`.\n+    pos = working_buffer.begin();\n+\n+    if (!impl)\n         impl = initialize();\n-        initialized = true;\n-    }\n \n     Stopwatch watch;\n-    auto res = impl->next();\n+    bool next_result = false;\n+\n+    for (Int64 attempt = static_cast<Int64>(s3_max_single_read_retries); attempt >= 0; --attempt)\n+    {\n+        if (!impl)\n+            impl = initialize();\n+\n+        try\n+        {\n+            next_result = impl->next();\n+            /// FIXME. 1. Poco `istream` cannot read less than buffer_size or this state is being discarded during\n+            ///           istream <-> iostream conversion. `gcount` always contains 0,\n+            ///           that's why we always have error \"Cannot read from istream at offset 0\".\n+\n+            break;\n+        }\n+        catch (const Exception & e)\n+        {\n+            ProfileEvents::increment(ProfileEvents::S3ReadRequestsErrors, 1);\n+\n+            LOG_INFO(log, \"Caught exception while reading S3 object. Bucket: {}, Key: {}, Offset: {}, Remaining attempts: {}, Message: {}\",\n+                    bucket, key, getPosition(), attempt, e.message());\n+\n+            impl.reset();\n+\n+            if (!attempt)\n+                throw;\n+        }\n+    }\n+\n     watch.stop();\n     ProfileEvents::increment(ProfileEvents::S3ReadMicroseconds, watch.elapsedMicroseconds());\n-\n-    if (!res)\n+    if (!next_result)\n         return false;\n     internal_buffer = impl->buffer();\n \n@@ -60,7 +94,7 @@ bool ReadBufferFromS3::nextImpl()\n \n off_t ReadBufferFromS3::seek(off_t offset_, int whence)\n {\n-    if (initialized)\n+    if (impl)\n         throw Exception(\"Seek is allowed only before first read attempt from the buffer.\", ErrorCodes::CANNOT_SEEK_THROUGH_FILE);\n \n     if (whence != SEEK_SET)\n@@ -74,7 +108,6 @@ off_t ReadBufferFromS3::seek(off_t offset_, int whence)\n     return offset;\n }\n \n-\n off_t ReadBufferFromS3::getPosition()\n {\n     return offset + count();\n@@ -82,13 +115,13 @@ off_t ReadBufferFromS3::getPosition()\n \n std::unique_ptr<ReadBuffer> ReadBufferFromS3::initialize()\n {\n-    LOG_TRACE(log, \"Read S3 object. Bucket: {}, Key: {}, Offset: {}\", bucket, key, std::to_string(offset));\n+    LOG_TRACE(log, \"Read S3 object. Bucket: {}, Key: {}, Offset: {}\", bucket, key, getPosition());\n \n     Aws::S3::Model::GetObjectRequest req;\n     req.SetBucket(bucket);\n     req.SetKey(key);\n-    if (offset != 0)\n-        req.SetRange(\"bytes=\" + std::to_string(offset) + \"-\");\n+    if (getPosition())\n+        req.SetRange(\"bytes=\" + std::to_string(getPosition()) + \"-\");\n \n     Aws::S3::Model::GetObjectOutcome outcome = client_ptr->GetObject(req);\n \ndiff --git a/src/IO/ReadBufferFromS3.h b/src/IO/ReadBufferFromS3.h\nindex 829b73d0af6e..1f4124d909fb 100644\n--- a/src/IO/ReadBufferFromS3.h\n+++ b/src/IO/ReadBufferFromS3.h\n@@ -27,8 +27,8 @@ class ReadBufferFromS3 : public SeekableReadBuffer\n     std::shared_ptr<Aws::S3::S3Client> client_ptr;\n     String bucket;\n     String key;\n+    UInt64 s3_max_single_read_retries;\n     size_t buffer_size;\n-    bool initialized = false;\n     off_t offset = 0;\n     Aws::S3::Model::GetObjectResult read_result;\n     std::unique_ptr<ReadBuffer> impl;\n@@ -40,6 +40,7 @@ class ReadBufferFromS3 : public SeekableReadBuffer\n         std::shared_ptr<Aws::S3::S3Client> client_ptr_,\n         const String & bucket_,\n         const String & key_,\n+        UInt64 s3_max_single_read_retries_,\n         size_t buffer_size_ = DBMS_DEFAULT_BUFFER_SIZE);\n \n     bool nextImpl() override;\ndiff --git a/src/Storages/StorageS3.cpp b/src/Storages/StorageS3.cpp\nindex a5cbd004d559..0bda8fac3ed6 100644\n--- a/src/Storages/StorageS3.cpp\n+++ b/src/Storages/StorageS3.cpp\n@@ -166,6 +166,7 @@ StorageS3Source::StorageS3Source(\n     ContextPtr context_,\n     const ColumnsDescription & columns_,\n     UInt64 max_block_size_,\n+    UInt64 s3_max_single_read_retries_,\n     const String compression_hint_,\n     const std::shared_ptr<Aws::S3::S3Client> & client_,\n     const String & bucket_,\n@@ -177,6 +178,7 @@ StorageS3Source::StorageS3Source(\n     , format(format_)\n     , columns_desc(columns_)\n     , max_block_size(max_block_size_)\n+    , s3_max_single_read_retries(s3_max_single_read_retries_)\n     , compression_hint(compression_hint_)\n     , client(client_)\n     , sample_block(sample_block_)\n@@ -197,7 +199,7 @@ bool StorageS3Source::initialize()\n     file_path = bucket + \"/\" + current_key;\n \n     read_buf = wrapReadBufferWithCompressionMethod(\n-        std::make_unique<ReadBufferFromS3>(client, bucket, current_key), chooseCompressionMethod(current_key, compression_hint));\n+        std::make_unique<ReadBufferFromS3>(client, bucket, current_key, s3_max_single_read_retries), chooseCompressionMethod(current_key, compression_hint));\n     auto input_format = FormatFactory::instance().getInput(format, *read_buf, sample_block, getContext(), max_block_size);\n     reader = std::make_shared<InputStreamFromInputFormat>(input_format);\n \n@@ -312,6 +314,7 @@ StorageS3::StorageS3(\n     const String & secret_access_key_,\n     const StorageID & table_id_,\n     const String & format_name_,\n+    UInt64 s3_max_single_read_retries_,\n     UInt64 min_upload_part_size_,\n     UInt64 max_single_part_upload_size_,\n     UInt64 max_connections_,\n@@ -323,6 +326,7 @@ StorageS3::StorageS3(\n     : IStorage(table_id_)\n     , client_auth{uri_, access_key_id_, secret_access_key_, max_connections_, {}, {}} /// Client and settings will be updated later\n     , format_name(format_name_)\n+    , s3_max_single_read_retries(s3_max_single_read_retries_)\n     , min_upload_part_size(min_upload_part_size_)\n     , max_single_part_upload_size(max_single_part_upload_size_)\n     , compression_method(compression_method_)\n@@ -389,6 +393,7 @@ Pipe StorageS3::read(\n             local_context,\n             metadata_snapshot->getColumns(),\n             max_block_size,\n+            s3_max_single_read_retries,\n             compression_method,\n             client_auth.client,\n             client_auth.uri.bucket,\n@@ -473,6 +478,7 @@ void registerStorageS3Impl(const String & name, StorageFactory & factory)\n             secret_access_key = engine_args[2]->as<ASTLiteral &>().value.safeGet<String>();\n         }\n \n+        UInt64 s3_max_single_read_retries = args.getLocalContext()->getSettingsRef().s3_max_single_read_retries;\n         UInt64 min_upload_part_size = args.getLocalContext()->getSettingsRef().s3_min_upload_part_size;\n         UInt64 max_single_part_upload_size = args.getLocalContext()->getSettingsRef().s3_max_single_part_upload_size;\n         UInt64 max_connections = args.getLocalContext()->getSettingsRef().s3_max_connections;\n@@ -496,6 +502,7 @@ void registerStorageS3Impl(const String & name, StorageFactory & factory)\n             secret_access_key,\n             args.table_id,\n             format_name,\n+            s3_max_single_read_retries,\n             min_upload_part_size,\n             max_single_part_upload_size,\n             max_connections,\ndiff --git a/src/Storages/StorageS3.h b/src/Storages/StorageS3.h\nindex 1e1d76fa6e3f..b068f82cfb1a 100644\n--- a/src/Storages/StorageS3.h\n+++ b/src/Storages/StorageS3.h\n@@ -55,6 +55,7 @@ class StorageS3Source : public SourceWithProgress, WithContext\n         ContextPtr context_,\n         const ColumnsDescription & columns_,\n         UInt64 max_block_size_,\n+        UInt64 s3_max_single_read_retries_,\n         const String compression_hint_,\n         const std::shared_ptr<Aws::S3::S3Client> & client_,\n         const String & bucket,\n@@ -71,6 +72,7 @@ class StorageS3Source : public SourceWithProgress, WithContext\n     String format;\n     ColumnsDescription columns_desc;\n     UInt64 max_block_size;\n+    UInt64 s3_max_single_read_retries;\n     String compression_hint;\n     std::shared_ptr<Aws::S3::S3Client> client;\n     Block sample_block;\n@@ -100,6 +102,7 @@ class StorageS3 : public ext::shared_ptr_helper<StorageS3>, public IStorage, Wit\n         const String & secret_access_key,\n         const StorageID & table_id_,\n         const String & format_name_,\n+        UInt64 s3_max_single_read_retries_,\n         UInt64 min_upload_part_size_,\n         UInt64 max_single_part_upload_size_,\n         UInt64 max_connections_,\n@@ -145,6 +148,7 @@ class StorageS3 : public ext::shared_ptr_helper<StorageS3>, public IStorage, Wit\n     ClientAuthentificaiton client_auth;\n \n     String format_name;\n+    UInt64 s3_max_single_read_retries;\n     size_t min_upload_part_size;\n     size_t max_single_part_upload_size;\n     String compression_method;\ndiff --git a/src/TableFunctions/TableFunctionS3.cpp b/src/TableFunctions/TableFunctionS3.cpp\nindex 2da597f49ff9..973899d21019 100644\n--- a/src/TableFunctions/TableFunctionS3.cpp\n+++ b/src/TableFunctions/TableFunctionS3.cpp\n@@ -83,6 +83,7 @@ StoragePtr TableFunctionS3::executeImpl(const ASTPtr & /*ast_function*/, Context\n {\n     Poco::URI uri (filename);\n     S3::URI s3_uri (uri);\n+    UInt64 s3_max_single_read_retries = context->getSettingsRef().s3_max_single_read_retries;\n     UInt64 min_upload_part_size = context->getSettingsRef().s3_min_upload_part_size;\n     UInt64 max_single_part_upload_size = context->getSettingsRef().s3_max_single_part_upload_size;\n     UInt64 max_connections = context->getSettingsRef().s3_max_connections;\n@@ -93,6 +94,7 @@ StoragePtr TableFunctionS3::executeImpl(const ASTPtr & /*ast_function*/, Context\n             secret_access_key,\n             StorageID(getDatabaseName(), table_name),\n             format,\n+            s3_max_single_read_retries,\n             min_upload_part_size,\n             max_single_part_upload_size,\n             max_connections,\ndiff --git a/src/TableFunctions/TableFunctionS3Cluster.cpp b/src/TableFunctions/TableFunctionS3Cluster.cpp\nindex 26ef07ef97fe..16f48c706089 100644\n--- a/src/TableFunctions/TableFunctionS3Cluster.cpp\n+++ b/src/TableFunctions/TableFunctionS3Cluster.cpp\n@@ -109,12 +109,17 @@ StoragePtr TableFunctionS3Cluster::executeImpl(\n         Poco::URI uri (filename);\n         S3::URI s3_uri (uri);\n         /// Actually this parameters are not used\n+        UInt64 s3_max_single_read_retries = context->getSettingsRef().s3_max_single_read_retries;\n         UInt64 min_upload_part_size = context->getSettingsRef().s3_min_upload_part_size;\n         UInt64 max_single_part_upload_size = context->getSettingsRef().s3_max_single_part_upload_size;\n         UInt64 max_connections = context->getSettingsRef().s3_max_connections;\n         storage = StorageS3::create(\n             s3_uri, access_key_id, secret_access_key, StorageID(getDatabaseName(), table_name),\n-            format, min_upload_part_size, max_single_part_upload_size, max_connections,\n+            format,\n+            s3_max_single_read_retries,\n+            min_upload_part_size,\n+            max_single_part_upload_size,\n+            max_connections,\n             getActualTableStructure(context), ConstraintsDescription{},\n             context, compression_method, /*distributed_processing=*/true);\n     }\n",
  "test_patch": "diff --git a/tests/integration/test_storage_s3/s3_mock/mock_s3.py b/tests/integration/test_storage_s3/s3_mocks/mock_s3.py\nsimilarity index 89%\nrename from tests/integration/test_storage_s3/s3_mock/mock_s3.py\nrename to tests/integration/test_storage_s3/s3_mocks/mock_s3.py\nindex 088cc883e578..3e876689175d 100644\n--- a/tests/integration/test_storage_s3/s3_mock/mock_s3.py\n+++ b/tests/integration/test_storage_s3/s3_mocks/mock_s3.py\n@@ -1,3 +1,5 @@\n+import sys\n+\n from bottle import abort, route, run, request, response\n \n \n@@ -21,4 +23,4 @@ def ping():\n     return 'OK'\n \n \n-run(host='0.0.0.0', port=8080)\n+run(host='0.0.0.0', port=int(sys.argv[1]))\ndiff --git a/tests/integration/test_storage_s3/s3_mocks/unstable_server.py b/tests/integration/test_storage_s3/s3_mocks/unstable_server.py\nnew file mode 100644\nindex 000000000000..4a27845ff9fe\n--- /dev/null\n+++ b/tests/integration/test_storage_s3/s3_mocks/unstable_server.py\n@@ -0,0 +1,90 @@\n+import http.server\n+import random\n+import re\n+import socket\n+import struct\n+import sys\n+\n+\n+def gen_n_digit_number(n):\n+    assert 0 < n < 19\n+    return random.randint(10**(n-1), 10**n-1)\n+\n+\n+def gen_line():\n+    columns = 4\n+\n+    row = []\n+    def add_number():\n+        digits = random.randint(1, 18)\n+        row.append(gen_n_digit_number(digits))\n+\n+    for i in range(columns // 2):\n+        add_number()\n+    row.append(1)\n+    for i in range(columns - 1 - columns // 2):\n+        add_number()\n+\n+    line = \",\".join(map(str, row)) + \"\\n\"\n+    return line.encode()\n+\n+\n+random.seed(\"Unstable server/1.0\")\n+lines = b\"\".join((gen_line() for _ in range(500000)))\n+\n+\n+class RequestHandler(http.server.BaseHTTPRequestHandler):\n+    def do_HEAD(self):\n+        if self.path == \"/root/test.csv\":\n+            self.from_bytes = 0\n+            self.end_bytes = len(lines)\n+            self.size = self.end_bytes\n+            self.send_block_size = 256\n+            self.stop_at = random.randint(900000, 1200000) // self.send_block_size # Block size is 1024**2.\n+\n+            if \"Range\" in self.headers:\n+                cr = self.headers[\"Range\"]\n+                parts = re.split(\"[ -/=]+\", cr)\n+                assert parts[0] == \"bytes\"\n+                self.from_bytes = int(parts[1])\n+                if parts[2]:\n+                    self.end_bytes = int(parts[2])+1\n+                self.send_response(206)\n+                self.send_header(\"Content-Range\", f\"bytes {self.from_bytes}-{self.end_bytes-1}/{self.size}\")\n+            else:\n+                self.send_response(200)\n+\n+            self.send_header(\"Accept-Ranges\", \"bytes\")\n+            self.send_header(\"Content-Type\", \"text/plain\")\n+            self.send_header(\"Content-Length\", f\"{self.end_bytes-self.from_bytes}\")\n+            self.end_headers()\n+\n+        elif self.path == \"/\":\n+            self.send_response(200)\n+            self.send_header(\"Content-Type\", \"text/plain\")\n+            self.end_headers()\n+\n+        else:\n+            self.send_response(404)\n+            self.send_header(\"Content-Type\", \"text/plain\")\n+            self.end_headers()\n+\n+\n+    def do_GET(self):\n+        self.do_HEAD()\n+        if self.path == \"/root/test.csv\":\n+            for c, i in enumerate(range(self.from_bytes, self.end_bytes, self.send_block_size)):\n+                self.wfile.write(lines[i:min(i+self.send_block_size, self.end_bytes)])\n+                if (c + 1) % self.stop_at == 0:\n+                    #self.wfile._sock.setsockopt(socket.SOL_SOCKET, socket.SO_LINGER, struct.pack(\"ii\", 0, 0))\n+                    #self.wfile._sock.shutdown(socket.SHUT_RDWR)\n+                    #self.wfile._sock.close()\n+                    print('Dropping connection')\n+                    break\n+\n+        elif self.path == \"/\":\n+            self.wfile.write(b\"OK\")\n+\n+\n+httpd = http.server.HTTPServer((\"0.0.0.0\", int(sys.argv[1])), RequestHandler)\n+httpd.serve_forever()\ndiff --git a/tests/integration/test_storage_s3/test.py b/tests/integration/test_storage_s3/test.py\nindex 9e91aae66b31..c239dc688107 100644\n--- a/tests/integration/test_storage_s3/test.py\n+++ b/tests/integration/test_storage_s3/test.py\n@@ -96,7 +96,7 @@ def cluster():\n \n         prepare_s3_bucket(cluster)\n         logging.info(\"S3 bucket created\")\n-        run_s3_mock(cluster)\n+        run_s3_mocks(cluster)\n \n         yield cluster\n     finally:\n@@ -384,26 +384,32 @@ def add_tales(start, end):\n     assert run_query(instance, query).splitlines() == [\"1001\\t1001\\t1001\\t1001\"]\n \n \n-def run_s3_mock(cluster):\n-    logging.info(\"Starting s3 mock\")\n-    container_id = cluster.get_container_id('resolver')\n-    current_dir = os.path.dirname(__file__)\n-    cluster.copy_file_to_container(container_id, os.path.join(current_dir, \"s3_mock\", \"mock_s3.py\"), \"mock_s3.py\")\n-    cluster.exec_in_container(container_id, [\"python\", \"mock_s3.py\"], detach=True)\n-\n-    # Wait for S3 mock start\n-    for attempt in range(10):\n-        ping_response = cluster.exec_in_container(cluster.get_container_id('resolver'),\n-                                                  [\"curl\", \"-s\", \"http://resolver:8080/\"], nothrow=True)\n-        if ping_response != 'OK':\n-            if attempt == 9:\n-                assert ping_response == 'OK', 'Expected \"OK\", but got \"{}\"'.format(ping_response)\n+def run_s3_mocks(cluster):\n+    logging.info(\"Starting s3 mocks\")\n+    mocks = (\n+        (\"mock_s3.py\", \"resolver\", \"8080\"),\n+        (\"unstable_server.py\", \"resolver\", \"8081\"),\n+    )\n+    for mock_filename, container, port in mocks:\n+        container_id = cluster.get_container_id(container)\n+        current_dir = os.path.dirname(__file__)\n+        cluster.copy_file_to_container(container_id, os.path.join(current_dir, \"s3_mocks\", mock_filename), mock_filename)\n+        cluster.exec_in_container(container_id, [\"python\", mock_filename, port], detach=True)\n+\n+    # Wait for S3 mocks to start\n+    for mock_filename, container, port in mocks:\n+        for attempt in range(10):\n+            ping_response = cluster.exec_in_container(cluster.get_container_id(container),\n+                                                      [\"curl\", \"-s\", f\"http://{container}:{port}/\"], nothrow=True)\n+            if ping_response != 'OK':\n+                if attempt == 9:\n+                    assert ping_response == 'OK', 'Expected \"OK\", but got \"{}\"'.format(ping_response)\n+                else:\n+                    time.sleep(1)\n             else:\n-                time.sleep(1)\n-        else:\n-            break\n+                break\n \n-    logging.info(\"S3 mock started\")\n+    logging.info(\"S3 mocks started\")\n \n \n def replace_config(old, new):\n@@ -523,6 +529,15 @@ def test_storage_s3_get_gzip(cluster, extension, method):\n         run_query(instance, f\"DROP TABLE {name}\")\n \n \n+def test_storage_s3_get_unstable(cluster):\n+    bucket = cluster.minio_bucket\n+    instance = cluster.instances[\"dummy\"]\n+    table_format = \"column1 Int64, column2 Int64, column3 Int64, column4 Int64\"\n+    get_query = f\"SELECT count(), sum(column3) FROM s3('http://resolver:8081/{cluster.minio_bucket}/test.csv', 'CSV', '{table_format}') FORMAT CSV\"\n+    result = run_query(instance, get_query)\n+    assert result.splitlines() == [\"500000,500000\"]\n+\n+\n def test_storage_s3_put_uncompressed(cluster):\n     bucket = cluster.minio_bucket\n     instance = cluster.instances[\"dummy\"]\n",
  "problem_statement": "Read error from S3\n**Describe the bug**\r\nWhen reading from table using diskS3, such exception occurs:\r\n\r\n```\r\nCode: 23. DB::Exception: Received from localhost:9000. DB::Exception: Cannot read from istream at offset 0: (while reading column LO_ORDERDATE): (while reading from part ... from mark 14287 with max_rows_to_read = 16384): While executing MergeTreeThread.\r\n```\r\n**Does it reproduce on recent release?**\r\nYes.\r\n\r\n**Stack trace**\r\n\r\n```\r\n2021.03.22 14:39:43.010425 [ 65795 ] {} <Error> DB::IBackgroundJobExecutor::jobExecutingTask()::<lambda()>: Code: 23, e.displayText() = DB::Exception: Cannot read from istream at offset 0: (while reading column LO_SUPPKEY): While executing MergeTreeSequentialSource, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0xd1610e0 in /essd/CK/s3/programs/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x3ecfda1 in /essd/CK/s3/programs/clickhouse\r\n2. DB::ReadBufferFromIStream::nextImpl() (.cold) @ 0x3c7f652 in /essd/CK/s3/programs/clickhouse\r\n3. DB::ReadBufferFromS3::nextImpl() @ 0xca457c7 in /essd/CK/s3/programs/clickhouse\r\n4. DB::ReadIndirectBufferFromS3::nextImpl() @ 0xae6e517 in /essd/CK/s3/programs/clickhouse\r\n5. DB::SeekAvoidingReadBuffer::nextImpl() @ 0xca509c2 in /essd/CK/s3/programs/clickhouse\r\n6. DB::ReadBuffer::readStrict(char*, unsigned long) @ 0x3f02af1 in /essd/CK/s3/programs/clickhouse\r\n7. DB::CompressedReadBufferBase::readCompressedData(unsigned long&, unsigned long&, bool) @ 0x9c85949 in /essd/CK/s3/programs/clickhouse\r\n8. DB::CompressedReadBufferFromFile::readBig(char*, unsigned long) @ 0x9c86b67 in /essd/CK/s3/programs/clickhouse\r\n9. DB::DataTypeNumberBase<unsigned int>::deserializeBinaryBulk(DB::IColumn&, DB::ReadBuffer&, unsigned long, double) const @ 0x9d66eb3 in /essd/CK/s3/programs/clickhouse\r\n10. DB::IDataType::deserializeBinaryBulkWithMultipleStreams(COW<DB::IColumn>::immutable_ptr<DB::IColumn>&, unsigned long, DB::IDataType::DeserializeBinaryBulkSettings&, std::__1::shared_ptr<DB::IDataType::DeserializeBinaryBulkState>&, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > >*) const @ 0x9d97f38 in /essd/CK/s3/programs/clickhouse\r\n11. DB::MergeTreeReaderCompact::readData(DB::NameAndTypePair const&, COW<DB::IColumn>::immutable_ptr<DB::IColumn>&, unsigned long, unsigned long, unsigned long, bool) @ 0xa4505b2 in /essd/CK/s3/programs/clickhouse\r\n12. DB::MergeTreeReaderCompact::readRows(unsigned long, bool, unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&) @ 0xa450a54 in /essd/CK/s3/programs/clickhouse\r\n13. DB::MergeTreeSequentialSource::generate() @ 0xa461ba9 in /essd/CK/s3/programs/clickhouse\r\n14. DB::ISource::work() @ 0xa5af04b in /essd/CK/s3/programs/clickhouse\r\n15. DB::SourceWithProgress::work() @ 0xa70aed7 in /essd/CK/s3/programs/clickhouse\r\n16. void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::'lambda'(), void ()> >(std::__1::__function::__policy_storage const*) @ 0xa5d5bed in /essd/CK/s3/programs/clickhouse\r\n17. DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic<bool>*) (.constprop.0) @ 0xa5dac24 in /essd/CK/s3/programs/clickhouse\r\n18. DB::PipelineExecutor::executeStep(std::__1::atomic<bool>*) @ 0xa5dc5a9 in /essd/CK/s3/programs/clickhouse\r\n19. DB::PullingPipelineExecutor::pull(DB::Chunk&) @ 0xa5e4475 in /essd/CK/s3/programs/clickhouse\r\n20. DB::PullingPipelineExecutor::pull(DB::Block&) @ 0xa5e4be7 in /essd/CK/s3/programs/clickhouse\r\n21. DB::PipelineExecutingBlockInputStream::readImpl() @ 0xa5d4a78 in /essd/CK/s3/programs/clickhouse\r\n22. DB::IBlockInputStream::read() @ 0x9c9ec3e in /essd/CK/s3/programs/clickhouse\r\n23. DB::MergeTreeDataMergerMutator::mergePartsToTemporaryPart(DB::FutureMergedMutatedPart const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::BackgroundProcessListEntry<DB::MergeListElement, DB::MergeInfo>&, std::__1::shared_ptr<DB::RWLockImpl::LockHolderImpl>&, long, DB::Context const&, std::__1::unique_ptr<DB::IReservation, std::__1::default_delete<DB::IReservation> > const&, bool, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xa3d3d65 in /essd/CK/s3/programs/clickhouse\r\n24. DB::StorageMergeTree::mergeSelectedParts(std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, bool, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, DB::StorageMergeTree::MergeMutateSelectedEntry&, std::__1::shared_ptr<DB::RWLockImpl::LockHolderImpl>&) @ 0xa22dfa7 in /essd/CK/s3/programs/clickhouse\r\n25. bool std::__1::__function::__policy_invoker<bool ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::StorageMergeTree::getDataProcessingJob()::'lambda'(), bool ()> >(std::__1::__function::__policy_storage const*) @ 0xa22e64a in /essd/CK/s3/programs/clickhouse\r\n26. void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::IBackgroundJobExecutor::jobExecutingTask()::'lambda'(), void ()> >(std::__1::__function::__policy_storage const*) @ 0xa33d1b0 in /essd/CK/s3/programs/clickhouse\r\n27. ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x3f2f4f3 in /essd/CK/s3/programs/clickhouse\r\n28. ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...)::'lambda'()::operator()() @ 0x3f2fbdf in /essd/CK/s3/programs/clickhouse\r\n29. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x3f2eb83 in /essd/CK/s3/programs/clickhouse\r\n30. void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()> >(void*) @ 0x3f2e44f in /essd/CK/s3/programs/clickhouse\r\n31. start_thread @ 0x7ebe in /home/fenglv/gentoo/lib64/libpthread-2.32.so\r\n (version 21.4.1.1)\r\n```\n",
  "hints_text": "",
  "created_at": "2021-04-12T08:57:53Z",
  "modified_files": [
    "src/Core/Settings.h",
    "src/Disks/S3/DiskS3.cpp",
    "src/Disks/S3/DiskS3.h",
    "src/Disks/S3/registerDiskS3.cpp",
    "src/IO/ReadBufferFromS3.cpp",
    "src/IO/ReadBufferFromS3.h",
    "src/Storages/StorageS3.cpp",
    "src/Storages/StorageS3.h",
    "src/TableFunctions/TableFunctionS3.cpp",
    "src/TableFunctions/TableFunctionS3Cluster.cpp"
  ],
  "modified_test_files": [
    "tests/integration/test_storage_s3/s3_mock/mock_s3.py",
    "b/tests/integration/test_storage_s3/s3_mocks/unstable_server.py",
    "tests/integration/test_storage_s3/test.py"
  ]
}