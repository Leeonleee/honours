{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 34433,
  "instance_id": "ClickHouse__ClickHouse-34433",
  "issue_numbers": [
    "33964"
  ],
  "base_commit": "2cfe8578b7246404368c423306a7e200f5f6d6a7",
  "patch": "diff --git a/src/IO/WriteBufferFromS3.cpp b/src/IO/WriteBufferFromS3.cpp\nindex 275fb3957bb5..9cb8942daf12 100644\n--- a/src/IO/WriteBufferFromS3.cpp\n+++ b/src/IO/WriteBufferFromS3.cpp\n@@ -147,6 +147,10 @@ void WriteBufferFromS3::createMultipartUpload()\n     Aws::S3::Model::CreateMultipartUploadRequest req;\n     req.SetBucket(bucket);\n     req.SetKey(key);\n+\n+    /// If we don't do it, AWS SDK can mistakenly set it to application/xml, see https://github.com/aws/aws-sdk-cpp/issues/1840\n+    req.SetContentType(\"binary/octet-stream\");\n+\n     if (object_metadata.has_value())\n         req.SetMetadata(object_metadata.value());\n \n@@ -155,7 +159,7 @@ void WriteBufferFromS3::createMultipartUpload()\n     if (outcome.IsSuccess())\n     {\n         multipart_upload_id = outcome.GetResult().GetUploadId();\n-        LOG_DEBUG(log, \"Multipart upload has created. Bucket: {}, Key: {}, Upload id: {}\", bucket, key, multipart_upload_id);\n+        LOG_TRACE(log, \"Multipart upload has created. Bucket: {}, Key: {}, Upload id: {}\", bucket, key, multipart_upload_id);\n     }\n     else\n         throw Exception(outcome.GetError().GetMessage(), ErrorCodes::S3_ERROR);\n@@ -165,14 +169,14 @@ void WriteBufferFromS3::writePart()\n {\n     auto size = temporary_buffer->tellp();\n \n-    LOG_DEBUG(log, \"Writing part. Bucket: {}, Key: {}, Upload_id: {}, Size: {}\", bucket, key, multipart_upload_id, size);\n+    LOG_TRACE(log, \"Writing part. Bucket: {}, Key: {}, Upload_id: {}, Size: {}\", bucket, key, multipart_upload_id, size);\n \n     if (size < 0)\n         throw Exception(\"Failed to write part. Buffer in invalid state.\", ErrorCodes::S3_ERROR);\n \n     if (size == 0)\n     {\n-        LOG_DEBUG(log, \"Skipping writing part. Buffer is empty.\");\n+        LOG_TRACE(log, \"Skipping writing part. Buffer is empty.\");\n         return;\n     }\n \n@@ -234,6 +238,9 @@ void WriteBufferFromS3::fillUploadRequest(Aws::S3::Model::UploadPartRequest & re\n     req.SetUploadId(multipart_upload_id);\n     req.SetContentLength(temporary_buffer->tellp());\n     req.SetBody(temporary_buffer);\n+\n+    /// If we don't do it, AWS SDK can mistakenly set it to application/xml, see https://github.com/aws/aws-sdk-cpp/issues/1840\n+    req.SetContentType(\"binary/octet-stream\");\n }\n \n void WriteBufferFromS3::processUploadRequest(UploadPartTask & task)\n@@ -243,7 +250,7 @@ void WriteBufferFromS3::processUploadRequest(UploadPartTask & task)\n     if (outcome.IsSuccess())\n     {\n         task.tag = outcome.GetResult().GetETag();\n-        LOG_DEBUG(log, \"Writing part finished. Bucket: {}, Key: {}, Upload_id: {}, Etag: {}, Parts: {}\", bucket, key, multipart_upload_id, task.tag, part_tags.size());\n+        LOG_TRACE(log, \"Writing part finished. Bucket: {}, Key: {}, Upload_id: {}, Etag: {}, Parts: {}\", bucket, key, multipart_upload_id, task.tag, part_tags.size());\n     }\n     else\n         throw Exception(outcome.GetError().GetMessage(), ErrorCodes::S3_ERROR);\n@@ -253,7 +260,7 @@ void WriteBufferFromS3::processUploadRequest(UploadPartTask & task)\n \n void WriteBufferFromS3::completeMultipartUpload()\n {\n-    LOG_DEBUG(log, \"Completing multipart upload. Bucket: {}, Key: {}, Upload_id: {}, Parts: {}\", bucket, key, multipart_upload_id, part_tags.size());\n+    LOG_TRACE(log, \"Completing multipart upload. Bucket: {}, Key: {}, Upload_id: {}, Parts: {}\", bucket, key, multipart_upload_id, part_tags.size());\n \n     if (part_tags.empty())\n         throw Exception(\"Failed to complete multipart upload. No parts have uploaded\", ErrorCodes::S3_ERROR);\n@@ -275,7 +282,7 @@ void WriteBufferFromS3::completeMultipartUpload()\n     auto outcome = client_ptr->CompleteMultipartUpload(req);\n \n     if (outcome.IsSuccess())\n-        LOG_DEBUG(log, \"Multipart upload has completed. Bucket: {}, Key: {}, Upload_id: {}, Parts: {}\", bucket, key, multipart_upload_id, part_tags.size());\n+        LOG_TRACE(log, \"Multipart upload has completed. Bucket: {}, Key: {}, Upload_id: {}, Parts: {}\", bucket, key, multipart_upload_id, part_tags.size());\n     else\n     {\n         throw Exception(ErrorCodes::S3_ERROR, \"{} Tags:{}\",\n@@ -289,14 +296,14 @@ void WriteBufferFromS3::makeSinglepartUpload()\n     auto size = temporary_buffer->tellp();\n     bool with_pool = bool(schedule);\n \n-    LOG_DEBUG(log, \"Making single part upload. Bucket: {}, Key: {}, Size: {}, WithPool: {}\", bucket, key, size, with_pool);\n+    LOG_TRACE(log, \"Making single part upload. Bucket: {}, Key: {}, Size: {}, WithPool: {}\", bucket, key, size, with_pool);\n \n     if (size < 0)\n         throw Exception(\"Failed to make single part upload. Buffer in invalid state\", ErrorCodes::S3_ERROR);\n \n     if (size == 0)\n     {\n-        LOG_DEBUG(log, \"Skipping single part upload. Buffer is empty.\");\n+        LOG_TRACE(log, \"Skipping single part upload. Buffer is empty.\");\n         return;\n     }\n \n@@ -343,6 +350,9 @@ void WriteBufferFromS3::fillPutRequest(Aws::S3::Model::PutObjectRequest & req)\n     req.SetBody(temporary_buffer);\n     if (object_metadata.has_value())\n         req.SetMetadata(object_metadata.value());\n+\n+    /// If we don't do it, AWS SDK can mistakenly set it to application/xml, see https://github.com/aws/aws-sdk-cpp/issues/1840\n+    req.SetContentType(\"binary/octet-stream\");\n }\n \n void WriteBufferFromS3::processPutRequest(PutObjectTask & task)\n@@ -351,7 +361,7 @@ void WriteBufferFromS3::processPutRequest(PutObjectTask & task)\n     bool with_pool = bool(schedule);\n \n     if (outcome.IsSuccess())\n-        LOG_DEBUG(log, \"Single part upload has completed. Bucket: {}, Key: {}, Object size: {}, WithPool: {}\", bucket, key, task.req.GetContentLength(), with_pool);\n+        LOG_TRACE(log, \"Single part upload has completed. Bucket: {}, Key: {}, Object size: {}, WithPool: {}\", bucket, key, task.req.GetContentLength(), with_pool);\n     else\n         throw Exception(outcome.GetError().GetMessage(), ErrorCodes::S3_ERROR);\n }\n",
  "test_patch": "diff --git a/docker/test/stateless/Dockerfile b/docker/test/stateless/Dockerfile\nindex 9b7fde7d542a..bfc6763e8c5b 100644\n--- a/docker/test/stateless/Dockerfile\n+++ b/docker/test/stateless/Dockerfile\n@@ -31,7 +31,8 @@ RUN apt-get update -y \\\n             wget \\\n             mysql-client=8.0* \\\n             postgresql-client \\\n-            sqlite3\n+            sqlite3 \\\n+            awscli\n \n RUN pip3 install numpy scipy pandas Jinja2\n \ndiff --git a/docker/test/stateless/setup_minio.sh b/docker/test/stateless/setup_minio.sh\nindex 7f8b90ee7414..df27b21b05b7 100755\n--- a/docker/test/stateless/setup_minio.sh\n+++ b/docker/test/stateless/setup_minio.sh\n@@ -55,3 +55,10 @@ for FILE in $(ls \"${MINIO_DATA_PATH}\"); do\n     echo \"$FILE\";\n     ./mc cp \"${MINIO_DATA_PATH}\"/\"$FILE\" clickminio/test/\"$FILE\";\n done\n+\n+mkdir -p ~/.aws\n+cat <<EOT >> ~/.aws/credentials\n+[default]\n+aws_access_key_id=clickhouse\n+aws_secret_access_key=clickhouse\n+EOT\ndiff --git a/tests/queries/0_stateless/02207_s3_content_type.reference b/tests/queries/0_stateless/02207_s3_content_type.reference\nnew file mode 100644\nindex 000000000000..2b0a5bcadc29\n--- /dev/null\n+++ b/tests/queries/0_stateless/02207_s3_content_type.reference\n@@ -0,0 +1,2 @@\n+ContentLength:2144451\n+ContentType:binary/octet-stream\ndiff --git a/tests/queries/0_stateless/02207_s3_content_type.sh b/tests/queries/0_stateless/02207_s3_content_type.sh\nnew file mode 100755\nindex 000000000000..5ede30e867c1\n--- /dev/null\n+++ b/tests/queries/0_stateless/02207_s3_content_type.sh\n@@ -0,0 +1,13 @@\n+#!/usr/bin/env bash\n+# Tags: no-fasttest\n+# Tag no-fasttest: needs s3\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+$CLICKHOUSE_CLIENT --query \"\n+INSERT INTO TABLE FUNCTION s3('http://localhost:11111/test/content-type.csv.gz', 'test', 'testtest', 'CSV', 'number UInt64') SELECT number FROM numbers(1000000) SETTINGS s3_max_single_part_upload_size = 10000, s3_truncate_on_insert = 1;\n+\"\n+\n+aws --endpoint-url http://localhost:11111 s3api head-object --bucket test --key content-type.csv.gz | grep Content | sed 's/[ \\t,\"]*//g'\n",
  "problem_statement": "Wrong s3 object content type for large files\nConnected to ClickHouse server version 21.13.1 revision 54455.\r\n\r\n```\r\nINSERT INTO FUNCTION s3('https://s3.us-east-1.amazonaws.com/dzhuravlev/small_file.xml.gz', 'XML', \r\n'number UInt64', 'gz') SELECT * FROM numbers(10);\r\n\r\nINSERT INTO FUNCTION s3('https://s3.us-east-1.amazonaws.com/dzhuravlev/large_file.xml.gz','XML', \r\n'number UInt64','gz') SELECT * from numbers(1e8);\r\n\r\naws s3api head-object --bucket  dzhuravlev  --key small_file.xml.gz\r\n{\r\n    \"AcceptRanges\": \"bytes\",\r\n    \"LastModified\": \"Mon, 24 Jan 2022 21:28:50 GMT\",\r\n    \"ContentLength\": 262,\r\n    \"ETag\": \"\\\"667796620f85cd6bad6957ea01d29548\\\"\",\r\n    \"ContentType\": \"binary/octet-stream\",\r\n    \"Metadata\": {}\r\n}\r\n\r\naws s3api head-object --bucket  dzhuravlev --key large_file.xml.gz\r\n{\r\n    \"AcceptRanges\": \"bytes\",\r\n    \"LastModified\": \"Mon, 24 Jan 2022 21:29:08 GMT\",\r\n    \"ContentLength\": 261909920,\r\n    \"ETag\": \"\\\"829dbc9697bcffd1528e8e6f55c564f4-8\\\"\",\r\n    \"ContentType\": \"application/xml\",                      ---<<<<<<<<<<<-- expected \"ContentType\": \"binary/octet-stream\",\r\n    \"Metadata\": {}\r\n}\r\n```\r\n\r\nSo it seems that if the object size is bigger than 32MB then Clickhouse sets the wrong ContentType `application/xml` instead of `binary/octet-stream`.\r\n\n",
  "hints_text": "We don't set Content-Type anywhere, so AWS decide it on their own.\r\nTo fix this issue, we have to start specifying Content-Type explicitly.\nAlways using `binary/octet-stream` is probably the most safe.\r\nAlthough if the data is uncompressed, we can use Content-Type from the format to allow nice data representation in web browser if static hosting is enabled.\nI have also faced this issue today.\nThis is a bug in AWS S3 SDK: https://github.com/aws/aws-sdk-cpp/issues/1840",
  "created_at": "2022-02-09T02:27:11Z"
}