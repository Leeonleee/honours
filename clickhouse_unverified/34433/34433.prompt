You will be provided with a partial code base and an issue statement explaining a problem to resolve.

<issue>
Wrong s3 object content type for large files
Connected to ClickHouse server version 21.13.1 revision 54455.

```
INSERT INTO FUNCTION s3('https://s3.us-east-1.amazonaws.com/dzhuravlev/small_file.xml.gz', 'XML', 
'number UInt64', 'gz') SELECT * FROM numbers(10);

INSERT INTO FUNCTION s3('https://s3.us-east-1.amazonaws.com/dzhuravlev/large_file.xml.gz','XML', 
'number UInt64','gz') SELECT * from numbers(1e8);

aws s3api head-object --bucket  dzhuravlev  --key small_file.xml.gz
{
    "AcceptRanges": "bytes",
    "LastModified": "Mon, 24 Jan 2022 21:28:50 GMT",
    "ContentLength": 262,
    "ETag": "\"667796620f85cd6bad6957ea01d29548\"",
    "ContentType": "binary/octet-stream",
    "Metadata": {}
}

aws s3api head-object --bucket  dzhuravlev --key large_file.xml.gz
{
    "AcceptRanges": "bytes",
    "LastModified": "Mon, 24 Jan 2022 21:29:08 GMT",
    "ContentLength": 261909920,
    "ETag": "\"829dbc9697bcffd1528e8e6f55c564f4-8\"",
    "ContentType": "application/xml",                      ---<<<<<<<<<<<-- expected "ContentType": "binary/octet-stream",
    "Metadata": {}
}
```

So it seems that if the object size is bigger than 32MB then Clickhouse sets the wrong ContentType `application/xml` instead of `binary/octet-stream`.
</issue>

I need you to solve the provided issue by generating a code fix that can be applied directly to the repository

Respond below:
