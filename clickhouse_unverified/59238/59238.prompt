You will be provided with a partial code base and an issue statement explaining a problem to resolve.

<issue>
MV deduplication not working as expected when passing `insert_deduplication_token` 
We are trying to emulate transactions (or to be able to have idempotent inserts) by using deduplication so we can safely retry inserts on failure. We can generate a static block of rows and assign a `$insertion_id` to each one. Having that set up and passing the following settings, we can retry inserts that have partially written data in any table in the data flow.

- insert_deduplicate = 1
- deduplicate_blocks_in_dependent_materialized_views = 1
- insert_deduplication_token = $insertion_id (automatically generated for each block of rows we ingest)

This has been working fine but we have encountered a corner case. Note that the following setup is a bit special since a landing table gets split into various MVs and reunited again into another table.

This is the schematics of the setup:

```
landing -┬--> mv_1_1 ---> ds_1_1 ---> mv_2_1 --┬-> ds_2_1 ---> mv_2_2 ---> ds_3_1
         |                                     |  
         └--> mv_1_2 ---> ds_1_2 ---> mv_2_2 --┘ 

```

And here is a script to reproduce the setup and the issue we are seeing: https://pastila.nl/?003e3b86/1785bacbca04dd3ef0c91f4e115b1e6f#e9/2/RQDxlV+QARpP3tw8g==

If you run the script you can see the following output from `system.part_log`:

```sql
┌─query_id──────┬──────────event_time─┬─database─┬─table───┬─name──────┬─error─┐
│ first_insert  │ 2024-01-24 12:10:06 │ dedup    │ landing │ all_0_0_0 │     0 │
│ first_insert  │ 2024-01-24 12:10:06 │ dedup    │ landing │ all_1_1_0 │     0 │
│ first_insert  │ 2024-01-24 12:10:06 │ dedup    │ ds_1_2  │ all_0_0_0 │     0 │
│ first_insert  │ 2024-01-24 12:10:06 │ dedup    │ ds_1_1  │ all_0_0_0 │     0 │
│ first_insert  │ 2024-01-24 12:10:06 │ dedup    │ ds_2_1  │ all_0_0_0 │     0 │
│ first_insert  │ 2024-01-24 12:10:06 │ dedup    │ ds_2_1  │ all_2_2_0 │   389 │  # <= Shouldn't be deduplicated
│ first_insert  │ 2024-01-24 12:10:06 │ dedup    │ ds_3_1  │ all_0_0_0 │     0 │
│ first_insert  │ 2024-01-24 12:10:06 │ dedup    │ ds_3_1  │ all_2_2_0 │   389 │  # <= Shouldn't be deduplicated
│ second_insert │ 2024-01-24 12:10:06 │ dedup    │ landing │ all_3_3_0 │   389 │
│ second_insert │ 2024-01-24 12:10:06 │ dedup    │ landing │ all_4_4_0 │   389 │
│ second_insert │ 2024-01-24 12:10:06 │ dedup    │ ds_1_2  │ all_2_2_0 │   389 │
│ second_insert │ 2024-01-24 12:10:06 │ dedup    │ ds_1_1  │ all_2_2_0 │   389 │
│ second_insert │ 2024-01-24 12:10:06 │ dedup    │ ds_2_1  │ all_3_3_0 │   389 │
│ second_insert │ 2024-01-24 12:10:06 │ dedup    │ ds_2_1  │ all_4_4_0 │   389 │
│ second_insert │ 2024-01-24 12:10:06 │ dedup    │ ds_3_1  │ all_3_3_0 │   389 │
│ second_insert │ 2024-01-24 12:10:06 │ dedup    │ ds_3_1  │ all_4_4_0 │   389 │
└───────────────┴─────────────────────┴──────────┴─────────┴───────────┴───────┘
```
1. first insert: We set a custom deduplication token and all inserts until `ds_2_1` are properly ingested (no deduplication). The issue starts at `ds_2_1` since it's receiving the "same block" in two different inserts (through `mv_2_1` and `mv_2_2`) , and the second one gets deduplicated. The deduplication then propagates to `ds_3_1` and any other DS downstream.
2. second insert: Everything gets correctly deduplicated. Something expected

I'd expect ClickHouse to handle this despite technically being the same ingested block. **Could this be considered a bug, or is it expected behavior?** How is the internal deduplication being executed? Is it not considering that they are two different parts coming from different MVs?

A possible solution could be to stop sending our custom token and rely on ClickHouse generated hashes using the data, but we see two possible drawbacks:
* If a MV is not deterministic (e.g. it's using `now()`) it will always generate different output, and retries are not going to get deduplicated
* If a MV generates the same output for two different inserts (e.g. MV doing only a `count()` and two inserts ingesting 1k different rows), the second one is going to be deduplicated when it shouldn't

We are aware of initiatives like microtransactions (https://github.com/ClickHouse/ClickHouse/issues/57815) but we wonder if there is something we can do meanwhile to fix this situation.
</issue>

I need you to solve the provided issue by generating a code fix that can be applied directly to the repository

Respond below:
