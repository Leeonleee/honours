{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 46537,
  "instance_id": "ClickHouse__ClickHouse-46537",
  "issue_numbers": [
    "44963"
  ],
  "base_commit": "e89e263457a01810938817aea2dbf8318812db58",
  "patch": "diff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex 54124c0988e7..ca89106dc082 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -611,6 +611,7 @@ class IColumn;\n     M(Bool, query_plan_aggregation_in_order, true, \"Use query plan for aggregation-in-order optimisation\", 0) \\\n     M(Bool, query_plan_remove_redundant_sorting, true, \"Remove redundant sorting in query plan. For example, sorting steps related to ORDER BY clauses in subqueries\", 0) \\\n     M(Bool, query_plan_remove_redundant_distinct, true, \"Remove redundant Distinct step in query plan\", 0) \\\n+    M(Bool, query_plan_optimize_projection, true, \"Use query plan for aggregation-in-order optimisation\", 0) \\\n     M(UInt64, regexp_max_matches_per_row, 1000, \"Max matches of any single regexp per row, used to safeguard 'extractAllGroupsHorizontal' against consuming too much memory with greedy RE.\", 0) \\\n     \\\n     M(UInt64, limit, 0, \"Limit on read rows from the most 'end' result for select query, default 0 means no limit length\", 0) \\\ndiff --git a/src/Interpreters/ActionsDAG.cpp b/src/Interpreters/ActionsDAG.cpp\nindex 1964f6fd8b31..ad809dca022b 100644\n--- a/src/Interpreters/ActionsDAG.cpp\n+++ b/src/Interpreters/ActionsDAG.cpp\n@@ -761,6 +761,86 @@ NameSet ActionsDAG::foldActionsByProjection(\n     return next_required_columns;\n }\n \n+\n+ActionsDAGPtr ActionsDAG::foldActionsByProjection(const std::unordered_map<const Node *, std::string> & new_inputs, const NodeRawConstPtrs & required_outputs)\n+{\n+    auto dag = std::make_unique<ActionsDAG>();\n+    std::unordered_map<const Node *, size_t> new_input_to_pos;\n+\n+    std::unordered_map<const Node *, const Node *> mapping;\n+    struct Frame\n+    {\n+        const Node * node;\n+        size_t next_child = 0;\n+    };\n+\n+    std::vector<Frame> stack;\n+    for (const auto * output : required_outputs)\n+    {\n+        if (mapping.contains(output))\n+            continue;\n+\n+        stack.push_back({.node = output});\n+        while (!stack.empty())\n+        {\n+            auto & frame = stack.back();\n+\n+            if (frame.next_child == 0)\n+            {\n+                auto it = new_inputs.find(frame.node);\n+                if (it != new_inputs.end())\n+                {\n+                    const auto & [new_input, rename] = *it;\n+\n+                    auto & node = mapping[frame.node];\n+\n+                    if (!node)\n+                    {\n+                        bool should_rename = !rename.empty() && new_input->result_name != rename;\n+                        const auto & input_name = should_rename ? rename : new_input->result_name;\n+                        node = &dag->addInput(input_name, new_input->result_type);\n+                        if (should_rename)\n+                            node = &dag->addAlias(*node, new_input->result_name);\n+                    }\n+\n+                    stack.pop_back();\n+                    continue;\n+                }\n+            }\n+\n+            const auto & children = frame.node->children;\n+\n+            while (frame.next_child < children.size() && !mapping.emplace(children[frame.next_child], nullptr).second)\n+                ++frame.next_child;\n+\n+            if (frame.next_child < children.size())\n+            {\n+                const auto * child = children[frame.next_child];\n+                ++frame.next_child;\n+                stack.push_back({.node = child});\n+                continue;\n+            }\n+\n+            if (frame.node->type == ActionType::INPUT)\n+                throw Exception(ErrorCodes::LOGICAL_ERROR,\n+                    \"Cannot fold actions for projection. Node {} requires input {} which does not belong to projection\",\n+                    stack.front().node->result_name, frame.node->result_name);\n+\n+            auto & node = dag->nodes.emplace_back(*frame.node);\n+            for (auto & child : node.children)\n+                child = mapping[child];\n+\n+            mapping[frame.node] = &node;\n+            stack.pop_back();\n+        }\n+    }\n+\n+    for (const auto * output : required_outputs)\n+        dag->outputs.push_back(mapping[output]);\n+\n+    return dag;\n+}\n+\n void ActionsDAG::reorderAggregationKeysForProjection(const std::unordered_map<std::string_view, size_t> & key_names_pos_map)\n {\n     ::sort(outputs.begin(), outputs.end(), [&key_names_pos_map](const Node * lhs, const Node * rhs)\ndiff --git a/src/Interpreters/ActionsDAG.h b/src/Interpreters/ActionsDAG.h\nindex 4b63a350d7d1..1859fda28088 100644\n--- a/src/Interpreters/ActionsDAG.h\n+++ b/src/Interpreters/ActionsDAG.h\n@@ -221,6 +221,28 @@ class ActionsDAG\n         const String & predicate_column_name = {},\n         bool add_missing_keys = true);\n \n+    /// Get an ActionsDAG where:\n+    /// * Subtrees from new_inputs are converted to inputs with specified names.\n+    /// * Outputs are taken from required_outputs.\n+    /// Here want to substitute some expressions to columns from projection.\n+    /// This function expects that all required_outputs can be calculated from nodes in new_inputs.\n+    /// If not, exception will happen.\n+    /// This function also expects that new_inputs and required_outputs are valid nodes from the same DAG.\n+    /// Example:\n+    /// DAG:                   new_inputs:                   Result DAG\n+    /// a      b               c * d -> \"(a + b) * d\"\n+    /// \\     /                e     -> \"\"\n+    ///  a + b\n+    ///     \\                  required_outputs:         =>  \"(a + b) * d\"    e\n+    ///   c (alias)   d        c * d - e                              \\      /\n+    ///       \\      /                                               c * d - e\n+    ///        c * d       e\n+    ///            \\      /\n+    ///            c * d - e\n+    static ActionsDAGPtr foldActionsByProjection(\n+        const std::unordered_map<const Node *, std::string> & new_inputs,\n+        const NodeRawConstPtrs & required_outputs);\n+\n     /// Reorder the output nodes using given position mapping.\n     void reorderAggregationKeysForProjection(const std::unordered_map<std::string_view, size_t> & key_names_pos_map);\n \ndiff --git a/src/Interpreters/Aggregator.cpp b/src/Interpreters/Aggregator.cpp\nindex 0f7cb961e34d..b0bcea234499 100644\n--- a/src/Interpreters/Aggregator.cpp\n+++ b/src/Interpreters/Aggregator.cpp\n@@ -1000,6 +1000,13 @@ void Aggregator::mergeOnBlockSmall(\n         result.key_sizes = key_sizes;\n     }\n \n+    if ((params.overflow_row || result.type == AggregatedDataVariants::Type::without_key) && !result.without_key)\n+    {\n+        AggregateDataPtr place = result.aggregates_pool->alignedAlloc(total_size_of_aggregate_states, align_aggregate_states);\n+        createAggregateStates(place);\n+        result.without_key = place;\n+    }\n+\n     if (false) {} // NOLINT\n #define M(NAME, IS_TWO_LEVEL) \\\n     else if (result.type == AggregatedDataVariants::Type::NAME) \\\n@@ -2930,6 +2937,13 @@ bool Aggregator::mergeOnBlock(Block block, AggregatedDataVariants & result, bool\n         LOG_TRACE(log, \"Aggregation method: {}\", result.getMethodName());\n     }\n \n+    if ((params.overflow_row || result.type == AggregatedDataVariants::Type::without_key) && !result.without_key)\n+    {\n+        AggregateDataPtr place = result.aggregates_pool->alignedAlloc(total_size_of_aggregate_states, align_aggregate_states);\n+        createAggregateStates(place);\n+        result.without_key = place;\n+    }\n+\n     if (result.type == AggregatedDataVariants::Type::without_key || block.info.is_overflows)\n         mergeBlockWithoutKeyStreamsImpl(std::move(block), result);\n #define M(NAME, IS_TWO_LEVEL) \\\ndiff --git a/src/Processors/QueryPlan/AggregatingStep.cpp b/src/Processors/QueryPlan/AggregatingStep.cpp\nindex 69dfa05899bb..4ac972e2a799 100644\n--- a/src/Processors/QueryPlan/AggregatingStep.cpp\n+++ b/src/Processors/QueryPlan/AggregatingStep.cpp\n@@ -25,6 +25,11 @@\n namespace DB\n {\n \n+namespace ErrorCodes\n+{\n+    extern const int LOGICAL_ERROR;\n+}\n+\n static bool memoryBoundMergingWillBeUsed(\n     bool should_produce_results_in_order_of_bucket_number,\n     bool memory_bound_merging_of_aggregation_results_enabled,\n@@ -508,6 +513,43 @@ void AggregatingStep::describePipeline(FormatSettings & settings) const\n     }\n }\n \n+bool AggregatingStep::canUseProjection() const\n+{\n+    /// For now, grouping sets are not supported.\n+    /// Aggregation in order should be applied after projection optimization if projection is full.\n+    /// Skip it here just in case.\n+    return grouping_sets_params.empty() && sort_description_for_merging.empty();\n+}\n+\n+void AggregatingStep::requestOnlyMergeForAggregateProjection(const DataStream & input_stream)\n+{\n+    if (!canUseProjection())\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Cannot aggregate from projection\");\n+\n+    auto output_header = getOutputStream().header;\n+    input_streams.front() = input_stream;\n+    params.only_merge = true;\n+    updateOutputStream();\n+    assertBlocksHaveEqualStructure(output_header, getOutputStream().header, \"AggregatingStep\");\n+}\n+\n+std::unique_ptr<AggregatingProjectionStep> AggregatingStep::convertToAggregatingProjection(const DataStream & input_stream) const\n+{\n+    if (!canUseProjection())\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"Cannot aggregate from projection\");\n+\n+    auto aggregating_projection = std::make_unique<AggregatingProjectionStep>(\n+        DataStreams{input_streams.front(), input_stream},\n+        params,\n+        final,\n+        merge_threads,\n+        temporary_data_merge_threads\n+    );\n+\n+    assertBlocksHaveEqualStructure(getOutputStream().header, aggregating_projection->getOutputStream().header, \"AggregatingStep\");\n+    return aggregating_projection;\n+}\n+\n void AggregatingStep::updateOutputStream()\n {\n     output_stream = createOutputStream(\n@@ -522,4 +564,88 @@ bool AggregatingStep::memoryBoundMergingWillBeUsed() const\n         should_produce_results_in_order_of_bucket_number, memory_bound_merging_of_aggregation_results_enabled, sort_description_for_merging);\n }\n \n+AggregatingProjectionStep::AggregatingProjectionStep(\n+    DataStreams input_streams_,\n+    Aggregator::Params params_,\n+    bool final_,\n+    size_t merge_threads_,\n+    size_t temporary_data_merge_threads_)\n+    : params(std::move(params_))\n+    , final(final_)\n+    , merge_threads(merge_threads_)\n+    , temporary_data_merge_threads(temporary_data_merge_threads_)\n+{\n+    input_streams = std::move(input_streams_);\n+\n+    if (input_streams.size() != 2)\n+        throw Exception(\n+            ErrorCodes::LOGICAL_ERROR,\n+            \"AggregatingProjectionStep is expected to have two input streams, got {}\",\n+            input_streams.size());\n+\n+    auto normal_parts_header = params.getHeader(input_streams.front().header, final);\n+    params.only_merge = true;\n+    auto projection_parts_header = params.getHeader(input_streams.back().header, final);\n+    params.only_merge = false;\n+\n+    assertBlocksHaveEqualStructure(normal_parts_header, projection_parts_header, \"AggregatingProjectionStep\");\n+    output_stream.emplace();\n+    output_stream->header = std::move(normal_parts_header);\n+}\n+\n+QueryPipelineBuilderPtr AggregatingProjectionStep::updatePipeline(\n+    QueryPipelineBuilders pipelines,\n+    const BuildQueryPipelineSettings &)\n+{\n+    auto & normal_parts_pipeline = pipelines.front();\n+    auto & projection_parts_pipeline = pipelines.back();\n+\n+    /// Here we create shared ManyAggregatedData for both projection and ordinary data.\n+    /// For ordinary data, AggregatedData is filled in a usual way.\n+    /// For projection data, AggregatedData is filled by merging aggregation states.\n+    /// When all AggregatedData is filled, we merge aggregation states together in a usual way.\n+    /// Pipeline will look like:\n+    /// ReadFromProjection   -> Aggregating (only merge states) ->\n+    /// ReadFromProjection   -> Aggregating (only merge states) ->\n+    /// ...                                                     -> Resize -> ConvertingAggregatedToChunks\n+    /// ReadFromOrdinaryPart -> Aggregating (usual)             ->           (added by last Aggregating)\n+    /// ReadFromOrdinaryPart -> Aggregating (usual)             ->\n+    /// ...\n+    auto many_data = std::make_shared<ManyAggregatedData>(normal_parts_pipeline->getNumStreams() + projection_parts_pipeline->getNumStreams());\n+    size_t counter = 0;\n+\n+    AggregatorListPtr aggregator_list_ptr = std::make_shared<AggregatorList>();\n+\n+    /// TODO apply optimize_aggregation_in_order here somehow\n+    auto build_aggregate_pipeline = [&](QueryPipelineBuilder & pipeline, bool projection)\n+    {\n+        auto params_copy = params;\n+        if (projection)\n+            params_copy.only_merge = true;\n+\n+        AggregatingTransformParamsPtr transform_params = std::make_shared<AggregatingTransformParams>(\n+            pipeline.getHeader(), std::move(params_copy), aggregator_list_ptr, final);\n+\n+        pipeline.resize(pipeline.getNumStreams(), true, true);\n+\n+        pipeline.addSimpleTransform([&](const Block & header)\n+        {\n+            return std::make_shared<AggregatingTransform>(\n+                header, transform_params, many_data, counter++, merge_threads, temporary_data_merge_threads);\n+        });\n+    };\n+\n+    build_aggregate_pipeline(*normal_parts_pipeline, false);\n+    build_aggregate_pipeline(*projection_parts_pipeline, true);\n+\n+    auto pipeline = std::make_unique<QueryPipelineBuilder>();\n+\n+    for (auto & cur_pipeline : pipelines)\n+        assertBlocksHaveEqualStructure(cur_pipeline->getHeader(), getOutputStream().header, \"AggregatingProjectionStep\");\n+\n+    *pipeline = QueryPipelineBuilder::unitePipelines(std::move(pipelines), 0, &processors);\n+    pipeline->resize(1);\n+    return pipeline;\n+}\n+\n }\ndiff --git a/src/Processors/QueryPlan/AggregatingStep.h b/src/Processors/QueryPlan/AggregatingStep.h\nindex 5f5557fb2043..3d128d788ac9 100644\n--- a/src/Processors/QueryPlan/AggregatingStep.h\n+++ b/src/Processors/QueryPlan/AggregatingStep.h\n@@ -22,6 +22,8 @@ using GroupingSetsParamsList = std::vector<GroupingSetsParams>;\n Block appendGroupingSetColumn(Block header);\n Block generateOutputHeader(const Block & input_header, const Names & keys, bool use_nulls);\n \n+class AggregatingProjectionStep;\n+\n /// Aggregation. See AggregatingTransform.\n class AggregatingStep : public ITransformingStep\n {\n@@ -65,6 +67,15 @@ class AggregatingStep : public ITransformingStep\n     bool memoryBoundMergingWillBeUsed() const;\n     void skipMerging() { skip_merging = true; }\n \n+    bool canUseProjection() const;\n+    /// When we apply aggregate projection (which is full), this step will only merge data.\n+    /// Argument input_stream replaces current single input.\n+    /// Probably we should replace this step to MergingAggregated later? (now, aggregation-in-order will not work)\n+    void requestOnlyMergeForAggregateProjection(const DataStream & input_stream);\n+    /// When we apply aggregate projection (which is partial), this step should be replaced to AggregatingProjection.\n+    /// Argument input_stream would be the second input (from projection).\n+    std::unique_ptr<AggregatingProjectionStep> convertToAggregatingProjection(const DataStream & input_stream) const;\n+\n private:\n     void updateOutputStream() override;\n \n@@ -99,4 +110,27 @@ class AggregatingStep : public ITransformingStep\n     Processors aggregating;\n };\n \n+class AggregatingProjectionStep : public IQueryPlanStep\n+{\n+public:\n+    AggregatingProjectionStep(\n+        DataStreams input_streams_,\n+        Aggregator::Params params_,\n+        bool final_,\n+        size_t merge_threads_,\n+        size_t temporary_data_merge_threads_\n+    );\n+\n+    String getName() const override { return \"AggregatingProjection\"; }\n+    QueryPipelineBuilderPtr updatePipeline(QueryPipelineBuilders pipelines, const BuildQueryPipelineSettings &) override;\n+\n+private:\n+    Aggregator::Params params;\n+    bool final;\n+    size_t merge_threads;\n+    size_t temporary_data_merge_threads;\n+\n+    Processors aggregating;\n+};\n+\n }\ndiff --git a/src/Processors/QueryPlan/Optimizations/Optimizations.h b/src/Processors/QueryPlan/Optimizations/Optimizations.h\nindex fbffcc29a9ca..de1d43bed1be 100644\n--- a/src/Processors/QueryPlan/Optimizations/Optimizations.h\n+++ b/src/Processors/QueryPlan/Optimizations/Optimizations.h\n@@ -108,6 +108,8 @@ void optimizePrimaryKeyCondition(const Stack & stack);\n void optimizePrewhere(Stack & stack, QueryPlan::Nodes & nodes);\n void optimizeReadInOrder(QueryPlan::Node & node, QueryPlan::Nodes & nodes);\n void optimizeAggregationInOrder(QueryPlan::Node & node, QueryPlan::Nodes &);\n+bool optimizeUseAggregateProjections(QueryPlan::Node & node, QueryPlan::Nodes & nodes);\n+bool optimizeUseNormalProjections(Stack & stack, QueryPlan::Nodes & nodes);\n \n /// Enable memory bound merging of aggregation states for remote queries\n /// in case it was enabled for local plan\ndiff --git a/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.cpp b/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.cpp\nindex afcd585a3b43..7db1ba1db713 100644\n--- a/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.cpp\n+++ b/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.cpp\n@@ -17,6 +17,8 @@ QueryPlanOptimizationSettings QueryPlanOptimizationSettings::fromSettings(const\n     settings.remove_redundant_sorting = from.query_plan_remove_redundant_sorting;\n     settings.aggregate_partitions_independently = from.allow_aggregate_partitions_independently;\n     settings.remove_redundant_distinct = from.query_plan_remove_redundant_distinct;\n+    settings.optimize_projection = from.allow_experimental_projection_optimization && from.query_plan_optimize_projection;\n+    settings.force_use_projection = settings.optimize_projection && from.force_optimize_projection;\n     return settings;\n }\n \ndiff --git a/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.h b/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.h\nindex db27993aeb1f..967cfdaca7fc 100644\n--- a/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.h\n+++ b/src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.h\n@@ -38,6 +38,10 @@ struct QueryPlanOptimizationSettings\n     /// If removing redundant distinct steps is enabled\n     bool remove_redundant_distinct = true;\n \n+    /// If reading from projection can be applied\n+    bool optimize_projection = false;\n+    bool force_use_projection = false;\n+\n     static QueryPlanOptimizationSettings fromSettings(const Settings & from);\n     static QueryPlanOptimizationSettings fromContext(ContextPtr from);\n };\ndiff --git a/src/Processors/QueryPlan/Optimizations/actionsDAGUtils.cpp b/src/Processors/QueryPlan/Optimizations/actionsDAGUtils.cpp\nindex 643e93146f41..c9cf46aaecac 100644\n--- a/src/Processors/QueryPlan/Optimizations/actionsDAGUtils.cpp\n+++ b/src/Processors/QueryPlan/Optimizations/actionsDAGUtils.cpp\n@@ -2,6 +2,7 @@\n \n #include <Core/Field.h>\n #include <Functions/IFunction.h>\n+#include <Columns/ColumnConst.h>\n \n #include <stack>\n \n@@ -11,7 +12,7 @@ MatchedTrees::Matches matchTrees(const ActionsDAG & inner_dag, const ActionsDAG\n {\n     using Parents = std::set<const ActionsDAG::Node *>;\n     std::unordered_map<const ActionsDAG::Node *, Parents> inner_parents;\n-    std::unordered_map<std::string_view, const ActionsDAG::Node *> inner_inputs_and_constants;\n+    std::unordered_map<std::string_view, const ActionsDAG::Node *> inner_inputs;\n \n     {\n         std::stack<const ActionsDAG::Node *> stack;\n@@ -27,8 +28,8 @@ MatchedTrees::Matches matchTrees(const ActionsDAG & inner_dag, const ActionsDAG\n                 const auto * node = stack.top();\n                 stack.pop();\n \n-                if (node->type == ActionsDAG::ActionType::INPUT || node->type == ActionsDAG::ActionType::COLUMN)\n-                    inner_inputs_and_constants.emplace(node->result_name, node);\n+                if (node->type == ActionsDAG::ActionType::INPUT)\n+                    inner_inputs.emplace(node->result_name, node);\n \n                 for (const auto * child : node->children)\n                 {\n@@ -84,10 +85,10 @@ MatchedTrees::Matches matchTrees(const ActionsDAG & inner_dag, const ActionsDAG\n             /// match.node will be set if match is found.\n             auto & match = matches[frame.node];\n \n-            if (frame.node->type == ActionsDAG::ActionType::INPUT || frame.node->type == ActionsDAG::ActionType::COLUMN)\n+            if (frame.node->type == ActionsDAG::ActionType::INPUT)\n             {\n                 const ActionsDAG::Node * mapped = nullptr;\n-                if (auto it = inner_inputs_and_constants.find(frame.node->result_name); it != inner_inputs_and_constants.end())\n+                if (auto it = inner_inputs.find(frame.node->result_name); it != inner_inputs.end())\n                     mapped = it->second;\n \n                 match.node = mapped;\n@@ -101,14 +102,20 @@ MatchedTrees::Matches matchTrees(const ActionsDAG & inner_dag, const ActionsDAG\n                 //std::cerr << \"... Processing \" << frame.node->function_base->getName() << std::endl;\n \n                 bool found_all_children = true;\n-                for (const auto * child : frame.mapped_children)\n-                    if (!child)\n+                const ActionsDAG::Node * any_child = nullptr;\n+                size_t num_children = frame.node->children.size();\n+                for (size_t i = 0; i < num_children; ++i)\n+                {\n+                    if (frame.mapped_children[i])\n+                        any_child = frame.mapped_children[i];\n+                    else if (!frame.node->children[i]->column || !isColumnConst(*frame.node->children[i]->column))\n                         found_all_children = false;\n+                }\n \n-                if (found_all_children && !frame.mapped_children.empty())\n+                if (found_all_children && any_child)\n                 {\n                     Parents container;\n-                    Parents * intersection = &inner_parents[frame.mapped_children[0]];\n+                    Parents * intersection = &inner_parents[any_child];\n \n                     if (frame.mapped_children.size() > 1)\n                     {\n@@ -116,7 +123,8 @@ MatchedTrees::Matches matchTrees(const ActionsDAG & inner_dag, const ActionsDAG\n                         size_t mapped_children_size = frame.mapped_children.size();\n                         other_parents.reserve(mapped_children_size);\n                         for (size_t i = 1; i < mapped_children_size; ++i)\n-                            other_parents.push_back(&inner_parents[frame.mapped_children[i]]);\n+                            if (frame.mapped_children[i])\n+                                other_parents.push_back(&inner_parents[frame.mapped_children[i]]);\n \n                         for (const auto * parent : *intersection)\n                         {\n@@ -148,12 +156,20 @@ MatchedTrees::Matches matchTrees(const ActionsDAG & inner_dag, const ActionsDAG\n                             if (parent->type == ActionsDAG::ActionType::FUNCTION && func_name == parent->function_base->getName())\n                             {\n                                 const auto & children = parent->children;\n-                                size_t num_children = children.size();\n-                                if (frame.mapped_children.size() == num_children)\n+                                if (children.size() == num_children)\n                                 {\n                                     bool all_children_matched = true;\n                                     for (size_t i = 0; all_children_matched && i < num_children; ++i)\n-                                        all_children_matched = frame.mapped_children[i] == children[i];\n+                                    {\n+                                        if (frame.mapped_children[i] == nullptr)\n+                                        {\n+                                            all_children_matched = children[i]->column && isColumnConst(*children[i]->column)\n+                                                && children[i]->result_type->equals(*frame.node->children[i]->result_type)\n+                                                && assert_cast<const ColumnConst &>(*children[i]->column).getField() == assert_cast<const ColumnConst &>(*frame.node->children[i]->column).getField();\n+                                        }\n+                                        else\n+                                            all_children_matched = frame.mapped_children[i] == children[i];\n+                                    }\n \n                                     if (all_children_matched)\n                                     {\n@@ -212,5 +228,4 @@ MatchedTrees::Matches matchTrees(const ActionsDAG & inner_dag, const ActionsDAG\n     return matches;\n }\n \n-\n }\ndiff --git a/src/Processors/QueryPlan/Optimizations/optimizeReadInOrder.cpp b/src/Processors/QueryPlan/Optimizations/optimizeReadInOrder.cpp\nindex 5d0288698e27..ce8a863611dc 100644\n--- a/src/Processors/QueryPlan/Optimizations/optimizeReadInOrder.cpp\n+++ b/src/Processors/QueryPlan/Optimizations/optimizeReadInOrder.cpp\n@@ -30,7 +30,7 @@\n namespace DB::QueryPlanOptimizations\n {\n \n-ISourceStep * checkSupportedReadingStep(IQueryPlanStep * step)\n+static ISourceStep * checkSupportedReadingStep(IQueryPlanStep * step)\n {\n     if (auto * reading = typeid_cast<ReadFromMergeTree *>(step))\n     {\n@@ -67,7 +67,7 @@ ISourceStep * checkSupportedReadingStep(IQueryPlanStep * step)\n \n using StepStack = std::vector<IQueryPlanStep*>;\n \n-QueryPlan::Node * findReadingStep(QueryPlan::Node & node, StepStack & backward_path)\n+static QueryPlan::Node * findReadingStep(QueryPlan::Node & node, StepStack & backward_path)\n {\n     IQueryPlanStep * step = node.step.get();\n     if (auto * reading = checkSupportedReadingStep(step))\n@@ -119,7 +119,7 @@ using FixedColumns = std::unordered_set<const ActionsDAG::Node *>;\n \n /// Right now we find only simple cases like 'and(..., and(..., and(column = value, ...), ...'\n /// Injective functions are supported here. For a condition 'injectiveFunction(x) = 5' column 'x' is fixed.\n-void appendFixedColumnsFromFilterExpression(const ActionsDAG::Node & filter_expression, FixedColumns & fixed_columns)\n+static void appendFixedColumnsFromFilterExpression(const ActionsDAG::Node & filter_expression, FixedColumns & fixed_columns)\n {\n     std::stack<const ActionsDAG::Node *> stack;\n     stack.push(&filter_expression);\n@@ -168,7 +168,7 @@ void appendFixedColumnsFromFilterExpression(const ActionsDAG::Node & filter_expr\n     }\n }\n \n-void appendExpression(ActionsDAGPtr & dag, const ActionsDAGPtr & expression)\n+static void appendExpression(ActionsDAGPtr & dag, const ActionsDAGPtr & expression)\n {\n     if (dag)\n         dag->mergeInplace(std::move(*expression->clone()));\n@@ -176,7 +176,7 @@ void appendExpression(ActionsDAGPtr & dag, const ActionsDAGPtr & expression)\n         dag = expression->clone();\n }\n \n-/// This function builds a common DAG which is a gerge of DAGs from Filter and Expression steps chain.\n+/// This function builds a common DAG which is a merge of DAGs from Filter and Expression steps chain.\n /// Additionally, build a set of fixed columns.\n void buildSortingDAG(QueryPlan::Node & node, ActionsDAGPtr & dag, FixedColumns & fixed_columns, size_t & limit)\n {\n@@ -982,6 +982,10 @@ void optimizeAggregationInOrder(QueryPlan::Node & node, QueryPlan::Nodes &)\n     if ((aggregating->inOrder() && !aggregating->explicitSortingRequired()) || aggregating->isGroupingSets())\n         return;\n \n+    /// It just does not work, see 02515_projections_with_totals\n+    if (aggregating->getParams().overflow_row)\n+        return;\n+\n     /// TODO: maybe add support for UNION later.\n     std::vector<IQueryPlanStep*> steps_to_update;\n     if (auto order_info = buildInputOrderInfo(*aggregating, *node.children.front(), steps_to_update); order_info.input_order)\ndiff --git a/src/Processors/QueryPlan/Optimizations/optimizeTree.cpp b/src/Processors/QueryPlan/Optimizations/optimizeTree.cpp\nindex c48119ece108..37e3b2f67d8c 100644\n--- a/src/Processors/QueryPlan/Optimizations/optimizeTree.cpp\n+++ b/src/Processors/QueryPlan/Optimizations/optimizeTree.cpp\n@@ -1,4 +1,5 @@\n #include <Common/Exception.h>\n+#include <Processors/QueryPlan/ReadFromMergeTree.h>\n #include <Processors/QueryPlan/MergingAggregatedStep.h>\n #include <Processors/QueryPlan/Optimizations/Optimizations.h>\n #include <Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.h>\n@@ -12,6 +13,7 @@ namespace DB\n namespace ErrorCodes\n {\n     extern const int TOO_MANY_QUERY_PLAN_OPTIMIZATIONS;\n+    extern const int PROJECTION_NOT_USED;\n }\n \n namespace QueryPlanOptimizations\n@@ -103,6 +105,10 @@ void optimizeTreeFirstPass(const QueryPlanOptimizationSettings & settings, Query\n \n void optimizeTreeSecondPass(const QueryPlanOptimizationSettings & optimization_settings, QueryPlan::Node & root, QueryPlan::Nodes & nodes)\n {\n+    size_t max_optimizations_to_apply = optimization_settings.max_optimizations_to_apply;\n+    size_t num_applied_projection = 0;\n+    bool has_reading_from_mt = false;\n+\n     Stack stack;\n     stack.push_back({.node = &root});\n \n@@ -112,9 +118,14 @@ void optimizeTreeSecondPass(const QueryPlanOptimizationSettings & optimization_s\n \n         if (frame.next_child == 0)\n         {\n+            has_reading_from_mt |= typeid_cast<const ReadFromMergeTree *>(frame.node->step.get()) != nullptr;\n+\n             if (optimization_settings.read_in_order)\n                 optimizeReadInOrder(*frame.node, nodes);\n \n+            if (optimization_settings.optimize_projection)\n+                num_applied_projection += optimizeUseAggregateProjections(*frame.node, nodes);\n+\n             if (optimization_settings.aggregation_in_order)\n                 optimizeAggregationInOrder(*frame.node, nodes);\n \n@@ -131,12 +142,35 @@ void optimizeTreeSecondPass(const QueryPlanOptimizationSettings & optimization_s\n             continue;\n         }\n \n+        if (optimization_settings.optimize_projection)\n+        {\n+            if (optimizeUseNormalProjections(stack, nodes))\n+            {\n+                ++num_applied_projection;\n+\n+                if (max_optimizations_to_apply && max_optimizations_to_apply < num_applied_projection)\n+                    throw Exception(ErrorCodes::TOO_MANY_QUERY_PLAN_OPTIMIZATIONS,\n+                                    \"Too many projection optimizations applied to query plan. Current limit {}\",\n+                                    max_optimizations_to_apply);\n+\n+                /// Stack is updated after this optimization and frame is not valid anymore.\n+                /// Try to apply optimizations again to newly added plan steps.\n+                --stack.back().next_child;\n+                continue;\n+            }\n+        }\n+\n         optimizePrewhere(stack, nodes);\n         optimizePrimaryKeyCondition(stack);\n         enableMemoryBoundMerging(*frame.node, nodes);\n \n         stack.pop_back();\n     }\n+\n+    if (optimization_settings.force_use_projection && has_reading_from_mt && num_applied_projection == 0)\n+        throw Exception(\n+            ErrorCodes::PROJECTION_NOT_USED,\n+            \"No projection is used when allow_experimental_projection_optimization = 1 and force_optimize_projection = 1\");\n }\n \n }\ndiff --git a/src/Processors/QueryPlan/Optimizations/optimizeUseAggregateProjection.cpp b/src/Processors/QueryPlan/Optimizations/optimizeUseAggregateProjection.cpp\nnew file mode 100644\nindex 000000000000..77b5547207c6\n--- /dev/null\n+++ b/src/Processors/QueryPlan/Optimizations/optimizeUseAggregateProjection.cpp\n@@ -0,0 +1,649 @@\n+#include <Processors/QueryPlan/Optimizations/projectionsCommon.h>\n+#include <Processors/QueryPlan/Optimizations/actionsDAGUtils.h>\n+#include <Processors/QueryPlan/AggregatingStep.h>\n+#include <Processors/QueryPlan/ReadFromMergeTree.h>\n+#include <Processors/QueryPlan/ExpressionStep.h>\n+#include <Processors/QueryPlan/FilterStep.h>\n+#include <Processors/QueryPlan/ReadFromPreparedSource.h>\n+\n+#include <Processors/Sources/SourceFromSingleChunk.h>\n+#include <Processors/Sources/NullSource.h>\n+\n+#include <AggregateFunctions/AggregateFunctionCount.h>\n+#include <Analyzer/JoinNode.h>\n+#include <Analyzer/TableNode.h>\n+#include <Analyzer/QueryTreeBuilder.h>\n+#include <Analyzer/QueryTreePassManager.h>\n+#include <Analyzer/QueryNode.h>\n+\n+#include <Common/logger_useful.h>\n+#include <Storages/StorageDummy.h>\n+#include <Planner/PlannerExpressionAnalysis.h>\n+#include <Interpreters/InterpreterSelectQuery.h>\n+#include <Interpreters/InterpreterSelectQueryAnalyzer.h>\n+#include <Storages/MergeTree/MergeTreeDataSelectExecutor.h>\n+#include <Storages/ProjectionsDescription.h>\n+#include <Parsers/queryToString.h>\n+\n+namespace DB::QueryPlanOptimizations\n+{\n+\n+using DAGIndex = std::unordered_map<std::string_view, const ActionsDAG::Node *>;\n+static DAGIndex buildDAGIndex(const ActionsDAG & dag)\n+{\n+    DAGIndex index;\n+    for (const auto * output : dag.getOutputs())\n+        index.emplace(output->result_name, output);\n+\n+    return index;\n+}\n+\n+/// Required analysis info from aggregate projection.\n+struct AggregateProjectionInfo\n+{\n+    ActionsDAGPtr before_aggregation;\n+    Names keys;\n+    AggregateDescriptions aggregates;\n+\n+    /// A context copy from interpreter which was used for analysis.\n+    /// Just in case it is used by some function.\n+    ContextPtr context;\n+};\n+\n+/// Get required info from aggregate projection.\n+/// Ideally, this should be pre-calculated and stored inside ProjectionDescription.\n+static AggregateProjectionInfo getAggregatingProjectionInfo(\n+    const ProjectionDescription & projection,\n+    const ContextPtr & context,\n+    const StorageMetadataPtr & metadata_snapshot,\n+    const Block & key_virtual_columns)\n+{\n+    /// This is a bad approach.\n+    /// We'd better have a separate interpreter for projections.\n+    /// Now it's not obvious we didn't miss anything here.\n+    InterpreterSelectQuery interpreter(\n+        projection.query_ast,\n+        context,\n+        Pipe(std::make_shared<SourceFromSingleChunk>(metadata_snapshot->getSampleBlock())),\n+        SelectQueryOptions{QueryProcessingStage::WithMergeableState});\n+\n+    const auto & analysis_result = interpreter.getAnalysisResult();\n+    const auto & query_analyzer = interpreter.getQueryAnalyzer();\n+\n+    AggregateProjectionInfo info;\n+    info.context = interpreter.getContext();\n+    info.before_aggregation = analysis_result.before_aggregation;\n+    info.keys = query_analyzer->aggregationKeys().getNames();\n+    info.aggregates = query_analyzer->aggregates();\n+\n+    /// Add part/partition virtual columns to projection aggregation keys.\n+    /// We can do it because projection is stored for every part separately.\n+    for (const auto & virt_column : key_virtual_columns)\n+    {\n+        const auto * input = &info.before_aggregation->addInput(virt_column);\n+        info.before_aggregation->getOutputs().push_back(input);\n+        info.keys.push_back(virt_column.name);\n+    }\n+\n+    return info;\n+}\n+\n+static bool hasNullableOrMissingColumn(const DAGIndex & index, const Names & names)\n+{\n+    for (const auto & query_name : names)\n+    {\n+        auto jt = index.find(query_name);\n+        if (jt == index.end() || jt->second->result_type->isNullable())\n+            return true;\n+    }\n+\n+    return false;\n+}\n+\n+\n+/// Here we try to match aggregate functions from the query to\n+/// aggregate functions from projection.\n+bool areAggregatesMatch(\n+    const AggregateProjectionInfo & info,\n+    const AggregateDescriptions & aggregates,\n+    const MatchedTrees::Matches & matches,\n+    const DAGIndex & query_index,\n+    const DAGIndex & proj_index)\n+{\n+    /// Index (projection agg function name) -> pos\n+    std::unordered_map<std::string, std::vector<size_t>> projection_aggregate_functions;\n+    for (size_t i = 0; i < info.aggregates.size(); ++i)\n+        projection_aggregate_functions[info.aggregates[i].function->getName()].push_back(i);\n+\n+    for (const auto & aggregate : aggregates)\n+    {\n+        /// Get a list of candidates by name first.\n+        auto it = projection_aggregate_functions.find(aggregate.function->getName());\n+        if (it == projection_aggregate_functions.end())\n+        {\n+            // LOG_TRACE(\n+            //     &Poco::Logger::get(\"optimizeUseProjections\"),\n+            //     \"Cannot match agg func {} by name {}\",\n+            //     aggregate.column_name, aggregate.function->getName());\n+\n+            return false;\n+        }\n+\n+        auto & candidates = it->second;\n+        bool found_match = false;\n+\n+        for (size_t idx : candidates)\n+        {\n+            const auto & candidate = info.aggregates[idx];\n+\n+            /// Note: this check is a bit strict.\n+            /// We check that aggregate function names, argument types and parameters are equal.\n+            /// In some cases it's possible only to check that states are equal,\n+            /// e.g. for quantile(0.3)(...) and quantile(0.5)(...).\n+            /// But also functions sum(...) and sumIf(...) will have equal states,\n+            /// and we can't replace one to another from projection.\n+            if (!candidate.function->getStateType()->equals(*aggregate.function->getStateType()))\n+            {\n+                LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Cannot match agg func {} vs {} by state {} vs {}\",\n+                    aggregate.column_name, candidate.column_name,\n+                    candidate.function->getStateType()->getName(), aggregate.function->getStateType()->getName());\n+                continue;\n+            }\n+\n+            /// This is a special case for the function count().\n+            /// We can assume that 'count(expr) == count()' if expr is not nullable.\n+            if (typeid_cast<const AggregateFunctionCount *>(candidate.function.get()))\n+            {\n+                bool has_nullable_or_missing_arg = false;\n+                has_nullable_or_missing_arg |= hasNullableOrMissingColumn(query_index, aggregate.argument_names);\n+                has_nullable_or_missing_arg |= hasNullableOrMissingColumn(proj_index, candidate.argument_names);\n+\n+                if (!has_nullable_or_missing_arg)\n+                {\n+                    /// we can ignore arguments for count()\n+                    found_match = true;\n+                    break;\n+                }\n+            }\n+\n+            /// Now, function names and types matched.\n+            /// Next, match arguments from DAGs.\n+\n+            size_t num_args = aggregate.argument_names.size();\n+            if (num_args != candidate.argument_names.size())\n+                continue;\n+\n+            size_t next_arg = 0;\n+            while (next_arg < num_args)\n+            {\n+                const auto & query_name = aggregate.argument_names[next_arg];\n+                const auto & proj_name = candidate.argument_names[next_arg];\n+\n+                auto jt = query_index.find(query_name);\n+                auto kt = proj_index.find(proj_name);\n+\n+                /// This should not happen ideally.\n+                if (jt == query_index.end() || kt == proj_index.end())\n+                    break;\n+\n+                const auto * query_node = jt->second;\n+                const auto * proj_node = kt->second;\n+\n+                auto mt = matches.find(query_node);\n+                if (mt == matches.end())\n+                {\n+                    // LOG_TRACE(\n+                    //     &Poco::Logger::get(\"optimizeUseProjections\"),\n+                    //     \"Cannot match agg func {} vs {} : can't match arg {} vs {} : no node in map\",\n+                    //     aggregate.column_name, candidate.column_name, query_name, proj_name);\n+\n+                    break;\n+                }\n+\n+                const auto & node_match = mt->second;\n+                if (node_match.node != proj_node || node_match.monotonicity)\n+                {\n+                    // LOG_TRACE(\n+                    //     &Poco::Logger::get(\"optimizeUseProjections\"),\n+                    //     \"Cannot match agg func {} vs {} : can't match arg {} vs {} : no match or monotonicity\",\n+                    //     aggregate.column_name, candidate.column_name, query_name, proj_name);\n+\n+                    break;\n+                }\n+\n+                ++next_arg;\n+            }\n+\n+            if (next_arg < aggregate.argument_names.size())\n+                continue;\n+\n+            found_match = true;\n+            break;\n+        }\n+\n+        if (!found_match)\n+            return false;\n+    }\n+\n+    return true;\n+}\n+\n+ActionsDAGPtr analyzeAggregateProjection(\n+    const AggregateProjectionInfo & info,\n+    const QueryDAG & query,\n+    const DAGIndex & query_index,\n+    const Names & keys,\n+    const AggregateDescriptions & aggregates)\n+{\n+    auto proj_index = buildDAGIndex(*info.before_aggregation);\n+\n+    MatchedTrees::Matches matches = matchTrees(*info.before_aggregation, *query.dag);\n+\n+    // for (const auto & [node, match] : matches)\n+    // {\n+    //     LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Match {} {} -> {} {} (with monotonicity : {})\",\n+    //         static_cast<const void *>(node), node->result_name,\n+    //         static_cast<const void *>(match.node), (match.node ? match.node->result_name : \"\"), match.monotonicity != std::nullopt);\n+    // }\n+\n+    if (!areAggregatesMatch(info, aggregates, matches, query_index, proj_index))\n+        return {};\n+\n+    ActionsDAG::NodeRawConstPtrs query_key_nodes;\n+    std::unordered_set<const ActionsDAG::Node *> proj_key_nodes;\n+\n+    {\n+        /// Just, filling the set above.\n+\n+        for (const auto & key : info.keys)\n+        {\n+            auto it = proj_index.find(key);\n+            /// This should not happen ideally.\n+            if (it == proj_index.end())\n+                return {};\n+\n+            proj_key_nodes.insert(it->second);\n+        }\n+\n+        query_key_nodes.reserve(keys.size() + 1);\n+\n+        /// We need to add filter column to keys set.\n+        /// It should be computable from projection keys.\n+        /// It will be removed in FilterStep.\n+        if (query.filter_node)\n+            query_key_nodes.push_back(query.filter_node);\n+\n+        for (const auto & key : keys)\n+        {\n+            auto it = query_index.find(key);\n+            /// This should not happen ideally.\n+            if (it == query_index.end())\n+                return {};\n+\n+            query_key_nodes.push_back(it->second);\n+        }\n+    }\n+\n+    /// Here we want to match query keys with projection keys.\n+    /// Query key can be any expression depending on projection keys.\n+\n+    struct Frame\n+    {\n+        const ActionsDAG::Node * node;\n+        size_t next_child_to_visit = 0;\n+    };\n+\n+    std::stack<Frame> stack;\n+    std::unordered_set<const ActionsDAG::Node *> visited;\n+    std::unordered_map<const ActionsDAG::Node *, std::string> new_inputs;\n+\n+    for (const auto * key_node : query_key_nodes)\n+    {\n+        if (visited.contains(key_node))\n+            continue;\n+\n+        stack.push({.node = key_node});\n+\n+        while (!stack.empty())\n+        {\n+            auto & frame = stack.top();\n+\n+            if (frame.next_child_to_visit == 0)\n+            {\n+                auto jt = matches.find(frame.node);\n+                if (jt != matches.end())\n+                {\n+                    auto & match = jt->second;\n+                    if (match.node && !match.monotonicity && proj_key_nodes.contains(match.node))\n+                    {\n+                        visited.insert(frame.node);\n+                        new_inputs[frame.node] = match.node->result_name;\n+                        stack.pop();\n+                        continue;\n+                    }\n+                }\n+            }\n+\n+            if (frame.next_child_to_visit < frame.node->children.size())\n+            {\n+                stack.push({.node = frame.node->children[frame.next_child_to_visit]});\n+                ++frame.next_child_to_visit;\n+                continue;\n+            }\n+\n+            /// Not a match and there is no matched child.\n+            if (frame.node->type == ActionsDAG::ActionType::INPUT)\n+            {\n+                // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Cannot find match for {}\", frame.node->result_name);\n+                return {};\n+            }\n+\n+            /// Not a match, but all children matched.\n+            visited.insert(frame.node);\n+            stack.pop();\n+        }\n+    }\n+\n+    // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Folding actions by projection\");\n+\n+    auto proj_dag = query.dag->foldActionsByProjection(new_inputs, query_key_nodes);\n+\n+    /// Just add all the aggregates to dag inputs.\n+    auto & proj_dag_outputs =  proj_dag->getOutputs();\n+    for (const auto & aggregate : aggregates)\n+        proj_dag_outputs.push_back(&proj_dag->addInput(aggregate.column_name, aggregate.function->getResultType()));\n+\n+    return proj_dag;\n+}\n+\n+\n+/// Aggregate projection analysis result in case it can be applied.\n+struct AggregateProjectionCandidate : public ProjectionCandidate\n+{\n+    AggregateProjectionInfo info;\n+\n+    /// Actions which need to be applied to columns from projection\n+    /// in order to get all the columns required for aggregation.\n+    ActionsDAGPtr dag;\n+};\n+\n+struct MinMaxProjectionCandidate\n+{\n+    AggregateProjectionCandidate candidate;\n+    Block block;\n+    MergeTreeData::DataPartsVector normal_parts;\n+};\n+\n+struct AggregateProjectionCandidates\n+{\n+    std::vector<AggregateProjectionCandidate> real;\n+    std::optional<MinMaxProjectionCandidate> minmax_projection;\n+\n+    /// This flag means that DAG for projection candidate should be used in FilterStep.\n+    bool has_filter = false;\n+};\n+\n+AggregateProjectionCandidates getAggregateProjectionCandidates(\n+    QueryPlan::Node & node,\n+    AggregatingStep & aggregating,\n+    ReadFromMergeTree & reading,\n+    const std::shared_ptr<PartitionIdToMaxBlock> & max_added_blocks)\n+{\n+    const auto & keys = aggregating.getParams().keys;\n+    const auto & aggregates = aggregating.getParams().aggregates;\n+    Block key_virtual_columns = reading.getMergeTreeData().getSampleBlockWithVirtualColumns();\n+\n+    AggregateProjectionCandidates candidates;\n+\n+    const auto & parts = reading.getParts();\n+    const auto & query_info = reading.getQueryInfo();\n+\n+    const auto metadata = reading.getStorageMetadata();\n+    ContextPtr context = reading.getContext();\n+\n+    const auto & projections = metadata->projections;\n+    std::vector<const ProjectionDescription *> agg_projections;\n+    for (const auto & projection : projections)\n+        if (projection.type == ProjectionDescription::Type::Aggregate)\n+            agg_projections.push_back(&projection);\n+\n+    bool can_use_minmax_projection = metadata->minmax_count_projection && !reading.getMergeTreeData().has_lightweight_delete_parts.load();\n+\n+    if (!can_use_minmax_projection && agg_projections.empty())\n+        return candidates;\n+\n+    // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Has agg projection\");\n+\n+    QueryDAG dag;\n+    if (!dag.build(*node.children.front()))\n+        return candidates;\n+\n+    auto query_index = buildDAGIndex(*dag.dag);\n+\n+    // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Query DAG: {}\", dag.dag->dumpDAG());\n+\n+    candidates.has_filter = dag.filter_node;\n+\n+    if (can_use_minmax_projection)\n+    {\n+        const auto * projection = &*(metadata->minmax_count_projection);\n+        // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Try projection {}\", projection->name);\n+        auto info = getAggregatingProjectionInfo(*projection, context, metadata, key_virtual_columns);\n+        // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Projection DAG {}\", info.before_aggregation->dumpDAG());\n+        if (auto proj_dag = analyzeAggregateProjection(info, dag, query_index, keys, aggregates))\n+        {\n+            // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Projection analyzed DAG {}\", proj_dag->dumpDAG());\n+            AggregateProjectionCandidate candidate{.info = std::move(info), .dag = std::move(proj_dag)};\n+            MergeTreeData::DataPartsVector minmax_projection_normal_parts;\n+\n+            // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Projection sample block {}\", sample_block.dumpStructure());\n+            auto block = reading.getMergeTreeData().getMinMaxCountProjectionBlock(\n+                metadata,\n+                candidate.dag->getRequiredColumnsNames(),\n+                dag.filter_node != nullptr,\n+                query_info,\n+                parts,\n+                minmax_projection_normal_parts,\n+                max_added_blocks.get(),\n+                context);\n+\n+            // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Projection sample block 2 {}\", block.dumpStructure());\n+\n+            if (block)\n+            {\n+                MinMaxProjectionCandidate minmax;\n+                minmax.candidate = std::move(candidate);\n+                minmax.block = std::move(block);\n+                minmax.normal_parts = std::move(minmax_projection_normal_parts);\n+                minmax.candidate.projection = projection;\n+                candidates.minmax_projection.emplace(std::move(minmax));\n+            }\n+        }\n+    }\n+\n+    if (!candidates.minmax_projection)\n+    {\n+        candidates.real.reserve(agg_projections.size());\n+        for (const auto * projection : agg_projections)\n+        {\n+            // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Try projection {}\", projection->name);\n+            auto info = getAggregatingProjectionInfo(*projection, context, metadata, key_virtual_columns);\n+            // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Projection DAG {}\", info.before_aggregation->dumpDAG());\n+            if (auto proj_dag = analyzeAggregateProjection(info, dag, query_index, keys, aggregates))\n+            {\n+                // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Projection analyzed DAG {}\", proj_dag->dumpDAG());\n+                AggregateProjectionCandidate candidate{.info = std::move(info), .dag = std::move(proj_dag)};\n+                candidate.projection = projection;\n+                candidates.real.emplace_back(std::move(candidate));\n+            }\n+        }\n+    }\n+\n+    return candidates;\n+}\n+\n+static QueryPlan::Node * findReadingStep(QueryPlan::Node & node)\n+{\n+    IQueryPlanStep * step = node.step.get();\n+    if (auto * reading = typeid_cast<ReadFromMergeTree *>(step))\n+        return &node;\n+\n+    if (node.children.size() != 1)\n+        return nullptr;\n+\n+    if (typeid_cast<ExpressionStep *>(step) || typeid_cast<FilterStep *>(step))\n+        return findReadingStep(*node.children.front());\n+\n+    return nullptr;\n+}\n+\n+bool optimizeUseAggregateProjections(QueryPlan::Node & node, QueryPlan::Nodes & nodes)\n+{\n+    if (node.children.size() != 1)\n+        return false;\n+\n+    auto * aggregating = typeid_cast<AggregatingStep *>(node.step.get());\n+    if (!aggregating)\n+        return false;\n+\n+    if (!aggregating->canUseProjection())\n+        return false;\n+\n+    QueryPlan::Node * reading_node = findReadingStep(*node.children.front());\n+    if (!reading_node)\n+        return false;\n+\n+    auto * reading = typeid_cast<ReadFromMergeTree *>(reading_node->step.get());\n+    if (!reading)\n+        return false;\n+\n+    if (!canUseProjectionForReadingStep(reading))\n+        return false;\n+\n+    std::shared_ptr<PartitionIdToMaxBlock> max_added_blocks = getMaxAddedBlocks(reading);\n+\n+    auto candidates = getAggregateProjectionCandidates(node, *aggregating, *reading, max_added_blocks);\n+\n+    AggregateProjectionCandidate * best_candidate = nullptr;\n+    if (candidates.minmax_projection)\n+        best_candidate = &candidates.minmax_projection->candidate;\n+    else if (candidates.real.empty())\n+        return false;\n+\n+    const auto & parts = reading->getParts();\n+    const auto & query_info = reading->getQueryInfo();\n+    const auto metadata = reading->getStorageMetadata();\n+    ContextPtr context = reading->getContext();\n+    MergeTreeDataSelectExecutor reader(reading->getMergeTreeData());\n+\n+    /// Selecting best candidate.\n+    for (auto & candidate : candidates.real)\n+    {\n+        auto required_column_names = candidate.dag->getRequiredColumnsNames();\n+        ActionDAGNodes added_filter_nodes;\n+        if (candidates.has_filter)\n+            added_filter_nodes.nodes.push_back(candidate.dag->getOutputs().front());\n+\n+        bool analyzed = analyzeProjectionCandidate(\n+            candidate, *reading, reader, required_column_names, parts,\n+            metadata, query_info, context, max_added_blocks, added_filter_nodes);\n+\n+        if (!analyzed)\n+            continue;\n+\n+        if (best_candidate == nullptr || best_candidate->sum_marks > candidate.sum_marks)\n+            best_candidate = &candidate;\n+    }\n+\n+    if (!best_candidate)\n+        return false;\n+\n+    QueryPlanStepPtr projection_reading;\n+    bool has_ordinary_parts;\n+\n+    /// Add reading from projection step.\n+    if (candidates.minmax_projection)\n+    {\n+        // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Minmax proj block {}\",\n+        //           candidates.minmax_projection->block.dumpStructure());\n+\n+        Pipe pipe(std::make_shared<SourceFromSingleChunk>(std::move(candidates.minmax_projection->block)));\n+        projection_reading = std::make_unique<ReadFromPreparedSource>(std::move(pipe));\n+\n+        has_ordinary_parts = !candidates.minmax_projection->normal_parts.empty();\n+        if (has_ordinary_parts)\n+            reading->resetParts(std::move(candidates.minmax_projection->normal_parts));\n+    }\n+    else\n+    {\n+        auto storage_snapshot = reading->getStorageSnapshot();\n+        auto proj_snapshot = std::make_shared<StorageSnapshot>(\n+            storage_snapshot->storage, storage_snapshot->metadata, storage_snapshot->object_columns);\n+        proj_snapshot->addProjection(best_candidate->projection);\n+\n+        auto query_info_copy = query_info;\n+        query_info_copy.prewhere_info = nullptr;\n+\n+        projection_reading = reader.readFromParts(\n+            {},\n+            best_candidate->dag->getRequiredColumnsNames(),\n+            proj_snapshot,\n+            query_info_copy,\n+            context,\n+            reading->getMaxBlockSize(),\n+            reading->getNumStreams(),\n+            max_added_blocks,\n+            best_candidate->merge_tree_projection_select_result_ptr,\n+            reading->isParallelReadingEnabled());\n+\n+        if (!projection_reading)\n+        {\n+            auto header = proj_snapshot->getSampleBlockForColumns(best_candidate->dag->getRequiredColumnsNames());\n+            Pipe pipe(std::make_shared<NullSource>(std::move(header)));\n+            projection_reading = std::make_unique<ReadFromPreparedSource>(std::move(pipe));\n+        }\n+\n+        has_ordinary_parts = best_candidate->merge_tree_ordinary_select_result_ptr != nullptr;\n+        if (has_ordinary_parts)\n+            reading->setAnalyzedResult(std::move(best_candidate->merge_tree_ordinary_select_result_ptr));\n+    }\n+\n+    // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Projection reading header {}\",\n+    //           projection_reading->getOutputStream().header.dumpStructure());\n+\n+    projection_reading->setStepDescription(best_candidate->projection->name);\n+\n+    auto & projection_reading_node = nodes.emplace_back(QueryPlan::Node{.step = std::move(projection_reading)});\n+    auto & expr_or_filter_node = nodes.emplace_back();\n+\n+    if (candidates.has_filter)\n+    {\n+        expr_or_filter_node.step = std::make_unique<FilterStep>(\n+            projection_reading_node.step->getOutputStream(),\n+            best_candidate->dag,\n+            best_candidate->dag->getOutputs().front()->result_name,\n+            true);\n+    }\n+    else\n+        expr_or_filter_node.step = std::make_unique<ExpressionStep>(\n+            projection_reading_node.step->getOutputStream(),\n+            best_candidate->dag);\n+\n+    expr_or_filter_node.children.push_back(&projection_reading_node);\n+\n+    if (!has_ordinary_parts)\n+    {\n+        /// All parts are taken from projection\n+        aggregating->requestOnlyMergeForAggregateProjection(expr_or_filter_node.step->getOutputStream());\n+        node.children.front() = &expr_or_filter_node;\n+    }\n+    else\n+    {\n+        node.step = aggregating->convertToAggregatingProjection(expr_or_filter_node.step->getOutputStream());\n+        node.children.push_back(&expr_or_filter_node);\n+    }\n+\n+    return true;\n+}\n+\n+}\ndiff --git a/src/Processors/QueryPlan/Optimizations/optimizeUseNormalProjection.cpp b/src/Processors/QueryPlan/Optimizations/optimizeUseNormalProjection.cpp\nnew file mode 100644\nindex 000000000000..eed3707fe9c3\n--- /dev/null\n+++ b/src/Processors/QueryPlan/Optimizations/optimizeUseNormalProjection.cpp\n@@ -0,0 +1,267 @@\n+#include <Processors/QueryPlan/Optimizations/Optimizations.h>\n+#include <Processors/QueryPlan/Optimizations/projectionsCommon.h>\n+#include <Processors/QueryPlan/ExpressionStep.h>\n+#include <Processors/QueryPlan/FilterStep.h>\n+#include <Processors/QueryPlan/ReadFromMergeTree.h>\n+#include <Processors/QueryPlan/UnionStep.h>\n+#include <Processors/QueryPlan/ReadFromPreparedSource.h>\n+#include <Processors/Sources/NullSource.h>\n+#include <Common/logger_useful.h>\n+#include <Storages/MergeTree/MergeTreeDataSelectExecutor.h>\n+#include <stack>\n+\n+namespace DB::QueryPlanOptimizations\n+{\n+\n+/// Normal projection analysis result in case it can be applied.\n+/// For now, it is empty.\n+/// Normal projection can be used only if it contains all required source columns.\n+/// It would not be hard to support pre-computed expressions and filtration.\n+struct NormalProjectionCandidate : public ProjectionCandidate\n+{\n+};\n+\n+static ActionsDAGPtr makeMaterializingDAG(const Block & proj_header, const Block main_header)\n+{\n+    /// Materialize constants in case we don't have it in output header.\n+    /// This may happen e.g. if we have PREWHERE.\n+\n+    size_t num_columns = main_header.columns();\n+    /// This is a error; will have block structure mismatch later.\n+    if (proj_header.columns() != num_columns)\n+        return nullptr;\n+\n+    std::vector<size_t> const_positions;\n+    for (size_t i = 0; i < num_columns; ++i)\n+    {\n+        auto col_proj = proj_header.getByPosition(i).column;\n+        auto col_main = main_header.getByPosition(i).column;\n+        bool is_proj_const = col_proj && isColumnConst(*col_proj);\n+        bool is_main_proj = col_main && isColumnConst(*col_main);\n+        if (is_proj_const && !is_main_proj)\n+            const_positions.push_back(i);\n+    }\n+\n+    if (const_positions.empty())\n+        return nullptr;\n+\n+    ActionsDAGPtr dag = std::make_unique<ActionsDAG>();\n+    auto & outputs = dag->getOutputs();\n+    for (const auto & col : proj_header.getColumnsWithTypeAndName())\n+        outputs.push_back(&dag->addInput(col));\n+\n+    for (auto pos : const_positions)\n+    {\n+        auto & output = outputs[pos];\n+        output = &dag->materializeNode(*output);\n+    }\n+\n+    return dag;\n+}\n+\n+static bool hasAllRequiredColumns(const ProjectionDescription * projection, const Names & required_columns)\n+{\n+    for (const auto & col : required_columns)\n+    {\n+        if (!projection->sample_block.has(col))\n+            return false;\n+    }\n+\n+    return true;\n+}\n+\n+\n+bool optimizeUseNormalProjections(Stack & stack, QueryPlan::Nodes & nodes)\n+{\n+    const auto & frame = stack.back();\n+\n+    auto * reading = typeid_cast<ReadFromMergeTree *>(frame.node->step.get());\n+    if (!reading)\n+        return false;\n+\n+    if (!canUseProjectionForReadingStep(reading))\n+        return false;\n+\n+    auto iter = stack.rbegin();\n+    while (std::next(iter) != stack.rend())\n+    {\n+        iter = std::next(iter);\n+\n+        if (!typeid_cast<FilterStep *>(iter->node->step.get()) &&\n+            !typeid_cast<ExpressionStep *>(iter->node->step.get()))\n+            break;\n+    }\n+\n+    const auto metadata = reading->getStorageMetadata();\n+    const auto & projections = metadata->projections;\n+\n+    std::vector<const ProjectionDescription *> normal_projections;\n+    for (const auto & projection : projections)\n+        if (projection.type == ProjectionDescription::Type::Normal)\n+            normal_projections.push_back(&projection);\n+\n+    if (normal_projections.empty())\n+        return false;\n+\n+    QueryDAG query;\n+    {\n+        auto & clild = iter->node->children[iter->next_child - 1];\n+        if (!query.build(*clild))\n+            return false;\n+\n+        if (query.dag)\n+        {\n+            query.dag->removeUnusedActions();\n+            // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Query DAG: {}\", query.dag->dumpDAG());\n+        }\n+    }\n+\n+    std::list<NormalProjectionCandidate> candidates;\n+    NormalProjectionCandidate * best_candidate = nullptr;\n+\n+    const Names & required_columns = reading->getRealColumnNames();\n+    const auto & parts = reading->getParts();\n+    const auto & query_info = reading->getQueryInfo();\n+    ContextPtr context = reading->getContext();\n+    MergeTreeDataSelectExecutor reader(reading->getMergeTreeData());\n+\n+    auto ordinary_reading_select_result = reading->selectRangesToRead(parts);\n+    size_t ordinary_reading_marks = ordinary_reading_select_result->marks();\n+\n+    // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"),\n+    //           \"Marks for ordinary reading {}\", ordinary_reading_marks);\n+\n+    std::shared_ptr<PartitionIdToMaxBlock> max_added_blocks = getMaxAddedBlocks(reading);\n+\n+    for (const auto * projection : normal_projections)\n+    {\n+        if (!hasAllRequiredColumns(projection, required_columns))\n+            continue;\n+\n+        auto & candidate = candidates.emplace_back();\n+        candidate.projection = projection;\n+\n+        ActionDAGNodes added_filter_nodes;\n+        if (query.filter_node)\n+            added_filter_nodes.nodes.push_back(query.filter_node);\n+\n+        bool analyzed = analyzeProjectionCandidate(\n+            candidate, *reading, reader, required_columns, parts,\n+            metadata, query_info, context, max_added_blocks, added_filter_nodes);\n+\n+        if (!analyzed)\n+            continue;\n+\n+        // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"),\n+        //           \"Marks for projection {} {}\", projection->name ,candidate.sum_marks);\n+\n+        if (candidate.sum_marks >= ordinary_reading_marks)\n+            continue;\n+\n+        if (best_candidate == nullptr || candidate.sum_marks < best_candidate->sum_marks)\n+            best_candidate = &candidate;\n+    }\n+\n+    if (!best_candidate)\n+    {\n+        reading->setAnalyzedResult(std::move(ordinary_reading_select_result));\n+        return false;\n+    }\n+\n+    auto storage_snapshot = reading->getStorageSnapshot();\n+    auto proj_snapshot = std::make_shared<StorageSnapshot>(\n+        storage_snapshot->storage, storage_snapshot->metadata, storage_snapshot->object_columns); //, storage_snapshot->data);\n+    proj_snapshot->addProjection(best_candidate->projection);\n+\n+    // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Proj snapshot {}\",\n+    //           proj_snapshot->getColumns(GetColumnsOptions::Kind::All).toString());\n+\n+    auto query_info_copy = query_info;\n+    query_info_copy.prewhere_info = nullptr;\n+\n+    auto projection_reading = reader.readFromParts(\n+        {},\n+        required_columns,\n+        proj_snapshot,\n+        query_info_copy,\n+        context,\n+        reading->getMaxBlockSize(),\n+        reading->getNumStreams(),\n+        max_added_blocks,\n+        best_candidate->merge_tree_projection_select_result_ptr,\n+        reading->isParallelReadingEnabled());\n+\n+    if (!projection_reading)\n+    {\n+        Pipe pipe(std::make_shared<NullSource>(proj_snapshot->getSampleBlockForColumns(required_columns)));\n+        projection_reading = std::make_unique<ReadFromPreparedSource>(std::move(pipe));\n+    }\n+\n+    bool has_ordinary_parts = best_candidate->merge_tree_ordinary_select_result_ptr != nullptr;\n+    if (has_ordinary_parts)\n+        reading->setAnalyzedResult(std::move(best_candidate->merge_tree_ordinary_select_result_ptr));\n+\n+    // LOG_TRACE(&Poco::Logger::get(\"optimizeUseProjections\"), \"Projection reading header {}\",\n+    //           projection_reading->getOutputStream().header.dumpStructure());\n+\n+    projection_reading->setStepDescription(best_candidate->projection->name);\n+\n+    auto & projection_reading_node = nodes.emplace_back(QueryPlan::Node{.step = std::move(projection_reading)});\n+    auto * next_node = &projection_reading_node;\n+\n+    if (query.dag)\n+    {\n+        auto & expr_or_filter_node = nodes.emplace_back();\n+\n+        if (query.filter_node)\n+        {\n+            expr_or_filter_node.step = std::make_unique<FilterStep>(\n+                projection_reading_node.step->getOutputStream(),\n+                query.dag,\n+                query.filter_node->result_name,\n+                true);\n+        }\n+        else\n+            expr_or_filter_node.step = std::make_unique<ExpressionStep>(\n+                projection_reading_node.step->getOutputStream(),\n+                query.dag);\n+\n+        expr_or_filter_node.children.push_back(&projection_reading_node);\n+        next_node = &expr_or_filter_node;\n+    }\n+\n+    if (!has_ordinary_parts)\n+    {\n+        /// All parts are taken from projection\n+        iter->node->children[iter->next_child - 1] = next_node;\n+    }\n+    else\n+    {\n+        const auto & main_stream = iter->node->children.front()->step->getOutputStream();\n+        const auto * proj_stream = &next_node->step->getOutputStream();\n+\n+        if (auto materializing = makeMaterializingDAG(proj_stream->header, main_stream.header))\n+        {\n+            auto converting = std::make_unique<ExpressionStep>(*proj_stream, materializing);\n+            proj_stream = &converting->getOutputStream();\n+            auto & expr_node = nodes.emplace_back();\n+            expr_node.step = std::move(converting);\n+            expr_node.children.push_back(next_node);\n+            next_node = &expr_node;\n+        }\n+\n+        auto & union_node = nodes.emplace_back();\n+        DataStreams input_streams = {main_stream, *proj_stream};\n+        union_node.step = std::make_unique<UnionStep>(std::move(input_streams));\n+        union_node.children = {iter->node->children.front(), next_node};\n+        iter->node->children[iter->next_child - 1] = &union_node;\n+    }\n+\n+    /// Here we remove last steps from stack to be able to optimize again.\n+    /// In theory, read-in-order can be applied to projection.\n+    stack.resize(iter.base() - stack.begin());\n+\n+    return true;\n+}\n+\n+}\ndiff --git a/src/Processors/QueryPlan/Optimizations/projectionsCommon.cpp b/src/Processors/QueryPlan/Optimizations/projectionsCommon.cpp\nnew file mode 100644\nindex 000000000000..a334450fb414\n--- /dev/null\n+++ b/src/Processors/QueryPlan/Optimizations/projectionsCommon.cpp\n@@ -0,0 +1,263 @@\n+#include <Processors/QueryPlan/Optimizations/projectionsCommon.h>\n+\n+#include <Processors/QueryPlan/ExpressionStep.h>\n+#include <Processors/QueryPlan/FilterStep.h>\n+#include <Processors/QueryPlan/ReadFromMergeTree.h>\n+\n+#include <Common/logger_useful.h>\n+#include <DataTypes/DataTypeNullable.h>\n+#include <Functions/IFunctionAdaptors.h>\n+#include <Functions/FunctionsLogical.h>\n+#include <Interpreters/InterpreterSelectQuery.h>\n+#include <Storages/StorageReplicatedMergeTree.h>\n+\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int ILLEGAL_TYPE_OF_COLUMN_FOR_FILTER;\n+}\n+\n+namespace QueryPlanOptimizations\n+{\n+\n+bool canUseProjectionForReadingStep(ReadFromMergeTree * reading)\n+{\n+    /// Probably some projection already was applied.\n+    if (reading->hasAnalyzedResult())\n+        return false;\n+\n+    if (reading->isQueryWithFinal())\n+        return false;\n+\n+    if (reading->isQueryWithSampling())\n+        return false;\n+\n+    if (reading->isParallelReadingEnabled())\n+        return false;\n+\n+    // Currently projection don't support deduplication when moving parts between shards.\n+    if (reading->getContext()->getSettingsRef().allow_experimental_query_deduplication)\n+        return false;\n+\n+    return true;\n+}\n+\n+std::shared_ptr<PartitionIdToMaxBlock> getMaxAddedBlocks(ReadFromMergeTree * reading)\n+{\n+    ContextPtr context = reading->getContext();\n+\n+    if (context->getSettingsRef().select_sequential_consistency)\n+    {\n+        if (const auto * replicated = dynamic_cast<const StorageReplicatedMergeTree *>(&reading->getMergeTreeData()))\n+            return std::make_shared<PartitionIdToMaxBlock>(replicated->getMaxAddedBlocks());\n+    }\n+\n+    return {};\n+}\n+\n+void QueryDAG::appendExpression(const ActionsDAGPtr & expression)\n+{\n+    if (dag)\n+        dag->mergeInplace(std::move(*expression->clone()));\n+    else\n+        dag = expression->clone();\n+}\n+\n+const ActionsDAG::Node * findInOutputs(ActionsDAG & dag, const std::string & name, bool remove)\n+{\n+    auto & outputs = dag.getOutputs();\n+    for (auto it = outputs.begin(); it != outputs.end(); ++it)\n+    {\n+        if ((*it)->result_name == name)\n+        {\n+            const auto * node = *it;\n+\n+            /// We allow to use Null as a filter.\n+            /// In this case, result is empty. Ignore optimizations.\n+            if (node->result_type->onlyNull())\n+                return nullptr;\n+\n+            if (!isUInt8(removeNullable(removeLowCardinality(node->result_type))))\n+                throw Exception(ErrorCodes::ILLEGAL_TYPE_OF_COLUMN_FOR_FILTER,\n+                    \"Illegal type {} of column {} for filter. Must be UInt8 or Nullable(UInt8).\",\n+                    node->result_type->getName(), name);\n+\n+            if (remove)\n+            {\n+                outputs.erase(it);\n+            }\n+            else\n+            {\n+                ColumnWithTypeAndName col;\n+                col.name = node->result_name;\n+                col.type = node->result_type;\n+                col.column = col.type->createColumnConst(1, 1);\n+                *it = &dag.addColumn(std::move(col));\n+            }\n+\n+            return node;\n+        }\n+    }\n+\n+    return nullptr;\n+}\n+\n+bool QueryDAG::buildImpl(QueryPlan::Node & node, ActionsDAG::NodeRawConstPtrs & filter_nodes)\n+{\n+    IQueryPlanStep * step = node.step.get();\n+    if (auto * reading = typeid_cast<ReadFromMergeTree *>(step))\n+    {\n+        if (const auto & prewhere_info = reading->getPrewhereInfo())\n+        {\n+            if (prewhere_info->row_level_filter)\n+            {\n+                appendExpression(prewhere_info->row_level_filter);\n+                if (const auto * filter_expression = findInOutputs(*dag, prewhere_info->row_level_column_name, false))\n+                    filter_nodes.push_back(filter_expression);\n+                else\n+                    return false;\n+            }\n+\n+            if (prewhere_info->prewhere_actions)\n+            {\n+                appendExpression(prewhere_info->prewhere_actions);\n+                if (const auto * filter_expression = findInOutputs(*dag, prewhere_info->prewhere_column_name, prewhere_info->remove_prewhere_column))\n+                    filter_nodes.push_back(filter_expression);\n+                else\n+                    return false;\n+            }\n+        }\n+        return true;\n+    }\n+\n+    if (node.children.size() != 1)\n+        return false;\n+\n+    if (!buildImpl(*node.children.front(), filter_nodes))\n+        return false;\n+\n+    if (auto * expression = typeid_cast<ExpressionStep *>(step))\n+    {\n+        const auto & actions = expression->getExpression();\n+        if (actions->hasArrayJoin())\n+            return false;\n+\n+        appendExpression(actions);\n+        return true;\n+    }\n+\n+    if (auto * filter = typeid_cast<FilterStep *>(step))\n+    {\n+        const auto & actions = filter->getExpression();\n+        if (actions->hasArrayJoin())\n+            return false;\n+\n+        appendExpression(actions);\n+        const auto * filter_expression = findInOutputs(*dag, filter->getFilterColumnName(), filter->removesFilterColumn());\n+        if (!filter_expression)\n+            return false;\n+\n+        filter_nodes.push_back(filter_expression);\n+        return true;\n+    }\n+\n+    return false;\n+}\n+\n+bool QueryDAG::build(QueryPlan::Node & node)\n+{\n+    ActionsDAG::NodeRawConstPtrs filter_nodes;\n+    if (!buildImpl(node, filter_nodes))\n+        return false;\n+\n+    if (!filter_nodes.empty())\n+    {\n+        filter_node = filter_nodes.back();\n+\n+        if (filter_nodes.size() > 1)\n+        {\n+            /// Add a conjunction of all the filters.\n+\n+            FunctionOverloadResolverPtr func_builder_and =\n+                std::make_unique<FunctionToOverloadResolverAdaptor>(\n+                    std::make_shared<FunctionAnd>());\n+\n+            filter_node = &dag->addFunction(func_builder_and, std::move(filter_nodes), {});\n+        }\n+        else\n+            filter_node = &dag->addAlias(*filter_node, \"_projection_filter\");\n+\n+        auto & outputs = dag->getOutputs();\n+        outputs.insert(outputs.begin(), filter_node);\n+    }\n+\n+    return true;\n+}\n+\n+bool analyzeProjectionCandidate(\n+    ProjectionCandidate & candidate,\n+    const ReadFromMergeTree & reading,\n+    const MergeTreeDataSelectExecutor & reader,\n+    const Names & required_column_names,\n+    const MergeTreeData::DataPartsVector & parts,\n+    const StorageMetadataPtr & metadata,\n+    const SelectQueryInfo & query_info,\n+    const ContextPtr & context,\n+    const std::shared_ptr<PartitionIdToMaxBlock> & max_added_blocks,\n+    const ActionDAGNodes & added_filter_nodes)\n+{\n+    MergeTreeData::DataPartsVector projection_parts;\n+    MergeTreeData::DataPartsVector normal_parts;\n+    for (const auto & part : parts)\n+    {\n+        const auto & created_projections = part->getProjectionParts();\n+        auto it = created_projections.find(candidate.projection->name);\n+        if (it != created_projections.end())\n+            projection_parts.push_back(it->second);\n+        else\n+            normal_parts.push_back(part);\n+    }\n+\n+    if (projection_parts.empty())\n+        return false;\n+\n+    auto projection_result_ptr = reader.estimateNumMarksToRead(\n+        std::move(projection_parts),\n+        nullptr,\n+        required_column_names,\n+        metadata,\n+        candidate.projection->metadata,\n+        query_info, /// How it is actually used? I hope that for index we need only added_filter_nodes\n+        added_filter_nodes,\n+        context,\n+        context->getSettingsRef().max_threads,\n+        max_added_blocks);\n+\n+    if (projection_result_ptr->error())\n+        return false;\n+\n+    candidate.merge_tree_projection_select_result_ptr = std::move(projection_result_ptr);\n+    candidate.sum_marks += candidate.merge_tree_projection_select_result_ptr->marks();\n+\n+    if (!normal_parts.empty())\n+    {\n+        auto normal_result_ptr = reading.selectRangesToRead(std::move(normal_parts));\n+\n+        if (normal_result_ptr->error())\n+            return false;\n+\n+        if (normal_result_ptr->marks() != 0)\n+        {\n+            candidate.sum_marks += normal_result_ptr->marks();\n+            candidate.merge_tree_ordinary_select_result_ptr = std::move(normal_result_ptr);\n+        }\n+    }\n+\n+    return true;\n+}\n+\n+}\n+}\ndiff --git a/src/Processors/QueryPlan/Optimizations/projectionsCommon.h b/src/Processors/QueryPlan/Optimizations/projectionsCommon.h\nnew file mode 100644\nindex 000000000000..1e9ab67c8fe2\n--- /dev/null\n+++ b/src/Processors/QueryPlan/Optimizations/projectionsCommon.h\n@@ -0,0 +1,82 @@\n+#pragma once\n+#include <Interpreters/ActionsDAG.h>\n+#include <Processors/QueryPlan/QueryPlan.h>\n+\n+namespace DB\n+{\n+\n+class ReadFromMergeTree;\n+\n+using PartitionIdToMaxBlock = std::unordered_map<String, Int64>;\n+\n+struct ProjectionDescription;\n+\n+class MergeTreeDataSelectExecutor;\n+\n+struct MergeTreeDataSelectAnalysisResult;\n+using MergeTreeDataSelectAnalysisResultPtr = std::shared_ptr<MergeTreeDataSelectAnalysisResult>;\n+\n+class IMergeTreeDataPart;\n+using DataPartPtr = std::shared_ptr<const IMergeTreeDataPart>;\n+using DataPartsVector = std::vector<DataPartPtr>;\n+\n+struct StorageInMemoryMetadata;\n+using StorageMetadataPtr = std::shared_ptr<const StorageInMemoryMetadata>;\n+\n+struct SelectQueryInfo;\n+\n+}\n+\n+namespace DB::QueryPlanOptimizations\n+{\n+\n+/// Common checks that projection can be used for this step.\n+bool canUseProjectionForReadingStep(ReadFromMergeTree * reading);\n+\n+/// Max blocks for sequential consistency reading from replicated table.\n+std::shared_ptr<PartitionIdToMaxBlock> getMaxAddedBlocks(ReadFromMergeTree * reading);\n+\n+/// This is a common DAG which is a merge of DAGs from Filter and Expression steps chain.\n+/// Additionally, for all the Filter steps, we collect filter conditions into filter_nodes.\n+/// Flag remove_last_filter_node is set in case if the last step is a Filter step and it should remove filter column.\n+struct QueryDAG\n+{\n+    ActionsDAGPtr dag;\n+    const ActionsDAG::Node * filter_node = nullptr;\n+\n+    bool build(QueryPlan::Node & node);\n+\n+private:\n+    bool buildImpl(QueryPlan::Node & node, ActionsDAG::NodeRawConstPtrs & filter_nodes);\n+    void appendExpression(const ActionsDAGPtr & expression);\n+};\n+\n+struct ProjectionCandidate\n+{\n+    const ProjectionDescription * projection;\n+\n+    /// The number of marks we are going to read\n+    size_t sum_marks = 0;\n+\n+    /// Analysis result, separate for parts with and without projection.\n+    /// Analysis is done in order to estimate the number of marks we are going to read.\n+    /// For chosen projection, it is reused for reading step.\n+    MergeTreeDataSelectAnalysisResultPtr merge_tree_projection_select_result_ptr;\n+    MergeTreeDataSelectAnalysisResultPtr merge_tree_ordinary_select_result_ptr;\n+};\n+\n+/// This function fills ProjectionCandidate structure for specified projection.\n+/// It returns false if for some reason we cannot read from projection.\n+bool analyzeProjectionCandidate(\n+    ProjectionCandidate & candidate,\n+    const ReadFromMergeTree & reading,\n+    const MergeTreeDataSelectExecutor & reader,\n+    const Names & required_column_names,\n+    const DataPartsVector & parts,\n+    const StorageMetadataPtr & metadata,\n+    const SelectQueryInfo & query_info,\n+    const ContextPtr & context,\n+    const std::shared_ptr<PartitionIdToMaxBlock> & max_added_blocks,\n+    const ActionDAGNodes & added_filter_nodes);\n+\n+}\ndiff --git a/src/Processors/QueryPlan/ReadFromMergeTree.cpp b/src/Processors/QueryPlan/ReadFromMergeTree.cpp\nindex 668978d2605b..a3e53df62796 100644\n--- a/src/Processors/QueryPlan/ReadFromMergeTree.cpp\n+++ b/src/Processors/QueryPlan/ReadFromMergeTree.cpp\n@@ -99,6 +99,7 @@ namespace ErrorCodes\n {\n     extern const int INDEX_NOT_USED;\n     extern const int LOGICAL_ERROR;\n+    extern const int TOO_MANY_ROWS;\n }\n \n static MergeTreeReaderSettings getMergeTreeReaderSettings(\n@@ -133,6 +134,41 @@ static bool checkAllPartsOnRemoteFS(const RangesInDataParts & parts)\n     return true;\n }\n \n+void ReadFromMergeTree::AnalysisResult::checkLimits(const Settings & settings, const SelectQueryInfo & query_info_) const\n+{\n+\n+    /// Do not check number of read rows if we have reading\n+    /// in order of sorting key with limit.\n+    /// In general case, when there exists WHERE clause\n+    /// it's impossible to estimate number of rows precisely,\n+    /// because we can stop reading at any time.\n+\n+    SizeLimits limits;\n+    if (settings.read_overflow_mode == OverflowMode::THROW\n+        && settings.max_rows_to_read\n+        && !query_info_.input_order_info)\n+        limits = SizeLimits(settings.max_rows_to_read, 0, settings.read_overflow_mode);\n+\n+    SizeLimits leaf_limits;\n+    if (settings.read_overflow_mode_leaf == OverflowMode::THROW\n+        && settings.max_rows_to_read_leaf\n+        && !query_info_.input_order_info)\n+        leaf_limits = SizeLimits(settings.max_rows_to_read_leaf, 0, settings.read_overflow_mode_leaf);\n+\n+    if (limits.max_rows || leaf_limits.max_rows)\n+    {\n+        /// Fail fast if estimated number of rows to read exceeds the limit\n+        size_t total_rows_estimate = selected_rows;\n+        if (query_info_.limit > 0 && total_rows_estimate > query_info_.limit)\n+        {\n+            total_rows_estimate = query_info_.limit;\n+        }\n+        limits.check(total_rows_estimate, 0, \"rows (controlled by 'max_rows_to_read' setting)\", ErrorCodes::TOO_MANY_ROWS);\n+        leaf_limits.check(\n+            total_rows_estimate, 0, \"rows (controlled by 'max_rows_to_read_leaf' setting)\", ErrorCodes::TOO_MANY_ROWS);\n+    }\n+}\n+\n ReadFromMergeTree::ReadFromMergeTree(\n     MergeTreeData::DataPartsVector parts_,\n     Names real_column_names_,\n@@ -1475,7 +1511,7 @@ ReadFromMergeTree::AnalysisResult ReadFromMergeTree::getAnalysisResult() const\n     if (std::holds_alternative<std::exception_ptr>(result_ptr->result))\n         std::rethrow_exception(std::get<std::exception_ptr>(result_ptr->result));\n \n-    return std::get<ReadFromMergeTree::AnalysisResult>(result_ptr->result);\n+    return std::get<AnalysisResult>(result_ptr->result);\n }\n \n bool ReadFromMergeTree::isQueryWithFinal() const\n@@ -1487,6 +1523,18 @@ bool ReadFromMergeTree::isQueryWithFinal() const\n         return select.final();\n }\n \n+bool ReadFromMergeTree::isQueryWithSampling() const\n+{\n+    if (context->getSettingsRef().parallel_replicas_count > 1 && data.supportsSampling())\n+        return true;\n+\n+    const auto & select = query_info.query->as<ASTSelectQuery &>();\n+    if (query_info.table_expression_modifiers)\n+        return query_info.table_expression_modifiers->getSampleSizeRatio() != std::nullopt;\n+    else\n+        return select.sampleSize() != nullptr;\n+}\n+\n Pipe ReadFromMergeTree::spreadMarkRanges(\n     RangesInDataParts && parts_with_ranges, size_t num_streams, AnalysisResult & result, ActionsDAGPtr & result_projection)\n {\n@@ -1581,6 +1629,8 @@ Pipe ReadFromMergeTree::groupStreamsByPartition(AnalysisResult & result, Actions\n void ReadFromMergeTree::initializePipeline(QueryPipelineBuilder & pipeline, const BuildQueryPipelineSettings &)\n {\n     auto result = getAnalysisResult();\n+    result.checkLimits(context->getSettingsRef(), query_info);\n+\n     LOG_DEBUG(\n         log,\n         \"Selected {}/{} parts by partition key, {} parts by primary key, {}/{} marks by primary key, {} marks to read from {} ranges\",\ndiff --git a/src/Processors/QueryPlan/ReadFromMergeTree.h b/src/Processors/QueryPlan/ReadFromMergeTree.h\nindex c17c3b23a391..5e4ba1179678 100644\n--- a/src/Processors/QueryPlan/ReadFromMergeTree.h\n+++ b/src/Processors/QueryPlan/ReadFromMergeTree.h\n@@ -91,6 +91,8 @@ class ReadFromMergeTree final : public SourceStepWithFilter\n         UInt64 selected_marks_pk = 0;\n         UInt64 total_marks_pk = 0;\n         UInt64 selected_rows = 0;\n+\n+        void checkLimits(const Settings & settings, const SelectQueryInfo & query_info_) const;\n     };\n \n     ReadFromMergeTree(\n@@ -145,6 +147,8 @@ class ReadFromMergeTree final : public SourceStepWithFilter\n         bool sample_factor_column_queried,\n         Poco::Logger * log);\n \n+    MergeTreeDataSelectAnalysisResultPtr selectRangesToRead(MergeTreeData::DataPartsVector parts) const;\n+\n     ContextPtr getContext() const { return context; }\n     const SelectQueryInfo & getQueryInfo() const { return query_info; }\n     StorageMetadataPtr getStorageMetadata() const { return metadata_for_reading; }\n@@ -156,11 +160,23 @@ class ReadFromMergeTree final : public SourceStepWithFilter\n     void updatePrewhereInfo(const PrewhereInfoPtr & prewhere_info_value);\n \n     static bool isFinal(const SelectQueryInfo & query_info);\n+    bool isQueryWithFinal() const;\n+    bool isQueryWithSampling() const;\n \n     /// Returns true if the optimisation is applicable (and applies it then).\n     bool requestOutputEachPartitionThroughSeparatePort();\n     bool willOutputEachPartitionThroughSeparatePort() const { return output_each_partition_through_separate_port; }\n \n+    bool hasAnalyzedResult() const { return analyzed_result_ptr != nullptr; }\n+    void setAnalyzedResult(MergeTreeDataSelectAnalysisResultPtr analyzed_result_ptr_) { analyzed_result_ptr = std::move(analyzed_result_ptr_); }\n+    void resetParts(MergeTreeData::DataPartsVector parts) { prepared_parts = std::move(parts); }\n+\n+    const MergeTreeData::DataPartsVector & getParts() const { return prepared_parts; }\n+    const MergeTreeData & getMergeTreeData() const { return data; }\n+    size_t getMaxBlockSize() const { return max_block_size; }\n+    size_t getNumStreams() const { return requested_num_streams; }\n+    bool isParallelReadingEnabled() const { return read_task_callback != std::nullopt; }\n+\n private:\n     static MergeTreeDataSelectAnalysisResultPtr selectRangesToReadImpl(\n         MergeTreeData::DataPartsVector parts,\n@@ -175,8 +191,6 @@ class ReadFromMergeTree final : public SourceStepWithFilter\n         bool sample_factor_column_queried,\n         Poco::Logger * log);\n \n-    bool isQueryWithFinal() const;\n-\n     int getSortDirection() const\n     {\n         const InputOrderInfoPtr & order_info = query_info.getInputOrderInfo();\n@@ -244,7 +258,6 @@ class ReadFromMergeTree final : public SourceStepWithFilter\n     Pipe spreadMarkRangesAmongStreamsFinal(\n         RangesInDataParts && parts, size_t num_streams, const Names & column_names, ActionsDAGPtr & out_projection);\n \n-    MergeTreeDataSelectAnalysisResultPtr selectRangesToRead(MergeTreeData::DataPartsVector parts) const;\n     ReadFromMergeTree::AnalysisResult getAnalysisResult() const;\n     MergeTreeDataSelectAnalysisResultPtr analyzed_result_ptr;\n \ndiff --git a/src/Processors/Transforms/FilterTransform.cpp b/src/Processors/Transforms/FilterTransform.cpp\nindex 2f5f3d69cff5..089393a14307 100644\n--- a/src/Processors/Transforms/FilterTransform.cpp\n+++ b/src/Processors/Transforms/FilterTransform.cpp\n@@ -3,10 +3,17 @@\n #include <Interpreters/ExpressionActions.h>\n #include <Columns/ColumnsCommon.h>\n #include <Core/Field.h>\n+#include <DataTypes/DataTypeNullable.h>\n+#include <DataTypes/DataTypeLowCardinality.h>\n \n namespace DB\n {\n \n+namespace ErrorCodes\n+{\n+    extern const int ILLEGAL_TYPE_OF_COLUMN_FOR_FILTER;\n+}\n+\n static void replaceFilterToConstant(Block & block, const String & filter_column_name)\n {\n     ConstantFilterDescription constant_filter_description;\n@@ -36,6 +43,12 @@ Block FilterTransform::transformHeader(\n     if (expression)\n         header = expression->updateHeader(std::move(header));\n \n+    auto filter_type = header.getByName(filter_column_name).type;\n+    if (!filter_type->onlyNull() && !isUInt8(removeNullable(removeLowCardinality(filter_type))))\n+        throw Exception(ErrorCodes::ILLEGAL_TYPE_OF_COLUMN_FOR_FILTER,\n+            \"Illegal type {} of column {} for filter. Must be UInt8 or Nullable(UInt8).\",\n+            filter_type->getName(), filter_column_name);\n+\n     if (remove_filter_column)\n         header.erase(filter_column_name);\n     else\ndiff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex 84c301e59860..acb5ed248c86 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -6238,7 +6238,7 @@ Block MergeTreeData::getMinMaxCountProjectionBlock(\n             agg_count->set(place, value.get<UInt64>());\n         else\n         {\n-            auto value_column = func->getResultType()->createColumnConst(1, value)->convertToFullColumnIfConst();\n+            auto value_column = func->getArgumentTypes().front()->createColumnConst(1, value)->convertToFullColumnIfConst();\n             const auto * value_column_ptr = value_column.get();\n             func->add(place, &value_column_ptr, 0, &arena);\n         }\n@@ -6445,6 +6445,9 @@ std::optional<ProjectionCandidate> MergeTreeData::getQueryProcessingStageWithAgg\n     const auto & metadata_snapshot = storage_snapshot->metadata;\n     const auto & settings = query_context->getSettingsRef();\n \n+    if (settings.query_plan_optimize_projection)\n+        return std::nullopt;\n+\n     /// TODO: Analyzer syntax analyzer result\n     if (!query_info.syntax_analyzer_result)\n         return std::nullopt;\ndiff --git a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\nindex 936b95617255..ff8862f0f36a 100644\n--- a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\n@@ -56,7 +56,6 @@ namespace ErrorCodes\n     extern const int ILLEGAL_TYPE_OF_COLUMN_FOR_FILTER;\n     extern const int ILLEGAL_COLUMN;\n     extern const int ARGUMENT_OUT_OF_BOUND;\n-    extern const int TOO_MANY_ROWS;\n     extern const int CANNOT_PARSE_TEXT;\n     extern const int TOO_MANY_PARTITIONS;\n     extern const int DUPLICATED_PART_UUIDS;\n@@ -157,7 +156,7 @@ QueryPlanPtr MergeTreeDataSelectExecutor::read(\n \n     if (!query_info.projection)\n     {\n-        auto plan = readFromParts(\n+        auto step = readFromParts(\n             query_info.merge_tree_select_result_ptr ? MergeTreeData::DataPartsVector{} : parts,\n             column_names_to_return,\n             storage_snapshot,\n@@ -169,11 +168,14 @@ QueryPlanPtr MergeTreeDataSelectExecutor::read(\n             query_info.merge_tree_select_result_ptr,\n             enable_parallel_reading);\n \n-        if (plan->isInitialized() && settings.allow_experimental_projection_optimization && settings.force_optimize_projection\n-            && !metadata_for_reading->projections.empty())\n+        if (!step && settings.allow_experimental_projection_optimization && settings.force_optimize_projection\n+            && !metadata_for_reading->projections.empty() && !settings.query_plan_optimize_projection)\n             throw Exception(ErrorCodes::PROJECTION_NOT_USED,\n                             \"No projection is used when allow_experimental_projection_optimization = 1 and force_optimize_projection = 1\");\n \n+        auto plan = std::make_unique<QueryPlan>();\n+        if (step)\n+            plan->addStep(std::move(step));\n         return plan;\n     }\n \n@@ -197,7 +199,7 @@ QueryPlanPtr MergeTreeDataSelectExecutor::read(\n     else if (query_info.projection->merge_tree_projection_select_result_ptr)\n     {\n         LOG_DEBUG(log, \"projection required columns: {}\", fmt::join(query_info.projection->required_columns, \", \"));\n-        projection_plan = readFromParts(\n+        projection_plan->addStep(readFromParts(\n             {},\n             query_info.projection->required_columns,\n             storage_snapshot,\n@@ -207,7 +209,7 @@ QueryPlanPtr MergeTreeDataSelectExecutor::read(\n             num_streams,\n             max_block_numbers_to_read,\n             query_info.projection->merge_tree_projection_select_result_ptr,\n-            enable_parallel_reading);\n+            enable_parallel_reading));\n     }\n \n     if (projection_plan->isInitialized())\n@@ -988,26 +990,6 @@ RangesInDataParts MergeTreeDataSelectExecutor::filterPartsByPrimaryKeyAndSkipInd\n \n     /// Let's find what range to read from each part.\n     {\n-        std::atomic<size_t> total_rows{0};\n-\n-        /// Do not check number of read rows if we have reading\n-        /// in order of sorting key with limit.\n-        /// In general case, when there exists WHERE clause\n-        /// it's impossible to estimate number of rows precisely,\n-        /// because we can stop reading at any time.\n-\n-        SizeLimits limits;\n-        if (settings.read_overflow_mode == OverflowMode::THROW\n-            && settings.max_rows_to_read\n-            && !query_info.input_order_info)\n-            limits = SizeLimits(settings.max_rows_to_read, 0, settings.read_overflow_mode);\n-\n-        SizeLimits leaf_limits;\n-        if (settings.read_overflow_mode_leaf == OverflowMode::THROW\n-            && settings.max_rows_to_read_leaf\n-            && !query_info.input_order_info)\n-            leaf_limits = SizeLimits(settings.max_rows_to_read_leaf, 0, settings.read_overflow_mode_leaf);\n-\n         auto mark_cache = context->getIndexMarkCache();\n         auto uncompressed_cache = context->getIndexUncompressedCache();\n \n@@ -1082,24 +1064,7 @@ RangesInDataParts MergeTreeDataSelectExecutor::filterPartsByPrimaryKeyAndSkipInd\n             }\n \n             if (!ranges.ranges.empty())\n-            {\n-                if (limits.max_rows || leaf_limits.max_rows)\n-                {\n-                    /// Fail fast if estimated number of rows to read exceeds the limit\n-                    auto current_rows_estimate = ranges.getRowsCount();\n-                    size_t prev_total_rows_estimate = total_rows.fetch_add(current_rows_estimate);\n-                    size_t total_rows_estimate = current_rows_estimate + prev_total_rows_estimate;\n-                    if (query_info.limit > 0 && total_rows_estimate > query_info.limit)\n-                    {\n-                        total_rows_estimate = query_info.limit;\n-                    }\n-                    limits.check(total_rows_estimate, 0, \"rows (controlled by 'max_rows_to_read' setting)\", ErrorCodes::TOO_MANY_ROWS);\n-                    leaf_limits.check(\n-                        total_rows_estimate, 0, \"rows (controlled by 'max_rows_to_read_leaf' setting)\", ErrorCodes::TOO_MANY_ROWS);\n-                }\n-\n                 parts_with_ranges[part_index] = std::move(ranges);\n-            }\n         };\n \n         size_t num_threads = std::min<size_t>(num_streams, parts.size());\n@@ -1332,7 +1297,7 @@ MergeTreeDataSelectAnalysisResultPtr MergeTreeDataSelectExecutor::estimateNumMar\n         log);\n }\n \n-QueryPlanPtr MergeTreeDataSelectExecutor::readFromParts(\n+QueryPlanStepPtr MergeTreeDataSelectExecutor::readFromParts(\n     MergeTreeData::DataPartsVector parts,\n     const Names & column_names_to_return,\n     const StorageSnapshotPtr & storage_snapshot,\n@@ -1348,10 +1313,10 @@ QueryPlanPtr MergeTreeDataSelectExecutor::readFromParts(\n     if (merge_tree_select_result_ptr)\n     {\n         if (merge_tree_select_result_ptr->marks() == 0)\n-            return std::make_unique<QueryPlan>();\n+            return {};\n     }\n     else if (parts.empty())\n-        return std::make_unique<QueryPlan>();\n+        return {};\n \n     Names real_column_names;\n     Names virt_column_names;\n@@ -1361,7 +1326,7 @@ QueryPlanPtr MergeTreeDataSelectExecutor::readFromParts(\n \n     selectColumnNames(column_names_to_return, data, real_column_names, virt_column_names, sample_factor_column_queried);\n \n-    auto read_from_merge_tree = std::make_unique<ReadFromMergeTree>(\n+    return std::make_unique<ReadFromMergeTree>(\n         std::move(parts),\n         real_column_names,\n         virt_column_names,\n@@ -1377,10 +1342,6 @@ QueryPlanPtr MergeTreeDataSelectExecutor::readFromParts(\n         merge_tree_select_result_ptr,\n         enable_parallel_reading\n     );\n-\n-    QueryPlanPtr plan = std::make_unique<QueryPlan>();\n-    plan->addStep(std::move(read_from_merge_tree));\n-    return plan;\n }\n \n \ndiff --git a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h\nindex 30d09312245e..a337574bb64e 100644\n--- a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h\n+++ b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.h\n@@ -39,7 +39,7 @@ class MergeTreeDataSelectExecutor\n         bool enable_parallel_reading = false) const;\n \n     /// The same as read, but with specified set of parts.\n-    QueryPlanPtr readFromParts(\n+    QueryPlanStepPtr readFromParts(\n         MergeTreeData::DataPartsVector parts,\n         const Names & column_names,\n         const StorageSnapshotPtr & storage_snapshot,\ndiff --git a/src/Storages/MergeTree/StorageFromMergeTreeDataPart.h b/src/Storages/MergeTree/StorageFromMergeTreeDataPart.h\nindex 7bad9947a886..2e0ad116d706 100644\n--- a/src/Storages/MergeTree/StorageFromMergeTreeDataPart.h\n+++ b/src/Storages/MergeTree/StorageFromMergeTreeDataPart.h\n@@ -67,7 +67,7 @@ class StorageFromMergeTreeDataPart final : public IStorage\n         size_t max_block_size,\n         size_t num_streams) override\n     {\n-        query_plan = std::move(*MergeTreeDataSelectExecutor(storage)\n+        query_plan.addStep(MergeTreeDataSelectExecutor(storage)\n                                               .readFromParts(\n                                                   parts,\n                                                   column_names,\ndiff --git a/src/Storages/StorageReplicatedMergeTree.h b/src/Storages/StorageReplicatedMergeTree.h\nindex 6328c8640552..270d4eb68b20 100644\n--- a/src/Storages/StorageReplicatedMergeTree.h\n+++ b/src/Storages/StorageReplicatedMergeTree.h\n@@ -326,12 +326,13 @@ class StorageReplicatedMergeTree final : public MergeTreeData\n     bool canUseZeroCopyReplication() const;\n \n     bool isTableReadOnly () { return is_readonly; }\n-private:\n-    std::atomic_bool are_restoring_replica {false};\n \n     /// Get a sequential consistent view of current parts.\n     ReplicatedMergeTreeQuorumAddedParts::PartitionIdToMaxBlock getMaxAddedBlocks() const;\n \n+private:\n+    std::atomic_bool are_restoring_replica {false};\n+\n     /// Delete old parts from disk and from ZooKeeper.\n     void clearOldPartsAndRemoveFromZK();\n \n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01505_trivial_count_with_partition_predicate.reference b/tests/queries/0_stateless/01505_trivial_count_with_partition_predicate.reference\nindex 5abc312652d4..b434c50b0703 100644\n--- a/tests/queries/0_stateless/01505_trivial_count_with_partition_predicate.reference\n+++ b/tests/queries/0_stateless/01505_trivial_count_with_partition_predicate.reference\n@@ -1,3 +1,4 @@\n+3\n 0\n 0\n 2\ndiff --git a/tests/queries/0_stateless/01505_trivial_count_with_partition_predicate.sql b/tests/queries/0_stateless/01505_trivial_count_with_partition_predicate.sql\nindex e8643a4468cf..030db421683f 100644\n--- a/tests/queries/0_stateless/01505_trivial_count_with_partition_predicate.sql\n+++ b/tests/queries/0_stateless/01505_trivial_count_with_partition_predicate.sql\n@@ -7,7 +7,7 @@ insert into test1 values ('2020-09-01 00:01:02', 1), ('2020-09-01 20:01:03', 2),\n \n set max_rows_to_read = 1;\n -- non-optimized\n-select count() from test1 settings max_parallel_replicas = 3; -- { serverError 158 }\n+select count() from test1 settings max_parallel_replicas = 3;\n -- optimized (toYear is monotonic and we provide the partition expr as is)\n select count() from test1 where toYear(toDate(p)) = 1999;\n -- non-optimized (toDate(DateTime) is always monotonic, but we cannot relaxing the predicates to do trivial count())\ndiff --git a/tests/queries/0_stateless/01710_normal_projection_fix1.sql b/tests/queries/0_stateless/01710_normal_projection_fix1.sql\nindex b4d7c6e8734b..e8ba830e5a58 100644\n--- a/tests/queries/0_stateless/01710_normal_projection_fix1.sql\n+++ b/tests/queries/0_stateless/01710_normal_projection_fix1.sql\n@@ -7,6 +7,7 @@ insert into t values (1, 2);\n alter table t add projection x (select * order by j);\n \n insert into t values (1, 4);\n+insert into t values (1, 5);\n \n set allow_experimental_projection_optimization = 1, force_optimize_projection = 1;\n \n@@ -14,4 +15,6 @@ select i from t prewhere j = 4;\n \n SELECT j = 2, i FROM t PREWHERE j = 2;\n \n+SELECT j = -1, j = NULL FROM t WHERE j = -1;\n+\n drop table t;\ndiff --git a/tests/queries/0_stateless/01710_projection_aggregation_in_order.sql b/tests/queries/0_stateless/01710_projection_aggregation_in_order.sql\nindex 31d32da0ed35..c7ed91eb19bc 100644\n--- a/tests/queries/0_stateless/01710_projection_aggregation_in_order.sql\n+++ b/tests/queries/0_stateless/01710_projection_aggregation_in_order.sql\n@@ -21,7 +21,7 @@ ENGINE = MergeTree\n ORDER BY (key, ts);\n \n INSERT INTO normal SELECT\n-    1,\n+    number,\n     toDateTime('2021-12-06 00:00:00') + number,\n     number\n FROM numbers(100000);\ndiff --git a/tests/queries/0_stateless/01710_projections_in_distributed_query.sql b/tests/queries/0_stateless/01710_projections_in_distributed_query.sql\nindex fa734b605cd4..7600340579d9 100644\n--- a/tests/queries/0_stateless/01710_projections_in_distributed_query.sql\n+++ b/tests/queries/0_stateless/01710_projections_in_distributed_query.sql\n@@ -1,5 +1,7 @@\n -- Tags: distributed\n \n+set enable_memory_bound_merging_of_aggregation_results=0;\n+\n drop table if exists projection_test;\n \n create table projection_test (dt DateTime, cost Int64, projection p (select toStartOfMinute(dt) dt_m, sum(cost) group by dt_m)) engine MergeTree partition by toDate(dt) order by dt;\ndiff --git a/tests/queries/0_stateless/02319_no_columns_in_row_level_filter.reference b/tests/queries/0_stateless/02319_no_columns_in_row_level_filter.reference\nindex c0911ffc5980..d344f57649d6 100644\n--- a/tests/queries/0_stateless/02319_no_columns_in_row_level_filter.reference\n+++ b/tests/queries/0_stateless/02319_no_columns_in_row_level_filter.reference\n@@ -1,4 +1,4 @@\n-1000000\n+0\n 0\n 0\n 0\ndiff --git a/tests/queries/0_stateless/02343_aggregation_pipeline.reference b/tests/queries/0_stateless/02343_aggregation_pipeline.reference\nindex ec9a394d05d8..ca838fdf4e0c 100644\n--- a/tests/queries/0_stateless/02343_aggregation_pipeline.reference\n+++ b/tests/queries/0_stateless/02343_aggregation_pipeline.reference\n@@ -92,12 +92,15 @@ ExpressionTransform \u00d7 16\n   (MergingAggregated)\n   Resize 1 \u2192 16\n     MergingAggregatedTransform\n-      Resize 2 \u2192 1\n+      Resize 17 \u2192 1\n         (Union)\n-          (ReadFromStorage)\n-          AggregatingTransform\n-            ExpressionTransform\n-              MergeTreeInOrder 0 \u2192 1\n+          (Aggregating)\n+          Resize 1 \u2192 16\n+            AggregatingTransform\n+              (Expression)\n+              ExpressionTransform\n+                (ReadFromMergeTree)\n+                MergeTreeInOrder 0 \u2192 1\n           (ReadFromRemote)\n explain pipeline SELECT k1, k3, sum(value) v FROM remote('127.0.0.{1,2}', currentDatabase(), proj_agg_02343) GROUP BY k1, k3 SETTINGS distributed_aggregation_memory_efficient = 1;\n (Expression)\n@@ -109,9 +112,11 @@ ExpressionTransform \u00d7 16\n         Resize 1 \u2192 16\n           GroupingAggregatedTransform 2 \u2192 1\n             (Union)\n-              (ReadFromStorage)\n+              (Aggregating)\n               AggregatingTransform\n+                (Expression)\n                 ExpressionTransform\n+                  (ReadFromMergeTree)\n                   MergeTreeInOrder 0 \u2192 1\n               (ReadFromRemote)\n -- { echoOn }\ndiff --git a/tests/queries/0_stateless/02515_projections_with_totals.reference b/tests/queries/0_stateless/02515_projections_with_totals.reference\nindex c6359cae0323..f086fcb10e9b 100644\n--- a/tests/queries/0_stateless/02515_projections_with_totals.reference\n+++ b/tests/queries/0_stateless/02515_projections_with_totals.reference\n@@ -1,3 +1,6 @@\n 0\n \n 0\n+0\n+\n+0\ndiff --git a/tests/queries/0_stateless/02515_projections_with_totals.sql b/tests/queries/0_stateless/02515_projections_with_totals.sql\nindex 4d43d5381da8..1e4b5c6f2553 100644\n--- a/tests/queries/0_stateless/02515_projections_with_totals.sql\n+++ b/tests/queries/0_stateless/02515_projections_with_totals.sql\n@@ -3,4 +3,8 @@ CREATE TABLE t (x UInt8, PROJECTION p (SELECT x GROUP BY x)) ENGINE = MergeTree\n INSERT INTO t VALUES (0);\n SET group_by_overflow_mode = 'any', max_rows_to_group_by = 1000, totals_mode = 'after_having_auto';\n SELECT x FROM t GROUP BY x WITH TOTALS;\n+\n+SET optimize_aggregation_in_order=1;\n+SELECT x FROM t GROUP BY x WITH TOTALS;\n+\n DROP TABLE t;\ndiff --git a/tests/queries/0_stateless/02521_cannot_find_column_in_projection.sql b/tests/queries/0_stateless/02521_cannot_find_column_in_projection.sql\nindex 255c6f56ab30..6ee8ec07178c 100644\n--- a/tests/queries/0_stateless/02521_cannot_find_column_in_projection.sql\n+++ b/tests/queries/0_stateless/02521_cannot_find_column_in_projection.sql\n@@ -1,5 +1,3 @@\n-SET allow_experimental_analyzer = 1;\n-\n drop table if exists test;\n create table test(day Date, id UInt32) engine=MergeTree partition by day order by tuple();\n insert into test select toDate('2023-01-05') AS day, number from numbers(10);\ndiff --git a/tests/queries/0_stateless/02680_illegal_type_of_filter_projection.reference b/tests/queries/0_stateless/02680_illegal_type_of_filter_projection.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/02680_illegal_type_of_filter_projection.sql b/tests/queries/0_stateless/02680_illegal_type_of_filter_projection.sql\nnew file mode 100644\nindex 000000000000..3ef3b8a4fe6a\n--- /dev/null\n+++ b/tests/queries/0_stateless/02680_illegal_type_of_filter_projection.sql\n@@ -0,0 +1,3 @@\n+CREATE TABLE test_tuple (`p` DateTime, `i` int, `j` int) ENGINE = MergeTree PARTITION BY (toDate(p), i) ORDER BY j SETTINGS index_granularity = 1;\n+insert into test_tuple values (1, 1, 1);\n+SELECT count() FROM test_tuple PREWHERE sipHash64(sipHash64(p, toString(toDate(p))), toString(toDate(p))) % -0. WHERE i > NULL settings optimize_trivial_count_query=0; -- { serverError ILLEGAL_TYPE_OF_COLUMN_FOR_FILTER }\ndiff --git a/tests/queries/1_stateful/00172_early_constant_folding.reference b/tests/queries/1_stateful/00172_early_constant_folding.reference\nindex da564dc694ea..6b72183c0665 100644\n--- a/tests/queries/1_stateful/00172_early_constant_folding.reference\n+++ b/tests/queries/1_stateful/00172_early_constant_folding.reference\n@@ -1,6 +1,9 @@\n (Expression)\n-ExpressionTransform\n-  (ReadFromStorage)\n-  AggregatingTransform\n-    ExpressionTransform\n-      SourceFromSingleChunk 0 \u2192 1\n+ExpressionTransform \u00d7 10\n+  (Aggregating)\n+  Resize 1 \u2192 10\n+    AggregatingTransform\n+      (Expression)\n+      ExpressionTransform\n+        (ReadFromPreparedSource)\n+        SourceFromSingleChunk 0 \u2192 1\ndiff --git a/tests/queries/1_stateful/00172_early_constant_folding.sql b/tests/queries/1_stateful/00172_early_constant_folding.sql\nindex b31e418b4926..1ed7b8719b4e 100644\n--- a/tests/queries/1_stateful/00172_early_constant_folding.sql\n+++ b/tests/queries/1_stateful/00172_early_constant_folding.sql\n@@ -1,3 +1,4 @@\n -- Tags: no-parallel-replicas\n \n+set max_threads=10;\n EXPLAIN PIPELINE SELECT count(JavaEnable) FROM test.hits WHERE WatchID = 1 OR Title = 'next' OR URL = 'prev' OR URL = '???' OR 1;\n",
  "problem_statement": "DB::Exception: Cannot find column `count(id_with)` in source stream\nHow to reproduce:\r\n```sql\r\n:) create table test(day Date, id UInt32) engine=MergeTree partition by day order by tuple()                                                    \r\n                                                                                                                                                                                                                    \r\nCREATE TABLE test                                                                                                                                                                                                   \r\n(                                                                                                                                                                                                                   \r\n    `day` Date,                                                                                                                                                                                                     \r\n    `id` UInt32                                                                                                                                                                                                     \r\n)                                                                                                                                                                                                                   \r\nENGINE = MergeTree                                                                                                                                                                                                  \r\nPARTITION BY day                                                                                                                                                                                                    \r\nORDER BY tuple()                                                                                                                                                                                                    \r\n                                                                                                                                                                                                                    \r\nQuery id: a81e4c06-cbc1-4e18-b1dd-1d68cc30a630                                                                                                                                                                      \r\n                                                                                                                                                                                                                    \r\nOk.                                                                                                                                                                                                                 \r\n                                                                                                                                                                                                                    \r\n0 rows in set. Elapsed: 0.011 sec.           \r\n\r\n:) insert into test select toDate('2023-01-05') AS day, number from numbers(10);\r\n\r\nINSERT INTO test SELECT\r\n    toDate('2023-01-05') AS day,\r\n    number\r\nFROM numbers(10)\r\n\r\nQuery id: 1ee62a35-6c91-4797-9a6e-08518f34995b\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.002 sec.\r\n\r\n :) with toUInt64(id) as id_with select day, count(id_with)  from test where day >= '2023-01-01' group by day limit 1000\r\n\r\nWITH toUInt64(id) AS id_with\r\nSELECT\r\n    day,\r\n    count(id_with)\r\nFROM test\r\nWHERE day >= '2023-01-01'\r\nGROUP BY day\r\nLIMIT 1000\r\n\r\nQuery id: 362bf093-d693-4655-9d2c-a3e4b687708c\r\n\r\n\r\n0 rows in set. Elapsed: 0.013 sec. \r\n\r\nReceived exception from server (version 22.13.1):\r\nCode: 8. DB::Exception: Received from localhost:9000. DB::Exception: Cannot find column `count(id_with)` in source stream, there are only columns: [day]. (THERE_IS_NO_COLUMN)\r\n```\n",
  "hints_text": "",
  "created_at": "2023-02-17T19:19:24Z"
}