{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 57414,
  "instance_id": "ClickHouse__ClickHouse-57414",
  "issue_numbers": [
    "48496"
  ],
  "base_commit": "ef8068ed03c09253ad31b3dec5033713be2cd291",
  "patch": "diff --git a/src/Planner/Planner.cpp b/src/Planner/Planner.cpp\nindex d6e0f42a06dd..95c61f8d0117 100644\n--- a/src/Planner/Planner.cpp\n+++ b/src/Planner/Planner.cpp\n@@ -1341,27 +1341,52 @@ void Planner::buildPlanForQueryNode()\n \n     const auto & settings = query_context->getSettingsRef();\n \n-    /// Check support for JOIN for parallel replicas with custom key\n-    if (planner_context->getTableExpressionNodeToData().size() > 1)\n+    if (settings.allow_experimental_parallel_reading_from_replicas > 0)\n     {\n-        if (settings.allow_experimental_parallel_reading_from_replicas == 1 || !settings.parallel_replicas_custom_key.value.empty())\n+        const auto & table_expression_nodes = planner_context->getTableExpressionNodeToData();\n+        for (const auto & it : table_expression_nodes)\n         {\n-            LOG_DEBUG(\n-                &Poco::Logger::get(\"Planner\"),\n-                \"JOINs are not supported with parallel replicas. Query will be executed without using them.\");\n+            auto * table_node = it.first->as<TableNode>();\n+            if (!table_node)\n+                continue;\n \n-            auto & mutable_context = planner_context->getMutableQueryContext();\n-            mutable_context->setSetting(\"allow_experimental_parallel_reading_from_replicas\", Field(0));\n-            mutable_context->setSetting(\"parallel_replicas_custom_key\", String{\"\"});\n+            const auto & modifiers = table_node->getTableExpressionModifiers();\n+            if (modifiers.has_value() && modifiers->hasFinal())\n+            {\n+                if (settings.allow_experimental_parallel_reading_from_replicas == 2)\n+                    throw Exception(ErrorCodes::SUPPORT_IS_DISABLED, \"FINAL modifier is not supported with parallel replicas\");\n+                else\n+                {\n+                    LOG_DEBUG(\n+                        &Poco::Logger::get(\"Planner\"),\n+                        \"FINAL modifier is not supported with parallel replicas. Query will be executed without using them.\");\n+                    auto & mutable_context = planner_context->getMutableQueryContext();\n+                    mutable_context->setSetting(\"allow_experimental_parallel_reading_from_replicas\", Field(0));\n+                }\n+            }\n         }\n-        else if (settings.allow_experimental_parallel_reading_from_replicas == 2)\n+    }\n+\n+    if (settings.allow_experimental_parallel_reading_from_replicas > 0 || !settings.parallel_replicas_custom_key.value.empty())\n+    {\n+        /// Check support for JOIN for parallel replicas with custom key\n+        if (planner_context->getTableExpressionNodeToData().size() > 1)\n         {\n-            throw Exception(ErrorCodes::SUPPORT_IS_DISABLED, \"JOINs are not supported with parallel replicas\");\n+            if (settings.allow_experimental_parallel_reading_from_replicas == 2)\n+                throw Exception(ErrorCodes::SUPPORT_IS_DISABLED, \"JOINs are not supported with parallel replicas\");\n+            else\n+            {\n+                LOG_DEBUG(\n+                    &Poco::Logger::get(\"Planner\"),\n+                    \"JOINs are not supported with parallel replicas. Query will be executed without using them.\");\n+\n+                auto & mutable_context = planner_context->getMutableQueryContext();\n+                mutable_context->setSetting(\"allow_experimental_parallel_reading_from_replicas\", Field(0));\n+                mutable_context->setSetting(\"parallel_replicas_custom_key\", String{\"\"});\n+            }\n         }\n     }\n \n-    /// TODO: Also disable parallel replicas in case of FINAL\n-\n     auto top_level_identifiers = collectTopLevelColumnIdentifiers(query_tree, planner_context);\n     auto join_tree_query_plan = buildJoinTreeQueryPlan(query_tree,\n         select_query_info,\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02932_parallel_replicas_fuzzer.reference b/tests/queries/0_stateless/02932_parallel_replicas_fuzzer.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/02932_parallel_replicas_fuzzer.sql b/tests/queries/0_stateless/02932_parallel_replicas_fuzzer.sql\nnew file mode 100644\nindex 000000000000..3daaf36188a8\n--- /dev/null\n+++ b/tests/queries/0_stateless/02932_parallel_replicas_fuzzer.sql\n@@ -0,0 +1,38 @@\n+SET parallel_replicas_for_non_replicated_merge_tree=1;\n+\n+-- https://github.com/ClickHouse/ClickHouse/issues/49559\n+CREATE TABLE join_inner_table__fuzz_146 (`id` UUID, `key` String, `number` Int64, `value1` String, `value2` String, `time` Nullable(Int64)) ENGINE = MergeTree ORDER BY (id, number, key);\n+INSERT INTO join_inner_table__fuzz_146 SELECT CAST('833c9e22-c245-4eb5-8745-117a9a1f26b1', 'UUID') AS id, CAST(rowNumberInAllBlocks(), 'String') AS key, * FROM generateRandom('number Int64, value1 String, value2 String, time Int64', 1, 10, 2) LIMIT 100;\n+SELECT key, value1, value2, toUInt64(min(time)) AS start_ts FROM join_inner_table__fuzz_146 GROUP BY key, value1, value2 WITH CUBE ORDER BY key ASC NULLS LAST, value2 DESC NULLS LAST LIMIT 9223372036854775806\n+    FORMAT Null\n+    SETTINGS\n+        max_parallel_replicas = 3,\n+        prefer_localhost_replica = 1,\n+        cluster_for_parallel_replicas = 'test_cluster_one_shard_three_replicas_localhost',\n+        allow_experimental_parallel_reading_from_replicas = 1,\n+        use_hedged_requests = 0;\n+\n+\n+-- https://github.com/ClickHouse/ClickHouse/issues/48496\n+CREATE TABLE t_02709__fuzz_23 (`key` Nullable(UInt8), `sign` Int8, `date` DateTime64(3)) ENGINE = CollapsingMergeTree(sign) PARTITION BY date ORDER BY key SETTINGS allow_nullable_key=1;\n+INSERT INTO t_02709__fuzz_23 values (1, 1, '2023-12-01 00:00:00.000');\n+SELECT NULL FROM t_02709__fuzz_23 FINAL\n+GROUP BY sign, '1023'\n+ORDER BY nan DESC, [0, NULL, NULL, NULL, NULL] DESC\n+FORMAT Null\n+SETTINGS\n+    max_parallel_replicas = 3,\n+    allow_experimental_parallel_reading_from_replicas = 1,\n+    use_hedged_requests = 0,\n+    cluster_for_parallel_replicas = 'test_cluster_one_shard_three_replicas_localhost';\n+\n+SELECT _CAST(NULL, 'Nullable(Nothing)') AS `NULL`\n+FROM t_02709__fuzz_23 FINAL\n+GROUP BY\n+    t_02709__fuzz_23.sign,\n+    '1023'\n+ORDER BY\n+    nan DESC,\n+    _CAST([0, NULL, NULL, NULL, NULL], 'Array(Nullable(UInt8))') DESC\n+FORMAT Null\n+SETTINGS receive_timeout = 10., receive_data_timeout_ms = 10000, use_hedged_requests = 0, allow_suspicious_low_cardinality_types = 1, max_parallel_replicas = 3, cluster_for_parallel_replicas = 'test_cluster_one_shard_three_replicas_localhost', allow_experimental_parallel_reading_from_replicas = 1, parallel_replicas_for_non_replicated_merge_tree = 1, log_queries = 1, table_function_remote_max_addresses = 200, allow_experimental_analyzer = 1;\n",
  "problem_statement": "Logical error: 'Chunk info was not set for chunk in GroupingAggregatedTransform.'\n**Describe the bug**\r\n\r\nhttps://s3.amazonaws.com/clickhouse-test-reports/0/3ad0a6ac1861292ef3e74d2738a7c67f44b0932d/fuzzer_astfuzzermsan/report.html\r\n```\r\n2023.04.06 04:03:52.021197 [ 495 ] {f2a512bd-c6c5-455b-bb60-53bba1c5f6d8} <Fatal> : Logical error: 'Chunk info was not set for chunk in GroupingAggregatedTransform.'.\r\n2023.04.06 04:03:52.022113 [ 502 ] {} <Fatal> BaseDaemon: ########################################\r\n2023.04.06 04:03:52.022301 [ 502 ] {} <Fatal> BaseDaemon: (version 23.4.1.1 (official build), build id: D27FC0279FD8E768868AC5CAC2AD96D9EF2EA873) (from thread 495) (query_id: f2a512bd-c6c5-455b-bb60-53bba1c5f6d8) (query: SELECT NULL FROM t_02709__fuzz_23 FINAL GROUP BY sign, '1023' ORDER BY nan DESC, [0, NULL, NULL, NULL, NULL] DESC SETTINGS max_parallel_replicas = 3, allow_experimental_parallel_reading_from_replicas = 1, use_hedged_requests = 0, cluster_for_parallel_replicas = 'parallel_replicas') Received signal Aborted (6)\r\n2023.04.06 04:03:52.022512 [ 502 ] {} <Fatal> BaseDaemon: \r\n2023.04.06 04:03:52.022697 [ 502 ] {} <Fatal> BaseDaemon: Stack trace: 0x7f96aa57200b 0x7f96aa551859 0x2994b2b1 0x2994c5ff 0x1670c3ba 0x4d3617bc 0x4d35f33c 0x4d364da3 0x2a074ebc 0x4c9236d0 0x4c909cc0 0x4c90f8ce 0x29cd4bf2 0x29ce5d23 0x7f96aa729609 0x7f96aa64e133\r\n2023.04.06 04:03:52.022896 [ 502 ] {} <Fatal> BaseDaemon: 4. raise @ 0x7f96aa57200b in ?\r\n2023.04.06 04:03:52.023056 [ 502 ] {} <Fatal> BaseDaemon: 5. abort @ 0x7f96aa551859 in ?\r\n2023.04.06 04:03:52.351159 [ 502 ] {} <Fatal> BaseDaemon: 6. ./build_docker/./src/Common/Exception.cpp:0: DB::abortOnFailedAssertion(String const&) @ 0x2994b2b1 in /workspace/clickhouse\r\n2023.04.06 04:03:52.667905 [ 502 ] {} <Fatal> BaseDaemon: 7.1. inlined from ./build_docker/./src/Common/Exception.cpp:0: DB::handle_error_code(String const&, int, bool, std::vector<void*, std::allocator<void*>> const&)\r\n2023.04.06 04:03:52.668116 [ 502 ] {} <Fatal> BaseDaemon: 7. ./build_docker/./src/Common/Exception.cpp:92: DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x2994c5ff in /workspace/clickhouse\r\n2023.04.06 04:04:01.382279 [ 502 ] {} <Fatal> BaseDaemon: 8. DB::Exception::Exception<char const (&) [65], void>(int, char const (&) [65]) @ 0x1670c3ba in /workspace/clickhouse\r\n2023.04.06 04:04:01.696671 [ 502 ] {} <Fatal> BaseDaemon: 9. ./build_docker/./src/Processors/Transforms/MergingAggregatedMemoryEfficientTransform.cpp:256: DB::GroupingAggregatedTransform::addChunk(DB::Chunk, unsigned long) @ 0x4d3617bc in /workspace/clickhouse\r\n2023.04.06 04:04:02.003459 [ 502 ] {} <Fatal> BaseDaemon: 10. ./build_docker/./src/Processors/Transforms/MergingAggregatedMemoryEfficientTransform.cpp:50: DB::GroupingAggregatedTransform::readFromAllInputs() @ 0x4d35f33c in /workspace/clickhouse\r\n2023.04.06 04:04:02.341257 [ 502 ] {} <Fatal> BaseDaemon: 11. ./build_docker/./src/Processors/Transforms/MergingAggregatedMemoryEfficientTransform.cpp:141: DB::GroupingAggregatedTransform::prepare() @ 0x4d364da3 in /workspace/clickhouse\r\n2023.04.06 04:04:03.821105 [ 502 ] {} <Fatal> BaseDaemon: 12. ./build_docker/./src/Processors/IProcessor.h:193: DB::IProcessor::prepare(std::vector<unsigned long, std::allocator<unsigned long>> const&, std::vector<unsigned long, std::allocator<unsigned long>> const&) @ 0x2a074ebc in /workspace/clickhouse\r\n2023.04.06 04:04:04.045051 [ 502 ] {} <Fatal> BaseDaemon: 13. ./build_docker/./src/Processors/Executors/ExecutingGraph.cpp:0: DB::ExecutingGraph::updateNode(unsigned long, std::queue<DB::ExecutingGraph::Node*, std::deque<DB::ExecutingGraph::Node*, std::allocator<DB::ExecutingGraph::Node*>>>&, std::queue<DB::ExecutingGraph::Node*, std::deque<DB::ExecutingGraph::Node*, std::allocator<DB::ExecutingGraph::Node*>>>&) @ 0x4c9236d0 in /workspace/clickhouse\r\n2023.04.06 04:04:04.283519 [ 502 ] {} <Fatal> BaseDaemon: 14. ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:0: DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*) @ 0x4c909cc0 in /workspace/clickhouse\r\n2023.04.06 04:04:04.553010 [ 502 ] {} <Fatal> BaseDaemon: 15.1. inlined from ./build_docker/./base/base/../base/scope_guard.h:48: ~BasicScopeGuard\r\n2023.04.06 04:04:04.553147 [ 502 ] {} <Fatal> BaseDaemon: 15.2. inlined from ./build_docker/./src/Processors/Executors/PipelineExecutor.cpp:345: operator()\r\n2023.04.06 04:04:04.553325 [ 502 ] {} <Fatal> BaseDaemon: 15.3. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__functional/invoke.h:394: decltype(std::declval<DB::PipelineExecutor::spawnThreads()::$_0&>()()) std::__invoke[abi:v15000]<DB::PipelineExecutor::spawnThreads()::$_0&>(DB::PipelineExecutor::spawnThreads()::$_0&)\r\n2023.04.06 04:04:04.553448 [ 502 ] {} <Fatal> BaseDaemon: 15.4. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/tuple:1789: decltype(auto) std::__apply_tuple_impl[abi:v15000]<DB::PipelineExecutor::spawnThreads()::$_0&, std::tuple<>&>(DB::PipelineExecutor::spawnThreads()::$_0&, std::tuple<>&, std::__tuple_indices<>)\r\n2023.04.06 04:04:04.553549 [ 502 ] {} <Fatal> BaseDaemon: 15.5. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/tuple:1798: decltype(auto) std::apply[abi:v15000]<DB::PipelineExecutor::spawnThreads()::$_0&, std::tuple<>&>(DB::PipelineExecutor::spawnThreads()::$_0&, std::tuple<>&)\r\n2023.04.06 04:04:04.553619 [ 502 ] {} <Fatal> BaseDaemon: 15.6. inlined from ./build_docker/./src/Common/ThreadPool.h:227: operator()\r\n2023.04.06 04:04:04.553764 [ 502 ] {} <Fatal> BaseDaemon: 15.7. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__functional/invoke.h:394: decltype(std::declval<DB::PipelineExecutor::spawnThreads()::$_0>()()) std::__invoke[abi:v15000]<ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<DB::PipelineExecutor::spawnThreads()::$_0>(DB::PipelineExecutor::spawnThreads()::$_0&&)::'lambda'()&>(DB::PipelineExecutor::spawnThreads()::$_0&&)\r\n2023.04.06 04:04:04.553931 [ 502 ] {} <Fatal> BaseDaemon: 15.8. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__functional/invoke.h:479: void std::__invoke_void_return_wrapper<void, true>::__call<ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<DB::PipelineExecutor::spawnThreads()::$_0>(DB::PipelineExecutor::spawnThreads()::$_0&&)::'lambda'()&>(ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<DB::PipelineExecutor::spawnThreads()::$_0>(DB::PipelineExecutor::spawnThreads()::$_0&&)::'lambda'()&)\r\n2023.04.06 04:04:04.554093 [ 502 ] {} <Fatal> BaseDaemon: 15.9. inlined from ./build_docker/./contrib/llvm-project/libcxx/include/__functional/function.h:235: std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<DB::PipelineExecutor::spawnThreads()::$_0>(DB::PipelineExecutor::spawnThreads()::$_0&&)::'lambda'(), void ()>::operator()[abi:v15000]()\r\n2023.04.06 04:04:04.554158 [ 502 ] {} <Fatal> BaseDaemon: 15. ./build_docker/./contrib/llvm-project/libcxx/include/__functional/function.h:716: void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<ThreadFromGlobalPoolImpl<true>::ThreadFromGlobalPoolImpl<DB::PipelineExecutor::spawnThreads()::$_0>(DB::PipelineExecutor::spawnThreads()::$_0&&)::'lambda'(), void ()>>(std::__function::__policy_storage const*) @ 0x4c90f8ce in /workspace/clickhouse\r\n2023.04.06 04:04:04.739078 [ 502 ] {} <Fatal> BaseDaemon: 16.1. inlined from ./build_docker/./base/base/../base/wide_integer_impl.h:789: bool wide::integer<128ul, unsigned int>::_impl::operator_eq<wide::integer<128ul, unsigned int>>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)\r\n2023.04.06 04:04:04.739235 [ 502 ] {} <Fatal> BaseDaemon: 16.2. inlined from ./build_docker/./base/base/../base/wide_integer_impl.h:1456: bool wide::operator==<128ul, unsigned int, 128ul, unsigned int>(wide::integer<128ul, unsigned int> const&, wide::integer<128ul, unsigned int> const&)\r\n2023.04.06 04:04:04.739337 [ 502 ] {} <Fatal> BaseDaemon: 16.3. inlined from ./build_docker/./base/base/../base/strong_typedef.h:42: StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag>::operator==(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&) const\r\n2023.04.06 04:04:04.739413 [ 502 ] {} <Fatal> BaseDaemon: 16.4. inlined from ./build_docker/./src/Common/OpenTelemetryTraceContext.h:65: DB::OpenTelemetry::Span::isTraceEnabled() const\r\n2023.04.06 04:04:04.739477 [ 502 ] {} <Fatal> BaseDaemon: 16. ./build_docker/./src/Common/ThreadPool.cpp:398: ThreadPoolImpl<std::thread>::worker(std::__list_iterator<std::thread, void*>) @ 0x29cd4bf2 in /workspace/clickhouse\r\n2023.04.06 04:04:04.975646 [ 502 ] {} <Fatal> BaseDaemon: 17. ./build_docker/./src/Common/ThreadPool.cpp:0: void* std::__thread_proxy[abi:v15000]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, long, std::optional<unsigned long>, bool)::'lambda0'()>>(void*) @ 0x29ce5d23 in /workspace/clickhouse\r\n2023.04.06 04:04:04.975794 [ 502 ] {} <Fatal> BaseDaemon: 18. ? @ 0x7f96aa729609 in ?\r\n2023.04.06 04:04:04.975869 [ 502 ] {} <Fatal> BaseDaemon: 19. clone @ 0x7f96aa64e133 in ?\r\n2023.04.06 04:04:07.935735 [ 502 ] {} <Fatal> BaseDaemon: Integrity check of the executable successfully passed (checksum: A45E3C6F7F2CD1160520AB34E02EABA7)\r\n2023.04.06 04:04:31.887038 [ 162 ] {} <Fatal> Application: Child process was terminated by signal 6.\r\n```\r\n\n",
  "hints_text": "Seems fixed (at least the reproducer from https://github.com/ClickHouse/ClickHouse/issues/49559 doesn't crash any more). Need to add a test to confirm and close",
  "created_at": "2023-12-01T09:58:32Z",
  "modified_files": [
    "src/Planner/Planner.cpp"
  ],
  "modified_test_files": [
    "b/tests/queries/0_stateless/02932_parallel_replicas_fuzzer.sql"
  ]
}