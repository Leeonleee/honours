{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 6438,
  "instance_id": "ClickHouse__ClickHouse-6438",
  "issue_numbers": [
    "6372"
  ],
  "base_commit": "1cd87078c2e363c9c77ced4b3c2ed509e0395385",
  "patch": "diff --git a/dbms/src/Storages/MergeTree/MergeTreeData.cpp b/dbms/src/Storages/MergeTree/MergeTreeData.cpp\nindex 8af8284efdd4..44e610b105c2 100644\n--- a/dbms/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/dbms/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -734,112 +734,135 @@ void MergeTreeData::loadDataParts(bool skip_sanity_checks)\n         part_file_names.push_back(it.name());\n     }\n \n+    auto part_lock = lockParts();\n+    data_parts_indexes.clear();\n+\n+    if (part_file_names.empty())\n+    {\n+        LOG_DEBUG(log, \"There is no data parts\");\n+        return;\n+    }\n+\n+    /// Parallel loading of data parts.\n+    size_t num_threads = std::min(size_t(settings.max_part_loading_threads), part_file_names.size());\n+\n+    std::mutex mutex;\n+\n     DataPartsVector broken_parts_to_remove;\n     DataPartsVector broken_parts_to_detach;\n     size_t suspicious_broken_parts = 0;\n \n-    auto lock = lockParts();\n-    data_parts_indexes.clear();\n+    std::atomic<bool> has_adaptive_parts = false;\n+    std::atomic<bool> has_non_adaptive_parts = false;\n+\n+    ThreadPool pool(num_threads);\n \n-    bool has_adaptive_parts = false;\n-    bool has_non_adaptive_parts = false;\n     for (const String & file_name : part_file_names)\n     {\n-        MergeTreePartInfo part_info;\n-        if (!MergeTreePartInfo::tryParsePartName(file_name, &part_info, format_version))\n-            continue;\n-\n-        MutableDataPartPtr part = std::make_shared<DataPart>(*this, file_name, part_info);\n-        part->relative_path = file_name;\n-        bool broken = false;\n-\n-        try\n-        {\n-            part->loadColumnsChecksumsIndexes(require_part_metadata, true);\n-        }\n-        catch (const Exception & e)\n-        {\n-            /// Don't count the part as broken if there is not enough memory to load it.\n-            /// In fact, there can be many similar situations.\n-            /// But it is OK, because there is a safety guard against deleting too many parts.\n-            if (e.code() == ErrorCodes::MEMORY_LIMIT_EXCEEDED\n-                || e.code() == ErrorCodes::CANNOT_ALLOCATE_MEMORY\n-                || e.code() == ErrorCodes::CANNOT_MUNMAP\n-                || e.code() == ErrorCodes::CANNOT_MREMAP)\n-                throw;\n-\n-            broken = true;\n-            tryLogCurrentException(__PRETTY_FUNCTION__);\n-        }\n-        catch (...)\n+        pool.schedule([&]\n         {\n-            broken = true;\n-            tryLogCurrentException(__PRETTY_FUNCTION__);\n-        }\n+            MergeTreePartInfo part_info;\n+            if (!MergeTreePartInfo::tryParsePartName(file_name, &part_info, format_version))\n+                return;\n \n-        /// Ignore and possibly delete broken parts that can appear as a result of hard server restart.\n-        if (broken)\n-        {\n-            if (part->info.level == 0)\n+            MutableDataPartPtr part = std::make_shared<DataPart>(*this, file_name, part_info);\n+            part->relative_path = file_name;\n+            bool broken = false;\n+\n+            try\n             {\n-                /// It is impossible to restore level 0 parts.\n-                LOG_ERROR(log, \"Considering to remove broken part \" << full_path + file_name << \" because it's impossible to repair.\");\n-                broken_parts_to_remove.push_back(part);\n+                part->loadColumnsChecksumsIndexes(require_part_metadata, true);\n             }\n-            else\n+            catch (const Exception & e)\n             {\n-                /// Count the number of parts covered by the broken part. If it is at least two, assume that\n-                /// the broken part was created as a result of merging them and we won't lose data if we\n-                /// delete it.\n-                size_t contained_parts = 0;\n-\n-                LOG_ERROR(log, \"Part \" << full_path + file_name << \" is broken. Looking for parts to replace it.\");\n+                /// Don't count the part as broken if there is not enough memory to load it.\n+                /// In fact, there can be many similar situations.\n+                /// But it is OK, because there is a safety guard against deleting too many parts.\n+                if (e.code() == ErrorCodes::MEMORY_LIMIT_EXCEEDED\n+                    || e.code() == ErrorCodes::CANNOT_ALLOCATE_MEMORY\n+                    || e.code() == ErrorCodes::CANNOT_MUNMAP\n+                    || e.code() == ErrorCodes::CANNOT_MREMAP)\n+                    throw;\n+\n+                broken = true;\n+                tryLogCurrentException(__PRETTY_FUNCTION__);\n+            }\n+            catch (...)\n+            {\n+                broken = true;\n+                tryLogCurrentException(__PRETTY_FUNCTION__);\n+            }\n \n-                for (const String & contained_name : part_file_names)\n+            /// Ignore and possibly delete broken parts that can appear as a result of hard server restart.\n+            if (broken)\n+            {\n+                if (part->info.level == 0)\n+                {\n+                    /// It is impossible to restore level 0 parts.\n+                    LOG_ERROR(log, \"Considering to remove broken part \" << full_path << file_name << \" because it's impossible to repair.\");\n+                    std::lock_guard loading_lock(mutex);\n+                    broken_parts_to_remove.push_back(part);\n+                }\n+                else\n                 {\n-                    if (contained_name == file_name)\n-                        continue;\n+                    /// Count the number of parts covered by the broken part. If it is at least two, assume that\n+                    /// the broken part was created as a result of merging them and we won't lose data if we\n+                    /// delete it.\n+                    size_t contained_parts = 0;\n \n-                    MergeTreePartInfo contained_part_info;\n-                    if (!MergeTreePartInfo::tryParsePartName(contained_name, &contained_part_info, format_version))\n-                        continue;\n+                    LOG_ERROR(log, \"Part \" << full_path << file_name << \" is broken. Looking for parts to replace it.\");\n \n-                    if (part->info.contains(contained_part_info))\n+                    for (const String & contained_name : part_file_names)\n                     {\n-                        LOG_ERROR(log, \"Found part \" << full_path + contained_name);\n-                        ++contained_parts;\n+                        if (contained_name == file_name)\n+                            continue;\n+\n+                        MergeTreePartInfo contained_part_info;\n+                        if (!MergeTreePartInfo::tryParsePartName(contained_name, &contained_part_info, format_version))\n+                            continue;\n+\n+                        if (part->info.contains(contained_part_info))\n+                        {\n+                            LOG_ERROR(log, \"Found part \" << full_path << contained_name);\n+                            ++contained_parts;\n+                        }\n                     }\n-                }\n \n-                if (contained_parts >= 2)\n-                {\n-                    LOG_ERROR(log, \"Considering to remove broken part \" << full_path + file_name << \" because it covers at least 2 other parts\");\n-                    broken_parts_to_remove.push_back(part);\n-                }\n-                else\n-                {\n-                    LOG_ERROR(log, \"Detaching broken part \" << full_path + file_name\n-                        << \" because it covers less than 2 parts. You need to resolve this manually\");\n-                    broken_parts_to_detach.push_back(part);\n-                    ++suspicious_broken_parts;\n+                    if (contained_parts >= 2)\n+                    {\n+                        LOG_ERROR(log, \"Considering to remove broken part \" << full_path << file_name << \" because it covers at least 2 other parts\");\n+                        std::lock_guard loading_lock(mutex);\n+                        broken_parts_to_remove.push_back(part);\n+                    }\n+                    else\n+                    {\n+                        LOG_ERROR(log, \"Detaching broken part \" << full_path << file_name\n+                            << \" because it covers less than 2 parts. You need to resolve this manually\");\n+                        std::lock_guard loading_lock(mutex);\n+                        broken_parts_to_detach.push_back(part);\n+                        ++suspicious_broken_parts;\n+                    }\n                 }\n-            }\n \n-            continue;\n-        }\n-        if (!part->index_granularity_info.is_adaptive)\n-            has_non_adaptive_parts = true;\n-        else\n-            has_adaptive_parts = true;\n+                return;\n+            }\n+            if (!part->index_granularity_info.is_adaptive)\n+                has_non_adaptive_parts.store(true, std::memory_order_relaxed);\n+            else\n+                has_adaptive_parts.store(true, std::memory_order_relaxed);\n \n-        part->modification_time = Poco::File(full_path + file_name).getLastModified().epochTime();\n-        /// Assume that all parts are Committed, covered parts will be detected and marked as Outdated later\n-        part->state = DataPartState::Committed;\n+            part->modification_time = Poco::File(full_path + file_name).getLastModified().epochTime();\n+            /// Assume that all parts are Committed, covered parts will be detected and marked as Outdated later\n+            part->state = DataPartState::Committed;\n \n-        if (!data_parts_indexes.insert(part).second)\n-            throw Exception(\"Part \" + part->name + \" already exists\", ErrorCodes::DUPLICATE_DATA_PART);\n+            std::lock_guard loading_lock(mutex);\n+            if (!data_parts_indexes.insert(part).second)\n+                throw Exception(\"Part \" + part->name + \" already exists\", ErrorCodes::DUPLICATE_DATA_PART);\n+        });\n     }\n \n+    pool.wait();\n+\n     if (has_non_adaptive_parts && has_adaptive_parts && !settings.enable_mixed_granularity_parts)\n         throw Exception(\"Table contains parts with adaptive and non adaptive marks, but `setting enable_mixed_granularity_parts` is disabled\", ErrorCodes::LOGICAL_ERROR);\n \n@@ -1063,15 +1086,40 @@ void MergeTreeData::removePartsFinally(const MergeTreeData::DataPartsVector & pa\n \n void MergeTreeData::clearOldPartsFromFilesystem()\n {\n-    auto parts_to_remove = grabOldParts();\n+    DataPartsVector parts_to_remove = grabOldParts();\n+    clearPartsFromFilesystem(parts_to_remove);\n+    removePartsFinally(parts_to_remove);\n+}\n \n-    for (const DataPartPtr & part : parts_to_remove)\n+void MergeTreeData::clearPartsFromFilesystem(const DataPartsVector & parts_to_remove)\n+{\n+    if (parts_to_remove.size() > 1 && settings.max_part_removal_threads > 1 && parts_to_remove.size() > settings.concurrent_part_removal_threshold)\n     {\n-        LOG_DEBUG(log, \"Removing part from filesystem \" << part->name);\n-        part->remove();\n-    }\n+        /// Parallel parts removal.\n \n-    removePartsFinally(parts_to_remove);\n+        size_t num_threads = std::min(size_t(settings.max_part_removal_threads), parts_to_remove.size());\n+        ThreadPool pool(num_threads);\n+\n+        /// NOTE: Under heavy system load you may get \"Cannot schedule a task\" from ThreadPool.\n+        for (const DataPartPtr & part : parts_to_remove)\n+        {\n+            pool.schedule([&]\n+            {\n+                LOG_DEBUG(log, \"Removing part from filesystem \" << part->name);\n+                part->remove();\n+            });\n+        }\n+\n+        pool.wait();\n+    }\n+    else\n+    {\n+        for (const DataPartPtr & part : parts_to_remove)\n+        {\n+            LOG_DEBUG(log, \"Removing part from filesystem \" << part->name);\n+            part->remove();\n+        }\n+    }\n }\n \n void MergeTreeData::setPath(const String & new_full_path)\n@@ -1093,6 +1141,8 @@ void MergeTreeData::dropAllData()\n \n     LOG_TRACE(log, \"dropAllData: removing data from memory.\");\n \n+    DataPartsVector all_parts(data_parts_by_info.begin(), data_parts_by_info.end());\n+\n     data_parts_indexes.clear();\n     column_sizes.clear();\n \n@@ -1101,8 +1151,7 @@ void MergeTreeData::dropAllData()\n     LOG_TRACE(log, \"dropAllData: removing data from filesystem.\");\n \n     /// Removing of each data part before recursive removal of directory is to speed-up removal, because there will be less number of syscalls.\n-    for (DataPartPtr part : data_parts_by_info) /// a copy intended\n-        part->remove();\n+    clearPartsFromFilesystem(all_parts);\n \n     Poco::File(full_path).remove(true);\n \ndiff --git a/dbms/src/Storages/MergeTree/MergeTreeData.h b/dbms/src/Storages/MergeTree/MergeTreeData.h\nindex c6b85e6d98fe..b43851eb7d9a 100644\n--- a/dbms/src/Storages/MergeTree/MergeTreeData.h\n+++ b/dbms/src/Storages/MergeTree/MergeTreeData.h\n@@ -477,6 +477,7 @@ class MergeTreeData : public IStorage\n \n     /// Delete irrelevant parts from memory and disk.\n     void clearOldPartsFromFilesystem();\n+    void clearPartsFromFilesystem(const DataPartsVector & parts);\n \n     /// Delete all directories which names begin with \"tmp\"\n     /// Set non-negative parameter value to override MergeTreeSettings temporary_directories_lifetime\ndiff --git a/dbms/src/Storages/MergeTree/MergeTreeSettings.h b/dbms/src/Storages/MergeTree/MergeTreeSettings.h\nindex 9bd58e77f9c4..e670000ecc5b 100644\n--- a/dbms/src/Storages/MergeTree/MergeTreeSettings.h\n+++ b/dbms/src/Storages/MergeTree/MergeTreeSettings.h\n@@ -80,7 +80,10 @@ struct MergeTreeSettings : public SettingsCollection<MergeTreeSettings>\n     M(SettingUInt64, index_granularity_bytes, 10 * 1024 * 1024, \"Approximate amount of bytes in single granule (0 - disabled).\") \\\n     M(SettingInt64, merge_with_ttl_timeout, 3600 * 24, \"Minimal time in seconds, when merge with TTL can be repeated.\") \\\n     M(SettingBool, write_final_mark, 1, \"Write final mark after end of column (0 - disabled, do nothing if index_granularity_bytes=0)\") \\\n-    M(SettingBool, enable_mixed_granularity_parts, 0, \"Enable parts with adaptive and non adaptive granularity\")\n+    M(SettingBool, enable_mixed_granularity_parts, 0, \"Enable parts with adaptive and non adaptive granularity\") \\\n+    M(SettingMaxThreads, max_part_loading_threads, 0, \"The number of theads to load data parts at startup.\") \\\n+    M(SettingMaxThreads, max_part_removal_threads, 0, \"The number of theads for concurrent removal of inactive data parts. One is usually enough, but in 'Google Compute Environment SSD Persistent Disks' file removal (unlink) operation is extraordinarily slow and you probably have to increase this number (recommended is up to 16).\") \\\n+    M(SettingUInt64, concurrent_part_removal_threshold, 100, \"Activate concurrent part removal (see 'max_part_removal_threads') only if the number of inactive data parts is at least this.\") \\\n \n     DECLARE_SETTINGS_COLLECTION(LIST_OF_MERGE_TREE_SETTINGS)\n \ndiff --git a/dbms/src/Storages/StorageMergeTree.cpp b/dbms/src/Storages/StorageMergeTree.cpp\nindex 83bfadc482b3..bb5e4c7f869f 100644\n--- a/dbms/src/Storages/StorageMergeTree.cpp\n+++ b/dbms/src/Storages/StorageMergeTree.cpp\n@@ -761,7 +761,7 @@ BackgroundProcessingPoolTaskResult StorageMergeTree::backgroundTask()\n         else\n             return BackgroundProcessingPoolTaskResult::ERROR;\n     }\n-    catch (Exception & e)\n+    catch (const Exception & e)\n     {\n         if (e.code() == ErrorCodes::ABORTED)\n         {\n",
  "test_patch": "diff --git a/dbms/tests/queries/0_stateless/00988_parallel_parts_removal.reference b/dbms/tests/queries/0_stateless/00988_parallel_parts_removal.reference\nnew file mode 100644\nindex 000000000000..aa6663eb9c78\n--- /dev/null\n+++ b/dbms/tests/queries/0_stateless/00988_parallel_parts_removal.reference\n@@ -0,0 +1,2 @@\n+1000\t499500\n+1000\t499500\ndiff --git a/dbms/tests/queries/0_stateless/00988_parallel_parts_removal.sql b/dbms/tests/queries/0_stateless/00988_parallel_parts_removal.sql\nnew file mode 100644\nindex 000000000000..0dccd3df0485\n--- /dev/null\n+++ b/dbms/tests/queries/0_stateless/00988_parallel_parts_removal.sql\n@@ -0,0 +1,18 @@\n+DROP TABLE IF EXISTS mt;\n+\n+CREATE TABLE mt (x UInt64) ENGINE = MergeTree ORDER BY x SETTINGS max_part_removal_threads = 16, cleanup_delay_period = 1, cleanup_delay_period_random_add = 0, old_parts_lifetime = 1, parts_to_delay_insert = 100000, parts_to_throw_insert = 100000;\n+\n+SYSTEM STOP MERGES;\n+\n+SET max_block_size = 1, min_insert_block_size_rows = 0, min_insert_block_size_bytes = 0;\n+INSERT INTO mt SELECT * FROM numbers(1000);\n+SET max_block_size = 65536;\n+\n+SELECT count(), sum(x) FROM mt;\n+\n+SYSTEM START MERGES;\n+OPTIMIZE TABLE mt FINAL;\n+\n+SELECT count(), sum(x) FROM mt;\n+\n+DROP TABLE mt;\ndiff --git a/dbms/tests/queries/0_stateless/00989_parallel_parts_loading.reference b/dbms/tests/queries/0_stateless/00989_parallel_parts_loading.reference\nnew file mode 100644\nindex 000000000000..aa6663eb9c78\n--- /dev/null\n+++ b/dbms/tests/queries/0_stateless/00989_parallel_parts_loading.reference\n@@ -0,0 +1,2 @@\n+1000\t499500\n+1000\t499500\ndiff --git a/dbms/tests/queries/0_stateless/00989_parallel_parts_loading.sql b/dbms/tests/queries/0_stateless/00989_parallel_parts_loading.sql\nnew file mode 100644\nindex 000000000000..5e0011483b3c\n--- /dev/null\n+++ b/dbms/tests/queries/0_stateless/00989_parallel_parts_loading.sql\n@@ -0,0 +1,19 @@\n+DROP TABLE IF EXISTS mt;\n+\n+CREATE TABLE mt (x UInt64) ENGINE = MergeTree ORDER BY x SETTINGS max_part_loading_threads = 16, parts_to_delay_insert = 100000, parts_to_throw_insert = 100000;\n+\n+SYSTEM STOP MERGES;\n+\n+SET max_block_size = 1, min_insert_block_size_rows = 0, min_insert_block_size_bytes = 0;\n+INSERT INTO mt SELECT * FROM numbers(1000);\n+SET max_block_size = 65536;\n+\n+SELECT count(), sum(x) FROM mt;\n+\n+DETACH TABLE mt;\n+ATTACH TABLE mt;\n+\n+SELECT count(), sum(x) FROM mt;\n+\n+SYSTEM START MERGES;\n+DROP TABLE mt;\n",
  "problem_statement": "Option to increase concurrency on MergeTreeDataPart removal during Cleanup.\nHello ClickHouse maintainers, I'm James and I am the person responsible for administrating our cluster at Sentry. We love what you've built at Yandex, so much that we [replaced our Postgres infrastructure with it](https://blog.sentry.io/2019/05/16/introducing-snuba-sentrys-new-search-infrastructure) and now use ClickHouse as [our primary search and storage system](https://blog.sentry.io/2019/07/25/field-guide-mitigating-risk-transitioning-databases). I'd like to pose a feature that will allow us to write to our production cluster at greater frequency while also enabling us to increase our column count.\r\n\r\nIn our current production cluster, write frequency is bound by the single-threaded cleanup of obsolete parts. This is due to the amount of files (from both column count and `Nullable` types) that need to be `unlink`ed for each part. If we increase our write frequency, the single thread cannot keep up with the amount of parts to remove, and the replica(s) will eventually refuse writes (as they should).\r\n\r\nIf this process was multi-threaded, we would be able to increase our write frequency and our column count. The disks we currently use are Google Compute Environment SSD Persistent Disks, which allow concurrent syscalls like `unlink`.\r\n\r\nEither an option to enable/disable concurrent cleanup, or a count of threads to submit to the `BackgroundSchedulePool` would be sufficient for us. Anything that could give us the concurrency that our disks support would be gratefully appreciated.\n",
  "hints_text": "Great to hear that Sentry is using ClickHouse now :) (sidenote: i've used Sentry for some petproject some time ago and it's great). \r\n\r\nCan I ask how many inserts per second per server do you have? What is insert batch size? Did you consider increasing insert batch size instead of increasing number of inserts? (usually it's much better option for ClickHouse) How did you find that unlinking is a bottleneck? \r\n\r\nI'm asking because usually on high insert rate background merges process is bottleneck, not unlinking. By default CH removes unneeded parts not immideately, but after some time interval (configured, and set to 7.5 min by default AFAIR). \nSure! Here is our breakdown:\r\nMain cluster is 3 shards with 3 replicas.\r\nInserts are roughly once a second in order to reduce latency in actions triggered off of individual rows that mean conditions ([alerts with time intervals](https://docs.sentry.io/workflow/notifications/alerts/)).\r\nWe have considered increasing batch size, but that ultimately increases our latency in between receiving an event and being able to trigger an alert off of it.\r\nThe `unlink` syscall is ultimately our bottleneck because of the direct io it performs. Our background merges are not a bottleneck because we can properly utilize page cache.\r\n\r\nIncreasing the batch size doesn't really aid our issue.\r\nFor example, if it takes roughly 500ms to `unlink` all of our column files (~320 .bin and .mark combined) and we write at 250ms intervals, CH could not sustain removing unneeded parts if they are all deleted in serial.\r\n\r\nWe found that the deleting of obsoleted parts was the bottleneck by watching CH only schedule a single thread per-table to do the removal. We continued to write at a faster rate than it could delete, and queried for non-active parts using the system table. While the maximum active parts per partition never rose over 20, the count of inactive parts continued to sawtooth upwards.\r\n\r\nI'll try and find a graph from our system to better illustrate the issue.\nHow does it relate to the mounting options of the filesystem? I mean noatime, nodiratime, nobarrier, writeback...\nParallel files removal is not a hard thing to do...\r\n\r\nPS. Do you use enough recent version? (recently we have made a small optimization in this place):\r\nhttps://github.com/yandex/ClickHouse/blob/master/CHANGELOG.md#performance-improvement-1\n@alexey-milovidov: All four of the specified mounting options are already in place. If I had to make a guess, the latency comes from GCP's Persistent Disk itself. The block storage system is able to record high marks on write iops by making concurrent calls. A single-threaded application can commit upwards to 2k write ops, but can achieve 25/30k by utilizing all vCPUs.\r\n\r\nOnce the changelog was shipped for 19.10/11, I did perform an upgrade to see if https://github.com/yandex/ClickHouse/pull/5648 would reduce the bottleneck. It removed the entirety of the `stat` calls from directory cleanup, but we only saw a 5% improvement because of how fast those calls return on normal block storages.\r\n\r\nHere is a sample `strace` count when following the cleanup thread.\r\nNote: I could not time this properly for the entire thread execution since it is part of a pool. It's just a sample portion.\r\n```\r\n% time     seconds  usecs/call     calls    errors syscall\r\n------ ----------- ----------- --------- --------- ----------------\r\n 99.76    1.773754         111     15975           unlink\r\n  0.20    0.003536          71        50           rmdir\r\n  0.02    0.000383           8        50        50 stat\r\n  0.02    0.000299           6        50           rename\r\n------ ----------- ----------- --------- --------- ----------------\r\n100.00    1.777972                 16125        50 total\r\n```\n@filimonov: Here is an illustration of my description.\r\n\r\nIn this graph, we're recording `SELECT count() AS parts FROM system.parts WHERE not active;`.\r\nI've doubled our frequency of writing at 9:18. The number of inactive parts climbs because we wait for that 7.5min inactivity marker you described, and then we start to clean them up. As you can see, the graph begins to saw-tooth since the single cleanup thread cannot sustain the volume of obsoleted parts.\r\nAn important distinction we have here is that we have set `cleanup_delay_period` to 1 so that we do not wait 30s to wake the thread up. We have done this because it leads to more consistent IO and less time spent not cleaning up obsoleted parts.\r\n<img width=\"1340\" alt=\"Screenshot 2019-08-11 09 41 03\" src=\"https://user-images.githubusercontent.com/1189446/62836874-c911bd80-bc1c-11e9-9e7e-6cd52995a819.png\">\r\n\r\nAdditionally, here is a graph during the same time of the `max_part_count_for_partition` metric to illustrate that merging low-level parts is not the bottleneck.\r\n<img width=\"1347\" alt=\"Screenshot 2019-08-11 09 41 52\" src=\"https://user-images.githubusercontent.com/1189446/62836887-1f7efc00-bc1d-11e9-99ad-8eaeeaf0eb7d.png\">\r\n\r\nEdit:\r\nI've included some extra graphs for potential follow-ups.\r\nIO Util as a percentage of 100:\r\n<img width=\"491\" alt=\"Screenshot 2019-08-11 09 49 01\" src=\"https://user-images.githubusercontent.com/1189446/62836919-987e5380-bc1d-11e9-8a1a-b8bd68e2584c.png\">\r\nIO Wait as a percentage of 100:\r\n<img width=\"491\" alt=\"Screenshot 2019-08-11 09 49 10\" src=\"https://user-images.githubusercontent.com/1189446/62836921-a207bb80-bc1d-11e9-92e8-4a88becc1c07.png\">\r\nWrite IOPS:\r\n<img width=\"491\" alt=\"Screenshot 2019-08-11 09 49 17\" src=\"https://user-images.githubusercontent.com/1189446/62836927-b6e44f00-bc1d-11e9-848a-e25559035dc6.png\">\r\nThe `background_schedule_pool_task` metric:\r\n<img width=\"494\" alt=\"Screenshot 2019-08-11 09 49 33\" src=\"https://user-images.githubusercontent.com/1189446/62836941-dd09ef00-bc1d-11e9-977e-2f1e24e28149.png\">\r\nThe `replicas_sum_merges_in_queue` metric:\r\n<img width=\"506\" alt=\"Screenshot 2019-08-11 09 50 16\" src=\"https://user-images.githubusercontent.com/1189446/62836943-ef842880-bc1d-11e9-896e-4c408aa78e1b.png\">\r\n\nBTW, one our customer has exactly the opposite experience:\r\n- when we removing data files, there is too high IO load, and they propose something to gradually truncate files before removal.",
  "created_at": "2019-08-11T19:15:28Z"
}