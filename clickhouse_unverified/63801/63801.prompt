You will be provided with a partial code base and an issue statement explaining a problem to resolve.

<issue>
HTTP inserts with RowBinary format are slow if many integers contain \x24 ($) byte in binary representation
**Describe the situation**

Uploading data to ClickHouse in `RowBinary` format using HTTP interface may be extremely slow and CPU-intensive if data contains `\x24` (`$`) bytes in binary representation and `max_query_size` setting is increased (e.g. few MiB).

In our case (32MiB `max_query_size`, uploading >32MiB of time-series data with timestamps containing `\x24` (`$`) byte on some position) it caused HTTPHandler threads to be stuck in parsing for more than 5 minutes (spending a lot of CPU time in `DB::Lexer::nextToken`),  rendering ClickHouse unresponsive once it happend to many threads.

It looks like it happens because lexer is trying to find heredoc tags in the binary representation of the input data, e.g. integers. AFAIU when data is inserted using HTTP interface ClickHouse runs Lexer on first `max_query_size` bytes of concatenated query and data from HTTP request body ([concatenation](https://github.com/ClickHouse/ClickHouse/blob/v24.3.2.23-lts/src/Server/HTTPHandler.cpp#L858), [running lexer on first `max_query_size` bytes](https://github.com/ClickHouse/ClickHouse/blob/v23.12.1.1368-stable/src/Parsers/TokenIterator.cpp#L7)). Lexer treats `$` bytes as markers for heredoc tags. If one is encountered it traverses the rest of input to find the matching heredoc ([here](https://github.com/ClickHouse/ClickHouse/blob/684b7388539d1add534c15b6d5b3f3541c721244/src/Parsers/Lexer.cpp#L452)), if there is none it consumes just few bytes and continues, so if there are many `$` bytes in input and heredoc parsing doesn't find a match the whole input will be traversed `O(max_query_size)` times.

It seems reasonable when input data is in text format, but it's rather unexpected when dollars are in **binary representation of integers in input data**. Especially considering that according to `max_query_size` documentation "Data in the VALUES clause of INSERT queries is processed by a separate stream parser (that consumes O(1) RAM) and not affected by this restriction."

In real world this can be triggered when inserting time-series data where many rows contains the same timestamp (every 256 seconds we have `$` on the least significant byte, every ~18 hours on 2nd least significant byte, etc). To be more precise we seen it being triggered by [carbon-clickhouse](https://github.com/go-graphite/carbon-clickhouse).

**How to reproduce**

To table:

```sql
CREATE TABLE test (timestamp UInt64, label String) ENGINE = MergeTree ORDER BY (timestamp, label);
```

insert many rows with the same 'timestamp' and different 64-bytes in 'labels':

```python3
import struct
import time

import requests


def measure_time_of_inserting_values(timestamp, rows_count, max_query_size):
    # For each row there is same 'timestamp' and different 'label'.
    # Make 'label' have 64 bytes to make heredoc scanning CPU expensive.
    data = b"".join(struct.pack('<Q', timestamp) + b"\x40" + f"{i:064}".encode("ascii") for i in range(rows_count))

    start_time = time.time()

    r = requests.post(
        'http://127.0.0.1:8123/',
        params={'query': 'INSERT INTO test (timestamp, label) FORMAT RowBinary', 'max_query_size': max_query_size},
        data=data,
    )
    r.raise_for_status()

    return time.time() - start_time


rows_count = 1024 * 1024  # insert ~70MiB of data

for max_query_size in [
        4 * 1024,
        256 * 1024,
        1 * 1024 * 1024,
        2 * 1024 * 1024,
        4 * 1024 * 1024,
        8 * 1024 * 1024,
        16 * 1024 * 1024,
        32 * 1024 * 1024
]:
    # without '$' byte
    time_without_dollar = measure_time_of_inserting_values(0, rows_count, max_query_size)
    print(f"max_query_size: {max_query_size}, without dollar: {time_without_dollar:.3f}s")

    # with '$' byte
    time_with_dollar = measure_time_of_inserting_values(36, rows_count, max_query_size)
    print(f"max_query_size: {max_query_size}, with dollar:    {time_with_dollar:.3f}s")

    print()
```

output on my machine:

```
$ python3 test.py
max_query_size: 4096, without dollar: 0.127s
max_query_size: 4096, with dollar:    0.121s

max_query_size: 262144, without dollar: 0.075s
max_query_size: 262144, with dollar:    0.122s

max_query_size: 1048576, without dollar: 0.077s
max_query_size: 1048576, with dollar:    0.690s

max_query_size: 2097152, without dollar: 0.090s
max_query_size: 2097152, with dollar:    2.387s

max_query_size: 4194304, without dollar: 0.122s
max_query_size: 4194304, with dollar:    9.668s

max_query_size: 8388608, without dollar: 0.142s
max_query_size: 8388608, with dollar:    39.079s

max_query_size: 16777216, without dollar: 0.127s
max_query_size: 16777216, with dollar:    167.822s

max_query_size: 33554432, without dollar: 0.268s
max_query_size: 33554432, with dollar:    735.191s
```

I tested this script on 24.3.2.23.

**Expected performance**

Performance of `INSERT` queries should not depend on whether binary representation of integers contains `\x24` bytes.
</issue>

I need you to solve the provided issue by generating a code fix that can be applied directly to the repository

Respond below:
