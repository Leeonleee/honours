{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 63801,
  "instance_id": "ClickHouse__ClickHouse-63801",
  "issue_numbers": [
    "62918"
  ],
  "base_commit": "96e858480ef92091b578079a7058f97eaa3cce87",
  "patch": "diff --git a/src/Parsers/TokenIterator.cpp b/src/Parsers/TokenIterator.cpp\nindex fa792e7c8b5b..08877e0b2fe7 100644\n--- a/src/Parsers/TokenIterator.cpp\n+++ b/src/Parsers/TokenIterator.cpp\n@@ -4,20 +4,6 @@\n namespace DB\n {\n \n-Tokens::Tokens(const char * begin, const char * end, size_t max_query_size, bool skip_insignificant)\n-{\n-    Lexer lexer(begin, end, max_query_size);\n-\n-    bool stop = false;\n-    do\n-    {\n-        Token token = lexer.nextToken();\n-        stop = token.isEnd() || token.type == TokenType::ErrorMaxQuerySizeExceeded;\n-        if (token.isSignificant() || (!skip_insignificant && !data.empty() && data.back().isSignificant()))\n-            data.emplace_back(std::move(token));\n-    } while (!stop);\n-}\n-\n UnmatchedParentheses checkUnmatchedParentheses(TokenIterator begin)\n {\n     /// We have just two kind of parentheses: () and [].\ndiff --git a/src/Parsers/TokenIterator.h b/src/Parsers/TokenIterator.h\nindex 192f2f55e6a8..207ddadb8bfd 100644\n--- a/src/Parsers/TokenIterator.h\n+++ b/src/Parsers/TokenIterator.h\n@@ -15,25 +15,44 @@ namespace DB\n   */\n \n /** Used as an input for parsers.\n-  * All whitespace and comment tokens are transparently skipped.\n+  * All whitespace and comment tokens are transparently skipped if `skip_insignificant`.\n   */\n class Tokens\n {\n private:\n     std::vector<Token> data;\n-    std::size_t last_accessed_index = 0;\n+    Lexer lexer;\n+    bool skip_insignificant;\n \n public:\n-    Tokens(const char * begin, const char * end, size_t max_query_size = 0, bool skip_insignificant = true);\n+    Tokens(const char * begin, const char * end, size_t max_query_size = 0, bool skip_insignificant_ = true)\n+        : lexer(begin, end, max_query_size), skip_insignificant(skip_insignificant_)\n+    {\n+    }\n \n-    ALWAYS_INLINE inline const Token & operator[](size_t index)\n+    const Token & operator[] (size_t index)\n     {\n-        assert(index < data.size());\n-        last_accessed_index = std::max(last_accessed_index, index);\n-        return data[index];\n+        while (true)\n+        {\n+            if (index < data.size())\n+                return data[index];\n+\n+            if (!data.empty() && data.back().isEnd())\n+                return data.back();\n+\n+            Token token = lexer.nextToken();\n+\n+            if (!skip_insignificant || token.isSignificant())\n+                data.emplace_back(token);\n+        }\n     }\n \n-    ALWAYS_INLINE inline const Token & max() { return data[last_accessed_index]; }\n+    const Token & max()\n+    {\n+        if (data.empty())\n+            return (*this)[0];\n+        return data.back();\n+    }\n };\n \n \n",
  "test_patch": "diff --git a/tests/queries/0_stateless/03154_lazy_token_iterator.reference b/tests/queries/0_stateless/03154_lazy_token_iterator.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/03154_lazy_token_iterator.sh b/tests/queries/0_stateless/03154_lazy_token_iterator.sh\nnew file mode 100755\nindex 000000000000..4794dafda4bc\n--- /dev/null\n+++ b/tests/queries/0_stateless/03154_lazy_token_iterator.sh\n@@ -0,0 +1,11 @@\n+#!/usr/bin/env bash\n+# Tags: no-fasttest, no-parallel, no-ordinary-database, long\n+\n+CUR_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CUR_DIR\"/../shell_config.sh\n+\n+# In previous versions this command took longer than ten minutes. Now it takes less than a second in release mode:\n+\n+python3 -c 'import sys; import struct; sys.stdout.buffer.write(b\"\".join(struct.pack(\"<Q\", 36) + b\"\\x40\" + f\"{i:064}\".encode(\"ascii\") for i in range(1024 * 1024)))' |\n+${CLICKHOUSE_CURL} \"${CLICKHOUSE_URL}&max_query_size=100000000&query=INSERT+INTO+FUNCTION+null('timestamp+UInt64,+label+String')+FORMAT+RowBinary\" --data-binary @-\n",
  "problem_statement": "HTTP inserts with RowBinary format are slow if many integers contain \\x24 ($) byte in binary representation\n**Describe the situation**\r\n\r\nUploading data to ClickHouse in `RowBinary` format using HTTP interface may be extremely slow and CPU-intensive if data contains `\\x24` (`$`) bytes in binary representation and `max_query_size` setting is increased (e.g. few MiB).\r\n\r\nIn our case (32MiB `max_query_size`, uploading >32MiB of time-series data with timestamps containing `\\x24` (`$`) byte on some position) it caused HTTPHandler threads to be stuck in parsing for more than 5 minutes (spending a lot of CPU time in `DB::Lexer::nextToken`),  rendering ClickHouse unresponsive once it happend to many threads.\r\n\r\nIt looks like it happens because lexer is trying to find heredoc tags in the binary representation of the input data, e.g. integers. AFAIU when data is inserted using HTTP interface ClickHouse runs Lexer on first `max_query_size` bytes of concatenated query and data from HTTP request body ([concatenation](https://github.com/ClickHouse/ClickHouse/blob/v24.3.2.23-lts/src/Server/HTTPHandler.cpp#L858), [running lexer on first `max_query_size` bytes](https://github.com/ClickHouse/ClickHouse/blob/v23.12.1.1368-stable/src/Parsers/TokenIterator.cpp#L7)). Lexer treats `$` bytes as markers for heredoc tags. If one is encountered it traverses the rest of input to find the matching heredoc ([here](https://github.com/ClickHouse/ClickHouse/blob/684b7388539d1add534c15b6d5b3f3541c721244/src/Parsers/Lexer.cpp#L452)), if there is none it consumes just few bytes and continues, so if there are many `$` bytes in input and heredoc parsing doesn't find a match the whole input will be traversed `O(max_query_size)` times.\r\n\r\nIt seems reasonable when input data is in text format, but it's rather unexpected when dollars are in **binary representation of integers in input data**. Especially considering that according to `max_query_size` documentation \"Data in the VALUES clause of INSERT queries is processed by a separate stream parser (that consumes O(1) RAM) and not affected by this restriction.\"\r\n\r\nIn real world this can be triggered when inserting time-series data where many rows contains the same timestamp (every 256 seconds we have `$` on the least significant byte, every ~18 hours on 2nd least significant byte, etc). To be more precise we seen it being triggered by [carbon-clickhouse](https://github.com/go-graphite/carbon-clickhouse).\r\n\r\n**How to reproduce**\r\n\r\nTo table:\r\n\r\n```sql\r\nCREATE TABLE test (timestamp UInt64, label String) ENGINE = MergeTree ORDER BY (timestamp, label);\r\n```\r\n\r\ninsert many rows with the same 'timestamp' and different 64-bytes in 'labels':\r\n\r\n```python3\r\nimport struct\r\nimport time\r\n\r\nimport requests\r\n\r\n\r\ndef measure_time_of_inserting_values(timestamp, rows_count, max_query_size):\r\n    # For each row there is same 'timestamp' and different 'label'.\r\n    # Make 'label' have 64 bytes to make heredoc scanning CPU expensive.\r\n    data = b\"\".join(struct.pack('<Q', timestamp) + b\"\\x40\" + f\"{i:064}\".encode(\"ascii\") for i in range(rows_count))\r\n\r\n    start_time = time.time()\r\n\r\n    r = requests.post(\r\n        'http://127.0.0.1:8123/',\r\n        params={'query': 'INSERT INTO test (timestamp, label) FORMAT RowBinary', 'max_query_size': max_query_size},\r\n        data=data,\r\n    )\r\n    r.raise_for_status()\r\n\r\n    return time.time() - start_time\r\n\r\n\r\nrows_count = 1024 * 1024  # insert ~70MiB of data\r\n\r\nfor max_query_size in [\r\n        4 * 1024,\r\n        256 * 1024,\r\n        1 * 1024 * 1024,\r\n        2 * 1024 * 1024,\r\n        4 * 1024 * 1024,\r\n        8 * 1024 * 1024,\r\n        16 * 1024 * 1024,\r\n        32 * 1024 * 1024\r\n]:\r\n    # without '$' byte\r\n    time_without_dollar = measure_time_of_inserting_values(0, rows_count, max_query_size)\r\n    print(f\"max_query_size: {max_query_size}, without dollar: {time_without_dollar:.3f}s\")\r\n\r\n    # with '$' byte\r\n    time_with_dollar = measure_time_of_inserting_values(36, rows_count, max_query_size)\r\n    print(f\"max_query_size: {max_query_size}, with dollar:    {time_with_dollar:.3f}s\")\r\n\r\n    print()\r\n```\r\n\r\noutput on my machine:\r\n\r\n```\r\n$ python3 test.py\r\nmax_query_size: 4096, without dollar: 0.127s\r\nmax_query_size: 4096, with dollar:    0.121s\r\n\r\nmax_query_size: 262144, without dollar: 0.075s\r\nmax_query_size: 262144, with dollar:    0.122s\r\n\r\nmax_query_size: 1048576, without dollar: 0.077s\r\nmax_query_size: 1048576, with dollar:    0.690s\r\n\r\nmax_query_size: 2097152, without dollar: 0.090s\r\nmax_query_size: 2097152, with dollar:    2.387s\r\n\r\nmax_query_size: 4194304, without dollar: 0.122s\r\nmax_query_size: 4194304, with dollar:    9.668s\r\n\r\nmax_query_size: 8388608, without dollar: 0.142s\r\nmax_query_size: 8388608, with dollar:    39.079s\r\n\r\nmax_query_size: 16777216, without dollar: 0.127s\r\nmax_query_size: 16777216, with dollar:    167.822s\r\n\r\nmax_query_size: 33554432, without dollar: 0.268s\r\nmax_query_size: 33554432, with dollar:    735.191s\r\n```\r\n\r\nI tested this script on 24.3.2.23.\r\n\r\n**Expected performance**\r\n\r\nPerformance of `INSERT` queries should not depend on whether binary representation of integers contains `\\x24` bytes.\r\n\n",
  "hints_text": "The issue was introduced in https://github.com/ClickHouse/ClickHouse/pull/42284, TokenIterator/Lexer is not lazy anymore:\r\n\r\n> Rework how Tokens work. Before it used to be on-demand, so whenever a token was needed it would return it if available or continue parsing the query until enough were available to return it. This is really bad in terms of cache locality and added lots of branches when parsing queries that weren't really necessary. Now the query is tokenized once, at the beginning, and all accesses can be done without needing to look for further tokens. Performance impact is noticeable (~1.1x)\r\n\r\nPossible ways to fix it:\r\n - Make it lazy again (but tokenize in batches, e.g. parse the next 100-1000 tokens)\r\n - Add a better estimation for the end of query (ad-hoc solution for INSERT queries only is okay)\r\n - Optimize Lexer (but I don't really like this option because tokenizing [binary] data for insert looks like trash)\r\n\r\n@Algunenano, WDYT?\nI need to think about this a bit more, but out of the 3 the first makes the most sense to me, although it's a PITA because it means introducing more branches and complexity on all other query types too.\n> although it's a PITA because it means introducing more branches and complexity on all other query types too\r\n\r\nMaybe start tokenizing lazily after `INSERT` and `INTO` keywords? :) \r\n(however, it will enable lazy lexer for INSERT SELECT queries)\nThe problem with laziness is that once it's introduced anywhere, `operator[]` needs to handle it, so it stops being a fast inlineable function. But I'll do some tests and see the impact.\nas far as I understood the problem is that substring search has linear complexity and we could do O(input_size) searches in theory. how parsing laziness could help with this?\nYou don\u2019t have to search for the end of the (useless) token if you don\u2019t parse the binary part at all and stop after finding the format:\r\n\r\nExample: `FORMAT $format (binary data is here)`\nlooks pretty reasonable. we should be able to do the same in the current implementation.",
  "created_at": "2024-05-15T01:33:54Z",
  "modified_files": [
    "src/Parsers/TokenIterator.cpp",
    "src/Parsers/TokenIterator.h"
  ],
  "modified_test_files": [
    "b/tests/queries/0_stateless/03154_lazy_token_iterator.sh"
  ]
}