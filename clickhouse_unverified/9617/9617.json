{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 9617,
  "instance_id": "ClickHouse__ClickHouse-9617",
  "issue_numbers": [
    "3268"
  ],
  "base_commit": "b4cf1410eddd28b849f096b5e9c5f512515e516b",
  "patch": "diff --git a/dbms/src/Interpreters/DDLWorker.cpp b/dbms/src/Interpreters/DDLWorker.cpp\nindex e37d706125da..f78b5bf9d434 100644\n--- a/dbms/src/Interpreters/DDLWorker.cpp\n+++ b/dbms/src/Interpreters/DDLWorker.cpp\n@@ -12,6 +12,7 @@\n #include <IO/Operators.h>\n #include <IO/ReadBufferFromString.h>\n #include <Storages/IStorage.h>\n+#include <Storages/StorageDistributed.h>\n #include <DataStreams/IBlockInputStream.h>\n #include <Interpreters/executeQuery.h>\n #include <Interpreters/Cluster.h>\n@@ -684,6 +685,12 @@ void DDLWorker::checkShardConfig(const String & table, const DDLTask & task, Sto\n     const auto & shard_info = task.cluster->getShardsInfo().at(task.host_shard_num);\n     bool config_is_replicated_shard = shard_info.hasInternalReplication();\n \n+    if (dynamic_cast<StorageDistributed *>(storage.get()))\n+    {\n+        LOG_TRACE(log, \"Table '\" + table + \"' is distributed, skip checking config.\");\n+        return;\n+    }\n+\n     if (storage->supportsReplication() && !config_is_replicated_shard)\n     {\n         throw Exception(\"Table '\" + table + \"' is replicated, but shard #\" + toString(task.host_shard_num + 1) +\n",
  "test_patch": "diff --git a/dbms/tests/integration/test_distributed_ddl/configs/config.d/clusters.xml b/dbms/tests/integration/test_distributed_ddl/configs/config.d/clusters.xml\nindex 9d944c708540..88c7117eee1c 100644\n--- a/dbms/tests/integration/test_distributed_ddl/configs/config.d/clusters.xml\n+++ b/dbms/tests/integration/test_distributed_ddl/configs/config.d/clusters.xml\n@@ -89,31 +89,5 @@\n         </shard>\n     </cluster_no_replicas>\n \n-    <!-- Cluster without internal replication -->\n-    <cluster_without_replication>\n-        <shard>\n-            <internal_replication>false</internal_replication>\n-            <replica>\n-                <host>ch1</host>\n-                <port>9000</port>\n-            </replica>\n-            <replica>\n-                <host>ch2</host>\n-                <port>9000</port>\n-            </replica>\n-        </shard>\n-        <shard>\n-            <internal_replication>false</internal_replication>\n-            <replica>\n-                <host>ch3</host>\n-                <port>9000</port>\n-            </replica>\n-            <replica>\n-                <host>ch4</host>\n-                <port>9000</port>\n-            </replica>\n-        </shard>\n-    </cluster_without_replication>\n-\n </remote_servers>\n </yandex>\n\\ No newline at end of file\ndiff --git a/dbms/tests/integration/test_distributed_ddl/configs_secure/config.d/clusters.xml b/dbms/tests/integration/test_distributed_ddl/configs_secure/config.d/clusters.xml\nindex fa99fc8b4af7..2aede3ba5bcc 100644\n--- a/dbms/tests/integration/test_distributed_ddl/configs_secure/config.d/clusters.xml\n+++ b/dbms/tests/integration/test_distributed_ddl/configs_secure/config.d/clusters.xml\n@@ -102,35 +102,5 @@\n         </shard>\n     </cluster_no_replicas>\n \n-    <!-- Cluster without internal replication -->\n-    <cluster_without_replication>\n-        <shard>\n-            <internal_replication>false</internal_replication>\n-            <replica>\n-                <host>ch1</host>\n-                <port>9440</port>\n-                <secure>1</secure>\n-            </replica>\n-            <replica>\n-                <host>ch2</host>\n-                <port>9440</port>\n-                <secure>1</secure>\n-            </replica>\n-        </shard>\n-        <shard>\n-            <internal_replication>false</internal_replication>\n-            <replica>\n-                <host>ch3</host>\n-                <port>9440</port>\n-                <secure>1</secure>\n-            </replica>\n-            <replica>\n-                <host>ch4</host>\n-                <port>9440</port>\n-                <secure>1</secure>\n-            </replica>\n-        </shard>\n-    </cluster_without_replication>\n-\n </remote_servers>\n </yandex>\ndiff --git a/dbms/tests/integration/test_distributed_ddl/test.py b/dbms/tests/integration/test_distributed_ddl/test.py\nindex d64fc7c788b9..8ef6f1892c52 100755\n--- a/dbms/tests/integration/test_distributed_ddl/test.py\n+++ b/dbms/tests/integration/test_distributed_ddl/test.py\n@@ -108,24 +108,24 @@ def test_on_session_expired(test_cluster):\n def test_simple_alters(test_cluster):\n     instance = test_cluster.instances['ch2']\n \n-    test_cluster.ddl_check_query(instance, \"DROP TABLE IF EXISTS merge ON CLUSTER cluster_without_replication\")\n-    test_cluster.ddl_check_query(instance, \"DROP TABLE IF EXISTS all_merge_32 ON CLUSTER cluster_without_replication\")\n-    test_cluster.ddl_check_query(instance, \"DROP TABLE IF EXISTS all_merge_64 ON CLUSTER cluster_without_replication\")\n+    test_cluster.ddl_check_query(instance, \"DROP TABLE IF EXISTS merge ON CLUSTER '{cluster}'\")\n+    test_cluster.ddl_check_query(instance, \"DROP TABLE IF EXISTS all_merge_32 ON CLUSTER '{cluster}'\")\n+    test_cluster.ddl_check_query(instance, \"DROP TABLE IF EXISTS all_merge_64 ON CLUSTER '{cluster}'\")\n \n     test_cluster.ddl_check_query(instance, \"\"\"\n-CREATE TABLE IF NOT EXISTS merge ON CLUSTER cluster_without_replication (p Date, i Int32)\n+CREATE TABLE IF NOT EXISTS merge ON CLUSTER '{cluster}' (p Date, i Int32)\n ENGINE = MergeTree(p, p, 1)\n \"\"\")\n     test_cluster.ddl_check_query(instance, \"\"\"\n-CREATE TABLE IF NOT EXISTS all_merge_32 ON CLUSTER cluster_without_replication (p Date, i Int32)\n-ENGINE = Distributed(cluster_without_replication, default, merge, i)\n+CREATE TABLE IF NOT EXISTS all_merge_32 ON CLUSTER '{cluster}' (p Date, i Int32)\n+ENGINE = Distributed('{cluster}', default, merge, i)\n \"\"\")\n     test_cluster.ddl_check_query(instance, \"\"\"\n-CREATE TABLE IF NOT EXISTS all_merge_64 ON CLUSTER cluster_without_replication (p Date, i Int64, s String)\n-ENGINE = Distributed(cluster_without_replication, default, merge, i)\n+CREATE TABLE IF NOT EXISTS all_merge_64 ON CLUSTER '{cluster}' (p Date, i Int64, s String)\n+ENGINE = Distributed('{cluster}', default, merge, i)\n \"\"\")\n \n-    for i in xrange(4):\n+    for i in xrange(0, 4, 2):\n         k = (i / 2) * 2\n         test_cluster.instances['ch{}'.format(i + 1)].query(\"INSERT INTO merge (i) VALUES ({})({})\".format(k, k+1))\n \n@@ -133,26 +133,26 @@ def test_simple_alters(test_cluster):\n \n \n     time.sleep(5)\n-    test_cluster.ddl_check_query(instance, \"ALTER TABLE merge ON CLUSTER cluster_without_replication MODIFY COLUMN i Int64\")\n+    test_cluster.ddl_check_query(instance, \"ALTER TABLE merge ON CLUSTER '{cluster}' MODIFY COLUMN i Int64\")\n     time.sleep(5)\n-    test_cluster.ddl_check_query(instance, \"ALTER TABLE merge ON CLUSTER cluster_without_replication ADD COLUMN s String DEFAULT toString(i) FORMAT TSV\")\n+    test_cluster.ddl_check_query(instance, \"ALTER TABLE merge ON CLUSTER '{cluster}' ADD COLUMN s String DEFAULT toString(i) FORMAT TSV\")\n \n     assert TSV(instance.query(\"SELECT i, s FROM all_merge_64 ORDER BY i\")) == TSV(''.join(['{}\\t{}\\n'.format(x,x) for x in xrange(4)]))\n \n \n-    for i in xrange(4):\n+    for i in xrange(0, 4, 2):\n         k = (i / 2) * 2 + 4\n         test_cluster.instances['ch{}'.format(i + 1)].query(\"INSERT INTO merge (p, i) VALUES (31, {})(31, {})\".format(k, k+1))\n \n     assert TSV(instance.query(\"SELECT i, s FROM all_merge_64 ORDER BY i\")) == TSV(''.join(['{}\\t{}\\n'.format(x,x) for x in xrange(8)]))\n \n \n-    test_cluster.ddl_check_query(instance, \"ALTER TABLE merge ON CLUSTER cluster_without_replication DETACH PARTITION 197002\")\n+    test_cluster.ddl_check_query(instance, \"ALTER TABLE merge ON CLUSTER '{cluster}' DETACH PARTITION 197002\")\n     assert TSV(instance.query(\"SELECT i, s FROM all_merge_64 ORDER BY i\")) == TSV(''.join(['{}\\t{}\\n'.format(x,x) for x in xrange(4)]))\n \n-    test_cluster.ddl_check_query(instance, \"DROP TABLE merge ON CLUSTER cluster_without_replication\")\n-    test_cluster.ddl_check_query(instance, \"DROP TABLE all_merge_32 ON CLUSTER cluster_without_replication\")\n-    test_cluster.ddl_check_query(instance, \"DROP TABLE all_merge_64 ON CLUSTER cluster_without_replication\")\n+    test_cluster.ddl_check_query(instance, \"DROP TABLE merge ON CLUSTER '{cluster}'\")\n+    test_cluster.ddl_check_query(instance, \"DROP TABLE all_merge_32 ON CLUSTER '{cluster}'\")\n+    test_cluster.ddl_check_query(instance, \"DROP TABLE all_merge_64 ON CLUSTER '{cluster}'\")\n \n \n def test_macro(test_cluster):\n",
  "problem_statement": "create & alter inconsistency with \"on cluster\" for ENGINE = Distributed \n```\r\n<vscluster>\r\n   <shard>\r\n       <internal_replication>true</internal_replication>\r\n       <replica><host>node1</host></replica>\r\n       <replica><host>node2</host></replica>\r\n   </shard>\r\n</vscluster>\r\n```\r\n\r\n**CREATE works! And creates Distributed table on both replicas.**\r\nCREATE TABLE  vs.t1_distrib ON CLUSTER vscluster (ts DATETIME,c1 VARCHAR) ENGINE=Distributed('vscluster','vs','t1')\r\n\r\n\r\n**ALTER does not work!**\r\nALTER TABLE vs.t1_distrib ON CLUSTER vscluster ADD COLUMN c2 VARCHAR\r\n\r\nCode: 371, e.displayText() = DB::Exception: Table t1_distrib isn't replicated, but shard #1 is replicated according to its cluster definition, e.what() = DB::Exception.\r\n\r\nhttps://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/clickhouse/BmsOrL2AMN4/eQnjeC1CAQAJ\r\n\n",
  "hints_text": "Is this ever going to be fixed?\nHi, I'm hitting this issue as well, any plan to get this fixed?\nBTW I always used a workaround. \r\nOne more cluster with all nodes as dedicated shards (internal_replication=false).\r\nSo for this cluster any DDL just works at all nodes.\nOur internal users also have faced this issue.\r\n\r\nIt should be trivial to fix: just skip the check for Distributed tables.",
  "created_at": "2020-03-12T06:50:36Z"
}