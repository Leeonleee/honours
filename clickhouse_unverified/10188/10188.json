{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 10188,
  "instance_id": "ClickHouse__ClickHouse-10188",
  "issue_numbers": [
    "9946"
  ],
  "base_commit": "21cb5671ec3f67db87e2f0f1b818559079f8ff7b",
  "patch": "diff --git a/src/Interpreters/InterserverIOHandler.h b/src/Interpreters/InterserverIOHandler.h\nindex 4651c8cb9786..952c99ae46d0 100644\n--- a/src/Interpreters/InterserverIOHandler.h\n+++ b/src/Interpreters/InterserverIOHandler.h\n@@ -32,7 +32,7 @@ class InterserverIOEndpoint\n public:\n     virtual std::string getId(const std::string & path) const = 0;\n     virtual void processQuery(const Poco::Net::HTMLForm & params, ReadBuffer & body, WriteBuffer & out, Poco::Net::HTTPServerResponse & response) = 0;\n-    virtual ~InterserverIOEndpoint() {}\n+    virtual ~InterserverIOEndpoint() = default;\n \n     /// You need to stop the data transfer if blocker is activated.\n     ActionBlocker blocker;\ndiff --git a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\nindex fcebc74cce15..2ab43f8f56c7 100644\n--- a/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp\n@@ -224,16 +224,28 @@ bool MergeTreeDataMergerMutator::selectPartsToMerge(\n     IMergeSelector::Partitions partitions;\n \n     const String * prev_partition_id = nullptr;\n+    /// Previous part only in boundaries of partition frame\n     const MergeTreeData::DataPartPtr * prev_part = nullptr;\n     bool has_part_with_expired_ttl = false;\n     for (const MergeTreeData::DataPartPtr & part : data_parts)\n     {\n+        /// Check predicate only for first part in each partition.\n+        if (!prev_part)\n+            /* Parts can be merged with themselves for TTL needs for example.\n+            * So we have to check if this part is currently being inserted with quorum and so on and so forth.\n+            * Obviously we have to check it manually only for the first part\n+            * of each partition because it will be automatically checked for a pair of parts. */\n+            if (!can_merge_callback(nullptr, part, nullptr))\n+                continue;\n+\n         const String & partition_id = part->info.partition_id;\n         if (!prev_partition_id || partition_id != *prev_partition_id || (prev_part && !can_merge_callback(*prev_part, part, nullptr)))\n         {\n             if (partitions.empty() || !partitions.back().empty())\n                 partitions.emplace_back();\n+            /// New partition frame.\n             prev_partition_id = &partition_id;\n+            prev_part = nullptr;\n         }\n \n         IMergeSelector::Part part_info;\ndiff --git a/src/Storages/MergeTree/MergeTreeDataMergerMutator.h b/src/Storages/MergeTree/MergeTreeDataMergerMutator.h\nindex 6f4f8a03e9aa..f2f4ea95fd08 100644\n--- a/src/Storages/MergeTree/MergeTreeDataMergerMutator.h\n+++ b/src/Storages/MergeTree/MergeTreeDataMergerMutator.h\n@@ -49,9 +49,8 @@ struct FutureMergedMutatedPart\n class MergeTreeDataMergerMutator\n {\n public:\n-    using AllowedMergingPredicate = std::function<bool (const MergeTreeData::DataPartPtr &, const MergeTreeData::DataPartPtr &, String * reason)>;\n+    using AllowedMergingPredicate = std::function<bool (const MergeTreeData::DataPartPtr &, const MergeTreeData::DataPartPtr &, String *)>;\n \n-public:\n     MergeTreeDataMergerMutator(MergeTreeData & data_, size_t background_pool_size);\n \n     /** Get maximum total size of parts to do merge, at current moment of time.\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp b/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp\nindex 72044ab832cb..534118274aec 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp\n@@ -1648,8 +1648,21 @@ ReplicatedMergeTreeMergePredicate::ReplicatedMergeTreeMergePredicate(\n }\n \n bool ReplicatedMergeTreeMergePredicate::operator()(\n-        const MergeTreeData::DataPartPtr & left, const MergeTreeData::DataPartPtr & right,\n-        String * out_reason) const\n+    const MergeTreeData::DataPartPtr & left,\n+    const MergeTreeData::DataPartPtr & right,\n+    String * out_reason) const\n+{\n+    if (left)\n+        return canMergeTwoParts(left, right, out_reason);\n+    else\n+        return canMergeSinglePart(right, out_reason);\n+}\n+\n+\n+bool ReplicatedMergeTreeMergePredicate::canMergeTwoParts(\n+    const MergeTreeData::DataPartPtr & left,\n+    const MergeTreeData::DataPartPtr & right,\n+    String * out_reason) const\n {\n     /// A sketch of a proof of why this method actually works:\n     ///\n@@ -1784,6 +1797,39 @@ bool ReplicatedMergeTreeMergePredicate::operator()(\n     return true;\n }\n \n+bool ReplicatedMergeTreeMergePredicate::canMergeSinglePart(\n+    const MergeTreeData::DataPartPtr & part,\n+    String * out_reason) const\n+{\n+    if (part->name == inprogress_quorum_part)\n+    {\n+        if (out_reason)\n+            *out_reason = \"Quorum insert for part \" + part->name + \" is currently in progress\";\n+        return false;\n+    }\n+\n+    if (prev_virtual_parts.getContainingPart(part->info).empty())\n+    {\n+        if (out_reason)\n+            *out_reason = \"Entry for part \" + part->name + \" hasn't been read from the replication log yet\";\n+        return false;\n+    }\n+\n+    std::lock_guard<std::mutex> lock(queue.state_mutex);\n+\n+    /// We look for containing parts in queue.virtual_parts (and not in prev_virtual_parts) because queue.virtual_parts is newer\n+    /// and it is guaranteed that it will contain all merges assigned before this object is constructed.\n+    String containing_part = queue.virtual_parts.getContainingPart(part->info);\n+    if (containing_part != part->name)\n+    {\n+        if (out_reason)\n+            *out_reason = \"Part \" + part->name + \" has already been assigned a merge into \" + containing_part;\n+        return false;\n+    }\n+\n+    return true;\n+}\n+\n \n std::optional<std::pair<Int64, int>> ReplicatedMergeTreeMergePredicate::getDesiredMutationVersion(const MergeTreeData::DataPartPtr & part) const\n {\ndiff --git a/src/Storages/MergeTree/ReplicatedMergeTreeQueue.h b/src/Storages/MergeTree/ReplicatedMergeTreeQueue.h\nindex 2191104a2918..8e58c8b7af22 100644\n--- a/src/Storages/MergeTree/ReplicatedMergeTreeQueue.h\n+++ b/src/Storages/MergeTree/ReplicatedMergeTreeQueue.h\n@@ -408,12 +408,22 @@ class ReplicatedMergeTreeMergePredicate\n public:\n     ReplicatedMergeTreeMergePredicate(ReplicatedMergeTreeQueue & queue_, zkutil::ZooKeeperPtr & zookeeper);\n \n+    /// Depending on the existence of left part checks a merge predicate for two parts or for single part.\n+    bool operator()(const MergeTreeData::DataPartPtr & left,\n+                    const MergeTreeData::DataPartPtr & right,\n+                    String * out_reason = nullptr) const;\n+\n     /// Can we assign a merge with these two parts?\n     /// (assuming that no merge was assigned after the predicate was constructed)\n     /// If we can't and out_reason is not nullptr, set it to the reason why we can't merge.\n-    bool operator()(\n-        const MergeTreeData::DataPartPtr & left, const MergeTreeData::DataPartPtr & right,\n-        String * out_reason = nullptr) const;\n+    bool canMergeTwoParts(const MergeTreeData::DataPartPtr & left,\n+                          const MergeTreeData::DataPartPtr & right,\n+                          String * out_reason = nullptr) const;\n+\n+    /// Can we assign a merge this part and some other part?\n+    /// For example a merge of a part and itself is needed for TTL.\n+    /// This predicate is checked for the first part of each partitition.\n+    bool canMergeSinglePart(const MergeTreeData::DataPartPtr & part, String * out_reason) const;\n \n     /// Return nonempty optional of desired mutation version and alter version.\n     /// If we have no alter (modify/drop) mutations in mutations queue, than we return biggest possible\ndiff --git a/src/Storages/StorageMergeTree.cpp b/src/Storages/StorageMergeTree.cpp\nindex 1aac67177285..5110258834e8 100644\n--- a/src/Storages/StorageMergeTree.cpp\n+++ b/src/Storages/StorageMergeTree.cpp\n@@ -552,8 +552,12 @@ bool StorageMergeTree::merge(\n     {\n         std::lock_guard lock(currently_processing_in_background_mutex);\n \n-        auto can_merge = [this, &lock] (const DataPartPtr & left, const DataPartPtr & right, String *)\n+        auto can_merge = [this, &lock] (const DataPartPtr & left, const DataPartPtr & right, String *) -> bool\n         {\n+            /// This predicate is checked for the first part of each partition.\n+            /// (left = nullptr, right = \"first part of partition\")\n+            if (!left)\n+                return !currently_merging_mutating_parts.count(right);\n             return !currently_merging_mutating_parts.count(left) && !currently_merging_mutating_parts.count(right)\n                 && getCurrentMutationVersion(left, lock) == getCurrentMutationVersion(right, lock);\n         };\ndiff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex 2a0dac0fe4c8..aa951ce19e1b 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -2223,7 +2223,7 @@ void StorageReplicatedMergeTree::mergeSelectingTask()\n \n             FutureMergedMutatedPart future_merged_part;\n             if (max_source_parts_size_for_merge > 0 &&\n-                merger_mutator.selectPartsToMerge(future_merged_part, false, max_source_parts_size_for_merge, merge_pred))\n+                merger_mutator.selectPartsToMerge(future_merged_part, false, max_source_parts_size_for_merge, merge_pred, nullptr))\n             {\n                 success = createLogEntryToMergeParts(zookeeper, future_merged_part.parts,\n                     future_merged_part.name, future_merged_part.type, deduplicate, force_ttl);\n@@ -2701,6 +2701,58 @@ void StorageReplicatedMergeTree::updateQuorum(const String & part_name)\n }\n \n \n+void StorageReplicatedMergeTree::cleanLastPartNode(const String & partition_id)\n+{\n+    auto zookeeper = getZooKeeper();\n+\n+    /// The name of the previous part for which the quorum was reached.\n+    const String quorum_last_part_path = zookeeper_path + \"/quorum/last_part\";\n+\n+    /// Delete information from \"last_part\" node.\n+\n+    while (true)\n+    {\n+        Coordination::Stat added_parts_stat;\n+        String old_added_parts = zookeeper->get(quorum_last_part_path, &added_parts_stat);\n+\n+        ReplicatedMergeTreeQuorumAddedParts parts_with_quorum(format_version);\n+\n+        if (!old_added_parts.empty())\n+            parts_with_quorum.fromString(old_added_parts);\n+\n+        /// Delete information about particular partition.\n+        if (!parts_with_quorum.added_parts.contains(partition_id))\n+        {\n+            /// There is no information about interested part.\n+            break;\n+        }\n+\n+        parts_with_quorum.added_parts.erase(partition_id);\n+\n+        String new_added_parts = parts_with_quorum.toString();\n+\n+        auto code = zookeeper->trySet(quorum_last_part_path, new_added_parts, added_parts_stat.version);\n+\n+        if (code == Coordination::ZOK)\n+        {\n+            break;\n+        }\n+        else if (code == Coordination::ZNONODE)\n+        {\n+            /// Node is deleted. It is impossible, but it is Ok.\n+            break;\n+        }\n+        else if (code == Coordination::ZBADVERSION)\n+        {\n+            /// Node was updated meanwhile. We must re-read it and repeat all the actions.\n+            continue;\n+        }\n+        else\n+            throw Coordination::Exception(code, quorum_last_part_path);\n+    }\n+}\n+\n+\n bool StorageReplicatedMergeTree::fetchPart(const String & part_name, const String & source_replica_path, bool to_detached, size_t quorum)\n {\n     const auto part_info = MergeTreePartInfo::fromPartName(part_name, format_version);\n@@ -3561,6 +3613,9 @@ void StorageReplicatedMergeTree::dropPartition(const ASTPtr & query, const ASTPt\n                 waitForAllReplicasToProcessLogEntry(entry);\n         }\n     }\n+\n+    /// Cleaning possibly stored information about parts from /quorum/last_part node in ZooKeeper.\n+    cleanLastPartNode(partition_id);\n }\n \n \n@@ -4961,10 +5016,10 @@ void StorageReplicatedMergeTree::replacePartitionFrom(const StoragePtr & source_\n     }\n }\n \n-void StorageReplicatedMergeTree::movePartitionToTable(const StoragePtr & dest_table, const ASTPtr & partition, const Context & context)\n+void StorageReplicatedMergeTree::movePartitionToTable(const StoragePtr & dest_table, const ASTPtr & partition, const Context & query_context)\n {\n-    auto lock1 = lockStructureForShare(false, context.getCurrentQueryId(), context.getSettingsRef().lock_acquire_timeout);\n-    auto lock2 = dest_table->lockStructureForShare(false, context.getCurrentQueryId(), context.getSettingsRef().lock_acquire_timeout);\n+    auto lock1 = lockStructureForShare(false, query_context.getCurrentQueryId(), query_context.getSettingsRef().lock_acquire_timeout);\n+    auto lock2 = dest_table->lockStructureForShare(false, query_context.getCurrentQueryId(), query_context.getSettingsRef().lock_acquire_timeout);\n \n     auto dest_table_storage = std::dynamic_pointer_cast<StorageReplicatedMergeTree>(dest_table);\n     if (!dest_table_storage)\n@@ -4979,7 +5034,7 @@ void StorageReplicatedMergeTree::movePartitionToTable(const StoragePtr & dest_ta\n     Stopwatch watch;\n     MergeTreeData & src_data = dest_table_storage->checkStructureAndGetMergeTreeData(*this);\n     auto src_data_id = src_data.getStorageID();\n-    String partition_id = getPartitionIDFromQuery(partition, context);\n+    String partition_id = getPartitionIDFromQuery(partition, query_context);\n \n     DataPartsVector src_all_parts = src_data.getDataPartsVectorInPartition(MergeTreeDataPartState::Committed, partition_id);\n     DataPartsVector src_parts;\n@@ -5135,7 +5190,7 @@ void StorageReplicatedMergeTree::movePartitionToTable(const StoragePtr & dest_ta\n     parts_to_remove.clear();\n     cleanup_thread.wakeup();\n \n-    if (context.getSettingsRef().replication_alter_partitions_sync > 1)\n+    if (query_context.getSettingsRef().replication_alter_partitions_sync > 1)\n     {\n         lock2.release();\n         dest_table_storage->waitForAllReplicasToProcessLogEntry(entry);\n@@ -5150,11 +5205,14 @@ void StorageReplicatedMergeTree::movePartitionToTable(const StoragePtr & dest_ta\n     log_znode_path = dynamic_cast<const Coordination::CreateResponse &>(*op_results.back()).path_created;\n     entry_delete.znode_name = log_znode_path.substr(log_znode_path.find_last_of('/') + 1);\n \n-    if (context.getSettingsRef().replication_alter_partitions_sync > 1)\n+    if (query_context.getSettingsRef().replication_alter_partitions_sync > 1)\n     {\n         lock1.release();\n         waitForAllReplicasToProcessLogEntry(entry_delete);\n     }\n+\n+    /// Cleaning possibly stored information about parts from /quorum/last_part node in ZooKeeper.\n+    cleanLastPartNode(partition_id);\n }\n \n void StorageReplicatedMergeTree::getCommitPartOps(\n@@ -5245,6 +5303,9 @@ bool StorageReplicatedMergeTree::waitForShrinkingQueueSize(size_t queue_size, UI\n \n     /// Let's fetch new log entries firstly\n     queue.pullLogsToQueue(getZooKeeper());\n+    /// This is significant, because the execution of this task could be delayed at BackgroundPool.\n+    /// And we force it to be executed.\n+    queue_task_handle->wake();\n \n     Poco::Event target_size_event;\n     auto callback = [&target_size_event, queue_size] (size_t new_queue_size)\ndiff --git a/src/Storages/StorageReplicatedMergeTree.h b/src/Storages/StorageReplicatedMergeTree.h\nindex 01dd32614f9b..17b454f27577 100644\n--- a/src/Storages/StorageReplicatedMergeTree.h\n+++ b/src/Storages/StorageReplicatedMergeTree.h\n@@ -464,6 +464,9 @@ class StorageReplicatedMergeTree final : public ext::shared_ptr_helper<StorageRe\n     /// With the quorum being tracked, add a replica to the quorum for the part.\n     void updateQuorum(const String & part_name);\n \n+    /// Deletes info from quorum/last_part node for particular partition_id.\n+    void cleanLastPartNode(const String & partition_id);\n+\n     /// Creates new block number if block with such block_id does not exist\n     std::optional<EphemeralLockInZooKeeper> allocateBlockNumber(\n         const String & partition_id, zkutil::ZooKeeperPtr & zookeeper,\n",
  "test_patch": "diff --git a/tests/integration/test_quorum_inserts/__init__.py b/tests/integration/test_quorum_inserts/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_quorum_inserts/configs/users.d/settings.xml b/tests/integration/test_quorum_inserts/configs/users.d/settings.xml\nnew file mode 100644\nindex 000000000000..29b762656aab\n--- /dev/null\n+++ b/tests/integration/test_quorum_inserts/configs/users.d/settings.xml\n@@ -0,0 +1,11 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+    <profiles>\n+        <default>\n+            <max_memory_usage>10000000000</max_memory_usage>\n+            <use_uncompressed_cache>0</use_uncompressed_cache>\n+            <insert_quorum>2</insert_quorum>\n+            <select_sequential_consistency>1</select_sequential_consistency>\n+        </default>\n+    </profiles>\n+</yandex>\ndiff --git a/tests/integration/test_quorum_inserts/test.py b/tests/integration/test_quorum_inserts/test.py\nnew file mode 100644\nindex 000000000000..279018426920\n--- /dev/null\n+++ b/tests/integration/test_quorum_inserts/test.py\n@@ -0,0 +1,294 @@\n+import time\n+\n+import pytest\n+\n+from helpers.test_tools import TSV\n+from helpers.cluster import ClickHouseCluster\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+zero = cluster.add_instance(\"zero\",\n+                            config_dir=\"configs\",\n+                            macros={\"cluster\": \"anime\", \"shard\": \"0\", \"replica\": \"zero\"},\n+                            with_zookeeper=True)\n+\n+first = cluster.add_instance(\"first\",\n+                             config_dir=\"configs\",\n+                             macros={\"cluster\": \"anime\", \"shard\": \"0\", \"replica\": \"first\"},\n+                             with_zookeeper=True)\n+\n+second = cluster.add_instance(\"second\",\n+                              config_dir=\"configs\",\n+                              macros={\"cluster\": \"anime\", \"shard\": \"0\", \"replica\": \"second\"},\n+                              with_zookeeper=True)\n+\n+def execute_on_all_cluster(query_):\n+    for node in [zero, first, second]:\n+        node.query(query_)\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    global cluster\n+    try:\n+        cluster.start()\n+        yield cluster\n+\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_simple_add_replica(started_cluster):\n+    execute_on_all_cluster(\"DROP TABLE IF EXISTS test_simple\")\n+\n+    create_query = \"CREATE TABLE test_simple \" \\\n+                   \"(a Int8, d Date) \" \\\n+                   \"Engine = ReplicatedMergeTree('/clickhouse/tables/{shard}/{table}', '{replica}') \" \\\n+                   \"PARTITION BY d ORDER BY a\"\n+\n+    zero.query(create_query)\n+    first.query(create_query)\n+\n+    first.query(\"SYSTEM STOP FETCHES test_simple\")\n+\n+    zero.query(\"INSERT INTO test_simple VALUES (1, '2011-01-01')\", settings={'insert_quorum' : 1})\n+\n+    assert '1\\t2011-01-01\\n' == zero.query(\"SELECT * from test_simple\")\n+    assert '' == first.query(\"SELECT * from test_simple\")\n+\n+    first.query(\"SYSTEM START FETCHES test_simple\")\n+\n+    first.query(\"SYSTEM SYNC REPLICA test_simple\", timeout=20)\n+\n+    assert '1\\t2011-01-01\\n' == zero.query(\"SELECT * from test_simple\")\n+    assert '1\\t2011-01-01\\n' == first.query(\"SELECT * from test_simple\")\n+\n+    second.query(create_query)\n+    second.query(\"SYSTEM SYNC REPLICA test_simple\", timeout=20)\n+\n+    assert '1\\t2011-01-01\\n' == zero.query(\"SELECT * from test_simple\")\n+    assert '1\\t2011-01-01\\n' == first.query(\"SELECT * from test_simple\")\n+    assert '1\\t2011-01-01\\n' == second.query(\"SELECT * from test_simple\")\n+\n+    execute_on_all_cluster(\"DROP TABLE IF EXISTS test_simple\")\n+\n+\n+\n+def test_drop_replica_and_achieve_quorum(started_cluster):\n+    execute_on_all_cluster(\"DROP TABLE IF EXISTS test_drop_replica_and_achieve_quorum\")\n+\n+    create_query = \"CREATE TABLE test_drop_replica_and_achieve_quorum \" \\\n+                   \"(a Int8, d Date) \" \\\n+                   \"Engine = ReplicatedMergeTree('/clickhouse/tables/{shard}/{table}', '{replica}') \" \\\n+                   \"PARTITION BY d ORDER BY a\"\n+\n+    print(\"Create Replicated table with two replicas\")\n+    zero.query(create_query)\n+    first.query(create_query)\n+\n+    print(\"Stop fetches on one replica. Since that, it will be isolated.\")\n+    first.query(\"SYSTEM STOP FETCHES test_drop_replica_and_achieve_quorum\")\n+\n+    print(\"Insert to other replica. This query will fail.\")\n+    quorum_timeout = zero.query_and_get_error(\"INSERT INTO test_drop_replica_and_achieve_quorum(a,d) VALUES (1, '2011-01-01')\",\n+                                              settings={'insert_quorum_timeout' : 5000})\n+    assert \"Timeout while waiting for quorum\" in quorum_timeout, \"Query must fail.\"\n+\n+    assert TSV(\"1\\t2011-01-01\\n\") == TSV(zero.query(\"SELECT * FROM test_drop_replica_and_achieve_quorum\",\n+                                          settings={'select_sequential_consistency' : 0}))\n+\n+    assert TSV(\"\") == TSV(zero.query(\"SELECT * FROM test_drop_replica_and_achieve_quorum\",\n+                                           settings={'select_sequential_consistency' : 1}))\n+\n+    #TODO:(Mikhaylov) begin; maybe delete this lines. I want clickhouse to fetch parts and update quorum.\n+    print(\"START FETCHES first replica\")\n+    first.query(\"SYSTEM START FETCHES test_drop_replica_and_achieve_quorum\")\n+\n+    print(\"SYNC first replica\")\n+    first.query(\"SYSTEM SYNC REPLICA test_drop_replica_and_achieve_quorum\", timeout=20)\n+    #TODO:(Mikhaylov) end\n+\n+    print(\"Add second replica\")\n+    second.query(create_query)\n+\n+    print(\"SYNC second replica\")\n+    second.query(\"SYSTEM SYNC REPLICA test_drop_replica_and_achieve_quorum\", timeout=20)\n+\n+    print(\"Quorum for previous insert achieved.\")\n+    assert TSV(\"1\\t2011-01-01\\n\") == TSV(second.query(\"SELECT * FROM test_drop_replica_and_achieve_quorum\",\n+                                            settings={'select_sequential_consistency' : 1}))\n+\n+    print(\"Now we can insert some other data.\")\n+    zero.query(\"INSERT INTO test_drop_replica_and_achieve_quorum(a,d) VALUES (2, '2012-02-02')\")\n+\n+    assert TSV(\"1\\t2011-01-01\\n2\\t2012-02-02\\n\") == TSV(zero.query(\"SELECT * FROM test_drop_replica_and_achieve_quorum ORDER BY a\"))\n+    assert TSV(\"1\\t2011-01-01\\n2\\t2012-02-02\\n\") == TSV(first.query(\"SELECT * FROM test_drop_replica_and_achieve_quorum ORDER BY a\"))\n+    assert TSV(\"1\\t2011-01-01\\n2\\t2012-02-02\\n\") == TSV(second.query(\"SELECT * FROM test_drop_replica_and_achieve_quorum ORDER BY a\"))\n+\n+    execute_on_all_cluster(\"DROP TABLE IF EXISTS test_drop_replica_and_achieve_quorum\")\n+\n+\n+@pytest.mark.parametrize(\n+    ('add_new_data'),\n+    [\n+        False,\n+        True\n+    ]\n+)\n+\n+def test_insert_quorum_with_drop_partition(started_cluster, add_new_data):\n+    execute_on_all_cluster(\"DROP TABLE IF EXISTS test_quorum_insert_with_drop_partition\")\n+\n+    create_query = \"CREATE TABLE test_quorum_insert_with_drop_partition \" \\\n+                   \"(a Int8, d Date) \" \\\n+                   \"Engine = ReplicatedMergeTree('/clickhouse/tables/{shard}/{table}', '{replica}') \" \\\n+                   \"PARTITION BY d ORDER BY a \"\n+\n+    print(\"Create Replicated table with three replicas\")\n+    zero.query(create_query)\n+    first.query(create_query)\n+    second.query(create_query)\n+\n+    print(\"Stop fetches for test_quorum_insert_with_drop_partition at first replica.\")\n+    first.query(\"SYSTEM STOP FETCHES test_quorum_insert_with_drop_partition\")\n+\n+    print(\"Insert with quorum. (zero and second)\")\n+    zero.query(\"INSERT INTO test_quorum_insert_with_drop_partition(a,d) VALUES(1, '2011-01-01')\")\n+\n+    print(\"Drop partition.\")\n+    zero.query(\"ALTER TABLE test_quorum_insert_with_drop_partition DROP PARTITION '2011-01-01'\")\n+\n+    if (add_new_data):\n+        print(\"Insert to deleted partition\")\n+        zero.query(\"INSERT INTO test_quorum_insert_with_drop_partition(a,d) VALUES(2, '2011-01-01')\")\n+\n+    print(\"Resume fetches for test_quorum_insert_with_drop_partition at first replica.\")\n+    first.query(\"SYSTEM START FETCHES test_quorum_insert_with_drop_partition\")\n+\n+    print(\"Sync first replica with others.\")\n+    first.query(\"SYSTEM SYNC REPLICA test_quorum_insert_with_drop_partition\")\n+\n+    assert \"20110101\" not in first.query(\"SELECT * FROM system.zookeeper \" \\\n+                                         \"where path='/clickhouse/tables/0/test_quorum_insert_with_drop_partition/quorum/last_part' \" \\\n+                                         \"format Vertical\")\n+\n+    print(\"Select from updated partition.\")\n+    if (add_new_data):\n+        assert TSV(\"2\\t2011-01-01\\n\") == TSV(zero.query(\"SELECT * FROM test_quorum_insert_with_drop_partition\"))\n+        assert TSV(\"2\\t2011-01-01\\n\") == TSV(second.query(\"SELECT * FROM test_quorum_insert_with_drop_partition\"))\n+    else:\n+        assert TSV(\"\") == TSV(zero.query(\"SELECT * FROM test_quorum_insert_with_drop_partition\"))\n+        assert TSV(\"\") == TSV(second.query(\"SELECT * FROM test_quorum_insert_with_drop_partition\"))\n+\n+    execute_on_all_cluster(\"DROP TABLE IF EXISTS test_quorum_insert_with_drop_partition\")\n+\n+\n+@pytest.mark.parametrize(\n+    ('add_new_data'),\n+    [\n+        False,\n+        True\n+    ]\n+)\n+\n+def test_insert_quorum_with_move_partition(started_cluster, add_new_data):\n+    execute_on_all_cluster(\"DROP TABLE IF EXISTS test_insert_quorum_with_move_partition_source\")\n+    execute_on_all_cluster(\"DROP TABLE IF EXISTS test_insert_quorum_with_move_partition_destination\")\n+\n+    create_source = \"CREATE TABLE test_insert_quorum_with_move_partition_source \" \\\n+                   \"(a Int8, d Date) \" \\\n+                   \"Engine = ReplicatedMergeTree('/clickhouse/tables/{shard}/{table}', '{replica}') \" \\\n+                   \"PARTITION BY d ORDER BY a \"\n+\n+    create_destination = \"CREATE TABLE test_insert_quorum_with_move_partition_destination \" \\\n+                    \"(a Int8, d Date) \" \\\n+                    \"Engine = ReplicatedMergeTree('/clickhouse/tables/{shard}/{table}', '{replica}') \" \\\n+                    \"PARTITION BY d ORDER BY a \"\n+\n+    print(\"Create source Replicated table with three replicas\")\n+    zero.query(create_source)\n+    first.query(create_source)\n+    second.query(create_source)\n+\n+    print(\"Create destination Replicated table with three replicas\")\n+    zero.query(create_destination)\n+    first.query(create_destination)\n+    second.query(create_destination)\n+\n+    print(\"Stop fetches for test_insert_quorum_with_move_partition_source at first replica.\")\n+    first.query(\"SYSTEM STOP FETCHES test_insert_quorum_with_move_partition_source\")\n+\n+    print(\"Insert with quorum. (zero and second)\")\n+    zero.query(\"INSERT INTO test_insert_quorum_with_move_partition_source(a,d) VALUES(1, '2011-01-01')\")\n+\n+    print(\"Drop partition.\")\n+    zero.query(\"ALTER TABLE test_insert_quorum_with_move_partition_source MOVE PARTITION '2011-01-01' TO TABLE test_insert_quorum_with_move_partition_destination\")\n+\n+    if (add_new_data):\n+        print(\"Insert to deleted partition\")\n+        zero.query(\"INSERT INTO test_insert_quorum_with_move_partition_source(a,d) VALUES(2, '2011-01-01')\")\n+\n+    print(\"Resume fetches for test_insert_quorum_with_move_partition_source at first replica.\")\n+    first.query(\"SYSTEM START FETCHES test_insert_quorum_with_move_partition_source\")\n+\n+    print(\"Sync first replica with others.\")\n+    first.query(\"SYSTEM SYNC REPLICA test_insert_quorum_with_move_partition_source\")\n+\n+    assert \"20110101\" not in first.query(\"SELECT * FROM system.zookeeper \" \\\n+                                         \"where path='/clickhouse/tables/0/test_insert_quorum_with_move_partition_source/quorum/last_part' \" \\\n+                                         \"format Vertical\")\n+\n+    print(\"Select from updated partition.\")\n+    if (add_new_data):\n+        assert TSV(\"2\\t2011-01-01\\n\") == TSV(zero.query(\"SELECT * FROM test_insert_quorum_with_move_partition_source\"))\n+        assert TSV(\"2\\t2011-01-01\\n\") == TSV(second.query(\"SELECT * FROM test_insert_quorum_with_move_partition_source\"))\n+    else:\n+        assert TSV(\"\") == TSV(zero.query(\"SELECT * FROM test_insert_quorum_with_move_partition_source\"))\n+        assert TSV(\"\") == TSV(second.query(\"SELECT * FROM test_insert_quorum_with_move_partition_source\"))\n+\n+    execute_on_all_cluster(\"DROP TABLE IF EXISTS test_insert_quorum_with_move_partition_source\")\n+    execute_on_all_cluster(\"DROP TABLE IF EXISTS test_insert_quorum_with_move_partition_destination\")\n+\n+\n+def test_insert_quorum_with_ttl(started_cluster):\n+    execute_on_all_cluster(\"DROP TABLE IF EXISTS test_insert_quorum_with_ttl\")\n+\n+    create_query = \"CREATE TABLE test_insert_quorum_with_ttl \" \\\n+                   \"(a Int8, d Date) \" \\\n+                   \"Engine = ReplicatedMergeTree('/clickhouse/tables/{table}', '{replica}') \" \\\n+                   \"PARTITION BY d ORDER BY a \" \\\n+                   \"TTL d + INTERVAL 5 second \" \\\n+                   \"SETTINGS merge_with_ttl_timeout=2 \"\n+\n+    print(\"Create Replicated table with two replicas\")\n+    zero.query(create_query)\n+    first.query(create_query)\n+\n+    print(\"Stop fetches for test_insert_quorum_with_ttl at first replica.\")\n+    first.query(\"SYSTEM STOP FETCHES test_insert_quorum_with_ttl\")\n+\n+    print(\"Insert should fail since it can not reach the quorum.\")\n+    quorum_timeout = zero.query_and_get_error(\"INSERT INTO test_insert_quorum_with_ttl(a,d) VALUES(1, '2011-01-01')\",\n+                                              settings={'insert_quorum_timeout' : 5000})\n+    assert \"Timeout while waiting for quorum\" in quorum_timeout, \"Query must fail.\"\n+\n+    print(\"Wait 10 seconds and TTL merge have to be executed. But it won't delete data.\")\n+    time.sleep(10)\n+    assert TSV(\"1\\t2011-01-01\\n\") == TSV(zero.query(\"SELECT * FROM test_insert_quorum_with_ttl\", settings={'select_sequential_consistency' : 0}))\n+\n+    print(\"Resume fetches for test_insert_quorum_with_ttl at first replica.\")\n+    first.query(\"SYSTEM START FETCHES test_insert_quorum_with_ttl\")\n+\n+    print(\"Sync first replica.\")\n+    first.query(\"SYSTEM SYNC REPLICA test_insert_quorum_with_ttl\")\n+\n+    zero.query(\"INSERT INTO test_insert_quorum_with_ttl(a,d) VALUES(1, '2011-01-01')\",\n+                                              settings={'insert_quorum_timeout' : 5000})\n+\n+\n+    assert TSV(\"1\\t2011-01-01\\n\") == TSV(first.query(\"SELECT * FROM test_insert_quorum_with_ttl\", settings={'select_sequential_consistency' : 0}))\n+    assert TSV(\"1\\t2011-01-01\\n\") == TSV(first.query(\"SELECT * FROM test_insert_quorum_with_ttl\", settings={'select_sequential_consistency' : 1}))\n+\n+    print(\"Inserts should resume.\")\n+    zero.query(\"INSERT INTO test_insert_quorum_with_ttl(a, d) VALUES(2, '2012-02-02')\")\n+\n+    execute_on_all_cluster(\"DROP TABLE IF EXISTS test_insert_quorum_with_ttl\")\n",
  "problem_statement": "drop partition vs insert_quorum \nScenario:\r\n1) some data from the past arrived\r\n2) external \u0441leanup script (cron) did drop part / partition before it get replicated.\r\n3) insert_quorum stuck ('Quorum for previous write has not been satisfied yet ... ')\r\n\r\nExpected:\r\nDROP PARTITION / PART should also clean /quorum znode if it removes corresponding partition. \r\n\r\n\r\n\r\n\n",
  "hints_text": "The same problem is reproduced with TTL deletes -- if TTL drops a partition with data not in quorum, quorum gets stuck forever.",
  "created_at": "2020-04-10T21:32:13Z"
}