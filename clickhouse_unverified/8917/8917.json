{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 8917,
  "instance_id": "ClickHouse__ClickHouse-8917",
  "issue_numbers": [
    "7259"
  ],
  "base_commit": "c8bf3b4bcd381878f5a7f097b1873261631dd264",
  "patch": "diff --git a/dbms/src/Storages/Kafka/KafkaBlockInputStream.cpp b/dbms/src/Storages/Kafka/KafkaBlockInputStream.cpp\nindex b19dd4bb911e..ec708422d467 100644\n--- a/dbms/src/Storages/Kafka/KafkaBlockInputStream.cpp\n+++ b/dbms/src/Storages/Kafka/KafkaBlockInputStream.cpp\n@@ -52,16 +52,21 @@ void KafkaBlockInputStream::readPrefixImpl()\n     if (!buffer)\n         return;\n \n-    buffer->subscribe(storage.getTopics());\n+    buffer->subscribe();\n \n     broken = true;\n }\n \n Block KafkaBlockInputStream::readImpl()\n {\n-    if (!buffer)\n+    if (!buffer || finished)\n         return Block();\n \n+    finished = true;\n+    // now it's one-time usage InputStream\n+    // one block of the needed size (or with desired flush timeout) is formed in one internal iteration\n+    // otherwise external iteration will reuse that and logic will became even more fuzzy\n+\n     MutableColumns result_columns  = non_virtual_header.cloneEmptyColumns();\n     MutableColumns virtual_columns = virtual_header.cloneEmptyColumns();\n \n@@ -126,6 +131,8 @@ Block KafkaBlockInputStream::readImpl()\n \n         auto new_rows = read_kafka_message();\n \n+        buffer->storeLastReadMessageOffset();\n+\n         auto _topic         = buffer->currentTopic();\n         auto _key           = buffer->currentKey();\n         auto _offset        = buffer->currentOffset();\n@@ -151,11 +158,18 @@ Block KafkaBlockInputStream::readImpl()\n \n         total_rows = total_rows + new_rows;\n         buffer->allowNext();\n-        if (!new_rows || total_rows >= max_block_size || !checkTimeLimit())\n+\n+        if (buffer->hasMorePolledMessages())\n+        {\n+            continue;\n+        }\n+        if (total_rows >= max_block_size || !checkTimeLimit())\n+        {\n             break;\n+        }\n     }\n \n-    if (total_rows == 0)\n+    if (buffer->rebalanceHappened() || total_rows == 0)\n         return Block();\n \n     /// MATERIALIZED columns can be added here, but I think\ndiff --git a/dbms/src/Storages/Kafka/KafkaBlockInputStream.h b/dbms/src/Storages/Kafka/KafkaBlockInputStream.h\nindex 5ab2df15ecd1..1c853a4d486b 100644\n--- a/dbms/src/Storages/Kafka/KafkaBlockInputStream.h\n+++ b/dbms/src/Storages/Kafka/KafkaBlockInputStream.h\n@@ -33,7 +33,8 @@ class KafkaBlockInputStream : public IBlockInputStream\n     UInt64 max_block_size;\n \n     ConsumerBufferPtr buffer;\n-    bool broken = true, claimed = false, commit_in_suffix;\n+    bool broken = true, finished = false, claimed = false, commit_in_suffix;\n+\n     const Block non_virtual_header, virtual_header;\n };\n \ndiff --git a/dbms/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp b/dbms/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp\nindex 44b8a119240f..10f5fd0b47af 100644\n--- a/dbms/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp\n+++ b/dbms/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp\n@@ -3,11 +3,14 @@\n #include <common/logger_useful.h>\n \n #include <cppkafka/cppkafka.h>\n+#include <boost/algorithm/string/join.hpp>\n \n namespace DB\n {\n \n using namespace std::chrono_literals;\n+const auto MAX_TIME_TO_WAIT_FOR_ASSIGNMENT_MS = 15000;\n+\n \n ReadBufferFromKafkaConsumer::ReadBufferFromKafkaConsumer(\n     ConsumerPtr consumer_,\n@@ -15,7 +18,8 @@ ReadBufferFromKafkaConsumer::ReadBufferFromKafkaConsumer(\n     size_t max_batch_size,\n     size_t poll_timeout_,\n     bool intermediate_commit_,\n-    const std::atomic<bool> & stopped_)\n+    const std::atomic<bool> & stopped_,\n+    const Names & _topics)\n     : ReadBuffer(nullptr, 0)\n     , consumer(consumer_)\n     , log(log_)\n@@ -24,7 +28,51 @@ ReadBufferFromKafkaConsumer::ReadBufferFromKafkaConsumer(\n     , intermediate_commit(intermediate_commit_)\n     , stopped(stopped_)\n     , current(messages.begin())\n+    , topics(_topics)\n {\n+    // called (synchroniously, during poll) when we enter the consumer group\n+    consumer->set_assignment_callback([this](const cppkafka::TopicPartitionList& topic_partitions)\n+    {\n+        LOG_TRACE(log, \"Topics/partitions assigned: \" << topic_partitions);\n+        assignment = topic_partitions;\n+    });\n+\n+    // called (synchroniously, during poll) when we leave the consumer group\n+    consumer->set_revocation_callback([this](const cppkafka::TopicPartitionList& topic_partitions)\n+    {\n+        // Rebalance is happening now, and now we have a chance to finish the work\n+        // with topics/partitions we were working with before rebalance\n+        LOG_TRACE(log, \"Rebalance initiated. Revoking partitions: \" << topic_partitions);\n+\n+        // we can not flush data to target from that point (it is pulled, not pushed)\n+        // so the best we can now it to\n+        // 1) repeat last commit in sync mode (async could be still in queue, we need to be sure is is properly committed before rebalance)\n+        // 2) stop / brake the current reading:\n+        //     * clean buffered non-commited messages\n+        //     * set flag / flush\n+\n+        messages.clear();\n+        current = messages.begin();\n+        BufferBase::set(nullptr, 0, 0);\n+\n+        rebalance_happened = true;\n+        assignment.clear();\n+\n+        // for now we use slower (but reliable) sync commit in main loop, so no need to repeat\n+        // try\n+        // {\n+        //     consumer->commit();\n+        // }\n+        // catch (cppkafka::HandleException & e)\n+        // {\n+        //     LOG_WARNING(log, \"Commit error: \" << e.what());\n+        // }\n+    });\n+\n+    consumer->set_rebalance_error_callback([this](cppkafka::Error err)\n+    {\n+        LOG_ERROR(log, \"Rebalance error: \" << err);\n+    });\n }\n \n ReadBufferFromKafkaConsumer::~ReadBufferFromKafkaConsumer()\n@@ -32,7 +80,7 @@ ReadBufferFromKafkaConsumer::~ReadBufferFromKafkaConsumer()\n     /// NOTE: see https://github.com/edenhill/librdkafka/issues/2077\n     consumer->unsubscribe();\n     consumer->unassign();\n-    while (consumer->get_consumer_queue().next_event(1s));\n+    while (consumer->get_consumer_queue().next_event(100ms));\n }\n \n void ReadBufferFromKafkaConsumer::commit()\n@@ -72,55 +120,60 @@ void ReadBufferFromKafkaConsumer::commit()\n \n     PrintOffsets(\"Polled offset\", consumer->get_offsets_position(consumer->get_assignment()));\n \n-    consumer->async_commit();\n+    if (hasMorePolledMessages())\n+    {\n+        LOG_WARNING(log,\"Logical error. Non all polled messages were processed.\");\n+    }\n+\n+    if (offsets_stored > 0)\n+    {\n+        // if we will do async commit here (which is faster)\n+        // we may need to repeat commit in sync mode in revocation callback,\n+        // but it seems like existing API doesn't allow us to to that\n+        // in a controlled manner (i.e. we don't know the offsets to commit then)\n+        consumer->commit();\n+    }\n+    else\n+    {\n+        LOG_TRACE(log,\"Nothing to commit.\");\n+    }\n \n     PrintOffsets(\"Committed offset\", consumer->get_offsets_committed(consumer->get_assignment()));\n+    offsets_stored = 0;\n \n     stalled = false;\n }\n \n-void ReadBufferFromKafkaConsumer::subscribe(const Names & topics)\n+void ReadBufferFromKafkaConsumer::subscribe()\n {\n-    {\n-        String message = \"Already subscribed to topics:\";\n-        for (const auto & topic : consumer->get_subscription())\n-            message += \" \" + topic;\n-        LOG_TRACE(log, message);\n-    }\n+    LOG_TRACE(log,\"Already subscribed to topics: [ \"\n+                    << boost::algorithm::join(consumer->get_subscription(), \", \")\n+                    << \" ]\");\n \n-    {\n-        String message = \"Already assigned to topics:\";\n-        for (const auto & toppar : consumer->get_assignment())\n-            message += \" \" + toppar.get_topic();\n-        LOG_TRACE(log, message);\n-    }\n+    LOG_TRACE(log, \"Already assigned to : \" << assignment);\n+\n+    size_t max_retries = 5;\n \n-    // While we wait for an assignment after subscribtion, we'll poll zero messages anyway.\n-    // If we're doing a manual select then it's better to get something after a wait, then immediate nothing.\n-    // But due to the nature of async pause/resume/subscribe we can't guarantee any persistent state:\n-    // see https://github.com/edenhill/librdkafka/issues/2455\n     while (consumer->get_subscription().empty())\n     {\n-        stalled = false;\n-\n+        --max_retries;\n         try\n         {\n             consumer->subscribe(topics);\n-            if (nextImpl())\n-                break;\n-\n             // FIXME: if we failed to receive \"subscribe\" response while polling and destroy consumer now, then we may hang up.\n             //        see https://github.com/edenhill/librdkafka/issues/2077\n         }\n         catch (cppkafka::HandleException & e)\n         {\n-            if (e.get_error() == RD_KAFKA_RESP_ERR__TIMED_OUT)\n+            if (max_retries > 0 && e.get_error() == RD_KAFKA_RESP_ERR__TIMED_OUT)\n                 continue;\n             throw;\n         }\n     }\n \n     stalled = false;\n+    rebalance_happened = false;\n+    offsets_stored = 0;\n }\n \n void ReadBufferFromKafkaConsumer::unsubscribe()\n@@ -134,13 +187,33 @@ void ReadBufferFromKafkaConsumer::unsubscribe()\n     consumer->unsubscribe();\n }\n \n+\n+bool ReadBufferFromKafkaConsumer::hasMorePolledMessages() const\n+{\n+    return (!stalled) && (current != messages.end());\n+}\n+\n+\n+void ReadBufferFromKafkaConsumer::resetToLastCommitted(const char * msg)\n+{\n+    if (assignment.empty())\n+    {\n+        LOG_TRACE(log, \"Not assignned. Can't reset to last committed position.\");\n+        return;\n+    }\n+    auto committed_offset = consumer->get_offsets_committed(consumer->get_assignment());\n+    consumer->assign(committed_offset);\n+    LOG_TRACE(log, msg << \"Returned to committed position: \" << committed_offset);\n+\n+}\n+\n /// Do commit messages implicitly after we processed the previous batch.\n bool ReadBufferFromKafkaConsumer::nextImpl()\n {\n     /// NOTE: ReadBuffer was implemented with an immutable underlying contents in mind.\n     ///       If we failed to poll any message once - don't try again.\n     ///       Otherwise, the |poll_timeout| expectations get flawn.\n-    if (stalled || stopped || !allowed)\n+    if (stalled || stopped || !allowed || rebalance_happened)\n         return false;\n \n     if (current == messages.end())\n@@ -148,18 +221,60 @@ bool ReadBufferFromKafkaConsumer::nextImpl()\n         if (intermediate_commit)\n             commit();\n \n-        /// Don't drop old messages immediately, since we may need them for virtual columns.\n-        auto new_messages = consumer->poll_batch(batch_size, std::chrono::milliseconds(poll_timeout));\n-        if (new_messages.empty())\n+        size_t waited_for_assignment = 0;\n+        while (1)\n         {\n-            LOG_TRACE(log, \"Stalled\");\n-            stalled = true;\n-            return false;\n-        }\n-        messages = std::move(new_messages);\n-        current = messages.begin();\n+            /// Don't drop old messages immediately, since we may need them for virtual columns.\n+            auto new_messages = consumer->poll_batch(batch_size, std::chrono::milliseconds(poll_timeout));\n \n-        LOG_TRACE(log, \"Polled batch of \" << messages.size() << \" messages\");\n+            if (rebalance_happened)\n+            {\n+                if (!new_messages.empty())\n+                {\n+                    // we have polled something just after rebalance.\n+                    // we will not use current batch, so we need to return to last commited position\n+                    // otherwise we will continue polling from that position\n+                    resetToLastCommitted(\"Rewind last poll after rebalance.\");\n+                }\n+\n+                offsets_stored = 0;\n+                return false;\n+            }\n+\n+            if (new_messages.empty())\n+            {\n+                // While we wait for an assignment after subscription, we'll poll zero messages anyway.\n+                // If we're doing a manual select then it's better to get something after a wait, then immediate nothing.\n+                if (assignment.empty())\n+                {\n+                    waited_for_assignment += poll_timeout; // slightly innaccurate, but rough calculation is ok.\n+                    if (waited_for_assignment < MAX_TIME_TO_WAIT_FOR_ASSIGNMENT_MS)\n+                    {\n+                        continue;\n+                    }\n+                    else\n+                    {\n+                        LOG_TRACE(log, \"Can't get assignment\");\n+                        stalled = true;\n+                        return false;\n+                    }\n+\n+                }\n+                else\n+                {\n+                    LOG_TRACE(log, \"Stalled\");\n+                    stalled = true;\n+                    return false;\n+                }\n+            }\n+            else\n+            {\n+                messages = std::move(new_messages);\n+                current = messages.begin();\n+                LOG_TRACE(log, \"Polled batch of \" << messages.size() << \" messages. Offset position: \" << consumer->get_offsets_position(consumer->get_assignment()));\n+                break;\n+            }\n+        }\n     }\n \n     if (auto err = current->get_error())\n@@ -176,12 +291,18 @@ bool ReadBufferFromKafkaConsumer::nextImpl()\n     BufferBase::set(new_position, current->get_payload().get_size(), 0);\n     allowed = false;\n \n-    /// Since we can poll more messages than we already processed - commit only processed messages.\n-    consumer->store_offset(*current);\n-\n     ++current;\n \n     return true;\n }\n \n+void ReadBufferFromKafkaConsumer::storeLastReadMessageOffset()\n+{\n+    if (!stalled && !rebalance_happened)\n+    {\n+        consumer->store_offset(*(current - 1));\n+        ++offsets_stored;\n+    }\n+}\n+\n }\ndiff --git a/dbms/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h b/dbms/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h\nindex d1ea961cef2b..700a69cf49bc 100644\n--- a/dbms/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h\n+++ b/dbms/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h\n@@ -25,16 +25,24 @@ class ReadBufferFromKafkaConsumer : public ReadBuffer\n         size_t max_batch_size,\n         size_t poll_timeout_,\n         bool intermediate_commit_,\n-        const std::atomic<bool> & stopped_);\n+        const std::atomic<bool> & stopped_,\n+        const Names & _topics\n+    );\n     ~ReadBufferFromKafkaConsumer() override;\n \n     void allowNext() { allowed = true; } // Allow to read next message.\n     void commit(); // Commit all processed messages.\n-    void subscribe(const Names & topics); // Subscribe internal consumer to topics.\n+    void subscribe(); // Subscribe internal consumer to topics.\n     void unsubscribe(); // Unsubscribe internal consumer in case of failure.\n \n     auto pollTimeout() const { return poll_timeout; }\n \n+    bool hasMorePolledMessages() const;\n+    auto rebalanceHappened() const { return rebalance_happened; }\n+\n+    void storeLastReadMessageOffset();\n+    void resetToLastCommitted(const char * msg);\n+\n     // Return values for the message that's being read.\n     String currentTopic() const { return current[-1].get_topic(); }\n     String currentKey() const { return current[-1].get_key(); }\n@@ -49,6 +57,7 @@ class ReadBufferFromKafkaConsumer : public ReadBuffer\n     Poco::Logger * log;\n     const size_t batch_size = 1;\n     const size_t poll_timeout = 0;\n+    size_t offsets_stored = 0;\n     bool stalled = false;\n     bool intermediate_commit = true;\n     bool allowed = true;\n@@ -58,6 +67,10 @@ class ReadBufferFromKafkaConsumer : public ReadBuffer\n     Messages messages;\n     Messages::const_iterator current;\n \n+    bool rebalance_happened = false;\n+    cppkafka::TopicPartitionList assignment;\n+    const Names topics;\n+\n     bool nextImpl() override;\n };\n \ndiff --git a/dbms/src/Storages/Kafka/StorageKafka.cpp b/dbms/src/Storages/Kafka/StorageKafka.cpp\nindex 1980637f4b6d..6b0bab72bb01 100644\n--- a/dbms/src/Storages/Kafka/StorageKafka.cpp\n+++ b/dbms/src/Storages/Kafka/StorageKafka.cpp\n@@ -272,7 +272,7 @@ ConsumerBufferPtr StorageKafka::createReadBuffer()\n     size_t poll_timeout = settings.stream_poll_timeout_ms.totalMilliseconds();\n \n     /// NOTE: we pass |stream_cancelled| by reference here, so the buffers should not outlive the storage.\n-    return std::make_shared<ReadBufferFromKafkaConsumer>(consumer, log, batch_size, poll_timeout, intermediate_commit, stream_cancelled);\n+    return std::make_shared<ReadBufferFromKafkaConsumer>(consumer, log, batch_size, poll_timeout, intermediate_commit, stream_cancelled, getTopics());\n }\n \n \n",
  "test_patch": "diff --git a/dbms/tests/integration/test_storage_kafka/configs/users.xml b/dbms/tests/integration/test_storage_kafka/configs/users.xml\nindex 8875fafe74af..246e6b069ef2 100644\n--- a/dbms/tests/integration/test_storage_kafka/configs/users.xml\n+++ b/dbms/tests/integration/test_storage_kafka/configs/users.xml\n@@ -2,7 +2,8 @@\n <yandex>\n     <profiles>\n         <default>\n-            <stream_poll_timeout_ms>30000</stream_poll_timeout_ms>\n+            <!--stream_poll_timeout_ms>1</stream_poll_timeout_ms>\n+            <stream_flush_interval_ms>100</stream_flush_interval_ms-->\n         </default>\n     </profiles>\n \ndiff --git a/dbms/tests/integration/test_storage_kafka/test.py b/dbms/tests/integration/test_storage_kafka/test.py\nindex cf438bf3c559..c37482ae18b7 100644\n--- a/dbms/tests/integration/test_storage_kafka/test.py\n+++ b/dbms/tests/integration/test_storage_kafka/test.py\n@@ -12,6 +12,7 @@\n import subprocess\n import kafka.errors\n from kafka import KafkaAdminClient, KafkaProducer, KafkaConsumer\n+from kafka.admin import NewTopic\n from google.protobuf.internal.encoder import _VarintBytes\n \n \"\"\"\n@@ -70,7 +71,7 @@ def kafka_produce(topic, messages, timestamp=None):\n     for message in messages:\n         producer.send(topic=topic, value=message, timestamp_ms=timestamp)\n         producer.flush()\n-    print (\"Produced {} messages for topic {}\".format(len(messages), topic))\n+#    print (\"Produced {} messages for topic {}\".format(len(messages), topic))\n \n \n def kafka_consume(topic):\n@@ -132,7 +133,7 @@ def kafka_setup_teardown():\n     wait_kafka_is_available()\n     print(\"kafka is available - running test\")\n     yield  # run test\n-    instance.query('DROP TABLE test.kafka')\n+    instance.query('DROP TABLE IF EXISTS test.kafka')\n \n \n # Tests\n@@ -727,6 +728,337 @@ def produce():\n     assert result == 1, 'Messages from kafka get duplicated!'\n \n \n+@pytest.mark.timeout(180)\n+def test_kafka_virtual_columns2(kafka_cluster):\n+\n+    admin_client = KafkaAdminClient(bootstrap_servers=\"localhost:9092\")\n+    topic_list = []\n+    topic_list.append(NewTopic(name=\"virt2_0\", num_partitions=2, replication_factor=1))\n+    topic_list.append(NewTopic(name=\"virt2_1\", num_partitions=2, replication_factor=1))\n+\n+    admin_client.create_topics(new_topics=topic_list, validate_only=False)\n+\n+    instance.query('''\n+        CREATE TABLE test.kafka (value UInt64)\n+            ENGINE = Kafka\n+            SETTINGS kafka_broker_list = 'kafka1:19092',\n+                     kafka_topic_list = 'virt2_0,virt2_1',\n+                     kafka_group_name = 'virt2',\n+                     kafka_format = 'JSONEachRow';\n+\n+        CREATE MATERIALIZED VIEW test.view Engine=Log AS\n+        SELECT value, _key, _topic, _partition, _offset, toUnixTimestamp(_timestamp) FROM test.kafka;\n+        ''')\n+\n+    producer = KafkaProducer(bootstrap_servers=\"localhost:9092\")\n+\n+    producer.send(topic='virt2_0', value=json.dumps({'value': 1}), partition=0, key='k1', timestamp_ms=1577836801000)\n+    producer.send(topic='virt2_0', value=json.dumps({'value': 2}), partition=0, key='k2', timestamp_ms=1577836802000)\n+    producer.flush()\n+    time.sleep(1)\n+\n+    producer.send(topic='virt2_0', value=json.dumps({'value': 3}), partition=1, key='k3', timestamp_ms=1577836803000)\n+    producer.send(topic='virt2_0', value=json.dumps({'value': 4}), partition=1, key='k4', timestamp_ms=1577836804000)\n+    producer.flush()\n+    time.sleep(1)\n+\n+    producer.send(topic='virt2_1', value=json.dumps({'value': 5}), partition=0, key='k5', timestamp_ms=1577836805000)\n+    producer.send(topic='virt2_1', value=json.dumps({'value': 6}), partition=0, key='k6', timestamp_ms=1577836806000)\n+    producer.flush()\n+    time.sleep(1)\n+\n+    producer.send(topic='virt2_1', value=json.dumps({'value': 7}), partition=1, key='k7', timestamp_ms=1577836807000)\n+    producer.send(topic='virt2_1', value=json.dumps({'value': 8}), partition=1, key='k8', timestamp_ms=1577836808000)\n+    producer.flush()\n+\n+    time.sleep(10)\n+\n+    result = instance.query(\"SELECT * FROM test.view ORDER BY value\", ignore_error=True)\n+\n+    expected = '''\\\n+1\tk1\tvirt2_0\t0\t0\t1577836801\n+2\tk2\tvirt2_0\t0\t1\t1577836802\n+3\tk3\tvirt2_0\t1\t0\t1577836803\n+4\tk4\tvirt2_0\t1\t1\t1577836804\n+5\tk5\tvirt2_1\t0\t0\t1577836805\n+6\tk6\tvirt2_1\t0\t1\t1577836806\n+7\tk7\tvirt2_1\t1\t0\t1577836807\n+8\tk8\tvirt2_1\t1\t1\t1577836808\n+'''\n+\n+    assert TSV(result) == TSV(expected)\n+\n+\n+@pytest.mark.timeout(600)\n+def test_kafka_flush_by_time(kafka_cluster):\n+    instance.query('''\n+        DROP TABLE IF EXISTS test.view;\n+        DROP TABLE IF EXISTS test.consumer;\n+\n+        CREATE TABLE test.kafka (key UInt64, value UInt64)\n+            ENGINE = Kafka\n+            SETTINGS kafka_broker_list = 'kafka1:19092',\n+                     kafka_topic_list = 'flush_by_time',\n+                     kafka_group_name = 'flush_by_time',\n+                     kafka_format = 'JSONEachRow',\n+                     kafka_max_block_size = 100,\n+                     kafka_row_delimiter = '\\\\n';\n+\n+        CREATE TABLE test.view (key UInt64, value UInt64)\n+            ENGINE = MergeTree()\n+            ORDER BY key;\n+\n+        CREATE MATERIALIZED VIEW test.consumer TO test.view AS\n+            SELECT * FROM test.kafka;\n+    ''')\n+\n+    cancel = threading.Event()\n+\n+    def produce():\n+        while not cancel.is_set():\n+            messages = []\n+            messages.append(json.dumps({'key': 0, 'value': 0}))\n+            kafka_produce('flush_by_time', messages)\n+            time.sleep(1)\n+\n+    kafka_thread = threading.Thread(target=produce)\n+    kafka_thread.start()\n+\n+    time.sleep(18)\n+\n+    result = instance.query('SELECT count() FROM test.view')\n+\n+    print(result)\n+    cancel.set()\n+    kafka_thread.join()\n+\n+    # kafka_cluster.open_bash_shell('instance')\n+\n+    instance.query('''\n+        DROP TABLE test.consumer;\n+        DROP TABLE test.view;\n+    ''')\n+\n+    # 40 = 2 flushes (7.5 sec), 15 polls each, about 1 mgs per 1.5 sec\n+    assert int(result) > 12, 'Messages from kafka should be flushed at least every stream_flush_interval_ms!'\n+\n+\n+@pytest.mark.timeout(600)\n+def test_kafka_flush_by_block_size(kafka_cluster):\n+    instance.query('''\n+        DROP TABLE IF EXISTS test.view;\n+        DROP TABLE IF EXISTS test.consumer;\n+\n+        CREATE TABLE test.kafka (key UInt64, value UInt64)\n+            ENGINE = Kafka\n+            SETTINGS kafka_broker_list = 'kafka1:19092',\n+                     kafka_topic_list = 'flush_by_block_size',\n+                     kafka_group_name = 'flush_by_block_size',\n+                     kafka_format = 'JSONEachRow',\n+                     kafka_max_block_size = 100,\n+                     kafka_row_delimiter = '\\\\n';\n+\n+        SELECT * FROM test.kafka;\n+\n+        CREATE TABLE test.view (key UInt64, value UInt64)\n+            ENGINE = MergeTree()\n+            ORDER BY key;\n+\n+        CREATE MATERIALIZED VIEW test.consumer TO test.view AS\n+            SELECT * FROM test.kafka;\n+    ''')\n+\n+    messages = []\n+    for _ in range(101):\n+        messages.append(json.dumps({'key': 0, 'value': 0}))\n+    kafka_produce('flush_by_block_size', messages)\n+\n+    time.sleep(1)\n+\n+    result = instance.query('SELECT count() FROM test.view')\n+    print(result)\n+\n+   # kafka_cluster.open_bash_shell('instance')\n+\n+    instance.query('''\n+        DROP TABLE test.consumer;\n+        DROP TABLE test.view;\n+    ''')\n+\n+    # 100 = first poll should return 100 messages (and rows)\n+    # not waiting for stream_flush_interval_ms\n+    assert int(result) == 100, 'Messages from kafka should be flushed at least every stream_flush_interval_ms!'\n+\n+\n+@pytest.mark.timeout(600)\n+def test_kafka_lot_of_partitions_partial_commit_of_bulk(kafka_cluster):\n+    admin_client = KafkaAdminClient(bootstrap_servers=\"localhost:9092\")\n+\n+    topic_list = []\n+    topic_list.append(NewTopic(name=\"topic_with_multiple_partitions2\", num_partitions=10, replication_factor=1))\n+    admin_client.create_topics(new_topics=topic_list, validate_only=False)\n+\n+    instance.query('''\n+        DROP TABLE IF EXISTS test.view;\n+        DROP TABLE IF EXISTS test.consumer;\n+        CREATE TABLE test.kafka (key UInt64, value UInt64)\n+            ENGINE = Kafka\n+            SETTINGS kafka_broker_list = 'kafka1:19092',\n+                     kafka_topic_list = 'topic_with_multiple_partitions2',\n+                     kafka_group_name = 'topic_with_multiple_partitions2',\n+                     kafka_format = 'JSONEachRow',\n+                     kafka_max_block_size = 211;\n+        CREATE TABLE test.view (key UInt64, value UInt64)\n+            ENGINE = MergeTree()\n+            ORDER BY key;\n+        CREATE MATERIALIZED VIEW test.consumer TO test.view AS\n+            SELECT * FROM test.kafka;\n+    ''')\n+\n+    messages = []\n+    count = 0\n+    for dummy_msg in range(1000):\n+        rows = []\n+        for dummy_row in range(random.randrange(3,10)):\n+            count = count + 1\n+            rows.append(json.dumps({'key': count, 'value': count}))\n+        messages.append(\"\\n\".join(rows))\n+    kafka_produce('topic_with_multiple_partitions2', messages)\n+\n+    time.sleep(30)\n+\n+    result = instance.query('SELECT count(), uniqExact(key), max(key) FROM test.view')\n+    print(result)\n+    assert TSV(result) == TSV('{0}\\t{0}\\t{0}'.format(count) )\n+\n+    instance.query('''\n+        DROP TABLE test.consumer;\n+        DROP TABLE test.view;\n+    ''')\n+\n+@pytest.mark.timeout(1200)\n+def test_kafka_rebalance(kafka_cluster):\n+\n+    NUMBER_OF_CONSURRENT_CONSUMERS=11\n+\n+    instance.query('''\n+        DROP TABLE IF EXISTS test.destination;\n+        CREATE TABLE test.destination (\n+            key UInt64,\n+            value UInt64,\n+            _topic String,\n+            _key String,\n+            _offset UInt64,\n+            _partition UInt64,\n+            _timestamp Nullable(DateTime),\n+            _consumed_by LowCardinality(String)\n+        )\n+        ENGINE = MergeTree()\n+        ORDER BY key;\n+    ''')\n+\n+   # kafka_cluster.open_bash_shell('instance')\n+\n+    #time.sleep(2)\n+\n+    admin_client = KafkaAdminClient(bootstrap_servers=\"localhost:9092\")\n+    topic_list = []\n+    topic_list.append(NewTopic(name=\"topic_with_multiple_partitions\", num_partitions=11, replication_factor=1))\n+    admin_client.create_topics(new_topics=topic_list, validate_only=False)\n+\n+    cancel = threading.Event()\n+\n+    msg_index = [0]\n+    def produce():\n+        while not cancel.is_set():\n+            messages = []\n+            for _ in range(59):\n+                messages.append(json.dumps({'key': msg_index[0], 'value': msg_index[0]}))\n+                msg_index[0] += 1\n+            kafka_produce('topic_with_multiple_partitions', messages)\n+\n+    kafka_thread = threading.Thread(target=produce)\n+    kafka_thread.start()\n+\n+    for consumer_index in range(NUMBER_OF_CONSURRENT_CONSUMERS):\n+        table_name = 'kafka_consumer{}'.format(consumer_index)\n+        print(\"Setting up {}\".format(table_name))\n+\n+        instance.query('''\n+            DROP TABLE IF EXISTS test.{0};\n+            DROP TABLE IF EXISTS test.{0}_mv;\n+            CREATE TABLE test.{0} (key UInt64, value UInt64)\n+                ENGINE = Kafka\n+                SETTINGS kafka_broker_list = 'kafka1:19092',\n+                        kafka_topic_list = 'topic_with_multiple_partitions',\n+                        kafka_group_name = 'rebalance_test_group',\n+                        kafka_format = 'JSONEachRow',\n+                        kafka_max_block_size = 33;\n+            CREATE MATERIALIZED VIEW test.{0}_mv TO test.destination AS\n+                SELECT\n+                key,\n+                value,\n+                _topic,\n+                _key,\n+                _offset,\n+                _partition,\n+                _timestamp,\n+                '{0}' as _consumed_by\n+            FROM test.{0};\n+        '''.format(table_name))\n+        # kafka_cluster.open_bash_shell('instance')\n+        while int(instance.query(\"SELECT count() FROM test.destination WHERE _consumed_by='{}'\".format(table_name))) == 0:\n+            print(\"Waiting for test.kafka_consumer{} to start consume\".format(consumer_index))\n+            time.sleep(1)\n+\n+    cancel.set()\n+\n+    # I leave last one working by intent (to finish consuming after all rebalances)\n+    for consumer_index in range(NUMBER_OF_CONSURRENT_CONSUMERS-1):\n+        print(\"Dropping test.kafka_consumer{}\".format(consumer_index))\n+        instance.query('DROP TABLE IF EXISTS test.kafka_consumer{}'.format(consumer_index))\n+        while int(instance.query(\"SELECT count() FROM system.tables WHERE database='test' AND name='kafka_consumer{}'\".format(consumer_index))) == 1:\n+            time.sleep(1)\n+\n+    # print(instance.query('SELECT count(), uniqExact(key), max(key) + 1 FROM test.destination'))\n+    # kafka_cluster.open_bash_shell('instance')\n+\n+    while 1:\n+        messages_consumed = int(instance.query('SELECT uniqExact(key) FROM test.destination'))\n+        if messages_consumed >= msg_index[0]:\n+            break\n+        time.sleep(1)\n+        print(\"Waiting for finishing consuming (have {}, should be {})\".format(messages_consumed,msg_index[0]))\n+\n+    print(instance.query('SELECT count(), uniqExact(key), max(key) + 1 FROM test.destination'))\n+\n+    # SELECT * FROM test.destination where key in (SELECT key FROM test.destination group by key having count() <> 1)\n+    # select number + 1 as key from numbers(4141) left join test.destination using (key) where  test.destination.key = 0;\n+    # SELECT * FROM test.destination WHERE key between 2360 and 2370 order by key;\n+    # select _partition from test.destination group by _partition having count() <> max(_offset) + 1;\n+    # select toUInt64(0) as _partition, number + 1 as _offset from numbers(400) left join test.destination using (_partition,_offset) where test.destination.key = 0 order by _offset;\n+    # SELECT * FROM test.destination WHERE _partition = 0 and _offset between 220 and 240 order by _offset;\n+\n+    result = int(instance.query('SELECT count() == uniqExact(key) FROM test.destination'))\n+\n+    for consumer_index in range(NUMBER_OF_CONSURRENT_CONSUMERS):\n+        print(\"kafka_consumer{}\".format(consumer_index))\n+        table_name = 'kafka_consumer{}'.format(consumer_index)\n+        instance.query('''\n+            DROP TABLE IF EXISTS test.{0};\n+            DROP TABLE IF EXISTS test.{0}_mv;\n+        '''.format(table_name))\n+\n+    instance.query('''\n+        DROP TABLE IF EXISTS test.destination;\n+    ''')\n+\n+    kafka_thread.join()\n+\n+    assert result == 1, 'Messages from kafka get duplicated!'\n+\n+\n+\n if __name__ == '__main__':\n     cluster.start()\n     raw_input(\"Cluster created, press any key to destroy...\")\n",
  "problem_statement": "Kafka: duplicates on consumer group rebalance \nhttps://gist.github.com/filimonov/3b0bfbb4a0bd8e3acaa1cbe145d104dd\n",
  "hints_text": "",
  "created_at": "2020-01-30T19:37:06Z",
  "modified_files": [
    "dbms/src/Storages/Kafka/KafkaBlockInputStream.cpp",
    "dbms/src/Storages/Kafka/KafkaBlockInputStream.h",
    "dbms/src/Storages/Kafka/ReadBufferFromKafkaConsumer.cpp",
    "dbms/src/Storages/Kafka/ReadBufferFromKafkaConsumer.h",
    "dbms/src/Storages/Kafka/StorageKafka.cpp"
  ],
  "modified_test_files": [
    "dbms/tests/integration/test_storage_kafka/configs/users.xml",
    "dbms/tests/integration/test_storage_kafka/test.py"
  ]
}