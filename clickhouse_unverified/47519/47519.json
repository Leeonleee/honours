{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 47519,
  "instance_id": "ClickHouse__ClickHouse-47519",
  "issue_numbers": [
    "47720",
    "47518"
  ],
  "base_commit": "8d3349ca93bec766bd200956974147859d95d3e8",
  "patch": "diff --git a/base/base/coverage.cpp b/base/base/coverage.cpp\nindex 043f97f95931..1027638be3db 100644\n--- a/base/base/coverage.cpp\n+++ b/base/base/coverage.cpp\n@@ -2,6 +2,8 @@\n \n #if WITH_COVERAGE\n \n+#pragma GCC diagnostic ignored \"-Wreserved-identifier\"\n+\n #    include <mutex>\n #    include <unistd.h>\n \ndiff --git a/contrib/croaring b/contrib/croaring\nindex 2c867e9f9c9e..f40ed52bcdd6 160000\n--- a/contrib/croaring\n+++ b/contrib/croaring\n@@ -1,1 +1,1 @@\n-Subproject commit 2c867e9f9c9e2a3a7032791f94c4c7ae3013f6e0\n+Subproject commit f40ed52bcdd635840a79877cef4857315dba817c\ndiff --git a/contrib/croaring-cmake/CMakeLists.txt b/contrib/croaring-cmake/CMakeLists.txt\nindex 0bb7d0bd221a..794c0426b969 100644\n--- a/contrib/croaring-cmake/CMakeLists.txt\n+++ b/contrib/croaring-cmake/CMakeLists.txt\n@@ -17,7 +17,8 @@ set(SRCS\n     \"${LIBRARY_DIR}/src/containers/run.c\"\n     \"${LIBRARY_DIR}/src/roaring.c\"\n     \"${LIBRARY_DIR}/src/roaring_priority_queue.c\"\n-    \"${LIBRARY_DIR}/src/roaring_array.c\")\n+    \"${LIBRARY_DIR}/src/roaring_array.c\"\n+    \"${LIBRARY_DIR}/src/memory.c\")\n \n add_library(_roaring ${SRCS})\n \ndiff --git a/docs/en/development/build.md b/docs/en/development/build.md\nindex d52b018a5a77..804aa8a3dc58 100644\n--- a/docs/en/development/build.md\n+++ b/docs/en/development/build.md\n@@ -159,4 +159,3 @@ The CI checks build the binaries on each commit to [ClickHouse](https://github.c\n 1. Find the type of package for your operating system that you need and download the files.\n \n ![build artifact check](images/find-build-artifact.png)\n-\ndiff --git a/src/AggregateFunctions/AggregateFunctionForEach.h b/src/AggregateFunctions/AggregateFunctionForEach.h\nindex f041dd11209e..81ba298bb8a6 100644\n--- a/src/AggregateFunctions/AggregateFunctionForEach.h\n+++ b/src/AggregateFunctions/AggregateFunctionForEach.h\n@@ -2,6 +2,7 @@\n \n #include <Columns/ColumnArray.h>\n #include <Common/assert_cast.h>\n+#include <base/arithmeticOverflow.h>\n #include <DataTypes/DataTypeArray.h>\n #include <AggregateFunctions/IAggregateFunction.h>\n \n@@ -20,6 +21,8 @@ namespace ErrorCodes\n     extern const int ILLEGAL_TYPE_OF_ARGUMENT;\n     extern const int NUMBER_OF_ARGUMENTS_DOESNT_MATCH;\n     extern const int SIZES_OF_ARRAYS_DONT_MATCH;\n+    extern const int TOO_LARGE_ARRAY_SIZE;\n+    extern const int LOGICAL_ERROR;\n }\n \n \n@@ -65,11 +68,17 @@ class AggregateFunctionForEach final : public IAggregateFunctionDataHelper<Aggre\n         size_t old_size = state.dynamic_array_size;\n         if (old_size < new_size)\n         {\n+            static constexpr size_t MAX_ARRAY_SIZE = 100_GiB;\n+            if (new_size > MAX_ARRAY_SIZE)\n+                throw Exception(ErrorCodes::TOO_LARGE_ARRAY_SIZE, \"Suspiciously large array size ({}) in -ForEach aggregate function\", new_size);\n+\n+            size_t allocation_size = 0;\n+            if (common::mulOverflow(new_size, nested_size_of_data, allocation_size))\n+                throw Exception(ErrorCodes::LOGICAL_ERROR, \"Allocation size ({} * {}) overflows in -ForEach aggregate function, but it should've been prevented by previous checks\", new_size, nested_size_of_data);\n+\n             char * old_state = state.array_of_aggregate_datas;\n \n-            char * new_state = arena.alignedAlloc(\n-                new_size * nested_size_of_data,\n-                nested_func->alignOfData());\n+            char * new_state = arena.alignedAlloc(allocation_size, nested_func->alignOfData());\n \n             size_t i;\n             try\ndiff --git a/src/AggregateFunctions/AggregateFunctionGroupArray.h b/src/AggregateFunctions/AggregateFunctionGroupArray.h\nindex eaffb04e2a98..5a799dc36413 100644\n--- a/src/AggregateFunctions/AggregateFunctionGroupArray.h\n+++ b/src/AggregateFunctions/AggregateFunctionGroupArray.h\n@@ -366,6 +366,8 @@ struct GroupArrayNodeBase\n     {\n         UInt64 size;\n         readVarUInt(size, buf);\n+        if (unlikely(size > AGGREGATE_FUNCTION_GROUP_ARRAY_MAX_ARRAY_SIZE))\n+            throw Exception(ErrorCodes::TOO_LARGE_ARRAY_SIZE, \"Too large array size\");\n \n         Node * node = reinterpret_cast<Node *>(arena->alignedAlloc(sizeof(Node) + size, alignof(Node)));\n         node->size = size;\ndiff --git a/src/AggregateFunctions/AggregateFunctionGroupBitmap.h b/src/AggregateFunctions/AggregateFunctionGroupBitmap.h\nindex 5fe3128fa205..a32bb330884b 100644\n--- a/src/AggregateFunctions/AggregateFunctionGroupBitmap.h\n+++ b/src/AggregateFunctions/AggregateFunctionGroupBitmap.h\n@@ -31,22 +31,28 @@ class AggregateFunctionBitmap final : public IAggregateFunctionDataHelper<Data,\n \n     void add(AggregateDataPtr __restrict place, const IColumn ** columns, size_t row_num, Arena *) const override\n     {\n-        this->data(place).rbs.add(assert_cast<const ColumnVector<T> &>(*columns[0]).getData()[row_num]);\n+        this->data(place).roaring_bitmap_with_small_set.add(assert_cast<const ColumnVector<T> &>(*columns[0]).getData()[row_num]);\n     }\n \n     void merge(AggregateDataPtr __restrict place, ConstAggregateDataPtr rhs, Arena *) const override\n     {\n-        this->data(place).rbs.merge(this->data(rhs).rbs);\n+        this->data(place).roaring_bitmap_with_small_set.merge(this->data(rhs).roaring_bitmap_with_small_set);\n     }\n \n-    void serialize(ConstAggregateDataPtr __restrict place, WriteBuffer & buf, std::optional<size_t> /* version */) const override { this->data(place).rbs.write(buf); }\n+    void serialize(ConstAggregateDataPtr __restrict place, WriteBuffer & buf, std::optional<size_t> /* version */) const override\n+    {\n+        this->data(place).roaring_bitmap_with_small_set.write(buf);\n+    }\n \n-    void deserialize(AggregateDataPtr __restrict place, ReadBuffer & buf, std::optional<size_t> /* version */, Arena *) const override { this->data(place).rbs.read(buf); }\n+    void deserialize(AggregateDataPtr __restrict place, ReadBuffer & buf, std::optional<size_t> /* version */, Arena *) const override\n+    {\n+        this->data(place).roaring_bitmap_with_small_set.read(buf);\n+    }\n \n     void insertResultInto(AggregateDataPtr __restrict place, IColumn & to, Arena *) const override\n     {\n         assert_cast<ColumnVector<T> &>(to).getData().push_back(\n-            static_cast<T>(this->data(place).rbs.size()));\n+            static_cast<T>(this->data(place).roaring_bitmap_with_small_set.size()));\n     }\n };\n \n@@ -81,7 +87,7 @@ class AggregateFunctionBitmapL2 final : public IAggregateFunctionDataHelper<Data\n         if (!data_lhs.init)\n         {\n             data_lhs.init = true;\n-            data_lhs.rbs.merge(data_rhs.rbs);\n+            data_lhs.roaring_bitmap_with_small_set.merge(data_rhs.roaring_bitmap_with_small_set);\n         }\n         else\n         {\n@@ -100,7 +106,7 @@ class AggregateFunctionBitmapL2 final : public IAggregateFunctionDataHelper<Data\n         if (!data_lhs.init)\n         {\n             data_lhs.init = true;\n-            data_lhs.rbs.merge(data_rhs.rbs);\n+            data_lhs.roaring_bitmap_with_small_set.merge(data_rhs.roaring_bitmap_with_small_set);\n         }\n         else\n         {\n@@ -128,7 +134,7 @@ class AggregateFunctionBitmapL2 final : public IAggregateFunctionDataHelper<Data\n         if (*version >= 1)\n             DB::writeBoolText(this->data(place).init, buf);\n \n-        this->data(place).rbs.write(buf);\n+        this->data(place).roaring_bitmap_with_small_set.write(buf);\n     }\n \n     void deserialize(AggregateDataPtr __restrict place, ReadBuffer & buf, std::optional<size_t> version, Arena *) const override\n@@ -138,13 +144,13 @@ class AggregateFunctionBitmapL2 final : public IAggregateFunctionDataHelper<Data\n \n         if (*version >= 1)\n             DB::readBoolText(this->data(place).init, buf);\n-        this->data(place).rbs.read(buf);\n+        this->data(place).roaring_bitmap_with_small_set.read(buf);\n     }\n \n     void insertResultInto(AggregateDataPtr __restrict place, IColumn & to, Arena *) const override\n     {\n         assert_cast<ColumnVector<T> &>(to).getData().push_back(\n-            static_cast<T>(this->data(place).rbs.size()));\n+            static_cast<T>(this->data(place).roaring_bitmap_with_small_set.size()));\n     }\n };\n \n@@ -154,7 +160,7 @@ class BitmapAndPolicy\n {\n public:\n     static constexpr auto name = \"groupBitmapAnd\";\n-    static void apply(Data & lhs, const Data & rhs) { lhs.rbs.rb_and(rhs.rbs); }\n+    static void apply(Data & lhs, const Data & rhs) { lhs.roaring_bitmap_with_small_set.rb_and(rhs.roaring_bitmap_with_small_set); }\n };\n \n template <typename Data>\n@@ -162,7 +168,7 @@ class BitmapOrPolicy\n {\n public:\n     static constexpr auto name = \"groupBitmapOr\";\n-    static void apply(Data & lhs, const Data & rhs) { lhs.rbs.rb_or(rhs.rbs); }\n+    static void apply(Data & lhs, const Data & rhs) { lhs.roaring_bitmap_with_small_set.rb_or(rhs.roaring_bitmap_with_small_set); }\n };\n \n template <typename Data>\n@@ -170,7 +176,7 @@ class BitmapXorPolicy\n {\n public:\n     static constexpr auto name = \"groupBitmapXor\";\n-    static void apply(Data & lhs, const Data & rhs) { lhs.rbs.rb_xor(rhs.rbs); }\n+    static void apply(Data & lhs, const Data & rhs) { lhs.roaring_bitmap_with_small_set.rb_xor(rhs.roaring_bitmap_with_small_set); }\n };\n \n template <typename T, typename Data>\ndiff --git a/src/AggregateFunctions/AggregateFunctionGroupBitmapData.h b/src/AggregateFunctions/AggregateFunctionGroupBitmapData.h\nindex 801526432ae1..62017251108d 100644\n--- a/src/AggregateFunctions/AggregateFunctionGroupBitmapData.h\n+++ b/src/AggregateFunctions/AggregateFunctionGroupBitmapData.h\n@@ -20,6 +20,12 @@\n namespace DB\n {\n \n+namespace ErrorCodes\n+{\n+    extern const int TOO_LARGE_ARRAY_SIZE;\n+    extern const int INCORRECT_DATA;\n+}\n+\n enum BitmapKind\n {\n     Small = 0,\n@@ -41,20 +47,19 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n     using ValueBuffer = std::vector<T>;\n     using RoaringBitmap = std::conditional_t<sizeof(T) >= 8, roaring::Roaring64Map, roaring::Roaring>;\n     using Value = std::conditional_t<sizeof(T) >= 8, UInt64, UInt32>;\n-    std::shared_ptr<RoaringBitmap> rb = nullptr;\n+    std::shared_ptr<RoaringBitmap> roaring_bitmap;\n \n     void toLarge()\n     {\n-        rb = std::make_shared<RoaringBitmap>();\n+        roaring_bitmap = std::make_shared<RoaringBitmap>();\n         for (const auto & x : small)\n-            rb->add(static_cast<Value>(x.getValue()));\n+            roaring_bitmap->add(static_cast<Value>(x.getValue()));\n         small.clear();\n     }\n \n public:\n-    bool isLarge() const { return rb != nullptr; }\n-\n-    bool isSmall() const { return rb == nullptr; }\n+    bool isLarge() const { return roaring_bitmap != nullptr; }\n+    bool isSmall() const { return roaring_bitmap == nullptr; }\n \n     void add(T value)\n     {\n@@ -63,17 +68,19 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n             if (small.find(value) == small.end())\n             {\n                 if (!small.full())\n+                {\n                     small.insert(value);\n+                }\n                 else\n                 {\n                     toLarge();\n-                    rb->add(static_cast<Value>(value));\n+                    roaring_bitmap->add(static_cast<Value>(value));\n                 }\n             }\n         }\n         else\n         {\n-            rb->add(static_cast<Value>(value));\n+            roaring_bitmap->add(static_cast<Value>(value));\n         }\n     }\n \n@@ -82,7 +89,7 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n         if (isSmall())\n             return small.size();\n         else\n-            return rb->cardinality();\n+            return roaring_bitmap->cardinality();\n     }\n \n     void merge(const RoaringBitmapWithSmallSet & r1)\n@@ -92,7 +99,7 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n             if (isSmall())\n                 toLarge();\n \n-            *rb |= *r1.rb;\n+            *roaring_bitmap |= *r1.roaring_bitmap;\n         }\n         else\n         {\n@@ -105,6 +112,7 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n     {\n         UInt8 kind;\n         readBinary(kind, in);\n+\n         if (BitmapKind::Small == kind)\n         {\n             small.read(in);\n@@ -113,26 +121,39 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n         {\n             size_t size;\n             readVarUInt(size, in);\n+\n+            static constexpr size_t max_size = 1_GiB;\n+\n+            if (size == 0)\n+                throw Exception(ErrorCodes::INCORRECT_DATA, \"Incorrect size (0) in groupBitmap.\");\n+            if (size > max_size)\n+                throw Exception(ErrorCodes::TOO_LARGE_ARRAY_SIZE, \"Too large array size in groupBitmap.\");\n+\n+            /// TODO: this is unnecessary copying - it will be better to read and deserialize in one pass.\n             std::unique_ptr<char[]> buf(new char[size]);\n             in.readStrict(buf.get(), size);\n-            rb = std::make_shared<RoaringBitmap>(RoaringBitmap::read(buf.get()));\n+\n+            roaring_bitmap = std::make_shared<RoaringBitmap>(RoaringBitmap::readSafe(buf.get(), size));\n         }\n+        else\n+            throw Exception(ErrorCodes::INCORRECT_DATA, \"Unknown type of roaring bitmap\");\n     }\n \n     void write(DB::WriteBuffer & out) const\n     {\n         UInt8 kind = isLarge() ? BitmapKind::Bitmap : BitmapKind::Small;\n         writeBinary(kind, out);\n+\n         if (BitmapKind::Small == kind)\n         {\n             small.write(out);\n         }\n         else if (BitmapKind::Bitmap == kind)\n         {\n-            auto size = rb->getSizeInBytes();\n+            auto size = roaring_bitmap->getSizeInBytes();\n             writeVarUInt(size, out);\n             std::unique_ptr<char[]> buf(new char[size]);\n-            rb->write(buf.get());\n+            roaring_bitmap->write(buf.get());\n             out.write(buf.get(), size);\n         }\n     }\n@@ -173,7 +194,7 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n         {\n             for (const auto & x : small)\n             {\n-                if (r1.rb->contains(static_cast<Value>(x.getValue())))\n+                if (r1.roaring_bitmap->contains(static_cast<Value>(x.getValue())))\n                     buffer.push_back(x.getValue());\n             }\n \n@@ -187,15 +208,18 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n         }\n         else\n         {\n-            std::shared_ptr<RoaringBitmap> new_rb = r1.isSmall() ? r1.getNewRoaringBitmapFromSmall() : r1.rb;\n-            *rb &= *new_rb;\n+            std::shared_ptr<RoaringBitmap> new_rb = r1.isSmall() ? r1.getNewRoaringBitmapFromSmall() : r1.roaring_bitmap;\n+            *roaring_bitmap &= *new_rb;\n         }\n     }\n \n     /**\n      * Computes the union between two bitmaps.\n      */\n-    void rb_or(const RoaringBitmapWithSmallSet & r1) { merge(r1); } /// NOLINT\n+    void rb_or(const RoaringBitmapWithSmallSet & r1)\n+    {\n+        merge(r1); /// NOLINT\n+    }\n \n     /**\n      * Computes the symmetric difference (xor) between two bitmaps.\n@@ -205,8 +229,8 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n         if (isSmall())\n             toLarge();\n \n-        std::shared_ptr<RoaringBitmap> new_rb = r1.isSmall() ? r1.getNewRoaringBitmapFromSmall() : r1.rb;\n-        *rb ^= *new_rb;\n+        std::shared_ptr<RoaringBitmap> new_rb = r1.isSmall() ? r1.getNewRoaringBitmapFromSmall() : r1.roaring_bitmap;\n+        *roaring_bitmap ^= *new_rb;\n     }\n \n     /**\n@@ -234,7 +258,7 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n         {\n             for (const auto & x : small)\n             {\n-                if (!r1.rb->contains(static_cast<Value>(x.getValue())))\n+                if (!r1.roaring_bitmap->contains(static_cast<Value>(x.getValue())))\n                     buffer.push_back(x.getValue());\n             }\n \n@@ -248,8 +272,8 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n         }\n         else\n         {\n-            std::shared_ptr<RoaringBitmap> new_rb = r1.isSmall() ? r1.getNewRoaringBitmapFromSmall() : r1.rb;\n-            *rb -= *new_rb;\n+            std::shared_ptr<RoaringBitmap> new_rb = r1.isSmall() ? r1.getNewRoaringBitmapFromSmall() : r1.roaring_bitmap;\n+            *roaring_bitmap -= *new_rb;\n         }\n     }\n \n@@ -269,14 +293,14 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n         {\n             for (const auto & x : small)\n             {\n-                if (r1.rb->contains(static_cast<Value>(x.getValue())))\n+                if (r1.roaring_bitmap->contains(static_cast<Value>(x.getValue())))\n                     ++ret;\n             }\n         }\n         else\n         {\n-            std::shared_ptr<RoaringBitmap> new_rb = r1.isSmall() ? r1.getNewRoaringBitmapFromSmall() : r1.rb;\n-            ret = (*rb & *new_rb).cardinality();\n+            std::shared_ptr<RoaringBitmap> new_rb = r1.isSmall() ? r1.getNewRoaringBitmapFromSmall() : r1.roaring_bitmap;\n+            ret = (*roaring_bitmap & *new_rb).cardinality();\n         }\n         return ret;\n     }\n@@ -321,8 +345,8 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n         if (isSmall())\n             toLarge();\n \n-        std::shared_ptr<RoaringBitmap> new_rb = r1.isSmall() ? r1.getNewRoaringBitmapFromSmall() : r1.rb;\n-        return *rb == *new_rb;\n+        std::shared_ptr<RoaringBitmap> new_rb = r1.isSmall() ? r1.getNewRoaringBitmapFromSmall() : r1.roaring_bitmap;\n+        return *roaring_bitmap == *new_rb;\n     }\n \n     /**\n@@ -343,7 +367,7 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n             {\n                 for (const auto & x : small)\n                 {\n-                    if (r1.rb->contains(static_cast<Value>(x.getValue())))\n+                    if (r1.roaring_bitmap->contains(static_cast<Value>(x.getValue())))\n                         return 1;\n                 }\n             }\n@@ -352,13 +376,13 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n         {\n             for (const auto & x : r1.small)\n             {\n-                if (rb->contains(static_cast<Value>(x.getValue())))\n+                if (roaring_bitmap->contains(static_cast<Value>(x.getValue())))\n                     return 1;\n             }\n         }\n         else\n         {\n-            if ((*rb & *r1.rb).cardinality() > 0)\n+            if ((*roaring_bitmap & *r1.roaring_bitmap).cardinality() > 0)\n                 return 1;\n         }\n \n@@ -396,7 +420,7 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n                 // greater then r1 is not a subset.\n                 for (const auto & x : small)\n                 {\n-                    if (!r1.rb->contains(static_cast<Value>(x.getValue())) && ++r1_size > small.size())\n+                    if (!r1.roaring_bitmap->contains(static_cast<Value>(x.getValue())) && ++r1_size > small.size())\n                         return 0;\n                 }\n             }\n@@ -405,13 +429,13 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n         {\n             for (const auto & x : r1.small)\n             {\n-                if (!rb->contains(static_cast<Value>(x.getValue())))\n+                if (!roaring_bitmap->contains(static_cast<Value>(x.getValue())))\n                     return 0;\n             }\n         }\n         else\n         {\n-            if (!r1.rb->isSubset(*rb))\n+            if (!r1.roaring_bitmap->isSubset(*roaring_bitmap))\n                 return 0;\n         }\n         return 1;\n@@ -428,46 +452,7 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n         if (isSmall())\n             return small.find(static_cast<T>(x)) != small.end();\n         else\n-            return rb->contains(static_cast<Value>(x));\n-    }\n-\n-    /**\n-     * Remove value\n-     */\n-    void rb_remove(UInt64 x) /// NOLINT\n-    {\n-        if (!std::is_same_v<T, UInt64> && x > rb_max())\n-            return;\n-\n-        if (isSmall())\n-            toLarge();\n-\n-        rb->remove(x);\n-    }\n-\n-    /**\n-     * compute (in place) the negation of the roaring bitmap within a specified\n-     * interval: [range_start, range_end). The number of negated values is\n-     * range_end - range_start.\n-     * Areas outside the range are passed through unchanged.\n-     */\n-    void rb_flip(UInt64 begin, UInt64 end) /// NOLINT\n-    {\n-        if (isSmall())\n-            toLarge();\n-\n-        rb->flip(begin, end);\n-    }\n-\n-    /**\n-     * returns the number of integers that are smaller or equal to offsetid.\n-     */\n-    UInt64 rb_rank(UInt64 x) /// NOLINT\n-    {\n-        if (isSmall())\n-            toLarge();\n-\n-        return rb->rank(x);\n+            return roaring_bitmap->contains(static_cast<Value>(x));\n     }\n \n     /**\n@@ -487,7 +472,7 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n         }\n         else\n         {\n-            for (auto it = rb->begin(); it != rb->end(); ++it)\n+            for (auto it = roaring_bitmap->begin(); it != roaring_bitmap->end(); ++it)\n             {\n                 res.emplace_back(*it);\n                 ++count;\n@@ -519,7 +504,7 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n         }\n         else\n         {\n-            for (auto it = rb->begin(); it != rb->end(); ++it)\n+            for (auto it = roaring_bitmap->begin(); it != roaring_bitmap->end(); ++it)\n             {\n                 if (*it < range_start)\n                     continue;\n@@ -569,7 +554,7 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n         else\n         {\n             UInt64 count = 0;\n-            for (auto it = rb->begin(); it != rb->end(); ++it)\n+            for (auto it = roaring_bitmap->begin(); it != roaring_bitmap->end(); ++it)\n             {\n                 if (*it < range_start)\n                     continue;\n@@ -607,11 +592,11 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n         {\n             UInt64 count = 0;\n             UInt64 offset_count = 0;\n-            auto it = rb->begin();\n-            for (;it != rb->end() && offset_count < offset; ++it)\n+            auto it = roaring_bitmap->begin();\n+            for (;it != roaring_bitmap->end() && offset_count < offset; ++it)\n                 ++offset_count;\n \n-            for (;it != rb->end() && count < limit; ++it, ++count)\n+            for (;it != roaring_bitmap->end() && count < limit; ++it, ++count)\n                 r1.add(*it);\n             return count;\n         }\n@@ -633,7 +618,7 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n             return min_val;\n         }\n         else\n-            return rb->minimum();\n+            return roaring_bitmap->minimum();\n     }\n \n     UInt64 rb_max() const /// NOLINT\n@@ -652,7 +637,7 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n             return max_val;\n         }\n         else\n-            return rb->maximum();\n+            return roaring_bitmap->maximum();\n     }\n \n     /**\n@@ -668,9 +653,9 @@ class RoaringBitmapWithSmallSet : private boost::noncopyable\n         {\n             if (from_vals[i] == to_vals[i])\n                 continue;\n-            bool changed = rb->removeChecked(static_cast<Value>(from_vals[i]));\n+            bool changed = roaring_bitmap->removeChecked(static_cast<Value>(from_vals[i]));\n             if (changed)\n-                rb->add(static_cast<Value>(to_vals[i]));\n+                roaring_bitmap->add(static_cast<Value>(to_vals[i]));\n         }\n     }\n };\n@@ -680,7 +665,7 @@ struct AggregateFunctionGroupBitmapData\n {\n     // If false, all bitmap operations will be treated as merge to initialize the state\n     bool init = false;\n-    RoaringBitmapWithSmallSet<T, 32> rbs;\n+    RoaringBitmapWithSmallSet<T, 32> roaring_bitmap_with_small_set;\n     static const char * name() { return \"groupBitmap\"; }\n };\n \ndiff --git a/src/AggregateFunctions/AggregateFunctionGroupUniqArray.h b/src/AggregateFunctions/AggregateFunctionGroupUniqArray.h\nindex f8e426363d84..bc7ccb082674 100644\n--- a/src/AggregateFunctions/AggregateFunctionGroupUniqArray.h\n+++ b/src/AggregateFunctions/AggregateFunctionGroupUniqArray.h\n@@ -181,7 +181,6 @@ class AggregateFunctionGroupUniqArrayGeneric\n         auto & set = this->data(place).value;\n         size_t size;\n         readVarUInt(size, buf);\n-        //TODO: set.reserve(size);\n \n         for (size_t i = 0; i < size; ++i)\n             set.insert(readStringBinaryInto(*arena, buf));\ndiff --git a/src/AggregateFunctions/AggregateFunctionHistogram.h b/src/AggregateFunctions/AggregateFunctionHistogram.h\nindex 35e5f241ec95..62ed071856a3 100644\n--- a/src/AggregateFunctions/AggregateFunctionHistogram.h\n+++ b/src/AggregateFunctions/AggregateFunctionHistogram.h\n@@ -292,6 +292,9 @@ class AggregateFunctionHistogramData\n         readVarUInt(size, buf);\n         if (size > max_bins * 2)\n             throw Exception(ErrorCodes::TOO_LARGE_ARRAY_SIZE, \"Too many bins\");\n+        static constexpr size_t max_size = 1_GiB;\n+        if (size > max_size)\n+            throw Exception(ErrorCodes::TOO_LARGE_ARRAY_SIZE, \"Too large array size in histogram.\");\n \n         buf.readStrict(reinterpret_cast<char *>(points), size * sizeof(WeightedValue));\n     }\ndiff --git a/src/AggregateFunctions/AggregateFunctionMap.h b/src/AggregateFunctions/AggregateFunctionMap.h\nindex 91530698bf4a..55f6611974e3 100644\n--- a/src/AggregateFunctions/AggregateFunctionMap.h\n+++ b/src/AggregateFunctions/AggregateFunctionMap.h\n@@ -61,15 +61,11 @@ struct AggregateFunctionMapCombinatorData<String>\n \n     static void writeKey(String key, WriteBuffer & buf)\n     {\n-        writeVarUInt(key.size(), buf);\n-        writeString(key, buf);\n+        writeStringBinary(key, buf);\n     }\n     static void readKey(String & key, ReadBuffer & buf)\n     {\n-        UInt64 size;\n-        readVarUInt(size, buf);\n-        key.resize(size);\n-        buf.readStrict(key.data(), size);\n+        readStringBinary(key, buf);\n     }\n };\n \ndiff --git a/src/AggregateFunctions/AggregateFunctionSequenceNextNode.h b/src/AggregateFunctions/AggregateFunctionSequenceNextNode.h\nindex 76610772b22a..4fd7db4160e5 100644\n--- a/src/AggregateFunctions/AggregateFunctionSequenceNextNode.h\n+++ b/src/AggregateFunctions/AggregateFunctionSequenceNextNode.h\n@@ -86,7 +86,7 @@ struct NodeBase\n     {\n         UInt64 size;\n         readVarUInt(size, buf);\n-        if unlikely (size > max_node_size_deserialize)\n+        if (unlikely(size > max_node_size_deserialize))\n             throw Exception(ErrorCodes::TOO_LARGE_ARRAY_SIZE, \"Too large node state size\");\n \n         Node * node = reinterpret_cast<Node *>(arena->alignedAlloc(sizeof(Node) + size, alignof(Node)));\n@@ -323,6 +323,9 @@ class SequenceNextNodeImpl final\n         if (unlikely(size == 0))\n             return;\n \n+        if (unlikely(size > max_node_size_deserialize))\n+            throw Exception(ErrorCodes::TOO_LARGE_ARRAY_SIZE, \"Too large array size\");\n+\n         auto & value = data(place).value;\n \n         value.resize(size, arena);\ndiff --git a/src/AggregateFunctions/CMakeLists.txt b/src/AggregateFunctions/CMakeLists.txt\nindex 0cb38fc729a6..a45adde1a36a 100644\n--- a/src/AggregateFunctions/CMakeLists.txt\n+++ b/src/AggregateFunctions/CMakeLists.txt\n@@ -28,3 +28,7 @@ target_link_libraries(clickhouse_aggregate_functions PRIVATE dbms PUBLIC ch_cont\n if(ENABLE_EXAMPLES)\n     add_subdirectory(examples)\n endif()\n+\n+if (ENABLE_FUZZING)\n+    add_subdirectory(fuzzers)\n+endif()\ndiff --git a/src/AggregateFunctions/QuantileExact.h b/src/AggregateFunctions/QuantileExact.h\nindex b7af17b52bf5..c67621a99cec 100644\n--- a/src/AggregateFunctions/QuantileExact.h\n+++ b/src/AggregateFunctions/QuantileExact.h\n@@ -8,6 +8,8 @@\n #include <base/sort.h>\n #include <base/types.h>\n \n+#define QUANTILE_EXACT_MAX_ARRAY_SIZE 1'000'000'000\n+\n \n namespace DB\n {\n@@ -17,6 +19,7 @@ namespace ErrorCodes\n {\n     extern const int NOT_IMPLEMENTED;\n     extern const int BAD_ARGUMENTS;\n+    extern const int TOO_LARGE_ARRAY_SIZE;\n }\n \n \n@@ -54,6 +57,8 @@ struct QuantileExactBase\n     {\n         size_t size = 0;\n         readVarUInt(size, buf);\n+        if (unlikely(size > QUANTILE_EXACT_MAX_ARRAY_SIZE))\n+            throw Exception(ErrorCodes::TOO_LARGE_ARRAY_SIZE, \"Too large array size\");\n         array.resize(size);\n         buf.readStrict(reinterpret_cast<char *>(array.data()), size * sizeof(array[0]));\n     }\ndiff --git a/src/AggregateFunctions/QuantileTiming.h b/src/AggregateFunctions/QuantileTiming.h\nindex 2c2e881c78f1..de6607b2527b 100644\n--- a/src/AggregateFunctions/QuantileTiming.h\n+++ b/src/AggregateFunctions/QuantileTiming.h\n@@ -16,6 +16,7 @@ struct Settings;\n namespace ErrorCodes\n {\n     extern const int LOGICAL_ERROR;\n+    extern const int INCORRECT_DATA;\n }\n \n /** Calculates quantile for time in milliseconds, less than 30 seconds.\n@@ -34,7 +35,7 @@ namespace ErrorCodes\n   * -- for values from 0 to 1023 - in increments of 1;\n   * -- for values from 1024 to 30,000 - in increments of 16;\n   *\n-  * NOTE: 64-bit integer weight can overflow, see also QantileExactWeighted.h::get()\n+  * NOTE: 64-bit integer weight can overflow, see also QuantileExactWeighted.h::get()\n   */\n \n #define TINY_MAX_ELEMS 31\n@@ -83,8 +84,12 @@ namespace detail\n \n         void deserialize(ReadBuffer & buf)\n         {\n-            readBinary(count, buf);\n-            buf.readStrict(reinterpret_cast<char *>(elems), count * sizeof(elems[0]));\n+            UInt16 new_count = 0;\n+            readBinary(new_count, buf);\n+            if (new_count > TINY_MAX_ELEMS)\n+                throw Exception(ErrorCodes::INCORRECT_DATA, \"The number of elements {} for the 'tiny' kind of quantileTiming is exceeding the maximum of {}\", new_count, TINY_MAX_ELEMS);\n+            buf.readStrict(reinterpret_cast<char *>(elems), new_count * sizeof(elems[0]));\n+            count = new_count;\n         }\n \n         /** This function must be called before get-functions. */\n@@ -167,6 +172,9 @@ namespace detail\n         {\n             size_t size = 0;\n             readBinary(size, buf);\n+            if (size > 10'000)\n+                throw Exception(ErrorCodes::INCORRECT_DATA, \"The number of elements {} for the 'medium' kind of quantileTiming is too large\", size);\n+\n             elems.resize(size);\n             buf.readStrict(reinterpret_cast<char *>(elems.data()), size * sizeof(elems[0]));\n         }\n@@ -714,6 +722,8 @@ class QuantileTiming : private boost::noncopyable\n             tinyToLarge();\n             large->deserialize(buf);\n         }\n+        else\n+            throw Exception(ErrorCodes::INCORRECT_DATA, \"Incorrect kind of QuantileTiming\");\n     }\n \n     /// Get the value of the `level` quantile. The level must be between 0 and 1.\ndiff --git a/src/AggregateFunctions/ReservoirSampler.h b/src/AggregateFunctions/ReservoirSampler.h\nindex b59f75b377e7..ef0e7c6566e3 100644\n--- a/src/AggregateFunctions/ReservoirSampler.h\n+++ b/src/AggregateFunctions/ReservoirSampler.h\n@@ -24,6 +24,7 @@ struct Settings;\n namespace ErrorCodes\n {\n     extern const int LOGICAL_ERROR;\n+    extern const int TOO_LARGE_ARRAY_SIZE;\n }\n }\n \n@@ -208,7 +209,13 @@ class ReservoirSampler\n     {\n         DB::readIntBinary<size_t>(sample_count, buf);\n         DB::readIntBinary<size_t>(total_values, buf);\n-        samples.resize(std::min(total_values, sample_count));\n+\n+        size_t size = std::min(total_values, sample_count);\n+        static constexpr size_t MAX_RESERVOIR_SIZE = 1_GiB;\n+        if (unlikely(size > MAX_RESERVOIR_SIZE))\n+            throw DB::Exception(DB::ErrorCodes::TOO_LARGE_ARRAY_SIZE, \"Too large array size\");\n+\n+        samples.resize(size);\n \n         std::string rng_string;\n         DB::readStringBinary(rng_string, buf);\ndiff --git a/src/AggregateFunctions/ReservoirSamplerDeterministic.h b/src/AggregateFunctions/ReservoirSamplerDeterministic.h\nindex 17e4ce0e4941..5e1d23ed2c2d 100644\n--- a/src/AggregateFunctions/ReservoirSamplerDeterministic.h\n+++ b/src/AggregateFunctions/ReservoirSamplerDeterministic.h\n@@ -22,6 +22,7 @@ struct Settings;\n namespace ErrorCodes\n {\n     extern const int LOGICAL_ERROR;\n+    extern const int TOO_LARGE_ARRAY_SIZE;\n }\n }\n \n@@ -163,6 +164,10 @@ class ReservoirSamplerDeterministic\n         if (size > total_values)\n             size = total_values;\n \n+        static constexpr size_t MAX_RESERVOIR_SIZE = 1_GiB;\n+        if (unlikely(size > MAX_RESERVOIR_SIZE))\n+            throw DB::Exception(DB::ErrorCodes::TOO_LARGE_ARRAY_SIZE, \"Too large array size\");\n+\n         samples.resize(size);\n         for (size_t i = 0; i < size; ++i)\n             DB::readPODBinary(samples[i], buf);\ndiff --git a/src/AggregateFunctions/fuzzers/CMakeLists.txt b/src/AggregateFunctions/fuzzers/CMakeLists.txt\nnew file mode 100644\nindex 000000000000..3876ffac7abf\n--- /dev/null\n+++ b/src/AggregateFunctions/fuzzers/CMakeLists.txt\n@@ -0,0 +1,2 @@\n+clickhouse_add_executable(aggregate_function_state_deserialization_fuzzer aggregate_function_state_deserialization_fuzzer.cpp ${SRCS})\n+target_link_libraries(aggregate_function_state_deserialization_fuzzer PRIVATE dbms clickhouse_aggregate_functions ${LIB_FUZZING_ENGINE})\ndiff --git a/src/AggregateFunctions/fuzzers/aggregate_function_state_deserialization_fuzzer.cpp b/src/AggregateFunctions/fuzzers/aggregate_function_state_deserialization_fuzzer.cpp\nnew file mode 100644\nindex 000000000000..39f57e00c483\n--- /dev/null\n+++ b/src/AggregateFunctions/fuzzers/aggregate_function_state_deserialization_fuzzer.cpp\n@@ -0,0 +1,82 @@\n+#include <base/types.h>\n+\n+#include <IO/ReadBufferFromMemory.h>\n+#include <IO/ReadHelpers.h>\n+\n+#include <DataTypes/DataTypeFactory.h>\n+#include <DataTypes/DataTypeAggregateFunction.h>\n+\n+#include <Common/MemoryTracker.h>\n+#include <Common/CurrentThread.h>\n+\n+#include <Interpreters/Context.h>\n+\n+#include <AggregateFunctions/registerAggregateFunctions.h>\n+\n+\n+extern \"C\" int LLVMFuzzerTestOneInput(const uint8_t * data, size_t size)\n+try\n+{\n+    using namespace DB;\n+\n+    static SharedContextHolder shared_context;\n+    static ContextMutablePtr context;\n+\n+    auto initialize = [&]() mutable\n+    {\n+        shared_context = Context::createShared();\n+        context = Context::createGlobal(shared_context.get());\n+        context->makeGlobalContext();\n+        context->setApplicationType(Context::ApplicationType::LOCAL);\n+\n+        MainThreadStatus::getInstance();\n+\n+        registerAggregateFunctions();\n+        return true;\n+    };\n+\n+    static bool initialized = initialize();\n+    (void) initialized;\n+\n+    total_memory_tracker.resetCounters();\n+    total_memory_tracker.setHardLimit(1_GiB);\n+    CurrentThread::get().memory_tracker.resetCounters();\n+    CurrentThread::get().memory_tracker.setHardLimit(1_GiB);\n+\n+    /// The input format is as follows:\n+    /// - the aggregate function name on the first line, possible with parameters, then data types of the arguments,\n+    ///   example: quantile(0.5), Float64\n+    /// - the serialized aggregation state for the rest of the input.\n+\n+    /// Compile the code as follows:\n+    ///   mkdir build_asan_fuzz\n+    ///   cd build_asan_fuzz\n+    ///   CC=clang CXX=clang++ cmake -D SANITIZE=address -D ENABLE_FUZZING=1 -D WITH_COVERAGE=1 ..\n+    ///\n+    /// The corpus is located here:\n+    /// https://github.com/ClickHouse/fuzz-corpus/tree/main/aggregate_function_state_deserialization\n+    ///\n+    /// The fuzzer can be run as follows:\n+    ///   ../../../build_asan_fuzz/src/DataTypes/fuzzers/aggregate_function_state_deserialization corpus -jobs=64 -rss_limit_mb=8192\n+\n+    DB::ReadBufferFromMemory in(data, size);\n+\n+    String args;\n+    readStringUntilNewlineInto(args, in);\n+    assertChar('\\n', in);\n+\n+    DataTypePtr type = DataTypeFactory::instance().get(fmt::format(\"AggregateFunction({})\", args));\n+    AggregateFunctionPtr func = assert_cast<const DataTypeAggregateFunction &>(*type).getFunction();\n+\n+    Arena arena;\n+    char * place = arena.alignedAlloc(func->sizeOfData(), func->alignOfData());\n+    func->create(place);\n+    SCOPE_EXIT(func->destroy(place));\n+    func->deserialize(place, in, {}, &arena);\n+\n+    return 0;\n+}\n+catch (...)\n+{\n+    return 1;\n+}\ndiff --git a/src/CMakeLists.txt b/src/CMakeLists.txt\nindex 6c5142813c50..edf638c53504 100644\n--- a/src/CMakeLists.txt\n+++ b/src/CMakeLists.txt\n@@ -75,6 +75,7 @@ add_subdirectory (Coordination)\n add_subdirectory (Bridge)\n add_subdirectory (Daemon)\n add_subdirectory (Loggers)\n+add_subdirectory (Formats)\n \n \n set(dbms_headers)\ndiff --git a/src/Common/HashTable/HashSet.h b/src/Common/HashTable/HashSet.h\nindex bac858b16a54..8f3761599ab0 100644\n--- a/src/Common/HashTable/HashSet.h\n+++ b/src/Common/HashTable/HashSet.h\n@@ -16,6 +16,7 @@ namespace DB\n namespace ErrorCodes\n {\n     extern const int LOGICAL_ERROR;\n+    extern const int TOO_LARGE_ARRAY_SIZE;\n }\n }\n \n@@ -60,6 +61,8 @@ class HashSetTable : public HashTable<Key, TCell, Hash, Grower, Allocator>\n \n         size_t new_size = 0;\n         DB::readVarUInt(new_size, rb);\n+        if (new_size > 100'000'000'000)\n+            throw DB::Exception(DB::ErrorCodes::TOO_LARGE_ARRAY_SIZE, \"The size of serialized hash table is suspiciously large: {}\", new_size);\n \n         this->resize(new_size);\n \ndiff --git a/src/Common/HashTable/HashTable.h b/src/Common/HashTable/HashTable.h\nindex 5c348f936d28..7ddcbc20b22e 100644\n--- a/src/Common/HashTable/HashTable.h\n+++ b/src/Common/HashTable/HashTable.h\n@@ -42,6 +42,7 @@ namespace ErrorCodes\n     extern const int LOGICAL_ERROR;\n     extern const int NO_AVAILABLE_DATA;\n     extern const int CANNOT_ALLOCATE_MEMORY;\n+    extern const int TOO_LARGE_ARRAY_SIZE;\n }\n }\n \n@@ -1318,6 +1319,8 @@ class HashTable : private boost::noncopyable,\n \n         size_t new_size = 0;\n         DB::readVarUInt(new_size, rb);\n+        if (new_size > 100'000'000'000)\n+            throw DB::Exception(DB::ErrorCodes::TOO_LARGE_ARRAY_SIZE, \"The size of serialized hash table is suspiciously large: {}\", new_size);\n \n         free();\n         Grower new_grower = grower;\ndiff --git a/src/Common/HashTable/SmallTable.h b/src/Common/HashTable/SmallTable.h\nindex f6253c3f0b95..3229e4748ea7 100644\n--- a/src/Common/HashTable/SmallTable.h\n+++ b/src/Common/HashTable/SmallTable.h\n@@ -9,6 +9,7 @@ namespace DB\n     {\n         extern const int NO_AVAILABLE_DATA;\n         extern const int INCORRECT_DATA;\n+        extern const int TOO_LARGE_ARRAY_SIZE;\n     }\n }\n \n@@ -279,6 +280,8 @@ class SmallTable :\n \n         size_t new_size = 0;\n         DB::readVarUInt(new_size, rb);\n+        if (new_size > 1000'000)\n+            throw DB::Exception(DB::ErrorCodes::TOO_LARGE_ARRAY_SIZE, \"The size of serialized small table is suspiciously large: {}\", new_size);\n \n         if (new_size > capacity)\n             throw DB::Exception(DB::ErrorCodes::INCORRECT_DATA, \"Illegal size\");\n@@ -346,4 +349,3 @@ template\n     size_t capacity\n >\n using SmallSet = SmallTable<Key, HashTableCell<Key, HashUnused>, capacity>;\n-\ndiff --git a/src/Common/SpaceSaving.h b/src/Common/SpaceSaving.h\nindex 84494e25ca5c..476e107067b2 100644\n--- a/src/Common/SpaceSaving.h\n+++ b/src/Common/SpaceSaving.h\n@@ -160,12 +160,11 @@ class SpaceSaving\n         // Key doesn't exist, but can fit in the top K\n         if (unlikely(size() < capacity()))\n         {\n-            auto * c = new Counter(arena.emplace(key), increment, error, hash);\n-            push(c);\n+            push(std::make_unique<Counter>(arena.emplace(key), increment, error, hash));\n             return;\n         }\n \n-        auto * min = counter_list.back();\n+        auto & min = counter_list.back();\n         // The key doesn't exist and cannot fit in the current top K, but\n         // the new key has a bigger weight and is virtually more present\n         // compared to the element who is less present on the set. This part\n@@ -173,7 +172,7 @@ class SpaceSaving\n         if (increment > min->count)\n         {\n             destroyLastElement();\n-            push(new Counter(arena.emplace(key), increment, error, hash));\n+            push(std::make_unique<Counter>(arena.emplace(key), increment, error, hash));\n             return;\n         }\n \n@@ -189,7 +188,7 @@ class SpaceSaving\n         alpha_map[min->hash & alpha_mask] = min->count;\n         destroyLastElement();\n \n-        push(new Counter(arena.emplace(key), alpha + increment, alpha + error, hash));\n+        push(std::make_unique<Counter>(arena.emplace(key), alpha + increment, alpha + error, hash));\n     }\n \n     /*\n@@ -219,7 +218,7 @@ class SpaceSaving\n          */\n         if (m2 > 0)\n         {\n-            for (auto * counter : counter_list)\n+            for (auto & counter : counter_list)\n             {\n                 counter->count += m2;\n                 counter->error += m2;\n@@ -227,7 +226,7 @@ class SpaceSaving\n         }\n \n         // The list is sorted in descending order, we have to scan in reverse\n-        for (auto * counter : boost::adaptors::reverse(rhs.counter_list))\n+        for (auto & counter : boost::adaptors::reverse(rhs.counter_list))\n         {\n             size_t hash = counter_map.hash(counter->key);\n             if (auto * current = findCounter(counter->key, hash))\n@@ -239,19 +238,16 @@ class SpaceSaving\n             else\n             {\n                 // Counters not monitored in S1\n-                counter_list.push_back(new Counter(arena.emplace(counter->key), counter->count + m1, counter->error + m1, hash));\n+                counter_list.push_back(std::make_unique<Counter>(arena.emplace(counter->key), counter->count + m1, counter->error + m1, hash));\n             }\n         }\n \n-        ::sort(counter_list.begin(), counter_list.end(), [](Counter * l, Counter * r) { return *l > *r; });\n+        ::sort(counter_list.begin(), counter_list.end(), [](const auto & l, const auto & r) { return *l > *r; });\n \n         if (counter_list.size() > m_capacity)\n         {\n             for (size_t i = m_capacity; i < counter_list.size(); ++i)\n-            {\n                 arena.free(counter_list[i]->key);\n-                delete counter_list[i];\n-            }\n             counter_list.resize(m_capacity);\n         }\n \n@@ -263,7 +259,7 @@ class SpaceSaving\n     std::vector<Counter> topK(size_t k) const\n     {\n         std::vector<Counter> res;\n-        for (auto * counter : counter_list)\n+        for (auto & counter : counter_list)\n         {\n             res.push_back(*counter);\n             if (res.size() == k)\n@@ -275,7 +271,7 @@ class SpaceSaving\n     void write(WriteBuffer & wb) const\n     {\n         writeVarUInt(size(), wb);\n-        for (auto * counter : counter_list)\n+        for (auto & counter : counter_list)\n             counter->write(wb);\n \n         writeVarUInt(alpha_map.size(), wb);\n@@ -291,10 +287,10 @@ class SpaceSaving\n \n         for (size_t i = 0; i < count; ++i)\n         {\n-            auto * counter = new Counter();\n+            std::unique_ptr counter = std::make_unique<Counter>();\n             counter->read(rb);\n             counter->hash = counter_map.hash(counter->key);\n-            push(counter);\n+            push(std::move(counter));\n         }\n \n         readAlphaMap(rb);\n@@ -313,12 +309,13 @@ class SpaceSaving\n     }\n \n protected:\n-    void push(Counter * counter)\n+    void push(std::unique_ptr<Counter> counter)\n     {\n         counter->slot = counter_list.size();\n-        counter_list.push_back(counter);\n-        counter_map[counter->key] = counter;\n-        percolate(counter);\n+        auto * ptr = counter.get();\n+        counter_list.push_back(std::move(counter));\n+        counter_map[ptr->key] = ptr;\n+        percolate(ptr);\n     }\n \n     // This is equivallent to one step of bubble sort\n@@ -326,7 +323,7 @@ class SpaceSaving\n     {\n         while (counter->slot > 0)\n         {\n-            auto * next = counter_list[counter->slot - 1];\n+            auto & next = counter_list[counter->slot - 1];\n             if (*counter > *next)\n             {\n                 std::swap(next->slot, counter->slot);\n@@ -340,11 +337,8 @@ class SpaceSaving\n private:\n     void destroyElements()\n     {\n-        for (auto * counter : counter_list)\n-        {\n+        for (auto & counter : counter_list)\n             arena.free(counter->key);\n-            delete counter;\n-        }\n \n         counter_map.clear();\n         counter_list.clear();\n@@ -353,10 +347,9 @@ class SpaceSaving\n \n     void destroyLastElement()\n     {\n-        auto last_element = counter_list.back();\n+        auto & last_element = counter_list.back();\n         counter_map.erase(last_element->key);\n         arena.free(last_element->key);\n-        delete last_element;\n         counter_list.pop_back();\n \n         ++removed_keys;\n@@ -377,14 +370,14 @@ class SpaceSaving\n     {\n         removed_keys = 0;\n         counter_map.clear();\n-        for (auto * counter : counter_list)\n-            counter_map[counter->key] = counter;\n+        for (auto & counter : counter_list)\n+            counter_map[counter->key] = counter.get();\n     }\n \n     using CounterMap = HashMapWithStackMemory<TKey, Counter *, Hash, 4>;\n \n     CounterMap counter_map;\n-    std::vector<Counter *, AllocatorWithMemoryTracking<Counter *>> counter_list;\n+    std::vector<std::unique_ptr<Counter>, AllocatorWithMemoryTracking<std::unique_ptr<Counter>>> counter_list;\n     std::vector<UInt64, AllocatorWithMemoryTracking<UInt64>> alpha_map;\n     SpaceSavingArena<TKey> arena;\n     size_t m_capacity;\ndiff --git a/src/Common/ThreadStatus.cpp b/src/Common/ThreadStatus.cpp\nindex 11f35bc7a6ba..aa1690890d8c 100644\n--- a/src/Common/ThreadStatus.cpp\n+++ b/src/Common/ThreadStatus.cpp\n@@ -6,8 +6,6 @@\n #include <Interpreters/Context.h>\n \n #include <Poco/Logger.h>\n-#include <base/getThreadId.h>\n-#include <base/getPageSize.h>\n \n #include <csignal>\n #include <sys/mman.h>\ndiff --git a/src/Core/ColumnWithTypeAndName.h b/src/Core/ColumnWithTypeAndName.h\nindex 592ad39d55ac..15807066e62d 100644\n--- a/src/Core/ColumnWithTypeAndName.h\n+++ b/src/Core/ColumnWithTypeAndName.h\n@@ -14,8 +14,6 @@ class WriteBuffer;\n   * Column data could be nullptr - to represent just 'header' of column.\n   * Name could be either name from a table or some temporary generated name during expression evaluation.\n   */\n-#pragma GCC diagnostic push\n-#pragma GCC diagnostic ignored \"-Wnull-dereference\"\n struct ColumnWithTypeAndName\n {\n     ColumnPtr column;\n@@ -37,6 +35,5 @@ struct ColumnWithTypeAndName\n     void dumpStructure(WriteBuffer & out) const;\n     String dumpStructure() const;\n };\n-#pragma GCC diagnostic pop\n \n }\ndiff --git a/src/Core/NamesAndTypes.h b/src/Core/NamesAndTypes.h\nindex 78535a751c3f..6cada7c8a694 100644\n--- a/src/Core/NamesAndTypes.h\n+++ b/src/Core/NamesAndTypes.h\n@@ -83,7 +83,6 @@ class NamesAndTypesList : public std::list<NameAndTypePair>\n     template <typename Iterator>\n     NamesAndTypesList(Iterator begin, Iterator end) : std::list<NameAndTypePair>(begin, end) {}\n \n-\n     void readText(ReadBuffer & buf);\n     void writeText(WriteBuffer & buf) const;\n \ndiff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex 755e52adb06c..3770d7f73a0e 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -844,6 +844,7 @@ class IColumn;\n     /** This setting is obsolete and do nothing, left for compatibility reasons. */ \\\n     M(Bool, input_format_avro_null_as_default, false, \"For Avro/AvroConfluent format: insert default in case of null and non Nullable column\", 0) \\\n     M(UInt64, format_binary_max_string_size, 1_GiB, \"The maximum allowed size for String in RowBinary format. It prevents allocating large amount of memory in case of corrupted data. 0 means there is no limit\", 0) \\\n+    M(UInt64, format_binary_max_array_size, 1_GiB, \"The maximum allowed size for Array in RowBinary format. It prevents allocating large amount of memory in case of corrupted data. 0 means there is no limit\", 0) \\\n     M(URI, format_avro_schema_registry_url, \"\", \"For AvroConfluent format: Confluent Schema Registry URL.\", 0) \\\n     \\\n     M(Bool, output_format_json_quote_64bit_integers, true, \"Controls quoting of 64-bit integers in JSON output format.\", 0) \\\ndiff --git a/src/DataTypes/CMakeLists.txt b/src/DataTypes/CMakeLists.txt\nindex 4a60d6c54cff..2902ef0a9557 100644\n--- a/src/DataTypes/CMakeLists.txt\n+++ b/src/DataTypes/CMakeLists.txt\n@@ -3,3 +3,7 @@ add_subdirectory (Serializations)\n if (ENABLE_EXAMPLES)\n     add_subdirectory (examples)\n endif ()\n+\n+if (ENABLE_FUZZING)\n+    add_subdirectory(fuzzers)\n+endif()\ndiff --git a/src/DataTypes/DataTypeFactory.cpp b/src/DataTypes/DataTypeFactory.cpp\nindex 93db16541883..415f24d8151f 100644\n--- a/src/DataTypes/DataTypeFactory.cpp\n+++ b/src/DataTypes/DataTypeFactory.cpp\n@@ -217,10 +217,12 @@ void DataTypeFactory::registerDataTypeCustom(const String & family_name, Creator\n     }, case_sensitiveness);\n }\n \n-void DataTypeFactory::registerSimpleDataTypeCustom(const String &name, SimpleCreatorWithCustom creator, CaseSensitiveness case_sensitiveness)\n+void DataTypeFactory::registerSimpleDataTypeCustom(const String & name, SimpleCreatorWithCustom creator, CaseSensitiveness case_sensitiveness)\n {\n-    registerDataTypeCustom(name, [creator](const ASTPtr & /*ast*/)\n+    registerDataTypeCustom(name, [name, creator](const ASTPtr & ast)\n     {\n+        if (ast)\n+            throw Exception(ErrorCodes::DATA_TYPE_CANNOT_HAVE_ARGUMENTS, \"Data type {} cannot have arguments\", name);\n         return creator();\n     }, case_sensitiveness);\n }\ndiff --git a/src/DataTypes/Serializations/SerializationArray.cpp b/src/DataTypes/Serializations/SerializationArray.cpp\nindex 73b232690c7e..5ed85cd1d1a4 100644\n--- a/src/DataTypes/Serializations/SerializationArray.cpp\n+++ b/src/DataTypes/Serializations/SerializationArray.cpp\n@@ -43,6 +43,14 @@ void SerializationArray::deserializeBinary(Field & field, ReadBuffer & istr, con\n {\n     size_t size;\n     readVarUInt(size, istr);\n+    if (settings.max_binary_array_size && size > settings.max_binary_array_size)\n+        throw Exception(\n+            ErrorCodes::TOO_LARGE_ARRAY_SIZE,\n+            \"Too large array size: {}. The maximum is: {}. To increase the maximum, use setting \"\n+            \"format_binary_max_array_size\",\n+            size,\n+            settings.max_binary_array_size);\n+\n     field = Array();\n     Array & arr = field.get<Array &>();\n     arr.reserve(size);\n@@ -75,6 +83,13 @@ void SerializationArray::deserializeBinary(IColumn & column, ReadBuffer & istr,\n \n     size_t size;\n     readVarUInt(size, istr);\n+    if (settings.max_binary_array_size && size > settings.max_binary_array_size)\n+        throw Exception(\n+            ErrorCodes::TOO_LARGE_ARRAY_SIZE,\n+            \"Too large array size: {}. The maximum is: {}. To increase the maximum, use setting \"\n+            \"format_binary_max_array_size\",\n+            size,\n+            settings.max_binary_array_size);\n \n     IColumn & nested_column = column_array.getData();\n \ndiff --git a/src/DataTypes/Serializations/SerializationMap.cpp b/src/DataTypes/Serializations/SerializationMap.cpp\nindex 34da0f11cae6..a176d4c07197 100644\n--- a/src/DataTypes/Serializations/SerializationMap.cpp\n+++ b/src/DataTypes/Serializations/SerializationMap.cpp\n@@ -20,6 +20,7 @@ namespace DB\n namespace ErrorCodes\n {\n     extern const int CANNOT_READ_MAP_FROM_TEXT;\n+    extern const int TOO_LARGE_ARRAY_SIZE;\n }\n \n SerializationMap::SerializationMap(const SerializationPtr & key_, const SerializationPtr & value_, const SerializationPtr & nested_)\n@@ -54,6 +55,13 @@ void SerializationMap::deserializeBinary(Field & field, ReadBuffer & istr, const\n {\n     size_t size;\n     readVarUInt(size, istr);\n+    if (settings.max_binary_array_size && size > settings.max_binary_array_size)\n+        throw Exception(\n+            ErrorCodes::TOO_LARGE_ARRAY_SIZE,\n+            \"Too large map size: {}. The maximum is: {}. To increase the maximum, use setting \"\n+            \"format_binary_max_array_size\",\n+            size,\n+            settings.max_binary_array_size);\n     field = Map();\n     Map & map = field.get<Map &>();\n     map.reserve(size);\ndiff --git a/src/DataTypes/Serializations/SerializationString.cpp b/src/DataTypes/Serializations/SerializationString.cpp\nindex 96608fbbc041..46fd9d5272d2 100644\n--- a/src/DataTypes/Serializations/SerializationString.cpp\n+++ b/src/DataTypes/Serializations/SerializationString.cpp\n@@ -14,6 +14,8 @@\n #include <IO/VarInt.h>\n #include <IO/ReadBufferFromString.h>\n \n+#include <base/unit.h>\n+\n #ifdef __SSE2__\n     #include <emmintrin.h>\n #endif\n@@ -158,6 +160,14 @@ static NO_INLINE void deserializeBinarySSE2(ColumnString::Chars & data, ColumnSt\n         UInt64 size;\n         readVarUInt(size, istr);\n \n+        static constexpr size_t max_string_size = 16_GiB;   /// Arbitrary value to prevent logical errors and overflows, but large enough.\n+        if (size > max_string_size)\n+            throw Exception(\n+                ErrorCodes::TOO_LARGE_STRING_SIZE,\n+                \"Too large string size: {}. The maximum is: {}.\",\n+                size,\n+                max_string_size);\n+\n         offset += size + 1;\n         offsets.push_back(offset);\n \ndiff --git a/src/DataTypes/fuzzers/CMakeLists.txt b/src/DataTypes/fuzzers/CMakeLists.txt\nnew file mode 100644\nindex 000000000000..d9c19cb7d012\n--- /dev/null\n+++ b/src/DataTypes/fuzzers/CMakeLists.txt\n@@ -0,0 +1,2 @@\n+clickhouse_add_executable(data_type_deserialization_fuzzer data_type_deserialization_fuzzer.cpp ${SRCS})\n+target_link_libraries(data_type_deserialization_fuzzer PRIVATE dbms clickhouse_aggregate_functions ${LIB_FUZZING_ENGINE})\ndiff --git a/src/DataTypes/fuzzers/data_type_deserialization_fuzzer.cpp b/src/DataTypes/fuzzers/data_type_deserialization_fuzzer.cpp\nnew file mode 100644\nindex 000000000000..31e4c470ee7e\n--- /dev/null\n+++ b/src/DataTypes/fuzzers/data_type_deserialization_fuzzer.cpp\n@@ -0,0 +1,82 @@\n+#include <base/types.h>\n+\n+#include <IO/ReadBufferFromMemory.h>\n+#include <IO/ReadHelpers.h>\n+\n+#include <DataTypes/DataTypeFactory.h>\n+\n+#include <Common/MemoryTracker.h>\n+#include <Common/CurrentThread.h>\n+\n+#include <Interpreters/Context.h>\n+\n+#include <AggregateFunctions/registerAggregateFunctions.h>\n+\n+\n+extern \"C\" int LLVMFuzzerTestOneInput(const uint8_t * data, size_t size)\n+try\n+{\n+    using namespace DB;\n+\n+    static SharedContextHolder shared_context;\n+    static ContextMutablePtr context;\n+\n+    auto initialize = [&]() mutable\n+    {\n+        shared_context = Context::createShared();\n+        context = Context::createGlobal(shared_context.get());\n+        context->makeGlobalContext();\n+        context->setApplicationType(Context::ApplicationType::LOCAL);\n+\n+        MainThreadStatus::getInstance();\n+\n+        registerAggregateFunctions();\n+        return true;\n+    };\n+\n+    static bool initialized = initialize();\n+    (void) initialized;\n+\n+    total_memory_tracker.resetCounters();\n+    total_memory_tracker.setHardLimit(1_GiB);\n+    CurrentThread::get().memory_tracker.resetCounters();\n+    CurrentThread::get().memory_tracker.setHardLimit(1_GiB);\n+\n+    /// The input format is as follows:\n+    /// - data type name on the first line,\n+    /// - the data for the rest of the input.\n+\n+    /// Compile the code as follows:\n+    ///   mkdir build_asan_fuzz\n+    ///   cd build_asan_fuzz\n+    ///   CC=clang CXX=clang++ cmake -D SANITIZE=address -D ENABLE_FUZZING=1 -D WITH_COVERAGE=1 ..\n+    ///\n+    /// The corpus is located here:\n+    /// https://github.com/ClickHouse/fuzz-corpus/tree/main/data_type_deserialization\n+    ///\n+    /// The fuzzer can be run as follows:\n+    ///   ../../../build_asan_fuzz/src/DataTypes/fuzzers/data_type_deserialization_fuzzer corpus -jobs=64 -rss_limit_mb=8192\n+\n+    /// clickhouse-local --query \"SELECT toJSONString(*) FROM (SELECT name FROM system.functions UNION ALL SELECT name FROM system.data_type_families)\" > dictionary\n+\n+    DB::ReadBufferFromMemory in(data, size);\n+\n+    String data_type;\n+    readStringUntilNewlineInto(data_type, in);\n+    assertChar('\\n', in);\n+\n+    DataTypePtr type = DataTypeFactory::instance().get(data_type);\n+\n+    FormatSettings settings;\n+    settings.max_binary_string_size = 100;\n+    settings.max_binary_array_size = 100;\n+\n+    Field field;\n+    type->getDefaultSerialization()->deserializeBinary(field, in, settings);\n+\n+    return 0;\n+}\n+catch (...)\n+{\n+    return 1;\n+}\ndiff --git a/src/Formats/CMakeLists.txt b/src/Formats/CMakeLists.txt\nnew file mode 100644\nindex 000000000000..316043093c06\n--- /dev/null\n+++ b/src/Formats/CMakeLists.txt\n@@ -0,0 +1,3 @@\n+if (ENABLE_FUZZING)\n+    add_subdirectory(fuzzers)\n+endif()\ndiff --git a/src/Formats/FormatFactory.cpp b/src/Formats/FormatFactory.cpp\nindex a951a7fdd928..c6d85a5d84f2 100644\n--- a/src/Formats/FormatFactory.cpp\n+++ b/src/Formats/FormatFactory.cpp\n@@ -194,6 +194,7 @@ FormatSettings getFormatSettings(ContextPtr context, const Settings & settings)\n     format_settings.bson.output_string_as_string = settings.output_format_bson_string_as_string;\n     format_settings.bson.skip_fields_with_unsupported_types_in_schema_inference = settings.input_format_bson_skip_fields_with_unsupported_types_in_schema_inference;\n     format_settings.max_binary_string_size = settings.format_binary_max_string_size;\n+    format_settings.max_binary_array_size = settings.format_binary_max_array_size;\n     format_settings.native.allow_types_conversion = settings.input_format_native_allow_types_conversion;\n     format_settings.max_parser_depth = context->getSettingsRef().max_parser_depth;\n     format_settings.client_protocol_version = context->getClientProtocolVersion();\n@@ -226,9 +227,7 @@ InputFormatPtr FormatFactory::getInput(\n         ? *_format_settings : getFormatSettings(context);\n \n     if (!getCreators(name).input_creator)\n-    {\n         throw Exception(ErrorCodes::FORMAT_IS_NOT_SUITABLE_FOR_INPUT, \"Format {} is not suitable for input\", name);\n-    }\n \n     const Settings & settings = context->getSettingsRef();\n     const auto & file_segmentation_engine = getCreators(name).file_segmentation_engine;\n@@ -270,21 +269,19 @@ InputFormatPtr FormatFactory::getInput(\n         ParallelParsingInputFormat::Params params{\n             buf, sample, parser_creator, file_segmentation_engine, name, settings.max_threads,\n             settings.min_chunk_bytes_for_parallel_parsing, max_block_size, context->getApplicationType() == Context::ApplicationType::SERVER};\n+\n         auto format = std::make_shared<ParallelParsingInputFormat>(params);\n         if (!settings.input_format_record_errors_file_path.toString().empty())\n-        {\n             format->setErrorsLogger(std::make_shared<ParallelInputFormatErrorsLogger>(context));\n-        }\n         return format;\n     }\n-\n-\n-    auto format = getInputFormat(name, buf, sample, context, max_block_size, format_settings);\n-    if (!settings.input_format_record_errors_file_path.toString().empty())\n+    else\n     {\n-        format->setErrorsLogger(std::make_shared<InputFormatErrorsLogger>(context));\n+        auto format = getInputFormat(name, buf, sample, context, max_block_size, format_settings);\n+        if (!settings.input_format_record_errors_file_path.toString().empty())\n+             format->setErrorsLogger(std::make_shared<InputFormatErrorsLogger>(context));\n+        return format;\n     }\n-    return format;\n }\n \n InputFormatPtr FormatFactory::getInputFormat(\ndiff --git a/src/Formats/FormatFactory.h b/src/Formats/FormatFactory.h\nindex 398548e4b220..6697a22984c4 100644\n--- a/src/Formats/FormatFactory.h\n+++ b/src/Formats/FormatFactory.h\n@@ -15,6 +15,7 @@\n #include <memory>\n #include <unordered_map>\n \n+\n namespace DB\n {\n \ndiff --git a/src/Formats/FormatSettings.h b/src/Formats/FormatSettings.h\nindex 7be7b5b98aa9..053b461244b2 100644\n--- a/src/Formats/FormatSettings.h\n+++ b/src/Formats/FormatSettings.h\n@@ -3,6 +3,7 @@\n #include <Core/Names.h>\n #include <Core/Defines.h>\n #include <base/types.h>\n+#include <base/unit.h>\n \n \n namespace DB\n@@ -81,7 +82,8 @@ struct FormatSettings\n     UInt64 input_allow_errors_num = 0;\n     Float32 input_allow_errors_ratio = 0;\n \n-    UInt64 max_binary_string_size = 0;\n+    UInt64 max_binary_string_size = 1_GiB;\n+    UInt64 max_binary_array_size = 1_GiB;\n     UInt64 client_protocol_version = 0;\n \n     UInt64 max_parser_depth = DBMS_DEFAULT_MAX_PARSER_DEPTH;\ndiff --git a/src/Formats/NativeReader.cpp b/src/Formats/NativeReader.cpp\nindex eca88a41c132..ff75ffb3c027 100644\n--- a/src/Formats/NativeReader.cpp\n+++ b/src/Formats/NativeReader.cpp\n@@ -27,6 +27,7 @@ namespace ErrorCodes\n     extern const int LOGICAL_ERROR;\n     extern const int CANNOT_READ_ALL_DATA;\n     extern const int INCORRECT_DATA;\n+    extern const int TOO_LARGE_ARRAY_SIZE;\n }\n \n \n@@ -136,6 +137,11 @@ Block NativeReader::read()\n     {\n         readVarUInt(columns, istr);\n         readVarUInt(rows, istr);\n+\n+        if (columns > 1'000'000uz)\n+            throw Exception(ErrorCodes::TOO_LARGE_ARRAY_SIZE, \"Suspiciously many columns in Native format: {}\", columns);\n+        if (rows > 1'000'000'000'000uz)\n+            throw Exception(ErrorCodes::TOO_LARGE_ARRAY_SIZE, \"Suspiciously many rows in Native format: {}\", rows);\n     }\n     else\n     {\ndiff --git a/src/Formats/fuzzers/CMakeLists.txt b/src/Formats/fuzzers/CMakeLists.txt\nnew file mode 100644\nindex 000000000000..984823f33601\n--- /dev/null\n+++ b/src/Formats/fuzzers/CMakeLists.txt\n@@ -0,0 +1,2 @@\n+clickhouse_add_executable(format_fuzzer format_fuzzer.cpp ${SRCS})\n+target_link_libraries(format_fuzzer PRIVATE dbms clickhouse_aggregate_functions ${LIB_FUZZING_ENGINE})\ndiff --git a/src/Formats/fuzzers/format_fuzzer.cpp b/src/Formats/fuzzers/format_fuzzer.cpp\nnew file mode 100644\nindex 000000000000..e84d0913d0da\n--- /dev/null\n+++ b/src/Formats/fuzzers/format_fuzzer.cpp\n@@ -0,0 +1,133 @@\n+#include <base/types.h>\n+\n+#include <IO/ReadBufferFromMemory.h>\n+#include <IO/ReadHelpers.h>\n+\n+#include <Formats/FormatFactory.h>\n+#include <Formats/registerFormats.h>\n+\n+#include <QueryPipeline/Pipe.h>\n+#include <QueryPipeline/QueryPipeline.h>\n+\n+#include <Processors/Formats/IInputFormat.h>\n+#include <Processors/Executors/PullingPipelineExecutor.h>\n+\n+#include <Common/MemoryTracker.h>\n+#include <Common/CurrentThread.h>\n+\n+#include <Interpreters/Context.h>\n+#include <Interpreters/parseColumnsListForTableFunction.h>\n+\n+#include <AggregateFunctions/registerAggregateFunctions.h>\n+\n+\n+extern \"C\" int LLVMFuzzerTestOneInput(const uint8_t * data, size_t size)\n+try\n+{\n+    using namespace DB;\n+\n+    static SharedContextHolder shared_context;\n+    static ContextMutablePtr context;\n+\n+    auto initialize = [&]() mutable\n+    {\n+        shared_context = Context::createShared();\n+        context = Context::createGlobal(shared_context.get());\n+        context->makeGlobalContext();\n+        context->setApplicationType(Context::ApplicationType::LOCAL);\n+\n+        MainThreadStatus::getInstance();\n+\n+        registerAggregateFunctions();\n+        registerFormats();\n+\n+        return true;\n+    };\n+\n+    static bool initialized = initialize();\n+    (void) initialized;\n+\n+    total_memory_tracker.resetCounters();\n+    total_memory_tracker.setHardLimit(1_GiB);\n+    CurrentThread::get().memory_tracker.resetCounters();\n+    CurrentThread::get().memory_tracker.setHardLimit(1_GiB);\n+\n+    /// The input format is as follows:\n+    /// - format name on the first line,\n+    /// - table structure on the second line,\n+    /// - the data for the rest of the input.\n+\n+    /** The corpus was generated as follows:\n+\n+    i=0; find ../../../../tests/queries -name '*.sql' |\n+        xargs -I{} bash -c \"tr '\\n' ' ' <{}; echo\" |\n+        rg -o -i 'CREATE TABLE\\s+\\w+\\s+\\(.+?\\) ENGINE' |\n+        sed -r -e 's/CREATE TABLE\\s+\\w+\\s+\\((.+?)\\) ENGINE/\\1/i' | sort | uniq |\n+        while read line; do\n+            i=$((i+1));\n+            clickhouse-local --query \"SELECT name FROM system.formats ORDER BY rand() LIMIT 1\" >> $i;\n+            echo \"$line\" >> $i;\n+            echo $RANDOM >> $i;\n+            echo $i;\n+        done\n+    */\n+\n+    /** And:\n+\n+    for format in $(clickhouse-client --query \"SELECT name FROM system.formats WHERE is_output\"); do\n+        echo $format;\n+        echo $format >> $format;\n+        echo \"WatchID Int64, JavaEnable Int16, Title String, GoodEvent Int16, EventTime DateTime, EventDate Date, CounterID Int32, ClientIP Int32, RegionID Int32, UserID Int64, CounterClass Int16, OS Int16, UserAgent Int16, URL String, Referer String, IsRefresh Int16, RefererCategoryID Int16, RefererRegionID Int32, URLCategoryID Int16, URLRegionID Int32, ResolutionWidth Int16, ResolutionHeight Int16, ResolutionDepth Int16, FlashMajor Int16, FlashMinor Int16, FlashMinor2 String, NetMajor Int16, NetMinor Int16, UserAgentMajor Int16, UserAgentMinor String, CookieEnable Int16, JavascriptEnable Int16, IsMobile Int16, MobilePhone Int16, MobilePhoneModel String, Params String, IPNetworkID Int32, TraficSourceID Int16, SearchEngineID Int16, SearchPhrase String, AdvEngineID Int16, IsArtifical Int16, WindowClientWidth Int16, WindowClientHeight Int16, ClientTimeZone Int16, ClientEventTime DateTime, SilverlightVersion1 Int16, SilverlightVersion2 Int16, SilverlightVersion3 Int32, SilverlightVersion4 Int16, PageCharset String, CodeVersion Int32, IsLink Int16, IsDownload Int16, IsNotBounce Int16, FUniqID Int64, OriginalURL String, HID Int32, IsOldCounter Int16, IsEvent Int16, IsParameter Int16, DontCountHits Int16, WithHash Int16, HitColor String, LocalEventTime DateTime, Age Int16, Sex Int16, Income Int16, Interests Int16, Robotness Int16, RemoteIP Int32, WindowName Int32, OpenerName Int32, HistoryLength Int16, BrowserLanguage String, BrowserCountry String, SocialNetwork String, SocialAction String, HTTPError Int16, SendTiming Int32, DNSTiming Int32, ConnectTiming Int32, ResponseStartTiming Int32, ResponseEndTiming Int32, FetchTiming Int32, SocialSourceNetworkID Int16, SocialSourcePage String, ParamPrice Int64, ParamOrderID String, ParamCurrency String, ParamCurrencyID Int16, OpenstatServiceName String, OpenstatCampaignID String, OpenstatAdID String, OpenstatSourceID String, UTMSource String, UTMMedium String, UTMCampaign String, UTMContent String, UTMTerm String, FromTag String, HasGCLID Int16, RefererHash Int64, URLHash Int64, CLID Int32\" >> $format;\n+        clickhouse-client --query \"SELECT * FROM hits LIMIT 10 FORMAT $format\" >> $format || rm $format;\n+    done\n+\n+    */\n+\n+    /// Compile the code as follows:\n+    ///   mkdir build_asan_fuzz\n+    ///   cd build_asan_fuzz\n+    ///   CC=clang CXX=clang++ cmake -D SANITIZE=address -D ENABLE_FUZZING=1 -D WITH_COVERAGE=1 ..\n+    ///\n+    /// The corpus is located here:\n+    /// https://github.com/ClickHouse/fuzz-corpus/tree/main/format_fuzzer\n+    ///\n+    /// The fuzzer can be run as follows:\n+    ///   ../../../build_asan_fuzz/src/Formats/fuzzers/format_fuzzer corpus -jobs=64 -rss_limit_mb=8192\n+\n+    DB::ReadBufferFromMemory in(data, size);\n+\n+    String format;\n+    readStringUntilNewlineInto(format, in);\n+    assertChar('\\n', in);\n+\n+    String structure;\n+    readStringUntilNewlineInto(structure, in);\n+    assertChar('\\n', in);\n+\n+    ColumnsDescription description = parseColumnsListFromString(structure, context);\n+    auto columns_info = description.getOrdinary();\n+\n+    Block header;\n+    for (const auto & info : columns_info)\n+    {\n+        ColumnWithTypeAndName column;\n+        column.name = info.name;\n+        column.type = info.type;\n+        column.column = column.type->createColumn();\n+        header.insert(std::move(column));\n+    }\n+\n+    InputFormatPtr input_format = context->getInputFormat(format, in, header, 13 /* small block size */);\n+\n+    QueryPipeline pipeline(Pipe(std::move(input_format)));\n+    PullingPipelineExecutor executor(pipeline);\n+    Block res;\n+    while (executor.pull(res))\n+        ;\n+\n+    return 0;\n+}\n+catch (...)\n+{\n+    return 1;\n+}\ndiff --git a/src/Functions/FunctionsBitmap.h b/src/Functions/FunctionsBitmap.h\nindex 29ab0abed87c..2292b896952f 100644\n--- a/src/Functions/FunctionsBitmap.h\n+++ b/src/Functions/FunctionsBitmap.h\n@@ -211,7 +211,7 @@ class FunctionBitmapBuildImpl : public IFunction\n                 = *reinterpret_cast<AggregateFunctionGroupBitmapData<T> *>(col_to->getData()[i]);\n             for (; pos < offsets[i]; ++pos)\n             {\n-                bitmap_data.rbs.add(input_data[pos]);\n+                bitmap_data.roaring_bitmap_with_small_set.add(input_data[pos]);\n             }\n         }\n         return col_to;\n@@ -303,7 +303,7 @@ class FunctionBitmapToArrayImpl : public IFunction\n         {\n             const AggregateFunctionGroupBitmapData<T> & bitmap_data_1\n                 = *reinterpret_cast<const AggregateFunctionGroupBitmapData<T> *>(column->getData()[i]);\n-            UInt64 count = bitmap_data_1.rbs.rb_to_array(res_data);\n+            UInt64 count = bitmap_data_1.roaring_bitmap_with_small_set.rb_to_array(res_data);\n             res_offset += count;\n             res_offsets.emplace_back(res_offset);\n         }\n@@ -449,7 +449,7 @@ struct BitmapSubsetInRangeImpl\n         UInt64 range_end,\n         AggregateFunctionGroupBitmapData<T> & bitmap_data_2)\n     {\n-        bitmap_data_0.rbs.rb_range(range_start, range_end, bitmap_data_2.rbs);\n+        bitmap_data_0.roaring_bitmap_with_small_set.rb_range(range_start, range_end, bitmap_data_2.roaring_bitmap_with_small_set);\n     }\n };\n \n@@ -464,7 +464,7 @@ struct BitmapSubsetLimitImpl\n         UInt64 range_end,\n         AggregateFunctionGroupBitmapData<T> & bitmap_data_2)\n     {\n-        bitmap_data_0.rbs.rb_limit(range_start, range_end, bitmap_data_2.rbs);\n+        bitmap_data_0.roaring_bitmap_with_small_set.rb_limit(range_start, range_end, bitmap_data_2.roaring_bitmap_with_small_set);\n     }\n };\n \n@@ -479,7 +479,7 @@ struct BitmapSubsetOffsetLimitImpl\n         UInt64 range_end,\n         AggregateFunctionGroupBitmapData<T> & bitmap_data_2)\n         {\n-        bitmap_data_0.rbs.rb_offset_limit(range_start, range_end, bitmap_data_2.rbs);\n+        bitmap_data_0.roaring_bitmap_with_small_set.rb_offset_limit(range_start, range_end, bitmap_data_2.roaring_bitmap_with_small_set);\n         }\n };\n \n@@ -649,8 +649,8 @@ class FunctionBitmapTransform : public IFunction\n             col_to->insertDefault();\n             AggregateFunctionGroupBitmapData<T> & bitmap_data_2\n                 = *reinterpret_cast<AggregateFunctionGroupBitmapData<T> *>(col_to->getData()[i]);\n-            bitmap_data_2.rbs.merge(bitmap_data_0.rbs);\n-            bitmap_data_2.rbs.rb_replace(&from_container[from_start], &to_container[to_start], from_end - from_start);\n+            bitmap_data_2.roaring_bitmap_with_small_set.merge(bitmap_data_0.roaring_bitmap_with_small_set);\n+            bitmap_data_2.roaring_bitmap_with_small_set.rb_replace(&from_container[from_start], &to_container[to_start], from_end - from_start);\n         }\n         return col_to;\n     }\n@@ -740,7 +740,7 @@ struct BitmapCardinalityImpl\n     template <typename T>\n     static UInt64 apply(const AggregateFunctionGroupBitmapData<T> & bitmap_data)\n     {\n-        return bitmap_data.rbs.size();\n+        return bitmap_data.roaring_bitmap_with_small_set.size();\n     }\n };\n \n@@ -751,7 +751,7 @@ struct BitmapMinImpl\n     template <typename T>\n     static UInt64 apply(const AggregateFunctionGroupBitmapData<T> & bitmap_data)\n     {\n-        return bitmap_data.rbs.rb_min();\n+        return bitmap_data.roaring_bitmap_with_small_set.rb_min();\n     }\n };\n \n@@ -762,7 +762,7 @@ struct BitmapMaxImpl\n     template <typename T>\n     static UInt64 apply(const AggregateFunctionGroupBitmapData<T> & bitmap_data)\n     {\n-        return bitmap_data.rbs.rb_max();\n+        return bitmap_data.roaring_bitmap_with_small_set.rb_max();\n     }\n };\n \n@@ -773,7 +773,7 @@ struct BitmapAndCardinalityImpl\n     static UInt64 apply(const AggregateFunctionGroupBitmapData<T> & bitmap_data_1, const AggregateFunctionGroupBitmapData<T> & bitmap_data_2)\n     {\n         // roaring_bitmap_and_cardinality( rb1, rb2 );\n-        return bitmap_data_1.rbs.rb_and_cardinality(bitmap_data_2.rbs);\n+        return bitmap_data_1.roaring_bitmap_with_small_set.rb_and_cardinality(bitmap_data_2.roaring_bitmap_with_small_set);\n     }\n };\n \n@@ -785,7 +785,7 @@ struct BitmapOrCardinalityImpl\n     static UInt64 apply(const AggregateFunctionGroupBitmapData<T> & bitmap_data_1, const AggregateFunctionGroupBitmapData<T> & bitmap_data_2)\n     {\n         // return roaring_bitmap_or_cardinality( rb1, rb2 );\n-        return bitmap_data_1.rbs.rb_or_cardinality(bitmap_data_2.rbs);\n+        return bitmap_data_1.roaring_bitmap_with_small_set.rb_or_cardinality(bitmap_data_2.roaring_bitmap_with_small_set);\n     }\n };\n \n@@ -796,7 +796,7 @@ struct BitmapXorCardinalityImpl\n     static UInt64 apply(const AggregateFunctionGroupBitmapData<T> & bitmap_data_1, const AggregateFunctionGroupBitmapData<T> & bitmap_data_2)\n     {\n         // return roaring_bitmap_xor_cardinality( rb1, rb2 );\n-        return bitmap_data_1.rbs.rb_xor_cardinality(bitmap_data_2.rbs);\n+        return bitmap_data_1.roaring_bitmap_with_small_set.rb_xor_cardinality(bitmap_data_2.roaring_bitmap_with_small_set);\n     }\n };\n \n@@ -807,7 +807,7 @@ struct BitmapAndnotCardinalityImpl\n     static UInt64 apply(const AggregateFunctionGroupBitmapData<T> & bitmap_data_1, const AggregateFunctionGroupBitmapData<T> & bitmap_data_2)\n     {\n         // roaring_bitmap_andnot_cardinality( rb1, rb2 );\n-        return bitmap_data_1.rbs.rb_andnot_cardinality(bitmap_data_2.rbs);\n+        return bitmap_data_1.roaring_bitmap_with_small_set.rb_andnot_cardinality(bitmap_data_2.roaring_bitmap_with_small_set);\n     }\n };\n \n@@ -817,7 +817,7 @@ struct BitmapHasAllImpl\n     using ReturnType = UInt8;\n     static UInt8 apply(const AggregateFunctionGroupBitmapData<T> & bitmap_data_1, const AggregateFunctionGroupBitmapData<T> & bitmap_data_2)\n     {\n-        return bitmap_data_1.rbs.rb_is_subset(bitmap_data_2.rbs);\n+        return bitmap_data_1.roaring_bitmap_with_small_set.rb_is_subset(bitmap_data_2.roaring_bitmap_with_small_set);\n     }\n };\n \n@@ -827,7 +827,7 @@ struct BitmapHasAnyImpl\n     using ReturnType = UInt8;\n     static UInt8 apply(const AggregateFunctionGroupBitmapData<T> & bitmap_data_1, const AggregateFunctionGroupBitmapData<T> & bitmap_data_2)\n     {\n-        return bitmap_data_1.rbs.rb_intersect(bitmap_data_2.rbs);\n+        return bitmap_data_1.roaring_bitmap_with_small_set.rb_intersect(bitmap_data_2.roaring_bitmap_with_small_set);\n     }\n };\n \n@@ -931,7 +931,7 @@ class FunctionBitmapContains : public IFunction\n             const UInt64 data1 = is_column_const[1] ? (*container1)[0] : (*container1)[i];\n             const AggregateFunctionGroupBitmapData<T> & bitmap_data_0\n                 = *reinterpret_cast<const AggregateFunctionGroupBitmapData<T> *>(data_ptr_0);\n-            vec_to[i] = bitmap_data_0.rbs.rb_contains(data1);\n+            vec_to[i] = bitmap_data_0.roaring_bitmap_with_small_set.rb_contains(data1);\n         }\n     }\n };\n@@ -1050,7 +1050,7 @@ struct BitmapAndImpl\n {\n     static void apply(AggregateFunctionGroupBitmapData<T> & bitmap_data_1, const AggregateFunctionGroupBitmapData<T> & bitmap_data_2)\n     {\n-        bitmap_data_1.rbs.rb_and(bitmap_data_2.rbs);\n+        bitmap_data_1.roaring_bitmap_with_small_set.rb_and(bitmap_data_2.roaring_bitmap_with_small_set);\n     }\n };\n \n@@ -1059,7 +1059,7 @@ struct BitmapOrImpl\n {\n     static void apply(AggregateFunctionGroupBitmapData<T> & bitmap_data_1, const AggregateFunctionGroupBitmapData<T> & bitmap_data_2)\n     {\n-        bitmap_data_1.rbs.rb_or(bitmap_data_2.rbs);\n+        bitmap_data_1.roaring_bitmap_with_small_set.rb_or(bitmap_data_2.roaring_bitmap_with_small_set);\n     }\n };\n \n@@ -1068,7 +1068,7 @@ struct BitmapXorImpl\n {\n     static void apply(AggregateFunctionGroupBitmapData<T> & bitmap_data_1, const AggregateFunctionGroupBitmapData<T> & bitmap_data_2)\n     {\n-        bitmap_data_1.rbs.rb_xor(bitmap_data_2.rbs);\n+        bitmap_data_1.roaring_bitmap_with_small_set.rb_xor(bitmap_data_2.roaring_bitmap_with_small_set);\n     }\n };\n \n@@ -1077,7 +1077,7 @@ struct BitmapAndnotImpl\n {\n     static void apply(AggregateFunctionGroupBitmapData<T> & bitmap_data_1, const AggregateFunctionGroupBitmapData<T> & bitmap_data_2)\n     {\n-        bitmap_data_1.rbs.rb_andnot(bitmap_data_2.rbs);\n+        bitmap_data_1.roaring_bitmap_with_small_set.rb_andnot(bitmap_data_2.roaring_bitmap_with_small_set);\n     }\n };\n \n@@ -1190,7 +1190,7 @@ class FunctionBitmap : public IFunction\n             auto * bm_2 = reinterpret_cast<AggregateFunctionGroupBitmapData<T> *>(data_ptr_1);\n \n             // check the name of operation (bitmapAnd) and check if it is the situation mentioned above\n-            auto need_exchange = (name == NameBitmapAnd::name) && bm_1->rbs.isLarge() && bm_2->rbs.isSmall();\n+            auto need_exchange = (name == NameBitmapAnd::name) && bm_1->roaring_bitmap_with_small_set.isLarge() && bm_2->roaring_bitmap_with_small_set.isSmall();\n             col_to->insertFrom(need_exchange ? data_ptr_1 : data_ptr_0);\n             AggregateFunctionGroupBitmapData<T> & bitmap_data_1 = *reinterpret_cast<AggregateFunctionGroupBitmapData<T> *>(col_to->getData()[i]);\n             const AggregateFunctionGroupBitmapData<T> & bitmap_data_2\ndiff --git a/src/IO/ReadHelpers.h b/src/IO/ReadHelpers.h\nindex f8931a7f6221..cac42c198b13 100644\n--- a/src/IO/ReadHelpers.h\n+++ b/src/IO/ReadHelpers.h\n@@ -61,6 +61,8 @@ namespace ErrorCodes\n     extern const int CANNOT_READ_ARRAY_FROM_TEXT;\n     extern const int CANNOT_PARSE_NUMBER;\n     extern const int INCORRECT_DATA;\n+    extern const int TOO_LARGE_STRING_SIZE;\n+    extern const int TOO_LARGE_ARRAY_SIZE;\n }\n \n /// Helper functions for formatted input.\n@@ -128,13 +130,13 @@ inline void readFloatBinary(T & x, ReadBuffer & buf)\n     readPODBinary(x, buf);\n }\n \n-inline void readStringBinary(std::string & s, ReadBuffer & buf, size_t MAX_STRING_SIZE = DEFAULT_MAX_STRING_SIZE)\n+inline void readStringBinary(std::string & s, ReadBuffer & buf, size_t max_string_size = DEFAULT_MAX_STRING_SIZE)\n {\n     size_t size = 0;\n     readVarUInt(size, buf);\n \n-    if (size > MAX_STRING_SIZE)\n-        throw Poco::Exception(\"Too large string size.\");\n+    if (size > max_string_size)\n+        throw Exception(ErrorCodes::TOO_LARGE_STRING_SIZE, \"Too large string size.\");\n \n     s.resize(size);\n     buf.readStrict(s.data(), size);\n@@ -146,6 +148,9 @@ inline StringRef readStringBinaryInto(Arena & arena, ReadBuffer & buf)\n     size_t size = 0;\n     readVarUInt(size, buf);\n \n+    if (unlikely(size > DEFAULT_MAX_STRING_SIZE))\n+        throw Exception(ErrorCodes::TOO_LARGE_STRING_SIZE, \"Too large string size.\");\n+\n     char * data = arena.alloc(size);\n     buf.readStrict(data, size);\n \n@@ -154,13 +159,13 @@ inline StringRef readStringBinaryInto(Arena & arena, ReadBuffer & buf)\n \n \n template <typename T>\n-void readVectorBinary(std::vector<T> & v, ReadBuffer & buf, size_t MAX_VECTOR_SIZE = DEFAULT_MAX_STRING_SIZE)\n+void readVectorBinary(std::vector<T> & v, ReadBuffer & buf)\n {\n     size_t size = 0;\n     readVarUInt(size, buf);\n \n-    if (size > MAX_VECTOR_SIZE)\n-        throw Poco::Exception(\"Too large vector size.\");\n+    if (size > DEFAULT_MAX_STRING_SIZE)\n+        throw Exception(ErrorCodes::TOO_LARGE_ARRAY_SIZE, \"Too large array size.\");\n \n     v.resize(size);\n     for (size_t i = 0; i < size; ++i)\ndiff --git a/src/Interpreters/fuzzers/execute_query_fuzzer.cpp b/src/Interpreters/fuzzers/execute_query_fuzzer.cpp\nindex 30db25668cf2..284e780ed1fa 100644\n--- a/src/Interpreters/fuzzers/execute_query_fuzzer.cpp\n+++ b/src/Interpreters/fuzzers/execute_query_fuzzer.cpp\n@@ -1,4 +1,3 @@\n-#include <iostream>\n #include <Interpreters/executeQuery.h>\n #include <Interpreters/Context.h>\n #include \"Processors/Executors/PullingPipelineExecutor.h\"\ndiff --git a/src/Interpreters/parseColumnsListForTableFunction.h b/src/Interpreters/parseColumnsListForTableFunction.h\nindex 212c378d3b5e..14119ab55da5 100644\n--- a/src/Interpreters/parseColumnsListForTableFunction.h\n+++ b/src/Interpreters/parseColumnsListForTableFunction.h\n@@ -31,7 +31,7 @@ struct DataTypeValidationSettings\n void validateDataType(const DataTypePtr & type, const DataTypeValidationSettings & settings);\n \n /// Parses a common argument for table functions such as table structure given in string\n-ColumnsDescription parseColumnsListFromString(const std::string & structure, const ContextPtr & context);\n+[[nodiscard]] ColumnsDescription parseColumnsListFromString(const std::string & structure, const ContextPtr & context);\n \n bool tryParseColumnsListFromString(const std::string & structure, ColumnsDescription & columns, const ContextPtr & context, String & error);\n \ndiff --git a/src/Parsers/ParserCreateQuery.h b/src/Parsers/ParserCreateQuery.h\nindex ef87988aab2f..2489b1080041 100644\n--- a/src/Parsers/ParserCreateQuery.h\n+++ b/src/Parsers/ParserCreateQuery.h\n@@ -265,6 +265,10 @@ bool IParserColumnDeclaration<NameParser>::parseImpl(Pos & pos, ASTPtr & node, E\n         }\n     }\n \n+    /// This will rule out unusual expressions like *, t.* that cannot appear in DEFAULT\n+    if (default_expression && !dynamic_cast<const ASTWithAlias *>(default_expression.get()))\n+        return false;\n+\n     if (require_type && !type && !default_expression)\n         return false; /// reject column name without type\n \ndiff --git a/src/Processors/Formats/Impl/BSONEachRowRowInputFormat.cpp b/src/Processors/Formats/Impl/BSONEachRowRowInputFormat.cpp\nindex ebc612a4ce31..9e3a4a85c9a1 100644\n--- a/src/Processors/Formats/Impl/BSONEachRowRowInputFormat.cpp\n+++ b/src/Processors/Formats/Impl/BSONEachRowRowInputFormat.cpp\n@@ -282,6 +282,8 @@ static void readAndInsertString(ReadBuffer & in, IColumn & column, BSONType bson\n     if (bson_type == BSONType::STRING || bson_type == BSONType::SYMBOL || bson_type == BSONType::JAVA_SCRIPT_CODE)\n     {\n         auto size = readBSONSize(in);\n+        if (size == 0)\n+            throw Exception(ErrorCodes::INCORRECT_DATA, \"Incorrect size of a string (zero) in BSON\");\n         readAndInsertStringImpl<is_fixed_string>(in, column, size - 1);\n         assertChar(0, in);\n     }\n@@ -1008,7 +1010,7 @@ fileSegmentationEngineBSONEachRow(ReadBuffer & in, DB::Memory<> & memory, size_t\n \n         size_t old_size = memory.size();\n         memory.resize(old_size + document_size);\n-        memcpy(memory.data() + old_size, reinterpret_cast<char *>(&document_size), sizeof(document_size));\n+        unalignedStore<BSONSizeT>(memory.data() + old_size, document_size);\n         in.readStrict(memory.data() + old_size + sizeof(document_size), document_size - sizeof(document_size));\n         ++number_of_rows;\n     }\ndiff --git a/src/Storages/MergeTree/RangesInDataPart.cpp b/src/Storages/MergeTree/RangesInDataPart.cpp\nindex 29a236c98652..ab76611a5073 100644\n--- a/src/Storages/MergeTree/RangesInDataPart.cpp\n+++ b/src/Storages/MergeTree/RangesInDataPart.cpp\n@@ -11,6 +11,12 @@\n namespace DB\n {\n \n+namespace ErrorCodes\n+{\n+    extern const int TOO_LARGE_ARRAY_SIZE;\n+}\n+\n+\n void RangesInDataPartDescription::serialize(WriteBuffer & out) const\n {\n     info.serialize(out);\n@@ -50,6 +56,8 @@ void RangesInDataPartsDescription::deserialize(ReadBuffer & in)\n {\n     size_t new_size = 0;\n     readVarUInt(new_size, in);\n+    if (new_size > 100'000'000'000)\n+        throw DB::Exception(DB::ErrorCodes::TOO_LARGE_ARRAY_SIZE, \"The size of serialized hash table is suspiciously large: {}\", new_size);\n \n     this->resize(new_size);\n     for (auto & desc : *this)\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02560_agg_state_deserialization_hash_table_crash.sql b/tests/queries/0_stateless/02560_agg_state_deserialization_hash_table_crash.sql\nindex 5b6662faeb3c..d85cacc70be2 100644\n--- a/tests/queries/0_stateless/02560_agg_state_deserialization_hash_table_crash.sql\n+++ b/tests/queries/0_stateless/02560_agg_state_deserialization_hash_table_crash.sql\n@@ -1,4 +1,4 @@\n DROP TABLE IF EXISTS tab;\n create table tab (d Int64, s AggregateFunction(groupUniqArrayArray, Array(UInt64)), c SimpleAggregateFunction(groupUniqArrayArray, Array(UInt64))) engine = SummingMergeTree() order by d;\n-INSERT INTO tab VALUES (1, '\u3053\u306e\u30b3\u30fc'); -- { clientError CANNOT_ALLOCATE_MEMORY }\n+INSERT INTO tab VALUES (1, '\u3053\u306e\u30b3\u30fc'); -- { clientError 128 }\n DROP TABLE tab;\ndiff --git a/tests/queries/0_stateless/02680_default_star.reference b/tests/queries/0_stateless/02680_default_star.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/02680_default_star.sql b/tests/queries/0_stateless/02680_default_star.sql\nnew file mode 100644\nindex 000000000000..d560bd01e417\n--- /dev/null\n+++ b/tests/queries/0_stateless/02680_default_star.sql\n@@ -0,0 +1,6 @@\n+-- These queries yield syntax error, not logical error.\n+\n+CREATE TEMPORARY TABLE test (ad DEFAULT *); -- { clientError SYNTAX_ERROR }\n+CREATE TEMPORARY TABLE test (ad INT DEFAULT *); -- { clientError SYNTAX_ERROR }\n+CREATE TEMPORARY TABLE test (ad DEFAULT * NOT NULL); -- { clientError SYNTAX_ERROR }\n+CREATE TEMPORARY TABLE test (ad DEFAULT t.* NOT NULL); -- { clientError SYNTAX_ERROR }\ndiff --git a/tests/queries/0_stateless/02681_group_array_too_large_size.reference b/tests/queries/0_stateless/02681_group_array_too_large_size.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/02681_group_array_too_large_size.sql b/tests/queries/0_stateless/02681_group_array_too_large_size.sql\nnew file mode 100644\nindex 000000000000..7b09f9b468e8\n--- /dev/null\n+++ b/tests/queries/0_stateless/02681_group_array_too_large_size.sql\n@@ -0,0 +1,8 @@\n+-- This query throw high-level exception instead of low-level \"too large size passed to allocator\":\n+\n+SELECT * FROM format(CSV, 'entitypArray AggregateFunction(groupArray, String)',\n+'295TMiews.viewN\ufffd\ufffd\ufffd\ufffd\ufffd\"\"\"\"\"\"TabSepar\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdated\n+d St\"\"\n+\n+\n+r'); -- { serverError TOO_LARGE_ARRAY_SIZE }\ndiff --git a/tests/queries/0_stateless/02682_quantiles_too_large_size.reference b/tests/queries/0_stateless/02682_quantiles_too_large_size.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/02682_quantiles_too_large_size.sql b/tests/queries/0_stateless/02682_quantiles_too_large_size.sql\nnew file mode 100644\nindex 000000000000..fff98f667c79\nBinary files /dev/null and b/tests/queries/0_stateless/02682_quantiles_too_large_size.sql differ\ndiff --git a/tests/queries/0_stateless/02683_native_too_large_size.reference b/tests/queries/0_stateless/02683_native_too_large_size.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/02683_native_too_large_size.sql b/tests/queries/0_stateless/02683_native_too_large_size.sql\nnew file mode 100644\nindex 000000000000..e8752477f2d0\nBinary files /dev/null and b/tests/queries/0_stateless/02683_native_too_large_size.sql differ\ndiff --git a/tests/queries/0_stateless/02684_bson.reference b/tests/queries/0_stateless/02684_bson.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/02684_bson.sql b/tests/queries/0_stateless/02684_bson.sql\nnew file mode 100644\nindex 000000000000..577bd4ffd27d\nBinary files /dev/null and b/tests/queries/0_stateless/02684_bson.sql differ\ndiff --git a/tests/queries/0_stateless/02685_bson2.reference b/tests/queries/0_stateless/02685_bson2.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/02685_bson2.sql b/tests/queries/0_stateless/02685_bson2.sql\nnew file mode 100644\nindex 000000000000..fc65d2952df8\nBinary files /dev/null and b/tests/queries/0_stateless/02685_bson2.sql differ\ndiff --git a/tests/queries/0_stateless/02686_bson3.reference b/tests/queries/0_stateless/02686_bson3.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/02686_bson3.sql b/tests/queries/0_stateless/02686_bson3.sql\nnew file mode 100644\nindex 000000000000..05a73e814dd1\nBinary files /dev/null and b/tests/queries/0_stateless/02686_bson3.sql differ\ndiff --git a/tests/queries/0_stateless/02687_native_fuzz.reference b/tests/queries/0_stateless/02687_native_fuzz.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/02687_native_fuzz.sql b/tests/queries/0_stateless/02687_native_fuzz.sql\nnew file mode 100644\nindex 000000000000..0cd11390918a\nBinary files /dev/null and b/tests/queries/0_stateless/02687_native_fuzz.sql differ\ndiff --git a/tests/queries/0_stateless/02688_aggregate_states.reference b/tests/queries/0_stateless/02688_aggregate_states.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/02688_aggregate_states.sql b/tests/queries/0_stateless/02688_aggregate_states.sql\nnew file mode 100644\nindex 000000000000..575f4b1bb3a3\n--- /dev/null\n+++ b/tests/queries/0_stateless/02688_aggregate_states.sql\n@@ -0,0 +1,7 @@\n+SELECT '\\x01\\x00'::AggregateFunction(groupBitmap, UInt32); -- { serverError INCORRECT_DATA }\n+SELECT '\\x01\\x01\\x01'::AggregateFunction(groupBitmap, UInt64); -- { serverError STD_EXCEPTION }\n+SELECT '\\x02\\x00\\x0d'::AggregateFunction(topK, UInt256); -- { serverError CANNOT_READ_ALL_DATA }\n+SELECT unhex('bebebebebebebebebebebebebebebebebebebebebebebebebebebebebebebe0c0c3131313131313131313131313173290aee00b300')::AggregateFunction(minDistinct, Int8); -- { serverError TOO_LARGE_ARRAY_SIZE }\n+SELECT unhex('01000b0b0b0d0d0d0d7175616e74696c6554696d696e672c20496e743332000300')::AggregateFunction(quantileTiming, Int32); -- { serverError INCORRECT_DATA }\n+SELECT unhex('010001')::AggregateFunction(quantileTiming, Int32); -- { serverError INCORRECT_DATA }\n+SELECT unhex('0a00797979797979797979790a0a6e')::AggregateFunction(minForEach, Ring); -- { serverError TOO_LARGE_ARRAY_SIZE }\ndiff --git a/tests/queries/0_stateless/02689_meaningless_data_types.reference b/tests/queries/0_stateless/02689_meaningless_data_types.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/02689_meaningless_data_types.sql b/tests/queries/0_stateless/02689_meaningless_data_types.sql\nnew file mode 100644\nindex 000000000000..8ae702d66df2\n--- /dev/null\n+++ b/tests/queries/0_stateless/02689_meaningless_data_types.sql\n@@ -0,0 +1,3 @@\n+SELECT 0::Bool(Upyachka); -- { serverError DATA_TYPE_CANNOT_HAVE_ARGUMENTS }\n+SELECT [(1, 2), (3, 4)]::Ring(Upyachka); -- { serverError DATA_TYPE_CANNOT_HAVE_ARGUMENTS }\n+SELECT '1.1.1.1'::IPv4('Hello, world!'); -- { serverError DATA_TYPE_CANNOT_HAVE_ARGUMENTS }\n",
  "problem_statement": "Meaningless type names are allowed.\n```\r\nmilovidov-desktop :) SELECT [(1, 2), (3, 4)]::Ring(Upyachka);\r\n\r\nSELECT CAST('[(1, 2), (3, 4)]', 'Ring(Upyachka)')\r\n\r\nQuery id: 2e6a6052-df92-4b04-96b7-b64069153742\r\n\r\n\u250c\u2500CAST('[(1, 2), (3, 4)]', 'Ring(Upyachka)')\u2500\u2510\r\n\u2502 [(1,2),(3,4)]                              \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.001 sec. \r\n\r\nmilovidov-desktop :) SELECT 0::Bool(Upyachka)\r\n\r\nSELECT CAST('0', 'Bool(Upyachka)')\r\n\r\nQuery id: 55ac5157-abc9-4437-8e4b-eec65e9627f9\r\n\r\n\u250c\u2500CAST('0', 'Bool(Upyachka)')\u2500\u2510\r\n\u2502 false                       \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.001 sec.\r\n```\nLogical error on incorrect DEFAULT in CREATE query\n**Describe the bug**\r\n```\r\nmilovidov-desktop :) CREATE TEMPORARY TABLE test (ad DEFAULT * NOT NULL)\r\n\r\nCREATE TEMPORARY TABLE test\r\n(\r\n    `ad` NOT NULL DEFAULT *\r\n)\r\n\r\nQuery id: 960a4a47-a9f4-40d9-87ee-2f6a84575006\r\n\r\n\r\n0 rows in set. Elapsed: 1.223 sec. \r\n\r\nReceived exception:\r\nCode: 49. DB::Exception: Can't set alias of *. (LOGICAL_ERROR)\r\n```\n",
  "hints_text": "\n",
  "created_at": "2023-03-13T04:28:59Z"
}