{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 37021,
  "instance_id": "ClickHouse__ClickHouse-37021",
  "issue_numbers": [
    "32107"
  ],
  "base_commit": "63483e132157f3c03b854ea3365c0292a1cb9c77",
  "patch": "diff --git a/src/AggregateFunctions/QuantileTDigest.h b/src/AggregateFunctions/QuantileTDigest.h\nindex adb02171f72e..b5f32bad2478 100644\n--- a/src/AggregateFunctions/QuantileTDigest.h\n+++ b/src/AggregateFunctions/QuantileTDigest.h\n@@ -103,8 +103,9 @@ class QuantileTDigest\n       */\n     static Value interpolate(Value x, Value x1, Value y1, Value x2, Value y2)\n     {\n+        /// Symmetric interpolation for better results with infinities.\n         double k = (x - x1) / (x2 - x1);\n-        return y1 + k * (y2 - y1);\n+        return (1 - k) * y1 + k * y2;\n     }\n \n     struct RadixSortTraits\n@@ -137,6 +138,11 @@ class QuantileTDigest\n             compress();\n     }\n \n+    inline bool canBeMerged(const BetterFloat & l_mean, const Value & r_mean)\n+    {\n+        return l_mean == r_mean || (!std::isinf(l_mean) && !std::isinf(r_mean));\n+    }\n+\n     void compressBrute()\n     {\n         if (centroids.size() <= params.max_centroids)\n@@ -149,13 +155,17 @@ class QuantileTDigest\n         BetterFloat l_mean = l->mean; // We have high-precision temporaries for numeric stability\n         BetterFloat l_count = l->count;\n         size_t batch_pos = 0;\n-        for (;r != centroids.end(); ++r)\n+\n+        for (; r != centroids.end(); ++r)\n         {\n             if (batch_pos < batch_size - 1)\n             {\n                 /// The left column \"eats\" the right. Middle of the batch\n                 l_count += r->count;\n-                l_mean += r->count * (r->mean - l_mean) / l_count; // Symmetric algo (M1*C1 + M2*C2)/(C1+C2) is numerically better, but slower\n+                if (r->mean != l_mean) /// Handling infinities of the same sign well.\n+                {\n+                    l_mean += r->count * (r->mean - l_mean) / l_count; // Symmetric algo (M1*C1 + M2*C2)/(C1+C2) is numerically better, but slower\n+                }\n                 l->mean = l_mean;\n                 l->count = l_count;\n                 batch_pos += 1;\n@@ -163,8 +173,11 @@ class QuantileTDigest\n             else\n             {\n                 // End of the batch, start the next one\n-                sum += l->count; // Not l_count, otherwise actual sum of elements will be different\n-                ++l;\n+                if (!std::isnan(l->mean)) /// Skip writing batch result if we compressed something to nan.\n+                {\n+                    sum += l->count; // Not l_count, otherwise actual sum of elements will be different\n+                    ++l;\n+                }\n \n                 /// We skip all the values \"eaten\" earlier.\n                 *l = *r;\n@@ -173,8 +186,17 @@ class QuantileTDigest\n                 batch_pos = 0;\n             }\n         }\n-        count = sum + l_count; // Update count, it might be different due to += inaccuracy\n-        centroids.resize(l - centroids.begin() + 1);\n+\n+        if (!std::isnan(l->mean))\n+        {\n+            count = sum + l_count; // Update count, it might be different due to += inaccuracy\n+            centroids.resize(l - centroids.begin() + 1);\n+        }\n+        else /// Skip writing last batch if (super unlikely) it's nan.\n+        {\n+            count = sum;\n+            centroids.resize(l - centroids.begin());\n+        }\n         // Here centroids.size() <= params.max_centroids\n     }\n \n@@ -200,11 +222,8 @@ class QuantileTDigest\n             BetterFloat l_count = l->count;\n             while (r != centroids.end())\n             {\n-                /// N.B. Piece of logic which compresses the same singleton centroids into one centroid is removed\n-                /// because: 1) singleton centroids are being processed in unusual way in recent version of algorithm\n-                /// and such compression would break this logic;\n-                /// 2) we shall not compress centroids further than `max_centroids` parameter requires because\n-                /// this will lead to uneven compression.\n+                /// N.B. We cannot merge all the same values into single centroids because this will lead to\n+                /// unbalanced compression and wrong results.\n                 /// For more information see: https://arxiv.org/abs/1902.04023\n \n                 /// The ratio of the part of the histogram to l, including the half l to the entire histogram. That is, what level quantile in position l.\n@@ -225,12 +244,15 @@ class QuantileTDigest\n                   *  and at the edges decreases and is approximately equal to the distance to the edge * 4.\n                   */\n \n-                if (l_count + r->count <= k)\n+                if (l_count + r->count <= k && canBeMerged(l_mean, r->mean))\n                 {\n                     // it is possible to merge left and right\n                     /// The left column \"eats\" the right.\n                     l_count += r->count;\n-                    l_mean += r->count * (r->mean - l_mean) / l_count; // Symmetric algo (M1*C1 + M2*C2)/(C1+C2) is numerically better, but slower\n+                    if (r->mean != l_mean) /// Handling infinities of the same sign well.\n+                    {\n+                        l_mean += r->count * (r->mean - l_mean) / l_count; // Symmetric algo (M1*C1 + M2*C2)/(C1+C2) is numerically better, but slower\n+                    }\n                     l->mean = l_mean;\n                     l->count = l_count;\n                 }\n@@ -254,6 +276,7 @@ class QuantileTDigest\n             centroids.resize(l - centroids.begin() + 1);\n             unmerged = 0;\n         }\n+\n         // Ensures centroids.size() < max_centroids, independent of unprovable floating point blackbox above\n         compressBrute();\n     }\n@@ -298,10 +321,17 @@ class QuantileTDigest\n \n         for (const auto & c : centroids)\n         {\n-            if (c.count <= 0 || std::isnan(c.count) || std::isnan(c.mean)) // invalid count breaks compress(), invalid mean breaks sort()\n+            if (c.count <= 0 || std::isnan(c.count)) // invalid count breaks compress()\n                 throw Exception(\"Invalid centroid \" + std::to_string(c.count) + \":\" + std::to_string(c.mean), ErrorCodes::CANNOT_PARSE_INPUT_ASSERTION_FAILED);\n-            count += c.count;\n+            if (!std::isnan(c.mean))\n+            {\n+                count += c.count;\n+            }\n         }\n+\n+        auto it = std::remove_if(centroids.begin(), centroids.end(), [](Centroid & c) { return std::isnan(c.mean); });\n+        centroids.erase(it, centroids.end());\n+\n         compress(); // Allows reading/writing TDigests with different epsilon/max_centroids params\n     }\n \n@@ -312,7 +342,7 @@ class QuantileTDigest\n     ResultType getImpl(Float64 level)\n     {\n         if (centroids.empty())\n-            return std::is_floating_point_v<ResultType> ? NAN : 0;\n+            return std::is_floating_point_v<ResultType> ? std::numeric_limits<ResultType>::quiet_NaN() : 0;\n \n         compress();\n \n@@ -395,7 +425,6 @@ class QuantileTDigest\n \n                 while (current_x >= x)\n                 {\n-\n                     if (x <= left)\n                         result[levels_permutation[result_num]] = prev_mean;\n                     else if (x >= right)\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02286_quantile_tdigest_infinity.reference b/tests/queries/0_stateless/02286_quantile_tdigest_infinity.reference\nnew file mode 100644\nindex 000000000000..fd6451177f5d\n--- /dev/null\n+++ b/tests/queries/0_stateless/02286_quantile_tdigest_infinity.reference\n@@ -0,0 +1,42 @@\n+1\n+[-inf,-inf,-inf,nan,inf,inf,inf]\n+[-inf,-inf,-inf,nan,inf,inf,inf]\n+[0,0,0,inf,inf,inf,inf]\n+[-inf,-inf,-inf,-inf,0,0,0]\n+[-inf,-inf,-inf,0,inf,inf,inf]\n+[-inf,-inf,-inf,0,inf,inf,inf]\n+2\n+[-inf]\n+[-inf]\n+[inf]\n+3\n+[nan]\n+[inf]\n+[nan]\n+[-inf]\n+4\n+[nan]\n+[nan]\n+[nan]\n+[nan]\n+[0]\n+[0]\n+[0]\n+5\n+6\n+inf\n+inf\n+-inf\n+-inf\n+7\n+-inf\n+-inf\n+8\n+-inf\n+inf\n+-inf\n+-inf\n+inf\n+inf\n+-inf\n+inf\ndiff --git a/tests/queries/0_stateless/02286_quantile_tdigest_infinity.sql b/tests/queries/0_stateless/02286_quantile_tdigest_infinity.sql\nnew file mode 100644\nindex 000000000000..d21f73526740\n--- /dev/null\n+++ b/tests/queries/0_stateless/02286_quantile_tdigest_infinity.sql\n@@ -0,0 +1,54 @@\n+SELECT '1';\n+SELECT quantilesTDigestArray(0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99)(arrayResize(arrayResize([inf], 500000, -inf), 1000000, inf));\n+SELECT quantilesTDigestArray(0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99)(arrayResize(arrayResize([inf], 500000, inf), 1000000, -inf));\n+SELECT quantilesTDigestArray(0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99)(arrayResize(arrayResize([inf], 500000, inf), 1000000, 0));\n+SELECT quantilesTDigestArray(0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99)(arrayResize(arrayResize([inf], 500000, -inf), 1000000, 0));\n+SELECT quantilesTDigestArray(0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99)(arrayResize(arrayResize([0], 500000, inf), 1000000, -inf));\n+SELECT quantilesTDigestArray(0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99)(arrayResize(arrayResize([0], 500000, -inf), 1000000, inf));\n+\n+SELECT '2';\n+SELECT quantilesTDigest(0.05)(x) FROM (SELECT inf*(number%2-0.5) x FROM numbers(300));\n+SELECT quantilesTDigest(0.5)(x) FROM (SELECT inf*(number%2-0.5) x FROM numbers(300));\n+SELECT quantilesTDigest(0.95)(x) FROM (SELECT inf*(number%2-0.5) x FROM numbers(300));\n+\n+SELECT '3';\n+SELECT quantiles(0.5)(inf) FROM numbers(5);\n+SELECT quantiles(0.5)(inf) FROM numbers(300);\n+SELECT quantiles(0.5)(-inf) FROM numbers(5);\n+SELECT quantiles(0.5)(-inf) FROM numbers(300);\n+\n+SELECT '4';\n+SELECT quantiles(0.5)(arrayJoin([inf, 0, -inf]));\n+SELECT quantiles(0.5)(arrayJoin([-inf, 0, inf]));\n+SELECT quantiles(0.5)(arrayJoin([inf, -inf, 0]));\n+SELECT quantiles(0.5)(arrayJoin([-inf, inf, 0]));\n+SELECT quantiles(0.5)(arrayJoin([inf, inf, 0, -inf, -inf, -0]));\n+SELECT quantiles(0.5)(arrayJoin([inf, -inf, 0, -inf, inf, -0]));\n+SELECT quantiles(0.5)(arrayJoin([-inf, -inf, 0, inf, inf, -0]));\n+\n+SELECT '5';\n+DROP TABLE IF EXISTS issue32107;\n+CREATE TABLE issue32107(A Int64, s_quantiles AggregateFunction(quantilesTDigest(0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99), Float64)) ENGINE = AggregatingMergeTree ORDER BY A;\n+INSERT INTO issue32107 SELECT A, quantilesTDigestState(0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99)(x) FROM (SELECT 1 A, arrayJoin(cast([2.0, inf, number / 33333],'Array(Float64)')) x FROM numbers(100)) GROUP BY A;\n+OPTIMIZE TABLE issue32107 FINAL;\n+DROP TABLE IF EXISTS issue32107;\n+\n+SELECT '6';\n+SELECT quantileTDigest(inf) FROM numbers(200);\n+SELECT quantileTDigest(inf) FROM numbers(500);\n+SELECT quantileTDigest(-inf) FROM numbers(200);\n+SELECT quantileTDigest(-inf) FROM numbers(500);\n+\n+SELECT '7';\n+SELECT quantileTDigest(x) FROM (SELECT inf AS x UNION ALL SELECT -inf);\n+SELECT quantileTDigest(x) FROM (SELECT -inf AS x UNION ALL SELECT inf);\n+\n+SELECT '8';\n+SELECT quantileTDigest(x) FROM (SELECT inf AS x UNION ALL SELECT -inf UNION ALL SELECT -inf);\n+SELECT quantileTDigest(x) FROM (SELECT inf AS x UNION ALL SELECT inf UNION ALL SELECT -inf);\n+SELECT quantileTDigest(x) FROM (SELECT -inf AS x UNION ALL SELECT -inf UNION ALL SELECT -inf);\n+SELECT quantileTDigest(x) FROM (SELECT -inf AS x UNION ALL SELECT inf UNION ALL SELECT -inf);\n+SELECT quantileTDigest(x) FROM (SELECT inf AS x UNION ALL SELECT -inf UNION ALL SELECT inf);\n+SELECT quantileTDigest(x) FROM (SELECT inf AS x UNION ALL SELECT inf UNION ALL SELECT inf);\n+SELECT quantileTDigest(x) FROM (SELECT -inf AS x UNION ALL SELECT -inf UNION ALL SELECT inf);\n+SELECT quantileTDigest(x) FROM (SELECT -inf AS x UNION ALL SELECT inf UNION ALL SELECT inf);\n",
  "problem_statement": "Error with AggregateFunction quantilesTDigest causing part merges to fail\n**Describe what's wrong**\r\n\r\nWe had an issue with `INSERT`s into ClickHouse throwing an error:\r\n\r\n```\r\n2021-11-29 18:26:13.177 ESTError message from worker: ru.yandex.clickhouse.except.ClickHouseUnknownException: ClickHouse exception, code: 1002, host: <clickhouse_host>, port: 8123; Code: 252. DB::Exception: Too many parts (301). Merges are processing significantly slower than inserts: while pushing to view default.metrics_shard_stat_10m_view (aa99702d-bd2c-4aca-aa99-702dbd2c6aca): while pushing to view default.metrics_shard_stat_1m_view (c2e4400c-c18e-46ae-82e4-400cc18ef6ae). (TOO_MANY_PARTS) (version 21.11.3.6 (official build))\r\n```\r\n\r\nUpon investigation it appeared that there were a large amount of unmerged parts for the `metrics_shard_stat_10m` table. This seems to be due to this error thrown during a `deserialize` of an `AggregateFunction` `quantilesTDigest()`. This was also causing the node to have a very high CPU usage.\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nWe're using ClickHouse server version ` 21.11.3.6`\r\n\r\n**Table Schema**\r\n\r\n`metrics_shard_stat_1m`:\r\n```\r\nCREATE TABLE default.metrics_shard_stat_1m ON CLUSTER testcluster\r\n(\r\n    metric_id                  UUID,\r\n    date                       Date DEFAULT toDate(bucket) CODEC (DoubleDelta),\r\n    bucket                     DateTime CODEC (DoubleDelta, LZ4),\r\n    s_avg                      AggregateFunction(avg, Float64),\r\n    s_count                    AggregateFunction(count, Float64),\r\n    sum                        SimpleAggregateFunction(sum, Float64) CODEC (Gorilla),\r\n    sum2                       SimpleAggregateFunction(sum, Float64) CODEC (Gorilla),\r\n    min                        SimpleAggregateFunction(min, Float64) CODEC (Gorilla),\r\n    max                        SimpleAggregateFunction(max, Float64) CODEC (Gorilla),\r\n    first                      SimpleAggregateFunction(any, Float64) CODEC (Gorilla),\r\n    last                       SimpleAggregateFunction(any, Float64) CODEC (Gorilla),\r\n    s_linear_regression        AggregateFunction(simpleLinearRegression, UInt32, Float64),\r\n    s_variance                 AggregateFunction(varSamp, Float64),\r\n    s_quantiles                AggregateFunction(quantilesTDigest(0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99),\r\n                                   Float64),\r\n    s_avg_nozero               AggregateFunction(avg, Float64),\r\n    s_count_nozero             AggregateFunction(count, Float64),\r\n    sum_nozero                 SimpleAggregateFunction(sum, Float64) CODEC (Gorilla),\r\n    min_nozero                 SimpleAggregateFunction(min, Float64) CODEC (Gorilla),\r\n    max_nozero                 SimpleAggregateFunction(max, Float64) CODEC (Gorilla),\r\n    first_nozero               SimpleAggregateFunction(any, Float64) CODEC (Gorilla),\r\n    last_nozero                SimpleAggregateFunction(any, Float64) CODEC (Gorilla),\r\n    s_linear_regression_nozero AggregateFunction(simpleLinearRegression, UInt32, Float64),\r\n    s_variance_nozero          AggregateFunction(varSamp, Float64),\r\n    s_quantiles_nozero         AggregateFunction(quantilesTDigest(0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99),\r\n                                   Float64)\r\n) ENGINE = ReplicatedAggregatingMergeTree() PARTITION BY toYYYYMM(bucket)\r\n      ORDER BY (metric_id, date, bucket) SETTINGS index_granularity = 64;\r\n\r\nCREATE MATERIALIZED VIEW default.metrics_shard_stat_1m_view ON CLUSTER testcluster TO default.metrics_shard_stat_1m AS\r\nSELECT metric_id,\r\n       date,\r\n       toStartOfMinute(toDateTime64(timestamp, 3))                                         as bucket,\r\n       avgState(value)                                                                     as s_avg,\r\n       countState(value)                                                                   as s_count,\r\n       sum(value)                                                                          as sum,\r\n       sum(value * value)                                                                  as sum2,\r\n       min(value)                                                                          as min,\r\n       max(value)                                                                          as max,\r\n       any(value)                                                                          as first,\r\n       anyLast(value)                                                                      as last,\r\n       quantilesTDigestState(0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99)(value)              as s_quantiles,\r\n       simpleLinearRegressionState(toUnixTimestamp(timestamp), value)                      as s_linear_regression,\r\n       varSampState(value)                                                                 as s_variance,\r\n       avgStateIf(value, value > 0)                                                        as s_avg_nozero,\r\n       countStateIf(value, value > 0)                                                      as s_count_nozero,\r\n       sumIf(value, value > 0)                                                             as sum_nozero,\r\n       minIf(value, value > 0)                                                             as min_nozero,\r\n       maxIf(value, value > 0)                                                             as max_nozero,\r\n       anyIf(value, value > 0)                                                             as first_nozero,\r\n       anyLastIf(value, value > 0)                                                         as last_nozero,\r\n       quantilesTDigestStateIf(0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99)(value, value > 0) as s_quantiles_nozero,\r\n       simpleLinearRegressionStateIf(toUnixTimestamp(timestamp), value, value >\r\n                                                                        0)                 as s_linear_regression_nozero,\r\n       varSampStateIf(value, value > 0)                                                    as s_variance_nozero\r\nFROM default.metrics_shard\r\nGROUP BY metric_id,\r\n         date,\r\n         bucket;\r\n\r\nCREATE TABLE default.metrics_stat_1m ON CLUSTER testcluster\r\n(\r\n    metric_id                  UUID,\r\n    date                       Date DEFAULT toDate(bucket) CODEC (DoubleDelta),\r\n    bucket                     DateTime CODEC (DoubleDelta, LZ4),\r\n    s_avg                      AggregateFunction(avg, Float64),\r\n    s_count                    AggregateFunction(count, Float64),\r\n    sum                        SimpleAggregateFunction(sum, Float64) CODEC (Gorilla),\r\n    sum2                       SimpleAggregateFunction(sum, Float64) CODEC (Gorilla),\r\n    min                        SimpleAggregateFunction(min, Float64) CODEC (Gorilla),\r\n    max                        SimpleAggregateFunction(max, Float64) CODEC (Gorilla),\r\n    first                      SimpleAggregateFunction(any, Float64) CODEC (Gorilla),\r\n    last                       SimpleAggregateFunction(any, Float64) CODEC (Gorilla),\r\n    s_quantiles                AggregateFunction(quantilesTDigest(0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99),\r\n                                   Float64),\r\n    s_linear_regression        AggregateFunction(simpleLinearRegression, UInt32, Float64),\r\n    s_variance                 AggregateFunction(varSamp, Float64),\r\n    s_avg_nozero               AggregateFunction(avg, Float64),\r\n    s_count_nozero             AggregateFunction(count, Float64),\r\n    sum_nozero                 SimpleAggregateFunction(sum, Float64) CODEC (Gorilla),\r\n    min_nozero                 SimpleAggregateFunction(min, Float64) CODEC (Gorilla),\r\n    max_nozero                 SimpleAggregateFunction(max, Float64) CODEC (Gorilla),\r\n    first_nozero               SimpleAggregateFunction(any, Float64) CODEC (Gorilla),\r\n    last_nozero                SimpleAggregateFunction(any, Float64) CODEC (Gorilla),\r\n    s_quantiles_nozero         AggregateFunction(quantilesTDigest(0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99),\r\n                                   Float64),\r\n    s_linear_regression_nozero AggregateFunction(simpleLinearRegression, UInt32, Float64),\r\n    s_variance_nozero          AggregateFunction(varSamp, Float64)\r\n) ENGINE = Distributed(\r\n           testcluster,\r\n           default,\r\n           metrics_shard_stat_1m,\r\n           farmFingerprint64(metric_id)\r\n    );\r\n```\r\n\r\n`metrics_shard_stat_10m`:\r\n```\r\nCREATE TABLE default.metrics_shard_stat_10m ON cluster testcluster\r\n(\r\n    metric_id                  UUID,\r\n    date                       Date DEFAULT toDate(bucket) CODEC (DoubleDelta),\r\n    bucket                     DateTime CODEC (DoubleDelta, LZ4),\r\n    s_avg                      AggregateFunction(avg, Float64),\r\n    s_count                    AggregateFunction(count, Float64),\r\n    sum                        SimpleAggregateFunction(sum, Float64) CODEC (Gorilla),\r\n    sum2                       SimpleAggregateFunction(sum, Float64) CODEC (Gorilla),\r\n    min                        SimpleAggregateFunction(min, Float64) CODEC (Gorilla),\r\n    max                        SimpleAggregateFunction(max, Float64) CODEC (Gorilla),\r\n    first                      SimpleAggregateFunction(any, Float64) CODEC (Gorilla),\r\n    last                       SimpleAggregateFunction(any, Float64) CODEC (Gorilla),\r\n    s_linear_regression        AggregateFunction(simpleLinearRegression, UInt32, Float64),\r\n    s_variance                 AggregateFunction(varSamp, Float64),\r\n    s_quantiles                AggregateFunction(quantilesTDigest(0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99),\r\n                                   Float64),\r\n    s_avg_nozero               AggregateFunction(avg, Float64),\r\n    s_count_nozero             AggregateFunction(count, Float64),\r\n    sum_nozero                 SimpleAggregateFunction(sum, Float64) CODEC (Gorilla),\r\n    min_nozero                 SimpleAggregateFunction(min, Float64) CODEC (Gorilla),\r\n    max_nozero                 SimpleAggregateFunction(max, Float64) CODEC (Gorilla),\r\n    first_nozero               SimpleAggregateFunction(any, Float64) CODEC (Gorilla),\r\n    last_nozero                SimpleAggregateFunction(any, Float64) CODEC (Gorilla),\r\n    s_linear_regression_nozero AggregateFunction(simpleLinearRegression, UInt32, Float64),\r\n    s_variance_nozero          AggregateFunction(varSamp, Float64),\r\n    s_quantiles_nozero         AggregateFunction(quantilesTDigest(0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99),\r\n                                   Float64)\r\n) ENGINE = ReplicatedAggregatingMergeTree() PARTITION BY toYYYYMM(bucket)\r\n      ORDER BY (metric_id, date, bucket) SETTINGS index_granularity = 64;\r\n\r\nCREATE MATERIALIZED VIEW default.metrics_shard_stat_10m_view ON CLUSTER testcluster TO default.metrics_shard_stat_10m AS\r\nSELECT metric_id,\r\n       date,\r\n       toStartOfTenMinutes(toDateTime64(bucket, 3))                                             as bucket,\r\n       avgMergeState(s_avg)                                                                     as s_avg,\r\n       countMergeState(s_count)                                                                 as s_count,\r\n       sum(sum)                                                                                 as sum,\r\n       sum(sum2)                                                                                as sum2,\r\n       min(min)                                                                                 as min,\r\n       max(max)                                                                                 as max,\r\n       any(first)                                                                               as first,\r\n       anyLast(last)                                                                            as last,\r\n       quantilesTDigestMergeState(0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99)(s_quantiles)        as s_quantiles,\r\n       simpleLinearRegressionMergeState(s_linear_regression)                                    as s_linear_regression,\r\n       varSampMergeState(s_variance)                                                            as s_variance,\r\n       avgMergeState(s_avg_nozero)                                                              as s_avg_nozero,\r\n       countMergeState(s_count_nozero)                                                          as s_count_nozero,\r\n       sum(sum_nozero)                                                                          as sum_nozero,\r\n       min(min_nozero)                                                                          as min_nozero,\r\n       max(max_nozero)                                                                          as max_nozero,\r\n       any(first_nozero)                                                                        as first_nozero,\r\n       anyLast(last_nozero)                                                                     as last_nozero,\r\n       quantilesTDigestMergeState(0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99)(s_quantiles_nozero) as s_quantiles_nozero,\r\n       simpleLinearRegressionMergeState(s_linear_regression_nozero)                             as s_linear_regression_nozero,\r\n       varSampMergeState(s_variance_nozero)                                                     as s_variance_nozero\r\nFROM default.metrics_shard_stat_1m\r\nGROUP BY metric_id,\r\n         date,\r\n         bucket;\r\n\r\nCREATE TABLE default.metrics_stat_10m ON CLUSTER testcluster\r\n(\r\n    metric_id                  UUID,\r\n    date                       Date DEFAULT toDate(bucket) CODEC (DoubleDelta),\r\n    bucket                     DateTime CODEC (DoubleDelta, LZ4),\r\n    s_avg                      AggregateFunction(avg, Float64),\r\n    s_count                    AggregateFunction(count, Float64),\r\n    sum                        SimpleAggregateFunction(sum, Float64) CODEC (Gorilla),\r\n    sum2                       SimpleAggregateFunction(sum, Float64) CODEC (Gorilla),\r\n    min                        SimpleAggregateFunction(min, Float64) CODEC (Gorilla),\r\n    max                        SimpleAggregateFunction(max, Float64) CODEC (Gorilla),\r\n    first                      SimpleAggregateFunction(any, Float64) CODEC (Gorilla),\r\n    last                       SimpleAggregateFunction(any, Float64) CODEC (Gorilla),\r\n    s_quantiles                AggregateFunction(quantilesTDigest(0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99),\r\n                                   Float64),\r\n    s_linear_regression        AggregateFunction(simpleLinearRegression, UInt32, Float64),\r\n    s_variance                 AggregateFunction(varSamp, Float64),\r\n    s_avg_nozero               AggregateFunction(avg, Float64),\r\n    s_count_nozero             AggregateFunction(count, Float64),\r\n    sum_nozero                 SimpleAggregateFunction(sum, Float64) CODEC (Gorilla),\r\n    min_nozero                 SimpleAggregateFunction(min, Float64) CODEC (Gorilla),\r\n    max_nozero                 SimpleAggregateFunction(max, Float64) CODEC (Gorilla),\r\n    first_nozero               SimpleAggregateFunction(any, Float64) CODEC (Gorilla),\r\n    last_nozero                SimpleAggregateFunction(any, Float64) CODEC (Gorilla),\r\n    s_quantiles_nozero         AggregateFunction(quantilesTDigest(0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99),\r\n                                   Float64),\r\n    s_linear_regression_nozero AggregateFunction(simpleLinearRegression, UInt32, Float64),\r\n    s_variance_nozero          AggregateFunction(varSamp, Float64)\r\n) ENGINE = Distributed(\r\n           testcluster,\r\n           default,\r\n           metrics_shard_stat_10m,\r\n           farmFingerprint64(metric_id)\r\n    );\r\n```\r\n\r\n**Expected behavior**\r\n\r\n`quantilesTDigest()` function should work or throw a meaningful without causing part merges to be blocked forever.\r\n\r\n**Error message and/or stacktrace**\r\n\r\n```\r\n2021.12.01 03:58:30.375438 [ 62 ] {} <Error> void DB::MergeTreeBackgroundExecutor<DB::MergeMutateRuntimeQueue>::routine(DB::TaskRuntimeDataPtr) [Queue = DB::MergeMutateRuntimeQueue]: Code: 27. DB::Exception: Invalid centroid 2.000000:-nan: (while reading column s_quantiles): (while reading from part /var/lib/clickhouse/store/29e/29ec85cb-e183-4946-a9ec-85cbe1837946/202111_5921759_5923975_485/ from mark 5 with max_rows_to_read = 64): While executing MergeTreeSequentialSource. (CANNOT_PARSE_INPUT_ASSERTION_FAILED), Stack trace (when copying this message, always include the lines below):\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x9b63054 in /usr/bin/clickhouse\r\n1. DB::QuantileTDigest<double>::deserialize(DB::ReadBuffer&) @ 0xa51b433 in /usr/bin/clickhouse\r\n2. DB::SerializationAggregateFunction::deserializeBinaryBulk(DB::IColumn&, DB::ReadBuffer&, unsigned long, double) const @ 0x11cd71fa in /usr/bin/clickhouse\r\n3. DB::ISerialization::deserializeBinaryBulkWithMultipleStreams(COW<DB::IColumn>::immutable_ptr<DB::IColumn>&, unsigned long, DB::ISerialization::DeserializeBinaryBulkSettings&, std::__1::shared_ptr<DB::ISerialization::DeserializeBinaryBulkState>&, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > >*) const @ 0x11cd4435 in /usr/bin/clickhouse\r\n4. DB::MergeTreeReaderWide::readData(DB::NameAndTypePair const&, COW<DB::IColumn>::immutable_ptr<DB::IColumn>&, unsigned long, bool, unsigned long, unsigned long, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > >&, bool) @ 0x12e854a2 in /usr/bin/clickhouse\r\n5. DB::MergeTreeReaderWide::readRows(unsigned long, unsigned long, bool, unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&) @ 0x12e84279 in /usr/bin/clickhouse\r\n6. DB::MergeTreeSequentialSource::generate() @ 0x12e88377 in /usr/bin/clickhouse\r\n7. DB::ISource::tryGenerate() @ 0x131156b5 in /usr/bin/clickhouse\r\n8. DB::ISource::work() @ 0x1311527a in /usr/bin/clickhouse\r\n9. DB::SourceWithProgress::work() @ 0x13321062 in /usr/bin/clickhouse\r\n10. ? @ 0x13130b1b in /usr/bin/clickhouse\r\n11. DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic<bool>*) @ 0x1312cad1 in /usr/bin/clickhouse\r\n12. DB::PipelineExecutor::executeStep(std::__1::atomic<bool>*) @ 0x1312b2e5 in /usr/bin/clickhouse\r\n13. DB::PullingPipelineExecutor::pull(DB::Chunk&) @ 0x13139d4b in /usr/bin/clickhouse\r\n14. DB::PullingPipelineExecutor::pull(DB::Block&) @ 0x13139fec in /usr/bin/clickhouse\r\n15. DB::MergeTask::ExecuteAndFinalizeHorizontalPart::executeImpl() @ 0x12d3d52b in /usr/bin/clickhouse\r\n16. DB::MergeTask::ExecuteAndFinalizeHorizontalPart::execute() @ 0x12d3d48b in /usr/bin/clickhouse\r\n17. DB::MergeTask::execute() @ 0x12d4217a in /usr/bin/clickhouse\r\n18. DB::MergePlainMergeTreeTask::executeStep() @ 0x12fbceec in /usr/bin/clickhouse\r\n19. DB::MergeTreeBackgroundExecutor<DB::MergeMutateRuntimeQueue>::routine(std::__1::shared_ptr<DB::TaskRuntimeData>) @ 0x12d50bdd in /usr/bin/clickhouse\r\n20. DB::MergeTreeBackgroundExecutor<DB::MergeMutateRuntimeQueue>::threadFunction() @ 0x12d5167a in /usr/bin/clickhouse\r\n21. ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x9ba7d0a in /usr/bin/clickhouse\r\n22. ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&&...)::'lambda'()::operator()() @ 0x9ba9b27 in /usr/bin/clickhouse\r\n23. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x9ba5117 in /usr/bin/clickhouse\r\n24. ? @ 0x9ba8b1d in /usr/bin/clickhouse\r\n25. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n26. clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n (version 21.11.3.6 (official build))\r\n ```\r\n \r\n **System Settings**\r\n ```\r\n connect_timeout_with_failover_ms,1000,1,Connection timeout for selecting first healthy replica.,,,0,Milliseconds\r\nload_balancing,random,1,Which replicas (among healthy replicas) to preferably send a query to (on the first attempt) for distributed processing.,,,0,LoadBalancing\r\ndistributed_aggregation_memory_efficient,1,1,Is the memory-saving mode of distributed aggregation enabled.,,,0,Bool\r\nlog_queries,1,1,Log requests and write the log to the system table.,,,0,Bool\r\nmax_memory_usage,10000000000,1,Maximum memory usage for processing of single query. Zero means unlimited.,,,0,UInt64\r\nparallel_view_processing,1,1,Enables pushing to attached views concurrently instead of sequentially.,,,0,Bool\r\ndefault_database_engine,Ordinary,1,Default database engine.,,,0,DefaultDatabaseEngine\r\n```\r\n\n",
  "hints_text": "CC @excitoon \nCan you share `show create table metrics_shard_stat_1m` and create for MatView ?\r\nAnd `select * from system.settings where changed`\n@den-crane I've added those now to the bug description\nrepro:\r\n\r\n```sql\r\ndrop table if exists issue32107;\r\n\r\ncreate table issue32107(A Int64, s_quantiles AggregateFunction(quantilesTDigest(0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99), Float64) )\r\nENGINE = AggregatingMergeTree ORDER BY A;\r\n\r\ninsert into issue32107 select A, quantilesTDigestState(0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99)(x) FROM\r\n( select 1 A, arrayJoin(cast([inf, -2.0, -inf, number / 33333],'Array(Float64)')) x\r\n from numbers(100)\r\n) group by A;\r\n\r\noptimize table issue32107 final;\r\n\r\n(version 22.4.1):\r\nCode: 27. DB::Exception: Received from localhost:9000. \r\nDB::Exception: Invalid centroid 2.000000:nan: (while reading column s_quantiles):\r\n While executing MergeTreeSequentialSource. (CANNOT_PARSE_INPUT_ASSERTION_FAILED)\r\n(query: optimize table issue32107 final;)\r\n```\r\n\r\n```sql\r\ndrop table if exists issue32107;\r\n\r\ncreate table issue32107(A Int64, s_quantiles AggregateFunction(quantilesTDigest(0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99), Float64) )\r\nENGINE = AggregatingMergeTree ORDER BY A;\r\n\r\ninsert into issue32107 select A, quantilesTDigestState(0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99)(x) FROM\r\n( select 1 A, arrayJoin(cast([2.0, inf, number / 33333],'Array(Float64)')) x\r\n from numbers(100)\r\n) group by A;\r\n\r\noptimize table issue32107 final;\r\n\r\nInvalid centroid 2.000000:nan: (while reading column s_quantiles): \r\nWhile executing MergeTreeSequentialSource. (CANNOT_PARSE_INPUT_ASSERTION_FAILED)\r\n```\n@den-crane I found even shorter test case:\r\n```\r\ncristal :) select quantilesTDigestState(0.5)(inf) FROM numbers(300)\r\n\r\nException on client:\r\nCode: 27. DB::Exception: Invalid centroid 2.000000:-nan: while receiving packet from localhost:9000. (CANNOT_PARSE_INPUT_ASSERTION_FAILED)\r\n```\n@jamesmaidment Can you please check: `SELECT count() FROM metrics_shard WHERE value == inf OR value == -inf`?\nI guess `-/+nan` and `-/+inf` can be omitted.\r\n\r\nI think it's UB with other `quantiles` functions.\r\n```sql\r\nselect quantiles(0.5)(inf) FROM numbers(5)\r\n\u250c\u2500quantiles(0.5)(inf)\u2500\u2510\r\n\u2502 [nan]               \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nselect quantiles(0.5)(inf) FROM numbers(300)\r\n\u250c\u2500quantiles(0.5)(inf)\u2500\u2510\r\n\u2502 [inf]               \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nselect quantiles(0.5)(arrayJoin([inf, 0, -inf]))\r\n\u250c\u2500quantiles(0.5)(arrayJoin([inf, 0, -inf]))\u2500\u2510\r\n\u2502 [nan]                                     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nselect quantiles(0.5)(arrayJoin([inf, inf, 0, -inf, -inf, -0]))\r\n\u250c\u2500quantiles(0.5)(arrayJoin([inf, inf, 0, -inf, -inf, 0]))\u2500\u2510\r\n\u2502 [0]                                                     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```\r\n\r\nhttps://www.wolframalpha.com/input?i2d=true&i=median%5C%2840%291%5C%2844%29++inf%5C%2841%29\r\n`median(1,  inf) -> (undefined)`\r\n\r\n\r\n\nIsn't it better to report an error?\r\n\r\nOn Fri, Apr 22, 2022, 12:13 AM Denny Crane ***@***.***> wrote:\r\n\r\n> I guess -/+nan and -/+inf can be omitted.\r\n>\r\n> I think it's UB with other quantiles functions.\r\n>\r\n> select quantiles(0.5)(inf) FROM numbers(5)\r\n>\r\n> \u250c\u2500quantiles(0.5)(inf)\u2500\u2510\r\n>\r\n> \u2502 [nan]               \u2502\r\n>\r\n> \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n>\r\n>\r\n> select quantiles(0.5)(inf) FROM numbers(300)\r\n>\r\n> \u250c\u2500quantiles(0.5)(inf)\u2500\u2510\r\n>\r\n> \u2502 [inf]               \u2502\r\n>\r\n> \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n>\r\n>\r\n> select quantiles(0.5)(arrayJoin([inf, 0, -inf]))\r\n>\r\n> \u250c\u2500quantiles(0.5)(arrayJoin([inf, 0, -inf]))\u2500\u2510\r\n>\r\n> \u2502 [nan]                                     \u2502\r\n>\r\n> \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n>\r\n>\r\n> select quantiles(0.5)(arrayJoin([inf, inf, 0, -inf, -inf, -0]))\r\n>\r\n> \u250c\u2500quantiles(0.5)(arrayJoin([inf, inf, 0, -inf, -inf, 0]))\u2500\u2510\r\n>\r\n> \u2502 [0]                                                     \u2502\r\n>\r\n> \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n>\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/ClickHouse/ClickHouse/issues/32107#issuecomment-1105765937>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AARGSBY7HRKK2IO5MNI3OHDVGHAIDANCNFSM5JFVAA4A>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n\n> Isn't it better to report an error?\r\n\r\nIt seem like you can have 2 valid states:\r\n1) any quantile of sequence `(-inf, -inf, -inf, ...)` is `-inf` without a doubt. \r\n2) the same: any quantile of sequence `(+inf, +inf, +inf, ...)` is `+inf`\r\n\r\nBut if you will try to merge those 2 the result is not defined and `NaN` should be returned. \r\nWe can not fail with the exception during the merge. And we can not filter out bad data (it was not bad at the time of insert).\r\n\nAnd from the above, we can conclude that any quantile of the sequence of `( ...., NaN, ...)` is `NaN` (because single nan can include that 'superposition' of -inf, and +inf). \nNow I think that all quantile functions should simply discard all -/+inf -/+nan values\r\n\r\n```sql\r\nSELECT\r\n    avg(x),\r\n    median(x),\r\n    quantile(x),\r\n    quantileExact(x),\r\n    quantileTDigest(x)\r\nFROM\r\n(\r\n    SELECT inf AS x\r\n    UNION ALL\r\n    SELECT -inf\r\n)\r\n\r\n\r\n\u250c\u2500avg(x)\u2500\u252c\u2500median(x)\u2500\u252c\u2500quantile(x)\u2500\u252c\u2500quantileExact(x)\u2500\u252c\u2500quantileTDigest(x)\u2500\u2510\r\n\u2502    nan \u2502       nan \u2502         nan \u2502              inf \u2502               -inf \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\n@den-crane Yes, all quantile functions should skip `NaN` but should not skip +-inf.\n@alexey-milovidov  \r\n>but should not skip +-inf.\r\n\r\nIt makes no practical sense.\r\nIt just creates more and more UB https://github.com/ClickHouse/ClickHouse/issues/36522\n@alexey-milovidov \r\n\r\n> @den-crane Yes, all quantile functions should skip `NaN` but should not skip +-inf.\r\n\r\nAs for TDigest, it does not handle +-inf properly, so I shall make it skip these values.\r\n",
  "created_at": "2022-05-08T17:32:05Z"
}