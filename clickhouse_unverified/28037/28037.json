{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 28037,
  "instance_id": "ClickHouse__ClickHouse-28037",
  "issue_numbers": [
    "27580"
  ],
  "base_commit": "04f469e8e070ea720a95d83b8d33dd4cb976588a",
  "patch": "diff --git a/contrib/aws b/contrib/aws\nindex 7d48b2c81936..06aa8759d17f 160000\n--- a/contrib/aws\n+++ b/contrib/aws\n@@ -1,1 +1,1 @@\n-Subproject commit 7d48b2c8193679cc4516e5bd68ae4a64b94dae7d\n+Subproject commit 06aa8759d17f2032ffd5efa83969270ca9ac727b\ndiff --git a/src/Storages/StorageS3.cpp b/src/Storages/StorageS3.cpp\nindex 0e84b7a26e71..1b22d96dc803 100644\n--- a/src/Storages/StorageS3.cpp\n+++ b/src/Storages/StorageS3.cpp\n@@ -1,4 +1,5 @@\n #include <Common/config.h>\n+#include \"Parsers/ASTCreateQuery.h\"\n \n #if USE_AWS_S3\n \n@@ -192,6 +193,7 @@ StorageS3Source::StorageS3Source(\n     String name_,\n     const Block & sample_block_,\n     ContextPtr context_,\n+    std::optional<FormatSettings> format_settings_,\n     const ColumnsDescription & columns_,\n     UInt64 max_block_size_,\n     UInt64 max_single_read_retries_,\n@@ -210,6 +212,7 @@ StorageS3Source::StorageS3Source(\n     , compression_hint(compression_hint_)\n     , client(client_)\n     , sample_block(sample_block_)\n+    , format_settings(format_settings_)\n     , with_file_column(need_file)\n     , with_path_column(need_path)\n     , file_iterator(file_iterator_)\n@@ -229,7 +232,7 @@ bool StorageS3Source::initialize()\n     read_buf = wrapReadBufferWithCompressionMethod(\n         std::make_unique<ReadBufferFromS3>(client, bucket, current_key, max_single_read_retries, DBMS_DEFAULT_BUFFER_SIZE),\n         chooseCompressionMethod(current_key, compression_hint));\n-    auto input_format = FormatFactory::instance().getInput(format, *read_buf, sample_block, getContext(), max_block_size);\n+    auto input_format = FormatFactory::instance().getInput(format, *read_buf, sample_block, getContext(), max_block_size, format_settings);\n     pipeline = std::make_unique<QueryPipeline>();\n     pipeline->init(Pipe(input_format));\n \n@@ -292,6 +295,7 @@ class StorageS3Sink : public SinkToStorage\n         const String & format,\n         const Block & sample_block_,\n         ContextPtr context,\n+        std::optional<FormatSettings> format_settings_,\n         const CompressionMethod compression_method,\n         const std::shared_ptr<Aws::S3::S3Client> & client,\n         const String & bucket,\n@@ -300,10 +304,11 @@ class StorageS3Sink : public SinkToStorage\n         size_t max_single_part_upload_size)\n         : SinkToStorage(sample_block_)\n         , sample_block(sample_block_)\n+        , format_settings(format_settings_)\n     {\n         write_buf = wrapWriteBufferWithCompressionMethod(\n             std::make_unique<WriteBufferFromS3>(client, bucket, key, min_upload_part_size, max_single_part_upload_size), compression_method, 3);\n-        writer = FormatFactory::instance().getOutputStreamParallelIfPossible(format, *write_buf, sample_block, context);\n+        writer = FormatFactory::instance().getOutputStreamParallelIfPossible(format, *write_buf, sample_block, context, {}, format_settings);\n     }\n \n     String getName() const override { return \"StorageS3Sink\"; }\n@@ -336,6 +341,7 @@ class StorageS3Sink : public SinkToStorage\n \n private:\n     Block sample_block;\n+    std::optional<FormatSettings> format_settings;\n     std::unique_ptr<WriteBuffer> write_buf;\n     BlockOutputStreamPtr writer;\n     bool is_first_chunk = true;\n@@ -350,6 +356,7 @@ class PartitionedStorageS3Sink : public SinkToStorage\n         const String & format_,\n         const Block & sample_block_,\n         ContextPtr context_,\n+        std::optional<FormatSettings> format_settings_,\n         const CompressionMethod compression_method_,\n         const std::shared_ptr<Aws::S3::S3Client> & client_,\n         const String & bucket_,\n@@ -366,7 +373,7 @@ class PartitionedStorageS3Sink : public SinkToStorage\n         , key(key_)\n         , min_upload_part_size(min_upload_part_size_)\n         , max_single_part_upload_size(max_single_part_upload_size_)\n-\n+        , format_settings(format_settings_)\n     {\n         std::vector<ASTPtr> arguments(1, partition_by);\n         ASTPtr partition_by_string = makeASTFunction(FunctionToString::name, std::move(arguments));\n@@ -441,6 +448,7 @@ class PartitionedStorageS3Sink : public SinkToStorage\n     const String key;\n     size_t min_upload_part_size;\n     size_t max_single_part_upload_size;\n+    std::optional<FormatSettings> format_settings;\n \n     ExpressionActionsPtr partition_by_expr;\n     String partition_by_column_name;\n@@ -467,6 +475,7 @@ class PartitionedStorageS3Sink : public SinkToStorage\n                 format,\n                 sample_block,\n                 context,\n+                format_settings,\n                 compression_method,\n                 client,\n                 partition_bucket,\n@@ -535,6 +544,7 @@ StorageS3::StorageS3(\n     const ConstraintsDescription & constraints_,\n     const String & comment,\n     ContextPtr context_,\n+    std::optional<FormatSettings> format_settings_,\n     const String & compression_method_,\n     bool distributed_processing_)\n     : IStorage(table_id_)\n@@ -546,6 +556,7 @@ StorageS3::StorageS3(\n     , compression_method(compression_method_)\n     , name(uri_.storage_name)\n     , distributed_processing(distributed_processing_)\n+    , format_settings(format_settings_)\n {\n     context_->getGlobalContext()->getRemoteHostFilter().checkURL(uri_.uri);\n     StorageInMemoryMetadata storage_metadata;\n@@ -606,6 +617,7 @@ Pipe StorageS3::read(\n             getName(),\n             metadata_snapshot->getSampleBlock(),\n             local_context,\n+            format_settings,\n             metadata_snapshot->getColumns(),\n             max_block_size,\n             max_single_read_retries,\n@@ -638,6 +650,7 @@ SinkToStoragePtr StorageS3::write(const ASTPtr & query, const StorageMetadataPtr\n             format_name,\n             sample_block,\n             local_context,\n+            format_settings,\n             chosen_compression_method,\n             client_auth.client,\n             client_auth.uri.bucket,\n@@ -651,6 +664,7 @@ SinkToStoragePtr StorageS3::write(const ASTPtr & query, const StorageMetadataPtr\n             format_name,\n             sample_block,\n             local_context,\n+            format_settings,\n             chosen_compression_method,\n             client_auth.client,\n             client_auth.uri.bucket,\n@@ -732,6 +746,34 @@ void registerStorageS3Impl(const String & name, StorageFactory & factory)\n         for (auto & engine_arg : engine_args)\n             engine_arg = evaluateConstantExpressionOrIdentifierAsLiteral(engine_arg, args.getLocalContext());\n \n+        // Use format settings from global server context + settings from\n+        // the SETTINGS clause of the create query. Settings from current\n+        // session and user are ignored.\n+        std::optional<FormatSettings> format_settings;\n+        if (args.storage_def->settings)\n+        {\n+            FormatFactorySettings user_format_settings;\n+\n+            // Apply changed settings from global context, but ignore the\n+            // unknown ones, because we only have the format settings here.\n+            const auto & changes = args.getContext()->getSettingsRef().changes();\n+            for (const auto & change : changes)\n+            {\n+                if (user_format_settings.has(change.name))\n+                {\n+                    user_format_settings.set(change.name, change.value);\n+                }\n+            }\n+\n+            // Apply changes from SETTINGS clause, with validation.\n+            user_format_settings.applyChanges(args.storage_def->settings->changes);\n+            format_settings = getFormatSettings(args.getContext(), user_format_settings);\n+        }\n+        else\n+        {\n+            format_settings = getFormatSettings(args.getContext());\n+        }\n+\n         String url = engine_args[0]->as<ASTLiteral &>().value.safeGet<String>();\n         Poco::URI uri (url);\n         S3::URI s3_uri (uri);\n@@ -776,9 +818,11 @@ void registerStorageS3Impl(const String & name, StorageFactory & factory)\n             args.constraints,\n             args.comment,\n             args.getContext(),\n+            format_settings,\n             compression_method);\n     },\n     {\n+        .supports_settings = true,\n         .source_access_type = AccessType::S3,\n     });\n }\ndiff --git a/src/Storages/StorageS3.h b/src/Storages/StorageS3.h\nindex df4112cbfdd1..cfd7e4969283 100644\n--- a/src/Storages/StorageS3.h\n+++ b/src/Storages/StorageS3.h\n@@ -54,6 +54,7 @@ class StorageS3Source : public SourceWithProgress, WithContext\n         String name_,\n         const Block & sample_block,\n         ContextPtr context_,\n+        std::optional<FormatSettings> format_settings_,\n         const ColumnsDescription & columns_,\n         UInt64 max_block_size_,\n         UInt64 max_single_read_retries_,\n@@ -77,6 +78,7 @@ class StorageS3Source : public SourceWithProgress, WithContext\n     String compression_hint;\n     std::shared_ptr<Aws::S3::S3Client> client;\n     Block sample_block;\n+    std::optional<FormatSettings> format_settings;\n \n \n     std::unique_ptr<ReadBuffer> read_buf;\n@@ -113,6 +115,7 @@ class StorageS3 : public shared_ptr_helper<StorageS3>, public IStorage, WithCont\n         const ConstraintsDescription & constraints_,\n         const String & comment,\n         ContextPtr context_,\n+        std::optional<FormatSettings> format_settings_,\n         const String & compression_method_ = \"\",\n         bool distributed_processing_ = false);\n \n@@ -162,6 +165,7 @@ class StorageS3 : public shared_ptr_helper<StorageS3>, public IStorage, WithCont\n     String compression_method;\n     String name;\n     const bool distributed_processing;\n+    std::optional<FormatSettings> format_settings;\n \n     static void updateClientAndAuthSettings(ContextPtr, ClientAuthentication &);\n };\ndiff --git a/src/TableFunctions/TableFunctionS3.cpp b/src/TableFunctions/TableFunctionS3.cpp\nindex 83149445a7df..9878ed725602 100644\n--- a/src/TableFunctions/TableFunctionS3.cpp\n+++ b/src/TableFunctions/TableFunctionS3.cpp\n@@ -103,6 +103,8 @@ StoragePtr TableFunctionS3::executeImpl(const ASTPtr & /*ast_function*/, Context\n         ConstraintsDescription{},\n         String{},\n         context,\n+        /// No format_settings for table function S3\n+        std::nullopt,\n         compression_method);\n \n     storage->startup();\ndiff --git a/src/TableFunctions/TableFunctionS3Cluster.cpp b/src/TableFunctions/TableFunctionS3Cluster.cpp\nindex 93f90bca75a7..313ad9d4dcc8 100644\n--- a/src/TableFunctions/TableFunctionS3Cluster.cpp\n+++ b/src/TableFunctions/TableFunctionS3Cluster.cpp\n@@ -128,6 +128,8 @@ StoragePtr TableFunctionS3Cluster::executeImpl(\n             ConstraintsDescription{},\n             String{},\n             context,\n+            // No format_settings for S3Cluster\n+            std::nullopt,\n             compression_method,\n             /*distributed_processing=*/true);\n     }\n",
  "test_patch": "diff --git a/docker/test/fasttest/run.sh b/docker/test/fasttest/run.sh\nindex 00af261f6c89..b59638813a61 100755\n--- a/docker/test/fasttest/run.sh\n+++ b/docker/test/fasttest/run.sh\n@@ -377,6 +377,7 @@ function run_tests\n \n         # Depends on AWS\n         01801_s3_cluster\n+        02012_settings_clause_for_s3\n \n         # needs psql\n         01889_postgresql_protocol_null_fields\ndiff --git a/docker/test/stateless/Dockerfile b/docker/test/stateless/Dockerfile\nindex 39c8a2e53580..b66fa055e7b3 100644\n--- a/docker/test/stateless/Dockerfile\n+++ b/docker/test/stateless/Dockerfile\n@@ -49,6 +49,17 @@ RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone\n ENV NUM_TRIES=1\n ENV MAX_RUN_TIME=0\n \n+\n+# Download Minio-related binaries\n+RUN wget 'https://dl.min.io/server/minio/release/linux-amd64/minio' \\\n+    && chmod +x ./minio \\\n+    && wget 'https://dl.min.io/client/mc/release/linux-amd64/mc' \\\n+    && chmod +x ./mc\n+\n+ENV MINIO_ROOT_USER=\"clickhouse\"\n+ENV MINIO_ROOT_PASSWORD=\"clickhouse\"\n+\n COPY run.sh /\n COPY process_functional_tests_result.py /\n+COPY setup_minio.sh /\n CMD [\"/bin/bash\", \"/run.sh\"]\ndiff --git a/docker/test/stateless/run.sh b/docker/test/stateless/run.sh\nindex e5ef72e747ae..154126a38800 100755\n--- a/docker/test/stateless/run.sh\n+++ b/docker/test/stateless/run.sh\n@@ -17,6 +17,8 @@ dpkg -i package_folder/clickhouse-test_*.deb\n # install test configs\n /usr/share/clickhouse-test/config/install.sh\n \n+./setup_minio.sh\n+\n # For flaky check we also enable thread fuzzer\n if [ \"$NUM_TRIES\" -gt \"1\" ]; then\n     export THREAD_FUZZER_CPU_TIME_PERIOD_US=1000\ndiff --git a/docker/test/stateless/setup_minio.sh b/docker/test/stateless/setup_minio.sh\nnew file mode 100755\nindex 000000000000..7f8b90ee7414\n--- /dev/null\n+++ b/docker/test/stateless/setup_minio.sh\n@@ -0,0 +1,57 @@\n+#!/bin/bash\n+\n+# Usage for local run:\n+#\n+# ./docker/test/stateless/setup_minio.sh ./tests/\n+#\n+\n+set -e -x -a -u\n+\n+ls -lha\n+\n+mkdir -p ./minio_data\n+\n+if [ ! -f ./minio ]; then\n+  echo 'MinIO binary not found, downloading...'\n+\n+  BINARY_TYPE=$(uname -s | tr '[:upper:]' '[:lower:]')\n+\n+  wget \"https://dl.min.io/server/minio/release/${BINARY_TYPE}-amd64/minio\" \\\n+    && chmod +x ./minio \\\n+    && wget \"https://dl.min.io/client/mc/release/${BINARY_TYPE}-amd64/mc\" \\\n+    && chmod +x ./mc\n+fi\n+\n+MINIO_ROOT_USER=${MINIO_ROOT_USER:-clickhouse}\n+MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD:-clickhouse}\n+\n+./minio server --address \":11111\" ./minio_data &\n+\n+while ! curl -v --silent http://localhost:11111 2>&1 | grep AccessDenied\n+do\n+  echo \"Trying to connect to minio\"\n+  sleep 1\n+done\n+\n+lsof -i :11111\n+\n+sleep 5\n+\n+./mc alias set clickminio http://localhost:11111 clickhouse clickhouse\n+./mc admin user add clickminio test testtest\n+./mc admin policy set clickminio readwrite user=test\n+./mc mb clickminio/test\n+\n+\n+# Upload data to Minio. By default after unpacking all tests will in\n+# /usr/share/clickhouse-test/queries\n+\n+TEST_PATH=${1:-/usr/share/clickhouse-test}\n+MINIO_DATA_PATH=${TEST_PATH}/queries/0_stateless/data_minio\n+\n+# Iterating over globs will cause redudant FILE variale to be a path to a file, not a filename\n+# shellcheck disable=SC2045\n+for FILE in $(ls \"${MINIO_DATA_PATH}\"); do\n+    echo \"$FILE\";\n+    ./mc cp \"${MINIO_DATA_PATH}\"/\"$FILE\" clickminio/test/\"$FILE\";\n+done\ndiff --git a/tests/queries/0_stateless/01801_s3_cluster.reference b/tests/queries/0_stateless/01801_s3_cluster.reference\nindex e69de29bb2d1..5e36c5923ce9 100644\n--- a/tests/queries/0_stateless/01801_s3_cluster.reference\n+++ b/tests/queries/0_stateless/01801_s3_cluster.reference\n@@ -0,0 +1,24 @@\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+1\t2\t3\n+4\t5\t6\n+7\t8\t9\n+10\t11\t12\n+13\t14\t15\n+16\t17\t18\n+20\t21\t22\n+23\t24\t25\n+26\t27\t28\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+1\t2\t3\n+4\t5\t6\n+7\t8\t9\n+10\t11\t12\n+13\t14\t15\n+16\t17\t18\n+20\t21\t22\n+23\t24\t25\n+26\t27\t28\ndiff --git a/tests/queries/0_stateless/01801_s3_cluster.sh b/tests/queries/0_stateless/01801_s3_cluster.sh\ndeleted file mode 100755\nindex 460f3856a2f8..000000000000\n--- a/tests/queries/0_stateless/01801_s3_cluster.sh\n+++ /dev/null\n@@ -1,21 +0,0 @@\n-#!/usr/bin/env bash\n-\n-# NOTE: this is a partial copy of the 01683_dist_INSERT_block_structure_mismatch,\n-# but this test also checks the log messages\n-\n-CUR_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n-# shellcheck source=../shell_config.sh\n-. \"$CUR_DIR\"/../shell_config.sh\n-\n-if [[ -z $S3_ACCESS_KEY_ID ]]; then\n-    echo \"@@SKIP@@: Missing \\$S3_ACCESS_KEY_ID\"\n-    exit 0\n-fi\n-\n-if [[ -z $S3_SECRET_ACCESS ]]; then\n-    echo \"@@SKIP@@: Missing \\$S3_SECRET_ACCESS\"\n-    exit 0\n-fi\n-\n-${CLICKHOUSE_CLIENT_BINARY} --send_logs_level=\"none\" -q \"SELECT *  FROM s3('https://s3.mds.yandex.net/clickhouse-test-reports/*/*/functional_stateless_tests_(ubsan)/test_results.tsv',  '$S3_ACCESS_KEY_ID', '$S3_SECRET_ACCESS', 'LineAsString', 'line String') limit 100 FORMAT Null;\"\n-${CLICKHOUSE_CLIENT_BINARY} --send_logs_level=\"none\" -q \"SELECT *  FROM s3Cluster('test_cluster_two_shards', 'https://s3.mds.yandex.net/clickhouse-test-reports/*/*/functional_stateless_tests_(ubsan)/test_results.tsv',  '$S3_ACCESS_KEY_ID', '$S3_SECRET_ACCESS', 'LineAsString', 'line String') limit 100 FORMAT Null;\"\ndiff --git a/tests/queries/0_stateless/01801_s3_cluster.sql b/tests/queries/0_stateless/01801_s3_cluster.sql\nnew file mode 100644\nindex 000000000000..9c13c2572498\n--- /dev/null\n+++ b/tests/queries/0_stateless/01801_s3_cluster.sql\n@@ -0,0 +1,2 @@\n+select * from s3('http://localhost:11111/test/{a,b,c}.tsv', 'test', 'testtest', 'TSV', 'a UInt64, b UInt64, c UInt64') ORDER BY a, b, c;\n+select * from s3Cluster('test_cluster_two_shards', 'http://localhost:11111/test/{a,b,c}.tsv', 'test', 'testtest', 'TSV', 'a UInt64, b UInt64, c UInt64') ORDER BY a, b, c;\ndiff --git a/tests/queries/0_stateless/02012_settings_clause_for_s3.reference b/tests/queries/0_stateless/02012_settings_clause_for_s3.reference\nnew file mode 100644\nindex 000000000000..b8f46d348ca0\n--- /dev/null\n+++ b/tests/queries/0_stateless/02012_settings_clause_for_s3.reference\n@@ -0,0 +1,3 @@\n+ClickHouse\t1\n+Hello\t2\n+Hedhehog\t3\ndiff --git a/tests/queries/0_stateless/02012_settings_clause_for_s3.sql b/tests/queries/0_stateless/02012_settings_clause_for_s3.sql\nnew file mode 100644\nindex 000000000000..4f750fb99c48\n--- /dev/null\n+++ b/tests/queries/0_stateless/02012_settings_clause_for_s3.sql\n@@ -0,0 +1,7 @@\n+DROP TABLE IF EXISTS table_with_range;\n+\n+CREATE TABLE table_with_range(`name` String,`number` UInt32)\u3000ENGINE = S3('http://localhost:11111/test/tsv_with_header.tsv', 'test', 'testtest', 'TSVWithNames')\u3000SETTINGS input_format_with_names_use_header = 1;\n+\n+select * from table_with_range;\n+\n+DROP TABLE IF EXISTS table_with_range;\ndiff --git a/tests/queries/0_stateless/data_minio/a.tsv b/tests/queries/0_stateless/data_minio/a.tsv\nnew file mode 100644\nindex 000000000000..acd7c60768bb\n--- /dev/null\n+++ b/tests/queries/0_stateless/data_minio/a.tsv\n@@ -0,0 +1,4 @@\n+1\t2\t3\n+4\t5\t6\n+7\t8\t9\n+0\t0\t0\ndiff --git a/tests/queries/0_stateless/data_minio/b.tsv b/tests/queries/0_stateless/data_minio/b.tsv\nnew file mode 100644\nindex 000000000000..a32392250a75\n--- /dev/null\n+++ b/tests/queries/0_stateless/data_minio/b.tsv\n@@ -0,0 +1,4 @@\n+10\t11\t12\n+13\t14\t15\n+16\t17\t18\n+0\t0\t0\ndiff --git a/tests/queries/0_stateless/data_minio/c.tsv b/tests/queries/0_stateless/data_minio/c.tsv\nnew file mode 100644\nindex 000000000000..c935e93430c9\n--- /dev/null\n+++ b/tests/queries/0_stateless/data_minio/c.tsv\n@@ -0,0 +1,4 @@\n+20\t21\t22\n+23\t24\t25\n+26\t27\t28\n+0\t0\t0\ndiff --git a/tests/queries/0_stateless/data_minio/tsv_with_header.tsv b/tests/queries/0_stateless/data_minio/tsv_with_header.tsv\nnew file mode 100644\nindex 000000000000..d93a8c9ba0cb\n--- /dev/null\n+++ b/tests/queries/0_stateless/data_minio/tsv_with_header.tsv\n@@ -0,0 +1,4 @@\n+number\tname\n+1\tClickHouse\n+2\tHello\n+3\tHedhehog\n",
  "problem_statement": "S3 table engine doesn't support SETTINGS\n**Describe the unexpected behaviour**\r\nIt should be possible to pass input_format related settings to S3 table engine. (like for URL table engine)\r\n\r\n**How to reproduce**\r\nClickhouse server 21.9\r\n\r\n```\r\nCREATE TABLE table_with_range (name String, value UInt32) ENGINE = S3('https://storage.yandexcloud.net/my-test-bucket-768/{some,another}_prefix/some_file_{1..3}', 'CSV') SETTINGS input_format_with_names_use_header=0;\r\n\r\nCREATE TABLE table_with_range\r\n(\r\n    `name` String,\r\n    `value` UInt32\r\n)\r\nENGINE = S3('https://storage.yandexcloud.net/my-test-bucket-768/{some,another}_prefix/some_file_{1..3}', 'CSV')\r\nSETTINGS input_format_with_names_use_header = 0\r\n\r\n0 rows in set. Elapsed: 0.011 sec.\r\n\r\nReceived exception from server (version 21.9.1):\r\nCode: 36. DB::Exception: Received from localhost:9000. DB::Exception: Engine S3 doesn't support SETTINGS clause. Currently only the following engines have support for the feature: [RabbitMQ, Kafka, MySQL, MaterializedPostgreSQL, Join, Set, MergeTree, Memory, URL, ReplicatedVersionedCollapsingMergeTree, ReplacingMergeTree, ReplicatedSummingMergeTree, ReplicatedAggregatingMergeTree, ReplicatedCollapsingMergeTree, File, ReplicatedGraphiteMergeTree, ReplicatedMergeTree, ReplicatedReplacingMergeTree, VersionedCollapsingMergeTree, SummingMergeTree, Distributed, TinyLog, GraphiteMergeTree, CollapsingMergeTree, AggregatingMergeTree, StripeLog, Log]. (BAD_ARGUMENTS)\r\n```\r\n\r\n\n",
  "hints_text": "",
  "created_at": "2021-08-23T19:08:51Z"
}