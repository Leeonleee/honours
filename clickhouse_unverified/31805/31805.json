{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 31805,
  "instance_id": "ClickHouse__ClickHouse-31805",
  "issue_numbers": [
    "31349"
  ],
  "base_commit": "f60fdc7a8b766f9c165bc4b094730e2f7f19c12c",
  "patch": "diff --git a/src/QueryPipeline/RemoteQueryExecutor.cpp b/src/QueryPipeline/RemoteQueryExecutor.cpp\nindex b01ed7ba9a2d..578a8171089f 100644\n--- a/src/QueryPipeline/RemoteQueryExecutor.cpp\n+++ b/src/QueryPipeline/RemoteQueryExecutor.cpp\n@@ -9,6 +9,7 @@\n #include \"Core/Protocol.h\"\n #include <QueryPipeline/Pipe.h>\n #include <Processors/Sources/SourceFromSingleChunk.h>\n+#include <Processors/Transforms/LimitsCheckingTransform.h>\n #include <Storages/IStorage.h>\n #include <Storages/SelectQueryInfo.h>\n #include <Interpreters/castColumn.h>\n@@ -494,6 +495,12 @@ void RemoteQueryExecutor::sendExternalTables()\n         external_tables_data.clear();\n         external_tables_data.reserve(count);\n \n+        StreamLocalLimits limits;\n+        const auto & settings = context->getSettingsRef();\n+        limits.mode = LimitsMode::LIMITS_TOTAL;\n+        limits.speed_limits.max_execution_time = settings.max_execution_time;\n+        limits.timeout_overflow_mode = settings.timeout_overflow_mode;\n+\n         for (size_t i = 0; i < count; ++i)\n         {\n             ExternalTablesData res;\n@@ -503,7 +510,7 @@ void RemoteQueryExecutor::sendExternalTables()\n \n                 auto data = std::make_unique<ExternalTableData>();\n                 data->table_name = table.first;\n-                data->creating_pipe_callback = [cur, context = this->context]()\n+                data->creating_pipe_callback = [cur, limits, context = this->context]()\n                 {\n                     SelectQueryInfo query_info;\n                     auto metadata_snapshot = cur->getInMemoryMetadataPtr();\n@@ -519,6 +526,8 @@ void RemoteQueryExecutor::sendExternalTables()\n                         return std::make_unique<Pipe>(\n                             std::make_shared<SourceFromSingleChunk>(metadata_snapshot->getSampleBlock(), Chunk()));\n \n+                    pipe.addTransform(std::make_shared<LimitsCheckingTransform>(pipe.getHeader(), limits));\n+\n                     return std::make_unique<Pipe>(std::move(pipe));\n                 };\n \n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02116_global_in_time_limit.reference b/tests/queries/0_stateless/02116_global_in_time_limit.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/02116_global_in_time_limit.sh b/tests/queries/0_stateless/02116_global_in_time_limit.sh\nnew file mode 100755\nindex 000000000000..2db8353dde57\n--- /dev/null\n+++ b/tests/queries/0_stateless/02116_global_in_time_limit.sh\n@@ -0,0 +1,16 @@\n+#!/usr/bin/env bash\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+# This test is pretty depend on query execution time\n+# In release, it works about 14 seconds with no timeout.\n+\n+function run\n+{\n+  ${CLICKHOUSE_CLIENT} -q \"select number from remote('127.0.0.2', numbers(10)) where number global in (select number + 9 from numbers(400000000)) settings max_execution_time=3\" 2>&1 | echo > /dev/null\n+}\n+\n+export -f run\n+timeout 6 bash -c run\n",
  "problem_statement": "execution of the query ignores the max_execution_time setting\n**Describe what's wrong**\r\nsome requests do not respect the limits set by the max_execution_time setting and are executed for an arbitrarily long time.\r\nthis has been seen with queries containing the expressions `global in (_subquery)` or `global join _data`.\r\nthe trace_log for these queries contains calls inside the `DB::Context::initializeexternaltablesifset()`\r\nit is expected that during the execution of the request, the server will check from time to time whether it has gone beyond the limits.\r\n\r\n**How to reproduce**\r\n\r\n```\r\n\u250c\u2500version()\u2500\u2500\u2510\r\n\u2502 21.8.10.19 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nclickhouse-client, http\r\nexecute query `select column, count() from table1 where col global in (select col from table2 where cond) group by column` with distributed tables table1/table2 on sufficiently large cluster.\r\n\r\n**Expected behavior**\r\n\r\nafter the timeout expires, the query ends with an error of 159/160/209\r\n\r\n**Actual behavior**\r\nremote servers spend unlimited time executing their part of an external distributed request. trace_log of the corresponding requests looks like this: \r\n```\r\n/usr/bin/clickhouse\tDB::Block::~Block()\r\n/usr/bin/clickhouse\tstd::__1::__shared_ptr_pointer<std::__1::vector<DB::Block, std::__1::allocator<DB::Block> > const*, std::__1::default_delete<std::__1::vector<DB::Block, std::__1::allocator<DB::Block> > const>, std::__1::allocator<std::__1::vector<DB::Block, std::__1::allocator<DB::Block> > const> >::__on_zero_shared()\r\n/usr/bin/clickhouse\tDB::MemoryBlockOutputStream::writeSuffix()\r\n/usr/bin/clickhouse\tDB::TCPHandler::receiveData(bool)\r\n/usr/bin/clickhouse\tDB::TCPHandler::receivePacket()\r\n/usr/bin/clickhouse\tDB::TCPHandler::readDataNext(unsigned long, long)\r\n/usr/bin/clickhouse\t\r\n/usr/bin/clickhouse\tDB::Context::initializeExternalTablesIfSet()\r\n/usr/bin/clickhouse\t\r\n/usr/bin/clickhouse\tDB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum, bool)\r\n/usr/bin/clickhouse\tDB::TCPHandler::runImpl()\r\n/usr/bin/clickhouse\tDB::TCPHandler::run()\r\n/usr/bin/clickhouse\tPoco::Net::TCPServerConnection::start()\r\n/usr/bin/clickhouse\tPoco::Net::TCPServerDispatcher::run()\r\n/usr/bin/clickhouse\tPoco::PooledThread::run()\r\n/usr/bin/clickhouse\tPoco::ThreadImpl::runnableEntry(void*)\r\n/lib/x86_64-linux-gnu/libpthread-2.19.so\tstart_thread\r\n/lib/x86_64-linux-gnu/libc-2.19.so\tclone\r\n\r\n/usr/bin/clickhouse\toperator new(unsigned long)\r\n/usr/bin/clickhouse\tstd::__1::__hash_table<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, std::__1::__unordered_map_hasher<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, true>, std::__1::__unordered_map_equal<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, true>, std::__1::allocator<std::__1::__hash_value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long> > >::__rehash(unsigned long)\r\n/usr/bin/clickhouse\tvoid std::__1::allocator<DB::Block>::construct<DB::Block, DB::Block&>(DB::Block*, DB::Block&)\r\n/usr/bin/clickhouse\tDB::MemoryBlockOutputStream::writeSuffix()\r\n/usr/bin/clickhouse\tDB::TCPHandler::receiveData(bool)\r\n/usr/bin/clickhouse\tDB::TCPHandler::receivePacket()\r\n/usr/bin/clickhouse\tDB::TCPHandler::readDataNext(unsigned long, long)\r\n/usr/bin/clickhouse\t\r\n/usr/bin/clickhouse\tDB::Context::initializeExternalTablesIfSet()\r\n/usr/bin/clickhouse\t\r\n/usr/bin/clickhouse\tDB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum, bool)\r\n/usr/bin/clickhouse\tDB::TCPHandler::runImpl()\r\n/usr/bin/clickhouse\tDB::TCPHandler::run()\r\n/usr/bin/clickhouse\tPoco::Net::TCPServerConnection::start()\r\n/usr/bin/clickhouse\tPoco::Net::TCPServerDispatcher::run()\r\n/usr/bin/clickhouse\tPoco::PooledThread::run()\r\n/usr/bin/clickhouse\tPoco::ThreadImpl::runnableEntry(void*)\r\n/lib/x86_64-linux-gnu/libpthread-2.19.so\tstart_thread\r\n/lib/x86_64-linux-gnu/libc-2.19.so\tclone\r\n\r\n/usr/bin/clickhouse\tstd::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> >::vector(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&)\r\n/usr/bin/clickhouse\tvoid std::__1::allocator<DB::Block>::construct<DB::Block, DB::Block&>(DB::Block*, DB::Block&)\r\n/usr/bin/clickhouse\tDB::MemoryBlockOutputStream::writeSuffix()\r\n/usr/bin/clickhouse\tDB::TCPHandler::receiveData(bool)\r\n/usr/bin/clickhouse\tDB::TCPHandler::receivePacket()\r\n/usr/bin/clickhouse\tDB::TCPHandler::readDataNext(unsigned long, long)\r\n/usr/bin/clickhouse\t\r\n/usr/bin/clickhouse\tDB::Context::initializeExternalTablesIfSet()\r\n/usr/bin/clickhouse\t\r\n/usr/bin/clickhouse\tDB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum, bool)\r\n/usr/bin/clickhouse\tDB::TCPHandler::runImpl()\r\n/usr/bin/clickhouse\tDB::TCPHandler::run()\r\n/usr/bin/clickhouse\tPoco::Net::TCPServerConnection::start()\r\n/usr/bin/clickhouse\tPoco::Net::TCPServerDispatcher::run()\r\n/usr/bin/clickhouse\tPoco::PooledThread::run()\r\n/usr/bin/clickhouse\tPoco::ThreadImpl::runnableEntry(void*)\r\n/lib/x86_64-linux-gnu/libpthread-2.19.so\tstart_thread\r\n/lib/x86_64-linux-gnu/libc-2.19.so\tclone\r\n```\r\n\r\n**Additional context**\r\nadditional heavy filters (like substring from long string column) in `cond` not only reduce the number of rows returned from the subquery, but also increase the overall query execution time.\r\n\r\n\n",
  "hints_text": "Check with version 21.11\r\n\r\nIt's a known design issue that all limiting checks and kill_session flag checks happen ONLY when a query pipeline switches to a next block.\nAt least I can reproduce that timeout is not checked while we send data for GLOBAL IN\r\n```\r\nSELECT number\r\nFROM remote('127.0.0.{2,3,4,5}', numbers(10))\r\nWHERE number GLOBAL IN (\r\n    SELECT number + 9\r\n    FROM numbers(1000000000)\r\n)\r\nSETTINGS max_execution_time = 6\r\n\r\nQuery id: cfa4f916-d1cc-4578-9acb-6b80aabffe27\r\n\r\n\r\n0 rows in set. Elapsed: 59.323 sec.\r\n\r\nReceived exception from server (version 21.12.1):\r\nCode: 159. DB::Exception: Received from localhost:9000. DB::Exception: Timeout exceeded: elapsed 54.165430252 seconds, maximum: 6: While executing Remote. (TIMEOUT_EXCEEDED)\r\n```\n> At least I can reproduce that timeout is not checked while we send data for GLOBAL IN\r\n\r\nThat case seems to be (accidentally) covered by https://github.com/ClickHouse/ClickHouse/pull/31636\r\n\r\n```\r\nSELECT number\r\nFROM remote('127.0.0.{2,3,4,5}', numbers(10))\r\nWHERE number GLOBAL IN (\r\n    SELECT number + 9\r\n    FROM numbers(1000000000)\r\n)\r\nSETTINGS max_execution_time = 6\r\n\r\nQuery id: 53c672e6-53db-40f2-8d37-8d643c6127e0\r\n\r\n\r\n0 rows in set. Elapsed: 7.297 sec. \r\n\r\nReceived exception from server (version 21.12.1):\r\nCode: 159. DB::Exception: Received from localhost:9000. DB::Exception: Timeout exceeded: elapsed 6.000049737 seconds, maximum: 6: While executing Numbers: While processing number GLOBAL IN (_subquery1). (TIMEOUT_EXCEEDED)\r\n```",
  "created_at": "2021-11-25T15:43:08Z"
}