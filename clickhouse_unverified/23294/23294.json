{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 23294,
  "instance_id": "ClickHouse__ClickHouse-23294",
  "issue_numbers": [
    "21419"
  ],
  "base_commit": "20a5fed53e8d1ad572d865ce1f322149f5a13c11",
  "patch": "diff --git a/src/Storages/StorageReplicatedMergeTree.cpp b/src/Storages/StorageReplicatedMergeTree.cpp\nindex 10061af22e71..3b4a1ec4e16d 100644\n--- a/src/Storages/StorageReplicatedMergeTree.cpp\n+++ b/src/Storages/StorageReplicatedMergeTree.cpp\n@@ -585,42 +585,24 @@ bool StorageReplicatedMergeTree::createTableIfNotExists(const StorageMetadataPtr\n             /// This is Ok because another replica is definitely going to drop the table.\n \n             LOG_WARNING(log, \"Removing leftovers from table {} (this might take several minutes)\", zookeeper_path);\n+            String drop_lock_path = zookeeper_path + \"/dropped/lock\";\n+            Coordination::Error code = zookeeper->tryCreate(drop_lock_path, \"\", zkutil::CreateMode::Ephemeral);\n \n-            Strings children;\n-            Coordination::Error code = zookeeper->tryGetChildren(zookeeper_path, children);\n-            if (code == Coordination::Error::ZNONODE)\n+            if (code == Coordination::Error::ZNONODE || code == Coordination::Error::ZNODEEXISTS)\n             {\n-                LOG_WARNING(log, \"Table {} is already finished removing by another replica right now\", replica_path);\n+                LOG_WARNING(log, \"The leftovers from table {} were removed by another replica\", zookeeper_path);\n+            }\n+            else if (code != Coordination::Error::ZOK)\n+            {\n+                throw Coordination::Exception(code, drop_lock_path);\n             }\n             else\n             {\n-                for (const auto & child : children)\n-                    if (child != \"dropped\")\n-                        zookeeper->tryRemoveRecursive(zookeeper_path + \"/\" + child);\n-\n-                Coordination::Requests ops;\n-                Coordination::Responses responses;\n-                ops.emplace_back(zkutil::makeRemoveRequest(zookeeper_path + \"/dropped\", -1));\n-                ops.emplace_back(zkutil::makeRemoveRequest(zookeeper_path, -1));\n-                code = zookeeper->tryMulti(ops, responses);\n-\n-                if (code == Coordination::Error::ZNONODE)\n-                {\n-                    LOG_WARNING(log, \"Table {} is already finished removing by another replica right now\", replica_path);\n-                }\n-                else if (code == Coordination::Error::ZNOTEMPTY)\n-                {\n-                    throw Exception(fmt::format(\n-                        \"The old table was not completely removed from ZooKeeper, {} still exists and may contain some garbage. But it should never happen according to the logic of operations (it's a bug).\", zookeeper_path), ErrorCodes::LOGICAL_ERROR);\n-                }\n-                else if (code != Coordination::Error::ZOK)\n-                {\n-                    /// It is still possible that ZooKeeper session is expired or server is killed in the middle of the delete operation.\n-                    zkutil::KeeperMultiException::check(code, ops, responses);\n-                }\n-                else\n+                auto metadata_drop_lock = zkutil::EphemeralNodeHolder::existing(drop_lock_path, *zookeeper);\n+                if (!removeTableNodesFromZooKeeper(zookeeper, zookeeper_path, metadata_drop_lock, log))\n                 {\n-                    LOG_WARNING(log, \"The leftovers from table {} was successfully removed from ZooKeeper\", zookeeper_path);\n+                    /// Someone is recursively removing table right now, we cannot create new table until old one is removed\n+                    continue;\n                 }\n             }\n         }\n@@ -633,10 +615,6 @@ bool StorageReplicatedMergeTree::createTableIfNotExists(const StorageMetadataPtr\n         Coordination::Requests ops;\n         ops.emplace_back(zkutil::makeCreateRequest(zookeeper_path, \"\", zkutil::CreateMode::Persistent));\n \n-        /// Check that the table is not being dropped right now.\n-        ops.emplace_back(zkutil::makeCreateRequest(zookeeper_path + \"/dropped\", \"\", zkutil::CreateMode::Persistent));\n-        ops.emplace_back(zkutil::makeRemoveRequest(zookeeper_path + \"/dropped\", -1));\n-\n         ops.emplace_back(zkutil::makeCreateRequest(zookeeper_path + \"/metadata\", metadata_str,\n             zkutil::CreateMode::Persistent));\n         ops.emplace_back(zkutil::makeCreateRequest(zookeeper_path + \"/columns\", metadata_snapshot->getColumns().toString(),\n@@ -824,10 +802,18 @@ void StorageReplicatedMergeTree::dropReplica(zkutil::ZooKeeperPtr zookeeper, con\n       * because table creation is executed in single transaction that will conflict with remaining nodes.\n       */\n \n+    /// Node /dropped works like a lock that protects from concurrent removal of old table and creation of new table.\n+    /// But recursive removal may fail in the middle of operation leaving some garbage in zookeeper_path, so\n+    /// we remove it on table creation if there is /dropped node. Creating thread may remove /dropped node created by\n+    /// removing thread, and it causes race condition if removing thread is not finished yet.\n+    /// To avoid this we also create ephemeral child before starting recursive removal.\n+    /// (The existence of child node does not allow to remove parent node).\n     Coordination::Requests ops;\n     Coordination::Responses responses;\n+    String drop_lock_path = zookeeper_path + \"/dropped/lock\";\n     ops.emplace_back(zkutil::makeRemoveRequest(zookeeper_path + \"/replicas\", -1));\n     ops.emplace_back(zkutil::makeCreateRequest(zookeeper_path + \"/dropped\", \"\", zkutil::CreateMode::Persistent));\n+    ops.emplace_back(zkutil::makeCreateRequest(drop_lock_path, \"\", zkutil::CreateMode::Ephemeral));\n     Coordination::Error code = zookeeper->tryMulti(ops, responses);\n \n     if (code == Coordination::Error::ZNONODE || code == Coordination::Error::ZNODEEXISTS)\n@@ -844,46 +830,55 @@ void StorageReplicatedMergeTree::dropReplica(zkutil::ZooKeeperPtr zookeeper, con\n     }\n     else\n     {\n+        auto metadata_drop_lock = zkutil::EphemeralNodeHolder::existing(drop_lock_path, *zookeeper);\n         LOG_INFO(logger, \"Removing table {} (this might take several minutes)\", zookeeper_path);\n+        removeTableNodesFromZooKeeper(zookeeper, zookeeper_path, metadata_drop_lock, logger);\n+    }\n+}\n \n-        Strings children;\n-        code = zookeeper->tryGetChildren(zookeeper_path, children);\n-        if (code == Coordination::Error::ZNONODE)\n-        {\n-            LOG_WARNING(logger, \"Table {} is already finished removing by another replica right now\", remote_replica_path);\n-        }\n-        else\n-        {\n-            for (const auto & child : children)\n-                if (child != \"dropped\")\n-                    zookeeper->tryRemoveRecursive(zookeeper_path + \"/\" + child);\n+bool StorageReplicatedMergeTree::removeTableNodesFromZooKeeper(zkutil::ZooKeeperPtr zookeeper,\n+        const String & zookeeper_path, const zkutil::EphemeralNodeHolder::Ptr & metadata_drop_lock, Poco::Logger * logger)\n+{\n+    bool completely_removed = false;\n+    Strings children;\n+    Coordination::Error code = zookeeper->tryGetChildren(zookeeper_path, children);\n+    if (code == Coordination::Error::ZNONODE)\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"There is a race condition between creation and removal of replicated table. It's a bug\");\n \n-            ops.clear();\n-            responses.clear();\n-            ops.emplace_back(zkutil::makeRemoveRequest(zookeeper_path + \"/dropped\", -1));\n-            ops.emplace_back(zkutil::makeRemoveRequest(zookeeper_path, -1));\n-            code = zookeeper->tryMulti(ops, responses);\n \n-            if (code == Coordination::Error::ZNONODE)\n-            {\n-                LOG_WARNING(logger, \"Table {} is already finished removing by another replica right now\", remote_replica_path);\n-            }\n-            else if (code == Coordination::Error::ZNOTEMPTY)\n-            {\n-                LOG_ERROR(logger, \"Table was not completely removed from ZooKeeper, {} still exists and may contain some garbage.\",\n-                          zookeeper_path);\n-            }\n-            else if (code != Coordination::Error::ZOK)\n-            {\n-                /// It is still possible that ZooKeeper session is expired or server is killed in the middle of the delete operation.\n-                zkutil::KeeperMultiException::check(code, ops, responses);\n-            }\n-            else\n-            {\n-                LOG_INFO(logger, \"Table {} was successfully removed from ZooKeeper\", zookeeper_path);\n-            }\n-        }\n+    for (const auto & child : children)\n+        if (child != \"dropped\")\n+            zookeeper->tryRemoveRecursive(zookeeper_path + \"/\" + child);\n+\n+    Coordination::Requests ops;\n+    Coordination::Responses responses;\n+    ops.emplace_back(zkutil::makeRemoveRequest(metadata_drop_lock->getPath(), -1));\n+    ops.emplace_back(zkutil::makeRemoveRequest(zookeeper_path + \"/dropped\", -1));\n+    ops.emplace_back(zkutil::makeRemoveRequest(zookeeper_path, -1));\n+    code = zookeeper->tryMulti(ops, responses);\n+\n+    if (code == Coordination::Error::ZNONODE)\n+    {\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"There is a race condition between creation and removal of replicated table. It's a bug\");\n+    }\n+    else if (code == Coordination::Error::ZNOTEMPTY)\n+    {\n+        LOG_ERROR(logger, \"Table was not completely removed from ZooKeeper, {} still exists and may contain some garbage,\"\n+                          \"but someone is removing it right now.\", zookeeper_path);\n+    }\n+    else if (code != Coordination::Error::ZOK)\n+    {\n+        /// It is still possible that ZooKeeper session is expired or server is killed in the middle of the delete operation.\n+        zkutil::KeeperMultiException::check(code, ops, responses);\n+    }\n+    else\n+    {\n+        metadata_drop_lock->setAlreadyRemoved();\n+        completely_removed = true;\n+        LOG_INFO(logger, \"Table {} was successfully removed from ZooKeeper\", zookeeper_path);\n     }\n+\n+    return completely_removed;\n }\n \n \ndiff --git a/src/Storages/StorageReplicatedMergeTree.h b/src/Storages/StorageReplicatedMergeTree.h\nindex 9122bdafbf0e..c70556f40dfa 100644\n--- a/src/Storages/StorageReplicatedMergeTree.h\n+++ b/src/Storages/StorageReplicatedMergeTree.h\n@@ -208,6 +208,10 @@ class StorageReplicatedMergeTree final : public ext::shared_ptr_helper<StorageRe\n      */\n     static void dropReplica(zkutil::ZooKeeperPtr zookeeper, const String & zookeeper_path, const String & replica, Poco::Logger * logger);\n \n+    /// Removes table from ZooKeeper after the last replica was dropped\n+    static bool removeTableNodesFromZooKeeper(zkutil::ZooKeeperPtr zookeeper, const String & zookeeper_path,\n+                                              const zkutil::EphemeralNodeHolder::Ptr & metadata_drop_lock, Poco::Logger * logger);\n+\n     /// Get job to execute in background pool (merge, mutate, drop range and so on)\n     std::optional<JobAndPool> getDataProcessingJob() override;\n \n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01305_replica_create_drop_zookeeper.sh b/tests/queries/0_stateless/01305_replica_create_drop_zookeeper.sh\nindex e7b8091284a3..6248813c9bad 100755\n--- a/tests/queries/0_stateless/01305_replica_create_drop_zookeeper.sh\n+++ b/tests/queries/0_stateless/01305_replica_create_drop_zookeeper.sh\n@@ -11,11 +11,10 @@ function thread()\n     while true; do\n         $CLICKHOUSE_CLIENT -n -q \"DROP TABLE IF EXISTS test_table_$1 SYNC;\n             CREATE TABLE test_table_$1 (a UInt8) ENGINE = ReplicatedMergeTree('/clickhouse/tables/$CLICKHOUSE_TEST_ZOOKEEPER_PREFIX/alter_table', 'r_$1') ORDER BY tuple();\" 2>&1 |\n-                grep -vP '(^$)|(^Received exception from server)|(^\\d+\\. )|because the last replica of the table was dropped right now|is already started to be removing by another replica right now|is already finished removing by another replica right now|Removing leftovers from table|Another replica was suddenly created|was successfully removed from ZooKeeper|was created by another server at the same moment|was suddenly removed|some other replicas were created at the same time'\n+                grep -vP '(^$)|(^Received exception from server)|(^\\d+\\. )|because the last replica of the table was dropped right now|is already started to be removing by another replica right now| were removed by another replica|Removing leftovers from table|Another replica was suddenly created|was created by another server at the same moment|was suddenly removed|some other replicas were created at the same time'\n     done\n }\n \n-\n # https://stackoverflow.com/questions/9954794/execute-a-shell-function-with-timeout\n export -f thread;\n \n",
  "problem_statement": "01305_replica_create_drop_zookeeper flap\nhttps://clickhouse-test-reports.s3.yandex.net/0/bd7b540b8255475ad584e814a8eeeb1386645fda/functional_stateless_tests_(release).html#fail1\r\nserver logs\r\n\r\n```\r\n2021.03.03 07:12:43.919384 [ 34343 ] {9e90754b-c3a1-4c51-a7a1-697104df992d} <Error> TCPHandler: Code: 999, e.displayText() = DB::Exception: Can't get data for node /clickhouse/tables/test_01305/alter_table/metadata: node doesn't exist (No \r\nnode), Stack trace:                                                                                                                                                                                                                            \r\n                                                                                                                                                                                                                                               \r\n0. Coordination::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Coordination::Error, int) @ 0xfbcd5c3 in /usr/lib/debug/.build-id/1c/f54413784e79ae4a9d0206c5db6611836f80c4\r\n.debug                                                                                                                                                                                                                                         \r\n1. Coordination::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Coordination::Error) @ 0xfbcd842 in /usr/lib/debug/.build-id/1c/f54413784e79ae4a9d0206c5db6611836f80c4.debu\r\ng                                                                                                                                                                                                                                              \r\n2. zkutil::ZooKeeper::get(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Coordination::Stat*, std::__1::shared_ptr<Poco::Event> const&) @ 0xfbd4ad2 in /usr/lib/debug/.build-id/1c/f54413784e79a\r\ne4a9d0206c5db6611836f80c4.debug                                                                                                                                                                                                                \r\n3. DB::StorageReplicatedMergeTree::checkTableStructure(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&) @ 0xf3b8f13 in /usr/lib/de\r\nbug/.build-id/1c/f54413784e79ae4a9d0206c5db6611836f80c4.debug                                                                                                                                                                                  \r\n4. DB::StorageReplicatedMergeTree::StorageReplicatedMergeTree(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>\r\n > const&, bool, DB::StorageID const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::StorageInMemoryMetadata const&, DB::Context&, std::__1::basic_string<char, std::__1::char_traits<char>\r\n, std::__1::allocator<char> > const&, DB::MergeTreeData::MergingParams const&, std::__1::unique_ptr<DB::MergeTreeSettings, std::__1::default_delete<DB::MergeTreeSettings> >, bool, bool) @ 0xf3b2965 in /usr/lib/debug/.build-id/1c/f54413784e\r\n79ae4a9d0206c5db6611836f80c4.debug\r\n5. DB::create(DB::StorageFactory::Arguments const&) @ 0xf7acdbf in /usr/lib/debug/.build-id/1c/f54413784e79ae4a9d0206c5db6611836f80c4.debug\r\n6. DB::StorageFactory::get(DB::ASTCreateQuery const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, DB::Context&, DB::ColumnsDescription const&, DB::ConstraintsDescription const\r\n&, bool) const @ 0xf2fcf59 in /usr/lib/debug/.build-id/1c/f54413784e79ae4a9d0206c5db6611836f80c4.debug\r\n7. DB::InterpreterCreateQuery::doCreateTable(DB::ASTCreateQuery&, DB::InterpreterCreateQuery::TableProperties const&) @ 0xebcc0b4 in /usr/lib/debug/.build-id/1c/f54413784e79ae4a9d0206c5db6611836f80c4.debug\r\n8. DB::InterpreterCreateQuery::createTable(DB::ASTCreateQuery&) @ 0xebc8f52 in /usr/lib/debug/.build-id/1c/f54413784e79ae4a9d0206c5db6611836f80c4.debug\r\n9. DB::InterpreterCreateQuery::execute() @ 0xebce4d0 in /usr/lib/debug/.build-id/1c/f54413784e79ae4a9d0206c5db6611836f80c4.debug\r\n10. DB::executeQueryImpl(char const*, char const*, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer*) @ 0xf11d9f2 in /usr/lib/debug/.build-id/1c/f54413784e79ae4a9d0206c5db6611836f80c4.debug\r\n11. DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool) @ 0xf11c333 in /usr/lib/debug/.build-id/1c/f54413784e79ae4a9d0206c\r\n5db6611836f80c4.debug\r\n12. DB::TCPHandler::runImpl() @ 0xf8927ad in /usr/lib/debug/.build-id/1c/f54413784e79ae4a9d0206c5db6611836f80c4.debug\r\n13. DB::TCPHandler::run() @ 0xf8a4b39 in /usr/lib/debug/.build-id/1c/f54413784e79ae4a9d0206c5db6611836f80c4.debug\r\n14. Poco::Net::TCPServerConnection::start() @ 0x11f5642f in /usr/lib/debug/.build-id/1c/f54413784e79ae4a9d0206c5db6611836f80c4.debug\r\n15. Poco::Net::TCPServerDispatcher::run() @ 0x11f57e41 in /usr/lib/debug/.build-id/1c/f54413784e79ae4a9d0206c5db6611836f80c4.debug\r\n16. Poco::PooledThread::run() @ 0x1208e569 in /usr/lib/debug/.build-id/1c/f54413784e79ae4a9d0206c5db6611836f80c4.debug\r\n17. Poco::ThreadImpl::runnableEntry(void*) @ 0x1208a3ca in /usr/lib/debug/.build-id/1c/f54413784e79ae4a9d0206c5db6611836f80c4.debug\r\n18. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n19. __clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n```\n",
  "hints_text": "https://clickhouse-test-reports.s3.yandex.net/0/b399f80b7e8b140ebcd552e443c8f537b5353bf3/functional_stateless_tests_(address).html\r\n\r\nhttps://clickhouse-test-reports.s3.yandex.net/0/b399f80b7e8b140ebcd552e443c8f537b5353bf3/functional_stateless_tests_(address)/test_run.txt.out.log\r\n```\r\n2021-03-10 15:54:56 [5696bab96c6b] 2021.03.10 15:54:50.502180 [ 70037 ] {f1985048-e5bd-4ce8-bc6e-80f6f494e7da} <Error> executeQuery: Code: 253, e.displayText() = DB::Exception: Cannot create table, because it is created concurrently every time or because of wrong zookeeper_path or because of logical error (version 21.4.1.6208 (official build)) (from [::1]:40240) (comment: '/usr/share/clickhouse-test/queries/0_stateless/01753_fix_clickhouse_format.sh') (in query: CREATE TABLE test_table_2 (a UInt8) ENGINE = ReplicatedMergeTree('/clickhouse/tables/test_01305/alter_table', 'r_2') ORDER BY tuple();), Stack trace (when copying this message, always include the lines below):\r\n2021-03-10 15:54:56 Code: 253. DB::Exception: Received from localhost:9000. DB::Exception: Cannot create table, because it is created concurrently every time or because of wrong zookeeper_path or because of logical error. \r\n2021-03-10 15:54:56 [5696bab96c6b] 2021.03.10 15:54:50.648438 [ 50065 ] {aada05ee-04b7-44d7-84ee-5d1df30a27a5} <Error> executeQuery: Code: 253, e.displayText() = DB::Exception: Cannot create table, because it is created concurrently every time or because of wrong zookeeper_path or because of logical error (version 21.4.1.6208 (official build)) (from [::1]:40246) (comment: '/usr/share/clickhouse-test/queries/0_stateless/01753_fix_clickhouse_format.sh') (in query: CREATE TABLE test_table_1 (a UInt8) ENGINE = ReplicatedMergeTree('/clickhouse/tables/test_01305/alter_table', 'r_1') ORDER BY tuple();), Stack trace (when copying this message, always include the lines below):\r\n2021-03-10 15:54:56 Code: 253. DB::Exception: Received from localhost:9000. DB::Exception: Cannot create table, because it is created concurrently every time or because of wrong zookeeper_path or because of logical error. \r\n2021-03-10 15:54:56 [5696bab96c6b] 2021.03.10 15:54:52.290361 [ 70037 ] {bb56d4e9-ce2b-4ee3-8248-892176d298c1} <Error> executeQuery: Code: 253, e.displayText() = DB::Exception: Cannot create table, because it is created concurrently every time or because of wrong zookeeper_path or because of logical error (version 21.4.1.6208 (official build)) (from [::1]:40254) (comment: '/usr/share/clickhouse-test/queries/0_stateless/01753_fix_clickhouse_format.sh') (in query: CREATE TABLE test_table_2 (a UInt8) ENGINE = ReplicatedMergeTree('/clickhouse/tables/test_01305/alter_table', 'r_2') ORDER BY tuple();), Stack trace (when copying this message, always include the lines below):\r\n2021-03-10 15:54:56 Code: 253. DB::Exception: Received from localhost:9000. DB::Exception: Cannot create table, because it is created concurrently every time or because of wrong zookeeper_path or because of logical error. \r\n2021-03-10 15:54:56 [5696bab96c6b] 2021.03.10 15:54:52.512489 [ 50065 ] {aa089801-0688-43bb-85d1-4d879a216b9d} <Error> executeQuery: Code: 253, e.displayText() = DB::Exception: Cannot create table, because it is created concurrently every time or because of wrong zookeeper_path or because of logical error (version 21.4.1.6208 (official build)) (from [::1]:40256) (comment: '/usr/share/clickhouse-test/queries/0_stateless/01753_fix_clickhouse_format.sh') (in query: CREATE TABLE test_table_1 (a UInt8) ENGINE = ReplicatedMergeTree('/clickhouse/tables/test_01305/alter_table', 'r_1') ORDER BY tuple();), Stack trace (when copying this message, always include the lines below):\r\n2021-03-10 15:54:56 Code: 253. DB::Exception: Received from localhost:9000. DB::Exception: Cannot create table, because it is created concurrently every time or because of wrong zookeeper_path or because of logical error. \r\n2021-03-10 15:54:56 [5696bab96c6b] 2021.03.10 15:54:54.118407 [ 70037 ] {bd08a65d-6a6b-44b4-9544-60ffff282fe6} <Error> executeQuery: Code: 253, e.displayText() = DB::Exception: Cannot create table, because it is created concurrently every time or because of wrong zookeeper_path or because of logical error (version 21.4.1.6208 (official build)) (from [::1]:40268) (comment: '/usr/share/clickhouse-test/queries/0_stateless/01753_fix_clickhouse_format.sh') (in query: CREATE TABLE test_table_2 (a UInt8) ENGINE = ReplicatedMergeTree('/clickhouse/tables/test_01305/alter_table', 'r_2') ORDER BY tuple();), Stack trace (when copying this message, always include the lines below):\r\n2021-03-10 15:54:56 Code: 253. DB::Exception: Received from localhost:9000. DB::Exception: Cannot create table, because it is created concurrently every time or because of wrong zookeeper_path or because of logical error. \r\n2021-03-10 15:54:56 [5696bab96c6b] 2021.03.10 15:54:54.376153 [ 50065 ] {11d43f8b-2912-424e-a287-5b475d9ef518} <Error> executeQuery: Code: 253, e.displayText() = DB::Exception: Cannot create table, because it is created concurrently every time or because of wrong zookeeper_path or because of logical error (version 21.4.1.6208 (official build)) (from [::1]:40270) (comment: '/usr/share/clickhouse-test/queries/0_stateless/01753_fix_clickhouse_format.sh') (in query: CREATE TABLE test_table_1 (a UInt8) ENGINE = ReplicatedMergeTree('/clickhouse/tables/test_01305/alter_table', 'r_1') ORDER BY tuple();), Stack trace (when copying this message, always include the lines below):\r\n2021-03-10 15:54:56 Code: 253. DB::Exception: Received from localhost:9000. DB::Exception: Cannot create table, because it is created concurrently every time or because of wrong zookeeper_path or because of logical error. \r\n2021-03-10 15:54:56 \r\n```",
  "created_at": "2021-04-19T08:26:42Z"
}