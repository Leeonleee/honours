{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 54164,
  "instance_id": "ClickHouse__ClickHouse-54164",
  "issue_numbers": [
    "54156"
  ],
  "base_commit": "97061f68061fed363a8e24164a1a3f07bcc4553b",
  "patch": "diff --git a/src/Processors/QueryPlan/PartsSplitter.cpp b/src/Processors/QueryPlan/PartsSplitter.cpp\nindex 61c6422de5aa..8ba146e2f0fc 100644\n--- a/src/Processors/QueryPlan/PartsSplitter.cpp\n+++ b/src/Processors/QueryPlan/PartsSplitter.cpp\n@@ -6,14 +6,17 @@\n #include <vector>\n \n #include <Core/Field.h>\n+#include <DataTypes/DataTypeLowCardinality.h>\n+#include <DataTypes/DataTypeNullable.h>\n #include <Interpreters/ExpressionAnalyzer.h>\n #include <Interpreters/TreeRewriter.h>\n #include <Parsers/ASTFunction.h>\n #include <Parsers/ASTIdentifier.h>\n #include <Processors/QueryPlan/PartsSplitter.h>\n #include <Processors/Transforms/FilterSortedStreamByRange.h>\n-#include <Storages/MergeTree/RangesInDataPart.h>\n #include <Storages/MergeTree/IMergeTreeDataPart.h>\n+#include <Storages/MergeTree/RangesInDataPart.h>\n+#include <Common/FieldVisitorsAccurateComparison.h>\n \n using namespace DB;\n \n@@ -38,7 +41,11 @@ class IndexAccess\n         const auto & index = parts[part_idx].data_part->index;\n         Values values(index.size());\n         for (size_t i = 0; i < values.size(); ++i)\n+        {\n             index[i]->get(mark, values[i]);\n+            if (values[i].isNull())\n+                values[i] = POSITIVE_INFINITY;\n+        }\n         return values;\n     }\n \n@@ -61,9 +68,10 @@ class IndexAccess\n /// Will try to produce exactly max_layer layers but may return less if data is distributed in not a very parallelizable way.\n std::pair<std::vector<Values>, std::vector<RangesInDataParts>> split(RangesInDataParts parts, size_t max_layers)\n {\n-    // We will advance the iterator pointing to the mark with the smallest PK value until there will be not less than rows_per_layer rows in the current layer (roughly speaking).\n-    // Then we choose the last observed value as the new border, so the current layer will consists of granules with values greater than the previous mark and less or equal\n-    // than the new border.\n+    // We will advance the iterator pointing to the mark with the smallest PK value until\n+    // there will be not less than rows_per_layer rows in the current layer (roughly speaking).\n+    // Then we choose the last observed value as the new border, so the current layer will consists\n+    // of granules with values greater than the previous mark and less or equal than the new border.\n \n     struct PartsRangesIterator\n     {\n@@ -78,7 +86,24 @@ std::pair<std::vector<Values>, std::vector<RangesInDataParts>> split(RangesInDat\n             RangeEnd,\n         };\n \n-        [[ maybe_unused ]] bool operator<(const PartsRangesIterator & other) const { return std::tie(value, event) > std::tie(other.value, other.event); }\n+        [[maybe_unused]] bool operator<(const PartsRangesIterator & other) const\n+        {\n+            // Accurate comparison of std::tie(value, event) > std::tie(other.value, other.event);\n+\n+            for (size_t i = 0; i < value.size(); ++i)\n+            {\n+                if (applyVisitor(FieldVisitorAccurateLess(), value[i], other.value[i]))\n+                    return false;\n+\n+                if (!applyVisitor(FieldVisitorAccurateEquals(), value[i], other.value[i]))\n+                    return true;\n+            }\n+\n+            if (event > other.event)\n+                return true;\n+\n+            return false;\n+        }\n \n         Values value;\n         MarkRangeWithPartIdx range;\n@@ -182,10 +207,7 @@ std::pair<std::vector<Values>, std::vector<RangesInDataParts>> split(RangesInDat\n \n             if (it.second)\n                 result_layers.back().emplace_back(\n-                    parts[part_idx].data_part,\n-                    parts[part_idx].alter_conversions,\n-                    parts[part_idx].part_index_in_query,\n-                    MarkRanges{mark});\n+                    parts[part_idx].data_part, parts[part_idx].alter_conversions, parts[part_idx].part_index_in_query, MarkRanges{mark});\n             else\n                 current_layer[it.first->second].ranges.push_back(mark);\n \n@@ -204,7 +226,7 @@ std::pair<std::vector<Values>, std::vector<RangesInDataParts>> split(RangesInDat\n }\n \n \n-/// Will return borders.size()+1 filters in total, i-th filter will accept rows with PK values within the range [borders[i-1], borders[i]).\n+/// Will return borders.size()+1 filters in total, i-th filter will accept rows with PK values within the range (borders[i-1], borders[i]].\n ASTs buildFilters(const KeyDescription & primary_key, const std::vector<Values> & borders)\n {\n     auto add_and_condition = [&](ASTPtr & result, const ASTPtr & foo) { result = (!result) ? foo : makeASTFunction(\"and\", result, foo); };\n@@ -212,23 +234,47 @@ ASTs buildFilters(const KeyDescription & primary_key, const std::vector<Values>\n     /// Produces ASTPtr to predicate (pk_col0, pk_col1, ... , pk_colN) > (value[0], value[1], ... , value[N]), possibly with conversions.\n     /// For example, if table PK is (a, toDate(d)), where `a` is UInt32 and `d` is DateTime, and PK columns values are (8192, 19160),\n     /// it will build the following predicate: greater(tuple(a, toDate(d)), tuple(8192, cast(19160, 'Date'))).\n-    auto lexicographically_greater = [&](const Values & value)\n+    auto lexicographically_greater = [&](const Values & values) -> ASTPtr\n     {\n-        // PK may contain functions of the table columns, so we need the actual PK AST with all expressions it contains.\n-        ASTPtr pk_columns_as_tuple = makeASTFunction(\"tuple\", primary_key.expression_list_ast->children);\n-\n-        ASTPtr value_ast = std::make_shared<ASTExpressionList>();\n-        for (size_t i = 0; i < value.size(); ++i)\n+        ASTs pks_ast;\n+        ASTs values_ast;\n+        for (size_t i = 0; i < values.size(); ++i)\n         {\n-            const auto & types = primary_key.data_types;\n-            ASTPtr component_ast = std::make_shared<ASTLiteral>(value[i]);\n+            /// NULL is treated as a terminator for > comparison.\n+            if (values[i].isNull())\n+                break;\n+\n+            const auto & type = primary_key.data_types.at(i);\n+\n+            // PK may contain functions of the table columns, so we need the actual PK AST with all expressions it contains.\n+            auto pk_ast = primary_key.expression_list_ast->children.at(i);\n+\n+            // If PK is nullable, prepend a null mask column for > comparison.\n+            // Also transform the AST into assumeNotNull(pk) so that the result type is not-nullable.\n+            if (type->isNullable())\n+            {\n+                pks_ast.push_back(makeASTFunction(\"isNull\", pk_ast));\n+                values_ast.push_back(std::make_shared<ASTLiteral>(0));\n+                pk_ast = makeASTFunction(\"assumeNotNull\", pk_ast);\n+            }\n+\n+            ASTPtr component_ast = std::make_shared<ASTLiteral>(values[i]);\n+            auto decayed_type = removeNullable(removeLowCardinality(primary_key.data_types.at(i)));\n             // Values of some types (e.g. Date, DateTime) are stored in columns as numbers and we get them as just numbers from the index.\n             // So we need an explicit Cast for them.\n-            if (isColumnedAsNumber(types.at(i)->getTypeId()) && !isNumber(types.at(i)->getTypeId()))\n-                component_ast = makeASTFunction(\"cast\", std::move(component_ast), std::make_shared<ASTLiteral>(types.at(i)->getName()));\n-            value_ast->children.push_back(std::move(component_ast));\n+            if (isColumnedAsNumber(decayed_type->getTypeId()) && !isNumber(decayed_type->getTypeId()))\n+                component_ast = makeASTFunction(\"cast\", std::move(component_ast), std::make_shared<ASTLiteral>(decayed_type->getName()));\n+\n+            pks_ast.push_back(std::move(pk_ast));\n+            values_ast.push_back(std::move(component_ast));\n         }\n-        ASTPtr values_as_tuple = makeASTFunction(\"tuple\", value_ast->children);\n+\n+        /// It indicates (pk1, ...) > (NULL, ...), which is an always false predicate.\n+        if (pks_ast.empty())\n+            return std::make_shared<ASTLiteral>(0u);\n+\n+        ASTPtr pk_columns_as_tuple = makeASTFunction(\"tuple\", pks_ast);\n+        ASTPtr values_as_tuple = makeASTFunction(\"tuple\", values_ast);\n \n         return makeASTFunction(\"greater\", pk_columns_as_tuple, values_as_tuple);\n     };\n@@ -289,7 +335,7 @@ Pipes buildPipesForReadingByPKRanges(\n     ReadingInOrderStepGetter && reading_step_getter)\n {\n     if (max_layers <= 1)\n-        throw Exception(ErrorCodes::LOGICAL_ERROR, \"max_layer should be greater than 1.\");\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"max_layer should be greater than 1\");\n \n     auto && [borders, result_layers] = split(std::move(parts), max_layers);\n     auto filters = buildFilters(primary_key, borders);\n@@ -308,7 +354,7 @@ Pipes buildPipesForReadingByPKRanges(\n         reorderColumns(*actions, pipes[i].getHeader(), filter_function->getColumnName());\n         ExpressionActionsPtr expression_actions = std::make_shared<ExpressionActions>(std::move(actions));\n         auto description = fmt::format(\n-            \"filter values in [{}, {})\", i ? ::toString(borders[i - 1]) : \"-inf\", i < borders.size() ? ::toString(borders[i]) : \"+inf\");\n+            \"filter values in ({}, {}]\", i ? ::toString(borders[i - 1]) : \"-inf\", i < borders.size() ? ::toString(borders[i]) : \"+inf\");\n         pipes[i].addSimpleTransform(\n             [&](const Block & header)\n             {\ndiff --git a/src/Processors/QueryPlan/PartsSplitter.h b/src/Processors/QueryPlan/PartsSplitter.h\nindex 4ba655a6f6d0..92ba6191e97d 100644\n--- a/src/Processors/QueryPlan/PartsSplitter.h\n+++ b/src/Processors/QueryPlan/PartsSplitter.h\n@@ -14,7 +14,8 @@ namespace DB\n using ReadingInOrderStepGetter = std::function<Pipe(RangesInDataParts)>;\n \n /// Splits parts into layers, each layer will contain parts subranges with PK values from its own range.\n-/// A separate pipe will be constructed for each layer with a reading step (provided by the reading_step_getter) and a filter for this layer's range of PK values.\n+/// A separate pipe will be constructed for each layer with a reading step (provided by the reading_step_getter) and\n+/// a filter for this layer's range of PK values.\n /// Will try to produce exactly max_layer pipes but may return less if data is distributed in not a very parallelizable way.\n Pipes buildPipesForReadingByPKRanges(\n     const KeyDescription & primary_key,\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01861_explain_pipeline.reference b/tests/queries/0_stateless/01861_explain_pipeline.reference\nindex 427b3eaefc08..6874d5f2f3d7 100644\n--- a/tests/queries/0_stateless/01861_explain_pipeline.reference\n+++ b/tests/queries/0_stateless/01861_explain_pipeline.reference\n@@ -18,11 +18,11 @@ ExpressionTransform \u00d7 2\n   ExpressionTransform \u00d7 2\n     ReplacingSorted\n       FilterSortedStreamByRange\n-      Description: filter values in [(5), +inf)\n+      Description: filter values in ((5), +inf]\n         ExpressionTransform\n           MergeTreeInOrder 0 \u2192 1\n             ReplacingSorted 2 \u2192 1\n               FilterSortedStreamByRange \u00d7 2\n-              Description: filter values in [-inf, (5))\n+              Description: filter values in (-inf, (5)]\n                 ExpressionTransform \u00d7 2\n                   MergeTreeInOrder \u00d7 2 0 \u2192 1\ndiff --git a/tests/queries/0_stateless/02780_final_streams_data_skipping_index.reference b/tests/queries/0_stateless/02780_final_streams_data_skipping_index.reference\nindex 5242c6253250..e7866cbc75d9 100644\n--- a/tests/queries/0_stateless/02780_final_streams_data_skipping_index.reference\n+++ b/tests/queries/0_stateless/02780_final_streams_data_skipping_index.reference\n@@ -10,12 +10,12 @@ ExpressionTransform \u00d7 2\n     ExpressionTransform \u00d7 2\n       AggregatingSortedTransform 2 \u2192 1\n         FilterSortedStreamByRange \u00d7 2\n-        Description: filter values in [(999424), +inf)\n+        Description: filter values in ((999424), +inf]\n           ExpressionTransform \u00d7 2\n             MergeTreeInOrder \u00d7 2 0 \u2192 1\n               AggregatingSortedTransform\n                 FilterSortedStreamByRange\n-                Description: filter values in [-inf, (999424))\n+                Description: filter values in (-inf, (999424)]\n                   ExpressionTransform\n                     MergeTreeInOrder 0 \u2192 1\n EXPLAIN PIPELINE SELECT * FROM data FINAL WHERE v1 >= now() - INTERVAL 180 DAY\n@@ -29,11 +29,11 @@ ExpressionTransform \u00d7 2\n     ExpressionTransform \u00d7 2\n       AggregatingSortedTransform 2 \u2192 1\n         FilterSortedStreamByRange \u00d7 2\n-        Description: filter values in [(999424), +inf)\n+        Description: filter values in ((999424), +inf]\n           ExpressionTransform \u00d7 2\n             MergeTreeInOrder \u00d7 2 0 \u2192 1\n               AggregatingSortedTransform\n                 FilterSortedStreamByRange\n-                Description: filter values in [-inf, (999424))\n+                Description: filter values in (-inf, (999424)]\n                   ExpressionTransform\n                     MergeTreeInOrder 0 \u2192 1\ndiff --git a/tests/queries/0_stateless/02867_nullable_primary_key_final.reference b/tests/queries/0_stateless/02867_nullable_primary_key_final.reference\nnew file mode 100644\nindex 000000000000..2e55b120f6eb\n--- /dev/null\n+++ b/tests/queries/0_stateless/02867_nullable_primary_key_final.reference\n@@ -0,0 +1,2 @@\n+2023-09-01\t2500000000\n+2023-09-01\t166167\ndiff --git a/tests/queries/0_stateless/02867_nullable_primary_key_final.sql b/tests/queries/0_stateless/02867_nullable_primary_key_final.sql\nnew file mode 100644\nindex 000000000000..05677789459f\n--- /dev/null\n+++ b/tests/queries/0_stateless/02867_nullable_primary_key_final.sql\n@@ -0,0 +1,57 @@\n+DROP TABLE IF EXISTS t;\n+\n+CREATE TABLE t\n+(\n+    `d` Nullable(Date),\n+    `f1` Nullable(String),\n+    `f2` Nullable(String),\n+    `c` Nullable(Int64)\n+)\n+ENGINE = ReplacingMergeTree\n+ORDER BY (f1, f2, d)\n+SETTINGS allow_nullable_key = 1;\n+\n+INSERT INTO t SELECT\n+    toDate('2023-09-10', 'UTC') AS d,\n+    [number % 99999, NULL][number % 2] AS f1,\n+    ['x', NULL][number % 2] AS f2,\n+    [number, NULL][number % 2] AS c\n+FROM numbers(100000);\n+\n+SELECT\n+    date_trunc('month', d),\n+    SUM(c)\n+FROM t\n+FINAL\n+WHERE f2 = 'x'\n+GROUP BY 1;\n+\n+DROP TABLE t;\n+\n+CREATE TABLE t\n+(\n+    `d` Nullable(Date),\n+    `f1` Nullable(String),\n+    `f2` Nullable(String),\n+    `c` Nullable(Int64)\n+)\n+ENGINE = SummingMergeTree\n+ORDER BY (f1, f2, d)\n+SETTINGS allow_nullable_key = 1, index_granularity = 1;\n+\n+INSERT INTO t SELECT\n+    toDate('2023-09-10', 'UTC') AS d,\n+    NULL AS f1,\n+    ['x', 'y', 'z'][number % 3] AS f2,\n+    number AS c\n+FROM numbers(1000);\n+\n+SELECT\n+    date_trunc('month', d),\n+    SUM(c)\n+FROM t\n+FINAL\n+WHERE f2 = 'x'\n+GROUP BY 1;\n+\n+DROP TABLE t;\n",
  "problem_statement": "ILLEGAL_TYPE_OF_ARGUMENT allow_nullable_key + final\nhttps://fiddle.clickhouse.com/796d6997-b571-4e4f-aede-1e18f670ec6f\r\n\r\n```sql\r\nCREATE TABLE t\r\n(\r\n    d Nullable(Date),\r\n    f1 Nullable(String),\r\n    f2 Nullable(String),\r\n    c Nullable(Int64)\r\n)\r\nENGINE = ReplacingMergeTree()\r\nORDER BY (f1, f2, d)\r\nSETTINGS allow_nullable_key = 1;\r\n\r\ninsert into t select today() d, \r\n  [number%999999, null][number%2] f1,  \r\n  ['x', null][number%2] f2, \r\n  [number, null][number%2] c \r\nfrom numbers(1000000);\r\n\r\nSELECT date_trunc('month', d), SUM( c )  \r\nFROM t FINAL WHERE f2 = 'x' GROUP BY 1;\r\n\r\nReceived exception from server (version 23.7.5):\r\nCode: 43. DB::Exception: Received from localhost:9000. \r\nDB::Exception: Illegal types of arguments (Date, UInt16) of function greater: \r\nWhile processing NOT ((f1, f2, d) > ('203217', 'x', 19600)). (ILLEGAL_TYPE_OF_ARGUMENT)\r\n```\r\n\r\ncc @amosbird \n",
  "hints_text": "cc: @amosbird ",
  "created_at": "2023-09-01T08:56:45Z"
}