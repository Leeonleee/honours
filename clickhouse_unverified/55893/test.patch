diff --git a/tests/queries/0_stateless/02010_lc_native.python b/tests/queries/0_stateless/02010_lc_native.python
index 219fdf044722..6c4220855c86 100755
--- a/tests/queries/0_stateless/02010_lc_native.python
+++ b/tests/queries/0_stateless/02010_lc_native.python
@@ -1,33 +1,227 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
+import socket
 import os
-import sys
+import uuid
 
-CURDIR = os.path.dirname(os.path.realpath(__file__))
-sys.path.insert(0, os.path.join(CURDIR, "helpers"))
+CLICKHOUSE_HOST = os.environ.get("CLICKHOUSE_HOST", "127.0.0.1")
+CLICKHOUSE_PORT = int(os.environ.get("CLICKHOUSE_PORT_TCP", "900000"))
+CLICKHOUSE_DATABASE = os.environ.get("CLICKHOUSE_DATABASE", "default")
+CLIENT_NAME = "simple native protocol"
 
-from tcp_client import (
-    TCPClient,
-    CLICKHOUSE_DATABASE,
-    writeVarUInt,
-    writeStringBinary,
-    serializeBlockInfo,
-    assertPacket,
-)
+
+def writeVarUInt(x, ba):
+    for _ in range(0, 9):
+        byte = x & 0x7F
+        if x > 0x7F:
+            byte |= 0x80
+
+        ba.append(byte)
+
+        x >>= 7
+        if x == 0:
+            return
+
+
+def writeStringBinary(s, ba):
+    b = bytes(s, "utf-8")
+    writeVarUInt(len(s), ba)
+    ba.extend(b)
+
+
+def readStrict(s, size=1):
+    res = bytearray()
+    while size:
+        cur = s.recv(size)
+        # if not res:
+        #     raise "Socket is closed"
+        size -= len(cur)
+        res.extend(cur)
+
+    return res
+
+
+def readUInt(s, size=1):
+    res = readStrict(s, size)
+    val = 0
+    for i in range(len(res)):
+        val += res[i] << (i * 8)
+    return val
+
+
+def readUInt8(s):
+    return readUInt(s)
+
+
+def readUInt16(s):
+    return readUInt(s, 2)
+
+
+def readUInt32(s):
+    return readUInt(s, 4)
+
+
+def readUInt64(s):
+    return readUInt(s, 8)
+
+
+def readVarUInt(s):
+    x = 0
+    for i in range(9):
+        byte = readStrict(s)[0]
+        x |= (byte & 0x7F) << (7 * i)
+
+        if not byte & 0x80:
+            return x
+
+    return x
+
+
+def readStringBinary(s):
+    size = readVarUInt(s)
+    s = readStrict(s, size)
+    return s.decode("utf-8")
+
+
+def sendHello(s):
+    ba = bytearray()
+    writeVarUInt(0, ba)  # Hello
+    writeStringBinary(CLIENT_NAME, ba)
+    writeVarUInt(21, ba)
+    writeVarUInt(9, ba)
+    writeVarUInt(54449, ba)
+    writeStringBinary("default", ba)  # database
+    writeStringBinary("default", ba)  # user
+    writeStringBinary("", ba)  # pwd
+    s.sendall(ba)
+
+
+def receiveHello(s):
+    p_type = readVarUInt(s)
+    assert p_type == 0  # Hello
+    server_name = readStringBinary(s)
+    # print("Server name: ", server_name)
+    server_version_major = readVarUInt(s)
+    # print("Major: ", server_version_major)
+    server_version_minor = readVarUInt(s)
+    # print("Minor: ", server_version_minor)
+    server_revision = readVarUInt(s)
+    # print("Revision: ", server_revision)
+    server_timezone = readStringBinary(s)
+    # print("Timezone: ", server_timezone)
+    server_display_name = readStringBinary(s)
+    # print("Display name: ", server_display_name)
+    server_version_patch = readVarUInt(s)
+    # print("Version patch: ", server_version_patch)
+
+
+def serializeClientInfo(ba, query_id):
+    writeStringBinary("default", ba)  # initial_user
+    writeStringBinary(query_id, ba)  # initial_query_id
+    writeStringBinary("127.0.0.1:9000", ba)  # initial_address
+    ba.extend([0] * 8)  # initial_query_start_time_microseconds
+    ba.append(1)  # TCP
+    writeStringBinary("os_user", ba)  # os_user
+    writeStringBinary("client_hostname", ba)  # client_hostname
+    writeStringBinary(CLIENT_NAME, ba)  # client_name
+    writeVarUInt(21, ba)
+    writeVarUInt(9, ba)
+    writeVarUInt(54449, ba)
+    writeStringBinary("", ba)  # quota_key
+    writeVarUInt(0, ba)  # distributed_depth
+    writeVarUInt(1, ba)  # client_version_patch
+    ba.append(0)  # No telemetry
+
+
+def sendQuery(s, query):
+    ba = bytearray()
+    query_id = uuid.uuid4().hex
+    writeVarUInt(1, ba)  # query
+    writeStringBinary(query_id, ba)
+
+    ba.append(1)  # INITIAL_QUERY
+
+    # client info
+    serializeClientInfo(ba, query_id)
+
+    writeStringBinary("", ba)  # No settings
+    writeStringBinary("", ba)  # No interserver secret
+    writeVarUInt(2, ba)  # Stage - Complete
+    ba.append(0)  # No compression
+    writeStringBinary(query, ba)  # query, finally
+    s.sendall(ba)
+
+
+def serializeBlockInfo(ba):
+    writeVarUInt(1, ba)  # 1
+    ba.append(0)  # is_overflows
+    writeVarUInt(2, ba)  # 2
+    writeVarUInt(0, ba)  # 0
+    ba.extend([0] * 4)  # bucket_num
+
+
+def sendEmptyBlock(s):
+    ba = bytearray()
+    writeVarUInt(2, ba)  # Data
+    writeStringBinary("", ba)
+    serializeBlockInfo(ba)
+    writeVarUInt(0, ba)  # rows
+    writeVarUInt(0, ba)  # columns
+    s.sendall(ba)
+
+
+def assertPacket(packet, expected):
+    assert packet == expected, packet
+
+
+def readHeader(s):
+    packet_type = readVarUInt(s)
+    if packet_type == 2:  # Exception
+        raise RuntimeError(readException(s))
+    assertPacket(packet_type, 1)  # Data
+
+    readStringBinary(s)  # external table name
+    # BlockInfo
+    assertPacket(readVarUInt(s), 1)  # 1
+    assertPacket(readUInt8(s), 0)  # is_overflows
+    assertPacket(readVarUInt(s), 2)  # 2
+    assertPacket(readUInt32(s), 4294967295)  # bucket_num
+    assertPacket(readVarUInt(s), 0)  # 0
+    columns = readVarUInt(s)  # rows
+    rows = readVarUInt(s)  # columns
+    print("Rows {} Columns {}".format(rows, columns))
+    for _ in range(columns):
+        col_name = readStringBinary(s)
+        type_name = readStringBinary(s)
+        print("Column {} type {}".format(col_name, type_name))
+
+
+def readException(s):
+    code = readUInt32(s)
+    name = readStringBinary(s)
+    text = readStringBinary(s)
+    readStringBinary(s)  # trace
+    assertPacket(readUInt8(s), 0)  # has_nested
+    return "code {}: {}".format(code, text.replace("DB::Exception:", ""))
 
 
 def insertValidLowCardinalityRow():
-    with TCPClient() as client:
-        client.sendQuery(
+    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
+        s.settimeout(30)
+        s.connect((CLICKHOUSE_HOST, CLICKHOUSE_PORT))
+        sendHello(s)
+        receiveHello(s)
+        sendQuery(
+            s,
             "insert into {}.tab settings input_format_defaults_for_omitted_fields=0 format TSV".format(
                 CLICKHOUSE_DATABASE
             ),
         )
 
         # external tables
-        client.sendEmptyBlock()
-        client.readHeader()
+        sendEmptyBlock(s)
+        readHeader(s)
 
         # Data
         ba = bytearray()
@@ -46,25 +240,31 @@ def insertValidLowCardinalityRow():
         writeStringBinary("hello", ba)  # key
         ba.extend([1] + [0] * 7)  # num_indexes
         ba.extend([0] * 8)  # UInt64 index (0 for 'hello')
-        client.send(ba)
+        s.sendall(ba)
 
         # Fin block
-        client.sendEmptyBlock()
+        sendEmptyBlock(s)
 
-        assertPacket(client.readVarUInt(), 5)  # End of stream
+        assertPacket(readVarUInt(s), 5)  # End of stream
+        s.close()
 
 
 def insertLowCardinalityRowWithIndexOverflow():
-    with TCPClient() as client:
-        client.sendQuery(
+    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
+        s.settimeout(30)
+        s.connect((CLICKHOUSE_HOST, CLICKHOUSE_PORT))
+        sendHello(s)
+        receiveHello(s)
+        sendQuery(
+            s,
             "insert into {}.tab settings input_format_defaults_for_omitted_fields=0 format TSV".format(
                 CLICKHOUSE_DATABASE
             ),
         )
 
         # external tables
-        client.sendEmptyBlock()
-        client.readHeader()
+        sendEmptyBlock(s)
+        readHeader(s)
 
         # Data
         ba = bytearray()
@@ -83,23 +283,29 @@ def insertLowCardinalityRowWithIndexOverflow():
         writeStringBinary("hello", ba)  # key
         ba.extend([1] + [0] * 7)  # num_indexes
         ba.extend([0] * 7 + [1])  # UInt64 index (overflow)
-        client.send(ba)
+        s.sendall(ba)
 
-        assertPacket(client.readVarUInt(), 2)  # Exception
-        print(client.readException())
+        assertPacket(readVarUInt(s), 2)
+        print(readException(s))
+        s.close()
 
 
 def insertLowCardinalityRowWithIncorrectDictType():
-    with TCPClient() as client:
-        client.sendQuery(
+    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
+        s.settimeout(30)
+        s.connect((CLICKHOUSE_HOST, CLICKHOUSE_PORT))
+        sendHello(s)
+        receiveHello(s)
+        sendQuery(
+            s,
             "insert into {}.tab settings input_format_defaults_for_omitted_fields=0 format TSV".format(
                 CLICKHOUSE_DATABASE
             ),
         )
 
         # external tables
-        client.sendEmptyBlock()
-        client.readHeader()
+        sendEmptyBlock(s)
+        readHeader(s)
 
         # Data
         ba = bytearray()
@@ -118,23 +324,29 @@ def insertLowCardinalityRowWithIncorrectDictType():
         writeStringBinary("hello", ba)  # key
         ba.extend([1] + [0] * 7)  # num_indexes
         ba.extend([0] * 8)  # UInt64 index (overflow)
-        client.send(ba)
+        s.sendall(ba)
 
-        assertPacket(client.readVarUInt(), 2)  # Exception
-        print(client.readException())
+        assertPacket(readVarUInt(s), 2)
+        print(readException(s))
+        s.close()
 
 
 def insertLowCardinalityRowWithIncorrectAdditionalKeys():
-    with TCPClient() as client:
-        client.sendQuery(
+    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
+        s.settimeout(30)
+        s.connect((CLICKHOUSE_HOST, CLICKHOUSE_PORT))
+        sendHello(s)
+        receiveHello(s)
+        sendQuery(
+            s,
             "insert into {}.tab settings input_format_defaults_for_omitted_fields=0 format TSV".format(
                 CLICKHOUSE_DATABASE
             ),
         )
 
         # external tables
-        client.sendEmptyBlock()
-        client.readHeader()
+        sendEmptyBlock(s)
+        readHeader(s)
 
         # Data
         ba = bytearray()
@@ -153,10 +365,11 @@ def insertLowCardinalityRowWithIncorrectAdditionalKeys():
         writeStringBinary("hello", ba)  # key
         ba.extend([1] + [0] * 7)  # num_indexes
         ba.extend([0] * 8)  # UInt64 index (0 for 'hello')
-        client.send(ba)
+        s.sendall(ba)
 
-        assertPacket(client.readVarUInt(), 2)  # Exception
-        print(client.readException())
+        assertPacket(readVarUInt(s), 2)
+        print(readException(s))
+        s.close()
 
 
 def main():
diff --git a/tests/queries/0_stateless/02210_processors_profile_log.reference b/tests/queries/0_stateless/02210_processors_profile_log.reference
index f480236111f0..41543d0706ae 100644
--- a/tests/queries/0_stateless/02210_processors_profile_log.reference
+++ b/tests/queries/0_stateless/02210_processors_profile_log.reference
@@ -38,5 +38,4 @@ LazyOutputFormat	1	1	1	0	0
 LimitsCheckingTransform	1	1	1	1	1
 NullSource	1	0	0	0	0
 NullSource	1	0	0	0	0
-NullSource	0	0	0	0	0
 SourceFromSingleChunk	1	0	0	1	1
diff --git a/tests/queries/0_stateless/02458_insert_select_progress_tcp.python b/tests/queries/0_stateless/02458_insert_select_progress_tcp.python
index fdc64a8dba86..92240e109c1e 100644
--- a/tests/queries/0_stateless/02458_insert_select_progress_tcp.python
+++ b/tests/queries/0_stateless/02458_insert_select_progress_tcp.python
@@ -1,30 +1,188 @@
 #!/usr/bin/env python3
 
-import json
+import socket
 import os
-import sys
+import uuid
+import json
+
+CLICKHOUSE_HOST = os.environ.get("CLICKHOUSE_HOST", "127.0.0.1")
+CLICKHOUSE_PORT = int(os.environ.get("CLICKHOUSE_PORT_TCP", "900000"))
+CLICKHOUSE_DATABASE = os.environ.get("CLICKHOUSE_DATABASE", "default")
+CLIENT_NAME = "simple native protocol"
+
+
+def writeVarUInt(x, ba):
+    for _ in range(0, 9):
+        byte = x & 0x7F
+        if x > 0x7F:
+            byte |= 0x80
+
+        ba.append(byte)
+
+        x >>= 7
+        if x == 0:
+            return
+
+
+def writeStringBinary(s, ba):
+    b = bytes(s, "utf-8")
+    writeVarUInt(len(s), ba)
+    ba.extend(b)
+
+
+def readStrict(s, size=1):
+    res = bytearray()
+    while size:
+        cur = s.recv(size)
+        # if not res:
+        #     raise "Socket is closed"
+        size -= len(cur)
+        res.extend(cur)
+
+    return res
+
+
+def readUInt(s, size=1):
+    res = readStrict(s, size)
+    val = 0
+    for i in range(len(res)):
+        val += res[i] << (i * 8)
+    return val
+
+
+def readUInt8(s):
+    return readUInt(s)
+
+
+def readUInt16(s):
+    return readUInt(s, 2)
+
+
+def readUInt32(s):
+    return readUInt(s, 4)
+
+
+def readUInt64(s):
+    return readUInt(s, 8)
+
+
+def readVarUInt(s):
+    x = 0
+    for i in range(9):
+        byte = readStrict(s)[0]
+        x |= (byte & 0x7F) << (7 * i)
+
+        if not byte & 0x80:
+            return x
+
+    return x
+
 
-CURDIR = os.path.dirname(os.path.realpath(__file__))
-sys.path.insert(0, os.path.join(CURDIR, "helpers"))
+def readStringBinary(s):
+    size = readVarUInt(s)
+    s = readStrict(s, size)
+    return s.decode("utf-8")
 
-from tcp_client import TCPClient
+
+def sendHello(s):
+    ba = bytearray()
+    writeVarUInt(0, ba)  # Hello
+    writeStringBinary(CLIENT_NAME, ba)
+    writeVarUInt(21, ba)
+    writeVarUInt(9, ba)
+    writeVarUInt(54449, ba)
+    writeStringBinary(CLICKHOUSE_DATABASE, ba)  # database
+    writeStringBinary("default", ba)  # user
+    writeStringBinary("", ba)  # pwd
+    s.sendall(ba)
+
+
+def receiveHello(s):
+    p_type = readVarUInt(s)
+    assert p_type == 0  # Hello
+    server_name = readStringBinary(s)
+    # print("Server name: ", server_name)
+    server_version_major = readVarUInt(s)
+    # print("Major: ", server_version_major)
+    server_version_minor = readVarUInt(s)
+    # print("Minor: ", server_version_minor)
+    server_revision = readVarUInt(s)
+    # print("Revision: ", server_revision)
+    server_timezone = readStringBinary(s)
+    # print("Timezone: ", server_timezone)
+    server_display_name = readStringBinary(s)
+    # print("Display name: ", server_display_name)
+    server_version_patch = readVarUInt(s)
+    # print("Version patch: ", server_version_patch)
+
+
+def serializeClientInfo(ba, query_id):
+    writeStringBinary("default", ba)  # initial_user
+    writeStringBinary(query_id, ba)  # initial_query_id
+    writeStringBinary("127.0.0.1:9000", ba)  # initial_address
+    ba.extend([0] * 8)  # initial_query_start_time_microseconds
+    ba.append(1)  # TCP
+    writeStringBinary("os_user", ba)  # os_user
+    writeStringBinary("client_hostname", ba)  # client_hostname
+    writeStringBinary(CLIENT_NAME, ba)  # client_name
+    writeVarUInt(21, ba)
+    writeVarUInt(9, ba)
+    writeVarUInt(54449, ba)
+    writeStringBinary("", ba)  # quota_key
+    writeVarUInt(0, ba)  # distributed_depth
+    writeVarUInt(1, ba)  # client_version_patch
+    ba.append(0)  # No telemetry
+
+
+def sendQuery(s, query):
+    ba = bytearray()
+    query_id = uuid.uuid4().hex
+    writeVarUInt(1, ba)  # query
+    writeStringBinary(query_id, ba)
+
+    ba.append(1)  # INITIAL_QUERY
+
+    # client info
+    serializeClientInfo(ba, query_id)
+
+    writeStringBinary("", ba)  # No settings
+    writeStringBinary("", ba)  # No interserver secret
+    writeVarUInt(2, ba)  # Stage - Complete
+    ba.append(0)  # No compression
+    writeStringBinary(query, ba)  # query, finally
+    s.sendall(ba)
+
+
+def serializeBlockInfo(ba):
+    writeVarUInt(1, ba)  # 1
+    ba.append(0)  # is_overflows
+    writeVarUInt(2, ba)  # 2
+    writeVarUInt(0, ba)  # 0
+    ba.extend([0] * 4)  # bucket_num
+
+
+def sendEmptyBlock(s):
+    ba = bytearray()
+    writeVarUInt(2, ba)  # Data
+    writeStringBinary("", ba)
+    serializeBlockInfo(ba)
+    writeVarUInt(0, ba)  # rows
+    writeVarUInt(0, ba)  # columns
+    s.sendall(ba)
+
+
+def assertPacket(packet, expected):
+    assert packet == expected, packet
 
 
 class Progress:
-    def __init__(
-        self,
-        read_rows=0,
-        read_bytes=0,
-        total_rows_to_read=0,
-        written_rows=0,
-        written_bytes=0,
-    ):
+    def __init__(self):
         # NOTE: this is done in ctor to initialize __dict__
-        self.read_rows = read_rows
-        self.read_bytes = read_bytes
-        self.total_rows_to_read = total_rows_to_read
-        self.written_rows = written_rows
-        self.written_bytes = written_bytes
+        self.read_rows = 0
+        self.read_bytes = 0
+        self.total_rows_to_read = 0
+        self.written_rows = 0
+        self.written_bytes = 0
 
     def __str__(self):
         return json.dumps(self.__dict__)
@@ -37,6 +195,13 @@ class Progress:
         self.written_bytes += b.written_bytes
         return self
 
+    def readPacket(self, s):
+        self.read_rows += readVarUInt(s)
+        self.read_bytes += readVarUInt(s)
+        self.total_rows_to_read += readVarUInt(s)
+        self.written_rows += readVarUInt(s)
+        self.written_bytes += readVarUInt(s)
+
     def __bool__(self):
         return (
             self.read_rows > 0
@@ -47,25 +212,52 @@ class Progress:
         )
 
 
+def readProgress(s):
+    packet_type = readVarUInt(s)
+    if packet_type == 2:  # Exception
+        raise RuntimeError(readException(s))
+
+    if packet_type == 5:  # End stream
+        return None
+
+    assertPacket(packet_type, 3)  # Progress
+
+    progress = Progress()
+    progress.readPacket(s)
+    return progress
+
+
+def readException(s):
+    code = readUInt32(s)
+    name = readStringBinary(s)
+    text = readStringBinary(s)
+    readStringBinary(s)  # trace
+    assertPacket(readUInt8(s), 0)  # has_nested
+    return "code {}: {}".format(code, text.replace("DB::Exception:", ""))
+
+
 def main():
-    with TCPClient() as client:
+    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
+        s.settimeout(30)
+        s.connect((CLICKHOUSE_HOST, CLICKHOUSE_PORT))
+        sendHello(s)
+        receiveHello(s)
         # For 1 second sleep and 1000ms of interactive_delay we definitelly should have non zero progress packet.
         # NOTE: interactive_delay=0 cannot be used since in this case CompletedPipelineExecutor will not call cancelled callback.
-        client.sendQuery(
+        sendQuery(
+            s,
             "insert into function null('_ Int') select sleep(1) from numbers(2) settings max_block_size=1, interactive_delay=1000",
         )
 
         # external tables
-        client.sendEmptyBlock()
+        sendEmptyBlock(s)
 
         summary_progress = Progress()
         non_empty_progress_packets = 0
         while True:
-            progress_info = client.readProgress()
-            if progress_info is None:
+            progress = readProgress(s)
+            if progress is None:
                 break
-
-            progress = Progress(*progress_info)
             summary_progress += progress
             if progress:
                 non_empty_progress_packets += 1
@@ -76,6 +268,8 @@ def main():
         # - 1 or 2 for each SELECT block
         assert non_empty_progress_packets in (3, 4), f"{non_empty_progress_packets=:}"
 
+        s.close()
+
 
 if __name__ == "__main__":
     main()
diff --git a/tests/queries/0_stateless/02750_settings_alias_tcp_protocol.python b/tests/queries/0_stateless/02750_settings_alias_tcp_protocol.python
index 1736807410f0..48b27d434eca 100644
--- a/tests/queries/0_stateless/02750_settings_alias_tcp_protocol.python
+++ b/tests/queries/0_stateless/02750_settings_alias_tcp_protocol.python
@@ -1,23 +1,217 @@
 #!/usr/bin/env python3
 
-
+import socket
 import os
-import sys
+import uuid
+import json
+
+CLICKHOUSE_HOST = os.environ.get("CLICKHOUSE_HOST", "127.0.0.1")
+CLICKHOUSE_PORT = int(os.environ.get("CLICKHOUSE_PORT_TCP", "900000"))
+CLICKHOUSE_DATABASE = os.environ.get("CLICKHOUSE_DATABASE", "default")
+CLIENT_NAME = "simple native protocol"
+
+
+def writeVarUInt(x, ba):
+    for _ in range(0, 9):
+        byte = x & 0x7F
+        if x > 0x7F:
+            byte |= 0x80
+
+        ba.append(byte)
+
+        x >>= 7
+        if x == 0:
+            return
+
+
+def writeStringBinary(s, ba):
+    b = bytes(s, "utf-8")
+    writeVarUInt(len(s), ba)
+    ba.extend(b)
+
+
+def readStrict(s, size=1):
+    res = bytearray()
+    while size:
+        cur = s.recv(size)
+        # if not res:
+        #     raise "Socket is closed"
+        size -= len(cur)
+        res.extend(cur)
+
+    return res
+
+
+def readUInt(s, size=1):
+    res = readStrict(s, size)
+    val = 0
+    for i in range(len(res)):
+        val += res[i] << (i * 8)
+    return val
+
+
+def readUInt8(s):
+    return readUInt(s)
+
+
+def readUInt16(s):
+    return readUInt(s, 2)
+
+
+def readUInt32(s):
+    return readUInt(s, 4)
+
+
+def readUInt64(s):
+    return readUInt(s, 8)
+
+
+def readVarUInt(s):
+    x = 0
+    for i in range(9):
+        byte = readStrict(s)[0]
+        x |= (byte & 0x7F) << (7 * i)
+
+        if not byte & 0x80:
+            return x
+
+    return x
 
-CURDIR = os.path.dirname(os.path.realpath(__file__))
-sys.path.insert(0, os.path.join(CURDIR, "helpers"))
 
-from tcp_client import TCPClient
+def readStringBinary(s):
+    size = readVarUInt(s)
+    s = readStrict(s, size)
+    return s.decode("utf-8")
+
+
+def sendHello(s):
+    ba = bytearray()
+    writeVarUInt(0, ba)  # Hello
+    writeStringBinary(CLIENT_NAME, ba)
+    writeVarUInt(21, ba)
+    writeVarUInt(9, ba)
+    writeVarUInt(54449, ba)
+    writeStringBinary(CLICKHOUSE_DATABASE, ba)  # database
+    writeStringBinary("default", ba)  # user
+    writeStringBinary("", ba)  # pwd
+    s.sendall(ba)
+
+
+def receiveHello(s):
+    p_type = readVarUInt(s)
+    assert p_type == 0  # Hello
+    _server_name = readStringBinary(s)
+    _server_version_major = readVarUInt(s)
+    _server_version_minor = readVarUInt(s)
+    _server_revision = readVarUInt(s)
+    _server_timezone = readStringBinary(s)
+    _server_display_name = readStringBinary(s)
+    _server_version_patch = readVarUInt(s)
+
+
+def serializeClientInfo(ba, query_id):
+    writeStringBinary("default", ba)  # initial_user
+    writeStringBinary(query_id, ba)  # initial_query_id
+    writeStringBinary("127.0.0.1:9000", ba)  # initial_address
+    ba.extend([0] * 8)  # initial_query_start_time_microseconds
+    ba.append(1)  # TCP
+    writeStringBinary("os_user", ba)  # os_user
+    writeStringBinary("client_hostname", ba)  # client_hostname
+    writeStringBinary(CLIENT_NAME, ba)  # client_name
+    writeVarUInt(21, ba)
+    writeVarUInt(9, ba)
+    writeVarUInt(54449, ba)
+    writeStringBinary("", ba)  # quota_key
+    writeVarUInt(0, ba)  # distributed_depth
+    writeVarUInt(1, ba)  # client_version_patch
+    ba.append(0)  # No telemetry
+
+
+def sendQuery(s, query, settings):
+    ba = bytearray()
+    query_id = uuid.uuid4().hex
+    writeVarUInt(1, ba)  # query
+    writeStringBinary(query_id, ba)
+
+    ba.append(1)  # INITIAL_QUERY
+
+    # client info
+    serializeClientInfo(ba, query_id)
+
+    # Settings
+    for key, value in settings.items():
+        writeStringBinary(key, ba)
+        writeVarUInt(1, ba)  # is_important
+        writeStringBinary(str(value), ba)
+    writeStringBinary("", ba)  # End of settings
+
+    writeStringBinary("", ba)  # No interserver secret
+    writeVarUInt(2, ba)  # Stage - Complete
+    ba.append(0)  # No compression
+    writeStringBinary(query, ba)  # query, finally
+    s.sendall(ba)
+
+
+def serializeBlockInfo(ba):
+    writeVarUInt(1, ba)  # 1
+    ba.append(0)  # is_overflows
+    writeVarUInt(2, ba)  # 2
+    writeVarUInt(0, ba)  # 0
+    ba.extend([0] * 4)  # bucket_num
+
+
+def sendEmptyBlock(s):
+    ba = bytearray()
+    writeVarUInt(2, ba)  # Data
+    writeStringBinary("", ba)
+    serializeBlockInfo(ba)
+    writeVarUInt(0, ba)  # rows
+    writeVarUInt(0, ba)  # columns
+    s.sendall(ba)
+
+
+def assertPacket(packet, expected):
+    assert packet == expected, "Got: {}, expected: {}".format(packet, expected)
+
+
+def readResponse(s):
+    packet_type = readVarUInt(s)
+    if packet_type == 2:  # Exception
+        raise RuntimeError(readException(s))
+
+    if packet_type == 1:  # Data
+        return None
+    if packet_type == 3:  # Progress
+        return None
+    if packet_type == 5:  # End stream
+        return None
+
+    raise RuntimeError("Unexpected packet: {}".format(packet_type))
+
+
+def readException(s):
+    code = readUInt32(s)
+    _name = readStringBinary(s)
+    text = readStringBinary(s)
+    readStringBinary(s)  # trace
+    assertPacket(readUInt8(s), 0)  # has_nested
+    return "code {}: {}".format(code, text.replace("DB::Exception:", ""))
 
 
 def main():
-    with TCPClient() as client:
-        client.sendQuery("select 1", {"replication_alter_partitions_sync": 1})
+    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
+        s.settimeout(30)
+        s.connect((CLICKHOUSE_HOST, CLICKHOUSE_PORT))
+        sendHello(s)
+        receiveHello(s)
+        sendQuery(s, "select 1", {"replication_alter_partitions_sync": 1})
         # external tables
-        client.sendEmptyBlock()
+        sendEmptyBlock(s)
 
-        while client.readResponse() is not None:
+        while readResponse(s) is not None:
             pass
+
+        s.close()
     print("OK")
 
 
diff --git a/tests/queries/0_stateless/02833_partial_sorting_result_during_query_execution.python b/tests/queries/0_stateless/02833_partial_sorting_result_during_query_execution.python
deleted file mode 100755
index 61ba0e14605d..000000000000
--- a/tests/queries/0_stateless/02833_partial_sorting_result_during_query_execution.python
+++ /dev/null
@@ -1,95 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-
-
-import os
-import sys
-
-CURDIR = os.path.dirname(os.path.realpath(__file__))
-sys.path.insert(0, os.path.join(CURDIR, "helpers"))
-
-from tcp_client import TCPClient
-
-
-def run_query_without_errors(query, support_partial_result):
-    with TCPClient() as client:
-        client.sendQuery(query, settings={"allow_experimental_partial_result": True})
-
-        # external tables
-        client.sendEmptyBlock()
-        client.readHeader()
-
-        # Partial result
-        partial_result = client.readDataWithoutProgress()[0]
-        if support_partial_result:
-            assert (
-                len(partial_result.value) > 0
-            ), "Expected at least one block with a non-empty partial result before getting the full result"
-
-            while True:
-                assert all(
-                    a >= b
-                    for a, b in zip(partial_result.value, partial_result.value[1:])
-                ), "Partial result always should be sorted for this test"
-
-                new_partial_result = client.readDataWithoutProgress(
-                    need_print_info=False
-                )[0]
-                if len(new_partial_result.value) == 0:
-                    break
-
-                data_size = len(partial_result.value)
-                assert all(
-                    partial_result.value[i] <= new_partial_result.value[i]
-                    for i in range(data_size)
-                ), f"New partial result values should always be greater then old one because a new block contains more information about the full data. New result {new_partial_result}. Previous result {partial_result}"
-
-                partial_result = new_partial_result
-        else:
-            block_rows = len(partial_result.value)
-            assert (
-                block_rows == 0
-            ), f"Expected only empty partial result block before getting the full result, but block has {block_rows} rows"
-
-        # Full result
-        full_result = client.readDataWithoutProgress()[0]
-
-        data_size = len(partial_result.value)
-        assert all(
-            partial_result.value[i] <= full_result.value[i] for i in range(data_size)
-        ), f"Full result values should always be greater then partial result values. Full result {full_result}. Partial result {partial_result}"
-
-        for result in full_result.value:
-            print(result)
-
-
-def main():
-    rows_number = 2e7 + 1
-
-    # Request with partial result limit less then full limit
-    run_query_without_errors(
-        f"SELECT number FROM numbers_mt({rows_number}) ORDER BY -number LIMIT 5 SETTINGS max_threads = 1, partial_result_update_duration_ms = 1, max_rows_in_partial_result = 3",
-        support_partial_result=True,
-    )
-
-    # Request with partial result limit greater then full limit
-    run_query_without_errors(
-        f"SELECT number FROM numbers_mt({rows_number}) ORDER BY -number LIMIT 3 SETTINGS max_threads = 1, partial_result_update_duration_ms = 1, max_rows_in_partial_result = 5",
-        support_partial_result=True,
-    )
-
-    # Request with OFFSET
-    run_query_without_errors(
-        f"SELECT number FROM numbers_mt({rows_number}) ORDER BY -number LIMIT 3 OFFSET 1 SETTINGS max_threads = 1, partial_result_update_duration_ms = 1, max_rows_in_partial_result = 5",
-        support_partial_result=True,
-    )
-
-    # Request with OFFSET greater then partial result limit (partial result pipeline use blocks with less then OFFSET, so there will be no elements in block after LimitPartialResultTransform)
-    run_query_without_errors(
-        f"SELECT number FROM numbers_mt({rows_number}) ORDER BY -number LIMIT 3 OFFSET 15 SETTINGS max_threads = 1, partial_result_update_duration_ms = 1, max_rows_in_partial_result = 5",
-        support_partial_result=False,
-    )
-
-
-if __name__ == "__main__":
-    main()
diff --git a/tests/queries/0_stateless/02833_partial_sorting_result_during_query_execution.reference b/tests/queries/0_stateless/02833_partial_sorting_result_during_query_execution.reference
deleted file mode 100644
index dd3a343560f0..000000000000
--- a/tests/queries/0_stateless/02833_partial_sorting_result_during_query_execution.reference
+++ /dev/null
@@ -1,38 +0,0 @@
-Rows 0 Columns 1
-Column number type UInt64
-Rows 3 Columns 1
-Column number type UInt64
-Rows 5 Columns 1
-Column number type UInt64
-20000000
-19999999
-19999998
-19999997
-19999996
-Rows 0 Columns 1
-Column number type UInt64
-Rows 3 Columns 1
-Column number type UInt64
-Rows 3 Columns 1
-Column number type UInt64
-20000000
-19999999
-19999998
-Rows 0 Columns 1
-Column number type UInt64
-Rows 3 Columns 1
-Column number type UInt64
-Rows 3 Columns 1
-Column number type UInt64
-19999999
-19999998
-19999997
-Rows 0 Columns 1
-Column number type UInt64
-Rows 0 Columns 1
-Column number type UInt64
-Rows 3 Columns 1
-Column number type UInt64
-19999985
-19999984
-19999983
diff --git a/tests/queries/0_stateless/02833_partial_sorting_result_during_query_execution.sh b/tests/queries/0_stateless/02833_partial_sorting_result_during_query_execution.sh
deleted file mode 100755
index 1ed15197dbff..000000000000
--- a/tests/queries/0_stateless/02833_partial_sorting_result_during_query_execution.sh
+++ /dev/null
@@ -1,8 +0,0 @@
-#!/usr/bin/env bash
-
-CURDIR=$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)
-# shellcheck source=../shell_config.sh
-. "$CURDIR"/../shell_config.sh
-
-# We should have correct env vars from shell_config.sh to run this test
-python3 "$CURDIR"/02833_partial_sorting_result_during_query_execution.python
diff --git a/tests/queries/0_stateless/02834_partial_aggregating_result_during_query_execution.python b/tests/queries/0_stateless/02834_partial_aggregating_result_during_query_execution.python
deleted file mode 100644
index a33c714e89c2..000000000000
--- a/tests/queries/0_stateless/02834_partial_aggregating_result_during_query_execution.python
+++ /dev/null
@@ -1,129 +0,0 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-
-
-import os
-import sys
-
-CURDIR = os.path.dirname(os.path.realpath(__file__))
-sys.path.insert(0, os.path.join(CURDIR, "helpers"))
-
-from tcp_client import TCPClient
-
-
-def get_keys(results):
-    return [key for key, _ in results]
-
-
-def check_new_result(new_results, old_results, invariants, rows_limit):
-    if rows_limit is not None:
-        assert (
-            len(new_results[0].value) <= rows_limit
-        ), f"Result should have no more then {rows_limit} rows. But it has {len(new_results[0].value)} rows"
-
-    for new_result, old_result in zip(new_results, old_results):
-        assert (
-            new_result.key == old_result.key
-        ), f"Keys in blocks should be in the same order. Full results keys {get_keys(full_results)}. Partial results keys  {get_keys(partial_results)}"
-
-        key = new_result.key
-        if key in invariants:
-            new_value = new_result.value
-            old_value = old_result.value
-            assert invariants[key](
-                old_value, new_value
-            ), f"Problem with the invariant between new and old result for key: {key}. New value {new_value}. Old value {old_value}"
-
-
-def run_query_without_errors(
-    query, support_partial_result, invariants=None, rows_limit=None
-):
-    if invariants is None:
-        invariants = {}
-
-    with TCPClient() as client:
-        client.sendQuery(query, settings={"allow_experimental_partial_result": True})
-
-        # external tables
-        client.sendEmptyBlock()
-        client.readHeader()
-
-        # Partial result
-        partial_results = client.readDataWithoutProgress()
-        if support_partial_result:
-            assert (
-                len(partial_results) > 0 and len(partial_results[0].value) > 0
-            ), "Expected at least one block with a non-empty partial result before getting the full result"
-            while True:
-                new_partial_results = client.readDataWithoutProgress(
-                    need_print_info=False
-                )
-                if len(new_partial_results[0].value) == 0:
-                    break
-
-                check_new_result(
-                    new_partial_results, partial_results, invariants, rows_limit
-                )
-                partial_results = new_partial_results
-        else:
-            block_rows = len(partial_results[0].value)
-            assert (
-                block_rows == 0
-            ), f"Expected only empty partial result block before getting the full result, but block has {block_rows} rows"
-
-        # Full result
-        full_results = client.readDataWithoutProgress()
-        if support_partial_result:
-            check_new_result(full_results, partial_results, invariants, rows_limit)
-
-        for data in full_results:
-            if isinstance(data.value[0], int):
-                print(data.key, data.value)
-
-
-def supported_scenarios_without_key():
-    rows_number = 2e7 + 1
-
-    # Simple aggregation query
-    query = f"select median(number), stddevSamp(number), stddevPop(number), max(number), min(number), any(number), count(number), avg(number), sum(number) from numbers_mt({rows_number}) settings max_threads = 1, partial_result_update_duration_ms = 1"
-    invariants = {
-        "median(number)": lambda old_value, new_value: old_value <= new_value,
-        "max(number)": lambda old_value, new_value: old_value <= new_value,
-        "min(number)": lambda old_value, new_value: old_value >= new_value,
-        "count(number)": lambda old_value, new_value: old_value <= new_value,
-        "avg(number)": lambda old_value, new_value: old_value <= new_value,
-        "sum(number)": lambda old_value, new_value: old_value <= new_value,
-    }
-    run_query_without_errors(
-        query, support_partial_result=True, invariants=invariants, rows_limit=1
-    )
-
-    # Aggregation query with a nested ORDER BY subquery
-    query = f"select median(number), stddevSamp(number), stddevPop(number), max(number), min(number), any(number), count(number), avg(number), sum(number) FROM (SELECT number FROM numbers_mt({rows_number}) ORDER BY -number LIMIT 3) settings max_threads = 1, partial_result_update_duration_ms=1"
-
-    # Aggregation receives small partial result blocks from ORDER BY which always sends blocks with bigger values
-    invariants["min(number)"] = lambda old_value, new_value: old_value <= new_value
-    run_query_without_errors(
-        query, support_partial_result=True, invariants=invariants, rows_limit=1
-    )
-
-
-def unsupported_scenarios():
-    rows_number = 2e7 + 1
-
-    # Currently aggregator for partial result supports only single thread aggregation without key
-    # Update test when multithreading or aggregation with GROUP BY will be supported for partial result updates
-    multithread_query = f"select sum(number) from numbers_mt({rows_number}) settings max_threads = 2, partial_result_update_duration_ms = 100"
-    run_query_without_errors(multithread_query, support_partial_result=False)
-
-    group_with_key_query = f"select mod2, sum(number) from numbers_mt({rows_number}) group by number % 2 as mod2 settings max_threads = 1, partial_result_update_duration_ms = 100"
-    run_query_without_errors(group_with_key_query, support_partial_result=False)
-
-
-def main():
-    supported_scenarios_without_key()
-    unsupported_scenarios()
-
-
-if __name__ == "__main__":
-    main()
diff --git a/tests/queries/0_stateless/02834_partial_aggregating_result_during_query_execution.reference b/tests/queries/0_stateless/02834_partial_aggregating_result_during_query_execution.reference
deleted file mode 100644
index aea61fad42f8..000000000000
--- a/tests/queries/0_stateless/02834_partial_aggregating_result_during_query_execution.reference
+++ /dev/null
@@ -1,88 +0,0 @@
-Rows 0 Columns 9
-Column median(number) type Float64
-Column stddevSamp(number) type Float64
-Column stddevPop(number) type Float64
-Column max(number) type UInt64
-Column min(number) type UInt64
-Column any(number) type UInt64
-Column count(number) type UInt64
-Column avg(number) type Float64
-Column sum(number) type UInt64
-Rows 1 Columns 9
-Column median(number) type Float64
-Column stddevSamp(number) type Float64
-Column stddevPop(number) type Float64
-Column max(number) type UInt64
-Column min(number) type UInt64
-Column any(number) type UInt64
-Column count(number) type UInt64
-Column avg(number) type Float64
-Column sum(number) type UInt64
-Rows 1 Columns 9
-Column median(number) type Float64
-Column stddevSamp(number) type Float64
-Column stddevPop(number) type Float64
-Column max(number) type UInt64
-Column min(number) type UInt64
-Column any(number) type UInt64
-Column count(number) type UInt64
-Column avg(number) type Float64
-Column sum(number) type UInt64
-max(number) [20000000]
-min(number) [0]
-any(number) [0]
-count(number) [20000001]
-sum(number) [200000010000000]
-Rows 0 Columns 9
-Column median(number) type Float64
-Column stddevSamp(number) type Float64
-Column stddevPop(number) type Float64
-Column max(number) type UInt64
-Column min(number) type UInt64
-Column any(number) type UInt64
-Column count(number) type UInt64
-Column avg(number) type Float64
-Column sum(number) type UInt64
-Rows 1 Columns 9
-Column median(number) type Float64
-Column stddevSamp(number) type Float64
-Column stddevPop(number) type Float64
-Column max(number) type UInt64
-Column min(number) type UInt64
-Column any(number) type UInt64
-Column count(number) type UInt64
-Column avg(number) type Float64
-Column sum(number) type UInt64
-Rows 1 Columns 9
-Column median(number) type Float64
-Column stddevSamp(number) type Float64
-Column stddevPop(number) type Float64
-Column max(number) type UInt64
-Column min(number) type UInt64
-Column any(number) type UInt64
-Column count(number) type UInt64
-Column avg(number) type Float64
-Column sum(number) type UInt64
-max(number) [20000000]
-min(number) [19999998]
-any(number) [20000000]
-count(number) [3]
-sum(number) [59999997]
-Rows 0 Columns 1
-Column sum(number) type UInt64
-Rows 0 Columns 1
-Column sum(number) type UInt64
-Rows 1 Columns 1
-Column sum(number) type UInt64
-sum(number) [200000010000000]
-Rows 0 Columns 2
-Column mod2 type UInt8
-Column sum(number) type UInt64
-Rows 0 Columns 2
-Column mod2 type UInt8
-Column sum(number) type UInt64
-Rows 2 Columns 2
-Column mod2 type UInt8
-Column sum(number) type UInt64
-mod2 [0, 1]
-sum(number) [100000010000000, 100000000000000]
diff --git a/tests/queries/0_stateless/02834_partial_aggregating_result_during_query_execution.sh b/tests/queries/0_stateless/02834_partial_aggregating_result_during_query_execution.sh
deleted file mode 100755
index e70a3c53ec43..000000000000
--- a/tests/queries/0_stateless/02834_partial_aggregating_result_during_query_execution.sh
+++ /dev/null
@@ -1,8 +0,0 @@
-#!/usr/bin/env bash
-
-CURDIR=$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)
-# shellcheck source=../shell_config.sh
-. "$CURDIR"/../shell_config.sh
-
-# We should have correct env vars from shell_config.sh to run this test
-python3 "$CURDIR"/02834_partial_aggregating_result_during_query_execution.python
diff --git a/tests/queries/0_stateless/02876_experimental_partial_result.reference b/tests/queries/0_stateless/02876_experimental_partial_result.reference
deleted file mode 100644
index e69de29bb2d1..000000000000
diff --git a/tests/queries/0_stateless/02876_experimental_partial_result.sql b/tests/queries/0_stateless/02876_experimental_partial_result.sql
deleted file mode 100644
index 8418f07c7502..000000000000
--- a/tests/queries/0_stateless/02876_experimental_partial_result.sql
+++ /dev/null
@@ -1,4 +0,0 @@
-
-SET partial_result_update_duration_ms = 10;
-
-SELECT sum(number) FROM numbers_mt(100_000) SETTINGS max_threads = 1; -- { serverError FUNCTION_NOT_ALLOWED }
diff --git a/tests/queries/0_stateless/02894_MergeSortingPartialResultTransform_empty_block.reference b/tests/queries/0_stateless/02894_MergeSortingPartialResultTransform_empty_block.reference
deleted file mode 100644
index e69de29bb2d1..000000000000
diff --git a/tests/queries/0_stateless/02894_MergeSortingPartialResultTransform_empty_block.sql b/tests/queries/0_stateless/02894_MergeSortingPartialResultTransform_empty_block.sql
deleted file mode 100644
index 9e665e0ae20e..000000000000
--- a/tests/queries/0_stateless/02894_MergeSortingPartialResultTransform_empty_block.sql
+++ /dev/null
@@ -1,11 +0,0 @@
-drop table if exists data;
-create table data (key Int) engine=MergeTree() order by key;
-insert into data select * from numbers(1);
-insert into data select * from numbers(1);
-system stop merges data;
--- need sleep to trigger partial results to uncover the bug with empty chunk after remerge due to empty array join, i.e.:
---
---   MergeSortingTransform: Re-merging intermediate ORDER BY data (1 blocks with 0 rows) to save memory consumption
---   MergeSortingTransform: Memory usage is lowered from 4.26 KiB to 0.00 B
---
-select key, sleepEachRow(1) from data array join [] as x order by key settings optimize_read_in_order=0, allow_experimental_partial_result=1, partial_result_update_duration_ms=1, max_threads=1, max_execution_time=0, max_block_size=1;
diff --git a/tests/queries/0_stateless/helpers/tcp_client.py b/tests/queries/0_stateless/helpers/tcp_client.py
deleted file mode 100644
index fdc4ab28e042..000000000000
--- a/tests/queries/0_stateless/helpers/tcp_client.py
+++ /dev/null
@@ -1,313 +0,0 @@
-import socket
-import os
-import uuid
-import struct
-
-CLICKHOUSE_HOST = os.environ.get("CLICKHOUSE_HOST", "127.0.0.1")
-CLICKHOUSE_PORT = int(os.environ.get("CLICKHOUSE_PORT_TCP", "900000"))
-CLICKHOUSE_DATABASE = os.environ.get("CLICKHOUSE_DATABASE", "default")
-CLIENT_NAME = "simple native protocol"
-
-
-def writeVarUInt(x, ba):
-    for _ in range(0, 9):
-        byte = x & 0x7F
-        if x > 0x7F:
-            byte |= 0x80
-
-        ba.append(byte)
-
-        x >>= 7
-        if x == 0:
-            return
-
-
-def writeStringBinary(s, ba):
-    b = bytes(s, "utf-8")
-    writeVarUInt(len(s), ba)
-    ba.extend(b)
-
-
-def serializeClientInfo(ba, query_id):
-    writeStringBinary("default", ba)  # initial_user
-    writeStringBinary(query_id, ba)  # initial_query_id
-    writeStringBinary("127.0.0.1:9000", ba)  # initial_address
-    ba.extend([0] * 8)  # initial_query_start_time_microseconds
-    ba.append(1)  # TCP
-    writeStringBinary("os_user", ba)  # os_user
-    writeStringBinary("client_hostname", ba)  # client_hostname
-    writeStringBinary(CLIENT_NAME, ba)  # client_name
-    writeVarUInt(21, ba)
-    writeVarUInt(9, ba)
-    writeVarUInt(54449, ba)
-    writeStringBinary("", ba)  # quota_key
-    writeVarUInt(0, ba)  # distributed_depth
-    writeVarUInt(1, ba)  # client_version_patch
-    ba.append(0)  # No telemetry
-
-
-def serializeBlockInfo(ba):
-    writeVarUInt(1, ba)  # 1
-    ba.append(0)  # is_overflows
-    writeVarUInt(2, ba)  # 2
-    writeVarUInt(0, ba)  # 0
-    ba.extend([0] * 4)  # bucket_num
-
-
-def assertPacket(packet, expected):
-    assert packet == expected, "Got: {}, expected: {}".format(packet, expected)
-
-
-class Data(object):
-    def __init__(self, key, value):
-        self.key = key
-        self.value = value
-
-
-class TCPClient(object):
-    def __init__(self, timeout=30):
-        self.timeout = timeout
-        self.socket = None
-
-    def __enter__(self):
-        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
-        self.socket.settimeout(self.timeout)
-        self.socket.connect((CLICKHOUSE_HOST, CLICKHOUSE_PORT))
-
-        self.sendHello()
-        self.receiveHello()
-
-        return self
-
-    def __exit__(self, exc_type, exc_value, traceback):
-        if self.socket:
-            self.socket.close()
-
-    def readStrict(self, size=1):
-        res = bytearray()
-        while size:
-            cur = self.socket.recv(size)
-            # if not res:
-            #     raise "Socket is closed"
-            size -= len(cur)
-            res.extend(cur)
-
-        return res
-
-    def readUInt(self, size=1):
-        res = self.readStrict(size)
-        val = 0
-        for i in range(len(res)):
-            val += res[i] << (i * 8)
-        return val
-
-    def readUInt8(self):
-        return self.readUInt()
-
-    def readUInt16(self):
-        return self.readUInt(2)
-
-    def readUInt32(self):
-        return self.readUInt(4)
-
-    def readUInt64(self):
-        return self.readUInt(8)
-
-    def readFloat16(self):
-        return struct.unpack("e", self.readStrict(2))
-
-    def readFloat32(self):
-        return struct.unpack("f", self.readStrict(4))
-
-    def readFloat64(self):
-        return struct.unpack("d", self.readStrict(8))
-
-    def readVarUInt(self):
-        x = 0
-        for i in range(9):
-            byte = self.readStrict()[0]
-            x |= (byte & 0x7F) << (7 * i)
-
-            if not byte & 0x80:
-                return x
-
-        return x
-
-    def readStringBinary(self):
-        size = self.readVarUInt()
-        s = self.readStrict(size)
-        return s.decode("utf-8")
-
-    def send(self, byte_array):
-        self.socket.sendall(byte_array)
-
-    def sendHello(self):
-        ba = bytearray()
-        writeVarUInt(0, ba)  # Hello
-        writeStringBinary(CLIENT_NAME, ba)
-        writeVarUInt(21, ba)
-        writeVarUInt(9, ba)
-        writeVarUInt(54449, ba)
-        writeStringBinary(CLICKHOUSE_DATABASE, ba)  # database
-        writeStringBinary("default", ba)  # user
-        writeStringBinary("", ba)  # pwd
-        self.send(ba)
-
-    def receiveHello(self):
-        p_type = self.readVarUInt()
-        assert p_type == 0  # Hello
-        _server_name = self.readStringBinary()
-        _server_version_major = self.readVarUInt()
-        _server_version_minor = self.readVarUInt()
-        _server_revision = self.readVarUInt()
-        _server_timezone = self.readStringBinary()
-        _server_display_name = self.readStringBinary()
-        _server_version_patch = self.readVarUInt()
-
-    def sendQuery(self, query, settings=None):
-        if settings == None:
-            settings = {}  # No settings
-
-        ba = bytearray()
-        query_id = uuid.uuid4().hex
-        writeVarUInt(1, ba)  # query
-        writeStringBinary(query_id, ba)
-
-        ba.append(1)  # INITIAL_QUERY
-
-        # client info
-        serializeClientInfo(ba, query_id)
-
-        # Settings
-        for key, value in settings.items():
-            writeStringBinary(key, ba)
-            writeVarUInt(1, ba)  # is_important
-            writeStringBinary(str(value), ba)
-        writeStringBinary("", ba)  # End of settings
-
-        writeStringBinary("", ba)  # No interserver secret
-        writeVarUInt(2, ba)  # Stage - Complete
-        ba.append(0)  # No compression
-        writeStringBinary(query, ba)  # query, finally
-        self.send(ba)
-
-    def sendEmptyBlock(self):
-        ba = bytearray()
-        writeVarUInt(2, ba)  # Data
-        writeStringBinary("", ba)
-        serializeBlockInfo(ba)
-        writeVarUInt(0, ba)  # rows
-        writeVarUInt(0, ba)  # columns
-        self.send(ba)
-
-    def readException(self):
-        code = self.readUInt32()
-        _name = self.readStringBinary()
-        text = self.readStringBinary()
-        self.readStringBinary()  # trace
-        assertPacket(self.readUInt8(), 0)  # has_nested
-        return "code {}: {}".format(code, text.replace("DB::Exception:", ""))
-
-    def readPacketType(self):
-        packet_type = self.readVarUInt()
-        if packet_type == 2:  # Exception
-            raise RuntimeError(self.readException())
-
-        return packet_type
-
-    def readResponse(self):
-        packet_type = self.readPacketType()
-        if packet_type == 1:  # Data
-            return None
-        if packet_type == 3:  # Progress
-            return None
-        if packet_type == 5:  # End stream
-            return None
-
-        raise RuntimeError("Unexpected packet: {}".format(packet_type))
-
-    def readProgressData(self):
-        read_rows = self.readVarUInt()
-        read_bytes = self.readVarUInt()
-        total_rows_to_read = self.readVarUInt()
-        written_rows = self.readVarUInt()
-        written_bytes = self.readVarUInt()
-
-        return read_rows, read_bytes, total_rows_to_read, written_rows, written_bytes
-
-    def readProgress(self):
-        packet_type = self.readPacketType()
-        if packet_type == 5:  # End stream
-            return None
-        assertPacket(packet_type, 3)  # Progress
-        return self.readProgressData()
-
-    def readHeaderInfo(self):
-        self.readStringBinary()  # external table name
-        # BlockInfo
-        assertPacket(self.readVarUInt(), 1)  # field number 1
-        assertPacket(self.readUInt8(), 0)  # is_overflows
-        assertPacket(self.readVarUInt(), 2)  # field number 2
-        assertPacket(self.readUInt32(), 4294967295)  # bucket_num
-        assertPacket(self.readVarUInt(), 0)  # 0
-        columns = self.readVarUInt()  # rows
-        rows = self.readVarUInt()  # columns
-
-        return columns, rows
-
-    def readHeader(self):
-        packet_type = self.readPacketType()
-        assertPacket(packet_type, 1)  # Data
-
-        columns, rows = self.readHeaderInfo()
-        print("Rows {} Columns {}".format(rows, columns))
-        for _ in range(columns):
-            col_name = self.readStringBinary()
-            type_name = self.readStringBinary()
-            print("Column {} type {}".format(col_name, type_name))
-
-    def readRow(self, row_type, rows):
-        supported_row_types = {
-            "UInt8": self.readUInt8,
-            "UInt16": self.readUInt16,
-            "UInt32": self.readUInt32,
-            "UInt64": self.readUInt64,
-            "Float16": self.readFloat16,
-            "Float32": self.readFloat32,
-            "Float64": self.readFloat64,
-        }
-        if row_type in supported_row_types:
-            read_type = supported_row_types[row_type]
-            row = [read_type() for _ in range(rows)]
-            return row
-        else:
-            raise RuntimeError(
-                "Current python version of tcp client doesn't support the following type of row: {}".format(
-                    row_type
-                )
-            )
-
-    def readDataWithoutProgress(self, need_print_info=True):
-        packet_type = self.readPacketType()
-        while packet_type == 3:  # Progress
-            self.readProgressData()
-            packet_type = self.readPacketType()
-
-        if packet_type == 5:  # End stream
-            return None
-        assertPacket(packet_type, 1)  # Data
-
-        columns, rows = self.readHeaderInfo()
-        data = []
-        if need_print_info:
-            print("Rows {} Columns {}".format(rows, columns))
-
-        for _ in range(columns):
-            col_name = self.readStringBinary()
-            type_name = self.readStringBinary()
-            if need_print_info:
-                print("Column {} type {}".format(col_name, type_name))
-
-            data.append(Data(col_name, self.readRow(type_name, rows)))
-
-        return data
