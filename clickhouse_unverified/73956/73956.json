{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 73956,
  "instance_id": "ClickHouse__ClickHouse-73956",
  "issue_numbers": [
    "35307",
    "64864"
  ],
  "base_commit": "a4678063b3cba453b39a0bd689821ad72ff74f32",
  "patch": "diff --git a/src/Core/Settings.cpp b/src/Core/Settings.cpp\nindex b1fd16640d2b..6c640943ad47 100644\n--- a/src/Core/Settings.cpp\n+++ b/src/Core/Settings.cpp\n@@ -5871,6 +5871,10 @@ Allows creation of [JSON](../../sql-reference/data-types/newjson.md) data type.\n )\", BETA) ALIAS(enable_json_type) \\\n     DECLARE(Bool, allow_general_join_planning, true, R\"(\n Allows a more general join planning algorithm that can handle more complex conditions, but only works with hash join. If hash join is not enabled, then the usual join planning algorithm is used regardless of the value of this setting.\n+)\", 0) \\\n+    DECLARE(UInt64, merge_table_max_tables_to_look_for_schema_inference, 1000, R\"(\n+When creating a `Merge` table without an explicit schema or when using the `merge` table function, infer schema as a union of not more than the specified number of matching tables.\n+If there is a larger number of tables, the schema will be inferred from the first specified number of tables.\n )\", 0) \\\n     DECLARE(Bool, validate_enum_literals_in_opearators, false, R\"(\n If enabled, validate enum literals in operators like `IN`, `NOT IN`, `==`, `!=` against the enum type and throw an exception if the literal is not a valid enum value.\ndiff --git a/src/Core/SettingsChangesHistory.cpp b/src/Core/SettingsChangesHistory.cpp\nindex e980f044a677..3fa5d3fe9211 100644\n--- a/src/Core/SettingsChangesHistory.cpp\n+++ b/src/Core/SettingsChangesHistory.cpp\n@@ -93,6 +93,7 @@ const VersionToSettingsChangesMap & getSettingsChangesHistory()\n             {\"output_format_pretty_fallback_to_vertical\", false, true, \"A new setting\"},\n             {\"output_format_pretty_fallback_to_vertical_max_rows_per_chunk\", 0, 100, \"A new setting\"},\n             {\"output_format_pretty_fallback_to_vertical_min_table_width\", 0, 1000, \"A new setting\"},\n+            {\"merge_table_max_tables_to_look_for_schema_inference\", 1, 1000, \"A new setting\"},\n             {\"max_autoincrement_series\", 1000, 1000, \"A new setting\"},\n             {\"validate_enum_literals_in_opearators\", false, false, \"A new setting\"},\n             {\"allow_experimental_kusto_dialect\", true, false, \"A new setting\"},\ndiff --git a/src/Formats/ReadSchemaUtils.cpp b/src/Formats/ReadSchemaUtils.cpp\nindex 6ad9fc0499ab..e144481d5a87 100644\n--- a/src/Formats/ReadSchemaUtils.cpp\n+++ b/src/Formats/ReadSchemaUtils.cpp\n@@ -440,7 +440,6 @@ try\n             Names names_order; /// Try to save original columns order;\n             std::unordered_map<String, DataTypePtr> names_to_types;\n \n-\n             for (const auto & [schema, file_name] : schemas_for_union_mode)\n             {\n                 for (const auto & [name, type] : schema)\ndiff --git a/src/Interpreters/Aggregator.cpp b/src/Interpreters/Aggregator.cpp\nindex 7a826ad48805..c6aacb8a6e95 100644\n--- a/src/Interpreters/Aggregator.cpp\n+++ b/src/Interpreters/Aggregator.cpp\n@@ -361,15 +361,16 @@ void Aggregator::Params::explain(WriteBuffer & out, size_t indent) const\n \n     {\n         /// Dump keys.\n-        out << prefix << \"Keys: \";\n+        out << prefix << \"Keys:\";\n \n         bool first = true;\n         for (const auto & key : keys)\n         {\n-            if (!first)\n+            if (first)\n+                out << \" \";\n+            else\n                 out << \", \";\n             first = false;\n-\n             out << key;\n         }\n \ndiff --git a/src/Interpreters/InterpreterInsertQuery.cpp b/src/Interpreters/InterpreterInsertQuery.cpp\nindex 3c47f758fef9..c8791f176c09 100644\n--- a/src/Interpreters/InterpreterInsertQuery.cpp\n+++ b/src/Interpreters/InterpreterInsertQuery.cpp\n@@ -377,7 +377,7 @@ Chain InterpreterInsertQuery::buildSink(\n     return out;\n }\n \n-bool InterpreterInsertQuery::shouldAddSquashingFroStorage(const StoragePtr & table) const\n+bool InterpreterInsertQuery::shouldAddSquashingForStorage(const StoragePtr & table) const\n {\n     auto context_ptr = getContext();\n     const Settings & settings = context_ptr->getSettingsRef();\n@@ -464,8 +464,7 @@ std::pair<std::vector<Chain>, std::vector<Chain>> InterpreterInsertQuery::buildP\n     StoragePtr table,\n     size_t view_level,\n     const StorageMetadataPtr & metadata_snapshot,\n-    const Block & query_sample_block\n-    )\n+    const Block & query_sample_block)\n {\n     chassert(presink_streams > 0);\n     chassert(sink_streams > 0);\n@@ -629,7 +628,7 @@ QueryPipeline InterpreterInsertQuery::buildInsertSelectPipeline(ASTInsertQuery &\n \n     pipeline.resize(1);\n \n-    if (shouldAddSquashingFroStorage(table))\n+    if (shouldAddSquashingForStorage(table))\n     {\n         pipeline.addSimpleTransform(\n             [&](const Block & in_header) -> ProcessorPtr\n@@ -685,7 +684,7 @@ QueryPipeline InterpreterInsertQuery::buildInsertSelectPipeline(ASTInsertQuery &\n \n     pipeline.resize(presink_chains.size());\n \n-    if (shouldAddSquashingFroStorage(table))\n+    if (shouldAddSquashingForStorage(table))\n     {\n         pipeline.addSimpleTransform(\n             [&](const Block & in_header) -> ProcessorPtr\n@@ -750,7 +749,7 @@ QueryPipeline InterpreterInsertQuery::buildInsertPipeline(ASTInsertQuery & query\n \n     chain.addSource(std::make_shared<DeduplicationToken::AddTokenInfoTransform>(chain.getInputHeader()));\n \n-    if (shouldAddSquashingFroStorage(table))\n+    if (shouldAddSquashingForStorage(table))\n     {\n         bool table_prefers_large_blocks = table->prefersLargeBlocks();\n \ndiff --git a/src/Interpreters/InterpreterInsertQuery.h b/src/Interpreters/InterpreterInsertQuery.h\nindex cc1d7b100fa9..eda66d9fc3ce 100644\n--- a/src/Interpreters/InterpreterInsertQuery.h\n+++ b/src/Interpreters/InterpreterInsertQuery.h\n@@ -67,7 +67,7 @@ class InterpreterInsertQuery : public IInterpreter, WithContext\n \n     void addBuffer(std::unique_ptr<ReadBuffer> buffer) { owned_buffers.push_back(std::move(buffer)); }\n \n-    bool shouldAddSquashingFroStorage(const StoragePtr & table) const;\n+    bool shouldAddSquashingForStorage(const StoragePtr & table) const;\n \n private:\n     static Block getSampleBlockImpl(const Names & names, const StoragePtr & table, const StorageMetadataPtr & metadata_snapshot, bool no_destination, bool allow_materialized);\ndiff --git a/src/Interpreters/addMissingDefaults.cpp b/src/Interpreters/addMissingDefaults.cpp\nindex 173478332f3d..b5cde92b50e6 100644\n--- a/src/Interpreters/addMissingDefaults.cpp\n+++ b/src/Interpreters/addMissingDefaults.cpp\n@@ -46,7 +46,7 @@ ActionsDAG addMissingDefaults(\n \n     FunctionOverloadResolverPtr func_builder_replicate = FunctionFactory::instance().get(\"replicate\", context);\n \n-    /// We take given columns from input block and missed columns without default value\n+    /// We take given columns from the input block and missed columns without default value\n     /// (default and materialized will be computed later).\n     for (const auto & column : required_columns)\n     {\ndiff --git a/src/Interpreters/addMissingDefaults.h b/src/Interpreters/addMissingDefaults.h\nindex 551583a0006f..7080b19ec60d 100644\n--- a/src/Interpreters/addMissingDefaults.h\n+++ b/src/Interpreters/addMissingDefaults.h\n@@ -11,10 +11,10 @@ class ColumnsDescription;\n \n class ActionsDAG;\n \n-/** Adds three types of columns into block\n-  * 1. Columns, that are missed inside request, but present in table without defaults (missed columns)\n-  * 2. Columns, that are missed inside request, but present in table with defaults (columns with default values)\n-  * 3. Columns that materialized from other columns (materialized columns)\n+/** Adds three types of columns into a block:\n+  * 1. Columns, that are missed in the query, but present in the table without defaults (missed columns)\n+  * 2. Columns, that are missed in the query, but present in the table with defaults (columns with default values)\n+  * 3. Columns that are materialized from other columns (materialized columns)\n   * Also can substitute NULL with DEFAULT value in case of INSERT SELECT query (null_as_default) if according setting is 1.\n   * All three types of columns are materialized (not constants).\n   */\ndiff --git a/src/Processors/QueryPlan/ExpressionStep.h b/src/Processors/QueryPlan/ExpressionStep.h\nindex 15fbbdad8078..f0bf62a1ca4d 100644\n--- a/src/Processors/QueryPlan/ExpressionStep.h\n+++ b/src/Processors/QueryPlan/ExpressionStep.h\n@@ -12,7 +12,6 @@ class JoiningTransform;\n class ExpressionStep : public ITransformingStep\n {\n public:\n-\n     explicit ExpressionStep(const Header & input_header_, ActionsDAG actions_dag_);\n     String getName() const override { return \"Expression\"; }\n \ndiff --git a/src/Processors/Transforms/buildPushingToViewsChain.cpp b/src/Processors/Transforms/buildPushingToViewsChain.cpp\nindex 3250ae932e69..78721c548eb9 100644\n--- a/src/Processors/Transforms/buildPushingToViewsChain.cpp\n+++ b/src/Processors/Transforms/buildPushingToViewsChain.cpp\n@@ -374,7 +374,7 @@ std::optional<Chain> generateViewChain(\n         bool check_access = !materialized_view->hasInnerTable() && materialized_view->getInMemoryMetadataPtr()->sql_security_type;\n         out = interpreter.buildChain(inner_table, view_level + 1, inner_metadata_snapshot, insert_columns, thread_status_holder, view_counter_ms, check_access);\n \n-        if (interpreter.shouldAddSquashingFroStorage(inner_table))\n+        if (interpreter.shouldAddSquashingForStorage(inner_table))\n         {\n             bool table_prefers_large_blocks = inner_table->prefersLargeBlocks();\n             const auto & settings = insert_context->getSettingsRef();\ndiff --git a/src/Storages/ColumnsDescription.cpp b/src/Storages/ColumnsDescription.cpp\nindex ef2251ae54ed..2beec76837df 100644\n--- a/src/Storages/ColumnsDescription.cpp\n+++ b/src/Storages/ColumnsDescription.cpp\n@@ -386,7 +386,9 @@ void ColumnsDescription::modifyColumnOrder(const String & column_name, const Str\n     };\n \n     if (first)\n+    {\n         reorder_column([&]() { return columns.cbegin(); });\n+    }\n     else if (!after_column.empty() && column_name != after_column)\n     {\n         /// Checked first\ndiff --git a/src/Storages/StorageMerge.cpp b/src/Storages/StorageMerge.cpp\nindex f718496c892e..579e927721dd 100644\n--- a/src/Storages/StorageMerge.cpp\n+++ b/src/Storages/StorageMerge.cpp\n@@ -1,4 +1,3 @@\n-#include <algorithm>\n #include <functional>\n #include <iterator>\n #include <Analyzer/ConstantNode.h>\n@@ -14,11 +13,9 @@\n #include <Columns/ColumnSet.h>\n #include <Columns/ColumnString.h>\n #include <Core/Settings.h>\n-#include <Core/SortDescription.h>\n #include <DataTypes/DataTypeString.h>\n+#include <DataTypes/getLeastSupertype.h>\n #include <DataTypes/IDataType.h>\n-#include <Databases/IDatabase.h>\n-#include <IO/WriteBufferFromString.h>\n #include <Interpreters/Context.h>\n #include <Interpreters/ExpressionActions.h>\n #include <Interpreters/IdentifierSemantic.h>\n@@ -29,11 +26,13 @@\n #include <Interpreters/evaluateConstantExpression.h>\n #include <Interpreters/getHeaderForProcessingStage.h>\n #include <Interpreters/replaceAliasColumnsInQuery.h>\n+#include <Interpreters/addMissingDefaults.h>\n #include <Parsers/ASTExpressionList.h>\n #include <Parsers/ASTFunction.h>\n #include <Parsers/ASTIdentifier.h>\n #include <Parsers/ASTLiteral.h>\n #include <Parsers/ASTSelectQuery.h>\n+#include <Parsers/queryToString.h>\n #include <Planner/PlannerActionsVisitor.h>\n #include <Planner/Utils.h>\n #include <Processors/QueryPlan/BuildQueryPipelineSettings.h>\n@@ -43,8 +42,6 @@\n #include <Processors/QueryPlan/QueryPlan.h>\n #include <Processors/QueryPlan/ReadFromMergeTree.h>\n #include <Processors/Sources/NullSource.h>\n-#include <Processors/Sources/SourceFromSingleChunk.h>\n-#include <Processors/Transforms/ExpressionTransform.h>\n #include <Processors/Transforms/FilterTransform.h>\n #include <Processors/Transforms/MaterializingTransform.h>\n #include <QueryPipeline/QueryPipelineBuilder.h>\n@@ -57,8 +54,6 @@\n #include <Storages/StorageView.h>\n #include <Storages/VirtualColumnUtils.h>\n #include <Storages/checkAndGetLiteralArgument.h>\n-#include <base/defines.h>\n-#include <base/range.h>\n #include <Common/Exception.h>\n #include <Common/assert_cast.h>\n #include <Common/checkStackSize.h>\n@@ -66,6 +61,7 @@\n #include <Core/NamesAndTypes.h>\n #include <Functions/FunctionFactory.h>\n \n+\n namespace DB\n {\n namespace Setting\n@@ -73,6 +69,7 @@ namespace Setting\n     extern const SettingsBool allow_experimental_analyzer;\n     extern const SettingsSeconds lock_acquire_timeout;\n     extern const SettingsFloat max_streams_multiplier_for_merge_tables;\n+    extern const SettingsUInt64 merge_table_max_tables_to_look_for_schema_inference;\n }\n \n namespace\n@@ -82,6 +79,7 @@ bool columnIsPhysical(ColumnDefaultKind kind)\n {\n     return kind == ColumnDefaultKind::Default || kind == ColumnDefaultKind::Materialized;\n }\n+\n bool columnDefaultKindHasSameType(ColumnDefaultKind lhs, ColumnDefaultKind rhs)\n {\n     if (lhs == rhs)\n@@ -153,7 +151,9 @@ StorageMerge::StorageMerge(\n         source_databases_and_tables_)\n {\n     StorageInMemoryMetadata storage_metadata;\n-    storage_metadata.setColumns(columns_.empty() ? getColumnsDescriptionFromSourceTables() : columns_);\n+    storage_metadata.setColumns(columns_.empty()\n+        ? getColumnsDescriptionFromSourceTables(context_->getSettingsRef()[Setting::merge_table_max_tables_to_look_for_schema_inference])\n+        : columns_);\n     storage_metadata.setComment(comment);\n     setInMemoryMetadata(storage_metadata);\n     setVirtuals(createVirtuals());\n@@ -176,7 +176,9 @@ StorageMerge::StorageMerge(\n         source_table_regexp_, {})\n {\n     StorageInMemoryMetadata storage_metadata;\n-    storage_metadata.setColumns(columns_.empty() ? getColumnsDescriptionFromSourceTables() : columns_);\n+    storage_metadata.setColumns(columns_.empty()\n+        ? getColumnsDescriptionFromSourceTables(context_->getSettingsRef()[Setting::merge_table_max_tables_to_look_for_schema_inference])\n+        : columns_);\n     storage_metadata.setComment(comment);\n     setInMemoryMetadata(storage_metadata);\n     setVirtuals(createVirtuals());\n@@ -187,12 +189,56 @@ StorageMerge::DatabaseTablesIterators StorageMerge::getDatabaseIterators(Context\n     return database_name_or_regexp.getDatabaseIterators(context_);\n }\n \n-ColumnsDescription StorageMerge::getColumnsDescriptionFromSourceTables() const\n+ColumnsDescription StorageMerge::unifyColumnsDescription(std::function<void(std::function<void(const StoragePtr &)>)> for_each_table)\n {\n-    auto table = getFirstTable([](auto && t) { return t; });\n-    if (!table)\n+    ColumnsDescription res;\n+\n+    for_each_table([&res](auto && t)\n+    {\n+        auto structure = t->getInMemoryMetadataPtr()->getColumns();\n+        String prev_column_name;\n+        for (const ColumnDescription & column : structure)\n+        {\n+            if (!res.has(column.name))\n+            {\n+                res.add(column, prev_column_name);\n+            }\n+            else if (column != res.get(column.name))\n+            {\n+                res.modify(column.name, [&column](ColumnDescription & what)\n+                {\n+                    what.type = getLeastSupertypeOrVariant(DataTypes{what.type, column.type});\n+                    if (what.default_desc != column.default_desc)\n+                        what.default_desc = {};\n+                });\n+            }\n+            prev_column_name = column.name;\n+        }\n+    });\n+\n+    if (res.empty())\n         throw Exception{ErrorCodes::CANNOT_EXTRACT_TABLE_STRUCTURE, \"There are no tables satisfied provided regexp, you must specify table structure manually\"};\n-    return table->getInMemoryMetadataPtr()->getColumns();\n+\n+    return res;\n+}\n+\n+ColumnsDescription StorageMerge::getColumnsDescriptionFromSourceTables(size_t max_tables_to_look) const\n+{\n+    size_t table_num = 0;\n+\n+    return unifyColumnsDescription([&table_num, max_tables_to_look, this](std::function<void(const StoragePtr &)> callback)\n+    {\n+        getFirstTable([&table_num, &callback, max_tables_to_look](auto && t)\n+        {\n+            if (!t)\n+                return false;\n+\n+            callback(t);\n+\n+            ++table_num;\n+            return table_num >= max_tables_to_look;\n+        });\n+    });\n }\n \n template <typename F>\n@@ -457,7 +503,7 @@ void ReadFromMerge::initializePipeline(QueryPipelineBuilder & pipeline, const Bu\n \n         const auto storage = std::get<1>(table);\n         const auto storage_metadata_snapshot = storage->getInMemoryMetadataPtr();\n-        const auto nested_storage_snaphsot = storage->getStorageSnapshot(storage_metadata_snapshot, context);\n+        const auto nested_storage_snapshot = storage->getStorageSnapshot(storage_metadata_snapshot, context);\n \n         Names column_names_as_aliases;\n         Aliases aliases;\n@@ -518,7 +564,6 @@ void ReadFromMerge::filterTablesAndCreateChildrenPlans()\n     }\n \n     selected_tables = getSelectedTables(context, has_database_virtual_column, has_table_virtual_column);\n-\n     child_plans = createChildrenPlans(query_info);\n }\n \n@@ -582,7 +627,7 @@ std::vector<ReadFromMerge::ChildPlan> ReadFromMerge::createChildrenPlans(SelectQ\n         Aliases aliases;\n         RowPolicyDataOpt row_policy_data_opt;\n         auto storage_metadata_snapshot = storage->getInMemoryMetadataPtr();\n-        auto nested_storage_snaphsot = storage->getStorageSnapshot(storage_metadata_snapshot, modified_context);\n+        auto nested_storage_snapshot = storage->getStorageSnapshot(storage_metadata_snapshot, modified_context);\n \n         Names column_names_as_aliases;\n         Names real_column_names = column_names;\n@@ -600,13 +645,13 @@ std::vector<ReadFromMerge::ChildPlan> ReadFromMerge::createChildrenPlans(SelectQ\n         }\n \n         auto modified_query_info\n-            = getModifiedQueryInfo(modified_context, table, nested_storage_snaphsot, real_column_names, column_names_as_aliases, aliases);\n+            = getModifiedQueryInfo(modified_context, table, nested_storage_snapshot, real_column_names, column_names_as_aliases, aliases);\n \n         if (!context->getSettingsRef()[Setting::allow_experimental_analyzer])\n         {\n             auto storage_columns = storage_metadata_snapshot->getColumns();\n             auto syntax_result = TreeRewriter(context).analyzeSelect(\n-                modified_query_info.query, TreeRewriterResult({}, storage, nested_storage_snaphsot));\n+                modified_query_info.query, TreeRewriterResult({}, storage, nested_storage_snapshot));\n \n             bool with_aliases = common_processed_stage == QueryProcessingStage::FetchColumns && !storage_columns.getAliases().empty();\n             if (with_aliases)\n@@ -655,13 +700,17 @@ std::vector<ReadFromMerge::ChildPlan> ReadFromMerge::createChildrenPlans(SelectQ\n             }\n         }\n \n+        Names column_names_to_read = column_names_as_aliases.empty() ? std::move(real_column_names) : std::move(column_names_as_aliases);\n+\n+        std::erase_if(column_names_to_read, [existing_columns = nested_storage_snapshot->getAllColumnsDescription()](const auto & column_name){ return !existing_columns.has(column_name); });\n+\n         auto child = createPlanForTable(\n-            nested_storage_snaphsot,\n+            nested_storage_snapshot,\n             modified_query_info,\n             common_processed_stage,\n             required_max_block_size,\n             table,\n-            column_names_as_aliases.empty() ? std::move(real_column_names) : std::move(column_names_as_aliases),\n+            column_names_to_read,\n             row_policy_data_opt,\n             modified_context,\n             current_streams);\n@@ -672,9 +721,9 @@ std::vector<ReadFromMerge::ChildPlan> ReadFromMerge::createChildrenPlans(SelectQ\n         {\n             addVirtualColumns(child, modified_query_info, common_processed_stage, table);\n \n-            /// Subordinary tables could have different but convertible types, like numeric types of different width.\n+            /// Source tables could have different but convertible types, like numeric types of different width.\n             /// We must return streams with structure equals to structure of Merge table.\n-            convertAndFilterSourceStream(common_header, modified_query_info, nested_storage_snaphsot, aliases, row_policy_data_opt, context, child);\n+            convertAndFilterSourceStream(common_header, modified_query_info, nested_storage_snapshot, aliases, row_policy_data_opt, context, child);\n \n             for (const auto & filter_info : pushed_down_filters)\n             {\n@@ -1163,7 +1212,7 @@ ReadFromMerge::ChildPlan ReadFromMerge::createPlanForTable(\n     QueryProcessingStage::Enum processed_stage,\n     UInt64 max_block_size,\n     const StorageWithLockAndName & storage_with_lock,\n-    Names && real_column_names,\n+    const Names & real_column_names_read_from_the_source_table,\n     const RowPolicyDataOpt & row_policy_data_opt,\n     ContextMutablePtr modified_context,\n     size_t streams_num) const\n@@ -1172,7 +1221,7 @@ ReadFromMerge::ChildPlan ReadFromMerge::createPlanForTable(\n \n     auto & modified_select = modified_query_info.query->as<ASTSelectQuery &>();\n \n-    if (!InterpreterSelectQuery::isQueryWithFinal(modified_query_info) && storage->needRewriteQueryWithFinal(real_column_names))\n+    if (!InterpreterSelectQuery::isQueryWithFinal(modified_query_info) && storage->needRewriteQueryWithFinal(real_column_names_read_from_the_source_table))\n     {\n         /// NOTE: It may not work correctly in some cases, because query was analyzed without final.\n         /// However, it's needed for Materialized...SQL and it's unlikely that someone will use it with Merge tables.\n@@ -1190,7 +1239,8 @@ ReadFromMerge::ChildPlan ReadFromMerge::createPlanForTable(\n \n     if (processed_stage <= storage_stage)\n     {\n-        /// If there are only virtual columns in query, you must request at least one other column.\n+        /// If there are only virtual columns in query, we must request at least one other column.\n+        Names real_column_names = real_column_names_read_from_the_source_table;\n         if (real_column_names.empty())\n             real_column_names.push_back(ExpressionActions::getSmallestColumn(storage_snapshot_->metadata->getColumns().getAllPhysical()).name);\n \n@@ -1485,7 +1535,6 @@ void ReadFromMerge::convertAndFilterSourceStream(\n {\n     Block before_block_header = child.plan.getCurrentHeader();\n \n-    auto storage_sample_block = snapshot->metadata->getSampleBlock();\n     auto pipe_columns = before_block_header.getNamesAndTypesList();\n \n     if (local_context->getSettingsRef()[Setting::allow_experimental_analyzer])\n@@ -1529,26 +1578,62 @@ void ReadFromMerge::convertAndFilterSourceStream(\n         }\n     }\n \n-    ActionsDAG::MatchColumnsMode convert_actions_match_columns_mode = ActionsDAG::MatchColumnsMode::Name;\n-\n-    /* Output headers may differ from what StorageMerge expects in some cases.\n-     * When the child table engine produces a query plan for the stage after FetchColumns,\n-     * execution names in the output header may be different.\n-     * The same happens with StorageDistributed, even in the case of FetchColumns.\n-     */\n-    if (local_context->getSettingsRef()[Setting::allow_experimental_analyzer]\n-        && (child.stage != QueryProcessingStage::FetchColumns || dynamic_cast<const StorageDistributed *>(&snapshot->storage) != nullptr))\n-        convert_actions_match_columns_mode = ActionsDAG::MatchColumnsMode::Position;\n-\n+    /// This is the filter for the individual source table, that's why filtering has to be done before all structure adaptations.\n     if (row_policy_data_opt)\n         row_policy_data_opt->addFilterTransform(child.plan);\n \n-    auto convert_actions_dag = ActionsDAG::makeConvertingActions(child.plan.getCurrentHeader().getColumnsWithTypeAndName(),\n-                                                                header.getColumnsWithTypeAndName(),\n-                                                                convert_actions_match_columns_mode);\n+    /** Output headers may differ from what StorageMerge expects in some cases.\n+      * When the child table engine produces a query plan for the stage after FetchColumns,\n+      * execution names in the output header may be different.\n+      * The same happens with StorageDistributed, even in the case of FetchColumns.\n+      */\n+\n+    /** Convert types of columns according to the resulting Merge table.\n+      * And convert column names to the expected ones.\n+       */\n+    ColumnsWithTypeAndName current_step_columns = child.plan.getCurrentHeader().getColumnsWithTypeAndName();\n+    ColumnsWithTypeAndName converted_columns;\n+    size_t size = current_step_columns.size();\n+    converted_columns.reserve(current_step_columns.size());\n+    for (size_t i = 0; i < size; ++i)\n+    {\n+        const auto & source_elem = current_step_columns[i];\n+        if (header.has(source_elem.name))\n+        {\n+            converted_columns.push_back(header.getByName(source_elem.name));\n+        }\n+        else if (header.columns() == current_step_columns.size())\n+        {\n+            /// Virtual columns and columns read from Distributed tables (having different name but matched by position).\n+            converted_columns.push_back(header.getByPosition(i));\n+        }\n+        else\n+        {\n+            /// Matching by name, but some columns are unneeded.\n+            converted_columns.push_back(source_elem);\n+        }\n+    }\n+\n+    auto convert_actions_dag = ActionsDAG::makeConvertingActions(\n+        current_step_columns,\n+        converted_columns,\n+        ActionsDAG::MatchColumnsMode::Position);\n \n     auto expression_step = std::make_unique<ExpressionStep>(child.plan.getCurrentHeader(), std::move(convert_actions_dag));\n     child.plan.addStep(std::move(expression_step));\n+\n+    /// Add missing columns for the resulting Merge table.\n+    {\n+        auto adding_missing_defaults_dag = addMissingDefaults(\n+            child.plan.getCurrentHeader(),\n+            header.getNamesAndTypesList(),\n+            snapshot->getAllColumnsDescription(),\n+            local_context,\n+            false);\n+\n+        auto adding_missing_defaults_step = std::make_unique<ExpressionStep>(child.plan.getCurrentHeader(), std::move(adding_missing_defaults_dag));\n+        child.plan.addStep(std::move(adding_missing_defaults_step));\n+    }\n }\n \n const ReadFromMerge::StorageListWithLocks & ReadFromMerge::getSelectedTables()\ndiff --git a/src/Storages/StorageMerge.h b/src/Storages/StorageMerge.h\nindex 82f8fb78feca..7a8f11204e22 100644\n--- a/src/Storages/StorageMerge.h\n+++ b/src/Storages/StorageMerge.h\n@@ -87,6 +87,10 @@ class StorageMerge final : public IStorage, WithContext\n     using DatabaseTablesIterators = std::vector<DatabaseTablesIteratorPtr>;\n     DatabaseTablesIterators getDatabaseIterators(ContextPtr context) const;\n \n+    /// Returns a unified column structure among multiple tables.\n+    /// Takes a function that invokes a callback for every table. NOTE: This is quite inconvenient.\n+    static ColumnsDescription unifyColumnsDescription(std::function<void(std::function<void(const StoragePtr &)>)> for_each_table);\n+\n private:\n     /// (Database, Table, Lock, TableName)\n     using StorageWithLockAndName = std::tuple<String, StoragePtr, TableLockHolder, String>;\n@@ -123,7 +127,7 @@ class StorageMerge final : public IStorage, WithContext\n \n     ColumnSizeByName getColumnSizes() const override;\n \n-    ColumnsDescription getColumnsDescriptionFromSourceTables() const;\n+    ColumnsDescription getColumnsDescriptionFromSourceTables(size_t max_tables_to_look) const;\n \n     static VirtualColumnsDescription createVirtuals();\n \n@@ -258,7 +262,7 @@ class ReadFromMerge final : public SourceStepWithFilter\n         QueryProcessingStage::Enum processed_stage,\n         UInt64 max_block_size,\n         const StorageWithLockAndName & storage_with_lock,\n-        Names && real_column_names,\n+        const Names & real_column_names_read_from_the_source_table,\n         const RowPolicyDataOpt & row_policy_data_opt,\n         ContextMutablePtr modified_context,\n         size_t streams_num) const;\ndiff --git a/src/TableFunctions/TableFunctionMerge.cpp b/src/TableFunctions/TableFunctionMerge.cpp\nindex cca0c2990f63..e6ada9e5fcaf 100644\n--- a/src/TableFunctions/TableFunctionMerge.cpp\n+++ b/src/TableFunctions/TableFunctionMerge.cpp\n@@ -1,5 +1,6 @@\n #include <Common/OptimizedRegularExpression.h>\n #include <Common/typeid_cast.h>\n+#include <Core/Settings.h>\n #include <Storages/StorageMerge.h>\n #include <Storages/checkAndGetLiteralArgument.h>\n #include <Parsers/ASTFunction.h>\n@@ -7,8 +8,8 @@\n #include <Analyzer/FunctionNode.h>\n #include <Analyzer/TableFunctionNode.h>\n #include <Interpreters/evaluateConstantExpression.h>\n-#include <Interpreters/Context.h>\n #include <Interpreters/DatabaseCatalog.h>\n+#include <Interpreters/Context.h>\n #include <Access/ContextAccess.h>\n #include <TableFunctions/TableFunctionFactory.h>\n #include <TableFunctions/registerTableFunctions.h>\n@@ -23,6 +24,11 @@ namespace ErrorCodes\n     extern const int BAD_ARGUMENTS;\n }\n \n+namespace Setting\n+{\n+    extern const SettingsUInt64 merge_table_max_tables_to_look_for_schema_inference;\n+}\n+\n namespace\n {\n \n@@ -159,25 +165,35 @@ const TableFunctionMerge::DBToTableSetMap & TableFunctionMerge::getSourceDatabas\n \n ColumnsDescription TableFunctionMerge::getActualTableStructure(ContextPtr context, bool /*is_insert_query*/) const\n {\n-    for (const auto & db_with_tables : getSourceDatabasesAndTables(context))\n+    size_t table_num = 0;\n+    size_t max_tables_to_look = context->getSettingsRef()[Setting::merge_table_max_tables_to_look_for_schema_inference];\n+\n+    return StorageMerge::unifyColumnsDescription([&table_num, &context, max_tables_to_look, this](std::function<void(const StoragePtr &)> callback)\n     {\n-        for (const auto & table : db_with_tables.second)\n+        for (const auto & db_with_tables : getSourceDatabasesAndTables(context))\n         {\n-            auto storage = DatabaseCatalog::instance().tryGetTable(StorageID{db_with_tables.first, table}, context);\n-            if (storage)\n-                return ColumnsDescription{storage->getInMemoryMetadataPtr()->getColumns().getAllPhysical()};\n+            for (const auto & table : db_with_tables.second)\n+            {\n+                if (table_num >= max_tables_to_look)\n+                    return;\n+\n+                auto storage = DatabaseCatalog::instance().tryGetTable(StorageID{db_with_tables.first, table}, context);\n+                if (storage)\n+                {\n+                    ++table_num;\n+                    callback(storage);\n+                }\n+            }\n         }\n-    }\n-\n-    throwNoTablesMatchRegexp(source_database_name_or_regexp, source_table_regexp);\n+    });\n }\n \n \n-StoragePtr TableFunctionMerge::executeImpl(const ASTPtr & /*ast_function*/, ContextPtr context, const std::string & table_name, ColumnsDescription /*cached_columns*/, bool is_insert_query) const\n+StoragePtr TableFunctionMerge::executeImpl(const ASTPtr & /*ast_function*/, ContextPtr context, const std::string & table_name, ColumnsDescription /*cached_columns*/, bool /*is_insert_query*/) const\n {\n     auto res = std::make_shared<StorageMerge>(\n         StorageID(getDatabaseName(), table_name),\n-        getActualTableStructure(context, is_insert_query),\n+        ColumnsDescription{},\n         String{},\n         source_database_name_or_regexp,\n         database_is_regexp,\n",
  "test_patch": "diff --git a/tests/integration/test_partition/test.py b/tests/integration/test_partition/test.py\nindex 6600d6712e04..7395e544816c 100644\n--- a/tests/integration/test_partition/test.py\n+++ b/tests/integration/test_partition/test.py\n@@ -372,6 +372,11 @@ def test_drop_detached_parts(drop_detached_parts_table):\n \n \n def test_system_detached_parts(drop_detached_parts_table):\n+    q(\"drop table if exists sdp_0 sync\")\n+    q(\"drop table if exists sdp_1 sync\")\n+    q(\"drop table if exists sdp_2 sync\")\n+    q(\"drop table if exists sdp_3 sync\")\n+\n     q(\n         \"create table sdp_0 (n int, x int) engine=MergeTree order by n SETTINGS compress_marks=false, compress_primary_key=false, ratio_of_defaults_for_sparse_serialization=1\"\n     )\n@@ -492,12 +497,13 @@ def test_system_detached_parts(drop_detached_parts_table):\n             q(\"alter table sdp_{} attach partition id '{}'\".format(i, p))\n \n     assert (\n-        q(\"select n, x, count() from merge('default', 'sdp_') group by n, x\")\n+        q(\"select n, x::int AS x, count() from merge('default', '^sdp_') group by n, x\")\n         == \"0\\t0\\t4\\n1\\t1\\t4\\n\"\n     )\n \n \n def test_detached_part_dir_exists(started_cluster):\n+    q(\"drop table if exists detached_part_dir_exists sync\")\n     q(\n         \"create table detached_part_dir_exists (n int) engine=MergeTree order by n \"\n         \"SETTINGS compress_marks=false, compress_primary_key=false, ratio_of_defaults_for_sparse_serialization=1, old_parts_lifetime=0\"\n@@ -551,6 +557,7 @@ def test_detached_part_dir_exists(started_cluster):\n \n \n def test_make_clone_in_detached(started_cluster):\n+    q(\"drop table if exists clone_in_detached sync\")\n     q(\n         \"create table clone_in_detached (n int, m String) engine=ReplicatedMergeTree('/clone_in_detached', '1') order by n SETTINGS compress_marks=false, compress_primary_key=false, ratio_of_defaults_for_sparse_serialization=1\"\n     )\ndiff --git a/tests/queries/0_stateless/00717_merge_and_distributed.sql b/tests/queries/0_stateless/00717_merge_and_distributed.sql\nindex 4d63757e65c3..69e9c12e6c05 100644\n--- a/tests/queries/0_stateless/00717_merge_and_distributed.sql\n+++ b/tests/queries/0_stateless/00717_merge_and_distributed.sql\n@@ -6,7 +6,8 @@ DROP TABLE IF EXISTS test_local_2;\n DROP TABLE IF EXISTS test_distributed_1;\n DROP TABLE IF EXISTS test_distributed_2;\n \n-set allow_deprecated_syntax_for_merge_tree=1;\n+SET merge_table_max_tables_to_look_for_schema_inference = 1;\n+SET allow_deprecated_syntax_for_merge_tree = 1;\n CREATE TABLE test_local_1 (date Date, value UInt32) ENGINE = MergeTree(date, date, 8192);\n CREATE TABLE test_local_2 (date Date, value UInt32) ENGINE = MergeTree(date, date, 8192);\n CREATE TABLE test_distributed_1 AS test_local_1 ENGINE = Distributed('test_shard_localhost', currentDatabase(), test_local_1, rand());\ndiff --git a/tests/queries/0_stateless/02024_merge_regexp_assert.sql b/tests/queries/0_stateless/02024_merge_regexp_assert.sql\nindex de1fdbbb56b1..feb1ce2f974b 100644\n--- a/tests/queries/0_stateless/02024_merge_regexp_assert.sql\n+++ b/tests/queries/0_stateless/02024_merge_regexp_assert.sql\n@@ -6,5 +6,5 @@ CREATE TABLE t (b UInt8) ENGINE = Memory;\n SELECT a FROM merge(REGEXP('.'), '^t$'); -- { serverError UNKNOWN_IDENTIFIER }\n SELECT a FROM merge(REGEXP('\\0'), '^t$'); -- { serverError UNKNOWN_IDENTIFIER }\n SELECT a FROM merge(REGEXP('\\0a'), '^t$'); -- { serverError UNKNOWN_IDENTIFIER }\n-SELECT a FROM merge(REGEXP('\\0a'), '^$'); -- { serverError BAD_ARGUMENTS }\n+SELECT a FROM merge(REGEXP('\\0a'), '^$'); -- { serverError CANNOT_EXTRACT_TABLE_STRUCTURE }\n DROP TABLE t;\ndiff --git a/tests/queries/0_stateless/02522_different_types_in_storage_merge.sql b/tests/queries/0_stateless/02522_different_types_in_storage_merge.sql\nindex db0a498fd82c..d15997f4f6bc 100644\n--- a/tests/queries/0_stateless/02522_different_types_in_storage_merge.sql\n+++ b/tests/queries/0_stateless/02522_different_types_in_storage_merge.sql\n@@ -1,3 +1,5 @@\n+SET merge_table_max_tables_to_look_for_schema_inference = 1;\n+\n CREATE TABLE test_s64_local (date Date, value Int64) ENGINE = MergeTree order by tuple();\n CREATE TABLE test_u64_local (date Date, value UInt64) ENGINE = MergeTree order by tuple();\n CREATE TABLE test_s64_distributed AS test_s64_local ENGINE = Distributed('test_shard_localhost', currentDatabase(), test_s64_local, rand());\ndiff --git a/tests/queries/0_stateless/02763_row_policy_storage_merge.reference b/tests/queries/0_stateless/02763_row_policy_storage_merge.reference\nindex 6510a3c933e1..82ee518c61db 100644\n--- a/tests/queries/0_stateless/02763_row_policy_storage_merge.reference\n+++ b/tests/queries/0_stateless/02763_row_policy_storage_merge.reference\n@@ -143,8 +143,8 @@ SELECT y from merge(currentDatabase(), 02763_merge)\n SELECT *\n SELECT x, lc\n SELECT *\n-1\t11\t111\t111\t42\n-1\t11\t111\t111\t42\n+1\t11\t111\t111\n+1\t11\t111\t111\n SELECT x, lc\n 1\t111\n 1\t111\n@@ -293,8 +293,8 @@ SELECT y from merge(currentDatabase(), 02763_merge)\n SELECT *\n SELECT x, lc\n SELECT *\n-1\t11\t111\t111\t42\n-1\t11\t111\t111\t42\n+1\t11\t111\t111\n+1\t11\t111\t111\n SELECT x, lc\n 1\t111\n 1\t111\ndiff --git a/tests/queries/0_stateless/02763_row_policy_storage_merge.sql.j2 b/tests/queries/0_stateless/02763_row_policy_storage_merge.sql.j2\nindex eabbde9e9dd7..f2d5e93b608e 100644\n--- a/tests/queries/0_stateless/02763_row_policy_storage_merge.sql.j2\n+++ b/tests/queries/0_stateless/02763_row_policy_storage_merge.sql.j2\n@@ -3,6 +3,7 @@ DROP TABLE IF EXISTS 02763_merge_log_2;\n DROP TABLE IF EXISTS 02763_merge_merge_1;\n DROP TABLE IF EXISTS 02763_merge_merge_2;\n DROP TABLE IF EXISTS 02763_merge_fancycols;\n+DROP TABLE IF EXISTS 02763_engine_merge_12;\n DROP ROW POLICY IF EXISTS 02763_filter_1 ON 02763_merge_log_1;\n DROP ROW POLICY IF EXISTS 02763_filter_2 ON 02763_merge_merge_1;\n DROP ROW POLICY IF EXISTS 02763_filter_3 ON 02763_merge_log_1;\ndiff --git a/tests/queries/0_stateless/02918_optimize_count_for_merge_tables.reference b/tests/queries/0_stateless/02918_optimize_count_for_merge_tables.reference\nindex 7278018f1d63..62d78339b14d 100644\n--- a/tests/queries/0_stateless/02918_optimize_count_for_merge_tables.reference\n+++ b/tests/queries/0_stateless/02918_optimize_count_for_merge_tables.reference\n@@ -7,9 +7,9 @@ Expression ((Projection + Before ORDER BY))\n   Aggregating\n     Expression (Before GROUP BY)\n       ReadFromMerge\n-        Expression\n+        Expression (( + ))\n           ReadFromMergeTree (default.mt1)\n-        Expression\n+        Expression (( + ))\n           ReadFromMergeTree (default.mt2)\n-        Expression\n+        Expression (( + ))\n           ReadFromStorage (TinyLog)\ndiff --git a/tests/queries/0_stateless/03217_filtering_in_storage_merge.reference b/tests/queries/0_stateless/03217_filtering_in_storage_merge.reference\nindex d366ad04c396..faf90275d117 100644\n--- a/tests/queries/0_stateless/03217_filtering_in_storage_merge.reference\n+++ b/tests/queries/0_stateless/03217_filtering_in_storage_merge.reference\n@@ -2,5 +2,5 @@ Expression ((Project names + Projection))\n   Aggregating\n     Expression (Before GROUP BY)\n       ReadFromMerge\n-        Filter (( + ( + )))\n+        Filter (( + ( + ( + ))))\n           ReadFromMergeTree (default.test_03217_merge_replica_1)\ndiff --git a/tests/queries/0_stateless/03302_analyzer_join_filter_push_down_bug.reference b/tests/queries/0_stateless/03302_analyzer_join_filter_push_down_bug.reference\nindex 07540b3d9d9e..6d05103be79c 100644\n--- a/tests/queries/0_stateless/03302_analyzer_join_filter_push_down_bug.reference\n+++ b/tests/queries/0_stateless/03302_analyzer_join_filter_push_down_bug.reference\n@@ -2,7 +2,7 @@ Expression ((Project names + Projection))\n Actions: INPUT :: 0 -> count() UInt64 : 0\n Positions: 0\n   Aggregating\n-  Keys: \n+  Keys:\n   Aggregates:\n       count()\n         Function: count() \u2192 UInt64\ndiff --git a/tests/queries/0_stateless/03302_merge_table_structure_unification.reference b/tests/queries/0_stateless/03302_merge_table_structure_unification.reference\nnew file mode 100644\nindex 000000000000..15565cb3091d\n--- /dev/null\n+++ b/tests/queries/0_stateless/03302_merge_table_structure_unification.reference\n@@ -0,0 +1,19 @@\n+-1\t\t['Goodbye']\t2025-01-01 02:03:04\n+1\tHello\t['World']\t1970-01-01 00:00:00\n+a\tInt32\t\t\t\t\t\n+b\tString\t\t\t\t\t\n+c\tArray(Nullable(String))\t\t\t\t\t\n+d\tDateTime(\\'UTC\\')\tDEFAULT\tnow()\t\t\t\n+-1\t\t['Goodbye']\t2025-01-01 02:03:04\n+-1\t\t['Goodbye']\t2025-01-01 02:03:04\n+1\tHello\t['World']\t1970-01-01 00:00:00\n+1\tHello\t['World']\t1970-01-01 00:00:00\n+-1\t\t['Goodbye']\t2025-01-01 02:03:04\n+1\tHello\t['World']\t1970-01-01 00:00:00\n+a\tUInt8\t\t\t\t\t\n+b\tString\t\t\t\t\t\n+c\tArray(String)\t\t\t\t\t\n+1\tHello\t['World']\n+1\tHello\t['World']\n+255\t\t['Goodbye']\n+255\t\t['Goodbye']\ndiff --git a/tests/queries/0_stateless/03302_merge_table_structure_unification.sql b/tests/queries/0_stateless/03302_merge_table_structure_unification.sql\nnew file mode 100644\nindex 000000000000..b6548b21527c\n--- /dev/null\n+++ b/tests/queries/0_stateless/03302_merge_table_structure_unification.sql\n@@ -0,0 +1,49 @@\n+SET enable_analyzer = 1;\n+\n+DROP TABLE IF EXISTS test_a;\n+DROP TABLE IF EXISTS test_b;\n+DROP TABLE IF EXISTS test_merge;\n+\n+CREATE TABLE test_a\n+(\n+    a UInt8,\n+    b String,\n+    c Array(String)\n+) ENGINE = Memory;\n+\n+CREATE TABLE test_b\n+(\n+    a Int32,\n+    c Array(Nullable(String)),\n+    d DateTime('UTC') DEFAULT now(),\n+) ENGINE = Memory;\n+\n+INSERT INTO test_a VALUES (1, 'Hello', ['World']);\n+INSERT INTO test_b VALUES (-1, ['Goodbye'], '2025-01-01 02:03:04');\n+\n+CREATE TABLE test_merge ENGINE = Merge(currentDatabase(), '^test_');\n+\n+-- TODO: defaults are not calculated\n+SELECT * FROM test_merge ORDER BY a;\n+\n+DESCRIBE merge('^test_');\n+\n+-- Note that this will also pick up the test_merge table, duplicating the results\n+SELECT * FROM merge('^test_') ORDER BY a;\n+\n+DROP TABLE test_merge;\n+\n+SET merge_table_max_tables_to_look_for_schema_inference = 1;\n+\n+CREATE TABLE test_merge ENGINE = Merge(currentDatabase(), '^test_');\n+\n+SELECT * FROM test_merge ORDER BY a;\n+\n+DESCRIBE merge('^test_');\n+\n+SELECT * FROM merge('^test_') ORDER BY a;\n+\n+DROP TABLE test_merge;\n+\n+DROP TABLE test_a;\n+DROP TABLE test_b;\n",
  "problem_statement": "Using Merge table engine and FINAL, columns specified in PREWHERE clause and not specified in SELECT section produces error\n**Describe the unexpected behavior**\r\nWhen using MaterializedMySQL database engine and Merge table engine, columns specified in `WHERE` clause and not specified in `SELECT` section produces error:\r\n```\r\nCode: 10. DB::Exception: Received from localhost:9000. DB::Exception: Not found column column2 in block. (NOT_FOUND_COLUMN_IN_BLOCK)\r\n```\r\n\r\n**How to reproduce**\r\n\r\nversions:\r\nMySQL: Ver 8.0.28 for Linux on x86_64 (MySQL Community Server - GPL)\r\nClickHouse: ClickHouse server version 22.2.2.1\r\n\r\nIn MySQL:\r\n```\r\ncreate database db4;\r\n\r\nCREATE TABLE db4.table_1 (\r\n    id INT,\r\n    column1 VARCHAR(10),\r\n    column2 VARCHAR(10),\r\n\tPRIMARY KEY (`id`),\r\n\tKEY `table_1_column2_index` (`column2`)\r\n) ENGINE = InnoDB;\r\n\r\ninsert into db4.table_1 \r\n(id, column1,column2) \r\nVALUES \r\n(1, 'abc', 'def');\r\n\r\n\r\nCREATE TABLE db4.table_2 (\r\n    id INT,\r\n    column1 VARCHAR(10),\r\n    column2 VARCHAR(10),\r\n\tPRIMARY KEY (`id`),\r\n\tKEY `table_2_column2_index` (`column2`)\r\n) ENGINE = InnoDB;\r\n\r\ninsert into db4.table_2 \r\n(id, column1, column2) \r\nVALUES \r\n(2, 'uvw','xyz');\r\n```\r\n\r\nIn ClickHouse:\r\n```\r\nset allow_experimental_database_materialized_mysql = 1;\r\nCREATE DATABASE db4_mysql ENGINE = MaterializedMySQL('192.168.1.248:3306', 'db4', 'clickhouse_user', 'ClickHouse_123');\r\n\r\ncreate database db5_merge;\r\n\r\nCREATE TABLE db5_merge.merge_table\r\n as db4_mysql.table_1\r\n     ENGINE = Merge('db4_mysql', '^table_\\\\d+$');\r\n```\r\n\r\n**Expected behavior**\r\nquery should be able to be executed with a different column specified than what is in `WHERE` clause:\r\n```\r\nch_env_2 :) select column1 from db5_merge.merge_table where column2 = 'def';\r\n\r\nSELECT column1\r\nFROM db5_merge.merge_table\r\nWHERE column2 = 'def'\r\n\r\nQuery id: 72c41db0-9006-4200-bbbb-d626aa1a9d9b\r\n\r\n\u250c\u2500column1\u2500\u2510\r\n\u2502 abc     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.038 sec.\r\n```\r\n\r\n**Error message and/or stacktrace**\r\n```\r\nch_env_2 :) select column1 from db5_merge.merge_table where column2 = 'def';\r\n\r\nSELECT column1\r\nFROM db5_merge.merge_table\r\nWHERE column2 = 'def'\r\n\r\nQuery id: f93bb8d4-8586-4b4f-8b6d-e1c2bc04b114\r\n\r\n\r\n0 rows in set. Elapsed: 0.011 sec.\r\n\r\nReceived exception from server (version 22.2.2):\r\nCode: 10. DB::Exception: Received from localhost:9000. DB::Exception: Not found column column2 in block. (NOT_FOUND_COLUMN_IN_BLOCK)\r\n```\r\n\r\n**Additional context**\r\nFound the current behavior and workarounds:\r\n1.  add the column that you would like to use in the where clause to the `SELECT` portion if the query.\r\n2. remove the index key from the column you'd like to use in the `WHERE` portion of the query.\r\n\r\nthanks.\nImprove usability of `Merge` tables\nCurrently, a `Merge` table takes the structure of the first (any) table.\r\n\r\nWhy don't take the union of columns of all tables and convert different types of identically named columns to the least supertype?\r\n\r\nAdditionally, we can allow inserts into Merge tables. They can be directed to the first table (if any), which data structure fits. A better option - among the tables with fitting data structures, insert to the table with the lexicographically greatest name (this allows usage of Merge tables for schema evolution, such as changing ORDER keys).\r\n\r\n**Motivation**\r\n\r\n```\r\nclickhouse-cloud :) SELECT sum(cityHash64(*)) FROM merge('^text_log')\r\n\r\nSELECT sum(cityHash64(*))\r\nFROM merge('^text_log')\r\n\r\nQuery id: a594ec22-fe6a-476f-aacf-78ad7fdc4999\r\n\r\n\r\nElapsed: 0.246 sec. \r\n\r\nReceived exception from server (version 24.5.1):\r\nCode: 10. DB::Exception: Received from kng4alm55c.us-east-2.aws.clickhouse-staging.com:9440. DB::Exception: Column `value1` not found in table default.text_log_10290440206184588366 (68320d6d-b8bc-4883-8fa3-9114c2a9ccdf). (NOT_FOUND_COLUMN_IN_BLOCK)\r\n```\n",
  "hints_text": "In fact, i think It has nothing to do with MaterializedMySQL, you can reproduce it only by Merge table engine:\r\n\r\n```\r\nCREATE TABLE db.table_1(`id` Int32,`column1` Nullable(String),  `column2` Nullable(String)) ENGINE = MergeTree ORDER BY (id, assumeNotNull(column2));\r\n\r\nCREATE TABLE db.table_2(`id` Int32,`column1` Nullable(String),  `column2` Nullable(String)) ENGINE = MergeTree ORDER BY (id, assumeNotNull(column2));\r\n\r\n\r\ninsert into db.table_1 (id, column1, column2)  VALUES  (1, 'abc', 'def');\r\ninsert into db.table_2 (id, column1, column2)  VALUES (2, 'uvw','xyz');\r\n\r\nCREATE TABLE db.merge_table as db.table_1 ENGINE = Merge('db', '^table_\\\\d+$');\r\n\r\nset optimize_move_to_prewhere = 0; \r\nselect column1 from db.merge_table final prewhere column2 = 'def';\r\n\r\n```\r\nMaterializedMySQL query comes with final semantics, and column2(index) is optimized for prewher, this why this problem appear in MaterializedMySQL case.\r\n\r\nThis problem should be related to optimization of prewhere by Merge table engine.\r\n\r\nI'll try to fix it(There's no guarantee it'll work out).\n@zzsmdfj Any chances?\n> In fact, i think It has nothing to do with MaterializedMySQL, you can reproduce it only by Merge table engine:\r\n> \r\n> ```\r\n> CREATE TABLE db.table_1(`id` Int32,`column1` Nullable(String),  `column2` Nullable(String)) ENGINE = MergeTree ORDER BY (id, assumeNotNull(column2));\r\n> \r\n> CREATE TABLE db.table_2(`id` Int32,`column1` Nullable(String),  `column2` Nullable(String)) ENGINE = MergeTree ORDER BY (id, assumeNotNull(column2));\r\n> \r\n> \r\n> insert into db.table_1 (id, column1, column2)  VALUES  (1, 'abc', 'def');\r\n> insert into db.table_2 (id, column1, column2)  VALUES (2, 'uvw','xyz');\r\n> \r\n> CREATE TABLE db.merge_table as db.table_1 ENGINE = Merge('db', '^table_\\\\d+$');\r\n> \r\n> set optimize_move_to_prewhere = 0; \r\n> select column1 from db.merge_table final prewhere column2 = 'def';\r\n> ```\r\n> \r\n> MaterializedMySQL query comes with final semantics, and column2(index) is optimized for prewher, this why this problem appear in MaterializedMySQL case.\r\n> \r\n> This problem should be related to optimization of prewhere by Merge table engine.\r\n> \r\n> I'll try to fix it(There's no guarantee it'll work out).\r\n\r\nThis example is actually invalid, because ordinary merge tree doesn't support FINAL. ~~The following works well:~~ Valid example can be:\r\n\r\n```sql\r\nCREATE TABLE db.replace_table_1(`id` Int32,`column1` Nullable(String),  `column2` Nullable(String)) ENGINE = ReplacingMergeTree ORDER BY (id, assumeNotNull(column2));\r\n\r\nCREATE TABLE db.replace_table_2(`id` Int32,`column1` Nullable(String),  `column2` Nullable(String)) ENGINE = ReplacingMergeTree ORDER BY (id, assumeNotNull(column2));\r\n\r\nINSERT INTO db.replace_table_1 (id, column1, column2)  VALUES  (1, 'abc', 'def');\r\nINSERT INTO db.replace_table_2 (id, column1, column2)  VALUES (2, 'uvw','xyz');\r\n\r\nCREATE TABLE db.replace_merge_table as db.replace_table_1 ENGINE = Merge('db', '^replace_table_\\\\d+$');\r\nSELECT column1 FROM db.replace_merge_table final prewhere column2 = 'def';\r\n```\r\n~~So the issue is not with PREWHERE, but rather the merge engine itself.~~\r\nSo there're 2 issues with merge engine here:\r\n- If any nested table doesn't support FINAL, it should throw exception instead of allowing to read from the nested table with FINAL\n- PREWHERE issue as described above\r\n\r\nI found the related logic, can try to fix it @alexey-milovidov \nFound the same issue\r\n\r\nhttps://fiddle.clickhouse.com/36f8b467-0c7a-4e3b-8e6b-f249b46eadfc\r\n```\r\nCREATE TABLE tdata0 (k Int64, dt DateTime, s String) \r\nENGINE=ReplacingMergeTree  \r\norder by (k,dt);\r\n\r\ncreate table tmerge as tdata0 Engine=Merge('default', 'tdata');\r\n\r\nINSERT INTO tdata0 \r\nselect number, '2020-01-01 00:00:00', '' from numbers(10);\r\n\r\nselect count() from tmerge final prewhere dt >= '2020-01-01 00:00:00';\r\n\r\nDB::Exception: Not found column dt:  in block k Int64 Int64(size = 0). \r\n(NOT_FOUND_COLUMN_IN_BLOCK)\r\n```\r\n\r\n\r\n\n",
  "created_at": "2024-12-29T19:26:52Z"
}