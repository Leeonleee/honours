{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 76621,
  "instance_id": "ClickHouse__ClickHouse-76621",
  "issue_numbers": [
    "40238",
    "41216"
  ],
  "base_commit": "dae433b0174c5906b794e8e2be7c719ab628a8c3",
  "patch": "diff --git a/contrib/librdkafka b/contrib/librdkafka\nindex 39d4ed49ccf3..1d2042c9cd39 160000\n--- a/contrib/librdkafka\n+++ b/contrib/librdkafka\n@@ -1,1 +1,1 @@\n-Subproject commit 39d4ed49ccf3406e2bf825d5d7b0903b5a290782\n+Subproject commit 1d2042c9cd39f00fcacf71b2cdc31ea23075d66f\ndiff --git a/contrib/librdkafka-cmake/CMakeLists.txt b/contrib/librdkafka-cmake/CMakeLists.txt\nindex d84abd06dec0..e2ae6304839a 100644\n--- a/contrib/librdkafka-cmake/CMakeLists.txt\n+++ b/contrib/librdkafka-cmake/CMakeLists.txt\n@@ -8,22 +8,24 @@ endif()\n set(RDKAFKA_SOURCE_DIR \"${ClickHouse_SOURCE_DIR}/contrib/librdkafka/src\")\n \n set(SRCS\n+#  \"${RDKAFKA_SOURCE_DIR}/cJSON.c\"    # see below\n   \"${RDKAFKA_SOURCE_DIR}/crc32c.c\"\n-#  \"${RDKAFKA_SOURCE_DIR}/lz4.c\"\n-#  \"${RDKAFKA_SOURCE_DIR}/lz4frame.c\"\n-#  \"${RDKAFKA_SOURCE_DIR}/lz4hc.c\"\n+#  \"${RDKAFKA_SOURCE_DIR}/lz4.c\"      # WITH_LZ4_EXT\n+#  \"${RDKAFKA_SOURCE_DIR}/lz4frame.c\" # WITH_LZ4_EXT\n+#  \"${RDKAFKA_SOURCE_DIR}/lz4hc.c\"    # WITH_LZ4_EXT\n   \"${RDKAFKA_SOURCE_DIR}/rdaddr.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdavl.c\"\n+  \"${RDKAFKA_SOURCE_DIR}/rdbase64.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdbuf.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdcrc32.c\"\n-  \"${RDKAFKA_SOURCE_DIR}/rddl.c\"\n+  #\"${RDKAFKA_SOURCE_DIR}/rddl.c\" #  WITH_LIBDL OR WIN32 - we don't want to support dynamic loading\n   \"${RDKAFKA_SOURCE_DIR}/rdfnv1a.c\"\n-  \"${RDKAFKA_SOURCE_DIR}/rdgz.c\"\n-  \"${RDKAFKA_SOURCE_DIR}/rdhdrhistogram.c\"\n-  \"${RDKAFKA_SOURCE_DIR}/rdkafka_admin.c\" # looks optional\n+  \"${RDKAFKA_SOURCE_DIR}/rdgz.c\" # WITH_ZLIB\n+  #\"${RDKAFKA_SOURCE_DIR}/rdhdrhistogram.c\" # WITH_HDRHISTOGRAM - allows to collect some better stats (not used so far)\n+  \"${RDKAFKA_SOURCE_DIR}/rdkafka_admin.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_assignment.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_assignor.c\"\n-  \"${RDKAFKA_SOURCE_DIR}/rdkafka_aux.c\" # looks optional\n+  \"${RDKAFKA_SOURCE_DIR}/rdkafka_aux.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_background.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_broker.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_buf.c\"\n@@ -35,6 +37,8 @@ set(SRCS\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_error.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_event.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_feature.c\"\n+  \"${RDKAFKA_SOURCE_DIR}/rdkafka_fetcher.c\"\n+  # \"${RDKAFKA_SOURCE_DIR}/rdhttp.c\" #  see WITH_CURL below\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_header.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_idempotence.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_interceptor.c\"\n@@ -51,25 +55,35 @@ set(SRCS\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_op.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_partition.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_pattern.c\"\n-  \"${RDKAFKA_SOURCE_DIR}/rdkafka_plugin.c\"\n+#  \"${RDKAFKA_SOURCE_DIR}/rdkafka_plugin.c\" # WITH_PLUGINS, we don't support plugins (dynamic loading)\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_queue.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_range_assignor.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_request.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_roundrobin_assignor.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_sasl.c\"\n-#  \"${RDKAFKA_SOURCE_DIR}/rdkafka_sasl_cyrus.c\"        # optionally included below\n-#  \"${RDKAFKA_SOURCE_DIR}/rdkafka_sasl_oauthbearer.c\"  # optionally included below\n+#  \"${RDKAFKA_SOURCE_DIR}/rdkafka_sasl_cyrus.c\"            # WITH_SASL_CYRUS, see below\n+#  \"${RDKAFKA_SOURCE_DIR}/rdkafka_sasl_oauthbearer.c\"      # WITH_SASL_OAUTHBEARER, see below\n+#  \"${RDKAFKA_SOURCE_DIR}/rdkafka_sasl_oauthbearer_oidc.c\" # WITH_OAUTHBEARER_OIDC, see below\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_sasl_plain.c\"\n-#  \"${RDKAFKA_SOURCE_DIR}/rdkafka_sasl_scram.c\"        # optionally included below\n-#  \"${RDKAFKA_SOURCE_DIR}/rdkafka_sasl_win32.c\"\n-#  \"${RDKAFKA_SOURCE_DIR}/rdkafka_ssl.c\"               # optionally included below\n+#  \"${RDKAFKA_SOURCE_DIR}/rdkafka_sasl_scram.c\"     # WITH_SASL_SCRAM, see below\n+#  \"${RDKAFKA_SOURCE_DIR}/rdkafka_sasl_win32.c\"     # WIN32\n+#  \"${RDKAFKA_SOURCE_DIR}/rdkafka_ssl.c\"            # WITH_SSL, see below\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_sticky_assignor.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_subscription.c\"\n+  \"${RDKAFKA_SOURCE_DIR}/rdkafka_telemetry.c\"\n+  \"${RDKAFKA_SOURCE_DIR}/rdkafka_telemetry_decode.c\"\n+  \"${RDKAFKA_SOURCE_DIR}/rdkafka_telemetry_encode.c\"\n+  \"${RDKAFKA_SOURCE_DIR}/nanopb/pb_encode.c\"\n+  \"${RDKAFKA_SOURCE_DIR}/nanopb/pb_decode.c\"\n+  \"${RDKAFKA_SOURCE_DIR}/nanopb/pb_common.c\"\n+  \"${RDKAFKA_SOURCE_DIR}/opentelemetry/metrics.pb.c\"\n+  \"${RDKAFKA_SOURCE_DIR}/opentelemetry/common.pb.c\"\n+  \"${RDKAFKA_SOURCE_DIR}/opentelemetry/resource.pb.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_timer.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_topic.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_transport.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdkafka_txnmgr.c\"\n-  \"${RDKAFKA_SOURCE_DIR}/rdkafka_zstd.c\"\n+  \"${RDKAFKA_SOURCE_DIR}/rdkafka_zstd.c\" # WITH_ZSTD\n   \"${RDKAFKA_SOURCE_DIR}/rdlist.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdlog.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdmap.c\"\n@@ -81,12 +95,17 @@ set(SRCS\n   \"${RDKAFKA_SOURCE_DIR}/rdunittest.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdvarint.c\"\n   \"${RDKAFKA_SOURCE_DIR}/rdxxhash.c\"\n-  # \"${RDKAFKA_SOURCE_DIR}/regexp.c\"\n+  # \"${RDKAFKA_SOURCE_DIR}/regexp.c\" # NOT HAVE_REGEX\n   \"${RDKAFKA_SOURCE_DIR}/snappy.c\"\n   \"${RDKAFKA_SOURCE_DIR}/tinycthread.c\"\n   \"${RDKAFKA_SOURCE_DIR}/tinycthread_extra.c\"\n )\n \n+if(TARGET ch_contrib::curl)\n+    message (STATUS \"librdkafka with curl\")\n+    set(WITH_CURL 1)\n+endif()\n+\n if(TARGET ch_contrib::sasl2)\n     message (STATUS \"librdkafka with SASL support\")\n     set(WITH_SASL_CYRUS 1)\n@@ -100,6 +119,12 @@ if(WITH_SASL_CYRUS)\n endif()\n list(APPEND SRCS \"${RDKAFKA_SOURCE_DIR}/rdkafka_ssl.c\")\n \n+if(WITH_SSL AND WITH_CURL)\n+  set(WITH_OAUTHBEARER_OIDC 1)\n+  list(APPEND SRCS \"${RDKAFKA_SOURCE_DIR}/rdhttp.c\") # needed for WITH_OAUTHBEARER_OIDC\n+  list(APPEND SRCS \"${RDKAFKA_SOURCE_DIR}/rdkafka_sasl_oauthbearer_oidc.c\")\n+endif()\n+\n if(WITH_SASL_CYRUS)\n   list(APPEND SRCS \"${RDKAFKA_SOURCE_DIR}/rdkafka_sasl_cyrus.c\") # needed to support Kerberos, requires cyrus-sasl\n endif()\n@@ -112,10 +137,19 @@ if(WITH_SASL_OAUTHBEARER)\n   list(APPEND SRCS \"${RDKAFKA_SOURCE_DIR}/rdkafka_sasl_oauthbearer.c\")\n endif()\n \n+## there is a conflict with the cJSON library from aws-s3\n+# so normally we link cJSON lirary from contrib/aws (which have some extra patches to be thread safe)\n+# but if we don't use aws-s3, we need to link the cJSON library from librdkafka (hacky)\n+# which is (potentially) not thread safe (see https://github.com/confluentinc/librdkafka/issues/4159 ),\n+if(NOT ENABLE_AWS_S3)\n+   list(APPEND SRCS \"${RDKAFKA_SOURCE_DIR}/cJSON.c\")\n+endif()\n+\n add_library(_rdkafka ${SRCS})\n add_library(ch_contrib::rdkafka ALIAS _rdkafka)\n-\n target_compile_options(_rdkafka PRIVATE -fno-sanitize=undefined)\n+target_compile_definitions(_rdkafka PRIVATE -DCJSON_HIDE_SYMBOLS)\n+\n # target_include_directories(_rdkafka SYSTEM PUBLIC include)\n target_include_directories(_rdkafka SYSTEM PUBLIC \"${CMAKE_CURRENT_SOURCE_DIR}/include\") # for \"librdkafka/rdkafka.h\"\n target_include_directories(_rdkafka SYSTEM PUBLIC ${RDKAFKA_SOURCE_DIR})                 # Because weird logic with \"include_next\" is used.\n@@ -131,6 +165,10 @@ if(WITH_SASL_CYRUS)\n     target_link_libraries(_rdkafka PRIVATE ch_contrib::sasl2)\n endif()\n \n+if(WITH_CURL)\n+  target_link_libraries(_rdkafka PRIVATE ch_contrib::curl)\n+endif()\n+\n file(MAKE_DIRECTORY \"${CMAKE_CURRENT_BINARY_DIR}/auxdir\")\n \n configure_file(\n@@ -138,3 +176,47 @@ configure_file(\n   \"${CMAKE_CURRENT_BINARY_DIR}/config.h\"\n   IMMEDIATE @ONLY\n )\n+\n+# -----------------------------------------------------------------------------\n+# Adjust to avoid the ClickHouse runtime trap for harmful functions:\n+#\n+# librdkafka uses the tinycthreads library for threading primitives.\n+# Note that tinycthreads is not actually using the C11 threads API,\n+# yet it defines and uses functions with the same names (e.g. mtx_lock,\n+# thrd_create, etc.). ClickHouse\u2019s runtime trap monitors for these names\n+# and will immediately terminate the program if they are present.\n+#\n+# To avoid triggering this runtime trap, we replace these symbols via compile\n+# definitions. That is, we redefine each harmful function to an alias prefixed\n+# with \"rd_kafka_\". This ensures that none of the original harmful symbol\n+# names appear in the final binary.\n+target_compile_definitions(_rdkafka PRIVATE\n+  thrd_create=rd_kafka_thrd_create\n+  thrd_equal=rd_kafka_thrd_equal\n+  thrd_current=rd_kafka_thrd_current\n+  thrd_sleep=rd_kafka_thrd_sleep\n+  thrd_yield=rd_kafka_thrd_yield\n+  thrd_exit=rd_kafka_thrd_exit\n+  thrd_detach=rd_kafka_thrd_detach\n+  thrd_join=rd_kafka_thrd_join\n+\n+  mtx_init=rd_kafka_mtx_init\n+  mtx_lock=rd_kafka_mtx_lock\n+  mtx_timedlock=rd_kafka_mtx_timedlock\n+  mtx_trylock=rd_kafka_mtx_trylock\n+  mtx_unlock=rd_kafka_mtx_unlock\n+  mtx_destroy=rd_kafka_mtx_destroy\n+  call_once=rd_kafka_call_once\n+\n+  cnd_init=rd_kafka_cnd_init\n+  cnd_signal=rd_kafka_cnd_signal\n+  cnd_broadcast=rd_kafka_cnd_broadcast\n+  cnd_wait=rd_kafka_cnd_wait\n+  cnd_timedwait=rd_kafka_cnd_timedwait\n+  cnd_destroy=rd_kafka_cnd_destroy\n+\n+  tss_create=rd_kafka_tss_create\n+  tss_get=rd_kafka_tss_get\n+  tss_set=rd_kafka_tss_set\n+  tss_delete=rd_kafka_tss_delete\n+)\ndiff --git a/contrib/librdkafka-cmake/config.h.in b/contrib/librdkafka-cmake/config.h.in\nindex f6ec3bc0e79c..707698586435 100644\n--- a/contrib/librdkafka-cmake/config.h.in\n+++ b/contrib/librdkafka-cmake/config.h.in\n@@ -1,33 +1,66 @@\n-// Originally generated by ./configure\n+// Automatically generated by ./configure\n+///\tNOTE: Version 2.8.0 was used to generate and manually modified after that. Therefore this should be fine\n+/// until we upgrade to something newer than that.\n+/// Commented out the followings:\n+/// - #define ARCH \"x86_64\": we build on multiple archs\n+/// - ENABLE_XXX: the commented out ones are only used in librdkafka's configure and CMake, but not in source\n+///   NOTE: ENABLE_DEVEL and ENABLE_REFCNT_DEBUG is used in the source at the time of writing\n+///\n+/// Commented out the following to disable them:\n+/// - ENABLE_C11THREADS: to maintain compatibility with old libc, maybe not necessary anymore\n+/// - WITH_GCC\n+/// - WITH_GXX\n+/// - WITH_INSTALL\n+/// - HAS_GNU_AR\n+/// - HAVE_PIC\n+/// - WITH_GNULD\n+/// - WITH_C11THREADS\n+/// - HAVE_PYTHON\n+///\n+/// Modified the follwoings:\n+/// - WITH_{CURL,SASL_CYRUS,SASL_OAUTHBEARER,SASL_SCRAM}: made them CMake dependant\n+/// - BUILT_WITH: tried to make some sense of it, don't spend to much time on it\n+///\n+/// Added:\n+/// - special handling of __APPLE__\n+\n #ifndef _CONFIG_H_\n #define _CONFIG_H_\n-#define BUILT_WITH \"GCC GXX PKGCONFIG OSXLD LIBDL PLUGINS ZLIB SSL SASL_CYRUS ZSTD HDRHISTOGRAM LZ4_EXT SNAPPY SOCKEM SASL_SCRAM CRC32C_HW\"\n+// BUILT_WITH\n+#define BUILT_WITH \"ZLIB ZSTD LZ4_EXT SNAPPY SSL SASL_CYRUS SASL_SCRAM SASL_OAUTHBEARER\"\n \n+// distro\n+#define SOLIB_EXT \".so\"\n+//#define ARCH \"x86_64\"\n #define CPU \"generic\"\n #define WITHOUT_OPTIMIZATION 0\n+#define WITH_STRIP 0\n #define ENABLE_DEVEL 0\n #define ENABLE_VALGRIND 0\n #define ENABLE_REFCNT_DEBUG 0\n-#define ENABLE_SHAREDPTR_DEBUG 0\n-#define ENABLE_LZ4_EXT 1\n-#define ENABLE_SSL 1\n-#define ENABLE_SASL 1\n+\n+// #define ENABLE_ZLIB 1\n+// #define ENABLE_ZSTD 1\n+// #define ENABLE_SSL 1\n+// #define ENABLE_GSSAPI 1\n+// #define ENABLE_LZ4_EXT 1\n+// #define ENABLE_REGEX_EXT 1\n+// #define ENABLE_C11THREADS \"try\"\n+// #define ENABLE_SYSLOG 1\n #define MKL_APP_NAME \"librdkafka\"\n #define MKL_APP_DESC_ONELINE \"The Apache Kafka C/C++ library\"\n-// distro\n-#define SOLIB_EXT \".so\"\n // gcc\n-//#define WITH_GCC 1\n+// #define WITH_GCC 1\n // gxx\n-//#define WITH_GXX 1\n-// pkgconfig\n-//#define WITH_PKGCONFIG 1\n+// #define WITH_GXX 1\n // install\n-//#define WITH_INSTALL 1\n+// #define WITH_INSTALL 1\n+// gnuar\n+// #define HAS_GNU_AR 1\n // PIC\n-//#define HAVE_PIC 1\n+// #define HAVE_PIC 1\n // gnulib\n-//#define WITH_GNULD 1\n+// #define WITH_GNULD 1\n // __atomic_32\n #define HAVE_ATOMICS_32 1\n // __atomic_32\n@@ -43,43 +76,53 @@\n // atomic_64\n #define ATOMIC_OP(OP1,OP2,PTR,VAL) __atomic_ ## OP1 ## _ ## OP2(PTR, VAL, __ATOMIC_SEQ_CST)\n // parseversion\n-#define RDKAFKA_VERSION_STR \"1.6.0\"\n+#define RDKAFKA_VERSION_STR \"2.8.0\"\n // parseversion\n-#define MKL_APP_VERSION \"1.6.0\"\n+#define MKL_APP_VERSION \"2.8.0\"\n+// disable C11 threads for compatibility with old libc, also C11 threads are condireder harmful\n+#define WITH_C11THREADS 0\n+// WITH_PLUGINS - see rd_kafka_plugin_new: it allow to dload some external plugins (no plans to support it in ClickHouse)\n+#define WITH_PLUGINS 0\n // libdl\n-#define WITH_LIBDL 1\n-// WITH_PLUGINS\n-#define WITH_PLUGINS 1\n+#define WITH_LIBDL 0\n // zlib\n #define WITH_ZLIB 1\n-// zstd\n+// libzstd\n #define WITH_ZSTD 1\n+// WITH_HDRHISTOGRAM - not used so far, can be useful for better stats\n+// #define WITH_HDRHISTOGRAM 1\n+// syslog\n+//#define WITH_SYSLOG 1\n // WITH_SNAPPY\n #define WITH_SNAPPY 1\n-// WITH_SOCKEM\n-#define WITH_SOCKEM 1\n+// WITH_SOCKEM - socket emulation (used for testing)\n+//#define WITH_SOCKEM 1\n // libssl\n #cmakedefine WITH_SSL 1\n // WITH_SASL_SCRAM\n #cmakedefine WITH_SASL_SCRAM 1\n // WITH_SASL_OAUTHBEARER\n #cmakedefine WITH_SASL_OAUTHBEARER 1\n+#cmakedefine WITH_OAUTHBEARER_OIDC 1\n+// libsasl2\n #cmakedefine WITH_SASL_CYRUS 1\n+// WITH_CURL\n+#cmakedefine WITH_CURL 1\n // crc32chw\n #if !defined(__PPC__) && !defined(__riscv) && !defined(__aarch64__) && !defined(__s390x__) && !defined(__loongarch64)\n #define WITH_CRC32C_HW 1\n #endif\n // regex\n #define HAVE_REGEX 1\n+// rand_r\n+#define HAVE_RAND_R 1\n // strndup\n #define HAVE_STRNDUP 1\n // strerror_r\n #define HAVE_STRERROR_R 1\n-// rand_r\n-#define HAVE_RAND_R 1\n-\n+// strcasestr\n+#define HAVE_STRCASESTR 1\n #ifdef __APPLE__\n-// pthread_setname_np\n #define HAVE_PTHREAD_SETNAME_DARWIN 1\n #if (__ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__ <= 101400)\n #define _TTHREAD_EMULATE_TIMESPEC_GET_\n@@ -90,8 +133,8 @@\n // pthread_setname_gnu\n #define HAVE_PTHREAD_SETNAME_GNU 1\n #endif\n-// python\n+// python3\n //#define HAVE_PYTHON 1\n-// disable C11 threads for compatibility with old libc\n-//#define WITH_C11THREADS 1\n+// getrusage - used in a single test tests/rusage.c only\n+//#define HAVE_GETRUSAGE 1\n #endif /* _CONFIG_H_ */\ndiff --git a/src/Storages/Kafka/KafkaConsumer.cpp b/src/Storages/Kafka/KafkaConsumer.cpp\nindex fbef36ddc26c..7b799ccec6f8 100644\n--- a/src/Storages/Kafka/KafkaConsumer.cpp\n+++ b/src/Storages/Kafka/KafkaConsumer.cpp\n@@ -151,19 +151,12 @@ void KafkaConsumer::createConsumer(cppkafka::Configuration consumer_config)\n \n ConsumerPtr && KafkaConsumer::moveConsumer()\n {\n+    // messages & assignment should be destroyed before consumer\n     cleanUnprocessed();\n-    if (!consumer->get_subscription().empty())\n-    {\n-        try\n-        {\n-            consumer->unsubscribe();\n-        }\n-        catch (const cppkafka::HandleException & e)\n-        {\n-            LOG_ERROR(log, \"Error during unsubscribe: {}\", e.what());\n-        }\n-        drain();\n-    }\n+    assignment.reset();\n+\n+    StorageKafkaUtils::consumerGracefulStop(*consumer, DRAIN_TIMEOUT_MS, log, [this](const cppkafka::Error & err) { setExceptionInfo(err); });\n+\n     return std::move(consumer);\n }\n \n@@ -173,36 +166,12 @@ KafkaConsumer::~KafkaConsumer()\n         return;\n \n     cleanUnprocessed();\n-    try\n-    {\n-        if (!consumer->get_subscription().empty())\n-        {\n-            try\n-            {\n-                consumer->unsubscribe();\n-            }\n-            catch (const cppkafka::HandleException & e)\n-            {\n-                LOG_ERROR(log, \"Error during unsubscribe: {}\", e.what());\n-            }\n-            drain();\n-        }\n-    }\n-    catch (const cppkafka::HandleException & e)\n-    {\n-        LOG_ERROR(log, \"Error while destructing consumer: {}\", e.what());\n-    }\n-}\n+    assignment.reset();\n \n-// Needed to drain rest of the messages / queued callback calls from the consumer\n-// after unsubscribe, otherwise consumer will hang on destruction\n-// see https://github.com/edenhill/librdkafka/issues/2077\n-//     https://github.com/confluentinc/confluent-kafka-go/issues/189 etc.\n-void KafkaConsumer::drain()\n-{\n-    StorageKafkaUtils::drainConsumer(*consumer, DRAIN_TIMEOUT_MS, log, [this](const cppkafka::Error & err) { setExceptionInfo(err); });\n+    StorageKafkaUtils::consumerGracefulStop(*consumer, DRAIN_TIMEOUT_MS, log, [this](const cppkafka::Error & err) { setExceptionInfo(err); });\n }\n \n+\n void KafkaConsumer::commit()\n {\n     auto print_offsets = [this] (const char * prefix, const cppkafka::TopicPartitionList & offsets)\n@@ -236,7 +205,7 @@ void KafkaConsumer::commit()\n \n     if (hasMorePolledMessages())\n     {\n-        LOG_WARNING(log, \"Logical error. Non all polled messages were processed.\");\n+        LOG_WARNING(log, \"Logical error. Not all polled messages were processed.\");\n     }\n \n     if (offsets_stored > 0)\n@@ -299,80 +268,105 @@ void KafkaConsumer::commit()\n \n void KafkaConsumer::subscribe()\n {\n-    LOG_TRACE(log, \"Already subscribed to topics: [{}]\", boost::algorithm::join(consumer->get_subscription(), \", \"));\n+    cleanUnprocessed();\n \n-    if (assignment.has_value())\n-    {\n-        LOG_TRACE(log, \"Already assigned to: {}\", assignment.value());\n-    }\n-    else\n+    // we can reset any flags (except of CONSUMER_STOPPED) before attempt of reading new block of data\n+    if (stalled_status != CONSUMER_STOPPED)\n+        stalled_status = NO_MESSAGES_RETURNED;\n+\n+    auto subscription = consumer->get_subscription();\n+\n+    if (!subscription.empty())\n     {\n-        LOG_TRACE(log, \"No assignment\");\n-    }\n+        LOG_TRACE(log, \"Already subscribed to topics: [{}]\", boost::algorithm::join(subscription, \", \"));\n+\n+        if (assignment.has_value())\n+            LOG_TRACE(log, \"Already assigned to: {}\", assignment.value());\n+        else\n+            LOG_TRACE(log, \"No assignment\");\n \n+        if (current_subscription_valid)\n+            return;\n+    }\n \n     size_t max_retries = 5;\n \n-    while (consumer->get_subscription().empty())\n+    while (true)\n     {\n         --max_retries;\n+\n+        if (stopped)\n+        {\n+            LOG_TRACE(log, \"Consumer is stopped; cannot subscribe.\");\n+            return;\n+        }\n+\n+        LOG_TRACE(log, \"Subscribing to topics: [{}]\", boost::algorithm::join(topics, \", \"));\n+\n         try\n         {\n             consumer->subscribe(topics);\n-            // FIXME: if we failed to receive \"subscribe\" response while polling and destroy consumer now, then we may hang up.\n-            //        see https://github.com/edenhill/librdkafka/issues/2077\n         }\n-        catch (cppkafka::HandleException & e)\n+        catch (const cppkafka::HandleException & e)\n         {\n+            LOG_ERROR(log, \"Exception during subscribe: {}\", e.what());\n+\n             if (max_retries > 0 && e.get_error() == RD_KAFKA_RESP_ERR__TIMED_OUT)\n                 continue;\n+\n+            setExceptionInfo(e.what());\n             throw;\n         }\n+\n+        subscription = consumer->get_subscription();\n+\n+        if (subscription.empty())\n+        {\n+            if (max_retries > 0)\n+            {\n+                LOG_WARNING(log, \"Subscription is empty. Will try to resubscribe.\");\n+                continue;\n+            }\n+            else\n+            {\n+                throw Exception(ErrorCodes::CANNOT_COMMIT_OFFSET, \"Can not get subscription.\");\n+            }\n+        }\n+        else\n+        {\n+            LOG_TRACE(log, \"Subscribed to topics: [{}]\", boost::algorithm::join(subscription, \", \"));\n+            break;\n+        }\n     }\n \n-    cleanUnprocessed();\n+    current_subscription_valid = true;\n \n-    // we can reset any flags (except of CONSUMER_STOPPED) before attempt of reading new block of data\n-    if (stalled_status != CONSUMER_STOPPED)\n-        stalled_status = NO_MESSAGES_RETURNED;\n+    // Immediately poll for messages (+callbacks) after successful subscription.\n+    doPoll();\n }\n \n void KafkaConsumer::cleanUnprocessed()\n {\n     messages.clear();\n-    current = messages.begin();\n+    current = messages.end();\n     offsets_stored = 0;\n }\n \n-void KafkaConsumer::unsubscribe()\n+void KafkaConsumer::markDirty()\n {\n-    LOG_TRACE(log, \"Re-joining claimed consumer after failure\");\n-    cleanUnprocessed();\n-\n-    // it should not raise exception as used in destructor\n-    try\n-    {\n-        // From docs: Any previous subscription will be unassigned and unsubscribed first.\n-        consumer->subscribe(topics);\n+    LOG_TRACE(log, \"Marking consumer as dirty after failure, so it will rejoin consumer group on the next usage.\");\n \n-        // I wanted to avoid explicit unsubscribe as it requires draining the messages\n-        // to close the consumer safely after unsubscribe\n-        // see https://github.com/edenhill/librdkafka/issues/2077\n-        //     https://github.com/confluentinc/confluent-kafka-go/issues/189 etc.\n-    }\n-    catch (const cppkafka::HandleException & e)\n-    {\n-        LOG_ERROR(log, \"Exception from KafkaConsumer::unsubscribe: {}\", e.what());\n-    }\n+    cleanUnprocessed();\n \n+    // Next subscribe call will redo subscription, causing a rebalance/offset reset and potential duplicates.\n+    current_subscription_valid = false;\n }\n \n-\n void KafkaConsumer::resetToLastCommitted(const char * msg)\n {\n     if (!assignment.has_value() || assignment->empty())\n     {\n-        LOG_TRACE(log, \"Not assigned. Can't reset to last committed position.\");\n+        LOG_TRACE(log, \"Not assigned; cannot reset to last committed position.\");\n         return;\n     }\n     auto committed_offset = consumer->get_offsets_committed(consumer->get_assignment());\n@@ -380,19 +374,10 @@ void KafkaConsumer::resetToLastCommitted(const char * msg)\n     LOG_TRACE(log, \"{} Returned to committed position: {}\", msg, committed_offset);\n }\n \n-// it do the poll when needed\n-ReadBufferPtr KafkaConsumer::consume()\n-{\n-    resetIfStopped();\n-\n-    if (polledDataUnusable())\n-        return nullptr;\n-\n-    if (hasMorePolledMessages())\n-        return getNextMessage();\n \n-    if (intermediate_commit)\n-        commit();\n+void KafkaConsumer::doPoll()\n+{\n+    assert(current == messages.end());\n \n     while (true)\n     {\n@@ -411,12 +396,16 @@ ReadBufferPtr KafkaConsumer::consume()\n         auto new_messages = consumer->poll_batch(batch_size,\n                             std::chrono::milliseconds(actual_poll_timeout_ms));\n         last_poll_timestamp = timeInSeconds(std::chrono::system_clock::now());\n+\n+        // Remove messages with errors and log any exceptions.\n+        auto num_errors = StorageKafkaUtils::eraseMessageErrors(new_messages, log, [this](const cppkafka::Error & err) { setExceptionInfo(err); });\n         num_messages_read += new_messages.size();\n \n         resetIfStopped();\n+\n         if (stalled_status == CONSUMER_STOPPED)\n         {\n-            return nullptr;\n+            return;\n         }\n         if (stalled_status == REBALANCE_HAPPENED)\n         {\n@@ -425,9 +414,9 @@ ReadBufferPtr KafkaConsumer::consume()\n                 // we have polled something just after rebalance.\n                 // we will not use current batch, so we need to return to last committed position\n                 // otherwise we will continue polling from that position\n-                resetToLastCommitted(\"Rewind last poll after rebalance.\");\n+                resetToLastCommitted(\"Rewinding last poll after rebalance.\");\n             }\n-            return nullptr;\n+            return;\n         }\n \n         if (new_messages.empty())\n@@ -436,7 +425,7 @@ ReadBufferPtr KafkaConsumer::consume()\n             // If we're doing a manual select then it's better to get something after a wait, then immediate nothing.\n             if (!assignment.has_value())\n             {\n-                waited_for_assignment += poll_timeout; // slightly inaccurate, but rough calculation is ok.\n+                waited_for_assignment += poll_timeout; // Rough calculation for total wait time.\n                 if (waited_for_assignment < MAX_TIME_TO_WAIT_FOR_ASSIGNMENT_MS)\n                 {\n                     continue;\n@@ -444,40 +433,61 @@ ReadBufferPtr KafkaConsumer::consume()\n \n                 LOG_WARNING(log, \"Can't get assignment. Will keep trying.\");\n                 stalled_status = NO_ASSIGNMENT;\n-                return nullptr;\n+                return;\n             }\n+\n             if (assignment->empty())\n             {\n                 LOG_TRACE(log, \"Empty assignment.\");\n-                return nullptr;\n+                return;\n             }\n \n-            LOG_TRACE(log, \"Stalled\");\n-            return nullptr;\n+            if (num_errors > 0)\n+            {\n+                LOG_WARNING(log, \"Only errors polled.\");\n+                stalled_status = ERRORS_RETURNED;\n+                return;\n+            }\n+\n+            LOG_TRACE(log, \"Stalled.\");\n+            return;\n         }\n \n         messages = std::move(new_messages);\n         current = messages.begin();\n+\n+        ProfileEvents::increment(ProfileEvents::KafkaMessagesPolled, messages.size());\n+\n         LOG_TRACE(\n             log,\n             \"Polled batch of {} messages. Offsets position: {}\",\n             messages.size(),\n             consumer->get_offsets_position(consumer->get_assignment()));\n+\n+        stalled_status = NOT_STALLED;\n+\n         break;\n     }\n+}\n \n-    filterMessageErrors();\n-    if (current == messages.end())\n-    {\n-        LOG_ERROR(log, \"Only errors left\");\n-        stalled_status = ERRORS_RETURNED;\n+/// Consumes a single message from the buffered polled batch\n+/// does the poll if needed\n+ReadBufferPtr KafkaConsumer::consume()\n+{\n+    resetIfStopped();\n+\n+    if (polledDataUnusable())\n         return nullptr;\n-    }\n \n-    ProfileEvents::increment(ProfileEvents::KafkaMessagesPolled, messages.size());\n+    if (hasMorePolledMessages())\n+        return getNextMessage();\n \n-    stalled_status = NOT_STALLED;\n-    return getNextMessage();\n+    if (intermediate_commit)\n+        commit();\n+\n+    doPoll();\n+\n+    return stalled_status == NOT_STALLED ? getNextMessage() : nullptr;\n }\n \n ReadBufferPtr KafkaConsumer::getNextMessage()\n@@ -495,13 +505,6 @@ ReadBufferPtr KafkaConsumer::getNextMessage()\n     return getNextMessage();\n }\n \n-void KafkaConsumer::filterMessageErrors()\n-{\n-    assert(current == messages.begin());\n-\n-    StorageKafkaUtils::eraseMessageErrors(messages, log, [this](const cppkafka::Error & err) { setExceptionInfo(err); });\n-    current = messages.begin();\n-}\n \n void KafkaConsumer::resetIfStopped()\n {\ndiff --git a/src/Storages/Kafka/KafkaConsumer.h b/src/Storages/Kafka/KafkaConsumer.h\nindex b7cd8bb78b1a..9e299f67c4a3 100644\n--- a/src/Storages/Kafka/KafkaConsumer.h\n+++ b/src/Storages/Kafka/KafkaConsumer.h\n@@ -80,7 +80,11 @@ class KafkaConsumer\n \n     void commit(); // Commit all processed messages.\n     void subscribe(); // Subscribe internal consumer to topics.\n-    void unsubscribe(); // Unsubscribe internal consumer in case of failure.\n+\n+    // used during exception processing to restart the consumption from last committed offset\n+    // Notes: duplicates can appear if the some data were already flushed\n+    // it causes rebalance (and is an expensive way of exception handling)\n+    void markDirty();\n \n     auto pollTimeout() const { return poll_timeout; }\n \n@@ -156,6 +160,7 @@ class KafkaConsumer\n     const size_t batch_size = 1;\n     const size_t poll_timeout = 0;\n     size_t offsets_stored = 0;\n+    bool current_subscription_valid = false;\n \n     StalledStatus stalled_status = NO_MESSAGES_RETURNED;\n \n@@ -189,10 +194,9 @@ class KafkaConsumer\n     /// Last used time (for TTL)\n     std::atomic<UInt64> last_used_usec = 0;\n \n-    void drain();\n+    void doPoll();\n     void cleanUnprocessed();\n     void resetIfStopped();\n-    void filterMessageErrors();\n     ReadBufferPtr getNextMessage();\n };\n \ndiff --git a/src/Storages/Kafka/KafkaConsumer2.cpp b/src/Storages/Kafka/KafkaConsumer2.cpp\nindex a70a73f9be5d..213b8e5f899f 100644\n--- a/src/Storages/Kafka/KafkaConsumer2.cpp\n+++ b/src/Storages/Kafka/KafkaConsumer2.cpp\n@@ -127,34 +127,7 @@ KafkaConsumer2::KafkaConsumer2(\n \n KafkaConsumer2::~KafkaConsumer2()\n {\n-    try\n-    {\n-        if (!consumer->get_subscription().empty())\n-        {\n-            try\n-            {\n-                consumer->unsubscribe();\n-            }\n-            catch (const cppkafka::HandleException & e)\n-            {\n-                LOG_ERROR(log, \"Error during unsubscribe: {}\", e.what());\n-            }\n-            drainConsumerQueue();\n-        }\n-    }\n-    catch (const cppkafka::HandleException & e)\n-    {\n-        LOG_ERROR(log, \"Error while destructing consumer: {}\", e.what());\n-    }\n-}\n-\n-// Needed to drain rest of the messages / queued callback calls from the consumer after unsubscribe, otherwise consumer\n-// will hang on destruction. Partition queues doesn't have to be attached as events are not handled by those queues.\n-// see https://github.com/edenhill/librdkafka/issues/2077\n-//     https://github.com/confluentinc/confluent-kafka-go/issues/189 etc.\n-void KafkaConsumer2::drainConsumerQueue()\n-{\n-    StorageKafkaUtils::drainConsumer(*consumer, DRAIN_TIMEOUT_MS, log);\n+    StorageKafkaUtils::consumerGracefulStop(*consumer, DRAIN_TIMEOUT_MS, log);\n }\n \n void KafkaConsumer2::pollEvents()\ndiff --git a/src/Storages/Kafka/KafkaConsumer2.h b/src/Storages/Kafka/KafkaConsumer2.h\nindex f928a39aeec0..6bba0bf89bbb 100644\n--- a/src/Storages/Kafka/KafkaConsumer2.h\n+++ b/src/Storages/Kafka/KafkaConsumer2.h\n@@ -145,7 +145,6 @@ class KafkaConsumer2\n     const Names topics;\n \n     bool polledDataUnusable(const TopicPartition & topic_partition) const;\n-    void drainConsumerQueue();\n     void resetIfStopped();\n     void filterMessageErrors();\n     ReadBufferPtr getNextMessage();\ndiff --git a/src/Storages/Kafka/KafkaSource.cpp b/src/Storages/Kafka/KafkaSource.cpp\nindex 2ce8bddd546f..ed8b2285ecf0 100644\n--- a/src/Storages/Kafka/KafkaSource.cpp\n+++ b/src/Storages/Kafka/KafkaSource.cpp\n@@ -64,7 +64,7 @@ KafkaSource::~KafkaSource()\n         return;\n \n     if (broken)\n-        consumer->unsubscribe();\n+        consumer->markDirty();\n \n     storage.pushConsumer(consumer);\n }\ndiff --git a/src/Storages/Kafka/StorageKafka.cpp b/src/Storages/Kafka/StorageKafka.cpp\nindex aa4a89f8df86..bb44e8dc9dc9 100644\n--- a/src/Storages/Kafka/StorageKafka.cpp\n+++ b/src/Storages/Kafka/StorageKafka.cpp\n@@ -287,6 +287,10 @@ void StorageKafka::startup()\n \n void StorageKafka::shutdown(bool)\n {\n+    // Interrupt streaming, inform consumers to stop\n+    for (auto & task : tasks)\n+        task->stream_cancelled = true;\n+\n     shutdown_called = true;\n     cleanup_cv.notify_one();\n \n@@ -306,9 +310,6 @@ void StorageKafka::shutdown(bool)\n         Stopwatch watch;\n         for (auto & task : tasks)\n         {\n-            // Interrupt streaming thread\n-            task->stream_cancelled = true;\n-\n             LOG_TEST(log, \"Waiting for cleanup of a task\");\n             task->holder->deactivate();\n         }\ndiff --git a/src/Storages/Kafka/StorageKafkaUtils.cpp b/src/Storages/Kafka/StorageKafkaUtils.cpp\nindex e3617cc1cf2e..42588fc43f51 100644\n--- a/src/Storages/Kafka/StorageKafkaUtils.cpp\n+++ b/src/Storages/Kafka/StorageKafkaUtils.cpp\n@@ -333,6 +333,75 @@ String getDefaultClientId(const StorageID & table_id)\n     return fmt::format(\"{}-{}-{}-{}\", VERSION_NAME, getFQDNOrHostName(), table_id.database_name, table_id.table_name);\n }\n \n+void consumerGracefulStop(\n+    cppkafka::Consumer & consumer, const std::chrono::milliseconds drain_timeout, const LoggerPtr & log, ErrorHandler error_handler)\n+{\n+    // Note: librdkafka is very sensitive to the proper termination sequence and have some race conditions there.\n+    // Before destruction, our objectives are:\n+    //   (1) Process all outstanding callbacks by polling the event queue.\n+    //   (2) Ensure that only special events (e.g. callbacks, rebalances) are polled (we don't want to poll regular messages).\n+    //\n+    // Previously, we performed an unsubscribe to stop message consumption and clear 'read' messages.\n+    // However, unsubscribe triggers a rebalance that schedules additional background tasks, such as locking\n+    // and removal of internal toppar queues. Meanwhile, polling to release callbacks may concurrently\n+    // cause those same queues to be destroyed.\n+    // This can lead to a situation where the background thread doing rebalance and the current thread doing polling access\n+    // the toppar queues simultaneously, potentially locking them in a different order, which risks a deadlock.\n+    //\n+    // To mitigate this, we now:\n+    //   (1) Avoid calling unsubscribe (letting rebalance occur naturally via consumer group timeout).\n+    //   (2) Set up different rebalance callbacks to repeat (3) if a rebalance will occur before consumer destruction.\n+    //   (3) Pause the consumer to stop processing new messages.\n+    //   (4) Disconnect the toppar queues to reduce the risk of lock inversion (less cascading locks).\n+    //   (5) Poll the event queue to process any remaining callbacks.\n+\n+    consumer.set_revocation_callback(\n+        [](const cppkafka::TopicPartitionList &)\n+        {\n+            // we don't care during the destruction\n+        });\n+\n+    consumer.set_assignment_callback(\n+        [&consumer](const cppkafka::TopicPartitionList & topic_partitions)\n+        {\n+            if (!topic_partitions.empty())\n+            {\n+                consumer.pause_partitions(topic_partitions);\n+            }\n+\n+            // it's not clear if get_partition_queue will work in that context\n+            // as just after processing the callback cppkafka will call run assign\n+            // and that can reset the queues\n+\n+        });\n+\n+    try\n+    {\n+        auto assignment = consumer.get_assignment();\n+\n+        if (!assignment.empty())\n+        {\n+            consumer.pause_partitions(assignment);\n+\n+            for (const auto& partition : assignment)\n+            {\n+                // that call disables the forwarding of the messages to the customer queue\n+                consumer.get_partition_queue(partition);\n+            }\n+        }\n+    }\n+    catch (const cppkafka::HandleException & e)\n+    {\n+        LOG_ERROR(log, \"Error during pause (consumerGracefulStop): {}\", e.what());\n+    }\n+\n+    drainConsumer(consumer, drain_timeout, log, std::move(error_handler));\n+}\n+\n+// Needed to drain rest of the messages / queued callback calls from the consumer after unsubscribe, otherwise consumer\n+// will hang on destruction. Partition queues doesn't have to be attached as events are not handled by those queues.\n+// see https://github.com/edenhill/librdkafka/issues/2077\n+//     https://github.com/confluentinc/confluent-kafka-go/issues/189 etc.\n void drainConsumer(\n     cppkafka::Consumer & consumer, const std::chrono::milliseconds drain_timeout, const LoggerPtr & log, ErrorHandler error_handler)\n {\n@@ -371,7 +440,7 @@ void drainConsumer(\n     }\n }\n \n-void eraseMessageErrors(Messages & messages, const LoggerPtr & log, ErrorHandler error_handler)\n+size_t eraseMessageErrors(Messages & messages, const LoggerPtr & log, ErrorHandler error_handler)\n {\n     size_t skipped = std::erase_if(\n         messages,\n@@ -389,6 +458,8 @@ void eraseMessageErrors(Messages & messages, const LoggerPtr & log, ErrorHandler\n \n     if (skipped)\n         LOG_ERROR(log, \"There were {} messages with an error\", skipped);\n+\n+    return skipped;\n }\n \n SettingsChanges createSettingsAdjustments(KafkaSettings & kafka_settings, const String & schema_name)\ndiff --git a/src/Storages/Kafka/StorageKafkaUtils.h b/src/Storages/Kafka/StorageKafkaUtils.h\nindex 5f681e94077e..c0ed83cb57d7 100644\n--- a/src/Storages/Kafka/StorageKafkaUtils.h\n+++ b/src/Storages/Kafka/StorageKafkaUtils.h\n@@ -34,6 +34,12 @@ String getDefaultClientId(const StorageID & table_id);\n \n using ErrorHandler = std::function<void(const cppkafka::Error &)>;\n \n+void consumerGracefulStop(\n+    cppkafka::Consumer & consumer,\n+    std::chrono::milliseconds drain_timeout,\n+    const LoggerPtr & log,\n+    ErrorHandler error_handler = [](const cppkafka::Error & /*err*/) {});\n+\n void drainConsumer(\n     cppkafka::Consumer & consumer,\n     std::chrono::milliseconds drain_timeout,\n@@ -41,7 +47,7 @@ void drainConsumer(\n     ErrorHandler error_handler = [](const cppkafka::Error & /*err*/) {});\n \n using Messages = std::vector<cppkafka::Message>;\n-void eraseMessageErrors(Messages & messages, const LoggerPtr & log, ErrorHandler error_handler = [](const cppkafka::Error & /*err*/) {});\n+size_t eraseMessageErrors(Messages & messages, const LoggerPtr & log, ErrorHandler error_handler = [](const cppkafka::Error & /*err*/) {});\n \n SettingsChanges createSettingsAdjustments(KafkaSettings & kafka_settings, const String & schema_name);\n \n",
  "test_patch": "diff --git a/tests/integration/test_storage_kafka/configs/kafka.xml b/tests/integration/test_storage_kafka/configs/kafka.xml\nindex a846fdbb2951..0e6132803439 100644\n--- a/tests/integration/test_storage_kafka/configs/kafka.xml\n+++ b/tests/integration/test_storage_kafka/configs/kafka.xml\n@@ -1,4 +1,5 @@\n <clickhouse>\n+    <background_message_broker_schedule_pool_size>128</background_message_broker_schedule_pool_size>\n     <kafka>\n         <!-- Debugging of possible issues, like:\n              - https://github.com/edenhill/librdkafka/issues/2077\n@@ -24,6 +25,12 @@\n             <heartbeat_interval_ms>302</heartbeat_interval_ms>\n         </kafka_separate_settings>\n         <consumer>\n+            <!-- In librdkafka 1.7.0 the default value of session.timeout.ms was changed -->\n+            <!-- from 10s to 45s. Let's keep the old behavior for the tests as some of -->\n+            <!-- are depending on this timing. It shouldn't cause any issues in normal -->\n+            <!-- use, only consumers should be more robust, see KIP-735. -->\n+            <session_timeout_ms>10000</session_timeout_ms>\n+\n             <auto_offset_reset>earliest</auto_offset_reset>\n             <kafka_topic>\n                 <!-- Setting for topic will be applied only for consumer -->\ndiff --git a/tests/integration/test_storage_kafka/test_batch_slow.py b/tests/integration/test_storage_kafka/test_batch_slow.py\nindex 0c8d6fd57f9e..7df3ea66193b 100644\n--- a/tests/integration/test_storage_kafka/test_batch_slow.py\n+++ b/tests/integration/test_storage_kafka/test_batch_slow.py\n@@ -986,9 +986,9 @@ def test_formats_errors(kafka_cluster):\n             instance.query(\"DROP TABLE test.view\")\n \n \n-def test_kafka_duplicates_when_commit_failed(kafka_cluster):\n+def test_kafka_handling_commit_failure(kafka_cluster):\n     messages = [json.dumps({\"key\": j + 1, \"value\": \"x\" * 300}) for j in range(22)]\n-    k.kafka_produce(kafka_cluster, \"duplicates_when_commit_failed\", messages)\n+    k.kafka_produce(kafka_cluster, \"handling_commit_failure\", messages)\n \n     instance.query(\n         \"\"\"\n@@ -998,8 +998,8 @@ def test_kafka_duplicates_when_commit_failed(kafka_cluster):\n         CREATE TABLE test.kafka (key UInt64, value String)\n             ENGINE = Kafka\n             SETTINGS kafka_broker_list = 'kafka1:19092',\n-                     kafka_topic_list = 'duplicates_when_commit_failed',\n-                     kafka_group_name = 'duplicates_when_commit_failed',\n+                     kafka_topic_list = 'handling_commit_failure',\n+                     kafka_group_name = 'handling_commit_failure',\n                      kafka_format = 'JSONEachRow',\n                      kafka_max_block_size = 20,\n                      kafka_flush_interval_ms = 1000;\n@@ -1024,21 +1024,17 @@ def test_kafka_duplicates_when_commit_failed(kafka_cluster):\n     # while materialized view is working to inject zookeeper failure\n \n     with kafka_cluster.pause_container(\"kafka1\"):\n-        # if we restore the connection too fast (<30sec) librdkafka will not report any timeout\n-        # (alternative is to decrease the default session timeouts for librdkafka)\n-        #\n-        # when the delay is too long (>50sec) broker will decide to remove us from the consumer group,\n-        # and will start answering \"Broker: Unknown member\"\n         instance.wait_for_log_line(\n-            \"Exception during commit attempt: Local: Waiting for coordinator\", timeout=45\n+            \"timeout\", timeout=60, look_behind_lines=100\n         )\n-        instance.wait_for_log_line(\"All commit attempts failed\", look_behind_lines=500)\n \n     # kafka_cluster.open_bash_shell('instance')\n     instance.wait_for_log_line(\"Committed offset 22\")\n \n-    result = instance.query(\"SELECT count(), uniqExact(key), max(key) FROM test.view\")\n-    logging.debug(result)\n+    uniq_and_max = instance.query(\"SELECT uniqExact(key), max(key) FROM test.view\")\n+    count = instance.query(\"SELECT count() FROM test.view\")\n+    logging.debug(uniq_and_max)\n+    logging.debug(count)\n \n     instance.query(\n         \"\"\"\n@@ -1050,7 +1046,10 @@ def test_kafka_duplicates_when_commit_failed(kafka_cluster):\n     # After https://github.com/edenhill/librdkafka/issues/2631\n     # timeout triggers rebalance, making further commits to the topic after getting back online\n     # impossible. So we have a duplicate in that scenario, but we report that situation properly.\n-    assert TSV(result) == TSV(\"42\\t22\\t22\")\n+    # It is okay to have duplicates in case of commit failure, the important thing to test is we\n+    # each message at least once.\n+    assert TSV(uniq_and_max) == TSV(\"22\\t22\")\n+    assert int(count) >= 22\n \n \n @pytest.mark.parametrize(\n",
  "problem_statement": "Fatal error when creating a table with a Kafka table engine\n**Describe what's wrong**\r\n\r\nWhen creating a table using a Kafka table engine I get this failed assert / stack trace and ClickHouse dies.\r\n\r\n```\r\n*** ../contrib/librdkafka/src/rdkafka_request.c:219:rd_kafka_buf_read_topic_partitions: assert: rkbuf->rkbuf_rkb ***\r\n[ 259 ] {} <Fatal> BaseDaemon: #####################################\r\n[ 259 ] {} <Fatal> BaseDaemon: (version 22.7.35 (official build), build id: A97EE8D81E41A58E) (from thread 243) (no query) Received signal Aborted (6)\r\n[ 259 ] {} <Fatal> BaseDaemon: \r\n[ 259 ] {} <Fatal> BaseDaemon: Stack trace: 0x7f09530df00b 0x7f09530be859 0x1a609a3d 0x1a6c7fdf 0x1a63ee67 0x1a63c7f4 0x1a606814 0x1a6adce1 0x1a6c1895 0x1a60c838 0x1a71d6b9 0x7f0953296609 0x7f09531bb133\r\n[ 259 ] {} <Fatal> BaseDaemon: 2. raise @ 0x7f09530df00b in ?\r\n[ 259 ] {} <Fatal> BaseDaemon: 3. abort @ 0x7f09530be859 in ?\r\n[ 259 ] {} <Fatal> BaseDaemon: 4. ? @ 0x1a609a3d in /usr/bin/clickhouse\r\n[ 259 ] {} <Fatal> BaseDaemon: 5. rd_kafka_buf_read_topic_partitions @ 0x1a6c7fdf in /usr/bin/clickhouse\r\n[ 259 ] {} <Fatal> BaseDaemon: 6. ? @ 0x1a63ee67 in /usr/bin/clickhouse\r\n[ 259 ] {} <Fatal> BaseDaemon: 7. ? @ 0x1a63c7f4 in /usr/bin/clickhouse\r\n[ 259 ] {} <Fatal> BaseDaemon: 8. rd_kafka_buf_callback @ 0x1a606814 in /usr/bin/clickhouse\r\n[ 259 ] {} <Fatal> BaseDaemon: 9. rd_kafka_op_handle @ 0x1a6adce1 in /usr/bin/clickhouse\r\n[ 259 ] {} <Fatal> BaseDaemon: 10. rd_kafka_q_serve @ 0x1a6c1895 in /usr/bin/clickhouse\r\n[ 259 ] {} <Fatal> BaseDaemon: 11. ? @ 0x1a60c838 in /usr/bin/clickhouse\r\n[ 259 ] {} <Fatal> BaseDaemon: 12. ? @ 0x1a71d6b9 in /usr/bin/clickhouse\r\n[ 259 ] {} <Fatal> BaseDaemon: 13. ? @ 0x7f0953296609 in ?\r\n[ 259 ] {} <Fatal> BaseDaemon: 14. __clone @ 0x7f09531bb133 in ?\r\n[ 259 ] {} <Fatal> BaseDaemon: Integrity check of the executable successfully passed (checksum: 35C037103309C93E3EF6D59CA6A2CF05)\r\n```\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nYes\r\n\r\n**How to reproduce**\r\n\r\n* Which ClickHouse server version to use: any\r\n* `CREATE TABLE` statements for all tables involved: any kafka table with default values\r\n\r\n**Expected behavior**\r\n\r\nNot a fatal error.\r\n\r\n**Error message and/or stacktrace**\r\n\r\nSee above.\r\n\r\n**Additional context**\r\n\r\nThat seems to be a known bug in librdkafka >1.4.2 & <1.9.0 and I believe you use 1.5.0 ?\r\n\r\nSee these links:\r\nhttps://github.com/edenhill/librdkafka/issues/3223\r\nhttps://github.com/confluentinc/confluent-kafka-go/issues/650#issuecomment-856661159\r\n\nUpgrade librdkafka version\nSee https://github.com/edenhill/librdkafka/blob/master/CHANGELOG.md\r\n\r\nThat should fix \r\nhttps://github.com/ClickHouse/ClickHouse/issues/40238\r\n\r\nAnd maybe\r\nhttps://github.com/ClickHouse/ClickHouse/issues/30989 \r\nhttps://github.com/ClickHouse/ClickHouse/issues/15473\r\n\r\nI've tried upgrade to 1.7.0 some time ago, and there were some issues with openssl (probably caused by https://github.com/edenhill/librdkafka/pull/3315 and may be by https://github.com/ClickHouse/librdkafka/commit/f68f261ceafb887824deb7848e1ff8d2eb33f956  ) \n",
  "hints_text": "> CREATE TABLE statements for all tables involved: any kafka table with default values\r\n\r\nIt is not reproducible with ANY kafka table :) \r\n\r\nMost probably it's related to some specific broker behaviour (\"malformed JoinGroupResponse consumer group metadata\"), i did not dig it deeper, but the links you added seems relevant, the fix should be there:\r\nhttps://github.com/edenhill/librdkafka/pull/3678/files \r\n\r\nI'll try to either upgrade librdkafka (there was some issues with opessl there) or will just backport the fix \nThank you for following up. I was able to narrow this crash down to only a specific broker like you said. However, I don't have access to the logs on the broker to provide more details.\r\n\r\nWhile doing more troubleshooting on this particular broker I noticed that simply changing the consumer group name would resolve the issue and let the Kafka Table start up and consume normally. That lines up with what you suspect to be the issue I think.\nthis issue can be reproduced in 24.3.2.23.\r\n\r\n```\r\nuse-server/server.key: error:02000002:system library:OPENSSL_internal:No such file or directory (version 24.3.2.23 (official build))\r\n*** contrib/librdkafka/src/rdkafka_request.c:219:rd_kafka_buf_read_topic_partitions: assert: rkbuf->rkbuf_rkb ***\r\n2024.04.16 15:58:43.608870 [ 754 ] {} <Fatal> BaseDaemon: ########## Short fault info ############\r\n2024.04.16 15:58:43.608905 [ 754 ] {} <Fatal> BaseDaemon: (version 24.3.2.23 (official build), build id: 5B5C43F049E2D5125E894547577CDA8EEC25B3C7, git hash: 8b7d910960cc2c6a0db07991fe2576a67fe98146) (from thread 747) Received signal 6\r\n2024.04.16 15:58:43.608916 [ 754 ] {} <Fatal> BaseDaemon: Signal description: Aborted\r\n2024.04.16 15:58:43.608920 [ 754 ] {} <Fatal> BaseDaemon:\r\n2024.04.16 15:58:43.608935 [ 754 ] {} <Fatal> BaseDaemon: Stack trace: 0x0000ffffb5471d78 0x0000ffffb545eaac 0x0000aaaae11b0f50 0x0000aaaae1233f60 0x0000aaaae11d11c8 0x0000aaaae11cefac 0x0000aaaae11ae310 0x0000aaaae1221b84 0x0000aaaae122f9b4 0x0000aaaae11b3938 0x0000aaaae126d720 0x0000ffffb55b8624 0x0000ffffb550f62c\r\n2024.04.16 15:58:43.608946 [ 754 ] {} <Fatal> BaseDaemon: ########################################\r\n2024.04.16 15:58:43.608953 [ 754 ] {} <Fatal> BaseDaemon: (version 24.3.2.23 (official build), build id: 5B5C43F049E2D5125E894547577CDA8EEC25B3C7, git hash: 8b7d910960cc2c6a0db07991fe2576a67fe98146) (from thread 747) (no query) Received signal Aborted (6)\r\n2024.04.16 15:58:43.608956 [ 754 ] {} <Fatal> BaseDaemon:\r\n2024.04.16 15:58:43.608960 [ 754 ] {} <Fatal> BaseDaemon: Stack trace: 0x0000ffffb5471d78 0x0000ffffb545eaac 0x0000aaaae11b0f50 0x0000aaaae1233f60 0x0000aaaae11d11c8 0x0000aaaae11cefac 0x0000aaaae11ae310 0x0000aaaae1221b84 0x0000aaaae122f9b4 0x0000aaaae11b3938 0x0000aaaae126d720 0x0000ffffb55b8624 0x0000ffffb550f62c\r\n2024.04.16 15:58:43.609011 [ 754 ] {} <Fatal> BaseDaemon: 2. ? @ 0x0000000000033d78\r\n2024.04.16 15:58:43.609021 [ 754 ] {} <Fatal> BaseDaemon: 3. ? @ 0x0000000000020aac\r\n2024.04.16 15:58:43.609042 [ 754 ] {} <Fatal> BaseDaemon: 4. ? @ 0x0000000012c70f50\r\n2024.04.16 15:58:43.609065 [ 754 ] {} <Fatal> BaseDaemon: 5. rd_kafka_buf_read_topic_partitions @ 0x0000000012cf3f60\r\n2024.04.16 15:58:43.609097 [ 754 ] {} <Fatal> BaseDaemon: 6. rd_kafka_group_MemberMetadata_consumer_read @ 0x0000000012c911c8\r\n2024.04.16 15:58:43.609107 [ 754 ] {} <Fatal> BaseDaemon: 7. rd_kafka_cgrp_handle_JoinGroup @ 0x0000000012c8efac\r\n2024.04.16 15:58:43.609116 [ 754 ] {} <Fatal> BaseDaemon: 8. rd_kafka_buf_callback @ 0x0000000012c6e310\r\n2024.04.16 15:58:43.609138 [ 754 ] {} <Fatal> BaseDaemon: 9. rd_kafka_op_handle @ 0x0000000012ce1b84\r\n2024.04.16 15:58:43.609162 [ 754 ] {} <Fatal> BaseDaemon: 10. rd_kafka_q_serve @ 0x0000000012cef9b4\r\n2024.04.16 15:58:43.609174 [ 754 ] {} <Fatal> BaseDaemon: 11. rd_kafka_thread_main @ 0x0000000012c73938\r\n2024.04.16 15:58:43.609199 [ 754 ] {} <Fatal> BaseDaemon: 12. _thrd_wrapper_function.llvm.4665451034283701737 @ 0x0000000012d2d720\r\n2024.04.16 15:58:43.609227 [ 754 ] {} <Fatal> BaseDaemon: 13. start_thread @ 0x0000000000007624\r\n2024.04.16 15:58:43.609233 [ 754 ] {} <Fatal> BaseDaemon: 14. ? @ 0x00000000000d162c\r\n2024.04.16 15:58:43.609237 [ 754 ] {} <Fatal> BaseDaemon: Integrity check of the executable skipped because the reference checksum could not be read.\r\n2024.04.16 15:58:43.609248 [ 754 ] {} <Fatal> BaseDaemon: Report this error to https://github.com/ClickHouse/ClickHouse/issues\r\n```\n@alexey-milovidov any updates on this issue?\n@filimonov which particular changelog entry should fix #40238?\n@filimonov Let's continue.\n@filimonov, Nothing was done on your side - closing.\nIt was a lot done actually.\r\n\r\nThe problem ie that there were no version of librdkafka newer than 2.0beta which can pass our CI/CD tests. :/ fixing it requires diving too deep into their code (it's a race), so we just reported that (long ago) to the library maintainers. Recently there was a PR which fixes the exact issue which lead to failures, but after appling that the another issue pops up...\r\n\r\nWe still not losing hope...",
  "created_at": "2025-02-21T22:57:43Z",
  "modified_files": [
    "contrib/librdkafka",
    "contrib/librdkafka-cmake/CMakeLists.txt",
    "contrib/librdkafka-cmake/config.h.in",
    "src/Storages/Kafka/KafkaConsumer.cpp",
    "src/Storages/Kafka/KafkaConsumer.h",
    "src/Storages/Kafka/KafkaConsumer2.cpp",
    "src/Storages/Kafka/KafkaConsumer2.h",
    "src/Storages/Kafka/KafkaSource.cpp",
    "src/Storages/Kafka/StorageKafka.cpp",
    "src/Storages/Kafka/StorageKafkaUtils.cpp",
    "src/Storages/Kafka/StorageKafkaUtils.h"
  ],
  "modified_test_files": [
    "tests/integration/test_storage_kafka/configs/kafka.xml",
    "tests/integration/test_storage_kafka/test_batch_slow.py"
  ]
}