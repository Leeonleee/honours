{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 51720,
  "instance_id": "ClickHouse__ClickHouse-51720",
  "issue_numbers": [
    "51574"
  ],
  "base_commit": "1eef5086d465e7c365678a5dc333de3e240c148d",
  "patch": "diff --git a/src/Common/SystemLogBase.cpp b/src/Common/SystemLogBase.cpp\nindex 86adcbbd31b2..5e9ee9a1e04a 100644\n--- a/src/Common/SystemLogBase.cpp\n+++ b/src/Common/SystemLogBase.cpp\n@@ -136,14 +136,40 @@ void SystemLogBase<LogElement>::add(const LogElement & element)\n \n template <typename LogElement>\n void SystemLogBase<LogElement>::flush(bool force)\n+{\n+    uint64_t this_thread_requested_offset = notifyFlushImpl(force);\n+    if (this_thread_requested_offset == uint64_t(-1))\n+        return;\n+\n+    // Use an arbitrary timeout to avoid endless waiting. 60s proved to be\n+    // too fast for our parallel functional tests, probably because they\n+    // heavily load the disk.\n+    const int timeout_seconds = 180;\n+    std::unique_lock lock(mutex);\n+    bool result = flush_event.wait_for(lock, std::chrono::seconds(timeout_seconds), [&]\n+    {\n+        return flushed_up_to >= this_thread_requested_offset && !is_force_prepare_tables;\n+    });\n+\n+    if (!result)\n+    {\n+        throw Exception(ErrorCodes::TIMEOUT_EXCEEDED, \"Timeout exceeded ({} s) while flushing system log '{}'.\",\n+            toString(timeout_seconds), demangle(typeid(*this).name()));\n+    }\n+}\n+\n+template <typename LogElement>\n+void SystemLogBase<LogElement>::notifyFlush(bool force) { notifyFlushImpl(force); }\n+\n+template <typename LogElement>\n+uint64_t SystemLogBase<LogElement>::notifyFlushImpl(bool force)\n {\n     uint64_t this_thread_requested_offset;\n \n     {\n         std::lock_guard lock(mutex);\n-\n         if (is_shutdown)\n-            return;\n+            return uint64_t(-1);\n \n         this_thread_requested_offset = queue_front_index + queue.size();\n \n@@ -156,22 +182,7 @@ void SystemLogBase<LogElement>::flush(bool force)\n     }\n \n     LOG_DEBUG(log, \"Requested flush up to offset {}\", this_thread_requested_offset);\n-\n-    // Use an arbitrary timeout to avoid endless waiting. 60s proved to be\n-    // too fast for our parallel functional tests, probably because they\n-    // heavily load the disk.\n-    const int timeout_seconds = 180;\n-    std::unique_lock lock(mutex);\n-    bool result = flush_event.wait_for(lock, std::chrono::seconds(timeout_seconds), [&]\n-    {\n-        return flushed_up_to >= this_thread_requested_offset && !is_force_prepare_tables;\n-    });\n-\n-    if (!result)\n-    {\n-        throw Exception(ErrorCodes::TIMEOUT_EXCEEDED, \"Timeout exceeded ({} s) while flushing system log '{}'.\",\n-            toString(timeout_seconds), demangle(typeid(*this).name()));\n-    }\n+    return this_thread_requested_offset;\n }\n \n #define INSTANTIATE_SYSTEM_LOG_BASE(ELEMENT) template class SystemLogBase<ELEMENT>;\ndiff --git a/src/Common/SystemLogBase.h b/src/Common/SystemLogBase.h\nindex f8febd8b1595..92409028c22f 100644\n--- a/src/Common/SystemLogBase.h\n+++ b/src/Common/SystemLogBase.h\n@@ -87,9 +87,12 @@ class SystemLogBase : public ISystemLog\n       */\n     void add(const LogElement & element);\n \n-    /// Flush data in the buffer to disk\n+    /// Flush data in the buffer to disk. Block the thread until the data is stored on disk.\n     void flush(bool force) override;\n \n+    /// Non-blocking flush data in the buffer to disk.\n+    void notifyFlush(bool force);\n+\n     String getName() const override { return LogElement::name(); }\n \n     static const char * getDefaultOrderBy() { return \"event_date, event_time\"; }\n@@ -112,6 +115,10 @@ class SystemLogBase : public ISystemLog\n     uint64_t flushed_up_to = 0;\n     // Logged overflow message at this queue front index\n     uint64_t logged_queue_full_at_index = -1;\n+\n+private:\n+    uint64_t notifyFlushImpl(bool force);\n+\n };\n \n }\ndiff --git a/src/Daemon/BaseDaemon.cpp b/src/Daemon/BaseDaemon.cpp\nindex 319d2bc8b5ba..3852ec5ada52 100644\n--- a/src/Daemon/BaseDaemon.cpp\n+++ b/src/Daemon/BaseDaemon.cpp\n@@ -173,6 +173,9 @@ static void signalHandler(int sig, siginfo_t * info, void * context)\n             /// This coarse method of synchronization is perfectly ok for fatal signals.\n             sleepForSeconds(1);\n         }\n+\n+        /// Wait for all logs flush operations\n+        sleepForSeconds(3);\n         call_default_signal_handler(sig);\n     }\n \ndiff --git a/src/Interpreters/CrashLog.cpp b/src/Interpreters/CrashLog.cpp\nindex 08c08ffecd10..379c9122cc81 100644\n--- a/src/Interpreters/CrashLog.cpp\n+++ b/src/Interpreters/CrashLog.cpp\n@@ -84,5 +84,8 @@ void collectCrashLog(Int32 signal, UInt64 thread_id, const String & query_id, co\n \n         CrashLogElement element{static_cast<time_t>(time / 1000000000), time, signal, thread_id, query_id, trace, trace_full};\n         crash_log_owned->add(element);\n+        /// Notify savingThreadFunction to start flushing crash log\n+        /// Crash log is storing in parallel with the signal processing thread.\n+        crash_log_owned->notifyFlush(true);\n     }\n }\n",
  "test_patch": "diff --git a/tests/integration/test_crash_log/__init__.py b/tests/integration/test_crash_log/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/integration/test_crash_log/test.py b/tests/integration/test_crash_log/test.py\nnew file mode 100644\nindex 000000000000..9f6eca794b1f\n--- /dev/null\n+++ b/tests/integration/test_crash_log/test.py\n@@ -0,0 +1,57 @@\n+import os\n+import time\n+import pytest\n+\n+import helpers.cluster\n+import helpers.test_tools\n+\n+SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def started_node():\n+    cluster = helpers.cluster.ClickHouseCluster(__file__)\n+    try:\n+        node = cluster.add_instance(\"node\", stay_alive=True)\n+\n+        cluster.start()\n+        yield node\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def send_signal(started_node, signal):\n+    started_node.exec_in_container(\n+        [\"bash\", \"-c\", f\"pkill -{signal} clickhouse\"], user=\"root\"\n+    )\n+\n+\n+def wait_for_clickhouse_stop(started_node):\n+    result = None\n+    for attempt in range(60):\n+        time.sleep(1)\n+        pid = started_node.get_process_pid(\"clickhouse\")\n+        if pid is None:\n+            result = \"OK\"\n+            break\n+    assert result == \"OK\", \"ClickHouse process is still running\"\n+\n+\n+def test_pkill(started_node):\n+    if (\n+        started_node.is_built_with_thread_sanitizer()\n+        or started_node.is_built_with_address_sanitizer()\n+        or started_node.is_built_with_memory_sanitizer()\n+    ):\n+        pytest.skip(\"doesn't fit in timeouts for stacktrace generation\")\n+\n+    crashes_count = 0\n+    for signal in [\"SEGV\", \"4\"]:\n+        send_signal(started_node, signal)\n+        wait_for_clickhouse_stop(started_node)\n+        started_node.restart_clickhouse()\n+        crashes_count += 1\n+        assert (\n+            started_node.query(\"SELECT COUNT(*) FROM system.crash_log\")\n+            == f\"{crashes_count}\\n\"\n+        )\n",
  "problem_statement": "system.crash_log is not created sometimes.\nTable system.crash_log is not created after a force crash.\r\n\r\n**Version**\r\n```bash\r\n./build/programs/clickhouse-client --version\r\nClickHouse client version 23.6.1.1.\r\n```\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nYes.\r\n\r\n**How to reproduce**\r\nFirst, you must delete a table that has already been created, if it exists.\r\n1) Start the ClickHouse server.\r\n2) Drop table system.crash_log\u00a0\r\n3) Stop the ClickHouse server.\r\n\r\n\r\nNow reproduce the problem:\r\n1) Start ClickHouse server.\r\n2) Promt bash command:\r\n```bash\r\nsudo kill -4 $(pgrep clickhouse)\r\n```\r\n\r\nIf table was successfully created repeat all steps again (drop system.crash_log, restart ClickHouse and kill the process)\r\n\r\n**Expected behavior**\r\n\r\nTable files are created, and a crash log can be selected by SQL command after server restart:\u00a0\r\n\r\n```SQL\r\nSELECT * FROM system.crash_log\r\n```\r\n\r\n**Actual behavior**\r\nThe problem reproduces unstably. Possible variants:\r\n1) No table files are created. (Table data can't be selected.)\r\n\r\n```bash\r\nls \u00a0/home/admin/ClickHouse2/ClickHouse/build/programs/data/system/crash_log\r\nls: cannot access '/home/admin/ClickHouse2/ClickHouse/build/programs/data/system/crash_log': No such file or directory\r\n\r\n```\r\n2) A table is created, but SELECT * FROM system.crash_log returns 0 entries:\r\n\r\n```bash\r\ntree /home/admin/ClickHouse2/ClickHouse/build/programs/data/system/crash_log\r\n/home/admin/ClickHouse2/ClickHouse/build/programs/data/system/crash_log\r\n\u251c\u2500\u2500 detached\r\n\u2514\u2500\u2500 format_version.txt\r\n```\r\n\r\n3) Crash_log was successfully created, and table data can be selected (No error here).\n",
  "hints_text": "Please assign me to the task. I just started analyzing it.",
  "created_at": "2023-07-03T07:41:23Z",
  "modified_files": [
    "src/Common/SystemLogBase.cpp",
    "src/Common/SystemLogBase.h",
    "src/Daemon/BaseDaemon.cpp",
    "src/Interpreters/CrashLog.cpp"
  ],
  "modified_test_files": [
    "b/tests/integration/test_crash_log/test.py"
  ]
}