{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 78500,
  "instance_id": "ClickHouse__ClickHouse-78500",
  "issue_numbers": [
    "76934"
  ],
  "base_commit": "808591daf7df507d1c90c9d1b0d256df4d877d67",
  "patch": "diff --git a/src/Core/Settings.cpp b/src/Core/Settings.cpp\nindex e305dc534dd1..43ab67b37612 100644\n--- a/src/Core/Settings.cpp\n+++ b/src/Core/Settings.cpp\n@@ -5777,7 +5777,7 @@ Only has an effect in ClickHouse Cloud. Wait time to lock cache for space reserv\n )\", 0) \\\n     \\\n     DECLARE(Bool, parallelize_output_from_storages, true, R\"(\n-Parallelize output for reading step from storage. It allows parallelization of  query processing right after reading from storage if possible\n+Parallelize output for reading step from storage. It allows parallelization of query processing right after reading from storage if possible\n )\", 0) \\\n     DECLARE(String, insert_deduplication_token, \"\", R\"(\n The setting allows a user to provide own deduplication semantic in MergeTree/ReplicatedMergeTree\ndiff --git a/src/Storages/IStorage.cpp b/src/Storages/IStorage.cpp\nindex bdc84221c57f..d8b74764e773 100644\n--- a/src/Storages/IStorage.cpp\n+++ b/src/Storages/IStorage.cpp\n@@ -23,6 +23,7 @@ namespace DB\n namespace Setting\n {\n     extern const SettingsBool parallelize_output_from_storages;\n+    extern const SettingsBool distributed_aggregation_memory_efficient;\n }\n \n namespace ErrorCodes\n@@ -185,7 +186,15 @@ void IStorage::read(\n     /// parallelize processing if not yet\n     const size_t output_ports = pipe.numOutputPorts();\n     const bool parallelize_output = context->getSettingsRef()[Setting::parallelize_output_from_storages];\n-    if (parallelize_output && parallelizeOutputAfterReading(context) && output_ports > 0 && output_ports < num_streams)\n+\n+    /// For distributed_aggregation_memory_efficient with Two-Level-Hash aggregation, the `GroupingAggregatedTransform`\n+    /// need to receive buckets from Remote in order of bucket number, while resize here will break the buckets order\n+    /// return from `RemoteSource`. See https://github.com/ClickHouse/ClickHouse/issues/76934.\n+    const bool should_not_resize = context->getSettingsRef()[Setting::distributed_aggregation_memory_efficient]\n+        && processed_stage == QueryProcessingStage::Enum::WithMergeableState;\n+\n+    if (!should_not_resize && parallelize_output && parallelizeOutputAfterReading(context) && output_ports > 0\n+        && output_ports < num_streams)\n         pipe.resize(num_streams);\n \n     readFromPipe(query_plan, std::move(pipe), column_names, storage_snapshot, query_info, context, shared_from_this());\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/03402_fix_pipe_resize_with_two_level_hash.reference b/tests/queries/0_stateless/03402_fix_pipe_resize_with_two_level_hash.reference\nnew file mode 100644\nindex 000000000000..098ace60d2de\n--- /dev/null\n+++ b/tests/queries/0_stateless/03402_fix_pipe_resize_with_two_level_hash.reference\n@@ -0,0 +1,10 @@\n+(Expression)\n+ExpressionTransform \u00d7 8\n+  (MergingAggregated)\n+  Resize 1 \u2192 8\n+    SortingAggregatedTransform 8 \u2192 1\n+      MergingAggregatedBucketTransform \u00d7 8\n+        Resize 1 \u2192 8\n+          GroupingAggregatedTransform\n+            (ReadFromCluster)\n+            Remote 0 \u2192 1\ndiff --git a/tests/queries/0_stateless/03402_fix_pipe_resize_with_two_level_hash.sh b/tests/queries/0_stateless/03402_fix_pipe_resize_with_two_level_hash.sh\nnew file mode 100755\nindex 000000000000..0af41fa3cdb8\n--- /dev/null\n+++ b/tests/queries/0_stateless/03402_fix_pipe_resize_with_two_level_hash.sh\n@@ -0,0 +1,23 @@\n+#!/usr/bin/env bash\n+# Tags: no-fasttest\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CURDIR\"/../shell_config.sh\n+\n+mkdir -p \"${USER_FILES_PATH}\"/\"${CLICKHOUSE_TEST_UNIQUE_NAME}\"/\n+\n+for i in {1..10}; do\n+    file_name=\"${USER_FILES_PATH}\"/\"${CLICKHOUSE_TEST_UNIQUE_NAME}\"/file\"$i\".csv\n+\n+    for ((j = 1; j <= 10000; j++)); do\n+        random_num=$((RANDOM % 1000000))\n+        echo \"$random_num\" >> \"$file_name\"\n+    done\n+done\n+\n+$CLICKHOUSE_CLIENT --query \"EXPLAIN PIPELINE SELECT cityHash64(n) % 65536 AS n, sum(1) FROM fileCluster('test_cluster_two_shards_localhost', '${CLICKHOUSE_TEST_UNIQUE_NAME}/file{1..10}.csv', 'CSV', 'n UInt32') GROUP BY n SETTINGS distributed_aggregation_memory_efficient=1, max_threads = 8\"\n+\n+$CLICKHOUSE_CLIENT --query \"SELECT cityHash64(n) % 65536 AS n, sum(1) FROM fileCluster('test_cluster_two_shards_localhost', '${CLICKHOUSE_TEST_UNIQUE_NAME}/file{1..10}.csv', 'CSV', 'n UInt32') GROUP BY n FORMAT NULL SETTINGS distributed_aggregation_memory_efficient=1, max_threads = 8\"\n+\n+rm \"${USER_FILES_PATH}\"/\"${CLICKHOUSE_TEST_UNIQUE_NAME}\"/file*.csv\n",
  "problem_statement": "A simple GROUP BY query failed with `distributed_aggregation_memory_efficient ` enabled and group by key items greater than 65535\n### Company or project name\n\n_No response_\n\n### Describe the unexpected behaviour\n\nOK:\n```sql\nSELECT\n    cityHash64(item_id) % 65535 AS itemid,\n    sum(1)\nFROM ice_wxg_euler.dws_finderlive_uin_ecom_pcoc_detail_hi AS a\nWHERE (a.day_ = toDate('2025-02-27')) AND (a.hour_ = toDateTime('2025-02-27 16:00:00')) AND (trimBoth(commentScene) = 'temp_2') AND ((stayTime > 5000) OR (duration > 5000)) AND ((stayTime + duration) > 0) AND (rankscore_str != '')\nGROUP BY itemid\nSETTINGS distributed_aggregation_memory_efficient = 1\nFORMAT `Null`\n\nQuery id: a372983e-0261-437e-85ce-231769087ee2\n\nOk.\n\n0 rows in set. Elapsed: 20.824 sec. Processed 388.99 million rows, 54.84 GB (18.68 million rows/s., 2.63 GB/s.)\n```\n\nNot OK:\n```sql\nSELECT\n    cityHash64(item_id) % 65536 AS itemid,\n    sum(1)\nFROM ice_wxg_euler.dws_finderlive_uin_ecom_pcoc_detail_hi AS a\nWHERE (a.day_ = toDate('2025-02-27')) AND (a.hour_ = toDateTime('2025-02-27 16:00:00')) AND (trimBoth(commentScene) = 'temp_2') AND ((stayTime > 5000) OR (duration > 5000)) AND ((stayTime + duration) > 0) AND (rankscore_str != '')\nGROUP BY itemid\nSETTINGS distributed_aggregation_memory_efficient = 1\nFORMAT `Null`\n\nQuery id: 53eca855-f4ff-4f2e-a4dd-b6e814b00028\n\n\u2193 Progress: 388.99 million rows, 54.84 GB (17.49 million rows/s., 2.46 GB/s.) \n0 rows in set. Elapsed: 22.247 sec. Processed 388.99 million rows, 54.84 GB (17.49 million rows/s., 2.46 GB/s.)\n\nReceived exception from server (version 23.8.5):\nCode: 49. DB::Exception: Received from mmdcchsvrnewtestsz1:28328. DB::Exception: SortingAggregatedTransform already got bucket with number 217. (LOGICAL_ERROR)\n```\n\n### How to reproduce\n\nNone\n\n### Expected behavior\n\n_No response_\n\n### Error message and/or stacktrace\n\n_No response_\n\n### Additional context\n\n_No response_\n",
  "hints_text": "could you pls share the table (schema and data)? or at least the logs\n> could you pls share the table (schema and data)? or at least the logs\n\n@nickitat It's a remote table reading by `HDFSCluster` engine and `item_id` is String type.\n\n```\n2025.03.02 00:20:02.045852 [ 35224 ] {4d799b18-3e86-4b1e-b158-25e7aead2a6a} <Trace> Aggregator: Merging partially aggregated blocks (bucket = 206).\n2025.03.02 00:20:02.045890 [ 35224 ] {4d799b18-3e86-4b1e-b158-25e7aead2a6a} <Debug> Aggregator: Merged partially aggregated blocks for bucket #206. Got 136 rows, 1.59 KiB from 433 source rows in 3.2746e-05 sec. (4153179.014 rows/sec., 47.53 MiB/sec.)\n2025.03.02 00:20:02.045910 [ 35224 ] {4d799b18-3e86-4b1e-b158-25e7aead2a6a} <Trace> Aggregator: Merging partially aggregated blocks (bucket = 207).\n2025.03.02 00:20:02.045943 [ 35224 ] {4d799b18-3e86-4b1e-b158-25e7aead2a6a} <Debug> Aggregator: Merged partially aggregated blocks for bucket #207. Got 123 rows, 1.44 KiB from 401 source rows in 2.7068e-05 sec. (4544111.128 rows/sec., 52.00 MiB/sec.)\n2025.03.02 00:20:02.045964 [ 35224 ] {4d799b18-3e86-4b1e-b158-25e7aead2a6a} <Trace> Aggregator: Merging partially aggregated blocks (bucket = 208).\n2025.03.02 00:20:02.045994 [ 35224 ] {4d799b18-3e86-4b1e-b158-25e7aead2a6a} <Debug> Aggregator: Merged partially aggregated blocks for bucket #208. Got 115 rows, 1.35 KiB from 375 source rows in 2.5321e-05 sec. (4541684.768 rows/sec., 51.98 MiB/sec.)\n2025.03.02 00:20:02.046016 [ 35224 ] {4d799b18-3e86-4b1e-b158-25e7aead2a6a} <Trace> Aggregator: Merging partially aggregated blocks (bucket = 209).\n2025.03.02 00:20:02.046048 [ 35224 ] {4d799b18-3e86-4b1e-b158-25e7aead2a6a} <Debug> Aggregator: Merged partially aggregated blocks for bucket #209. Got 125 rows, 1.46 KiB from 405 source rows in 2.7319e-05 sec. (4575570.116 rows/sec., 52.36 MiB/sec.)\n2025.03.02 00:20:02.046076 [ 35224 ] {4d799b18-3e86-4b1e-b158-25e7aead2a6a} <Trace> Aggregator: Merging partially aggregated blocks (bucket = 210).\n2025.03.02 00:20:02.046114 [ 35224 ] {4d799b18-3e86-4b1e-b158-25e7aead2a6a} <Debug> Aggregator: Merged partially aggregated blocks for bucket #210. Got 128 rows, 1.50 KiB from 408 source rows in 3.0476e-05 sec. (4200026.250 rows/sec., 48.07 MiB/sec.)\n2025.03.02 00:20:02.046140 [ 35224 ] {4d799b18-3e86-4b1e-b158-25e7aead2a6a} <Trace> Aggregator: Merging partially aggregated blocks (bucket = 211).\n2025.03.02 00:20:02.046173 [ 35224 ] {4d799b18-3e86-4b1e-b158-25e7aead2a6a} <Debug> Aggregator: Merged partially aggregated blocks for bucket #211. Got 120 rows, 1.41 KiB from 381 source rows in 2.7152e-05 sec. (4419563.936 rows/sec., 50.58 MiB/sec.)\n2025.03.02 00:20:02.046195 [ 35224 ] {4d799b18-3e86-4b1e-b158-25e7aead2a6a} <Trace> Aggregator: Merging partially aggregated blocks (bucket = 212).\n2025.03.02 00:20:02.046246 [ 35224 ] {4d799b18-3e86-4b1e-b158-25e7aead2a6a} <Debug> Aggregator: Merged partially aggregated blocks for bucket #212. Got 131 rows, 1.54 KiB from 408 source rows in 3.9686e-05 sec. (3300912.160 rows/sec., 37.78 MiB/sec.)\n2025.03.02 00:20:02.051398 [ 39786 ] {4d799b18-3e86-4b1e-b158-25e7aead2a6a} <Error> executeQuery: Code: 49. DB::Exception: SortingAggregatedTransform already got bucket with number 212. (LOGICAL_ERROR) (version 23.8.5.3) (from 9.146.224.79:43574) (in query: SELECT cityHash64(item_id) % 65536 AS itemid, sum(1) FROM ice_wxg_euler.dws_finderlive_uin_ecom_pcoc_detail_hi AS a WHERE (a.day_ = toDate('2025-02-27')) AND (a.hour_ = toDateTime('2025-02-27 16:00:00')) AND (trimBoth(commentScene) = 'temp_2') AND ((stayTime > 5000) OR (duration > 5000)) AND ((stayTime + duration) > 0) AND (rankscore_str != '') GROUP BY itemid SETTINGS distributed_aggregation_memory_efficient = 1 FORMAT `Null`;), Stack trace (when copying this message, always include the lines below):\n2025.03.02 00:20:02.051464 [ 39786 ] {4d799b18-3e86-4b1e-b158-25e7aead2a6a} <Trace> increaseProfileEvent: Report detailed classification(QueryErrorDueToLogical) of ErrorCode(LOGICAL_ERROR)\n2025.03.02 00:20:02.052891 [ 39786 ] {4d799b18-3e86-4b1e-b158-25e7aead2a6a} <Error> TCPHandler: Code: 49. DB::Exception: SortingAggregatedTransform already got bucket with number 212. (LOGICAL_ERROR), Stack trace (when copying this message, always include the lines below):\n2025.03.02 00:20:02.052965 [ 39786 ] {4d799b18-3e86-4b1e-b158-25e7aead2a6a} <Debug> TCPHandler: Processed in 20.830634387 sec.\n2025.03.02 00:20:02.053055 [ 39786 ] {4d799b18-3e86-4b1e-b158-25e7aead2a6a} <Debug> MemoryTracker: Peak memory usage (for query): 17.54 MiB.\n0. Poco::Exception::Exception(String const&, int) @ 0x000000001498d719 in /usr/local/qspace/mmdcchsvrnewtest/sbin/mmdcchsvrnewtest\n1. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000b51e237 in /usr/local/qspace/mmdcchsvrnewtest/sbin/mmdcchsvrnewtest\n2. DB::Exception::Exception<int&>(int, FormatStringHelperImpl<std::type_identity<int&>::type>, int&) @ 0x000000000990104f in /usr/local/qspace/mmdcchsvrnewtest/sbin/mmdcchsvrnewtest\n3. DB::SortingAggregatedTransform::addChunk(DB::Chunk, unsigned long) @ 0x000000001242ee3e in /usr/local/qspace/mmdcchsvrnewtest/sbin/mmdcchsvrnewtest\n4. DB::SortingAggregatedTransform::prepare() @ 0x000000001242f567 in /usr/local/qspace/mmdcchsvrnewtest/sbin/mmdcchsvrnewtest\n5. DB::ExecutingGraph::updateNode(unsigned long, std::queue<DB::ExecutingGraph::Node*, std::deque<DB::ExecutingGraph::Node*, std::allocator<DB::ExecutingGraph::Node*>>>&, std::queue<DB::ExecutingGraph::Node*, std::deque<DB::ExecutingGraph::Node*, std::allocator<DB::ExecutingGraph::Node*>>>&) @ 0x00000000121e50e6 in /usr/local/qspace/mmdcchsvrnewtest/sbin/mmdcchsvrnewtest\n6. DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*) @ 0x00000000121e1979 in /usr/local/qspace/mmdcchsvrnewtest/sbin/mmdcchsvrnewtest\n7. void std::__function::__policy_invoker<void ()>::__call_impl<std::__function::__default_alloc_func<DB::PipelineExecutor::spawnThreads()::$_0, void ()>>(std::__function::__policy_storage const*) @ 0x00000000121e275b in /usr/local/qspace/mmdcchsvrnewtest/sbin/mmdcchsvrnewtest\n8. ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::worker(std::__list_iterator<ThreadFromGlobalPoolImpl<false>, void*>) @ 0x000000000b5f32a3 in /usr/local/qspace/mmdcchsvrnewtest/sbin/mmdcchsvrnewtest\n9. ThreadFromGlobalPoolImpl<false>::ThreadFromGlobalPoolImpl<void ThreadPoolImpl<ThreadFromGlobalPoolImpl<false>>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>(void&&)::'lambda'()::operator()() @ 0x000000000b5f639f in /usr/local/qspace/mmdcchsvrnewtest/sbin/mmdcchsvrnewtest\n10. ThreadPoolImpl<std::thread>::worker(std::__list_iterator<std::thread, void*>) @ 0x000000000b5f0947 in /usr/local/qspace/mmdcchsvrnewtest/sbin/mmdcchsvrnewtest\n11. void* std::__thread_proxy[abi:v15000]<std::tuple<std::unique_ptr<std::__thread_struct, std::default_delete<std::__thread_struct>>, void ThreadPoolImpl<std::thread>::scheduleImpl<void>(std::function<void ()>, Priority, std::optional<unsigned long>, bool)::'lambda0'()>>(void*) @ 0x000000000b5f482e in /usr/local/qspace/mmdcchsvrnewtest/sbin/mmdcchsvrnewtest\n12. start_thread @ 0x0000000000007ea5 in /usr/lib64/libpthread-2.17.so\n13. __clone @ 0x00000000000fe96d in /usr/lib64/libc-2.17.so\n```\n \nThe difference of the two queries above is that the first one use Fix-Size hash table to do aggregating(65535) and the second query use Two-Level hash map to do aggregating.\n\nBTW, it works well when `force_aggregation_in_order` enabled as well.\n\n```sql\nSELECT\n    cityHash64(item_id) % 65536 AS itemid,\n    sum(1)\nFROM ice_wxg_euler.dws_finderlive_uin_ecom_pcoc_detail_hi AS a\nWHERE (a.day_ = toDate('2025-02-27')) AND (a.hour_ = toDateTime('2025-02-27 16:00:00')) AND (trimBoth(commentScene) = 'temp_2') AND ((stayTime > 5000) OR (duration > 5000)) AND ((stayTime + duration) > 0) AND (rankscore_str != '')\nGROUP BY itemid\nSETTINGS distributed_aggregation_memory_efficient = 1, force_aggregation_in_order = 1\nFORMAT `Null`\n\nQuery id: 7f765774-7684-4104-8e67-dde428fa932c\n\nOk.\n\n0 rows in set. Elapsed: 25.031 sec. Processed 388.99 million rows, 54.84 GB (15.54 million rows/s., 2.19 GB/s.)\n```\n\nSo it may related to https://github.com/ClickHouse/ClickHouse/pull/40879\n\n@nickitat Hi, could you take a look at it?\nI don't have ideas, honestly. It is hard to believe we wouldn't find it in CI unless there is something really odd happening. A repro would help a lot. Do you run an official release binary? Is any strange combination of settings employed?\n@nickitat I found it also related to https://github.com/ClickHouse/ClickHouse/pull/53504, without it, works well:\n```sql\nEXPLAIN PIPELINE\nSELECT\n    cityHash64(item_id) % 65536 AS itemid,\n    sum(1)\nFROM ice_wxg_euler.dws_finderlive_uin_ecom_pcoc_detail_hi AS a\nWHERE (a.day_ = toDate('2025-02-27')) AND (a.hour_ = toDateTime('2025-02-27 16:00:00')) AND (trimBoth(commentScene) = 'temp_2') AND ((stayTime > 5000) OR (duration > 5000)) AND ((stayTime + duration) > 0) AND (rankscore_str != '')\nGROUP BY itemid\nSETTINGS distributed_aggregation_memory_efficient = 1\n\nQuery id: 7de50c16-d0a6-4b0b-9eec-ba9178fc4c4b\n\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 (Expression)                                   \u2502\n\u2502 ExpressionTransform \u00d7 32                       \u2502\n\u2502   (MergingAggregated)                          \u2502\n\u2502   Resize 1 \u2192 32                                \u2502\n\u2502     SortingAggregatedTransform 32 \u2192 1          \u2502\n\u2502       MergingAggregatedBucketTransform \u00d7 32    \u2502\n\u2502         Resize 1 \u2192 32                          \u2502\n\u2502           GroupingAggregatedTransform 1024 \u2192 1 \u2502\n\u2502             (ReadFromStorage)                  \u2502\n\u2502             Resize 6 \u2192 1024                    \u2502\n\u2502               Remote \u00d7 6 0 \u2192 1                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nWith it:\n```sql\nEXPLAIN PIPELINE\nSELECT\n    cityHash64(item_id) % 65536 AS itemid,\n    sum(1)\nFROM ice_wxg_euler.dws_finderlive_uin_ecom_pcoc_detail_hi AS a\nWHERE (a.day_ = toDate('2025-02-27')) AND (a.hour_ = toDateTime('2025-02-27 16:00:00')) AND (trimBoth(commentScene) = 'temp_2') AND ((stayTime > 5000) OR (duration > 5000)) AND ((stayTime + duration) > 0) AND (rankscore_str != '')\nGROUP BY itemid\nSETTINGS distributed_aggregation_memory_efficient = 1\n\nQuery id: 7de50c16-d0a6-4b0b-9eec-ba9178fc4c4b\n\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 (Expression)                                   \u2502\n\u2502 ExpressionTransform \u00d7 32                       \u2502\n\u2502   (MergingAggregated)                          \u2502\n\u2502   Resize 1 \u2192 32                                \u2502\n\u2502     SortingAggregatedTransform 32 \u2192 1          \u2502\n\u2502       MergingAggregatedBucketTransform \u00d7 32    \u2502\n\u2502         Resize 1 \u2192 32                          \u2502\n\u2502           GroupingAggregatedTransform 32 \u2192 1 \u2502\n\u2502             (ReadFromStorage)                  \u2502\n\u2502             Resize 6 \u2192 32                    \u2502\n\u2502               Remote \u00d7 6 0 \u2192 1                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nAnd the strange thing is that same bucket appear twice in log:\n\n![Image](https://github.com/user-attachments/assets/dd5e30e7-059f-4c1a-acc9-89e1d95131d5)\n\nSo the error happened.\nyou've posted the same `explain` output twice (could you pls share the actual output for the alternative case). but I assume that we may output blocks from different servers through the same `RemoteSource`, and it will break everything.\n> you've posted the same `explain` output twice (could you pls share the actual output for the alternative case). but I assume that we may output blocks from different servers through the same `RemoteSource`, and it will break everything.\n\n@nickitat The explain updated.\n\nwhat does  'actual output for the alternative case' means?\n> what does 'actual output for the alternative case' means?\n\nthe explain output for the second case. but you've already added it, thanks.\n> > what does 'actual output for the alternative case' means?\n> \n> the explain output for the second case. but you've already added it, thanks.\n\nDo you have any idea about it?\nunfortunately, no\nWe need a repro, otherwise, it is not actionable \n![Image](https://github.com/user-attachments/assets/f23d7241-d2ac-4ecb-b9a4-97156715352a)\n\n@nickitat I have found the problem here, the `GroupingAggregatedTransform ` need to receive `buckets` from `Remote` in order of bucket number, and also `Remote` produce the `buckets` in order of bucket number, while the pipeline resize from 6 to 32 break the orders. It works well when I set `max_threads` to 6, because no resize is needed in this case, and the buckets order be keept.\n> ![Image](https://github.com/user-attachments/assets/f23d7241-d2ac-4ecb-b9a4-97156715352a)\n> \n> [@nickitat](https://github.com/nickitat) I have found the problem here, the `GroupingAggregatedTransform ` need to receive `buckets` from `Remote` in order of bucket number, and also `Remote` produce the `buckets` in order of bucket number, while the pipeline resize from 6 to 32 break the orders. It works well when I set `max_threads` to 6, because no resize is needed in this case, and the buckets order be keept.\n\nthat makes sense. but i don't see any resizes in `ReadFromPreparedSource`. also this plan looks strange to me, e.g. I don't see `UnionStep` before grouping. let me ask once again, do you use the official CH build?\n> > ![Image](https://github.com/user-attachments/assets/f23d7241-d2ac-4ecb-b9a4-97156715352a)\n> > [@nickitat](https://github.com/nickitat) I have found the problem here, the `GroupingAggregatedTransform ` need to receive `buckets` from `Remote` in order of bucket number, and also `Remote` produce the `buckets` in order of bucket number, while the pipeline resize from 6 to 32 break the orders. It works well when I set `max_threads` to 6, because no resize is needed in this case, and the buckets order be keept.\n> \n> that makes sense. but i don't see any resizes in `ReadFromPreparedSource`. also this plan looks strange to me, e.g. I don't see `UnionStep` before grouping. let me ask once again, do you use the official CH build?\n\nNot an official build, but we didn't do any modification about pipeline.\nIt's introduced in https://github.com/ClickHouse/ClickHouse/pull/48727",
  "created_at": "2025-04-01T06:41:23Z"
}