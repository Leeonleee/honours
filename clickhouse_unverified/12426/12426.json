{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 12426,
  "instance_id": "ClickHouse__ClickHouse-12426",
  "issue_numbers": [
    "12402"
  ],
  "base_commit": "0fbad9dbec097075913812095e897fec97f7ee38",
  "patch": "diff --git a/src/Common/FileChecker.cpp b/src/Common/FileChecker.cpp\nindex 687b4dccca76..6cbec3bda77b 100644\n--- a/src/Common/FileChecker.cpp\n+++ b/src/Common/FileChecker.cpp\n@@ -12,6 +12,12 @@\n namespace DB\n {\n \n+namespace ErrorCodes\n+{\n+    extern const int UNEXPECTED_END_OF_FILE;\n+}\n+\n+\n FileChecker::FileChecker(DiskPtr disk_, const String & file_info_path_) : disk(std::move(disk_))\n {\n     setPath(file_info_path_);\n@@ -24,19 +30,15 @@ void FileChecker::setPath(const String & file_info_path_)\n     tmp_files_info_path = parentPath(files_info_path) + \"tmp_\" + fileName(files_info_path);\n }\n \n-void FileChecker::update(const String & file_path)\n+void FileChecker::update(const String & full_file_path)\n {\n     initialize();\n-    updateImpl(file_path);\n-    save();\n+    map[fileName(full_file_path)] = disk->getFileSize(full_file_path);\n }\n \n-void FileChecker::update(const Strings::const_iterator & begin, const Strings::const_iterator & end)\n+void FileChecker::setEmpty(const String & full_file_path)\n {\n-    initialize();\n-    for (auto it = begin; it != end; ++it)\n-        updateImpl(*it);\n-    save();\n+    map[fileName(full_file_path)] = 0;\n }\n \n CheckResults FileChecker::check() const\n@@ -73,6 +75,28 @@ CheckResults FileChecker::check() const\n     return results;\n }\n \n+void FileChecker::repair()\n+{\n+    for (const auto & name_size : map)\n+    {\n+        const String & name = name_size.first;\n+        size_t expected_size = name_size.second;\n+        String path = parentPath(files_info_path) + name;\n+        bool exists = disk->exists(path);\n+        auto real_size = exists ? disk->getFileSize(path) : 0;  /// No race condition assuming no one else is working with these files.\n+\n+        if (real_size < expected_size)\n+            throw Exception(ErrorCodes::UNEXPECTED_END_OF_FILE, \"Size of {} is less than expected. Size is {} but should be {}.\",\n+                path, real_size, expected_size);\n+\n+        if (real_size > expected_size)\n+        {\n+            LOG_WARNING(&Poco::Logger::get(\"FileChecker\"), \"Will truncate file {} that has size {} to size {}\", path, real_size, expected_size);\n+            disk->truncateFile(path, expected_size);\n+        }\n+    }\n+}\n+\n void FileChecker::initialize()\n {\n     if (initialized)\n@@ -82,11 +106,6 @@ void FileChecker::initialize()\n     initialized = true;\n }\n \n-void FileChecker::updateImpl(const String & file_path)\n-{\n-    map[fileName(file_path)] = disk->getFileSize(file_path);\n-}\n-\n void FileChecker::save() const\n {\n     {\ndiff --git a/src/Common/FileChecker.h b/src/Common/FileChecker.h\nindex 83db397e78c5..015d4cadb079 100644\n--- a/src/Common/FileChecker.h\n+++ b/src/Common/FileChecker.h\n@@ -14,19 +14,25 @@ class FileChecker\n public:\n     FileChecker(DiskPtr disk_, const String & file_info_path_);\n     void setPath(const String & file_info_path_);\n-    void update(const String & file_path);\n-    void update(const Strings::const_iterator & begin, const Strings::const_iterator & end);\n+\n+    void update(const String & full_file_path);\n+    void setEmpty(const String & full_file_path);\n+    void save() const;\n \n     /// Check the files whose parameters are specified in sizes.json\n     CheckResults check() const;\n \n+    /// Truncate files that have excessive size to the expected size.\n+    /// Throw exception if the file size is less than expected.\n+    /// The purpose of this function is to rollback a group of unfinished writes.\n+    void repair();\n+\n private:\n     /// File name -> size.\n     using Map = std::map<String, UInt64>;\n \n     void initialize();\n     void updateImpl(const String & file_path);\n-    void save() const;\n     void load(Map & local_map, const String & path) const;\n \n     DiskPtr disk;\ndiff --git a/src/Disks/DiskLocal.cpp b/src/Disks/DiskLocal.cpp\nindex 68f5ee99a7a8..f9e988211da2 100644\n--- a/src/Disks/DiskLocal.cpp\n+++ b/src/Disks/DiskLocal.cpp\n@@ -19,6 +19,7 @@ namespace ErrorCodes\n     extern const int EXCESSIVE_ELEMENT_IN_CONFIG;\n     extern const int PATH_ACCESS_DENIED;\n     extern const int INCORRECT_DISK_INDEX;\n+    extern const int CANNOT_TRUNCATE_FILE;\n }\n \n std::mutex DiskLocal::reservation_mutex;\n@@ -261,6 +262,13 @@ void DiskLocal::createHardLink(const String & src_path, const String & dst_path)\n     DB::createHardLink(disk_path + src_path, disk_path + dst_path);\n }\n \n+void DiskLocal::truncateFile(const String & path, size_t size)\n+{\n+    int res = truncate((disk_path + path).c_str(), size);\n+    if (-1 == res)\n+        throwFromErrnoWithPath(\"Cannot truncate file \" + path, path, ErrorCodes::CANNOT_TRUNCATE_FILE);\n+}\n+\n void DiskLocal::createFile(const String & path)\n {\n     Poco::File(disk_path + path).createFile();\ndiff --git a/src/Disks/DiskLocal.h b/src/Disks/DiskLocal.h\nindex 3dab4614d5d6..71c4dc0aec9b 100644\n--- a/src/Disks/DiskLocal.h\n+++ b/src/Disks/DiskLocal.h\n@@ -99,6 +99,8 @@ class DiskLocal : public IDisk\n \n     void createHardLink(const String & src_path, const String & dst_path) override;\n \n+    void truncateFile(const String & path, size_t size) override;\n+\n     const String getType() const override { return \"local\"; }\n \n private:\ndiff --git a/src/Disks/DiskMemory.cpp b/src/Disks/DiskMemory.cpp\nindex 3e43d159ba5b..96d9e22c414f 100644\n--- a/src/Disks/DiskMemory.cpp\n+++ b/src/Disks/DiskMemory.cpp\n@@ -408,6 +408,17 @@ void DiskMemory::setReadOnly(const String &)\n     throw Exception(\"Method setReadOnly is not implemented for memory disks\", ErrorCodes::NOT_IMPLEMENTED);\n }\n \n+void DiskMemory::truncateFile(const String & path, size_t size)\n+{\n+    std::lock_guard lock(mutex);\n+\n+    auto file_it = files.find(path);\n+    if (file_it == files.end())\n+        throw Exception(\"File '\" + path + \"' doesn't exist\", ErrorCodes::FILE_DOESNT_EXIST);\n+\n+    file_it->second.data.resize(size);\n+}\n+\n \n using DiskMemoryPtr = std::shared_ptr<DiskMemory>;\n \ndiff --git a/src/Disks/DiskMemory.h b/src/Disks/DiskMemory.h\nindex f7948019fe8b..fc265ddef031 100644\n--- a/src/Disks/DiskMemory.h\n+++ b/src/Disks/DiskMemory.h\n@@ -90,6 +90,8 @@ class DiskMemory : public IDisk\n \n     void createHardLink(const String & src_path, const String & dst_path) override;\n \n+    void truncateFile(const String & path, size_t size) override;\n+\n     const String getType() const override { return \"memory\"; }\n \n private:\ndiff --git a/src/Disks/IDisk.cpp b/src/Disks/IDisk.cpp\nindex 837ddf1b6b26..9d7424d12862 100644\n--- a/src/Disks/IDisk.cpp\n+++ b/src/Disks/IDisk.cpp\n@@ -8,6 +8,11 @@\n namespace DB\n {\n \n+namespace ErrorCodes\n+{\n+    extern const int NOT_IMPLEMENTED;\n+}\n+\n bool IDisk::isDirectoryEmpty(const String & path)\n {\n     return !iterateDirectory(path)->isValid();\n@@ -42,4 +47,9 @@ void IDisk::copy(const String & from_path, const std::shared_ptr<IDisk> & to_dis\n     }\n }\n \n+void IDisk::truncateFile(const String &, size_t)\n+{\n+    throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"Truncate operation is not implemented for disk of type {}\", getType());\n+}\n+\n }\ndiff --git a/src/Disks/IDisk.h b/src/Disks/IDisk.h\nindex 77a52a7a5d63..0a977feb9a13 100644\n--- a/src/Disks/IDisk.h\n+++ b/src/Disks/IDisk.h\n@@ -172,6 +172,9 @@ class IDisk : public Space\n     /// Create hardlink from `src_path` to `dst_path`.\n     virtual void createHardLink(const String & src_path, const String & dst_path) = 0;\n \n+    /// Truncate file to specified size.\n+    virtual void truncateFile(const String & path, size_t size);\n+\n     /// Return disk type - \"local\", \"s3\", etc.\n     virtual const String getType() const = 0;\n };\ndiff --git a/src/Storages/StorageLog.cpp b/src/Storages/StorageLog.cpp\nindex 39fa1d1af709..9cfc906108ac 100644\n--- a/src/Storages/StorageLog.cpp\n+++ b/src/Storages/StorageLog.cpp\n@@ -127,7 +127,12 @@ class LogBlockOutputStream final : public IBlockOutputStream\n     {\n         try\n         {\n-            writeSuffix();\n+            if (!done)\n+            {\n+                /// Rollback partial writes.\n+                streams.clear();\n+                storage.file_checker.repair();\n+            }\n         }\n         catch (...)\n         {\n@@ -298,7 +303,6 @@ void LogBlockOutputStream::writeSuffix()\n {\n     if (done)\n         return;\n-    done = true;\n \n     WrittenStreams written_streams;\n     IDataType::SerializeBinaryBulkSettings settings;\n@@ -323,9 +327,12 @@ void LogBlockOutputStream::writeSuffix()\n         column_files.push_back(storage.files[name_stream.first].data_file_path);\n     column_files.push_back(storage.marks_file_path);\n \n-    storage.file_checker.update(column_files.begin(), column_files.end());\n+    for (const auto & file : column_files)\n+        storage.file_checker.update(file);\n+    storage.file_checker.save();\n \n     streams.clear();\n+    done = true;\n }\n \n \n@@ -427,6 +434,7 @@ StorageLog::StorageLog(\n     const StorageID & table_id_,\n     const ColumnsDescription & columns_,\n     const ConstraintsDescription & constraints_,\n+    bool attach,\n     size_t max_compress_block_size_)\n     : IStorage(table_id_)\n     , disk(std::move(disk_))\n@@ -442,13 +450,31 @@ StorageLog::StorageLog(\n     if (relative_path_.empty())\n         throw Exception(\"Storage \" + getName() + \" requires data path\", ErrorCodes::INCORRECT_FILE_NAME);\n \n-    /// create directories if they do not exist\n-    disk->createDirectories(table_path);\n+    if (!attach)\n+    {\n+        /// create directories if they do not exist\n+        disk->createDirectories(table_path);\n+    }\n+    else\n+    {\n+        try\n+        {\n+            file_checker.repair();\n+        }\n+        catch (...)\n+        {\n+            tryLogCurrentException(__PRETTY_FUNCTION__);\n+        }\n+    }\n \n     for (const auto & column : storage_metadata.getColumns().getAllPhysical())\n         addFiles(column.name, *column.type);\n \n     marks_file_path = table_path + DBMS_STORAGE_LOG_MARKS_FILE_NAME;\n+\n+    if (!attach)\n+        for (const auto & file : files)\n+            file_checker.setEmpty(file.second.data_file_path);\n }\n \n \n@@ -655,7 +681,7 @@ void registerStorageLog(StorageFactory & factory)\n \n         return StorageLog::create(\n             disk, args.relative_data_path, args.table_id, args.columns, args.constraints,\n-            args.context.getSettings().max_compress_block_size);\n+            args.attach, args.context.getSettings().max_compress_block_size);\n     }, features);\n }\n \ndiff --git a/src/Storages/StorageLog.h b/src/Storages/StorageLog.h\nindex d020f9066090..96acb1668e20 100644\n--- a/src/Storages/StorageLog.h\n+++ b/src/Storages/StorageLog.h\n@@ -54,6 +54,7 @@ class StorageLog final : public ext::shared_ptr_helper<StorageLog>, public IStor\n         const StorageID & table_id_,\n         const ColumnsDescription & columns_,\n         const ConstraintsDescription & constraints_,\n+        bool attach,\n         size_t max_compress_block_size_);\n \n private:\ndiff --git a/src/Storages/StorageStripeLog.cpp b/src/Storages/StorageStripeLog.cpp\nindex e55cc190f80d..ae8162d5f1b1 100644\n--- a/src/Storages/StorageStripeLog.cpp\n+++ b/src/Storages/StorageStripeLog.cpp\n@@ -161,11 +161,12 @@ class StripeLogBlockOutputStream final : public IBlockOutputStream\n         , lock(storage.rwlock)\n         , data_out_file(storage.table_path + \"data.bin\")\n         , data_out_compressed(storage.disk->writeFile(data_out_file, DBMS_DEFAULT_BUFFER_SIZE, WriteMode::Append))\n-        , data_out(*data_out_compressed, CompressionCodecFactory::instance().getDefaultCodec(), storage.max_compress_block_size)\n+        , data_out(std::make_unique<CompressedWriteBuffer>(\n+            *data_out_compressed, CompressionCodecFactory::instance().getDefaultCodec(), storage.max_compress_block_size))\n         , index_out_file(storage.table_path + \"index.mrk\")\n         , index_out_compressed(storage.disk->writeFile(index_out_file, DBMS_DEFAULT_BUFFER_SIZE, WriteMode::Append))\n-        , index_out(*index_out_compressed)\n-        , block_out(data_out, 0, metadata_snapshot->getSampleBlock(), false, &index_out, storage.disk->getFileSize(data_out_file))\n+        , index_out(std::make_unique<CompressedWriteBuffer>(*index_out_compressed))\n+        , block_out(*data_out, 0, metadata_snapshot->getSampleBlock(), false, index_out.get(), storage.disk->getFileSize(data_out_file))\n     {\n     }\n \n@@ -173,7 +174,16 @@ class StripeLogBlockOutputStream final : public IBlockOutputStream\n     {\n         try\n         {\n-            writeSuffix();\n+            if (!done)\n+            {\n+                /// Rollback partial writes.\n+                data_out.reset();\n+                data_out_compressed.reset();\n+                index_out.reset();\n+                index_out_compressed.reset();\n+\n+                storage.file_checker.repair();\n+            }\n         }\n         catch (...)\n         {\n@@ -194,13 +204,14 @@ class StripeLogBlockOutputStream final : public IBlockOutputStream\n             return;\n \n         block_out.writeSuffix();\n-        data_out.next();\n+        data_out->next();\n         data_out_compressed->next();\n-        index_out.next();\n+        index_out->next();\n         index_out_compressed->next();\n \n         storage.file_checker.update(data_out_file);\n         storage.file_checker.update(index_out_file);\n+        storage.file_checker.save();\n \n         done = true;\n     }\n@@ -212,10 +223,10 @@ class StripeLogBlockOutputStream final : public IBlockOutputStream\n \n     String data_out_file;\n     std::unique_ptr<WriteBuffer> data_out_compressed;\n-    CompressedWriteBuffer data_out;\n+    std::unique_ptr<CompressedWriteBuffer> data_out;\n     String index_out_file;\n     std::unique_ptr<WriteBuffer> index_out_compressed;\n-    CompressedWriteBuffer index_out;\n+    std::unique_ptr<CompressedWriteBuffer> index_out;\n     NativeBlockOutputStream block_out;\n \n     bool done = false;\n@@ -249,6 +260,20 @@ StorageStripeLog::StorageStripeLog(\n     {\n         /// create directories if they do not exist\n         disk->createDirectories(table_path);\n+\n+        file_checker.setEmpty(table_path + \"data.bin\");\n+        file_checker.setEmpty(table_path + \"index.mrk\");\n+    }\n+    else\n+    {\n+        try\n+        {\n+            file_checker.repair();\n+        }\n+        catch (...)\n+        {\n+            tryLogCurrentException(__PRETTY_FUNCTION__);\n+        }\n     }\n }\n \ndiff --git a/src/Storages/StorageTinyLog.cpp b/src/Storages/StorageTinyLog.cpp\nindex ef8c30cacbe1..b68ac6ae5f15 100644\n--- a/src/Storages/StorageTinyLog.cpp\n+++ b/src/Storages/StorageTinyLog.cpp\n@@ -118,7 +118,12 @@ class TinyLogBlockOutputStream final : public IBlockOutputStream\n     {\n         try\n         {\n-            writeSuffix();\n+            if (!done)\n+            {\n+                /// Rollback partial writes.\n+                streams.clear();\n+                storage.file_checker.repair();\n+            }\n         }\n         catch (...)\n         {\n@@ -277,11 +282,13 @@ void TinyLogBlockOutputStream::writeSuffix()\n {\n     if (done)\n         return;\n-    done = true;\n \n     /// If nothing was written - leave the table in initial state.\n     if (streams.empty())\n+    {\n+        done = true;\n         return;\n+    }\n \n     WrittenStreams written_streams;\n     IDataType::SerializeBinaryBulkSettings settings;\n@@ -303,9 +310,12 @@ void TinyLogBlockOutputStream::writeSuffix()\n     for (auto & pair : streams)\n         column_files.push_back(storage.files[pair.first].data_file_path);\n \n-    storage.file_checker.update(column_files.begin(), column_files.end());\n+    for (const auto & file : column_files)\n+        storage.file_checker.update(file);\n+    storage.file_checker.save();\n \n     streams.clear();\n+    done = true;\n }\n \n \n@@ -352,9 +362,24 @@ StorageTinyLog::StorageTinyLog(\n         /// create directories if they do not exist\n         disk->createDirectories(table_path);\n     }\n+    else\n+    {\n+        try\n+        {\n+            file_checker.repair();\n+        }\n+        catch (...)\n+        {\n+            tryLogCurrentException(__PRETTY_FUNCTION__);\n+        }\n+    }\n \n     for (const auto & col : storage_metadata.getColumns().getAllPhysical())\n         addFiles(col.name, *col.type);\n+\n+    if (!attach)\n+        for (const auto & file : files)\n+            file_checker.setEmpty(file.second.data_file_path);\n }\n \n \n",
  "test_patch": "diff --git a/src/Storages/tests/gtest_storage_log.cpp b/src/Storages/tests/gtest_storage_log.cpp\nindex c97adaf118d7..13c96fbab542 100644\n--- a/src/Storages/tests/gtest_storage_log.cpp\n+++ b/src/Storages/tests/gtest_storage_log.cpp\n@@ -31,7 +31,7 @@ DB::StoragePtr createStorage(DB::DiskPtr & disk)\n     names_and_types.emplace_back(\"a\", std::make_shared<DataTypeUInt64>());\n \n     StoragePtr table = StorageLog::create(\n-        disk, \"table/\", StorageID(\"test\", \"test\"), ColumnsDescription{names_and_types}, ConstraintsDescription{}, 1048576);\n+        disk, \"table/\", StorageID(\"test\", \"test\"), ColumnsDescription{names_and_types}, ConstraintsDescription{}, false, 1048576);\n \n     table->startup();\n \n@@ -100,6 +100,7 @@ std::string writeData(int rows, DB::StoragePtr & table, const DB::Context & cont\n \n     BlockOutputStreamPtr out = table->write({}, metadata_snapshot, context);\n     out->write(block);\n+    out->writeSuffix();\n \n     return data;\n }\n@@ -115,7 +116,8 @@ std::string readData(DB::StoragePtr & table, const DB::Context & context)\n \n     QueryProcessingStage::Enum stage = table->getQueryProcessingStage(context);\n \n-    BlockInputStreamPtr in = std::make_shared<TreeExecutorBlockInputStream>(std::move(table->read(column_names, metadata_snapshot, {}, context, stage, 8192, 1)[0]));\n+    BlockInputStreamPtr in = std::make_shared<TreeExecutorBlockInputStream>(\n+        std::move(table->read(column_names, metadata_snapshot, {}, context, stage, 8192, 1)[0]));\n \n     Block sample;\n     {\ndiff --git a/tests/queries/0_stateless/01383_log_broken_table.reference b/tests/queries/0_stateless/01383_log_broken_table.reference\nnew file mode 100644\nindex 000000000000..1bc7c914e46a\n--- /dev/null\n+++ b/tests/queries/0_stateless/01383_log_broken_table.reference\n@@ -0,0 +1,3 @@\n+Testing TinyLog\n+Testing StripeLog\n+Testing Log\ndiff --git a/tests/queries/0_stateless/01383_log_broken_table.sh b/tests/queries/0_stateless/01383_log_broken_table.sh\nnew file mode 100755\nindex 000000000000..2afac11e7c2e\n--- /dev/null\n+++ b/tests/queries/0_stateless/01383_log_broken_table.sh\n@@ -0,0 +1,40 @@\n+#!/usr/bin/env bash\n+\n+CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+CLICKHOUSE_CLIENT_SERVER_LOGS_LEVEL=none\n+. $CURDIR/../shell_config.sh\n+\n+\n+function test()\n+{\n+    ENGINE=$1\n+    MAX_MEM=4096\n+\n+    echo \"Testing $ENGINE\"\n+\n+    $CLICKHOUSE_CLIENT --query \"DROP TABLE IF EXISTS log\";\n+    $CLICKHOUSE_CLIENT --query \"CREATE TABLE log (x UInt64, y UInt64, z UInt64) ENGINE = $ENGINE\";\n+\n+    while true; do\n+        MAX_MEM=$((2 * $MAX_MEM))\n+\n+        $CLICKHOUSE_CLIENT --query \"INSERT INTO log SELECT number, number, number FROM numbers(1000000)\" --max_memory_usage $MAX_MEM > ${CLICKHOUSE_TMP}/insert_result 2>&1\n+\n+        grep -o -F 'Memory limit' ${CLICKHOUSE_TMP}/insert_result || cat ${CLICKHOUSE_TMP}/insert_result\n+\n+        $CLICKHOUSE_CLIENT --query \"SELECT count(), sum(x + y + z) FROM log\" > ${CLICKHOUSE_TMP}/select_result 2>&1;\n+\n+        grep -o -F 'File not found' ${CLICKHOUSE_TMP}/select_result || cat ${CLICKHOUSE_TMP}/select_result\n+\n+        [[ $MAX_MEM -gt 200000000 ]] && break;\n+    done\n+\n+    $CLICKHOUSE_CLIENT --query \"DROP TABLE log\";\n+}\n+\n+test TinyLog | grep -v -P '^(Memory limit|0\\t0|File not found|[1-9]000000\\t)'\n+test StripeLog | grep -v -P '^(Memory limit|0\\t0|File not found|[1-9]000000\\t)'\n+test Log | grep -v -P '^(Memory limit|0\\t0|File not found|[1-9]000000\\t)'\n+\n+rm \"${CLICKHOUSE_TMP}/insert_result\"\n+rm \"${CLICKHOUSE_TMP}/select_result\"\n",
  "problem_statement": "Invalid number of rows in Chunk column Int32: expected 99, got 5: While executing TinyLog \n**How to reproduce**\r\nMost likely problem is a result of failed INSERT in TinyLog as bug is triggered often on small VM and rarely on big server. There is an error about memory limit before error.\r\n\r\n```\r\n2020.07.10 14:16:09.298115 [ 73 ] {} <Error> DynamicQueryHandler: Code: 241, e.displayText() = DB::Exception: Memory limit (total) exceeded: would use 39.79 GiB (attempt to alloc\r\nate chunk of 4202772 bytes), maximum: 39.79 GiB, Stack trace (when copying this message, always include the lines below):\r\n```\r\n\r\n```\r\n2020.07.10 14:16:09.298194 [ 102 ] {7457fbb7-6c87-46e4-81b4-b1c05af4372b} <Error> executeQuery: Code: 49, e.displayText() = DB::Exception: Invalid number of rows in Chunk column Int32: expected 99, got 5: While executing TinyLog (version 20.5.2.7 (official build)) (from 172.17.0.1:60726) (in query: SELECT DISTINCT * FROM (SELECT t0.c0 FROM t0 WHERE t0.c1 GROUP BY t0.c0 UNION ALL SELECT t0.c0 FROM t0 WHERE (NOT (t0.c1)) GROUP BY t0.c0 UNION ALL SELECT t0.c0 FROM t0 WHERE ((t0.c1) IS NULL) GROUP BY t0.c0) FORMAT TabSeparatedWithNamesAndTypes;), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x10ed0da0 in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x95c923d in /usr/bin/clickhouse\r\n2. ? @ 0xe6d3587 in /usr/bin/clickhouse\r\n3. DB::Chunk::Chunk(std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >, unsigned long) @ 0xe6d2739 in /usr/bin/clickhouse\r\n4. DB::TinyLogSource::generate() @ 0xe32e474 in /usr/bin/clickhouse\r\n5. DB::ISource::work() @ 0xe6dd3ab in /usr/bin/clickhouse\r\n6. DB::SourceWithProgress::work() @ 0xe91f337 in /usr/bin/clickhouse\r\n7. ? @ 0xe70aa21 in /usr/bin/clickhouse\r\n8. ? @ 0xe70f1a6 in /usr/bin/clickhouse\r\n9. ? @ 0xe70f7f2 in /usr/bin/clickhouse\r\n10. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x95f6e97 in /usr/bin/clickhouse\r\n11. ? @ 0x95f5383 in /usr/bin/clickhouse\r\n12. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n13. __clone @ 0x12188f in /lib/x86_64-linux-gnu/libc-2.27.so\r\n```\r\n\r\n**Additional context**\r\nBug found with SQLancer TLPDistinct oracle.\n",
  "hints_text": "Same problem triggered by GROUP BY query\r\n```\r\nSELECT t0.c1, t0.c0 FROM t0 GROUP BY t0.c1, t0.c0\r\n\r\nCaused by: ru.yandex.clickhouse.except.ClickHouseException: ClickHouse exception, code: 49, host: localhost, port: 8123; Code: 49, e.displayText() = DB::Exception: Invalid number of rows in Chunk column String: expected 39, got 34: While executing TinyLog (version 20.5.2.7 (official build))\r\n```\nLooks like the documented behaviour of Log table engines: https://clickhouse.tech/docs/en/engines/table-engines/log-family/\r\n\r\nIt's reasonable to rewrite SQLancer scripts to use MergeTree tables.\r\nFor small tables, Memory is also Ok.\r\n\r\nI will think on how to avoid this issue nevertheless.",
  "created_at": "2020-07-12T02:34:14Z"
}