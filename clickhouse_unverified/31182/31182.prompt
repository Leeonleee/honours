You will be provided with a partial code base and an issue statement explaining a problem to resolve.

<issue>
Memory overcommit for queries.
**Use case**

Allow query to use more memory than it's guaranteed if free memory is available while providing guarantees on possible memory usage for other queries.

E.g. a server has 512 GiB of memory and limiting every query to default 10 GiB just in case is unreasonable.
But simply raising the max_memory_usage to 500 GiB is also unreasonable.


**Describe the solution you'd like**

Add setting `max_guaranteed_memory_usage` to accompany with the existing `max_memory_usage`
and `max_guaranteed_memory_usage_for_user` to accompany with the existing `max_memory_usage_for_user`.

`max_guaranteed_memory_usage` will denote soft memory limit while `max_memory_usage` will remain for hard memory limit.

If `max_memory_usage` is reached, exception will be thrown (the same as current behaviour).

If `max_memory_usage_for_user` or `max_server_memory_usage` is reached, we will take the ratio of the query memory usage to the `max_guaranteed_memory_usage` or `max_guaranteed_memory_usage_for_user` (overcommit ratio), of our query and all other running queries in a group (of all queries from the same user or all running queries), sort by this value, and if current query is at the top, we immediately throw exception and signal condvar on query finish. Other queries will wait on condvar but no more than specified amount of time, and continue if memory is freed. In case of timeout, an exception is thrown as usual.

Add setting `memory_usage_overcommit_max_wait_microseconds` to limit waiting time.

The overcommit_ratio should be exposed in system.processes table.


**Notes**

The idea is to add waiting on condvar inside MemoryTracker.
I'm not 100% sure if it will work ok, especially in dynamics. But at least it looks fairly easy.

No deadlocks should be possible as the most hungry query will throw exception, free resources and signal condvar. Then if memory limit is reached again, the next most hungry query will throw exception and so on.

To avoid unnecessary waiting we can force the most hungry query (by overcommit ratio) to throw exception slightly earlier than reaching user/total memory limit (e.g. if it is 90% or one GB to reach the limit).

Waiting inside (almost) arbitrary place in code can prevent proper utilization of CPU in presense of single query execution pipeline. But it should not be a concern as this wait is guaranteed to be short.

One of the goals of this task is to enable fallback to external sorting and external aggregation by default. But it is slightly more difficult because it has to be activated before reaching memory limit (the fallback itself requires some extra memory). To solve it, we can provide settings similar to `max_bytes_before_external_group_by` but in form of ratio or delta related to the memory limit. So, it will be activated before memory limit is reached.


**Alternative solutions**

Another way is to simply specify limits in form of ratio to free memory. Like every query can use 50% of the remaining memory.
Then single query can occupy half of all RAM and concurrent queries will line up in beutiful geometric sequence. But it does not solve the task.


**Additional context**

See also #8449.
</issue>

I need you to solve the provided issue by generating a code fix that can be applied directly to the repository

Respond below:
