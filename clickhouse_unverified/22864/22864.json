{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 22864,
  "instance_id": "ClickHouse__ClickHouse-22864",
  "issue_numbers": [
    "22679"
  ],
  "base_commit": "d550c06ed8a5a904cb73d1d5ca1fb81f62de0d16",
  "patch": "diff --git a/src/Storages/MergeTree/DataPartsExchange.cpp b/src/Storages/MergeTree/DataPartsExchange.cpp\nindex d5f4d91850cb..2c830c56524f 100644\n--- a/src/Storages/MergeTree/DataPartsExchange.cpp\n+++ b/src/Storages/MergeTree/DataPartsExchange.cpp\n@@ -470,9 +470,36 @@ MergeTreeData::MutableDataPartPtr Fetcher::fetchPart(\n         size_t sum_files_size = 0;\n         readBinary(sum_files_size, in);\n         IMergeTreeDataPart::TTLInfos ttl_infos;\n-        /// Skip ttl infos, not required for S3 metadata\n         String ttl_infos_string;\n         readBinary(ttl_infos_string, in);\n+        ReadBufferFromString ttl_infos_buffer(ttl_infos_string);\n+        assertString(\"ttl format version: 1\\n\", ttl_infos_buffer);\n+        ttl_infos.read(ttl_infos_buffer);\n+\n+        ReservationPtr reservation\n+            = data.balancedReservation(metadata_snapshot, sum_files_size, 0, part_name, part_info, {}, tagger_ptr, &ttl_infos, true);\n+        if (!reservation)\n+            reservation\n+                = data.reserveSpacePreferringTTLRules(metadata_snapshot, sum_files_size, ttl_infos, std::time(nullptr), 0, true);\n+        if (reservation)\n+        {\n+            /// When we have multi-volume storage, one of them was chosen, depends on TTL, free space, etc.\n+            /// Chosen one may be S3 or not.\n+            DiskPtr disk = reservation->getDisk();\n+            if (disk && disk->getType() == DiskType::Type::S3)\n+            {\n+                for (const auto & d : disks_s3)\n+                {\n+                    if (d->getPath() == disk->getPath())\n+                    {\n+                        Disks disks_tmp = { disk };\n+                        disks_s3.swap(disks_tmp);\n+                        break;\n+                    }\n+                }\n+            }\n+        }\n+\n         String part_type = \"Wide\";\n         readStringBinary(part_type, in);\n         if (part_type == \"InMemory\")\n@@ -795,7 +822,6 @@ MergeTreeData::MutableDataPartPtr Fetcher::downloadPartToS3(\n     readBinary(files, in);\n \n     auto volume = std::make_shared<SingleDiskVolume>(\"volume_\" + part_name, disk);\n-    MergeTreeData::MutableDataPartPtr new_data_part = data.createPart(part_name, volume, part_relative_path);\n \n     for (size_t i = 0; i < files; ++i)\n     {\n@@ -805,7 +831,7 @@ MergeTreeData::MutableDataPartPtr Fetcher::downloadPartToS3(\n         readStringBinary(file_name, in);\n         readBinary(file_size, in);\n \n-        String data_path = new_data_part->getFullRelativePath() + file_name;\n+        String data_path = part_download_path + file_name;\n         String metadata_file = fullPath(disk, data_path);\n \n         {\n@@ -837,6 +863,7 @@ MergeTreeData::MutableDataPartPtr Fetcher::downloadPartToS3(\n \n     assertEOF(in);\n \n+    MergeTreeData::MutableDataPartPtr new_data_part = data.createPart(part_name, volume, part_relative_path);\n     new_data_part->is_temp = true;\n     new_data_part->modification_time = time(nullptr);\n     new_data_part->loadColumnsChecksumsIndexes(true, false);\ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.cpp b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\nindex dc68a1dcb41e..8a35ce3dcb88 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.cpp\n@@ -429,9 +429,9 @@ void IMergeTreeDataPart::removeIfNeeded()\n             }\n \n             if (parent_part)\n-                projectionRemove(parent_part->getFullRelativePath());\n+                projectionRemove(parent_part->getFullRelativePath(), keep_s3_on_delete);\n             else\n-                remove(false);\n+                remove(keep_s3_on_delete);\n \n             if (state == State::DeleteOnDestroy)\n             {\n@@ -1108,7 +1108,7 @@ void IMergeTreeDataPart::remove(bool keep_s3) const\n     if (isProjectionPart())\n     {\n         LOG_WARNING(storage.log, \"Projection part {} should be removed by its parent {}.\", name, parent_part->name);\n-        projectionRemove(parent_part->getFullRelativePath());\n+        projectionRemove(parent_part->getFullRelativePath(), keep_s3);\n         return;\n     }\n \n@@ -1158,7 +1158,7 @@ void IMergeTreeDataPart::remove(bool keep_s3) const\n     std::unordered_set<String> projection_directories;\n     for (const auto & [p_name, projection_part] : projection_parts)\n     {\n-        projection_part->projectionRemove(to);\n+        projection_part->projectionRemove(to, keep_s3);\n         projection_directories.emplace(p_name + \".proj\");\n     }\n \n@@ -1207,7 +1207,7 @@ void IMergeTreeDataPart::remove(bool keep_s3) const\n }\n \n \n-void IMergeTreeDataPart::projectionRemove(const String & parent_to) const\n+void IMergeTreeDataPart::projectionRemove(const String & parent_to, bool keep_s3) const\n {\n     String to = parent_to + \"/\" + relative_path;\n     auto disk = volume->getDisk();\n@@ -1219,7 +1219,7 @@ void IMergeTreeDataPart::projectionRemove(const String & parent_to) const\n             \"Cannot quickly remove directory {} by removing files; fallback to recursive removal. Reason: checksums.txt is missing\",\n             fullPath(disk, to));\n         /// If the part is not completely written, we cannot use fast path by listing files.\n-        disk->removeRecursive(to + \"/\");\n+        disk->removeSharedRecursive(to + \"/\", keep_s3);\n     }\n     else\n     {\n@@ -1232,17 +1232,17 @@ void IMergeTreeDataPart::projectionRemove(const String & parent_to) const\n     #    pragma GCC diagnostic ignored \"-Wunused-variable\"\n     #endif\n             for (const auto & [file, _] : checksums.files)\n-                disk->removeFile(to + \"/\" + file);\n+                disk->removeSharedFile(to + \"/\" + file, keep_s3);\n     #if !defined(__clang__)\n     #    pragma GCC diagnostic pop\n     #endif\n \n             for (const auto & file : {\"checksums.txt\", \"columns.txt\"})\n-                disk->removeFile(to + \"/\" + file);\n-            disk->removeFileIfExists(to + \"/\" + DEFAULT_COMPRESSION_CODEC_FILE_NAME);\n-            disk->removeFileIfExists(to + \"/\" + DELETE_ON_DESTROY_MARKER_FILE_NAME);\n+                disk->removeSharedFile(to + \"/\" + file, keep_s3);\n+            disk->removeSharedFileIfExists(to + \"/\" + DEFAULT_COMPRESSION_CODEC_FILE_NAME, keep_s3);\n+            disk->removeSharedFileIfExists(to + \"/\" + DELETE_ON_DESTROY_MARKER_FILE_NAME, keep_s3);\n \n-            disk->removeDirectory(to);\n+            disk->removeSharedRecursive(to, keep_s3);\n         }\n         catch (...)\n         {\n@@ -1250,7 +1250,7 @@ void IMergeTreeDataPart::projectionRemove(const String & parent_to) const\n \n             LOG_ERROR(storage.log, \"Cannot quickly remove directory {} by removing files; fallback to recursive removal. Reason: {}\", fullPath(disk, to), getCurrentExceptionMessage(false));\n \n-            disk->removeRecursive(to + \"/\");\n+            disk->removeSharedRecursive(to + \"/\", keep_s3);\n          }\n      }\n  }\ndiff --git a/src/Storages/MergeTree/IMergeTreeDataPart.h b/src/Storages/MergeTree/IMergeTreeDataPart.h\nindex 5f12498c08cf..a8a49680dd7d 100644\n--- a/src/Storages/MergeTree/IMergeTreeDataPart.h\n+++ b/src/Storages/MergeTree/IMergeTreeDataPart.h\n@@ -130,7 +130,7 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n \n     void remove(bool keep_s3 = false) const;\n \n-    void projectionRemove(const String & parent_to) const;\n+    void projectionRemove(const String & parent_to, bool keep_s3 = false) const;\n \n     /// Initialize columns (from columns.txt if exists, or create from column files if not).\n     /// Load checksums from checksums.txt if exists. Load index if required.\n@@ -199,18 +199,21 @@ class IMergeTreeDataPart : public std::enable_shared_from_this<IMergeTreeDataPar\n     /// Frozen by ALTER TABLE ... FREEZE ... It is used for information purposes in system.parts table.\n     mutable std::atomic<bool> is_frozen {false};\n \n+    /// Flag for keep S3 data when zero-copy replication over S3 turned on.\n+    mutable bool keep_s3_on_delete = false;\n+\n     /**\n      * Part state is a stage of its lifetime. States are ordered and state of a part could be increased only.\n      * Part state should be modified under data_parts mutex.\n      *\n      * Possible state transitions:\n-     * Temporary -> Precommitted:   we are trying to commit a fetched, inserted or merged part to active set\n-     * Precommitted -> Outdated:    we could not add a part to active set and are doing a rollback (for example it is duplicated part)\n-     * Precommitted -> Committed:   we successfully committed a part to active dataset\n-     * Precommitted -> Outdated:    a part was replaced by a covering part or DROP PARTITION\n-     * Outdated -> Deleting:        a cleaner selected this part for deletion\n-     * Deleting -> Outdated:        if an ZooKeeper error occurred during the deletion, we will retry deletion\n-     * Committed -> DeleteOnDestroy if part was moved to another disk\n+     * Temporary -> Precommitted:    we are trying to commit a fetched, inserted or merged part to active set\n+     * Precommitted -> Outdated:     we could not add a part to active set and are doing a rollback (for example it is duplicated part)\n+     * Precommitted -> Committed:    we successfully committed a part to active dataset\n+     * Precommitted -> Outdated:     a part was replaced by a covering part or DROP PARTITION\n+     * Outdated -> Deleting:         a cleaner selected this part for deletion\n+     * Deleting -> Outdated:         if an ZooKeeper error occurred during the deletion, we will retry deletion\n+     * Committed -> DeleteOnDestroy: if part was moved to another disk\n      */\n     enum class State\n     {\ndiff --git a/src/Storages/MergeTree/MergeTreeData.cpp b/src/Storages/MergeTree/MergeTreeData.cpp\nindex 41adca37c609..dc7ecc2166d7 100644\n--- a/src/Storages/MergeTree/MergeTreeData.cpp\n+++ b/src/Storages/MergeTree/MergeTreeData.cpp\n@@ -2726,6 +2726,22 @@ void MergeTreeData::swapActivePart(MergeTreeData::DataPartPtr part_copy)\n             if (active_part_it == data_parts_by_info.end())\n                 throw Exception(\"Cannot swap part '\" + part_copy->name + \"', no such active part.\", ErrorCodes::NO_SUCH_DATA_PART);\n \n+            /// We do not check allow_s3_zero_copy_replication here because data may be shared\n+            /// when allow_s3_zero_copy_replication turned on and off again\n+\n+            original_active_part->keep_s3_on_delete = false;\n+\n+            if (original_active_part->volume->getDisk()->getType() == DiskType::Type::S3)\n+            {\n+                if (part_copy->volume->getDisk()->getType() == DiskType::Type::S3\n+                        && original_active_part->getUniqueId() == part_copy->getUniqueId())\n+                {   /// May be when several volumes use the same S3 storage\n+                    original_active_part->keep_s3_on_delete = true;\n+                }\n+                else\n+                    original_active_part->keep_s3_on_delete = !unlockSharedData(*original_active_part);\n+            }\n+\n             modifyPartState(original_active_part, DataPartState::DeleteOnDestroy);\n             data_parts_indexes.erase(active_part_it);\n \n",
  "test_patch": "diff --git a/tests/integration/test_s3_zero_copy_replication/configs/config.d/s3.xml b/tests/integration/test_s3_zero_copy_replication/configs/config.d/s3.xml\nindex ec28840054a3..db639cabb63d 100644\n--- a/tests/integration/test_s3_zero_copy_replication/configs/config.d/s3.xml\n+++ b/tests/integration/test_s3_zero_copy_replication/configs/config.d/s3.xml\n@@ -8,6 +8,18 @@\n                 <access_key_id>minio</access_key_id>\n                 <secret_access_key>minio123</secret_access_key>\n             </s31>\n+            <s31_again>\n+                <type>s3</type>\n+                <endpoint>http://minio1:9001/root/data/</endpoint>\n+                <access_key_id>minio</access_key_id>\n+                <secret_access_key>minio123</secret_access_key>\n+            </s31_again>\n+            <s32>\n+                <type>s3</type>\n+                <endpoint>http://minio1:9001/root/data2/</endpoint>\n+                <access_key_id>minio</access_key_id>\n+                <secret_access_key>minio123</secret_access_key>\n+            </s32>\n         </disks>\n         <policies>\n             <s3>\n@@ -28,11 +40,31 @@\n                 </volumes>\n                 <move_factor>0.0</move_factor>\n             </hybrid>\n+            <tiered>\n+                <volumes>\n+                    <main>\n+                        <disk>s31</disk>\n+                    </main>\n+                    <external>\n+                        <disk>s32</disk>\n+                    </external>\n+                </volumes>\n+            </tiered>\n+            <tiered_copy>\n+                <volumes>\n+                    <main>\n+                        <disk>s31</disk>\n+                    </main>\n+                    <external>\n+                        <disk>s31_again</disk>\n+                    </external>\n+                </volumes>\n+            </tiered_copy>\n         </policies>\n     </storage_configuration>\n \n     <merge_tree>\n-        <min_bytes_for_wide_part>0</min_bytes_for_wide_part>\n+        <min_bytes_for_wide_part>1024</min_bytes_for_wide_part>\n         <old_parts_lifetime>1</old_parts_lifetime>\n         <allow_s3_zero_copy_replication>1</allow_s3_zero_copy_replication>\n     </merge_tree>\ndiff --git a/tests/integration/test_s3_zero_copy_replication/test.py b/tests/integration/test_s3_zero_copy_replication/test.py\nindex f7078d55c330..d9f7cca4a3a7 100644\n--- a/tests/integration/test_s3_zero_copy_replication/test.py\n+++ b/tests/integration/test_s3_zero_copy_replication/test.py\n@@ -1,3 +1,4 @@\n+import datetime\n import logging\n import time\n \n@@ -27,10 +28,10 @@ def cluster():\n         cluster.shutdown()\n \n \n-def get_large_objects_count(cluster, size=100):\n+def get_large_objects_count(cluster, size=100, folder='data'):\n     minio = cluster.minio_client\n     counter = 0\n-    for obj in minio.list_objects(cluster.minio_bucket, 'data/'):\n+    for obj in minio.list_objects(cluster.minio_bucket, '{}/'.format(folder)):\n         if obj.size >= size:\n             counter = counter + 1\n     return counter\n@@ -38,11 +39,11 @@ def get_large_objects_count(cluster, size=100):\n \n def wait_for_large_objects_count(cluster, expected, size=100, timeout=30):\n     while timeout > 0:\n-        if get_large_objects_count(cluster, size) == expected:\n+        if get_large_objects_count(cluster, size=size) == expected:\n             return\n         timeout -= 1\n         time.sleep(1)\n-    assert get_large_objects_count(cluster, size) == expected\n+    assert get_large_objects_count(cluster, size=size) == expected\n \n \n @pytest.mark.parametrize(\n@@ -63,7 +64,7 @@ def test_s3_zero_copy_replication(cluster, policy):\n     )\n \n     node1.query(\"INSERT INTO s3_test VALUES (0,'data'),(1,'data')\")\n-    time.sleep(1)\n+    node2.query(\"SYSTEM SYNC REPLICA s3_test\")\n     assert node1.query(\"SELECT * FROM s3_test order by id FORMAT Values\") == \"(0,'data'),(1,'data')\"\n     assert node2.query(\"SELECT * FROM s3_test order by id FORMAT Values\") == \"(0,'data'),(1,'data')\"\n \n@@ -71,14 +72,15 @@ def test_s3_zero_copy_replication(cluster, policy):\n     assert get_large_objects_count(cluster) == 1\n \n     node2.query(\"INSERT INTO s3_test VALUES (2,'data'),(3,'data')\")\n-    time.sleep(1)\n+    node1.query(\"SYSTEM SYNC REPLICA s3_test\")\n+\n     assert node2.query(\"SELECT * FROM s3_test order by id FORMAT Values\") == \"(0,'data'),(1,'data'),(2,'data'),(3,'data')\"\n     assert node1.query(\"SELECT * FROM s3_test order by id FORMAT Values\") == \"(0,'data'),(1,'data'),(2,'data'),(3,'data')\"\n \n     # Based on version 20.x - two parts\n     wait_for_large_objects_count(cluster, 2)\n \n-    node1.query(\"OPTIMIZE TABLE s3_test\")\n+    node1.query(\"OPTIMIZE TABLE s3_test FINAL\")\n \n     # Based on version 20.x - after merge, two old parts and one merged\n     wait_for_large_objects_count(cluster, 3)\n@@ -105,8 +107,7 @@ def test_s3_zero_copy_on_hybrid_storage(cluster):\n     )\n \n     node1.query(\"INSERT INTO hybrid_test VALUES (0,'data'),(1,'data')\")\n-\n-    time.sleep(1)\n+    node2.query(\"SYSTEM SYNC REPLICA hybrid_test\")\n \n     assert node1.query(\"SELECT * FROM hybrid_test ORDER BY id FORMAT Values\") == \"(0,'data'),(1,'data')\"\n     assert node2.query(\"SELECT * FROM hybrid_test ORDER BY id FORMAT Values\") == \"(0,'data'),(1,'data')\"\n@@ -120,7 +121,7 @@ def test_s3_zero_copy_on_hybrid_storage(cluster):\n     assert node2.query(\"SELECT partition_id,disk_name FROM system.parts WHERE table='hybrid_test' FORMAT Values\") == \"('all','default')\"\n \n     # Total objects in S3\n-    s3_objects = get_large_objects_count(cluster, 0)\n+    s3_objects = get_large_objects_count(cluster, size=0)\n \n     node2.query(\"ALTER TABLE hybrid_test MOVE PARTITION ID 'all' TO DISK 's31'\")\n \n@@ -135,3 +136,115 @@ def test_s3_zero_copy_on_hybrid_storage(cluster):\n \n     node1.query(\"DROP TABLE IF EXISTS hybrid_test NO DELAY\")\n     node2.query(\"DROP TABLE IF EXISTS hybrid_test NO DELAY\")\n+\n+\n+def insert_data_time(node, table, number_of_mb, time, start=0):\n+    values = ','.join(f\"({x},{time})\" for x in range(start, int((1024 * 1024 * number_of_mb) / 8) + start + 1))\n+    node.query(f\"INSERT INTO {table} VALUES {values}\")\n+\n+\n+def insert_large_data(node, table):\n+    tm = time.mktime((datetime.date.today() - datetime.timedelta(days=7)).timetuple())\n+    insert_data_time(node, table, 1, tm, 0)\n+    tm = time.mktime((datetime.date.today() - datetime.timedelta(days=3)).timetuple())\n+    insert_data_time(node, table, 1, tm, 1024*1024)\n+    tm = time.mktime(datetime.date.today().timetuple())\n+    insert_data_time(node, table, 10, tm, 1024*1024*2)\n+\n+\n+@pytest.mark.parametrize(\n+    (\"storage_policy\", \"large_data\", \"iterations\"),\n+    [\n+        (\"tiered\", False, 10),\n+        (\"tiered_copy\", False, 10),\n+        (\"tiered\", True, 3),\n+        (\"tiered_copy\", True, 3),\n+    ]\n+)\n+def test_s3_zero_copy_with_ttl_move(cluster, storage_policy, large_data, iterations):\n+    node1 = cluster.instances[\"node1\"]\n+    node2 = cluster.instances[\"node2\"]\n+\n+    node1.query(\"DROP TABLE IF EXISTS ttl_move_test NO DELAY\")\n+    node2.query(\"DROP TABLE IF EXISTS ttl_move_test NO DELAY\")\n+\n+    for i in range(iterations):\n+        node1.query(\n+            \"\"\"\n+            CREATE TABLE ttl_move_test ON CLUSTER test_cluster (d UInt64, d1 DateTime)\n+            ENGINE=ReplicatedMergeTree('/clickhouse/tables/ttl_move_test', '{}')\n+            ORDER BY d\n+            TTL d1 + INTERVAL 2 DAY TO VOLUME 'external'\n+            SETTINGS storage_policy='{}'\n+            \"\"\"\n+                .format('{replica}', storage_policy)\n+        )\n+\n+        if large_data:\n+            insert_large_data(node1, 'ttl_move_test')\n+        else:\n+            node1.query(\"INSERT INTO ttl_move_test VALUES (10, now() - INTERVAL 3 DAY)\")\n+            node1.query(\"INSERT INTO ttl_move_test VALUES (11, now() - INTERVAL 1 DAY)\")\n+\n+        node1.query(\"OPTIMIZE TABLE ttl_move_test FINAL\")\n+        node2.query(\"SYSTEM SYNC REPLICA ttl_move_test\")\n+\n+        if large_data:\n+            assert node1.query(\"SELECT count() FROM ttl_move_test FORMAT Values\") == \"(1572867)\"\n+            assert node2.query(\"SELECT count() FROM ttl_move_test FORMAT Values\") == \"(1572867)\"\n+        else:\n+            assert node1.query(\"SELECT count() FROM ttl_move_test FORMAT Values\") == \"(2)\"\n+            assert node2.query(\"SELECT count() FROM ttl_move_test FORMAT Values\") == \"(2)\"\n+            assert node1.query(\"SELECT d FROM ttl_move_test ORDER BY d FORMAT Values\") == \"(10),(11)\"\n+            assert node2.query(\"SELECT d FROM ttl_move_test ORDER BY d FORMAT Values\") == \"(10),(11)\"\n+\n+        node1.query(\"DROP TABLE IF EXISTS ttl_move_test NO DELAY\")\n+        node2.query(\"DROP TABLE IF EXISTS ttl_move_test NO DELAY\")\n+\n+\n+@pytest.mark.parametrize(\n+    (\"large_data\", \"iterations\"),\n+    [\n+        (False, 10),\n+        (True, 3),\n+    ]\n+)\n+def test_s3_zero_copy_with_ttl_delete(cluster, large_data, iterations):\n+    node1 = cluster.instances[\"node1\"]\n+    node2 = cluster.instances[\"node2\"]\n+\n+    node1.query(\"DROP TABLE IF EXISTS ttl_delete_test NO DELAY\")\n+    node2.query(\"DROP TABLE IF EXISTS ttl_delete_test NO DELAY\")\n+\n+    for i in range(iterations):\n+        node1.query(\n+            \"\"\"\n+            CREATE TABLE ttl_delete_test ON CLUSTER test_cluster (d UInt64, d1 DateTime)\n+            ENGINE=ReplicatedMergeTree('/clickhouse/tables/ttl_delete_test', '{}')\n+            ORDER BY d\n+            TTL d1 + INTERVAL 2 DAY\n+            SETTINGS storage_policy='tiered'\n+            \"\"\"\n+                .format('{replica}')\n+        )\n+\n+        if large_data:\n+            insert_large_data(node1, 'ttl_delete_test')\n+        else:\n+            node1.query(\"INSERT INTO ttl_delete_test VALUES (10, now() - INTERVAL 3 DAY)\")\n+            node1.query(\"INSERT INTO ttl_delete_test VALUES (11, now() - INTERVAL 1 DAY)\")\n+\n+        node1.query(\"OPTIMIZE TABLE ttl_delete_test FINAL\")\n+        node2.query(\"SYSTEM SYNC REPLICA ttl_delete_test\")\n+\n+        if large_data:\n+            assert node1.query(\"SELECT count() FROM ttl_delete_test FORMAT Values\") == \"(1310721)\"\n+            assert node2.query(\"SELECT count() FROM ttl_delete_test FORMAT Values\") == \"(1310721)\"\n+        else:\n+            assert node1.query(\"SELECT count() FROM ttl_delete_test FORMAT Values\") == \"(1)\"\n+            assert node2.query(\"SELECT count() FROM ttl_delete_test FORMAT Values\") == \"(1)\"\n+            assert node1.query(\"SELECT d FROM ttl_delete_test ORDER BY d FORMAT Values\") == \"(11)\"\n+            assert node2.query(\"SELECT d FROM ttl_delete_test ORDER BY d FORMAT Values\") == \"(11)\"\n+\n+        node1.query(\"DROP TABLE IF EXISTS ttl_delete_test NO DELAY\")\n+        node2.query(\"DROP TABLE IF EXISTS ttl_delete_test NO DELAY\")\n",
  "problem_statement": "Replicated tables with TTL moves or delete do not work when <allow_s3_zero_copy_replication> is set to 1\n**Describe the bug**\r\nWhen <allow_s3_zero_copy_replication> is set to 1 in the <merge_tree> configuration, replicated tables with TTL moves or delete are broken. SELECT from replicated tables with TTL moves or delete causes errors.\r\n\r\n**Does it reproduce on recent release?**\r\n[The list of releases](https://github.com/ClickHouse/ClickHouse/blob/master/utils/list-versions/version_date.tsv)\r\n\r\n* 21.4.1 revision 54447\r\n\r\n**How to reproduce**\r\n* Version: 21.4.1 revision 54447\r\n* Storage configuration: \r\n```\r\n<yandex>\r\n  <storage_configuration>\r\n    <disks>\r\n      <external>\r\n        <type>s3</type>\r\n        <endpoint>[AWS S3 endpoint]</endpoint>\r\n        <access_key_id>[access_key_id]</access_key_id>\r\n        <secret_access_key>[secret_access_key]</secret_access_key>\r\n      </external>\r\n      <external_tiered>\r\n        <type>s3</type>\r\n        <endpoint>[AWS S3 endpoint]</endpoint>\r\n        <access_key_id>[access_key_id]</access_key_id>\r\n        <secret_access_key>[secret_access_key]</secret_access_key>\r\n      </external_tiered>\r\n    </disks>\r\n    <policies>\r\n      <external>\r\n        <volumes>\r\n          <external>\r\n            <disk>external</disk>\r\n          </external>\r\n        </volumes>\r\n      </external>\r\n      <tiered>\r\n        <volumes>\r\n          <default>\r\n            <disk>external</disk>\r\n          </default>\r\n          <external>\r\n            <disk>external_tiered</disk>\r\n          </external>\r\n        </volumes>\r\n      </tiered>\r\n    </policies>\r\n  </storage_configuration>\r\n</yandex>\r\n```\r\n* Merge tree configuration:\r\n```\r\n<yandex>\r\n  <merge_tree>\r\n    <allow_s3_zero_copy_replication>1</allow_s3_zero_copy_replication>\r\n  </merge_tree>\r\n</yandex>\r\n```\r\n* Statements to reproduce error: \r\nNote: either of the two following create statements could be used -- the error occurs with both TTL move and TTL delete.\r\n\r\nCreate replicated table with TTL move on two instances of ClickHouse:\r\n```\r\nCREATE TABLE zero_copy_replication ( d UInt64, d1 DateTime ) ENGINE = ReplicatedMergeTree('/clickhouse/zero_copy_replication_ttl', '1') ORDER BY d \r\nTTL d1 + interval 2 day to volume 'external'\r\nSETTINGS storage_policy='tiered';\r\nCREATE TABLE zero_copy_replication ( d UInt64, d1 DateTime ) ENGINE = ReplicatedMergeTree('/clickhouse/zero_copy_replication_ttl', '2') ORDER BY d \r\nTTL d1 + interval 2 day to volume 'external'\r\nSETTINGS storage_policy='tiered';\r\n```\r\nAlternate create statement, replicated table with TTL delete on two instances of ClickHouse:\r\n```\r\nCREATE TABLE zero_copy_replication ( d UInt64, d1 DateTime ) ENGINE = ReplicatedMergeTree('/clickhouse/zero_copy_replication_ttl', '1') ORDER BY d \r\nTTL d1 + interval 2 day\r\nSETTINGS storage_policy='tiered';\r\nCREATE TABLE zero_copy_replication ( d UInt64, d1 DateTime ) ENGINE = ReplicatedMergeTree('/clickhouse/zero_copy_replication_ttl', '2') ORDER BY d \r\nTTL d1 + interval 2 day\r\nSETTINGS storage_policy='tiered';\r\n```\r\nInsert data into the table, with some rows before the 2 day TTL, and some rows after the 2 day TTL.\r\nThen, SELECT using a command similar to the following:\r\n```\r\nSELECT COUNT() FROM zero_copy_replication\r\n```\r\n\r\n**Expected behavior**\r\nExpect to be able to SELECT from a replicated table with TTL with <allow_s3_zero_copy_replication> set to 1.\r\n\r\n**Error message and/or stacktrace**\r\nError message:\r\n```\r\nCode: 499. DB::Exception: Received from localhost:9000. \r\nDB::Exception: <?xml version=\"1.0\" encoding=\"UTF-8\"?><Error><Code>NoSuchKey</Code>\r\n<Message>The specified key does not exist.</Message>\r\n<Key>data/zero-copy-replication/xqzqytlaumwdavygqraanbwlllelhijs</Key>\r\n<RequestId>0W01GFT6FX2J4VDF</RequestId>\r\n<HostId>FGeqq3GokrJx9qHMxu9TeFgmWrm/2nH0c6xMLTAGxcWXBeZgnapkwOfJixZZN9JXjjjd6Ei6CKY=</HostId>\r\n</Error>: (while reading column d): (while reading from part \r\n/var/lib/clickhouse/disks/external/store/83b/83b6fb95-f932-417a-aaaf-c709d864e7a5/all_2_2_0/ \r\nfrom mark 23 with max_rows_to_read = 8192): While executing MergeTreeReverse.\r\n```\r\n\n",
  "hints_text": "",
  "created_at": "2021-04-08T18:36:40Z"
}