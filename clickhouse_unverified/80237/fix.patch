diff --git a/src/Processors/Formats/IInputFormat.h b/src/Processors/Formats/IInputFormat.h
index cf2397ff1cb6..6a8208e295a3 100644
--- a/src/Processors/Formats/IInputFormat.h
+++ b/src/Processors/Formats/IInputFormat.h
@@ -14,6 +14,14 @@ struct SelectQueryInfo;
 
 using ColumnMappingPtr = std::shared_ptr<ColumnMapping>;
 
+struct ChunkInfoRowNumOffset : public ChunkInfoCloneable<ChunkInfoRowNumOffset>
+{
+    ChunkInfoRowNumOffset(const ChunkInfoRowNumOffset & other) = default;
+    explicit ChunkInfoRowNumOffset(size_t row_num_offset_) : row_num_offset(row_num_offset_) { }
+
+    const size_t row_num_offset;
+};
+
 /** Input format is a source, that reads data from ReadBuffer.
   */
 class IInputFormat : public ISource
diff --git a/src/Processors/Formats/Impl/ParquetBlockInputFormat.cpp b/src/Processors/Formats/Impl/ParquetBlockInputFormat.cpp
index e27c0dee8c27..b3e033416776 100644
--- a/src/Processors/Formats/Impl/ParquetBlockInputFormat.cpp
+++ b/src/Processors/Formats/Impl/ParquetBlockInputFormat.cpp
@@ -578,6 +578,7 @@ ParquetBlockInputFormat::ParquetBlockInputFormat(
     , skip_row_groups(format_settings.parquet.skip_row_groups)
     , parser_group(std::move(parser_group_))
     , min_bytes_for_seek(min_bytes_for_seek_)
+    , header(header_)
     , pending_chunks(PendingChunk::Compare { .row_group_first = format_settings_.parquet.preserve_order })
     , previous_block_missing_values(getPort().getHeader().columns())
 {
@@ -731,13 +732,11 @@ void ParquetBlockInputFormat::initializeIfNeeded()
 
         KeyCondition::ColumnIndexToBloomFilter column_index_to_bloom_filter;
 
-        const auto & header = getPort().getHeader();
-
-        std::vector<Range> hyperrectangle(header.columns(), Range::createWholeUniverse());
+        std::vector<Range> hyperrectangle(getPort().getHeader().columns(), Range::createWholeUniverse());
 
         if (format_settings.parquet.filter_push_down)
         {
-            hyperrectangle = getHyperrectangleForRowGroup(*metadata, row_group, header, format_settings);
+            hyperrectangle = getHyperrectangleForRowGroup(*metadata, row_group, getPort().getHeader(), format_settings);
         }
 
         if (format_settings.parquet.bloom_filter_push_down)
@@ -750,20 +749,29 @@ void ParquetBlockInputFormat::initializeIfNeeded()
         return !maybe_exists;
     };
 
+    // The first one stores the skipped rows for all the skipped row groups before the first row group batch.
+    row_group_batches_skipped_rows.push_back(0);
     for (int row_group = 0; row_group < num_row_groups; ++row_group)
     {
         if (skip_row_groups.contains(row_group))
+        {
+            row_group_batches_skipped_rows.back() += metadata->RowGroup(row_group)->num_rows();
             continue;
+        }
 
         if (key_condition_with_bloom_filter_data && skip_row_group_based_on_filters(row_group))
         {
             ProfileEvents::increment(ProfileEvents::ParquetPrunedRowGroups);
+            row_group_batches_skipped_rows.back() += metadata->RowGroup(row_group)->num_rows();
             continue;
         }
 
         // When single-threaded parsing, can prefetch row groups, so need to put all row groups in the same row_group_batch
         if (row_group_batches.empty() || (!prefetch_group && row_group_batches.back().total_bytes_compressed >= min_bytes_for_seek))
+        {
             row_group_batches.emplace_back();
+            row_group_batches_skipped_rows.push_back(0);
+        }
 
         ProfileEvents::increment(ProfileEvents::ParquetReadRowGroups);
         row_group_batches.back().row_groups_idxs.push_back(row_group);
@@ -1055,6 +1063,7 @@ void ParquetBlockInputFormat::decodeOneChunk(size_t row_group_batch_idx, std::un
 
     lock.lock();
 
+    row_group_batch.chunk_sizes.push_back(res.chunk.getNumRows());
     ++row_group_batch.next_chunk_idx;
     ++row_group_batch.num_pending_chunks;
     pending_chunks.push(std::move(res));
@@ -1096,7 +1105,16 @@ Chunk ParquetBlockInputFormat::read()
         return {};
 
     if (need_only_count)
-        return getChunkForCount(row_group_batches[row_group_batches_completed++].total_rows);
+    {
+        auto chunk = getChunkForCount(row_group_batches[row_group_batches_completed].total_rows);
+        int total_rows_before = row_group_batches_skipped_rows[0];
+        for (size_t i = 0; i < row_group_batches_completed; ++i)
+            total_rows_before += row_group_batches[i].total_rows + row_group_batches_skipped_rows[i+1];
+
+        row_group_batches_completed++;
+        chunk.getChunkInfos().add(std::make_shared<ChunkInfoRowNumOffset>(total_rows_before));
+        return chunk;
+    }
 
     std::unique_lock lock(mutex);
 
@@ -1128,6 +1146,15 @@ Chunk ParquetBlockInputFormat::read()
 
             previous_block_missing_values = std::move(chunk.block_missing_values);
             previous_approx_bytes_read_for_chunk = chunk.approx_original_chunk_size;
+
+            int total_rows_before = row_group_batches_skipped_rows[0];
+            for (size_t i = 0; i < chunk.row_group_batch_idx; ++i)
+                total_rows_before += row_group_batches[i].total_rows + row_group_batches_skipped_rows[i+1];
+            for (size_t i = 0; i < chunk.chunk_idx; ++i)
+                total_rows_before += row_group.chunk_sizes[i];
+
+            chunk.chunk.getChunkInfos().add(std::make_shared<ChunkInfoRowNumOffset>(total_rows_before));
+
             return std::move(chunk.chunk);
         }
 
@@ -1154,6 +1181,7 @@ void ParquetBlockInputFormat::resetParser()
     metadata.reset();
     column_indices.clear();
     row_group_batches.clear();
+    row_group_batches_skipped_rows.clear();
     while (!pending_chunks.empty())
         pending_chunks.pop();
     row_group_batches_completed = 0;
diff --git a/src/Processors/Formats/Impl/ParquetBlockInputFormat.h b/src/Processors/Formats/Impl/ParquetBlockInputFormat.h
index 57397ee82bd1..2c50ec097b71 100644
--- a/src/Processors/Formats/Impl/ParquetBlockInputFormat.h
+++ b/src/Processors/Formats/Impl/ParquetBlockInputFormat.h
@@ -211,6 +211,7 @@ class ParquetBlockInputFormat : public IInputFormat
         //  (at most max_pending_chunks_per_row_group)
 
         size_t next_chunk_idx = 0;
+        std::vector<size_t> chunk_sizes;
         size_t num_pending_chunks = 0;
 
         size_t total_rows = 0;
@@ -319,7 +320,9 @@ class ParquetBlockInputFormat : public IInputFormat
     // Wakes up the read() call, if any.
     std::condition_variable condvar;
 
+    Block header;
     std::vector<RowGroupBatchState> row_group_batches;
+    std::vector<int> row_group_batches_skipped_rows;
     std::priority_queue<PendingChunk, std::vector<PendingChunk>, PendingChunk::Compare> pending_chunks;
     size_t row_group_batches_completed = 0;
 
diff --git a/src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h b/src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h
index 8f4928c038f2..537ab259fd19 100644
--- a/src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h
+++ b/src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h
@@ -115,6 +115,24 @@ class DataLakeConfiguration : public BaseStorageConfiguration, public std::enabl
         return current_metadata->getSchemaTransformer(local_context, data_path);
     }
 
+    bool hasPositionDeleteTransformer(const ObjectInfoPtr & object_info) const override
+    {
+        if (!current_metadata)
+            return false;
+        return current_metadata->hasPositionDeleteTransformer(object_info);
+    }
+
+    std::shared_ptr<ISimpleTransform> getPositionDeleteTransformer(
+        const ObjectInfoPtr & object_info,
+        const Block & header,
+        const std::optional<FormatSettings> & format_settings,
+        ContextPtr context_) const override
+    {
+        if (!current_metadata)
+            return {};
+        return current_metadata->getPositionDeleteTransformer(object_info, header, format_settings, context_);
+    }
+
     bool hasExternalDynamicMetadata() override
     {
         assertInitialized();
diff --git a/src/Storages/ObjectStorage/DataLakes/IDataLakeMetadata.h b/src/Storages/ObjectStorage/DataLakes/IDataLakeMetadata.h
index 987aabc8a7e7..98520bfa486f 100644
--- a/src/Storages/ObjectStorage/DataLakes/IDataLakeMetadata.h
+++ b/src/Storages/ObjectStorage/DataLakes/IDataLakeMetadata.h
@@ -1,8 +1,8 @@
 #pragma once
 #include <Core/NamesAndTypes.h>
 #include <Core/Types.h>
-#include <boost/noncopyable.hpp>
-#include "Interpreters/ActionsDAG.h"
+#include <Interpreters/ActionsDAG.h>
+#include <Processors/ISimpleTransform.h>
 #include <Storages/ObjectStorage/IObjectIterator.h>
 #include <Storages/prepareReadingFromFormat.h>
 
@@ -44,6 +44,16 @@ class IDataLakeMetadata : boost::noncopyable
     virtual std::shared_ptr<NamesAndTypesList> getInitialSchemaByPath(ContextPtr, const String & /* path */) const { return {}; }
     virtual std::shared_ptr<const ActionsDAG> getSchemaTransformer(ContextPtr, const String & /* path */) const { return {}; }
 
+    virtual bool hasPositionDeleteTransformer(const ObjectInfoPtr & /*object_info*/) const { return false; }
+    virtual std::shared_ptr<ISimpleTransform> getPositionDeleteTransformer(
+        const ObjectInfoPtr & /* object_info */,
+        const Block & /* header */,
+        const std::optional<FormatSettings> & /* format_settings */,
+        ContextPtr /*context*/) const
+    {
+        return {};
+    }
+
     /// Whether metadata is updateable (instead of recreation from scratch)
     /// to the latest version of table state in data lake.
     virtual bool supportsUpdate() const { return false; }
@@ -59,7 +69,7 @@ class IDataLakeMetadata : boost::noncopyable
     virtual std::optional<size_t> totalBytes(ContextPtr) const { return {}; }
 
 protected:
-    ObjectIterator createKeysIterator(
+    virtual ObjectIterator createKeysIterator(
         Strings && data_files_,
         ObjectStoragePtr object_storage_,
         IDataLakeMetadata::FileProgressCallback callback_) const;
diff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/Constant.h b/src/Storages/ObjectStorage/DataLakes/Iceberg/Constant.h
index ca9e7b8d4b7c..0444d92f8fc4 100644
--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/Constant.h
+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/Constant.h
@@ -39,7 +39,6 @@ DEFINE_ICEBERG_FIELD(time);
 DEFINE_ICEBERG_FIELD(timestamp);
 DEFINE_ICEBERG_FIELD(timestamptz);
 DEFINE_ICEBERG_FIELD(type)
-DEFINE_ICEBERG_FIELD(transform);
 DEFINE_ICEBERG_FIELD(uuid);
 DEFINE_ICEBERG_FIELD(value);
 /// These variables replace `-` with underscore `_` to be compatible with c++ code.
@@ -52,11 +51,14 @@ DEFINE_ICEBERG_FIELD_ALIAS(schema_id, schema-id);
 DEFINE_ICEBERG_FIELD_ALIAS(current_schema_id, current-schema-id);
 DEFINE_ICEBERG_FIELD_ALIAS(table_uuid, table-uuid);
 DEFINE_ICEBERG_FIELD_ALIAS(total_records, total-records);
+DEFINE_ICEBERG_FIELD_ALIAS(total_position_deletes, total-position-deletes);
 DEFINE_ICEBERG_FIELD_ALIAS(total_files_size, total-files-size);
 DEFINE_ICEBERG_FIELD_ALIAS(manifest_list, manifest-list);
 DEFINE_ICEBERG_FIELD_ALIAS(timestamp_ms, timestamp-ms);
 DEFINE_ICEBERG_FIELD_ALIAS(last_updated_ms, last-updated-ms);
 DEFINE_ICEBERG_FIELD_ALIAS(source_id, source-id);
+DEFINE_ICEBERG_FIELD_ALIAS(partition_transform, transform);
+DEFINE_ICEBERG_FIELD_ALIAS(partition_name, name);
 /// These are compound fields like `data_file.file_path`, we use prefix 'c_' to distinguish them.
 DEFINE_ICEBERG_FIELD_COMPOUND(data_file, file_path);
 DEFINE_ICEBERG_FIELD_COMPOUND(data_file, content);
@@ -66,4 +68,5 @@ DEFINE_ICEBERG_FIELD_COMPOUND(data_file, column_sizes);
 DEFINE_ICEBERG_FIELD_COMPOUND(data_file, null_value_counts);
 DEFINE_ICEBERG_FIELD_COMPOUND(data_file, lower_bounds);
 DEFINE_ICEBERG_FIELD_COMPOUND(data_file, upper_bounds);
+DEFINE_ICEBERG_FIELD_COMPOUND(data_file, referenced_data_file);
 }
diff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp b/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp
index 70e6f781c490..35e0d5f6ad81 100644
--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp
+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp
@@ -4,6 +4,7 @@
 
 #include <Core/Settings.h>
 #include <Core/NamesAndTypes.h>
+#include <Disks/ObjectStorages/StoredObject.h>
 #include <Formats/FormatFactory.h>
 #include <IO/ReadBufferFromFileBase.h>
 #include <IO/ReadBufferFromString.h>
@@ -13,7 +14,7 @@
 #include <Storages/ObjectStorage/DataLakes/Common.h>
 #include <Storages/ObjectStorage/StorageObjectStorageSource.h>
 #include <Storages/ObjectStorage/DataLakes/DataLakeStorageSettings.h>
-#include "Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadataFilesCache.h"
+#include <Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadataFilesCache.h>
 #include <Interpreters/ExpressionActions.h>
 #include <IO/CompressedReadBufferWrapper.h>
 
@@ -23,6 +24,7 @@
 #include <Storages/ObjectStorage/DataLakes/Iceberg/Snapshot.h>
 #include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFilesPruning.h>
 #include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h>
+#include <Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.h>
 #include <Storages/ObjectStorage/DataLakes/Iceberg/Constant.h>
 
 #include <Common/logger_useful.h>
@@ -51,6 +53,7 @@ extern const int FILE_DOESNT_EXIST;
 extern const int BAD_ARGUMENTS;
 extern const int LOGICAL_ERROR;
 extern const int ICEBERG_SPECIFICATION_VIOLATION;
+extern const int UNSUPPORTED_METHOD;
 }
 
 namespace Setting
@@ -143,7 +146,6 @@ Poco::JSON::Object::Ptr getMetadataJSONObject(
 
 }
 
-
 IcebergMetadata::IcebergMetadata(
     ObjectStoragePtr object_storage_,
     ConfigurationObserverPtr configuration_,
@@ -513,6 +515,7 @@ bool IcebergMetadata::update(const ContextPtr & local_context)
     if (previous_snapshot_id != relevant_snapshot_id)
     {
         cached_unprunned_files_for_last_processed_snapshot = std::nullopt;
+        cached_unprunned_position_deletes_files_for_last_processed_snapshot = std::nullopt;
         schema_id_by_data_file_initialized.store(false);
         return true;
     }
@@ -542,6 +545,7 @@ void IcebergMetadata::updateSnapshot(ContextPtr local_context, Poco::JSON::Objec
                     configuration_ptr->getPath());
             std::optional<size_t> total_rows;
             std::optional<size_t> total_bytes;
+            std::optional<size_t> total_position_deletes;
 
             if (snapshot->has(f_summary))
             {
@@ -551,12 +555,20 @@ void IcebergMetadata::updateSnapshot(ContextPtr local_context, Poco::JSON::Objec
 
                 if (summary_object->has(f_total_files_size))
                     total_bytes = summary_object->getValue<Int64>(f_total_files_size);
+
+                if (summary_object->has(f_total_position_deletes))
+                {
+                    total_position_deletes = summary_object->getValue<Int64>(f_total_position_deletes);
+                }
             }
 
             relevant_snapshot = IcebergSnapshot{
                 getManifestList(local_context, getProperFilePathFromMetadataInfo(
                     snapshot->getValue<String>(f_manifest_list), configuration_ptr->getPath(), table_location)),
-                relevant_snapshot_id, total_rows, total_bytes};
+                relevant_snapshot_id,
+                total_rows,
+                total_bytes,
+                total_position_deletes};
 
             if (!snapshot->has(f_schema_id))
                 throw Exception(
@@ -694,10 +706,9 @@ void IcebergMetadata::initializeSchemasFromManifestList(ContextPtr local_context
 
 void IcebergMetadata::initializeSchemasFromManifestFile(ManifestFilePtr manifest_file_ptr) const
 {
-    for (const auto & manifest_file_entry : manifest_file_ptr->getFiles())
+    for (const auto & manifest_file_entry : manifest_file_ptr->getFiles(FileContentType::DATA))
     {
-        if (std::holds_alternative<DataFileEntry>(manifest_file_entry.file))
-            schema_id_by_data_file.emplace(std::get<DataFileEntry>(manifest_file_entry.file).file_name, manifest_file_ptr->getSchemaId());
+        schema_id_by_data_file.emplace(manifest_file_entry.file_path, manifest_file_ptr->getSchemaId());
     }
 }
 
@@ -869,47 +880,67 @@ ManifestFilePtr IcebergMetadata::getManifestFile(ContextPtr local_context, const
     return create_fn();
 }
 
-Strings IcebergMetadata::getDataFiles(const ActionsDAG * filter_dag, ContextPtr local_context) const
+std::vector<Iceberg::ManifestFileEntry>
+IcebergMetadata::getFilesImpl(const ActionsDAG * filter_dag, FileContentType file_content_type, ContextPtr local_context) const
 {
     if (!relevant_snapshot)
         return {};
-
+    if (!local_context && filter_dag)
+    {
+        throw DB::Exception(
+            DB::ErrorCodes::LOGICAL_ERROR,
+            "Context is required with non-empty filter_dag to implement partition pruning for Iceberg table");
+    }
+    std::optional<std::vector<Iceberg::ManifestFileEntry>> & cached_files = (file_content_type == FileContentType::DATA)
+        ? cached_unprunned_files_for_last_processed_snapshot
+        : cached_unprunned_position_deletes_files_for_last_processed_snapshot;
     bool use_partition_pruning = filter_dag && local_context->getSettingsRef()[Setting::use_iceberg_partition_pruning];
+    if (!use_partition_pruning && cached_files.has_value())
+        return cached_files.value();
 
-    if (!use_partition_pruning && cached_unprunned_files_for_last_processed_snapshot.has_value())
-        return cached_unprunned_files_for_last_processed_snapshot.value();
-
-    Strings data_files;
+    std::vector<Iceberg::ManifestFileEntry> files;
     for (const auto & manifest_list_entry : relevant_snapshot->manifest_list_entries)
     {
         auto manifest_file_ptr = getManifestFile(local_context, manifest_list_entry.manifest_file_path, manifest_list_entry.added_sequence_number);
         initializeSchemasFromManifestFile(manifest_file_ptr);
-        ManifestFilesPruner pruner(
-            schema_processor, relevant_snapshot_schema_id,
-            use_partition_pruning ? filter_dag : nullptr,
-            *manifest_file_ptr, local_context);
-        const auto & data_files_in_manifest = manifest_file_ptr->getFiles();
-        for (const auto & manifest_file_entry : data_files_in_manifest)
+        const auto & files_in_manifest = manifest_file_ptr->getFiles(file_content_type);
+        for (const auto & manifest_file_entry : files_in_manifest)
         {
-            if (manifest_file_entry.status != ManifestEntryStatus::DELETED)
-            {
-                if (!pruner.canBePruned(manifest_file_entry))
+            ManifestFilesPruner pruner(
+                schema_processor, relevant_snapshot_schema_id,
+                use_partition_pruning ? filter_dag : nullptr,
+                *manifest_file_ptr, local_context);
+                if (manifest_file_entry.status != ManifestEntryStatus::DELETED)
                 {
-                    if (std::holds_alternative<DataFileEntry>(manifest_file_entry.file))
-                        data_files.push_back(std::get<DataFileEntry>(manifest_file_entry.file).file_name);
+                    if (!pruner.canBePruned(manifest_file_entry))
+                    {
+                        files.push_back(manifest_file_entry);
+                    }
                 }
-            }
         }
     }
 
+    std::sort(files.begin(), files.end());
+
     schema_id_by_data_file_initialized = true;
     if (!use_partition_pruning)
     {
-        cached_unprunned_files_for_last_processed_snapshot = data_files;
-        return cached_unprunned_files_for_last_processed_snapshot.value();
+        cached_files = files;
+        return cached_files.value();
     }
 
-    return data_files;
+    return files;
+}
+
+
+std::vector<Iceberg::ManifestFileEntry> IcebergMetadata::getDataFiles(const ActionsDAG * filter_dag, ContextPtr local_context) const
+{
+    return getFilesImpl(filter_dag, FileContentType::DATA, local_context);
+}
+
+std::vector<Iceberg::ManifestFileEntry> IcebergMetadata::getPositionalDeleteFiles(const ActionsDAG * filter_dag, ContextPtr local_context) const
+{
+    return getFilesImpl(filter_dag, FileContentType::POSITIONAL_DELETE, local_context);
 }
 
 std::optional<size_t> IcebergMetadata::totalRows(ContextPtr local_context) const
@@ -924,23 +955,25 @@ std::optional<size_t> IcebergMetadata::totalRows(ContextPtr local_context) const
         return 0;
     }
 
+
     /// All these "hints" with total rows or bytes are optional both in
     /// metadata files and in manifest files, so we try all of them one by one
-    if (relevant_snapshot->total_rows.has_value())
+    if (relevant_snapshot->getTotalRows())
     {
         ProfileEvents::increment(ProfileEvents::IcebergTrivialCountOptimizationApplied);
-        return relevant_snapshot->total_rows;
+        return relevant_snapshot->getTotalRows();
     }
 
     Int64 result = 0;
     for (const auto & manifest_list_entry : relevant_snapshot->manifest_list_entries)
     {
         auto manifest_file_ptr = getManifestFile(local_context, manifest_list_entry.manifest_file_path, manifest_list_entry.added_sequence_number);
-        auto count = manifest_file_ptr->getRowsCountInAllDataFilesExcludingDeleted();
-        if (!count.has_value())
+        auto data_count = manifest_file_ptr->getRowsCountInAllFilesExcludingDeleted(FileContentType::DATA);
+        auto position_deletes_count = manifest_file_ptr->getRowsCountInAllFilesExcludingDeleted(FileContentType::POSITIONAL_DELETE);
+        if (!data_count.has_value() || !position_deletes_count.has_value())
             return {};
 
-        result += count.value();
+        result += data_count.value() - position_deletes_count.value();
     }
 
     ProfileEvents::increment(ProfileEvents::IcebergTrivialCountOptimizationApplied);
@@ -982,9 +1015,119 @@ ObjectIterator IcebergMetadata::iterate(
     size_t /* list_batch_size */,
     ContextPtr local_context) const
 {
-    return createKeysIterator(getDataFiles(filter_dag, local_context), object_storage, callback);
+    return std::make_shared<IcebergKeysIterator>(*this, getDataFiles(filter_dag, local_context), getPositionalDeleteFiles(filter_dag, local_context), object_storage, callback);
+}
+
+bool IcebergMetadata::hasPositionDeleteTransformer(const ObjectInfoPtr & object_info) const
+{
+    auto iceberg_object_info = std::dynamic_pointer_cast<IcebergDataObjectInfo>(object_info);
+    if (!iceberg_object_info)
+        return false;
+
+    return !iceberg_object_info->position_deletes_objects.empty();
+}
+
+std::shared_ptr<ISimpleTransform> IcebergMetadata::getPositionDeleteTransformer(
+    const ObjectInfoPtr & object_info,
+    const Block & header,
+    const std::optional<FormatSettings> & format_settings,
+    ContextPtr context_) const
+{
+    auto iceberg_object_info = std::dynamic_pointer_cast<IcebergDataObjectInfo>(object_info);
+    if (!iceberg_object_info)
+        throw Exception(ErrorCodes::LOGICAL_ERROR, "The object info is not IcebergDataObjectInfo");
+
+    auto configuration_ptr = configuration.lock();
+    if (!configuration_ptr)
+    {
+        throw DB::Exception(ErrorCodes::LOGICAL_ERROR, "Iceberg configuration has expired");
+    }
+
+    String delete_object_format = configuration_ptr->format;
+    String delete_object_compression_method = configuration_ptr->compression_method;
+
+    return std::make_shared<IcebergBitmapPositionDeleteTransform>(
+        header, iceberg_object_info, object_storage, format_settings, context_, delete_object_format, delete_object_compression_method);
+}
+
+
+IcebergDataObjectInfo::IcebergDataObjectInfo(
+    const IcebergMetadata & iceberg_metadata,
+    Iceberg::ManifestFileEntry data_object_,
+    std::optional<ObjectMetadata> metadata_,
+    const std::vector<Iceberg::ManifestFileEntry> & position_deletes_objects_)
+    : RelativePathWithMetadata(data_object_.file_path, std::move(metadata_))
+    , data_object(data_object_)
+{
+    ///Object in position_deletes_objects_ are sorted by common_partition_specification, partition_key_value and added_sequence_number.
+    /// It is done to have an invariant that position deletes objects which corresponds
+    /// to the data object form a subsegment in a position_deletes_objects_ vector.
+    /// We need to take all position deletes objects which has the same partition schema and value and has added_sequence_number
+    /// greater than or equal to the data object added_sequence_number (https://iceberg.apache.org/spec/#scan-planning)
+    /// ManifestFileEntry has comparator by default which helps to do that.
+    auto beg_it = std::lower_bound(position_deletes_objects_.begin(), position_deletes_objects_.end(), data_object_);
+    auto end_it = std::upper_bound(
+        position_deletes_objects_.begin(),
+        position_deletes_objects_.end(),
+        data_object_,
+        [](const Iceberg::ManifestFileEntry & lhs, const Iceberg::ManifestFileEntry & rhs)
+        {
+            return std::tie(lhs.common_partition_specification, lhs.partition_key_value)
+                < std::tie(rhs.common_partition_specification, rhs.partition_key_value);
+        });
+    if (beg_it - position_deletes_objects_.begin() > end_it - position_deletes_objects_.begin())
+    {
+        throw Exception(
+            ErrorCodes::LOGICAL_ERROR,
+            "Position deletes objects are not sorted by common_partition_specification and partition_key_value, "
+            "beginning: {}, end: {}, position_deletes_objects size: {}",
+            beg_it - position_deletes_objects_.begin(),
+            end_it - position_deletes_objects_.begin(),
+            position_deletes_objects_.size());
+    }
+    position_deletes_objects = std::span<const Iceberg::ManifestFileEntry>{beg_it, end_it};
+    if (!position_deletes_objects.empty() && iceberg_metadata.configuration.lock()->format != "Parquet")
+    {
+        throw Exception(
+            ErrorCodes::UNSUPPORTED_METHOD,
+            "Position deletes are only supported for data files of Parquet format in Iceberg, but got {}",
+            iceberg_metadata.configuration.lock()->format);
+    }
+};
+
+IcebergKeysIterator::IcebergKeysIterator(
+    const IcebergMetadata & iceberg_metadata_,
+    std::vector<Iceberg::ManifestFileEntry> && data_files_,
+    std::vector<Iceberg::ManifestFileEntry> && position_deletes_files_,
+    ObjectStoragePtr object_storage_,
+    IDataLakeMetadata::FileProgressCallback callback_)
+    : iceberg_metadata(iceberg_metadata_)
+    , data_files(data_files_)
+    , position_deletes_files(position_deletes_files_)
+    , object_storage(object_storage_)
+    , callback(callback_)
+{
 }
 
+
+ObjectInfoPtr IcebergKeysIterator::next(size_t)
+{
+    while (true)
+    {
+        size_t current_index = index.fetch_add(1, std::memory_order_relaxed);
+        if (current_index >= data_files.size())
+            return nullptr;
+
+        auto key = data_files[current_index].file_path;
+        auto object_metadata = object_storage->getObjectMetadata(key);
+
+        if (callback)
+            callback(FileProgress(0, object_metadata.size_bytes));
+
+        return std::make_shared<IcebergDataObjectInfo>(
+            iceberg_metadata, data_files[current_index], std::move(object_metadata), position_deletes_files);
+    }
+}
 }
 
 #endif
diff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h b/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h
index 4108e41f614b..13033911edd6 100644
--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h
+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h
@@ -3,23 +3,19 @@
 
 #if USE_AVRO
 
-#include <Core/Types.h>
-#include <Disks/ObjectStorages/IObjectStorage.h>
-#include <Interpreters/Context_fwd.h>
-#include <Storages/ObjectStorage/DataLakes/IDataLakeMetadata.h>
-#include <Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadataFilesCache.h>
-#include <Storages/ObjectStorage/StorageObjectStorage.h>
-
 #include <Poco/JSON/Array.h>
 #include <Poco/JSON/Object.h>
 #include <Poco/JSON/Parser.h>
 
+#include <Core/Types.h>
+#include <Disks/ObjectStorages/IObjectStorage.h>
+#include <Interpreters/Context_fwd.h>
 #include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h>
 #include <Storages/ObjectStorage/DataLakes/Iceberg/SchemaProcessor.h>
 #include <Storages/ObjectStorage/DataLakes/Iceberg/Snapshot.h>
-
-#include <tuple>
-
+#include <Storages/ObjectStorage/DataLakes/IDataLakeMetadata.h>
+#include <Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadataFilesCache.h>
+#include <Storages/ObjectStorage/StorageObjectStorage.h>
 
 namespace DB
 {
@@ -73,6 +69,14 @@ class IcebergMetadata : public IDataLakeMetadata
             : nullptr;
     }
 
+    bool hasPositionDeleteTransformer(const ObjectInfoPtr & object_info) const override;
+
+    std::shared_ptr<ISimpleTransform> getPositionDeleteTransformer(
+        const ObjectInfoPtr & /* object_info */,
+        const Block & /* header */,
+        const std::optional<FormatSettings> & /* format_settings */,
+        ContextPtr /* context */) const override;
+
     bool supportsSchemaEvolution() const override { return true; }
 
     static Int32
@@ -87,6 +91,10 @@ class IcebergMetadata : public IDataLakeMetadata
     std::optional<size_t> totalRows(ContextPtr Local_context) const override;
     std::optional<size_t> totalBytes(ContextPtr Local_context) const override;
 
+
+    friend class IcebergKeysIterator;
+    friend struct IcebergDataObjectInfo;
+
 protected:
     ObjectIterator iterate(
         const ActionsDAG * filter_dag,
@@ -117,11 +125,14 @@ class IcebergMetadata : public IDataLakeMetadata
     Int64 relevant_snapshot_id{-1};
     String table_location;
 
-    mutable std::optional<Strings> cached_unprunned_files_for_last_processed_snapshot;
+    mutable std::optional<std::vector<Iceberg::ManifestFileEntry>> cached_unprunned_files_for_last_processed_snapshot;
+    mutable std::optional<std::vector<Iceberg::ManifestFileEntry>> cached_unprunned_position_deletes_files_for_last_processed_snapshot;
 
     void updateState(const ContextPtr & local_context, Poco::JSON::Object::Ptr metadata_object, bool metadata_file_changed);
 
-    Strings getDataFiles(const ActionsDAG * filter_dag, ContextPtr local_context) const;
+    std::vector<Iceberg::ManifestFileEntry> getDataFiles(const ActionsDAG * filter_dag, ContextPtr local_context) const;
+
+    std::vector<Iceberg::ManifestFileEntry> getPositionalDeleteFiles(const ActionsDAG * filter_dag, ContextPtr local_context) const;
 
     void updateSnapshot(ContextPtr local_context, Poco::JSON::Object::Ptr metadata_object);
 
@@ -141,7 +152,52 @@ class IcebergMetadata : public IDataLakeMetadata
     std::optional<String> getRelevantManifestList(const Poco::JSON::Object::Ptr & metadata);
 
     Iceberg::ManifestFilePtr tryGetManifestFile(const String & filename) const;
+
+    std::vector<Iceberg::ManifestFileEntry> getFilesImpl(const ActionsDAG * filter_dag, Iceberg::FileContentType file_content_type, ContextPtr local_context) const;
+};
+
+struct IcebergDataObjectInfo : public RelativePathWithMetadata
+{
+    explicit IcebergDataObjectInfo(
+        const IcebergMetadata & iceberg_metadata,
+        Iceberg::ManifestFileEntry data_object_,
+        std::optional<ObjectMetadata> metadata_ = std::nullopt,
+        const std::vector<Iceberg::ManifestFileEntry> & position_deletes_objects_ = {});
+
+    const Iceberg::ManifestFileEntry data_object;
+    std::span<const Iceberg::ManifestFileEntry> position_deletes_objects;
+
+    // Return the path in the Iceberg metadata
+    std::string getIcebergDataPath() const { return data_object.file_path_key; }
 };
+using IcebergDataObjectInfoPtr = std::shared_ptr<IcebergDataObjectInfo>;
+
+class IcebergKeysIterator : public IObjectIterator
+{
+public:
+    IcebergKeysIterator(
+        const IcebergMetadata & iceberg_metadata_,
+        std::vector<Iceberg::ManifestFileEntry>&& data_files_,
+        std::vector<Iceberg::ManifestFileEntry>&& position_deletes_files_,
+        ObjectStoragePtr object_storage_,
+        IDataLakeMetadata::FileProgressCallback callback_);
+
+    size_t estimatedKeysCount() override
+    {
+        return data_files.size();
+    }
+
+    ObjectInfoPtr next(size_t) override;
+
+private:
+    const IcebergMetadata & iceberg_metadata;
+    std::vector<Iceberg::ManifestFileEntry> data_files;
+    std::vector<Iceberg::ManifestFileEntry> position_deletes_files;
+    ObjectStoragePtr object_storage;
+    std::atomic<size_t> index = 0;
+    IDataLakeMetadata::FileProgressCallback callback;
+};
+
 }
 
 #endif
diff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp
index a295071f2600..0fe83092735f 100644
--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp
+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp
@@ -2,6 +2,8 @@
 
 #if USE_AVRO
 
+#include <compare>
+
 #include <Storages/ObjectStorage/DataLakes/Iceberg/Constant.h>
 #include <Storages/ObjectStorage/DataLakes/Iceberg/Utils.h>
 #include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h>
@@ -26,6 +28,19 @@ namespace DB::ErrorCodes
 namespace Iceberg
 {
 
+String FileContentTypeToString(FileContentType type)
+{
+    switch (type)
+    {
+        case FileContentType::DATA:
+            return "data";
+        case FileContentType::POSITIONAL_DELETE:
+            return "position_deletes";
+        case FileContentType::EQUALITY_DELETE:
+            return "equality_deletes";
+    }
+}
+
 namespace
 {
     /// Iceberg stores lower_bounds and upper_bounds serialized with some custom deserialization as bytes array
@@ -106,11 +121,17 @@ namespace
 
 }
 
-const std::vector<ManifestFileEntry> & ManifestFileContent::getFiles() const
+const std::vector<ManifestFileEntry> & ManifestFileContent::getFiles(FileContentType content_type) const
 {
-    return files;
+    if (content_type == FileContentType::DATA)
+        return data_files;
+    else if (content_type == FileContentType::POSITIONAL_DELETE)
+        return position_deletes_files;
+    else
+        throw DB::Exception(DB::ErrorCodes::UNSUPPORTED_METHOD, "Unsupported content type: {}", static_cast<int>(content_type));
 }
 
+
 Int32 ManifestFileContent::getSchemaId() const
 {
     return schema_id;
@@ -167,7 +188,10 @@ ManifestFileContent::ManifestFileContent(
         /// we use column internal number as it's name.
         auto numeric_column_name = DB::backQuote(DB::toString(source_id));
         DB::NameAndTypePair manifest_file_column_characteristics = schema_processor.getFieldCharacteristics(schema_id, source_id);
-        auto partition_ast = getASTFromTransform(partition_specification_field->getValue<String>(f_transform), numeric_column_name);
+        auto transform_name = partition_specification_field->getValue<String>(f_partition_transform);
+        auto partition_name = partition_specification_field->getValue<String>(f_partition_name);
+        common_partition_specification.emplace_back(source_id, transform_name, partition_name);
+        auto partition_ast = getASTFromTransform(transform_name, numeric_column_name);
         /// Unsupported partition key expression
         if (partition_ast == nullptr)
             continue;
@@ -185,12 +209,13 @@ ManifestFileContent::ManifestFileContent(
         if (format_version_ > 1)
         {
             content_type = FileContentType(manifest_file_deserializer.getValueFromRowByName(i, c_data_file_content, TypeIndex::Int32).safeGet<UInt64>());
-            if (content_type != FileContentType::DATA)
+            if (content_type == FileContentType::EQUALITY_DELETE)
                 throw Exception(
-                    ErrorCodes::UNSUPPORTED_METHOD, "Cannot read Iceberg table: positional and equality deletes are not supported");
+                    ErrorCodes::UNSUPPORTED_METHOD, "Cannot read Iceberg table: files of content type {} are not supported", content_type);
         }
         const auto status = ManifestEntryStatus(manifest_file_deserializer.getValueFromRowByName(i, f_status, TypeIndex::Int32).safeGet<UInt64>());
 
+        const auto file_path_key = manifest_file_deserializer.getValueFromRowByName(i, c_data_file_file_path, TypeIndex::String).safeGet<String>();
         const auto file_path = getProperFilePathFromMetadataInfo(manifest_file_deserializer.getValueFromRowByName(i, c_data_file_file_path, TypeIndex::String).safeGet<String>(), common_path, table_location);
 
         /// NOTE: This is weird, because in manifest file partition looks like this:
@@ -234,48 +259,49 @@ ManifestFileContent::ManifestFileContent(
             }
         }
 
-        std::unordered_map<Int32, std::pair<Field, Field>> value_for_bounds;
-        for (const auto & path : {c_data_file_lower_bounds, c_data_file_upper_bounds})
+        if (content_type == FileContentType::DATA)
         {
-            if (manifest_file_deserializer.hasPath(path))
+            std::unordered_map<Int32, std::pair<Field, Field>> value_for_bounds;
+            for (const auto & path : {c_data_file_lower_bounds, c_data_file_upper_bounds})
             {
-                Field bounds = manifest_file_deserializer.getValueFromRowByName(i, path);
-                for (const auto & column_stats : bounds.safeGet<Array>())
+                if (manifest_file_deserializer.hasPath(path))
                 {
-                    const auto & column_number_and_bound = column_stats.safeGet<Tuple>();
-                    Int32 number = column_number_and_bound[0].safeGet<Int32>();
-                    const Field & bound_value = column_number_and_bound[1];
-
-                    if (path == c_data_file_lower_bounds)
-                        value_for_bounds[number].first = bound_value;
-                    else
-                        value_for_bounds[number].second = bound_value;
-
-                    column_ids_which_have_bounds.insert(number);
+                    Field bounds = manifest_file_deserializer.getValueFromRowByName(i, path);
+                    for (const auto & column_stats : bounds.safeGet<Array>())
+                    {
+                        const auto & column_number_and_bound = column_stats.safeGet<Tuple>();
+                        Int32 number = column_number_and_bound[0].safeGet<Int32>();
+                        const Field & bound_value = column_number_and_bound[1];
+
+                        if (path == c_data_file_lower_bounds)
+                            value_for_bounds[number].first = bound_value;
+                        else
+                            value_for_bounds[number].second = bound_value;
+
+                        column_ids_which_have_bounds.insert(number);
+                    }
                 }
             }
-        }
 
-        for (const auto & [column_id, bounds] : value_for_bounds)
-        {
-            DB::NameAndTypePair name_and_type = schema_processor.getFieldCharacteristics(schema_id, column_id);
+            for (const auto & [column_id, bounds] : value_for_bounds)
+            {
+                DB::NameAndTypePair name_and_type = schema_processor.getFieldCharacteristics(schema_id, column_id);
 
-            String left_str;
-            String right_str;
-            /// lower_bound and upper_bound may be NULL.
-            if (!bounds.first.tryGet(left_str) || !bounds.second.tryGet(right_str))
-                continue;
+                String left_str;
+                String right_str;
+                /// lower_bound and upper_bound may be NULL.
+                if (!bounds.first.tryGet(left_str) || !bounds.second.tryGet(right_str))
+                    continue;
 
-            auto left = deserializeFieldFromBinaryRepr(left_str, name_and_type.type, true);
-            auto right = deserializeFieldFromBinaryRepr(right_str, name_and_type.type, false);
-            if (!left || !right)
-                continue;
+                auto left = deserializeFieldFromBinaryRepr(left_str, name_and_type.type, true);
+                auto right = deserializeFieldFromBinaryRepr(right_str, name_and_type.type, false);
+                if (!left || !right)
+                    continue;
 
-            columns_infos[column_id].hyperrectangle.emplace(*left, true, *right, true);
+                columns_infos[column_id].hyperrectangle.emplace(*left, true, *right, true);
+            }
         }
 
-        FileEntry file = FileEntry{DataFileEntry{file_path}};
-
         Int64 added_sequence_number = 0;
         if (format_version_ > 1)
         {
@@ -300,7 +326,43 @@ ManifestFileContent::ManifestFileContent(
                     break;
             }
         }
-        this->files.emplace_back(status, added_sequence_number, file, partition_key_value, columns_infos);
+        switch (content_type)
+        {
+            case FileContentType::DATA:
+                this->data_files.emplace_back(
+                    file_path_key,
+                    file_path,
+                    status,
+                    added_sequence_number,
+                    partition_key_value,
+                    common_partition_specification,
+                    columns_infos,
+                    /*reference_data_file = */ std::nullopt);
+                break;
+            case FileContentType::POSITIONAL_DELETE: {
+                /// reference_file_path can be absent in schema for some reason, though it is present in specification: https://iceberg.apache.org/spec/#manifests
+                std::optional<String> reference_file_path = std::nullopt;
+                if (manifest_file_deserializer.hasPath(c_data_file_referenced_data_file))
+                {
+                    reference_file_path
+                        = manifest_file_deserializer.getValueFromRowByName(i, c_data_file_referenced_data_file, TypeIndex::String)
+                              .safeGet<String>();
+                }
+                this->position_deletes_files.emplace_back(
+                    file_path_key,
+                    file_path,
+                    status,
+                    added_sequence_number,
+                    partition_key_value,
+                    common_partition_specification,
+                    columns_infos,
+                    reference_file_path);
+                break;
+            }
+            default:
+                throw Exception(
+                    ErrorCodes::UNSUPPORTED_METHOD, "FileContentType {} is not supported", content_type);
+        }
     }
 }
 
@@ -332,14 +394,16 @@ size_t ManifestFileContent::getSizeInMemory() const
     if (partition_key_description)
         total_size += sizeof(DB::KeyDescription);
     total_size += column_ids_which_have_bounds.size() * sizeof(Int32);
-    total_size += files.capacity() * sizeof(ManifestFileEntry);
+    total_size += data_files.capacity() * sizeof(ManifestFileEntry);
+    total_size += position_deletes_files.capacity() * sizeof(ManifestFileEntry);
     return total_size;
 }
 
-std::optional<Int64> ManifestFileContent::getRowsCountInAllDataFilesExcludingDeleted() const
+std::optional<Int64> ManifestFileContent::getRowsCountInAllFilesExcludingDeleted(FileContentType content) const
 {
     Int64 result = 0;
-    for (const auto & file : files)
+
+    for (const auto & file : getFiles(content))
     {
         /// Have at least one column with rows count
         bool found = false;
@@ -353,17 +417,17 @@ std::optional<Int64> ManifestFileContent::getRowsCountInAllDataFilesExcludingDel
                 break;
             }
         }
-
         if (!found)
             return std::nullopt;
     }
     return result;
 }
 
+
 std::optional<Int64> ManifestFileContent::getBytesCountInAllDataFiles() const
 {
     Int64 result = 0;
-    for (const auto & file : files)
+    for (const auto & file : data_files)
     {
         /// Have at least one column with bytes count
         bool found = false;
@@ -383,6 +447,35 @@ std::optional<Int64> ManifestFileContent::getBytesCountInAllDataFiles() const
     return result;
 }
 
+std::strong_ordering operator<=>(const PartitionSpecsEntry & lhs, const PartitionSpecsEntry & rhs)
+{
+    return std::tie(lhs.source_id, lhs.transform_name, lhs.partition_name)
+        <=> std::tie(rhs.source_id, rhs.transform_name, rhs.partition_name);
+}
+
+template <typename A>
+bool less(const std::vector<A> & lhs, const std::vector<A> & rhs)
+{
+    if (lhs.size() != rhs.size())
+        return lhs.size() < rhs.size();
+    return std::lexicographical_compare(lhs.begin(), lhs.end(), rhs.begin(), rhs.end(), [](const A & a, const A & b) { return a < b; });
+}
+
+bool operator<(const PartitionSpecification & lhs, const PartitionSpecification & rhs)
+{
+    return less(lhs, rhs);
+}
+
+bool operator<(const DB::Row & lhs, const DB::Row & rhs)
+{
+    return less(lhs, rhs);
+}
+
+std::weak_ordering operator<=>(const ManifestFileEntry & lhs, const ManifestFileEntry & rhs)
+{
+    return std::tie(lhs.common_partition_specification, lhs.partition_key_value, lhs.added_sequence_number)
+        <=> std::tie(rhs.common_partition_specification, rhs.partition_key_value, rhs.added_sequence_number);
+}
 }
 
 #endif
diff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h
index 0fc613c65946..ce82f970cb40 100644
--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h
+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h
@@ -26,14 +26,11 @@ enum class ManifestEntryStatus : uint8_t
 enum class FileContentType : uint8_t
 {
     DATA = 0,
-    POSITION_DELETES = 1,
-    EQUALITY_DELETES = 2,
+    POSITIONAL_DELETE = 1,
+    EQUALITY_DELETE = 2
 };
 
-struct DataFileEntry
-{
-    String file_name;
-};
+String FileContentTypeToString(FileContentType type);
 
 struct ColumnInfo
 {
@@ -43,17 +40,30 @@ struct ColumnInfo
     std::optional<DB::Range> hyperrectangle;
 };
 
-using FileEntry = std::variant<DataFileEntry>; // In the future we will add PositionalDeleteFileEntry and EqualityDeleteFileEntry here
+struct PartitionSpecsEntry
+{
+    Int32 source_id;
+    String transform_name;
+    String partition_name;
+};
+using PartitionSpecification = std::vector<PartitionSpecsEntry>;
 
 /// Description of Data file in manifest file
 struct ManifestFileEntry
 {
+    // It's the original string in the Iceberg metadata
+    String file_path_key;
+    // It's a processed file path to be used by Object Storage
+    String file_path;
+
     ManifestEntryStatus status;
     Int64 added_sequence_number;
 
-    FileEntry file;
     DB::Row partition_key_value;
+    PartitionSpecification common_partition_specification;
     std::unordered_map<Int32, ColumnInfo> columns_infos;
+
+    std::optional<String> reference_data_file_path; // For position delete files only.
 };
 
 /**
@@ -82,7 +92,7 @@ struct ManifestFileEntry
  * └────────┴─────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
  */
 
-class ManifestFileContent
+class ManifestFileContent : public boost::noncopyable
 {
 public:
     explicit ManifestFileContent(
@@ -96,7 +106,7 @@ class ManifestFileContent
         const std::string & table_location,
         DB::ContextPtr context);
 
-    const std::vector<ManifestFileEntry> & getFiles() const;
+    const std::vector<ManifestFileEntry> & getFiles(FileContentType content_type) const;
     Int32 getSchemaId() const;
 
     bool hasPartitionKey() const;
@@ -108,18 +118,25 @@ class ManifestFileContent
 
     /// Fields with rows count in manifest files are optional
     /// they can be absent.
-    std::optional<Int64> getRowsCountInAllDataFilesExcludingDeleted() const;
+    std::optional<Int64> getRowsCountInAllFilesExcludingDeleted(FileContentType content) const;
     std::optional<Int64> getBytesCountInAllDataFiles() const;
 
     bool hasBoundsInfoInManifests() const;
     const std::set<Int32> & getColumnsIDsWithBounds() const;
+
+    ManifestFileContent(ManifestFileContent &&) = delete;
+    ManifestFileContent & operator=(ManifestFileContent &&) = delete;
+
 private:
 
     Int32 schema_id;
     Poco::JSON::Object::Ptr schema_object;
+    PartitionSpecification common_partition_specification;
     std::optional<DB::KeyDescription> partition_key_description;
     // Size - number of files
-    std::vector<ManifestFileEntry> files;
+    std::vector<ManifestFileEntry> data_files;
+    // Partition level deletes files
+    std::vector<ManifestFileEntry> position_deletes_files;
 
     std::set<Int32> column_ids_which_have_bounds;
 
@@ -127,6 +144,11 @@ class ManifestFileContent
 
 using ManifestFilePtr = std::shared_ptr<const ManifestFileContent>;
 
+bool operator<(const PartitionSpecification & lhs, const PartitionSpecification & rhs);
+bool operator<(const DB::Row & lhs, const DB::Row & rhs);
+
+
+std::weak_ordering operator<=>(const ManifestFileEntry & lhs, const ManifestFileEntry & rhs);
 }
 
 #endif
diff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.cpp b/src/Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.cpp
new file mode 100644
index 000000000000..d9c75b372fb5
--- /dev/null
+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.cpp
@@ -0,0 +1,161 @@
+#include "config.h"
+
+#if USE_AVRO
+
+#include <Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.h>
+
+#include <Core/Settings.h>
+#include <Formats/FormatFactory.h>
+#include <Formats/ReadSchemaUtils.h>
+#include <IO/CompressionMethod.h>
+#include <IO/ReadBufferFromFileBase.h>
+#include <Interpreters/Context.h>
+#include <Interpreters/ExpressionAnalyzer.h>
+#include <Parsers/ASTFunction.h>
+#include <Parsers/ASTIdentifier.h>
+#include <Parsers/ASTLiteral.h>
+#include <Processors/Formats/ISchemaReader.h>
+#include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h>
+#include <Storages/ObjectStorage/StorageObjectStorageSource.h>
+
+namespace DB
+{
+
+namespace Setting
+{
+extern const SettingsNonZeroUInt64 max_block_size;
+}
+
+namespace ErrorCodes
+{
+extern const int LOGICAL_ERROR;
+}
+
+void IcebergPositionDeleteTransform::initializeDeleteSources()
+{
+    /// Create filter on the data object to get interested rows
+    auto iceberg_data_path = iceberg_object_info->getIcebergDataPath();
+    ASTPtr where_ast = makeASTFunction(
+        "equals",
+        std::make_shared<ASTIdentifier>(IcebergPositionDeleteTransform::data_file_path_column_name),
+        std::make_shared<ASTLiteral>(Field(iceberg_data_path)));
+
+    for (const auto & position_deletes_object : iceberg_object_info->position_deletes_objects)
+    {
+        /// Skip position deletes that do not match the data file path.
+        if (position_deletes_object.reference_data_file_path.has_value()
+            && position_deletes_object.reference_data_file_path != iceberg_data_path)
+            continue;
+
+        auto object_path = position_deletes_object.file_path;
+        auto object_metadata = object_storage->getObjectMetadata(object_path);
+        auto object_info = std::make_shared<ObjectInfo>(object_path, object_metadata);
+
+
+        Block initial_header;
+        {
+            std::unique_ptr<ReadBuffer> read_buf_schema
+                = StorageObjectStorageSource::createReadBuffer(*object_info, object_storage, context, log);
+            auto schema_reader = FormatFactory::instance().getSchemaReader(delete_object_format, *read_buf_schema, context);
+            auto columns_with_names = schema_reader->readSchema();
+            ColumnsWithTypeAndName initial_header_data;
+            for (const auto & elem : columns_with_names)
+            {
+                initial_header_data.push_back(ColumnWithTypeAndName(elem.type, elem.name));
+            }
+            initial_header = Block(initial_header_data);
+        }
+
+        CompressionMethod compression_method = chooseCompressionMethod(object_path, delete_object_compression_method);
+
+        delete_read_buffers.push_back(StorageObjectStorageSource::createReadBuffer(*object_info, object_storage, context, log));
+
+        auto syntax_result = TreeRewriter(context).analyze(where_ast, initial_header.getNamesAndTypesList());
+        ExpressionAnalyzer analyzer(where_ast, syntax_result, context);
+        std::optional<ActionsDAG> actions = analyzer.getActionsDAG(true);
+        std::shared_ptr<const ActionsDAG> actions_dag_ptr = [&actions]()
+        {
+            if (actions.has_value())
+                return std::make_shared<const ActionsDAG>(std::move(actions.value()));
+            return std::shared_ptr<const ActionsDAG>();
+        }();
+
+        auto delete_format = FormatFactory::instance().getInput(
+            delete_object_format,
+            *delete_read_buffers.back(),
+            initial_header,
+            context,
+            context->getSettingsRef()[DB::Setting::max_block_size],
+            format_settings,
+            std::make_shared<FormatParserGroup>(context->getSettingsRef(), 1, actions_dag_ptr, context),
+            true /* is_remote_fs */,
+            compression_method);
+
+        delete_sources.push_back(std::move(delete_format));
+    }
+}
+
+size_t IcebergPositionDeleteTransform::getColumnIndex(const std::shared_ptr<IInputFormat> & delete_source, const String & column_name)
+{
+    const auto & delete_header = delete_source->getOutputs().back().getHeader();
+    for (size_t i = 0; i < delete_header.getNames().size(); ++i)
+    {
+        if (delete_header.getNames()[i] == column_name)
+        {
+            return i;
+        }
+    }
+    throw Exception(ErrorCodes::LOGICAL_ERROR, "Could not find column {} in chunk", column_name);
+}
+
+void IcebergBitmapPositionDeleteTransform::transform(Chunk & chunk)
+{
+    size_t num_rows = chunk.getNumRows();
+    IColumn::Filter delete_vector(num_rows, true);
+    size_t num_rows_after_filtration = num_rows;
+
+    auto chunk_info = chunk.getChunkInfos().get<ChunkInfoRowNumOffset>();
+    if (!chunk_info)
+        throw Exception(ErrorCodes::LOGICAL_ERROR, "ChunkInfoRowNumOffset does not exist");
+
+    size_t row_num_offset = chunk_info->row_num_offset;
+    for (size_t i = 0; i < num_rows; i++)
+    {
+        size_t row_idx = row_num_offset + i;
+        if (bitmap.rb_contains(row_idx))
+        {
+            delete_vector[i] = false;
+            num_rows_after_filtration--;
+        }
+    }
+
+    auto columns = chunk.detachColumns();
+    for (auto & column : columns)
+        column = column->filter(delete_vector, -1);
+
+    chunk.setColumns(std::move(columns), num_rows_after_filtration);
+}
+
+void IcebergBitmapPositionDeleteTransform::initialize()
+{
+    for (auto & delete_source : delete_sources)
+    {
+        while (auto delete_chunk = delete_source->read())
+        {
+            int position_index = getColumnIndex(delete_source, IcebergPositionDeleteTransform::positions_column_name);
+            int filename_index = getColumnIndex(delete_source, IcebergPositionDeleteTransform::data_file_path_column_name);
+
+            auto position_column = delete_chunk.getColumns()[position_index];
+            auto filename_column = delete_chunk.getColumns()[filename_index];
+
+            for (size_t i = 0; i < delete_chunk.getNumRows(); ++i)
+            {
+                auto position_to_delete = position_column->get64(i);
+                bitmap.add(position_to_delete);
+            }
+        }
+    }
+}
+}
+
+#endif
diff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.h b/src/Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.h
new file mode 100644
index 000000000000..633c1cecae23
--- /dev/null
+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.h
@@ -0,0 +1,95 @@
+#pragma once
+#include "config.h"
+
+#if USE_AVRO
+
+#include <AggregateFunctions/AggregateFunctionGroupBitmapData.h>
+#include <Processors/ISimpleTransform.h>
+#include <Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h>
+
+namespace DB
+{
+
+class IcebergPositionDeleteTransform : public ISimpleTransform
+{
+public:
+    static constexpr const char * positions_column_name = "pos";
+    static constexpr const char * data_file_path_column_name = "file_path";
+
+    IcebergPositionDeleteTransform(
+        const Block & header_,
+        IcebergDataObjectInfoPtr iceberg_object_info_,
+        const ObjectStoragePtr object_storage_,
+        const std::optional<FormatSettings> & format_settings_,
+        ContextPtr context_,
+        String delete_object_format_,
+        String delete_object_compression_method_)
+        : ISimpleTransform(header_, header_, false)
+        , header(header_)
+        , iceberg_object_info(iceberg_object_info_)
+        , object_storage(object_storage_)
+        , format_settings(format_settings_)
+        , context(context_)
+        , delete_object_format(delete_object_format_)
+        , delete_object_compression_method(delete_object_compression_method_)
+    {
+        initializeDeleteSources();
+    }
+
+    String getName() const override { return "IcebergPositionDeleteTransform"; }
+
+private:
+    void initializeDeleteSources();
+
+protected:
+    LoggerPtr log = getLogger("IcebergPositionDeleteTransform");
+    static size_t getColumnIndex(const std::shared_ptr<IInputFormat> & delete_source, const String & column_name);
+
+    Block header;
+    IcebergDataObjectInfoPtr iceberg_object_info;
+    const ObjectStoragePtr object_storage;
+    const std::optional<FormatSettings> format_settings;
+    ContextPtr context;
+    const String delete_object_format;
+    const String delete_object_compression_method;
+
+    /// We need to keep the read buffers alive since the delete_sources depends on them.
+    std::vector<std::unique_ptr<ReadBuffer>> delete_read_buffers;
+    std::vector<std::shared_ptr<IInputFormat>> delete_sources;
+};
+
+class IcebergBitmapPositionDeleteTransform : public IcebergPositionDeleteTransform
+{
+public:
+    IcebergBitmapPositionDeleteTransform(
+        const Block & header_,
+        IcebergDataObjectInfoPtr iceberg_object_info_,
+        const ObjectStoragePtr object_storage_,
+        const std::optional<FormatSettings> & format_settings_,
+        ContextPtr context_,
+        String delete_object_format_,
+        String delete_object_compression_method_ = "auto")
+        : IcebergPositionDeleteTransform(
+              header_,
+              iceberg_object_info_,
+              object_storage_,
+              format_settings_,
+              context_,
+              delete_object_format_,
+              delete_object_compression_method_)
+    {
+        initialize();
+    }
+
+    String getName() const override { return "IcebergBitmapPositionDeleteTransform"; }
+
+    void transform(Chunk & chunk) override;
+
+private:
+    void initialize();
+    RoaringBitmapWithSmallSet<size_t, 32> bitmap;
+};
+
+}
+
+#endif
diff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/Snapshot.h b/src/Storages/ObjectStorage/DataLakes/Iceberg/Snapshot.h
index 6a0abf753c7e..a2a591567863 100644
--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/Snapshot.h
+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/Snapshot.h
@@ -16,6 +16,14 @@ struct IcebergSnapshot
     Int64 snapshot_id;
     std::optional<size_t> total_rows;
     std::optional<size_t> total_bytes;
+    std::optional<size_t> total_positional_delete_rows;
+
+    std::optional<size_t> getTotalRows() const
+    {
+        if (total_rows.has_value() && total_positional_delete_rows.has_value())
+            return *total_rows - *total_positional_delete_rows;
+        return std::nullopt;
+    }
 };
 
 struct IcebergHistoryRecord
diff --git a/src/Storages/ObjectStorage/StorageObjectStorage.h b/src/Storages/ObjectStorage/StorageObjectStorage.h
index e7eb1ef3e60f..30ffccd61194 100644
--- a/src/Storages/ObjectStorage/StorageObjectStorage.h
+++ b/src/Storages/ObjectStorage/StorageObjectStorage.h
@@ -1,18 +1,19 @@
 #pragma once
 #include <Core/SchemaInferenceMode.h>
 #include <Disks/ObjectStorages/IObjectStorage.h>
+#include <Formats/FormatSettings.h>
+#include <Interpreters/ActionsDAG.h>
+#include <Interpreters/Context_fwd.h>
 #include <Parsers/IAST_fwd.h>
 #include <Processors/Formats/IInputFormat.h>
+#include <Processors/ISimpleTransform.h>
+#include <Storages/ColumnsDescription.h>
 #include <Storages/IStorage.h>
+#include <Storages/ObjectStorage/DataLakes/DataLakeStorageSettings.h>
+#include <Storages/ObjectStorage/DataLakes/IDataLakeMetadata.h>
 #include <Storages/ObjectStorage/IObjectIterator.h>
 #include <Storages/prepareReadingFromFormat.h>
 #include <Common/threadPoolCallbackRunner.h>
-#include <Interpreters/ActionsDAG.h>
-#include <Storages/ColumnsDescription.h>
-#include <Storages/ObjectStorage/DataLakes/IDataLakeMetadata.h>
-#include <Storages/ObjectStorage/DataLakes/DataLakeStorageSettings.h>
-#include <Formats/FormatSettings.h>
-#include <Interpreters/Context_fwd.h>
 
 #include <memory>
 
@@ -27,7 +28,7 @@ using StorageObjectStorageSettingsPtr = std::shared_ptr<StorageObjectStorageSett
 
 namespace ErrorCodes
 {
-    extern const int NOT_IMPLEMENTED;
+extern const int NOT_IMPLEMENTED;
 }
 
 
@@ -90,17 +91,11 @@ class StorageObjectStorage : public IStorage
         size_t max_block_size,
         size_t num_streams) override;
 
-    SinkToStoragePtr write(
-        const ASTPtr & query,
-        const StorageMetadataPtr & metadata_snapshot,
-        ContextPtr context,
-        bool async_insert) override;
+    SinkToStoragePtr
+    write(const ASTPtr & query, const StorageMetadataPtr & metadata_snapshot, ContextPtr context, bool async_insert) override;
 
     void truncate(
-        const ASTPtr & query,
-        const StorageMetadataPtr & metadata_snapshot,
-        ContextPtr local_context,
-        TableExclusiveLockHolder &) override;
+        const ASTPtr & query, const StorageMetadataPtr & metadata_snapshot, ContextPtr local_context, TableExclusiveLockHolder &) override;
 
     bool supportsPartitionBy() const override { return true; }
 
@@ -142,7 +137,6 @@ class StorageObjectStorage : public IStorage
     void addInferredEngineArgsToCreateQuery(ASTs & args, const ContextPtr & context) const override;
 
     bool updateExternalDynamicMetadataIfExists(ContextPtr query_context) override;
-
     IDataLakeMetadata * getExternalMetadata(ContextPtr query_context);
 
     std::optional<UInt64> totalRows(ContextPtr query_context) const override;
@@ -218,7 +212,8 @@ class StorageObjectStorage::Configuration
 
     /// Add/replace structure and format arguments in the AST arguments if they have 'auto' values.
     virtual void addStructureAndFormatToArgsIfNeeded(
-        ASTs & args, const String & structure_, const String & format_, ContextPtr context, bool with_structure) = 0;
+        ASTs & args, const String & structure_, const String & format_, ContextPtr context, bool with_structure)
+        = 0;
 
     bool withPartitionWildcard() const;
     bool withGlobs() const { return isPathWithGlobs() || isNamespaceWithGlobs(); }
@@ -232,7 +227,7 @@ class StorageObjectStorage::Configuration
     virtual std::string getPathInArchive() const;
 
     virtual void check(ContextPtr context) const;
-    virtual void validateNamespace(const String & /* name */) const {}
+    virtual void validateNamespace(const String & /* name */) const { }
 
     virtual ObjectStoragePtr createObjectStorage(ContextPtr context, bool is_readonly) = 0;
     virtual bool isStaticConfiguration() const { return true; }
@@ -280,9 +275,24 @@ class StorageObjectStorage::Configuration
         bool if_not_updated_before,
         bool check_consistent_with_previous_metadata);
 
+    virtual bool hasPositionDeleteTransformer(const ObjectInfoPtr & /*object_info*/) const { return false; }
+
+    virtual std::shared_ptr<ISimpleTransform> getPositionDeleteTransformer(
+        const ObjectInfoPtr & /*object_info*/,
+        const Block & /*header*/,
+        const std::optional<FormatSettings> & /*format_settings*/,
+        ContextPtr /*context_*/) const
+    {
+        throw Exception(
+            ErrorCodes::NOT_IMPLEMENTED,
+            "Method getPositionDeleteTransformer() is not implemented for configuration type {}",
+            getTypeName());
+    }
+
     virtual const DataLakeStorageSettings & getDataLakeSettings() const
     {
-        throw Exception(ErrorCodes::NOT_IMPLEMENTED, "Method getDataLakeSettings() is not implemented for configuration type {}", getTypeName());
+        throw Exception(
+            ErrorCodes::NOT_IMPLEMENTED, "Method getDataLakeSettings() is not implemented for configuration type {}", getTypeName());
     }
 
     String format = "auto";
diff --git a/src/Storages/ObjectStorage/StorageObjectStorageSource.cpp b/src/Storages/ObjectStorage/StorageObjectStorageSource.cpp
index 735e1704d6df..75d8acab0adf 100644
--- a/src/Storages/ObjectStorage/StorageObjectStorageSource.cpp
+++ b/src/Storages/ObjectStorage/StorageObjectStorageSource.cpp
@@ -1,4 +1,5 @@
 #include "StorageObjectStorageSource.h"
+
 #include <memory>
 #include <optional>
 #include <Common/SipHash.h>
@@ -487,6 +488,14 @@ StorageObjectStorageSource::ReaderHolder StorageObjectStorageSource::createReade
 
         builder.init(Pipe(input_format));
 
+        if (configuration->hasPositionDeleteTransformer(object_info))
+        {
+            builder.addSimpleTransform(
+                [&](const Block & header)
+                { return configuration->getPositionDeleteTransformer(object_info, header, format_settings, context_); });
+        }
+
+
         std::shared_ptr<const ActionsDAG> transformer;
         if (object_info->data_lake_metadata)
             transformer = object_info->data_lake_metadata->transform;
