{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 80237,
  "instance_id": "ClickHouse__ClickHouse-80237",
  "issue_numbers": [
    "79678"
  ],
  "base_commit": "9f3659c9101bfec29e7f238c6c8a00b08eae6ac9",
  "patch": "diff --git a/src/Processors/Formats/IInputFormat.h b/src/Processors/Formats/IInputFormat.h\nindex cf2397ff1cb6..6a8208e295a3 100644\n--- a/src/Processors/Formats/IInputFormat.h\n+++ b/src/Processors/Formats/IInputFormat.h\n@@ -14,6 +14,14 @@ struct SelectQueryInfo;\n \n using ColumnMappingPtr = std::shared_ptr<ColumnMapping>;\n \n+struct ChunkInfoRowNumOffset : public ChunkInfoCloneable<ChunkInfoRowNumOffset>\n+{\n+    ChunkInfoRowNumOffset(const ChunkInfoRowNumOffset & other) = default;\n+    explicit ChunkInfoRowNumOffset(size_t row_num_offset_) : row_num_offset(row_num_offset_) { }\n+\n+    const size_t row_num_offset;\n+};\n+\n /** Input format is a source, that reads data from ReadBuffer.\n   */\n class IInputFormat : public ISource\ndiff --git a/src/Processors/Formats/Impl/ParquetBlockInputFormat.cpp b/src/Processors/Formats/Impl/ParquetBlockInputFormat.cpp\nindex e27c0dee8c27..b3e033416776 100644\n--- a/src/Processors/Formats/Impl/ParquetBlockInputFormat.cpp\n+++ b/src/Processors/Formats/Impl/ParquetBlockInputFormat.cpp\n@@ -578,6 +578,7 @@ ParquetBlockInputFormat::ParquetBlockInputFormat(\n     , skip_row_groups(format_settings.parquet.skip_row_groups)\n     , parser_group(std::move(parser_group_))\n     , min_bytes_for_seek(min_bytes_for_seek_)\n+    , header(header_)\n     , pending_chunks(PendingChunk::Compare { .row_group_first = format_settings_.parquet.preserve_order })\n     , previous_block_missing_values(getPort().getHeader().columns())\n {\n@@ -731,13 +732,11 @@ void ParquetBlockInputFormat::initializeIfNeeded()\n \n         KeyCondition::ColumnIndexToBloomFilter column_index_to_bloom_filter;\n \n-        const auto & header = getPort().getHeader();\n-\n-        std::vector<Range> hyperrectangle(header.columns(), Range::createWholeUniverse());\n+        std::vector<Range> hyperrectangle(getPort().getHeader().columns(), Range::createWholeUniverse());\n \n         if (format_settings.parquet.filter_push_down)\n         {\n-            hyperrectangle = getHyperrectangleForRowGroup(*metadata, row_group, header, format_settings);\n+            hyperrectangle = getHyperrectangleForRowGroup(*metadata, row_group, getPort().getHeader(), format_settings);\n         }\n \n         if (format_settings.parquet.bloom_filter_push_down)\n@@ -750,20 +749,29 @@ void ParquetBlockInputFormat::initializeIfNeeded()\n         return !maybe_exists;\n     };\n \n+    // The first one stores the skipped rows for all the skipped row groups before the first row group batch.\n+    row_group_batches_skipped_rows.push_back(0);\n     for (int row_group = 0; row_group < num_row_groups; ++row_group)\n     {\n         if (skip_row_groups.contains(row_group))\n+        {\n+            row_group_batches_skipped_rows.back() += metadata->RowGroup(row_group)->num_rows();\n             continue;\n+        }\n \n         if (key_condition_with_bloom_filter_data && skip_row_group_based_on_filters(row_group))\n         {\n             ProfileEvents::increment(ProfileEvents::ParquetPrunedRowGroups);\n+            row_group_batches_skipped_rows.back() += metadata->RowGroup(row_group)->num_rows();\n             continue;\n         }\n \n         // When single-threaded parsing, can prefetch row groups, so need to put all row groups in the same row_group_batch\n         if (row_group_batches.empty() || (!prefetch_group && row_group_batches.back().total_bytes_compressed >= min_bytes_for_seek))\n+        {\n             row_group_batches.emplace_back();\n+            row_group_batches_skipped_rows.push_back(0);\n+        }\n \n         ProfileEvents::increment(ProfileEvents::ParquetReadRowGroups);\n         row_group_batches.back().row_groups_idxs.push_back(row_group);\n@@ -1055,6 +1063,7 @@ void ParquetBlockInputFormat::decodeOneChunk(size_t row_group_batch_idx, std::un\n \n     lock.lock();\n \n+    row_group_batch.chunk_sizes.push_back(res.chunk.getNumRows());\n     ++row_group_batch.next_chunk_idx;\n     ++row_group_batch.num_pending_chunks;\n     pending_chunks.push(std::move(res));\n@@ -1096,7 +1105,16 @@ Chunk ParquetBlockInputFormat::read()\n         return {};\n \n     if (need_only_count)\n-        return getChunkForCount(row_group_batches[row_group_batches_completed++].total_rows);\n+    {\n+        auto chunk = getChunkForCount(row_group_batches[row_group_batches_completed].total_rows);\n+        int total_rows_before = row_group_batches_skipped_rows[0];\n+        for (size_t i = 0; i < row_group_batches_completed; ++i)\n+            total_rows_before += row_group_batches[i].total_rows + row_group_batches_skipped_rows[i+1];\n+\n+        row_group_batches_completed++;\n+        chunk.getChunkInfos().add(std::make_shared<ChunkInfoRowNumOffset>(total_rows_before));\n+        return chunk;\n+    }\n \n     std::unique_lock lock(mutex);\n \n@@ -1128,6 +1146,15 @@ Chunk ParquetBlockInputFormat::read()\n \n             previous_block_missing_values = std::move(chunk.block_missing_values);\n             previous_approx_bytes_read_for_chunk = chunk.approx_original_chunk_size;\n+\n+            int total_rows_before = row_group_batches_skipped_rows[0];\n+            for (size_t i = 0; i < chunk.row_group_batch_idx; ++i)\n+                total_rows_before += row_group_batches[i].total_rows + row_group_batches_skipped_rows[i+1];\n+            for (size_t i = 0; i < chunk.chunk_idx; ++i)\n+                total_rows_before += row_group.chunk_sizes[i];\n+\n+            chunk.chunk.getChunkInfos().add(std::make_shared<ChunkInfoRowNumOffset>(total_rows_before));\n+\n             return std::move(chunk.chunk);\n         }\n \n@@ -1154,6 +1181,7 @@ void ParquetBlockInputFormat::resetParser()\n     metadata.reset();\n     column_indices.clear();\n     row_group_batches.clear();\n+    row_group_batches_skipped_rows.clear();\n     while (!pending_chunks.empty())\n         pending_chunks.pop();\n     row_group_batches_completed = 0;\ndiff --git a/src/Processors/Formats/Impl/ParquetBlockInputFormat.h b/src/Processors/Formats/Impl/ParquetBlockInputFormat.h\nindex 57397ee82bd1..2c50ec097b71 100644\n--- a/src/Processors/Formats/Impl/ParquetBlockInputFormat.h\n+++ b/src/Processors/Formats/Impl/ParquetBlockInputFormat.h\n@@ -211,6 +211,7 @@ class ParquetBlockInputFormat : public IInputFormat\n         //  (at most max_pending_chunks_per_row_group)\n \n         size_t next_chunk_idx = 0;\n+        std::vector<size_t> chunk_sizes;\n         size_t num_pending_chunks = 0;\n \n         size_t total_rows = 0;\n@@ -319,7 +320,9 @@ class ParquetBlockInputFormat : public IInputFormat\n     // Wakes up the read() call, if any.\n     std::condition_variable condvar;\n \n+    Block header;\n     std::vector<RowGroupBatchState> row_group_batches;\n+    std::vector<int> row_group_batches_skipped_rows;\n     std::priority_queue<PendingChunk, std::vector<PendingChunk>, PendingChunk::Compare> pending_chunks;\n     size_t row_group_batches_completed = 0;\n \ndiff --git a/src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h b/src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h\nindex 8f4928c038f2..537ab259fd19 100644\n--- a/src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h\n+++ b/src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h\n@@ -115,6 +115,24 @@ class DataLakeConfiguration : public BaseStorageConfiguration, public std::enabl\n         return current_metadata->getSchemaTransformer(local_context, data_path);\n     }\n \n+    bool hasPositionDeleteTransformer(const ObjectInfoPtr & object_info) const override\n+    {\n+        if (!current_metadata)\n+            return false;\n+        return current_metadata->hasPositionDeleteTransformer(object_info);\n+    }\n+\n+    std::shared_ptr<ISimpleTransform> getPositionDeleteTransformer(\n+        const ObjectInfoPtr & object_info,\n+        const Block & header,\n+        const std::optional<FormatSettings> & format_settings,\n+        ContextPtr context_) const override\n+    {\n+        if (!current_metadata)\n+            return {};\n+        return current_metadata->getPositionDeleteTransformer(object_info, header, format_settings, context_);\n+    }\n+\n     bool hasExternalDynamicMetadata() override\n     {\n         assertInitialized();\ndiff --git a/src/Storages/ObjectStorage/DataLakes/IDataLakeMetadata.h b/src/Storages/ObjectStorage/DataLakes/IDataLakeMetadata.h\nindex 987aabc8a7e7..98520bfa486f 100644\n--- a/src/Storages/ObjectStorage/DataLakes/IDataLakeMetadata.h\n+++ b/src/Storages/ObjectStorage/DataLakes/IDataLakeMetadata.h\n@@ -1,8 +1,8 @@\n #pragma once\n #include <Core/NamesAndTypes.h>\n #include <Core/Types.h>\n-#include <boost/noncopyable.hpp>\n-#include \"Interpreters/ActionsDAG.h\"\n+#include <Interpreters/ActionsDAG.h>\n+#include <Processors/ISimpleTransform.h>\n #include <Storages/ObjectStorage/IObjectIterator.h>\n #include <Storages/prepareReadingFromFormat.h>\n \n@@ -44,6 +44,16 @@ class IDataLakeMetadata : boost::noncopyable\n     virtual std::shared_ptr<NamesAndTypesList> getInitialSchemaByPath(ContextPtr, const String & /* path */) const { return {}; }\n     virtual std::shared_ptr<const ActionsDAG> getSchemaTransformer(ContextPtr, const String & /* path */) const { return {}; }\n \n+    virtual bool hasPositionDeleteTransformer(const ObjectInfoPtr & /*object_info*/) const { return false; }\n+    virtual std::shared_ptr<ISimpleTransform> getPositionDeleteTransformer(\n+        const ObjectInfoPtr & /* object_info */,\n+        const Block & /* header */,\n+        const std::optional<FormatSettings> & /* format_settings */,\n+        ContextPtr /*context*/) const\n+    {\n+        return {};\n+    }\n+\n     /// Whether metadata is updateable (instead of recreation from scratch)\n     /// to the latest version of table state in data lake.\n     virtual bool supportsUpdate() const { return false; }\n@@ -59,7 +69,7 @@ class IDataLakeMetadata : boost::noncopyable\n     virtual std::optional<size_t> totalBytes(ContextPtr) const { return {}; }\n \n protected:\n-    ObjectIterator createKeysIterator(\n+    virtual ObjectIterator createKeysIterator(\n         Strings && data_files_,\n         ObjectStoragePtr object_storage_,\n         IDataLakeMetadata::FileProgressCallback callback_) const;\ndiff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/Constant.h b/src/Storages/ObjectStorage/DataLakes/Iceberg/Constant.h\nindex ca9e7b8d4b7c..0444d92f8fc4 100644\n--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/Constant.h\n+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/Constant.h\n@@ -39,7 +39,6 @@ DEFINE_ICEBERG_FIELD(time);\n DEFINE_ICEBERG_FIELD(timestamp);\n DEFINE_ICEBERG_FIELD(timestamptz);\n DEFINE_ICEBERG_FIELD(type)\n-DEFINE_ICEBERG_FIELD(transform);\n DEFINE_ICEBERG_FIELD(uuid);\n DEFINE_ICEBERG_FIELD(value);\n /// These variables replace `-` with underscore `_` to be compatible with c++ code.\n@@ -52,11 +51,14 @@ DEFINE_ICEBERG_FIELD_ALIAS(schema_id, schema-id);\n DEFINE_ICEBERG_FIELD_ALIAS(current_schema_id, current-schema-id);\n DEFINE_ICEBERG_FIELD_ALIAS(table_uuid, table-uuid);\n DEFINE_ICEBERG_FIELD_ALIAS(total_records, total-records);\n+DEFINE_ICEBERG_FIELD_ALIAS(total_position_deletes, total-position-deletes);\n DEFINE_ICEBERG_FIELD_ALIAS(total_files_size, total-files-size);\n DEFINE_ICEBERG_FIELD_ALIAS(manifest_list, manifest-list);\n DEFINE_ICEBERG_FIELD_ALIAS(timestamp_ms, timestamp-ms);\n DEFINE_ICEBERG_FIELD_ALIAS(last_updated_ms, last-updated-ms);\n DEFINE_ICEBERG_FIELD_ALIAS(source_id, source-id);\n+DEFINE_ICEBERG_FIELD_ALIAS(partition_transform, transform);\n+DEFINE_ICEBERG_FIELD_ALIAS(partition_name, name);\n /// These are compound fields like `data_file.file_path`, we use prefix 'c_' to distinguish them.\n DEFINE_ICEBERG_FIELD_COMPOUND(data_file, file_path);\n DEFINE_ICEBERG_FIELD_COMPOUND(data_file, content);\n@@ -66,4 +68,5 @@ DEFINE_ICEBERG_FIELD_COMPOUND(data_file, column_sizes);\n DEFINE_ICEBERG_FIELD_COMPOUND(data_file, null_value_counts);\n DEFINE_ICEBERG_FIELD_COMPOUND(data_file, lower_bounds);\n DEFINE_ICEBERG_FIELD_COMPOUND(data_file, upper_bounds);\n+DEFINE_ICEBERG_FIELD_COMPOUND(data_file, referenced_data_file);\n }\ndiff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp b/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp\nindex 70e6f781c490..35e0d5f6ad81 100644\n--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp\n+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp\n@@ -4,6 +4,7 @@\n \n #include <Core/Settings.h>\n #include <Core/NamesAndTypes.h>\n+#include <Disks/ObjectStorages/StoredObject.h>\n #include <Formats/FormatFactory.h>\n #include <IO/ReadBufferFromFileBase.h>\n #include <IO/ReadBufferFromString.h>\n@@ -13,7 +14,7 @@\n #include <Storages/ObjectStorage/DataLakes/Common.h>\n #include <Storages/ObjectStorage/StorageObjectStorageSource.h>\n #include <Storages/ObjectStorage/DataLakes/DataLakeStorageSettings.h>\n-#include \"Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadataFilesCache.h\"\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadataFilesCache.h>\n #include <Interpreters/ExpressionActions.h>\n #include <IO/CompressedReadBufferWrapper.h>\n \n@@ -23,6 +24,7 @@\n #include <Storages/ObjectStorage/DataLakes/Iceberg/Snapshot.h>\n #include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFilesPruning.h>\n #include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.h>\n #include <Storages/ObjectStorage/DataLakes/Iceberg/Constant.h>\n \n #include <Common/logger_useful.h>\n@@ -51,6 +53,7 @@ extern const int FILE_DOESNT_EXIST;\n extern const int BAD_ARGUMENTS;\n extern const int LOGICAL_ERROR;\n extern const int ICEBERG_SPECIFICATION_VIOLATION;\n+extern const int UNSUPPORTED_METHOD;\n }\n \n namespace Setting\n@@ -143,7 +146,6 @@ Poco::JSON::Object::Ptr getMetadataJSONObject(\n \n }\n \n-\n IcebergMetadata::IcebergMetadata(\n     ObjectStoragePtr object_storage_,\n     ConfigurationObserverPtr configuration_,\n@@ -513,6 +515,7 @@ bool IcebergMetadata::update(const ContextPtr & local_context)\n     if (previous_snapshot_id != relevant_snapshot_id)\n     {\n         cached_unprunned_files_for_last_processed_snapshot = std::nullopt;\n+        cached_unprunned_position_deletes_files_for_last_processed_snapshot = std::nullopt;\n         schema_id_by_data_file_initialized.store(false);\n         return true;\n     }\n@@ -542,6 +545,7 @@ void IcebergMetadata::updateSnapshot(ContextPtr local_context, Poco::JSON::Objec\n                     configuration_ptr->getPath());\n             std::optional<size_t> total_rows;\n             std::optional<size_t> total_bytes;\n+            std::optional<size_t> total_position_deletes;\n \n             if (snapshot->has(f_summary))\n             {\n@@ -551,12 +555,20 @@ void IcebergMetadata::updateSnapshot(ContextPtr local_context, Poco::JSON::Objec\n \n                 if (summary_object->has(f_total_files_size))\n                     total_bytes = summary_object->getValue<Int64>(f_total_files_size);\n+\n+                if (summary_object->has(f_total_position_deletes))\n+                {\n+                    total_position_deletes = summary_object->getValue<Int64>(f_total_position_deletes);\n+                }\n             }\n \n             relevant_snapshot = IcebergSnapshot{\n                 getManifestList(local_context, getProperFilePathFromMetadataInfo(\n                     snapshot->getValue<String>(f_manifest_list), configuration_ptr->getPath(), table_location)),\n-                relevant_snapshot_id, total_rows, total_bytes};\n+                relevant_snapshot_id,\n+                total_rows,\n+                total_bytes,\n+                total_position_deletes};\n \n             if (!snapshot->has(f_schema_id))\n                 throw Exception(\n@@ -694,10 +706,9 @@ void IcebergMetadata::initializeSchemasFromManifestList(ContextPtr local_context\n \n void IcebergMetadata::initializeSchemasFromManifestFile(ManifestFilePtr manifest_file_ptr) const\n {\n-    for (const auto & manifest_file_entry : manifest_file_ptr->getFiles())\n+    for (const auto & manifest_file_entry : manifest_file_ptr->getFiles(FileContentType::DATA))\n     {\n-        if (std::holds_alternative<DataFileEntry>(manifest_file_entry.file))\n-            schema_id_by_data_file.emplace(std::get<DataFileEntry>(manifest_file_entry.file).file_name, manifest_file_ptr->getSchemaId());\n+        schema_id_by_data_file.emplace(manifest_file_entry.file_path, manifest_file_ptr->getSchemaId());\n     }\n }\n \n@@ -869,47 +880,67 @@ ManifestFilePtr IcebergMetadata::getManifestFile(ContextPtr local_context, const\n     return create_fn();\n }\n \n-Strings IcebergMetadata::getDataFiles(const ActionsDAG * filter_dag, ContextPtr local_context) const\n+std::vector<Iceberg::ManifestFileEntry>\n+IcebergMetadata::getFilesImpl(const ActionsDAG * filter_dag, FileContentType file_content_type, ContextPtr local_context) const\n {\n     if (!relevant_snapshot)\n         return {};\n-\n+    if (!local_context && filter_dag)\n+    {\n+        throw DB::Exception(\n+            DB::ErrorCodes::LOGICAL_ERROR,\n+            \"Context is required with non-empty filter_dag to implement partition pruning for Iceberg table\");\n+    }\n+    std::optional<std::vector<Iceberg::ManifestFileEntry>> & cached_files = (file_content_type == FileContentType::DATA)\n+        ? cached_unprunned_files_for_last_processed_snapshot\n+        : cached_unprunned_position_deletes_files_for_last_processed_snapshot;\n     bool use_partition_pruning = filter_dag && local_context->getSettingsRef()[Setting::use_iceberg_partition_pruning];\n+    if (!use_partition_pruning && cached_files.has_value())\n+        return cached_files.value();\n \n-    if (!use_partition_pruning && cached_unprunned_files_for_last_processed_snapshot.has_value())\n-        return cached_unprunned_files_for_last_processed_snapshot.value();\n-\n-    Strings data_files;\n+    std::vector<Iceberg::ManifestFileEntry> files;\n     for (const auto & manifest_list_entry : relevant_snapshot->manifest_list_entries)\n     {\n         auto manifest_file_ptr = getManifestFile(local_context, manifest_list_entry.manifest_file_path, manifest_list_entry.added_sequence_number);\n         initializeSchemasFromManifestFile(manifest_file_ptr);\n-        ManifestFilesPruner pruner(\n-            schema_processor, relevant_snapshot_schema_id,\n-            use_partition_pruning ? filter_dag : nullptr,\n-            *manifest_file_ptr, local_context);\n-        const auto & data_files_in_manifest = manifest_file_ptr->getFiles();\n-        for (const auto & manifest_file_entry : data_files_in_manifest)\n+        const auto & files_in_manifest = manifest_file_ptr->getFiles(file_content_type);\n+        for (const auto & manifest_file_entry : files_in_manifest)\n         {\n-            if (manifest_file_entry.status != ManifestEntryStatus::DELETED)\n-            {\n-                if (!pruner.canBePruned(manifest_file_entry))\n+            ManifestFilesPruner pruner(\n+                schema_processor, relevant_snapshot_schema_id,\n+                use_partition_pruning ? filter_dag : nullptr,\n+                *manifest_file_ptr, local_context);\n+                if (manifest_file_entry.status != ManifestEntryStatus::DELETED)\n                 {\n-                    if (std::holds_alternative<DataFileEntry>(manifest_file_entry.file))\n-                        data_files.push_back(std::get<DataFileEntry>(manifest_file_entry.file).file_name);\n+                    if (!pruner.canBePruned(manifest_file_entry))\n+                    {\n+                        files.push_back(manifest_file_entry);\n+                    }\n                 }\n-            }\n         }\n     }\n \n+    std::sort(files.begin(), files.end());\n+\n     schema_id_by_data_file_initialized = true;\n     if (!use_partition_pruning)\n     {\n-        cached_unprunned_files_for_last_processed_snapshot = data_files;\n-        return cached_unprunned_files_for_last_processed_snapshot.value();\n+        cached_files = files;\n+        return cached_files.value();\n     }\n \n-    return data_files;\n+    return files;\n+}\n+\n+\n+std::vector<Iceberg::ManifestFileEntry> IcebergMetadata::getDataFiles(const ActionsDAG * filter_dag, ContextPtr local_context) const\n+{\n+    return getFilesImpl(filter_dag, FileContentType::DATA, local_context);\n+}\n+\n+std::vector<Iceberg::ManifestFileEntry> IcebergMetadata::getPositionalDeleteFiles(const ActionsDAG * filter_dag, ContextPtr local_context) const\n+{\n+    return getFilesImpl(filter_dag, FileContentType::POSITIONAL_DELETE, local_context);\n }\n \n std::optional<size_t> IcebergMetadata::totalRows(ContextPtr local_context) const\n@@ -924,23 +955,25 @@ std::optional<size_t> IcebergMetadata::totalRows(ContextPtr local_context) const\n         return 0;\n     }\n \n+\n     /// All these \"hints\" with total rows or bytes are optional both in\n     /// metadata files and in manifest files, so we try all of them one by one\n-    if (relevant_snapshot->total_rows.has_value())\n+    if (relevant_snapshot->getTotalRows())\n     {\n         ProfileEvents::increment(ProfileEvents::IcebergTrivialCountOptimizationApplied);\n-        return relevant_snapshot->total_rows;\n+        return relevant_snapshot->getTotalRows();\n     }\n \n     Int64 result = 0;\n     for (const auto & manifest_list_entry : relevant_snapshot->manifest_list_entries)\n     {\n         auto manifest_file_ptr = getManifestFile(local_context, manifest_list_entry.manifest_file_path, manifest_list_entry.added_sequence_number);\n-        auto count = manifest_file_ptr->getRowsCountInAllDataFilesExcludingDeleted();\n-        if (!count.has_value())\n+        auto data_count = manifest_file_ptr->getRowsCountInAllFilesExcludingDeleted(FileContentType::DATA);\n+        auto position_deletes_count = manifest_file_ptr->getRowsCountInAllFilesExcludingDeleted(FileContentType::POSITIONAL_DELETE);\n+        if (!data_count.has_value() || !position_deletes_count.has_value())\n             return {};\n \n-        result += count.value();\n+        result += data_count.value() - position_deletes_count.value();\n     }\n \n     ProfileEvents::increment(ProfileEvents::IcebergTrivialCountOptimizationApplied);\n@@ -982,9 +1015,119 @@ ObjectIterator IcebergMetadata::iterate(\n     size_t /* list_batch_size */,\n     ContextPtr local_context) const\n {\n-    return createKeysIterator(getDataFiles(filter_dag, local_context), object_storage, callback);\n+    return std::make_shared<IcebergKeysIterator>(*this, getDataFiles(filter_dag, local_context), getPositionalDeleteFiles(filter_dag, local_context), object_storage, callback);\n+}\n+\n+bool IcebergMetadata::hasPositionDeleteTransformer(const ObjectInfoPtr & object_info) const\n+{\n+    auto iceberg_object_info = std::dynamic_pointer_cast<IcebergDataObjectInfo>(object_info);\n+    if (!iceberg_object_info)\n+        return false;\n+\n+    return !iceberg_object_info->position_deletes_objects.empty();\n+}\n+\n+std::shared_ptr<ISimpleTransform> IcebergMetadata::getPositionDeleteTransformer(\n+    const ObjectInfoPtr & object_info,\n+    const Block & header,\n+    const std::optional<FormatSettings> & format_settings,\n+    ContextPtr context_) const\n+{\n+    auto iceberg_object_info = std::dynamic_pointer_cast<IcebergDataObjectInfo>(object_info);\n+    if (!iceberg_object_info)\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"The object info is not IcebergDataObjectInfo\");\n+\n+    auto configuration_ptr = configuration.lock();\n+    if (!configuration_ptr)\n+    {\n+        throw DB::Exception(ErrorCodes::LOGICAL_ERROR, \"Iceberg configuration has expired\");\n+    }\n+\n+    String delete_object_format = configuration_ptr->format;\n+    String delete_object_compression_method = configuration_ptr->compression_method;\n+\n+    return std::make_shared<IcebergBitmapPositionDeleteTransform>(\n+        header, iceberg_object_info, object_storage, format_settings, context_, delete_object_format, delete_object_compression_method);\n+}\n+\n+\n+IcebergDataObjectInfo::IcebergDataObjectInfo(\n+    const IcebergMetadata & iceberg_metadata,\n+    Iceberg::ManifestFileEntry data_object_,\n+    std::optional<ObjectMetadata> metadata_,\n+    const std::vector<Iceberg::ManifestFileEntry> & position_deletes_objects_)\n+    : RelativePathWithMetadata(data_object_.file_path, std::move(metadata_))\n+    , data_object(data_object_)\n+{\n+    ///Object in position_deletes_objects_ are sorted by common_partition_specification, partition_key_value and added_sequence_number.\n+    /// It is done to have an invariant that position deletes objects which corresponds\n+    /// to the data object form a subsegment in a position_deletes_objects_ vector.\n+    /// We need to take all position deletes objects which has the same partition schema and value and has added_sequence_number\n+    /// greater than or equal to the data object added_sequence_number (https://iceberg.apache.org/spec/#scan-planning)\n+    /// ManifestFileEntry has comparator by default which helps to do that.\n+    auto beg_it = std::lower_bound(position_deletes_objects_.begin(), position_deletes_objects_.end(), data_object_);\n+    auto end_it = std::upper_bound(\n+        position_deletes_objects_.begin(),\n+        position_deletes_objects_.end(),\n+        data_object_,\n+        [](const Iceberg::ManifestFileEntry & lhs, const Iceberg::ManifestFileEntry & rhs)\n+        {\n+            return std::tie(lhs.common_partition_specification, lhs.partition_key_value)\n+                < std::tie(rhs.common_partition_specification, rhs.partition_key_value);\n+        });\n+    if (beg_it - position_deletes_objects_.begin() > end_it - position_deletes_objects_.begin())\n+    {\n+        throw Exception(\n+            ErrorCodes::LOGICAL_ERROR,\n+            \"Position deletes objects are not sorted by common_partition_specification and partition_key_value, \"\n+            \"beginning: {}, end: {}, position_deletes_objects size: {}\",\n+            beg_it - position_deletes_objects_.begin(),\n+            end_it - position_deletes_objects_.begin(),\n+            position_deletes_objects_.size());\n+    }\n+    position_deletes_objects = std::span<const Iceberg::ManifestFileEntry>{beg_it, end_it};\n+    if (!position_deletes_objects.empty() && iceberg_metadata.configuration.lock()->format != \"Parquet\")\n+    {\n+        throw Exception(\n+            ErrorCodes::UNSUPPORTED_METHOD,\n+            \"Position deletes are only supported for data files of Parquet format in Iceberg, but got {}\",\n+            iceberg_metadata.configuration.lock()->format);\n+    }\n+};\n+\n+IcebergKeysIterator::IcebergKeysIterator(\n+    const IcebergMetadata & iceberg_metadata_,\n+    std::vector<Iceberg::ManifestFileEntry> && data_files_,\n+    std::vector<Iceberg::ManifestFileEntry> && position_deletes_files_,\n+    ObjectStoragePtr object_storage_,\n+    IDataLakeMetadata::FileProgressCallback callback_)\n+    : iceberg_metadata(iceberg_metadata_)\n+    , data_files(data_files_)\n+    , position_deletes_files(position_deletes_files_)\n+    , object_storage(object_storage_)\n+    , callback(callback_)\n+{\n }\n \n+\n+ObjectInfoPtr IcebergKeysIterator::next(size_t)\n+{\n+    while (true)\n+    {\n+        size_t current_index = index.fetch_add(1, std::memory_order_relaxed);\n+        if (current_index >= data_files.size())\n+            return nullptr;\n+\n+        auto key = data_files[current_index].file_path;\n+        auto object_metadata = object_storage->getObjectMetadata(key);\n+\n+        if (callback)\n+            callback(FileProgress(0, object_metadata.size_bytes));\n+\n+        return std::make_shared<IcebergDataObjectInfo>(\n+            iceberg_metadata, data_files[current_index], std::move(object_metadata), position_deletes_files);\n+    }\n+}\n }\n \n #endif\ndiff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h b/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h\nindex 4108e41f614b..13033911edd6 100644\n--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h\n+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h\n@@ -3,23 +3,19 @@\n \n #if USE_AVRO\n \n-#include <Core/Types.h>\n-#include <Disks/ObjectStorages/IObjectStorage.h>\n-#include <Interpreters/Context_fwd.h>\n-#include <Storages/ObjectStorage/DataLakes/IDataLakeMetadata.h>\n-#include <Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadataFilesCache.h>\n-#include <Storages/ObjectStorage/StorageObjectStorage.h>\n-\n #include <Poco/JSON/Array.h>\n #include <Poco/JSON/Object.h>\n #include <Poco/JSON/Parser.h>\n \n+#include <Core/Types.h>\n+#include <Disks/ObjectStorages/IObjectStorage.h>\n+#include <Interpreters/Context_fwd.h>\n #include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h>\n #include <Storages/ObjectStorage/DataLakes/Iceberg/SchemaProcessor.h>\n #include <Storages/ObjectStorage/DataLakes/Iceberg/Snapshot.h>\n-\n-#include <tuple>\n-\n+#include <Storages/ObjectStorage/DataLakes/IDataLakeMetadata.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadataFilesCache.h>\n+#include <Storages/ObjectStorage/StorageObjectStorage.h>\n \n namespace DB\n {\n@@ -73,6 +69,14 @@ class IcebergMetadata : public IDataLakeMetadata\n             : nullptr;\n     }\n \n+    bool hasPositionDeleteTransformer(const ObjectInfoPtr & object_info) const override;\n+\n+    std::shared_ptr<ISimpleTransform> getPositionDeleteTransformer(\n+        const ObjectInfoPtr & /* object_info */,\n+        const Block & /* header */,\n+        const std::optional<FormatSettings> & /* format_settings */,\n+        ContextPtr /* context */) const override;\n+\n     bool supportsSchemaEvolution() const override { return true; }\n \n     static Int32\n@@ -87,6 +91,10 @@ class IcebergMetadata : public IDataLakeMetadata\n     std::optional<size_t> totalRows(ContextPtr Local_context) const override;\n     std::optional<size_t> totalBytes(ContextPtr Local_context) const override;\n \n+\n+    friend class IcebergKeysIterator;\n+    friend struct IcebergDataObjectInfo;\n+\n protected:\n     ObjectIterator iterate(\n         const ActionsDAG * filter_dag,\n@@ -117,11 +125,14 @@ class IcebergMetadata : public IDataLakeMetadata\n     Int64 relevant_snapshot_id{-1};\n     String table_location;\n \n-    mutable std::optional<Strings> cached_unprunned_files_for_last_processed_snapshot;\n+    mutable std::optional<std::vector<Iceberg::ManifestFileEntry>> cached_unprunned_files_for_last_processed_snapshot;\n+    mutable std::optional<std::vector<Iceberg::ManifestFileEntry>> cached_unprunned_position_deletes_files_for_last_processed_snapshot;\n \n     void updateState(const ContextPtr & local_context, Poco::JSON::Object::Ptr metadata_object, bool metadata_file_changed);\n \n-    Strings getDataFiles(const ActionsDAG * filter_dag, ContextPtr local_context) const;\n+    std::vector<Iceberg::ManifestFileEntry> getDataFiles(const ActionsDAG * filter_dag, ContextPtr local_context) const;\n+\n+    std::vector<Iceberg::ManifestFileEntry> getPositionalDeleteFiles(const ActionsDAG * filter_dag, ContextPtr local_context) const;\n \n     void updateSnapshot(ContextPtr local_context, Poco::JSON::Object::Ptr metadata_object);\n \n@@ -141,7 +152,52 @@ class IcebergMetadata : public IDataLakeMetadata\n     std::optional<String> getRelevantManifestList(const Poco::JSON::Object::Ptr & metadata);\n \n     Iceberg::ManifestFilePtr tryGetManifestFile(const String & filename) const;\n+\n+    std::vector<Iceberg::ManifestFileEntry> getFilesImpl(const ActionsDAG * filter_dag, Iceberg::FileContentType file_content_type, ContextPtr local_context) const;\n+};\n+\n+struct IcebergDataObjectInfo : public RelativePathWithMetadata\n+{\n+    explicit IcebergDataObjectInfo(\n+        const IcebergMetadata & iceberg_metadata,\n+        Iceberg::ManifestFileEntry data_object_,\n+        std::optional<ObjectMetadata> metadata_ = std::nullopt,\n+        const std::vector<Iceberg::ManifestFileEntry> & position_deletes_objects_ = {});\n+\n+    const Iceberg::ManifestFileEntry data_object;\n+    std::span<const Iceberg::ManifestFileEntry> position_deletes_objects;\n+\n+    // Return the path in the Iceberg metadata\n+    std::string getIcebergDataPath() const { return data_object.file_path_key; }\n };\n+using IcebergDataObjectInfoPtr = std::shared_ptr<IcebergDataObjectInfo>;\n+\n+class IcebergKeysIterator : public IObjectIterator\n+{\n+public:\n+    IcebergKeysIterator(\n+        const IcebergMetadata & iceberg_metadata_,\n+        std::vector<Iceberg::ManifestFileEntry>&& data_files_,\n+        std::vector<Iceberg::ManifestFileEntry>&& position_deletes_files_,\n+        ObjectStoragePtr object_storage_,\n+        IDataLakeMetadata::FileProgressCallback callback_);\n+\n+    size_t estimatedKeysCount() override\n+    {\n+        return data_files.size();\n+    }\n+\n+    ObjectInfoPtr next(size_t) override;\n+\n+private:\n+    const IcebergMetadata & iceberg_metadata;\n+    std::vector<Iceberg::ManifestFileEntry> data_files;\n+    std::vector<Iceberg::ManifestFileEntry> position_deletes_files;\n+    ObjectStoragePtr object_storage;\n+    std::atomic<size_t> index = 0;\n+    IDataLakeMetadata::FileProgressCallback callback;\n+};\n+\n }\n \n #endif\ndiff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp\nindex a295071f2600..0fe83092735f 100644\n--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp\n+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp\n@@ -2,6 +2,8 @@\n \n #if USE_AVRO\n \n+#include <compare>\n+\n #include <Storages/ObjectStorage/DataLakes/Iceberg/Constant.h>\n #include <Storages/ObjectStorage/DataLakes/Iceberg/Utils.h>\n #include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h>\n@@ -26,6 +28,19 @@ namespace DB::ErrorCodes\n namespace Iceberg\n {\n \n+String FileContentTypeToString(FileContentType type)\n+{\n+    switch (type)\n+    {\n+        case FileContentType::DATA:\n+            return \"data\";\n+        case FileContentType::POSITIONAL_DELETE:\n+            return \"position_deletes\";\n+        case FileContentType::EQUALITY_DELETE:\n+            return \"equality_deletes\";\n+    }\n+}\n+\n namespace\n {\n     /// Iceberg stores lower_bounds and upper_bounds serialized with some custom deserialization as bytes array\n@@ -106,11 +121,17 @@ namespace\n \n }\n \n-const std::vector<ManifestFileEntry> & ManifestFileContent::getFiles() const\n+const std::vector<ManifestFileEntry> & ManifestFileContent::getFiles(FileContentType content_type) const\n {\n-    return files;\n+    if (content_type == FileContentType::DATA)\n+        return data_files;\n+    else if (content_type == FileContentType::POSITIONAL_DELETE)\n+        return position_deletes_files;\n+    else\n+        throw DB::Exception(DB::ErrorCodes::UNSUPPORTED_METHOD, \"Unsupported content type: {}\", static_cast<int>(content_type));\n }\n \n+\n Int32 ManifestFileContent::getSchemaId() const\n {\n     return schema_id;\n@@ -167,7 +188,10 @@ ManifestFileContent::ManifestFileContent(\n         /// we use column internal number as it's name.\n         auto numeric_column_name = DB::backQuote(DB::toString(source_id));\n         DB::NameAndTypePair manifest_file_column_characteristics = schema_processor.getFieldCharacteristics(schema_id, source_id);\n-        auto partition_ast = getASTFromTransform(partition_specification_field->getValue<String>(f_transform), numeric_column_name);\n+        auto transform_name = partition_specification_field->getValue<String>(f_partition_transform);\n+        auto partition_name = partition_specification_field->getValue<String>(f_partition_name);\n+        common_partition_specification.emplace_back(source_id, transform_name, partition_name);\n+        auto partition_ast = getASTFromTransform(transform_name, numeric_column_name);\n         /// Unsupported partition key expression\n         if (partition_ast == nullptr)\n             continue;\n@@ -185,12 +209,13 @@ ManifestFileContent::ManifestFileContent(\n         if (format_version_ > 1)\n         {\n             content_type = FileContentType(manifest_file_deserializer.getValueFromRowByName(i, c_data_file_content, TypeIndex::Int32).safeGet<UInt64>());\n-            if (content_type != FileContentType::DATA)\n+            if (content_type == FileContentType::EQUALITY_DELETE)\n                 throw Exception(\n-                    ErrorCodes::UNSUPPORTED_METHOD, \"Cannot read Iceberg table: positional and equality deletes are not supported\");\n+                    ErrorCodes::UNSUPPORTED_METHOD, \"Cannot read Iceberg table: files of content type {} are not supported\", content_type);\n         }\n         const auto status = ManifestEntryStatus(manifest_file_deserializer.getValueFromRowByName(i, f_status, TypeIndex::Int32).safeGet<UInt64>());\n \n+        const auto file_path_key = manifest_file_deserializer.getValueFromRowByName(i, c_data_file_file_path, TypeIndex::String).safeGet<String>();\n         const auto file_path = getProperFilePathFromMetadataInfo(manifest_file_deserializer.getValueFromRowByName(i, c_data_file_file_path, TypeIndex::String).safeGet<String>(), common_path, table_location);\n \n         /// NOTE: This is weird, because in manifest file partition looks like this:\n@@ -234,48 +259,49 @@ ManifestFileContent::ManifestFileContent(\n             }\n         }\n \n-        std::unordered_map<Int32, std::pair<Field, Field>> value_for_bounds;\n-        for (const auto & path : {c_data_file_lower_bounds, c_data_file_upper_bounds})\n+        if (content_type == FileContentType::DATA)\n         {\n-            if (manifest_file_deserializer.hasPath(path))\n+            std::unordered_map<Int32, std::pair<Field, Field>> value_for_bounds;\n+            for (const auto & path : {c_data_file_lower_bounds, c_data_file_upper_bounds})\n             {\n-                Field bounds = manifest_file_deserializer.getValueFromRowByName(i, path);\n-                for (const auto & column_stats : bounds.safeGet<Array>())\n+                if (manifest_file_deserializer.hasPath(path))\n                 {\n-                    const auto & column_number_and_bound = column_stats.safeGet<Tuple>();\n-                    Int32 number = column_number_and_bound[0].safeGet<Int32>();\n-                    const Field & bound_value = column_number_and_bound[1];\n-\n-                    if (path == c_data_file_lower_bounds)\n-                        value_for_bounds[number].first = bound_value;\n-                    else\n-                        value_for_bounds[number].second = bound_value;\n-\n-                    column_ids_which_have_bounds.insert(number);\n+                    Field bounds = manifest_file_deserializer.getValueFromRowByName(i, path);\n+                    for (const auto & column_stats : bounds.safeGet<Array>())\n+                    {\n+                        const auto & column_number_and_bound = column_stats.safeGet<Tuple>();\n+                        Int32 number = column_number_and_bound[0].safeGet<Int32>();\n+                        const Field & bound_value = column_number_and_bound[1];\n+\n+                        if (path == c_data_file_lower_bounds)\n+                            value_for_bounds[number].first = bound_value;\n+                        else\n+                            value_for_bounds[number].second = bound_value;\n+\n+                        column_ids_which_have_bounds.insert(number);\n+                    }\n                 }\n             }\n-        }\n \n-        for (const auto & [column_id, bounds] : value_for_bounds)\n-        {\n-            DB::NameAndTypePair name_and_type = schema_processor.getFieldCharacteristics(schema_id, column_id);\n+            for (const auto & [column_id, bounds] : value_for_bounds)\n+            {\n+                DB::NameAndTypePair name_and_type = schema_processor.getFieldCharacteristics(schema_id, column_id);\n \n-            String left_str;\n-            String right_str;\n-            /// lower_bound and upper_bound may be NULL.\n-            if (!bounds.first.tryGet(left_str) || !bounds.second.tryGet(right_str))\n-                continue;\n+                String left_str;\n+                String right_str;\n+                /// lower_bound and upper_bound may be NULL.\n+                if (!bounds.first.tryGet(left_str) || !bounds.second.tryGet(right_str))\n+                    continue;\n \n-            auto left = deserializeFieldFromBinaryRepr(left_str, name_and_type.type, true);\n-            auto right = deserializeFieldFromBinaryRepr(right_str, name_and_type.type, false);\n-            if (!left || !right)\n-                continue;\n+                auto left = deserializeFieldFromBinaryRepr(left_str, name_and_type.type, true);\n+                auto right = deserializeFieldFromBinaryRepr(right_str, name_and_type.type, false);\n+                if (!left || !right)\n+                    continue;\n \n-            columns_infos[column_id].hyperrectangle.emplace(*left, true, *right, true);\n+                columns_infos[column_id].hyperrectangle.emplace(*left, true, *right, true);\n+            }\n         }\n \n-        FileEntry file = FileEntry{DataFileEntry{file_path}};\n-\n         Int64 added_sequence_number = 0;\n         if (format_version_ > 1)\n         {\n@@ -300,7 +326,43 @@ ManifestFileContent::ManifestFileContent(\n                     break;\n             }\n         }\n-        this->files.emplace_back(status, added_sequence_number, file, partition_key_value, columns_infos);\n+        switch (content_type)\n+        {\n+            case FileContentType::DATA:\n+                this->data_files.emplace_back(\n+                    file_path_key,\n+                    file_path,\n+                    status,\n+                    added_sequence_number,\n+                    partition_key_value,\n+                    common_partition_specification,\n+                    columns_infos,\n+                    /*reference_data_file = */ std::nullopt);\n+                break;\n+            case FileContentType::POSITIONAL_DELETE: {\n+                /// reference_file_path can be absent in schema for some reason, though it is present in specification: https://iceberg.apache.org/spec/#manifests\n+                std::optional<String> reference_file_path = std::nullopt;\n+                if (manifest_file_deserializer.hasPath(c_data_file_referenced_data_file))\n+                {\n+                    reference_file_path\n+                        = manifest_file_deserializer.getValueFromRowByName(i, c_data_file_referenced_data_file, TypeIndex::String)\n+                              .safeGet<String>();\n+                }\n+                this->position_deletes_files.emplace_back(\n+                    file_path_key,\n+                    file_path,\n+                    status,\n+                    added_sequence_number,\n+                    partition_key_value,\n+                    common_partition_specification,\n+                    columns_infos,\n+                    reference_file_path);\n+                break;\n+            }\n+            default:\n+                throw Exception(\n+                    ErrorCodes::UNSUPPORTED_METHOD, \"FileContentType {} is not supported\", content_type);\n+        }\n     }\n }\n \n@@ -332,14 +394,16 @@ size_t ManifestFileContent::getSizeInMemory() const\n     if (partition_key_description)\n         total_size += sizeof(DB::KeyDescription);\n     total_size += column_ids_which_have_bounds.size() * sizeof(Int32);\n-    total_size += files.capacity() * sizeof(ManifestFileEntry);\n+    total_size += data_files.capacity() * sizeof(ManifestFileEntry);\n+    total_size += position_deletes_files.capacity() * sizeof(ManifestFileEntry);\n     return total_size;\n }\n \n-std::optional<Int64> ManifestFileContent::getRowsCountInAllDataFilesExcludingDeleted() const\n+std::optional<Int64> ManifestFileContent::getRowsCountInAllFilesExcludingDeleted(FileContentType content) const\n {\n     Int64 result = 0;\n-    for (const auto & file : files)\n+\n+    for (const auto & file : getFiles(content))\n     {\n         /// Have at least one column with rows count\n         bool found = false;\n@@ -353,17 +417,17 @@ std::optional<Int64> ManifestFileContent::getRowsCountInAllDataFilesExcludingDel\n                 break;\n             }\n         }\n-\n         if (!found)\n             return std::nullopt;\n     }\n     return result;\n }\n \n+\n std::optional<Int64> ManifestFileContent::getBytesCountInAllDataFiles() const\n {\n     Int64 result = 0;\n-    for (const auto & file : files)\n+    for (const auto & file : data_files)\n     {\n         /// Have at least one column with bytes count\n         bool found = false;\n@@ -383,6 +447,35 @@ std::optional<Int64> ManifestFileContent::getBytesCountInAllDataFiles() const\n     return result;\n }\n \n+std::strong_ordering operator<=>(const PartitionSpecsEntry & lhs, const PartitionSpecsEntry & rhs)\n+{\n+    return std::tie(lhs.source_id, lhs.transform_name, lhs.partition_name)\n+        <=> std::tie(rhs.source_id, rhs.transform_name, rhs.partition_name);\n+}\n+\n+template <typename A>\n+bool less(const std::vector<A> & lhs, const std::vector<A> & rhs)\n+{\n+    if (lhs.size() != rhs.size())\n+        return lhs.size() < rhs.size();\n+    return std::lexicographical_compare(lhs.begin(), lhs.end(), rhs.begin(), rhs.end(), [](const A & a, const A & b) { return a < b; });\n+}\n+\n+bool operator<(const PartitionSpecification & lhs, const PartitionSpecification & rhs)\n+{\n+    return less(lhs, rhs);\n+}\n+\n+bool operator<(const DB::Row & lhs, const DB::Row & rhs)\n+{\n+    return less(lhs, rhs);\n+}\n+\n+std::weak_ordering operator<=>(const ManifestFileEntry & lhs, const ManifestFileEntry & rhs)\n+{\n+    return std::tie(lhs.common_partition_specification, lhs.partition_key_value, lhs.added_sequence_number)\n+        <=> std::tie(rhs.common_partition_specification, rhs.partition_key_value, rhs.added_sequence_number);\n+}\n }\n \n #endif\ndiff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h\nindex 0fc613c65946..ce82f970cb40 100644\n--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h\n+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h\n@@ -26,14 +26,11 @@ enum class ManifestEntryStatus : uint8_t\n enum class FileContentType : uint8_t\n {\n     DATA = 0,\n-    POSITION_DELETES = 1,\n-    EQUALITY_DELETES = 2,\n+    POSITIONAL_DELETE = 1,\n+    EQUALITY_DELETE = 2\n };\n \n-struct DataFileEntry\n-{\n-    String file_name;\n-};\n+String FileContentTypeToString(FileContentType type);\n \n struct ColumnInfo\n {\n@@ -43,17 +40,30 @@ struct ColumnInfo\n     std::optional<DB::Range> hyperrectangle;\n };\n \n-using FileEntry = std::variant<DataFileEntry>; // In the future we will add PositionalDeleteFileEntry and EqualityDeleteFileEntry here\n+struct PartitionSpecsEntry\n+{\n+    Int32 source_id;\n+    String transform_name;\n+    String partition_name;\n+};\n+using PartitionSpecification = std::vector<PartitionSpecsEntry>;\n \n /// Description of Data file in manifest file\n struct ManifestFileEntry\n {\n+    // It's the original string in the Iceberg metadata\n+    String file_path_key;\n+    // It's a processed file path to be used by Object Storage\n+    String file_path;\n+\n     ManifestEntryStatus status;\n     Int64 added_sequence_number;\n \n-    FileEntry file;\n     DB::Row partition_key_value;\n+    PartitionSpecification common_partition_specification;\n     std::unordered_map<Int32, ColumnInfo> columns_infos;\n+\n+    std::optional<String> reference_data_file_path; // For position delete files only.\n };\n \n /**\n@@ -82,7 +92,7 @@ struct ManifestFileEntry\n  * \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n  */\n \n-class ManifestFileContent\n+class ManifestFileContent : public boost::noncopyable\n {\n public:\n     explicit ManifestFileContent(\n@@ -96,7 +106,7 @@ class ManifestFileContent\n         const std::string & table_location,\n         DB::ContextPtr context);\n \n-    const std::vector<ManifestFileEntry> & getFiles() const;\n+    const std::vector<ManifestFileEntry> & getFiles(FileContentType content_type) const;\n     Int32 getSchemaId() const;\n \n     bool hasPartitionKey() const;\n@@ -108,18 +118,25 @@ class ManifestFileContent\n \n     /// Fields with rows count in manifest files are optional\n     /// they can be absent.\n-    std::optional<Int64> getRowsCountInAllDataFilesExcludingDeleted() const;\n+    std::optional<Int64> getRowsCountInAllFilesExcludingDeleted(FileContentType content) const;\n     std::optional<Int64> getBytesCountInAllDataFiles() const;\n \n     bool hasBoundsInfoInManifests() const;\n     const std::set<Int32> & getColumnsIDsWithBounds() const;\n+\n+    ManifestFileContent(ManifestFileContent &&) = delete;\n+    ManifestFileContent & operator=(ManifestFileContent &&) = delete;\n+\n private:\n \n     Int32 schema_id;\n     Poco::JSON::Object::Ptr schema_object;\n+    PartitionSpecification common_partition_specification;\n     std::optional<DB::KeyDescription> partition_key_description;\n     // Size - number of files\n-    std::vector<ManifestFileEntry> files;\n+    std::vector<ManifestFileEntry> data_files;\n+    // Partition level deletes files\n+    std::vector<ManifestFileEntry> position_deletes_files;\n \n     std::set<Int32> column_ids_which_have_bounds;\n \n@@ -127,6 +144,11 @@ class ManifestFileContent\n \n using ManifestFilePtr = std::shared_ptr<const ManifestFileContent>;\n \n+bool operator<(const PartitionSpecification & lhs, const PartitionSpecification & rhs);\n+bool operator<(const DB::Row & lhs, const DB::Row & rhs);\n+\n+\n+std::weak_ordering operator<=>(const ManifestFileEntry & lhs, const ManifestFileEntry & rhs);\n }\n \n #endif\ndiff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.cpp b/src/Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.cpp\nnew file mode 100644\nindex 000000000000..d9c75b372fb5\n--- /dev/null\n+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.cpp\n@@ -0,0 +1,161 @@\n+#include \"config.h\"\n+\n+#if USE_AVRO\n+\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.h>\n+\n+#include <Core/Settings.h>\n+#include <Formats/FormatFactory.h>\n+#include <Formats/ReadSchemaUtils.h>\n+#include <IO/CompressionMethod.h>\n+#include <IO/ReadBufferFromFileBase.h>\n+#include <Interpreters/Context.h>\n+#include <Interpreters/ExpressionAnalyzer.h>\n+#include <Parsers/ASTFunction.h>\n+#include <Parsers/ASTIdentifier.h>\n+#include <Parsers/ASTLiteral.h>\n+#include <Processors/Formats/ISchemaReader.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h>\n+#include <Storages/ObjectStorage/StorageObjectStorageSource.h>\n+\n+namespace DB\n+{\n+\n+namespace Setting\n+{\n+extern const SettingsNonZeroUInt64 max_block_size;\n+}\n+\n+namespace ErrorCodes\n+{\n+extern const int LOGICAL_ERROR;\n+}\n+\n+void IcebergPositionDeleteTransform::initializeDeleteSources()\n+{\n+    /// Create filter on the data object to get interested rows\n+    auto iceberg_data_path = iceberg_object_info->getIcebergDataPath();\n+    ASTPtr where_ast = makeASTFunction(\n+        \"equals\",\n+        std::make_shared<ASTIdentifier>(IcebergPositionDeleteTransform::data_file_path_column_name),\n+        std::make_shared<ASTLiteral>(Field(iceberg_data_path)));\n+\n+    for (const auto & position_deletes_object : iceberg_object_info->position_deletes_objects)\n+    {\n+        /// Skip position deletes that do not match the data file path.\n+        if (position_deletes_object.reference_data_file_path.has_value()\n+            && position_deletes_object.reference_data_file_path != iceberg_data_path)\n+            continue;\n+\n+        auto object_path = position_deletes_object.file_path;\n+        auto object_metadata = object_storage->getObjectMetadata(object_path);\n+        auto object_info = std::make_shared<ObjectInfo>(object_path, object_metadata);\n+\n+\n+        Block initial_header;\n+        {\n+            std::unique_ptr<ReadBuffer> read_buf_schema\n+                = StorageObjectStorageSource::createReadBuffer(*object_info, object_storage, context, log);\n+            auto schema_reader = FormatFactory::instance().getSchemaReader(delete_object_format, *read_buf_schema, context);\n+            auto columns_with_names = schema_reader->readSchema();\n+            ColumnsWithTypeAndName initial_header_data;\n+            for (const auto & elem : columns_with_names)\n+            {\n+                initial_header_data.push_back(ColumnWithTypeAndName(elem.type, elem.name));\n+            }\n+            initial_header = Block(initial_header_data);\n+        }\n+\n+        CompressionMethod compression_method = chooseCompressionMethod(object_path, delete_object_compression_method);\n+\n+        delete_read_buffers.push_back(StorageObjectStorageSource::createReadBuffer(*object_info, object_storage, context, log));\n+\n+        auto syntax_result = TreeRewriter(context).analyze(where_ast, initial_header.getNamesAndTypesList());\n+        ExpressionAnalyzer analyzer(where_ast, syntax_result, context);\n+        std::optional<ActionsDAG> actions = analyzer.getActionsDAG(true);\n+        std::shared_ptr<const ActionsDAG> actions_dag_ptr = [&actions]()\n+        {\n+            if (actions.has_value())\n+                return std::make_shared<const ActionsDAG>(std::move(actions.value()));\n+            return std::shared_ptr<const ActionsDAG>();\n+        }();\n+\n+        auto delete_format = FormatFactory::instance().getInput(\n+            delete_object_format,\n+            *delete_read_buffers.back(),\n+            initial_header,\n+            context,\n+            context->getSettingsRef()[DB::Setting::max_block_size],\n+            format_settings,\n+            std::make_shared<FormatParserGroup>(context->getSettingsRef(), 1, actions_dag_ptr, context),\n+            true /* is_remote_fs */,\n+            compression_method);\n+\n+        delete_sources.push_back(std::move(delete_format));\n+    }\n+}\n+\n+size_t IcebergPositionDeleteTransform::getColumnIndex(const std::shared_ptr<IInputFormat> & delete_source, const String & column_name)\n+{\n+    const auto & delete_header = delete_source->getOutputs().back().getHeader();\n+    for (size_t i = 0; i < delete_header.getNames().size(); ++i)\n+    {\n+        if (delete_header.getNames()[i] == column_name)\n+        {\n+            return i;\n+        }\n+    }\n+    throw Exception(ErrorCodes::LOGICAL_ERROR, \"Could not find column {} in chunk\", column_name);\n+}\n+\n+void IcebergBitmapPositionDeleteTransform::transform(Chunk & chunk)\n+{\n+    size_t num_rows = chunk.getNumRows();\n+    IColumn::Filter delete_vector(num_rows, true);\n+    size_t num_rows_after_filtration = num_rows;\n+\n+    auto chunk_info = chunk.getChunkInfos().get<ChunkInfoRowNumOffset>();\n+    if (!chunk_info)\n+        throw Exception(ErrorCodes::LOGICAL_ERROR, \"ChunkInfoRowNumOffset does not exist\");\n+\n+    size_t row_num_offset = chunk_info->row_num_offset;\n+    for (size_t i = 0; i < num_rows; i++)\n+    {\n+        size_t row_idx = row_num_offset + i;\n+        if (bitmap.rb_contains(row_idx))\n+        {\n+            delete_vector[i] = false;\n+            num_rows_after_filtration--;\n+        }\n+    }\n+\n+    auto columns = chunk.detachColumns();\n+    for (auto & column : columns)\n+        column = column->filter(delete_vector, -1);\n+\n+    chunk.setColumns(std::move(columns), num_rows_after_filtration);\n+}\n+\n+void IcebergBitmapPositionDeleteTransform::initialize()\n+{\n+    for (auto & delete_source : delete_sources)\n+    {\n+        while (auto delete_chunk = delete_source->read())\n+        {\n+            int position_index = getColumnIndex(delete_source, IcebergPositionDeleteTransform::positions_column_name);\n+            int filename_index = getColumnIndex(delete_source, IcebergPositionDeleteTransform::data_file_path_column_name);\n+\n+            auto position_column = delete_chunk.getColumns()[position_index];\n+            auto filename_column = delete_chunk.getColumns()[filename_index];\n+\n+            for (size_t i = 0; i < delete_chunk.getNumRows(); ++i)\n+            {\n+                auto position_to_delete = position_column->get64(i);\n+                bitmap.add(position_to_delete);\n+            }\n+        }\n+    }\n+}\n+}\n+\n+#endif\ndiff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.h b/src/Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.h\nnew file mode 100644\nindex 000000000000..633c1cecae23\n--- /dev/null\n+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.h\n@@ -0,0 +1,95 @@\n+#pragma once\n+#include \"config.h\"\n+\n+#if USE_AVRO\n+\n+#include <AggregateFunctions/AggregateFunctionGroupBitmapData.h>\n+#include <Processors/ISimpleTransform.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h>\n+\n+namespace DB\n+{\n+\n+class IcebergPositionDeleteTransform : public ISimpleTransform\n+{\n+public:\n+    static constexpr const char * positions_column_name = \"pos\";\n+    static constexpr const char * data_file_path_column_name = \"file_path\";\n+\n+    IcebergPositionDeleteTransform(\n+        const Block & header_,\n+        IcebergDataObjectInfoPtr iceberg_object_info_,\n+        const ObjectStoragePtr object_storage_,\n+        const std::optional<FormatSettings> & format_settings_,\n+        ContextPtr context_,\n+        String delete_object_format_,\n+        String delete_object_compression_method_)\n+        : ISimpleTransform(header_, header_, false)\n+        , header(header_)\n+        , iceberg_object_info(iceberg_object_info_)\n+        , object_storage(object_storage_)\n+        , format_settings(format_settings_)\n+        , context(context_)\n+        , delete_object_format(delete_object_format_)\n+        , delete_object_compression_method(delete_object_compression_method_)\n+    {\n+        initializeDeleteSources();\n+    }\n+\n+    String getName() const override { return \"IcebergPositionDeleteTransform\"; }\n+\n+private:\n+    void initializeDeleteSources();\n+\n+protected:\n+    LoggerPtr log = getLogger(\"IcebergPositionDeleteTransform\");\n+    static size_t getColumnIndex(const std::shared_ptr<IInputFormat> & delete_source, const String & column_name);\n+\n+    Block header;\n+    IcebergDataObjectInfoPtr iceberg_object_info;\n+    const ObjectStoragePtr object_storage;\n+    const std::optional<FormatSettings> format_settings;\n+    ContextPtr context;\n+    const String delete_object_format;\n+    const String delete_object_compression_method;\n+\n+    /// We need to keep the read buffers alive since the delete_sources depends on them.\n+    std::vector<std::unique_ptr<ReadBuffer>> delete_read_buffers;\n+    std::vector<std::shared_ptr<IInputFormat>> delete_sources;\n+};\n+\n+class IcebergBitmapPositionDeleteTransform : public IcebergPositionDeleteTransform\n+{\n+public:\n+    IcebergBitmapPositionDeleteTransform(\n+        const Block & header_,\n+        IcebergDataObjectInfoPtr iceberg_object_info_,\n+        const ObjectStoragePtr object_storage_,\n+        const std::optional<FormatSettings> & format_settings_,\n+        ContextPtr context_,\n+        String delete_object_format_,\n+        String delete_object_compression_method_ = \"auto\")\n+        : IcebergPositionDeleteTransform(\n+              header_,\n+              iceberg_object_info_,\n+              object_storage_,\n+              format_settings_,\n+              context_,\n+              delete_object_format_,\n+              delete_object_compression_method_)\n+    {\n+        initialize();\n+    }\n+\n+    String getName() const override { return \"IcebergBitmapPositionDeleteTransform\"; }\n+\n+    void transform(Chunk & chunk) override;\n+\n+private:\n+    void initialize();\n+    RoaringBitmapWithSmallSet<size_t, 32> bitmap;\n+};\n+\n+}\n+\n+#endif\ndiff --git a/src/Storages/ObjectStorage/DataLakes/Iceberg/Snapshot.h b/src/Storages/ObjectStorage/DataLakes/Iceberg/Snapshot.h\nindex 6a0abf753c7e..a2a591567863 100644\n--- a/src/Storages/ObjectStorage/DataLakes/Iceberg/Snapshot.h\n+++ b/src/Storages/ObjectStorage/DataLakes/Iceberg/Snapshot.h\n@@ -16,6 +16,14 @@ struct IcebergSnapshot\n     Int64 snapshot_id;\n     std::optional<size_t> total_rows;\n     std::optional<size_t> total_bytes;\n+    std::optional<size_t> total_positional_delete_rows;\n+\n+    std::optional<size_t> getTotalRows() const\n+    {\n+        if (total_rows.has_value() && total_positional_delete_rows.has_value())\n+            return *total_rows - *total_positional_delete_rows;\n+        return std::nullopt;\n+    }\n };\n \n struct IcebergHistoryRecord\ndiff --git a/src/Storages/ObjectStorage/StorageObjectStorage.h b/src/Storages/ObjectStorage/StorageObjectStorage.h\nindex e7eb1ef3e60f..30ffccd61194 100644\n--- a/src/Storages/ObjectStorage/StorageObjectStorage.h\n+++ b/src/Storages/ObjectStorage/StorageObjectStorage.h\n@@ -1,18 +1,19 @@\n #pragma once\n #include <Core/SchemaInferenceMode.h>\n #include <Disks/ObjectStorages/IObjectStorage.h>\n+#include <Formats/FormatSettings.h>\n+#include <Interpreters/ActionsDAG.h>\n+#include <Interpreters/Context_fwd.h>\n #include <Parsers/IAST_fwd.h>\n #include <Processors/Formats/IInputFormat.h>\n+#include <Processors/ISimpleTransform.h>\n+#include <Storages/ColumnsDescription.h>\n #include <Storages/IStorage.h>\n+#include <Storages/ObjectStorage/DataLakes/DataLakeStorageSettings.h>\n+#include <Storages/ObjectStorage/DataLakes/IDataLakeMetadata.h>\n #include <Storages/ObjectStorage/IObjectIterator.h>\n #include <Storages/prepareReadingFromFormat.h>\n #include <Common/threadPoolCallbackRunner.h>\n-#include <Interpreters/ActionsDAG.h>\n-#include <Storages/ColumnsDescription.h>\n-#include <Storages/ObjectStorage/DataLakes/IDataLakeMetadata.h>\n-#include <Storages/ObjectStorage/DataLakes/DataLakeStorageSettings.h>\n-#include <Formats/FormatSettings.h>\n-#include <Interpreters/Context_fwd.h>\n \n #include <memory>\n \n@@ -27,7 +28,7 @@ using StorageObjectStorageSettingsPtr = std::shared_ptr<StorageObjectStorageSett\n \n namespace ErrorCodes\n {\n-    extern const int NOT_IMPLEMENTED;\n+extern const int NOT_IMPLEMENTED;\n }\n \n \n@@ -90,17 +91,11 @@ class StorageObjectStorage : public IStorage\n         size_t max_block_size,\n         size_t num_streams) override;\n \n-    SinkToStoragePtr write(\n-        const ASTPtr & query,\n-        const StorageMetadataPtr & metadata_snapshot,\n-        ContextPtr context,\n-        bool async_insert) override;\n+    SinkToStoragePtr\n+    write(const ASTPtr & query, const StorageMetadataPtr & metadata_snapshot, ContextPtr context, bool async_insert) override;\n \n     void truncate(\n-        const ASTPtr & query,\n-        const StorageMetadataPtr & metadata_snapshot,\n-        ContextPtr local_context,\n-        TableExclusiveLockHolder &) override;\n+        const ASTPtr & query, const StorageMetadataPtr & metadata_snapshot, ContextPtr local_context, TableExclusiveLockHolder &) override;\n \n     bool supportsPartitionBy() const override { return true; }\n \n@@ -142,7 +137,6 @@ class StorageObjectStorage : public IStorage\n     void addInferredEngineArgsToCreateQuery(ASTs & args, const ContextPtr & context) const override;\n \n     bool updateExternalDynamicMetadataIfExists(ContextPtr query_context) override;\n-\n     IDataLakeMetadata * getExternalMetadata(ContextPtr query_context);\n \n     std::optional<UInt64> totalRows(ContextPtr query_context) const override;\n@@ -218,7 +212,8 @@ class StorageObjectStorage::Configuration\n \n     /// Add/replace structure and format arguments in the AST arguments if they have 'auto' values.\n     virtual void addStructureAndFormatToArgsIfNeeded(\n-        ASTs & args, const String & structure_, const String & format_, ContextPtr context, bool with_structure) = 0;\n+        ASTs & args, const String & structure_, const String & format_, ContextPtr context, bool with_structure)\n+        = 0;\n \n     bool withPartitionWildcard() const;\n     bool withGlobs() const { return isPathWithGlobs() || isNamespaceWithGlobs(); }\n@@ -232,7 +227,7 @@ class StorageObjectStorage::Configuration\n     virtual std::string getPathInArchive() const;\n \n     virtual void check(ContextPtr context) const;\n-    virtual void validateNamespace(const String & /* name */) const {}\n+    virtual void validateNamespace(const String & /* name */) const { }\n \n     virtual ObjectStoragePtr createObjectStorage(ContextPtr context, bool is_readonly) = 0;\n     virtual bool isStaticConfiguration() const { return true; }\n@@ -280,9 +275,24 @@ class StorageObjectStorage::Configuration\n         bool if_not_updated_before,\n         bool check_consistent_with_previous_metadata);\n \n+    virtual bool hasPositionDeleteTransformer(const ObjectInfoPtr & /*object_info*/) const { return false; }\n+\n+    virtual std::shared_ptr<ISimpleTransform> getPositionDeleteTransformer(\n+        const ObjectInfoPtr & /*object_info*/,\n+        const Block & /*header*/,\n+        const std::optional<FormatSettings> & /*format_settings*/,\n+        ContextPtr /*context_*/) const\n+    {\n+        throw Exception(\n+            ErrorCodes::NOT_IMPLEMENTED,\n+            \"Method getPositionDeleteTransformer() is not implemented for configuration type {}\",\n+            getTypeName());\n+    }\n+\n     virtual const DataLakeStorageSettings & getDataLakeSettings() const\n     {\n-        throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"Method getDataLakeSettings() is not implemented for configuration type {}\", getTypeName());\n+        throw Exception(\n+            ErrorCodes::NOT_IMPLEMENTED, \"Method getDataLakeSettings() is not implemented for configuration type {}\", getTypeName());\n     }\n \n     String format = \"auto\";\ndiff --git a/src/Storages/ObjectStorage/StorageObjectStorageSource.cpp b/src/Storages/ObjectStorage/StorageObjectStorageSource.cpp\nindex 735e1704d6df..75d8acab0adf 100644\n--- a/src/Storages/ObjectStorage/StorageObjectStorageSource.cpp\n+++ b/src/Storages/ObjectStorage/StorageObjectStorageSource.cpp\n@@ -1,4 +1,5 @@\n #include \"StorageObjectStorageSource.h\"\n+\n #include <memory>\n #include <optional>\n #include <Common/SipHash.h>\n@@ -487,6 +488,14 @@ StorageObjectStorageSource::ReaderHolder StorageObjectStorageSource::createReade\n \n         builder.init(Pipe(input_format));\n \n+        if (configuration->hasPositionDeleteTransformer(object_info))\n+        {\n+            builder.addSimpleTransform(\n+                [&](const Block & header)\n+                { return configuration->getPositionDeleteTransformer(object_info, header, format_settings, context_); });\n+        }\n+\n+\n         std::shared_ptr<const ActionsDAG> transformer;\n         if (object_info->data_lake_metadata)\n             transformer = object_info->data_lake_metadata->transform;\n",
  "test_patch": "diff --git a/tests/integration/test_storage_iceberg/test.py b/tests/integration/test_storage_iceberg/test.py\nindex f4d866ea62bb..c0c13e5f6bd2 100644\n--- a/tests/integration/test_storage_iceberg/test.py\n+++ b/tests/integration/test_storage_iceberg/test.py\n@@ -1567,17 +1567,23 @@ def execute_spark_query(query: str):\n \n \n @pytest.mark.parametrize(\"storage_type\", [\"s3\", \"azure\", \"local\"])\n-def test_row_based_deletes(started_cluster, storage_type):\n+def test_position_deletes(started_cluster, storage_type):\n     instance = started_cluster.instances[\"node1\"]\n     spark = started_cluster.spark_session\n-    TABLE_NAME = \"test_row_based_deletes_\" + storage_type + \"_\" + get_uuid_str()\n+    TABLE_NAME = \"test_position_deletes_\" + storage_type + \"_\" + get_uuid_str()\n \n     spark.sql(\n-        f\"CREATE TABLE {TABLE_NAME} (id bigint, data string) USING iceberg TBLPROPERTIES ('format-version' = '2', 'write.update.mode'='merge-on-read', 'write.delete.mode'='merge-on-read', 'write.merge.mode'='merge-on-read')\"\n-    )\n-    spark.sql(\n-        f\"INSERT INTO {TABLE_NAME} select id, char(id + ascii('a')) from range(100)\"\n+        f\"\"\"\n+        CREATE TABLE {TABLE_NAME} (id bigint, data string) USING iceberg PARTITIONED BY (bucket(5, id)) TBLPROPERTIES ('format-version' = '2', 'write.update.mode'=\n+        'merge-on-read', 'write.delete.mode'='merge-on-read', 'write.merge.mode'='merge-on-read')\n+        \"\"\"\n     )\n+    spark.sql(f\"INSERT INTO {TABLE_NAME} select id, char(id + ascii('a')) from range(10, 100)\")\n+\n+    def get_array(query_result: str):\n+        arr = sorted([int(x) for x in query_result.strip().split(\"\\n\")])\n+        print(arr)\n+        return arr\n \n     default_upload_directory(\n         started_cluster,\n@@ -1588,9 +1594,9 @@ def test_row_based_deletes(started_cluster, storage_type):\n \n     create_iceberg_table(storage_type, instance, TABLE_NAME, started_cluster)\n \n-    assert int(instance.query(f\"SELECT count() FROM {TABLE_NAME}\")) == 100\n+    assert int(instance.query(f\"SELECT count() FROM {TABLE_NAME}\")) == 90\n \n-    spark.sql(f\"DELETE FROM {TABLE_NAME} WHERE id < 10\")\n+    spark.sql(f\"DELETE FROM {TABLE_NAME} WHERE id < 20\")\n     default_upload_directory(\n         started_cluster,\n         storage_type,\n@@ -1598,8 +1604,64 @@ def test_row_based_deletes(started_cluster, storage_type):\n         \"\",\n     )\n \n-    error = instance.query_and_get_error(f\"SELECT * FROM {TABLE_NAME}\")\n-    assert \"UNSUPPORTED_METHOD\" in error\n+    assert get_array(instance.query(f\"SELECT id FROM {TABLE_NAME}\")) == list(range(20, 100))\n+\n+    # Check that filters are applied after deletes\n+    assert int(instance.query(f\"SELECT count() FROM {TABLE_NAME} where id >= 15\")) == 80\n+    assert (\n+        int(\n+            instance.query(\n+                f\"SELECT count() FROM {TABLE_NAME} where id >= 15 SETTINGS optimize_trivial_count_query=1\"\n+            )\n+        )\n+        == 80\n+    )\n+\n+    # Check deletes after deletes\n+    spark.sql(f\"DELETE FROM {TABLE_NAME} WHERE id >= 90\")\n+    default_upload_directory(\n+        started_cluster,\n+        storage_type,\n+        f\"/iceberg_data/default/{TABLE_NAME}/\",\n+        \"\",\n+    )\n+    assert get_array(instance.query(f\"SELECT id FROM {TABLE_NAME}\")) == list(range(20, 90))\n+\n+    spark.sql(f\"ALTER TABLE {TABLE_NAME} ADD PARTITION FIELD truncate(1, data)\")\n+\n+    # Check adds after deletes\n+    spark.sql(\n+        f\"INSERT INTO {TABLE_NAME} select id, char(id + ascii('a')) from range(100, 200)\"\n+    )\n+    default_upload_directory(\n+        started_cluster,\n+        storage_type,\n+        f\"/iceberg_data/default/{TABLE_NAME}/\",\n+        f\"/iceberg_data/default/{TABLE_NAME}/\",\n+    )\n+    assert get_array(instance.query(f\"SELECT id FROM {TABLE_NAME}\")) == list(range(20, 90)) + list(\n+        range(100, 200)\n+    )\n+\n+    # Check deletes after adds\n+    spark.sql(f\"DELETE FROM {TABLE_NAME} WHERE id >= 150\")\n+    default_upload_directory(\n+        started_cluster,\n+        storage_type,\n+        f\"/iceberg_data/default/{TABLE_NAME}/\",\n+        \"\",\n+    )\n+    assert get_array(instance.query(f\"SELECT id FROM {TABLE_NAME}\")) == list(range(20, 90)) + list(\n+        range(100, 150)\n+    )\n+\n+    assert get_array(\n+        instance.query(\n+            f\"SELECT id FROM {TABLE_NAME} WHERE id = 70 SETTINGS use_iceberg_partition_pruning = 1\"\n+        )\n+    ) == [70]\n+\n+    # Clean up\n     instance.query(f\"DROP TABLE {TABLE_NAME}\")\n \n \n",
  "problem_statement": "Add more info to the `RelativePathWithMetadata` for Iceberg data object\n### Company or project name\n\n_No response_\n\n### Use case\n\nFor an Iceberg data object, to generate meaningful data, it may need more info from other objects:\n- position deletes objects for Iceberg V2\n- deletion vector objects for Iceberg V3\n\n### Describe the solution you'd like\n\nCreate a new struct inherited from `RelativePathWithMetadata` with more properties for the deletion information, like\n```\nstruct DeletionFileEntry\n{\n    String file_name;\n    FileContentType type;\n};\n\n// It can be reused for all of the data files under the same partition for Iceberg V2\nstruct DeletionFileEntryList\n{\n    std::vector<DeletionFileEntry> deletion_files;\n}\n\n// DeletionFileEntryList for V2, DeletionFileEntry for V3\nusing DeletionInfo = std::variant<DeletionFileEntryList, DeletionFileEntry>;\n\nstruct IcebergDataObjectInfo : public RelativePathWithMetadata\n{\n    std::optional<DeletionInfo> deletion_info;\n}\n```\n\n### Describe alternatives you've considered\n\n_No response_\n\n### Additional context\n\n_No response_\n",
  "hints_text": "",
  "created_at": "2025-05-15T06:11:57Z",
  "modified_files": [
    "src/Processors/Formats/IInputFormat.h",
    "src/Processors/Formats/Impl/ParquetBlockInputFormat.cpp",
    "src/Processors/Formats/Impl/ParquetBlockInputFormat.h",
    "src/Storages/ObjectStorage/DataLakes/DataLakeConfiguration.h",
    "src/Storages/ObjectStorage/DataLakes/IDataLakeMetadata.h",
    "src/Storages/ObjectStorage/DataLakes/Iceberg/Constant.h",
    "src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.cpp",
    "src/Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h",
    "src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.cpp",
    "src/Storages/ObjectStorage/DataLakes/Iceberg/ManifestFile.h",
    "b/src/Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.cpp",
    "b/src/Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.h",
    "src/Storages/ObjectStorage/DataLakes/Iceberg/Snapshot.h",
    "src/Storages/ObjectStorage/StorageObjectStorage.h",
    "src/Storages/ObjectStorage/StorageObjectStorageSource.cpp"
  ],
  "modified_test_files": [
    "tests/integration/test_storage_iceberg/test.py"
  ]
}