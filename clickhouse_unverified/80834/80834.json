{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 80834,
  "instance_id": "ClickHouse__ClickHouse-80834",
  "issue_numbers": [
    "80443"
  ],
  "base_commit": "dc3490a0a4c27e69dcf59ccb2a160c9511d93325",
  "patch": "diff --git a/src/Interpreters/ActionsDAG.cpp b/src/Interpreters/ActionsDAG.cpp\nindex e39a9fb5750f..f16e1a8883b7 100644\n--- a/src/Interpreters/ActionsDAG.cpp\n+++ b/src/Interpreters/ActionsDAG.cpp\n@@ -2485,12 +2485,13 @@ ColumnsWithTypeAndName prepareFunctionArguments(const ActionsDAG::NodeRawConstPt\n ///\n /// Result actions add single column with conjunction result (it is always first in outputs).\n /// No other columns are added or removed.\n-std::optional<ActionsDAG> ActionsDAG::createActionsForConjunction(NodeRawConstPtrs conjunction, const ColumnsWithTypeAndName & all_inputs)\n+std::optional<ActionsDAG::ActionsForFilterPushDown> ActionsDAG::createActionsForConjunction(NodeRawConstPtrs conjunction, const ColumnsWithTypeAndName & all_inputs)\n {\n     if (conjunction.empty())\n         return {};\n \n     ActionsDAG actions;\n+    bool remove_filter = true;\n \n     FunctionOverloadResolverPtr func_builder_and = std::make_unique<FunctionToOverloadResolverAdaptor>(std::make_shared<FunctionAnd>());\n \n@@ -2557,7 +2558,8 @@ std::optional<ActionsDAG> ActionsDAG::createActionsForConjunction(NodeRawConstPt\n         result_predicate = &actions.addFunction(func_builder_and, std::move(args), {});\n     }\n \n-    actions.outputs.push_back(result_predicate);\n+    size_t filter_pos = 0;\n+    bool has_input_name_collision = false;\n \n     for (const auto & col : all_inputs)\n     {\n@@ -2573,14 +2575,39 @@ std::optional<ActionsDAG> ActionsDAG::createActionsForConjunction(NodeRawConstPt\n         }\n \n         /// We should not add result_predicate into the outputs for the second time.\n-        if (input->result_name != result_predicate->result_name)\n-            actions.outputs.push_back(input);\n+        /// If the predicate is an input, do not remove it.\n+        if (input == result_predicate)\n+        {\n+            remove_filter = false;\n+            filter_pos = actions.outputs.size();\n+        }\n+        /// Predicate name has a collision with another node. Need to rename it.\n+        else if (input->result_name == result_predicate->result_name)\n+            has_input_name_collision = true;\n+\n+        actions.outputs.push_back(input);\n     }\n \n-    return actions;\n+    if (has_input_name_collision)\n+    {\n+        for (size_t idx = 0;; ++idx)\n+        {\n+            std::string rename = fmt::format(\"_filter_{}_{}\", result_predicate->result_name, idx);\n+            if (required_inputs.contains(rename))\n+                continue;\n+\n+            result_predicate = &actions.addAlias(*result_predicate, std::move(rename));\n+            break;\n+        }\n+    }\n+\n+    if (remove_filter)\n+        actions.outputs.insert(actions.outputs.begin(), result_predicate);\n+\n+    return ActionsForFilterPushDown{std::move(actions), filter_pos, remove_filter};\n }\n \n-std::optional<ActionsDAG> ActionsDAG::splitActionsForFilterPushDown(\n+std::optional<ActionsDAG::ActionsForFilterPushDown> ActionsDAG::splitActionsForFilterPushDown(\n     const std::string & filter_name,\n     bool removes_filter,\n     const Names & available_inputs,\n@@ -2744,12 +2771,14 @@ ActionsDAG::ActionsForJOINFilterPushDown ActionsDAG::splitActionsForJOINFilterPu\n     auto left_stream_filter_to_push_down = createActionsForConjunction(left_stream_allowed_conjunctions, left_stream_header.getColumnsWithTypeAndName());\n     auto right_stream_filter_to_push_down = createActionsForConjunction(right_stream_allowed_conjunctions, right_stream_header.getColumnsWithTypeAndName());\n \n-    auto replace_equivalent_columns_in_filter = [](const ActionsDAG & filter,\n+    auto replace_equivalent_columns_in_filter = [](\n+        const ActionsDAG & filter,\n+        size_t filter_pos,\n         const Block & stream_header,\n         const std::unordered_map<std::string, ColumnWithTypeAndName> & columns_to_replace)\n     {\n-        auto updated_filter = ActionsDAG::buildFilterActionsDAG({filter.getOutputs()[0]}, columns_to_replace);\n-        chassert(updated_filter->getOutputs().size() == 1);\n+        auto updated_filter = *ActionsDAG::buildFilterActionsDAG({filter.getOutputs()[filter_pos]}, columns_to_replace);\n+        chassert(updated_filter.getOutputs().size() == 1);\n \n         /** If result filter to left or right stream has column that is one of the stream inputs, we need distinguish filter column from\n           * actual input column. It is necessary because after filter step, filter column became constant column with value 1, and\n@@ -2757,16 +2786,16 @@ ActionsDAG::ActionsForJOINFilterPushDown ActionsDAG::splitActionsForJOINFilterPu\n           *\n           * Example: SELECT key FROM ( SELECT key FROM t1 ) AS t1 JOIN ( SELECT key FROM t1 ) AS t2 ON t1.key = t2.key WHERE key;\n           */\n-        const auto * stream_filter_node = updated_filter->getOutputs()[0];\n+        const auto * stream_filter_node = updated_filter.getOutputs()[0];\n         if (stream_header.has(stream_filter_node->result_name))\n         {\n-            const auto & alias_node = updated_filter->addAlias(*stream_filter_node, \"__filter\" + stream_filter_node->result_name);\n-            updated_filter->getOutputs()[0] = &alias_node;\n+            const auto & alias_node = updated_filter.addAlias(*stream_filter_node, \"__filter\" + stream_filter_node->result_name);\n+            updated_filter.getOutputs()[0] = &alias_node;\n         }\n \n         std::unordered_map<std::string, std::list<const Node *>> updated_filter_inputs;\n \n-        for (const auto & input : updated_filter->getInputs())\n+        for (const auto & input : updated_filter.getInputs())\n             updated_filter_inputs[input->result_name].push_back(input);\n \n         for (const auto & input : filter.getInputs())\n@@ -2778,9 +2807,9 @@ ActionsDAG::ActionsForJOINFilterPushDown ActionsDAG::splitActionsForJOINFilterPu\n \n             auto it = columns_to_replace.find(input->result_name);\n             if (it != columns_to_replace.end())\n-                updated_filter_input_node = &updated_filter->addInput(it->second);\n+                updated_filter_input_node = &updated_filter.addInput(it->second);\n             else\n-                updated_filter_input_node = &updated_filter->addInput({input->column, input->result_type, input->result_name});\n+                updated_filter_input_node = &updated_filter.addInput({input->column, input->result_type, input->result_name});\n \n             updated_filter_inputs[input->result_name].push_back(updated_filter_input_node);\n         }\n@@ -2791,7 +2820,7 @@ ActionsDAG::ActionsForJOINFilterPushDown ActionsDAG::splitActionsForJOINFilterPu\n             auto & list = updated_filter_inputs[input_column.name];\n             if (list.empty())\n             {\n-                input = &updated_filter->addInput(input_column);\n+                input = &updated_filter.addInput(input_column);\n             }\n             else\n             {\n@@ -2799,20 +2828,22 @@ ActionsDAG::ActionsForJOINFilterPushDown ActionsDAG::splitActionsForJOINFilterPu\n                 list.pop_front();\n             }\n \n-            if (input != updated_filter->getOutputs()[0])\n-                updated_filter->outputs.push_back(input);\n+            if (input != updated_filter.getOutputs()[0])\n+                updated_filter.outputs.push_back(input);\n         }\n \n         return updated_filter;\n     };\n \n     if (left_stream_filter_to_push_down)\n-        left_stream_filter_to_push_down = replace_equivalent_columns_in_filter(*left_stream_filter_to_push_down,\n+        left_stream_filter_to_push_down->dag = replace_equivalent_columns_in_filter(left_stream_filter_to_push_down->dag,\n+            left_stream_filter_to_push_down->filter_pos,\n             left_stream_header,\n             equivalent_right_stream_column_to_left_stream_column);\n \n     if (right_stream_filter_to_push_down)\n-        right_stream_filter_to_push_down = replace_equivalent_columns_in_filter(*right_stream_filter_to_push_down,\n+        right_stream_filter_to_push_down->dag = replace_equivalent_columns_in_filter(right_stream_filter_to_push_down->dag,\n+            right_stream_filter_to_push_down->filter_pos,\n             right_stream_header,\n             equivalent_left_stream_column_to_right_stream_column);\n \n@@ -2827,24 +2858,28 @@ ActionsDAG::ActionsForJOINFilterPushDown ActionsDAG::splitActionsForJOINFilterPu\n \n     bool left_stream_filter_removes_filter = true;\n     bool right_stream_filter_removes_filter = true;\n+    std::optional<ActionsDAG> left_stream_filter_to_push_down_dag;\n+    std::optional<ActionsDAG> right_stream_filter_to_push_down_dag;\n \n     if (left_stream_filter_to_push_down)\n     {\n-        const auto & left_stream_filter_column_name = left_stream_filter_to_push_down->getOutputs()[0]->result_name;\n+        const auto & left_stream_filter_column_name = left_stream_filter_to_push_down->dag.getOutputs()[0]->result_name;\n         left_stream_filter_removes_filter = !left_stream_header.has(left_stream_filter_column_name);\n+        left_stream_filter_to_push_down_dag = std::move(left_stream_filter_to_push_down->dag);\n     }\n \n     if (right_stream_filter_to_push_down)\n     {\n-        const auto & right_stream_filter_column_name = right_stream_filter_to_push_down->getOutputs()[0]->result_name;\n+        const auto & right_stream_filter_column_name = right_stream_filter_to_push_down->dag.getOutputs()[0]->result_name;\n         right_stream_filter_removes_filter = !right_stream_header.has(right_stream_filter_column_name);\n+        right_stream_filter_to_push_down_dag = std::move(right_stream_filter_to_push_down->dag);\n     }\n \n     ActionsDAG::ActionsForJOINFilterPushDown result\n     {\n-        .left_stream_filter_to_push_down = std::move(left_stream_filter_to_push_down),\n+        .left_stream_filter_to_push_down = std::move(left_stream_filter_to_push_down_dag),\n         .left_stream_filter_removes_filter = left_stream_filter_removes_filter,\n-        .right_stream_filter_to_push_down = std::move(right_stream_filter_to_push_down),\n+        .right_stream_filter_to_push_down = std::move(right_stream_filter_to_push_down_dag),\n         .right_stream_filter_removes_filter = right_stream_filter_removes_filter\n     };\n \ndiff --git a/src/Interpreters/ActionsDAG.h b/src/Interpreters/ActionsDAG.h\nindex 9e2e94c173fe..50c18b513819 100644\n--- a/src/Interpreters/ActionsDAG.h\n+++ b/src/Interpreters/ActionsDAG.h\n@@ -369,6 +369,7 @@ class ActionsDAG\n       */\n     bool isFilterAlwaysFalseForDefaultValueInputs(const std::string & filter_name, const Block & input_stream_header) const;\n \n+    struct ActionsForFilterPushDown;\n     /// Create actions which may calculate part of filter using only available_inputs.\n     /// If nothing may be calculated, returns nullptr.\n     /// Otherwise, return actions which inputs are from available_inputs.\n@@ -386,7 +387,7 @@ class ActionsDAG\n     /// columns will be transformed like `x, y, z` -> `z > 0, z, x, y` -(remove filter)-> `z, x, y`.\n     /// To avoid it, add inputs from `all_inputs` list,\n     /// so actions `x, y, z -> z > 0, x, y, z` -(remove filter)-> `x, y, z` will not change columns order.\n-    std::optional<ActionsDAG> splitActionsForFilterPushDown(\n+    std::optional<ActionsForFilterPushDown> splitActionsForFilterPushDown(\n         const std::string & filter_name,\n         bool removes_filter,\n         const Names & available_inputs,\n@@ -475,7 +476,7 @@ class ActionsDAG\n     void compileFunctions(size_t min_count_to_compile_expression, const std::unordered_set<const Node *> & lazy_executed_nodes = {});\n #endif\n \n-    static std::optional<ActionsDAG> createActionsForConjunction(NodeRawConstPtrs conjunction, const ColumnsWithTypeAndName & all_inputs);\n+    static std::optional<ActionsForFilterPushDown> createActionsForConjunction(NodeRawConstPtrs conjunction, const ColumnsWithTypeAndName & all_inputs);\n \n     void removeUnusedConjunctions(NodeRawConstPtrs rejected_conjunctions, Node * predicate, bool removes_filter);\n };\n@@ -487,6 +488,13 @@ struct ActionsDAG::SplitResult\n     std::unordered_map<const Node *, const Node *> split_nodes_mapping;\n };\n \n+struct ActionsDAG::ActionsForFilterPushDown\n+{\n+    ActionsDAG dag;\n+    size_t filter_pos;\n+    bool remove_filter;\n+};\n+\n struct ActionsDAG::ActionsForJOINFilterPushDown\n {\n     std::optional<ActionsDAG> left_stream_filter_to_push_down;\ndiff --git a/src/Processors/QueryPlan/Optimizations/filterPushDown.cpp b/src/Processors/QueryPlan/Optimizations/filterPushDown.cpp\nindex fac2471422cb..e04e53c98c6e 100644\n--- a/src/Processors/QueryPlan/Optimizations/filterPushDown.cpp\n+++ b/src/Processors/QueryPlan/Optimizations/filterPushDown.cpp\n@@ -37,17 +37,6 @@ namespace DB::ErrorCodes\n namespace DB::QueryPlanOptimizations\n {\n \n-static bool filterColumnIsNotAmongAggregatesArguments(const AggregateDescriptions & aggregates, const std::string & filter_column_name)\n-{\n-    for (const auto & aggregate : aggregates)\n-    {\n-        const auto & argument_names = aggregate.argument_names;\n-        if (std::find(argument_names.begin(), argument_names.end(), filter_column_name) != argument_names.end())\n-            return false;\n-    }\n-    return true;\n-}\n-\n /// Assert that `node->children` has at least `child_num` elements\n static void checkChildrenSize(QueryPlan::Node * node, size_t child_num)\n {\n@@ -104,7 +93,7 @@ static NameSet findIdentifiersOfNode(const ActionsDAG::Node * node)\n     return res;\n }\n \n-static std::optional<ActionsDAG> splitFilter(QueryPlan::Node * parent_node, bool step_changes_the_number_of_rows, const Names & available_inputs, size_t child_idx = 0)\n+static std::optional<ActionsDAG::ActionsForFilterPushDown> splitFilter(QueryPlan::Node * parent_node, bool step_changes_the_number_of_rows, const Names & available_inputs, size_t child_idx = 0)\n {\n     QueryPlan::Node * child_node = parent_node->children.front();\n     checkChildrenSize(child_node, child_idx + 1);\n@@ -122,9 +111,11 @@ static std::optional<ActionsDAG> splitFilter(QueryPlan::Node * parent_node, bool\n     return expression.splitActionsForFilterPushDown(filter_column_name, removes_filter, available_inputs, all_inputs, allow_deterministic_functions);\n }\n \n-static size_t\n-addNewFilterStepOrThrow(QueryPlan::Node * parent_node, QueryPlan::Nodes & nodes, ActionsDAG split_filter,\n-                    bool can_remove_filter = true, size_t child_idx = 0, bool update_parent_filter = true)\n+static size_t addNewFilterStepOrThrow(\n+    QueryPlan::Node * parent_node,\n+    QueryPlan::Nodes & nodes,\n+    ActionsDAG::ActionsForFilterPushDown split_filter,\n+    size_t child_idx = 0, bool update_parent_filter = true)\n {\n     QueryPlan::Node * child_node = parent_node->children.front();\n     checkChildrenSize(child_node, child_idx + 1);\n@@ -151,18 +142,9 @@ addNewFilterStepOrThrow(QueryPlan::Node * parent_node, QueryPlan::Nodes & nodes,\n     /// Expression/Filter -> Child -> Filter -> Something\n \n     /// New filter column is the first one.\n-    String split_filter_column_name = split_filter.getOutputs().front()->result_name;\n-\n-    // If no new columns added, filter just used one of the input columns as-is and moved it to the front, move it back to keep aggregation key in order.\n-    if (const auto & input = node.children.at(0)->step->getOutputHeader(); split_filter.getOutputs().size() == input.columns())\n-    {\n-        auto pos = input.getPositionByName(split_filter_column_name);\n-        if (pos != 0)\n-            std::rotate(split_filter.getOutputs().begin(), split_filter.getOutputs().begin() + 1, split_filter.getOutputs().begin() + pos + 1);\n-    }\n-\n+    String split_filter_column_name = split_filter.dag.getOutputs()[split_filter.filter_pos]->result_name;\n     node.step = std::make_unique<FilterStep>(\n-        node.children.at(0)->step->getOutputHeader(), std::move(split_filter), std::move(split_filter_column_name), can_remove_filter);\n+        node.children.at(0)->step->getOutputHeader(), std::move(split_filter.dag), std::move(split_filter_column_name), split_filter.remove_filter);\n \n     child->updateInputHeader(node.step->getOutputHeader(), child_idx);\n \n@@ -186,12 +168,15 @@ addNewFilterStepOrThrow(QueryPlan::Node * parent_node, QueryPlan::Nodes & nodes,\n     return 3;\n }\n \n-static size_t\n-tryAddNewFilterStep(QueryPlan::Node * parent_node, bool step_changes_the_number_of_rows, QueryPlan::Nodes & nodes, const Names & allowed_inputs,\n-                    bool can_remove_filter = true, size_t child_idx = 0)\n+static size_t tryAddNewFilterStep(\n+    QueryPlan::Node * parent_node,\n+    bool step_changes_the_number_of_rows,\n+    QueryPlan::Nodes & nodes,\n+    const Names & allowed_inputs,\n+    size_t child_idx = 0)\n {\n     if (auto split_filter = splitFilter(parent_node, step_changes_the_number_of_rows, allowed_inputs, child_idx))\n-        return addNewFilterStepOrThrow(parent_node, nodes, std::move(*split_filter), can_remove_filter, child_idx);\n+        return addNewFilterStepOrThrow(parent_node, nodes, std::move(*split_filter), child_idx);\n     return 0;\n }\n \n@@ -417,8 +402,7 @@ static size_t tryPushDownOverJoinStep(QueryPlan::Node * parent_node, QueryPlan::\n         const auto & result_name = join_filter_push_down_actions.left_stream_filter_to_push_down->getOutputs()[0]->result_name;\n         updated_steps += addNewFilterStepOrThrow(parent_node,\n             nodes,\n-            std::move(*join_filter_push_down_actions.left_stream_filter_to_push_down),\n-            join_filter_push_down_actions.left_stream_filter_removes_filter,\n+            {std::move(*join_filter_push_down_actions.left_stream_filter_to_push_down), 0, join_filter_push_down_actions.left_stream_filter_removes_filter},\n             0 /*child_idx*/,\n             false /*update_parent_filter*/);\n         LOG_DEBUG(&Poco::Logger::get(\"QueryPlanOptimizations\"),\n@@ -442,8 +426,7 @@ static size_t tryPushDownOverJoinStep(QueryPlan::Node * parent_node, QueryPlan::\n         const auto & result_name = join_filter_push_down_actions.right_stream_filter_to_push_down->getOutputs()[0]->result_name;\n         updated_steps += addNewFilterStepOrThrow(parent_node,\n             nodes,\n-            std::move(*join_filter_push_down_actions.right_stream_filter_to_push_down),\n-            join_filter_push_down_actions.right_stream_filter_removes_filter,\n+            {std::move(*join_filter_push_down_actions.right_stream_filter_to_push_down), 0, join_filter_push_down_actions.right_stream_filter_removes_filter},\n             1 /*child_idx*/,\n             false /*update_parent_filter*/);\n         LOG_DEBUG(&Poco::Logger::get(\"QueryPlanOptimizations\"),\n@@ -536,14 +519,7 @@ size_t tryPushDownFilter(QueryPlan::Node * parent_node, QueryPlan::Nodes & nodes\n         if (keys.empty())\n             return 0;\n \n-        const bool filter_column_is_not_among_aggregation_keys\n-            = std::find(keys.begin(), keys.end(), filter->getFilterColumnName()) == keys.end();\n-\n-        /// When we only merging aggregated data, we do not need aggregation arguments.\n-        bool filter_is_not_among_aggregates_arguments = merging_aggregated || filterColumnIsNotAmongAggregatesArguments(params.aggregates, filter->getFilterColumnName());\n-        const bool can_remove_filter = filter_column_is_not_among_aggregation_keys && filter_is_not_among_aggregates_arguments;\n-\n-        if (auto updated_steps = tryAddNewFilterStep(parent_node, true, nodes, keys, can_remove_filter))\n+        if (auto updated_steps = tryAddNewFilterStep(parent_node, true, nodes, keys))\n             return updated_steps;\n     }\n \n@@ -645,34 +621,24 @@ size_t tryPushDownFilter(QueryPlan::Node * parent_node, QueryPlan::Nodes & nodes\n     // {\n     // }\n \n-    if (auto * sorting = typeid_cast<SortingStep *>(child.get()))\n+    if (typeid_cast<SortingStep *>(child.get()))\n     {\n-        const auto & sort_description = sorting->getSortDescription();\n-        auto sort_description_it = std::find_if(sort_description.begin(), sort_description.end(), [&](auto & sort_column_description)\n-        {\n-            return sort_column_description.column_name == filter->getFilterColumnName();\n-        });\n-        bool can_remove_filter = sort_description_it == sort_description.end();\n-\n         Names allowed_inputs = child->getOutputHeader().getNames();\n-        if (auto updated_steps = tryAddNewFilterStep(parent_node, false, nodes, allowed_inputs, can_remove_filter))\n+        if (auto updated_steps = tryAddNewFilterStep(parent_node, false, nodes, allowed_inputs))\n             return updated_steps;\n     }\n \n     if (typeid_cast<CustomMetricLogViewStep *>(child.get()))\n     {\n         Names allowed_inputs = {\"event_date\", \"event_time\", \"hostname\"};\n-        if (auto updated_steps = tryAddNewFilterStep(parent_node, true, nodes, allowed_inputs, true))\n+        if (auto updated_steps = tryAddNewFilterStep(parent_node, true, nodes, allowed_inputs))\n             return updated_steps;\n     }\n \n-    if (const auto * join_filter_set_step = typeid_cast<CreateSetAndFilterOnTheFlyStep *>(child.get()))\n+    if (typeid_cast<CreateSetAndFilterOnTheFlyStep *>(child.get()))\n     {\n-        const auto & filter_column_name = assert_cast<const FilterStep *>(parent_node->step.get())->getFilterColumnName();\n-        bool can_remove_filter = !join_filter_set_step->isColumnPartOfSetKey(filter_column_name);\n-\n         Names allowed_inputs = child->getOutputHeader().getNames();\n-        if (auto updated_steps = tryAddNewFilterStep(parent_node, false, nodes, allowed_inputs, can_remove_filter))\n+        if (auto updated_steps = tryAddNewFilterStep(parent_node, false, nodes, allowed_inputs))\n             return updated_steps;\n     }\n \n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02576_predicate_push_down_sorting_fix.reference b/tests/queries/0_stateless/02576_predicate_push_down_sorting_fix.reference\nindex d391c365ea7b..263a76a33f3a 100644\n--- a/tests/queries/0_stateless/02576_predicate_push_down_sorting_fix.reference\n+++ b/tests/queries/0_stateless/02576_predicate_push_down_sorting_fix.reference\n@@ -10,14 +10,20 @@ Positions: 2\n   Header: ignore(2_UInt8) UInt8\n           __table2.number UInt64\n   Sort description: ignore(2_UInt8) ASC\n-    Filter (( + (Before ORDER BY + (Projection + Change column names to column identifiers))))\n+    Expression (( + (Before ORDER BY + (Projection + Change column names to column identifiers))))\n     Header: ignore(2_UInt8) UInt8\n             __table2.number UInt64\n-    Filter column: ignore(2_UInt8)\n     Actions: INPUT : 0 -> number UInt64 : 0\n              COLUMN Const(UInt8) -> 2_UInt8 UInt8 : 1\n              ALIAS number :: 0 -> __table2.number UInt64 : 2\n              FUNCTION ignore(2_UInt8 :: 1) -> ignore(2_UInt8) UInt8 : 0\n     Positions: 0 2\n-      ReadFromSystemNumbers\n+      Filter ((( + (Before ORDER BY + (Projection + Change column names to column identifiers))))[split])\n       Header: number UInt64\n+      Filter column: _filter_ignore(2_UInt8)_0 (removed)\n+      Actions: COLUMN Const(UInt8) -> 2_UInt8 UInt8 : 0\n+               FUNCTION ignore(2_UInt8 :: 0) -> ignore(2_UInt8) UInt8 : 1\n+               ALIAS ignore(2_UInt8) :: 1 -> _filter_ignore(2_UInt8)_0 UInt8 : 0\n+      Positions: 0\n+        ReadFromSystemNumbers\n+        Header: number UInt64\ndiff --git a/tests/queries/0_stateless/03519_fulter_push_down_duplicate_column_name_bug.reference b/tests/queries/0_stateless/03519_fulter_push_down_duplicate_column_name_bug.reference\nnew file mode 100644\nindex 000000000000..0ee2d1e918a2\n--- /dev/null\n+++ b/tests/queries/0_stateless/03519_fulter_push_down_duplicate_column_name_bug.reference\n@@ -0,0 +1,10 @@\n+0\t0\n+1\t1\n+2\t1\n+3\t1\n+4\t1\n+5\t1\n+6\t1\n+7\t1\n+8\t1\n+9\t1\ndiff --git a/tests/queries/0_stateless/03519_fulter_push_down_duplicate_column_name_bug.sql b/tests/queries/0_stateless/03519_fulter_push_down_duplicate_column_name_bug.sql\nnew file mode 100644\nindex 000000000000..8965bf1050c1\n--- /dev/null\n+++ b/tests/queries/0_stateless/03519_fulter_push_down_duplicate_column_name_bug.sql\n@@ -0,0 +1,8 @@\n+SELECT\n+    number,\n+    countIf(1, number > 0)\n+FROM numbers(10)\n+GROUP BY number\n+HAVING (count() <= 10) AND 1\n+ORDER BY number ASC\n+SETTINGS enable_analyzer = 1;\n",
  "problem_statement": "Filter pushdown makes query fail with NOT_FOUND_COLUMN_IN_BLOCK\n### Company or project name\n\n_No response_\n\n### Describe what's wrong\n\nQuery fail with `NOT_FOUND_COLUMN_IN_BLOCK` when there is constant predicate in `HAVING`:\n```sql\nSELECT\n    number,\n    countIf(1, number > 0)\nFROM numbers(10)\nGROUP BY number\nHAVING (count() <= 10) AND 1\nORDER BY number ASC\nSETTINGS enable_analyzer = 1, query_plan_filter_push_down = 1\n\nQuery id: 79293344-68ec-48bc-808b-805c30c24911\n\n\nElapsed: 0.004 sec.\n\nReceived exception from server (version 25.3.3):\nCode: 10. DB::Exception: Received from int:9001. DB::Exception: Not found column 1_UInt8 in block. There are only columns: __table1.number, greater(__table1.number, 0_UInt8). (NOT_FOUND_COLUMN_IN_BLOCK)\n```\n\nTurn off filter push down --> query success:\n\n```sql\nSELECT\n    number,\n    countIf(1, number > 0)\nFROM numbers(10)\nGROUP BY number\nHAVING (count() <= 10) AND 1\nORDER BY number ASC\nSETTINGS enable_analyzer = 1, query_plan_filter_push_down = 0\n\nQuery id: 948e0b1a-c9b7-42b6-a4ea-345085708cbf\n\n    \u250c\u2500number\u2500\u252c\u2500countIf(1, g\u22efnumber, 0))\u2500\u2510\n 1. \u2502      0 \u2502                        0 \u2502\n 2. \u2502      1 \u2502                        1 \u2502\n 3. \u2502      2 \u2502                        1 \u2502\n 4. \u2502      3 \u2502                        1 \u2502\n 5. \u2502      4 \u2502                        1 \u2502\n 6. \u2502      5 \u2502                        1 \u2502\n 7. \u2502      6 \u2502                        1 \u2502\n 8. \u2502      7 \u2502                        1 \u2502\n 9. \u2502      8 \u2502                        1 \u2502\n10. \u2502      9 \u2502                        1 \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nReplace `1` in `HAVING` with `2` --> also success:\n\n```sql\nSELECT\n    number,\n    countIf(1, number > 0)\nFROM numbers(10)\nGROUP BY number\nHAVING (count() <= 10) AND 2\nORDER BY number ASC\nSETTINGS enable_analyzer = 1, query_plan_filter_push_down = 1\n\nQuery id: 17d04480-93ba-4bdd-9202-801fcc00407f\n\n    \u250c\u2500number\u2500\u252c\u2500countIf(1, g\u22efnumber, 0))\u2500\u2510\n 1. \u2502      0 \u2502                        0 \u2502\n 2. \u2502      1 \u2502                        1 \u2502\n 3. \u2502      2 \u2502                        1 \u2502\n 4. \u2502      3 \u2502                        1 \u2502\n 5. \u2502      4 \u2502                        1 \u2502\n 6. \u2502      5 \u2502                        1 \u2502\n 7. \u2502      6 \u2502                        1 \u2502\n 8. \u2502      7 \u2502                        1 \u2502\n 9. \u2502      8 \u2502                        1 \u2502\n10. \u2502      9 \u2502                        1 \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\nSo some identifier resolution saga here.\n\n### Does it reproduce on the most recent release?\n\nYes\n\n### How to reproduce\n\nFiddle cannot load while I'm writing this, but the query to reproduce is simple, can run anywhere.\n\n### Expected behavior\n\n_No response_\n\n### Error message and/or stacktrace\n\n_No response_\n\n### Additional context\n\nOur workaround is to not pushing down predicate in `HAVING` if it contains constant expression.\n",
  "hints_text": "query_plan_filter_push_down=0 does not fix this one, but same error:\n\n```sql\nSELECT\n    toStartOfInterval(toDateTime(event_time), INTERVAL 10 second) AS event_time_h,\n    toString(normalizedQueryHash(query)) As hash,\n    sum(query_duration_ms) as total\nFROM clusterAllReplicas('error-analytics', system.query_log)\nWHERE ((query_kind = 'Select') \n  AND (type = 'QueryFinish') \n  AND event_time >= toDateTime(1747761813) AND event_time <= toDateTime(1747763613)\n  AND query like '%events_v5%'\n  AND query not like '%system.query_log%'\n  AND match(hash, toString('.*')))\nGROUP BY ALL\nORDER BY event_time_h ASC\nLIMIT 5 by event_time_h\n```\n\n\n> \"error querying the database: clickhouse [execute]:: 500 code: Code: 10. DB::Exception: Not found column normalizedQueryHash(query) in block. There are only columns: toStartOfInterval(toDateTime(event_time), toIntervalSecond(10)), toString(normalizedQueryHash(query)), sum(query_duration_ms): While executing Remote. (NOT_FOUND_COLUMN_IN_BLOCK) (version 24.12.3.47 (official build))\n\"\n\nchanging `toString(normalizedQueryHash(query))` to `normalizedQueryHash(query)::String` makes it work",
  "created_at": "2025-05-26T12:17:24Z",
  "modified_files": [
    "src/Interpreters/ActionsDAG.cpp",
    "src/Interpreters/ActionsDAG.h",
    "src/Processors/QueryPlan/Optimizations/filterPushDown.cpp"
  ],
  "modified_test_files": [
    "tests/queries/0_stateless/02576_predicate_push_down_sorting_fix.reference",
    "b/tests/queries/0_stateless/03519_fulter_push_down_duplicate_column_name_bug.reference",
    "b/tests/queries/0_stateless/03519_fulter_push_down_duplicate_column_name_bug.sql"
  ]
}