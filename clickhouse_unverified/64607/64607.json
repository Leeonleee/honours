{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 64607,
  "instance_id": "ClickHouse__ClickHouse-64607",
  "issue_numbers": [
    "17364",
    "40583",
    "40675",
    "11482"
  ],
  "base_commit": "fca47bbd01a1bf775c24923ebcbb343d45fc8206",
  "patch": "diff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex 9dfb51340001..07427c6c85ac 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -530,6 +530,7 @@ class IColumn;\n     M(Bool, optimize_read_in_order, true, \"Enable ORDER BY optimization for reading data in corresponding order in MergeTree tables.\", 0) \\\n     M(Bool, optimize_read_in_window_order, true, \"Enable ORDER BY optimization in window clause for reading data in corresponding order in MergeTree tables.\", 0) \\\n     M(Bool, optimize_aggregation_in_order, false, \"Enable GROUP BY optimization for aggregating data in corresponding order in MergeTree tables.\", 0) \\\n+    M(Bool, read_in_order_use_buffering, true, \"Use buffering before merging while reading in order of primary key. It increases the parallelism of query execution\", 0) \\\n     M(UInt64, aggregation_in_order_max_block_bytes, 50000000, \"Maximal size of block in bytes accumulated during aggregation in order of primary key. Lower block size allows to parallelize more final merge stage of aggregation.\", 0) \\\n     M(UInt64, read_in_order_two_level_merge_threshold, 100, \"Minimal number of parts to read to run preliminary merge step during multithread reading in order of primary key.\", 0) \\\n     M(Bool, low_cardinality_allow_in_native_format, true, \"Use LowCardinality type in Native format. Otherwise, convert LowCardinality columns to ordinary for select query, and convert ordinary columns to required LowCardinality for insert query.\", 0) \\\ndiff --git a/src/Core/SettingsChangesHistory.cpp b/src/Core/SettingsChangesHistory.cpp\nindex 886ca0fa2463..d59183975e15 100644\n--- a/src/Core/SettingsChangesHistory.cpp\n+++ b/src/Core/SettingsChangesHistory.cpp\n@@ -58,6 +58,7 @@ String ClickHouseVersion::toString() const\n static std::initializer_list<std::pair<ClickHouseVersion, SettingsChangesHistory::SettingsChanges>> settings_changes_history_initializer =\n {\n     {\"24.7\", {{\"output_format_parquet_write_page_index\", false, true, \"Add a possibility to write page index into parquet files.\"},\n+              {\"read_in_order_use_buffering\", false, true, \"Use buffering before merging while reading in order of primary key\"},\n               {\"optimize_functions_to_subcolumns\", false, true, \"Enable optimization by default\"},\n               {\"input_format_json_ignore_key_case\", false, false, \"Ignore json key case while read json field from string.\"},\n               {\"optimize_trivial_insert_select\", true, false, \"The optimization does not make sense in many cases.\"},\ndiff --git a/src/Processors/QueryPlan/BufferChunksTransform.cpp b/src/Processors/QueryPlan/BufferChunksTransform.cpp\nnew file mode 100644\nindex 000000000000..3601a68d36e6\n--- /dev/null\n+++ b/src/Processors/QueryPlan/BufferChunksTransform.cpp\n@@ -0,0 +1,85 @@\n+#include <Processors/QueryPlan/BufferChunksTransform.h>\n+\n+namespace DB\n+{\n+\n+BufferChunksTransform::BufferChunksTransform(\n+    const Block & header_,\n+    size_t max_rows_to_buffer_,\n+    size_t max_bytes_to_buffer_,\n+    size_t limit_)\n+    : IProcessor({header_}, {header_})\n+    , input(inputs.front())\n+    , output(outputs.front())\n+    , max_rows_to_buffer(max_rows_to_buffer_)\n+    , max_bytes_to_buffer(max_bytes_to_buffer_)\n+    , limit(limit_)\n+{\n+}\n+\n+IProcessor::Status BufferChunksTransform::prepare()\n+{\n+    if (output.isFinished())\n+    {\n+        chunks = {};\n+        input.close();\n+        return Status::Finished;\n+    }\n+\n+    if (input.isFinished() && chunks.empty())\n+    {\n+        output.finish();\n+        return Status::Finished;\n+    }\n+\n+    if (output.canPush())\n+    {\n+        input.setNeeded();\n+\n+        if (!chunks.empty())\n+        {\n+            auto chunk = std::move(chunks.front());\n+            chunks.pop();\n+\n+            num_buffered_rows -= chunk.getNumRows();\n+            num_buffered_bytes -= chunk.bytes();\n+\n+            output.push(std::move(chunk));\n+        }\n+        else if (input.hasData())\n+        {\n+            auto chunk = pullChunk();\n+            output.push(std::move(chunk));\n+        }\n+    }\n+\n+    if (input.hasData() && (num_buffered_rows < max_rows_to_buffer || num_buffered_bytes < max_bytes_to_buffer))\n+    {\n+        auto chunk = pullChunk();\n+        num_buffered_rows += chunk.getNumRows();\n+        num_buffered_bytes += chunk.bytes();\n+        chunks.push(std::move(chunk));\n+    }\n+\n+    if (num_buffered_rows >= max_rows_to_buffer && num_buffered_bytes >= max_bytes_to_buffer)\n+    {\n+        input.setNotNeeded();\n+        return Status::PortFull;\n+    }\n+\n+    input.setNeeded();\n+    return Status::NeedData;\n+}\n+\n+Chunk BufferChunksTransform::pullChunk()\n+{\n+    auto chunk = input.pull();\n+    num_processed_rows += chunk.getNumRows();\n+\n+    if (limit && num_processed_rows >= limit)\n+        input.close();\n+\n+    return chunk;\n+}\n+\n+}\ndiff --git a/src/Processors/QueryPlan/BufferChunksTransform.h b/src/Processors/QueryPlan/BufferChunksTransform.h\nnew file mode 100644\nindex 000000000000..752f9910734e\n--- /dev/null\n+++ b/src/Processors/QueryPlan/BufferChunksTransform.h\n@@ -0,0 +1,42 @@\n+#pragma once\n+#include <Processors/IProcessor.h>\n+#include <queue>\n+\n+namespace DB\n+{\n+\n+/// Transform that buffers chunks from the input\n+/// up to the certain limit  and pushes chunks to\n+/// the output whenever it is ready. It can be used\n+/// to increase parallelism of execution, for example\n+/// when it is adeded before MergingSortedTransform.\n+class BufferChunksTransform : public IProcessor\n+{\n+public:\n+    /// OR condition is used for the limits on rows and bytes.\n+    BufferChunksTransform(\n+        const Block & header_,\n+        size_t max_rows_to_buffer_,\n+        size_t max_bytes_to_buffer_,\n+        size_t limit_);\n+\n+    Status prepare() override;\n+    String getName() const override { return \"BufferChunks\"; }\n+\n+private:\n+    Chunk pullChunk();\n+\n+    InputPort & input;\n+    OutputPort & output;\n+\n+    size_t max_rows_to_buffer;\n+    size_t max_bytes_to_buffer;\n+    size_t limit;\n+\n+    std::queue<Chunk> chunks;\n+    size_t num_buffered_rows = 0;\n+    size_t num_buffered_bytes = 0;\n+    size_t num_processed_rows = 0;\n+};\n+\n+}\ndiff --git a/src/Processors/QueryPlan/Optimizations/optimizeReadInOrder.cpp b/src/Processors/QueryPlan/Optimizations/optimizeReadInOrder.cpp\nindex 537555afa2a7..e1ef38022b57 100644\n--- a/src/Processors/QueryPlan/Optimizations/optimizeReadInOrder.cpp\n+++ b/src/Processors/QueryPlan/Optimizations/optimizeReadInOrder.cpp\n@@ -919,15 +919,23 @@ void optimizeReadInOrder(QueryPlan::Node & node, QueryPlan::Nodes & nodes)\n     {\n         auto & union_node = node.children.front();\n \n-        std::vector<InputOrderInfoPtr> infos;\n+        bool use_buffering = false;\n         const SortDescription * max_sort_descr = nullptr;\n+\n+        std::vector<InputOrderInfoPtr> infos;\n         infos.reserve(node.children.size());\n+\n         for (auto * child : union_node->children)\n         {\n             infos.push_back(buildInputOrderInfo(*sorting, *child, steps_to_update));\n \n-            if (infos.back() && (!max_sort_descr || max_sort_descr->size() < infos.back()->sort_description_for_merging.size()))\n-                max_sort_descr = &infos.back()->sort_description_for_merging;\n+            if (infos.back())\n+            {\n+                if (!max_sort_descr || max_sort_descr->size() < infos.back()->sort_description_for_merging.size())\n+                    max_sort_descr = &infos.back()->sort_description_for_merging;\n+\n+                use_buffering |= infos.back()->limit == 0;\n+            }\n         }\n \n         if (!max_sort_descr || max_sort_descr->empty())\n@@ -972,12 +980,13 @@ void optimizeReadInOrder(QueryPlan::Node & node, QueryPlan::Nodes & nodes)\n             }\n         }\n \n-        sorting->convertToFinishSorting(*max_sort_descr);\n+        sorting->convertToFinishSorting(*max_sort_descr, use_buffering);\n     }\n     else if (auto order_info = buildInputOrderInfo(*sorting, *node.children.front(), steps_to_update))\n     {\n-        sorting->convertToFinishSorting(order_info->sort_description_for_merging);\n-        /// update data stream's sorting properties\n+        /// Use buffering only if have filter or don't have limit.\n+        bool use_buffering = order_info->limit == 0;\n+        sorting->convertToFinishSorting(order_info->sort_description_for_merging, use_buffering);\n         updateStepsDataStreams(steps_to_update);\n     }\n }\n@@ -1091,7 +1100,7 @@ size_t tryReuseStorageOrderingForWindowFunctions(QueryPlan::Node * parent_node,\n         bool can_read = read_from_merge_tree->requestReadingInOrder(order_info->used_prefix_of_sorting_key_size, order_info->direction, order_info->limit);\n         if (!can_read)\n             return 0;\n-        sorting->convertToFinishSorting(order_info->sort_description_for_merging);\n+        sorting->convertToFinishSorting(order_info->sort_description_for_merging, false);\n     }\n \n     return 0;\ndiff --git a/src/Processors/QueryPlan/SortingStep.cpp b/src/Processors/QueryPlan/SortingStep.cpp\nindex 8f40e523b42a..1c40f84d23db 100644\n--- a/src/Processors/QueryPlan/SortingStep.cpp\n+++ b/src/Processors/QueryPlan/SortingStep.cpp\n@@ -1,5 +1,4 @@\n #include <memory>\n-#include <stdexcept>\n #include <IO/Operators.h>\n #include <Interpreters/Context.h>\n #include <Processors/Merges/MergingSortedTransform.h>\n@@ -8,6 +7,7 @@\n #include <Processors/Transforms/LimitsCheckingTransform.h>\n #include <Processors/Transforms/MergeSortingTransform.h>\n #include <Processors/Transforms/PartialSortingTransform.h>\n+#include <Processors/QueryPlan/BufferChunksTransform.h>\n #include <QueryPipeline/QueryPipelineBuilder.h>\n #include <Common/JSONBuilder.h>\n \n@@ -38,6 +38,7 @@ SortingStep::Settings::Settings(const Context & context)\n     tmp_data = context.getTempDataOnDisk();\n     min_free_disk_space = settings.min_free_disk_space_for_temporary_data;\n     max_block_bytes = settings.prefer_external_sort_block_bytes;\n+    read_in_order_use_buffering = settings.read_in_order_use_buffering;\n }\n \n SortingStep::Settings::Settings(size_t max_block_size_)\n@@ -153,10 +154,11 @@ void SortingStep::updateLimit(size_t limit_)\n     }\n }\n \n-void SortingStep::convertToFinishSorting(SortDescription prefix_description_)\n+void SortingStep::convertToFinishSorting(SortDescription prefix_description_, bool use_buffering_)\n {\n     type = Type::FinishSorting;\n     prefix_description = std::move(prefix_description_);\n+    use_buffering = use_buffering_;\n }\n \n void SortingStep::scatterByPartitionIfNeeded(QueryPipelineBuilder& pipeline)\n@@ -244,6 +246,14 @@ void SortingStep::mergingSorted(QueryPipelineBuilder & pipeline, const SortDescr\n     /// If there are several streams, then we merge them into one\n     if (pipeline.getNumStreams() > 1)\n     {\n+        if (use_buffering && sort_settings.read_in_order_use_buffering)\n+        {\n+            pipeline.addSimpleTransform([&](const Block & header)\n+            {\n+                return std::make_shared<BufferChunksTransform>(header, sort_settings.max_block_size, sort_settings.max_block_bytes, limit_);\n+            });\n+        }\n+\n         auto transform = std::make_shared<MergingSortedTransform>(\n             pipeline.getHeader(),\n             pipeline.getNumStreams(),\n@@ -373,9 +383,8 @@ void SortingStep::transformPipeline(QueryPipelineBuilder & pipeline, const Build\n         mergingSorted(pipeline, prefix_description, (need_finish_sorting ? 0 : limit));\n \n         if (need_finish_sorting)\n-        {\n             finishSorting(pipeline, prefix_description, result_description, limit);\n-        }\n+\n         return;\n     }\n \ndiff --git a/src/Processors/QueryPlan/SortingStep.h b/src/Processors/QueryPlan/SortingStep.h\nindex 49dcf9f31215..b4a49394a13f 100644\n--- a/src/Processors/QueryPlan/SortingStep.h\n+++ b/src/Processors/QueryPlan/SortingStep.h\n@@ -28,6 +28,7 @@ class SortingStep : public ITransformingStep\n         TemporaryDataOnDiskScopePtr tmp_data = nullptr;\n         size_t min_free_disk_space = 0;\n         size_t max_block_bytes = 0;\n+        size_t read_in_order_use_buffering = 0;\n \n         explicit Settings(const Context & context);\n         explicit Settings(size_t max_block_size_);\n@@ -80,7 +81,7 @@ class SortingStep : public ITransformingStep\n \n     const SortDescription & getSortDescription() const { return result_description; }\n \n-    void convertToFinishSorting(SortDescription prefix_description);\n+    void convertToFinishSorting(SortDescription prefix_description, bool use_buffering_);\n \n     Type getType() const { return type; }\n     const Settings & getSettings() const { return sort_settings; }\n@@ -126,6 +127,7 @@ class SortingStep : public ITransformingStep\n \n     UInt64 limit;\n     bool always_read_till_end = false;\n+    bool use_buffering = false;\n \n     Settings sort_settings;\n \n",
  "test_patch": "diff --git a/tests/queries/0_stateless/03168_read_in_order_buffering_1.reference b/tests/queries/0_stateless/03168_read_in_order_buffering_1.reference\nnew file mode 100644\nindex 000000000000..306885a09748\n--- /dev/null\n+++ b/tests/queries/0_stateless/03168_read_in_order_buffering_1.reference\n@@ -0,0 +1,6 @@\n+1\n+0\n+1\n+0\n+0\n+0\ndiff --git a/tests/queries/0_stateless/03168_read_in_order_buffering_1.sql b/tests/queries/0_stateless/03168_read_in_order_buffering_1.sql\nnew file mode 100644\nindex 000000000000..75025dcadc80\n--- /dev/null\n+++ b/tests/queries/0_stateless/03168_read_in_order_buffering_1.sql\n@@ -0,0 +1,45 @@\n+DROP TABLE IF EXISTS t_read_in_order_1;\n+\n+CREATE TABLE t_read_in_order_1 (id UInt64, v UInt64)\n+ENGINE = MergeTree ORDER BY id\n+SETTINGS index_granularity = 1024, index_granularity_bytes = '10M';\n+\n+INSERT INTO t_read_in_order_1 SELECT number, number FROM numbers(1000000);\n+\n+SET max_threads = 8;\n+SET optimize_read_in_order = 1;\n+SET read_in_order_use_buffering = 1;\n+\n+SELECT count() FROM\n+(\n+    EXPLAIN PIPELINE SELECT * FROM t_read_in_order_1 ORDER BY id\n+) WHERE explain LIKE '%BufferChunks%';\n+\n+SELECT count() FROM\n+(\n+    EXPLAIN PIPELINE SELECT * FROM t_read_in_order_1 ORDER BY id LIMIT 10\n+) WHERE explain LIKE '%BufferChunks%';\n+\n+SELECT count() FROM\n+(\n+    EXPLAIN PIPELINE SELECT * FROM t_read_in_order_1 WHERE v % 10 = 0 ORDER BY id LIMIT 10\n+) WHERE explain LIKE '%BufferChunks%';\n+\n+SET read_in_order_use_buffering = 0;\n+\n+SELECT count() FROM\n+(\n+    EXPLAIN PIPELINE SELECT * FROM t_read_in_order_1 ORDER BY id\n+) WHERE explain LIKE '%BufferChunks%';\n+\n+SELECT count() FROM\n+(\n+    EXPLAIN PIPELINE SELECT * FROM t_read_in_order_1 ORDER BY id LIMIT 10\n+) WHERE explain LIKE '%BufferChunks%';\n+\n+SELECT count() FROM\n+(\n+    EXPLAIN PIPELINE SELECT * FROM t_read_in_order_1 WHERE v % 10 = 0 ORDER BY id LIMIT 10\n+) WHERE explain LIKE '%BufferChunks%';\n+\n+DROP TABLE t_read_in_order_1;\ndiff --git a/tests/queries/0_stateless/03168_read_in_order_buffering_2.reference b/tests/queries/0_stateless/03168_read_in_order_buffering_2.reference\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/queries/0_stateless/03168_read_in_order_buffering_2.sql b/tests/queries/0_stateless/03168_read_in_order_buffering_2.sql\nnew file mode 100644\nindex 000000000000..1d3a75412e02\n--- /dev/null\n+++ b/tests/queries/0_stateless/03168_read_in_order_buffering_2.sql\n@@ -0,0 +1,17 @@\n+-- Tags: long, no-random-settings, no-tsan, no-asan, no-msan, no-s3-storage\n+\n+DROP TABLE IF EXISTS t_read_in_order_2;\n+\n+CREATE TABLE t_read_in_order_2 (id UInt64, v UInt64) ENGINE = MergeTree ORDER BY id;\n+\n+INSERT INTO t_read_in_order_2 SELECT number, number FROM numbers(10000000);\n+OPTIMIZE TABLE t_read_in_order_2 FINAL;\n+\n+SET optimize_read_in_order = 1;\n+SET max_threads = 4;\n+SET read_in_order_use_buffering = 1;\n+SET max_memory_usage = '100M';\n+\n+SELECT * FROM t_read_in_order_2 ORDER BY id FORMAT Null;\n+\n+DROP TABLE t_read_in_order_2;\n",
  "problem_statement": "optimize_read_in_order slows down queries which takes only a few rows from set.\n**Describe the situation**\r\nIf you have some query which returns a small subset of rows from a big table and sort it, optimize_read_in_order slows down query a lot.\r\n\r\n**How to reproduce**\r\nClickhouse server 20.10, 20.11.4.13\r\n```\r\nCREATE TABLE default.test_scan\r\n(\r\n    `_time` DateTime,\r\n    `key` UInt32,\r\n    `value` UInt32,\r\n    `dt` Date DEFAULT toDate(_time),\r\n    `epoch` UInt64\r\n)\r\nENGINE = MergeTree()\r\nPARTITION BY toDate(_time)\r\nORDER BY (_time, epoch)\r\nSETTINGS index_granularity = 8192\r\n\r\nINSERT INTO test_scan(_time, key, value, epoch) SELECT now() + intDiv(number,100000), 1 as key, rand() % 250000 as value, now64(6) + (number * 10) FROM numbers(1000000000);\r\n\r\nSELECT key, value FROM test_scan WHERE value = 3123 FORMAT Null\r\n0 rows in set. Elapsed: 0.291 sec. Processed 1.00 billion rows, 4.07 GB (3.44 billion rows/s., 13.99 GB/s.)\r\n\r\nSELECT key, value FROM test_scan WHERE value = 3123 ORDER BY _time, epoch FORMAT Null\r\n0 rows in set. Elapsed: 2.614 sec. Processed 1.00 billion rows, 4.26 GB (382.52 million rows/s., 1.63 GB/s.)\r\n\r\nSELECT key, value FROM test_scan WHERE value = 3123 AND not ignore(_time, epoch) FORMAT Null\r\n0 rows in set. Elapsed: 0.388 sec. Processed 1.00 billion rows, 4.26 GB (2.58 billion rows/s., 11.00 GB/s.)\r\n\r\nSELECT  key, value FROM (SELECT key, value, _time, epoch FROM test_scan WHERE value = 3123) ORDER BY _time, epoch FORMAT Null;\r\n0 rows in set. Elapsed: 0.380 sec. Processed 1.00 billion rows, 4.26 GB (2.63 billion rows/s., 11.23 GB/s.)\r\n\r\nset optimize_read_in_order=0;\r\nSELECT key, value FROM test_scan WHERE value = 3123 ORDER BY _time, epoch FORMAT Null\r\n0 rows in set. Elapsed: 0.359 sec. Processed 1.00 billion rows, 4.26 GB (2.79 billion rows/s., 11.88 GB/s.)\r\n\r\n```\r\n**Expected performance**\r\nQueries would have similar performance.\r\n\nSlow queries when reading in order\n**Describe the situation**\r\nWe have a certain data-set with user logs, trying to get all data by certain tags spanning several hours.\r\nQueries with ordering by part of ordering keys work slower than queries which don't use ordering by order-key\r\n\r\n**How to reproduce**\r\nTried on ClickHouse 22.3 initially, same behavior on `22.7.4.16` too\r\n\r\nCREATE statement:\r\n```\r\nCREATE TABLE logs_time_dt\r\n(\r\n    `time` DateTime64(9) Codec(Delta, ZSTD(7)),\r\n    `project` LowCardinality(String) CODEC(ZSTD(7)),\r\n    `service` LowCardinality(String) CODEC(ZSTD(7)),\r\n    `message` String CODEC(ZSTD(7)),\r\n    `tags_hash` Array(UInt64) CODEC(ZSTD(7)),\r\n    INDEX idx_message message TYPE ngrambf_v1(3, 512, 2, 0) GRANULARITY 3,\r\n    INDEX idx_tags_hash tags_hash TYPE bloom_filter(0.01) GRANULARITY 1\r\n)\r\nENGINE = MergeTree\r\nPARTITION BY toStartOfHour(time)\r\nORDER BY (project, service, time)\r\nSETTINGS index_granularity = 1024\r\n```\r\n\r\nexample dataset, several hours worth of data, the result set by particular tag is 20k rows:\r\n```\r\ninsert into logs_time_dt\r\n(time, project, service, message, tags_hash)\r\nselect\r\nfromUnixTimestamp64Nano(toInt64(toUnixTimestamp64Nano(toDateTime64('2022-08-01',9))+number/(2777)*1e9)),\r\n'test' as project,\r\n'test' as service,\r\n'foo',\r\n[ number % 3000 ]\r\nfrom system.numbers\r\nlimit 60*1e6\r\n```\r\n\r\nslow query reading by key in reverse order:\r\n```\r\nSELECT *\r\nFROM logs_time_dt\r\nWHERE (project = 'test') AND (service = 'test') AND has(tags_hash, 42)\r\nORDER BY time DESC\r\nFORMAT `Null`\r\n\r\nQuery id: f3277eec-3a99-4442-849d-1209ad402548\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 8.770 sec. Processed 29.27 million rows, 1.11 GB (3.34 million rows/s., 126.91 MB/s.)\r\n```\r\n\r\nreading in order seems to be faster in this particular example, but still slow:\r\n```\r\nSELECT *\r\nFROM logs_time_dt\r\nWHERE (project = 'test') AND (service = 'test') AND has(tags_hash, 42)\r\nORDER BY time ASC\r\nFORMAT `Null`\r\n\r\nQuery id: a3ce44be-5ea2-4ffc-9c72-7f4fc656a5e1\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 3.788 sec. Processed 29.27 million rows, 1.11 GB (7.73 million rows/s., 293.59 MB/s.)\r\n```\r\n\r\nsorting by negative timestamp works a lot faster with this case:\r\n```\r\nSELECT *\r\nFROM logs_time_dt\r\nWHERE (project = 'test') AND (service = 'test') AND has(tags_hash, 42)\r\nORDER BY -toUnixTimestamp64Nano(time) ASC\r\nFORMAT `Null`\r\n\r\nQuery id: e5415b8d-556c-4ffa-910a-81902b252f55\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.442 sec. Processed 29.27 million rows, 1.11 GB (66.28 million rows/s., 2.52 GB/s.)\r\n```\r\n\r\n\r\n\r\n**Expected performance**\r\nI expect the \"correct\" query with ordering by primary key suffix wouldn't be slower than ordering by a computed expression.\r\n\r\n\r\n\nMergingSortedTransform has low performance in read in order optimization \n**Describe the situation**\r\nQueries taken from #40583 \r\n\r\n```sql\r\nCREATE TABLE logs_time_dt\r\n(\r\n    `time` DateTime64(9) Codec(Delta, ZSTD(7)),\r\n    `project` LowCardinality(String) CODEC(ZSTD(7)),\r\n    `service` LowCardinality(String) CODEC(ZSTD(7)),\r\n    `message` String CODEC(ZSTD(7)),\r\n    `tags_hash` Array(UInt64) CODEC(ZSTD(7)),\r\n    INDEX idx_message message TYPE ngrambf_v1(3, 512, 2, 0) GRANULARITY 3,\r\n    INDEX idx_tags_hash tags_hash TYPE bloom_filter(0.01) GRANULARITY 1\r\n)\r\nENGINE = MergeTree\r\nPARTITION BY toStartOfHour(time)\r\nORDER BY (project, service, time)\r\nSETTINGS index_granularity = 1024;\r\n\r\ninsert into logs_time_dt\r\n(time, project, service, message, tags_hash)\r\nselect\r\nfromUnixTimestamp64Nano(toInt64(toUnixTimestamp64Nano(toDateTime64('2022-08-01',9))+number/(2777)*1e9)),\r\n'test' as project,\r\n'test' as service,\r\n'foo',\r\n[ number % 3000 ]\r\nfrom system.numbers\r\nlimit 60*1e6;\r\n\r\nSELECT *\r\nFROM logs_time_dt\r\nWHERE (project = 'test') AND (service = 'test') AND has(tags_hash, 42)\r\nORDER BY time\r\nFORMAT `Null`;\r\n```\r\n\r\nQuery pipeline:\r\n```sql\r\n(Expression)\r\nExpressionTransform\r\n  (Sorting)\r\n  MergingSortedTransform 45 \u2192 1\r\n    (Expression)\r\n    ExpressionTransform \u00d7 45\r\n      (Filter)\r\n      FilterTransform \u00d7 45\r\n        (ReadFromMergeTree)\r\n        MergeTreeInOrder \u00d7 45 0 \u2192 1\r\n````\r\n\r\nPerformance:\r\n```sql\r\n0 rows in set. Elapsed: 3.425 sec. Processed 29.23 million rows, 1.11 GB (8.54 million rows/s., 324.36 MB/s.)\r\n```\r\n\r\nFor read in order optimization, sometime the inputs of MergingSortedTransform are already sorted. Therefore, when executing, the processor will read all chunks from input port 0, then all chunks from input port 1, and so on so far (the indices may be not correct, just understand that the processor will scan the input ports sequentially).\r\n\r\nTry a simple fix by adding buffer for each input:\r\n```sql\r\n(Expression)\r\nExpressionTransform\r\n (Sorting)\r\n MergingSortedTransform 45 \u2192 1\r\n   QueueBuffer \u00d7 45\r\n     (Expression)\r\n     ExpressionTransform \u00d7 45\r\n       (Filter)\r\n       FilterTransform \u00d7 45\r\n         (ReadFromMergeTree)\r\n         MergeTreeInOrder \u00d7 45 0 \u2192 1\r\n```\r\nPerformance:\r\n```sql\r\n0 rows in set. Elapsed: 0.281 sec. Processed 29.23 million rows, 1.11 GB (104.21 million rows/s., 3.96 GB/s.)\r\n```\r\n\r\nHowever, this simple fix has 2 main drawbacks:\r\n1. Cannot take advantage of limit pushdown (can fix easily)\r\n2. When the result set is big, OOM can happens, so need to add on-disk buffer.\r\n\r\nWe can discuss a solution to make read in order and normal read to have similar performance.\r\n\r\n\nClichouse v20.4 performance issue\n**Describe the situation**\r\n\r\nI'm seeing slower than expected queries on v20.4.4.18-stable compared to v20.1:20.1.4.14-stable.\r\n\r\nMy schema:\r\n\r\n```sql\r\nCREATE TABLE jaeger_index_v2 (\r\n  timestamp DateTime CODEC(Delta, ZSTD(1)),\r\n  traceID String CODEC(ZSTD(1)),\r\n  service LowCardinality(String) CODEC(ZSTD(1)),\r\n  operation LowCardinality(String) CODEC(ZSTD(1)),\r\n  durationUs UInt64 CODEC(ZSTD(1)),\r\n  tags Array(String) CODEC(ZSTD(1)),\r\n  INDEX idx_tags tags TYPE bloom_filter(0.01) GRANULARITY 64\r\n) ENGINE MergeTree()\r\nPARTITION BY toDate(timestamp)\r\nORDER BY (service, timestamp)\r\nSETTINGS index_granularity=1024\r\n```\r\n\r\nMy query:\r\n\r\n```sql\r\nSELECT DISTINCT traceID\r\n  FROM jaeger_index_v2\r\nPREWHERE service = 'nginx-fl'\r\n   AND timestamp >= now() - 3600\r\n   AND timestamp <= now()\r\n ORDER BY service DESC,\r\n       timestamp DESC\r\n LIMIT 20\r\n```\r\n\r\nOn v20.1 I see ~0.5s per query\r\nOn v20.4 I see a very different picture for 1.5s per query.\n",
  "hints_text": "> @CurtizJ : \r\n> Well, you can't do much there. If `WHERE` selects a small range of rows, it will be slower with `optimize_read_in_order` than without, because the reading itself is slower. If only somehow tricky to rewrite the pipeline or disable the optimization.\nBut if we don't have LIMIT in the query, do we actually gain any benefit from optimize_read_in_order then?\nClickHouse version 22.3\r\n\r\n\r\nSelected 182/362 parts by partition key, 182 parts by primary key, 200/211458 marks by primary key, 200 marks to read from 182 ranges\r\n\r\n```\r\nSET optimize_read_in_order=1;\r\n\r\n<Debug> MemoryTracker: Peak memory usage (for query): 2.36 GiB.\r\n15 rows in set. Elapsed: 1.269 sec. Processed 4.64 million rows, 2.20 GB (3.66 million rows/s., 1.74 GB/s.)\r\n```\r\n\r\n```\r\nSET read_in_order_two_level_merge_threshold=200;\r\nSET optimize_read_in_order=1;\r\n\r\n<Debug> MemoryTracker: Peak memory usage (for query): 1.45 GiB.\r\n15 rows in set. Elapsed: 0.659 sec. Processed 4.64 million rows, 2.20 GB (7.05 million rows/s., 3.34 GB/s.)\r\n```\r\n\r\n```\r\nSET optimize_read_in_order=0;\r\n\r\n<Debug> MemoryTracker: Peak memory usage (for query): 55.13 MiB.\r\n\r\n15 rows in set. Elapsed: 0.419 sec. Processed 4.44 million rows, 2.11 GB (10.61 million rows/s., 5.04 GB/s.)\r\n```\r\n\r\n\r\nFeature request to \"fix\" this problem\r\nhttps://github.com/ClickHouse/ClickHouse/issues/17941\r\n\r\nBut still not explain that huge memory usage\nyou need to remove LowCardinality\r\n\r\n```sql\r\nCREATE TABLE logs_time_dt\r\n(\r\n    `time` DateTime64(9) CODEC(Delta, ZSTD(7)),\r\n    `project` String CODEC(ZSTD(7)),\r\n    `service` String CODEC(ZSTD(7)),\r\n    `message` String CODEC(ZSTD(7)),\r\n    `tags_hash` Array(UInt64) CODEC(ZSTD(7)),\r\n    INDEX idx_message message TYPE ngrambf_v1(3, 512, 2, 0) GRANULARITY 3,\r\n    INDEX idx_tags_hash tags_hash TYPE bloom_filter(0.01) GRANULARITY 1\r\n)\r\nENGINE = MergeTree\r\nPARTITION BY toStartOfHour(time)\r\nORDER BY (project, service, time)\r\nSETTINGS index_granularity = 1024;\r\n\r\n\r\nSELECT *\r\n              FROM logs_time_dt\r\n              WHERE (project = 'test') AND (service = 'test') AND has(tags_hash, 42)\r\n              ORDER BY time DESC \r\nlimit 10\r\n              FORMAT `Null`\r\n              ;\r\n0 rows in set. Elapsed: 0.076 sec. Processed 112.02 thousand rows, 6.95 MB (1.48 million rows/s., 91.81 MB/s.)\r\n```\nSandbox with all queries and one simple query\r\n```\r\nSET optimize_read_in_order = 0;\r\n```\r\nhttps://fiddle.clickhouse.com/ba6e3a90-c41c-4680-ab77-06614f49cd6b\r\n```\r\nSET optimize_read_in_order = 1;\r\n```\r\nhttps://fiddle.clickhouse.com/dc407df3-099e-4515-beb5-832f619c75e7\r\n<img width=\"1500\" alt=\"Screen Shot 2022-08-24 at 21 49 05\" src=\"https://user-images.githubusercontent.com/504986/186500667-75acc4cd-261a-4173-a5d4-cacf3c2c72dd.png\">\r\n<img width=\"1501\" alt=\"Screen Shot 2022-08-24 at 21 48 14\" src=\"https://user-images.githubusercontent.com/504986/186500701-6bc1392e-0215-4b14-90d8-b898b6984ed0.png\">\r\n\r\n\n> you need to remove LowCardinality\r\n> \r\n> ```sql\r\n> CREATE TABLE logs_time_dt\r\n> (\r\n>     `time` DateTime64(9) CODEC(Delta, ZSTD(7)),\r\n>     `project` String CODEC(ZSTD(7)),\r\n>     `service` String CODEC(ZSTD(7)),\r\n>     `message` String CODEC(ZSTD(7)),\r\n>     `tags_hash` Array(UInt64) CODEC(ZSTD(7)),\r\n>     INDEX idx_message message TYPE ngrambf_v1(3, 512, 2, 0) GRANULARITY 3,\r\n>     INDEX idx_tags_hash tags_hash TYPE bloom_filter(0.01) GRANULARITY 1\r\n> )\r\n> ENGINE = MergeTree\r\n> PARTITION BY toStartOfHour(time)\r\n> ORDER BY (project, service, time)\r\n> SETTINGS index_granularity = 1024;\r\n> \r\n> \r\n> SELECT *\r\n>               FROM logs_time_dt\r\n>               WHERE (project = 'test') AND (service = 'test') AND has(tags_hash, 42)\r\n>               ORDER BY time DESC \r\n> limit 10\r\n>               FORMAT `Null`\r\n>               ;\r\n> 0 rows in set. Elapsed: 0.076 sec. Processed 112.02 thousand rows, 6.95 MB (1.48 million rows/s., 91.81 MB/s.)\r\n> ```\r\n\r\n@den-crane This query has limit, so the limit pushdown optimization reduces a vast amount of data. I think it doesn't relate to LowCardinality type here.\r\n\r\n@holycheater  If you want to keep order by primary key, temporary set `optimize_read_in_order` to `0`. There's certainly a flaw in read in order optimization that make it worse in this case.\nFor a reason, reading in order is used for the slow queries, which is not beneficial for the queries. The workaround is setting `optimize_read_in_order=0`.\r\n\r\n<pre>\r\nSELECT *\r\nFROM logs_time_dt\r\nWHERE (project = 'test') AND (service = 'test') AND has(tags_hash, 42)\r\nORDER BY time ASC\r\n<b>SETTINGS optimize_read_in_order = 0</b>\r\nFORMAT `Null`\r\n\r\n0 rows in set. Elapsed: <b>0.168 sec</b>. Processed 29.27 million rows, 1.11 GB (174.12 million rows/s., 6.62 GB/s.)\r\n</pre>\r\n`EXPLAIN PIPELINE` can be used to see if `reading in order` is used:\r\n<pre>\r\nEXPLAIN PIPELINE\r\nSELECT *\r\nFROM logs_time_dt\r\nWHERE (project = 'test') AND (service = 'test') AND has(tags_hash, 42)\r\nORDER BY time ASC\r\n<b>SETTINGS optimize_read_in_order = 0</b>\r\n\r\nQuery id: 78f9ca2a-8a87-42df-92ec-c144ec2cb330\r\n\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 (Expression)                             \u2502\r\n\u2502 ExpressionTransform                      \u2502\r\n\u2502   (Sorting)                              \u2502\r\n\u2502   MergingSortedTransform 16 \u2192 1          \u2502\r\n\u2502     MergeSortingTransform \u00d7 16           \u2502\r\n\u2502       LimitsCheckingTransform \u00d7 16       \u2502\r\n\u2502         PartialSortingTransform \u00d7 16     \u2502\r\n\u2502           (Expression)                   \u2502\r\n\u2502           ExpressionTransform \u00d7 16       \u2502\r\n\u2502             (Filter)                     \u2502\r\n\u2502             FilterTransform \u00d7 16         \u2502\r\n\u2502               (ReadFromMergeTree)        \u2502\r\n\u2502               <b>MergeTreeThread \u00d7 16 0 \u2192 1 \u2502</b>\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n</pre>\r\n<pre>\r\nEXPLAIN PIPELINE\r\nSELECT *\r\nFROM logs_time_dt\r\nWHERE (project = 'test') AND (service = 'test') AND has(tags_hash, 42)\r\nORDER BY time ASC\r\n<b>SETTINGS optimize_read_in_order = 1</b>\r\n\r\nQuery id: bd609a03-c96a-4fed-8e10-03be4f921106\r\n\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 (Expression)                        \u2502\r\n\u2502 ExpressionTransform                 \u2502\r\n\u2502   (Sorting)                         \u2502\r\n\u2502   MergingSortedTransform 46 \u2192 1     \u2502\r\n\u2502     (Expression)                    \u2502\r\n\u2502     ExpressionTransform \u00d7 46        \u2502\r\n\u2502       (Filter)                      \u2502\r\n\u2502       FilterTransform \u00d7 46          \u2502\r\n\u2502         (ReadFromMergeTree)         \u2502\r\n\u2502         <b>MergeTreeInOrder \u00d7 46 0 \u2192 1 \u2502</b>\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n</pre>\nThe query slow down caused by optimization done in https://github.com/ClickHouse/ClickHouse/pull/32748 (`project` and `service` columns will become constant after filtering due to `(project = 'test') AND (service = 'test')` expression in `WHERE`)\r\nFor the query in this issue, reading in order is slower than w/o order, it's true for both reading in order implementations, - original one (`optimize_read_in_order` setting) and on top of query plan (`query_plan_read_in_order` setting)\r\n\r\n@KochetovNicolai @CurtizJ any ideas what we can do here?\r\n\r\nThe easiest thing, probably, is to introduce yet another setting to control optimization from #32748, but I'm not sure, it looks too fine-grained one\nThe best solution to me is not to disable `optimize_read_in_order`, but try to improve performance for such type of queries.\r\nFYI: `query_plan_read_in_order` is a setting to change read-in-order analysis (to check if we want to apply optimization); execution was not changed.\r\n\r\nIn this case, likely, a query with `optimize_read_in_order = 0` works faster because of multi-threaded execution. I can expect that due to good filter condition the result set is small. Sorting of full result set can fast in this case, and reading in parallel is more efficient vs. almost single-threaded reading with `optimize_read_in_order = 1`. (It is my guess and good to check).\r\n\r\nI think that we can make reading more parallelized when `read-in-order` enabled. We can try different approaches.\r\n\r\n**First idea**\r\nLet's try to add buffering processor into query pipeline, after `FilterTransform` and before `MergingSortedTransform`. It will\r\n* if LIMIT is not specified for `MergingSortedTransform`, allow reading more data and buffer it (e.g. up to `max_block_size * 2` rows, or any other constant)\r\n* if LIMIT is specified, the same, but formula will be like `min(LIMIT, max_block_size * 2)`\r\n\r\n**Second idea**\r\nTry to build more parallel pipeline. Maybe split ` MergeTreeInOrder \u00d7 46` input streams between different threads, and do `MergingSortedTransform` separately in every thread (also, final `MergingSortedTransform` will be needed). So that pipeline will be like:\r\n```\r\nMergingSortedTransform N \u2192 1\r\n  MergingSortedTransform M_1 \u2192 1\r\n    MergeTreeInOrder \u00d7 M_1\r\n...\r\n  MergingSortedTransform M_N \u2192 1\r\n    MergeTreeInOrder \u00d7 M_N\r\n```\r\nwhere `M_1 + ... + M_N = 46`.\r\n\r\nBoth ideas is not good for the case when there is a single sorted part.\n> In this case, likely, a query with `optimize_read_in_order = 0` works faster because of multi-threaded execution. I can expect that due to good filter condition the result set is small. Sorting of full result set can fast in this case, and reading in parallel is more efficient vs. almost single-threaded reading with `optimize_read_in_order = 1`. (It is my guess and good to check).\r\n\r\nI think your guess is correct. I opened a related issue in #40675. When the data is sorted globally, `MergingSortedTransform` works like a concat processor and limits the reading to (almost) single thread.\r\n\r\n> First idea\r\n> Let's try to add buffering processor into query pipeline, after FilterTransform and before MergingSortedTransform. It will\r\n\r\n> if LIMIT is not specified for MergingSortedTransform, allow reading more data and buffer it (e.g. up to max_block_size * 2 rows, or any other constant)\r\n> if LIMIT is specified, the same, but formula will be like min(LIMIT, max_block_size * 2)\r\n\r\nI implemented a prototype of this solution in #40675 and I think it works perfectly well. But there's a concern that if the result set is big, it can cause OOM.\r\n\n@canhld94 https://github.com/ClickHouse/ClickHouse/issues/40675 is an issue, not a PR. What PR do you refer to?\n> @canhld94 https://github.com/ClickHouse/ClickHouse/issues/40675 is an issue, not a PR. What PR do you refer to?\n\nI implemented it locally but didn't send a PR, just described how it was implemented in #40675. There's already a processor named QueueBuffer that we can put it before MergingSortedTransform, just need a little modification to support limit pushdown.\nRelated https://github.com/ClickHouse/ClickHouse/issues/11482\r\n\r\nMaybe related? https://github.com/ClickHouse/ClickHouse/issues/17364\nTldr: it's a feature not a bug.\n\nRead the previous discussions about read in order optimization. The team already be aware of this issue when developing the feature, and it's the user's responsibility to decide when to use this optimization. So close this.\n> Read the previous discussions about read in order optimization. The team already be aware of this issue when developing the feature, and it's the user's responsibility to decide when to use this optimization. So close this.\r\n\r\nHey @canhld94 - I just stumbled across something very similar. Where are the previous discussions? Can you elaborate more on what you mean by how the user would decide on how to choose the optimization?\n> Hey @canhld94 - I just stumbled across something very similar. Where are the previous discussions?\r\n\r\nHi @et I cannot find all the previous discussions, you may read one explanation here: https://github.com/ClickHouse/ClickHouse/issues/40583#issuecomment-1324048895.\r\n\r\n> Can you elaborate more on what you mean by how the user would decide on how to choose the optimization?\r\n\r\nWhat I mean is: if you think `optimize_read_in_order` cause bad effects, you can disable it by adding `settings optimize_read_in_order = 0` to your queries. The issue I elaborated above is only one in many possible issues. Knowing when to turn it on or off requires expertise on Clickhouse query execution.\n\r\n\r\n\r\n\r\n> Try a simple fix by adding buffer for each input:\r\n> \r\n> ```sql\r\n> (Expression)\r\n> ExpressionTransform\r\n>  (Sorting)\r\n>  MergingSortedTransform 45 \u2192 1\r\n>    QueueBuffer \u00d7 45\r\n>      (Expression)\r\n>      ExpressionTransform \u00d7 45\r\n>        (Filter)\r\n>        FilterTransform \u00d7 45\r\n>          (ReadFromMergeTree)\r\n>          MergeTreeInOrder \u00d7 45 0 \u2192 1\r\n> ```\r\n> \r\n> Performance:\r\n> \r\n> ```sql\r\n> 0 rows in set. Elapsed: 0.281 sec. Processed 29.23 million rows, 1.11 GB (104.21 million rows/s., 3.96 GB/s.)\r\n> ```\r\n> \r\n> However, this simple fix has 2 main drawbacks:\r\n> \r\n> 1. Cannot take advantage of limit pushdown (can fix easily)\r\n> 2. When the result set is big, OOM can happens, so need to add on-disk buffer.\r\n> \r\n> We can discuss a solution to make read in order and normal read to have similar performance.\r\n\r\n@KochetovNicolai do you think it makes sense to bring this solution to upstream? In https://github.com/ClickHouse/ClickHouse/issues/40583#issuecomment-1324048895 you propose the same idea.\nSTR\r\n\r\n```\r\ndrop table  if exists jaeger_index_v2;\r\n\r\nCREATE TABLE jaeger_index_v2 (\r\n  timestamp DateTime CODEC(Delta, ZSTD(1)),\r\n  traceID String CODEC(ZSTD(1)),\r\n  service LowCardinality(String) CODEC(ZSTD(1))\r\n) ENGINE MergeTree()\r\nPARTITION BY toDate(timestamp)\r\nORDER BY (service, timestamp)\r\nSETTINGS index_granularity=512;\r\n\r\ninsert into jaeger_index_v2(timestamp, service) select toDateTime('2020-06-06 15:49:56')+intDiv(number, 10000), 'xxxx' from numbers(300000000);\r\n\r\n\r\nSELECT distinct traceID\r\n  FROM jaeger_index_v2\r\nPREWHERE service = 'xxxx'\r\n   AND timestamp >= '2020-06-06 15:49:56'\r\n   AND timestamp <= '2020-06-06 17:49:56'\r\n ORDER BY service DESC,\r\n       timestamp DESC;\r\n\r\n20.5.1.3629\r\n1 rows in set. Elapsed: 1.210 sec. Processed 72.01 million rows, 1.01 GB (59.51 million rows/s., 833.23 MB/s.)\r\n1 rows in set. Elapsed: 1.185 sec. Processed 72.01 million rows, 1.01 GB (60.75 million rows/s., 850.53 MB/s.)\r\n1 rows in set. Elapsed: 1.219 sec. Processed 72.01 million rows, 1.01 GB (59.07 million rows/s., 826.94 MB/s.)\r\nset optimize_read_in_order=0\r\n1 rows in set. Elapsed: 0.184 sec. Processed 72.01 million rows, 1.01 GB (392.22 million rows/s., 5.49 GB/s.)\r\n1 rows in set. Elapsed: 0.182 sec. Processed 72.01 million rows, 1.01 GB (396.49 million rows/s., 5.55 GB/s.)\r\n1 rows in set. Elapsed: 0.177 sec. Processed 72.01 million rows, 1.01 GB (406.49 million rows/s., 5.69 GB/s.)\r\n\r\n20.4.4.18\r\n1 rows in set. Elapsed: 1.294 sec. Processed 72.01 million rows, 1.01 GB (55.65 million rows/s., 779.18 MB/s.)\r\n1 rows in set. Elapsed: 1.329 sec. Processed 72.01 million rows, 1.01 GB (54.20 million rows/s., 758.88 MB/s.)\r\n1 rows in set. Elapsed: 1.250 sec. Processed 72.01 million rows, 1.01 GB (57.60 million rows/s., 806.47 MB/s.)\r\n\r\n20.3.10.75\r\n1 rows in set. Elapsed: 1.281 sec. Processed 72.01 million rows, 1.01 GB (56.20 million rows/s., 786.87 MB/s.)\r\n1 rows in set. Elapsed: 1.296 sec. Processed 72.01 million rows, 1.01 GB (55.57 million rows/s., 777.95 MB/s.)\r\n\r\n20.1.4.14\r\n1 rows in set. Elapsed: 0.468 sec. Processed 72.01 million rows, 1.01 GB (153.86 million rows/s., 2.15 GB/s.)\r\n1 rows in set. Elapsed: 0.424 sec. Processed 72.01 million rows, 1.01 GB (169.89 million rows/s., 2.38 GB/s.)\r\n1 rows in set. Elapsed: 0.438 sec. Processed 72.01 million rows, 1.01 GB (164.50 million rows/s., 2.30 GB/s.)\r\n\r\n19.16.19.85\r\n1 rows in set. Elapsed: 0.376 sec. Processed 72.01 million rows, 1.01 GB (191.30 million rows/s., 2.68 GB/s.)\r\n1 rows in set. Elapsed: 0.384 sec. Processed 72.01 million rows, 1.01 GB (187.29 million rows/s., 2.62 GB/s.)\r\n1 rows in set. Elapsed: 0.375 sec. Processed 72.01 million rows, 1.01 GB (192.20 million rows/s., 2.69 GB/s.)\r\nset optimize_read_in_order=0\r\n1 rows in set. Elapsed: 0.148 sec. Processed 72.01 million rows, 1.01 GB (486.19 million rows/s., 6.81 GB/s.)\r\n1 rows in set. Elapsed: 0.154 sec. Processed 72.01 million rows, 1.01 GB (468.09 million rows/s., 6.55 GB/s.)\r\n1 rows in set. Elapsed: 0.147 sec. Processed 72.01 million rows, 1.01 GB (488.50 million rows/s., 6.84 GB/s.)\r\n\r\n19.13.7.57\r\n1 rows in set. Elapsed: 0.145 sec. Processed 72.01 million rows, 1.01 GB (498.13 million rows/s., 6.97 GB/s.)\r\n1 rows in set. Elapsed: 0.145 sec. Processed 72.01 million rows, 1.01 GB (497.25 million rows/s., 6.96 GB/s.)\r\n1 rows in set. Elapsed: 0.130 sec. Processed 72.01 million rows, 1.01 GB (553.85 million rows/s., 7.75 GB/s.)\r\n```\r\n\n@bobrik check your query with `set optimize_read_in_order=0`\nI was able to replicate your findings with synthetic data, but unfortunately they don't seem to apply to production data in a straightforward way.\r\n\r\n```\r\nSELECT count() as count,\r\n       countIf(service = 'nginx-fl') target_count\r\n  FROM jaeger_index_v2\r\n WHERE timestamp >= now() - 3600\r\n   AND timestamp <= now()\r\n\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500count\u2500\u252c\u2500target_count\u2500\u2510\r\n\u2502 576462864 \u2502    315336946 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nExecution with `optimize_read_in_order = 0`:\r\n\r\n```\r\n$ echo \"SELECT DISTINCT traceID FROM jaeger_index_v2 PREWHERE service = 'nginx-fl' AND timestamp >= now() - 3600 AND timestamp <= now() ORDER BY service DESC, timestamp DESC LIMIT 20\" | docker exec -i clickhouse-jaeger clickhouse-client --send_logs_level=trace --database jaeger --optimize_read_in_order=0\r\n[36ssds310] 2020.06.06 19:33:43.152544 [ 200 ] {64487cff-5fe7-4bda-83cb-6ee048a31e8d} <Debug> executeQuery: (from 127.0.0.1:45018) SELECT DISTINCT traceID FROM jaeger_index_v2 PREWHERE (service = 'nginx-fl') AND (timestamp >= (now() - 3600)) AND (timestamp <= now()) ORDER BY service DESC, timestamp DESC LIMIT 20\r\n[36ssds310] 2020.06.06 19:33:43.159753 [ 200 ] {64487cff-5fe7-4bda-83cb-6ee048a31e8d} <Trace> ContextAccess (default): Access granted: SELECT(timestamp, traceID, service) ON jaeger.jaeger_index_v2\r\n[36ssds310] 2020.06.06 19:33:43.161202 [ 200 ] {64487cff-5fe7-4bda-83cb-6ee048a31e8d} <Debug> jaeger.jaeger_index_v2 (SelectExecutor): Key condition: (column 0 in ['nginx-fl', 'nginx-fl']), (column 1 in [1591468423, +inf)), and, (column 1 in (-inf, 1591472023]), and\r\n[36ssds310] 2020.06.06 19:33:43.161278 [ 200 ] {64487cff-5fe7-4bda-83cb-6ee048a31e8d} <Debug> jaeger.jaeger_index_v2 (SelectExecutor): MinMax index condition: unknown, (column 0 in [1591468423, +inf)), and, (column 0 in (-inf, 1591472023]), and\r\n[36ssds310] 2020.06.06 19:33:43.659832 [ 200 ] {64487cff-5fe7-4bda-83cb-6ee048a31e8d} <Debug> jaeger.jaeger_index_v2 (SelectExecutor): Selected 43 parts by date, 43 parts by key, 308026 marks to read from 43 ranges\r\n[36ssds310] 2020.06.06 19:33:43.660968 [ 200 ] {64487cff-5fe7-4bda-83cb-6ee048a31e8d} <Trace> jaeger.jaeger_index_v2 (SelectExecutor): Reading approx. 315418624 rows with 24 streams\r\n[36ssds310] 2020.06.06 19:33:43.661785 [ 200 ] {64487cff-5fe7-4bda-83cb-6ee048a31e8d} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n[36ssds310] 2020.06.06 19:33:45.694154 [ 53 ] {64487cff-5fe7-4bda-83cb-6ee048a31e8d} <Debug> MemoryTracker: Current memory usage (for query): 1.00 GiB.\r\n1f5d23339ea7f255\r\n648e8404e228a9ab\r\nf575d14ef529fe78\r\n6866bf82b6958cfd\r\n6293b0c686c07185\r\n8fd5e776a6d2af99\r\n0d6e71d19d3e34cf\r\nbb8ff46f251c2bf8\r\n0d95092a87b8a622\r\naf97b905da3df0b4\r\n4c5f54127baacb86\r\n95aa64f20118af14\r\nc20d6d95943e8ebf\r\nb1c54d1e6fa4ed2c\r\n5579554e8f21d71b\r\n0db92d3d6e374e9b\r\nbac6c1714be1037e\r\n8b75ba81545fbff8\r\n8a2997254958763f\r\n3a7b8856e137908e\r\n[36ssds310] 2020.06.06 19:33:45.817737 [ 164 ] {64487cff-5fe7-4bda-83cb-6ee048a31e8d} <Debug> MergingSortedTransform: Merge sorted 1 blocks, 65536 rows in 2.16 sec., 30410.28 rows/sec., 1.76 MB/sec.\r\n[36ssds310] 2020.06.06 19:33:45.859868 [ 200 ] {64487cff-5fe7-4bda-83cb-6ee048a31e8d} <Information> executeQuery: Read 315418144 rows, 8.83 GiB in 2.707 sec., 116511983 rows/sec., 3.26 GiB/sec.\r\n[36ssds310] 2020.06.06 19:33:45.860020 [ 200 ] {64487cff-5fe7-4bda-83cb-6ee048a31e8d} <Debug> MemoryTracker: Peak memory usage (for query): 1.13 GiB.\r\n```\r\n\r\nExecution with `optimize_read_in_order = 1`:\r\n\r\n```\r\n$ echo \"SELECT DISTINCT traceID FROM jaeger_index_v2 PREWHERE service = 'nginx-fl' AND timestamp >= now() - 3600 AND timestamp <= now() ORDER BY service DESC, timestamp DESC LIMIT 20\" | docker exec -i clickhouse-jaeger clickhouse-client --send_logs_level=trace --database jaeger --optimize_read_in_order=1\r\n[36ssds310] 2020.06.06 19:33:47.476368 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Debug> executeQuery: (from 127.0.0.1:45022) SELECT DISTINCT traceID FROM jaeger_index_v2 PREWHERE (service = 'nginx-fl') AND (timestamp >= (now() - 3600)) AND (timestamp <= now()) ORDER BY service DESC, timestamp DESC LIMIT 20\r\n[36ssds310] 2020.06.06 19:33:47.477724 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> ContextAccess (default): Access granted: SELECT(timestamp, traceID, service) ON jaeger.jaeger_index_v2\r\n[36ssds310] 2020.06.06 19:33:47.480341 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Debug> jaeger.jaeger_index_v2 (SelectExecutor): Key condition: (column 0 in ['nginx-fl', 'nginx-fl']), (column 1 in [1591468427, +inf)), and, (column 1 in (-inf, 1591472027]), and\r\n[36ssds310] 2020.06.06 19:33:47.480385 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Debug> jaeger.jaeger_index_v2 (SelectExecutor): MinMax index condition: unknown, (column 0 in [1591468427, +inf)), and, (column 0 in (-inf, 1591472027]), and\r\n[36ssds310] 2020.06.06 19:33:48.068661 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Debug> jaeger.jaeger_index_v2 (SelectExecutor): Selected 45 parts by date, 45 parts by key, 308071 marks to read from 45 ranges\r\n[36ssds310] 2020.06.06 19:33:48.068925 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 3 ranges in reverse order from part 20200606_993262_993262_0, approx. 10240, up to 6144 rows starting from 4096\r\n[36ssds310] 2020.06.06 19:33:48.069229 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 3 ranges in reverse order from part 20200606_993261_993261_0, approx. 10240, up to 6144 rows starting from 4096\r\n[36ssds310] 2020.06.06 19:33:48.069487 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 3 ranges in reverse order from part 20200606_993260_993260_0, approx. 10240, up to 7168 rows starting from 3072\r\n[36ssds310] 2020.06.06 19:33:48.069740 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 3 ranges in reverse order from part 20200606_993259_993259_0, approx. 10240, up to 6144 rows starting from 3072\r\n[36ssds310] 2020.06.06 19:33:48.069989 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 3 ranges in reverse order from part 20200606_993258_993258_0, approx. 10240, up to 7168 rows starting from 3072\r\n[36ssds310] 2020.06.06 19:33:48.070213 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 3 ranges in reverse order from part 20200606_993257_993257_0, approx. 10240, up to 6144 rows starting from 4096\r\n[36ssds310] 2020.06.06 19:33:48.070438 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 3 ranges in reverse order from part 20200606_993256_993256_0, approx. 10240, up to 6144 rows starting from 3072\r\n[36ssds310] 2020.06.06 19:33:48.070678 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 3 ranges in reverse order from part 20200606_993255_993255_0, approx. 10240, up to 6144 rows starting from 3072\r\n[36ssds310] 2020.06.06 19:33:48.070909 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 6 ranges in reverse order from part 20200606_993249_993254_1, approx. 60416, up to 33792 rows starting from 22528\r\n[36ssds310] 2020.06.06 19:33:48.071141 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 6 ranges in reverse order from part 20200606_993243_993248_1, approx. 60416, up to 33792 rows starting from 21504\r\n[36ssds310] 2020.06.06 19:33:48.071377 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 6 ranges in reverse order from part 20200606_993237_993242_1, approx. 60416, up to 32768 rows starting from 23552\r\n[36ssds310] 2020.06.06 19:33:48.071661 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 5 ranges in reverse order from part 20200606_993231_993236_1, approx. 60416, up to 31744 rows starting from 24576\r\n[36ssds310] 2020.06.06 19:33:48.071946 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 7 ranges in reverse order from part 20200606_993215_993230_2, approx. 160768, up to 88064 rows starting from 59392\r\n[36ssds310] 2020.06.06 19:33:48.072206 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 7 ranges in reverse order from part 20200606_993194_993214_2, approx. 210944, up to 110592 rows starting from 81920\r\n[36ssds310] 2020.06.06 19:33:48.072522 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 7 ranges in reverse order from part 20200606_993178_993193_2, approx. 160768, up to 86016 rows starting from 60416\r\n[36ssds310] 2020.06.06 19:33:48.072876 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 7 ranges in reverse order from part 20200606_993157_993177_2, approx. 210944, up to 114688 rows starting from 77824\r\n[36ssds310] 2020.06.06 19:33:48.073145 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 7 ranges in reverse order from part 20200606_993140_993156_2, approx. 171008, up to 95232 rows starting from 61440\r\n[36ssds310] 2020.06.06 19:33:48.073365 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 6 ranges in reverse order from part 20200606_993134_993139_1, approx. 60416, up to 35840 rows starting from 19456\r\n[36ssds310] 2020.06.06 19:33:48.073622 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 6 ranges in reverse order from part 20200606_993128_993133_1, approx. 60416, up to 35840 rows starting from 19456\r\n[36ssds310] 2020.06.06 19:33:48.073875 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 7 ranges in reverse order from part 20200606_993112_993127_2, approx. 160768, up to 89088 rows starting from 58368\r\n[36ssds310] 2020.06.06 19:33:48.074097 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 9 ranges in reverse order from part 20200606_993065_993111_3, approx. 470016, up to 261120 rows starting from 172032\r\n[36ssds310] 2020.06.06 19:33:48.074339 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 9 ranges in reverse order from part 20200606_993022_993064_3, approx. 430080, up to 239616 rows starting from 153600\r\n[36ssds310] 2020.06.06 19:33:48.074583 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 13 ranges in reverse order from part 20200606_992927_993021_4, approx. 950272, up to 510976 rows starting from 363520\r\n[36ssds310] 2020.06.06 19:33:48.074866 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 10 ranges in reverse order from part 20200606_992868_992926_3, approx. 590848, up to 316416 rows starting from 225280\r\n[36ssds310] 2020.06.06 19:33:48.075106 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 10 ranges in reverse order from part 20200606_992809_992867_3, approx. 590848, up to 319488 rows starting from 222208\r\n[36ssds310] 2020.06.06 19:33:48.075390 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 36 ranges in reverse order from part 20200606_992453_992808_4, approx. 3560448, up to 1990656 rows starting from 1272832\r\n[36ssds310] 2020.06.06 19:33:48.076092 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 34 ranges in reverse order from part 20200606_992128_992452_5, approx. 3250176, up to 1846272 rows starting from 1124352\r\n[36ssds310] 2020.06.06 19:33:48.076754 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 24 ranges in reverse order from part 20200606_991910_992127_4, approx. 2180096, up to 1204224 rows starting from 792576\r\n[36ssds310] 2020.06.06 19:33:48.077194 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 23 ranges in reverse order from part 20200606_991708_991909_4, approx. 2020352, up to 1131520 rows starting from 717824\r\n[36ssds310] 2020.06.06 19:33:48.077693 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 12 ranges in reverse order from part 20200606_991628_991707_3, approx. 800768, up to 456704 rows starting from 282624\r\n[36ssds310] 2020.06.06 19:33:48.077911 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 9 ranges in reverse order from part 20200606_991585_991627_3, approx. 430080, up to 242688 rows starting from 149504\r\n[36ssds310] 2020.06.06 19:33:48.078108 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 22 ranges in reverse order from part 20200606_991387_991584_4, approx. 1980416, up to 1110016 rows starting from 695296\r\n[36ssds310] 2020.06.06 19:33:48.078371 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 23 ranges in reverse order from part 20200606_991182_991386_4, approx. 2050048, up to 1170432 rows starting from 704512\r\n[36ssds310] 2020.06.06 19:33:48.078664 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 11 ranges in reverse order from part 20200606_991119_991181_3, approx. 630784, up to 350208 rows starting from 228352\r\n[36ssds310] 2020.06.06 19:33:48.078869 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 23 ranges in reverse order from part 20200606_990884_991118_5, approx. 2350080, up to 1156096 rows starting from 898048\r\n[36ssds310] 2020.06.06 19:33:48.079438 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 7 ranges in reverse order from part 20200606_990884_991118_5, approx. 2350080, up to 99328 rows starting from 2054144\r\n[36ssds310] 2020.06.06 19:33:48.079630 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 29 ranges in reverse order from part 20200606_990599_990883_4, approx. 2850816, up to 1562624 rows starting from 1051648\r\n[36ssds310] 2020.06.06 19:33:48.079990 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 131 ranges in reverse order from part 20200606_989083_990598_6, approx. 15160320, up to 8209408 rows starting from 5700608\r\n[36ssds310] 2020.06.06 19:33:48.085186 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 55 ranges in reverse order from part 20200606_988071_989082_5, approx. 10120192, up to 3273728 rows starting from 3835904\r\n[36ssds310] 2020.06.06 19:33:48.087595 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 39 ranges in reverse order from part 20200606_988071_989082_5, approx. 10120192, up to 2183168 rows starting from 7109632\r\n[36ssds310] 2020.06.06 19:33:48.088262 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 83 ranges in reverse order from part 20200606_987153_988070_5, approx. 9180160, up to 5101568 rows starting from 3301376\r\n[36ssds310] 2020.06.06 19:33:48.090799 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 74 ranges in reverse order from part 20200606_986356_987152_5, approx. 7970816, up to 4468736 rows starting from 2824192\r\n[36ssds310] 2020.06.06 19:33:48.092906 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 27 ranges in reverse order from part 20200606_985116_986355_5, approx. 12400640, up to 1391616 rows starting from 4530176\r\n[36ssds310] 2020.06.06 19:33:48.095113 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 89 ranges in reverse order from part 20200606_985116_986355_5, approx. 12400640, up to 5451776 rows starting from 5921792\r\n[36ssds310] 2020.06.06 19:33:48.097575 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 123 ranges in reverse order from part 20200606_977102_985115_6, approx. 80140288, up to 7693312 rows starting from 29346816\r\n[36ssds310] 2020.06.06 19:33:48.138604 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 206 ranges in reverse order from part 20200606_977102_985115_6, approx. 80140288, up to 13145088 rows starting from 37040128\r\n[36ssds310] 2020.06.06 19:33:48.192925 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 206 ranges in reverse order from part 20200606_977102_985115_6, approx. 80140288, up to 13145088 rows starting from 50185216\r\n[36ssds310] 2020.06.06 19:33:48.228761 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 161 ranges in reverse order from part 20200606_977102_985115_6, approx. 80140288, up to 10165248 rows starting from 63330304\r\n[36ssds310] 2020.06.06 19:33:48.242918 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 51 ranges in reverse order from part 20200606_970018_977101_6, approx. 70840320, up to 2979840 rows starting from 26207232\r\n[36ssds310] 2020.06.06 19:33:48.259436 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 206 ranges in reverse order from part 20200606_970018_977101_6, approx. 70840320, up to 13145088 rows starting from 29187072\r\n[36ssds310] 2020.06.06 19:33:48.310801 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 206 ranges in reverse order from part 20200606_970018_977101_6, approx. 70840320, up to 13145088 rows starting from 42332160\r\n[36ssds310] 2020.06.06 19:33:48.342446 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 151 ranges in reverse order from part 20200606_970018_977101_6, approx. 70840320, up to 9518080 rows starting from 55477248\r\n[36ssds310] 2020.06.06 19:33:48.358888 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 61 ranges in reverse order from part 20200606_936886_970017_8, approx. 331200512, up to 3627008 rows starting from 123008000\r\n[36ssds310] 2020.06.06 19:33:48.474615 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 206 ranges in reverse order from part 20200606_936886_970017_8, approx. 331200512, up to 13145088 rows starting from 126635008\r\n[36ssds310] 2020.06.06 19:33:48.808268 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 206 ranges in reverse order from part 20200606_936886_970017_8, approx. 331200512, up to 13145088 rows starting from 139780096\r\n[36ssds310] 2020.06.06 19:33:49.146921 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 206 ranges in reverse order from part 20200606_936886_970017_8, approx. 331200512, up to 13145088 rows starting from 152925184\r\n[36ssds310] 2020.06.06 19:33:49.419445 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 206 ranges in reverse order from part 20200606_936886_970017_8, approx. 331200512, up to 13145088 rows starting from 166070272\r\n[36ssds310] 2020.06.06 19:33:49.701678 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 206 ranges in reverse order from part 20200606_936886_970017_8, approx. 331200512, up to 13145088 rows starting from 179215360\r\n[36ssds310] 2020.06.06 19:33:49.893840 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 206 ranges in reverse order from part 20200606_936886_970017_8, approx. 331200512, up to 13145088 rows starting from 192360448\r\n[36ssds310] 2020.06.06 19:33:50.066948 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 206 ranges in reverse order from part 20200606_936886_970017_8, approx. 331200512, up to 13145088 rows starting from 205505536\r\n[36ssds310] 2020.06.06 19:33:50.239383 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 206 ranges in reverse order from part 20200606_936886_970017_8, approx. 331200512, up to 13145088 rows starting from 218650624\r\n[36ssds310] 2020.06.06 19:33:50.405565 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 206 ranges in reverse order from part 20200606_936886_970017_8, approx. 331200512, up to 13145088 rows starting from 231795712\r\n[36ssds310] 2020.06.06 19:33:50.528907 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 206 ranges in reverse order from part 20200606_936886_970017_8, approx. 331200512, up to 13145088 rows starting from 244940800\r\n[36ssds310] 2020.06.06 19:33:50.636777 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 206 ranges in reverse order from part 20200606_936886_970017_8, approx. 331200512, up to 13145088 rows starting from 258085888\r\n[36ssds310] 2020.06.06 19:33:50.725546 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 206 ranges in reverse order from part 20200606_936886_970017_8, approx. 331200512, up to 13145088 rows starting from 271230976\r\n[36ssds310] 2020.06.06 19:33:50.794451 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 206 ranges in reverse order from part 20200606_936886_970017_8, approx. 331200512, up to 13145088 rows starting from 284376064\r\n[36ssds310] 2020.06.06 19:33:50.847645 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 106 ranges in reverse order from part 20200606_936886_970017_8, approx. 331200512, up to 6580224 rows starting from 297521152\r\n[36ssds310] 2020.06.06 19:33:50.874528 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> MergeTreeReverseSelectProcessor: Reading 105 ranges in reverse order from part 20200606_930592_936885_7, approx. 62940160, up to 6547456 rows starting from 51316736\r\n[36ssds310] 2020.06.06 19:33:50.884108 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\na06569ad523abb16\r\n04f476fec7c07565\r\n415ae19a79e637d5\r\n002de4d37364326a\r\ne6c2cfe630eaf307\r\n00040cdfca2db082\r\n8664cb271fd67be1\r\n2a850ba25d1092d7\r\n68c837d4e5eb54bc\r\n5fad87c6004efd54\r\n7f6d1a3d6e5145c9\r\n0a300bd7cd48ec24\r\n0122f9fb7c07a9ea\r\n605eaa27c7d9bc9a\r\n47ba85776278aa84\r\na1044437b9478cb0\r\n861e8b53b58ba6d3\r\n8d180416b8266361\r\ndc36e11b6528e52b\r\n3e6e19c8f4d13231\r\n[36ssds310] 2020.06.06 19:33:50.898340 [ 39 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Debug> MergingSortedTransform: Merge sorted 1 blocks, 104 rows in 0.01 sec., 7999.77 rows/sec., 1.31 MB/sec.\r\n[36ssds310] 2020.06.06 19:33:50.898522 [ 39 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Debug> MergingSortedTransform: Merge sorted 2 blocks, 2362 rows in 2.81 sec., 840.55 rows/sec., 0.05 MB/sec.\r\n[36ssds310] 2020.06.06 19:33:50.898617 [ 39 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Debug> MergingSortedTransform: Merge sorted 2 blocks, 2350 rows in 2.80 sec., 838.06 rows/sec., 0.05 MB/sec.\r\n[36ssds310] 2020.06.06 19:33:50.898681 [ 39 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Debug> MergingSortedTransform: Merge sorted 2 blocks, 2413 rows in 2.76 sec., 873.93 rows/sec., 0.05 MB/sec.\r\n[36ssds310] 2020.06.06 19:33:50.898655 [ 162 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Debug> MergingSortedTransform: Merge sorted 2 blocks, 1052 rows in 2.82 sec., 373.04 rows/sec., 0.01 MB/sec.\r\n[36ssds310] 2020.06.06 19:33:50.898755 [ 39 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Debug> MergingSortedTransform: Merge sorted 2 blocks, 2128 rows in 2.64 sec., 806.04 rows/sec., 0.05 MB/sec.\r\n[36ssds310] 2020.06.06 19:33:50.898835 [ 39 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Debug> MergingSortedTransform: Merge sorted 2 blocks, 2787 rows in 2.42 sec., 1149.72 rows/sec., 0.06 MB/sec.\r\n[36ssds310] 2020.06.06 19:33:50.898936 [ 39 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Debug> MergingSortedTransform: Merge sorted 2 blocks, 2303 rows in 0.02 sec., 153528.99 rows/sec., 9.16 MB/sec.\r\n[36ssds310] 2020.06.06 19:33:50.902640 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Information> executeQuery: Read 295760 rows, 8.16 MiB in 3.426 sec., 86323 rows/sec., 2.38 MiB/sec.\r\n[36ssds310] 2020.06.06 19:33:50.902755 [ 180 ] {d7ee244e-6bc1-45b5-84e3-1401fcb828e2} <Debug> MemoryTracker: Peak memory usage (for query): 16.42 MiB.\r\n```\r\n\r\nWhile it's better with optimization disabled, it's still not as great as it used to be. CPU is also saturated when `optimize_read_in_order` is disabled (all ~20 spare cores).\nLooks like `--call-graph dwarf` is pretty unreliable with Clickhouse running in a Docker container, so I rebuilt it with frame pointer support. Missing zstd frames were a giveaway.\r\n\r\n* Query with `optimize_read_in_order = 0` running in a loop:\r\n\r\n![image](https://user-images.githubusercontent.com/89186/83955153-c06e2180-a804-11ea-9818-1c1b01e537af.png)\r\n\r\n* Query with `optimize_read_in_order = 1` running in a loop:\r\n\r\n![image](https://user-images.githubusercontent.com/89186/83955162-cebc3d80-a804-11ea-9eaa-b42028b6e2bf.png)\r\n\n> pretty unreliable with Clickhouse running in a Docker container, so I rebuilt it with frame pointer support.\r\n\r\nSo it looks like not DateTime64 related. Can you please update the issue title and description correspondingly?\nI've reorganized my data to read it in forward order rather than in reverse. Let's close this one and hope I was alone with my issue.",
  "created_at": "2024-05-29T22:45:48Z"
}