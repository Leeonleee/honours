You will be provided with a partial code base and an issue statement explaining a problem to resolve.

<issue>
optimize_read_in_order slows down queries which takes only a few rows from set.
**Describe the situation**
If you have some query which returns a small subset of rows from a big table and sort it, optimize_read_in_order slows down query a lot.

**How to reproduce**
Clickhouse server 20.10, 20.11.4.13
```
CREATE TABLE default.test_scan
(
    `_time` DateTime,
    `key` UInt32,
    `value` UInt32,
    `dt` Date DEFAULT toDate(_time),
    `epoch` UInt64
)
ENGINE = MergeTree()
PARTITION BY toDate(_time)
ORDER BY (_time, epoch)
SETTINGS index_granularity = 8192

INSERT INTO test_scan(_time, key, value, epoch) SELECT now() + intDiv(number,100000), 1 as key, rand() % 250000 as value, now64(6) + (number * 10) FROM numbers(1000000000);

SELECT key, value FROM test_scan WHERE value = 3123 FORMAT Null
0 rows in set. Elapsed: 0.291 sec. Processed 1.00 billion rows, 4.07 GB (3.44 billion rows/s., 13.99 GB/s.)

SELECT key, value FROM test_scan WHERE value = 3123 ORDER BY _time, epoch FORMAT Null
0 rows in set. Elapsed: 2.614 sec. Processed 1.00 billion rows, 4.26 GB (382.52 million rows/s., 1.63 GB/s.)

SELECT key, value FROM test_scan WHERE value = 3123 AND not ignore(_time, epoch) FORMAT Null
0 rows in set. Elapsed: 0.388 sec. Processed 1.00 billion rows, 4.26 GB (2.58 billion rows/s., 11.00 GB/s.)

SELECT  key, value FROM (SELECT key, value, _time, epoch FROM test_scan WHERE value = 3123) ORDER BY _time, epoch FORMAT Null;
0 rows in set. Elapsed: 0.380 sec. Processed 1.00 billion rows, 4.26 GB (2.63 billion rows/s., 11.23 GB/s.)

set optimize_read_in_order=0;
SELECT key, value FROM test_scan WHERE value = 3123 ORDER BY _time, epoch FORMAT Null
0 rows in set. Elapsed: 0.359 sec. Processed 1.00 billion rows, 4.26 GB (2.79 billion rows/s., 11.88 GB/s.)

```
**Expected performance**
Queries would have similar performance.

Slow queries when reading in order
**Describe the situation**
We have a certain data-set with user logs, trying to get all data by certain tags spanning several hours.
Queries with ordering by part of ordering keys work slower than queries which don't use ordering by order-key

**How to reproduce**
Tried on ClickHouse 22.3 initially, same behavior on `22.7.4.16` too

CREATE statement:
```
CREATE TABLE logs_time_dt
(
    `time` DateTime64(9) Codec(Delta, ZSTD(7)),
    `project` LowCardinality(String) CODEC(ZSTD(7)),
    `service` LowCardinality(String) CODEC(ZSTD(7)),
    `message` String CODEC(ZSTD(7)),
    `tags_hash` Array(UInt64) CODEC(ZSTD(7)),
    INDEX idx_message message TYPE ngrambf_v1(3, 512, 2, 0) GRANULARITY 3,
    INDEX idx_tags_hash tags_hash TYPE bloom_filter(0.01) GRANULARITY 1
)
ENGINE = MergeTree
PARTITION BY toStartOfHour(time)
ORDER BY (project, service, time)
SETTINGS index_granularity = 1024
```

example dataset, several hours worth of data, the result set by particular tag is 20k rows:
```
insert into logs_time_dt
(time, project, service, message, tags_hash)
select
fromUnixTimestamp64Nano(toInt64(toUnixTimestamp64Nano(toDateTime64('2022-08-01',9))+number/(2777)*1e9)),
'test' as project,
'test' as service,
'foo',
[ number % 3000 ]
from system.numbers
limit 60*1e6
```

slow query reading by key in reverse order:
```
SELECT *
FROM logs_time_dt
WHERE (project = 'test') AND (service = 'test') AND has(tags_hash, 42)
ORDER BY time DESC
FORMAT `Null`

Query id: f3277eec-3a99-4442-849d-1209ad402548

Ok.

0 rows in set. Elapsed: 8.770 sec. Processed 29.27 million rows, 1.11 GB (3.34 million rows/s., 126.91 MB/s.)
```

reading in order seems to be faster in this particular example, but still slow:
```
SELECT *
FROM logs_time_dt
WHERE (project = 'test') AND (service = 'test') AND has(tags_hash, 42)
ORDER BY time ASC
FORMAT `Null`

Query id: a3ce44be-5ea2-4ffc-9c72-7f4fc656a5e1

Ok.

0 rows in set. Elapsed: 3.788 sec. Processed 29.27 million rows, 1.11 GB (7.73 million rows/s., 293.59 MB/s.)
```

sorting by negative timestamp works a lot faster with this case:
```
SELECT *
FROM logs_time_dt
WHERE (project = 'test') AND (service = 'test') AND has(tags_hash, 42)
ORDER BY -toUnixTimestamp64Nano(time) ASC
FORMAT `Null`

Query id: e5415b8d-556c-4ffa-910a-81902b252f55

Ok.

0 rows in set. Elapsed: 0.442 sec. Processed 29.27 million rows, 1.11 GB (66.28 million rows/s., 2.52 GB/s.)
```



**Expected performance**
I expect the "correct" query with ordering by primary key suffix wouldn't be slower than ordering by a computed expression.



MergingSortedTransform has low performance in read in order optimization 
**Describe the situation**
Queries taken from #40583 

```sql
CREATE TABLE logs_time_dt
(
    `time` DateTime64(9) Codec(Delta, ZSTD(7)),
    `project` LowCardinality(String) CODEC(ZSTD(7)),
    `service` LowCardinality(String) CODEC(ZSTD(7)),
    `message` String CODEC(ZSTD(7)),
    `tags_hash` Array(UInt64) CODEC(ZSTD(7)),
    INDEX idx_message message TYPE ngrambf_v1(3, 512, 2, 0) GRANULARITY 3,
    INDEX idx_tags_hash tags_hash TYPE bloom_filter(0.01) GRANULARITY 1
)
ENGINE = MergeTree
PARTITION BY toStartOfHour(time)
ORDER BY (project, service, time)
SETTINGS index_granularity = 1024;

insert into logs_time_dt
(time, project, service, message, tags_hash)
select
fromUnixTimestamp64Nano(toInt64(toUnixTimestamp64Nano(toDateTime64('2022-08-01',9))+number/(2777)*1e9)),
'test' as project,
'test' as service,
'foo',
[ number % 3000 ]
from system.numbers
limit 60*1e6;

SELECT *
FROM logs_time_dt
WHERE (project = 'test') AND (service = 'test') AND has(tags_hash, 42)
ORDER BY time
FORMAT `Null`;
```

Query pipeline:
```sql
(Expression)
ExpressionTransform
  (Sorting)
  MergingSortedTransform 45 → 1
    (Expression)
    ExpressionTransform × 45
      (Filter)
      FilterTransform × 45
        (ReadFromMergeTree)
        MergeTreeInOrder × 45 0 → 1
````

Performance:
```sql
0 rows in set. Elapsed: 3.425 sec. Processed 29.23 million rows, 1.11 GB (8.54 million rows/s., 324.36 MB/s.)
```

For read in order optimization, sometime the inputs of MergingSortedTransform are already sorted. Therefore, when executing, the processor will read all chunks from input port 0, then all chunks from input port 1, and so on so far (the indices may be not correct, just understand that the processor will scan the input ports sequentially).

Try a simple fix by adding buffer for each input:
```sql
(Expression)
ExpressionTransform
 (Sorting)
 MergingSortedTransform 45 → 1
   QueueBuffer × 45
     (Expression)
     ExpressionTransform × 45
       (Filter)
       FilterTransform × 45
         (ReadFromMergeTree)
         MergeTreeInOrder × 45 0 → 1
```
Performance:
```sql
0 rows in set. Elapsed: 0.281 sec. Processed 29.23 million rows, 1.11 GB (104.21 million rows/s., 3.96 GB/s.)
```

However, this simple fix has 2 main drawbacks:
1. Cannot take advantage of limit pushdown (can fix easily)
2. When the result set is big, OOM can happens, so need to add on-disk buffer.

We can discuss a solution to make read in order and normal read to have similar performance.


Clichouse v20.4 performance issue
**Describe the situation**

I'm seeing slower than expected queries on v20.4.4.18-stable compared to v20.1:20.1.4.14-stable.

My schema:

```sql
CREATE TABLE jaeger_index_v2 (
  timestamp DateTime CODEC(Delta, ZSTD(1)),
  traceID String CODEC(ZSTD(1)),
  service LowCardinality(String) CODEC(ZSTD(1)),
  operation LowCardinality(String) CODEC(ZSTD(1)),
  durationUs UInt64 CODEC(ZSTD(1)),
  tags Array(String) CODEC(ZSTD(1)),
  INDEX idx_tags tags TYPE bloom_filter(0.01) GRANULARITY 64
) ENGINE MergeTree()
PARTITION BY toDate(timestamp)
ORDER BY (service, timestamp)
SETTINGS index_granularity=1024
```

My query:

```sql
SELECT DISTINCT traceID
  FROM jaeger_index_v2
PREWHERE service = 'nginx-fl'
   AND timestamp >= now() - 3600
   AND timestamp <= now()
 ORDER BY service DESC,
       timestamp DESC
 LIMIT 20
```

On v20.1 I see ~0.5s per query
On v20.4 I see a very different picture for 1.5s per query.
</issue>

I need you to solve the provided issue by generating a code fix that can be applied directly to the repository

Respond below:
