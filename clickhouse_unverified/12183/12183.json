{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 12183,
  "instance_id": "ClickHouse__ClickHouse-12183",
  "issue_numbers": [
    "12132"
  ],
  "base_commit": "c4861155f7094c3c48e8f5efb1434a54e9159b01",
  "patch": "diff --git a/src/Compression/CachedCompressedReadBuffer.cpp b/src/Compression/CachedCompressedReadBuffer.cpp\nindex 1b083c004c09..3fb45ab09482 100644\n--- a/src/Compression/CachedCompressedReadBuffer.cpp\n+++ b/src/Compression/CachedCompressedReadBuffer.cpp\n@@ -72,9 +72,10 @@ bool CachedCompressedReadBuffer::nextImpl()\n }\n \n CachedCompressedReadBuffer::CachedCompressedReadBuffer(\n-    const std::string & path_, std::function<std::unique_ptr<ReadBufferFromFileBase>()> file_in_creator_, UncompressedCache * cache_)\n+    const std::string & path_, std::function<std::unique_ptr<ReadBufferFromFileBase>()> file_in_creator_, UncompressedCache * cache_, bool allow_different_codecs_)\n     : ReadBuffer(nullptr, 0), file_in_creator(std::move(file_in_creator_)), cache(cache_), path(path_), file_pos(0)\n {\n+    allow_different_codecs = allow_different_codecs_;\n }\n \n void CachedCompressedReadBuffer::seek(size_t offset_in_compressed_file, size_t offset_in_decompressed_block)\ndiff --git a/src/Compression/CachedCompressedReadBuffer.h b/src/Compression/CachedCompressedReadBuffer.h\nindex 88bcec8197d3..c2338f6f841b 100644\n--- a/src/Compression/CachedCompressedReadBuffer.h\n+++ b/src/Compression/CachedCompressedReadBuffer.h\n@@ -38,7 +38,7 @@ class CachedCompressedReadBuffer : public CompressedReadBufferBase, public ReadB\n     clockid_t clock_type {};\n \n public:\n-    CachedCompressedReadBuffer(const std::string & path, std::function<std::unique_ptr<ReadBufferFromFileBase>()> file_in_creator, UncompressedCache * cache_);\n+    CachedCompressedReadBuffer(const std::string & path, std::function<std::unique_ptr<ReadBufferFromFileBase>()> file_in_creator, UncompressedCache * cache_, bool allow_different_codecs_ = false);\n \n     void seek(size_t offset_in_compressed_file, size_t offset_in_decompressed_block);\n \ndiff --git a/src/Compression/CompressedReadBufferBase.cpp b/src/Compression/CompressedReadBufferBase.cpp\nindex a05b5cd7f64b..be2f697e1b36 100644\n--- a/src/Compression/CompressedReadBufferBase.cpp\n+++ b/src/Compression/CompressedReadBufferBase.cpp\n@@ -105,13 +105,24 @@ size_t CompressedReadBufferBase::readCompressedData(size_t & size_decompressed,\n     uint8_t method = ICompressionCodec::readMethod(own_compressed_buffer.data());\n \n     if (!codec)\n+    {\n         codec = CompressionCodecFactory::instance().get(method);\n+    }\n     else if (method != codec->getMethodByte())\n-        throw Exception(\"Data compressed with different methods, given method byte 0x\"\n-                        + getHexUIntLowercase(method)\n-                        + \", previous method byte 0x\"\n-                        + getHexUIntLowercase(codec->getMethodByte()),\n-                        ErrorCodes::CANNOT_DECOMPRESS);\n+    {\n+        if (allow_different_codecs)\n+        {\n+            codec = CompressionCodecFactory::instance().get(method);\n+        }\n+        else\n+        {\n+            throw Exception(\"Data compressed with different methods, given method byte 0x\"\n+                            + getHexUIntLowercase(method)\n+                            + \", previous method byte 0x\"\n+                            + getHexUIntLowercase(codec->getMethodByte()),\n+                            ErrorCodes::CANNOT_DECOMPRESS);\n+        }\n+    }\n \n     size_compressed_without_checksum = ICompressionCodec::readCompressedBlockSize(own_compressed_buffer.data());\n     size_decompressed = ICompressionCodec::readDecompressedBlockSize(own_compressed_buffer.data());\n@@ -163,21 +174,32 @@ void CompressedReadBufferBase::decompress(char * to, size_t size_decompressed, s\n     uint8_t method = ICompressionCodec::readMethod(compressed_buffer);\n \n     if (!codec)\n+    {\n         codec = CompressionCodecFactory::instance().get(method);\n+    }\n     else if (codec->getMethodByte() != method)\n-        throw Exception(\"Data compressed with different methods, given method byte \"\n-                        + getHexUIntLowercase(method)\n-                        + \", previous method byte \"\n-                        + getHexUIntLowercase(codec->getMethodByte()),\n-                        ErrorCodes::CANNOT_DECOMPRESS);\n+    {\n+        if (allow_different_codecs)\n+        {\n+            codec = CompressionCodecFactory::instance().get(method);\n+        }\n+        else\n+        {\n+            throw Exception(\"Data compressed with different methods, given method byte \"\n+                            + getHexUIntLowercase(method)\n+                            + \", previous method byte \"\n+                            + getHexUIntLowercase(codec->getMethodByte()),\n+                            ErrorCodes::CANNOT_DECOMPRESS);\n+        }\n+    }\n \n     codec->decompress(compressed_buffer, size_compressed_without_checksum, to);\n }\n \n \n /// 'compressed_in' could be initialized lazily, but before first call of 'readCompressedData'.\n-CompressedReadBufferBase::CompressedReadBufferBase(ReadBuffer * in)\n-    : compressed_in(in), own_compressed_buffer(0)\n+CompressedReadBufferBase::CompressedReadBufferBase(ReadBuffer * in, bool allow_different_codecs_)\n+    : compressed_in(in), own_compressed_buffer(0), allow_different_codecs(allow_different_codecs_)\n {\n }\n \ndiff --git a/src/Compression/CompressedReadBufferBase.h b/src/Compression/CompressedReadBufferBase.h\nindex f44140dcd043..71dc5274d5b3 100644\n--- a/src/Compression/CompressedReadBufferBase.h\n+++ b/src/Compression/CompressedReadBufferBase.h\n@@ -26,6 +26,9 @@ class CompressedReadBufferBase\n     /// Don't checksum on decompressing.\n     bool disable_checksum = false;\n \n+    /// Allow reading data, compressed by different codecs from one file.\n+    bool allow_different_codecs;\n+\n     /// Read compressed data into compressed_buffer. Get size of decompressed data from block header. Checksum if need.\n     /// Returns number of compressed bytes read.\n     size_t readCompressedData(size_t & size_decompressed, size_t & size_compressed_without_checksum);\n@@ -34,7 +37,7 @@ class CompressedReadBufferBase\n \n public:\n     /// 'compressed_in' could be initialized lazily, but before first call of 'readCompressedData'.\n-    CompressedReadBufferBase(ReadBuffer * in = nullptr);\n+    CompressedReadBufferBase(ReadBuffer * in = nullptr, bool allow_different_codecs_ = false);\n     ~CompressedReadBufferBase();\n \n     /** Disable checksums.\ndiff --git a/src/Compression/CompressedReadBufferFromFile.cpp b/src/Compression/CompressedReadBufferFromFile.cpp\nindex 8d6a42eacd34..f3fa2d6bc103 100644\n--- a/src/Compression/CompressedReadBufferFromFile.cpp\n+++ b/src/Compression/CompressedReadBufferFromFile.cpp\n@@ -36,20 +36,22 @@ bool CompressedReadBufferFromFile::nextImpl()\n     return true;\n }\n \n-CompressedReadBufferFromFile::CompressedReadBufferFromFile(std::unique_ptr<ReadBufferFromFileBase> buf)\n+CompressedReadBufferFromFile::CompressedReadBufferFromFile(std::unique_ptr<ReadBufferFromFileBase> buf, bool allow_different_codecs_)\n     : BufferWithOwnMemory<ReadBuffer>(0), p_file_in(std::move(buf)), file_in(*p_file_in)\n {\n     compressed_in = &file_in;\n+    allow_different_codecs = allow_different_codecs_;\n }\n \n \n CompressedReadBufferFromFile::CompressedReadBufferFromFile(\n-    const std::string & path, size_t estimated_size, size_t aio_threshold, size_t mmap_threshold, size_t buf_size)\n+    const std::string & path, size_t estimated_size, size_t aio_threshold, size_t mmap_threshold, size_t buf_size, bool allow_different_codecs_)\n     : BufferWithOwnMemory<ReadBuffer>(0)\n     , p_file_in(createReadBufferFromFileBase(path, estimated_size, aio_threshold, mmap_threshold, buf_size))\n     , file_in(*p_file_in)\n {\n     compressed_in = &file_in;\n+    allow_different_codecs = allow_different_codecs_;\n }\n \n \ndiff --git a/src/Compression/CompressedReadBufferFromFile.h b/src/Compression/CompressedReadBufferFromFile.h\nindex 1729490f6063..166b2595ef92 100644\n--- a/src/Compression/CompressedReadBufferFromFile.h\n+++ b/src/Compression/CompressedReadBufferFromFile.h\n@@ -28,10 +28,11 @@ class CompressedReadBufferFromFile : public CompressedReadBufferBase, public Buf\n     bool nextImpl() override;\n \n public:\n-    CompressedReadBufferFromFile(std::unique_ptr<ReadBufferFromFileBase> buf);\n+    CompressedReadBufferFromFile(std::unique_ptr<ReadBufferFromFileBase> buf, bool allow_different_codecs_ = false);\n \n     CompressedReadBufferFromFile(\n-        const std::string & path, size_t estimated_size, size_t aio_threshold, size_t mmap_threshold, size_t buf_size = DBMS_DEFAULT_BUFFER_SIZE);\n+        const std::string & path, size_t estimated_size, size_t aio_threshold, size_t mmap_threshold,\n+        size_t buf_size = DBMS_DEFAULT_BUFFER_SIZE, bool allow_different_codecs_ = false);\n \n     void seek(size_t offset_in_compressed_file, size_t offset_in_decompressed_block);\n \ndiff --git a/src/Compression/CompressionCodecDelta.cpp b/src/Compression/CompressionCodecDelta.cpp\nindex 51bd19f646b3..ecb7c36b205b 100644\n--- a/src/Compression/CompressionCodecDelta.cpp\n+++ b/src/Compression/CompressionCodecDelta.cpp\n@@ -36,6 +36,11 @@ ASTPtr CompressionCodecDelta::getCodecDesc() const\n     return makeASTFunction(\"Delta\", literal);\n }\n \n+void CompressionCodecDelta::updateHash(SipHash & hash) const\n+{\n+    getCodecDesc()->updateTreeHash(hash);\n+}\n+\n namespace\n {\n \ndiff --git a/src/Compression/CompressionCodecDelta.h b/src/Compression/CompressionCodecDelta.h\nindex 5c3979e063e8..a192fab051af 100644\n--- a/src/Compression/CompressionCodecDelta.h\n+++ b/src/Compression/CompressionCodecDelta.h\n@@ -14,7 +14,10 @@ class CompressionCodecDelta : public ICompressionCodec\n \n     ASTPtr getCodecDesc() const override;\n \n+    void updateHash(SipHash & hash) const override;\n+\n protected:\n+\n     UInt32 doCompressData(const char * source, UInt32 source_size, char * dest) const override;\n \n     void doDecompressData(const char * source, UInt32 source_size, char * dest, UInt32 uncompressed_size) const override;\ndiff --git a/src/Compression/CompressionCodecDoubleDelta.cpp b/src/Compression/CompressionCodecDoubleDelta.cpp\nindex 157e2df1a3f1..dd2e95a916df 100644\n--- a/src/Compression/CompressionCodecDoubleDelta.cpp\n+++ b/src/Compression/CompressionCodecDoubleDelta.cpp\n@@ -339,6 +339,12 @@ ASTPtr CompressionCodecDoubleDelta::getCodecDesc() const\n     return std::make_shared<ASTIdentifier>(\"DoubleDelta\");\n }\n \n+void CompressionCodecDoubleDelta::updateHash(SipHash & hash) const\n+{\n+    getCodecDesc()->updateTreeHash(hash);\n+    hash.update(data_bytes_size);\n+}\n+\n UInt32 CompressionCodecDoubleDelta::getMaxCompressedDataSize(UInt32 uncompressed_size) const\n {\n     const auto result = 2 // common header\ndiff --git a/src/Compression/CompressionCodecDoubleDelta.h b/src/Compression/CompressionCodecDoubleDelta.h\nindex a2690d244144..30ef086077d4 100644\n--- a/src/Compression/CompressionCodecDoubleDelta.h\n+++ b/src/Compression/CompressionCodecDoubleDelta.h\n@@ -100,7 +100,10 @@ class CompressionCodecDoubleDelta : public ICompressionCodec\n \n     ASTPtr getCodecDesc() const override;\n \n+    void updateHash(SipHash & hash) const override;\n+\n protected:\n+\n     UInt32 doCompressData(const char * source, UInt32 source_size, char * dest) const override;\n \n     void doDecompressData(const char * source, UInt32 source_size, char * dest, UInt32 uncompressed_size) const override;\ndiff --git a/src/Compression/CompressionCodecGorilla.cpp b/src/Compression/CompressionCodecGorilla.cpp\nindex 042835f4a32f..3d08734fe91f 100644\n--- a/src/Compression/CompressionCodecGorilla.cpp\n+++ b/src/Compression/CompressionCodecGorilla.cpp\n@@ -254,6 +254,12 @@ ASTPtr CompressionCodecGorilla::getCodecDesc() const\n     return std::make_shared<ASTIdentifier>(\"Gorilla\");\n }\n \n+void CompressionCodecGorilla::updateHash(SipHash & hash) const\n+{\n+    getCodecDesc()->updateTreeHash(hash);\n+    hash.update(data_bytes_size);\n+}\n+\n UInt32 CompressionCodecGorilla::getMaxCompressedDataSize(UInt32 uncompressed_size) const\n {\n     const auto result = 2 // common header\ndiff --git a/src/Compression/CompressionCodecGorilla.h b/src/Compression/CompressionCodecGorilla.h\nindex 523add0700f3..df0f329dc31c 100644\n--- a/src/Compression/CompressionCodecGorilla.h\n+++ b/src/Compression/CompressionCodecGorilla.h\n@@ -97,7 +97,10 @@ class CompressionCodecGorilla : public ICompressionCodec\n \n     ASTPtr getCodecDesc() const override;\n \n+    void updateHash(SipHash & hash) const override;\n+\n protected:\n+\n     UInt32 doCompressData(const char * source, UInt32 source_size, char * dest) const override;\n \n     void doDecompressData(const char * source, UInt32 source_size, char * dest, UInt32 uncompressed_size) const override;\ndiff --git a/src/Compression/CompressionCodecLZ4.cpp b/src/Compression/CompressionCodecLZ4.cpp\nindex cf3622cd7022..1370349d68db 100644\n--- a/src/Compression/CompressionCodecLZ4.cpp\n+++ b/src/Compression/CompressionCodecLZ4.cpp\n@@ -35,6 +35,11 @@ ASTPtr CompressionCodecLZ4::getCodecDesc() const\n     return std::make_shared<ASTIdentifier>(\"LZ4\");\n }\n \n+void CompressionCodecLZ4::updateHash(SipHash & hash) const\n+{\n+    getCodecDesc()->updateTreeHash(hash);\n+}\n+\n UInt32 CompressionCodecLZ4::getMaxCompressedDataSize(UInt32 uncompressed_size) const\n {\n     return LZ4_COMPRESSBOUND(uncompressed_size);\ndiff --git a/src/Compression/CompressionCodecLZ4.h b/src/Compression/CompressionCodecLZ4.h\nindex 2f19af08185d..229e25481e65 100644\n--- a/src/Compression/CompressionCodecLZ4.h\n+++ b/src/Compression/CompressionCodecLZ4.h\n@@ -18,6 +18,8 @@ class CompressionCodecLZ4 : public ICompressionCodec\n \n     UInt32 getAdditionalSizeAtTheEndOfBuffer() const override { return LZ4::ADDITIONAL_BYTES_AT_END_OF_BUFFER; }\n \n+    void updateHash(SipHash & hash) const override;\n+\n protected:\n     UInt32 doCompressData(const char * source, UInt32 source_size, char * dest) const override;\n \ndiff --git a/src/Compression/CompressionCodecMultiple.cpp b/src/Compression/CompressionCodecMultiple.cpp\nindex 868df90825e0..77f0fc132fed 100644\n--- a/src/Compression/CompressionCodecMultiple.cpp\n+++ b/src/Compression/CompressionCodecMultiple.cpp\n@@ -37,6 +37,12 @@ ASTPtr CompressionCodecMultiple::getCodecDesc() const\n     return result;\n }\n \n+void CompressionCodecMultiple::updateHash(SipHash & hash) const\n+{\n+    for (const auto & codec : codecs)\n+        codec->updateHash(hash);\n+}\n+\n UInt32 CompressionCodecMultiple::getMaxCompressedDataSize(UInt32 uncompressed_size) const\n {\n     UInt32 compressed_size = uncompressed_size;\ndiff --git a/src/Compression/CompressionCodecMultiple.h b/src/Compression/CompressionCodecMultiple.h\nindex cd50d3250e3e..6bac189bdf7d 100644\n--- a/src/Compression/CompressionCodecMultiple.h\n+++ b/src/Compression/CompressionCodecMultiple.h\n@@ -19,7 +19,10 @@ class CompressionCodecMultiple final : public ICompressionCodec\n \n     static std::vector<uint8_t> getCodecsBytesFromData(const char * source);\n \n+    void updateHash(SipHash & hash) const override;\n+\n protected:\n+\n     UInt32 doCompressData(const char * source, UInt32 source_size, char * dest) const override;\n \n     void doDecompressData(const char * source, UInt32 source_size, char * dest, UInt32 decompressed_size) const override;\ndiff --git a/src/Compression/CompressionCodecNone.cpp b/src/Compression/CompressionCodecNone.cpp\nindex 50c19b2b5474..f727c4b48604 100644\n--- a/src/Compression/CompressionCodecNone.cpp\n+++ b/src/Compression/CompressionCodecNone.cpp\n@@ -17,6 +17,11 @@ ASTPtr CompressionCodecNone::getCodecDesc() const\n     return std::make_shared<ASTIdentifier>(\"NONE\");\n }\n \n+void CompressionCodecNone::updateHash(SipHash & hash) const\n+{\n+    getCodecDesc()->updateTreeHash(hash);\n+}\n+\n UInt32 CompressionCodecNone::doCompressData(const char * source, UInt32 source_size, char * dest) const\n {\n     memcpy(dest, source, source_size);\ndiff --git a/src/Compression/CompressionCodecNone.h b/src/Compression/CompressionCodecNone.h\nindex ed6040631982..370ef3016947 100644\n--- a/src/Compression/CompressionCodecNone.h\n+++ b/src/Compression/CompressionCodecNone.h\n@@ -15,7 +15,10 @@ class CompressionCodecNone : public ICompressionCodec\n \n     ASTPtr getCodecDesc() const override;\n \n+    void updateHash(SipHash & hash) const override;\n+\n protected:\n+\n     UInt32 doCompressData(const char * source, UInt32 source_size, char * dest) const override;\n \n     void doDecompressData(const char * source, UInt32 source_size, char * dest, UInt32 uncompressed_size) const override;\ndiff --git a/src/Compression/CompressionCodecT64.cpp b/src/Compression/CompressionCodecT64.cpp\nindex 16462e50ebd7..30972a5fe1f0 100644\n--- a/src/Compression/CompressionCodecT64.cpp\n+++ b/src/Compression/CompressionCodecT64.cpp\n@@ -646,6 +646,13 @@ ASTPtr CompressionCodecT64::getCodecDesc() const\n     return makeASTFunction(\"T64\", literal);\n }\n \n+void CompressionCodecT64::updateHash(SipHash & hash) const\n+{\n+    getCodecDesc()->updateTreeHash(hash);\n+    hash.update(type_idx);\n+    hash.update(variant);\n+}\n+\n void registerCodecT64(CompressionCodecFactory & factory)\n {\n     auto reg_func = [&](const ASTPtr & arguments, DataTypePtr type) -> CompressionCodecPtr\ndiff --git a/src/Compression/CompressionCodecT64.h b/src/Compression/CompressionCodecT64.h\nindex 11efbea0955d..9671eb81ce1a 100644\n--- a/src/Compression/CompressionCodecT64.h\n+++ b/src/Compression/CompressionCodecT64.h\n@@ -35,6 +35,8 @@ class CompressionCodecT64 : public ICompressionCodec\n \n     ASTPtr getCodecDesc() const override;\n \n+    void updateHash(SipHash & hash) const override;\n+\n protected:\n     UInt32 doCompressData(const char * src, UInt32 src_size, char * dst) const override;\n     void doDecompressData(const char * src, UInt32 src_size, char * dst, UInt32 uncompressed_size) const override;\ndiff --git a/src/Compression/CompressionCodecZSTD.cpp b/src/Compression/CompressionCodecZSTD.cpp\nindex ab48580533e3..3b317884fece 100644\n--- a/src/Compression/CompressionCodecZSTD.cpp\n+++ b/src/Compression/CompressionCodecZSTD.cpp\n@@ -32,6 +32,11 @@ ASTPtr CompressionCodecZSTD::getCodecDesc() const\n     return makeASTFunction(\"ZSTD\", literal);\n }\n \n+void CompressionCodecZSTD::updateHash(SipHash & hash) const\n+{\n+    getCodecDesc()->updateTreeHash(hash);\n+}\n+\n UInt32 CompressionCodecZSTD::getMaxCompressedDataSize(UInt32 uncompressed_size) const\n {\n     return ZSTD_compressBound(uncompressed_size);\ndiff --git a/src/Compression/CompressionCodecZSTD.h b/src/Compression/CompressionCodecZSTD.h\nindex 2ad893083c30..3bfb6bb1d4d7 100644\n--- a/src/Compression/CompressionCodecZSTD.h\n+++ b/src/Compression/CompressionCodecZSTD.h\n@@ -21,7 +21,10 @@ class CompressionCodecZSTD : public ICompressionCodec\n \n     UInt32 getMaxCompressedDataSize(UInt32 uncompressed_size) const override;\n \n+    void updateHash(SipHash & hash) const override;\n+\n protected:\n+\n     UInt32 doCompressData(const char * source, UInt32 source_size, char * dest) const override;\n \n     void doDecompressData(const char * source, UInt32 source_size, char * dest, UInt32 uncompressed_size) const override;\ndiff --git a/src/Compression/ICompressionCodec.cpp b/src/Compression/ICompressionCodec.cpp\nindex 4aafc2986581..5de015b2680a 100644\n--- a/src/Compression/ICompressionCodec.cpp\n+++ b/src/Compression/ICompressionCodec.cpp\n@@ -35,6 +35,13 @@ ASTPtr ICompressionCodec::getFullCodecDesc() const\n     return result;\n }\n \n+UInt64 ICompressionCodec::getHash() const\n+{\n+    SipHash hash;\n+    updateHash(hash);\n+    return hash.get64();\n+}\n+\n UInt32 ICompressionCodec::compress(const char * source, UInt32 source_size, char * dest) const\n {\n     assert(source != nullptr && dest != nullptr);\ndiff --git a/src/Compression/ICompressionCodec.h b/src/Compression/ICompressionCodec.h\nindex fa1f73ce4ddf..8f72ba552007 100644\n--- a/src/Compression/ICompressionCodec.h\n+++ b/src/Compression/ICompressionCodec.h\n@@ -5,6 +5,7 @@\n #include <Compression/CompressionInfo.h>\n #include <Core/Types.h>\n #include <Parsers/IAST.h>\n+#include <Common/SipHash.h>\n \n \n namespace DB\n@@ -36,6 +37,10 @@ class ICompressionCodec : private boost::noncopyable\n     /// \"CODEC(LZ4,LZ4HC(5))\"\n     ASTPtr getFullCodecDesc() const;\n \n+    /// Hash, that depends on codec ast and optional parameters like data type\n+    virtual void updateHash(SipHash & hash) const = 0;\n+    UInt64 getHash() const;\n+\n     /// Compressed bytes from uncompressed source to dest. Dest should preallocate memory\n     UInt32 compress(const char * source, UInt32 source_size, char * dest) const;\n \ndiff --git a/src/IO/ReadBuffer.h b/src/IO/ReadBuffer.h\nindex a35e5206e49c..3d6eb6970cee 100644\n--- a/src/IO/ReadBuffer.h\n+++ b/src/IO/ReadBuffer.h\n@@ -123,6 +123,11 @@ class ReadBuffer : public BufferBase\n         return bytes_ignored;\n     }\n \n+    void ignoreAll()\n+    {\n+        tryIgnore(std::numeric_limits<size_t>::max());\n+    }\n+\n     /** Reads a single byte. */\n     bool ALWAYS_INLINE read(char & c)\n     {\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\nindex acf4df879acd..9c3325d3d5a3 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.cpp\n@@ -15,19 +15,29 @@ MergeTreeDataPartWriterCompact::MergeTreeDataPartWriterCompact(\n     : MergeTreeDataPartWriterOnDisk(data_part_, columns_list_, metadata_snapshot_,\n         indices_to_recalc_, marks_file_extension_,\n         default_codec_, settings_, index_granularity_)\n+    , plain_file(data_part->volume->getDisk()->writeFile(\n+            part_path + MergeTreeDataPartCompact::DATA_FILE_NAME_WITH_EXTENSION,\n+            settings.max_compress_block_size,\n+            WriteMode::Rewrite,\n+            settings.estimated_size,\n+            settings.aio_threshold))\n+    , plain_hashing(*plain_file)\n+    , marks_file(data_part->volume->getDisk()->writeFile(\n+        part_path + MergeTreeDataPartCompact::DATA_FILE_NAME + marks_file_extension_,\n+        4096,\n+        WriteMode::Rewrite))\n+    , marks(*marks_file)\n {\n-    using DataPart = MergeTreeDataPartCompact;\n-    String data_file_name = DataPart::DATA_FILE_NAME;\n-\n-    stream = std::make_unique<Stream>(\n-        data_file_name,\n-        data_part->volume->getDisk(),\n-        part_path + data_file_name, DataPart::DATA_FILE_EXTENSION,\n-        part_path + data_file_name, marks_file_extension,\n-        default_codec,\n-        settings.max_compress_block_size,\n-        settings.estimated_size,\n-        settings.aio_threshold);\n+    const auto & storage_columns = metadata_snapshot->getColumns();\n+    for (const auto & column : columns_list)\n+    {\n+        auto codec = storage_columns.getCodecOrDefault(column.name, default_codec);\n+        auto & stream = streams_by_codec[codec->getHash()];\n+        if (!stream)\n+            stream = std::make_shared<CompressedStream>(plain_hashing, codec);\n+\n+        compressed_streams.push_back(stream);\n+    }\n }\n \n void MergeTreeDataPartWriterCompact::write(\n@@ -97,15 +107,21 @@ void MergeTreeDataPartWriterCompact::writeBlock(const Block & block)\n         if (rows_to_write)\n             data_written = true;\n \n-        for (const auto & column : columns_list)\n+        auto name_and_type = columns_list.begin();\n+        for (size_t i = 0; i < columns_list.size(); ++i, ++name_and_type)\n         {\n-            writeIntBinary(stream->plain_hashing.count(), stream->marks);\n-            writeIntBinary(stream->compressed.offset(), stream->marks);\n+            auto & stream = compressed_streams[i];\n+\n+            /// Offset should be 0, because compressed block is written for every granule.\n+            assert(stream->hashing_buf.offset() == 0);\n \n-            writeColumnSingleGranule(block.getByName(column.name), current_row, rows_to_write);\n+            writeIntBinary(plain_hashing.count(), marks);\n+            writeIntBinary(UInt64(0), marks);\n+\n+            writeColumnSingleGranule(block.getByName(name_and_type->name), stream, current_row, rows_to_write);\n \n             /// Write one compressed block per column in granule for more optimal reading.\n-            stream->compressed.next();\n+            stream->hashing_buf.next();\n         }\n \n         ++from_mark;\n@@ -120,19 +136,22 @@ void MergeTreeDataPartWriterCompact::writeBlock(const Block & block)\n             index_granularity.appendMark(rows_written);\n         }\n \n-        writeIntBinary(rows_to_write, stream->marks);\n+        writeIntBinary(rows_to_write, marks);\n     }\n \n     next_index_offset = 0;\n     next_mark = from_mark;\n }\n \n-void MergeTreeDataPartWriterCompact::writeColumnSingleGranule(const ColumnWithTypeAndName & column, size_t from_row, size_t number_of_rows) const\n+void MergeTreeDataPartWriterCompact::writeColumnSingleGranule(\n+    const ColumnWithTypeAndName & column,\n+    const CompressedStreamPtr & stream,\n+    size_t from_row, size_t number_of_rows)\n {\n     IDataType::SerializeBinaryBulkStatePtr state;\n     IDataType::SerializeBinaryBulkSettings serialize_settings;\n \n-    serialize_settings.getter = [this](IDataType::SubstreamPath) -> WriteBuffer * { return &stream->compressed; };\n+    serialize_settings.getter = [&stream](IDataType::SubstreamPath) -> WriteBuffer * { return &stream->hashing_buf; };\n     serialize_settings.position_independent_encoding = true;\n     serialize_settings.low_cardinality_max_dictionary_size = 0;\n \n@@ -146,19 +165,25 @@ void MergeTreeDataPartWriterCompact::finishDataSerialization(IMergeTreeDataPart:\n     if (columns_buffer.size() != 0)\n         writeBlock(header.cloneWithColumns(columns_buffer.releaseColumns()));\n \n+#ifndef NDEBUG\n+    /// Offsets should be 0, because compressed block is written for every granule.\n+    for (const auto & [_, stream] : streams_by_codec)\n+        assert(stream->hashing_buf.offset() == 0);\n+#endif\n+\n     if (with_final_mark && data_written)\n     {\n         for (size_t i = 0; i < columns_list.size(); ++i)\n         {\n-            writeIntBinary(stream->plain_hashing.count(), stream->marks);\n-            writeIntBinary(stream->compressed.offset(), stream->marks);\n+            writeIntBinary(plain_hashing.count(), marks);\n+            writeIntBinary(UInt64(0), marks);\n         }\n-        writeIntBinary(0ULL, stream->marks);\n+        writeIntBinary(UInt64(0), marks);\n     }\n \n-    stream->finalize();\n-    stream->addToChecksums(checksums);\n-    stream.reset();\n+    plain_file->next();\n+    marks.next();\n+    addToChecksums(checksums);\n }\n \n static void fillIndexGranularityImpl(\n@@ -199,6 +224,32 @@ void MergeTreeDataPartWriterCompact::fillIndexGranularity(size_t index_granulari\n         rows_in_block);\n }\n \n+void MergeTreeDataPartWriterCompact::addToChecksums(MergeTreeDataPartChecksums & checksums)\n+{\n+    String data_file_name = MergeTreeDataPartCompact::DATA_FILE_NAME_WITH_EXTENSION;\n+    String marks_file_name = MergeTreeDataPartCompact::DATA_FILE_NAME +  marks_file_extension;\n+\n+    size_t uncompressed_size = 0;\n+    CityHash_v1_0_2::uint128 uncompressed_hash{0, 0};\n+\n+    for (const auto & [_, stream] : streams_by_codec)\n+    {\n+        uncompressed_size += stream->hashing_buf.count();\n+        auto stream_hash = stream->hashing_buf.getHash();\n+        uncompressed_hash = CityHash_v1_0_2::CityHash128WithSeed(\n+            reinterpret_cast<char *>(&stream_hash), sizeof(stream_hash), uncompressed_hash);\n+    }\n+\n+    checksums.files[data_file_name].is_compressed = true;\n+    checksums.files[data_file_name].uncompressed_size = uncompressed_size;\n+    checksums.files[data_file_name].uncompressed_hash = uncompressed_hash;\n+    checksums.files[data_file_name].file_size = plain_hashing.count();\n+    checksums.files[data_file_name].file_hash = plain_hashing.getHash();\n+\n+    checksums.files[marks_file_name].file_size = marks.count();\n+    checksums.files[marks_file_name].file_hash = marks.getHash();\n+}\n+\n void MergeTreeDataPartWriterCompact::ColumnsBuffer::add(MutableColumns && columns)\n {\n     if (accumulated_columns.empty())\ndiff --git a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h\nindex 4c706e9ba762..fecf5ce40e86 100644\n--- a/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h\n+++ b/src/Storages/MergeTree/MergeTreeDataPartWriterCompact.h\n@@ -26,15 +26,9 @@ class MergeTreeDataPartWriterCompact : public MergeTreeDataPartWriterOnDisk\n     void fillIndexGranularity(size_t index_granularity_for_block, size_t rows_in_block) override;\n \n private:\n-    /// Write single granule of one column (rows between 2 marks)\n-    void writeColumnSingleGranule(\n-        const ColumnWithTypeAndName & column,\n-        size_t from_row,\n-        size_t number_of_rows) const;\n-\n     void writeBlock(const Block & block);\n \n-    StreamPtr stream;\n+    void addToChecksums(MergeTreeDataPartChecksums & checksums);\n \n     Block header;\n \n@@ -53,6 +47,38 @@ class MergeTreeDataPartWriterCompact : public MergeTreeDataPartWriterOnDisk\n     };\n \n     ColumnsBuffer columns_buffer;\n+\n+    /// hashing_buf -> compressed_buf -> plain_hashing -> plain_file\n+    std::unique_ptr<WriteBufferFromFileBase> plain_file;\n+    HashingWriteBuffer plain_hashing;\n+\n+    struct CompressedStream\n+    {\n+        CompressedWriteBuffer compressed_buf;\n+        HashingWriteBuffer hashing_buf;\n+\n+        CompressedStream(WriteBuffer & buf, const CompressionCodecPtr & codec)\n+            : compressed_buf(buf, codec), hashing_buf(compressed_buf) {}\n+    };\n+\n+    using CompressedStreamPtr = std::shared_ptr<CompressedStream>;\n+\n+    /// Create compressed stream for every different codec.\n+    std::unordered_map<UInt64, CompressedStreamPtr> streams_by_codec;\n+\n+    /// For better performance save pointer to stream by every column.\n+    std::vector<CompressedStreamPtr> compressed_streams;\n+\n+    /// marks -> marks_file\n+    std::unique_ptr<WriteBufferFromFileBase> marks_file;\n+    HashingWriteBuffer marks;\n+\n+    /// Write single granule of one column (rows between 2 marks)\n+    static void writeColumnSingleGranule(\n+        const ColumnWithTypeAndName & column,\n+        const CompressedStreamPtr & stream,\n+        size_t from_row,\n+        size_t number_of_rows);\n };\n \n }\ndiff --git a/src/Storages/MergeTree/MergeTreeReaderCompact.cpp b/src/Storages/MergeTree/MergeTreeReaderCompact.cpp\nindex 5499b8cc2c63..87b3f0a43298 100644\n--- a/src/Storages/MergeTree/MergeTreeReaderCompact.cpp\n+++ b/src/Storages/MergeTree/MergeTreeReaderCompact.cpp\n@@ -76,17 +76,18 @@ MergeTreeReaderCompact::MergeTreeReaderCompact(\n         if (uncompressed_cache)\n         {\n             auto buffer = std::make_unique<CachedCompressedReadBuffer>(\n-                    fullPath(data_part->volume->getDisk(), full_data_path),\n-                    [this, full_data_path, buffer_size]()\n-                    {\n-                        return data_part->volume->getDisk()->readFile(\n-                                full_data_path,\n-                                buffer_size,\n-                                0,\n-                                settings.min_bytes_to_use_direct_io,\n-                                settings.min_bytes_to_use_mmap_io);\n-                    },\n-                    uncompressed_cache);\n+                fullPath(data_part->volume->getDisk(), full_data_path),\n+                [this, full_data_path, buffer_size]()\n+                {\n+                    return data_part->volume->getDisk()->readFile(\n+                        full_data_path,\n+                        buffer_size,\n+                        0,\n+                        settings.min_bytes_to_use_direct_io,\n+                        settings.min_bytes_to_use_mmap_io);\n+                },\n+                uncompressed_cache,\n+                /* allow_different_codecs = */ true);\n \n             if (profile_callback_)\n                 buffer->setProfileCallback(profile_callback_, clock_type_);\n@@ -97,9 +98,10 @@ MergeTreeReaderCompact::MergeTreeReaderCompact(\n         else\n         {\n             auto buffer =\n-                    std::make_unique<CompressedReadBufferFromFile>(\n-                            data_part->volume->getDisk()->readFile(\n-                                    full_data_path, buffer_size, 0, settings.min_bytes_to_use_direct_io, settings.min_bytes_to_use_mmap_io));\n+                std::make_unique<CompressedReadBufferFromFile>(\n+                    data_part->volume->getDisk()->readFile(\n+                        full_data_path, buffer_size, 0, settings.min_bytes_to_use_direct_io, settings.min_bytes_to_use_mmap_io),\n+                    /* allow_different_codecs = */ true);\n \n             if (profile_callback_)\n                 buffer->setProfileCallback(profile_callback_, clock_type_);\n@@ -248,7 +250,6 @@ void MergeTreeReaderCompact::seekToMark(size_t row_index, size_t column_index)\n     }\n }\n \n-\n bool MergeTreeReaderCompact::isContinuousReading(size_t mark, size_t column_position)\n {\n     if (!last_read_granule)\ndiff --git a/src/Storages/MergeTree/MergeTreeReaderCompact.h b/src/Storages/MergeTree/MergeTreeReaderCompact.h\nindex 2b4b0c922edc..9ef887165791 100644\n--- a/src/Storages/MergeTree/MergeTreeReaderCompact.h\n+++ b/src/Storages/MergeTree/MergeTreeReaderCompact.h\n@@ -2,6 +2,7 @@\n \n #include <Core/NamesAndTypes.h>\n #include <Storages/MergeTree/IMergeTreeReader.h>\n+#include <IO/ReadBufferFromFileBase.h>\n \n \n namespace DB\ndiff --git a/src/Storages/MergeTree/checkDataPart.cpp b/src/Storages/MergeTree/checkDataPart.cpp\nindex 76a160fc0f9d..63b061b9702d 100644\n--- a/src/Storages/MergeTree/checkDataPart.cpp\n+++ b/src/Storages/MergeTree/checkDataPart.cpp\n@@ -89,7 +89,7 @@ IMergeTreeDataPart::Checksums checkDataPart(\n         CompressedReadBuffer uncompressing_buf(compressed_hashing_buf);\n         HashingReadBuffer uncompressed_hashing_buf(uncompressing_buf);\n \n-        uncompressed_hashing_buf.tryIgnore(std::numeric_limits<size_t>::max());\n+        uncompressed_hashing_buf.ignoreAll();\n         return IMergeTreeDataPart::Checksums::Checksum\n         {\n             compressed_hashing_buf.count(), compressed_hashing_buf.getHash(),\n@@ -97,11 +97,24 @@ IMergeTreeDataPart::Checksums checkDataPart(\n         };\n     };\n \n+    /// This function calculates only checksum of file content (compressed or uncompressed).\n+    auto checksum_file = [](const DiskPtr & disk_, const String & file_path)\n+    {\n+        auto file_buf = disk_->readFile(file_path);\n+        HashingReadBuffer hashing_buf(*file_buf);\n+        hashing_buf.ignoreAll();\n+        return IMergeTreeDataPart::Checksums::Checksum{hashing_buf.count(), hashing_buf.getHash()};\n+    };\n+\n+    bool check_uncompressed = true;\n     /// First calculate checksums for columns data\n     if (part_type == MergeTreeDataPartType::COMPACT)\n     {\n         const auto & file_name = MergeTreeDataPartCompact::DATA_FILE_NAME_WITH_EXTENSION;\n-        checksums_data.files[file_name] = checksum_compressed_file(disk, path + file_name);\n+        checksums_data.files[file_name] = checksum_file(disk, path + file_name);\n+        /// Uncompressed checksums in compact parts are computed in a complex way.\n+        /// We check only checksum of compressed file.\n+        check_uncompressed = false;\n     }\n     else if (part_type == MergeTreeDataPartType::WIDE)\n     {\n@@ -142,10 +155,7 @@ IMergeTreeDataPart::Checksums checkDataPart(\n             if (txt_checksum_it == checksum_files_txt.end() || txt_checksum_it->second.uncompressed_size == 0)\n             {\n                 /// The file is not compressed.\n-                auto file_buf = disk->readFile(it->path());\n-                HashingReadBuffer hashing_buf(*file_buf);\n-                hashing_buf.tryIgnore(std::numeric_limits<size_t>::max());\n-                checksums_data.files[file_name] = IMergeTreeDataPart::Checksums::Checksum(hashing_buf.count(), hashing_buf.getHash());\n+                checksums_data.files[file_name] = checksum_file(disk, it->path());\n             }\n             else /// If we have both compressed and uncompressed in txt, than calculate them\n             {\n@@ -158,7 +168,7 @@ IMergeTreeDataPart::Checksums checkDataPart(\n         return {};\n \n     if (require_checksums || !checksums_txt.files.empty())\n-        checksums_txt.checkEqual(checksums_data, true);\n+        checksums_txt.checkEqual(checksums_data, check_uncompressed);\n \n     return checksums_data;\n }\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/01375_compact_parts_codecs.reference b/tests/queries/0_stateless/01375_compact_parts_codecs.reference\nnew file mode 100644\nindex 000000000000..b48892597b61\n--- /dev/null\n+++ b/tests/queries/0_stateless/01375_compact_parts_codecs.reference\n@@ -0,0 +1,9 @@\n+12000\t11890\n+499500\t499500\t999\n+499500\t499500\t999\n+11965\t11890\n+499500\t499500\t999\n+499500\t499500\t999\n+5858\t11890\n+499500\t499500\t999\n+499500\t499500\t999\ndiff --git a/tests/queries/0_stateless/01375_compact_parts_codecs.sql b/tests/queries/0_stateless/01375_compact_parts_codecs.sql\nnew file mode 100644\nindex 000000000000..698f4148a151\n--- /dev/null\n+++ b/tests/queries/0_stateless/01375_compact_parts_codecs.sql\n@@ -0,0 +1,52 @@\n+DROP TABLE IF EXISTS codecs;\n+\n+CREATE TABLE codecs (id UInt32, val UInt32, s String) \n+    ENGINE = MergeTree ORDER BY id\n+    SETTINGS min_rows_for_wide_part = 10000;\n+INSERT INTO codecs SELECT number, number, toString(number) FROM numbers(1000);\n+SELECT sum(data_compressed_bytes), sum(data_uncompressed_bytes) \n+    FROM system.parts \n+    WHERE table = 'codecs' AND database = currentDatabase();\n+\n+SELECT sum(id), sum(val), max(s) FROM codecs;\n+\n+DETACH TABLE codecs;\n+ATTACH table codecs;\n+\n+SELECT sum(id), sum(val), max(s) FROM codecs;\n+\n+DROP TABLE codecs;\n+\n+CREATE TABLE codecs (id UInt32 CODEC(NONE), val UInt32 CODEC(NONE), s String CODEC(NONE)) \n+    ENGINE = MergeTree ORDER BY id\n+    SETTINGS min_rows_for_wide_part = 10000;\n+INSERT INTO codecs SELECT number, number, toString(number) FROM numbers(1000);\n+SELECT sum(data_compressed_bytes), sum(data_uncompressed_bytes) \n+    FROM system.parts \n+    WHERE table = 'codecs' AND database = currentDatabase();\n+\n+SELECT sum(id), sum(val), max(s) FROM codecs;\n+\n+DETACH TABLE codecs;\n+ATTACH table codecs;\n+\n+SELECT sum(id), sum(val), max(s) FROM codecs;\n+\n+DROP TABLE codecs;\n+\n+CREATE TABLE codecs (id UInt32, val UInt32 CODEC(Delta, ZSTD), s String CODEC(ZSTD)) \n+    ENGINE = MergeTree ORDER BY id\n+    SETTINGS min_rows_for_wide_part = 10000;\n+INSERT INTO codecs SELECT number, number, toString(number) FROM numbers(1000);\n+SELECT sum(data_compressed_bytes), sum(data_uncompressed_bytes) \n+    FROM system.parts \n+    WHERE table = 'codecs' AND database = currentDatabase();\n+\n+SELECT sum(id), sum(val), max(s) FROM codecs;\n+\n+DETACH TABLE codecs;\n+ATTACH table codecs;\n+\n+SELECT sum(id), sum(val), max(s) FROM codecs;\n+\n+DROP TABLE codecs;\ndiff --git a/tests/queries/0_stateless/01390_check_table_codec.reference b/tests/queries/0_stateless/01390_check_table_codec.reference\nnew file mode 100644\nindex 000000000000..3025e6463d81\n--- /dev/null\n+++ b/tests/queries/0_stateless/01390_check_table_codec.reference\n@@ -0,0 +1,2 @@\n+all_1_1_0\t1\t\n+all_1_1_0\t1\t\ndiff --git a/tests/queries/0_stateless/01390_check_table_codec.sql b/tests/queries/0_stateless/01390_check_table_codec.sql\nnew file mode 100644\nindex 000000000000..639d5bea6e4d\n--- /dev/null\n+++ b/tests/queries/0_stateless/01390_check_table_codec.sql\n@@ -0,0 +1,15 @@\n+SET check_query_single_value_result = 0;\n+\n+DROP TABLE IF EXISTS check_codec;\n+\n+CREATE TABLE check_codec(a Int, b Int CODEC(Delta, ZSTD)) ENGINE = MergeTree ORDER BY a SETTINGS min_bytes_for_wide_part = 0;\n+INSERT INTO check_codec SELECT number, number * 2 FROM numbers(1000);\n+CHECK TABLE check_codec;\n+\n+DROP TABLE check_codec;\n+\n+CREATE TABLE check_codec(a Int, b Int CODEC(Delta, ZSTD)) ENGINE = MergeTree ORDER BY a SETTINGS min_bytes_for_wide_part = '10M';\n+INSERT INTO check_codec SELECT number, number * 2 FROM numbers(1000);\n+CHECK TABLE check_codec;\n+\n+DROP TABLE check_codec;\n",
  "problem_statement": "Support columns' special codecs in compact parts\nProbably it should be controlled by setting, beacuse of performace purposes.\r\n\r\nSee discussion: https://github.com/ClickHouse/ClickHouse/pull/11913#issuecomment-653544593\n",
  "hints_text": "",
  "created_at": "2020-07-07T00:18:20Z"
}