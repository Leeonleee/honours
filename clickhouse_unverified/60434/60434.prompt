You will be provided with a partial code base and an issue statement explaining a problem to resolve.

<issue>
Setting max_parser_depth too high can cause Segmentation fault
**Describe what's wrong**

You can use [max_parser_depth](https://clickhouse.com/docs/en/operations/settings/settings#max_parser_depth) to control the stack size. It's default value is 1000. 
User can increase it if there are really large queries that exceed this limit.

But setting it too high (in my example below to 100,000) can crash the server with Segmentation fault. 

**Does it reproduce on the most recent release?**
Yes, I tested this on `24.2.1.354`


**How to reproduce**

1). Install CH binary. 
2). Use clickhouse-client to run following query: 

> ./clickhouse client -q "select 'create table test (x ' || repeat('Array(', 10000) || 'UInt64' || repeat(')', 10000) || ') engine=Memory' format TSVRaw" | curl 'http://localhost:8123?max_parser_depth=100000' --data-binary @-

3). It will crash with below info: 

```
2024.02.06 06:02:48.952240 [ 2185 ] {} <Trace> DynamicQueryHandler: Request URI: /?max_parser_depth=100000
2024.02.06 06:02:48.952547 [ 2185 ] {} <Trace> HTTP-Session: efee4087-7f45-4101-8997-8127f5ad46ad Creating query context from session context, user_id: 94309d50-4f52-5250-31bd-74fecac179db, parent context user: default
2024.02.06 06:02:48.971146 [ 2183 ] {} <Trace> BaseDaemon: Received signal 11
2024.02.06 06:02:48.971400 [ 2832 ] {} <Fatal> BaseDaemon: ########## Short fault info ############
2024.02.06 06:02:48.971444 [ 2832 ] {} <Fatal> BaseDaemon: (version 24.2.1.354 (official build), build id: EC9C6D940F82DA4246E3BA422F5253E4A410D8DD, git hash: d8555af44836d6ecf4ce2d9bae04b1e7d7ae8f6b) (from thread 2185) Received signal 11
2024.02.06 06:02:48.971578 [ 2832 ] {} <Fatal> BaseDaemon: Signal description: Segmentation fault
2024.02.06 06:02:48.971651 [ 2832 ] {} <Fatal> BaseDaemon: Address: 0x7f0fb71feff8. Access: write. Attempted access has violated the permissions assigned to the memory area.
2024.02.06 06:02:48.971678 [ 2832 ] {} <Fatal> BaseDaemon: Stack trace: 0x00000000131d5d09
2024.02.06 06:02:48.971743 [ 2832 ] {} <Fatal> BaseDaemon: ########################################
2024.02.06 06:02:48.971763 [ 2832 ] {} <Fatal> BaseDaemon: (version 24.2.1.354 (official build), build id: EC9C6D940F82DA4246E3BA422F5253E4A410D8DD, git hash: d8555af44836d6ecf4ce2d9bae04b1e7d7ae8f6b) (from thread 2185) (query_id: fbddfcf2-28db-4868-afc6-7a83e888911d) (query: ) Received signal Segmentation fault (11)
2024.02.06 06:02:48.971782 [ 2832 ] {} <Fatal> BaseDaemon: Address: 0x7f0fb71feff8. Access: write. Attempted access has violated the permissions assigned to the memory area.
2024.02.06 06:02:48.971796 [ 2832 ] {} <Fatal> BaseDaemon: Stack trace: 0x00000000131d5d09
2024.02.06 06:02:48.971873 [ 2832 ] {} <Fatal> BaseDaemon: 2. DB::ParserIdentifier::parseImpl(DB::IParser::Pos&, std::shared_ptr<DB::IAST>&, DB::Expected&) @ 0x00000000131d5d09 in /home/ubuntu/clickhouse
2024.02.06 06:02:49.000662 [ 2815 ] {} <Trace> AsynchronousMetrics: MemoryTracking: was 423.95 MiB, peak 423.95 MiB, free memory in arenas 11.51 MiB, will set to 431.38 MiB (RSS), difference: 7.43 MiB
2024.02.06 06:02:49.147894 [ 2832 ] {} <Fatal> BaseDaemon: Integrity check of the executable successfully passed (checksum: 6FD11E78D45680053EF89061AE811EC8)
2024.02.06 06:02:49.147954 [ 2832 ] {} <Information> SentryWriter: Not sending crash report
2024.02.06 06:02:49.147979 [ 2832 ] {} <Fatal> BaseDaemon: Report this error to https://github.com/ClickHouse/ClickHouse/issues
2024.02.06 06:02:49.148107 [ 2832 ] {} <Fatal> BaseDaemon: Changed settings: max_parser_depth = 100000
2024.02.06 06:02:50.162727 [ 2209 ] {} <Debug> DNSResolver: Updating DNS cache
2024.02.06 06:02:50.162809 [ 2209 ] {} <Debug> DNSResolver: Updated DNS cache
Segmentation fault (core dumped)
```

Potential workaround: 
Have a check in [checkStackSize](https://github.com/ClickHouse/ClickHouse/blob/ea6f90b4f2c3cc2f7d8b846c769b7f3e84907e47/src/Common/checkStackSize.cpp) to avoid this problem.
</issue>

I need you to solve the provided issue by generating a code fix that can be applied directly to the repository

Respond below:
