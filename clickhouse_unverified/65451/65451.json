{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 65451,
  "instance_id": "ClickHouse__ClickHouse-65451",
  "issue_numbers": [
    "65426"
  ],
  "base_commit": "180ada06224bb3d7e76e8ffe976dbe81ebe387e5",
  "patch": "diff --git a/src/AggregateFunctions/AggregateFunctionGroupConcat.cpp b/src/AggregateFunctions/AggregateFunctionGroupConcat.cpp\nnew file mode 100644\nindex 000000000000..1c059dc52aa9\n--- /dev/null\n+++ b/src/AggregateFunctions/AggregateFunctionGroupConcat.cpp\n@@ -0,0 +1,283 @@\n+#include <AggregateFunctions/IAggregateFunction.h>\n+#include <AggregateFunctions/AggregateFunctionFactory.h>\n+#include <AggregateFunctions/FactoryHelpers.h>\n+\n+#include <Columns/IColumn.h>\n+#include <Columns/ColumnNullable.h>\n+#include <Columns/ColumnString.h>\n+\n+#include <Core/ServerSettings.h>\n+#include <Core/ColumnWithTypeAndName.h>\n+\n+#include <Common/ArenaAllocator.h>\n+#include <Common/assert_cast.h>\n+#include <Interpreters/castColumn.h>\n+\n+#include <DataTypes/IDataType.h>\n+#include <DataTypes/DataTypeArray.h>\n+#include <DataTypes/DataTypeString.h>\n+#include <DataTypes/DataTypesNumber.h>\n+\n+#include <IO/ReadHelpers.h>\n+#include <IO/WriteHelpers.h>\n+\n+\n+namespace DB\n+{\n+struct Settings;\n+\n+namespace ErrorCodes\n+{\n+    extern const int TOO_MANY_ARGUMENTS_FOR_FUNCTION;\n+    extern const int ILLEGAL_TYPE_OF_ARGUMENT;\n+    extern const int BAD_ARGUMENTS;\n+}\n+\n+namespace\n+{\n+\n+struct GroupConcatDataBase\n+{\n+    UInt64 data_size = 0;\n+    UInt64 allocated_size = 0;\n+    char * data = nullptr;\n+\n+    void checkAndUpdateSize(UInt64 add, Arena * arena)\n+    {\n+        if (data_size + add >= allocated_size)\n+        {\n+            auto old_size = allocated_size;\n+            allocated_size = std::max(2 * allocated_size, data_size + add);\n+            data = arena->realloc(data, old_size, allocated_size);\n+        }\n+    }\n+\n+    void insertChar(const char * str, UInt64 str_size, Arena * arena)\n+    {\n+        checkAndUpdateSize(str_size, arena);\n+        memcpy(data + data_size, str, str_size);\n+        data_size += str_size;\n+    }\n+\n+    void insert(const IColumn * column, const SerializationPtr & serialization, size_t row_num, Arena * arena)\n+    {\n+        WriteBufferFromOwnString buff;\n+        serialization->serializeText(*column, row_num, buff, FormatSettings{});\n+        auto string = buff.stringView();\n+        insertChar(string.data(), string.size(), arena);\n+    }\n+\n+};\n+\n+template <bool has_limit>\n+struct GroupConcatData;\n+\n+template<>\n+struct GroupConcatData<false> final : public GroupConcatDataBase\n+{\n+};\n+\n+template<>\n+struct GroupConcatData<true> final : public GroupConcatDataBase\n+{\n+    using Offset = UInt64;\n+    using Allocator = MixedAlignedArenaAllocator<alignof(Offset), 4096>;\n+    using Offsets = PODArray<Offset, 32, Allocator>;\n+\n+    /// offset[i * 2] - beginning of the i-th row, offset[i * 2 + 1] - end of the i-th row\n+    Offsets offsets;\n+    UInt64 num_rows = 0;\n+\n+    UInt64 getSize(size_t i) const { return offsets[i * 2 + 1] - offsets[i * 2]; }\n+\n+    UInt64 getString(size_t i) const { return offsets[i * 2]; }\n+\n+    void insert(const IColumn * column, const SerializationPtr & serialization, size_t row_num, Arena * arena)\n+    {\n+        WriteBufferFromOwnString buff;\n+        serialization->serializeText(*column, row_num, buff, {});\n+        auto string = buff.stringView();\n+\n+        checkAndUpdateSize(string.size(), arena);\n+        memcpy(data + data_size, string.data(), string.size());\n+        offsets.push_back(data_size, arena);\n+        data_size += string.size();\n+        offsets.push_back(data_size, arena);\n+        num_rows++;\n+    }\n+};\n+\n+template <bool has_limit>\n+class GroupConcatImpl final\n+    : public IAggregateFunctionDataHelper<GroupConcatData<has_limit>, GroupConcatImpl<has_limit>>\n+{\n+    static constexpr auto name = \"groupConcat\";\n+\n+    SerializationPtr serialization;\n+    UInt64 limit;\n+    const String delimiter;\n+\n+public:\n+    GroupConcatImpl(const DataTypePtr & data_type_, const Array & parameters_, UInt64 limit_, const String & delimiter_)\n+        : IAggregateFunctionDataHelper<GroupConcatData<has_limit>, GroupConcatImpl<has_limit>>(\n+            {data_type_}, parameters_, std::make_shared<DataTypeString>())\n+        , serialization(this->argument_types[0]->getDefaultSerialization())\n+        , limit(limit_)\n+        , delimiter(delimiter_)\n+    {\n+    }\n+\n+    String getName() const override { return name; }\n+\n+    void add(AggregateDataPtr __restrict place, const IColumn ** columns, size_t row_num, Arena * arena) const override\n+    {\n+        auto & cur_data = this->data(place);\n+\n+        if constexpr (has_limit)\n+            if (cur_data.num_rows >= limit)\n+                return;\n+\n+        if (cur_data.data_size != 0)\n+            cur_data.insertChar(delimiter.c_str(), delimiter.size(), arena);\n+\n+        cur_data.insert(columns[0], serialization, row_num, arena);\n+    }\n+\n+    void merge(AggregateDataPtr __restrict place, ConstAggregateDataPtr rhs, Arena * arena) const override\n+    {\n+        auto & cur_data = this->data(place);\n+        auto & rhs_data = this->data(rhs);\n+\n+        if (rhs_data.data_size == 0)\n+            return;\n+\n+        if constexpr (has_limit)\n+        {\n+            UInt64 new_elems_count = std::min(rhs_data.num_rows, limit - cur_data.num_rows);\n+            for (UInt64 i = 0; i < new_elems_count; ++i)\n+            {\n+                if (cur_data.data_size != 0)\n+                    cur_data.insertChar(delimiter.c_str(), delimiter.size(), arena);\n+\n+                cur_data.offsets.push_back(cur_data.data_size, arena);\n+                cur_data.insertChar(rhs_data.data + rhs_data.getString(i), rhs_data.getSize(i), arena);\n+                cur_data.num_rows++;\n+                cur_data.offsets.push_back(cur_data.data_size, arena);\n+            }\n+        }\n+        else\n+        {\n+            if (cur_data.data_size != 0)\n+                cur_data.insertChar(delimiter.c_str(), delimiter.size(), arena);\n+\n+            cur_data.insertChar(rhs_data.data, rhs_data.data_size, arena);\n+        }\n+    }\n+\n+    void serialize(ConstAggregateDataPtr __restrict place, WriteBuffer & buf, std::optional<size_t> /* version */) const override\n+    {\n+        auto & cur_data = this->data(place);\n+\n+        writeVarUInt(cur_data.data_size, buf);\n+\n+        buf.write(cur_data.data, cur_data.data_size);\n+\n+        if constexpr (has_limit)\n+        {\n+            writeVarUInt(cur_data.num_rows, buf);\n+            for (const auto & offset : cur_data.offsets)\n+                writeVarUInt(offset, buf);\n+        }\n+    }\n+\n+    void deserialize(AggregateDataPtr __restrict place, ReadBuffer & buf, std::optional<size_t> /* version */, Arena * arena) const override\n+    {\n+        auto & cur_data = this->data(place);\n+\n+        UInt64 temp_size = 0;\n+        readVarUInt(temp_size, buf);\n+\n+        cur_data.checkAndUpdateSize(temp_size, arena);\n+\n+        buf.readStrict(cur_data.data + cur_data.data_size, temp_size);\n+        cur_data.data_size = temp_size;\n+\n+        if constexpr (has_limit)\n+        {\n+            readVarUInt(cur_data.num_rows, buf);\n+            cur_data.offsets.resize_exact(cur_data.num_rows * 2, arena);\n+            for (auto & offset : cur_data.offsets)\n+                readVarUInt(offset, buf);\n+        }\n+    }\n+\n+    void insertResultInto(AggregateDataPtr __restrict place, IColumn & to, Arena *) const override\n+    {\n+        auto & cur_data = this->data(place);\n+\n+        if (cur_data.data_size == 0)\n+        {\n+            to.insertDefault();\n+            return;\n+        }\n+\n+        auto & column_string = assert_cast<ColumnString &>(to);\n+        column_string.insertData(cur_data.data, cur_data.data_size);\n+    }\n+\n+    bool allocatesMemoryInArena() const override { return true; }\n+};\n+\n+AggregateFunctionPtr createAggregateFunctionGroupConcat(\n+    const std::string & name, const DataTypes & argument_types, const Array & parameters, const Settings *)\n+{\n+    assertUnary(name, argument_types);\n+\n+    bool has_limit = false;\n+    UInt64 limit = 0;\n+    String delimiter;\n+\n+    if (parameters.size() > 2)\n+        throw Exception(ErrorCodes::TOO_MANY_ARGUMENTS_FOR_FUNCTION,\n+            \"Incorrect number of parameters for aggregate function {}, should be 0, 1 or 2, got: {}\", name, parameters.size());\n+\n+    if (!parameters.empty())\n+    {\n+        auto type = parameters[0].getType();\n+        if (type != Field::Types::String)\n+            throw Exception(ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT, \"First parameter for aggregate function {} should be string\", name);\n+\n+        delimiter = parameters[0].get<String>();\n+    }\n+    if (parameters.size() == 2)\n+    {\n+        auto type = parameters[1].getType();\n+\n+        if (type != Field::Types::Int64 && type != Field::Types::UInt64)\n+            throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Second parameter for aggregate function {} should be a positive number\", name);\n+\n+        if ((type == Field::Types::Int64 && parameters[1].get<Int64>() <= 0) ||\n+            (type == Field::Types::UInt64 && parameters[1].get<UInt64>() == 0))\n+            throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Second parameter for aggregate function {} should be a positive number, got: {}\", name, parameters[1].get<Int64>());\n+\n+        has_limit = true;\n+        limit = parameters[1].get<UInt64>();\n+    }\n+\n+    if (has_limit)\n+        return std::make_shared<GroupConcatImpl</* has_limit= */ true>>(argument_types[0], parameters, limit, delimiter);\n+    else\n+        return std::make_shared<GroupConcatImpl</* has_limit= */ false>>(argument_types[0], parameters, limit, delimiter);\n+}\n+\n+}\n+\n+void registerAggregateFunctionGroupConcat(AggregateFunctionFactory & factory)\n+{\n+    AggregateFunctionProperties properties = { .returns_default_when_only_null = false, .is_order_dependent = true };\n+\n+    factory.registerFunction(\"groupConcat\", { createAggregateFunctionGroupConcat, properties });\n+    factory.registerAlias(\"group_concat\", \"groupConcat\", AggregateFunctionFactory::CaseInsensitive);\n+}\n+\n+}\ndiff --git a/src/AggregateFunctions/registerAggregateFunctions.cpp b/src/AggregateFunctions/registerAggregateFunctions.cpp\nindex 58e657d37233..4ac25e14ee64 100644\n--- a/src/AggregateFunctions/registerAggregateFunctions.cpp\n+++ b/src/AggregateFunctions/registerAggregateFunctions.cpp\n@@ -19,6 +19,7 @@ void registerAggregateFunctionGroupArraySorted(AggregateFunctionFactory & factor\n void registerAggregateFunctionGroupUniqArray(AggregateFunctionFactory &);\n void registerAggregateFunctionGroupArrayInsertAt(AggregateFunctionFactory &);\n void registerAggregateFunctionGroupArrayIntersect(AggregateFunctionFactory &);\n+void registerAggregateFunctionGroupConcat(AggregateFunctionFactory &);\n void registerAggregateFunctionsQuantile(AggregateFunctionFactory &);\n void registerAggregateFunctionsQuantileDeterministic(AggregateFunctionFactory &);\n void registerAggregateFunctionsQuantileExact(AggregateFunctionFactory &);\n@@ -120,6 +121,7 @@ void registerAggregateFunctions()\n         registerAggregateFunctionGroupUniqArray(factory);\n         registerAggregateFunctionGroupArrayInsertAt(factory);\n         registerAggregateFunctionGroupArrayIntersect(factory);\n+        registerAggregateFunctionGroupConcat(factory);\n         registerAggregateFunctionsQuantile(factory);\n         registerAggregateFunctionsQuantileDeterministic(factory);\n         registerAggregateFunctionsQuantileExact(factory);\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/03156_group_concat.reference b/tests/queries/0_stateless/03156_group_concat.reference\nnew file mode 100644\nindex 000000000000..c1ab35e96c0e\n--- /dev/null\n+++ b/tests/queries/0_stateless/03156_group_concat.reference\n@@ -0,0 +1,19 @@\n+0\t95\tabc\t[1,2,3]\n+1\t\\N\ta\t[993,986,979,972]\n+2\t123\tmakson95\t[]\n+95123\n+abcamakson95\n+[1,2,3][993,986,979,972][]\n+[1,2,3]\n+abcamakson95\n+95123\n+95\\n123\n+95,123\n+abc,a,makson95\n+[1,2,3],[993,986,979,972]\n+\\N\n+951239512395123\n+abc,a,makson95,abc,a,makson95,abc,a,makson95\n+[1,2,3][993,986,979,972][][1,2,3][993,986,979,972][][1,2,3][993,986,979,972][]\n+488890\n+488890\ndiff --git a/tests/queries/0_stateless/03156_group_concat.sql b/tests/queries/0_stateless/03156_group_concat.sql\nnew file mode 100644\nindex 000000000000..0d561c69f0af\n--- /dev/null\n+++ b/tests/queries/0_stateless/03156_group_concat.sql\n@@ -0,0 +1,57 @@\n+DROP TABLE IF EXISTS test_groupConcat;\n+CREATE TABLE test_groupConcat\n+(\n+    id UInt64,\n+    p_int Int32 NULL,\n+    p_string String,\n+    p_array Array(Int32)\n+) ENGINE = MergeTree ORDER BY id;\n+\n+SET max_insert_threads = 1, max_threads = 1, min_insert_block_size_rows = 0, min_insert_block_size_bytes = 0;\n+INSERT INTO test_groupConcat VALUES (0, 95, 'abc', [1, 2, 3]), (1, NULL, 'a', [993, 986, 979, 972]), (2, 123, 'makson95', []);\n+\n+SELECT * FROM test_groupConcat;\n+\n+SELECT groupConcat(p_int) FROM test_groupConcat;\n+SELECT groupConcat(p_string) FROM test_groupConcat;\n+SELECT groupConcat(p_array) FROM test_groupConcat;\n+\n+SELECT groupConcat('', 1)(p_array) FROM test_groupConcat;\n+SELECT groupConcat('', 3)(p_string) FROM test_groupConcat;\n+SELECT groupConcat('', 2)(p_int) FROM test_groupConcat;\n+SELECT groupConcat('\\n', 3)(p_int) FROM test_groupConcat;\n+\n+SELECT groupConcat(',')(p_int) FROM test_groupConcat;\n+SELECT groupConcat(',')(p_string) FROM test_groupConcat;\n+SELECT groupConcat(',', 2)(p_array) FROM test_groupConcat;\n+\n+SELECT groupConcat(p_int) FROM test_groupConcat WHERE id = 1;\n+\n+INSERT INTO test_groupConcat VALUES (0, 95, 'abc', [1, 2, 3]), (1, NULL, 'a', [993, 986, 979, 972]), (2, 123, 'makson95', []);\n+INSERT INTO test_groupConcat VALUES (0, 95, 'abc', [1, 2, 3]), (1, NULL, 'a', [993, 986, 979, 972]), (2, 123, 'makson95', []);\n+\n+SELECT groupConcat(p_int) FROM test_groupConcat;\n+SELECT groupConcat(',')(p_string) FROM test_groupConcat;\n+SELECT groupConcat(p_array) FROM test_groupConcat;\n+\n+SELECT groupConcat(123)(number) FROM numbers(10); -- { serverError ILLEGAL_TYPE_OF_ARGUMENT }\n+SELECT groupConcat(',', '3')(number) FROM numbers(10); -- { serverError BAD_ARGUMENTS }\n+SELECT groupConcat(',', 0)(number) FROM numbers(10); -- { serverError BAD_ARGUMENTS }\n+SELECT groupConcat(',', -1)(number) FROM numbers(10); -- { serverError BAD_ARGUMENTS }\n+SELECT groupConcat(',', 3, 3)(number) FROM numbers(10); -- { serverError TOO_MANY_ARGUMENTS_FOR_FUNCTION }\n+\n+SELECT length(groupConcat(number)) FROM numbers(100000);\n+\n+DROP TABLE IF EXISTS test_groupConcat;\n+\n+CREATE TABLE test_groupConcat\n+(\n+    id UInt64,\n+    p_int Int32,\n+) ENGINE = MergeTree ORDER BY id;\n+\n+INSERT INTO test_groupConcat SELECT number, number FROM numbers(100000) SETTINGS min_insert_block_size_rows = 2000;\n+\n+SELECT length(groupConcat(p_int)) FROM test_groupConcat;\n+\n+DROP TABLE IF EXISTS test_groupConcat;\ndiff --git a/tests/queries/0_stateless/03195_group_concat_deserialization_fix.reference b/tests/queries/0_stateless/03195_group_concat_deserialization_fix.reference\nnew file mode 100644\nindex 000000000000..1696fc465548\n--- /dev/null\n+++ b/tests/queries/0_stateless/03195_group_concat_deserialization_fix.reference\n@@ -0,0 +1,3 @@\n+First\n+First\n+Second\ndiff --git a/tests/queries/0_stateless/03195_group_concat_deserialization_fix.sql b/tests/queries/0_stateless/03195_group_concat_deserialization_fix.sql\nnew file mode 100644\nindex 000000000000..337f1f3db241\n--- /dev/null\n+++ b/tests/queries/0_stateless/03195_group_concat_deserialization_fix.sql\n@@ -0,0 +1,23 @@\n+DROP TABLE IF EXISTS test_serialization;\n+\n+CREATE TABLE test_serialization\n+(\n+    id UInt64,\n+    text AggregateFunction(groupConcat, String)\n+) ENGINE = AggregatingMergeTree() ORDER BY id;\n+\n+INSERT INTO test_serialization SELECT\n+    1,\n+    groupConcatState('First');\n+\n+SELECT groupConcatMerge(text) AS concatenated_text FROM test_serialization GROUP BY id;\n+\n+INSERT INTO test_serialization SELECT\n+    2,\n+    groupConcatState('Second');\n+\n+SELECT groupConcatMerge(text) AS concatenated_text FROM test_serialization GROUP BY id ORDER BY id;\n+\n+DROP TABLE IF EXISTS test_serialization;\n+\n+\n",
  "problem_statement": "Segfault in `AggregateFunctionGroupConcat.cpp:193`\nhttps://s3.amazonaws.com/clickhouse-test-reports/65239/ece3efe09cc584f3ac0320e83e3829684153c1d9/stress_test__debug_.html\r\n\r\nquery: `SELECT groupConcat(p_int) FROM test_groupConcat`\r\n```\r\n2024.06.18 21:15:50.140805 [ 15147 ] {} <Fatal> BaseDaemon: ########## Short fault info ############\r\n2024.06.18 21:15:50.145900 [ 15147 ] {} <Fatal> BaseDaemon: (version 24.6.1.4137, build id: D160E7D4D9D03A27F7E62BAE039D3F3ADC9E0467, git hash: 3d440d5d6b60fdd47b31bad277c9aeb8d25e2ff9) (from thread 13005) Received signal 11\r\n2024.06.18 21:15:50.147155 [ 15147 ] {} <Fatal> BaseDaemon: Signal description: Segmentation fault\r\n2024.06.18 21:15:50.148160 [ 15147 ] {} <Fatal> BaseDaemon: Address: 0x1. Access: write. Address not mapped to object.\r\n2024.06.18 21:15:50.149914 [ 15147 ] {} <Fatal> BaseDaemon: Stack trace: 0x00000000130e374a 0x00000000130e3640 0x000000001350a831 0x00007fd64afc0520 0x00000000268ed634 0x00000000268ed545 0x000000000b14f9ac 0x0000000013159c68 0x0000000013cfdbe5 0x0000000015e08ae1 0x000000001a3f9e9a 0x000000001a3ea8ba 0x000000001d912296 0x000000001d9131a0 0x000000001d711efe 0x000000001d711a28 0x000000001d711371 0x000000001d779958 0x000000001a2d2a91 0x000000001a2d15c9 0x000000001a2d14d8 0x000000001a2d1468 0x000000001a2d1428 0x000000001a2d135e 0x000000001a2d107d\r\n2024.06.18 21:15:50.152028 [ 15147 ] {} <Fatal> BaseDaemon: ########################################\r\n2024.06.18 21:15:50.161079 [ 15147 ] {} <Fatal> BaseDaemon: (version 24.6.1.4137, build id: D160E7D4D9D03A27F7E62BAE039D3F3ADC9E0467, git hash: 3d440d5d6b60fdd47b31bad277c9aeb8d25e2ff9) (from thread 13005) (query_id: 708401fe-5db0-43e9-9801-862371b114b2) (query: SELECT groupConcat(p_int) FROM test_groupConcat;) Received signal Segmentation fault (11)\r\n2024.06.18 21:15:50.175099 [ 15147 ] {} <Fatal> BaseDaemon: Address: 0x1. Access: write. Address not mapped to object.\r\n2024.06.18 21:15:50.196934 [ 15147 ] {} <Fatal> BaseDaemon: Stack trace: 0x00000000130e374a 0x00000000130e3640 0x000000001350a831 0x00007fd64afc0520 0x00000000268ed634 0x00000000268ed545 0x000000000b14f9ac 0x0000000013159c68 0x0000000013cfdbe5 0x0000000015e08ae1 0x000000001a3f9e9a 0x000000001a3ea8ba 0x000000001d912296 0x000000001d9131a0 0x000000001d711efe 0x000000001d711a28 0x000000001d711371 0x000000001d779958 0x000000001a2d2a91 0x000000001a2d15c9 0x000000001a2d14d8 0x000000001a2d1468 0x000000001a2d1428 0x000000001a2d135e 0x000000001a2d107d\r\n2024.06.18 21:15:50.374388 [ 15147 ] {} <Fatal> BaseDaemon: 0. /build/src/Common/StackTrace.cpp:349: StackTrace::tryCapture() @ 0x00000000130e374a\r\n2024.06.18 21:15:50.680279 [ 15147 ] {} <Fatal> BaseDaemon: 1. /build/src/Common/StackTrace.cpp:323: StackTrace::StackTrace(ucontext_t const&) @ 0x00000000130e3640\r\n2024.06.18 21:15:51.137841 [ 15147 ] {} <Fatal> BaseDaemon: 2. /build/src/Daemon/BaseDaemon.cpp:158: signalHandler(int, siginfo_t*, void*) @ 0x000000001350a831\r\n2024.06.18 21:15:51.145260 [ 15147 ] {} <Fatal> BaseDaemon: 3. ? @ 0x00007fd64afc0520\r\n2024.06.18 21:15:51.233016 [ 15147 ] {} <Fatal> BaseDaemon: 4. /build/base/glibc-compatibility/memcpy/memcpy.h:127: inline_memcpy(void*, void const*, unsigned long) @ 0x00000000268ed634\r\n2024.06.18 21:15:51.259755 [ 15147 ] {} <Fatal> BaseDaemon: 5. /build/base/glibc-compatibility/memcpy/memcpy.cpp:6: memcpy @ 0x00000000268ed545\r\n2024.06.18 21:15:52.570077 [ 15147 ] {} <Fatal> BaseDaemon: 6. /build/src/IO/ReadBuffer.h:186: DB::ReadBuffer::read(char*, unsigned long) @ 0x000000000b14f9ac\r\n2024.06.18 21:15:52.680891 [ 15147 ] {} <Fatal> BaseDaemon: 7. /build/src/IO/ReadBuffer.h:197: DB::ReadBuffer::readStrict(char*, unsigned long) @ 0x0000000013159c68\r\n2024.06.18 21:15:53.101062 [ 15147 ] {} <Fatal> BaseDaemon: 8. /build/src/AggregateFunctions/AggregateFunctionGroupConcat.cpp:193: DB::(anonymous namespace)::GroupConcatImpl<false>::deserialize(char*, DB::ReadBuffer&, std::optional<unsigned long>, DB::Arena*) const @ 0x0000000013cfdbe5\r\n2024.06.18 21:15:53.887451 [ 15147 ] {} <Fatal> BaseDaemon: 9. /build/src/AggregateFunctions/Combinators/AggregateFunctionNull.h:190: DB::AggregateFunctionNullBase<true, true, DB::AggregateFunctionNullUnary<true, true>>::deserialize(char*, DB::ReadBuffer&, std::optional<unsigned long>, DB::Arena*) const @ 0x0000000015e08ae1\r\n2024.06.18 21:15:53.976340 [ 15147 ] {} <Fatal> BaseDaemon: 10. /build/src/DataTypes/Serializations/SerializationAggregateFunction.cpp:94: DB::SerializationAggregateFunction::deserializeBinaryBulk(DB::IColumn&, DB::ReadBuffer&, unsigned long, double) const @ 0x000000001a3f9e9a\r\n2024.06.18 21:15:54.088859 [ 15147 ] {} <Fatal> BaseDaemon: 11. /build/src/DataTypes/Serializations/ISerialization.cpp:145: DB::ISerialization::deserializeBinaryBulkWithMultipleStreams(COW<DB::IColumn>::immutable_ptr<DB::IColumn>&, unsigned long, DB::ISerialization::DeserializeBinaryBulkSettings&, std::shared_ptr<DB::ISerialization::DeserializeBinaryBulkState>&, std::unordered_map<String, COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, COW<DB::IColumn>::immutable_ptr<DB::IColumn>>>>*) const @ 0x000000001a3ea8ba\r\n2024.06.18 21:15:54.248546 [ 15147 ] {} <Fatal> BaseDaemon: 12. /build/src/Formats/NativeReader.cpp:97: DB::NativeReader::readData(DB::ISerialization const&, COW<DB::IColumn>::immutable_ptr<DB::IColumn>&, DB::ReadBuffer&, unsigned long, double) @ 0x000000001d912296\r\n2024.06.18 21:15:54.409934 [ 15147 ] {} <Fatal> BaseDaemon: 13. /build/src/Formats/NativeReader.cpp:206: DB::NativeReader::read() @ 0x000000001d9131a0\r\n2024.06.18 21:15:55.325776 [ 15147 ] {} <Fatal> BaseDaemon: 14. /build/src/Client/Connection.cpp:1175: DB::Connection::receiveDataImpl(DB::NativeReader&) @ 0x000000001d711efe\r\n2024.06.18 21:15:56.370338 [ 15147 ] {} <Fatal> BaseDaemon: 15. /build/src/Client/Connection.cpp:1156: DB::Connection::receiveData() @ 0x000000001d711a28\r\n2024.06.18 21:15:57.249517 [ 15147 ] {} <Fatal> BaseDaemon: 16. /build/src/Client/Connection.cpp:1082: DB::Connection::receivePacket() @ 0x000000001d711371\r\n2024.06.18 21:15:58.164920 [ 15147 ] {} <Fatal> BaseDaemon: 17. /build/src/Client/MultiplexedConnections.cpp:333: DB::MultiplexedConnections::receivePacketUnlocked(std::function<void (int, Poco::Timespan, DB::AsyncEventTimeoutType, String const&, unsigned int)>) @ 0x000000001d779958\r\n2024.06.18 21:15:58.451946 [ 15147 ] {} <Fatal> BaseDaemon: 18. /build/src/QueryPipeline/RemoteQueryExecutorReadContext.cpp:50: DB::RemoteQueryExecutorReadContext::Task::run(std::function<void (int, Poco::Timespan, DB::AsyncEventTimeoutType, String const&, unsigned int)>, std::function<void ()>) @ 0x000000001a2d2a91\r\n2024.06.18 21:15:58.604807 [ 15147 ] {} <Fatal> BaseDaemon: 19. /build/src/Common/AsyncTaskExecutor.cpp:88: DB::AsyncTaskExecutor::Routine::operator()(std::function<void ()>) @ 0x000000001a2d15c9\r\n2024.06.18 21:15:58.993834 [ 15147 ] {} <Fatal> BaseDaemon: 20. /build/src/Common/Fiber.h:76: Fiber::RoutineImpl<DB::AsyncTaskExecutor::Routine>::operator()(boost::context::fiber&&) @ 0x000000001a2d14d8\r\n2024.06.18 21:15:59.284764 [ 15147 ] {} <Fatal> BaseDaemon: 21. /build/contrib/llvm-project/libcxx/include/__functional/invoke.h:394: ? @ 0x000000001a2d1468\r\n2024.06.18 21:15:59.499838 [ 15147 ] {} <Fatal> BaseDaemon: 22. /build/contrib/llvm-project/libcxx/include/__functional/invoke.h:531: ? @ 0x000000001a2d1428\r\n2024.06.18 21:15:59.678279 [ 15147 ] {} <Fatal> BaseDaemon: 23. /build/contrib/boost/boost/context/fiber_fcontext.hpp:157: boost::context::detail::fiber_record<boost::context::fiber, FiberStack&, Fiber::RoutineImpl<DB::AsyncTaskExecutor::Routine>>::run(void*) @ 0x000000001a2d135e\r\n2024.06.18 21:15:59.972923 [ 15147 ] {} <Fatal> BaseDaemon: 24. /build/contrib/boost/boost/context/fiber_fcontext.hpp:97: void boost::context::detail::fiber_entry<boost::context::detail::fiber_record<boost::context::fiber, FiberStack&, Fiber::RoutineImpl<DB::AsyncTaskExecutor::Routine>>>(boost::context::detail::transfer_t) @ 0x000000001a2d107d\r\n2024.06.18 21:15:59.997560 [ 15147 ] {} <Fatal> BaseDaemon: Integrity check of the executable skipped because the reference checksum could not be read.\r\n2024.06.18 21:16:09.233320 [ 15147 ] {} <Fatal> BaseDaemon: This ClickHouse version is not official and should be upgraded to the official build.\r\n2024.06.18 21:16:09.257731 [ 15147 ] {} <Fatal> BaseDaemon: Changed settings: min_compress_block_size = 2477463, max_compress_block_size = 1387141, max_block_size = 30829, min_insert_block_size_rows = 0, min_insert_block_size_bytes = 0, min_external_table_block_size_bytes = 100000000, max_joined_block_size_rows = 14304, max_insert_threads = 1, max_threads = 1, max_parsing_threads = 10, max_read_buffer_size = 605364, connect_timeout_with_failover_ms = 2000, connect_timeout_with_failover_secure_ms = 3000, idle_connection_timeout = 36000, s3_check_objects_after_upload = true, stream_like_engine_allow_direct_select = true, replication_wait_for_inactive_replica_timeout = 30, min_count_to_compile_expression = 0, compile_sort_description = false, group_by_two_level_threshold = 1000000, group_by_two_level_threshold_bytes = 1, distributed_aggregation_memory_efficient = false, allow_nonconst_timezone_arguments = true, max_parallel_replicas = 3, cluster_for_parallel_replicas = 'parallel_replicas', allow_experimental_parallel_reading_from_replicas = 1, parallel_replicas_for_non_replicated_merge_tree = true, input_format_parallel_parsing = false, min_chunk_bytes_for_parallel_parsing = 10162012, merge_tree_coarse_index_granularity = 24, min_bytes_to_use_direct_io = 10737418240, min_bytes_to_use_mmap_io = 10648023850, log_queries = true, insert_quorum_timeout = 60000, table_function_remote_max_addresses = 200, merge_tree_read_split_ranges_into_intersecting_and_non_intersecting_injection_probability = 0.4000000059604645, http_response_buffer_size = 4850398, fsync_metadata = false, http_send_timeout = 60., http_receive_timeout = 60., opentelemetry_start_trace_probability = 0.10000000149011612, prefer_external_sort_block_bytes = 0, max_bytes_before_external_sort = 10737418240, max_bytes_before_remerge_sort = 243713462, max_execution_time = 60., cross_join_min_rows_to_compress = 0, cross_join_min_bytes_to_compress = 1, max_memory_usage = 10000000000, max_memory_usage_for_user = 20000532480, max_untracked_memory = 1048576, memory_profiler_step = 1048576, log_comment = '03156_group_concat.sql', send_logs_level = 'fatal', prefer_localhost_replica = false, aggregation_in_order_max_block_bytes = 33373741, read_in_order_two_level_merge_threshold = 43, allow_introspection_functions = true, database_atomic_wait_for_drop_and_detach_synchronously = true, optimize_or_like_chain = true, optimize_if_transform_strings_to_enum = true, optimize_append_index = true, distributed_ddl_entry_format_version = 6, local_filesystem_read_method = 'mmap', local_filesystem_read_prefetch = true, merge_tree_compact_parts_min_granules_to_multibuffer_read = 119, async_insert_busy_timeout_max_ms = 5000, enable_filesystem_cache = true, enable_filesystem_cache_on_write_operations = true, throw_on_error_from_cache_on_write_operations = true, filesystem_cache_segments_batch_size = 10, load_marks_asynchronously = true, allow_prefetched_read_pool_for_remote_filesystem = false, allow_prefetched_read_pool_for_local_filesystem = false, filesystem_prefetch_step_bytes = 104857600, filesystem_prefetch_min_bytes_for_single_read_task = 8388608, filesystem_prefetch_max_memory_usage = 33554432, filesystem_prefetches_limit = 0, allow_deprecated_database_ordinary = true, optimize_sorting_by_input_stream_properties = false, insert_keeper_max_retries = 100, insert_keeper_retry_initial_backoff_ms = 1, insert_keeper_retry_max_backoff_ms = 1, insert_keeper_fault_injection_probability = 0.009999999776482582, ignore_drop_queries_probability = 0.5, optimize_distinct_in_order = false, session_timezone = 'Africa/Khartoum', prefer_warmed_unmerged_parts_seconds = 2\r\n2024.06.18 21:16:13.351871 [ 3216 ] {} <Fatal> Application: Child process was terminated by signal 11.\r\n```\r\n\n",
  "hints_text": "Repro:\r\n\r\n```\r\nCREATE TABLE test_groupConcat\r\n(\r\n    `id` UInt64,\r\n    `p_int` Int32 NULL,\r\n    `p_string` String,\r\n    `p_array` Array(Int32)\r\n)\r\nENGINE = MergeTree\r\nORDER BY id;\r\n\r\n\r\nSET max_insert_threads = 1, max_threads = 1, min_insert_block_size_rows = 0, min_insert_block_size_bytes = 0;\r\n\r\n\r\nINSERT INTO test_groupConcat VALUES (0, 95, 'abc', [1, 2, 3]), (1, NULL, 'a', [993, 986, 979, 972]), (2, 123, 'makson95', []);\r\n\r\nSELECT *\r\nFROM test_groupConcat;\r\n\r\n/*\r\n\r\n   \u250c\u2500id\u2500\u252c\u2500p_int\u2500\u252c\u2500p_string\u2500\u252c\u2500p_array\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n1. \u2502  0 \u2502    95 \u2502 abc      \u2502 [1,2,3]           \u2502\r\n2. \u2502  1 \u2502  \u1d3a\u1d41\u1d38\u1d38 \u2502 a        \u2502 [993,986,979,972] \u2502\r\n3. \u2502  2 \u2502   123 \u2502 makson95 \u2502 []                \u2502\r\n   \u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n3 rows in set. Elapsed: 0.011 sec. \r\n\r\n*/\r\n\r\n\r\nSELECT groupConcat(p_int)\r\nFROM test_groupConcat;\r\n```",
  "created_at": "2024-06-19T11:59:10Z"
}