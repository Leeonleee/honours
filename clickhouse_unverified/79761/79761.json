{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 79761,
  "instance_id": "ClickHouse__ClickHouse-79761",
  "issue_numbers": [
    "79638"
  ],
  "base_commit": "90b1f878b8c3f41c2a21ed7298b042c30d2a7c8f",
  "patch": "diff --git a/src/Processors/QueryPlan/ReadFromRemote.cpp b/src/Processors/QueryPlan/ReadFromRemote.cpp\nindex a483f646afad..48ba842b907c 100644\n--- a/src/Processors/QueryPlan/ReadFromRemote.cpp\n+++ b/src/Processors/QueryPlan/ReadFromRemote.cpp\n@@ -506,14 +506,16 @@ void ReadFromRemote::addLazyPipe(Pipes & pipes, const ClusterProxy::SelectStream\n         auto timeouts = ConnectionTimeouts::getTCPTimeoutsWithFailover(current_settings)\n                             .getSaturated(current_settings[Setting::max_execution_time]);\n \n+        // In case reading from parallel replicas is allowed, lazy case is not triggered,\n+        // so in this case it's required to get only one connection from the pool\n         std::vector<ConnectionPoolWithFailover::TryResult> try_results;\n         try\n         {\n             if (my_table_func_ptr)\n-                try_results = my_shard.shard_info.pool->getManyForTableFunction(timeouts, current_settings, PoolMode::GET_MANY);\n+                try_results = my_shard.shard_info.pool->getManyForTableFunction(timeouts, current_settings, PoolMode::GET_ONE);\n             else\n                 try_results = my_shard.shard_info.pool->getManyChecked(\n-                    timeouts, current_settings, PoolMode::GET_MANY,\n+                    timeouts, current_settings, PoolMode::GET_ONE,\n                     my_shard.main_table ? my_shard.main_table.getQualifiedName() : my_main_table.getQualifiedName());\n         }\n         catch (const Exception & ex)\n",
  "test_patch": "diff --git a/tests/integration/test_delayed_replica_failover/test.py b/tests/integration/test_delayed_replica_failover/test.py\nindex ca14263faa60..e8cd4e35f648 100644\n--- a/tests/integration/test_delayed_replica_failover/test.py\n+++ b/tests/integration/test_delayed_replica_failover/test.py\n@@ -67,26 +67,40 @@ def test(started_cluster):\n         \"SYSTEM DISABLE FAILPOINT replicated_merge_tree_all_replicas_stale\"\n     )\n     with PartitionManager() as pm:\n+        # Insert initial records and wait until they are replicated across all nodes\n+        node_1_2.query(\"INSERT INTO replicated VALUES ('2017-05-08', 1)\")\n+        node_2_2.query(\"INSERT INTO replicated VALUES ('2017-05-08', 2)\")\n+\n+        for _ in range(100):\n+            count = node_1_1.query(\n+                \"SELECT count() FROM clusterAllReplicas(test_cluster, default.replicated)\"\n+            ).strip()\n+            if count == \"4\":\n+                break\n+        else:\n+            raise Exception(\"Failed to replicate initial records\")\n+\n         # Hinder replication between replicas of the same shard, but leave the possibility of distributed connection.\n         pm.partition_instances(node_1_1, node_1_2, port=9009)\n         pm.partition_instances(node_2_1, node_2_2, port=9009)\n \n-        node_1_2.query(\"INSERT INTO replicated VALUES ('2017-05-08', 1)\")\n-        node_2_2.query(\"INSERT INTO replicated VALUES ('2017-05-08', 2)\")\n+        # Insert additional records which won't be replicated\n+        node_1_2.query(\"INSERT INTO replicated VALUES ('2017-05-08', 3)\")\n+        node_2_2.query(\"INSERT INTO replicated VALUES ('2017-05-08', 4)\")\n \n         time.sleep(1)  # accrue replica delay\n \n-        assert node_1_1.query(\"SELECT sum(x) FROM replicated\").strip() == \"0\"\n-        assert node_1_2.query(\"SELECT sum(x) FROM replicated\").strip() == \"1\"\n-        assert node_2_1.query(\"SELECT sum(x) FROM replicated\").strip() == \"0\"\n-        assert node_2_2.query(\"SELECT sum(x) FROM replicated\").strip() == \"2\"\n+        assert node_1_1.query(\"SELECT sum(x) FROM replicated\").strip() == \"1\"\n+        assert node_1_2.query(\"SELECT sum(x) FROM replicated\").strip() == \"4\"\n+        assert node_2_1.query(\"SELECT sum(x) FROM replicated\").strip() == \"2\"\n+        assert node_2_2.query(\"SELECT sum(x) FROM replicated\").strip() == \"6\"\n \n         # With in_order balancing first replicas are chosen.\n         assert (\n             instance_with_dist_table.query(\n-                \"SELECT count() FROM distributed SETTINGS load_balancing='in_order'\"\n+                \"SELECT sum(x) FROM distributed SETTINGS load_balancing='in_order'\"\n             ).strip()\n-            == \"0\"\n+            == \"3\"\n         )\n \n         # When we set max_replica_delay, first replicas must be excluded.\n@@ -98,7 +112,7 @@ def test(started_cluster):\n     max_replica_delay_for_distributed_queries=1\n \"\"\"\n             ).strip()\n-            == \"3\"\n+            == \"10\"\n         )\n \n         assert (\n@@ -109,7 +123,7 @@ def test(started_cluster):\n     max_replica_delay_for_distributed_queries=1\n \"\"\"\n             ).strip()\n-            == \"3\\n\\n3\"\n+            == \"10\\n\\n10\"\n         )\n \n         pm.drop_instance_zk_connections(node_1_2)\n@@ -139,7 +153,7 @@ def test(started_cluster):\n     max_replica_delay_for_distributed_queries=1\n \"\"\"\n             ).strip()\n-            == \"3\"\n+            == \"10\"\n         )\n \n         # Prefer fallback_to_stale_replicas over skip_unavailable_shards\n@@ -152,7 +166,7 @@ def test(started_cluster):\n     max_replica_delay_for_distributed_queries=1\n \"\"\"\n             ).strip()\n-            == \"3\"\n+            == \"10\"\n         )\n \n         # If we forbid stale replicas, the query must fail. But sometimes we must have bigger timeouts.\n@@ -183,5 +197,5 @@ def test(started_cluster):\n     max_replica_delay_for_distributed_queries=1\n \"\"\"\n             ).strip()\n-            == \"2\"\n+            == \"7\"\n         )\n",
  "problem_statement": "Duplicates of local replica data with max_replica_delay_for_distributed_queries setting\n### Company or project name\n\n_No response_\n\n### Describe the unexpected behaviour\n\nWhen executing a query with the max_replica_delay_for_distributed_queries setting, I am getting duplicate rows that are stored on the current stale replica.\n\n\n### How to reproduce\n\nclickhouse: 25.3.2.39\nclickhouse-keeper: 25.3.2\n\nMy test cluster consists of 2 shards, with 2 replicas per shard.\n\nHere is the DDL to create the tables:\n```sql\nCREATE TABLE test_table ON CLUSTER 'my_cluster'\n(\n    id UInt64,\n    value String,\n    timestamp                 DateTime        DEFAULT toDateTime(now())\n) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/{shard}/{database}/test_table', '{replica}', timestamp)\n    ORDER BY (id);\n\nCREATE TABLE test_table_dist ON CLUSTER 'my_cluster'\n(\n    id UInt64,\n    value String,\n    timestamp                 DateTime        DEFAULT toDateTime(now())\n)\n    ENGINE = Distributed('my_cluster', 'default', 'test_table', id);\n```\n\nNext, insert some data into the table.\nIn my case, I used the following rows:\n\n```sql\nINSERT INTO default.test_table_dist (id, value, timestamp)\nVALUES  (1, 'v1', '2025-04-26 22:54:34'),\n        (2, 'v2', '2025-04-26 22:54:34'),\n        (3, 'v3', '2025-04-26 22:54:34'),\n        (4, 'v4', '2025-04-26 22:54:34'),\n        (5, 'v5', '2025-04-26 22:59:48');\n```\n\nThen, on node 12 (shard 1, replica 2), execute the following query to stop replication queues:\n```sql\nSYSTEM STOP REPLICATION QUEUES default.test_table;\n```\n\nAfter that, insert a new row into replica 11:\n```sql\nINSERT INTO test_table_dist\nSELECT 6 AS id, 'v6' as value, toDateTime(now()) as timestamp;\n```\n\nNow, select data on replica 12:\n```sql\nSELECT *\nFROM test_table_dist\nORDER BY id\nSETTINGS max_replica_delay_for_distributed_queries=5;\n```\nresult:\n```\n+--+-----+-------------------+\n|id|value|timestamp          |\n+--+-----+-------------------+\n|1 |v1   |2025-04-26 22:54:34|\n|2 |v2   |2025-04-26 22:54:34|\n|3 |v3   |2025-04-26 22:54:34|\n|4 |v4   |2025-04-26 22:54:34|\n|2 |v2   |2025-04-26 22:54:34|\n|4 |v4   |2025-04-26 22:54:34|\n|5 |v5   |2025-04-26 22:59:48|\n|6 |v6   |2025-04-26 23:01:32|\n+--+-----+-------------------+\n```\n\nThe result on the other cluster nodes:\n```\n+--+-----+-------------------+\n|id|value|timestamp          |\n+--+-----+-------------------+\n|1 |v1   |2025-04-26 22:54:34|\n|2 |v2   |2025-04-26 22:54:34|\n|3 |v3   |2025-04-26 22:54:34|\n|4 |v4   |2025-04-26 22:54:34|\n|5 |v5   |2025-04-26 22:59:48|\n|6 |v6   |2025-04-26 23:01:32|\n+--+-----+-------------------+\n```\n\n\n### Expected behavior\n\nWhen selecting data from a distributed table with \u2018max_replica_delay_for_distributed_queries\u2019 enabled on a lagging replica, the result must not include duplicate data from the local source table.\n\n### Error message and/or stacktrace\n\n_No response_\n\n### Additional context\n\n_No response_\n",
  "hints_text": "related to max_parallel_replicas = 1000.\n* https://github.com/ClickHouse/ClickHouse/pull/74504\n\nrelated to\n* https://github.com/ClickHouse/ClickHouse/issues/76177\n\n25.3.3.42:\n\n```\nSELECT *, fqdn()\nFROM test_table_dist\nORDER BY id ASC\nSETTINGS max_replica_delay_for_distributed_queries = 1, max_parallel_replicas = 1;\n\n-- Reads from R1 because the current replica (R2) is stale (>1)\n   \u250c\u2500id\u2500\u252c\u2500value\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500timestamp\u2500\u252c\u2500fqdn()\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n1. \u2502  1 \u2502 v1    \u2502 2025-04-26 22:54:34 \u2502 R1                          \u2502 \n2. \u2502  2 \u2502 v2    \u2502 2025-04-26 22:54:34 \u2502 R1                          \u2502\n3. \u2502  3 \u2502 v3    \u2502 2025-04-26 22:54:34 \u2502 R1                          \u2502\n4. \u2502  4 \u2502 v4    \u2502 2025-04-26 22:54:34 \u2502 R1                          \u2502\n5. \u2502  5 \u2502 v5    \u2502 2025-04-26 22:59:48 \u2502 R1                          \u2502\n6. \u2502  6 \u2502 v6    \u2502 2025-04-28 17:17:06 \u2502 R1                          \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\nSELECT *, fqdn()\nFROM test_table_dist\nORDER BY id ASC\nSETTINGS max_replica_delay_for_distributed_queries = 1, max_parallel_replicas = 1000;\n\n-- Reads from both replicas (R2 & R1) because of the bug \n   \u250c\u2500id\u2500\u252c\u2500value\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500timestamp\u2500\u252c\u2500fqdn()\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n1. \u2502  1 \u2502 v1    \u2502 2025-04-26 22:54:34 \u2502 R2                          \u2502 \n2. \u2502  2 \u2502 v2    \u2502 2025-04-26 22:54:34 \u2502 R2                          \u2502 \n3. \u2502  3 \u2502 v3    \u2502 2025-04-26 22:54:34 \u2502 R2                          \u2502\n4. \u2502  4 \u2502 v4    \u2502 2025-04-26 22:54:34 \u2502 R2                          \u2502\n5. \u2502  5 \u2502 v5    \u2502 2025-04-26 22:59:48 \u2502 R2                          \u2502\n6. \u2502  1 \u2502 v1    \u2502 2025-04-26 22:54:34 \u2502 R1                          \u2502\n7. \u2502  2 \u2502 v2    \u2502 2025-04-26 22:54:34 \u2502 R1                          \u2502\n8. \u2502  3 \u2502 v3    \u2502 2025-04-26 22:54:34 \u2502 R1                          \u2502\n9. \u2502  4 \u2502 v4    \u2502 2025-04-26 22:54:34 \u2502 R1                          \u2502\n0. \u2502  5 \u2502 v5    \u2502 2025-04-26 22:59:48 \u2502 R1                          \u2502\n1. \u2502  6 \u2502 v6    \u2502 2025-04-28 17:17:06 \u2502 R1                          \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\nSELECT *, fqdn()\nFROM test_table_dist\nORDER BY id ASC\nSETTINGS max_replica_delay_for_distributed_queries = 3600, max_parallel_replicas = 1000;\n\n-- Reads from the current replica (R2) because it's not stale ( < 3600)\n   \u250c\u2500id\u2500\u252c\u2500value\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500timestamp\u2500\u252c\u2500fqdn()\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n1. \u2502  1 \u2502 v1    \u2502 2025-04-26 22:54:34 \u2502 R2                          \u2502 \n2. \u2502  2 \u2502 v2    \u2502 2025-04-26 22:54:34 \u2502 R2                          \u2502\n3. \u2502  3 \u2502 v3    \u2502 2025-04-26 22:54:34 \u2502 R2                          \u2502\n4. \u2502  4 \u2502 v4    \u2502 2025-04-26 22:54:34 \u2502 R2                          \u2502\n5. \u2502  5 \u2502 v5    \u2502 2025-04-26 22:59:48 \u2502 R2                          \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n```\n@azat ",
  "created_at": "2025-05-01T14:18:02Z",
  "modified_files": [
    "src/Processors/QueryPlan/ReadFromRemote.cpp"
  ],
  "modified_test_files": [
    "tests/integration/test_delayed_replica_failover/test.py"
  ]
}