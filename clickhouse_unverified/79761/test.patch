diff --git a/tests/integration/test_delayed_replica_failover/test.py b/tests/integration/test_delayed_replica_failover/test.py
index ca14263faa60..e8cd4e35f648 100644
--- a/tests/integration/test_delayed_replica_failover/test.py
+++ b/tests/integration/test_delayed_replica_failover/test.py
@@ -67,26 +67,40 @@ def test(started_cluster):
         "SYSTEM DISABLE FAILPOINT replicated_merge_tree_all_replicas_stale"
     )
     with PartitionManager() as pm:
+        # Insert initial records and wait until they are replicated across all nodes
+        node_1_2.query("INSERT INTO replicated VALUES ('2017-05-08', 1)")
+        node_2_2.query("INSERT INTO replicated VALUES ('2017-05-08', 2)")
+
+        for _ in range(100):
+            count = node_1_1.query(
+                "SELECT count() FROM clusterAllReplicas(test_cluster, default.replicated)"
+            ).strip()
+            if count == "4":
+                break
+        else:
+            raise Exception("Failed to replicate initial records")
+
         # Hinder replication between replicas of the same shard, but leave the possibility of distributed connection.
         pm.partition_instances(node_1_1, node_1_2, port=9009)
         pm.partition_instances(node_2_1, node_2_2, port=9009)
 
-        node_1_2.query("INSERT INTO replicated VALUES ('2017-05-08', 1)")
-        node_2_2.query("INSERT INTO replicated VALUES ('2017-05-08', 2)")
+        # Insert additional records which won't be replicated
+        node_1_2.query("INSERT INTO replicated VALUES ('2017-05-08', 3)")
+        node_2_2.query("INSERT INTO replicated VALUES ('2017-05-08', 4)")
 
         time.sleep(1)  # accrue replica delay
 
-        assert node_1_1.query("SELECT sum(x) FROM replicated").strip() == "0"
-        assert node_1_2.query("SELECT sum(x) FROM replicated").strip() == "1"
-        assert node_2_1.query("SELECT sum(x) FROM replicated").strip() == "0"
-        assert node_2_2.query("SELECT sum(x) FROM replicated").strip() == "2"
+        assert node_1_1.query("SELECT sum(x) FROM replicated").strip() == "1"
+        assert node_1_2.query("SELECT sum(x) FROM replicated").strip() == "4"
+        assert node_2_1.query("SELECT sum(x) FROM replicated").strip() == "2"
+        assert node_2_2.query("SELECT sum(x) FROM replicated").strip() == "6"
 
         # With in_order balancing first replicas are chosen.
         assert (
             instance_with_dist_table.query(
-                "SELECT count() FROM distributed SETTINGS load_balancing='in_order'"
+                "SELECT sum(x) FROM distributed SETTINGS load_balancing='in_order'"
             ).strip()
-            == "0"
+            == "3"
         )
 
         # When we set max_replica_delay, first replicas must be excluded.
@@ -98,7 +112,7 @@ def test(started_cluster):
     max_replica_delay_for_distributed_queries=1
 """
             ).strip()
-            == "3"
+            == "10"
         )
 
         assert (
@@ -109,7 +123,7 @@ def test(started_cluster):
     max_replica_delay_for_distributed_queries=1
 """
             ).strip()
-            == "3

3"
+            == "10

10"
         )
 
         pm.drop_instance_zk_connections(node_1_2)
@@ -139,7 +153,7 @@ def test(started_cluster):
     max_replica_delay_for_distributed_queries=1
 """
             ).strip()
-            == "3"
+            == "10"
         )
 
         # Prefer fallback_to_stale_replicas over skip_unavailable_shards
@@ -152,7 +166,7 @@ def test(started_cluster):
     max_replica_delay_for_distributed_queries=1
 """
             ).strip()
-            == "3"
+            == "10"
         )
 
         # If we forbid stale replicas, the query must fail. But sometimes we must have bigger timeouts.
@@ -183,5 +197,5 @@ def test(started_cluster):
     max_replica_delay_for_distributed_queries=1
 """
             ).strip()
-            == "2"
+            == "7"
         )
