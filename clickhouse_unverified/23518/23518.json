{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 23518,
  "instance_id": "ClickHouse__ClickHouse-23518",
  "issue_numbers": [
    "9159"
  ],
  "base_commit": "deeaa98af519ede9751c58c729d29c84338045b9",
  "patch": "diff --git a/programs/copier/ClusterCopier.cpp b/programs/copier/ClusterCopier.cpp\nindex a60896388a00..6863c6e7c190 100644\n--- a/programs/copier/ClusterCopier.cpp\n+++ b/programs/copier/ClusterCopier.cpp\n@@ -1,12 +1,15 @@\n #include \"ClusterCopier.h\"\n \n #include \"Internals.h\"\n+#include \"StatusAccumulator.h\"\n \n #include <Common/ZooKeeper/ZooKeeper.h>\n #include <Common/ZooKeeper/KeeperException.h>\n #include <Common/setThreadName.h>\n #include <IO/ConnectionTimeoutsContext.h>\n-\n+#include <Interpreters/InterpreterInsertQuery.h>\n+#include <Processors/Transforms/ExpressionTransform.h>\n+#include <DataStreams/ExpressionBlockInputStream.h>\n \n namespace DB\n {\n@@ -29,17 +32,16 @@ void ClusterCopier::init()\n         if (response.error != Coordination::Error::ZOK)\n             return;\n         UInt64 version = ++task_description_version;\n-        LOG_DEBUG(log, \"Task description should be updated, local version {}\", version);\n+        LOG_INFO(log, \"Task description should be updated, local version {}\", version);\n     };\n \n     task_description_path = task_zookeeper_path + \"/description\";\n     task_cluster = std::make_unique<TaskCluster>(task_zookeeper_path, working_database_name);\n \n     reloadTaskDescription();\n-    task_cluster_initial_config = task_cluster_current_config;\n \n-    task_cluster->loadTasks(*task_cluster_initial_config);\n-    getContext()->setClustersConfig(task_cluster_initial_config, task_cluster->clusters_prefix);\n+    task_cluster->loadTasks(*task_cluster_current_config);\n+    getContext()->setClustersConfig(task_cluster_current_config, task_cluster->clusters_prefix);\n \n     /// Set up shards and their priority\n     task_cluster->random_engine.seed(task_cluster->random_device());\n@@ -50,12 +52,14 @@ void ClusterCopier::init()\n         task_table.initShards(task_cluster->random_engine);\n     }\n \n-    LOG_DEBUG(log, \"Will process {} table tasks\", task_cluster->table_tasks.size());\n+    LOG_INFO(log, \"Will process {} table tasks\", task_cluster->table_tasks.size());\n \n     /// Do not initialize tables, will make deferred initialization in process()\n \n     zookeeper->createAncestors(getWorkersPathVersion() + \"/\");\n     zookeeper->createAncestors(getWorkersPath() + \"/\");\n+    /// Init status node\n+    zookeeper->createIfNotExists(task_zookeeper_path + \"/status\", \"{}\");\n }\n \n template <typename T>\n@@ -138,7 +142,7 @@ void ClusterCopier::discoverShardPartitions(const ConnectionTimeouts & timeouts,\n         {\n             if (!task_table.enabled_partitions_set.count(partition_name))\n             {\n-                LOG_DEBUG(log, \"Partition {} will not be processed, since it is not in enabled_partitions of {}\", partition_name, task_table.table_id);\n+                LOG_INFO(log, \"Partition {} will not be processed, since it is not in enabled_partitions of {}\", partition_name, task_table.table_id);\n             }\n         }\n     }\n@@ -173,7 +177,7 @@ void ClusterCopier::discoverShardPartitions(const ConnectionTimeouts & timeouts,\n         LOG_WARNING(log, \"There are no {} partitions from enabled_partitions in shard {} :{}\", missing_partitions.size(), task_shard->getDescription(), ss.str());\n     }\n \n-    LOG_DEBUG(log, \"Will copy {} partitions from shard {}\", task_shard->partition_tasks.size(), task_shard->getDescription());\n+    LOG_INFO(log, \"Will copy {} partitions from shard {}\", task_shard->partition_tasks.size(), task_shard->getDescription());\n }\n \n void ClusterCopier::discoverTablePartitions(const ConnectionTimeouts & timeouts, TaskTable & task_table, UInt64 num_threads)\n@@ -189,7 +193,7 @@ void ClusterCopier::discoverTablePartitions(const ConnectionTimeouts & timeouts,\n                 discoverShardPartitions(timeouts, task_shard);\n             });\n \n-        LOG_DEBUG(log, \"Waiting for {} setup jobs\", thread_pool.active());\n+        LOG_INFO(log, \"Waiting for {} setup jobs\", thread_pool.active());\n         thread_pool.wait();\n     }\n }\n@@ -213,7 +217,7 @@ void ClusterCopier::uploadTaskDescription(const std::string & task_path, const s\n     if (code != Coordination::Error::ZOK && force)\n         zookeeper->createOrUpdate(local_task_description_path, task_config_str, zkutil::CreateMode::Persistent);\n \n-    LOG_DEBUG(log, \"Task description {} uploaded to {} with result {} ({})\",\n+    LOG_INFO(log, \"Task description {} uploaded to {} with result {} ({})\",\n         ((code != Coordination::Error::ZOK && !force) ? \"not \" : \"\"), local_task_description_path, code, Coordination::errorMessage(code));\n }\n \n@@ -222,23 +226,17 @@ void ClusterCopier::reloadTaskDescription()\n     auto zookeeper = getContext()->getZooKeeper();\n     task_description_watch_zookeeper = zookeeper;\n \n-    String task_config_str;\n     Coordination::Stat stat{};\n-    Coordination::Error code;\n \n-    zookeeper->tryGetWatch(task_description_path, task_config_str, &stat, task_description_watch_callback, &code);\n-    if (code != Coordination::Error::ZOK)\n-        throw Exception(\"Can't get description node \" + task_description_path, ErrorCodes::BAD_ARGUMENTS);\n+    /// It will throw exception if such a node doesn't exist.\n+    auto task_config_str = zookeeper->get(task_description_path, &stat);\n \n-    LOG_DEBUG(log, \"Loading description, zxid={}\", task_description_current_stat.czxid);\n-    auto config = getConfigurationFromXMLString(task_config_str);\n+    LOG_INFO(log, \"Loading task description\");\n+    task_cluster_current_config = getConfigurationFromXMLString(task_config_str);\n \n     /// Setup settings\n-    task_cluster->reloadSettings(*config);\n+    task_cluster->reloadSettings(*task_cluster_current_config);\n     getContext()->setSettings(task_cluster->settings_common);\n-\n-    task_cluster_current_config = config;\n-    task_description_current_stat = stat;\n }\n \n void ClusterCopier::updateConfigIfNeeded()\n@@ -250,7 +248,7 @@ void ClusterCopier::updateConfigIfNeeded()\n     if (!is_outdated_version && !is_expired_session)\n         return;\n \n-    LOG_DEBUG(log, \"Updating task description\");\n+    LOG_INFO(log, \"Updating task description\");\n     reloadTaskDescription();\n \n     task_description_current_version = version_to_update;\n@@ -361,7 +359,7 @@ zkutil::EphemeralNodeHolder::Ptr ClusterCopier::createTaskWorkerNodeAndWaitIfNee\n \n         if (static_cast<UInt64>(stat.numChildren) >= task_cluster->max_workers)\n         {\n-            LOG_DEBUG(log, \"Too many workers ({}, maximum {}). Postpone processing {}\", stat.numChildren, task_cluster->max_workers, description);\n+            LOG_INFO(log, \"Too many workers ({}, maximum {}). Postpone processing {}\", stat.numChildren, task_cluster->max_workers, description);\n \n             if (unprioritized)\n                 current_sleep_time = std::min(max_sleep_time, current_sleep_time + default_sleep_time);\n@@ -387,7 +385,7 @@ zkutil::EphemeralNodeHolder::Ptr ClusterCopier::createTaskWorkerNodeAndWaitIfNee\n                 /// Try to make fast retries\n                 if (num_bad_version_errors > 3)\n                 {\n-                    LOG_DEBUG(log, \"A concurrent worker has just been added, will check free worker slots again\");\n+                    LOG_INFO(log, \"A concurrent worker has just been added, will check free worker slots again\");\n                     std::chrono::milliseconds random_sleep_time(std::uniform_int_distribution<int>(1, 1000)(task_cluster->random_engine));\n                     std::this_thread::sleep_for(random_sleep_time);\n                     num_bad_version_errors = 0;\n@@ -422,7 +420,7 @@ bool ClusterCopier::checkAllPiecesInPartitionAreDone(const TaskTable & task_tabl\n     {\n         bool piece_is_done = checkPartitionPieceIsDone(task_table, partition_name, piece_number, shards_with_partition);\n         if (!piece_is_done)\n-            LOG_DEBUG(log, \"Partition {} piece {} is not already done.\", partition_name, piece_number);\n+            LOG_INFO(log, \"Partition {} piece {} is not already done.\", partition_name, piece_number);\n         answer &= piece_is_done;\n     }\n \n@@ -438,7 +436,7 @@ bool ClusterCopier::checkAllPiecesInPartitionAreDone(const TaskTable & task_tabl\n bool ClusterCopier::checkPartitionPieceIsDone(const TaskTable & task_table, const String & partition_name,\n                                size_t piece_number, const TasksShard & shards_with_partition)\n {\n-    LOG_DEBUG(log, \"Check that all shards processed partition {} piece {} successfully\", partition_name, piece_number);\n+    LOG_INFO(log, \"Check that all shards processed partition {} piece {} successfully\", partition_name, piece_number);\n \n     auto zookeeper = getContext()->getZooKeeper();\n \n@@ -530,7 +528,7 @@ TaskStatus ClusterCopier::tryMoveAllPiecesToDestinationTable(const TaskTable & t\n         inject_fault = value < move_fault_probability;\n     }\n \n-    LOG_DEBUG(log, \"Try to move {} to destination table\", partition_name);\n+    LOG_INFO(log, \"Try to move {} to destination table\", partition_name);\n \n     auto zookeeper = getContext()->getZooKeeper();\n \n@@ -548,7 +546,7 @@ TaskStatus ClusterCopier::tryMoveAllPiecesToDestinationTable(const TaskTable & t\n     {\n         if (e.code == Coordination::Error::ZNODEEXISTS)\n         {\n-            LOG_DEBUG(log, \"Someone is already moving pieces {}\", current_partition_attach_is_active);\n+            LOG_INFO(log, \"Someone is already moving pieces {}\", current_partition_attach_is_active);\n             return TaskStatus::Active;\n         }\n \n@@ -565,13 +563,13 @@ TaskStatus ClusterCopier::tryMoveAllPiecesToDestinationTable(const TaskTable & t\n             TaskStateWithOwner status = TaskStateWithOwner::fromString(status_data);\n             if (status.state == TaskState::Finished)\n             {\n-                LOG_DEBUG(log, \"All pieces for partition from this task {} has been successfully moved to destination table by {}\", current_partition_attach_is_active, status.owner);\n+                LOG_INFO(log, \"All pieces for partition from this task {} has been successfully moved to destination table by {}\", current_partition_attach_is_active, status.owner);\n                 return TaskStatus::Finished;\n             }\n \n             /// Task is abandoned, because previously we created ephemeral node, possibly in other copier's process.\n             /// Initialize DROP PARTITION\n-            LOG_DEBUG(log, \"Moving piece for partition {} has not been successfully finished by {}. Will try to move by myself.\", current_partition_attach_is_active, status.owner);\n+            LOG_INFO(log, \"Moving piece for partition {} has not been successfully finished by {}. Will try to move by myself.\", current_partition_attach_is_active, status.owner);\n \n             /// Remove is_done marker.\n             zookeeper->remove(current_partition_attach_is_done);\n@@ -585,10 +583,25 @@ TaskStatus ClusterCopier::tryMoveAllPiecesToDestinationTable(const TaskTable & t\n         zookeeper->create(current_partition_attach_is_done, start_state, zkutil::CreateMode::Persistent);\n     }\n \n+\n+    /// Try to drop destination partition in original table\n+    if (task_table.allow_to_drop_target_partitions)\n+    {\n+        DatabaseAndTableName original_table = task_table.table_push;\n+\n+        WriteBufferFromOwnString ss;\n+        ss << \"ALTER TABLE \" << getQuotedTable(original_table) << ((partition_name == \"'all'\") ? \" DROP PARTITION ID \" : \" DROP PARTITION \") << partition_name;\n+\n+        UInt64 num_shards_drop_partition = executeQueryOnCluster(task_table.cluster_push, ss.str(), task_cluster->settings_push, ClusterExecutionMode::ON_EACH_SHARD);\n+\n+        LOG_INFO(log, \"Drop partition {} in original table {} have been executed successfully on {} shards of {}\",\n+            partition_name, getQuotedTable(original_table), num_shards_drop_partition, task_table.cluster_push->getShardCount());\n+    }\n+\n     /// Move partition to original destination table.\n     for (size_t current_piece_number = 0; current_piece_number < task_table.number_of_splits; ++current_piece_number)\n     {\n-        LOG_DEBUG(log, \"Trying to move partition {} piece {} to original table\", partition_name, toString(current_piece_number));\n+        LOG_INFO(log, \"Trying to move partition {} piece {} to original table\", partition_name, toString(current_piece_number));\n \n         ASTPtr query_alter_ast;\n         String query_alter_ast_string;\n@@ -600,18 +613,15 @@ TaskStatus ClusterCopier::tryMoveAllPiecesToDestinationTable(const TaskTable & t\n \n         Settings settings_push = task_cluster->settings_push;\n         ClusterExecutionMode execution_mode = ClusterExecutionMode::ON_EACH_NODE;\n-        UInt64 max_successful_executions_per_shard = 0;\n+\n         if (settings_push.replication_alter_partitions_sync == 1)\n-        {\n             execution_mode = ClusterExecutionMode::ON_EACH_SHARD;\n-            max_successful_executions_per_shard = 1;\n-        }\n \n         query_alter_ast_string += \" ALTER TABLE \" + getQuotedTable(original_table) +\n                                   ((partition_name == \"'all'\") ? \" ATTACH PARTITION ID \" : \" ATTACH PARTITION \") + partition_name +\n                                   \" FROM \" + getQuotedTable(helping_table);\n \n-        LOG_DEBUG(log, \"Executing ALTER query: {}\", query_alter_ast_string);\n+        LOG_INFO(log, \"Executing ALTER query: {}\", query_alter_ast_string);\n \n         try\n         {\n@@ -620,9 +630,7 @@ TaskStatus ClusterCopier::tryMoveAllPiecesToDestinationTable(const TaskTable & t\n                 task_table.cluster_push,\n                 query_alter_ast_string,\n                 task_cluster->settings_push,\n-                PoolMode::GET_MANY,\n-                execution_mode,\n-                max_successful_executions_per_shard);\n+                execution_mode);\n \n             if (settings_push.replication_alter_partitions_sync == 1)\n             {\n@@ -634,9 +642,7 @@ TaskStatus ClusterCopier::tryMoveAllPiecesToDestinationTable(const TaskTable & t\n                     task_table.cluster_push->getShardCount());\n \n                 if (num_nodes != task_table.cluster_push->getShardCount())\n-                {\n                     return TaskStatus::Error;\n-                }\n             }\n             else\n             {\n@@ -645,50 +651,46 @@ TaskStatus ClusterCopier::tryMoveAllPiecesToDestinationTable(const TaskTable & t\n         }\n         catch (...)\n         {\n-            LOG_DEBUG(log, \"Error while moving partition {} piece {} to original table\", partition_name, toString(current_piece_number));\n+            LOG_INFO(log, \"Error while moving partition {} piece {} to original table\", partition_name, toString(current_piece_number));\n+            LOG_WARNING(log, \"In case of non-replicated tables it can cause duplicates.\");\n             throw;\n         }\n \n         if (inject_fault)\n             throw Exception(\"Copy fault injection is activated\", ErrorCodes::UNFINISHED);\n-\n-        try\n-        {\n-            String query_deduplicate_ast_string;\n-            if (!task_table.isReplicatedTable())\n-            {\n-                query_deduplicate_ast_string += \" OPTIMIZE TABLE \" + getQuotedTable(original_table) +\n-                                                ((partition_name == \"'all'\") ? \" PARTITION ID \" : \" PARTITION \") + partition_name + \" DEDUPLICATE;\";\n-\n-                LOG_DEBUG(log, \"Executing OPTIMIZE DEDUPLICATE query: {}\", query_deduplicate_ast_string);\n-\n-                UInt64 num_nodes = executeQueryOnCluster(\n-                        task_table.cluster_push,\n-                        query_deduplicate_ast_string,\n-                        task_cluster->settings_push,\n-                        PoolMode::GET_MANY);\n-\n-                LOG_INFO(log, \"Number of shard that executed OPTIMIZE DEDUPLICATE query successfully : {}\", toString(num_nodes));\n-            }\n-        }\n-        catch (...)\n-        {\n-            LOG_DEBUG(log, \"Error while executing OPTIMIZE DEDUPLICATE partition {}in the original table\", partition_name);\n-            throw;\n-        }\n     }\n \n     /// Create node to signal that we finished moving\n     {\n         String state_finished = TaskStateWithOwner::getData(TaskState::Finished, host_id);\n         zookeeper->set(current_partition_attach_is_done, state_finished, 0);\n+        /// Also increment a counter of processed partitions\n+        while (true)\n+        {\n+            Coordination::Stat stat;\n+            auto status_json = zookeeper->get(task_zookeeper_path + \"/status\", &stat);\n+            auto statuses = StatusAccumulator::fromJSON(status_json);\n+\n+            /// Increment status for table.\n+            auto status_for_table = (*statuses)[task_table.name_in_config];\n+            status_for_table.processed_partitions_count += 1;\n+            (*statuses)[task_table.name_in_config] = status_for_table;\n+\n+            auto statuses_to_commit = StatusAccumulator::serializeToJSON(statuses);\n+            auto error = zookeeper->trySet(task_zookeeper_path + \"/status\", statuses_to_commit, stat.version, &stat);\n+            if (error == Coordination::Error::ZOK)\n+                break;\n+        }\n     }\n \n     return TaskStatus::Finished;\n }\n \n-/// Removes MATERIALIZED and ALIAS columns from create table query\n-ASTPtr ClusterCopier::removeAliasColumnsFromCreateQuery(const ASTPtr & query_ast)\n+/// This is needed to create internal Distributed table\n+/// Removes column's TTL expression from `CREATE` query\n+/// Removes MATEREALIZED or ALIAS columns not to copy additional and useless data over the network.\n+/// Removes data skipping indices.\n+ASTPtr ClusterCopier::removeAliasMaterializedAndTTLColumnsFromCreateQuery(const ASTPtr & query_ast, bool allow_to_copy_alias_and_materialized_columns)\n {\n     const ASTs & column_asts = query_ast->as<ASTCreateQuery &>().columns_list->columns->children;\n     auto new_columns = std::make_shared<ASTExpressionList>();\n@@ -697,14 +699,21 @@ ASTPtr ClusterCopier::removeAliasColumnsFromCreateQuery(const ASTPtr & query_ast\n     {\n         const auto & column = column_ast->as<ASTColumnDeclaration &>();\n \n-        if (!column.default_specifier.empty())\n+        /// Skip this columns\n+        if (!column.default_specifier.empty() && !allow_to_copy_alias_and_materialized_columns)\n         {\n             ColumnDefaultKind kind = columnDefaultKindFromString(column.default_specifier);\n             if (kind == ColumnDefaultKind::Materialized || kind == ColumnDefaultKind::Alias)\n                 continue;\n         }\n \n-        new_columns->children.emplace_back(column_ast->clone());\n+        /// Remove TTL on columns definition.\n+        auto new_column_ast = column_ast->clone();\n+        auto & new_column = new_column_ast->as<ASTColumnDeclaration &>();\n+        if (new_column.ttl)\n+            new_column.ttl.reset();\n+\n+        new_columns->children.emplace_back(new_column_ast);\n     }\n \n     ASTPtr new_query_ast = query_ast->clone();\n@@ -712,10 +721,8 @@ ASTPtr ClusterCopier::removeAliasColumnsFromCreateQuery(const ASTPtr & query_ast\n \n     auto new_columns_list = std::make_shared<ASTColumns>();\n     new_columns_list->set(new_columns_list->columns, new_columns);\n-    if (const auto * indices = query_ast->as<ASTCreateQuery>()->columns_list->indices)\n-        new_columns_list->set(new_columns_list->indices, indices->clone());\n-    if (const auto * projections = query_ast->as<ASTCreateQuery>()->columns_list->projections)\n-        new_columns_list->set(new_columns_list->projections, projections->clone());\n+\n+    /// Skip indices and projections are not needed, because distributed table doesn't support it.\n \n     new_query.replace(new_query.columns_list, new_columns_list);\n \n@@ -739,6 +746,8 @@ std::shared_ptr<ASTCreateQuery> rewriteCreateQueryStorage(const ASTPtr & create_\n     res->children.clear();\n     res->set(res->columns_list, create.columns_list->clone());\n     res->set(res->storage, new_storage_ast->clone());\n+    /// Just to make it better and don't store additional flag like `is_table_created` somewhere else\n+    res->if_not_exists = true;\n \n     return res;\n }\n@@ -771,7 +780,7 @@ bool ClusterCopier::tryDropPartitionPiece(\n     {\n         if (e.code == Coordination::Error::ZNODEEXISTS)\n         {\n-            LOG_DEBUG(log, \"Partition {} piece {} is cleaning now by somebody, sleep\", task_partition.name, toString(current_piece_number));\n+            LOG_INFO(log, \"Partition {} piece {} is cleaning now by somebody, sleep\", task_partition.name, toString(current_piece_number));\n             std::this_thread::sleep_for(default_sleep_time);\n             return false;\n         }\n@@ -784,7 +793,7 @@ bool ClusterCopier::tryDropPartitionPiece(\n     {\n         if (stat.numChildren != 0)\n         {\n-            LOG_DEBUG(log, \"Partition {} contains {} active workers while trying to drop it. Going to sleep.\", task_partition.name, stat.numChildren);\n+            LOG_INFO(log, \"Partition {} contains {} active workers while trying to drop it. Going to sleep.\", task_partition.name, stat.numChildren);\n             std::this_thread::sleep_for(default_sleep_time);\n             return false;\n         }\n@@ -804,7 +813,7 @@ bool ClusterCopier::tryDropPartitionPiece(\n         {\n             if (e.code == Coordination::Error::ZNODEEXISTS)\n             {\n-                LOG_DEBUG(log, \"Partition {} is being filled now by somebody, sleep\", task_partition.name);\n+                LOG_INFO(log, \"Partition {} is being filled now by somebody, sleep\", task_partition.name);\n                 return false;\n             }\n \n@@ -842,12 +851,11 @@ bool ClusterCopier::tryDropPartitionPiece(\n         /// It is important, DROP PARTITION must be done synchronously\n         settings_push.replication_alter_partitions_sync = 2;\n \n-        LOG_DEBUG(log, \"Execute distributed DROP PARTITION: {}\", query);\n+        LOG_INFO(log, \"Execute distributed DROP PARTITION: {}\", query);\n         /// We have to drop partition_piece on each replica\n         size_t num_shards = executeQueryOnCluster(\n                 cluster_push, query,\n                 settings_push,\n-                PoolMode::GET_MANY,\n                 ClusterExecutionMode::ON_EACH_NODE);\n \n         LOG_INFO(log, \"DROP PARTITION was successfully executed on {} nodes of a cluster.\", num_shards);\n@@ -863,7 +871,7 @@ bool ClusterCopier::tryDropPartitionPiece(\n         }\n         else\n         {\n-            LOG_DEBUG(log, \"Clean state is altered when dropping the partition, cowardly bailing\");\n+            LOG_INFO(log, \"Clean state is altered when dropping the partition, cowardly bailing\");\n             /// clean state is stale\n             return false;\n         }\n@@ -889,6 +897,31 @@ bool ClusterCopier::tryProcessTable(const ConnectionTimeouts & timeouts, TaskTab\n         LOG_WARNING(log, \"Create destination Tale Failed \");\n         return false;\n     }\n+\n+    /// Set all_partitions_count for table in Zookeeper\n+    auto zookeeper = getContext()->getZooKeeper();\n+    while (true)\n+    {\n+        Coordination::Stat stat;\n+        auto status_json = zookeeper->get(task_zookeeper_path + \"/status\", &stat);\n+        auto statuses = StatusAccumulator::fromJSON(status_json);\n+\n+        /// Exit if someone already set the initial value for this table.\n+        if (statuses->find(task_table.name_in_config) != statuses->end())\n+            break;\n+        (*statuses)[task_table.name_in_config] = StatusAccumulator::TableStatus\n+        {\n+            /*all_partitions_count=*/task_table.ordered_partition_names.size(),\n+            /*processed_partition_count=*/0\n+        };\n+\n+        auto statuses_to_commit = StatusAccumulator::serializeToJSON(statuses);\n+        auto error = zookeeper->trySet(task_zookeeper_path + \"/status\", statuses_to_commit, stat.version);\n+        if (error == Coordination::Error::ZOK)\n+            break;\n+    }\n+\n+\n     /// An heuristic: if previous shard is already done, then check next one without sleeps due to max_workers constraint\n     bool previous_shard_is_instantly_finished = false;\n \n@@ -907,7 +940,7 @@ bool ClusterCopier::tryProcessTable(const ConnectionTimeouts & timeouts, TaskTab\n \n         ++cluster_partition.total_tries;\n \n-        LOG_DEBUG(log, \"Processing partition {} for the whole cluster\", partition_name);\n+        LOG_INFO(log, \"Processing partition {} for the whole cluster\", partition_name);\n \n         /// Process each source shard having current partition and copy current partition\n         /// NOTE: shards are sorted by \"distance\" to current host\n@@ -929,7 +962,7 @@ bool ClusterCopier::tryProcessTable(const ConnectionTimeouts & timeouts, TaskTab\n                     {\n                         const size_t number_of_splits = task_table.number_of_splits;\n                         shard->partition_tasks.emplace(partition_name, ShardPartition(*shard, partition_name, number_of_splits));\n-                        LOG_DEBUG(log, \"Discovered partition {} in shard {}\", partition_name, shard->getDescription());\n+                        LOG_INFO(log, \"Discovered partition {} in shard {}\", partition_name, shard->getDescription());\n                         /// To save references in the future.\n                         auto shard_partition_it = shard->partition_tasks.find(partition_name);\n                         PartitionPieces & shard_partition_pieces = shard_partition_it->second.pieces;\n@@ -942,7 +975,7 @@ bool ClusterCopier::tryProcessTable(const ConnectionTimeouts & timeouts, TaskTab\n                     }\n                     else\n                     {\n-                        LOG_DEBUG(log, \"Found that shard {} does not contain current partition {}\", shard->getDescription(), partition_name);\n+                        LOG_INFO(log, \"Found that shard {} does not contain current partition {}\", shard->getDescription(), partition_name);\n                         continue;\n                     }\n                 }\n@@ -1100,18 +1133,14 @@ TaskStatus ClusterCopier::tryCreateDestinationTable(const ConnectionTimeouts & t\n         InterpreterCreateQuery::prepareOnClusterQuery(create, getContext(), task_table.cluster_push_name);\n         String query = queryToString(create_query_push_ast);\n \n-        LOG_DEBUG(log, \"Create destination tables. Query: {}\", query);\n-        UInt64 shards = executeQueryOnCluster(task_table.cluster_push, query, task_cluster->settings_push, PoolMode::GET_MANY);\n+        LOG_INFO(log, \"Create destination tables. Query: \\n {}\", query);\n+        UInt64 shards = executeQueryOnCluster(task_table.cluster_push, query, task_cluster->settings_push, ClusterExecutionMode::ON_EACH_NODE);\n         LOG_INFO(\n             log,\n             \"Destination tables {} have been created on {} shards of {}\",\n             getQuotedTable(task_table.table_push),\n             shards,\n             task_table.cluster_push->getShardCount());\n-        if (shards != task_table.cluster_push->getShardCount())\n-        {\n-            return TaskStatus::Error;\n-        }\n     }\n     catch (...)\n     {\n@@ -1226,17 +1255,17 @@ TaskStatus ClusterCopier::processPartitionPieceTaskImpl(\n     auto create_is_dirty_node = [&] (const CleanStateClock & clock)\n     {\n         if (clock.is_stale())\n-            LOG_DEBUG(log, \"Clean state clock is stale while setting dirty flag, cowardly bailing\");\n+            LOG_INFO(log, \"Clean state clock is stale while setting dirty flag, cowardly bailing\");\n         else if (!clock.is_clean())\n-            LOG_DEBUG(log, \"Thank you, Captain Obvious\");\n+            LOG_INFO(log, \"Thank you, Captain Obvious\");\n         else if (clock.discovery_version)\n         {\n-            LOG_DEBUG(log, \"Updating clean state clock\");\n+            LOG_INFO(log, \"Updating clean state clock\");\n             zookeeper->set(piece_is_dirty_flag_path, host_id, clock.discovery_version.value());\n         }\n         else\n         {\n-            LOG_DEBUG(log, \"Creating clean state clock\");\n+            LOG_INFO(log, \"Creating clean state clock\");\n             zookeeper->create(piece_is_dirty_flag_path, host_id, zkutil::CreateMode::Persistent);\n         }\n     };\n@@ -1262,6 +1291,8 @@ TaskStatus ClusterCopier::processPartitionPieceTaskImpl(\n         if (!limit.empty())\n             query += \" LIMIT \" + limit;\n \n+        query += \"FORMAT Native\";\n+\n         ParserQuery p_query(query.data() + query.size());\n \n         const auto & settings = getContext()->getSettingsRef();\n@@ -1271,7 +1302,7 @@ TaskStatus ClusterCopier::processPartitionPieceTaskImpl(\n     /// Load balancing\n     auto worker_node_holder = createTaskWorkerNodeAndWaitIfNeed(zookeeper, current_task_piece_status_path, is_unprioritized_task);\n \n-    LOG_DEBUG(log, \"Processing {}\", current_task_piece_status_path);\n+    LOG_INFO(log, \"Processing {}\", current_task_piece_status_path);\n \n     const String piece_status_path = partition_piece.getPartitionPieceShardsPath();\n \n@@ -1282,12 +1313,12 @@ TaskStatus ClusterCopier::processPartitionPieceTaskImpl(\n     /// Do not start if partition piece is dirty, try to clean it\n     if (is_clean)\n     {\n-        LOG_DEBUG(log, \"Partition {} piece {} appears to be clean\", task_partition.name, current_piece_number);\n+        LOG_INFO(log, \"Partition {} piece {} appears to be clean\", task_partition.name, current_piece_number);\n         zookeeper->createAncestors(current_task_piece_status_path);\n     }\n     else\n     {\n-        LOG_DEBUG(log, \"Partition {} piece {} is dirty, try to drop it\", task_partition.name, current_piece_number);\n+        LOG_INFO(log, \"Partition {} piece {} is dirty, try to drop it\", task_partition.name, current_piece_number);\n \n         try\n         {\n@@ -1312,7 +1343,7 @@ TaskStatus ClusterCopier::processPartitionPieceTaskImpl(\n     {\n         if (e.code == Coordination::Error::ZNODEEXISTS)\n         {\n-            LOG_DEBUG(log, \"Someone is already processing {}\", current_task_piece_is_active_path);\n+            LOG_INFO(log, \"Someone is already processing {}\", current_task_piece_is_active_path);\n             return TaskStatus::Active;\n         }\n \n@@ -1328,13 +1359,13 @@ TaskStatus ClusterCopier::processPartitionPieceTaskImpl(\n             TaskStateWithOwner status = TaskStateWithOwner::fromString(status_data);\n             if (status.state == TaskState::Finished)\n             {\n-                LOG_DEBUG(log, \"Task {} has been successfully executed by {}\", current_task_piece_status_path, status.owner);\n+                LOG_INFO(log, \"Task {} has been successfully executed by {}\", current_task_piece_status_path, status.owner);\n                 return TaskStatus::Finished;\n             }\n \n             /// Task is abandoned, because previously we created ephemeral node, possibly in other copier's process.\n             /// Initialize DROP PARTITION\n-            LOG_DEBUG(log, \"Task {} has not been successfully finished by {}. Partition will be dropped and refilled.\", current_task_piece_status_path, status.owner);\n+            LOG_INFO(log, \"Task {} has not been successfully finished by {}. Partition will be dropped and refilled.\", current_task_piece_status_path, status.owner);\n \n             create_is_dirty_node(clean_state_clock);\n             return TaskStatus::Error;\n@@ -1342,6 +1373,47 @@ TaskStatus ClusterCopier::processPartitionPieceTaskImpl(\n     }\n \n \n+    /// Try create table (if not exists) on each shard\n+    /// We have to create this table even in case that partition piece is empty\n+    /// This is significant, because we will have simpler code\n+    {\n+        /// 1) Get columns description from any replica of destination cluster\n+        /// 2) Change ENGINE, database and table name\n+        /// 3) Create helping table on the whole destination cluster\n+        auto & settings_push = task_cluster->settings_push;\n+\n+        auto connection = task_table.cluster_push->getAnyShardInfo().pool->get(timeouts, &settings_push, true);\n+        String create_query = getRemoteCreateTable(task_shard.task_table.table_push, *connection, settings_push);\n+\n+        ParserCreateQuery parser_create_query;\n+        auto create_query_ast = parseQuery(parser_create_query, create_query, settings_push.max_query_size, settings_push.max_parser_depth);\n+        /// Define helping table database and name for current partition piece\n+        DatabaseAndTableName database_and_table_for_current_piece\n+        {\n+            task_table.table_push.first,\n+            task_table.table_push.second + \"_piece_\" + toString(current_piece_number)\n+        };\n+\n+\n+        auto new_engine_push_ast = task_table.engine_push_ast;\n+        if (task_table.isReplicatedTable())\n+            new_engine_push_ast = task_table.rewriteReplicatedCreateQueryToPlain();\n+\n+        /// Take columns definition from destination table, new database and table name, and new engine (non replicated variant of MergeTree)\n+        auto create_query_push_ast = rewriteCreateQueryStorage(create_query_ast, database_and_table_for_current_piece, new_engine_push_ast);\n+        String query = queryToString(create_query_push_ast);\n+\n+        LOG_INFO(log, \"Create destination tables. Query: \\n {}\", query);\n+        UInt64 shards = executeQueryOnCluster(task_table.cluster_push, query, task_cluster->settings_push, ClusterExecutionMode::ON_EACH_NODE);\n+        LOG_INFO(\n+            log,\n+            \"Destination tables {} have been created on {} shards of {}\",\n+            getQuotedTable(task_table.table_push),\n+            shards,\n+            task_table.cluster_push->getShardCount());\n+    }\n+\n+\n     /// Exit if current piece is absent on this shard. Also mark it as finished, because we will check\n     /// whether each shard have processed each partitition (and its pieces).\n     if (partition_piece.is_absent_piece)\n@@ -1349,9 +1421,9 @@ TaskStatus ClusterCopier::processPartitionPieceTaskImpl(\n         String state_finished = TaskStateWithOwner::getData(TaskState::Finished, host_id);\n         auto res = zookeeper->tryCreate(current_task_piece_status_path, state_finished, zkutil::CreateMode::Persistent);\n         if (res == Coordination::Error::ZNODEEXISTS)\n-            LOG_DEBUG(log, \"Partition {} piece {} is absent on current replica of a shard. But other replicas have already marked it as done.\", task_partition.name, current_piece_number);\n+            LOG_INFO(log, \"Partition {} piece {} is absent on current replica of a shard. But other replicas have already marked it as done.\", task_partition.name, current_piece_number);\n         if (res == Coordination::Error::ZOK)\n-            LOG_DEBUG(log, \"Partition {} piece {} is absent on current replica of a shard. Will mark it as done. Other replicas will do the same.\", task_partition.name, current_piece_number);\n+            LOG_INFO(log, \"Partition {} piece {} is absent on current replica of a shard. Will mark it as done. Other replicas will do the same.\", task_partition.name, current_piece_number);\n         return TaskStatus::Finished;\n     }\n \n@@ -1415,40 +1487,6 @@ TaskStatus ClusterCopier::processPartitionPieceTaskImpl(\n         zookeeper->create(current_task_piece_status_path, start_state, zkutil::CreateMode::Persistent);\n     }\n \n-    /// Try create table (if not exists) on each shard\n-    {\n-        /// Define push table for current partition piece\n-        auto database_and_table_for_current_piece= std::pair<String, String>(\n-                task_table.table_push.first,\n-                task_table.table_push.second + \"_piece_\" + toString(current_piece_number));\n-\n-        auto new_engine_push_ast = task_table.engine_push_ast;\n-        if (task_table.isReplicatedTable())\n-        {\n-            new_engine_push_ast = task_table.rewriteReplicatedCreateQueryToPlain();\n-        }\n-\n-        auto create_query_push_ast = rewriteCreateQueryStorage(\n-                task_shard.current_pull_table_create_query,\n-                database_and_table_for_current_piece, new_engine_push_ast);\n-\n-        create_query_push_ast->as<ASTCreateQuery &>().if_not_exists = true;\n-        String query = queryToString(create_query_push_ast);\n-\n-        LOG_DEBUG(log, \"Create destination tables. Query: {}\", query);\n-        UInt64 shards = executeQueryOnCluster(task_table.cluster_push, query, task_cluster->settings_push, PoolMode::GET_MANY);\n-        LOG_INFO(\n-            log,\n-            \"Destination tables {} have been created on {} shards of {}\",\n-            getQuotedTable(task_table.table_push),\n-            shards,\n-            task_table.cluster_push->getShardCount());\n-\n-        if (shards != task_table.cluster_push->getShardCount())\n-        {\n-            return TaskStatus::Error;\n-        }\n-    }\n \n     /// Do the copying\n     {\n@@ -1462,18 +1500,18 @@ TaskStatus ClusterCopier::processPartitionPieceTaskImpl(\n         // Select all fields\n         ASTPtr query_select_ast = get_select_query(task_shard.table_read_shard, \"*\", /*enable_splitting*/ true, inject_fault ? \"1\" : \"\");\n \n-        LOG_DEBUG(log, \"Executing SELECT query and pull from {} : {}\", task_shard.getDescription(), queryToString(query_select_ast));\n+        LOG_INFO(log, \"Executing SELECT query and pull from {} : {}\", task_shard.getDescription(), queryToString(query_select_ast));\n \n         ASTPtr query_insert_ast;\n         {\n             String query;\n-            query += \"INSERT INTO \" + getQuotedTable(split_table_for_current_piece) + \" VALUES \";\n+            query += \"INSERT INTO \" + getQuotedTable(split_table_for_current_piece) + \" FORMAT Native  \";\n \n             ParserQuery p_query(query.data() + query.size());\n             const auto & settings = getContext()->getSettingsRef();\n             query_insert_ast = parseQuery(p_query, query, settings.max_query_size, settings.max_parser_depth);\n \n-            LOG_DEBUG(log, \"Executing INSERT query: {}\", query);\n+            LOG_INFO(log, \"Executing INSERT query: {}\", query);\n         }\n \n         try\n@@ -1491,8 +1529,19 @@ TaskStatus ClusterCopier::processPartitionPieceTaskImpl(\n                 BlockIO io_select = InterpreterFactory::get(query_select_ast, context_select)->execute();\n                 BlockIO io_insert = InterpreterFactory::get(query_insert_ast, context_insert)->execute();\n \n-                input = io_select.getInputStream();\n+                auto pure_input = io_select.getInputStream();\n                 output = io_insert.out;\n+\n+                /// Add converting actions to make it possible to copy blocks with slightly different schema\n+                const auto & select_block = pure_input->getHeader();\n+                const auto & insert_block = output->getHeader();\n+                auto actions_dag = ActionsDAG::makeConvertingActions(\n+                        select_block.getColumnsWithTypeAndName(),\n+                        insert_block.getColumnsWithTypeAndName(),\n+                        ActionsDAG::MatchColumnsMode::Position);\n+                auto actions = std::make_shared<ExpressionActions>(actions_dag, ExpressionActionsSettings::fromContext(getContext()));\n+\n+                input = std::make_shared<ExpressionBlockInputStream>(pure_input, actions);\n             }\n \n             /// Fail-fast optimization to abort copying when the current clean state expires\n@@ -1600,7 +1649,7 @@ void ClusterCopier::dropLocalTableIfExists(const DatabaseAndTableName & table_na\n \n void ClusterCopier::dropHelpingTablesByPieceNumber(const TaskTable & task_table, size_t current_piece_number)\n {\n-    LOG_DEBUG(log, \"Removing helping tables piece {}\", current_piece_number);\n+    LOG_INFO(log, \"Removing helping tables piece {}\", current_piece_number);\n \n     DatabaseAndTableName original_table = task_table.table_push;\n     DatabaseAndTableName helping_table\n@@ -1611,17 +1660,17 @@ void ClusterCopier::dropHelpingTablesByPieceNumber(const TaskTable & task_table,\n     const ClusterPtr & cluster_push = task_table.cluster_push;\n     Settings settings_push = task_cluster->settings_push;\n \n-    LOG_DEBUG(log, \"Execute distributed DROP TABLE: {}\", query);\n+    LOG_INFO(log, \"Execute distributed DROP TABLE: {}\", query);\n \n     /// We have to drop partition_piece on each replica\n-    UInt64 num_nodes = executeQueryOnCluster(cluster_push, query, settings_push, PoolMode::GET_MANY, ClusterExecutionMode::ON_EACH_NODE);\n+    UInt64 num_nodes = executeQueryOnCluster(cluster_push, query, settings_push, ClusterExecutionMode::ON_EACH_NODE);\n \n     LOG_INFO(log, \"DROP TABLE query was successfully executed on {} nodes.\", toString(num_nodes));\n }\n \n void ClusterCopier::dropHelpingTables(const TaskTable & task_table)\n {\n-    LOG_DEBUG(log, \"Removing helping tables\");\n+    LOG_INFO(log, \"Removing helping tables\");\n     for (size_t current_piece_number = 0; current_piece_number < task_table.number_of_splits; ++current_piece_number)\n     {\n         dropHelpingTablesByPieceNumber(task_table, current_piece_number);\n@@ -1630,7 +1679,7 @@ void ClusterCopier::dropHelpingTables(const TaskTable & task_table)\n \n void ClusterCopier::dropParticularPartitionPieceFromAllHelpingTables(const TaskTable & task_table, const String & partition_name)\n {\n-    LOG_DEBUG(log, \"Try drop partition partition from all helping tables.\");\n+    LOG_INFO(log, \"Try drop partition partition from all helping tables.\");\n     for (size_t current_piece_number = 0; current_piece_number < task_table.number_of_splits; ++current_piece_number)\n     {\n         DatabaseAndTableName original_table = task_table.table_push;\n@@ -1641,17 +1690,16 @@ void ClusterCopier::dropParticularPartitionPieceFromAllHelpingTables(const TaskT\n         const ClusterPtr & cluster_push = task_table.cluster_push;\n         Settings settings_push = task_cluster->settings_push;\n \n-        LOG_DEBUG(log, \"Execute distributed DROP PARTITION: {}\", query);\n+        LOG_INFO(log, \"Execute distributed DROP PARTITION: {}\", query);\n         /// We have to drop partition_piece on each replica\n         UInt64 num_nodes = executeQueryOnCluster(\n                 cluster_push, query,\n                 settings_push,\n-                PoolMode::GET_MANY,\n                 ClusterExecutionMode::ON_EACH_NODE);\n \n         LOG_INFO(log, \"DROP PARTITION query was successfully executed on {} nodes.\", toString(num_nodes));\n     }\n-    LOG_DEBUG(log, \"All helping tables dropped partition {}\", partition_name);\n+    LOG_INFO(log, \"All helping tables dropped partition {}\", partition_name);\n }\n \n String ClusterCopier::getRemoteCreateTable(const DatabaseAndTableName & table, Connection & connection, const Settings & settings)\n@@ -1666,6 +1714,7 @@ String ClusterCopier::getRemoteCreateTable(const DatabaseAndTableName & table, C\n     return typeid_cast<const ColumnString &>(*block.safeGetByPosition(0).column).getDataAt(0).toString();\n }\n \n+\n ASTPtr ClusterCopier::getCreateTableForPullShard(const ConnectionTimeouts & timeouts, TaskShard & task_shard)\n {\n     /// Fetch and parse (possibly) new definition\n@@ -1680,6 +1729,7 @@ ASTPtr ClusterCopier::getCreateTableForPullShard(const ConnectionTimeouts & time\n     return parseQuery(parser_create_query, create_query_pull_str, settings.max_query_size, settings.max_parser_depth);\n }\n \n+\n /// If it is implicitly asked to create split Distributed table for certain piece on current shard, we will do it.\n void ClusterCopier::createShardInternalTables(const ConnectionTimeouts & timeouts,\n         TaskShard & task_shard, bool create_split)\n@@ -1709,7 +1759,9 @@ void ClusterCopier::createShardInternalTables(const ConnectionTimeouts & timeout\n \n     auto storage_shard_ast = createASTStorageDistributed(shard_read_cluster_name, task_table.table_pull.first, task_table.table_pull.second);\n \n-    auto create_query_ast = removeAliasColumnsFromCreateQuery(task_shard.current_pull_table_create_query);\n+    auto create_query_ast = removeAliasMaterializedAndTTLColumnsFromCreateQuery(\n+        task_shard.current_pull_table_create_query,\n+        task_table.allow_to_copy_alias_and_materialized_columns);\n \n     auto create_table_pull_ast = rewriteCreateQueryStorage(create_query_ast, task_shard.table_read_shard, storage_shard_ast);\n     dropAndCreateLocalTable(create_table_pull_ast);\n@@ -1768,7 +1820,7 @@ std::set<String> ClusterCopier::getShardPartitions(const ConnectionTimeouts & ti\n     const auto & settings = getContext()->getSettingsRef();\n     ASTPtr query_ast = parseQuery(parser_query, query, settings.max_query_size, settings.max_parser_depth);\n \n-    LOG_DEBUG(log, \"Computing destination partition set, executing query: {}\", query);\n+    LOG_INFO(log, \"Computing destination partition set, executing query: \\n {}\", query);\n \n     auto local_context = Context::createCopy(context);\n     local_context->setSettings(task_cluster->settings_pull);\n@@ -1787,7 +1839,7 @@ std::set<String> ClusterCopier::getShardPartitions(const ConnectionTimeouts & ti\n         }\n     }\n \n-    LOG_DEBUG(log, \"There are {} destination partitions in shard {}\", res.size(), task_shard.getDescription());\n+    LOG_INFO(log, \"There are {} destination partitions in shard {}\", res.size(), task_shard.getDescription());\n \n     return res;\n }\n@@ -1799,21 +1851,22 @@ bool ClusterCopier::checkShardHasPartition(const ConnectionTimeouts & timeouts,\n \n     TaskTable & task_table = task_shard.task_table;\n \n-    std::string query = \"SELECT 1 FROM \" + getQuotedTable(task_shard.table_read_shard)\n-                        + \" WHERE (\" + queryToString(task_table.engine_push_partition_key_ast) +\n-                        \" = (\" + partition_quoted_name + \" AS partition_key))\";\n-\n+    WriteBufferFromOwnString ss;\n+    ss << \"SELECT 1 FROM \" << getQuotedTable(task_shard.table_read_shard);\n+    ss << \" WHERE (\" << queryToString(task_table.engine_push_partition_key_ast);\n+    ss << \" = (\" + partition_quoted_name << \" AS partition_key))\";\n     if (!task_table.where_condition_str.empty())\n-        query += \" AND (\" + task_table.where_condition_str + \")\";\n-\n-    query += \" LIMIT 1\";\n-\n-    LOG_DEBUG(log, \"Checking shard {} for partition {} existence, executing query: {}\", task_shard.getDescription(), partition_quoted_name, query);\n+        ss << \" AND (\" << task_table.where_condition_str << \")\";\n+    ss << \" LIMIT 1\";\n+    auto query = ss.str();\n \n     ParserQuery parser_query(query.data() + query.size());\n     const auto & settings = getContext()->getSettingsRef();\n     ASTPtr query_ast = parseQuery(parser_query, query, settings.max_query_size, settings.max_parser_depth);\n \n+    LOG_INFO(log, \"Checking shard {} for partition {} existence, executing query: \\n {}\",\n+        task_shard.getDescription(), partition_quoted_name, query_ast->formatForErrorMessage());\n+\n     auto local_context = Context::createCopy(context);\n     local_context->setSettings(task_cluster->settings_pull);\n     return InterpreterFactory::get(query_ast, local_context)->execute().getInputStream()->read().rows() != 0;\n@@ -1847,7 +1900,7 @@ bool ClusterCopier::checkPresentPartitionPiecesOnCurrentShard(const ConnectionTi\n \n     query += \" LIMIT 1\";\n \n-    LOG_DEBUG(log, \"Checking shard {} for partition {} piece {} existence, executing query: {}\", task_shard.getDescription(), partition_quoted_name, std::to_string(current_piece_number), query);\n+    LOG_INFO(log, \"Checking shard {} for partition {} piece {} existence, executing query: \\n \\u001b[36m {}\", task_shard.getDescription(), partition_quoted_name, std::to_string(current_piece_number), query);\n \n     ParserQuery parser_query(query.data() + query.size());\n     const auto & settings = getContext()->getSettingsRef();\n@@ -1857,12 +1910,13 @@ bool ClusterCopier::checkPresentPartitionPiecesOnCurrentShard(const ConnectionTi\n     local_context->setSettings(task_cluster->settings_pull);\n     auto result = InterpreterFactory::get(query_ast, local_context)->execute().getInputStream()->read().rows();\n     if (result != 0)\n-        LOG_DEBUG(log, \"Partition {} piece number {} is PRESENT on shard {}\", partition_quoted_name, std::to_string(current_piece_number), task_shard.getDescription());\n+        LOG_INFO(log, \"Partition {} piece number {} is PRESENT on shard {}\", partition_quoted_name, std::to_string(current_piece_number), task_shard.getDescription());\n     else\n-        LOG_DEBUG(log, \"Partition {} piece number {} is ABSENT on shard {}\", partition_quoted_name, std::to_string(current_piece_number), task_shard.getDescription());\n+        LOG_INFO(log, \"Partition {} piece number {} is ABSENT on shard {}\", partition_quoted_name, std::to_string(current_piece_number), task_shard.getDescription());\n     return result != 0;\n }\n \n+\n /** Executes simple query (without output streams, for example DDL queries) on each shard of the cluster\n   * Returns number of shards for which at least one replica executed query successfully\n   */\n@@ -1870,112 +1924,69 @@ UInt64 ClusterCopier::executeQueryOnCluster(\n         const ClusterPtr & cluster,\n         const String & query,\n         const Settings & current_settings,\n-        PoolMode pool_mode,\n-        ClusterExecutionMode execution_mode,\n-        UInt64 max_successful_executions_per_shard) const\n+        ClusterExecutionMode execution_mode) const\n {\n-    auto num_shards = cluster->getShardsInfo().size();\n-    std::vector<UInt64> per_shard_num_successful_replicas(num_shards, 0);\n-\n-    ParserQuery p_query(query.data() + query.size());\n-    ASTPtr query_ast = parseQuery(p_query, query, current_settings.max_query_size, current_settings.max_parser_depth);\n-\n-    /// We will have to execute query on each replica of a shard.\n+    ClusterPtr cluster_for_query = cluster;\n     if (execution_mode == ClusterExecutionMode::ON_EACH_NODE)\n-        max_successful_executions_per_shard = 0;\n+        cluster_for_query = cluster->getClusterWithReplicasAsShards(current_settings);\n \n-    std::atomic<size_t> origin_replicas_number = 0;\n+    std::vector<std::shared_ptr<Connection>> connections;\n+    connections.reserve(cluster->getShardCount());\n \n-    /// We need to execute query on one replica at least\n-    auto do_for_shard = [&] (UInt64 shard_index, Settings shard_settings)\n-    {\n-        setThreadName(\"QueryForShard\");\n-\n-        const Cluster::ShardInfo & shard = cluster->getShardsInfo().at(shard_index);\n-        UInt64 & num_successful_executions = per_shard_num_successful_replicas.at(shard_index);\n-        num_successful_executions = 0;\n+    std::atomic<UInt64> successfully_executed = 0;\n \n-        auto increment_and_check_exit = [&] () -> bool\n-        {\n-            ++num_successful_executions;\n-            return max_successful_executions_per_shard && num_successful_executions >= max_successful_executions_per_shard;\n-        };\n-\n-        UInt64 num_replicas = cluster->getShardsAddresses().at(shard_index).size();\n-\n-        origin_replicas_number += num_replicas;\n-        UInt64 num_local_replicas = shard.getLocalNodeCount();\n-        UInt64 num_remote_replicas = num_replicas - num_local_replicas;\n-\n-        /// In that case we don't have local replicas, but do it just in case\n-        for (UInt64 i = 0; i < num_local_replicas; ++i)\n-        {\n-            auto interpreter = InterpreterFactory::get(query_ast, getContext());\n-            interpreter->execute();\n-\n-            if (increment_and_check_exit())\n-                return;\n-        }\n-\n-        /// Will try to make as many as possible queries\n-        if (shard.hasRemoteConnections())\n+    for (const auto & replicas : cluster_for_query->getShardsAddresses())\n+    {\n+        for (const auto & node : replicas)\n         {\n-            shard_settings.max_parallel_replicas = num_remote_replicas ? num_remote_replicas : 1;\n+            try\n+            {\n+                connections.emplace_back(std::make_shared<Connection>(\n+                    node.host_name, node.port, node.default_database,\n+                    node.user, node.password, node.cluster, node.cluster_secret,\n+                    \"ClusterCopier\", node.compression, node.secure\n+                ));\n \n-            auto timeouts = ConnectionTimeouts::getTCPTimeoutsWithFailover(shard_settings).getSaturated(shard_settings.max_execution_time);\n-            auto connections = shard.pool->getMany(timeouts, &shard_settings, pool_mode);\n+                /// We execute only Alter, Create and Drop queries.\n+                const auto header = Block{};\n \n-            auto shard_context = Context::createCopy(context);\n-            shard_context->setSettings(shard_settings);\n-\n-            for (auto & connection : connections)\n-            {\n-                if (connection.isNull())\n-                    continue;\n+                /// For unknown reason global context is passed to IStorage::read() method\n+                /// So, task_identifier is passed as constructor argument. It is more obvious.\n+                auto remote_query_executor = std::make_shared<RemoteQueryExecutor>(\n+                        *connections.back(), query, header, getContext(),\n+                        /*throttler=*/nullptr, Scalars(), Tables(), QueryProcessingStage::Complete);\n \n                 try\n                 {\n-                    /// CREATE TABLE and DROP PARTITION queries return empty block\n-                    RemoteBlockInputStream stream{*connection, query, Block{}, shard_context};\n-                    NullBlockOutputStream output{Block{}};\n-                    copyData(stream, output);\n-\n-                    if (increment_and_check_exit())\n-                        return;\n+                    remote_query_executor->sendQuery();\n                 }\n-                catch (const Exception &)\n+                catch (...)\n                 {\n-                    LOG_INFO(log, getCurrentExceptionMessage(false, true));\n+                    LOG_WARNING(log, \"Seemns like node with address {} is unreachable.\", node.host_name);\n+                    continue;\n                 }\n-            }\n-        }\n-    };\n \n-    {\n-        ThreadPool thread_pool(std::min<UInt64>(num_shards, getNumberOfPhysicalCPUCores()));\n-\n-        for (UInt64 shard_index = 0; shard_index < num_shards; ++shard_index)\n-            thread_pool.scheduleOrThrowOnError([=, shard_settings = current_settings] { do_for_shard(shard_index, std::move(shard_settings)); });\n-\n-        thread_pool.wait();\n-    }\n-\n-    UInt64 successful_nodes = 0;\n-    for (UInt64 num_replicas : per_shard_num_successful_replicas)\n-    {\n-        if (execution_mode == ClusterExecutionMode::ON_EACH_NODE)\n-            successful_nodes += num_replicas;\n-        else\n-            /// Count only successful shards\n-            successful_nodes += (num_replicas > 0);\n-    }\n+                while (true)\n+                {\n+                    auto block = remote_query_executor->read();\n+                    if (!block)\n+                        break;\n+                }\n \n-    if (execution_mode == ClusterExecutionMode::ON_EACH_NODE && successful_nodes != origin_replicas_number)\n-    {\n-        LOG_INFO(log, \"There was an error while executing ALTER on each node. Query was executed on {} nodes. But had to be executed on {}\", toString(successful_nodes), toString(origin_replicas_number.load()));\n+                remote_query_executor->finish();\n+                ++successfully_executed;\n+                break;\n+            }\n+            catch (...)\n+            {\n+                LOG_WARNING(log, \"An error occurred while processing query : \\n {}\", query);\n+                tryLogCurrentException(log);\n+                continue;\n+            }\n+        }\n     }\n \n-    return successful_nodes;\n+    return successfully_executed.load();\n }\n \n }\ndiff --git a/programs/copier/ClusterCopier.h b/programs/copier/ClusterCopier.h\nindex 085fa2ece063..387b089724ad 100644\n--- a/programs/copier/ClusterCopier.h\n+++ b/programs/copier/ClusterCopier.h\n@@ -18,12 +18,13 @@ class ClusterCopier : WithMutableContext\n     ClusterCopier(const String & task_path_,\n                   const String & host_id_,\n                   const String & proxy_database_name_,\n-                  ContextMutablePtr context_)\n+                  ContextMutablePtr context_,\n+                  Poco::Logger * log_)\n             : WithMutableContext(context_),\n             task_zookeeper_path(task_path_),\n             host_id(host_id_),\n             working_database_name(proxy_database_name_),\n-            log(&Poco::Logger::get(\"ClusterCopier\")) {}\n+            log(log_) {}\n \n     void init();\n \n@@ -117,14 +118,14 @@ class ClusterCopier : WithMutableContext\n     TaskStatus tryMoveAllPiecesToDestinationTable(const TaskTable & task_table, const String & partition_name);\n \n     /// Removes MATERIALIZED and ALIAS columns from create table query\n-    static ASTPtr removeAliasColumnsFromCreateQuery(const ASTPtr & query_ast);\n+    static ASTPtr removeAliasMaterializedAndTTLColumnsFromCreateQuery(const ASTPtr & query_ast, bool allow_to_copy_alias_and_materialized_columns);\n \n     bool tryDropPartitionPiece(ShardPartition & task_partition, size_t current_piece_number,\n             const zkutil::ZooKeeperPtr & zookeeper, const CleanStateClock & clean_state_clock);\n \n     static constexpr UInt64 max_table_tries = 3;\n     static constexpr UInt64 max_shard_partition_tries = 3;\n-    static constexpr UInt64 max_shard_partition_piece_tries_for_alter = 3;\n+    static constexpr UInt64 max_shard_partition_piece_tries_for_alter = 10;\n \n     bool tryProcessTable(const ConnectionTimeouts & timeouts, TaskTable & task_table);\n \n@@ -189,9 +190,7 @@ class ClusterCopier : WithMutableContext\n             const ClusterPtr & cluster,\n             const String & query,\n             const Settings & current_settings,\n-            PoolMode pool_mode = PoolMode::GET_ALL,\n-            ClusterExecutionMode execution_mode = ClusterExecutionMode::ON_EACH_SHARD,\n-            UInt64 max_successful_executions_per_shard = 0) const;\n+            ClusterExecutionMode execution_mode = ClusterExecutionMode::ON_EACH_SHARD) const;\n \n private:\n     String task_zookeeper_path;\n@@ -208,7 +207,6 @@ class ClusterCopier : WithMutableContext\n \n     ConfigurationPtr task_cluster_initial_config;\n     ConfigurationPtr task_cluster_current_config;\n-    Coordination::Stat task_description_current_stat{};\n \n     std::unique_ptr<TaskCluster> task_cluster;\n \ndiff --git a/programs/copier/ClusterCopierApp.cpp b/programs/copier/ClusterCopierApp.cpp\nindex 8925ab63f99b..7a0b81309b09 100644\n--- a/programs/copier/ClusterCopierApp.cpp\n+++ b/programs/copier/ClusterCopierApp.cpp\n@@ -22,8 +22,9 @@ void ClusterCopierApp::initialize(Poco::Util::Application & self)\n \n     config_xml_path = config().getString(\"config-file\");\n     task_path = config().getString(\"task-path\");\n-    log_level = config().getString(\"log-level\", \"trace\");\n+    log_level = config().getString(\"log-level\", \"info\");\n     is_safe_mode = config().has(\"safe-mode\");\n+    is_status_mode = config().has(\"status\");\n     if (config().has(\"copy-fault-probability\"))\n         copy_fault_probability = std::max(std::min(config().getDouble(\"copy-fault-probability\"), 1.0), 0.0);\n     if (config().has(\"move-fault-probability\"))\n@@ -97,6 +98,7 @@ void ClusterCopierApp::defineOptions(Poco::Util::OptionSet & options)\n                           .argument(\"base-dir\").binding(\"base-dir\"));\n     options.addOption(Poco::Util::Option(\"experimental-use-sample-offset\", \"\", \"Use SAMPLE OFFSET query instead of cityHash64(PRIMARY KEY) % n == k\")\n                           .argument(\"experimental-use-sample-offset\").binding(\"experimental-use-sample-offset\"));\n+    options.addOption(Poco::Util::Option(\"status\", \"\", \"Get for status for current execution\").binding(\"status\"));\n \n     using Me = std::decay_t<decltype(*this)>;\n     options.addOption(Poco::Util::Option(\"help\", \"\", \"produce this help message\").binding(\"help\")\n@@ -106,6 +108,25 @@ void ClusterCopierApp::defineOptions(Poco::Util::OptionSet & options)\n \n void ClusterCopierApp::mainImpl()\n {\n+    /// Status command\n+    {\n+        if (is_status_mode)\n+        {\n+            SharedContextHolder shared_context = Context::createShared();\n+            auto context = Context::createGlobal(shared_context.get());\n+            context->makeGlobalContext();\n+            SCOPE_EXIT_SAFE(context->shutdown());\n+\n+            auto zookeeper = context->getZooKeeper();\n+            auto status_json = zookeeper->get(task_path + \"/status\");\n+\n+            LOG_INFO(&logger(), \"{}\", status_json);\n+            std::cout << status_json << std::endl;\n+\n+            context->resetZooKeeper();\n+            return;\n+        }\n+    }\n     StatusFile status_file(process_path + \"/status\", StatusFile::write_full_info);\n     ThreadStatus thread_status;\n \n@@ -136,7 +157,7 @@ void ClusterCopierApp::mainImpl()\n     /// Initialize query scope just in case.\n     CurrentThread::QueryScope query_scope(context);\n \n-    auto copier = std::make_unique<ClusterCopier>(task_path, host_id, default_database, context);\n+    auto copier = std::make_unique<ClusterCopier>(task_path, host_id, default_database, context, log);\n     copier->setSafeMode(is_safe_mode);\n     copier->setCopyFaultProbability(copy_fault_probability);\n     copier->setMoveFaultProbability(move_fault_probability);\ndiff --git a/programs/copier/ClusterCopierApp.h b/programs/copier/ClusterCopierApp.h\nindex 257b10cf1961..cce07e338c05 100644\n--- a/programs/copier/ClusterCopierApp.h\n+++ b/programs/copier/ClusterCopierApp.h\n@@ -76,8 +76,9 @@ class ClusterCopierApp : public BaseDaemon\n \n     std::string config_xml_path;\n     std::string task_path;\n-    std::string log_level = \"trace\";\n+    std::string log_level = \"info\";\n     bool is_safe_mode = false;\n+    bool is_status_mode = false;\n     double copy_fault_probability = 0.0;\n     double move_fault_probability = 0.0;\n     bool is_help = false;\ndiff --git a/programs/copier/StatusAccumulator.h b/programs/copier/StatusAccumulator.h\nnew file mode 100644\nindex 000000000000..6e20e3dc95db\n--- /dev/null\n+++ b/programs/copier/StatusAccumulator.h\n@@ -0,0 +1,65 @@\n+#pragma once\n+\n+\n+#include <Poco/JSON/Parser.h>\n+#include <Poco/JSON/JSON.h>\n+#include <Poco/JSON/Object.h>\n+#include <Poco/JSON/Stringifier.h>\n+\n+#include <unordered_map>\n+#include <memory>\n+#include <string>\n+#include <iostream>\n+\n+namespace DB\n+{\n+\n+class StatusAccumulator\n+{\n+    public:\n+        struct TableStatus\n+        {\n+            size_t all_partitions_count;\n+            size_t processed_partitions_count;\n+        };\n+\n+        using Map = std::unordered_map<std::string, TableStatus>;\n+        using MapPtr = std::shared_ptr<Map>;\n+\n+        static MapPtr fromJSON(std::string state_json)\n+        {\n+            Poco::JSON::Parser parser;\n+            auto state = parser.parse(state_json).extract<Poco::JSON::Object::Ptr>();\n+            MapPtr result_ptr = std::make_shared<Map>();\n+            for (const auto & table_name : state->getNames())\n+            {\n+                auto table_status_json = state->getValue<std::string>(table_name);\n+                auto table_status = parser.parse(table_status_json).extract<Poco::JSON::Object::Ptr>();\n+                /// Map entry will be created if it is absent\n+                auto & map_table_status = (*result_ptr)[table_name];\n+                map_table_status.all_partitions_count += table_status->getValue<size_t>(\"all_partitions_count\");\n+                map_table_status.processed_partitions_count += table_status->getValue<size_t>(\"processed_partitions_count\");\n+            }\n+            return result_ptr;\n+        }\n+\n+        static std::string serializeToJSON(MapPtr statuses)\n+        {\n+            Poco::JSON::Object result_json;\n+            for (const auto & [table_name, table_status] : *statuses)\n+            {\n+                Poco::JSON::Object status_json;\n+                status_json.set(\"all_partitions_count\", table_status.all_partitions_count);\n+                status_json.set(\"processed_partitions_count\", table_status.processed_partitions_count);\n+\n+                result_json.set(table_name, status_json);\n+            }\n+            std::ostringstream oss;     // STYLE_CHECK_ALLOW_STD_STRING_STREAM\n+            oss.exceptions(std::ios::failbit);\n+            Poco::JSON::Stringifier::stringify(result_json, oss);\n+            auto result = oss.str();\n+            return result;\n+        }\n+};\n+\n+}\ndiff --git a/programs/copier/TaskCluster.h b/programs/copier/TaskCluster.h\nindex 1a50597d07fe..7d8f01ba15fc 100644\n--- a/programs/copier/TaskCluster.h\n+++ b/programs/copier/TaskCluster.h\n@@ -77,6 +77,8 @@ inline void DB::TaskCluster::reloadSettings(const Poco::Util::AbstractConfigurat\n     if (config.has(prefix + \"settings\"))\n         settings_common.loadSettingsFromConfig(prefix + \"settings\", config);\n \n+    settings_common.prefer_localhost_replica = 0;\n+\n     settings_pull = settings_common;\n     if (config.has(prefix + \"settings_pull\"))\n         settings_pull.loadSettingsFromConfig(prefix + \"settings_pull\", config);\n@@ -92,11 +94,15 @@ inline void DB::TaskCluster::reloadSettings(const Poco::Util::AbstractConfigurat\n \n     /// Override important settings\n     settings_pull.readonly = 1;\n-    settings_push.insert_distributed_sync = 1;\n+    settings_pull.prefer_localhost_replica = false;\n+    settings_push.insert_distributed_sync = true;\n+    settings_push.prefer_localhost_replica = false;\n+\n     set_default_value(settings_pull.load_balancing, LoadBalancing::NEAREST_HOSTNAME);\n     set_default_value(settings_pull.max_threads, 1);\n     set_default_value(settings_pull.max_block_size, 8192UL);\n     set_default_value(settings_pull.preferred_block_size_bytes, 0);\n+\n     set_default_value(settings_push.insert_distributed_timeout, 0);\n     set_default_value(settings_push.replication_alter_partitions_sync, 2);\n }\ndiff --git a/programs/copier/TaskTableAndShard.h b/programs/copier/TaskTableAndShard.h\nindex 4f5bfb443e6b..30b057440bbd 100644\n--- a/programs/copier/TaskTableAndShard.h\n+++ b/programs/copier/TaskTableAndShard.h\n@@ -36,27 +36,33 @@ struct TaskTable\n \n     String getPartitionAttachIsDonePath(const String & partition_name) const;\n \n-    String getPartitionPiecePath(const String & partition_name, const size_t piece_number) const;\n+    String getPartitionPiecePath(const String & partition_name, size_t piece_number) const;\n \n     String getCertainPartitionIsDirtyPath(const String & partition_name) const;\n \n-    String getCertainPartitionPieceIsDirtyPath(const String & partition_name, const size_t piece_number) const;\n+    String getCertainPartitionPieceIsDirtyPath(const String & partition_name, size_t piece_number) const;\n \n     String getCertainPartitionIsCleanedPath(const String & partition_name) const;\n \n-    String getCertainPartitionPieceIsCleanedPath(const String & partition_name, const size_t piece_number) const;\n+    String getCertainPartitionPieceIsCleanedPath(const String & partition_name, size_t piece_number) const;\n \n     String getCertainPartitionTaskStatusPath(const String & partition_name) const;\n \n-    String getCertainPartitionPieceTaskStatusPath(const String & partition_name, const size_t piece_number) const;\n-\n+    String getCertainPartitionPieceTaskStatusPath(const String & partition_name, size_t piece_number) const;\n \n     bool isReplicatedTable() const { return is_replicated_table; }\n \n+    /// These nodes are used for check-status option\n+    String getStatusAllPartitionCount() const;\n+    String getStatusProcessedPartitionsCount() const;\n+\n     /// Partitions will be split into number-of-splits pieces.\n     /// Each piece will be copied independently. (10 by default)\n     size_t number_of_splits;\n \n+    bool allow_to_copy_alias_and_materialized_columns{false};\n+    bool allow_to_drop_target_partitions{false};\n+\n     String name_in_config;\n \n     /// Used as task ID\n@@ -83,7 +89,7 @@ struct TaskTable\n     String engine_push_zk_path;\n     bool is_replicated_table;\n \n-    ASTPtr rewriteReplicatedCreateQueryToPlain();\n+    ASTPtr rewriteReplicatedCreateQueryToPlain() const;\n \n     /*\n      * A Distributed table definition used to split data\n@@ -181,6 +187,7 @@ struct TaskShard\n \n     /// Last CREATE TABLE query of the table of the shard\n     ASTPtr current_pull_table_create_query;\n+    ASTPtr current_push_table_create_query;\n \n     /// Internal distributed tables\n     DatabaseAndTableName table_read_shard;\n@@ -242,6 +249,16 @@ inline String TaskTable::getCertainPartitionPieceTaskStatusPath(const String & p\n     return getPartitionPiecePath(partition_name, piece_number) + \"/shards\";\n }\n \n+inline String TaskTable::getStatusAllPartitionCount() const\n+{\n+    return task_cluster.task_zookeeper_path + \"/status/all_partitions_count\";\n+}\n+\n+inline String TaskTable::getStatusProcessedPartitionsCount() const\n+{\n+    return task_cluster.task_zookeeper_path + \"/status/processed_partitions_count\";\n+}\n+\n inline TaskTable::TaskTable(TaskCluster & parent, const Poco::Util::AbstractConfiguration & config,\n                      const String & prefix_, const String & table_key)\n         : task_cluster(parent)\n@@ -250,7 +267,10 @@ inline TaskTable::TaskTable(TaskCluster & parent, const Poco::Util::AbstractConf\n \n     name_in_config = table_key;\n \n-    number_of_splits = config.getUInt64(table_prefix + \"number_of_splits\", 10);\n+    number_of_splits = config.getUInt64(table_prefix + \"number_of_splits\", 3);\n+\n+    allow_to_copy_alias_and_materialized_columns = config.getBool(table_prefix + \"allow_to_copy_alias_and_materialized_columns\", false);\n+    allow_to_drop_target_partitions = config.getBool(table_prefix + \"allow_to_drop_target_partitions\", false);\n \n     cluster_pull_name = config.getString(table_prefix + \"cluster_pull\");\n     cluster_push_name = config.getString(table_prefix + \"cluster_push\");\n@@ -343,7 +363,7 @@ inline void TaskTable::initShards(RandomEngine && random_engine)\n     std::uniform_int_distribution<UInt8> get_urand(0, std::numeric_limits<UInt8>::max());\n \n     // Compute the priority\n-    for (auto & shard_info : cluster_pull->getShardsInfo())\n+    for (const auto & shard_info : cluster_pull->getShardsInfo())\n     {\n         TaskShardPtr task_shard = std::make_shared<TaskShard>(*this, shard_info);\n         const auto & replicas = cluster_pull->getShardsAddresses().at(task_shard->indexInCluster());\n@@ -369,7 +389,7 @@ inline void TaskTable::initShards(RandomEngine && random_engine)\n     local_shards.assign(all_shards.begin(), it_first_remote);\n }\n \n-inline ASTPtr TaskTable::rewriteReplicatedCreateQueryToPlain()\n+inline ASTPtr TaskTable::rewriteReplicatedCreateQueryToPlain() const\n {\n     ASTPtr prev_engine_push_ast = engine_push_ast->clone();\n \n@@ -383,9 +403,15 @@ inline ASTPtr TaskTable::rewriteReplicatedCreateQueryToPlain()\n     {\n         auto & replicated_table_arguments = new_engine_ast.arguments->children;\n \n-        /// Delete first two arguments of Replicated...MergeTree() table.\n-        replicated_table_arguments.erase(replicated_table_arguments.begin());\n-        replicated_table_arguments.erase(replicated_table_arguments.begin());\n+\n+        /// In some cases of Atomic database engine usage ReplicatedMergeTree tables\n+        /// could be created without arguments.\n+        if (!replicated_table_arguments.empty())\n+        {\n+            /// Delete first two arguments of Replicated...MergeTree() table.\n+            replicated_table_arguments.erase(replicated_table_arguments.begin());\n+            replicated_table_arguments.erase(replicated_table_arguments.begin());\n+        }\n     }\n \n     return new_storage_ast.clone();\n@@ -400,7 +426,7 @@ inline String DB::TaskShard::getDescription() const\n \n inline String DB::TaskShard::getHostNameExample() const\n {\n-    auto & replicas = task_table.cluster_pull->getShardsAddresses().at(indexInCluster());\n+    const auto & replicas = task_table.cluster_pull->getShardsAddresses().at(indexInCluster());\n     return replicas.at(0).readableString();\n }\n \ndiff --git a/src/Storages/StorageDistributed.cpp b/src/Storages/StorageDistributed.cpp\nindex 256875454dbe..d42fa33166f6 100644\n--- a/src/Storages/StorageDistributed.cpp\n+++ b/src/Storages/StorageDistributed.cpp\n@@ -1042,7 +1042,7 @@ ClusterPtr StorageDistributed::skipUnusedShards(\n \n     if (!limit)\n     {\n-        LOG_TRACE(log,\n+        LOG_DEBUG(log,\n             \"Number of values for sharding key exceeds optimize_skip_unused_shards_limit={}, \"\n             \"try to increase it, but note that this may increase query processing time.\",\n             local_context->getSettingsRef().optimize_skip_unused_shards_limit);\n",
  "test_patch": "diff --git a/tests/integration/helpers/cluster.py b/tests/integration/helpers/cluster.py\nindex ee9cdd3550a2..4ddab2a2e7a0 100644\n--- a/tests/integration/helpers/cluster.py\n+++ b/tests/integration/helpers/cluster.py\n@@ -2196,6 +2196,7 @@ def create_dir(self, destroy_dir=True):\n             odbc_bridge_volume = \"- \" + self.odbc_bridge_bin_path + \":/usr/share/clickhouse-odbc-bridge_fresh\"\n             library_bridge_volume = \"- \" + self.library_bridge_bin_path + \":/usr/share/clickhouse-library-bridge_fresh\"\n \n+\n         with open(self.docker_compose_path, 'w') as docker_compose:\n             docker_compose.write(DOCKER_COMPOSE_TEMPLATE.format(\n                 image=self.image,\ndiff --git a/tests/integration/test_cluster_copier/configs/conf.d/clusters_trivial.xml b/tests/integration/test_cluster_copier/configs/conf.d/clusters_trivial.xml\nnew file mode 100644\nindex 000000000000..dea9c119f2bf\n--- /dev/null\n+++ b/tests/integration/test_cluster_copier/configs/conf.d/clusters_trivial.xml\n@@ -0,0 +1,21 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+    <remote_servers>\n+        <source_trivial_cluster>\n+            <shard>\n+                <replica>\n+                    <host>first_trivial</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+        </source_trivial_cluster>\n+        <destination_trivial_cluster>\n+            <shard>\n+                <replica>\n+                    <host>second_trivial</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+        </destination_trivial_cluster>\n+    </remote_servers>\n+</yandex>\ndiff --git a/tests/integration/test_cluster_copier/configs/config-copier.xml b/tests/integration/test_cluster_copier/configs/config-copier.xml\nindex 12640034104b..6db67efed6f1 100644\n--- a/tests/integration/test_cluster_copier/configs/config-copier.xml\n+++ b/tests/integration/test_cluster_copier/configs/config-copier.xml\n@@ -1,6 +1,6 @@\n <yandex>\n     <logger>\n-        <level>trace</level>\n+        <level>information</level>\n         <log>/var/log/clickhouse-server/copier/log.log</log>\n         <errorlog>/var/log/clickhouse-server/copier/log.err.log</errorlog>\n         <size>1000M</size>\ndiff --git a/tests/integration/test_cluster_copier/configs_three_nodes/conf.d/clusters.xml b/tests/integration/test_cluster_copier/configs_three_nodes/conf.d/clusters.xml\nnew file mode 100644\nindex 000000000000..6993a7ad7fd8\n--- /dev/null\n+++ b/tests/integration/test_cluster_copier/configs_three_nodes/conf.d/clusters.xml\n@@ -0,0 +1,28 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+    <remote_servers>\n+        <events>\n+             <shard>\n+                 <internal_replication>false</internal_replication>\n+                 <replica>\n+                     <host>first</host>\n+                     <port>9000</port>\n+                 </replica>\n+             </shard>\n+             <shard>\n+                 <internal_replication>false</internal_replication>\n+                 <replica>\n+                     <host>second</host>\n+                     <port>9000</port>\n+                 </replica>\n+             </shard>\n+             <shard>\n+                 <internal_replication>false</internal_replication>\n+                 <replica>\n+                     <host>third</host>\n+                     <port>9000</port>\n+                 </replica>\n+             </shard>\n+         </events>\n+    </remote_servers>\n+</yandex>\ndiff --git a/tests/integration/test_cluster_copier/configs_three_nodes/conf.d/ddl.xml b/tests/integration/test_cluster_copier/configs_three_nodes/conf.d/ddl.xml\nnew file mode 100644\nindex 000000000000..4bff11fb6936\n--- /dev/null\n+++ b/tests/integration/test_cluster_copier/configs_three_nodes/conf.d/ddl.xml\n@@ -0,0 +1,6 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+    <distributed_ddl>\n+        <path>/clickhouse/task_queue/ddl</path>\n+    </distributed_ddl>\n+</yandex>\n\\ No newline at end of file\ndiff --git a/tests/integration/test_cluster_copier/configs_three_nodes/config-copier.xml b/tests/integration/test_cluster_copier/configs_three_nodes/config-copier.xml\nnew file mode 100644\nindex 000000000000..ede3dcb1228b\n--- /dev/null\n+++ b/tests/integration/test_cluster_copier/configs_three_nodes/config-copier.xml\n@@ -0,0 +1,28 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+    <logger>\n+        <level>information</level>\n+        <log>/var/log/clickhouse-server/copier/log.log</log>\n+        <errorlog>/var/log/clickhouse-server/copier/log.err.log</errorlog>\n+        <size>1000M</size>\n+        <count>10</count>\n+        <stderr>/var/log/clickhouse-server/copier/stderr.log</stderr>\n+        <stdout>/var/log/clickhouse-server/copier/stdout.log</stdout>\n+    </logger>\n+\n+    <zookeeper>\n+        <node index=\"1\">\n+            <host>zoo1</host>\n+            <port>2181</port>\n+        </node>\n+        <node index=\"2\">\n+            <host>zoo2</host>\n+            <port>2181</port>\n+        </node>\n+            <node index=\"3\">\n+            <host>zoo3</host>\n+            <port>2181</port>\n+        </node>\n+        <session_timeout_ms>2000</session_timeout_ms>\n+    </zookeeper>\n+</yandex>\ndiff --git a/tests/integration/test_cluster_copier/configs_three_nodes/users.xml b/tests/integration/test_cluster_copier/configs_three_nodes/users.xml\nnew file mode 100644\nindex 000000000000..023598304f20\n--- /dev/null\n+++ b/tests/integration/test_cluster_copier/configs_three_nodes/users.xml\n@@ -0,0 +1,32 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+    <profiles>\n+        <default>\n+            <log_queries>1</log_queries>\n+        </default>\n+    </profiles>\n+\n+    <users>\n+        <default>\n+            <password></password>\n+            <networks incl=\"networks\" replace=\"replace\">\n+                <ip>::/0</ip>\n+            </networks>\n+            <profile>default</profile>\n+            <quota>default</quota>\n+        </default>\n+        <dbuser>\n+            <password>12345678</password>\n+            <networks incl=\"networks\" replace=\"replace\">\n+                <ip>::/0</ip>\n+            </networks>\n+            <profile>default</profile>\n+            <quota>default</quota>\n+        </dbuser>\n+    </users>\n+\n+    <quotas>\n+        <default>\n+        </default>\n+    </quotas>\n+</yandex>\ndiff --git a/tests/integration/test_cluster_copier/configs_two_nodes/conf.d/clusters.xml b/tests/integration/test_cluster_copier/configs_two_nodes/conf.d/clusters.xml\nnew file mode 100644\nindex 000000000000..fd1321d5218d\n--- /dev/null\n+++ b/tests/integration/test_cluster_copier/configs_two_nodes/conf.d/clusters.xml\n@@ -0,0 +1,23 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+    <remote_servers>\n+        <source>\n+             <shard>\n+                 <internal_replication>false</internal_replication>\n+                 <replica>\n+                     <host>first_of_two</host>\n+                     <port>9000</port>\n+                 </replica>\n+             </shard>\n+        </source>\n+        <destination>\n+             <shard>\n+                 <internal_replication>false</internal_replication>\n+                 <replica>\n+                     <host>second_of_two</host>\n+                     <port>9000</port>\n+                 </replica>\n+             </shard>\n+        </destination>\n+    </remote_servers>\n+</yandex>\ndiff --git a/tests/integration/test_cluster_copier/configs_two_nodes/conf.d/ddl.xml b/tests/integration/test_cluster_copier/configs_two_nodes/conf.d/ddl.xml\nnew file mode 100644\nindex 000000000000..4bff11fb6936\n--- /dev/null\n+++ b/tests/integration/test_cluster_copier/configs_two_nodes/conf.d/ddl.xml\n@@ -0,0 +1,6 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+    <distributed_ddl>\n+        <path>/clickhouse/task_queue/ddl</path>\n+    </distributed_ddl>\n+</yandex>\n\\ No newline at end of file\ndiff --git a/tests/integration/test_cluster_copier/configs_two_nodes/conf.d/storage_configuration.xml b/tests/integration/test_cluster_copier/configs_two_nodes/conf.d/storage_configuration.xml\nnew file mode 100644\nindex 000000000000..07b5c5772675\n--- /dev/null\n+++ b/tests/integration/test_cluster_copier/configs_two_nodes/conf.d/storage_configuration.xml\n@@ -0,0 +1,34 @@\n+<yandex>\n+\n+<storage_configuration>\n+    <disks>\n+        <default>\n+        </default>\n+        <jbod1>\n+            <path>/jbod1/</path>\n+        </jbod1>\n+        <jbod2>\n+            <path>/jbod2/</path>\n+        </jbod2>\n+        <external>\n+            <path>/external/</path>\n+        </external>\n+    </disks>\n+\n+    <policies>\n+        <external_with_jbods>\n+            <volumes>\n+                <external>\n+                    <disk>external</disk>\n+                </external>\n+                <main>\n+                    <disk>jbod1</disk>\n+                    <disk>jbod2</disk>\n+                </main>\n+            </volumes>\n+        </external_with_jbods>\n+    </policies>\n+\n+</storage_configuration>\n+\n+</yandex>\ndiff --git a/tests/integration/test_cluster_copier/configs_two_nodes/config-copier.xml b/tests/integration/test_cluster_copier/configs_two_nodes/config-copier.xml\nnew file mode 100644\nindex 000000000000..642998c6d878\n--- /dev/null\n+++ b/tests/integration/test_cluster_copier/configs_two_nodes/config-copier.xml\n@@ -0,0 +1,20 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+    <logger>\n+        <level>information</level>\n+        <log>/var/log/clickhouse-server/copier/log.log</log>\n+        <errorlog>/var/log/clickhouse-server/copier/log.err.log</errorlog>\n+        <size>1000M</size>\n+        <count>10</count>\n+        <stderr>/var/log/clickhouse-server/copier/stderr.log</stderr>\n+        <stdout>/var/log/clickhouse-server/copier/stdout.log</stdout>\n+    </logger>\n+\n+    <zookeeper>\n+        <node index=\"1\">\n+            <host>zoo1</host>\n+            <port>2181</port>\n+        </node>\n+        <session_timeout_ms>2000</session_timeout_ms>\n+    </zookeeper>\n+</yandex>\ndiff --git a/tests/integration/test_cluster_copier/configs_two_nodes/users.xml b/tests/integration/test_cluster_copier/configs_two_nodes/users.xml\nnew file mode 100644\nindex 000000000000..023598304f20\n--- /dev/null\n+++ b/tests/integration/test_cluster_copier/configs_two_nodes/users.xml\n@@ -0,0 +1,32 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+    <profiles>\n+        <default>\n+            <log_queries>1</log_queries>\n+        </default>\n+    </profiles>\n+\n+    <users>\n+        <default>\n+            <password></password>\n+            <networks incl=\"networks\" replace=\"replace\">\n+                <ip>::/0</ip>\n+            </networks>\n+            <profile>default</profile>\n+            <quota>default</quota>\n+        </default>\n+        <dbuser>\n+            <password>12345678</password>\n+            <networks incl=\"networks\" replace=\"replace\">\n+                <ip>::/0</ip>\n+            </networks>\n+            <profile>default</profile>\n+            <quota>default</quota>\n+        </dbuser>\n+    </users>\n+\n+    <quotas>\n+        <default>\n+        </default>\n+    </quotas>\n+</yandex>\ndiff --git a/tests/integration/test_cluster_copier/task_drop_target_partition.xml b/tests/integration/test_cluster_copier/task_drop_target_partition.xml\nnew file mode 100644\nindex 000000000000..6b2ede19d25b\n--- /dev/null\n+++ b/tests/integration/test_cluster_copier/task_drop_target_partition.xml\n@@ -0,0 +1,42 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+<remote_servers>\n+        <source>\n+             <shard>\n+                 <internal_replication>false</internal_replication>\n+                 <replica>\n+                     <host>first_of_two</host>\n+                     <port>9000</port>\n+                 </replica>\n+             </shard>\n+        </source>\n+        <destination>\n+             <shard>\n+                 <internal_replication>false</internal_replication>\n+                 <replica>\n+                     <host>second_of_two</host>\n+                     <port>9000</port>\n+                 </replica>\n+             </shard>\n+        </destination>\n+    </remote_servers>\n+\n+   <max_workers>2</max_workers>\n+\n+   <tables>\n+     <table_events>\n+         <cluster_pull>source</cluster_pull>\n+         <database_pull>db_drop_target_partition</database_pull>\n+         <table_pull>source</table_pull>\n+\n+         <cluster_push>destination</cluster_push>\n+         <database_push>db_drop_target_partition</database_push>\n+         <table_push>destination</table_push>\n+\n+         <allow_to_drop_target_partitions>true</allow_to_drop_target_partitions>\n+\n+         <engine>ENGINE = MergeTree() PARTITION BY toYYYYMMDD(Column3) ORDER BY (Column3, Column2, Column1)</engine>\n+         <sharding_key>rand()</sharding_key>\n+     </table_events>\n+   </tables>\n+ </yandex>\ndiff --git a/tests/integration/test_cluster_copier/task_skip_index.xml b/tests/integration/test_cluster_copier/task_skip_index.xml\nnew file mode 100644\nindex 000000000000..5bc161813236\n--- /dev/null\n+++ b/tests/integration/test_cluster_copier/task_skip_index.xml\n@@ -0,0 +1,40 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+<remote_servers>\n+        <source>\n+             <shard>\n+                 <internal_replication>false</internal_replication>\n+                 <replica>\n+                     <host>first_of_two</host>\n+                     <port>9000</port>\n+                 </replica>\n+             </shard>\n+        </source>\n+        <destination>\n+             <shard>\n+                 <internal_replication>false</internal_replication>\n+                 <replica>\n+                     <host>second_of_two</host>\n+                     <port>9000</port>\n+                 </replica>\n+             </shard>\n+        </destination>\n+    </remote_servers>\n+\n+   <max_workers>2</max_workers>\n+\n+   <tables>\n+     <table_events>\n+         <cluster_pull>source</cluster_pull>\n+         <database_pull>db_skip_index</database_pull>\n+         <table_pull>source</table_pull>\n+\n+         <cluster_push>destination</cluster_push>\n+         <database_push>db_skip_index</database_push>\n+         <table_push>destination</table_push>\n+\n+         <engine>ENGINE = MergeTree() PARTITION BY toYYYYMMDD(Column3) ORDER BY (Column3, Column2, Column1)</engine>\n+         <sharding_key>rand()</sharding_key>\n+     </table_events>\n+   </tables>\n+ </yandex>\ndiff --git a/tests/integration/test_cluster_copier/task_taxi_data.xml b/tests/integration/test_cluster_copier/task_taxi_data.xml\nnew file mode 100644\nindex 000000000000..fafffe3ebc93\n--- /dev/null\n+++ b/tests/integration/test_cluster_copier/task_taxi_data.xml\n@@ -0,0 +1,43 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+   <remote_servers>\n+         <events>\n+             <shard>\n+                 <internal_replication>false</internal_replication>\n+                 <replica>\n+                     <host>first</host>\n+                     <port>9000</port>\n+                 </replica>\n+             </shard>\n+             <shard>\n+                 <internal_replication>false</internal_replication>\n+                 <replica>\n+                     <host>second</host>\n+                     <port>9000</port>\n+                 </replica>\n+             </shard>\n+             <shard>\n+                 <internal_replication>false</internal_replication>\n+                 <replica>\n+                     <host>third</host>\n+                     <port>9000</port>\n+                 </replica>\n+             </shard>\n+         </events>\n+   </remote_servers>\n+\n+   <max_workers>2</max_workers>\n+\n+   <tables>\n+     <table_events>\n+         <cluster_pull>events</cluster_pull>\n+         <database_pull>dailyhistory</database_pull>\n+         <table_pull>yellow_tripdata_staging</table_pull>\n+         <cluster_push>events</cluster_push>\n+         <database_push>monthlyhistory</database_push>\n+         <table_push>yellow_tripdata_staging</table_push>\n+         <engine>Engine=ReplacingMergeTree() PRIMARY KEY (tpep_pickup_datetime, id) ORDER BY (tpep_pickup_datetime, id) PARTITION BY (pickup_location_id, toYYYYMM(tpep_pickup_datetime))</engine>\n+         <sharding_key>sipHash64(id) % 3</sharding_key>\n+     </table_events>\n+   </tables>\n+ </yandex>\n\\ No newline at end of file\ndiff --git a/tests/integration/test_cluster_copier/task_trivial.xml b/tests/integration/test_cluster_copier/task_trivial.xml\nindex 27af6f64bf4c..ddf0d8a52a15 100644\n--- a/tests/integration/test_cluster_copier/task_trivial.xml\n+++ b/tests/integration/test_cluster_copier/task_trivial.xml\n@@ -44,7 +44,7 @@\n         <source_trivial_cluster>\n             <shard>\n                 <replica>\n-                    <host>s0_0_0</host>\n+                    <host>first_trivial</host>\n                     <port>9000</port>\n                 </replica>\n             </shard>\n@@ -54,11 +54,11 @@\n         <destination_trivial_cluster>\n             <shard>\n                 <replica>\n-                    <host>s1_0_0</host>\n+                    <host>second_trivial</host>\n                     <port>9000</port>\n                 </replica>\n             </shard>\n         </destination_trivial_cluster>\n     </remote_servers>\n \n-</yandex>\n\\ No newline at end of file\n+</yandex>\ndiff --git a/tests/integration/test_cluster_copier/task_trivial_without_arguments.xml b/tests/integration/test_cluster_copier/task_trivial_without_arguments.xml\nnew file mode 100644\nindex 000000000000..86f383e056e5\n--- /dev/null\n+++ b/tests/integration/test_cluster_copier/task_trivial_without_arguments.xml\n@@ -0,0 +1,64 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+    <!-- How many simualteneous workers are posssible -->\n+    <max_workers>3</max_workers>\n+\n+    <!-- Common setting for pull and push operations -->\n+    <settings>\n+        <connect_timeout>1</connect_timeout>\n+    </settings>\n+\n+    <!-- Setting used to fetch data -->\n+    <settings_pull>\n+        <max_rows_in_distinct>0</max_rows_in_distinct>\n+    </settings_pull>\n+\n+    <!-- Setting used to insert data -->\n+    <settings_push>\n+    </settings_push>\n+\n+    <!-- Tasks -->\n+    <tables>\n+        <hits>\n+            <cluster_pull>source_trivial_cluster</cluster_pull>\n+            <database_pull>default</database_pull>\n+            <table_pull>trivial_without_arguments</table_pull>\n+\n+            <cluster_push>destination_trivial_cluster</cluster_push>\n+            <database_push>default</database_push>\n+            <table_push>trivial_without_arguments</table_push>\n+\n+            <!-- Engine of destination tables -->\n+            <engine>ENGINE=ReplicatedMergeTree() PARTITION BY d % 5 ORDER BY (d, sipHash64(d)) SAMPLE BY sipHash64(d) SETTINGS index_granularity = 16</engine>\n+\n+            <!-- Which sarding key to use while copying -->\n+            <sharding_key>d + 1</sharding_key>\n+\n+            <!-- Optional expression that filter copying data -->\n+            <where_condition>d - d = 0</where_condition>\n+        </hits>\n+    </tables>\n+\n+    <!-- Configuration of clusters -->\n+    <remote_servers>\n+        <source_trivial_cluster>\n+            <shard>\n+                <replica>\n+                    <host>first_trivial</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+        </source_trivial_cluster>\n+\n+\n+        <destination_trivial_cluster>\n+            <shard>\n+                <replica>\n+                    <host>second_trivial</host>\n+                    <port>9000</port>\n+                </replica>\n+            </shard>\n+        </destination_trivial_cluster>\n+    </remote_servers>\n+\n+</yandex>\ndiff --git a/tests/integration/test_cluster_copier/task_ttl_columns.xml b/tests/integration/test_cluster_copier/task_ttl_columns.xml\nnew file mode 100644\nindex 000000000000..68868877d316\n--- /dev/null\n+++ b/tests/integration/test_cluster_copier/task_ttl_columns.xml\n@@ -0,0 +1,40 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+<remote_servers>\n+        <source>\n+             <shard>\n+                 <internal_replication>false</internal_replication>\n+                 <replica>\n+                     <host>first_of_two</host>\n+                     <port>9000</port>\n+                 </replica>\n+             </shard>\n+        </source>\n+        <destination>\n+             <shard>\n+                 <internal_replication>false</internal_replication>\n+                 <replica>\n+                     <host>second_of_two</host>\n+                     <port>9000</port>\n+                 </replica>\n+             </shard>\n+        </destination>\n+    </remote_servers>\n+\n+   <max_workers>2</max_workers>\n+\n+   <tables>\n+     <table_events>\n+         <cluster_pull>source</cluster_pull>\n+         <database_pull>db_ttl_columns</database_pull>\n+         <table_pull>source</table_pull>\n+\n+         <cluster_push>destination</cluster_push>\n+         <database_push>db_ttl_columns</database_push>\n+         <table_push>destination</table_push>\n+\n+         <engine>ENGINE = MergeTree() PARTITION BY toYYYYMMDD(Column3) ORDER BY (Column3, Column2, Column1)</engine>\n+         <sharding_key>rand()</sharding_key>\n+     </table_events>\n+   </tables>\n+ </yandex>\ndiff --git a/tests/integration/test_cluster_copier/task_ttl_move_to_volume.xml b/tests/integration/test_cluster_copier/task_ttl_move_to_volume.xml\nnew file mode 100644\nindex 000000000000..051988964d2f\n--- /dev/null\n+++ b/tests/integration/test_cluster_copier/task_ttl_move_to_volume.xml\n@@ -0,0 +1,40 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+<remote_servers>\n+        <source>\n+             <shard>\n+                 <internal_replication>false</internal_replication>\n+                 <replica>\n+                     <host>first_of_two</host>\n+                     <port>9000</port>\n+                 </replica>\n+             </shard>\n+        </source>\n+        <destination>\n+             <shard>\n+                 <internal_replication>false</internal_replication>\n+                 <replica>\n+                     <host>second_of_two</host>\n+                     <port>9000</port>\n+                 </replica>\n+             </shard>\n+        </destination>\n+    </remote_servers>\n+\n+   <max_workers>2</max_workers>\n+\n+   <tables>\n+     <table_events>\n+         <cluster_pull>source</cluster_pull>\n+         <database_pull>db_move_to_volume</database_pull>\n+         <table_pull>source</table_pull>\n+\n+         <cluster_push>destination</cluster_push>\n+         <database_push>db_move_to_volume</database_push>\n+         <table_push>destination</table_push>\n+\n+         <engine>ENGINE = MergeTree() PARTITION BY toYYYYMMDD(Column3) ORDER BY (Column3, Column2, Column1) TTL Column3 + INTERVAL 1 MONTH TO VOLUME 'external' SETTINGS storage_policy = 'external_with_jbods'</engine>\n+         <sharding_key>rand()</sharding_key>\n+     </table_events>\n+   </tables>\n+ </yandex>\ndiff --git a/tests/integration/test_cluster_copier/task_with_different_schema.xml b/tests/integration/test_cluster_copier/task_with_different_schema.xml\nnew file mode 100644\nindex 000000000000..409ca7d2e997\n--- /dev/null\n+++ b/tests/integration/test_cluster_copier/task_with_different_schema.xml\n@@ -0,0 +1,40 @@\n+<?xml version=\"1.0\"?>\n+<yandex>\n+<remote_servers>\n+        <source>\n+             <shard>\n+                 <internal_replication>false</internal_replication>\n+                 <replica>\n+                     <host>first_of_two</host>\n+                     <port>9000</port>\n+                 </replica>\n+             </shard>\n+        </source>\n+        <destination>\n+             <shard>\n+                 <internal_replication>false</internal_replication>\n+                 <replica>\n+                     <host>second_of_two</host>\n+                     <port>9000</port>\n+                 </replica>\n+             </shard>\n+        </destination>\n+    </remote_servers>\n+\n+   <max_workers>2</max_workers>\n+\n+   <tables>\n+     <table_events>\n+         <cluster_pull>source</cluster_pull>\n+         <database_pull>db_different_schema</database_pull>\n+         <table_pull>source</table_pull>\n+\n+         <cluster_push>destination</cluster_push>\n+         <database_push>db_different_schema</database_push>\n+         <table_push>destination</table_push>\n+\n+         <engine>ENGINE = MergeTree() PARTITION BY toYYYYMMDD(Column3) ORDER BY (Column9, Column1, Column2, Column3, Column4)</engine>\n+         <sharding_key>rand()</sharding_key>\n+     </table_events>\n+   </tables>\n+ </yandex>\ndiff --git a/tests/integration/test_cluster_copier/test.py b/tests/integration/test_cluster_copier/test.py\nindex c6068e3a6e90..7fe1d8c9d29b 100644\n--- a/tests/integration/test_cluster_copier/test.py\n+++ b/tests/integration/test_cluster_copier/test.py\n@@ -2,21 +2,26 @@\n import random\n import sys\n import time\n-from contextlib import contextmanager\n-\n-import docker\n import kazoo\n import pytest\n+import string\n+import random\n+from contextlib import contextmanager\n from helpers.cluster import ClickHouseCluster\n from helpers.test_tools import TSV\n \n+import docker\n+\n CURRENT_TEST_DIR = os.path.dirname(os.path.abspath(__file__))\n sys.path.insert(0, os.path.dirname(CURRENT_TEST_DIR))\n \n COPYING_FAIL_PROBABILITY = 0.2\n MOVING_FAIL_PROBABILITY = 0.2\n \n-cluster = ClickHouseCluster(__file__)\n+cluster = ClickHouseCluster(__file__, name='copier_test')\n+\n+def generateRandomString(count):\n+    return ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(count))\n \n \n def check_all_hosts_sucesfully_executed(tsv_content, num_hosts):\n@@ -72,8 +77,13 @@ class Task1:\n \n     def __init__(self, cluster):\n         self.cluster = cluster\n-        self.zk_task_path = \"/clickhouse-copier/task_simple\"\n-        self.copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task0_description.xml'), 'r').read()\n+        self.zk_task_path = \"/clickhouse-copier/task_simple_\" + generateRandomString(10)\n+        self.container_task_file = \"/task0_description.xml\"\n+\n+        for instance_name, _ in cluster.instances.items():\n+            instance = cluster.instances[instance_name]\n+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task0_description.xml'), self.container_task_file)\n+            print(\"Copied task file to container of '{}' instance. Path {}\".format(instance_name, self.container_task_file))\n \n     def start(self):\n         instance = cluster.instances['s0_0_0']\n@@ -112,9 +122,14 @@ class Task2:\n \n     def __init__(self, cluster, unique_zk_path):\n         self.cluster = cluster\n-        self.zk_task_path = \"/clickhouse-copier/task_month_to_week_partition\"\n-        self.copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task_month_to_week_description.xml'), 'r').read()\n-        self.unique_zk_path = unique_zk_path\n+        self.zk_task_path = \"/clickhouse-copier/task_month_to_week_partition_\" + generateRandomString(5)\n+        self.unique_zk_path = generateRandomString(10)\n+        self.container_task_file = \"/task_month_to_week_description.xml\"\n+\n+        for instance_name, _ in cluster.instances.items():\n+            instance = cluster.instances[instance_name]\n+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_month_to_week_description.xml'), self.container_task_file)\n+            print(\"Copied task file to container of '{}' instance. Path {}\".format(instance_name, self.container_task_file))\n \n     def start(self):\n         instance = cluster.instances['s0_0_0']\n@@ -163,9 +178,14 @@ class Task_test_block_size:\n \n     def __init__(self, cluster):\n         self.cluster = cluster\n-        self.zk_task_path = \"/clickhouse-copier/task_test_block_size\"\n-        self.copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task_test_block_size.xml'), 'r').read()\n+        self.zk_task_path = \"/clickhouse-copier/task_test_block_size_\" + generateRandomString(5)\n         self.rows = 1000000\n+        self.container_task_file = \"/task_test_block_size.xml\"\n+\n+        for instance_name, _ in cluster.instances.items():\n+            instance = cluster.instances[instance_name]\n+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_test_block_size.xml'), self.container_task_file)\n+            print(\"Copied task file to container of '{}' instance. Path {}\".format(instance_name, self.container_task_file))\n \n     def start(self):\n         instance = cluster.instances['s0_0_0']\n@@ -192,13 +212,19 @@ class Task_no_index:\n \n     def __init__(self, cluster):\n         self.cluster = cluster\n-        self.zk_task_path = \"/clickhouse-copier/task_no_index\"\n-        self.copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task_no_index.xml'), 'r').read()\n+        self.zk_task_path = \"/clickhouse-copier/task_no_index_\" + generateRandomString(5)\n         self.rows = 1000000\n+        self.container_task_file = \"/task_no_index.xml\"\n+\n+        for instance_name, _ in cluster.instances.items():\n+            instance = cluster.instances[instance_name]\n+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_no_index.xml'), self.container_task_file)\n+            print(\"Copied task file to container of '{}' instance. Path {}\".format(instance_name, self.container_task_file))\n \n     def start(self):\n         instance = cluster.instances['s0_0_0']\n-        instance.query(\"create table ontime (Year UInt16, FlightDate String) ENGINE = Memory\")\n+        instance.query(\"DROP TABLE IF EXISTS ontime SYNC\")\n+        instance.query(\"create table IF NOT EXISTS ontime (Year UInt16, FlightDate String) ENGINE = Memory\")\n         instance.query(\"insert into ontime values (2016, 'test6'), (2017, 'test7'), (2018, 'test8')\")\n \n     def check(self):\n@@ -214,32 +240,44 @@ class Task_no_arg:\n     def __init__(self, cluster):\n         self.cluster = cluster\n         self.zk_task_path = \"/clickhouse-copier/task_no_arg\"\n-        self.copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task_no_arg.xml'), 'r').read()\n         self.rows = 1000000\n+        self.container_task_file = \"/task_no_arg.xml\"\n+\n+        for instance_name, _ in cluster.instances.items():\n+            instance = cluster.instances[instance_name]\n+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_no_arg.xml'), self.container_task_file)\n+            print(\"Copied task file to container of '{}' instance. Path {}\".format(instance_name, self.container_task_file))\n \n     def start(self):\n         instance = cluster.instances['s0_0_0']\n+        instance.query(\"DROP TABLE IF EXISTS copier_test1 SYNC\")\n         instance.query(\n-            \"create table copier_test1 (date Date, id UInt32) engine = MergeTree PARTITION BY date ORDER BY date SETTINGS index_granularity = 8192\")\n+            \"create table if not exists copier_test1 (date Date, id UInt32) engine = MergeTree PARTITION BY date ORDER BY date SETTINGS index_granularity = 8192\")\n         instance.query(\"insert into copier_test1 values ('2016-01-01', 10);\")\n \n     def check(self):\n         assert TSV(self.cluster.instances['s1_1_0'].query(\"SELECT date FROM copier_test1_1\")) == TSV(\"2016-01-01\\n\")\n         instance = cluster.instances['s0_0_0']\n-        instance.query(\"DROP TABLE copier_test1\")\n+        instance.query(\"DROP TABLE copier_test1 SYNC\")\n         instance = cluster.instances['s1_1_0']\n-        instance.query(\"DROP TABLE copier_test1_1\")\n+        instance.query(\"DROP TABLE copier_test1_1 SYNC\")\n \n class Task_non_partitioned_table:\n \n     def __init__(self, cluster):\n         self.cluster = cluster\n         self.zk_task_path = \"/clickhouse-copier/task_non_partitoned_table\"\n-        self.copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task_non_partitioned_table.xml'), 'r').read()\n         self.rows = 1000000\n+        self.container_task_file = \"/task_non_partitioned_table.xml\"\n+\n+        for instance_name, _ in cluster.instances.items():\n+            instance = cluster.instances[instance_name]\n+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_non_partitioned_table.xml'), self.container_task_file)\n+            print(\"Copied task file to container of '{}' instance. Path {}\".format(instance_name, self.container_task_file))\n \n     def start(self):\n         instance = cluster.instances['s0_0_0']\n+        instance.query(\"DROP TABLE IF EXISTS copier_test1 SYNC\")\n         instance.query(\n             \"create table copier_test1 (date Date, id UInt32) engine = MergeTree ORDER BY date SETTINGS index_granularity = 8192\")\n         instance.query(\"insert into copier_test1 values ('2016-01-01', 10);\")\n@@ -256,16 +294,23 @@ class Task_self_copy:\n     def __init__(self, cluster):\n         self.cluster = cluster\n         self.zk_task_path = \"/clickhouse-copier/task_self_copy\"\n-        self.copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task_self_copy.xml'), 'r').read()\n+        self.container_task_file = \"/task_self_copy.xml\"\n+\n+        for instance_name, _ in cluster.instances.items():\n+            instance = cluster.instances[instance_name]\n+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_self_copy.xml'), self.container_task_file)\n+            print(\"Copied task file to container of '{}' instance. Path {}\".format(instance_name, self.container_task_file))\n \n     def start(self):\n         instance = cluster.instances['s0_0_0']\n-        instance.query(\"CREATE DATABASE db1;\")\n+        instance.query(\"DROP DATABASE IF EXISTS db1 SYNC\")\n+        instance.query(\"DROP DATABASE IF EXISTS db2 SYNC\")\n+        instance.query(\"CREATE DATABASE IF NOT EXISTS db1;\")\n         instance.query(\n-            \"CREATE TABLE db1.source_table (`a` Int8, `b` String, `c` Int8) ENGINE = MergeTree PARTITION BY a ORDER BY a SETTINGS index_granularity = 8192\")\n-        instance.query(\"CREATE DATABASE db2;\")\n+            \"CREATE TABLE IF NOT EXISTS db1.source_table (`a` Int8, `b` String, `c` Int8) ENGINE = MergeTree PARTITION BY a ORDER BY a SETTINGS index_granularity = 8192\")\n+        instance.query(\"CREATE DATABASE IF NOT EXISTS db2;\")\n         instance.query(\n-            \"CREATE TABLE db2.destination_table (`a` Int8, `b` String, `c` Int8) ENGINE = MergeTree PARTITION BY a ORDER BY a SETTINGS index_granularity = 8192\")\n+            \"CREATE TABLE IF NOT EXISTS db2.destination_table (`a` Int8, `b` String, `c` Int8) ENGINE = MergeTree PARTITION BY a ORDER BY a SETTINGS index_granularity = 8192\")\n         instance.query(\"INSERT INTO db1.source_table VALUES (1, 'ClickHouse', 1);\")\n         instance.query(\"INSERT INTO db1.source_table VALUES (2, 'Copier', 2);\")\n \n@@ -273,8 +318,8 @@ def check(self):\n         instance = cluster.instances['s0_0_0']\n         assert TSV(instance.query(\"SELECT * FROM db2.destination_table ORDER BY a\")) == TSV(instance.query(\"SELECT * FROM db1.source_table ORDER BY a\"))\n         instance = cluster.instances['s0_0_0']\n-        instance.query(\"DROP DATABASE db1 SYNC\")\n-        instance.query(\"DROP DATABASE db2 SYNC\")\n+        instance.query(\"DROP DATABASE IF EXISTS db1 SYNC\")\n+        instance.query(\"DROP DATABASE IF EXISTS db2 SYNC\")\n \n \n def execute_task(started_cluster, task, cmd_options):\n@@ -283,26 +328,27 @@ def execute_task(started_cluster, task, cmd_options):\n     zk = started_cluster.get_kazoo_client('zoo1')\n     print(\"Use ZooKeeper server: {}:{}\".format(zk.hosts[0][0], zk.hosts[0][1]))\n \n+\n     try:\n         zk.delete(\"/clickhouse-copier\", recursive=True)\n     except kazoo.exceptions.NoNodeError:\n         print(\"No node /clickhouse-copier. It is Ok in first test.\")\n \n-    zk_task_path = task.zk_task_path\n-    zk.ensure_path(zk_task_path)\n-    zk.create(zk_task_path + \"/description\", task.copier_task_config.encode())\n-\n     # Run cluster-copier processes on each node\n     docker_api = started_cluster.docker_client.api\n     copiers_exec_ids = []\n \n     cmd = ['/usr/bin/clickhouse', 'copier',\n            '--config', '/etc/clickhouse-server/config-copier.xml',\n-           '--task-path', zk_task_path,\n+           '--task-path', task.zk_task_path,\n+           '--task-file', task.container_task_file,\n+           '--task-upload-force', 'true',\n            '--base-dir', '/var/log/clickhouse-server/copier']\n     cmd += cmd_options\n \n-    copiers = random.sample(list(cluster.instances.keys()), 3)\n+    print(cmd)\n+\n+    copiers = random.sample(list(started_cluster.instances.keys()), 3)\n \n     for instance_name in copiers:\n         instance = started_cluster.instances[instance_name]\n@@ -330,18 +376,12 @@ def execute_task(started_cluster, task, cmd_options):\n     try:\n         task.check()\n     finally:\n-        zk.delete(zk_task_path, recursive=True)\n+        zk.delete(task.zk_task_path, recursive=True)\n \n \n # Tests\n \n-@pytest.mark.parametrize(\n-    ('use_sample_offset'),\n-    [\n-        False,\n-        True\n-    ]\n-)\n+@pytest.mark.parametrize(('use_sample_offset'), [False, True])\n def test_copy_simple(started_cluster, use_sample_offset):\n     if use_sample_offset:\n         execute_task(started_cluster, Task1(started_cluster), ['--experimental-use-sample-offset', '1'])\n@@ -349,13 +389,7 @@ def test_copy_simple(started_cluster, use_sample_offset):\n         execute_task(started_cluster, Task1(started_cluster), [])\n \n \n-@pytest.mark.parametrize(\n-    ('use_sample_offset'),\n-    [\n-        False,\n-        True\n-    ]\n-)\n+@pytest.mark.parametrize(('use_sample_offset'),[False, True])\n def test_copy_with_recovering(started_cluster, use_sample_offset):\n     if use_sample_offset:\n         execute_task(started_cluster, Task1(started_cluster), ['--copy-fault-probability', str(COPYING_FAIL_PROBABILITY),\n@@ -364,13 +398,7 @@ def test_copy_with_recovering(started_cluster, use_sample_offset):\n         execute_task(started_cluster, Task1(started_cluster), ['--copy-fault-probability', str(COPYING_FAIL_PROBABILITY)])\n \n \n-@pytest.mark.parametrize(\n-    ('use_sample_offset'),\n-    [\n-        False,\n-        True\n-    ]\n-)\n+@pytest.mark.parametrize(('use_sample_offset'),[False, True])\n def test_copy_with_recovering_after_move_faults(started_cluster, use_sample_offset):\n     if use_sample_offset:\n         execute_task(started_cluster, Task1(started_cluster), ['--move-fault-probability', str(MOVING_FAIL_PROBABILITY),\n@@ -412,9 +440,3 @@ def test_non_partitioned_table(started_cluster):\n \n def test_self_copy(started_cluster):\n     execute_task(started_cluster, Task_self_copy(started_cluster), [])\n-\n-if __name__ == '__main__':\n-    with contextmanager(started_cluster)() as cluster:\n-        for name, instance in list(cluster.instances.items()):\n-            print(name, instance.ip_address)\n-        input(\"Cluster created, press any key to destroy...\")\ndiff --git a/tests/integration/test_cluster_copier/test_three_nodes.py b/tests/integration/test_cluster_copier/test_three_nodes.py\nnew file mode 100644\nindex 000000000000..acdc191154cd\n--- /dev/null\n+++ b/tests/integration/test_cluster_copier/test_three_nodes.py\n@@ -0,0 +1,238 @@\n+import os\n+import sys\n+import time\n+import logging\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+from helpers.test_tools import TSV\n+\n+import docker\n+\n+CURRENT_TEST_DIR = os.path.dirname(os.path.abspath(__file__))\n+sys.path.insert(0, os.path.dirname(CURRENT_TEST_DIR))\n+\n+cluster = ClickHouseCluster(__file__, name='copier_test_three_nodes')\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    global cluster\n+    try:\n+\n+        for name in [\"first\", \"second\", \"third\"]:\n+            cluster.add_instance(name,\n+                main_configs=[\"configs_three_nodes/conf.d/clusters.xml\", \"configs_three_nodes/conf.d/ddl.xml\"], user_configs=[\"configs_three_nodes/users.xml\"],\n+                with_zookeeper=True)\n+\n+        cluster.start()\n+        yield cluster\n+\n+    finally:\n+        cluster.shutdown()\n+\n+class Task:\n+    def __init__(self, cluster):\n+        self.cluster = cluster\n+        self.zk_task_path = '/clickhouse-copier/task'\n+        self.container_task_file = \"/task_taxi_data.xml\"\n+\n+        for instance_name, _ in cluster.instances.items():\n+            instance = cluster.instances[instance_name]\n+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_taxi_data.xml'), self.container_task_file)\n+            print(\"Copied task file to container of '{}' instance. Path {}\".format(instance_name, self.container_task_file))\n+\n+\n+    def start(self):\n+        for name in [\"first\", \"second\", \"third\"]:\n+            node = cluster.instances[name]\n+            node.query(\"DROP DATABASE IF EXISTS dailyhistory SYNC;\")\n+            node.query(\"DROP DATABASE IF EXISTS monthlyhistory SYNC;\")\n+\n+        instance = cluster.instances['first']\n+\n+        # daily partition database\n+        instance.query(\"CREATE DATABASE IF NOT EXISTS dailyhistory on cluster events;\")\n+        instance.query(\"\"\"CREATE TABLE dailyhistory.yellow_tripdata_staging ON CLUSTER events\n+        (\n+            id UUID DEFAULT generateUUIDv4(),\n+            vendor_id String,\n+            tpep_pickup_datetime DateTime('UTC'),\n+            tpep_dropoff_datetime DateTime('UTC'),\n+            passenger_count Nullable(Float64),\n+            trip_distance String,\n+            pickup_longitude Float64,\n+            pickup_latitude Float64,\n+            rate_code_id String,\n+            store_and_fwd_flag String,\n+            dropoff_longitude Float64,\n+            dropoff_latitude Float64,\n+            payment_type String,\n+            fare_amount String,\n+            extra String,\n+            mta_tax String,\n+            tip_amount String,\n+            tolls_amount String,\n+            improvement_surcharge String,\n+            total_amount String,\n+            pickup_location_id String,\n+            dropoff_location_id String,\n+            congestion_surcharge String,\n+            junk1 String,  junk2 String\n+        )\n+        Engine = ReplacingMergeTree()\n+        PRIMARY KEY (tpep_pickup_datetime, id)\n+        ORDER BY (tpep_pickup_datetime, id)\n+        PARTITION BY (toYYYYMMDD(tpep_pickup_datetime))\"\"\")\n+\n+        instance.query(\"\"\"CREATE TABLE dailyhistory.yellow_tripdata\n+            ON CLUSTER events\n+            AS dailyhistory.yellow_tripdata_staging\n+            ENGINE = Distributed('events', 'dailyhistory', yellow_tripdata_staging, sipHash64(id) % 3);\"\"\")\n+\n+        instance.query(\"\"\"INSERT INTO dailyhistory.yellow_tripdata\n+            SELECT * FROM generateRandom(\n+                'id UUID DEFAULT generateUUIDv4(),\n+                vendor_id String,\n+                tpep_pickup_datetime DateTime(\\\\'UTC\\\\'),\n+                tpep_dropoff_datetime DateTime(\\\\'UTC\\\\'),\n+                passenger_count Nullable(Float64),\n+                trip_distance String,\n+                pickup_longitude Float64,\n+                pickup_latitude Float64,\n+                rate_code_id String,\n+                store_and_fwd_flag String,\n+                dropoff_longitude Float64,\n+                dropoff_latitude Float64,\n+                payment_type String,\n+                fare_amount String,\n+                extra String,\n+                mta_tax String,\n+                tip_amount String,\n+                tolls_amount String,\n+                improvement_surcharge String,\n+                total_amount String,\n+                pickup_location_id String,\n+                dropoff_location_id String,\n+                congestion_surcharge String,\n+                junk1 String,\n+                junk2 String',\n+            1, 10, 2) LIMIT 50;\"\"\")\n+\n+        # monthly partition database\n+        instance.query(\"create database IF NOT EXISTS monthlyhistory on cluster events;\")\n+        instance.query(\"\"\"CREATE TABLE monthlyhistory.yellow_tripdata_staging ON CLUSTER events\n+        (\n+            id UUID DEFAULT generateUUIDv4(),\n+            vendor_id String,\n+            tpep_pickup_datetime DateTime('UTC'),\n+            tpep_dropoff_datetime DateTime('UTC'),\n+            passenger_count Nullable(Float64),\n+            trip_distance String,\n+            pickup_longitude Float64,\n+            pickup_latitude Float64,\n+            rate_code_id String,\n+            store_and_fwd_flag String,\n+            dropoff_longitude Float64,\n+            dropoff_latitude Float64,\n+            payment_type String,\n+            fare_amount String,\n+            extra String,\n+            mta_tax String,\n+            tip_amount String,\n+            tolls_amount String,\n+            improvement_surcharge String,\n+            total_amount String,\n+            pickup_location_id String,\n+            dropoff_location_id String,\n+            congestion_surcharge String,\n+            junk1 String,\n+            junk2 String\n+        )\n+        Engine = ReplacingMergeTree()\n+        PRIMARY KEY (tpep_pickup_datetime, id)\n+        ORDER BY (tpep_pickup_datetime, id)\n+        PARTITION BY (pickup_location_id, toYYYYMM(tpep_pickup_datetime))\"\"\")\n+\n+        instance.query(\"\"\"CREATE TABLE monthlyhistory.yellow_tripdata\n+            ON CLUSTER events\n+            AS monthlyhistory.yellow_tripdata_staging\n+            ENGINE = Distributed('events', 'monthlyhistory', yellow_tripdata_staging, sipHash64(id) % 3);\"\"\")\n+\n+\n+    def check(self):\n+        instance = cluster.instances[\"first\"]\n+        a = TSV(instance.query(\"SELECT count() from dailyhistory.yellow_tripdata\"))\n+        b = TSV(instance.query(\"SELECT count() from monthlyhistory.yellow_tripdata\"))\n+        assert a == b, \"Distributed tables\"\n+\n+        for instance_name, instance in cluster.instances.items():\n+            instance = cluster.instances[instance_name]\n+            a = instance.query(\"SELECT count() from dailyhistory.yellow_tripdata_staging\")\n+            b = instance.query(\"SELECT count() from monthlyhistory.yellow_tripdata_staging\")\n+            assert a == b, \"MergeTree tables on each shard\"\n+\n+            a = TSV(instance.query(\"SELECT sipHash64(*) from dailyhistory.yellow_tripdata_staging ORDER BY id\"))\n+            b = TSV(instance.query(\"SELECT sipHash64(*) from monthlyhistory.yellow_tripdata_staging ORDER BY id\"))\n+\n+            assert a == b, \"Data on each shard\"\n+\n+        for name in [\"first\", \"second\", \"third\"]:\n+            node = cluster.instances[name]\n+            node.query(\"DROP DATABASE IF EXISTS dailyhistory SYNC;\")\n+            node.query(\"DROP DATABASE IF EXISTS monthlyhistory SYNC;\")\n+\n+\n+\n+def execute_task(started_cluster, task, cmd_options):\n+    task.start()\n+\n+    zk = started_cluster.get_kazoo_client('zoo1')\n+    print(\"Use ZooKeeper server: {}:{}\".format(zk.hosts[0][0], zk.hosts[0][1]))\n+\n+    # Run cluster-copier processes on each node\n+    docker_api = docker.from_env().api\n+    copiers_exec_ids = []\n+\n+    cmd = ['/usr/bin/clickhouse', 'copier',\n+           '--config', '/etc/clickhouse-server/config-copier.xml',\n+           '--task-path', task.zk_task_path,\n+           '--task-file', task.container_task_file,\n+           '--task-upload-force', 'true',\n+           '--base-dir', '/var/log/clickhouse-server/copier']\n+    cmd += cmd_options\n+\n+    print(cmd)\n+\n+    for instance_name, instance in started_cluster.instances.items():\n+        instance = started_cluster.instances[instance_name]\n+        container = instance.get_docker_handle()\n+        instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, \"configs_three_nodes/config-copier.xml\"), \"/etc/clickhouse-server/config-copier.xml\")\n+        logging.info(\"Copied copier config to {}\".format(instance.name))\n+        exec_id = docker_api.exec_create(container.id, cmd, stderr=True)\n+        output = docker_api.exec_start(exec_id).decode('utf8')\n+        logging.info(output)\n+        copiers_exec_ids.append(exec_id)\n+        logging.info(\"Copier for {} ({}) has started\".format(instance.name, instance.ip_address))\n+\n+    # time.sleep(1000)\n+\n+    # Wait for copiers stopping and check their return codes\n+    for exec_id, instance in zip(copiers_exec_ids, iter(started_cluster.instances.values())):\n+        while True:\n+            res = docker_api.exec_inspect(exec_id)\n+            if not res['Running']:\n+                break\n+            time.sleep(1)\n+\n+        assert res['ExitCode'] == 0, \"Instance: {} ({}). Info: {}\".format(instance.name, instance.ip_address, repr(res))\n+\n+    try:\n+        task.check()\n+    finally:\n+        zk.delete(task.zk_task_path, recursive=True)\n+\n+\n+# Tests\n+@pytest.mark.timeout(600)\n+def test(started_cluster):\n+    execute_task(started_cluster, Task(started_cluster), [])\ndiff --git a/tests/integration/test_cluster_copier/test_trivial.py b/tests/integration/test_cluster_copier/test_trivial.py\nnew file mode 100644\nindex 000000000000..e58c6edcb4de\n--- /dev/null\n+++ b/tests/integration/test_cluster_copier/test_trivial.py\n@@ -0,0 +1,182 @@\n+import os\n+import sys\n+import time\n+import random\n+import string\n+\n+from helpers.cluster import ClickHouseCluster\n+from helpers.test_tools import TSV\n+\n+import kazoo\n+import pytest\n+import docker\n+\n+\n+CURRENT_TEST_DIR = os.path.dirname(os.path.abspath(__file__))\n+sys.path.insert(0, os.path.dirname(CURRENT_TEST_DIR))\n+\n+\n+COPYING_FAIL_PROBABILITY = 0.1\n+MOVING_FAIL_PROBABILITY = 0.1\n+\n+cluster = ClickHouseCluster(__file__, name='copier_test_trivial')\n+\n+\n+def generateRandomString(count):\n+    return ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(count))\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    global cluster\n+    try:\n+        for name in [\"first_trivial\", \"second_trivial\"]:\n+            instance = cluster.add_instance(name,\n+                main_configs=[\"configs/conf.d/clusters_trivial.xml\"],\n+                user_configs=[\"configs_two_nodes/users.xml\"],\n+                macros={\"cluster\" : name, \"shard\" : \"the_only_shard\", \"replica\" : \"the_only_replica\"},\n+                with_zookeeper=True)\n+\n+        cluster.start()\n+        yield cluster\n+\n+    finally:\n+        cluster.shutdown()\n+\n+\n+class TaskTrivial:\n+    def __init__(self, cluster):\n+        self.cluster = cluster\n+        self.zk_task_path = \"/clickhouse-copier/task_trivial\"\n+        self.copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task_trivial.xml'), 'r').read()\n+\n+    def start(self):\n+        source = cluster.instances['first_trivial']\n+        destination = cluster.instances['second_trivial']\n+\n+        for node in [source, destination]:\n+            node.query(\"DROP DATABASE IF EXISTS default\")\n+            node.query(\"CREATE DATABASE IF NOT EXISTS default\")\n+\n+        source.query(\"CREATE TABLE trivial (d UInt64, d1 UInt64 MATERIALIZED d+1)\"\n+                     \"ENGINE=ReplicatedMergeTree('/clickhouse/tables/source_trivial_cluster/1/trivial/{}', '1') \"\n+                     \"PARTITION BY d % 5 ORDER BY (d, sipHash64(d)) SAMPLE BY sipHash64(d) SETTINGS index_granularity = 16\".format(generateRandomString(10)))\n+\n+        source.query(\"INSERT INTO trivial SELECT * FROM system.numbers LIMIT 1002\",\n+                     settings={\"insert_distributed_sync\": 1})\n+\n+    def check(self):\n+        zk = cluster.get_kazoo_client('zoo1')\n+        status_data, _ = zk.get(self.zk_task_path + \"/status\")\n+        assert status_data == b'{\"hits\":{\"all_partitions_count\":5,\"processed_partitions_count\":5}}'\n+\n+        source = cluster.instances['first_trivial']\n+        destination = cluster.instances['second_trivial']\n+\n+        assert TSV(source.query(\"SELECT count() FROM trivial\")) == TSV(\"1002\\n\")\n+        assert TSV(destination.query(\"SELECT count() FROM trivial\")) == TSV(\"1002\\n\")\n+\n+        for node in [source, destination]:\n+            node.query(\"DROP TABLE trivial\")\n+\n+\n+class TaskReplicatedWithoutArguments:\n+    def __init__(self, cluster):\n+        self.cluster = cluster\n+        self.zk_task_path = \"/clickhouse-copier/task_trivial_without_arguments\"\n+        self.copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task_trivial_without_arguments.xml'), 'r').read()\n+\n+    def start(self):\n+        source = cluster.instances['first_trivial']\n+        destination = cluster.instances['second_trivial']\n+\n+        for node in [source, destination]:\n+            node.query(\"DROP DATABASE IF EXISTS default\")\n+            node.query(\"CREATE DATABASE IF NOT EXISTS default\")\n+\n+        source.query(\"CREATE TABLE trivial_without_arguments ON CLUSTER source_trivial_cluster (d UInt64, d1 UInt64 MATERIALIZED d+1) \"\n+                     \"ENGINE=ReplicatedMergeTree() \"\n+                     \"PARTITION BY d % 5 ORDER BY (d, sipHash64(d)) SAMPLE BY sipHash64(d) SETTINGS index_granularity = 16\")\n+\n+        source.query(\"INSERT INTO trivial_without_arguments SELECT * FROM system.numbers LIMIT 1002\",\n+                     settings={\"insert_distributed_sync\": 1})\n+\n+    def check(self):\n+        zk = cluster.get_kazoo_client('zoo1')\n+        status_data, _ = zk.get(self.zk_task_path + \"/status\")\n+        assert status_data == b'{\"hits\":{\"all_partitions_count\":5,\"processed_partitions_count\":5}}'\n+\n+        source = cluster.instances['first_trivial']\n+        destination = cluster.instances['second_trivial']\n+\n+        assert TSV(source.query(\"SELECT count() FROM trivial_without_arguments\")) == TSV(\"1002\\n\")\n+        assert TSV(destination.query(\"SELECT count() FROM trivial_without_arguments\")) == TSV(\"1002\\n\")\n+\n+        for node in [source, destination]:\n+            node.query(\"DROP TABLE trivial_without_arguments\")\n+\n+\n+def execute_task(started_cluster, task, cmd_options):\n+    task.start()\n+\n+    zk = started_cluster.get_kazoo_client('zoo1')\n+    print(\"Use ZooKeeper server: {}:{}\".format(zk.hosts[0][0], zk.hosts[0][1]))\n+\n+    try:\n+        zk.delete(\"/clickhouse-copier\", recursive=True)\n+    except kazoo.exceptions.NoNodeError:\n+        print(\"No node /clickhouse-copier. It is Ok in first test.\")\n+\n+    zk_task_path = task.zk_task_path\n+    zk.ensure_path(zk_task_path)\n+    zk.create(zk_task_path + \"/description\", task.copier_task_config.encode())\n+\n+    # Run cluster-copier processes on each node\n+    docker_api = started_cluster.docker_client.api\n+    copiers_exec_ids = []\n+\n+    cmd = ['/usr/bin/clickhouse', 'copier',\n+           '--config', '/etc/clickhouse-server/config-copier.xml',\n+           '--task-path', zk_task_path,\n+           '--base-dir', '/var/log/clickhouse-server/copier']\n+    cmd += cmd_options\n+\n+    copiers = list(started_cluster.instances.keys())\n+\n+    for instance_name in copiers:\n+        instance = started_cluster.instances[instance_name]\n+        container = instance.get_docker_handle()\n+        instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, \"configs/config-copier.xml\"),\n+                                        \"/etc/clickhouse-server/config-copier.xml\")\n+        print(\"Copied copier config to {}\".format(instance.name))\n+        exec_id = docker_api.exec_create(container.id, cmd, stderr=True)\n+        output = docker_api.exec_start(exec_id).decode('utf8')\n+        print(output)\n+        copiers_exec_ids.append(exec_id)\n+        print(\"Copier for {} ({}) has started\".format(instance.name, instance.ip_address))\n+\n+    # Wait for copiers stopping and check their return codes\n+    for exec_id, instance_name in zip(copiers_exec_ids, copiers):\n+        instance = started_cluster.instances[instance_name]\n+        while True:\n+            res = docker_api.exec_inspect(exec_id)\n+            if not res['Running']:\n+                break\n+            time.sleep(0.5)\n+\n+        assert res['ExitCode'] == 0, \"Instance: {} ({}). Info: {}\".format(instance.name, instance.ip_address, repr(res))\n+\n+    try:\n+        task.check()\n+    finally:\n+        zk.delete(zk_task_path, recursive=True)\n+\n+\n+# Tests\n+\n+def test_trivial_copy(started_cluster):\n+    execute_task(started_cluster, TaskTrivial(started_cluster), [])\n+\n+\n+def test_trivial_without_arguments(started_cluster):\n+    execute_task(started_cluster, TaskReplicatedWithoutArguments(started_cluster), [])\ndiff --git a/tests/integration/test_cluster_copier/test_two_nodes.py b/tests/integration/test_cluster_copier/test_two_nodes.py\nnew file mode 100644\nindex 000000000000..a6b2c82e00f9\n--- /dev/null\n+++ b/tests/integration/test_cluster_copier/test_two_nodes.py\n@@ -0,0 +1,493 @@\n+import os\n+import sys\n+import time\n+import logging\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+from helpers.test_tools import TSV\n+\n+import docker\n+\n+CURRENT_TEST_DIR = os.path.dirname(os.path.abspath(__file__))\n+sys.path.insert(0, os.path.dirname(CURRENT_TEST_DIR))\n+\n+cluster = ClickHouseCluster(__file__, name='copier_test_two_nodes')\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    global cluster\n+    try:\n+\n+        for name in [\"first_of_two\", \"second_of_two\"]:\n+            instance = cluster.add_instance(name,\n+                main_configs=[\n+                    \"configs_two_nodes/conf.d/clusters.xml\",\n+                    \"configs_two_nodes/conf.d/ddl.xml\",\n+                    \"configs_two_nodes/conf.d/storage_configuration.xml\"],\n+                user_configs=[\"configs_two_nodes/users.xml\"],\n+                with_zookeeper=True)\n+\n+        cluster.start()\n+\n+        for name in [\"first_of_two\", \"second_of_two\"]:\n+            instance = cluster.instances[name]\n+            instance.exec_in_container(['bash', '-c', 'mkdir /jbod1'])\n+            instance.exec_in_container(['bash', '-c', 'mkdir /jbod2'])\n+            instance.exec_in_container(['bash', '-c', 'mkdir /external'])\n+\n+        yield cluster\n+\n+    finally:\n+        cluster.shutdown()\n+\n+# Will copy table from `first` node to `second`\n+class TaskWithDifferentSchema:\n+    def __init__(self, cluster):\n+        self.cluster = cluster\n+        self.zk_task_path = '/clickhouse-copier/task_with_different_schema'\n+        self.container_task_file = \"/task_with_different_schema.xml\"\n+\n+        for instance_name, _ in cluster.instances.items():\n+            instance = cluster.instances[instance_name]\n+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_with_different_schema.xml'), self.container_task_file)\n+            print(\"Copied task file to container of '{}' instance. Path {}\".format(instance_name, self.container_task_file))\n+\n+    def start(self):\n+        first = cluster.instances[\"first_of_two\"]\n+        second = cluster.instances[\"second_of_two\"]\n+\n+        first.query(\"DROP DATABASE IF EXISTS db_different_schema SYNC\")\n+        second.query(\"DROP DATABASE IF EXISTS db_different_schema SYNC\")\n+\n+        first.query(\"CREATE DATABASE IF NOT EXISTS db_different_schema;\")\n+        first.query(\"\"\"CREATE TABLE db_different_schema.source\n+        (\n+            Column1 String,\n+            Column2 UInt32,\n+            Column3 Date,\n+            Column4 DateTime,\n+            Column5 UInt16,\n+            Column6 String,\n+            Column7 String,\n+            Column8 String,\n+            Column9 String,\n+            Column10 String,\n+            Column11 String,\n+            Column12 Decimal(3, 1),\n+            Column13 DateTime,\n+            Column14 UInt16\n+        )\n+        ENGINE = MergeTree()\n+        PARTITION BY (toYYYYMMDD(Column3), Column3)\n+        PRIMARY KEY (Column1, Column2, Column3, Column4, Column6, Column7, Column8, Column9)\n+        ORDER BY (Column1, Column2, Column3, Column4, Column6, Column7, Column8, Column9)\n+        SETTINGS index_granularity = 8192\"\"\")\n+\n+        first.query(\"\"\"INSERT INTO db_different_schema.source SELECT * FROM generateRandom(\n+            'Column1 String, Column2 UInt32, Column3 Date, Column4 DateTime, Column5 UInt16,\n+            Column6 String, Column7 String, Column8 String, Column9 String, Column10 String,\n+            Column11 String, Column12 Decimal(3, 1), Column13 DateTime, Column14 UInt16', 1, 10, 2) LIMIT 50;\"\"\")\n+\n+\n+        second.query(\"CREATE DATABASE IF NOT EXISTS db_different_schema;\")\n+        second.query(\"\"\"CREATE TABLE db_different_schema.destination\n+        (\n+            Column1 LowCardinality(String) CODEC(LZ4),\n+            Column2 UInt32 CODEC(LZ4),\n+            Column3 Date CODEC(DoubleDelta, LZ4),\n+            Column4 DateTime CODEC(DoubleDelta, LZ4),\n+            Column5 UInt16 CODEC(LZ4),\n+            Column6 LowCardinality(String) CODEC(ZSTD),\n+            Column7 LowCardinality(String) CODEC(ZSTD),\n+            Column8 LowCardinality(String) CODEC(ZSTD),\n+            Column9 LowCardinality(String) CODEC(ZSTD),\n+            Column10 String CODEC(ZSTD(6)),\n+            Column11 LowCardinality(String) CODEC(LZ4),\n+            Column12 Decimal(3,1) CODEC(LZ4),\n+            Column13 DateTime CODEC(DoubleDelta, LZ4),\n+            Column14 UInt16 CODEC(LZ4)\n+        ) ENGINE = MergeTree()\n+        PARTITION BY toYYYYMMDD(Column3)\n+        ORDER BY (Column9, Column1, Column2, Column3, Column4);\"\"\")\n+\n+        print(\"Preparation completed\")\n+\n+    def check(self):\n+        first = cluster.instances[\"first_of_two\"]\n+        second = cluster.instances[\"second_of_two\"]\n+\n+        a = first.query(\"SELECT count() from db_different_schema.source\")\n+        b = second.query(\"SELECT count() from db_different_schema.destination\")\n+        assert a == b, \"Count\"\n+\n+        a = TSV(first.query(\"\"\"SELECT sipHash64(*) from db_different_schema.source\n+            ORDER BY (Column1, Column2, Column3, Column4, Column5, Column6, Column7, Column8, Column9, Column10, Column11, Column12, Column13, Column14)\"\"\"))\n+        b = TSV(second.query(\"\"\"SELECT sipHash64(*) from db_different_schema.destination\n+            ORDER BY (Column1, Column2, Column3, Column4, Column5, Column6, Column7, Column8, Column9, Column10, Column11, Column12, Column13, Column14)\"\"\"))\n+        assert a == b, \"Data\"\n+\n+        first.query(\"DROP DATABASE IF EXISTS db_different_schema SYNC\")\n+        second.query(\"DROP DATABASE IF EXISTS db_different_schema SYNC\")\n+\n+\n+# Just simple copying, but table schema has TTL on columns\n+# Also table will have slightly different schema\n+class TaskTTL:\n+    def __init__(self, cluster):\n+        self.cluster = cluster\n+        self.zk_task_path = '/clickhouse-copier/task_ttl_columns'\n+        self.container_task_file = \"/task_ttl_columns.xml\"\n+\n+        for instance_name, _ in cluster.instances.items():\n+            instance = cluster.instances[instance_name]\n+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_ttl_columns.xml'), self.container_task_file)\n+            print(\"Copied task file to container of '{}' instance. Path {}\".format(instance_name, self.container_task_file))\n+\n+    def start(self):\n+        first = cluster.instances[\"first_of_two\"]\n+        second = cluster.instances[\"second_of_two\"]\n+\n+        first.query(\"DROP DATABASE IF EXISTS db_ttl_columns SYNC\")\n+        second.query(\"DROP DATABASE IF EXISTS db_ttl_columns SYNC\")\n+\n+        first.query(\"CREATE DATABASE IF NOT EXISTS db_ttl_columns;\")\n+        first.query(\"\"\"CREATE TABLE db_ttl_columns.source\n+        (\n+            Column1 String,\n+            Column2 UInt32,\n+            Column3 Date,\n+            Column4 DateTime,\n+            Column5 UInt16,\n+            Column6 String TTL now() + INTERVAL 1 MONTH,\n+            Column7 Decimal(3, 1) TTL now() + INTERVAL 1 MONTH,\n+            Column8 Tuple(Float64, Float64) TTL now() + INTERVAL 1 MONTH\n+        )\n+        ENGINE = MergeTree()\n+        PARTITION BY (toYYYYMMDD(Column3), Column3)\n+        PRIMARY KEY (Column1, Column2, Column3)\n+        ORDER BY (Column1, Column2, Column3)\n+        SETTINGS index_granularity = 8192\"\"\")\n+\n+        first.query(\"\"\"INSERT INTO db_ttl_columns.source SELECT * FROM generateRandom(\n+            'Column1 String, Column2 UInt32, Column3 Date, Column4 DateTime, Column5 UInt16,\n+            Column6 String, Column7 Decimal(3, 1), Column8 Tuple(Float64, Float64)', 1, 10, 2) LIMIT 50;\"\"\")\n+\n+        second.query(\"CREATE DATABASE IF NOT EXISTS db_ttl_columns;\")\n+        second.query(\"\"\"CREATE TABLE db_ttl_columns.destination\n+        (\n+            Column1 String,\n+            Column2 UInt32,\n+            Column3 Date,\n+            Column4 DateTime TTL now() + INTERVAL 1 MONTH,\n+            Column5 UInt16 TTL now() + INTERVAL 1 MONTH,\n+            Column6 String TTL now() + INTERVAL 1 MONTH,\n+            Column7 Decimal(3, 1) TTL now() + INTERVAL 1 MONTH,\n+            Column8 Tuple(Float64, Float64)\n+        ) ENGINE = MergeTree()\n+        PARTITION BY toYYYYMMDD(Column3)\n+        ORDER BY (Column3, Column2, Column1);\"\"\")\n+\n+        print(\"Preparation completed\")\n+\n+    def check(self):\n+        first = cluster.instances[\"first_of_two\"]\n+        second = cluster.instances[\"second_of_two\"]\n+\n+        a = first.query(\"SELECT count() from db_ttl_columns.source\")\n+        b = second.query(\"SELECT count() from db_ttl_columns.destination\")\n+        assert a == b, \"Count\"\n+\n+        a = TSV(first.query(\"\"\"SELECT sipHash64(*) from db_ttl_columns.source\n+            ORDER BY (Column1, Column2, Column3, Column4, Column5, Column6, Column7, Column8)\"\"\"))\n+        b = TSV(second.query(\"\"\"SELECT sipHash64(*) from db_ttl_columns.destination\n+            ORDER BY (Column1, Column2, Column3, Column4, Column5, Column6, Column7, Column8)\"\"\"))\n+        assert a == b, \"Data\"\n+\n+        first.query(\"DROP DATABASE IF EXISTS db_ttl_columns SYNC\")\n+        second.query(\"DROP DATABASE IF EXISTS db_ttl_columns SYNC\")\n+\n+\n+class TaskSkipIndex:\n+    def __init__(self, cluster):\n+        self.cluster = cluster\n+        self.zk_task_path = '/clickhouse-copier/task_skip_index'\n+        self.container_task_file = \"/task_skip_index.xml\"\n+\n+        for instance_name, _ in cluster.instances.items():\n+            instance = cluster.instances[instance_name]\n+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_skip_index.xml'), self.container_task_file)\n+            print(\"Copied task file to container of '{}' instance. Path {}\".format(instance_name, self.container_task_file))\n+\n+    def start(self):\n+        first = cluster.instances[\"first_of_two\"]\n+        second = cluster.instances[\"second_of_two\"]\n+\n+        first.query(\"DROP DATABASE IF EXISTS db_skip_index SYNC\")\n+        second.query(\"DROP DATABASE IF EXISTS db_skip_index SYNC\")\n+\n+        first.query(\"CREATE DATABASE IF NOT EXISTS db_skip_index;\")\n+        first.query(\"\"\"CREATE TABLE db_skip_index.source\n+        (\n+            Column1 UInt64,\n+            Column2 Int32,\n+            Column3 Date,\n+            Column4 DateTime,\n+            Column5 String,\n+            INDEX a (Column1 * Column2, Column5) TYPE minmax GRANULARITY 3,\n+            INDEX b (Column1 * length(Column5)) TYPE set(1000) GRANULARITY 4\n+        )\n+        ENGINE = MergeTree()\n+        PARTITION BY (toYYYYMMDD(Column3), Column3)\n+        PRIMARY KEY (Column1, Column2, Column3)\n+        ORDER BY (Column1, Column2, Column3)\n+        SETTINGS index_granularity = 8192\"\"\")\n+\n+        first.query(\"\"\"INSERT INTO db_skip_index.source SELECT * FROM generateRandom(\n+            'Column1 UInt64, Column2 Int32, Column3 Date, Column4 DateTime, Column5 String', 1, 10, 2) LIMIT 100;\"\"\")\n+\n+        second.query(\"CREATE DATABASE IF NOT EXISTS db_skip_index;\")\n+        second.query(\"\"\"CREATE TABLE db_skip_index.destination\n+        (\n+            Column1 UInt64,\n+            Column2 Int32,\n+            Column3 Date,\n+            Column4 DateTime,\n+            Column5 String,\n+            INDEX a (Column1 * Column2, Column5) TYPE minmax GRANULARITY 3,\n+            INDEX b (Column1 * length(Column5)) TYPE set(1000) GRANULARITY 4\n+        ) ENGINE = MergeTree()\n+        PARTITION BY toYYYYMMDD(Column3)\n+        ORDER BY (Column3, Column2, Column1);\"\"\")\n+\n+        print(\"Preparation completed\")\n+\n+    def check(self):\n+        first = cluster.instances[\"first_of_two\"]\n+        second = cluster.instances[\"second_of_two\"]\n+\n+        a = first.query(\"SELECT count() from db_skip_index.source\")\n+        b = second.query(\"SELECT count() from db_skip_index.destination\")\n+        assert a == b, \"Count\"\n+\n+        a = TSV(first.query(\"\"\"SELECT sipHash64(*) from db_skip_index.source\n+            ORDER BY (Column1, Column2, Column3, Column4, Column5)\"\"\"))\n+        b = TSV(second.query(\"\"\"SELECT sipHash64(*) from db_skip_index.destination\n+            ORDER BY (Column1, Column2, Column3, Column4, Column5)\"\"\"))\n+        assert a == b, \"Data\"\n+\n+        first.query(\"DROP DATABASE IF EXISTS db_skip_index SYNC\")\n+        second.query(\"DROP DATABASE IF EXISTS db_skip_index SYNC\")\n+\n+\n+class TaskTTLMoveToVolume:\n+    def __init__(self, cluster):\n+        self.cluster = cluster\n+        self.zk_task_path = '/clickhouse-copier/task_ttl_move_to_volume'\n+        self.container_task_file = \"/task_ttl_move_to_volume.xml\"\n+\n+        for instance_name, _ in cluster.instances.items():\n+            instance = cluster.instances[instance_name]\n+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_ttl_move_to_volume.xml'), self.container_task_file)\n+            print(\"Copied task file to container of '{}' instance. Path {}\".format(instance_name, self.container_task_file))\n+\n+    def start(self):\n+        first = cluster.instances[\"first_of_two\"]\n+        second = cluster.instances[\"first_of_two\"]\n+\n+        first.query(\"DROP DATABASE IF EXISTS db_move_to_volume SYNC\")\n+        second.query(\"DROP DATABASE IF EXISTS db_move_to_volume SYNC\")\n+\n+        first.query(\"CREATE DATABASE IF NOT EXISTS db_move_to_volume;\")\n+        first.query(\"\"\"CREATE TABLE db_move_to_volume.source\n+        (\n+            Column1 UInt64,\n+            Column2 Int32,\n+            Column3 Date,\n+            Column4 DateTime,\n+            Column5 String\n+        )\n+        ENGINE = MergeTree()\n+        PARTITION BY (toYYYYMMDD(Column3), Column3)\n+        PRIMARY KEY (Column1, Column2, Column3)\n+        ORDER BY (Column1, Column2, Column3)\n+        TTL Column3 + INTERVAL 1 MONTH TO VOLUME 'external'\n+        SETTINGS storage_policy = 'external_with_jbods';\"\"\")\n+\n+        first.query(\"\"\"INSERT INTO db_move_to_volume.source SELECT * FROM generateRandom(\n+            'Column1 UInt64, Column2 Int32, Column3 Date, Column4 DateTime, Column5 String', 1, 10, 2) LIMIT 100;\"\"\")\n+\n+        second.query(\"CREATE DATABASE IF NOT EXISTS db_move_to_volume;\")\n+        second.query(\"\"\"CREATE TABLE db_move_to_volume.destination\n+        (\n+            Column1 UInt64,\n+            Column2 Int32,\n+            Column3 Date,\n+            Column4 DateTime,\n+            Column5 String\n+        ) ENGINE = MergeTree()\n+        PARTITION BY toYYYYMMDD(Column3)\n+        ORDER BY (Column3, Column2, Column1)\n+        TTL Column3 + INTERVAL 1 MONTH TO VOLUME 'external'\n+        SETTINGS storage_policy = 'external_with_jbods';\"\"\")\n+\n+        print(\"Preparation completed\")\n+\n+    def check(self):\n+        first = cluster.instances[\"first_of_two\"]\n+        second = cluster.instances[\"second_of_two\"]\n+\n+        a = first.query(\"SELECT count() from db_move_to_volume.source\")\n+        b = second.query(\"SELECT count() from db_move_to_volume.destination\")\n+        assert a == b, \"Count\"\n+\n+        a = TSV(first.query(\"\"\"SELECT sipHash64(*) from db_move_to_volume.source\n+            ORDER BY (Column1, Column2, Column3, Column4, Column5)\"\"\"))\n+        b = TSV(second.query(\"\"\"SELECT sipHash64(*) from db_move_to_volume.destination\n+            ORDER BY (Column1, Column2, Column3, Column4, Column5)\"\"\"))\n+        assert a == b, \"Data\"\n+\n+        first.query(\"DROP DATABASE IF EXISTS db_move_to_volume SYNC\")\n+        second.query(\"DROP DATABASE IF EXISTS db_move_to_volume SYNC\")\n+\n+\n+class TaskDropTargetPartition:\n+    def __init__(self, cluster):\n+        self.cluster = cluster\n+        self.zk_task_path = '/clickhouse-copier/task_drop_target_partition'\n+        self.container_task_file = \"/task_drop_target_partition.xml\"\n+\n+        for instance_name, _ in cluster.instances.items():\n+            instance = cluster.instances[instance_name]\n+            instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, './task_drop_target_partition.xml'), self.container_task_file)\n+            print(\"Copied task file to container of '{}' instance. Path {}\".format(instance_name, self.container_task_file))\n+\n+    def start(self):\n+        first = cluster.instances[\"first_of_two\"]\n+        second = cluster.instances[\"second_of_two\"]\n+\n+        first.query(\"DROP DATABASE IF EXISTS db_drop_target_partition SYNC\")\n+        second.query(\"DROP DATABASE IF EXISTS db_drop_target_partition SYNC\")\n+\n+        first.query(\"CREATE DATABASE IF NOT EXISTS db_drop_target_partition;\")\n+        first.query(\"\"\"CREATE TABLE db_drop_target_partition.source\n+        (\n+            Column1 UInt64,\n+            Column2 Int32,\n+            Column3 Date,\n+            Column4 DateTime,\n+            Column5 String\n+        )\n+        ENGINE = MergeTree()\n+        PARTITION BY (toYYYYMMDD(Column3), Column3)\n+        PRIMARY KEY (Column1, Column2, Column3)\n+        ORDER BY (Column1, Column2, Column3);\"\"\")\n+\n+        first.query(\"\"\"INSERT INTO db_drop_target_partition.source SELECT * FROM generateRandom(\n+            'Column1 UInt64, Column2 Int32, Column3 Date, Column4 DateTime, Column5 String', 1, 10, 2) LIMIT 100;\"\"\")\n+\n+\n+        second.query(\"CREATE DATABASE IF NOT EXISTS db_drop_target_partition;\")\n+        second.query(\"\"\"CREATE TABLE db_drop_target_partition.destination\n+        (\n+            Column1 UInt64,\n+            Column2 Int32,\n+            Column3 Date,\n+            Column4 DateTime,\n+            Column5 String\n+        ) ENGINE = MergeTree()\n+        PARTITION BY toYYYYMMDD(Column3)\n+        ORDER BY (Column3, Column2, Column1);\"\"\")\n+\n+        # Insert data in target too. It has to be dropped.\n+        first.query(\"\"\"INSERT INTO db_drop_target_partition.destination SELECT * FROM db_drop_target_partition.source;\"\"\")\n+\n+        print(\"Preparation completed\")\n+\n+    def check(self):\n+        first = cluster.instances[\"first_of_two\"]\n+        second = cluster.instances[\"second_of_two\"]\n+\n+        a = first.query(\"SELECT count() from db_drop_target_partition.source\")\n+        b = second.query(\"SELECT count() from db_drop_target_partition.destination\")\n+        assert a == b, \"Count\"\n+\n+        a = TSV(first.query(\"\"\"SELECT sipHash64(*) from db_drop_target_partition.source\n+            ORDER BY (Column1, Column2, Column3, Column4, Column5)\"\"\"))\n+        b = TSV(second.query(\"\"\"SELECT sipHash64(*) from db_drop_target_partition.destination\n+            ORDER BY (Column1, Column2, Column3, Column4, Column5)\"\"\"))\n+        assert a == b, \"Data\"\n+\n+        first.query(\"DROP DATABASE IF EXISTS db_drop_target_partition SYNC\")\n+        second.query(\"DROP DATABASE IF EXISTS db_drop_target_partition SYNC\")\n+\n+\n+def execute_task(started_cluster, task, cmd_options):\n+    task.start()\n+\n+    zk = started_cluster.get_kazoo_client('zoo1')\n+    print(\"Use ZooKeeper server: {}:{}\".format(zk.hosts[0][0], zk.hosts[0][1]))\n+\n+    # Run cluster-copier processes on each node\n+    docker_api = docker.from_env().api\n+    copiers_exec_ids = []\n+\n+    cmd = ['/usr/bin/clickhouse', 'copier',\n+           '--config', '/etc/clickhouse-server/config-copier.xml',\n+           '--task-path', task.zk_task_path,\n+           '--task-file', task.container_task_file,\n+           '--task-upload-force', 'true',\n+           '--base-dir', '/var/log/clickhouse-server/copier']\n+    cmd += cmd_options\n+\n+    print(cmd)\n+\n+    for instance_name, instance in started_cluster.instances.items():\n+        instance = started_cluster.instances[instance_name]\n+        container = instance.get_docker_handle()\n+        instance.copy_file_to_container(os.path.join(CURRENT_TEST_DIR, \"configs_two_nodes/config-copier.xml\"), \"/etc/clickhouse-server/config-copier.xml\")\n+        logging.info(\"Copied copier config to {}\".format(instance.name))\n+        exec_id = docker_api.exec_create(container.id, cmd, stderr=True)\n+        output = docker_api.exec_start(exec_id).decode('utf8')\n+        logging.info(output)\n+        copiers_exec_ids.append(exec_id)\n+        logging.info(\"Copier for {} ({}) has started\".format(instance.name, instance.ip_address))\n+\n+    # time.sleep(1000)\n+\n+    # Wait for copiers stopping and check their return codes\n+    for exec_id, instance in zip(copiers_exec_ids, iter(started_cluster.instances.values())):\n+        while True:\n+            res = docker_api.exec_inspect(exec_id)\n+            if not res['Running']:\n+                break\n+            time.sleep(1)\n+\n+        assert res['ExitCode'] == 0, \"Instance: {} ({}). Info: {}\".format(instance.name, instance.ip_address, repr(res))\n+\n+    try:\n+        task.check()\n+    finally:\n+        zk.delete(task.zk_task_path, recursive=True)\n+\n+\n+# Tests\n+@pytest.mark.timeout(600)\n+def test_different_schema(started_cluster):\n+    execute_task(started_cluster, TaskWithDifferentSchema(started_cluster), [])\n+\n+\n+@pytest.mark.timeout(600)\n+def test_ttl_columns(started_cluster):\n+    execute_task(started_cluster, TaskTTL(started_cluster), [])\n+\n+\n+@pytest.mark.timeout(600)\n+def test_skip_index(started_cluster):\n+    execute_task(started_cluster, TaskSkipIndex(started_cluster), [])\n+\n+\n+@pytest.mark.skip(reason=\"Too flaky :(\")\n+def test_ttl_move_to_volume(started_cluster):\n+    execute_task(started_cluster, TaskTTLMoveToVolume(started_cluster), [])\ndiff --git a/tests/integration/test_cluster_copier/trivial_test.py b/tests/integration/test_cluster_copier/trivial_test.py\ndeleted file mode 100644\nindex 717ff9d8d34f..000000000000\n--- a/tests/integration/test_cluster_copier/trivial_test.py\n+++ /dev/null\n@@ -1,180 +0,0 @@\n-import os\n-import sys\n-import time\n-from contextlib import contextmanager\n-\n-import docker\n-import pytest\n-\n-CURRENT_TEST_DIR = os.path.dirname(os.path.abspath(__file__))\n-sys.path.insert(0, os.path.dirname(CURRENT_TEST_DIR))\n-from helpers.cluster import ClickHouseCluster\n-from helpers.test_tools import TSV\n-\n-COPYING_FAIL_PROBABILITY = 0.33\n-MOVING_FAIL_PROBABILITY = 0.1\n-cluster = None\n-\n-\n-@pytest.fixture(scope=\"function\")\n-def started_cluster():\n-    global cluster\n-    try:\n-        clusters_schema = {\n-            \"0\": {\"0\": [\"0\"]},\n-            \"1\": {\"0\": [\"0\"]}\n-        }\n-\n-        cluster = ClickHouseCluster(__file__)\n-\n-        for cluster_name, shards in clusters_schema.items():\n-            for shard_name, replicas in shards.items():\n-                for replica_name in replicas:\n-                    name = \"s{}_{}_{}\".format(cluster_name, shard_name, replica_name)\n-                    cluster.add_instance(name,\n-                                         main_configs=[], user_configs=[],\n-                                         macros={\"cluster\": cluster_name, \"shard\": shard_name, \"replica\": replica_name},\n-                                         with_zookeeper=True)\n-\n-        cluster.start()\n-        yield cluster\n-\n-    finally:\n-        pass\n-        cluster.shutdown()\n-\n-\n-class TaskTrivial:\n-    def __init__(self, cluster, use_sample_offset):\n-        self.cluster = cluster\n-        if use_sample_offset:\n-            self.zk_task_path = \"/clickhouse-copier/task_trivial_use_sample_offset\"\n-        else:\n-            self.zk_task_path = \"/clickhouse-copier/task_trivial\"\n-        self.copier_task_config = open(os.path.join(CURRENT_TEST_DIR, 'task_trivial.xml'), 'r').read()\n-\n-    def start(self):\n-        source = cluster.instances['s0_0_0']\n-        destination = cluster.instances['s1_0_0']\n-\n-        for node in [source, destination]:\n-            node.query(\"DROP DATABASE IF EXISTS default\")\n-            node.query(\"CREATE DATABASE IF NOT EXISTS default\")\n-\n-        source.query(\"CREATE TABLE trivial (d UInt64, d1 UInt64 MATERIALIZED d+1) \"\n-                     \"ENGINE=ReplicatedMergeTree('/clickhouse/tables/source_trivial_cluster/1/trivial', '1') \"\n-                     \"PARTITION BY d % 5 ORDER BY (d, sipHash64(d)) SAMPLE BY sipHash64(d) SETTINGS index_granularity = 16\")\n-\n-        source.query(\"INSERT INTO trivial SELECT * FROM system.numbers LIMIT 1002\",\n-                     settings={\"insert_distributed_sync\": 1})\n-\n-    def check(self):\n-        source = cluster.instances['s0_0_0']\n-        destination = cluster.instances['s1_0_0']\n-\n-        assert TSV(source.query(\"SELECT count() FROM trivial\")) == TSV(\"1002\\n\")\n-        assert TSV(destination.query(\"SELECT count() FROM trivial\")) == TSV(\"1002\\n\")\n-\n-        for node in [source, destination]:\n-            node.query(\"DROP TABLE trivial\")\n-\n-\n-def execute_task(started_cluster, task, cmd_options):\n-    task.start()\n-\n-    zk = started_cluster.get_kazoo_client('zoo1')\n-    print(\"Use ZooKeeper server: {}:{}\".format(zk.hosts[0][0], zk.hosts[0][1]))\n-\n-    zk_task_path = task.zk_task_path\n-    zk.ensure_path(zk_task_path)\n-    zk.create(zk_task_path + \"/description\", task.copier_task_config)\n-\n-    # Run cluster-copier processes on each node\n-    docker_api = started_cluster.docker_client.api\n-    copiers_exec_ids = []\n-\n-    cmd = ['/usr/bin/clickhouse', 'copier',\n-           '--config', '/etc/clickhouse-server/config-copier.xml',\n-           '--task-path', zk_task_path,\n-           '--base-dir', '/var/log/clickhouse-server/copier']\n-    cmd += cmd_options\n-\n-    print(cmd)\n-\n-    for instance_name, instance in started_cluster.instances.items():\n-        container = instance.get_docker_handle()\n-        exec_id = docker_api.exec_create(container.id, cmd, stderr=True)\n-        docker_api.exec_start(exec_id, detach=True)\n-\n-        copiers_exec_ids.append(exec_id)\n-        print(\"Copier for {} ({}) has started\".format(instance.name, instance.ip_address))\n-\n-    # Wait for copiers stopping and check their return codes\n-    for exec_id, instance in zip(copiers_exec_ids, iter(started_cluster.instances.values())):\n-        while True:\n-            res = docker_api.exec_inspect(exec_id)\n-            if not res['Running']:\n-                break\n-            time.sleep(1)\n-\n-        assert res['ExitCode'] == 0, \"Instance: {} ({}). Info: {}\".format(instance.name, instance.ip_address, repr(res))\n-\n-    try:\n-        task.check()\n-    finally:\n-        zk.delete(zk_task_path, recursive=True)\n-\n-\n-# Tests\n-\n-\n-@pytest.mark.parametrize(\n-    ('use_sample_offset'),\n-    [\n-        False,\n-        True\n-    ]\n-)\n-def test_trivial_copy(started_cluster, use_sample_offset):\n-    if use_sample_offset:\n-        execute_task(started_cluster, TaskTrivial(started_cluster, use_sample_offset), ['--experimental-use-sample-offset', '1'])\n-    else:\n-        print(\"AAAAA\")\n-        execute_task(started_cluster, TaskTrivial(started_cluster, use_sample_offset), [])\n-\n-\n-@pytest.mark.parametrize(\n-    ('use_sample_offset'),\n-    [\n-        False,\n-        True\n-    ]\n-)\n-def test_trivial_copy_with_copy_fault(started_cluster, use_sample_offset):\n-    if use_sample_offset:\n-        execute_task(started_cluster, TaskTrivial(started_cluster), ['--copy-fault-probability', str(COPYING_FAIL_PROBABILITY),\n-                                                    '--experimental-use-sample-offset', '1'])\n-    else:\n-        execute_task(started_cluster, TaskTrivial(started_cluster), ['--copy-fault-probability', str(COPYING_FAIL_PROBABILITY)])\n-\n-\n-@pytest.mark.parametrize(\n-    ('use_sample_offset'),\n-    [\n-        False,\n-        True\n-    ]\n-)\n-def test_trivial_copy_with_move_fault(started_cluster, use_sample_offset):\n-    if use_sample_offset:\n-        execute_task(started_cluster, TaskTrivial(started_cluster), ['--move-fault-probability', str(MOVING_FAIL_PROBABILITY),\n-                                                    '--experimental-use-sample-offset', '1'])\n-    else:\n-        execute_task(started_cluster, TaskTrivial(started_cluster), ['--move-fault-probability', str(MOVING_FAIL_PROBABILITY)])\n-\n-\n-if __name__ == '__main__':\n-    with contextmanager(started_cluster)() as cluster:\n-        for name, instance in list(cluster.instances.items()):\n-            print(name, instance.ip_address)\n-        input(\"Cluster created, press any key to destroy...\")\n",
  "problem_statement": "clickhouse-copier with slightly different schema of the target table\nHello.\r\nI am trying to use **clickhouse-copier** to migrate some tables to new Clickhouse cluster. But at the same time I want to alter some column types (for example, replace **String** with **LowCardinality(String)** . So I manually created all destination tables with new types and wrote a test task for **clickhouse-copier**.\r\n\r\nBut **clickhouse-copier** tasks are failing with error:\r\n```\r\nDB::Exception: Block structure mismatch in RemoteBlockOutputStream stream: different types:\r\n```\r\nwhile ordinary `INSERT INTO db.table SELECT * FROM remote(...) WHERE ...` queries are working fine.\r\n\r\nCan I use clickhouse-copier with different (but compatible) column types on source and destination clusters?\r\n\r\nSource cluster: **Clickhouse 18.16.1**\r\nDestination cluster: **Clickhouse 19.16.11.47**\n",
  "hints_text": "Could you please provide us more informative stacktrace? \r\nI think that you can't copy tables with different schema, because \"custom\" insert-select is implemented in this program. But this issue will be evaluated as a feature request :wink:\n@nikitamikhaylov Here is anonymized stacktrace and table DDL. I don't have any materialized or alias columns.\r\n\r\nclickhouse-copier stack trace:\r\n```\r\n2020.02.17 16:50:31.645519 [ 1 ] {} <Debug> ClusterCopier: Processing /clickhouse/copytasks/test-task/tables/destination_cluster.<db>.<table>/20201223/shards/1\r\n2020.02.17 16:50:31.646605 [ 1 ] {} <Debug> ClusterCopier: Partition 20201223 appears to be clean\r\n2020.02.17 16:50:31.670795 [ 1 ] {} <Trace> InterpreterSelectQuery: WithMergeableState -> Complete\r\n2020.02.17 16:50:31.670908 [ 1 ] {} <Trace> Aggregator: Reading blocks of partially aggregated data.\r\n2020.02.17 16:50:31.673832 [ 1 ] {} <Trace> Aggregator: Read 4 blocks of partially aggregated data, total 4 rows.\r\n2020.02.17 16:50:31.673877 [ 1 ] {} <Trace> Aggregator: Merging partially aggregated single-level data.\r\n2020.02.17 16:50:31.673902 [ 1 ] {} <Trace> Aggregator: Merged partially aggregated single-level data.\r\n2020.02.17 16:50:31.673927 [ 1 ] {} <Trace> Aggregator: Converting aggregated data to blocks\r\n2020.02.17 16:50:31.673961 [ 1 ] {} <Trace> Aggregator: Converted aggregated data to blocks. 1 rows, 0.000 MiB in 0.000 sec. (158002.844 rows/sec., 1.205 MiB/sec.)\r\n2020.02.17 16:50:31.673997 [ 1 ] {} <Trace> UnionBlockInputStream: Waiting for threads to finish\r\n2020.02.17 16:50:31.674020 [ 1 ] {} <Trace> UnionBlockInputStream: Waited for threads to finish\r\n2020.02.17 16:50:31.678025 [ 1 ] {} <Debug> ClusterCopier: Create destination tables. Query: CREATE TABLE IF NOT EXISTS <db>.<table> (`Column1` String, `Column2` UInt32, `Column3` Date, `Column4` DateTime, `Column5` UInt16, `Column6` String, `Column7` String, `Column8` String, `Column9` String, `Column10` String, `Column11` String, `Column12` Decimal(3, 1), `Column13` DateTime, `Column14` UInt16) ENGINE = MergeTree() PARTITION BY toYYYYMMDD(Column3) ORDER BY (Column9, Column1, Column2, Column3, Column4)\r\n2020.02.17 16:50:31.679754 [ 1 ] {} <Debug> ClusterCopier: Destination tables <db>.<table> have been created on 4 shards of 4\r\n2020.02.17 16:50:31.679947 [ 1 ] {} <Debug> ClusterCopier: Executing SELECT query and pull from N1 (having a replica 192.168.228.165:9000, pull table <db>.<table> of cluster source_cluster) : SELECT * FROM _local.`.read_shard_0.destination_cluster.<db>.<table>` WHERE toYYYYMMDD(Column3) = (20201223 AS partition_key)\r\n2020.02.17 16:50:31.680011 [ 1 ] {} <Debug> ClusterCopier: Executing INSERT query: INSERT INTO _local.`.split.destination_cluster.<db>.<table>` VALUES\r\n2020.02.17 16:50:31.680983 [ 1 ] {} <Trace> InterpreterSelectQuery: Complete -> Complete\r\n2020.02.17 16:50:31.772476 [ 1 ] {} <Error> ClusterCopier: An error occurred during copying, partition will be marked as dirty: Code: 171, e.displayText() = DB::Exception: Block structure mismatch in RemoteBlockOutputStream stream: different types:\r\n  Column1 String String(size = 3), Column2 UInt32 UInt32(size = 3), Column3 Date UInt16(size = 3), Column4 DateTime UInt32(size = 3), Column5 UInt16 UInt16(size = 3), Column6 String String(size = 3), Column7 String String(size = 3), Column8 String String(size = 3), Column9 String String(size = 3), Column10 String String(size = 3), Column11 String String(size = 3), Column12 Decimal(3, 1) Decimal32(size = 3), Column13 DateTime UInt32(size = 3), Column14 UInt16 UInt16(size = 3)\r\nColumn1 LowCardinality(String) ColumnLowCardinality(size = 0, UInt8(size = 0), ColumnUnique(size = 1, String(size = 1))), Column2 UInt16 UInt16(size = 0), Column3 Date UInt16(size = 0), Column4 DateTime UInt32(size = 0), Column5 UInt16 UInt16(size = 0), Column6 LowCardinality(String) ColumnLowCardinality(size = 0, UInt8(size = 0), ColumnUnique(size = 1, String(size = 1))), Column7 LowCardinality(String) ColumnLowCardinality(size = 0, UInt8(size = 0), ColumnUnique(size = 1, String(size = 1))), Column8 LowCardinality(String) ColumnLowCardinality(size = 0, UInt8(size = 0), ColumnUnique(size = 1, String(size = 1))), Column9 LowCardinality(String) ColumnLowCardinality(size = 0, UInt8(size = 0), ColumnUnique(size = 1, String(size = 1))), Column10 String String(size = 0), Column11 LowCardinality(String) ColumnLowCardinality(size = 0, UInt8(size = 0), ColumnUnique(size = 1, String(size = 1))), Column12 Decimal(3, 1) Decimal32(size = 0), Column13 DateTime UInt32(size = 0), Column14 UInt16 UInt16(size = 0): Insertion status:\r\n  Wrote 0 blocks and 0 rows on shard 0 replica 0, <server1>:9000 (average 2 ms per block)\r\nWrote 0 blocks and 0 rows on shard 1 replica 0, <server2>:9000 (average 2 ms per block)\r\nWrote 0 blocks and 0 rows on shard 2 replica 0, <server3>:9000 (average 3 ms per block)\r\nWrote 0 blocks and 0 rows on shard 3 replica 0, <server4>:9000 (average 3 ms per block)\r\n, Stack trace:\r\n  \r\n  0. 0x55ba5cf3f780 StackTrace::StackTrace() /usr/bin/clickhouse\r\n1. 0x55ba5cf3f555 DB::Exception::Exception(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) /usr/bin/clickhouse\r\n2. 0x55ba5cc30e40 ? /usr/bin/clickhouse\r\n3. 0x55ba5cc31afb ? /usr/bin/clickhouse\r\n4. 0x55ba608cb319 DB::RemoteBlockOutputStream::write(DB::Block const&) /usr/bin/clickhouse\r\n5. 0x55ba60588fb9 ? /usr/bin/clickhouse\r\n6. 0x55ba5cf8c11e ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::_List_iterator<ThreadFromGlobalPool>) /usr/bin/clickhouse\r\n7. 0x55ba5cf8c72e ThreadFromGlobalPool::ThreadFromGlobalPool<ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::function<void ()>, int, std::optional<unsigned long>)::{lambda()#3}>(ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::function<void ()>, int, std::optional<unsigned long>)::{lambda()#3}&&)::{lambda()#1}::operator()() const /usr/bin/clickhouse\r\n  8. 0x55ba5cf89bec ThreadPoolImpl<std::thread>::worker(std::_List_iterator<std::thread>) /usr/bin/clickhouse\r\n  9. 0x55ba62caca60 ? /usr/bin/clickhouse\r\n  10. 0x7f89334124a4 start_thread /lib/x86_64-linux-gnu/libpthread-2.24.so\r\n  11. 0x7f8932d48d0f clone /lib/x86_64-linux-gnu/libc-2.24.so\r\n  (version 19.16.11.47 (official build))\r\n```\r\n\r\nTable definition on old cluster:\r\n```sql\r\nCREATE TABLE <db>.<table>\r\n(\r\n    Column1 String,\r\n    Column2 UInt32,\r\n    Column3 Date,\r\n    Column4 DateTime,\r\n    Column5 UInt16,\r\n    Column6 String,\r\n    Column7 String,\r\n    Column8 String,\r\n    Column9 String,\r\n    Column10 String,\r\n    Column11 String,\r\n    Column12 Decimal(3, 1),\r\n    Column13 DateTime,\r\n    Column14 UInt16\r\n)\r\nENGINE = MergeTree()\r\nPARTITION BY (toYYYYMMDD(Column3), Column3)\r\nPRIMARY KEY (Column1, Column2, Column3, Column4, Column6, Column7, Column8, Column9)\r\nORDER BY (Column1, Column2, Column3, Column4, Column6, Column7, Column8, Column9)\r\nSETTINGS index_granularity = 8192\r\n```\r\n\r\ntable definition on new cluster:\r\n```sql\r\nCREATE TABLE <db>.<table> (\r\n  Column1 LowCardinality(String) CODEC(LZ4),\r\n  Column2 UInt16 CODEC(LZ4),\r\n  Column3 Date CODEC(DoubleDelta, LZ4),\r\n  Column4 DateTime CODEC(DoubleDelta, LZ4),\r\n  Column5 UInt16 CODEC(LZ4),\r\n  Column6 LowCardinality(String) CODEC(ZSTD),\r\n  Column7 LowCardinality(String) CODEC(ZSTD),\r\n  Column8 LowCardinality(String) CODEC(ZSTD),\r\n  Column9 LowCardinality(String) CODEC(ZSTD),\r\n  Column10 String CODEC(ZSTD(6)),\r\n  Column11 LowCardinality(String) CODEC(LZ4),\r\n  Column12 Decimal(3,1) CODEC(LZ4),\r\n  Column13 DateTime CODEC(DoubleDelta, LZ4),\r\n  Column14 UInt16 CODEC(LZ4)\r\n) ENGINE = MergeTree()\r\nPARTITION BY toYYYYMMDD(Column3)\r\nORDER BY (Column9, Column1, Column2, Column3, Column4);\r\n```\n@vpanfilov Try to install package clickhouse-common-static-dbg to get normal stacktrace. \nSorry, but I can't provide more detailed stack trace. But I am pretty sure that the actual error (_Block structure mismatch in RemoteBlockOutputStream stream: different types_) is triggered here: https://github.com/ClickHouse/ClickHouse/blob/575ddefa6c32c692f2db7ee8688ca586f3b1d622/dbms/src/Core/Block.cpp#L479-L481\r\n\r\nActual column structures are present in clickhouse-copier log above. I created a markdown table from it:\r\n\r\n| Column Name | Type in old cluster (Clickhouse 18.16.1)   | Type in new cluster  (Clickhouse 19.16.11.47)      |\r\n|------------------|------------------------------------|-------------------------------------------------------------------------------------------------------------------|\r\n|  Column1         |  String String(size = 3)           |  LowCardinality(String) ColumnLowCardinality(size = 0, UInt8(size = 0), ColumnUnique(size = 1, String(size = 1))) |\r\n|  Column2         |  UInt32 UInt32(size = 3)           |  UInt16 UInt16(size = 0)                                                                                          |\r\n|  Column3         |  Date UInt16(size = 3)             |  Date UInt16(size = 0)                                                                                            |\r\n|  Column4         |  DateTime UInt32(size = 3)         |  DateTime UInt32(size = 0)                                                                                        |\r\n|  Column5         |  UInt16 UInt16(size = 3)           |  UInt16 UInt16(size = 0)                                                                                          |\r\n|  Column6         |  String String(size = 3)           |  LowCardinality(String) ColumnLowCardinality(size = 0, UInt8(size = 0), ColumnUnique(size = 1, String(size = 1))) |\r\n|  Column7         |  String String(size = 3)           |  LowCardinality(String) ColumnLowCardinality(size = 0, UInt8(size = 0), ColumnUnique(size = 1, String(size = 1))) |\r\n|  Column8         |  String String(size = 3)           |  LowCardinality(String) ColumnLowCardinality(size = 0, UInt8(size = 0), ColumnUnique(size = 1, String(size = 1))) |\r\n|  Column9         |  String String(size = 3)           |  LowCardinality(String) ColumnLowCardinality(size = 0, UInt8(size = 0), ColumnUnique(size = 1, String(size = 1))) |\r\n|  Column10        |  String String(size = 3)           |  String String(size = 0),                                                                                         |\r\n|  Column11        |  String String(size = 3)           |  LowCardinality(String) ColumnLowCardinality(size = 0, UInt8(size = 0), ColumnUnique(size = 1, String(size = 1))) |\r\n|  Column12        |  Decimal(3, 1) Decimal32(size = 3) |  Decimal(3, 1) Decimal32(size = 0)                                                                                |\r\n|  Column13        |  DateTime UInt32(size = 3)         |  DateTime UInt32(size = 0)                                                                                        |\r\n|  Column14        |  UInt16 UInt16(size = 3)           |  UInt16 UInt16(size = 0)                                                                                          |",
  "created_at": "2021-04-22T18:09:52Z"
}