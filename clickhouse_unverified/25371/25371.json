{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 25371,
  "instance_id": "ClickHouse__ClickHouse-25371",
  "issue_numbers": [
    "9621"
  ],
  "base_commit": "93f15e443de2fd7e08c2ac88486f92b24ff71f4d",
  "patch": "diff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex 84e7500b064a..ecdecca156fb 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -469,6 +469,7 @@ class IColumn;\n     M(UnionMode, union_default_mode, UnionMode::Unspecified, \"Set default Union Mode in SelectWithUnion query. Possible values: empty string, 'ALL', 'DISTINCT'. If empty, query without Union Mode will throw exception.\", 0) \\\n     M(Bool, optimize_aggregators_of_group_by_keys, true, \"Eliminates min/max/any/anyLast aggregators of GROUP BY keys in SELECT section\", 0) \\\n     M(Bool, optimize_group_by_function_keys, true, \"Eliminates functions of other keys in GROUP BY section\", 0) \\\n+    M(Bool, legacy_column_name_of_tuple_literal, false, \"List all names of element of large tuple literals in their column names instead of hash. This settings exists only for compatibility reasons. It makes sense to set to 'true', while doing rolling update of cluster from version lower than 21.7 to higher.\", 0) \\\n     \\\n     M(Bool, query_plan_enable_optimizations, true, \"Apply optimizations to query plan\", 0) \\\n     M(UInt64, query_plan_max_optimizations_to_apply, 10000, \"Limit the total number of optimizations applied to query plan. If zero, ignored. If limit reached, throw exception\", 0) \\\ndiff --git a/src/Interpreters/ActionsVisitor.cpp b/src/Interpreters/ActionsVisitor.cpp\nindex 20f54e9b66e8..7aad11252cb8 100644\n--- a/src/Interpreters/ActionsVisitor.cpp\n+++ b/src/Interpreters/ActionsVisitor.cpp\n@@ -348,7 +348,7 @@ SetPtr makeExplicitSet(\n     const ASTPtr & left_arg = args.children.at(0);\n     const ASTPtr & right_arg = args.children.at(1);\n \n-    auto column_name = left_arg->getColumnName();\n+    auto column_name = left_arg->getColumnName(context->getSettingsRef());\n     const auto & dag_node = actions.findInIndex(column_name);\n     const DataTypePtr & left_arg_type = dag_node.result_type;\n \n@@ -641,7 +641,7 @@ std::optional<NameAndTypePair> ActionsMatcher::getNameAndTypeFromAST(const ASTPt\n {\n     // If the argument is a literal, we generated a unique column name for it.\n     // Use it instead of a generic display name.\n-    auto child_column_name = ast->getColumnName();\n+    auto child_column_name = ast->getColumnName(data.getContext()->getSettingsRef());\n     const auto * as_literal = ast->as<ASTLiteral>();\n     if (as_literal)\n     {\n@@ -703,7 +703,7 @@ ASTs ActionsMatcher::doUntuple(const ASTFunction * function, ActionsMatcher::Dat\n             func->setAlias(data.getUniqueName(\"_ut_\" + name));\n \n         auto function_builder = FunctionFactory::instance().get(func->name, data.getContext());\n-        data.addFunction(function_builder, {tuple_name_type->name, literal->getColumnName()}, func->getColumnName());\n+        data.addFunction(function_builder, {tuple_name_type->name, literal->getColumnName(data.getContext()->getSettingsRef())}, func->getColumnName(data.getContext()->getSettingsRef()));\n \n         columns.push_back(std::move(func));\n     }\n@@ -740,6 +740,7 @@ void ActionsMatcher::visit(ASTExpressionList & expression_list, const ASTPtr &,\n \n void ActionsMatcher::visit(const ASTIdentifier & identifier, const ASTPtr &, Data & data)\n {\n+\n     auto column_name = identifier.getColumnName();\n     if (data.hasColumn(column_name))\n         return;\n@@ -766,7 +767,7 @@ void ActionsMatcher::visit(const ASTIdentifier & identifier, const ASTPtr &, Dat\n \n void ActionsMatcher::visit(const ASTFunction & node, const ASTPtr & ast, Data & data)\n {\n-    auto column_name = ast->getColumnName();\n+    auto column_name = ast->getColumnName(data.getContext()->getSettingsRef());\n     if (data.hasColumn(column_name))\n         return;\n \n@@ -782,7 +783,7 @@ void ActionsMatcher::visit(const ASTFunction & node, const ASTPtr & ast, Data &\n         ASTPtr arg = node.arguments->children.at(0);\n         visit(arg, data);\n         if (!data.only_consts)\n-            data.addArrayJoin(arg->getColumnName(), column_name);\n+            data.addArrayJoin(arg->getColumnName(data.getContext()->getSettingsRef()), column_name);\n \n         return;\n     }\n@@ -804,7 +805,7 @@ void ActionsMatcher::visit(const ASTFunction & node, const ASTPtr & ast, Data &\n                 /// We are in the part of the tree that we are not going to compute. You just need to define types.\n                 /// Do not subquery and create sets. We replace \"in*\" function to \"in*IgnoreSet\".\n \n-                auto argument_name = node.arguments->children.at(0)->getColumnName();\n+                auto argument_name = node.arguments->children.at(0)->getColumnName(data.getContext()->getSettingsRef());\n \n                 data.addFunction(\n                         FunctionFactory::instance().get(node.name + \"IgnoreSet\", data.getContext()),\n@@ -933,7 +934,7 @@ void ActionsMatcher::visit(const ASTFunction & node, const ASTPtr & ast, Data &\n                 if (!prepared_set->empty())\n                     column.name = data.getUniqueName(\"__set\");\n                 else\n-                    column.name = child->getColumnName();\n+                    column.name = child->getColumnName(data.getContext()->getSettingsRef());\n \n                 if (!data.hasColumn(column.name))\n                 {\n@@ -1012,7 +1013,7 @@ void ActionsMatcher::visit(const ASTFunction & node, const ASTPtr & ast, Data &\n                     visit(lambda->arguments->children.at(1), data);\n                     auto lambda_dag = data.actions_stack.popLevel();\n \n-                    String result_name = lambda->arguments->children.at(1)->getColumnName();\n+                    String result_name = lambda->arguments->children.at(1)->getColumnName(data.getContext()->getSettingsRef());\n                     lambda_dag->removeUnusedActions(Names(1, result_name));\n \n                     auto lambda_actions = std::make_shared<ExpressionActions>(\n@@ -1027,7 +1028,7 @@ void ActionsMatcher::visit(const ASTFunction & node, const ASTPtr & ast, Data &\n                         if (findColumn(required_arg, lambda_arguments) == lambda_arguments.end())\n                             captured.push_back(required_arg);\n \n-                    /// We can not name `getColumnName()`,\n+                    /// We can not name `getColumnName(data.getContext()->getSettingsRef())`,\n                     ///  because it does not uniquely define the expression (the types of arguments can be different).\n                     String lambda_name = data.getUniqueName(\"__lambda\");\n \n@@ -1057,7 +1058,7 @@ void ActionsMatcher::visit(const ASTFunction & node, const ASTPtr & ast, Data &\n     if (arguments_present)\n     {\n         /// Calculate column name here again, because AST may be changed here (in case of untuple).\n-        data.addFunction(function_builder, argument_names, ast->getColumnName());\n+        data.addFunction(function_builder, argument_names, ast->getColumnName(data.getContext()->getSettingsRef()));\n     }\n }\n \n@@ -1071,7 +1072,7 @@ void ActionsMatcher::visit(const ASTLiteral & literal, const ASTPtr & /* ast */,\n     // AST here? Anyway, do not modify the column name if it is set already.\n     if (literal.unique_column_name.empty())\n     {\n-        const auto default_name = literal.getColumnName();\n+        const auto default_name = literal.getColumnName(data.getContext()->getSettingsRef());\n         const auto & index = data.actions_stack.getLastActionsIndex();\n         const auto * existing_column = index.tryGetNode(default_name);\n \n@@ -1151,7 +1152,7 @@ SetPtr ActionsMatcher::makeSet(const ASTFunction & node, Data & data, bool no_su\n         }\n \n         /// We get the stream of blocks for the subquery. Create Set and put it in place of the subquery.\n-        String set_id = right_in_operand->getColumnName();\n+        String set_id = right_in_operand->getColumnName(data.getContext()->getSettingsRef());\n \n         SubqueryForSet & subquery_for_set = data.subqueries_for_sets[set_id];\n \n@@ -1187,7 +1188,7 @@ SetPtr ActionsMatcher::makeSet(const ASTFunction & node, Data & data, bool no_su\n     {\n         const auto & last_actions = data.actions_stack.getLastActions();\n         const auto & index = data.actions_stack.getLastActionsIndex();\n-        if (index.contains(left_in_operand->getColumnName()))\n+        if (index.contains(left_in_operand->getColumnName(data.getContext()->getSettingsRef())))\n             /// An explicit enumeration of values in parentheses.\n             return makeExplicitSet(&node, last_actions, false, data.getContext(), data.set_size_limit, data.prepared_sets);\n         else\ndiff --git a/src/Interpreters/ExpressionAnalyzer.cpp b/src/Interpreters/ExpressionAnalyzer.cpp\nindex fe52b30da7b6..de7148d3adb6 100644\n--- a/src/Interpreters/ExpressionAnalyzer.cpp\n+++ b/src/Interpreters/ExpressionAnalyzer.cpp\n@@ -244,7 +244,7 @@ void ExpressionAnalyzer::analyzeAggregation()\n                     ssize_t size = group_asts.size();\n                     getRootActionsNoMakeSet(group_asts[i], true, temp_actions, false);\n \n-                    const auto & column_name = group_asts[i]->getColumnName();\n+                    const auto & column_name = group_asts[i]->getColumnName(getContext()->getSettingsRef());\n                     const auto * node = temp_actions->tryFindInIndex(column_name);\n                     if (!node)\n                         throw Exception(\"Unknown identifier (in GROUP BY): \" + column_name, ErrorCodes::UNKNOWN_IDENTIFIER);\n@@ -398,7 +398,7 @@ void SelectQueryExpressionAnalyzer::makeSetsForIndex(const ASTPtr & node)\n                 auto temp_actions = std::make_shared<ActionsDAG>(columns_after_join);\n                 getRootActions(left_in_operand, true, temp_actions);\n \n-                if (temp_actions->tryFindInIndex(left_in_operand->getColumnName()))\n+                if (temp_actions->tryFindInIndex(left_in_operand->getColumnName(getContext()->getSettingsRef())))\n                     makeExplicitSet(func, *temp_actions, true, getContext(), settings.size_limits_for_set, prepared_sets);\n             }\n         }\n@@ -446,7 +446,7 @@ bool ExpressionAnalyzer::makeAggregateDescriptions(ActionsDAGPtr & actions)\n         if (node->arguments)\n             getRootActionsNoMakeSet(node->arguments, true, actions);\n \n-        aggregate.column_name = node->getColumnName();\n+        aggregate.column_name = node->getColumnName(getContext()->getSettingsRef());\n \n         const ASTs & arguments = node->arguments ? node->arguments->children : ASTs();\n         aggregate.argument_names.resize(arguments.size());\n@@ -454,7 +454,7 @@ bool ExpressionAnalyzer::makeAggregateDescriptions(ActionsDAGPtr & actions)\n \n         for (size_t i = 0; i < arguments.size(); ++i)\n         {\n-            const std::string & name = arguments[i]->getColumnName();\n+            const std::string & name = arguments[i]->getColumnName(getContext()->getSettingsRef());\n             const auto * dag_node = actions->tryFindInIndex(name);\n             if (!dag_node)\n             {\n@@ -647,7 +647,7 @@ void ExpressionAnalyzer::makeWindowDescriptions(ActionsDAGPtr actions)\n         WindowFunctionDescription window_function;\n         window_function.function_node = function_node;\n         window_function.column_name\n-            = window_function.function_node->getColumnName();\n+            = window_function.function_node->getColumnName(getContext()->getSettingsRef());\n         window_function.function_parameters\n             = window_function.function_node->parameters\n                 ? getAggregateFunctionParametersArray(\n@@ -666,7 +666,7 @@ void ExpressionAnalyzer::makeWindowDescriptions(ActionsDAGPtr actions)\n         window_function.argument_names.resize(arguments.size());\n         for (size_t i = 0; i < arguments.size(); ++i)\n         {\n-            const std::string & name = arguments[i]->getColumnName();\n+            const std::string & name = arguments[i]->getColumnName(getContext()->getSettingsRef());\n             const auto * node = actions->tryFindInIndex(name);\n \n             if (!node)\n@@ -964,7 +964,7 @@ ActionsDAGPtr SelectQueryExpressionAnalyzer::appendPrewhere(\n \n     auto & step = chain.lastStep(sourceColumns());\n     getRootActions(select_query->prewhere(), only_types, step.actions());\n-    String prewhere_column_name = select_query->prewhere()->getColumnName();\n+    String prewhere_column_name = select_query->prewhere()->getColumnName(getContext()->getSettingsRef());\n     step.addRequiredOutput(prewhere_column_name);\n \n     const auto & node = step.actions()->findInIndex(prewhere_column_name);\n@@ -1061,7 +1061,7 @@ bool SelectQueryExpressionAnalyzer::appendWhere(ExpressionActionsChain & chain,\n \n     getRootActions(select_query->where(), only_types, step.actions());\n \n-    auto where_column_name = select_query->where()->getColumnName();\n+    auto where_column_name = select_query->where()->getColumnName(getContext()->getSettingsRef());\n     step.addRequiredOutput(where_column_name);\n \n     const auto & node = step.actions()->findInIndex(where_column_name);\n@@ -1086,7 +1086,7 @@ bool SelectQueryExpressionAnalyzer::appendGroupBy(ExpressionActionsChain & chain\n     ASTs asts = select_query->groupBy()->children;\n     for (const auto & ast : asts)\n     {\n-        step.addRequiredOutput(ast->getColumnName());\n+        step.addRequiredOutput(ast->getColumnName(getContext()->getSettingsRef()));\n         getRootActions(ast, only_types, step.actions());\n     }\n \n@@ -1114,7 +1114,7 @@ void SelectQueryExpressionAnalyzer::appendAggregateFunctionsArguments(Expression\n         for (const auto & name : desc.argument_names)\n             step.addRequiredOutput(name);\n \n-    /// Collect aggregates removing duplicates by node.getColumnName()\n+    /// Collect aggregates removing duplicates by node.getColumnName(getContext()->getSettingsRef())\n     /// It's not clear why we recollect aggregates (for query parts) while we're able to use previously collected ones (for entire query)\n     /// @note The original recollection logic didn't remove duplicates.\n     GetAggregatesVisitor::Data data;\n@@ -1169,7 +1169,7 @@ void SelectQueryExpressionAnalyzer::appendWindowFunctionsArguments(\n             // (2b) Required function argument columns.\n             for (const auto & a : f.function_node->arguments->children)\n             {\n-                step.addRequiredOutput(a->getColumnName());\n+                step.addRequiredOutput(a->getColumnName(getContext()->getSettingsRef()));\n             }\n         }\n \n@@ -1191,7 +1191,7 @@ bool SelectQueryExpressionAnalyzer::appendHaving(ExpressionActionsChain & chain,\n     ExpressionActionsChain::Step & step = chain.lastStep(aggregated_columns);\n \n     getRootActionsForHaving(select_query->having(), only_types, step.actions());\n-    step.addRequiredOutput(select_query->having()->getColumnName());\n+    step.addRequiredOutput(select_query->having()->getColumnName(getContext()->getSettingsRef()));\n \n     return true;\n }\n@@ -1215,7 +1215,7 @@ void SelectQueryExpressionAnalyzer::appendSelect(ExpressionActionsChain & chain,\n             continue;\n         }\n \n-        step.addRequiredOutput(child->getColumnName());\n+        step.addRequiredOutput(child->getColumnName(getContext()->getSettingsRef()));\n     }\n }\n \n@@ -1243,7 +1243,7 @@ ActionsDAGPtr SelectQueryExpressionAnalyzer::appendOrderBy(ExpressionActionsChai\n         if (!ast || ast->children.empty())\n             throw Exception(\"Bad order expression AST\", ErrorCodes::UNKNOWN_TYPE_OF_AST_NODE);\n         ASTPtr order_expression = ast->children.at(0);\n-        step.addRequiredOutput(order_expression->getColumnName());\n+        step.addRequiredOutput(order_expression->getColumnName(getContext()->getSettingsRef()));\n \n         if (ast->with_fill)\n             with_fill = true;\n@@ -1293,7 +1293,7 @@ bool SelectQueryExpressionAnalyzer::appendLimitBy(ExpressionActionsChain & chain\n \n     for (const auto & child : select_query->limitBy()->children)\n     {\n-        auto child_name = child->getColumnName();\n+        auto child_name = child->getColumnName(getContext()->getSettingsRef());\n         if (!aggregated_names.count(child_name))\n             step.addRequiredOutput(std::move(child_name));\n     }\n@@ -1309,13 +1309,15 @@ ActionsDAGPtr SelectQueryExpressionAnalyzer::appendProjectResult(ExpressionActio\n \n     NamesWithAliases result_columns;\n \n+    const auto & settings = getContext()->getSettingsRef();\n+\n     ASTs asts = select_query->select()->children;\n     for (const auto & ast : asts)\n     {\n-        String result_name = ast->getAliasOrColumnName();\n+        String result_name = ast->getAliasOrColumnName(settings);\n         if (required_result_columns.empty() || required_result_columns.count(result_name))\n         {\n-            std::string source_name = ast->getColumnName();\n+            std::string source_name = ast->getColumnName(settings);\n \n             /*\n              * For temporary columns created by ExpressionAnalyzer for literals,\n@@ -1357,7 +1359,7 @@ void ExpressionAnalyzer::appendExpression(ExpressionActionsChain & chain, const\n {\n     ExpressionActionsChain::Step & step = chain.lastStep(sourceColumns());\n     getRootActions(expr, only_types, step.actions());\n-    step.addRequiredOutput(expr->getColumnName());\n+    step.addRequiredOutput(expr->getColumnName(getContext()->getSettingsRef()));\n }\n \n \n@@ -1374,12 +1376,13 @@ ActionsDAGPtr ExpressionAnalyzer::getActionsDAG(bool add_aliases, bool project_r\n     else\n         asts = ASTs(1, query);\n \n+    const auto & settings = getContext()->getSettingsRef();\n     for (const auto & ast : asts)\n     {\n-        std::string name = ast->getColumnName();\n+        std::string name = ast->getColumnName(settings);\n         std::string alias;\n         if (add_aliases)\n-            alias = ast->getAliasOrColumnName();\n+            alias = ast->getAliasOrColumnName(settings);\n         else\n             alias = name;\n         result_columns.emplace_back(name, alias);\n@@ -1514,7 +1517,7 @@ ExpressionAnalysisResult::ExpressionAnalysisResult(\n \n         if (auto actions = query_analyzer.appendPrewhere(chain, !first_stage, additional_required_columns_after_prewhere))\n         {\n-            prewhere_info = std::make_shared<PrewhereDAGInfo>(actions, query.prewhere()->getColumnName());\n+            prewhere_info = std::make_shared<PrewhereDAGInfo>(actions, query.prewhere()->getColumnName(settings));\n \n             if (allowEarlyConstantFolding(*prewhere_info->prewhere_actions, settings))\n             {\n@@ -1524,7 +1527,7 @@ ExpressionAnalysisResult::ExpressionAnalysisResult(\n                     ExpressionActions(\n                         prewhere_info->prewhere_actions,\n                         ExpressionActionsSettings::fromSettings(context->getSettingsRef())).execute(before_prewhere_sample);\n-                    auto & column_elem = before_prewhere_sample.getByName(query.prewhere()->getColumnName());\n+                    auto & column_elem = before_prewhere_sample.getByName(query.prewhere()->getColumnName(settings));\n                     /// If the filter column is a constant, record it.\n                     if (column_elem.column)\n                         prewhere_constant_filter_description = ConstantFilterDescription(*column_elem.column);\n@@ -1559,7 +1562,7 @@ ExpressionAnalysisResult::ExpressionAnalysisResult(\n                     ExpressionActions(\n                         before_where,\n                         ExpressionActionsSettings::fromSettings(context->getSettingsRef())).execute(before_where_sample);\n-                    auto & column_elem = before_where_sample.getByName(query.where()->getColumnName());\n+                    auto & column_elem = before_where_sample.getByName(query.where()->getColumnName(settings));\n                     /// If the filter column is a constant, record it.\n                     if (column_elem.column)\n                         where_constant_filter_description = ConstantFilterDescription(*column_elem.column);\n@@ -1650,7 +1653,7 @@ ExpressionAnalysisResult::ExpressionAnalysisResult(\n             const auto * select_query = query_analyzer.getSelectQuery();\n             for (const auto & child : select_query->select()->children)\n             {\n-                step.addRequiredOutput(child->getColumnName());\n+                step.addRequiredOutput(child->getColumnName(settings));\n             }\n         }\n \n@@ -1706,7 +1709,8 @@ void ExpressionAnalysisResult::finalize(const ExpressionActionsChain & chain, si\n \n     if (hasWhere())\n     {\n-        where_column_name = query.where()->getColumnName();\n+        const auto & settings = chain.getContext()->getSettingsRef();\n+        where_column_name = query.where()->getColumnName(settings);\n         remove_where_filter = chain.steps.at(where_step_num)->required_output.find(where_column_name)->second;\n     }\n }\ndiff --git a/src/Interpreters/InterpreterSelectQuery.cpp b/src/Interpreters/InterpreterSelectQuery.cpp\nindex 7cca527cbc13..67c4ee76bdbe 100644\n--- a/src/Interpreters/InterpreterSelectQuery.cpp\n+++ b/src/Interpreters/InterpreterSelectQuery.cpp\n@@ -143,7 +143,7 @@ String InterpreterSelectQuery::generateFilterActions(ActionsDAGPtr & actions, co\n     SelectQueryExpressionAnalyzer analyzer(query_ast, syntax_result, context, metadata_snapshot);\n     actions = analyzer.simpleSelectActions();\n \n-    auto column_name = expr_list->children.at(0)->getColumnName();\n+    auto column_name = expr_list->children.at(0)->getColumnName(context->getSettingsRef());\n     actions->removeUnusedActions(NameSet{column_name});\n     actions->projectInput(false);\n \n@@ -779,7 +779,7 @@ static SortDescription getSortDescription(const ASTSelectQuery & query, ContextP\n     order_descr.reserve(query.orderBy()->children.size());\n     for (const auto & elem : query.orderBy()->children)\n     {\n-        String name = elem->children.front()->getColumnName();\n+        String name = elem->children.front()->getColumnName(context->getSettingsRef());\n         const auto & order_by_elem = elem->as<ASTOrderByElement &>();\n \n         std::shared_ptr<Collator> collator;\n@@ -798,14 +798,14 @@ static SortDescription getSortDescription(const ASTSelectQuery & query, ContextP\n     return order_descr;\n }\n \n-static SortDescription getSortDescriptionFromGroupBy(const ASTSelectQuery & query)\n+static SortDescription getSortDescriptionFromGroupBy(const ASTSelectQuery & query, ContextPtr context)\n {\n     SortDescription order_descr;\n     order_descr.reserve(query.groupBy()->children.size());\n \n     for (const auto & elem : query.groupBy()->children)\n     {\n-        String name = elem->getColumnName();\n+        String name = elem->getColumnName(context->getSettingsRef());\n         order_descr.emplace_back(name, 1, 1);\n     }\n \n@@ -1948,13 +1948,13 @@ void InterpreterSelectQuery::executeFetchColumns(QueryProcessingStage::Enum proc\n                 {\n                     query_info.projection->order_optimizer = std::make_shared<ReadInOrderOptimizer>(\n                         query_info.projection->group_by_elements_actions,\n-                        getSortDescriptionFromGroupBy(query),\n+                        getSortDescriptionFromGroupBy(query, context),\n                         query_info.syntax_analyzer_result);\n                 }\n                 else\n                 {\n                     query_info.order_optimizer = std::make_shared<ReadInOrderOptimizer>(\n-                        analysis_result.group_by_elements_actions, getSortDescriptionFromGroupBy(query), query_info.syntax_analyzer_result);\n+                        analysis_result.group_by_elements_actions, getSortDescriptionFromGroupBy(query, context), query_info.syntax_analyzer_result);\n                 }\n             }\n \n@@ -2033,7 +2033,7 @@ void InterpreterSelectQuery::executeFetchColumns(QueryProcessingStage::Enum proc\n void InterpreterSelectQuery::executeWhere(QueryPlan & query_plan, const ActionsDAGPtr & expression, bool remove_filter)\n {\n     auto where_step = std::make_unique<FilterStep>(\n-        query_plan.getCurrentDataStream(), expression, getSelectQuery().where()->getColumnName(), remove_filter);\n+        query_plan.getCurrentDataStream(), expression, getSelectQuery().where()->getColumnName(context->getSettingsRef()), remove_filter);\n \n     where_step->setStepDescription(\"WHERE\");\n     query_plan.addStep(std::move(where_step));\n@@ -2080,7 +2080,7 @@ void InterpreterSelectQuery::executeAggregation(QueryPlan & query_plan, const Ac\n     SortDescription group_by_sort_description;\n \n     if (group_by_info && settings.optimize_aggregation_in_order)\n-        group_by_sort_description = getSortDescriptionFromGroupBy(getSelectQuery());\n+        group_by_sort_description = getSortDescriptionFromGroupBy(getSelectQuery(), context);\n     else\n         group_by_info = nullptr;\n \n@@ -2128,7 +2128,7 @@ void InterpreterSelectQuery::executeMergeAggregated(QueryPlan & query_plan, bool\n void InterpreterSelectQuery::executeHaving(QueryPlan & query_plan, const ActionsDAGPtr & expression)\n {\n     auto having_step\n-        = std::make_unique<FilterStep>(query_plan.getCurrentDataStream(), expression, getSelectQuery().having()->getColumnName(), false);\n+        = std::make_unique<FilterStep>(query_plan.getCurrentDataStream(), expression, getSelectQuery().having()->getColumnName(context->getSettingsRef()), false);\n \n     having_step->setStepDescription(\"HAVING\");\n     query_plan.addStep(std::move(having_step));\n@@ -2144,7 +2144,7 @@ void InterpreterSelectQuery::executeTotalsAndHaving(\n         query_plan.getCurrentDataStream(),\n         overflow_row,\n         expression,\n-        has_having ? getSelectQuery().having()->getColumnName() : \"\",\n+        has_having ? getSelectQuery().having()->getColumnName(context->getSettingsRef()) : \"\",\n         settings.totals_mode,\n         settings.totals_auto_threshold,\n         final);\n@@ -2461,7 +2461,7 @@ void InterpreterSelectQuery::executeLimitBy(QueryPlan & query_plan)\n \n     Names columns;\n     for (const auto & elem : query.limitBy()->children)\n-        columns.emplace_back(elem->getColumnName());\n+        columns.emplace_back(elem->getColumnName(context->getSettingsRef()));\n \n     UInt64 length = getLimitUIntValue(query.limitByLength(), context, \"LIMIT\");\n     UInt64 offset = (query.limitByOffset() ? getLimitUIntValue(query.limitByOffset(), context, \"OFFSET\") : 0);\ndiff --git a/src/Interpreters/evaluateConstantExpression.cpp b/src/Interpreters/evaluateConstantExpression.cpp\nindex 2525f9672edc..90f6ac84afc5 100644\n--- a/src/Interpreters/evaluateConstantExpression.cpp\n+++ b/src/Interpreters/evaluateConstantExpression.cpp\n@@ -39,7 +39,7 @@ std::pair<Field, std::shared_ptr<const IDataType>> evaluateConstantExpression(co\n     if (context->getSettingsRef().normalize_function_names)\n         FunctionNameNormalizer().visit(ast.get());\n \n-    String name = ast->getColumnName();\n+    String name = ast->getColumnName(context->getSettingsRef());\n     auto syntax_result = TreeRewriter(context).analyze(ast, source_columns);\n     ExpressionActionsPtr expr_for_constant_folding = ExpressionAnalyzer(ast, syntax_result, context).getConstActions();\n \ndiff --git a/src/Parsers/ASTFunction.cpp b/src/Parsers/ASTFunction.cpp\nindex 6931008c7753..dfbf2532f1fd 100644\n--- a/src/Parsers/ASTFunction.cpp\n+++ b/src/Parsers/ASTFunction.cpp\n@@ -24,6 +24,16 @@ namespace ErrorCodes\n }\n \n void ASTFunction::appendColumnNameImpl(WriteBuffer & ostr) const\n+{\n+    appendColumnNameImpl(ostr, nullptr);\n+}\n+\n+void ASTFunction::appendColumnNameImpl(WriteBuffer & ostr, const Settings & settings) const\n+{\n+    appendColumnNameImpl(ostr, &settings);\n+}\n+\n+void ASTFunction::appendColumnNameImpl(WriteBuffer & ostr, const Settings * settings) const\n {\n     if (name == \"view\")\n         throw Exception(\"Table function view cannot be used as an expression\", ErrorCodes::UNEXPECTED_EXPRESSION);\n@@ -37,19 +47,30 @@ void ASTFunction::appendColumnNameImpl(WriteBuffer & ostr) const\n         {\n             if (it != parameters->children.begin())\n                 writeCString(\", \", ostr);\n-            (*it)->appendColumnName(ostr);\n+\n+            if (settings)\n+                (*it)->appendColumnName(ostr, *settings);\n+            else\n+                (*it)->appendColumnName(ostr);\n         }\n         writeChar(')', ostr);\n     }\n \n     writeChar('(', ostr);\n     if (arguments)\n+    {\n         for (auto it = arguments->children.begin(); it != arguments->children.end(); ++it)\n         {\n             if (it != arguments->children.begin())\n                 writeCString(\", \", ostr);\n-            (*it)->appendColumnName(ostr);\n+\n+            if (settings)\n+                (*it)->appendColumnName(ostr, *settings);\n+            else\n+                (*it)->appendColumnName(ostr);\n         }\n+    }\n+\n     writeChar(')', ostr);\n \n     if (is_window_function)\n@@ -61,11 +82,11 @@ void ASTFunction::appendColumnNameImpl(WriteBuffer & ostr) const\n         }\n         else\n         {\n-            FormatSettings settings{ostr, true /* one_line */};\n+            FormatSettings format_settings{ostr, true /* one_line */};\n             FormatState state;\n             FormatStateStacked frame;\n             writeCString(\"(\", ostr);\n-            window_definition->formatImpl(settings, state, frame);\n+            window_definition->formatImpl(format_settings, state, frame);\n             writeCString(\")\", ostr);\n         }\n     }\ndiff --git a/src/Parsers/ASTFunction.h b/src/Parsers/ASTFunction.h\nindex 685aaaadd262..8e657afbf6e5 100644\n--- a/src/Parsers/ASTFunction.h\n+++ b/src/Parsers/ASTFunction.h\n@@ -54,6 +54,10 @@ class ASTFunction : public ASTWithAlias\n protected:\n     void formatImplWithoutAlias(const FormatSettings & settings, FormatState & state, FormatStateStacked frame) const override;\n     void appendColumnNameImpl(WriteBuffer & ostr) const override;\n+    void appendColumnNameImpl(WriteBuffer & ostr, const Settings & settings) const override;\n+\n+private:\n+    void appendColumnNameImpl(WriteBuffer & ostr, const Settings * settings) const;\n };\n \n \ndiff --git a/src/Parsers/ASTLiteral.cpp b/src/Parsers/ASTLiteral.cpp\nindex f7947fe7a24e..c456cb3e9334 100644\n--- a/src/Parsers/ASTLiteral.cpp\n+++ b/src/Parsers/ASTLiteral.cpp\n@@ -17,8 +17,10 @@ void ASTLiteral::updateTreeHashImpl(SipHash & hash_state) const\n     applyVisitor(FieldVisitorHash(hash_state), value);\n }\n \n+namespace\n+{\n+\n /// Writes 'tuple' word before tuple literals for backward compatibility reasons.\n-/// TODO: remove, when versions lower than 20.3 will be rarely used.\n class FieldVisitorToColumnName : public StaticVisitor<String>\n {\n public:\n@@ -46,14 +48,51 @@ String FieldVisitorToColumnName::operator() (const Tuple & x) const\n     return wb.str();\n }\n \n+}\n+\n+void ASTLiteral::appendColumnNameImpl(WriteBuffer & ostr, const Settings & settings) const\n+{\n+    if (settings.legacy_column_name_of_tuple_literal)\n+        appendColumnNameImplLegacy(ostr);\n+    else\n+        appendColumnNameImpl(ostr);\n+}\n+\n void ASTLiteral::appendColumnNameImpl(WriteBuffer & ostr) const\n {\n     /// 100 - just arbitrary value.\n     constexpr auto min_elements_for_hashing = 100;\n \n+    /// Special case for very large arrays and tuples. Instead of listing all elements, will use hash of them.\n+    /// (Otherwise column name will be too long, that will lead to significant slowdown of expression analysis.)\n+    auto type = value.getType();\n+    if ((type == Field::Types::Array && value.get<const Array &>().size() > min_elements_for_hashing)\n+        || (type == Field::Types::Tuple && value.get<const Tuple &>().size() > min_elements_for_hashing))\n+    {\n+        SipHash hash;\n+        applyVisitor(FieldVisitorHash(hash), value);\n+        UInt64 low, high;\n+        hash.get128(low, high);\n+\n+        writeCString(type == Field::Types::Array ? \"__array_\" : \"__tuple_\", ostr);\n+        writeText(low, ostr);\n+        ostr.write('_');\n+        writeText(high, ostr);\n+    }\n+    else\n+    {\n+        String column_name = applyVisitor(FieldVisitorToString(), value);\n+        writeString(column_name, ostr);\n+    }\n+}\n+\n+void ASTLiteral::appendColumnNameImplLegacy(WriteBuffer & ostr) const\n+{\n+     /// 100 - just arbitrary value.\n+    constexpr auto min_elements_for_hashing = 100;\n+\n     /// Special case for very large arrays. Instead of listing all elements, will use hash of them.\n     /// (Otherwise column name will be too long, that will lead to significant slowdown of expression analysis.)\n-    /// TODO: Also do hashing for large tuples, when versions lower than 20.3 will be rarely used, because it breaks backward compatibility.\n     auto type = value.getType();\n     if ((type == Field::Types::Array && value.get<const Array &>().size() > min_elements_for_hashing))\n     {\ndiff --git a/src/Parsers/ASTLiteral.h b/src/Parsers/ASTLiteral.h\nindex 66d013d78a90..c17310f719bc 100644\n--- a/src/Parsers/ASTLiteral.h\n+++ b/src/Parsers/ASTLiteral.h\n@@ -44,6 +44,13 @@ class ASTLiteral : public ASTWithAlias\n     void formatImplWithoutAlias(const FormatSettings & settings, FormatState &, FormatStateStacked) const override;\n \n     void appendColumnNameImpl(WriteBuffer & ostr) const override;\n+    void appendColumnNameImpl(WriteBuffer & ostr, const Settings & settings) const override;\n+\n+private:\n+    /// Legacy version of 'appendColumnNameImpl'. It differs only with tuple literals.\n+    /// It's only needed to continue working of queries with tuple literals\n+    /// in distributed tables while rolling update.\n+    void appendColumnNameImplLegacy(WriteBuffer & ostr) const;\n };\n \n }\ndiff --git a/src/Parsers/ASTWithAlias.cpp b/src/Parsers/ASTWithAlias.cpp\nindex 88f6568a7196..0f5b86763e0c 100644\n--- a/src/Parsers/ASTWithAlias.cpp\n+++ b/src/Parsers/ASTWithAlias.cpp\n@@ -48,6 +48,14 @@ void ASTWithAlias::appendColumnName(WriteBuffer & ostr) const\n         appendColumnNameImpl(ostr);\n }\n \n+void ASTWithAlias::appendColumnName(WriteBuffer & ostr, const Settings & settings) const\n+{\n+    if (prefer_alias_to_column_name && !alias.empty())\n+        writeString(alias, ostr);\n+    else\n+        appendColumnNameImpl(ostr, settings);\n+}\n+\n void ASTWithAlias::appendColumnNameWithoutAlias(WriteBuffer & ostr) const\n {\n     appendColumnNameImpl(ostr);\ndiff --git a/src/Parsers/ASTWithAlias.h b/src/Parsers/ASTWithAlias.h\nindex ea4419402b0b..249be17b74ca 100644\n--- a/src/Parsers/ASTWithAlias.h\n+++ b/src/Parsers/ASTWithAlias.h\n@@ -21,8 +21,10 @@ class ASTWithAlias : public IAST\n     using IAST::IAST;\n \n     void appendColumnName(WriteBuffer & ostr) const final;\n+    void appendColumnName(WriteBuffer & ostr, const Settings & settings) const final;\n     void appendColumnNameWithoutAlias(WriteBuffer & ostr) const final;\n     String getAliasOrColumnName() const override { return alias.empty() ? getColumnName() : alias; }\n+    String getAliasOrColumnName(const Settings & settings) const override { return alias.empty() ? getColumnName(settings) : alias; }\n     String tryGetAlias() const override { return alias; }\n     void setAlias(const String & to) override { alias = to; }\n \n@@ -33,6 +35,7 @@ class ASTWithAlias : public IAST\n \n protected:\n     virtual void appendColumnNameImpl(WriteBuffer & ostr) const = 0;\n+    virtual void appendColumnNameImpl(WriteBuffer & ostr, const Settings &) const { appendColumnNameImpl(ostr); }\n };\n \n /// helper for setting aliases and chaining result to other functions\ndiff --git a/src/Parsers/IAST.cpp b/src/Parsers/IAST.cpp\nindex 3a21d704eb90..0f38fcf98dd8 100644\n--- a/src/Parsers/IAST.cpp\n+++ b/src/Parsers/IAST.cpp\n@@ -109,6 +109,14 @@ String IAST::getColumnName() const\n }\n \n \n+String IAST::getColumnName(const Settings & settings) const\n+{\n+    WriteBufferFromOwnString write_buffer;\n+    appendColumnName(write_buffer, settings);\n+    return write_buffer.str();\n+}\n+\n+\n String IAST::getColumnNameWithoutAlias() const\n {\n     WriteBufferFromOwnString write_buffer;\ndiff --git a/src/Parsers/IAST.h b/src/Parsers/IAST.h\nindex 54e08b2700e0..143094e1d7af 100644\n--- a/src/Parsers/IAST.h\n+++ b/src/Parsers/IAST.h\n@@ -5,6 +5,7 @@\n #include <Parsers/IdentifierQuotingStyle.h>\n #include <Common/Exception.h>\n #include <Common/TypePromotion.h>\n+#include <Core/Settings.h>\n #include <IO/WriteBufferFromString.h>\n \n #include <algorithm>\n@@ -41,13 +42,18 @@ class IAST : public std::enable_shared_from_this<IAST>, public TypePromotion<IAS\n \n     /** Get the canonical name of the column if the element is a column */\n     String getColumnName() const;\n+    String getColumnName(const Settings & settings) const;\n+\n     /** Same as the above but ensure no alias names are used. This is for index analysis */\n     String getColumnNameWithoutAlias() const;\n+\n     virtual void appendColumnName(WriteBuffer &) const\n     {\n         throw Exception(\"Trying to get name of not a column: \" + getID(), ErrorCodes::LOGICAL_ERROR);\n     }\n \n+    virtual void appendColumnName(WriteBuffer & ostr, const Settings &) const { appendColumnName(ostr); }\n+\n     virtual void appendColumnNameWithoutAlias(WriteBuffer &) const\n     {\n         throw Exception(\"Trying to get name of not a column: \" + getID(), ErrorCodes::LOGICAL_ERROR);\n@@ -55,6 +61,7 @@ class IAST : public std::enable_shared_from_this<IAST>, public TypePromotion<IAS\n \n     /** Get the alias, if any, or the canonical name of the column, if it is not. */\n     virtual String getAliasOrColumnName() const { return getColumnName(); }\n+    virtual String getAliasOrColumnName(const Settings & settings) const { return getColumnName(settings); }\n \n     /** Get the alias, if any, or an empty string if it does not exist, or if the element does not support aliases. */\n     virtual String tryGetAlias() const { return String(); }\n",
  "test_patch": "diff --git a/tests/integration/test_distributed_backward_compatability/configs/legacy.xml b/tests/integration/test_distributed_backward_compatability/configs/legacy.xml\nnew file mode 100644\nindex 000000000000..01bd56de8454\n--- /dev/null\n+++ b/tests/integration/test_distributed_backward_compatability/configs/legacy.xml\n@@ -0,0 +1,7 @@\n+<yandex>\n+    <profiles>\n+        <default>\n+            <legacy_column_name_of_tuple_literal>1</legacy_column_name_of_tuple_literal>\n+        </default>\n+    </profiles>\n+</yandex>\ndiff --git a/tests/integration/test_distributed_backward_compatability/test.py b/tests/integration/test_distributed_backward_compatability/test.py\nindex eb18019c8dfd..0d36aaa23f41 100644\n--- a/tests/integration/test_distributed_backward_compatability/test.py\n+++ b/tests/integration/test_distributed_backward_compatability/test.py\n@@ -5,8 +5,8 @@\n cluster = ClickHouseCluster(__file__)\n \n node_old = cluster.add_instance('node1', main_configs=['configs/remote_servers.xml'], image='yandex/clickhouse-server',\n-                                tag='19.17.8.54', stay_alive=True, with_installed_binary=True)\n-node_new = cluster.add_instance('node2', main_configs=['configs/remote_servers.xml'])\n+                                tag='20.8.9.6', stay_alive=True, with_installed_binary=True)\n+node_new = cluster.add_instance('node2', main_configs=['configs/remote_servers.xml'], user_configs=['configs/legacy.xml'])\n \n \n @pytest.fixture(scope=\"module\")\ndiff --git a/tests/queries/0_stateless/01447_json_strings.reference b/tests/queries/0_stateless/01447_json_strings.reference\nindex ab88e2f36965..7892cb82922d 100644\n--- a/tests/queries/0_stateless/01447_json_strings.reference\n+++ b/tests/queries/0_stateless/01447_json_strings.reference\n@@ -14,7 +14,7 @@\n \t\t\t\"type\": \"Array(UInt8)\"\n \t\t},\n \t\t{\n-\t\t\t\"name\": \"tuple(1, 'a')\",\n+\t\t\t\"name\": \"(1, 'a')\",\n \t\t\t\"type\": \"Tuple(UInt8, String)\"\n \t\t},\n \t\t{\n@@ -33,7 +33,7 @@\n \t\t\t\"1\": \"1\",\n \t\t\t\"'a'\": \"a\",\n \t\t\t\"[1, 2, 3]\": \"[1,2,3]\",\n-\t\t\t\"tuple(1, 'a')\": \"(1,'a')\",\n+\t\t\t\"(1, 'a')\": \"(1,'a')\",\n \t\t\t\"NULL\": \"\u1d3a\u1d41\u1d38\u1d38\",\n \t\t\t\"nan\": \"nan\"\n \t\t}\ndiff --git a/tests/queries/0_stateless/01449_json_compact_strings.reference b/tests/queries/0_stateless/01449_json_compact_strings.reference\nindex 1c6f073c0d07..53dba71d6ff1 100644\n--- a/tests/queries/0_stateless/01449_json_compact_strings.reference\n+++ b/tests/queries/0_stateless/01449_json_compact_strings.reference\n@@ -14,7 +14,7 @@\n \t\t\t\"type\": \"Array(UInt8)\"\n \t\t},\n \t\t{\n-\t\t\t\"name\": \"tuple(1, 'a')\",\n+\t\t\t\"name\": \"(1, 'a')\",\n \t\t\t\"type\": \"Tuple(UInt8, String)\"\n \t\t},\n \t\t{\ndiff --git a/tests/queries/0_stateless/01913_names_of_tuple_literal.reference b/tests/queries/0_stateless/01913_names_of_tuple_literal.reference\nnew file mode 100644\nindex 000000000000..a4c05ad853a5\n--- /dev/null\n+++ b/tests/queries/0_stateless/01913_names_of_tuple_literal.reference\n@@ -0,0 +1,4 @@\n+((1, 2), (2, 3), (3, 4))\n+((1,2),(2,3),(3,4))\n+tuple(tuple(1, 2), tuple(2, 3), tuple(3, 4))\n+((1,2),(2,3),(3,4))\ndiff --git a/tests/queries/0_stateless/01913_names_of_tuple_literal.sql b/tests/queries/0_stateless/01913_names_of_tuple_literal.sql\nnew file mode 100644\nindex 000000000000..09de9e8cf37e\n--- /dev/null\n+++ b/tests/queries/0_stateless/01913_names_of_tuple_literal.sql\n@@ -0,0 +1,2 @@\n+SELECT ((1, 2), (2, 3), (3, 4)) FORMAT TSVWithNames;\n+SELECT ((1, 2), (2, 3), (3, 4)) FORMAT TSVWithNames SETTINGS legacy_column_name_of_tuple_literal = 1;\ndiff --git a/tests/testflows/extended_precision_data_types/snapshots/common.py.tests.snapshot b/tests/testflows/extended_precision_data_types/snapshots/common.py.tests.snapshot\nindex d0b7b3423d89..c8b57ffdd1c4 100644\n--- a/tests/testflows/extended_precision_data_types/snapshots/common.py.tests.snapshot\n+++ b/tests/testflows/extended_precision_data_types/snapshots/common.py.tests.snapshot\n@@ -653,7 +653,7 @@ a\n \"\"\"\n \n Inline___Int128___arrayReduceInRanges__sum_____1__5____ = r\"\"\"\n-arrayReduceInRanges(\\'sum\\', array(tuple(1, 5)), array(toInt128(\\'3\\'), toInt128(\\'2\\'), toInt128(\\'1\\')))\n+arrayReduceInRanges(\\'sum\\', array((1, 5)), array(toInt128(\\'3\\'), toInt128(\\'2\\'), toInt128(\\'1\\')))\n [6]\n \"\"\"\n \n@@ -1181,7 +1181,7 @@ a\n \"\"\"\n \n Inline___Int256___arrayReduceInRanges__sum_____1__5____ = r\"\"\"\n-arrayReduceInRanges(\\'sum\\', array(tuple(1, 5)), array(toInt256(\\'3\\'), toInt256(\\'2\\'), toInt256(\\'1\\')))\n+arrayReduceInRanges(\\'sum\\', array((1, 5)), array(toInt256(\\'3\\'), toInt256(\\'2\\'), toInt256(\\'1\\')))\n [6]\n \"\"\"\n \n@@ -1709,7 +1709,7 @@ a\n \"\"\"\n \n Inline___UInt128___arrayReduceInRanges__sum_____1__5____ = r\"\"\"\n-arrayReduceInRanges(\\'sum\\', array(tuple(1, 5)), array(toUInt128(\\'3\\'), toUInt128(\\'2\\'), toUInt128(\\'1\\')))\n+arrayReduceInRanges(\\'sum\\', array((1, 5)), array(toUInt128(\\'3\\'), toUInt128(\\'2\\'), toUInt128(\\'1\\')))\n [6]\n \"\"\"\n \n@@ -2237,7 +2237,7 @@ a\n \"\"\"\n \n Inline___UInt256___arrayReduceInRanges__sum_____1__5____ = r\"\"\"\n-arrayReduceInRanges(\\'sum\\', array(tuple(1, 5)), array(toUInt256(\\'3\\'), toUInt256(\\'2\\'), toUInt256(\\'1\\')))\n+arrayReduceInRanges(\\'sum\\', array((1, 5)), array(toUInt256(\\'3\\'), toUInt256(\\'2\\'), toUInt256(\\'1\\')))\n [6]\n \"\"\"\n \n@@ -2765,7 +2765,7 @@ a\n \"\"\"\n \n Inline___Decimal256_0____arrayReduceInRanges__sum_____1__5____ = r\"\"\"\n-arrayReduceInRanges(\\'sum\\', array(tuple(1, 5)), array(toDecimal256(\\'3\\', 0), toDecimal256(\\'2\\', 0), toDecimal256(\\'1\\', 0)))\n+arrayReduceInRanges(\\'sum\\', array((1, 5)), array(toDecimal256(\\'3\\', 0), toDecimal256(\\'2\\', 0), toDecimal256(\\'1\\', 0)))\n [6]\n \"\"\"\n \n",
  "problem_statement": "High server load after upgrade to 20.x\nHello,\r\n\r\nI have tried to switch from Altinity 19.17.4.11 rpms to Clickhouse 20.1.6.30 rpms on RH7.\r\nNo changes in config. After a short time system load increases until the system is unresponsive.\r\n\r\nperf top is showing this 3 lines at the top \r\n```\r\n    50.26%  ParalInputsProc  clickhouse               [.] std::__1::__tree<std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, std::__1::__map_value_compare<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, true>, std::__1::allocator<std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long> > >::find<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >\r\n    18.73%  ParalInputsProc  clickhouse               [.] memcpy\r\n    16.62%  ParalInputsProc  clickhouse               [.] std::__1::__tree<std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, std::__1::__map_value_compare<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, true>, std::__1::allocator<std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long> > >::find<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >\r\n```\r\nIs there anything I haven't seen or read that makes it not possible ?\r\n\r\nRalph\n",
  "hints_text": ">After a short time system load increases until the system is unresponsive.\r\n\r\nNot related to rpm provider change. But related to 20.1.6.30 version. \r\nNeed a reproducible example.\nIt's a 4 node cluster with ring replication configured as described in https://www.altinity.com/blog/2018/5/10/circular-replication-cluster-topology-in-clickhouse with ReplicatedReplacingMergeTree engine.\r\n\r\ncarbon-clickhouse feeds in data into 3 distributed tables on every node.\r\n\r\nThe local tables look like\r\n\r\n```sql\r\nCREATE TABLE r0.index (\r\n    `Date` Date,\r\n    `Level` UInt32,\r\n    `Path` String,\r\n    `Version` UInt32\r\n)\r\nENGINE = ReplicatedReplacingMergeTree('/clickhouse/clusters/metrics/tables/{table17}/data', '{host1}')\r\nPARTITION BY toYYYYMM(Date)\r\nORDER BY (Level, Path, Date)\r\nSETTINGS index_granularity = 8192;\r\n\r\nCREATE TABLE r0.tagged (\r\n    `Date` Date,\r\n    `Tag1` String,\r\n    `Path` String,\r\n    `Tags` Array(String),\r\n    `Version` UInt32\r\n)\r\nENGINE = ReplicatedReplacingMergeTree('/clickhouse/clusters/metrics/tables/{table4}/data', '{host1}')\r\nPARTITION BY toYYYYMM(Date)\r\nORDER BY (Tag1, Path, Date)\r\nSETTINGS index_granularity = 8192;\r\n\r\nCREATE TABLE r0.metrics (\r\n    `Path` String CODEC(ZSTD(15)),\r\n    `Value` Float64 CODEC(Gorilla, ZSTD(15)),\r\n    `Time` UInt32 CODEC(DoubleDelta, ZSTD(15)),\r\n    `Date` Date CODEC(Delta(2), ZSTD(15)),\r\n    `Timestamp` UInt32 CODEC(ZSTD(15))\r\n)\r\nENGINE = ReplicatedReplacingMergeTree('/clickhouse/clusters/cluster/tables/{table1}/data', '{host1}')\r\nPARTITION BY toYYYYMM(Date)\r\nORDER BY (Path, Time)\r\nSETTINGS index_granularity = 8192;\r\n```\r\n\r\nWhat other information do you need ?\r\n\r\nRalph\n`select table, elapsed, progress from system.merges`\r\n\r\n`select * from system.merges`\r\n\r\n`select  * from system.replication_queue `\r\n\r\n`select  * from system.processes`\nAt this point load has started increasing already. Normally it's between 1.5 and 3. At this point it's 18 and increases until 600. Stopping clickhouse, downgrading to the previous version and everything is OK.\r\n\r\n[select_systemtables.txt](https://github.com/ClickHouse/ClickHouse/files/4325930/select_systemtables.txt)\r\n\n\r\nPlease provide:\r\n```\r\nSELECT\r\n    value,\r\n    changed\r\nFROM system.merge_tree_settings\r\nWHERE name = 'max_bytes_to_merge_at_max_space_in_pool'\r\n```\r\n\r\n\r\n\r\n```\r\ntotal_size_bytes_compressed = 292 375 344 413\r\n\r\nmuch bigger than default        214 748 364 800\r\n```\r\n\r\n\r\n\r\n\r\n```\r\n\u250c\u2500database\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500table\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500elapsed\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500progress\u2500\u252c\u2500num_parts\u2500\u252c\u2500total_size_bytes_compressed\u2500\u252c\u2500total_size_marks\u2500\u252c\u2500bytes_read_uncompressed\u2500\u252c\u2500rows_read\u2500\u252c\u2500bytes_written_uncompressed\u2500\u252c\u2500rows_written\u2500\u252c\u2500columns_written\u2500\u252c\u2500memory_usage\u2500\u252c\u2500thread_number\u2500\u2510\r\n\u2502 r0              \u2502 metrics      \u2502 259.277846756 \u2502 0.00364799522772465 \u2502        15 \u2502             292 375 344 413 \u2502         10172718 \u2502             20912177320 \u2502 304005121 \u2502                20907669070 \u2502    303931392 \u2502               0 \u2502     91751800 \u2502            13 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\n@ralphM78 We need the result of SHOW PROCESSLIST at the moment of slowdown.\r\n(What kind of queries are processed.)\nprocesslist and merge_tree_settings.\r\nLooks like the CPU usage goes up to 100 % at the first SELECT query,  but I don't know.\r\n\r\n[processlist_mergetree_settings.txt](https://github.com/ClickHouse/ClickHouse/files/4326299/processlist_mergetree_settings.txt)\r\n\nToday v20.3.2.1-stable has been released. Try to upgrade.\nv20.3.2.1 makes more trouble. Load increases and errors like\r\n\r\nHTTP status code: 500 Internal Server Error, body: Code: 480, e.displayText() = DB::Exception: Unsupported fetch protocol version, Stack trace:\r\n\r\nIt also creates some folders inside the distributed table folders , data/default/index for example, \r\n\r\nshard1_replica1,shard1_replica2\r\nshard2_replica1,shard2_replica2\r\nshard3_replica1,shard3_replica2\r\nshard4_replica1,shard4_replica2\r\n\r\nwhich prevents starting clickhouse after downgrade. Deleting these folders solved the restart issue.\r\n\r\n[20_3_2_1.txt](https://github.com/ClickHouse/ClickHouse/files/4326551/20_3_2_1.txt)\r\n\r\n\r\n\n```\r\n2020.03.12 21:35:12.759171 [ 34197 ] {} <Error> r0.index: DB::StorageReplicatedMergeTree::queueTask()::<lambda(DB::StorageReplicatedMergeTree::LogEntryPtr&)>: Code: 86, e.displayText() = DB::Exception: Received error from remote server ?endpoint=DataPartsExchange%3A%2Fclickhouse%2Fclusters%2Fosb4_metrics%2Ftables%2Fr0_index_host3%3E%2Fdata%2Freplicas%2Fsi0vm03149&part=197002_1447488_1447536_8&client_protocol_version=2&compress=false. HTTP status code: 500 Internal Server Error, body: Code: 480, e.displayText() = DB::Exception: Unsupported fetch protocol version, Stack trace:\r\n\r\n0. 0x3512b60 StackTrace::StackTrace() /usr/bin/clickhouse\r\n1. 0x351cdaf DB::Exception::Exception(std::string const&, int) /usr/bin/clickhouse\r\n2. 0x64591b3 DB::DataPartsExchange::Service::processQuery(Poco::Net::HTMLForm const&, DB::ReadBuffer&, DB::WriteBuffer&, Poco::Net::HTTPServerResponse&) /usr/bin/clickhouse\r\n3. 0x35a5933 DB::InterserverIOHTTPHandler::processQuery(Poco::Net::HTTPServerRequest&, Poco::Net::HTTPServerResponse&, DB::InterserverIOHTTPHandler::Output&) /usr/bin/clickhouse\r\n4. 0x35a6339 DB::InterserverIOHTTPHandler::handleRequest(Poco::Net::HTTPServerRequest&, Poco::Net::HTTPServerResponse&) /usr/bin/clickhouse\r\n5. 0x6dbcc59 Poco::Net::HTTPServerConnection::run() /usr/bin/clickhouse\r\n6. 0x6db98bf Poco::Net::TCPServerConnection::start() /usr/bin/clickhouse\r\n7. 0x6db9fb5 Poco::Net::TCPServerDispatcher::run() /usr/bin/clickhouse\r\n8. 0x723f481 Poco::PooledThread::run() /usr/bin/clickhouse\r\n9. 0x723b208 Poco::ThreadImpl::runnableEntry(void*) /usr/bin/clickhouse\r\n10. 0x791d69f ? /usr/bin/clickhouse\r\n11. 0x7fa232938ea5 start_thread /usr/lib64/libpthread-2.17.so\r\n12. 0x7fa23245d8cd __clone /usr/lib64/libc-2.17.so\r\n (version 19.17.4.11), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x10278d1c in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x8f02989 in /usr/bin/clickhouse\r\n2. ? @ 0x8f901f2 in /usr/bin/clickhouse\r\n3. DB::receiveResponse(Poco::Net::HTTPClientSession&, Poco::Net::HTTPRequest const&, Poco::Net::HTTPResponse&, bool) @ 0x8f8deef in /usr/bin/clickhouse\r\n4. DB::detail::ReadWriteBufferFromHTTPBase<std::__1::shared_ptr<DB::UpdatablePooledSession> >::call(Poco::URI, Poco::Net::HTTPResponse&) @ 0xd6a5eb0 in /usr/bin/clickhouse\r\n5. DB::detail::ReadWriteBufferFromHTTPBase<std::__1::shared_ptr<DB::UpdatablePooledSession> >::ReadWriteBufferFromHTTPBase(std::__1::shared_ptr<DB::UpdatablePooledSession>, Poco::URI, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::function<void (std::__1::basic_ostream<char, std::__1::char_traits<char> >&)>, Poco::Net::HTTPBasicCredentials const&, unsigned long, std::__1::vector<std::__1::tuple<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::tuple<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > >, DB::RemoteHostFilter const&) @ 0xd6a6462 in /usr/bin/clickhouse\r\n6. DB::DataPartsExchange::Fetcher::fetchPart(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, DB::ConnectionTimeouts const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0xd6a1a33 in /usr/bin/clickhouse\r\n7. ? @ 0xd546ef3 in /usr/bin/clickhouse\r\n8. DB::StorageReplicatedMergeTree::fetchPart(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, unsigned long) @ 0xd58c302 in /usr/bin/clickhouse\r\n9. DB::StorageReplicatedMergeTree::executeFetch(DB::ReplicatedMergeTreeLogEntry&) @ 0xd58fa2a in /usr/bin/clickhouse\r\n10. DB::StorageReplicatedMergeTree::executeLogEntry(DB::ReplicatedMergeTreeLogEntry&) @ 0xd5912cd in /usr/bin/clickhouse\r\n11. ? @ 0xd59171f in /usr/bin/clickhouse\r\n12. DB::ReplicatedMergeTreeQueue::processEntry(std::__1::function<std::__1::shared_ptr<zkutil::ZooKeeper> ()>, std::__1::shared_ptr<DB::ReplicatedMergeTreeLogEntry>&, std::__1::function<bool (std::__1::shared_ptr<DB::ReplicatedMergeTreeLogEntry>&)>) @ 0xd8c168e in /usr/bin/clickhouse\r\n13. DB::StorageReplicatedMergeTree::queueTask() @ 0xd548a5a in /usr/bin/clickhouse\r\n14. DB::BackgroundProcessingPool::threadFunction() @ 0xd69c1ab in /usr/bin/clickhouse\r\n15. ? @ 0xd69cb70 in /usr/bin/clickhouse\r\n16. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x8f25b07 in /usr/bin/clickhouse\r\n17. ? @ 0x8f2400f in /usr/bin/clickhouse\r\n18. start_thread @ 0x7ea5 in /usr/lib64/libpthread-2.17.so\r\n19. clone @ 0xfe8cd in /usr/lib64/libc-2.17.so\r\n (version 20.3.2.1 (official build))\r\n```\r\n@alesapin is it related to #9645 or smth else? \r\n\r\n> It also creates some folders inside the distributed table folders , data/default/index for example,\r\n\r\nFixed here: #9647\nRegarding original issue.\r\n\r\nThe query which work slower now is:\r\n```\r\nSELECT \r\n\tPath,\r\n\tTime,\r\n\tValue,\r\n\tTimestamp\r\nFROM\r\n\tosb4_reverse\r\nPREWHERE (Date >= '2020-03-12') AND (Date <= '2020-03-12')\r\nWHERE (Path IN ( ... 100Kb of strings here ...) )\r\n```\r\nIs that correct, or the real query has something else at the end? (it is cut in processlist).\r\n\r\nProbably we need to create perf test for that.\r\nHow many threads are used when query hungs?\r\nCan you try to put all the needed paths into table with Engine=Set?\n@filimonov This is typical query from Graphouse. We have to create a perf test for it.\n@filimonov if clickhouse-client doesn't cut the output by itself, then it's the full query from processlist. It's not Graphouse but similar. https://github.com/lomik/graphite-clickhouse . \n@CurtizJ is responsible for query performance related to the usage of index for large IN sets.\nSeems it's solved in master and 20.3.8.53\r\n\r\n```\r\nCREATE TABLE metrics (`Path` String, `Value` Float64, `Time` UInt32, `Date` Date, `Timestamp` UInt32) \r\nENGINE = MergeTree PARTITION BY toYYYYMM(Date) ORDER BY (Path, Time);\r\n\r\ninsert into metrics select number, 0, toDateTime('2000-01-01 00:00:00') + intDiv(number,100), '2000-01-01', 0 \r\nfrom numbers(1000000000)\r\n```\r\n\r\n20.1.6.30\r\n```\r\necho 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(x), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n10000\r\n2.584\r\n\r\necho 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(cityHash64(x))||'--'||toString(farmHash64(x))||'-'||toString(xxHash64(x)), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n7.101\r\n```\r\n\r\n20.1.9.54\r\n```\r\necho 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(x), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n10000\r\n2.473\r\n\r\n echo 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(cityHash64(x))||'--'||toString(farmHash64(x))||'-'||toString(xxHash64(x)), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n6.560\r\n```\r\n\r\n20.1.11.73\r\n```\r\necho 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(x), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n10000\r\n2.478\r\n\r\necho 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(cityHash64(x))||'--'||toString(farmHash64(x))||'-'||toString(xxHash64(x)), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n6.492\r\n```\r\n\r\n19.17.9.60\r\n\r\n```\r\necho 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(x), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n10000\r\n1.884\r\n\r\necho 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(cityHash64(x))||'--'||toString(farmHash64(x))||'-'||toString(xxHash64(x)), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n1.989\r\n```\r\n\r\n20.4.1.3163\r\n\r\n```\r\necho 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(x), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n10000\r\n1.247\r\n\r\necho 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(cityHash64(x))||'--'||toString(farmHash64(x))||'-'||toString(xxHash64(x)), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n1.177\r\n```\r\n\r\n20.3.8.53\r\n\r\n```\r\necho 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(x), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n10000\r\n1.630\r\n\r\necho 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(cityHash64(x))||'--'||toString(farmHash64(x))||'-'||toString(xxHash64(x)), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n1.597\r\n```\nHello. I have tried to upgrade all nodes to clickhouse-server-20.4.4.18-2. Without any select queries everything looks good. As soon as the first select requests start, load increases and the processlist is growing. After blocking all select queries everything went to normal. \"perf top\" shows\r\n\r\n```\r\n50.26%  ParalInputsProc  clickhouse               [.] std::__1::__tree<std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, std::__1::__map_value_compare<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long>, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, true>, std::__1::allocator<std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned long> > >::find<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >\r\n18.73%  ParalInputsProc  clickhouse               [.] memcpy\r\n```\r\nMost of the time memcpy is on top of the list.\r\nAfter downgrading to clickhouse-server-19.17.4.11-1 on all nodes, the system load is 1.5 on average.\r\nAny new ideas ?\r\n\r\nRalph \n@den-crane What exactly do you need ?\n@ralphM78 it seems that the issue is back. Try this  exact version 20.4.1.3163 / or 20.4.2.9.\r\n\r\n```\r\n\r\n20.4.1.3163\r\necho 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(x), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n10000\r\n1.651\r\n\r\necho 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(cityHash64(x))||'--'||toString(farmHash64(x))||'-'||toString(xxHash64(x)), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n1.424\r\n\r\n\r\n\r\n20.4.2.9\r\necho 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(x), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n10000\r\n1.640\r\n\r\necho 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(cityHash64(x))||'--'||toString(farmHash64(x))||'-'||toString(xxHash64(x)), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n1.443\r\n\r\n\r\n\r\n20.4.3.16\r\necho 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(x), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n10000\r\n1.686\r\n\r\necho 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(cityHash64(x))||'--'||toString(farmHash64(x))||'-'||toString(xxHash64(x)), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n4.034\r\n\r\n\r\n\r\n20.4.4.18\r\necho 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(x), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n10000\r\n1.667\r\n\r\necho 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(cityHash64(x))||'--'||toString(farmHash64(x))||'-'||toString(xxHash64(x)), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n4.096\r\n\r\n\r\n\r\n20.5.1.3629\r\necho 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(x), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n10000\r\n2.454\r\n\r\necho 'select count() from metrics PREWHERE (Date >= '\"'\"2000-01-01\"'\"') AND (Date <= '\"'\"2000-01-01\"'\"') where (Path in('`clickhouse-client -q \"select arrayMap(x -> toString(cityHash64(x))||'--'||toString(farmHash64(x))||'-'||toString(xxHash64(x)), range(10000))\"|sed 's/\\[\\|\\]//g'`'))'|clickhouse-client -t --max_query_size=100000000000 --max_threads=1\r\n4.886\r\n\r\n```\n@den-crane I have tried 20.4.2.9 and now everything looks good. :thumbsup:\r\nHopefully you are able to find the problem in 2.4.4.18.\r\n\r\nRalph\n@ralphM78 thank you. Now we have some confidence that I reproduced your issue but not something else. :)\nWe have implemented a patch for backward compatibility that leads to some performance decrease: https://github.com/ClickHouse/ClickHouse/pull/10889\r\nIt is needed for rolling upgrade when different server versions work on the same cluster.\r\n\r\nWe will remove this patch after the oldest supported version that requires it (19.14) will end of life (in September).\n@CurtizJ as 19.14 no longer supported, we can remove compatibility code.\nIt seems Curtiz will never remove it. I think it pretty important task.\nI think that @CurtizJ will do it.\nHi All,\r\n\r\nMay I know is there any update on this issue?\n@CurtizJ will remove the compatibility code.",
  "created_at": "2021-06-16T18:10:30Z"
}