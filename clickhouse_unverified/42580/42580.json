{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 42580,
  "instance_id": "ClickHouse__ClickHouse-42580",
  "issue_numbers": [
    "7071"
  ],
  "base_commit": "b746224217ad4fac373c6bbd5c536f8c3e97ad5c",
  "patch": "diff --git a/src/Interpreters/InterpreterSelectQuery.cpp b/src/Interpreters/InterpreterSelectQuery.cpp\nindex 79deb38317c1..adf341f5ffdf 100644\n--- a/src/Interpreters/InterpreterSelectQuery.cpp\n+++ b/src/Interpreters/InterpreterSelectQuery.cpp\n@@ -2143,6 +2143,8 @@ void InterpreterSelectQuery::executeFetchColumns(QueryProcessingStage::Enum proc\n \n     auto [limit_length, limit_offset] = getLimitLengthAndOffset(query, context);\n \n+    auto local_limits = getStorageLimits(*context, options);\n+\n     /** Optimization - if not specified DISTINCT, WHERE, GROUP, HAVING, ORDER, JOIN, LIMIT BY, WITH TIES\n      *  but LIMIT is specified, and limit + offset < max_block_size,\n      *  then as the block size we will use limit + offset (not to read more from the table than requested),\n@@ -2161,17 +2163,22 @@ void InterpreterSelectQuery::executeFetchColumns(QueryProcessingStage::Enum proc\n         && !query_analyzer->hasAggregation()\n         && !query_analyzer->hasWindow()\n         && query.limitLength()\n-        && limit_length <= std::numeric_limits<UInt64>::max() - limit_offset\n-        && limit_length + limit_offset < max_block_size)\n+        && limit_length <= std::numeric_limits<UInt64>::max() - limit_offset)\n     {\n-        max_block_size = std::max<UInt64>(1, limit_length + limit_offset);\n-        max_threads_execute_query = max_streams = 1;\n+        if (limit_length + limit_offset < max_block_size)\n+        {\n+            max_block_size = std::max<UInt64>(1, limit_length + limit_offset);\n+            max_threads_execute_query = max_streams = 1;\n+        }\n+        if (limit_length + limit_offset < local_limits.local_limits.size_limits.max_rows)\n+        {\n+            query_info.limit = limit_length + limit_offset;\n+        }\n     }\n \n     if (!max_block_size)\n         throw Exception(\"Setting 'max_block_size' cannot be zero\", ErrorCodes::PARAMETER_OUT_OF_BOUND);\n \n-    auto local_limits = getStorageLimits(*context, options);\n     storage_limits.emplace_back(local_limits);\n \n     /// Initialize the initial data streams to which the query transforms are superimposed. Table or subquery or prepared input?\ndiff --git a/src/Processors/QueryPlan/ReadFromMergeTree.cpp b/src/Processors/QueryPlan/ReadFromMergeTree.cpp\nindex b340073e73db..ba7b0e963eb8 100644\n--- a/src/Processors/QueryPlan/ReadFromMergeTree.cpp\n+++ b/src/Processors/QueryPlan/ReadFromMergeTree.cpp\n@@ -173,6 +173,9 @@ Pipe ReadFromMergeTree::readFromPool(\n         total_rows += part.getRowsCount();\n     }\n \n+    if (query_info.limit > 0 && query_info.limit < total_rows)\n+        total_rows = query_info.limit;\n+\n     const auto & settings = context->getSettingsRef();\n     const auto & client_info = context->getClientInfo();\n     MergeTreeReadPool::BackoffSettings backoff_settings(settings);\n@@ -246,10 +249,26 @@ ProcessorPtr ReadFromMergeTree::createSource(\n         };\n     }\n \n-    return std::make_shared<TSource>(\n+    auto total_rows = part.getRowsCount();\n+    if (query_info.limit > 0 && query_info.limit < total_rows)\n+        total_rows = query_info.limit;\n+\n+    /// Actually it means that parallel reading from replicas enabled\n+    /// and we have to collaborate with initiator.\n+    /// In this case we won't set approximate rows, because it will be accounted multiple times.\n+    /// Also do not count amount of read rows if we read in order of sorting key,\n+    /// because we don't know actual amount of read rows in case when limit is set.\n+    bool set_rows_approx = !extension.has_value() && !reader_settings.read_in_order;\n+\n+    auto source = std::make_shared<TSource>(\n             data, storage_snapshot, part.data_part, max_block_size, preferred_block_size_bytes,\n             preferred_max_column_in_block_size_bytes, required_columns, part.ranges, use_uncompressed_cache, prewhere_info,\n             actions_settings, reader_settings, virt_column_names, part.part_index_in_query, has_limit_below_one_block, std::move(extension));\n+\n+    if (set_rows_approx)\n+        source -> addTotalRowsApprox(total_rows);\n+\n+    return source;\n }\n \n Pipe ReadFromMergeTree::readInOrder(\ndiff --git a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\nindex 91ecb3a37a06..0eddaac2facc 100644\n--- a/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\n+++ b/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp\n@@ -1061,6 +1061,10 @@ RangesInDataParts MergeTreeDataSelectExecutor::filterPartsByPrimaryKeyAndSkipInd\n                     auto current_rows_estimate = ranges.getRowsCount();\n                     size_t prev_total_rows_estimate = total_rows.fetch_add(current_rows_estimate);\n                     size_t total_rows_estimate = current_rows_estimate + prev_total_rows_estimate;\n+                    if (query_info.limit > 0 && total_rows_estimate > query_info.limit)\n+                    {\n+                        total_rows_estimate = query_info.limit;\n+                    }\n                     limits.check(total_rows_estimate, 0, \"rows (controlled by 'max_rows_to_read' setting)\", ErrorCodes::TOO_MANY_ROWS);\n                     leaf_limits.check(\n                         total_rows_estimate, 0, \"rows (controlled by 'max_rows_to_read_leaf' setting)\", ErrorCodes::TOO_MANY_ROWS);\ndiff --git a/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp b/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp\nindex 59cbae3f914d..2490eb777721 100644\n--- a/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp\n+++ b/src/Storages/MergeTree/MergeTreeSelectProcessor.cpp\n@@ -38,14 +38,6 @@ MergeTreeSelectProcessor::MergeTreeSelectProcessor(\n     has_limit_below_one_block(has_limit_below_one_block_),\n     total_rows(data_part->index_granularity.getRowsCountInRanges(all_mark_ranges))\n {\n-    /// Actually it means that parallel reading from replicas enabled\n-    /// and we have to collaborate with initiator.\n-    /// In this case we won't set approximate rows, because it will be accounted multiple times.\n-    /// Also do not count amount of read rows if we read in order of sorting key,\n-    /// because we don't know actual amount of read rows in case when limit is set.\n-    if (!extension_.has_value() && !reader_settings.read_in_order)\n-        addTotalRowsApprox(total_rows);\n-\n     ordered_names = header_without_virtual_columns.getNames();\n }\n \ndiff --git a/src/Storages/SelectQueryInfo.h b/src/Storages/SelectQueryInfo.h\nindex f2835ab4dbf3..565594569ce0 100644\n--- a/src/Storages/SelectQueryInfo.h\n+++ b/src/Storages/SelectQueryInfo.h\n@@ -220,6 +220,9 @@ struct SelectQueryInfo\n     Block minmax_count_projection_block;\n     MergeTreeDataSelectAnalysisResultPtr merge_tree_select_result_ptr;\n \n+    // If limit is not 0, that means it's a trivial limit query.\n+    UInt64 limit = 0;\n+\n     InputOrderInfoPtr getInputOrderInfo() const\n     {\n         return input_order_info ? input_order_info : (projection ? projection->input_order_info : nullptr);\ndiff --git a/src/Storages/System/StorageSystemNumbers.cpp b/src/Storages/System/StorageSystemNumbers.cpp\nindex 523ec25b89c4..fbcd449051ff 100644\n--- a/src/Storages/System/StorageSystemNumbers.cpp\n+++ b/src/Storages/System/StorageSystemNumbers.cpp\n@@ -2,6 +2,7 @@\n #include <Columns/ColumnsNumber.h>\n #include <DataTypes/DataTypesNumber.h>\n #include <Storages/System/StorageSystemNumbers.h>\n+#include <Storages/SelectQueryInfo.h>\n \n #include <Processors/ISource.h>\n #include <QueryPipeline/Pipe.h>\n@@ -125,7 +126,7 @@ StorageSystemNumbers::StorageSystemNumbers(const StorageID & table_id, bool mult\n Pipe StorageSystemNumbers::read(\n     const Names & column_names,\n     const StorageSnapshotPtr & storage_snapshot,\n-    SelectQueryInfo &,\n+    SelectQueryInfo & query_info,\n     ContextPtr /*context*/,\n     QueryProcessingStage::Enum /*processed_stage*/,\n     size_t max_block_size,\n@@ -154,7 +155,12 @@ Pipe StorageSystemNumbers::read(\n             auto source = std::make_shared<NumbersMultiThreadedSource>(state, max_block_size, max_counter);\n \n             if (i == 0)\n-                source->addTotalRowsApprox(*limit);\n+            {\n+                auto rows_appr = *limit;\n+                if (query_info.limit > 0 && query_info.limit < rows_appr)\n+                    rows_appr = query_info.limit;\n+                source->addTotalRowsApprox(rows_appr);\n+            }\n \n             pipe.addSource(std::move(source));\n         }\n@@ -167,7 +173,12 @@ Pipe StorageSystemNumbers::read(\n         auto source = std::make_shared<NumbersSource>(max_block_size, offset + i * max_block_size, num_streams * max_block_size);\n \n         if (limit && i == 0)\n-            source->addTotalRowsApprox(*limit);\n+        {\n+            auto rows_appr = *limit;\n+            if (query_info.limit > 0 && query_info.limit < rows_appr)\n+                rows_appr = query_info.limit;\n+            source->addTotalRowsApprox(rows_appr);\n+        }\n \n         pipe.addSource(std::move(source));\n     }\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/02465_limit_trivial_max_rows_to_read.reference b/tests/queries/0_stateless/02465_limit_trivial_max_rows_to_read.reference\nnew file mode 100644\nindex 000000000000..87370760038e\n--- /dev/null\n+++ b/tests/queries/0_stateless/02465_limit_trivial_max_rows_to_read.reference\n@@ -0,0 +1,7 @@\n+0\n+0\n+1\n+2\n+3\n+4\n+0\ndiff --git a/tests/queries/0_stateless/02465_limit_trivial_max_rows_to_read.sql b/tests/queries/0_stateless/02465_limit_trivial_max_rows_to_read.sql\nnew file mode 100644\nindex 000000000000..ee7a4e6b6b50\n--- /dev/null\n+++ b/tests/queries/0_stateless/02465_limit_trivial_max_rows_to_read.sql\n@@ -0,0 +1,22 @@\n+DROP TABLE IF EXISTS t_max_rows_to_read;\n+\n+CREATE TABLE t_max_rows_to_read (a UInt64)\n+ENGINE = MergeTree ORDER BY a\n+SETTINGS index_granularity = 4;\n+\n+INSERT INTO t_max_rows_to_read SELECT number FROM numbers(100);\n+\n+SET max_block_size = 10;\n+SET max_rows_to_read = 20;\n+SET read_overflow_mode = 'throw';\n+\n+SELECT number FROM numbers(30); -- { serverError 158 }\n+SELECT number FROM numbers(30) LIMIT 21; -- { serverError 158 }\n+SELECT number FROM numbers(30) LIMIT 1;\n+SELECT number FROM numbers(5);\n+\n+SELECT a FROM t_max_rows_to_read LIMIT 1;\n+SELECT a FROM t_max_rows_to_read LIMIT 11 offset 11; -- { serverError 158 }\n+SELECT a FROM t_max_rows_to_read WHERE a > 50 LIMIT 1; -- { serverError 158 }\n+\n+DROP TABLE t_max_rows_to_read;\n",
  "problem_statement": "Do not trigger the limit on number of rows to read if it's a trivial query with LIMIT.\n```\r\nmilovidov-Pro-P30 :) SET max_rows_to_read = 1000000\r\n\r\nSET max_rows_to_read = 1000000\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.007 sec. \r\n\r\nmilovidov-Pro-P30 :) SELECT number FROM numbers(1000000000) LIMIT 1\r\n\r\nSELECT number\r\nFROM numbers(1000000000)\r\nLIMIT 1\r\n\r\n\u2192 Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.) Received exception from server (version 19.15.1):\r\nCode: 158. DB::Exception: Received from localhost:9000. DB::Exception: Limit for rows to read exceeded: 1000000000 rows read (or to read), maximum: 1000000.\r\n``` \r\n\n",
  "hints_text": "It's not trivial. The table may require implicit filtering and will read more rows.\r\nThe only way is to analyze limit inside IStorage::read (for MergeTree and Numbers storages) and adapt the predicted number of rows to read.\nSome insights:\r\n\r\n###Why this kind of query fails in advance?\r\n\r\nbefore we actually run a query, we will compare the row limit and approx row number. It's not always accurate, especially for a simple `limit`-only query. We will overestimate the row number to read.\r\n\r\n###How can we get accurate estimated row numbers?\r\n\r\nAlways being accurate is not possible, especially when **limit** meets **where**. But in this case, we can adjust the estimated when we find there is a simple `limit`-only query.\r\n\r\nWe need to add a `limit` field to `SelectQueryInfo` and set it iff the interperter finds it is a `limit-only` query. In the IStorage::read function we check the `limit` to ajust the approx total rows.\r\n\r\n###What is the usage of estimated row numbers and what is the risk?\r\n\r\nAFAIK, the estimation is only for restricting rows to read and showing the process, so adjusting it is not risky.\r\n\r\nThe only effect is making query_info more fat and making check logic more complex.\r\n\r\n@KochetovNicolai PTAL\nMaybe we can do it in a very simple way?\r\nIf a query with trivial limit cannot contain where, prewhere, array join and other stuff, we can:\r\n1. Check if `limit` < `max_rows_to_read`\r\n2.  Just change [OwerflowMode](https://github.com/ClickHouse/ClickHouse/blob/984fe4f05889c60266acd1290d2644ebff7fee1e/src/QueryPipeline/SizeLimits.h#L13) from THROW to BREAK and newer throw `Limit for rows to read exceeded`.\r\n\r\nIf query has where or prewhere, it would be really very hard to limit well. Especially if we read from storage in-parallel (basically, it would not make sense).\nAnother one funny implementation idea is to cut chunk up to `max_rows_to_read - read_rows` size instead of throwing `Limit for rows to read exceeded` exception. Then, put into pipeline this shorter chunk, and then exception. Exception will be re-thrown if more then `max_rows_to_read` are actually needed.\r\n\r\nThis idea is bad because:\r\n* Still, does not make a lot of sense with parallel execution.\r\n* Does not support prewhere\r\n* Difficult to implement\n> If a query with trivial limit cannot contain where, prewhere, array join and other stuff, we can\r\n\r\nMaybe this variant will also require checking for row policies?\r\n\r\n> We need to add a limit field to SelectQueryInfo and set it iff the interperter finds it is a limit-only query. In the IStorage::read function we check the limit to ajust the approx total rows.\r\n\r\nThis is a good variant because the progress bar will be also adjusted.\n@KochetovNicolai I'm afraid this method cannot work out. There are two reasons:\r\n\r\n1. When querying with MergeTree, we do size limit check at [here](https://github.com/ClickHouse/ClickHouse/blob/master/src/Storages/MergeTree/MergeTreeDataSelectExecutor.cpp#L1064), which set `overflow mode` by settings\r\n2. Actually we append two \"LocalLimits\" in Interpreter. The first one is at [here](https://github.com/ClickHouse/ClickHouse/blob/master/src/Interpreters/InterpreterSelectWithUnionQuery.cpp#L287), which cannot be modified according to limit value.\r\n\r\nThe second point doesn't make sense to me. Why do we set limit check at this place? Is it redundant?\r\n\r\nAccording to these two points, I think we can use my solution adding a field in query_info.",
  "created_at": "2022-10-22T16:34:07Z"
}