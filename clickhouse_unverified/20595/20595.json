{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 20595,
  "instance_id": "ClickHouse__ClickHouse-20595",
  "issue_numbers": [
    "10863",
    "20252",
    "19714",
    "21517",
    "20194"
  ],
  "base_commit": "ec1d0c8e863f4fccfc59b124cc4f67f3abf3e708",
  "patch": "diff --git a/contrib/CMakeLists.txt b/contrib/CMakeLists.txt\nindex 20b4fad0437a..bf4bf5eb472b 100644\n--- a/contrib/CMakeLists.txt\n+++ b/contrib/CMakeLists.txt\n@@ -32,6 +32,7 @@ endif()\n \n set_property(DIRECTORY PROPERTY EXCLUDE_FROM_ALL 1)\n \n+add_subdirectory (abseil-cpp-cmake)\n add_subdirectory (antlr4-runtime-cmake)\n add_subdirectory (boost-cmake)\n add_subdirectory (cctz-cmake)\ndiff --git a/contrib/abseil-cpp-cmake/CMakeLists.txt b/contrib/abseil-cpp-cmake/CMakeLists.txt\nnew file mode 100644\nindex 000000000000..c8cb512066a5\n--- /dev/null\n+++ b/contrib/abseil-cpp-cmake/CMakeLists.txt\n@@ -0,0 +1,18 @@\n+set(ABSL_ROOT_DIR \"${ClickHouse_SOURCE_DIR}/contrib/abseil-cpp\")\n+if(NOT EXISTS \"${ABSL_ROOT_DIR}/CMakeLists.txt\")\n+  message(FATAL_ERROR \" submodule third_party/abseil-cpp is missing. To fix try run: \\n git submodule update --init --recursive\")\n+endif()\n+add_subdirectory(\"${ABSL_ROOT_DIR}\" \"${ClickHouse_BINARY_DIR}/contrib/abseil-cpp\")\n+\n+add_library(abseil_swiss_tables INTERFACE)\n+\n+target_link_libraries(abseil_swiss_tables INTERFACE\n+  absl::flat_hash_map\n+  absl::flat_hash_set\n+)\n+\n+get_target_property(FLAT_HASH_MAP_INCLUDE_DIR absl::flat_hash_map INTERFACE_INCLUDE_DIRECTORIES)\n+target_include_directories (abseil_swiss_tables SYSTEM BEFORE INTERFACE ${FLAT_HASH_MAP_INCLUDE_DIR})\n+\n+get_target_property(FLAT_HASH_SET_INCLUDE_DIR absl::flat_hash_set INTERFACE_INCLUDE_DIRECTORIES)\n+target_include_directories (abseil_swiss_tables SYSTEM BEFORE INTERFACE ${FLAT_HASH_SET_INCLUDE_DIR})\ndiff --git a/contrib/grpc-cmake/CMakeLists.txt b/contrib/grpc-cmake/CMakeLists.txt\nindex 97ca3fab4db7..b93968f62f96 100644\n--- a/contrib/grpc-cmake/CMakeLists.txt\n+++ b/contrib/grpc-cmake/CMakeLists.txt\n@@ -39,11 +39,6 @@ set(_gRPC_SSL_LIBRARIES ${OPENSSL_LIBRARIES})\n \n # Use abseil-cpp from ClickHouse contrib, not from gRPC third_party.\n set(gRPC_ABSL_PROVIDER \"clickhouse\" CACHE STRING \"\" FORCE)\n-set(ABSL_ROOT_DIR \"${ClickHouse_SOURCE_DIR}/contrib/abseil-cpp\")\n-if(NOT EXISTS \"${ABSL_ROOT_DIR}/CMakeLists.txt\")\n-  message(FATAL_ERROR \" grpc: submodule third_party/abseil-cpp is missing. To fix try run: \\n git submodule update --init --recursive\")\n-endif()\n-add_subdirectory(\"${ABSL_ROOT_DIR}\" \"${ClickHouse_BINARY_DIR}/contrib/abseil-cpp\")\n \n # Choose to build static or shared library for c-ares.\n if (MAKE_STATIC_LIBRARIES)\ndiff --git a/src/Columns/ColumnAggregateFunction.cpp b/src/Columns/ColumnAggregateFunction.cpp\nindex d0a5e120a073..df7b3cce7290 100644\n--- a/src/Columns/ColumnAggregateFunction.cpp\n+++ b/src/Columns/ColumnAggregateFunction.cpp\n@@ -24,6 +24,7 @@ namespace ErrorCodes\n     extern const int PARAMETER_OUT_OF_BOUND;\n     extern const int SIZES_OF_COLUMNS_DOESNT_MATCH;\n     extern const int ILLEGAL_TYPE_OF_ARGUMENT;\n+    extern const int NOT_IMPLEMENTED;\n }\n \n \n@@ -553,6 +554,11 @@ const char * ColumnAggregateFunction::deserializeAndInsertFromArena(const char *\n     return read_buffer.position();\n }\n \n+const char * ColumnAggregateFunction::skipSerializedInArena(const char *) const\n+{\n+    throw Exception(\"Method skipSerializedInArena is not supported for \" + getName(), ErrorCodes::NOT_IMPLEMENTED);\n+}\n+\n void ColumnAggregateFunction::popBack(size_t n)\n {\n     size_t size = data.size();\ndiff --git a/src/Columns/ColumnAggregateFunction.h b/src/Columns/ColumnAggregateFunction.h\nindex f023177d7f23..5cb9aaa4ad52 100644\n--- a/src/Columns/ColumnAggregateFunction.h\n+++ b/src/Columns/ColumnAggregateFunction.h\n@@ -155,6 +155,8 @@ class ColumnAggregateFunction final : public COWHelper<IColumn, ColumnAggregateF\n \n     const char * deserializeAndInsertFromArena(const char * src_arena) override;\n \n+    const char * skipSerializedInArena(const char *) const override;\n+\n     void updateHashWithValue(size_t n, SipHash & hash) const override;\n \n     void updateWeakHash32(WeakHash32 & hash) const override;\ndiff --git a/src/Columns/ColumnArray.cpp b/src/Columns/ColumnArray.cpp\nindex d8821a646ae2..5267bc5db5d0 100644\n--- a/src/Columns/ColumnArray.cpp\n+++ b/src/Columns/ColumnArray.cpp\n@@ -239,6 +239,16 @@ const char * ColumnArray::deserializeAndInsertFromArena(const char * pos)\n     return pos;\n }\n \n+const char * ColumnArray::skipSerializedInArena(const char * pos) const\n+{\n+    size_t array_size = unalignedLoad<size_t>(pos);\n+    pos += sizeof(array_size);\n+\n+    for (size_t i = 0; i < array_size; ++i)\n+        pos = getData().skipSerializedInArena(pos);\n+\n+    return pos;\n+}\n \n void ColumnArray::updateHashWithValue(size_t n, SipHash & hash) const\n {\ndiff --git a/src/Columns/ColumnArray.h b/src/Columns/ColumnArray.h\nindex 7d01d04735b5..75bd4a6dba40 100644\n--- a/src/Columns/ColumnArray.h\n+++ b/src/Columns/ColumnArray.h\n@@ -61,6 +61,7 @@ class ColumnArray final : public COWHelper<IColumn, ColumnArray>\n     void insertData(const char * pos, size_t length) override;\n     StringRef serializeValueIntoArena(size_t n, Arena & arena, char const *& begin) const override;\n     const char * deserializeAndInsertFromArena(const char * pos) override;\n+    const char * skipSerializedInArena(const char * pos) const override;\n     void updateHashWithValue(size_t n, SipHash & hash) const override;\n     void updateWeakHash32(WeakHash32 & hash) const override;\n     void updateHashFast(SipHash & hash) const override;\ndiff --git a/src/Columns/ColumnCompressed.h b/src/Columns/ColumnCompressed.h\nindex a31147b0702b..3cc2c0147326 100644\n--- a/src/Columns/ColumnCompressed.h\n+++ b/src/Columns/ColumnCompressed.h\n@@ -85,6 +85,7 @@ class ColumnCompressed : public COWHelper<IColumn, ColumnCompressed>\n     void popBack(size_t) override { throwMustBeDecompressed(); }\n     StringRef serializeValueIntoArena(size_t, Arena &, char const *&) const override { throwMustBeDecompressed(); }\n     const char * deserializeAndInsertFromArena(const char *) override { throwMustBeDecompressed(); }\n+    const char * skipSerializedInArena(const char *) const override { throwMustBeDecompressed(); }\n     void updateHashWithValue(size_t, SipHash &) const override { throwMustBeDecompressed(); }\n     void updateWeakHash32(WeakHash32 &) const override { throwMustBeDecompressed(); }\n     void updateHashFast(SipHash &) const override { throwMustBeDecompressed(); }\ndiff --git a/src/Columns/ColumnConst.h b/src/Columns/ColumnConst.h\nindex 9441f339085e..a19e0615dd7c 100644\n--- a/src/Columns/ColumnConst.h\n+++ b/src/Columns/ColumnConst.h\n@@ -163,6 +163,11 @@ class ColumnConst final : public COWHelper<IColumn, ColumnConst>\n         return res;\n     }\n \n+    const char * skipSerializedInArena(const char * pos) const override\n+    {\n+        return data->skipSerializedInArena(pos);\n+    }\n+\n     void updateHashWithValue(size_t, SipHash & hash) const override\n     {\n         data->updateHashWithValue(0, hash);\ndiff --git a/src/Columns/ColumnDecimal.cpp b/src/Columns/ColumnDecimal.cpp\nindex 4a47919adf18..ec08db274b39 100644\n--- a/src/Columns/ColumnDecimal.cpp\n+++ b/src/Columns/ColumnDecimal.cpp\n@@ -79,6 +79,12 @@ const char * ColumnDecimal<T>::deserializeAndInsertFromArena(const char * pos)\n     return pos + sizeof(T);\n }\n \n+template <typename T>\n+const char * ColumnDecimal<T>::skipSerializedInArena(const char * pos) const\n+{\n+    return pos + sizeof(T);\n+}\n+\n template <typename T>\n UInt64 ColumnDecimal<T>::get64([[maybe_unused]] size_t n) const\n {\ndiff --git a/src/Columns/ColumnDecimal.h b/src/Columns/ColumnDecimal.h\nindex 33eb29461224..3187b5c02537 100644\n--- a/src/Columns/ColumnDecimal.h\n+++ b/src/Columns/ColumnDecimal.h\n@@ -129,6 +129,7 @@ class ColumnDecimal final : public COWHelper<ColumnVectorHelper, ColumnDecimal<T\n \n     StringRef serializeValueIntoArena(size_t n, Arena & arena, char const *& begin) const override;\n     const char * deserializeAndInsertFromArena(const char * pos) override;\n+    const char * skipSerializedInArena(const char * pos) const override;\n     void updateHashWithValue(size_t n, SipHash & hash) const override;\n     void updateWeakHash32(WeakHash32 & hash) const override;\n     void updateHashFast(SipHash & hash) const override;\ndiff --git a/src/Columns/ColumnFixedString.cpp b/src/Columns/ColumnFixedString.cpp\nindex 84bd0561f012..4d54a46c924f 100644\n--- a/src/Columns/ColumnFixedString.cpp\n+++ b/src/Columns/ColumnFixedString.cpp\n@@ -100,6 +100,11 @@ const char * ColumnFixedString::deserializeAndInsertFromArena(const char * pos)\n     return pos + n;\n }\n \n+const char * ColumnFixedString::skipSerializedInArena(const char * pos) const\n+{\n+    return pos + n;\n+}\n+\n void ColumnFixedString::updateHashWithValue(size_t index, SipHash & hash) const\n {\n     hash.update(reinterpret_cast<const char *>(&chars[n * index]), n);\ndiff --git a/src/Columns/ColumnFixedString.h b/src/Columns/ColumnFixedString.h\nindex 58f6d8142fb2..5fd482aef6e0 100644\n--- a/src/Columns/ColumnFixedString.h\n+++ b/src/Columns/ColumnFixedString.h\n@@ -112,6 +112,8 @@ class ColumnFixedString final : public COWHelper<ColumnVectorHelper, ColumnFixed\n \n     const char * deserializeAndInsertFromArena(const char * pos) override;\n \n+    const char * skipSerializedInArena(const char * pos) const override;\n+\n     void updateHashWithValue(size_t index, SipHash & hash) const override;\n \n     void updateWeakHash32(WeakHash32 & hash) const override;\ndiff --git a/src/Columns/ColumnFunction.h b/src/Columns/ColumnFunction.h\nindex 6080a94d1fb5..fa605e741aa7 100644\n--- a/src/Columns/ColumnFunction.h\n+++ b/src/Columns/ColumnFunction.h\n@@ -98,6 +98,11 @@ class ColumnFunction final : public COWHelper<IColumn, ColumnFunction>\n         throw Exception(\"Cannot deserialize to \" + getName(), ErrorCodes::NOT_IMPLEMENTED);\n     }\n \n+    const char * skipSerializedInArena(const char*) const override\n+    {\n+        throw Exception(\"Cannot skip serialized \" + getName(), ErrorCodes::NOT_IMPLEMENTED);\n+    }\n+\n     void updateHashWithValue(size_t, SipHash &) const override\n     {\n         throw Exception(\"updateHashWithValue is not implemented for \" + getName(), ErrorCodes::NOT_IMPLEMENTED);\ndiff --git a/src/Columns/ColumnLowCardinality.cpp b/src/Columns/ColumnLowCardinality.cpp\nindex e420fd78a39a..9433bf079e25 100644\n--- a/src/Columns/ColumnLowCardinality.cpp\n+++ b/src/Columns/ColumnLowCardinality.cpp\n@@ -247,6 +247,11 @@ const char * ColumnLowCardinality::deserializeAndInsertFromArena(const char * po\n     return new_pos;\n }\n \n+const char * ColumnLowCardinality::skipSerializedInArena(const char * pos) const\n+{\n+    return getDictionary().skipSerializedInArena(pos);\n+}\n+\n void ColumnLowCardinality::updateWeakHash32(WeakHash32 & hash) const\n {\n     auto s = size();\ndiff --git a/src/Columns/ColumnLowCardinality.h b/src/Columns/ColumnLowCardinality.h\nindex 54ddb8ce68b1..92bf7ff0f95e 100644\n--- a/src/Columns/ColumnLowCardinality.h\n+++ b/src/Columns/ColumnLowCardinality.h\n@@ -94,6 +94,8 @@ class ColumnLowCardinality final : public COWHelper<IColumn, ColumnLowCardinalit\n \n     const char * deserializeAndInsertFromArena(const char * pos) override;\n \n+    const char * skipSerializedInArena(const char * pos) const override;\n+\n     void updateHashWithValue(size_t n, SipHash & hash) const override\n     {\n         return getDictionary().updateHashWithValue(getIndexes().getUInt(n), hash);\ndiff --git a/src/Columns/ColumnMap.cpp b/src/Columns/ColumnMap.cpp\nindex 883a70db4352..05c0e0458d8e 100644\n--- a/src/Columns/ColumnMap.cpp\n+++ b/src/Columns/ColumnMap.cpp\n@@ -116,6 +116,11 @@ const char * ColumnMap::deserializeAndInsertFromArena(const char * pos)\n     return nested->deserializeAndInsertFromArena(pos);\n }\n \n+const char * ColumnMap::skipSerializedInArena(const char * pos) const\n+{\n+    return nested->skipSerializedInArena(pos);\n+}\n+\n void ColumnMap::updateHashWithValue(size_t n, SipHash & hash) const\n {\n     nested->updateHashWithValue(n, hash);\ndiff --git a/src/Columns/ColumnMap.h b/src/Columns/ColumnMap.h\nindex 3987d36b19d1..17f0ccc422c1 100644\n--- a/src/Columns/ColumnMap.h\n+++ b/src/Columns/ColumnMap.h\n@@ -58,6 +58,7 @@ class ColumnMap final : public COWHelper<IColumn, ColumnMap>\n     void popBack(size_t n) override;\n     StringRef serializeValueIntoArena(size_t n, Arena & arena, char const *& begin) const override;\n     const char * deserializeAndInsertFromArena(const char * pos) override;\n+    const char * skipSerializedInArena(const char * pos) const override;\n     void updateHashWithValue(size_t n, SipHash & hash) const override;\n     void updateWeakHash32(WeakHash32 & hash) const override;\n     void updateHashFast(SipHash & hash) const override;\ndiff --git a/src/Columns/ColumnNullable.cpp b/src/Columns/ColumnNullable.cpp\nindex df5b8789bfc9..1e5297514373 100644\n--- a/src/Columns/ColumnNullable.cpp\n+++ b/src/Columns/ColumnNullable.cpp\n@@ -152,6 +152,17 @@ const char * ColumnNullable::deserializeAndInsertFromArena(const char * pos)\n     return pos;\n }\n \n+const char * ColumnNullable::skipSerializedInArena(const char * pos) const\n+{\n+    UInt8 val = unalignedLoad<UInt8>(pos);\n+    pos += sizeof(val);\n+\n+    if (val == 0)\n+        return getNestedColumn().skipSerializedInArena(pos);\n+\n+    return pos;\n+}\n+\n void ColumnNullable::insertRangeFrom(const IColumn & src, size_t start, size_t length)\n {\n     const ColumnNullable & nullable_col = assert_cast<const ColumnNullable &>(src);\ndiff --git a/src/Columns/ColumnNullable.h b/src/Columns/ColumnNullable.h\nindex 0d68a6a0a3fd..963b3e1e8fad 100644\n--- a/src/Columns/ColumnNullable.h\n+++ b/src/Columns/ColumnNullable.h\n@@ -71,6 +71,7 @@ class ColumnNullable final : public COWHelper<IColumn, ColumnNullable>\n     void insertData(const char * pos, size_t length) override;\n     StringRef serializeValueIntoArena(size_t n, Arena & arena, char const *& begin) const override;\n     const char * deserializeAndInsertFromArena(const char * pos) override;\n+    const char * skipSerializedInArena(const char * pos) const override;\n     void insertRangeFrom(const IColumn & src, size_t start, size_t length) override;\n     void insert(const Field & x) override;\n     void insertFrom(const IColumn & src, size_t n) override;\ndiff --git a/src/Columns/ColumnString.cpp b/src/Columns/ColumnString.cpp\nindex 31f2b2f9275f..c1eddd539c99 100644\n--- a/src/Columns/ColumnString.cpp\n+++ b/src/Columns/ColumnString.cpp\n@@ -237,6 +237,12 @@ const char * ColumnString::deserializeAndInsertFromArena(const char * pos)\n     return pos + string_size;\n }\n \n+const char * ColumnString::skipSerializedInArena(const char * pos) const\n+{\n+    const size_t string_size = unalignedLoad<size_t>(pos);\n+    pos += sizeof(string_size);\n+    return pos + string_size;\n+}\n \n ColumnPtr ColumnString::index(const IColumn & indexes, size_t limit) const\n {\ndiff --git a/src/Columns/ColumnString.h b/src/Columns/ColumnString.h\nindex cf053d59b4d1..5f570afcdbb1 100644\n--- a/src/Columns/ColumnString.h\n+++ b/src/Columns/ColumnString.h\n@@ -189,6 +189,8 @@ class ColumnString final : public COWHelper<IColumn, ColumnString>\n \n     const char * deserializeAndInsertFromArena(const char * pos) override;\n \n+    const char * skipSerializedInArena(const char * pos) const override;\n+\n     void updateHashWithValue(size_t n, SipHash & hash) const override\n     {\n         size_t string_size = sizeAt(n);\ndiff --git a/src/Columns/ColumnTuple.cpp b/src/Columns/ColumnTuple.cpp\nindex 7128b428b1a9..bb59d58b75d8 100644\n--- a/src/Columns/ColumnTuple.cpp\n+++ b/src/Columns/ColumnTuple.cpp\n@@ -180,6 +180,14 @@ const char * ColumnTuple::deserializeAndInsertFromArena(const char * pos)\n     return pos;\n }\n \n+const char * ColumnTuple::skipSerializedInArena(const char * pos) const\n+{\n+    for (const auto & column : columns)\n+        pos = column->skipSerializedInArena(pos);\n+\n+    return pos;\n+}\n+\n void ColumnTuple::updateHashWithValue(size_t n, SipHash & hash) const\n {\n     for (const auto & column : columns)\ndiff --git a/src/Columns/ColumnTuple.h b/src/Columns/ColumnTuple.h\nindex 858eff7a75a6..3f5422c77195 100644\n--- a/src/Columns/ColumnTuple.h\n+++ b/src/Columns/ColumnTuple.h\n@@ -61,6 +61,7 @@ class ColumnTuple final : public COWHelper<IColumn, ColumnTuple>\n     void popBack(size_t n) override;\n     StringRef serializeValueIntoArena(size_t n, Arena & arena, char const *& begin) const override;\n     const char * deserializeAndInsertFromArena(const char * pos) override;\n+    const char * skipSerializedInArena(const char * pos) const override;\n     void updateHashWithValue(size_t n, SipHash & hash) const override;\n     void updateWeakHash32(WeakHash32 & hash) const override;\n     void updateHashFast(SipHash & hash) const override;\ndiff --git a/src/Columns/ColumnUnique.h b/src/Columns/ColumnUnique.h\nindex fbd3c3641b55..652487c2b090 100644\n--- a/src/Columns/ColumnUnique.h\n+++ b/src/Columns/ColumnUnique.h\n@@ -26,6 +26,7 @@ namespace ErrorCodes\n {\n     extern const int LOGICAL_ERROR;\n     extern const int ILLEGAL_COLUMN;\n+    extern const int NOT_IMPLEMENTED;\n }\n \n /** Stores another column with unique values\n@@ -78,6 +79,7 @@ class ColumnUnique final : public COWHelper<IColumnUnique, ColumnUnique<ColumnTy\n     bool getBool(size_t n) const override { return getNestedColumn()->getBool(n); }\n     bool isNullAt(size_t n) const override { return is_nullable && n == getNullValueIndex(); }\n     StringRef serializeValueIntoArena(size_t n, Arena & arena, char const *& begin) const override;\n+    const char * skipSerializedInArena(const char * pos) const override;\n     void updateHashWithValue(size_t n, SipHash & hash_func) const override\n     {\n         return getNestedColumn()->updateHashWithValue(n, hash_func);\n@@ -373,6 +375,12 @@ size_t ColumnUnique<ColumnType>::uniqueDeserializeAndInsertFromArena(const char\n     return uniqueInsertData(pos, string_size - 1);\n }\n \n+template <typename ColumnType>\n+const char * ColumnUnique<ColumnType>::skipSerializedInArena(const char *) const\n+{\n+    throw Exception(\"Method skipSerializedInArena is not supported for \" + this->getName(), ErrorCodes::NOT_IMPLEMENTED);\n+}\n+\n template <typename ColumnType>\n int ColumnUnique<ColumnType>::compareAt(size_t n, size_t m, const IColumn & rhs, int nan_direction_hint) const\n {\ndiff --git a/src/Columns/ColumnVector.cpp b/src/Columns/ColumnVector.cpp\nindex 19ba86c51201..a64906ba2578 100644\n--- a/src/Columns/ColumnVector.cpp\n+++ b/src/Columns/ColumnVector.cpp\n@@ -50,6 +50,12 @@ const char * ColumnVector<T>::deserializeAndInsertFromArena(const char * pos)\n     return pos + sizeof(T);\n }\n \n+template <typename T>\n+const char * ColumnVector<T>::skipSerializedInArena(const char * pos) const\n+{\n+    return pos + sizeof(T);\n+}\n+\n template <typename T>\n void ColumnVector<T>::updateHashWithValue(size_t n, SipHash & hash) const\n {\ndiff --git a/src/Columns/ColumnVector.h b/src/Columns/ColumnVector.h\nindex 5af5ef20310a..3e6b90e739e6 100644\n--- a/src/Columns/ColumnVector.h\n+++ b/src/Columns/ColumnVector.h\n@@ -154,6 +154,8 @@ class ColumnVector final : public COWHelper<ColumnVectorHelper, ColumnVector<T>>\n \n     const char * deserializeAndInsertFromArena(const char * pos) override;\n \n+    const char * skipSerializedInArena(const char * pos) const override;\n+\n     void updateHashWithValue(size_t n, SipHash & hash) const override;\n \n     void updateWeakHash32(WeakHash32 & hash) const override;\ndiff --git a/src/Columns/IColumn.h b/src/Columns/IColumn.h\nindex 9ed064ede148..1dedd191e1d6 100644\n--- a/src/Columns/IColumn.h\n+++ b/src/Columns/IColumn.h\n@@ -207,6 +207,10 @@ class IColumn : public COW<IColumn>\n     /// Returns pointer to the position after the read data.\n     virtual const char * deserializeAndInsertFromArena(const char * pos) = 0;\n \n+    /// Skip previously serialized value that was serialized using IColumn::serializeValueIntoArena method.\n+    /// Returns a pointer to the position after the deserialized data.\n+    virtual const char * skipSerializedInArena(const char *) const = 0;\n+\n     /// Update state of hash function with value of n-th element.\n     /// On subsequent calls of this method for sequence of column values of arbitrary types,\n     ///  passed bytes to hash must identify sequence of values unambiguously.\ndiff --git a/src/Columns/IColumnDummy.h b/src/Columns/IColumnDummy.h\nindex bb08e86bb301..ff405184b7ae 100644\n--- a/src/Columns/IColumnDummy.h\n+++ b/src/Columns/IColumnDummy.h\n@@ -67,6 +67,11 @@ class IColumnDummy : public IColumn\n         return pos;\n     }\n \n+    const char * skipSerializedInArena(const char * pos) const override\n+    {\n+        return pos;\n+    }\n+\n     void updateHashWithValue(size_t /*n*/, SipHash & /*hash*/) const override\n     {\n     }\ndiff --git a/src/Common/HashTable/Hash.h b/src/Common/HashTable/Hash.h\nindex ef20b70917d1..0abe96497bd9 100644\n--- a/src/Common/HashTable/Hash.h\n+++ b/src/Common/HashTable/Hash.h\n@@ -1,8 +1,9 @@\n #pragma once\n \n #include <common/types.h>\n-#include <Common/UInt128.h>\n #include <common/unaligned.h>\n+#include <common/StringRef.h>\n+#include <Common/UInt128.h>\n \n #include <type_traits>\n \n@@ -178,13 +179,19 @@ inline size_t DefaultHash64(std::enable_if_t<(sizeof(T) <= sizeof(UInt64)), T> k\n }\n \n template <typename T>\n-inline size_t DefaultHash64(std::enable_if_t<(sizeof(T) > sizeof(UInt64)), T> key)\n+static constexpr bool UseDefaultHashForBigInts =\n+    std::is_same_v<T, DB::Int128>  ||\n+    std::is_same_v<T, DB::UInt128> ||\n+    (is_big_int_v<T> && sizeof(T) == 32);\n+\n+template <typename T>\n+inline size_t DefaultHash64(std::enable_if_t<(sizeof(T) > sizeof(UInt64) && UseDefaultHashForBigInts<T>), T> key)\n {\n     if constexpr (std::is_same_v<T, DB::Int128>)\n     {\n         return intHash64(static_cast<UInt64>(key) ^ static_cast<UInt64>(key >> 64));\n     }\n-    if constexpr (std::is_same_v<T, DB::UInt128>)\n+    else if constexpr (std::is_same_v<T, DB::UInt128>)\n     {\n         return intHash64(key.low ^ key.high);\n     }\n@@ -195,6 +202,8 @@ inline size_t DefaultHash64(std::enable_if_t<(sizeof(T) > sizeof(UInt64)), T> ke\n             static_cast<UInt64>(key >> 128) ^\n             static_cast<UInt64>(key >> 256));\n     }\n+\n+    assert(false);\n     __builtin_unreachable();\n }\n \n@@ -341,6 +350,11 @@ struct IntHash32\n         }\n         else if constexpr (sizeof(T) <= sizeof(UInt64))\n             return intHash32<salt>(key);\n+\n+        assert(false);\n         __builtin_unreachable();\n     }\n };\n+\n+template <>\n+struct DefaultHash<StringRef> : public StringRefHash {};\ndiff --git a/src/Common/HashTable/LRUHashMap.h b/src/Common/HashTable/LRUHashMap.h\nindex 292006f24386..df9766c5ee83 100644\n--- a/src/Common/HashTable/LRUHashMap.h\n+++ b/src/Common/HashTable/LRUHashMap.h\n@@ -77,7 +77,7 @@ struct LRUHashMapCellNodeTraits\n     static void set_previous(node * __restrict ptr, node * __restrict prev) { ptr->prev = prev; }\n };\n \n-template <typename TKey, typename TValue, typename Hash, bool save_hash_in_cells>\n+template <typename TKey, typename TValue, typename Disposer, typename Hash, bool save_hash_in_cells>\n class LRUHashMapImpl :\n     private HashMapTable<\n         TKey,\n@@ -108,24 +108,33 @@ class LRUHashMapImpl :\n         boost::intrusive::value_traits<LRUHashMapCellIntrusiveValueTraits>,\n         boost::intrusive::constant_time_size<false>>;\n \n+    using LookupResult = typename Base::LookupResult;\n+    using ConstLookupResult = typename Base::ConstLookupResult;\n+\n     using iterator = typename LRUList::iterator;\n     using const_iterator = typename LRUList::const_iterator;\n     using reverse_iterator = typename LRUList::reverse_iterator;\n     using const_reverse_iterator = typename LRUList::const_reverse_iterator;\n \n-    LRUHashMapImpl(size_t max_size_, bool preallocate_max_size_in_hash_map = false)\n+    explicit LRUHashMapImpl(size_t max_size_, bool preallocate_max_size_in_hash_map = false, Disposer disposer_ = Disposer())\n         : Base(preallocate_max_size_in_hash_map ? max_size_ : 32)\n         , max_size(max_size_)\n+        , disposer(std::move(disposer_))\n     {\n         assert(max_size > 0);\n     }\n \n-    std::pair<Cell *, bool> insert(const Key & key, const Value & value)\n+    ~LRUHashMapImpl()\n+    {\n+        clear();\n+    }\n+\n+    std::pair<Cell *, bool> ALWAYS_INLINE insert(const Key & key, const Value & value)\n     {\n         return emplace(key, value);\n     }\n \n-    std::pair<Cell *, bool> insert(const Key & key, Value && value)\n+    std::pair<Cell *, bool> ALWAYS_INLINE insert(const Key & key, Value && value)\n     {\n         return emplace(key, std::move(value));\n     }\n@@ -147,15 +156,16 @@ class LRUHashMapImpl :\n         if (size() == max_size)\n         {\n             /// Erase least recently used element from front of the list\n-            Cell & node = lru_list.front();\n+            Cell copy_node = lru_list.front();\n \n-            const Key & element_to_remove_key = node.getKey();\n-            size_t key_hash = node.getHash(*this);\n+            const Key & element_to_remove_key = copy_node.getKey();\n \n             lru_list.pop_front();\n \n-            [[maybe_unused]] bool erased = Base::erase(element_to_remove_key, key_hash);\n+            [[maybe_unused]] bool erased = Base::erase(element_to_remove_key);\n             assert(erased);\n+\n+            disposer(element_to_remove_key, copy_node.getMapped());\n         }\n \n         [[maybe_unused]] bool inserted;\n@@ -174,46 +184,64 @@ class LRUHashMapImpl :\n         return std::make_pair(it, true);\n     }\n \n-    using Base::find;\n-\n-    Value & get(const Key & key)\n+    LookupResult ALWAYS_INLINE find(const Key & key)\n     {\n         auto it = Base::find(key);\n-        assert(it);\n \n-        Value & value = it->getMapped();\n+        if (!it)\n+            return nullptr;\n \n         /// Put cell to the end of lru list\n         lru_list.splice(lru_list.end(), lru_list, lru_list.iterator_to(*it));\n \n-        return value;\n+        return it;\n+    }\n+\n+    ConstLookupResult ALWAYS_INLINE find(const Key & key) const\n+    {\n+        return const_cast<std::decay_t<decltype(*this)> *>(this)->find(key);\n     }\n \n-    const Value & get(const Key & key) const\n+    Value & ALWAYS_INLINE get(const Key & key)\n+    {\n+        auto it = find(key);\n+        assert(it);\n+\n+        return it->getMapped();\n+    }\n+\n+    const Value & ALWAYS_INLINE get(const Key & key) const\n     {\n         return const_cast<std::decay_t<decltype(*this)> *>(this)->get(key);\n     }\n \n-    bool contains(const Key & key) const\n+    bool ALWAYS_INLINE contains(const Key & key) const\n     {\n-        return Base::has(key);\n+        return find(key) != nullptr;\n     }\n \n-    bool erase(const Key & key)\n+    bool ALWAYS_INLINE erase(const Key & key)\n     {\n-        auto hash = Base::hash(key);\n-        auto it = Base::find(key, hash);\n+        auto key_hash = Base::hash(key);\n+        auto it = Base::find(key, key_hash);\n \n         if (!it)\n             return false;\n \n         lru_list.erase(lru_list.iterator_to(*it));\n \n-        return Base::erase(key, hash);\n+        Cell copy_node = *it;\n+        Base::erase(key, key_hash);\n+        disposer(copy_node.getKey(), copy_node.getMapped());\n+\n+        return true;\n     }\n \n-    void clear()\n+    void ALWAYS_INLINE clear()\n     {\n+        for (auto & cell : lru_list)\n+            disposer(cell.getKey(), cell.getMapped());\n+\n         lru_list.clear();\n         Base::clear();\n     }\n@@ -222,6 +250,10 @@ class LRUHashMapImpl :\n \n     size_t getMaxSize() const { return max_size; }\n \n+    size_t getSizeInBytes() const { return Base::getBufferSizeInBytes(); }\n+\n+    using Base::hash;\n+\n     iterator begin() { return lru_list.begin(); }\n     const_iterator begin() const { return lru_list.cbegin(); }\n     iterator end() { return lru_list.end(); }\n@@ -235,10 +267,17 @@ class LRUHashMapImpl :\n private:\n     size_t max_size;\n     LRUList lru_list;\n+    Disposer disposer;\n+};\n+\n+template <typename Key, typename Mapped>\n+struct DefaultCellDisposer\n+{\n+    void operator()(const Key &, const Mapped &) const {}\n };\n \n-template <typename Key, typename Value, typename Hash = DefaultHash<Key>>\n-using LRUHashMap = LRUHashMapImpl<Key, Value, Hash, false>;\n+template <typename Key, typename Value, typename Disposer = DefaultCellDisposer<Key, Value>, typename Hash = DefaultHash<Key>>\n+using LRUHashMap = LRUHashMapImpl<Key, Value, Disposer, Hash, false>;\n \n-template <typename Key, typename Value, typename Hash = DefaultHash<Key>>\n-using LRUHashMapWithSavedHash = LRUHashMapImpl<Key, Value, Hash, true>;\n+template <typename Key, typename Value, typename Disposer = DefaultCellDisposer<Key, Value>, typename Hash = DefaultHash<Key>>\n+using LRUHashMapWithSavedHash = LRUHashMapImpl<Key, Value, Disposer, Hash, true>;\ndiff --git a/src/Dictionaries/BucketCache.h b/src/Dictionaries/BucketCache.h\ndeleted file mode 100644\nindex 381110066a65..000000000000\n--- a/src/Dictionaries/BucketCache.h\n+++ /dev/null\n@@ -1,226 +0,0 @@\n-#pragma once\n-\n-#include <Common/HashTable/Hash.h>\n-#include <common/logger_useful.h>\n-#include <type_traits>\n-#include <vector>\n-\n-namespace DB\n-{\n-\n-namespace\n-{\n-    inline size_t roundUpToPowerOfTwoOrZero(size_t x)\n-    {\n-        size_t r = 8;\n-        while (x > r)\n-            r <<= 1;\n-        return r;\n-    }\n-}\n-\n-struct EmptyDeleter {};\n-\n-struct Int64Hasher\n-{\n-    size_t operator()(const size_t x) const\n-    {\n-        return intHash64(x);\n-    }\n-};\n-\n-\n-/*\n-    Class for storing cache index.\n-    It consists of two arrays.\n-    The first one is split into buckets (each stores 8 elements (cells)) determined by hash of the element key.\n-    The second one is split into 4bit numbers, which are positions in bucket for next element write (So cache uses FIFO eviction algorithm inside each bucket).\n-*/\n-template <typename K, typename V, typename Hasher, typename Deleter = EmptyDeleter>\n-class BucketCacheIndex\n-{\n-    struct Cell\n-    {\n-        K key;\n-        V index;\n-    };\n-\n-public:\n-    template <typename = std::enable_if<std::is_same_v<EmptyDeleter, Deleter>>>\n-    BucketCacheIndex(size_t cells_)\n-        : buckets(roundUpToPowerOfTwoOrZero(cells_) / bucket_size)\n-        , bucket_mask(buckets - 1)\n-        , cells(buckets * bucket_size)\n-        , positions((buckets / 2) + 1)\n-    {\n-        for (auto & cell : cells)\n-            cell.index.setNotExists();\n-        for (size_t bucket = 0; bucket < buckets; ++bucket)\n-            setPosition(bucket, 0);\n-    }\n-\n-    template <typename = std::enable_if<!std::is_same_v<EmptyDeleter, Deleter>>>\n-    BucketCacheIndex(size_t cells_, Deleter deleter_)\n-        : deleter(deleter_)\n-        , buckets(roundUpToPowerOfTwoOrZero(cells_) / bucket_size)\n-        , bucket_mask(buckets - 1)\n-        , cells(buckets * bucket_size)\n-        , positions((buckets / 2) + 1)\n-    {\n-        for (auto & cell : cells)\n-            cell.index.setNotExists();\n-        for (size_t bucket = 0; bucket < buckets; ++bucket)\n-            setPosition(bucket, 0);\n-    }\n-\n-    void set(K key, V val)\n-    {\n-        const size_t bucket = (hash(key) & bucket_mask);\n-        const size_t idx = getCellIndex(key, bucket);\n-        if (!cells[idx].index.exists())\n-        {\n-            incPosition(bucket);\n-            ++sz;\n-        }\n-\n-        cells[idx].key = key;\n-        cells[idx].index = val;\n-    }\n-\n-    template <typename = std::enable_if<!std::is_same_v<EmptyDeleter, Deleter>>>\n-    void setWithDelete(K key, V val)\n-    {\n-        const size_t bucket = (hash(key) & bucket_mask);\n-        const size_t idx = getCellIndex(key, bucket);\n-        if (!cells[idx].index.exists())\n-        {\n-            incPosition(bucket);\n-            ++sz;\n-        }\n-        else\n-        {\n-            deleter(cells[idx].key);\n-        }\n-\n-        cells[idx].key = key;\n-        cells[idx].index = val;\n-    }\n-\n-    bool get(K key, V & val) const\n-    {\n-        const size_t bucket = (hash(key) & bucket_mask);\n-        const size_t idx = getCellIndex(key, bucket);\n-        if (!cells[idx].index.exists() || cells[idx].key != key)\n-            return false;\n-        val = cells[idx].index;\n-        return true;\n-    }\n-\n-    bool getKeyAndValue(K & key, V & val) const\n-    {\n-        const size_t bucket = (hash(key) & bucket_mask);\n-        const size_t idx = getCellIndex(key, bucket);\n-        if (!cells[idx].index.exists() || cells[idx].key != key)\n-            return false;\n-        key = cells[idx].key;\n-        val = cells[idx].index;\n-        return true;\n-    }\n-\n-    bool erase(K key)\n-    {\n-        const size_t bucket = (hash(key) & bucket_mask);\n-        const size_t idx = getCellIndex(key, bucket);\n-        if (!cells[idx].index.exists() || cells[idx].key != key)\n-            return false;\n-\n-        cells[idx].index.setNotExists();\n-        --sz;\n-        if constexpr (!std::is_same_v<EmptyDeleter, Deleter>)\n-            deleter(cells[idx].key);\n-\n-        return true;\n-    }\n-\n-    size_t size() const\n-    {\n-        return sz;\n-    }\n-\n-    size_t capacity() const\n-    {\n-        return cells.size();\n-    }\n-\n-    auto keys() const\n-    {\n-        std::vector<K> res;\n-        for (const auto & cell : cells)\n-        {\n-            if (cell.index.exists())\n-            {\n-                res.push_back(cell.key);\n-            }\n-        }\n-        return res;\n-    }\n-\n-private:\n-    /// Searches for the key in the bucket.\n-    /// Returns index of cell with provided key.\n-    size_t getCellIndex(const K key, const size_t bucket) const\n-    {\n-        const size_t pos = getPosition(bucket);\n-        for (int idx = 7; idx >= 0; --idx)\n-        {\n-            const size_t cur = ((pos + 1 + idx) & pos_mask);\n-            if (cells[bucket * bucket_size + cur].index.exists() &&\n-                cells[bucket * bucket_size + cur].key == key)\n-            {\n-                return bucket * bucket_size + cur;\n-            }\n-        }\n-\n-        return bucket * bucket_size + pos;\n-    }\n-\n-    /// Returns current position for write in the bucket.\n-    size_t getPosition(const size_t bucket) const\n-    {\n-        const size_t idx = (bucket >> 1);\n-        if ((bucket & 1) == 0)\n-            return ((positions[idx] >> 4) & pos_mask);\n-        return (positions[idx] & pos_mask);\n-    }\n-\n-    /// Sets current posiotion in the bucket.\n-    void setPosition(const size_t bucket, const size_t pos)\n-    {\n-        const size_t idx = bucket >> 1;\n-        if ((bucket & 1) == 0)\n-            positions[idx] = ((pos << 4) | (positions[idx] & ((1 << 4) - 1)));\n-        else\n-            positions[idx] = (pos | (positions[idx] & (((1 << 4) - 1) << 4)));\n-    }\n-\n-    void incPosition(const size_t bucket)\n-    {\n-        setPosition(bucket, (getPosition(bucket) + 1) & pos_mask);\n-    }\n-\n-    static constexpr size_t bucket_size = 8;\n-    static constexpr size_t pos_size = 3;\n-    static constexpr size_t pos_mask = (1 << pos_size) - 1;\n-\n-    Hasher hash;\n-    Deleter deleter;\n-\n-    size_t buckets;\n-    size_t bucket_mask;\n-\n-    std::vector<Cell> cells;\n-    std::vector<char> positions;\n-    size_t sz = 0;\n-};\n-\n-}\ndiff --git a/src/Dictionaries/CMakeLists.txt b/src/Dictionaries/CMakeLists.txt\nindex 4d6ab4b85f88..563c0f3914ba 100644\n--- a/src/Dictionaries/CMakeLists.txt\n+++ b/src/Dictionaries/CMakeLists.txt\n@@ -20,6 +20,10 @@ target_link_libraries(clickhouse_dictionaries\n         string_utils\n )\n \n+target_link_libraries(clickhouse_dictionaries\n+    PUBLIC\n+        abseil_swiss_tables)\n+\n if(USE_CASSANDRA)\n     target_include_directories(clickhouse_dictionaries SYSTEM PRIVATE ${CASSANDRA_INCLUDE_DIR})\n endif()\ndiff --git a/src/Dictionaries/CacheDictionary.cpp b/src/Dictionaries/CacheDictionary.cpp\nindex 67bcab109ea2..fe777355ca19 100644\n--- a/src/Dictionaries/CacheDictionary.cpp\n+++ b/src/Dictionaries/CacheDictionary.cpp\n@@ -1,25 +1,19 @@\n #include \"CacheDictionary.h\"\n \n #include <memory>\n-#include <Columns/ColumnString.h>\n-#include <Common/BitHelpers.h>\n-#include <Common/CurrentMetrics.h>\n-#include <Common/HashTable/Hash.h>\n-#include <Common/ProfileEvents.h>\n-#include <Common/ProfilingScopedRWLock.h>\n-#include <Common/randomSeed.h>\n-#include <Common/typeid_cast.h>\n-#include <Core/Defines.h>\n-#include <IO/WriteBufferFromOStream.h>\n+\n #include <ext/range.h>\n #include <ext/size.h>\n #include <ext/map.h>\n #include <ext/chrono_io.h>\n-#include <Common/setThreadName.h>\n-#include <DataTypes/DataTypesDecimal.h>\n-#include \"DictionaryBlockInputStream.h\"\n-#include \"DictionaryFactory.h\"\n-#include <Functions/FunctionHelpers.h>\n+\n+#include <Core/Defines.h>\n+#include <Common/CurrentMetrics.h>\n+#include <Common/HashTable/Hash.h>\n+#include <Common/HashTable/HashSet.h>\n+#include <Common/ProfileEvents.h>\n+#include <Common/ProfilingScopedRWLock.h>\n+#include <Dictionaries/DictionaryBlockInputStream.h>\n \n namespace ProfileEvents\n {\n@@ -40,88 +34,85 @@ namespace CurrentMetrics\n extern const Metric DictCacheRequests;\n }\n \n-\n namespace DB\n {\n namespace ErrorCodes\n {\n     extern const int CACHE_DICTIONARY_UPDATE_FAIL;\n     extern const int TYPE_MISMATCH;\n-    extern const int BAD_ARGUMENTS;\n     extern const int UNSUPPORTED_METHOD;\n-    extern const int TOO_SMALL_BUFFER_SIZE;\n-    extern const int TIMEOUT_EXCEEDED;\n-}\n-\n-\n-inline size_t CacheDictionary::getCellIdx(const Key id) const\n-{\n-    const auto hash = intHash64(id);\n-    const auto idx = hash & size_overlap_mask;\n-    return idx;\n }\n \n-\n-CacheDictionary::CacheDictionary(\n+template <DictionaryKeyType dictionary_key_type>\n+CacheDictionary<dictionary_key_type>::CacheDictionary(\n     const StorageID & dict_id_,\n     const DictionaryStructure & dict_struct_,\n     DictionarySourcePtr source_ptr_,\n+    CacheDictionaryStoragePtr cache_storage_ptr_,\n+    CacheDictionaryUpdateQueueConfiguration update_queue_configuration_,\n     DictionaryLifetime dict_lifetime_,\n-    size_t strict_max_lifetime_seconds_,\n-    size_t size_,\n-    bool allow_read_expired_keys_,\n-    size_t max_update_queue_size_,\n-    size_t update_queue_push_timeout_milliseconds_,\n-    size_t query_wait_timeout_milliseconds_,\n-    size_t max_threads_for_updates_)\n+    bool allow_read_expired_keys_)\n     : IDictionary(dict_id_)\n     , dict_struct(dict_struct_)\n     , source_ptr{std::move(source_ptr_)}\n+    , cache_storage_ptr(cache_storage_ptr_)\n+    , update_queue(\n+        dict_id_.getNameForLogs(),\n+        update_queue_configuration_,\n+        [this](CacheDictionaryUpdateUnitPtr<dictionary_key_type> unit_to_update)\n+        {\n+            update(unit_to_update);\n+        })\n     , dict_lifetime(dict_lifetime_)\n-    , strict_max_lifetime_seconds(strict_max_lifetime_seconds_)\n-    , allow_read_expired_keys(allow_read_expired_keys_)\n-    , max_update_queue_size(max_update_queue_size_)\n-    , update_queue_push_timeout_milliseconds(update_queue_push_timeout_milliseconds_)\n-    , query_wait_timeout_milliseconds(query_wait_timeout_milliseconds_)\n-    , max_threads_for_updates(max_threads_for_updates_)\n     , log(&Poco::Logger::get(\"ExternalDictionaries\"))\n-    , size{roundUpToPowerOfTwoOrZero(std::max(size_, size_t(max_collision_length)))}\n-    , size_overlap_mask{this->size - 1}\n-    , cells{this->size}\n+    , allow_read_expired_keys(allow_read_expired_keys_)\n     , rnd_engine(randomSeed())\n-    , update_queue(max_update_queue_size_)\n-    , update_pool(max_threads_for_updates)\n {\n     if (!source_ptr->supportsSelectiveLoad())\n         throw Exception{full_name + \": source cannot be used with CacheDictionary\", ErrorCodes::UNSUPPORTED_METHOD};\n \n-    createAttributes();\n-    for (size_t i = 0; i < max_threads_for_updates; ++i)\n-        update_pool.scheduleOrThrowOnError([this] { updateThreadFunction(); });\n+    setupHierarchicalAttribute();\n }\n \n-CacheDictionary::~CacheDictionary()\n+template <DictionaryKeyType dictionary_key_type>\n+CacheDictionary<dictionary_key_type>::~CacheDictionary()\n {\n-    finished = true;\n-    update_queue.clear();\n-    for (size_t i = 0; i < max_threads_for_updates; ++i)\n-    {\n-        auto empty_finishing_ptr = std::make_shared<UpdateUnit>(std::vector<Key>());\n-        update_queue.push(empty_finishing_ptr);\n-    }\n-    update_pool.wait();\n+    update_queue.stopAndWait();\n }\n \n-size_t CacheDictionary::getBytesAllocated() const\n+template <DictionaryKeyType dictionary_key_type>\n+size_t CacheDictionary<dictionary_key_type>::getElementCount() const\n+{\n+    const ProfilingScopedReadRWLock read_lock{rw_lock, ProfileEvents::DictCacheLockReadNs};\n+    return cache_storage_ptr->getSize();\n+}\n+\n+template <DictionaryKeyType dictionary_key_type>\n+size_t CacheDictionary<dictionary_key_type>::getBytesAllocated() const\n {\n     /// In case of existing string arena we check the size of it.\n     /// But the same appears in setAttributeValue() function, which is called from update() function\n     /// which in turn is called from another thread.\n     const ProfilingScopedReadRWLock read_lock{rw_lock, ProfileEvents::DictCacheLockReadNs};\n-    return bytes_allocated + (string_arena ? string_arena->size() : 0);\n+    return cache_storage_ptr->getBytesAllocated();\n+}\n+\n+template <DictionaryKeyType dictionary_key_type>\n+double CacheDictionary<dictionary_key_type>::getLoadFactor() const\n+{\n+    const ProfilingScopedReadRWLock read_lock{rw_lock, ProfileEvents::DictCacheLockReadNs};\n+    return static_cast<double>(cache_storage_ptr->getSize()) / cache_storage_ptr->getMaxSize();\n+}\n+\n+template <DictionaryKeyType dictionary_key_type>\n+std::exception_ptr CacheDictionary<dictionary_key_type>::getLastException() const\n+{\n+    const ProfilingScopedReadRWLock read_lock{rw_lock, ProfileEvents::DictCacheLockReadNs};\n+    return last_exception;\n }\n \n-const IDictionarySource * CacheDictionary::getSource() const\n+template <DictionaryKeyType dictionary_key_type>\n+const IDictionarySource * CacheDictionary<dictionary_key_type>::getSource() const\n {\n     /// Mutex required here because of the getSourceAndUpdateIfNeeded() function\n     /// which is used from another thread.\n@@ -129,34 +120,51 @@ const IDictionarySource * CacheDictionary::getSource() const\n     return source_ptr.get();\n }\n \n-void CacheDictionary::toParent(const PaddedPODArray<Key> & ids, PaddedPODArray<Key> & out) const\n+template <DictionaryKeyType dictionary_key_type>\n+void CacheDictionary<dictionary_key_type>::toParent(const PaddedPODArray<UInt64> & ids [[maybe_unused]], PaddedPODArray<UInt64> & out [[maybe_unused]]) const\n {\n-    const auto null_value = std::get<UInt64>(hierarchical_attribute->null_value);\n-    DictionaryDefaultValueExtractor<UInt64> default_value_extractor(null_value);\n-    getItemsNumberImpl<UInt64, UInt64>(*hierarchical_attribute, ids, out, default_value_extractor);\n+    if constexpr (dictionary_key_type == DictionaryKeyType::simple)\n+    {\n+        /// Run update on requested keys before fetch from storage\n+        const auto & attribute_name = hierarchical_attribute->name;\n+\n+        auto result_type = std::make_shared<DataTypeUInt64>();\n+        auto input_column = result_type->createColumn();\n+        auto & input_column_typed = assert_cast<ColumnVector<UInt64> &>(*input_column);\n+        auto & data = input_column_typed.getData();\n+        data.insert(ids.begin(), ids.end());\n+\n+        auto column = getColumn({attribute_name}, result_type, {std::move(input_column)}, {result_type}, {nullptr});\n+        const auto & result_column_typed = assert_cast<const ColumnVector<UInt64> &>(*column);\n+        const auto & result_data = result_column_typed.getData();\n+\n+        out.assign(result_data);\n+    }\n+    else\n+        throw Exception(\"Hierarchy is not supported for complex key CacheDictionary\", ErrorCodes::UNSUPPORTED_METHOD);\n }\n \n \n /// Allow to use single value in same way as array.\n-static inline CacheDictionary::Key getAt(const PaddedPODArray<CacheDictionary::Key> & arr, const size_t idx)\n+static inline UInt64 getAt(const PaddedPODArray<UInt64> & arr, const size_t idx)\n {\n     return arr[idx];\n }\n-static inline CacheDictionary::Key getAt(const CacheDictionary::Key & value, const size_t)\n+static inline UInt64 getAt(const UInt64 & value, const size_t)\n {\n     return value;\n }\n \n-\n+template <DictionaryKeyType dictionary_key_type>\n template <typename AncestorType>\n-void CacheDictionary::isInImpl(const PaddedPODArray<Key> & child_ids, const AncestorType & ancestor_ids, PaddedPODArray<UInt8> & out) const\n+void CacheDictionary<dictionary_key_type>::isInImpl(const PaddedPODArray<Key> & child_ids, const AncestorType & ancestor_ids, PaddedPODArray<UInt8> & out) const\n {\n     /// Transform all children to parents until ancestor id or null_value will be reached.\n \n     size_t out_size = out.size();\n     memset(out.data(), 0xFF, out_size); /// 0xFF means \"not calculated\"\n \n-    const auto null_value = std::get<UInt64>(hierarchical_attribute->null_value);\n+    const auto null_value = hierarchical_attribute->null_value.get<UInt64>();\n \n     PaddedPODArray<Key> children(out_size, 0);\n     PaddedPODArray<Key> parents(child_ids.begin(), child_ids.end());\n@@ -213,22 +221,25 @@ void CacheDictionary::isInImpl(const PaddedPODArray<Key> & child_ids, const Ance\n     }\n }\n \n-void CacheDictionary::isInVectorVector(\n-    const PaddedPODArray<Key> & child_ids, const PaddedPODArray<Key> & ancestor_ids, PaddedPODArray<UInt8> & out) const\n+template <DictionaryKeyType dictionary_key_type>\n+void CacheDictionary<dictionary_key_type>::isInVectorVector(\n+    const PaddedPODArray<UInt64> & child_ids, const PaddedPODArray<UInt64> & ancestor_ids, PaddedPODArray<UInt8> & out) const\n {\n     isInImpl(child_ids, ancestor_ids, out);\n }\n \n-void CacheDictionary::isInVectorConstant(const PaddedPODArray<Key> & child_ids, const Key ancestor_id, PaddedPODArray<UInt8> & out) const\n+template <DictionaryKeyType dictionary_key_type>\n+void CacheDictionary<dictionary_key_type>::isInVectorConstant(const PaddedPODArray<UInt64> & child_ids, const UInt64 ancestor_id, PaddedPODArray<UInt8> & out) const\n {\n     isInImpl(child_ids, ancestor_id, out);\n }\n \n-void CacheDictionary::isInConstantVector(const Key child_id, const PaddedPODArray<Key> & ancestor_ids, PaddedPODArray<UInt8> & out) const\n+template <DictionaryKeyType dictionary_key_type>\n+void CacheDictionary<dictionary_key_type>::isInConstantVector(const UInt64 child_id, const PaddedPODArray<UInt64> & ancestor_ids, PaddedPODArray<UInt8> & out) const\n {\n     /// Special case with single child value.\n \n-    const auto null_value = std::get<UInt64>(hierarchical_attribute->null_value);\n+    const auto null_value = hierarchical_attribute->null_value.get<UInt64>();\n \n     PaddedPODArray<Key> child(1, child_id);\n     PaddedPODArray<Key> parent(1);\n@@ -251,1032 +262,435 @@ void CacheDictionary::isInConstantVector(const Key child_id, const PaddedPODArra\n         out[i] = std::find(ancestors.begin(), ancestors.end(), ancestor_ids[i]) != ancestors.end();\n }\n \n-ColumnPtr CacheDictionary::getColumn(\n-    const std::string & attribute_name,\n-    const DataTypePtr & result_type,\n-    const Columns & key_columns,\n-    const DataTypes &,\n-    const ColumnPtr default_values_column) const\n+template <DictionaryKeyType dictionary_key_type>\n+void CacheDictionary<dictionary_key_type>::setupHierarchicalAttribute()\n {\n-    ColumnPtr result;\n-\n-    PaddedPODArray<Key> backup_storage;\n-    const auto & keys = getColumnVectorData(this, key_columns.front(), backup_storage);\n-    auto keys_size = keys.size();\n-\n-    auto & attribute = getAttribute(attribute_name);\n-    const auto & dictionary_attribute = dict_struct.getAttribute(attribute_name, result_type);\n-\n-    auto type_call = [&](const auto &dictionary_attribute_type)\n+    /// TODO: Move this to DictionaryStructure\n+    for (const auto & attribute : dict_struct.attributes)\n     {\n-        using Type = std::decay_t<decltype(dictionary_attribute_type)>;\n-        using AttributeType = typename Type::AttributeType;\n-        using ColumnProvider = DictionaryAttributeColumnProvider<AttributeType>;\n-\n-        const auto & null_value = std::get<AttributeType>(attribute.null_value);\n-        DictionaryDefaultValueExtractor<AttributeType> default_value_extractor(null_value, default_values_column);\n-\n-        auto column = ColumnProvider::getColumn(dictionary_attribute, keys_size);\n-\n-        if constexpr (std::is_same_v<AttributeType, String>)\n-        {\n-            getItemsString(attribute, keys, column.get(), default_value_extractor);\n-        }\n-        else\n+        if (attribute.hierarchical)\n         {\n-            auto & out = column->getData();\n-            getItemsNumberImpl<AttributeType, AttributeType>(attribute, keys, out, default_value_extractor);\n-        }\n+            hierarchical_attribute = &attribute;\n \n-        result = std::move(column);\n-    };\n-\n-    callOnDictionaryAttributeType(attribute.type, type_call);\n-\n-    return result;\n+            if (attribute.underlying_type != AttributeUnderlyingType::utUInt64)\n+                throw Exception{full_name + \": hierarchical attribute must be UInt64.\", ErrorCodes::TYPE_MISMATCH};\n+        }\n+    }\n }\n \n-template <typename AttributeType, typename OutputType, typename DefaultValueExtractor>\n-void CacheDictionary::getItemsNumberImpl(\n-    Attribute & attribute,\n-    const PaddedPODArray<Key> & ids,\n-    ResultArrayType<OutputType> & out,\n-    DefaultValueExtractor & default_value_extractor) const\n+template <DictionaryKeyType dictionary_key_type>\n+ColumnPtr CacheDictionary<dictionary_key_type>::getColumn(\n+    const std::string & attribute_name,\n+    const DataTypePtr & result_type,\n+    const Columns & key_columns,\n+    const DataTypes & key_types,\n+    const ColumnPtr & default_values_column) const\n {\n-    /// First fill everything with default values\n-    const auto rows = ext::size(ids);\n-    for (const auto row : ext::range(0, rows))\n-        out[row] = default_value_extractor[row];\n-\n-    /// Maybe there are duplicate keys, so we remember their indices.\n-    std::unordered_map<Key, std::vector<size_t>> cache_expired_or_not_found_ids;\n-\n-    auto & attribute_array = std::get<ContainerPtrType<AttributeType>>(attribute.arrays);\n-\n-    size_t cache_hit = 0;\n-    size_t cache_not_found_count = 0;\n-    size_t cache_expired_cound = 0;\n-\n-    {\n-        const ProfilingScopedReadRWLock read_lock{rw_lock, ProfileEvents::DictCacheLockReadNs};\n-\n-        const auto now = std::chrono::system_clock::now();\n-\n-        auto insert_to_answer_routine = [&](size_t row, size_t idx)\n-        {\n-            auto & cell = cells[idx];\n-            if (!cell.isDefault())\n-                out[row] = static_cast<OutputType>(attribute_array[idx]);\n-        };\n-\n-        /// fetch up-to-date values, decide which ones require update\n-        for (const auto row : ext::range(0, rows))\n-        {\n-            const auto id = ids[row];\n-\n-            /** cell should be updated if either:\n-                *    1. ids do not match,\n-                *    2. cell has expired,\n-                *    3. explicit defaults were specified and cell was set default. */\n-\n-            const auto [cell_idx, state] = findCellIdxForGet(id, now);\n+    return getColumns({attribute_name}, {result_type}, key_columns, key_types, {default_values_column}).front();\n+}\n \n-            if (state == ResultState::FoundAndValid)\n-            {\n-                ++cache_hit;\n-                insert_to_answer_routine(row, cell_idx);\n-            }\n-            else if (state == ResultState::NotFound || state == ResultState::FoundButExpiredPermanently)\n-            {\n-                ++cache_not_found_count;\n-                cache_expired_or_not_found_ids[id].push_back(row);\n-            }\n-            else if (state == ResultState::FoundButExpired)\n-            {\n-                cache_expired_cound++;\n-                cache_expired_or_not_found_ids[id].push_back(row);\n+template <DictionaryKeyType dictionary_key_type>\n+Columns CacheDictionary<dictionary_key_type>::getColumns(\n+    const Strings & attribute_names,\n+    const DataTypes &,\n+    const Columns & key_columns,\n+    const DataTypes & key_types,\n+    const Columns & default_values_columns) const\n+{\n+    if (dictionary_key_type == DictionaryKeyType::complex)\n+        dict_struct.validateKeyTypes(key_types);\n \n-                if (allow_read_expired_keys)\n-                    insert_to_answer_routine(row, cell_idx);\n-            }\n-        }\n-    }\n+    Arena complex_keys_arena;\n+    DictionaryKeysExtractor<dictionary_key_type> extractor(key_columns, complex_keys_arena);\n+    auto & keys = extractor.getKeys();\n \n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysExpired, cache_expired_cound);\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysNotFound, cache_not_found_count);\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysHit, cache_hit);\n+    return getColumnsImpl(attribute_names, key_columns, keys, default_values_columns);\n+}\n \n-    query_count.fetch_add(rows, std::memory_order_relaxed);\n-    hit_count.fetch_add(rows - cache_not_found_count - cache_expired_cound, std::memory_order_release);\n+template <DictionaryKeyType dictionary_key_type>\n+Columns CacheDictionary<dictionary_key_type>::getColumnsImpl(\n+    const Strings & attribute_names,\n+    const Columns & key_columns,\n+    const PaddedPODArray<KeyType> & keys,\n+    const Columns & default_values_columns) const\n+{\n+    /**\n+    * Flow of getColumsImpl\n+    * 1. Get fetch result from storage\n+    * 2. If all keys are found in storage and not expired\n+    *   2.1. If storage returns fetched columns in order of keys then result is returned to client.\n+    *   2.2. If storage does not return fetched columns in order of keys then reorder\n+    *    result columns and return result to client.\n+    * 3. If all keys are found in storage but some of them are expired and we allow to read expired keys\n+    * start async request to source and perform actions from step 2 for result returned from storage.\n+    * 4. If some keys are found and some are not, start sync update from source.\n+    * 5. Aggregate columns returned from storage and source, if key is not found in storage and in source\n+    * use default value.\n+    */\n+\n+    DictionaryStorageFetchRequest request(dict_struct, attribute_names, default_values_columns);\n+\n+    FetchResult result_of_fetch_from_storage;\n \n-    if (!cache_not_found_count)\n     {\n-        /// Nothing to update - return\n-        if (!cache_expired_cound)\n-            return;\n-\n-        /// Update async only if allow_read_expired_keys_is_enabledadd condvar usage and better code\n-        if (allow_read_expired_keys)\n-        {\n-            std::vector<Key> required_expired_ids;\n-            required_expired_ids.reserve(cache_expired_cound);\n-            std::transform(std::begin(cache_expired_or_not_found_ids), std::end(cache_expired_or_not_found_ids),\n-                           std::back_inserter(required_expired_ids), [](auto & pair) { return pair.first; });\n-\n-            /// request new values\n-            auto update_unit_ptr = std::make_shared<UpdateUnit>(std::move(required_expired_ids));\n-\n-            tryPushToUpdateQueueOrThrow(update_unit_ptr);\n+        /// Write lock on storage\n+        const ProfilingScopedWriteRWLock write_lock{rw_lock, ProfileEvents::DictCacheLockWriteNs};\n \n-            /// Nothing to do - return\n-            return;\n-        }\n+        result_of_fetch_from_storage = cache_storage_ptr->fetchColumnsForKeys(keys, request);\n     }\n \n-    /// From this point we have to update all keys sync.\n-    /// Maybe allow_read_expired_keys_from_cache_dictionary is disabled\n-    /// and there no cache_not_found_ids but some cache_expired.\n-\n-    std::vector<Key> required_ids;\n-    required_ids.reserve(cache_not_found_count + cache_expired_cound);\n-    std::transform(std::begin(cache_expired_or_not_found_ids), std::end(cache_expired_or_not_found_ids),\n-                   std::back_inserter(required_ids), [](auto & pair) { return pair.first; });\n+    size_t found_keys_size = result_of_fetch_from_storage.found_keys_size;\n+    size_t expired_keys_size = result_of_fetch_from_storage.expired_keys_size;\n+    size_t not_found_keys_size = result_of_fetch_from_storage.not_found_keys_size;\n \n-    /// Request new values\n-    auto update_unit_ptr = std::make_shared<UpdateUnit>(std::move(required_ids));\n+    ProfileEvents::increment(ProfileEvents::DictCacheKeysHit, found_keys_size);\n+    ProfileEvents::increment(ProfileEvents::DictCacheKeysExpired, expired_keys_size);\n+    ProfileEvents::increment(ProfileEvents::DictCacheKeysNotFound, not_found_keys_size);\n \n-    tryPushToUpdateQueueOrThrow(update_unit_ptr);\n-    waitForCurrentUpdateFinish(update_unit_ptr);\n+    query_count.fetch_add(keys.size());\n+    hit_count.fetch_add(found_keys_size);\n \n-    /// Add updated keys to answer.\n+    MutableColumns & fetched_columns_from_storage = result_of_fetch_from_storage.fetched_columns;\n+    const PaddedPODArray<KeyState> & key_index_to_state_from_storage = result_of_fetch_from_storage.key_index_to_state;\n \n-    const size_t attribute_index = getAttributeIndex(attribute.name);\n+    bool source_returns_fetched_columns_in_order_of_keys = cache_storage_ptr->returnsFetchedColumnsInOrderOfRequestedKeys();\n \n-    for (auto & [key, value] : update_unit_ptr->found_ids)\n+    if (not_found_keys_size == 0 && expired_keys_size == 0)\n     {\n-        if (value.found)\n+        /// All keys were found in storage\n+\n+        if (source_returns_fetched_columns_in_order_of_keys)\n+            return request.filterRequestedColumns(fetched_columns_from_storage);\n+        else\n         {\n-            for (const size_t row : cache_expired_or_not_found_ids[key])\n-                out[row] = std::get<OutputType>(value.values[attribute_index]);\n+            /// Reorder result from storage to requested keys indexes\n+            MutableColumns aggregated_columns = aggregateColumnsInOrderOfKeys(\n+                keys,\n+                request,\n+                fetched_columns_from_storage,\n+                key_index_to_state_from_storage);\n+\n+            return request.filterRequestedColumns(aggregated_columns);\n         }\n     }\n-}\n-\n-void CacheDictionary::getItemsString(\n-    Attribute & attribute,\n-    const PaddedPODArray<Key> & ids,\n-    ColumnString * out,\n-    DictionaryDefaultValueExtractor<String> & default_value_extractor) const\n-{\n-    const auto rows = ext::size(ids);\n-\n-    /// Save on some allocations.\n-    out->getOffsets().reserve(rows);\n \n-    auto & attribute_array = std::get<ContainerPtrType<StringRef>>(attribute.arrays);\n+    size_t keys_to_update_size = not_found_keys_size + expired_keys_size;\n+    auto update_unit = std::make_shared<CacheDictionaryUpdateUnit<dictionary_key_type>>(key_columns, key_index_to_state_from_storage, request, keys_to_update_size);\n \n-    auto found_outdated_values = false;\n+    HashMap<KeyType, size_t> requested_keys_to_fetched_columns_during_update_index;\n+    MutableColumns fetched_columns_during_update = request.makeAttributesResultColumns();\n \n-    /// Perform optimistic version, fallback to pessimistic if failed.\n+    if (not_found_keys_size == 0 && expired_keys_size > 0 && allow_read_expired_keys)\n     {\n-        const ProfilingScopedReadRWLock read_lock{rw_lock, ProfileEvents::DictCacheLockReadNs};\n-\n-        const auto now = std::chrono::system_clock::now();\n+        /// Start async update only if allow read expired keys and all keys are found\n+        update_queue.tryPushToUpdateQueueOrThrow(update_unit);\n \n-        /// Fetch up-to-date values, discard on fail.\n-        for (const auto row : ext::range(0, rows))\n+        if (source_returns_fetched_columns_in_order_of_keys)\n+            return request.filterRequestedColumns(fetched_columns_from_storage);\n+        else\n         {\n-            const auto id = ids[row];\n-            const auto [cell_idx, state] = findCellIdxForGet(id, now);\n-\n-            if (state == ResultState::FoundAndValid)\n-            {\n-                auto & cell = cells[cell_idx];\n-                const auto string_ref = cell.isDefault() ? default_value_extractor[row] : attribute_array[cell_idx];\n-                out->insertData(string_ref.data, string_ref.size);\n-            }\n-            else\n-            {\n-                found_outdated_values = true;\n-                break;\n-            }\n+            /// Reorder result from storage to requested keys indexes\n+            MutableColumns aggregated_columns = aggregateColumnsInOrderOfKeys(\n+                keys,\n+                request,\n+                fetched_columns_from_storage,\n+                key_index_to_state_from_storage);\n+\n+            return request.filterRequestedColumns(aggregated_columns);\n         }\n     }\n-\n-    /// Optimistic code completed successfully.\n-    if (!found_outdated_values)\n-    {\n-        query_count.fetch_add(rows, std::memory_order_relaxed);\n-        hit_count.fetch_add(rows, std::memory_order_release);\n-        ProfileEvents::increment(ProfileEvents::DictCacheKeysHit, ids.size());\n-        return;\n-    }\n-\n-    /// Now onto the pessimistic one, discard possible partial results from the optimistic path.\n-    out->getChars().resize_assume_reserved(0);\n-    out->getOffsets().resize_assume_reserved(0);\n-\n-    /// Mapping: <id> -> { all indices `i` of `ids` such that `ids[i]` = <id> }\n-    std::unordered_map<Key, std::vector<size_t>> cache_expired_or_not_found_ids;\n-    /// we are going to store every string separately\n-    std::unordered_map<Key, String> local_cache;\n-\n-    size_t cache_not_found_count = 0;\n-    size_t cache_expired_count = 0;\n-\n-    size_t total_length = 0;\n-    size_t cache_hit = 0;\n+    else\n     {\n-        const ProfilingScopedReadRWLock read_lock{rw_lock, ProfileEvents::DictCacheLockReadNs};\n-\n-        const auto now = std::chrono::system_clock::now();\n+        /// Start sync update\n+        update_queue.tryPushToUpdateQueueOrThrow(update_unit);\n+        update_queue.waitForCurrentUpdateFinish(update_unit);\n \n-        auto insert_value_routine = [&](size_t row, size_t id, size_t cell_idx)\n-        {\n-            const auto & cell = cells[cell_idx];\n-            const auto string_ref = cell.isDefault() ? default_value_extractor[row] : attribute_array[cell_idx];\n-\n-            /// Do not store default, but count it in total length.\n-            if (!cell.isDefault())\n-                local_cache[id] = String{string_ref};\n-\n-            total_length += string_ref.size + 1;\n-        };\n-\n-        for (const auto row : ext::range(0, ids.size()))\n-        {\n-            const auto id = ids[row];\n-            const auto [cell_idx, state] = findCellIdxForGet(id, now);\n-\n-            if (state == ResultState::FoundAndValid)\n-            {\n-                ++cache_hit;\n-                insert_value_routine(row, id, cell_idx);\n-            }\n-            else if (state == ResultState::NotFound || state == ResultState::FoundButExpiredPermanently)\n-            {\n-                ++cache_not_found_count;\n-                cache_expired_or_not_found_ids[id].push_back(row);\n-            }\n-            else if (state == ResultState::FoundButExpired)\n-            {\n-                ++cache_expired_count;\n-                cache_expired_or_not_found_ids[id].push_back(row);\n-\n-                if (allow_read_expired_keys)\n-                    insert_value_routine(row, id, cell_idx);\n-            }\n-        }\n+        requested_keys_to_fetched_columns_during_update_index = std::move(update_unit->requested_keys_to_fetched_columns_during_update_index);\n+        fetched_columns_during_update = std::move(update_unit->fetched_columns_during_update);\n     }\n \n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysExpired, cache_expired_count);\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysNotFound, cache_not_found_count);\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysHit, cache_hit);\n+    MutableColumns aggregated_columns = aggregateColumns(\n+        keys,\n+        request,\n+        fetched_columns_from_storage,\n+        key_index_to_state_from_storage,\n+        fetched_columns_during_update,\n+        requested_keys_to_fetched_columns_during_update_index);\n \n-    query_count.fetch_add(rows, std::memory_order_relaxed);\n-    hit_count.fetch_add(rows - cache_expired_count - cache_not_found_count, std::memory_order_release);\n-\n-    /// Async update of expired keys.\n-    if (!cache_not_found_count)\n-    {\n-        if (allow_read_expired_keys && cache_expired_count)\n-        {\n-            std::vector<Key> required_expired_ids;\n-            required_expired_ids.reserve(cache_expired_count);\n-            std::transform(std::begin(cache_expired_or_not_found_ids), std::end(cache_expired_or_not_found_ids),\n-                           std::back_inserter(required_expired_ids), [](auto & pair) { return pair.first; });\n+    return request.filterRequestedColumns(aggregated_columns);\n+}\n \n-            auto update_unit_ptr = std::make_shared<UpdateUnit>(std::move(required_expired_ids));\n+template <DictionaryKeyType dictionary_key_type>\n+ColumnUInt8::Ptr CacheDictionary<dictionary_key_type>::hasKeys(const Columns & key_columns, const DataTypes & key_types) const\n+{\n+    /**\n+    * Flow of hasKeys. It is similar to getColumns. But there is an important detail, if key is identified with default value in storage\n+    * it means that in hasKeys result this key will be false.\n+    *\n+    * 1. Get fetch result from storage\n+    * 2. If all keys are found in storage and not expired and there are no default keys return that we have all keys.\n+    * Otherwise set allow_expired_keys_during_aggregation and go to step 5.\n+    * 3. If all keys are found in storage and some of them are expired and allow_read_expired keys is true return that we have all keys.\n+    * Otherwise set allow_expired_keys_during_aggregation and go to step 5.\n+    * 4. If not all keys are found in storage start sync update from source.\n+    * 5. Start aggregation of keys from source and storage.\n+    * If we allow read expired keys from step 2 or 3 then count them as founded in storage.\n+    * Check if key was found in storage not default for that key set true in result array.\n+    * Check that key was fetched during update for that key set true in result array.\n+    */\n \n-            tryPushToUpdateQueueOrThrow(update_unit_ptr);\n+    if (dictionary_key_type == DictionaryKeyType::complex)\n+        dict_struct.validateKeyTypes(key_types);\n \n-            /// Insert all found keys and defaults to output array.\n-            out->getChars().reserve(total_length);\n+    Arena complex_keys_arena;\n+    DictionaryKeysExtractor<dictionary_key_type> extractor(key_columns, complex_keys_arena);\n+    const auto & keys = extractor.getKeys();\n \n-            for (const auto row : ext::range(0, ext::size(ids)))\n-            {\n-                const auto id = ids[row];\n-                StringRef value;\n+    /// We make empty request just to fetch if keys exists\n+    DictionaryStorageFetchRequest request(dict_struct, {}, {});\n \n-                /// Previously we stored found keys in map.\n-                const auto it = local_cache.find(id);\n-                if (it != local_cache.end())\n-                    value = StringRef(it->second);\n-                else\n-                    value = default_value_extractor[row];\n+    FetchResult result_of_fetch_from_storage;\n \n-                out->insertData(value.data, value.size);\n-            }\n+    {\n+        /// Write lock on storage\n+        const ProfilingScopedWriteRWLock write_lock{rw_lock, ProfileEvents::DictCacheLockWriteNs};\n \n-            /// Nothing to do else.\n-            return;\n-        }\n+        result_of_fetch_from_storage = cache_storage_ptr->fetchColumnsForKeys(keys, request);\n     }\n \n-    /// We will request both cache_not_found_ids and cache_expired_ids sync.\n-    std::vector<Key> required_ids;\n-    required_ids.reserve(cache_not_found_count + cache_expired_count);\n-    std::transform(\n-        std::begin(cache_expired_or_not_found_ids), std::end(cache_expired_or_not_found_ids),\n-        std::back_inserter(required_ids), [](auto & pair) { return pair.first; });\n+    size_t found_keys_size = result_of_fetch_from_storage.found_keys_size;\n+    size_t expired_keys_size = result_of_fetch_from_storage.expired_keys_size;\n+    size_t not_found_keys_size = result_of_fetch_from_storage.not_found_keys_size;\n \n-    auto update_unit_ptr = std::make_shared<UpdateUnit>(std::move(required_ids));\n+    ProfileEvents::increment(ProfileEvents::DictCacheKeysHit, found_keys_size);\n+    ProfileEvents::increment(ProfileEvents::DictCacheKeysExpired, expired_keys_size);\n+    ProfileEvents::increment(ProfileEvents::DictCacheKeysNotFound, not_found_keys_size);\n \n-    tryPushToUpdateQueueOrThrow(update_unit_ptr);\n-    waitForCurrentUpdateFinish(update_unit_ptr);\n+    query_count.fetch_add(keys.size());\n+    hit_count.fetch_add(found_keys_size);\n \n-    const size_t attribute_index = getAttributeIndex(attribute.name);\n+    size_t keys_to_update_size = expired_keys_size + not_found_keys_size;\n+    auto update_unit = std::make_shared<CacheDictionaryUpdateUnit<dictionary_key_type>>(key_columns, result_of_fetch_from_storage.key_index_to_state, request, keys_to_update_size);\n \n-    /// Only calculate the total length.\n-    for (auto & [key, value] : update_unit_ptr->found_ids)\n-    {\n-        if (value.found)\n-        {\n-            const auto found_value_ref = std::get<String>(value.values[attribute_index]);\n-            total_length += (found_value_ref.size() + 1) * cache_expired_or_not_found_ids[key].size();\n-        }\n-        else\n-        {\n-            for (const auto row : cache_expired_or_not_found_ids[key])\n-                total_length += default_value_extractor[row].size + 1;\n-        }\n-    }\n-\n-    out->getChars().reserve(total_length);\n+    HashMap<KeyType, size_t> requested_keys_to_fetched_columns_during_update_index;\n+    bool allow_expired_keys_during_aggregation = false;\n \n-    for (const auto row : ext::range(0, ext::size(ids)))\n+    if (not_found_keys_size == 0 && expired_keys_size == 0)\n     {\n-        const auto id = ids[row];\n-        StringRef value;\n-\n-        /// We have two maps: found in cache and found in source.\n-        const auto local_it = local_cache.find(id);\n-        if (local_it != local_cache.end())\n-            value = StringRef(local_it->second);\n-        else\n-        {\n-            const auto found_it = update_unit_ptr->found_ids.find(id);\n-\n-            /// Previously we didn't store defaults in local cache.\n-            if (found_it != update_unit_ptr->found_ids.end() && found_it->second.found)\n-                value = std::get<String>(found_it->second.values[attribute_index]);\n-            else\n-                value = default_value_extractor[row];\n-        }\n+        /// All keys were found in storage\n \n-        out->insertData(value.data, value.size);\n-    }\n-}\n+        if (result_of_fetch_from_storage.default_keys_size == 0)\n+            return ColumnUInt8::create(keys.size(), true);\n \n-\n-template<class... Ts>\n-struct Overloaded : Ts... {using Ts::operator()...;};\n-\n-template<class... Ts>\n-Overloaded(Ts...) -> Overloaded<Ts...>;\n-\n-std::string CacheDictionary::AttributeValuesForKey::dump()\n-{\n-    WriteBufferFromOwnString os;\n-    for (auto & attr : values)\n-        std::visit(Overloaded {\n-            [&os](UInt8 arg)   { os << \"type: UInt8, value: \"   <<  std::to_string(arg) << \"\\n\"; },\n-            [&os](UInt16 arg)  { os << \"type: UInt16, value: \"  <<  std::to_string(arg) << \"\\n\"; },\n-            [&os](UInt32 arg)  { os << \"type: UInt32, value: \"  <<  std::to_string(arg) << \"\\n\"; },\n-            [&os](UInt64 arg)  { os << \"type: UInt64, value: \"  <<  std::to_string(arg) << \"\\n\"; },\n-            [&os](UInt128 arg) { os << \"type: UInt128, value: \" << arg.toHexString() << \"\\n\"; },\n-            [&os](Int8 arg)   { os << \"type: Int8, value: \"   <<  std::to_string(arg) << \"\\n\"; },\n-            [&os](Int16 arg)  { os << \"type: Int16, value: \"  <<  std::to_string(arg) << \"\\n\"; },\n-            [&os](Int32 arg)  { os << \"type: Int32, value: \"  <<  std::to_string(arg) << \"\\n\"; },\n-            [&os](Int64 arg)  { os << \"type: Int64, value: \"  <<  std::to_string(arg) << \"\\n\"; },\n-            [&os](Decimal32 arg)   { os << \"type: Decimal32, value: \"  <<  std::to_string(arg) << \"\\n\"; },\n-            [&os](Decimal64 arg)   { os << \"type: Decimal64, value: \"  <<  std::to_string(arg) << \"\\n\"; },\n-            [&os](Decimal128)  { os << \"type: Decimal128, value: ???\" << \"\\n\" ; },\n-            [&os](Float32 arg)   { os << \"type: Float32, value: \"  <<  std::to_string(arg) << \"\\n\"; },\n-            [&os](Float64 arg)   { os << \"type: Float64, value: \"  <<  std::to_string(arg) << \"\\n\"; },\n-            [&os](String arg)  { os << \"type: String, value: \" <<  arg + \"\\n\"; }\n-        }, attr);\n-    return os.str();\n-};\n-\n-\n-std::string CacheDictionary::UpdateUnit::dumpFoundIds()\n-{\n-    WriteBufferFromOwnString os;\n-    for (auto it : found_ids)\n-    {\n-        os << \"Key: \" << std::to_string(it.first) << \"\\n\";\n-        if (it.second.found)\n-            os << it.second.dump() << \"\\n\";\n+        allow_expired_keys_during_aggregation = true;\n     }\n-    return os.str();\n-};\n-\n-/// Returns cell_idx in handmade open addressing cache table and the state of the cell stored the key.\n-CacheDictionary::FindResult CacheDictionary::findCellIdxForGet(const Key & id, const time_point_t now) const\n-{\n-    auto pos = getCellIdx(id);\n-    const auto stop = pos + max_collision_length;\n-    for (; pos < stop; ++pos)\n+    else if (not_found_keys_size == 0 && expired_keys_size > 0 && allow_read_expired_keys)\n     {\n-        const auto cell_idx = pos & size_overlap_mask;\n-        const auto & cell = cells[cell_idx];\n-\n-        if (cell.id != id)\n-            continue;\n-\n-        if (isExpiredPermanently(now, cell.expiresAt()))\n-            return {cell_idx, ResultState::FoundButExpiredPermanently};\n+        /// Start async update only if allow read expired keys and all keys are found\n+        update_queue.tryPushToUpdateQueueOrThrow(update_unit);\n \n-        if (isExpired(now, cell.expiresAt()))\n-            return {cell_idx, ResultState::FoundButExpired};\n+        if (result_of_fetch_from_storage.default_keys_size == 0)\n+            return ColumnUInt8::create(keys.size(), true);\n \n-        return {cell_idx, ResultState::FoundAndValid};\n+        allow_expired_keys_during_aggregation = true;\n     }\n-\n-    return {pos & size_overlap_mask, ResultState::NotFound};\n-}\n-\n-/// Returns cell_idx such that cells[cell_idx].id = id or the oldest cell in bounds of max_coolision_length.\n-size_t CacheDictionary::findCellIdxForSet(const Key & id) const\n-{\n-    auto pos = getCellIdx(id);\n-    auto oldest_id = pos;\n-    auto oldest_time = time_point_t::max();\n-    const auto stop = pos + max_collision_length;\n-    for (; pos < stop; ++pos)\n+    else\n     {\n-        const auto cell_idx = pos & size_overlap_mask;\n-        const auto & cell = cells[cell_idx];\n+        /// Start sync update\n+        update_queue.tryPushToUpdateQueueOrThrow(update_unit);\n+        update_queue.waitForCurrentUpdateFinish(update_unit);\n \n-        if (cell.id != id)\n-        {\n-            /// maybe we already found nearest expired cell (try minimize collision_length on insert)\n-            if (cell.expiresAt() < oldest_time)\n-            {\n-                oldest_time = cell.expiresAt();\n-                oldest_id = cell_idx;\n-            }\n-            continue;\n-        }\n-\n-        /// We found the exact place for id.\n-        return cell_idx;\n+        requested_keys_to_fetched_columns_during_update_index = std::move(update_unit->requested_keys_to_fetched_columns_during_update_index);\n     }\n \n-    return oldest_id;\n-}\n-\n-ColumnUInt8::Ptr CacheDictionary::hasKeys(const Columns & key_columns, const DataTypes &) const\n-{\n-    PaddedPODArray<Key> backup_storage;\n-    const auto& ids = getColumnVectorData(this, key_columns.front(), backup_storage);\n-\n-    auto result = ColumnUInt8::create(ext::size(ids));\n-    auto& out = result->getData();\n-\n-    /// There are three types of ids.\n-    /// - Valid ids. These ids are presented in local cache and their lifetime is not expired.\n-    /// - CacheExpired ids. Ids that are in local cache, but their values are rotted (lifetime is expired).\n-    /// - CacheNotFound ids. We have to go to external storage to know its value.\n-\n-    /// Mark everything as absent.\n-    const auto rows = ext::size(ids);\n-    for (const auto row : ext::range(0, rows))\n-        out[row] = false;\n-\n-    /// Mapping: <id> -> { all indices `i` of `ids` such that `ids[i]` = <id> }\n-    std::unordered_map<Key, std::vector<size_t>> cache_expired_or_not_found_ids;\n-\n-    size_t cache_hit = 0;\n-\n-    size_t cache_expired_count = 0;\n-    size_t cache_not_found_count = 0;\n+    auto result = ColumnUInt8::create(keys.size(), false);\n+    auto & data = result->getData();\n \n+    for (size_t key_index = 0; key_index < keys.size(); ++key_index)\n     {\n-        const ProfilingScopedReadRWLock read_lock{rw_lock, ProfileEvents::DictCacheLockReadNs};\n+        auto key = keys[key_index];\n \n-        const auto now = std::chrono::system_clock::now();\n-        /// fetch up-to-date values, decide which ones require update\n-        for (const auto row : ext::range(0, rows))\n-        {\n-            const auto id = ids[row];\n-            const auto [cell_idx, state] = findCellIdxForGet(id, now);\n-            auto & cell = cells[cell_idx];\n-\n-            auto insert_to_answer_routine = [&] ()\n-            {\n-                out[row] = !cell.isDefault();\n-            };\n+        bool valid_expired_key = allow_expired_keys_during_aggregation && result_of_fetch_from_storage.key_index_to_state[key_index].isExpired();\n \n-            if (state == ResultState::FoundAndValid)\n-            {\n-                ++cache_hit;\n-                insert_to_answer_routine();\n-            }\n-            else if (state == ResultState::NotFound || state == ResultState::FoundButExpiredPermanently)\n-            {\n-                /// Permanently expired equals to not found semantically.\n-                ++cache_not_found_count;\n-                cache_expired_or_not_found_ids[id].push_back(row);\n-            }\n-            else if (state == ResultState::FoundButExpired)\n-            {\n-                cache_expired_count++;\n-                cache_expired_or_not_found_ids[id].push_back(row);\n-\n-                if (allow_read_expired_keys)\n-                    insert_to_answer_routine();\n-            }\n+        if (result_of_fetch_from_storage.key_index_to_state[key_index].isFound() || valid_expired_key)\n+        {\n+            /// Check if key was fetched from cache\n+            data[key_index] = !result_of_fetch_from_storage.key_index_to_state[key_index].isDefault();\n         }\n-    }\n-\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysExpired, cache_expired_count);\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysNotFound, cache_not_found_count);\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysHit, cache_hit);\n-\n-    query_count.fetch_add(rows, std::memory_order_relaxed);\n-    hit_count.fetch_add(rows - cache_expired_count - cache_not_found_count, std::memory_order_release);\n \n-    if (!cache_not_found_count)\n-    {\n-        /// Nothing to update - return;\n-        if (!cache_expired_count)\n-            return result;\n-\n-        if (allow_read_expired_keys)\n+        if (requested_keys_to_fetched_columns_during_update_index.has(key))\n         {\n-            std::vector<Key> required_expired_ids;\n-            required_expired_ids.reserve(cache_expired_count);\n-            std::transform(\n-                    std::begin(cache_expired_or_not_found_ids), std::end(cache_expired_or_not_found_ids),\n-                    std::back_inserter(required_expired_ids), [](auto & pair) { return pair.first; });\n-\n-            auto update_unit_ptr = std::make_shared<UpdateUnit>(std::move(required_expired_ids));\n-\n-            tryPushToUpdateQueueOrThrow(update_unit_ptr);\n-            /// Update is async - no need to wait.\n-            return result;\n+            /// Check if key was not in cache and was fetched during update\n+            data[key_index] = true;\n         }\n     }\n \n-    /// At this point we have two situations.\n-    /// There may be both types of keys: expired and not_found.\n-    /// We will update them all synchronously.\n-\n-    std::vector<Key> required_ids;\n-    required_ids.reserve(cache_not_found_count + cache_expired_count);\n-    std::transform(\n-            std::begin(cache_expired_or_not_found_ids), std::end(cache_expired_or_not_found_ids),\n-            std::back_inserter(required_ids), [](auto & pair) { return pair.first; });\n-\n-    auto update_unit_ptr = std::make_shared<UpdateUnit>(std::move(required_ids));\n-\n-    tryPushToUpdateQueueOrThrow(update_unit_ptr);\n-    waitForCurrentUpdateFinish(update_unit_ptr);\n-\n-    for (auto & [key, value] : update_unit_ptr->found_ids)\n-    {\n-        if (value.found)\n-            for (const auto row : cache_expired_or_not_found_ids[key])\n-                out[row] = true;\n-    }\n-\n     return result;\n }\n \n-\n-void CacheDictionary::createAttributes()\n+template <DictionaryKeyType dictionary_key_type>\n+MutableColumns CacheDictionary<dictionary_key_type>::aggregateColumnsInOrderOfKeys(\n+    const PaddedPODArray<KeyType> & keys,\n+    const DictionaryStorageFetchRequest & request,\n+    const MutableColumns & fetched_columns,\n+    const PaddedPODArray<KeyState> & key_index_to_state)\n {\n-    const auto attributes_size = dict_struct.attributes.size();\n-    attributes.reserve(attributes_size);\n+    MutableColumns aggregated_columns = request.makeAttributesResultColumns();\n \n-    bytes_allocated += size * sizeof(CellMetadata);\n-    bytes_allocated += attributes_size * sizeof(attributes.front());\n+    /// If keys were returned not in order of keys, aggregate fetched columns in order of requested keys.\n \n-    for (const auto & attribute : dict_struct.attributes)\n+    for (size_t fetch_request_index = 0; fetch_request_index < request.attributesSize(); ++fetch_request_index)\n     {\n-        attribute_index_by_name.emplace(attribute.name, attributes.size());\n-        attributes.push_back(createAttributeWithTypeAndName(attribute.underlying_type, attribute.name, attribute.null_value));\n+        if (!request.shouldFillResultColumnWithIndex(fetch_request_index))\n+            continue;\n \n-        if (attribute.hierarchical)\n-        {\n-            hierarchical_attribute = &attributes.back();\n+        const auto & aggregated_column = aggregated_columns[fetch_request_index];\n+        const auto & fetched_column = fetched_columns[fetch_request_index];\n \n-            if (hierarchical_attribute->type != AttributeUnderlyingType::utUInt64)\n-                throw Exception{full_name + \": hierarchical attribute must be UInt64.\", ErrorCodes::TYPE_MISMATCH};\n-        }\n-    }\n-}\n+        for (size_t key_index = 0; key_index < keys.size(); ++key_index)\n+        {\n+            auto state = key_index_to_state[key_index];\n \n-/* For unknown reason clang-tidy wants this function to be static, but it uses bytes_allocated, which is a class member.\n- * NOLINT(readability-convert-member-functions-to-static) */\n-CacheDictionary::Attribute CacheDictionary::createAttributeWithTypeAndName(const AttributeUnderlyingType type, const String & name, const Field & null_value)\n-{\n-    Attribute attr{type, name, {}, {}};\n+            if (state.isNotFound())\n+                continue;\n \n-    switch (type)\n-    {\n-        /* Macro argument should be enclosed in parentheses, but if do so we cannot initialize \\\n-         * NearestFieldType which takes TYPE as a template parameter. */\n-#define DISPATCH(TYPE)\\\n-        case AttributeUnderlyingType::ut##TYPE:\\\n-        {\\\n-            attr.null_value = TYPE(null_value.get<NearestFieldType<TYPE>>()); /* NOLINT(bugprone-macro-parentheses) */ \\\n-            attr.arrays = std::make_unique<ContainerType<TYPE>>(size); /* NOLINT(bugprone-macro-parentheses) */ \\\n-            bytes_allocated += size * sizeof(TYPE);\\\n-            break;\\\n-        }\n-        DISPATCH(UInt8)\n-        DISPATCH(UInt16)\n-        DISPATCH(UInt32)\n-        DISPATCH(UInt64)\n-        DISPATCH(UInt128)\n-        DISPATCH(Int8)\n-        DISPATCH(Int16)\n-        DISPATCH(Int32)\n-        DISPATCH(Int64)\n-        DISPATCH(Decimal32)\n-        DISPATCH(Decimal64)\n-        DISPATCH(Decimal128)\n-        DISPATCH(Float32)\n-        DISPATCH(Float64)\n-#undef DISPATCH\n-        case AttributeUnderlyingType::utString: {\n-            attr.null_value = null_value.get<String>();\n-            attr.arrays = std::make_unique<ContainerType<StringRef>>(size);\n-            bytes_allocated += size * sizeof(StringRef);\n-            if (!string_arena)\n-                string_arena = std::make_unique<ArenaWithFreeLists>();\n-            break;\n+            aggregated_column->insertFrom(*fetched_column, state.getFetchedColumnIndex());\n         }\n     }\n \n-    return attr;\n+    return aggregated_columns;\n }\n \n-void CacheDictionary::setDefaultAttributeValue(Attribute & attribute, const Key idx) const\n+template <DictionaryKeyType dictionary_key_type>\n+MutableColumns CacheDictionary<dictionary_key_type>::aggregateColumns(\n+        const PaddedPODArray<KeyType> & keys,\n+        const DictionaryStorageFetchRequest & request,\n+        const MutableColumns & fetched_columns_from_storage,\n+        const PaddedPODArray<KeyState> & key_index_to_fetched_columns_from_storage_result,\n+        const MutableColumns & fetched_columns_during_update,\n+        const HashMap<KeyType, size_t> & found_keys_to_fetched_columns_during_update_index)\n {\n-    switch (attribute.type)\n-    {\n-        /* Macro argument should be enclosed in parentheses, but if do so we cannot initialize \\\n-        * NearestFieldType which takes TYPE as a template parameter.  */\n-#define DISPATCH(TYPE)\\\n-        case AttributeUnderlyingType::ut##TYPE:\\\n-            std::get<ContainerPtrType<TYPE>>(attribute.arrays)[idx] = std::get<TYPE>(attribute.null_value); /* NOLINT(bugprone-macro-parentheses) */ \\\n-            break;\n-        DISPATCH(UInt8)\n-        DISPATCH(UInt16)\n-        DISPATCH(UInt32)\n-        DISPATCH(UInt64)\n-        DISPATCH(UInt128)\n-        DISPATCH(Int8)\n-        DISPATCH(Int16)\n-        DISPATCH(Int32)\n-        DISPATCH(Int64)\n-        DISPATCH(Decimal32)\n-        DISPATCH(Decimal64)\n-        DISPATCH(Decimal128)\n-        DISPATCH(Float32)\n-        DISPATCH(Float64)\n-#undef DISPATCH\n-        case AttributeUnderlyingType::utString:\n-        {\n-            const auto & null_value_ref = std::get<String>(attribute.null_value);\n-            auto & string_ref = std::get<ContainerPtrType<StringRef>>(attribute.arrays)[idx];\n-\n-            if (string_ref.data != null_value_ref.data())\n-            {\n-                if (string_ref.data)\n-                    string_arena->free(const_cast<char *>(string_ref.data), string_ref.size);\n+    /**\n+    * Aggregation of columns fetched from storage and from source during update.\n+    * If key was found in storage add it to result.\n+    * If key was found in source during update add it to result.\n+    * If key was not found in storage or in source during update add default value.\n+    */\n \n-                string_ref = StringRef{null_value_ref};\n-            }\n+    MutableColumns aggregated_columns = request.makeAttributesResultColumns();\n \n-            break;\n-        }\n-    }\n-}\n-\n-void CacheDictionary::setAttributeValue(Attribute & attribute, const Key idx, const Field & value) const\n-{\n-    switch (attribute.type)\n+    for (size_t fetch_request_index = 0; fetch_request_index < request.attributesSize(); ++fetch_request_index)\n     {\n-        case AttributeUnderlyingType::utUInt8:\n-            std::get<ContainerPtrType<UInt8>>(attribute.arrays)[idx] = value.get<UInt64>();\n-            break;\n-        case AttributeUnderlyingType::utUInt16:\n-            std::get<ContainerPtrType<UInt16>>(attribute.arrays)[idx] = value.get<UInt64>();\n-            break;\n-        case AttributeUnderlyingType::utUInt32:\n-            std::get<ContainerPtrType<UInt32>>(attribute.arrays)[idx] = value.get<UInt64>();\n-            break;\n-        case AttributeUnderlyingType::utUInt64:\n-            std::get<ContainerPtrType<UInt64>>(attribute.arrays)[idx] = value.get<UInt64>();\n-            break;\n-        case AttributeUnderlyingType::utUInt128:\n-            std::get<ContainerPtrType<UInt128>>(attribute.arrays)[idx] = value.get<UInt128>();\n-            break;\n-        case AttributeUnderlyingType::utInt8:\n-            std::get<ContainerPtrType<Int8>>(attribute.arrays)[idx] = value.get<Int64>();\n-            break;\n-        case AttributeUnderlyingType::utInt16:\n-            std::get<ContainerPtrType<Int16>>(attribute.arrays)[idx] = value.get<Int64>();\n-            break;\n-        case AttributeUnderlyingType::utInt32:\n-            std::get<ContainerPtrType<Int32>>(attribute.arrays)[idx] = value.get<Int64>();\n-            break;\n-        case AttributeUnderlyingType::utInt64:\n-            std::get<ContainerPtrType<Int64>>(attribute.arrays)[idx] = value.get<Int64>();\n-            break;\n-        case AttributeUnderlyingType::utFloat32:\n-            std::get<ContainerPtrType<Float32>>(attribute.arrays)[idx] = value.get<Float64>();\n-            break;\n-        case AttributeUnderlyingType::utFloat64:\n-            std::get<ContainerPtrType<Float64>>(attribute.arrays)[idx] = value.get<Float64>();\n-            break;\n-        case AttributeUnderlyingType::utDecimal32:\n-            std::get<ContainerPtrType<Decimal32>>(attribute.arrays)[idx] = value.get<Decimal32>();\n-            break;\n-        case AttributeUnderlyingType::utDecimal64:\n-            std::get<ContainerPtrType<Decimal64>>(attribute.arrays)[idx] = value.get<Decimal64>();\n-            break;\n-        case AttributeUnderlyingType::utDecimal128:\n-            std::get<ContainerPtrType<Decimal128>>(attribute.arrays)[idx] = value.get<Decimal128>();\n-            break;\n+        if (!request.shouldFillResultColumnWithIndex(fetch_request_index))\n+            continue;\n+\n+        const auto & aggregated_column = aggregated_columns[fetch_request_index];\n+        const auto & fetched_column_from_storage = fetched_columns_from_storage[fetch_request_index];\n+        const auto & fetched_column_during_update = fetched_columns_during_update[fetch_request_index];\n+        const auto & default_value_provider = request.defaultValueProviderAtIndex(fetch_request_index);\n \n-        case AttributeUnderlyingType::utString:\n+        for (size_t key_index = 0; key_index < keys.size(); ++key_index)\n         {\n-            const auto & string = value.get<String>();\n-            auto & string_ref = std::get<ContainerPtrType<StringRef>>(attribute.arrays)[idx];\n-            const auto & null_value_ref = std::get<String>(attribute.null_value);\n+            auto key = keys[key_index];\n \n-            /// free memory unless it points to a null_value\n-            if (string_ref.data && string_ref.data != null_value_ref.data())\n-                string_arena->free(const_cast<char *>(string_ref.data), string_ref.size);\n+            auto key_state_from_storage = key_index_to_fetched_columns_from_storage_result[key_index];\n+            if (key_state_from_storage.isFound())\n+            {\n+                /// Check and insert value if key was fetched from cache\n+                aggregated_column->insertFrom(*fetched_column_from_storage, key_state_from_storage.getFetchedColumnIndex());\n+                continue;\n+            }\n \n-            const auto str_size = string.size();\n-            if (str_size != 0)\n+            /// Check and insert value if key was not in cache and was fetched during update\n+            const auto * find_iterator_in_fetch_during_update = found_keys_to_fetched_columns_during_update_index.find(key);\n+            if (find_iterator_in_fetch_during_update)\n             {\n-                auto * string_ptr = string_arena->alloc(str_size + 1);\n-                std::copy(string.data(), string.data() + str_size + 1, string_ptr);\n-                string_ref = StringRef{string_ptr, str_size};\n+                aggregated_column->insertFrom(*fetched_column_during_update, find_iterator_in_fetch_during_update->getMapped());\n+                continue;\n             }\n-            else\n-                string_ref = {};\n \n-            break;\n+            /// Insert default value\n+            aggregated_column->insert(default_value_provider.getDefaultValue(key_index));\n         }\n     }\n-}\n-\n-CacheDictionary::Attribute & CacheDictionary::getAttribute(const std::string & attribute_name) const\n-{\n-    const size_t attr_index = getAttributeIndex(attribute_name);\n-    return attributes[attr_index];\n-}\n-\n-size_t CacheDictionary::getAttributeIndex(const std::string & attribute_name) const\n-{\n-    const auto it = attribute_index_by_name.find(attribute_name);\n-    if (it == std::end(attribute_index_by_name))\n-        throw Exception{full_name + \": no such attribute '\" + attribute_name + \"'\", ErrorCodes::BAD_ARGUMENTS};\n-\n-    return it->second;\n-}\n-\n-bool CacheDictionary::isEmptyCell(const UInt64 idx) const\n-{\n-    return (idx != zero_cell_idx && cells[idx].id == 0) || (cells[idx].deadline == time_point_t());\n-}\n \n-\n-PaddedPODArray<CacheDictionary::Key> CacheDictionary::getCachedIds() const\n-{\n-    const ProfilingScopedReadRWLock read_lock{rw_lock, ProfileEvents::DictCacheLockReadNs};\n-\n-    PaddedPODArray<Key> array;\n-    for (size_t idx = 0; idx < cells.size(); ++idx)\n-    {\n-        auto & cell = cells[idx];\n-        if (!isEmptyCell(idx) && !cells[idx].isDefault())\n-            array.push_back(cell.id);\n-    }\n-    return array;\n+    return aggregated_columns;\n }\n \n-BlockInputStreamPtr CacheDictionary::getBlockInputStream(const Names & column_names, size_t max_block_size) const\n+template <DictionaryKeyType dictionary_key_type>\n+BlockInputStreamPtr CacheDictionary<dictionary_key_type>::getBlockInputStream(const Names & column_names, size_t max_block_size) const\n {\n     using BlockInputStreamType = DictionaryBlockInputStream<Key>;\n-    return std::make_shared<BlockInputStreamType>(shared_from_this(), max_block_size, getCachedIds(), column_names);\n-}\n-\n-std::exception_ptr CacheDictionary::getLastException() const\n-{\n-    const ProfilingScopedReadRWLock read_lock{rw_lock, ProfileEvents::DictCacheLockReadNs};\n-    return last_exception;\n-}\n-\n-void registerDictionaryCache(DictionaryFactory & factory)\n-{\n-    auto create_layout = [=](const std::string & full_name,\n-                             const DictionaryStructure & dict_struct,\n-                             const Poco::Util::AbstractConfiguration & config,\n-                             const std::string & config_prefix,\n-                             DictionarySourcePtr source_ptr) -> DictionaryPtr\n-    {\n-        if (dict_struct.key)\n-            throw Exception{\"'key' is not supported for dictionary of layout 'cache'\",\n-                            ErrorCodes::UNSUPPORTED_METHOD};\n-\n-        if (dict_struct.range_min || dict_struct.range_max)\n-            throw Exception{full_name\n-                                + \": elements .structure.range_min and .structure.range_max should be defined only \"\n-                                  \"for a dictionary of layout 'range_hashed'\",\n-                            ErrorCodes::BAD_ARGUMENTS};\n-        const auto & layout_prefix = config_prefix + \".layout\";\n-\n-        const size_t size = config.getUInt64(layout_prefix + \".cache.size_in_cells\");\n-        if (size == 0)\n-            throw Exception{full_name + \": dictionary of layout 'cache' cannot have 0 cells\",\n-                            ErrorCodes::TOO_SMALL_BUFFER_SIZE};\n-\n-        const bool require_nonempty = config.getBool(config_prefix + \".require_nonempty\", false);\n-        if (require_nonempty)\n-            throw Exception{full_name + \": dictionary of layout 'cache' cannot have 'require_nonempty' attribute set\",\n-                            ErrorCodes::BAD_ARGUMENTS};\n-\n-        const auto dict_id = StorageID::fromDictionaryConfig(config, config_prefix);\n-        const DictionaryLifetime dict_lifetime{config, config_prefix + \".lifetime\"};\n-\n-        const size_t strict_max_lifetime_seconds =\n-                config.getUInt64(layout_prefix + \".cache.strict_max_lifetime_seconds\", static_cast<size_t>(dict_lifetime.max_sec));\n-\n-        const size_t max_update_queue_size =\n-                config.getUInt64(layout_prefix + \".cache.max_update_queue_size\", 100000);\n-        if (max_update_queue_size == 0)\n-            throw Exception{full_name + \": dictionary of layout 'cache' cannot have empty update queue of size 0\",\n-                            ErrorCodes::TOO_SMALL_BUFFER_SIZE};\n-\n-        const bool allow_read_expired_keys =\n-                config.getBool(layout_prefix + \".cache.allow_read_expired_keys\", false);\n-\n-        const size_t update_queue_push_timeout_milliseconds =\n-                config.getUInt64(layout_prefix + \".cache.update_queue_push_timeout_milliseconds\", 10);\n-        if (update_queue_push_timeout_milliseconds < 10)\n-            throw Exception{full_name + \": dictionary of layout 'cache' have too little update_queue_push_timeout\",\n-                            ErrorCodes::BAD_ARGUMENTS};\n-\n-        const size_t query_wait_timeout_milliseconds =\n-                config.getUInt64(layout_prefix + \".cache.query_wait_timeout_milliseconds\", 60000);\n-\n-        const size_t max_threads_for_updates =\n-                config.getUInt64(layout_prefix + \".max_threads_for_updates\", 4);\n-        if (max_threads_for_updates == 0)\n-            throw Exception{full_name + \": dictionary of layout 'cache' cannot have zero threads for updates.\",\n-                            ErrorCodes::BAD_ARGUMENTS};\n-\n-        return std::make_unique<CacheDictionary>(\n-                dict_id,\n-                dict_struct,\n-                std::move(source_ptr),\n-                dict_lifetime,\n-                strict_max_lifetime_seconds,\n-                size,\n-                allow_read_expired_keys,\n-                max_update_queue_size,\n-                update_queue_push_timeout_milliseconds,\n-                query_wait_timeout_milliseconds,\n-                max_threads_for_updates);\n-    };\n-    factory.registerLayout(\"cache\", create_layout, false);\n-}\n+    std::shared_ptr<BlockInputStreamType> stream;\n \n-void CacheDictionary::updateThreadFunction()\n-{\n-    setThreadName(\"AsyncUpdater\");\n-    while (!finished)\n     {\n-        UpdateUnitPtr popped;\n-        update_queue.pop(popped);\n-\n-        if (finished)\n-            break;\n-\n-        try\n-        {\n-            /// Update a bunch of ids.\n-            update(popped);\n+        /// Write lock on storage\n+        const ProfilingScopedWriteRWLock write_lock{rw_lock, ProfileEvents::DictCacheLockWriteNs};\n \n-            /// Notify thread about finished updating the bunch of ids\n-            /// where their own ids were included.\n-            std::unique_lock<std::mutex> lock(update_mutex);\n-\n-            popped->is_done = true;\n-            is_update_finished.notify_all();\n-        }\n-        catch (...)\n+        if constexpr (dictionary_key_type == DictionaryKeyType::simple)\n+            stream = std::make_shared<BlockInputStreamType>(shared_from_this(), max_block_size, cache_storage_ptr->getCachedSimpleKeys(), column_names);\n+        else\n         {\n-            std::unique_lock<std::mutex> lock(update_mutex);\n-\n-            popped->current_exception = std::current_exception();\n-            is_update_finished.notify_all();\n+            auto keys = cache_storage_ptr->getCachedComplexKeys();\n+            stream = std::make_shared<BlockInputStreamType>(shared_from_this(), max_block_size, keys, column_names);\n         }\n     }\n-}\n \n-void CacheDictionary::waitForCurrentUpdateFinish(UpdateUnitPtr & update_unit_ptr) const\n-{\n-    std::unique_lock<std::mutex> update_lock(update_mutex);\n-\n-    bool result = is_update_finished.wait_for(\n-            update_lock,\n-            std::chrono::milliseconds(query_wait_timeout_milliseconds),\n-            [&] { return update_unit_ptr->is_done || update_unit_ptr->current_exception; });\n-\n-    if (!result)\n-    {\n-        throw DB::Exception(ErrorCodes::TIMEOUT_EXCEEDED,\n-                            \"Dictionary {} source seems unavailable, because {}ms timeout exceeded.\",\n-                            getDictionaryID().getNameForLogs(), toString(query_wait_timeout_milliseconds));\n-    }\n+    return stream;\n+}\n+\n+template <DictionaryKeyType dictionary_key_type>\n+void CacheDictionary<dictionary_key_type>::update(CacheDictionaryUpdateUnitPtr<dictionary_key_type> update_unit_ptr)\n+{\n+    /**\n+    * Update has following flow.\n+    * 1. Filter only necessary keys to request, keys that are expired or not found.\n+    * And create not_found_keys hash_set including each requested key.\n+    * In case of simple_keys we need to fill requested_keys_vector with requested value key.\n+    * In case of complex_keys we need to fill requested_complex_key_rows with requested row.\n+    * 2. Create stream from source with necessary keys to request using method for simple or complex keys.\n+    * 3. Create fetched columns during update variable. This columns will aggregate columns that we fetch from source.\n+    * 4. When block is fetched from source. Split it into keys columns and attributes columns.\n+    * Insert attributes columns into associated fetched columns during update.\n+    * Create KeysExtractor and extract keys from keys columns.\n+    * Update map of requested found key to fetched column index.\n+    * Remove found key from not_found_keys.\n+    * 5. Add aggregated columns during update into storage.\n+    * 6. Add not found keys as default into storage.\n+    */\n+    CurrentMetrics::Increment metric_increment{CurrentMetrics::DictCacheRequests};\n \n+    size_t found_keys_size = 0;\n \n-    if (update_unit_ptr->current_exception)\n-    {\n-        // Don't just rethrow it, because sharing the same exception object\n-        // between multiple threads can lead to weird effects if they decide to\n-        // modify it, for example, by adding some error context.\n-        try\n-        {\n-            std::rethrow_exception(update_unit_ptr->current_exception);\n-        }\n-        catch (...)\n-        {\n-            throw DB::Exception(ErrorCodes::CACHE_DICTIONARY_UPDATE_FAIL,\n-                \"Update failed for dictionary '{}': {}\",\n-                getDictionaryID().getNameForLogs(),\n-                getCurrentExceptionMessage(true /*with stack trace*/,\n-                    true /*check embedded stack trace*/));\n-        }\n-    }\n-}\n+    DictionaryKeysExtractor<dictionary_key_type> requested_keys_extractor(update_unit_ptr->key_columns, update_unit_ptr->complex_key_arena);\n+    const auto & requested_keys = requested_keys_extractor.getKeys();\n \n-void CacheDictionary::tryPushToUpdateQueueOrThrow(UpdateUnitPtr & update_unit_ptr) const\n-{\n-    if (!update_queue.tryPush(update_unit_ptr, update_queue_push_timeout_milliseconds))\n-        throw DB::Exception(ErrorCodes::CACHE_DICTIONARY_UPDATE_FAIL,\n-                \"Cannot push to internal update queue in dictionary {}. \"\n-                \"Timelimit of {} ms. exceeded. Current queue size is {}\",\n-                getDictionaryID().getNameForLogs(), std::to_string(update_queue_push_timeout_milliseconds),\n-                std::to_string(update_queue.size()));\n-}\n+    HashSet<KeyType> not_found_keys;\n \n+    std::vector<UInt64> requested_keys_vector;\n+    std::vector<size_t> requested_complex_key_rows;\n \n-std::vector<CacheDictionary::AttributeValue> CacheDictionary::getAttributeValuesFromBlockAtPosition(const std::vector<const IColumn *> & column_ptrs, size_t position)\n-{\n-    std::vector<AttributeValue> answer;\n-    answer.reserve(column_ptrs.size());\n+    auto & key_index_to_state_from_storage = update_unit_ptr->key_index_to_state;\n \n-    for (const auto * pure_column : column_ptrs)\n+    for (size_t i = 0; i < key_index_to_state_from_storage.size(); ++i)\n     {\n-#define DISPATCH(TYPE) \\\n-        if (const auto * column = typeid_cast<const Column##TYPE *>(pure_column)) { \\\n-            answer.emplace_back(column->getElement(position)); \\\n-            continue; \\\n-        }\n-        DISPATCH(UInt8)\n-        DISPATCH(UInt16)\n-        DISPATCH(UInt32)\n-        DISPATCH(UInt64)\n-        DISPATCH(UInt128)\n-        DISPATCH(Int8)\n-        DISPATCH(Int16)\n-        DISPATCH(Int32)\n-        DISPATCH(Int64)\n-        DISPATCH(Decimal<Decimal32>)\n-        DISPATCH(Decimal<Decimal64>)\n-        DISPATCH(Decimal<Decimal128>)\n-        DISPATCH(Float32)\n-        DISPATCH(Float64)\n-#undef DISPATCH\n-        if (const auto * column = typeid_cast<const ColumnString *>(pure_column))\n+        if (key_index_to_state_from_storage[i].isExpired()\n+            || key_index_to_state_from_storage[i].isNotFound())\n         {\n-            answer.emplace_back(column->getDataAt(position).toString());\n-            continue;\n+            if constexpr (dictionary_key_type == DictionaryKeyType::simple)\n+                requested_keys_vector.emplace_back(requested_keys[i]);\n+            else\n+                requested_complex_key_rows.emplace_back(i);\n+\n+            auto requested_key = requested_keys[i];\n+            not_found_keys.insert(requested_key);\n         }\n     }\n-    return answer;\n-}\n-\n-void CacheDictionary::update(UpdateUnitPtr & update_unit_ptr)\n-{\n-    CurrentMetrics::Increment metric_increment{CurrentMetrics::DictCacheRequests};\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysRequested, update_unit_ptr->requested_ids.size());\n \n-    auto & map_ids = update_unit_ptr->found_ids;\n+    size_t requested_keys_size = update_unit_ptr->keys_to_update_size;\n+    ProfileEvents::increment(ProfileEvents::DictCacheKeysRequested, requested_keys_size);\n \n-    size_t found_num = 0;\n+    const auto & fetch_request = update_unit_ptr->request;\n \n     const auto now = std::chrono::system_clock::now();\n \n@@ -1287,86 +701,77 @@ void CacheDictionary::update(UpdateUnitPtr & update_unit_ptr)\n             auto current_source_ptr = getSourceAndUpdateIfNeeded();\n \n             Stopwatch watch;\n+            BlockInputStreamPtr stream;\n \n-            BlockInputStreamPtr stream = current_source_ptr->loadIds(update_unit_ptr->requested_ids);\n-            stream->readPrefix();\n+            if constexpr (dictionary_key_type == DictionaryKeyType::simple)\n+                stream = current_source_ptr->loadIds(requested_keys_vector);\n+            else\n+                stream = current_source_ptr->loadKeys(update_unit_ptr->key_columns, requested_complex_key_rows);\n \n-            while (true)\n-            {\n-                Block block = stream->read();\n-                if (!block)\n-                    break;\n+            stream->readPrefix();\n \n-                const auto * id_column = typeid_cast<const ColumnUInt64 *>(block.safeGetByPosition(0).column.get());\n-                if (!id_column)\n-                    throw Exception{ErrorCodes::TYPE_MISMATCH,\n-                                    \"{}: id column has type different from UInt64.\", getDictionaryID().getNameForLogs()};\n+            size_t skip_keys_size_offset = dict_struct.getKeysSize();\n+            PaddedPODArray<KeyType> found_keys_in_source;\n \n-                const auto & ids = id_column->getData();\n+            Columns fetched_columns_during_update = fetch_request.makeAttributesResultColumnsNonMutable();\n \n-                /// cache column pointers\n-                const auto column_ptrs = ext::map<std::vector>(\n-                        ext::range(0, attributes.size()), [&block](size_t i) { return block.safeGetByPosition(i + 1).column.get(); });\n+            while (Block block = stream->read())\n+            {\n+                Columns key_columns;\n+                key_columns.reserve(skip_keys_size_offset);\n \n-                found_num += ids.size();\n+                auto block_columns = block.getColumns();\n \n-                for (const auto i : ext::range(0, ids.size()))\n+                /// Split into keys columns and attribute columns\n+                for (size_t i = 0; i < skip_keys_size_offset; ++i)\n                 {\n-                    /// Modifying cache with write lock\n-                    ProfilingScopedWriteRWLock write_lock{rw_lock, ProfileEvents::DictCacheLockWriteNs};\n-                    const auto id = ids[i];\n-\n-                    const auto cell_idx = findCellIdxForSet(id);\n-                    auto & cell = cells[cell_idx];\n-\n-                    auto it = map_ids.find(id);\n+                    key_columns.emplace_back(*block_columns.begin());\n+                    block_columns.erase(block_columns.begin());\n+                }\n \n-                    /// We have some extra keys from source. Won't add them to cache.\n-                    if (it == map_ids.end())\n-                        continue;\n+                DictionaryKeysExtractor<dictionary_key_type> keys_extractor(key_columns, update_unit_ptr->complex_key_arena);\n+                const auto & keys_extracted_from_block = keys_extractor.getKeys();\n \n-                    auto & all_attributes = it->second;\n-                    all_attributes.found = true;\n-                    all_attributes.values = getAttributeValuesFromBlockAtPosition(column_ptrs, i);\n+                for (size_t index_of_attribute = 0; index_of_attribute < fetched_columns_during_update.size(); ++index_of_attribute)\n+                {\n+                    auto & column_to_update = fetched_columns_during_update[index_of_attribute];\n+                    auto column = block.safeGetByPosition(skip_keys_size_offset + index_of_attribute).column;\n+                    column_to_update->assumeMutable()->insertRangeFrom(*column, 0, keys_extracted_from_block.size());\n+                }\n \n-                    for (const auto attribute_idx : ext::range(0, attributes.size()))\n-                    {\n-                        const auto & attribute_column = *column_ptrs[attribute_idx];\n-                        auto & attribute = attributes[attribute_idx];\n+                for (size_t i = 0; i < keys_extracted_from_block.size(); ++i)\n+                {\n+                    auto fetched_key_from_source = keys_extracted_from_block[i];\n+                    not_found_keys.erase(fetched_key_from_source);\n+                    update_unit_ptr->requested_keys_to_fetched_columns_during_update_index[fetched_key_from_source] = found_keys_size;\n+                    found_keys_in_source.emplace_back(fetched_key_from_source);\n+                    ++found_keys_size;\n+                }\n+            }\n \n-                        setAttributeValue(attribute, cell_idx, attribute_column[i]);\n-                    }\n+            PaddedPODArray<KeyType> not_found_keys_in_source;\n+            not_found_keys_in_source.reserve(not_found_keys.size());\n \n-                    /// if cell id is zero and zero does not map to this cell, then the cell is unused\n-                    if (cell.id == 0 && cell_idx != zero_cell_idx)\n-                        element_count.fetch_add(1, std::memory_order_relaxed);\n+            for (auto & cell : not_found_keys)\n+                not_found_keys_in_source.emplace_back(cell.getKey());\n \n-                    cell.id = id;\n-                    setLifetime(cell, now);\n-                }\n-            }\n+            auto & update_unit_ptr_mutable_columns = update_unit_ptr->fetched_columns_during_update;\n+            for (const auto & fetched_column : fetched_columns_during_update)\n+                update_unit_ptr_mutable_columns.emplace_back(fetched_column->assumeMutable());\n \n             stream->readSuffix();\n \n-            /// Lock for cache modification\n-            ProfilingScopedWriteRWLock write_lock{rw_lock, ProfileEvents::DictCacheLockWriteNs};\n-\n-            for (auto & [key, value] : update_unit_ptr->found_ids)\n             {\n-                if (!value.found)\n-                {\n-                    auto cell_idx = findCellIdxForSet(key);\n-                    auto & cell = cells[cell_idx];\n-                    cell.id = key;\n-                    setLifetime(cell, now);\n-                    cell.setDefault();\n-                }\n+                /// Lock for cache modification\n+                ProfilingScopedWriteRWLock write_lock{rw_lock, ProfileEvents::DictCacheLockWriteNs};\n+                cache_storage_ptr->insertColumnsForKeys(found_keys_in_source, fetched_columns_during_update);\n+                cache_storage_ptr->insertDefaultKeys(not_found_keys_in_source);\n+\n+                error_count = 0;\n+                last_exception = std::exception_ptr{};\n+                backoff_end_time = std::chrono::system_clock::time_point{};\n             }\n \n-            error_count = 0;\n-            last_exception = std::exception_ptr{};\n-            backoff_end_time = std::chrono::system_clock::time_point{};\n-\n             ProfileEvents::increment(ProfileEvents::DictCacheRequestTimeNs, watch.elapsed());\n         }\n         catch (...)\n@@ -1394,10 +799,9 @@ void CacheDictionary::update(UpdateUnitPtr & update_unit_ptr)\n             }\n         }\n \n-\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysRequestedMiss, update_unit_ptr->requested_ids.size() - found_num);\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysRequestedFound, found_num);\n-    ProfileEvents::increment(ProfileEvents::DictCacheRequests);\n+        ProfileEvents::increment(ProfileEvents::DictCacheKeysRequestedMiss, requested_keys_size - found_keys_size);\n+        ProfileEvents::increment(ProfileEvents::DictCacheKeysRequestedFound, found_keys_size);\n+        ProfileEvents::increment(ProfileEvents::DictCacheRequests);\n     }\n     else\n     {\n@@ -1409,4 +813,7 @@ void CacheDictionary::update(UpdateUnitPtr & update_unit_ptr)\n     }\n }\n \n+template class CacheDictionary<DictionaryKeyType::simple>;\n+template class CacheDictionary<DictionaryKeyType::complex>;\n+\n }\ndiff --git a/src/Dictionaries/CacheDictionary.h b/src/Dictionaries/CacheDictionary.h\nindex 35d38f03cbe3..1192db737370 100644\n--- a/src/Dictionaries/CacheDictionary.h\n+++ b/src/Dictionaries/CacheDictionary.h\n@@ -3,72 +3,76 @@\n #include <atomic>\n #include <chrono>\n #include <cmath>\n-#include <map>\n #include <mutex>\n #include <shared_mutex>\n #include <utility>\n-#include <variant>\n #include <vector>\n+\n+#include <pcg_random.hpp>\n+\n #include <common/logger_useful.h>\n-#include <Columns/ColumnDecimal.h>\n-#include <Columns/ColumnString.h>\n+\n+#include <Common/randomSeed.h>\n #include <Common/ThreadPool.h>\n-#include <Common/ConcurrentBoundedQueue.h>\n-#include <pcg_random.hpp>\n-#include <Common/ArenaWithFreeLists.h>\n #include <Common/CurrentMetrics.h>\n-#include <ext/bit_cast.h>\n-#include \"DictionaryStructure.h\"\n-#include \"IDictionary.h\"\n-#include \"IDictionarySource.h\"\n-#include \"DictionaryHelpers.h\"\n-\n-namespace CurrentMetrics\n-{\n-    extern const Metric CacheDictionaryUpdateQueueBatches;\n-    extern const Metric CacheDictionaryUpdateQueueKeys;\n-}\n \n+#include <Dictionaries/IDictionary.h>\n+#include <Dictionaries/ICacheDictionaryStorage.h>\n+#include <Dictionaries/DictionaryStructure.h>\n+#include <Dictionaries/IDictionarySource.h>\n+#include <Dictionaries/DictionaryHelpers.h>\n+#include <Dictionaries/CacheDictionaryUpdateQueue.h>\n \n namespace DB\n {\n+/** CacheDictionary store keys in cache storage and can asynchronous and synchronous updates during keys fetch.\n \n-namespace ErrorCodes\n-{\n-}\n+    If keys are not found in storage during fetch, dictionary start update operation with update queue.\n+\n+    During update operation necessary keys are fetched from source and inserted into storage.\n+\n+    After that data from storage and source are aggregated and returned to the client.\n \n-/*\n- *\n- * This dictionary is stored in a cache that has a fixed number of cells.\n- * These cells contain frequently used elements.\n- * When searching for a dictionary, the cache is searched first and special heuristic is used:\n- * while looking for the key, we take a look only at max_collision_length elements.\n- * So, our cache is not perfect. It has errors like \"the key is in cache, but the cache says that it does not\".\n- * And in this case we simply ask external source for the key which is faster.\n- * You have to keep this logic in mind.\n- * */\n+    Typical flow:\n+\n+    1. Client request data during for example getColumn function call.\n+    2. CacheDictionary request data from storage and if all data is found in storage it returns result to client.\n+    3. If some data is not in storage cache dictionary try to perform update.\n+\n+    If all keys are just expired and allow_read_expired_keys option is set dictionary starts asynchronous update and\n+    return result to client.\n+\n+    If there are not found keys dictionary start synchronous update and wait for result.\n+\n+    4. After getting result from synchronous update dictionary aggregates data that was previously fetched from\n+    storage and data that was fetched during update and return result to client.\n+ */\n+template <DictionaryKeyType dictionary_key_type>\n class CacheDictionary final : public IDictionary\n {\n public:\n+    using KeyType = std::conditional_t<dictionary_key_type == DictionaryKeyType::simple, UInt64, StringRef>;\n+    static_assert(dictionary_key_type != DictionaryKeyType::range, \"Range key type is not supported by cache dictionary\");\n+\n     CacheDictionary(\n         const StorageID & dict_id_,\n         const DictionaryStructure & dict_struct_,\n         DictionarySourcePtr source_ptr_,\n+        CacheDictionaryStoragePtr cache_storage_ptr_,\n+        CacheDictionaryUpdateQueueConfiguration update_queue_configuration_,\n         DictionaryLifetime dict_lifetime_,\n-        size_t strict_max_lifetime_seconds,\n-        size_t size_,\n-        bool allow_read_expired_keys_,\n-        size_t max_update_queue_size_,\n-        size_t update_queue_push_timeout_milliseconds_,\n-        size_t query_wait_timeout_milliseconds,\n-        size_t max_threads_for_updates);\n+        bool allow_read_expired_keys_);\n \n     ~CacheDictionary() override;\n \n-    std::string getTypeName() const override { return \"Cache\"; }\n+    std::string getTypeName() const override { return cache_storage_ptr->getName(); }\n+\n+    size_t getElementCount() const override;\n \n     size_t getBytesAllocated() const override;\n \n+    double getLoadFactor() const override;\n+\n     size_t getQueryCount() const override { return query_count.load(std::memory_order_relaxed); }\n \n     double getHitRate() const override\n@@ -76,10 +80,6 @@ class CacheDictionary final : public IDictionary\n         return static_cast<double>(hit_count.load(std::memory_order_acquire)) / query_count.load(std::memory_order_relaxed);\n     }\n \n-    size_t getElementCount() const override { return element_count.load(std::memory_order_relaxed); }\n-\n-    double getLoadFactor() const override { return static_cast<double>(element_count.load(std::memory_order_relaxed)) / size; }\n-\n     bool supportUpdates() const override { return false; }\n \n     std::shared_ptr<const IExternalLoadable> clone() const override\n@@ -88,14 +88,10 @@ class CacheDictionary final : public IDictionary\n                 getDictionaryID(),\n                 dict_struct,\n                 getSourceAndUpdateIfNeeded()->clone(),\n+                cache_storage_ptr,\n+                update_queue.getConfiguration(),\n                 dict_lifetime,\n-                strict_max_lifetime_seconds,\n-                size,\n-                allow_read_expired_keys,\n-                max_update_queue_size,\n-                update_queue_push_timeout_milliseconds,\n-                query_wait_timeout_milliseconds,\n-                max_threads_for_updates);\n+                allow_read_expired_keys);\n     }\n \n     const IDictionarySource * getSource() const override;\n@@ -106,133 +102,78 @@ class CacheDictionary final : public IDictionary\n \n     bool isInjective(const std::string & attribute_name) const override\n     {\n-        return dict_struct.attributes[&getAttribute(attribute_name) - attributes.data()].injective;\n+        return dict_struct.getAttribute(attribute_name).injective;\n     }\n \n-    bool hasHierarchy() const override { return hierarchical_attribute; }\n-\n-    void toParent(const PaddedPODArray<Key> & ids, PaddedPODArray<Key> & out) const override;\n-\n-    void isInVectorVector(\n-        const PaddedPODArray<Key> & child_ids, const PaddedPODArray<Key> & ancestor_ids, PaddedPODArray<UInt8> & out) const override;\n-    void isInVectorConstant(const PaddedPODArray<Key> & child_ids, const Key ancestor_id, PaddedPODArray<UInt8> & out) const override;\n-    void isInConstantVector(const Key child_id, const PaddedPODArray<Key> & ancestor_ids, PaddedPODArray<UInt8> & out) const override;\n-\n-    std::exception_ptr getLastException() const override;\n-\n-    DictionaryKeyType getKeyType() const override { return DictionaryKeyType::simple; }\n+    DictionaryKeyType getKeyType() const override\n+    {\n+        return dictionary_key_type == DictionaryKeyType::simple ? DictionaryKeyType::simple : DictionaryKeyType::complex;\n+    }\n \n     ColumnPtr getColumn(\n         const std::string& attribute_name,\n         const DataTypePtr & result_type,\n         const Columns & key_columns,\n         const DataTypes & key_types,\n-        const ColumnPtr default_values_column) const override;\n+        const ColumnPtr & default_values_column) const override;\n \n-    ColumnUInt8::Ptr hasKeys(const Columns & key_columns, const DataTypes & key_types) const override;\n+    Columns getColumns(\n+        const Strings & attribute_names,\n+        const DataTypes & result_types,\n+        const Columns & key_columns,\n+        const DataTypes & key_types,\n+        const Columns & default_values_columns) const override;\n \n-    template <typename T>\n-    using ResultArrayType = std::conditional_t<IsDecimalNumber<T>, DecimalPaddedPODArray<T>, PaddedPODArray<T>>;\n+    ColumnUInt8::Ptr hasKeys(const Columns & key_columns, const DataTypes & key_types) const override;\n \n     BlockInputStreamPtr getBlockInputStream(const Names & column_names, size_t max_block_size) const override;\n \n+    std::exception_ptr getLastException() const override;\n+\n+    bool hasHierarchy() const override { return dictionary_key_type == DictionaryKeyType::simple && hierarchical_attribute; }\n+\n+    void toParent(const PaddedPODArray<UInt64> & ids, PaddedPODArray<UInt64> & out) const override;\n+\n+    void isInVectorVector(\n+        const PaddedPODArray<UInt64> & child_ids,\n+        const PaddedPODArray<UInt64> & ancestor_ids,\n+        PaddedPODArray<UInt8> & out) const override;\n+\n+    void isInVectorConstant(\n+        const PaddedPODArray<UInt64> & child_ids,\n+        const UInt64 ancestor_id, PaddedPODArray<UInt8> & out) const override;\n+\n+    void isInConstantVector(\n+        const UInt64 child_id,\n+        const PaddedPODArray<UInt64> & ancestor_ids,\n+        PaddedPODArray<UInt8> & out) const override;\n+\n private:\n-    template <typename Value>\n-    using ContainerType = Value[];\n-    template <typename Value>\n-    using ContainerPtrType = std::unique_ptr<ContainerType<Value>>;\n+    using FetchResult = std::conditional_t<dictionary_key_type == DictionaryKeyType::simple, SimpleKeysStorageFetchResult, ComplexKeysStorageFetchResult>;\n \n-    using time_point_t = std::chrono::system_clock::time_point;\n+    Columns getColumnsImpl(\n+        const Strings & attribute_names,\n+        const Columns & key_columns,\n+        const PaddedPODArray<KeyType> & keys,\n+        const Columns & default_values_columns) const;\n \n-    struct CellMetadata final\n-    {\n-        UInt64 id;\n-        time_point_t deadline;\n-        bool is_default{false};\n-\n-        time_point_t expiresAt() const { return deadline; }\n-        void setExpiresAt(const time_point_t & t) { deadline = t; is_default = false; }\n-        bool isDefault() const { return is_default; }\n-        void setDefault() { is_default = true; }\n-    };\n-\n-    using AttributeValue = std::variant<\n-        UInt8, UInt16, UInt32, UInt64, UInt128,\n-        Int8, Int16, Int32, Int64,\n-        Decimal32, Decimal64, Decimal128,\n-        Float32, Float64, String>;\n-\n-    struct AttributeValuesForKey\n-    {\n-        bool found{false};\n-        std::vector<AttributeValue> values;\n+    static MutableColumns aggregateColumnsInOrderOfKeys(\n+        const PaddedPODArray<KeyType> & keys,\n+        const DictionaryStorageFetchRequest & request,\n+        const MutableColumns & fetched_columns,\n+        const PaddedPODArray<KeyState> & key_index_to_state);\n \n-        std::string dump();\n-    };\n+    static MutableColumns aggregateColumns(\n+        const PaddedPODArray<KeyType> & keys,\n+        const DictionaryStorageFetchRequest & request,\n+        const MutableColumns & fetched_columns_from_storage,\n+        const PaddedPODArray<KeyState> & key_index_to_fetched_columns_from_storage_result,\n+        const MutableColumns & fetched_columns_during_update,\n+        const HashMap<KeyType, size_t> & found_keys_to_fetched_columns_during_update_index);\n \n-    using FoundValuesForKeys = std::unordered_map<Key, AttributeValuesForKey>;\n+    void setupHierarchicalAttribute();\n \n-    struct Attribute final\n-    {\n-        AttributeUnderlyingType type;\n-        String name;\n-        /// Default value for each type. Could be defined in config.\n-        AttributeValue null_value;\n-        /// We store attribute value for all keys. It is a \"row\" in a hand-made open addressing hashtable,\n-        /// where \"column\" is key.\n-        std::variant<\n-            ContainerPtrType<UInt8>,\n-            ContainerPtrType<UInt16>,\n-            ContainerPtrType<UInt32>,\n-            ContainerPtrType<UInt64>,\n-            ContainerPtrType<UInt128>,\n-            ContainerPtrType<Int8>,\n-            ContainerPtrType<Int16>,\n-            ContainerPtrType<Int32>,\n-            ContainerPtrType<Int64>,\n-            ContainerPtrType<Decimal32>,\n-            ContainerPtrType<Decimal64>,\n-            ContainerPtrType<Decimal128>,\n-            ContainerPtrType<Float32>,\n-            ContainerPtrType<Float64>,\n-            ContainerPtrType<StringRef>>\n-            arrays;\n-    };\n-\n-    void createAttributes();\n-\n-    /* NOLINTNEXTLINE(readability-convert-member-functions-to-static) */\n-    Attribute createAttributeWithTypeAndName(const AttributeUnderlyingType type, const String & name, const Field & null_value);\n-\n-    template <typename AttributeType, typename OutputType, typename DefaultValueExtractor>\n-    void getItemsNumberImpl(\n-        Attribute & attribute,\n-        const PaddedPODArray<Key> & ids,\n-        ResultArrayType<OutputType> & out,\n-        DefaultValueExtractor & default_value_extractor) const;\n-\n-    void getItemsString(\n-        Attribute & attribute,\n-        const PaddedPODArray<Key> & ids,\n-        ColumnString * out,\n-        DictionaryDefaultValueExtractor<String> & default_value_extractor) const;\n-\n-    PaddedPODArray<Key> getCachedIds() const;\n-\n-    bool isEmptyCell(const UInt64 idx) const;\n-\n-    size_t getCellIdx(const Key id) const;\n-\n-    void setDefaultAttributeValue(Attribute & attribute, const Key idx) const;\n-\n-    void setAttributeValue(Attribute & attribute, const Key idx, const Field & value) const;\n-\n-    static std::vector<AttributeValue> getAttributeValuesFromBlockAtPosition(const std::vector<const IColumn *> & column_ptrs, size_t position);\n-\n-    Attribute & getAttribute(const std::string & attribute_name) const;\n-    size_t getAttributeIndex(const std::string & attribute_name) const;\n-\n-    using SharedDictionarySourcePtr = std::shared_ptr<IDictionarySource>;\n+    void update(CacheDictionaryUpdateUnitPtr<dictionary_key_type> update_unit_ptr);\n \n     /// Update dictionary source pointer if required and return it. Thread safe.\n     /// MultiVersion is not used here because it works with constant pointers.\n@@ -252,47 +193,6 @@ class CacheDictionary final : public IDictionary\n         return source_ptr;\n     }\n \n-    inline void setLifetime(CellMetadata & cell, time_point_t now)\n-    {\n-        if (dict_lifetime.min_sec != 0 && dict_lifetime.max_sec != 0)\n-        {\n-            std::uniform_int_distribution<UInt64> distribution{dict_lifetime.min_sec, dict_lifetime.max_sec};\n-            cell.setExpiresAt(now + std::chrono::seconds{distribution(rnd_engine)});\n-        }\n-        else\n-        {\n-            /// This maybe not obvious, but when we define is this cell is expired or expired permanently, we add strict_max_lifetime_seconds\n-            /// to the expiration time. And it overflows pretty well.\n-            cell.setExpiresAt(std::chrono::time_point<std::chrono::system_clock>::max() - 2 * std::chrono::seconds(strict_max_lifetime_seconds));\n-        }\n-    }\n-\n-    inline bool isExpired(time_point_t now, time_point_t deadline) const\n-    {\n-        return now > deadline;\n-    }\n-\n-    inline bool isExpiredPermanently(time_point_t now, time_point_t deadline) const\n-    {\n-        return now > deadline + std::chrono::seconds(strict_max_lifetime_seconds);\n-    }\n-\n-    enum class ResultState\n-    {\n-        NotFound,\n-        FoundAndValid,\n-        FoundButExpired,\n-        /// Here is a gap between there two states in which a key could be read\n-        /// with an enabled setting in config enable_read_expired_keys.\n-        FoundButExpiredPermanently\n-    };\n-\n-    using FindResult = std::pair<size_t, ResultState>;\n-\n-    FindResult findCellIdxForGet(const Key & id, const time_point_t now) const;\n-\n-    size_t findCellIdxForSet(const Key & id) const;\n-\n     template <typename AncestorType>\n     void isInImpl(const PaddedPODArray<Key> & child_ids, const AncestorType & ancestor_ids, PaddedPODArray<UInt8> & out) const;\n \n@@ -302,110 +202,34 @@ class CacheDictionary final : public IDictionary\n     mutable std::mutex source_mutex;\n     mutable SharedDictionarySourcePtr source_ptr;\n \n+    CacheDictionaryStoragePtr cache_storage_ptr;\n+    mutable CacheDictionaryUpdateQueue<dictionary_key_type> update_queue;\n+\n     const DictionaryLifetime dict_lifetime;\n-    const size_t strict_max_lifetime_seconds;\n-    const bool allow_read_expired_keys;\n-    const size_t max_update_queue_size;\n-    const size_t update_queue_push_timeout_milliseconds;\n-    const size_t query_wait_timeout_milliseconds;\n-    const size_t max_threads_for_updates;\n \n     Poco::Logger * log;\n \n+    const bool allow_read_expired_keys;\n+\n+    mutable pcg64 rnd_engine;\n+\n     /// This lock is used for the inner cache state update function lock it for\n     /// write, when it need to update cache state all other functions just\n     /// readers. Surprisingly this lock is also used for last_exception pointer.\n     mutable std::shared_mutex rw_lock;\n \n-    /// Actual size will be increased to match power of 2\n-    const size_t size;\n-\n-    /// all bits to 1  mask (size - 1) (0b1000 - 1 = 0b111)\n-    const size_t size_overlap_mask;\n-\n-    /// Max tries to find cell, overlapped with mask: if size = 16 and start_cell=10: will try cells: 10,11,12,13,14,15,0,1,2,3\n-    static constexpr size_t max_collision_length = 10;\n-\n-    const size_t zero_cell_idx{getCellIdx(0)};\n-    std::map<std::string, size_t> attribute_index_by_name;\n-    mutable std::vector<Attribute> attributes;\n-    mutable std::vector<CellMetadata> cells;\n-    Attribute * hierarchical_attribute = nullptr;\n-    std::unique_ptr<ArenaWithFreeLists> string_arena;\n+    const DictionaryAttribute * hierarchical_attribute = nullptr;\n \n     mutable std::exception_ptr last_exception;\n-    mutable std::atomic<size_t> error_count{0};\n+    mutable std::atomic<size_t> error_count {0};\n     mutable std::atomic<std::chrono::system_clock::time_point> backoff_end_time{std::chrono::system_clock::time_point{}};\n \n-    mutable pcg64 rnd_engine;\n-\n-    mutable size_t bytes_allocated = 0;\n-    mutable std::atomic<size_t> element_count{0};\n     mutable std::atomic<size_t> hit_count{0};\n     mutable std::atomic<size_t> query_count{0};\n \n-    /*\n-     * How the update goes: we basically have a method like get(keys)->values. Values are cached, so sometimes we\n-     * can return them from the cache. For values not in cache, we query them from the source, and add to the\n-     * cache. The cache is lossy, so we can't expect it to store all the keys, and we store them separately.\n-     * So, there is a map of found keys to all its attributes.\n-     */\n-    struct UpdateUnit\n-    {\n-        explicit UpdateUnit(std::vector<Key> && requested_ids_) :\n-                requested_ids(std::move(requested_ids_)),\n-                alive_keys(CurrentMetrics::CacheDictionaryUpdateQueueKeys, requested_ids.size())\n-        {\n-            found_ids.reserve(requested_ids.size());\n-            for (const auto id : requested_ids)\n-                found_ids.insert({id, {}});\n-        }\n-\n-        std::vector<Key> requested_ids;\n-        FoundValuesForKeys found_ids;\n-\n-        std::atomic<bool> is_done{false};\n-        std::exception_ptr current_exception{nullptr};\n-\n-        /// While UpdateUnit is alive, it is accounted in update_queue size.\n-        CurrentMetrics::Increment alive_batch{CurrentMetrics::CacheDictionaryUpdateQueueBatches};\n-        CurrentMetrics::Increment alive_keys;\n-\n-        std::string dumpFoundIds();\n-    };\n-\n-    using UpdateUnitPtr = std::shared_ptr<UpdateUnit>;\n-    using UpdateQueue = ConcurrentBoundedQueue<UpdateUnitPtr>;\n-\n-    mutable UpdateQueue update_queue;\n-\n-    ThreadPool update_pool;\n-\n-    /*\n-     *  Actually, we can divide all requested keys into two 'buckets'. There are only four possible states and they\n-     * are described in the table.\n-     *\n-     * cache_not_found_ids  |0|0|1|1|\n-     * cache_expired_ids    |0|1|0|1|\n-     *\n-     * 0 - if set is empty, 1 - otherwise\n-     *\n-     * Only if there are no cache_not_found_ids and some cache_expired_ids\n-     * (with allow_read_expired_keys setting) we can perform async update.\n-     * Otherwise we have no concatenate ids and update them sync.\n-     *\n-     */\n-    void updateThreadFunction();\n-    void update(UpdateUnitPtr & update_unit_ptr);\n-\n-\n-    void tryPushToUpdateQueueOrThrow(UpdateUnitPtr & update_unit_ptr) const;\n-    void waitForCurrentUpdateFinish(UpdateUnitPtr & update_unit_ptr) const;\n-\n-    mutable std::mutex update_mutex;\n-    mutable std::condition_variable is_update_finished;\n-\n-    std::atomic<bool> finished{false};\n };\n \n+extern template class CacheDictionary<DictionaryKeyType::simple>;\n+extern template class CacheDictionary<DictionaryKeyType::complex>;\n+\n }\ndiff --git a/src/Dictionaries/CacheDictionaryStorage.h b/src/Dictionaries/CacheDictionaryStorage.h\nnew file mode 100644\nindex 000000000000..cf0b74e8bd26\n--- /dev/null\n+++ b/src/Dictionaries/CacheDictionaryStorage.h\n@@ -0,0 +1,418 @@\n+#pragma once\n+\n+#include <chrono>\n+\n+#include <pcg_random.hpp>\n+\n+#include <Common/randomSeed.h>\n+#include <Common/Arena.h>\n+#include <Common/ArenaWithFreeLists.h>\n+#include <Common/HashTable/LRUHashMap.h>\n+#include <Dictionaries/DictionaryStructure.h>\n+#include <Dictionaries/ICacheDictionaryStorage.h>\n+#include <Dictionaries/DictionaryHelpers.h>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int NOT_IMPLEMENTED;\n+}\n+\n+struct CacheDictionaryStorageConfiguration\n+{\n+    /// Max size of storage in cells\n+    const size_t max_size_in_cells;\n+    /// Needed to perform check if cell is expired or not found. Default value is dictionary max lifetime.\n+    const size_t strict_max_lifetime_seconds;\n+    /// Lifetime of dictionary. Cell deadline is random value between lifetime min and max seconds.\n+    const DictionaryLifetime lifetime;\n+};\n+\n+/** Keys are stored in LRUCache and column values are serialized into arena.\n+\n+    Cell in LRUCache consists of allocated size and place in arena were columns serialized data is stored.\n+\n+    Columns are serialized by rows.\n+\n+    When cell is removed from LRUCache data associated with it is also removed from arena.\n+\n+    In case of complex key we also store key data in arena and it is removed from arena.\n+*/\n+template <DictionaryKeyType dictionary_key_type>\n+class CacheDictionaryStorage final : public ICacheDictionaryStorage\n+{\n+public:\n+    using KeyType = std::conditional_t<dictionary_key_type == DictionaryKeyType::simple, UInt64, StringRef>;\n+    static_assert(dictionary_key_type != DictionaryKeyType::range, \"Range key type is not supported by CacheDictionaryStorage\");\n+\n+    explicit CacheDictionaryStorage(CacheDictionaryStorageConfiguration & configuration_)\n+        : configuration(configuration_)\n+        , rnd_engine(randomSeed())\n+        , cache(configuration.max_size_in_cells, false, { arena })\n+    {\n+    }\n+\n+    bool returnsFetchedColumnsInOrderOfRequestedKeys() const override { return true; }\n+\n+    String getName() const override\n+    {\n+        if (dictionary_key_type == DictionaryKeyType::simple)\n+            return \"Cache\";\n+        else\n+            return \"ComplexKeyCache\";\n+    }\n+\n+    bool supportsSimpleKeys() const override { return dictionary_key_type == DictionaryKeyType::simple; }\n+\n+    SimpleKeysStorageFetchResult fetchColumnsForKeys(\n+        const PaddedPODArray<UInt64> & keys,\n+        const DictionaryStorageFetchRequest & fetch_request) override\n+    {\n+        if constexpr (dictionary_key_type == DictionaryKeyType::simple)\n+        {\n+            return fetchColumnsForKeysImpl<SimpleKeysStorageFetchResult>(keys, fetch_request);\n+        }\n+        else\n+            throw Exception(\"Method fetchColumnsForKeys is not supported for complex key storage\", ErrorCodes::NOT_IMPLEMENTED);\n+    }\n+\n+    void insertColumnsForKeys(const PaddedPODArray<UInt64> & keys, Columns columns) override\n+    {\n+        if constexpr (dictionary_key_type == DictionaryKeyType::simple)\n+            insertColumnsForKeysImpl(keys, columns);\n+        else\n+            throw Exception(\"Method insertColumnsForKeys is not supported for complex key storage\", ErrorCodes::NOT_IMPLEMENTED);\n+    }\n+\n+    void insertDefaultKeys(const PaddedPODArray<UInt64> & keys) override\n+    {\n+        if constexpr (dictionary_key_type == DictionaryKeyType::simple)\n+            insertDefaultKeysImpl(keys);\n+        else\n+            throw Exception(\"Method insertDefaultKeysImpl is not supported for complex key storage\", ErrorCodes::NOT_IMPLEMENTED);\n+    }\n+\n+    PaddedPODArray<UInt64> getCachedSimpleKeys() const override\n+    {\n+        if constexpr (dictionary_key_type == DictionaryKeyType::simple)\n+            return getCachedKeysImpl();\n+        else\n+            throw Exception(\"Method getCachedSimpleKeys is not supported for complex key storage\", ErrorCodes::NOT_IMPLEMENTED);\n+    }\n+\n+    bool supportsComplexKeys() const override { return dictionary_key_type == DictionaryKeyType::complex; }\n+\n+    ComplexKeysStorageFetchResult fetchColumnsForKeys(\n+        const PaddedPODArray<StringRef> & keys,\n+        const DictionaryStorageFetchRequest & column_fetch_requests) override\n+    {\n+        if constexpr (dictionary_key_type == DictionaryKeyType::complex)\n+        {\n+            return fetchColumnsForKeysImpl<ComplexKeysStorageFetchResult>(keys, column_fetch_requests);\n+        }\n+        else\n+            throw Exception(\"Method fetchColumnsForKeys is not supported for simple key storage\", ErrorCodes::NOT_IMPLEMENTED);\n+    }\n+\n+    void insertColumnsForKeys(const PaddedPODArray<StringRef> & keys, Columns columns) override\n+    {\n+        if constexpr (dictionary_key_type == DictionaryKeyType::complex)\n+            insertColumnsForKeysImpl(keys, columns);\n+        else\n+            throw Exception(\"Method insertColumnsForKeys is not supported for simple key storage\", ErrorCodes::NOT_IMPLEMENTED);\n+    }\n+\n+    void insertDefaultKeys(const PaddedPODArray<StringRef> & keys) override\n+    {\n+        if constexpr (dictionary_key_type == DictionaryKeyType::complex)\n+            insertDefaultKeysImpl(keys);\n+        else\n+            throw Exception(\"Method insertDefaultKeysImpl is not supported for simple key storage\", ErrorCodes::NOT_IMPLEMENTED);\n+    }\n+\n+    PaddedPODArray<StringRef> getCachedComplexKeys() const override\n+    {\n+        if constexpr (dictionary_key_type == DictionaryKeyType::complex)\n+            return getCachedKeysImpl();\n+        else\n+            throw Exception(\"Method getCachedComplexKeys is not supported for simple key storage\", ErrorCodes::NOT_IMPLEMENTED);\n+    }\n+\n+    size_t getSize() const override { return cache.size(); }\n+\n+    size_t getMaxSize() const override { return cache.getMaxSize(); }\n+\n+    size_t getBytesAllocated() const override { return arena.size() + cache.getSizeInBytes(); }\n+\n+private:\n+\n+    template <typename KeysStorageFetchResult>\n+    ALWAYS_INLINE KeysStorageFetchResult fetchColumnsForKeysImpl(\n+        const PaddedPODArray<KeyType> & keys,\n+        const DictionaryStorageFetchRequest & fetch_request)\n+    {\n+        KeysStorageFetchResult result;\n+\n+        result.fetched_columns = fetch_request.makeAttributesResultColumns();\n+        result.key_index_to_state.resize_fill(keys.size(), {KeyState::not_found});\n+\n+        const auto now = std::chrono::system_clock::now();\n+\n+        size_t fetched_columns_index = 0;\n+\n+        std::chrono::seconds max_lifetime_seconds(configuration.strict_max_lifetime_seconds);\n+\n+        size_t keys_size = keys.size();\n+\n+        for (size_t key_index = 0; key_index < keys_size; ++key_index)\n+        {\n+            auto key = keys[key_index];\n+            auto * it = cache.find(key);\n+\n+            if (it)\n+            {\n+                /// Columns values for key are serialized in cache now deserialize them\n+                const auto & cell = it->getMapped();\n+\n+                bool has_deadline = cellHasDeadline(cell);\n+\n+                if (has_deadline && now > cell.deadline + max_lifetime_seconds)\n+                {\n+                    result.key_index_to_state[key_index] = {KeyState::not_found};\n+                    ++result.not_found_keys_size;\n+                    continue;\n+                }\n+                else if (has_deadline && now > cell.deadline)\n+                {\n+                    result.key_index_to_state[key_index] = {KeyState::expired, fetched_columns_index};\n+                    ++result.expired_keys_size;\n+                }\n+                else\n+                {\n+                    result.key_index_to_state[key_index] = {KeyState::found, fetched_columns_index};\n+                    ++result.found_keys_size;\n+                }\n+\n+                ++fetched_columns_index;\n+\n+                if (cell.isDefault())\n+                {\n+                    result.key_index_to_state[key_index].setDefault();\n+                    ++result.default_keys_size;\n+                    insertDefaultValuesIntoColumns(result.fetched_columns, fetch_request, key_index);\n+                }\n+                else\n+                {\n+                    const char * place_for_serialized_columns = cell.place_for_serialized_columns;\n+                    deserializeAndInsertIntoColumns(result.fetched_columns, fetch_request, place_for_serialized_columns);\n+                }\n+            }\n+            else\n+            {\n+                result.key_index_to_state[key_index] = {KeyState::not_found};\n+                ++result.not_found_keys_size;\n+            }\n+        }\n+\n+        return result;\n+    }\n+\n+    void insertColumnsForKeysImpl(const PaddedPODArray<KeyType> & keys, Columns columns)\n+    {\n+        Arena temporary_values_pool;\n+\n+        size_t columns_to_serialize_size = columns.size();\n+        PaddedPODArray<StringRef> temporary_column_data(columns_to_serialize_size);\n+\n+        const auto now = std::chrono::system_clock::now();\n+\n+        size_t keys_size = keys.size();\n+\n+        for (size_t key_index = 0; key_index < keys_size; ++key_index)\n+        {\n+            size_t allocated_size_for_columns = 0;\n+            const char * block_start = nullptr;\n+\n+            auto key = keys[key_index];\n+            auto * it = cache.find(key);\n+\n+            for (size_t column_index = 0; column_index < columns_to_serialize_size; ++column_index)\n+            {\n+                auto & column = columns[column_index];\n+                temporary_column_data[column_index] = column->serializeValueIntoArena(key_index, temporary_values_pool, block_start);\n+                allocated_size_for_columns += temporary_column_data[column_index].size;\n+            }\n+\n+            char * place_for_serialized_columns = arena.alloc(allocated_size_for_columns);\n+            memcpy(reinterpret_cast<void*>(place_for_serialized_columns), reinterpret_cast<const void*>(block_start), allocated_size_for_columns);\n+\n+            if (it)\n+            {\n+                /// Cell exists need to free previous serialized place and update deadline\n+                auto & cell = it->getMapped();\n+\n+                if (cell.place_for_serialized_columns)\n+                    arena.free(cell.place_for_serialized_columns, cell.allocated_size_for_columns);\n+\n+                setCellDeadline(cell, now);\n+                cell.allocated_size_for_columns = allocated_size_for_columns;\n+                cell.place_for_serialized_columns = place_for_serialized_columns;\n+            }\n+            else\n+            {\n+                /// No cell exists so create and put in cache\n+                Cell cell;\n+\n+                setCellDeadline(cell, now);\n+                cell.allocated_size_for_columns = allocated_size_for_columns;\n+                cell.place_for_serialized_columns = place_for_serialized_columns;\n+\n+                insertCellInCache(key, cell);\n+            }\n+\n+            temporary_values_pool.rollback(allocated_size_for_columns);\n+        }\n+    }\n+\n+    void insertDefaultKeysImpl(const PaddedPODArray<KeyType> & keys)\n+    {\n+        const auto now = std::chrono::system_clock::now();\n+\n+        for (auto key : keys)\n+        {\n+            auto * it = cache.find(key);\n+\n+            if (it)\n+            {\n+                auto & cell = it->getMapped();\n+\n+                setCellDeadline(cell, now);\n+\n+                if (cell.place_for_serialized_columns)\n+                    arena.free(cell.place_for_serialized_columns, cell.allocated_size_for_columns);\n+\n+                cell.allocated_size_for_columns = 0;\n+                cell.place_for_serialized_columns = nullptr;\n+            }\n+            else\n+            {\n+                Cell cell;\n+\n+                setCellDeadline(cell, now);\n+                cell.allocated_size_for_columns = 0;\n+                cell.place_for_serialized_columns = nullptr;\n+\n+                insertCellInCache(key, cell);\n+            }\n+        }\n+    }\n+\n+    PaddedPODArray<KeyType> getCachedKeysImpl() const\n+    {\n+        PaddedPODArray<KeyType> result;\n+        result.reserve(cache.size());\n+\n+        for (auto & node : cache)\n+        {\n+            auto & cell = node.getMapped();\n+\n+            if (cell.isDefault())\n+                continue;\n+\n+            result.emplace_back(node.getKey());\n+        }\n+\n+        return result;\n+    }\n+\n+    using TimePoint = std::chrono::system_clock::time_point;\n+\n+    struct Cell\n+    {\n+        TimePoint deadline;\n+        size_t allocated_size_for_columns;\n+        char * place_for_serialized_columns;\n+\n+        inline bool isDefault() const { return place_for_serialized_columns == nullptr; }\n+        inline void setDefault()\n+        {\n+            place_for_serialized_columns = nullptr;\n+            allocated_size_for_columns = 0;\n+        }\n+    };\n+\n+    void insertCellInCache(KeyType & key, const Cell & cell)\n+    {\n+        if constexpr (dictionary_key_type == DictionaryKeyType::complex)\n+        {\n+            /// Copy complex key into arena and put in cache\n+            size_t key_size = key.size;\n+            char * place_for_key = arena.alloc(key_size);\n+            memcpy(reinterpret_cast<void *>(place_for_key), reinterpret_cast<const void *>(key.data), key_size);\n+            KeyType updated_key{place_for_key, key_size};\n+            key = updated_key;\n+        }\n+\n+        cache.insert(key, cell);\n+    }\n+\n+    inline static bool cellHasDeadline(const Cell & cell)\n+    {\n+        return cell.deadline != std::chrono::system_clock::from_time_t(0);\n+    }\n+\n+    inline void setCellDeadline(Cell & cell, TimePoint now)\n+    {\n+        if (configuration.lifetime.min_sec == 0 && configuration.lifetime.max_sec == 0)\n+        {\n+            cell.deadline = std::chrono::system_clock::from_time_t(0);\n+            return;\n+        }\n+\n+        size_t min_sec_lifetime = configuration.lifetime.min_sec;\n+        size_t max_sec_lifetime = configuration.lifetime.max_sec;\n+\n+        std::uniform_int_distribution<UInt64> distribution{min_sec_lifetime, max_sec_lifetime};\n+        cell.deadline = now + std::chrono::seconds(distribution(rnd_engine));\n+    }\n+\n+    template <typename>\n+    friend class ArenaCellDisposer;\n+\n+    CacheDictionaryStorageConfiguration configuration;\n+\n+    ArenaWithFreeLists arena;\n+\n+    pcg64 rnd_engine;\n+\n+    class ArenaCellDisposer\n+    {\n+    public:\n+        ArenaWithFreeLists & arena;\n+\n+        template <typename Key, typename Value>\n+        void operator()(const Key & key, const Value & value) const\n+        {\n+            /// In case of complex key we keep it in arena\n+            if constexpr (std::is_same_v<Key, StringRef>)\n+                arena.free(const_cast<char *>(key.data), key.size);\n+\n+            if (value.place_for_serialized_columns)\n+                arena.free(value.place_for_serialized_columns, value.allocated_size_for_columns);\n+        }\n+    };\n+\n+    using SimpleKeyLRUHashMap = LRUHashMap<UInt64, Cell, ArenaCellDisposer>;\n+    using ComplexKeyLRUHashMap = LRUHashMapWithSavedHash<StringRef, Cell, ArenaCellDisposer>;\n+\n+    using CacheLRUHashMap = std::conditional_t<\n+        dictionary_key_type == DictionaryKeyType::simple,\n+        SimpleKeyLRUHashMap,\n+        ComplexKeyLRUHashMap>;\n+\n+    CacheLRUHashMap cache;\n+};\n+\n+}\ndiff --git a/src/Dictionaries/CacheDictionaryUpdateQueue.cpp b/src/Dictionaries/CacheDictionaryUpdateQueue.cpp\nnew file mode 100644\nindex 000000000000..eca833f62daf\n--- /dev/null\n+++ b/src/Dictionaries/CacheDictionaryUpdateQueue.cpp\n@@ -0,0 +1,162 @@\n+#include \"CacheDictionaryUpdateQueue.h\"\n+\n+#include <Dictionaries/CacheDictionaryUpdateQueue.h>\n+\n+#include <Common/setThreadName.h>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int CACHE_DICTIONARY_UPDATE_FAIL;\n+    extern const int UNSUPPORTED_METHOD;\n+    extern const int TIMEOUT_EXCEEDED;\n+}\n+\n+template class CacheDictionaryUpdateUnit<DictionaryKeyType::simple>;\n+template class CacheDictionaryUpdateUnit<DictionaryKeyType::complex>;\n+\n+template <DictionaryKeyType dictionary_key_type>\n+CacheDictionaryUpdateQueue<dictionary_key_type>::CacheDictionaryUpdateQueue(\n+    String dictionary_name_for_logs_,\n+    CacheDictionaryUpdateQueueConfiguration configuration_,\n+    UpdateFunction && update_func_)\n+    : dictionary_name_for_logs(std::move(dictionary_name_for_logs_))\n+    , configuration(configuration_)\n+    , update_func(std::move(update_func_))\n+    , update_queue(configuration.max_update_queue_size)\n+    , update_pool(configuration.max_threads_for_updates)\n+{\n+    for (size_t i = 0; i < configuration.max_threads_for_updates; ++i)\n+        update_pool.scheduleOrThrowOnError([this] { updateThreadFunction(); });\n+}\n+\n+template <DictionaryKeyType dictionary_key_type>\n+CacheDictionaryUpdateQueue<dictionary_key_type>::~CacheDictionaryUpdateQueue()\n+{\n+    try {\n+        if (!finished)\n+            stopAndWait();\n+    }\n+    catch (...)\n+    {\n+        /// TODO: Write log\n+    }\n+}\n+\n+template <DictionaryKeyType dictionary_key_type>\n+void CacheDictionaryUpdateQueue<dictionary_key_type>::tryPushToUpdateQueueOrThrow(CacheDictionaryUpdateUnitPtr<dictionary_key_type> & update_unit_ptr)\n+{\n+    if (finished)\n+        throw Exception{\"CacheDictionaryUpdateQueue finished\", ErrorCodes::UNSUPPORTED_METHOD};\n+\n+    if (!update_queue.tryPush(update_unit_ptr, configuration.update_queue_push_timeout_milliseconds))\n+        throw DB::Exception(\n+            ErrorCodes::CACHE_DICTIONARY_UPDATE_FAIL,\n+            \"Cannot push to internal update queue in dictionary {}. \"\n+            \"Timelimit of {} ms. exceeded. Current queue size is {}\",\n+            dictionary_name_for_logs,\n+            std::to_string(configuration.update_queue_push_timeout_milliseconds),\n+            std::to_string(update_queue.size()));\n+}\n+\n+template <DictionaryKeyType dictionary_key_type>\n+void CacheDictionaryUpdateQueue<dictionary_key_type>::waitForCurrentUpdateFinish(CacheDictionaryUpdateUnitPtr<dictionary_key_type> & update_unit_ptr) const\n+{\n+    if (finished)\n+        throw Exception{\"CacheDictionaryUpdateQueue finished\", ErrorCodes::UNSUPPORTED_METHOD};\n+\n+    std::unique_lock<std::mutex> update_lock(update_mutex);\n+\n+    bool result = is_update_finished.wait_for(\n+        update_lock,\n+        std::chrono::milliseconds(configuration.query_wait_timeout_milliseconds),\n+        [&]\n+        {\n+            return update_unit_ptr->is_done || update_unit_ptr->current_exception;\n+        });\n+\n+    if (!result)\n+    {\n+        throw DB::Exception(\n+            ErrorCodes::TIMEOUT_EXCEEDED,\n+            \"Dictionary {} source seems unavailable, because {} ms timeout exceeded.\",\n+            dictionary_name_for_logs,\n+            toString(configuration.query_wait_timeout_milliseconds));\n+    }\n+\n+    if (update_unit_ptr->current_exception)\n+    {\n+        // Don't just rethrow it, because sharing the same exception object\n+        // between multiple threads can lead to weird effects if they decide to\n+        // modify it, for example, by adding some error context.\n+        try\n+        {\n+            std::rethrow_exception(update_unit_ptr->current_exception);\n+        }\n+        catch (...)\n+        {\n+            throw DB::Exception(\n+                ErrorCodes::CACHE_DICTIONARY_UPDATE_FAIL,\n+                \"Update failed for dictionary '{}': {}\",\n+                dictionary_name_for_logs,\n+                getCurrentExceptionMessage(true /*with stack trace*/, true /*check embedded stack trace*/));\n+        }\n+    }\n+}\n+\n+template <DictionaryKeyType dictionary_key_type>\n+void CacheDictionaryUpdateQueue<dictionary_key_type>::stopAndWait()\n+{\n+    finished = true;\n+    update_queue.clear();\n+\n+    for (size_t i = 0; i < configuration.max_threads_for_updates; ++i)\n+    {\n+        auto empty_finishing_ptr = std::make_shared<CacheDictionaryUpdateUnit<dictionary_key_type>>();\n+        update_queue.push(empty_finishing_ptr);\n+    }\n+\n+    update_pool.wait();\n+}\n+\n+template <DictionaryKeyType dictionary_key_type>\n+void CacheDictionaryUpdateQueue<dictionary_key_type>::updateThreadFunction()\n+{\n+    setThreadName(\"UpdQueue\");\n+\n+    while (!finished)\n+    {\n+        CacheDictionaryUpdateUnitPtr<dictionary_key_type> unit_to_update;\n+        update_queue.pop(unit_to_update);\n+\n+        if (finished)\n+            break;\n+\n+        try\n+        {\n+            /// Update\n+            update_func(unit_to_update);\n+\n+            /// Notify thread about finished updating the bunch of ids\n+            /// where their own ids were included.\n+            std::unique_lock<std::mutex> lock(update_mutex);\n+\n+            unit_to_update->is_done = true;\n+            is_update_finished.notify_all();\n+        }\n+        catch (...)\n+        {\n+            std::unique_lock<std::mutex> lock(update_mutex);\n+\n+            unit_to_update->current_exception = std::current_exception(); // NOLINT(bugprone-throw-keyword-missing)\n+            is_update_finished.notify_all();\n+        }\n+    }\n+}\n+\n+template class CacheDictionaryUpdateQueue<DictionaryKeyType::simple>;\n+template class CacheDictionaryUpdateQueue<DictionaryKeyType::complex>;\n+\n+}\ndiff --git a/src/Dictionaries/CacheDictionaryUpdateQueue.h b/src/Dictionaries/CacheDictionaryUpdateQueue.h\nnew file mode 100644\nindex 000000000000..2e636af6db66\n--- /dev/null\n+++ b/src/Dictionaries/CacheDictionaryUpdateQueue.h\n@@ -0,0 +1,172 @@\n+#pragma once\n+\n+#include <atomic>\n+#include <mutex>\n+#include <shared_mutex>\n+#include <utility>\n+#include <vector>\n+#include <functional>\n+\n+#include <Common/ThreadPool.h>\n+#include <Common/ConcurrentBoundedQueue.h>\n+#include <Common/CurrentMetrics.h>\n+#include <Common/PODArray.h>\n+#include <Common/HashTable/HashMap.h>\n+#include <Columns/IColumn.h>\n+#include <Dictionaries/ICacheDictionaryStorage.h>\n+\n+namespace CurrentMetrics\n+{\n+    extern const Metric CacheDictionaryUpdateQueueBatches;\n+    extern const Metric CacheDictionaryUpdateQueueKeys;\n+}\n+\n+namespace DB\n+{\n+\n+/** This class is passed between update queue and update queue client during update.\n+\n+    For simple keys we pass simple keys.\n+\n+    For complex keys we pass complex keys columns and requested rows to update.\n+\n+    During update cache dictionary should fill requested_keys_to_fetched_columns_during_update_index and\n+    fetched_columns_during_update.\n+\n+    For complex key to extend lifetime of key complex key arena should be used.\n+*/\n+template <DictionaryKeyType dictionary_key_type>\n+class CacheDictionaryUpdateUnit\n+{\n+public:\n+    using KeyType = std::conditional_t<dictionary_key_type == DictionaryKeyType::simple, UInt64, StringRef>;\n+\n+    /// Constructor for complex keys update request\n+    explicit CacheDictionaryUpdateUnit(\n+        const Columns & key_columns_,\n+        const PaddedPODArray<KeyState> & key_index_to_state_from_storage_,\n+        const DictionaryStorageFetchRequest & request_,\n+        size_t keys_to_update_size_)\n+        : key_columns(key_columns_)\n+        , key_index_to_state(key_index_to_state_from_storage_.begin(), key_index_to_state_from_storage_.end())\n+        , request(request_)\n+        , keys_to_update_size(keys_to_update_size_)\n+        , alive_keys(CurrentMetrics::CacheDictionaryUpdateQueueKeys, keys_to_update_size)\n+    {}\n+\n+    CacheDictionaryUpdateUnit()\n+        : keys_to_update_size(0)\n+        , alive_keys(CurrentMetrics::CacheDictionaryUpdateQueueKeys, 0)\n+    {}\n+\n+    const Columns key_columns;\n+    const PaddedPODArray<KeyState> key_index_to_state;\n+    const DictionaryStorageFetchRequest request;\n+    const size_t keys_to_update_size;\n+\n+    HashMap<KeyType, size_t> requested_keys_to_fetched_columns_during_update_index;\n+    MutableColumns fetched_columns_during_update;\n+    /// Complex keys are serialized in this arena\n+    Arena complex_key_arena;\n+\n+private:\n+    template <DictionaryKeyType>\n+    friend class CacheDictionaryUpdateQueue;\n+\n+    std::atomic<bool> is_done{false};\n+    std::exception_ptr current_exception{nullptr};\n+\n+    /// While UpdateUnit is alive, it is accounted in update_queue size.\n+    CurrentMetrics::Increment alive_batch{CurrentMetrics::CacheDictionaryUpdateQueueBatches};\n+    CurrentMetrics::Increment alive_keys;\n+};\n+\n+template <DictionaryKeyType dictionary_key_type>\n+using CacheDictionaryUpdateUnitPtr = std::shared_ptr<CacheDictionaryUpdateUnit<dictionary_key_type>>;\n+\n+extern template class CacheDictionaryUpdateUnit<DictionaryKeyType::simple>;\n+extern template class CacheDictionaryUpdateUnit<DictionaryKeyType::complex>;\n+\n+struct CacheDictionaryUpdateQueueConfiguration\n+{\n+    /// Size of update queue\n+    const size_t max_update_queue_size;\n+    /// Size in thead pool of update queue\n+    const size_t max_threads_for_updates;\n+    /// Timeout for trying to push update unit into queue\n+    const size_t update_queue_push_timeout_milliseconds;\n+    /// Timeout during sync waititing of update unit\n+    const size_t query_wait_timeout_milliseconds;\n+};\n+\n+/** Responsibility of this class is to provide asynchronous and synchronous update support for CacheDictionary\n+\n+    It is responsibility of CacheDictionary to perform update with UpdateUnit using UpdateFunction.\n+*/\n+template <DictionaryKeyType dictionary_key_type>\n+class CacheDictionaryUpdateQueue\n+{\n+public:\n+    /// Client of update queue must provide this function in constructor and perform update using update unit.\n+    using UpdateFunction = std::function<void (CacheDictionaryUpdateUnitPtr<dictionary_key_type>)>;\n+    static_assert(dictionary_key_type != DictionaryKeyType::range, \"Range key type is not supported by CacheDictionaryUpdateQueue\");\n+\n+    CacheDictionaryUpdateQueue(\n+        String dictionary_name_for_logs_,\n+        CacheDictionaryUpdateQueueConfiguration configuration_,\n+        UpdateFunction && update_func_);\n+\n+    ~CacheDictionaryUpdateQueue();\n+\n+    /// Get configuration that was passed to constructor\n+    const CacheDictionaryUpdateQueueConfiguration & getConfiguration() const { return configuration; }\n+\n+    /// Is queue finished\n+    bool isFinished() const { return finished; }\n+\n+    /// Synchronous wait for update queue to stop\n+    void stopAndWait();\n+\n+    /** Try to add update unit into queue.\n+\n+        If queue is full and oush cannot be performed in update_queue_push_timeout_milliseconds from configuration\n+        an exception will be thrown.\n+\n+        If queue already finished an exception will be thrown.\n+    */\n+    void tryPushToUpdateQueueOrThrow(CacheDictionaryUpdateUnitPtr<dictionary_key_type> & update_unit_ptr);\n+\n+    /** Try to synchronously wait for update completion.\n+\n+        If exception was passed from update function during update it will be rethrowed.\n+\n+        If update will not be finished in query_wait_timeout_milliseconds from configuration\n+        an exception will be thrown.\n+\n+        If queue already finished an exception will be thrown.\n+    */\n+    void waitForCurrentUpdateFinish(CacheDictionaryUpdateUnitPtr<dictionary_key_type> & update_unit_ptr) const;\n+\n+private:\n+    void updateThreadFunction();\n+\n+    using UpdateQueue = ConcurrentBoundedQueue<CacheDictionaryUpdateUnitPtr<dictionary_key_type>>;\n+\n+    String dictionary_name_for_logs;\n+\n+    CacheDictionaryUpdateQueueConfiguration configuration;\n+    UpdateFunction update_func;\n+\n+    UpdateQueue update_queue;\n+    ThreadPool update_pool;\n+\n+    mutable std::mutex update_mutex;\n+    mutable std::condition_variable is_update_finished;\n+\n+    std::atomic<bool> finished{false};\n+};\n+\n+extern template class CacheDictionaryUpdateQueue<DictionaryKeyType::simple>;\n+extern template class CacheDictionaryUpdateQueue<DictionaryKeyType::complex>;\n+\n+}\ndiff --git a/src/Dictionaries/ComplexKeyCacheDictionary.cpp b/src/Dictionaries/ComplexKeyCacheDictionary.cpp\ndeleted file mode 100644\nindex cbb57f817937..000000000000\n--- a/src/Dictionaries/ComplexKeyCacheDictionary.cpp\n+++ /dev/null\n@@ -1,915 +0,0 @@\n-#include \"ComplexKeyCacheDictionary.h\"\n-#include <Common/Arena.h>\n-#include <Common/BitHelpers.h>\n-#include <Common/CurrentMetrics.h>\n-#include <Common/ProfileEvents.h>\n-#include <Common/ProfilingScopedRWLock.h>\n-#include <Common/Stopwatch.h>\n-#include <Common/randomSeed.h>\n-#include <ext/map.h>\n-#include <ext/range.h>\n-#include \"DictionaryBlockInputStream.h\"\n-#include \"DictionaryFactory.h\"\n-#include <Functions/FunctionHelpers.h>\n-#include <DataTypes/DataTypesDecimal.h>\n-\n-namespace ProfileEvents\n-{\n-extern const Event DictCacheKeysRequested;\n-extern const Event DictCacheKeysRequestedMiss;\n-extern const Event DictCacheKeysRequestedFound;\n-extern const Event DictCacheKeysExpired;\n-extern const Event DictCacheKeysNotFound;\n-extern const Event DictCacheKeysHit;\n-extern const Event DictCacheRequestTimeNs;\n-extern const Event DictCacheLockWriteNs;\n-extern const Event DictCacheLockReadNs;\n-}\n-\n-namespace CurrentMetrics\n-{\n-extern const Metric DictCacheRequests;\n-}\n-\n-\n-namespace DB\n-{\n-namespace ErrorCodes\n-{\n-    extern const int TYPE_MISMATCH;\n-    extern const int BAD_ARGUMENTS;\n-    extern const int UNSUPPORTED_METHOD;\n-    extern const int TOO_SMALL_BUFFER_SIZE;\n-}\n-\n-\n-inline UInt64 ComplexKeyCacheDictionary::getCellIdx(const StringRef key) const\n-{\n-    const auto hash = StringRefHash{}(key);\n-    const auto idx = hash & size_overlap_mask;\n-    return idx;\n-}\n-\n-\n-ComplexKeyCacheDictionary::ComplexKeyCacheDictionary(\n-    const StorageID & dict_id_,\n-    const DictionaryStructure & dict_struct_,\n-    DictionarySourcePtr source_ptr_,\n-    const DictionaryLifetime dict_lifetime_,\n-    const size_t size_)\n-    : IDictionaryBase(dict_id_)\n-    , dict_struct(dict_struct_)\n-    , source_ptr{std::move(source_ptr_)}\n-    , dict_lifetime(dict_lifetime_)\n-    , size{roundUpToPowerOfTwoOrZero(std::max(size_, size_t(max_collision_length)))}\n-    , size_overlap_mask{this->size - 1}\n-    , rnd_engine(randomSeed())\n-{\n-    if (!this->source_ptr->supportsSelectiveLoad())\n-        throw Exception{full_name + \": source cannot be used with ComplexKeyCacheDictionary\", ErrorCodes::UNSUPPORTED_METHOD};\n-\n-    createAttributes();\n-}\n-\n-ColumnPtr ComplexKeyCacheDictionary::getColumn(\n-    const std::string & attribute_name,\n-    const DataTypePtr & result_type,\n-    const Columns & key_columns,\n-    const DataTypes & key_types,\n-    const ColumnPtr default_values_column) const\n-{\n-    dict_struct.validateKeyTypes(key_types);\n-\n-    ColumnPtr result;\n-\n-    auto & attribute = getAttribute(attribute_name);\n-    const auto & dictionary_attribute = dict_struct.getAttribute(attribute_name, result_type);\n-\n-    auto keys_size = key_columns.front()->size();\n-\n-    auto type_call = [&](const auto &dictionary_attribute_type)\n-    {\n-        using Type = std::decay_t<decltype(dictionary_attribute_type)>;\n-        using AttributeType = typename Type::AttributeType;\n-        using ColumnProvider = DictionaryAttributeColumnProvider<AttributeType>;\n-\n-        const auto & null_value = std::get<AttributeType>(attribute.null_values);\n-        DictionaryDefaultValueExtractor<AttributeType> default_value_extractor(null_value, default_values_column);\n-\n-        auto column = ColumnProvider::getColumn(dictionary_attribute, keys_size);\n-\n-        if constexpr (std::is_same_v<AttributeType, String>)\n-        {\n-            auto * out = column.get();\n-            getItemsString(attribute, key_columns, out, default_value_extractor);\n-        }\n-        else\n-        {\n-            auto & out = column->getData();\n-            getItemsNumberImpl<AttributeType, AttributeType>(attribute, key_columns, out, default_value_extractor);\n-        }\n-\n-        result = std::move(column);\n-    };\n-\n-    callOnDictionaryAttributeType(attribute.type, type_call);\n-\n-    return result;\n-}\n-\n-/// returns cell_idx (always valid for replacing), 'cell is valid' flag, 'cell is outdated' flag,\n-/// true  false   found and valid\n-/// false true    not found (something outdated, maybe our cell)\n-/// false false   not found (other id stored with valid data)\n-/// true  true    impossible\n-///\n-/// todo: split this func to two: find_for_get and find_for_set\n-ComplexKeyCacheDictionary::FindResult\n-ComplexKeyCacheDictionary::findCellIdx(const StringRef & key, const CellMetadata::time_point_t now, const size_t hash) const\n-{\n-    auto pos = hash;\n-    auto oldest_id = pos;\n-    auto oldest_time = CellMetadata::time_point_t::max();\n-    const auto stop = pos + max_collision_length;\n-\n-    for (; pos < stop; ++pos)\n-    {\n-        const auto cell_idx = pos & size_overlap_mask;\n-        const auto & cell = cells[cell_idx];\n-\n-        if (cell.hash != hash || cell.key != key)\n-        {\n-            /// maybe we already found nearest expired cell\n-            if (oldest_time > now && oldest_time > cell.expiresAt())\n-            {\n-                oldest_time = cell.expiresAt();\n-                oldest_id = cell_idx;\n-            }\n-\n-            continue;\n-        }\n-\n-        if (cell.expiresAt() < now)\n-        {\n-            return {cell_idx, false, true};\n-        }\n-\n-        return {cell_idx, true, false};\n-    }\n-\n-    oldest_id &= size_overlap_mask;\n-    return {oldest_id, false, false};\n-}\n-\n-ColumnUInt8::Ptr ComplexKeyCacheDictionary::hasKeys(const Columns & key_columns, const DataTypes & key_types) const\n-{\n-    dict_struct.validateKeyTypes(key_types);\n-\n-    const auto rows_num = key_columns.front()->size();\n-\n-    auto result = ColumnUInt8::create(rows_num);\n-    auto& out = result->getData();\n-\n-    for (const auto row : ext::range(0, rows_num))\n-        out[row] = false;\n-\n-    /// Mapping: <key> -> { all indices `i` of `key_columns` such that `key_columns[i]` = <key> }\n-    MapType<std::vector<size_t>> outdated_keys;\n-\n-    const auto keys_size = dict_struct.key->size();\n-    StringRefs keys(keys_size);\n-    Arena temporary_keys_pool;\n-    PODArray<StringRef> keys_array(rows_num);\n-\n-    size_t cache_expired = 0, cache_not_found = 0, cache_hit = 0;\n-    {\n-        const ProfilingScopedReadRWLock read_lock{rw_lock, ProfileEvents::DictCacheLockReadNs};\n-\n-        const auto now = std::chrono::system_clock::now();\n-        /// fetch up-to-date values, decide which ones require update\n-        for (const auto row : ext::range(0, rows_num))\n-        {\n-            const StringRef key = placeKeysInPool(row, key_columns, keys, *dict_struct.key, temporary_keys_pool);\n-            keys_array[row] = key;\n-            const auto find_result = findCellIdx(key, now);\n-            const auto & cell_idx = find_result.cell_idx;\n-            /** cell should be updated if either:\n-                *    1. keys (or hash) do not match,\n-                *    2. cell has expired,\n-                *    3. explicit defaults were specified and cell was set default. */\n-            if (!find_result.valid)\n-            {\n-                outdated_keys[key].push_back(row);\n-                if (find_result.outdated)\n-                    ++cache_expired;\n-                else\n-                    ++cache_not_found;\n-            }\n-            else\n-            {\n-                ++cache_hit;\n-                const auto & cell = cells[cell_idx];\n-                out[row] = !cell.isDefault();\n-            }\n-        }\n-    }\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysExpired, cache_expired);\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysNotFound, cache_not_found);\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysHit, cache_hit);\n-\n-    query_count.fetch_add(rows_num, std::memory_order_relaxed);\n-    hit_count.fetch_add(rows_num - outdated_keys.size(), std::memory_order_release);\n-\n-    if (outdated_keys.empty())\n-        return result;\n-\n-    std::vector<size_t> required_rows(outdated_keys.size());\n-    std::transform(\n-        std::begin(outdated_keys), std::end(outdated_keys), std::begin(required_rows), [](auto & pair) { return pair.getMapped().front(); });\n-\n-    /// request new values\n-    update(\n-        key_columns,\n-        keys_array,\n-        required_rows,\n-        [&](const StringRef key, const auto)\n-        {\n-            for (const auto out_idx : outdated_keys[key])\n-                out[out_idx] = true;\n-        },\n-        [&](const StringRef key, const auto)\n-        {\n-            for (const auto out_idx : outdated_keys[key])\n-                out[out_idx] = false;\n-        });\n-\n-    return result;\n-}\n-\n-\n-template <typename AttributeType, typename OutputType, typename DefaultValueExtractor>\n-void ComplexKeyCacheDictionary::getItemsNumberImpl(\n-    Attribute & attribute,\n-    const Columns & key_columns,\n-    PaddedPODArray<OutputType> & out,\n-    DefaultValueExtractor & default_value_extractor) const\n-{\n-    /// Mapping: <key> -> { all indices `i` of `key_columns` such that `key_columns[i]` = <key> }\n-    MapType<std::vector<size_t>> outdated_keys;\n-    auto & attribute_array = std::get<ContainerPtrType<AttributeType>>(attribute.arrays);\n-\n-    const auto rows_num = key_columns.front()->size();\n-    const auto keys_size = dict_struct.key->size();\n-    StringRefs keys(keys_size);\n-    Arena temporary_keys_pool;\n-    PODArray<StringRef> keys_array(rows_num);\n-\n-    size_t cache_expired = 0, cache_not_found = 0, cache_hit = 0;\n-    {\n-        const ProfilingScopedReadRWLock read_lock{rw_lock, ProfileEvents::DictCacheLockReadNs};\n-\n-        const auto now = std::chrono::system_clock::now();\n-        /// fetch up-to-date values, decide which ones require update\n-        for (const auto row : ext::range(0, rows_num))\n-        {\n-            const StringRef key = placeKeysInPool(row, key_columns, keys, *dict_struct.key, temporary_keys_pool);\n-            keys_array[row] = key;\n-            const auto find_result = findCellIdx(key, now);\n-\n-            /** cell should be updated if either:\n-                *    1. keys (or hash) do not match,\n-                *    2. cell has expired,\n-                *    3. explicit defaults were specified and cell was set default. */\n-\n-            if (!find_result.valid)\n-            {\n-                outdated_keys[key].push_back(row);\n-                if (find_result.outdated)\n-                    ++cache_expired;\n-                else\n-                    ++cache_not_found;\n-            }\n-            else\n-            {\n-                ++cache_hit;\n-                const auto & cell_idx = find_result.cell_idx;\n-                const auto & cell = cells[cell_idx];\n-                out[row] = cell.isDefault() ? default_value_extractor[row] : static_cast<OutputType>(attribute_array[cell_idx]);\n-            }\n-        }\n-    }\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysExpired, cache_expired);\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysNotFound, cache_not_found);\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysHit, cache_hit);\n-    query_count.fetch_add(rows_num, std::memory_order_relaxed);\n-    hit_count.fetch_add(rows_num - outdated_keys.size(), std::memory_order_release);\n-\n-    if (outdated_keys.empty())\n-        return;\n-\n-    std::vector<size_t> required_rows(outdated_keys.size());\n-    std::transform(std::begin(outdated_keys), std::end(outdated_keys), std::begin(required_rows), [](auto & pair)\n-    {\n-        return pair.getMapped().front();\n-    });\n-\n-    /// request new values\n-    update(\n-        key_columns,\n-        keys_array,\n-        required_rows,\n-        [&](const StringRef key, const size_t cell_idx)\n-        {\n-            for (const auto row : outdated_keys[key])\n-                out[row] = static_cast<OutputType>(attribute_array[cell_idx]);\n-        },\n-        [&](const StringRef key, const size_t)\n-        {\n-            for (const auto row : outdated_keys[key])\n-                out[row] = default_value_extractor[row];\n-        });\n-}\n-\n-void ComplexKeyCacheDictionary::getItemsString(\n-    Attribute & attribute,\n-    const Columns & key_columns,\n-    ColumnString * out,\n-    DictionaryDefaultValueExtractor<String> & default_value_extractor) const\n-{\n-    const auto rows_num = key_columns.front()->size();\n-    /// save on some allocations\n-    out->getOffsets().reserve(rows_num);\n-\n-    const auto keys_size = dict_struct.key->size();\n-    StringRefs keys(keys_size);\n-    Arena temporary_keys_pool;\n-\n-    auto & attribute_array = std::get<ContainerPtrType<StringRef>>(attribute.arrays);\n-\n-    auto found_outdated_values = false;\n-\n-    /// perform optimistic version, fallback to pessimistic if failed\n-    {\n-        const ProfilingScopedReadRWLock read_lock{rw_lock, ProfileEvents::DictCacheLockReadNs};\n-\n-        const auto now = std::chrono::system_clock::now();\n-        /// fetch up-to-date values, discard on fail\n-        for (const auto row : ext::range(0, rows_num))\n-        {\n-            const StringRef key = placeKeysInPool(row, key_columns, keys, *dict_struct.key, temporary_keys_pool);\n-            SCOPE_EXIT(temporary_keys_pool.rollback(key.size));\n-            const auto find_result = findCellIdx(key, now);\n-\n-            if (!find_result.valid)\n-            {\n-                found_outdated_values = true;\n-                break;\n-            }\n-            else\n-            {\n-                const auto & cell_idx = find_result.cell_idx;\n-                const auto & cell = cells[cell_idx];\n-                const auto string_ref = cell.isDefault() ? default_value_extractor[row] : attribute_array[cell_idx];\n-                out->insertData(string_ref.data, string_ref.size);\n-            }\n-        }\n-    }\n-\n-    /// optimistic code completed successfully\n-    if (!found_outdated_values)\n-    {\n-        query_count.fetch_add(rows_num, std::memory_order_relaxed);\n-        hit_count.fetch_add(rows_num, std::memory_order_release);\n-        return;\n-    }\n-\n-    /// now onto the pessimistic one, discard possible partial results from the optimistic path\n-    out->getChars().resize_assume_reserved(0);\n-    out->getOffsets().resize_assume_reserved(0);\n-\n-    /// Mapping: <key> -> { all indices `i` of `key_columns` such that `key_columns[i]` = <key> }\n-    MapType<std::vector<size_t>> outdated_keys;\n-    /// we are going to store every string separately\n-    MapType<StringRef> map;\n-    PODArray<StringRef> keys_array(rows_num);\n-\n-    size_t total_length = 0;\n-    size_t cache_expired = 0, cache_not_found = 0, cache_hit = 0;\n-    {\n-        const ProfilingScopedReadRWLock read_lock{rw_lock, ProfileEvents::DictCacheLockReadNs};\n-\n-        const auto now = std::chrono::system_clock::now();\n-        for (const auto row : ext::range(0, rows_num))\n-        {\n-            const StringRef key = placeKeysInPool(row, key_columns, keys, *dict_struct.key, temporary_keys_pool);\n-            keys_array[row] = key;\n-            const auto find_result = findCellIdx(key, now);\n-\n-            if (!find_result.valid)\n-            {\n-                outdated_keys[key].push_back(row);\n-                if (find_result.outdated)\n-                    ++cache_expired;\n-                else\n-                    ++cache_not_found;\n-            }\n-            else\n-            {\n-                ++cache_hit;\n-                const auto & cell_idx = find_result.cell_idx;\n-                const auto & cell = cells[cell_idx];\n-                const auto string_ref = cell.isDefault() ? default_value_extractor[row] : attribute_array[cell_idx];\n-\n-                if (!cell.isDefault())\n-                    map[key] = copyIntoArena(string_ref, temporary_keys_pool);\n-\n-                total_length += string_ref.size + 1;\n-            }\n-        }\n-    }\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysExpired, cache_expired);\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysNotFound, cache_not_found);\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysHit, cache_hit);\n-\n-    query_count.fetch_add(rows_num, std::memory_order_relaxed);\n-    hit_count.fetch_add(rows_num - outdated_keys.size(), std::memory_order_release);\n-\n-    /// request new values\n-    if (!outdated_keys.empty())\n-    {\n-        std::vector<size_t> required_rows(outdated_keys.size());\n-        std::transform(std::begin(outdated_keys), std::end(outdated_keys), std::begin(required_rows), [](auto & pair)\n-        {\n-            return pair.getMapped().front();\n-        });\n-\n-        update(\n-            key_columns,\n-            keys_array,\n-            required_rows,\n-            [&](const StringRef key, const size_t cell_idx)\n-            {\n-                const StringRef attribute_value = attribute_array[cell_idx];\n-\n-                /// We must copy key and value to own memory, because it may be replaced with another\n-                ///  in next iterations of inner loop of update.\n-                const StringRef copied_key = copyIntoArena(key, temporary_keys_pool);\n-                const StringRef copied_value = copyIntoArena(attribute_value, temporary_keys_pool);\n-\n-                map[copied_key] = copied_value;\n-                total_length += (attribute_value.size + 1) * outdated_keys[key].size();\n-            },\n-            [&](const StringRef key, const size_t)\n-            {\n-                for (const auto row : outdated_keys[key])\n-                    total_length += default_value_extractor[row].size + 1;\n-            });\n-    }\n-\n-    out->getChars().reserve(total_length);\n-\n-    for (const auto row : ext::range(0, ext::size(keys_array)))\n-    {\n-        const StringRef key = keys_array[row];\n-        auto * const it = map.find(key);\n-        const auto string_ref = it ? it->getMapped() : default_value_extractor[row];\n-        out->insertData(string_ref.data, string_ref.size);\n-    }\n-}\n-\n-template <typename PresentKeyHandler, typename AbsentKeyHandler>\n-void ComplexKeyCacheDictionary::update(\n-    const Columns & in_key_columns,\n-    const PODArray<StringRef> & in_keys,\n-    const std::vector<size_t> & in_requested_rows,\n-    PresentKeyHandler && on_cell_updated,\n-    AbsentKeyHandler && on_key_not_found) const\n-{\n-    MapType<bool> remaining_keys{in_requested_rows.size()};\n-    for (const auto row : in_requested_rows)\n-        remaining_keys.insert({in_keys[row], false});\n-\n-    std::uniform_int_distribution<UInt64> distribution(dict_lifetime.min_sec, dict_lifetime.max_sec);\n-\n-    const ProfilingScopedWriteRWLock write_lock{rw_lock, ProfileEvents::DictCacheLockWriteNs};\n-    {\n-        Stopwatch watch;\n-        auto stream = source_ptr->loadKeys(in_key_columns, in_requested_rows);\n-        stream->readPrefix();\n-\n-        const auto keys_size = dict_struct.key->size();\n-        StringRefs keys(keys_size);\n-\n-        const auto attributes_size = attributes.size();\n-        const auto now = std::chrono::system_clock::now();\n-\n-        while (const auto block = stream->read())\n-        {\n-            /// cache column pointers\n-            const auto key_columns = ext::map<Columns>(\n-                ext::range(0, keys_size), [&](const size_t attribute_idx) { return block.safeGetByPosition(attribute_idx).column; });\n-\n-            const auto attribute_columns = ext::map<Columns>(ext::range(0, attributes_size), [&](const size_t attribute_idx)\n-            {\n-                return block.safeGetByPosition(keys_size + attribute_idx).column;\n-            });\n-\n-            const auto rows_num = block.rows();\n-\n-            for (const auto row : ext::range(0, rows_num))\n-            {\n-                auto key = allocKey(row, key_columns, keys);\n-                const auto hash = StringRefHash{}(key);\n-                const auto find_result = findCellIdx(key, now, hash);\n-                const auto & cell_idx = find_result.cell_idx;\n-                auto & cell = cells[cell_idx];\n-\n-                for (const auto attribute_idx : ext::range(0, attributes.size()))\n-                {\n-                    const auto & attribute_column = *attribute_columns[attribute_idx];\n-                    auto & attribute = attributes[attribute_idx];\n-\n-                    setAttributeValue(attribute, cell_idx, attribute_column[row]);\n-                }\n-\n-                /// if cell id is zero and zero does not map to this cell, then the cell is unused\n-                if (cell.key == StringRef{} && cell_idx != zero_cell_idx)\n-                    element_count.fetch_add(1, std::memory_order_relaxed);\n-\n-                /// handle memory allocated for old key\n-                if (key == cell.key)\n-                {\n-                    freeKey(key);\n-                    key = cell.key;\n-                }\n-                else\n-                {\n-                    /// new key is different from the old one\n-                    if (cell.key.data)\n-                        freeKey(cell.key);\n-\n-                    cell.key = key;\n-                }\n-\n-                cell.hash = hash;\n-\n-                if (dict_lifetime.min_sec != 0 && dict_lifetime.max_sec != 0)\n-                    cell.setExpiresAt(std::chrono::system_clock::now() + std::chrono::seconds{distribution(rnd_engine)});\n-                else\n-                    cell.setExpiresAt(std::chrono::time_point<std::chrono::system_clock>::max());\n-\n-                /// inform caller\n-                on_cell_updated(key, cell_idx);\n-                /// mark corresponding id as found\n-                remaining_keys[key] = true;\n-            }\n-        }\n-\n-        stream->readSuffix();\n-\n-        ProfileEvents::increment(ProfileEvents::DictCacheKeysRequested, in_requested_rows.size());\n-        ProfileEvents::increment(ProfileEvents::DictCacheRequestTimeNs, watch.elapsed());\n-    }\n-\n-    size_t found_num = 0;\n-    size_t not_found_num = 0;\n-\n-    const auto now = std::chrono::system_clock::now();\n-\n-    /// Check which ids have not been found and require setting null_value\n-    for (const auto & key_found_pair : remaining_keys)\n-    {\n-        if (key_found_pair.getMapped())\n-        {\n-            ++found_num;\n-            continue;\n-        }\n-\n-        ++not_found_num;\n-\n-        auto key = key_found_pair.getKey();\n-        const auto hash = StringRefHash{}(key);\n-        const auto find_result = findCellIdx(key, now, hash);\n-        const auto & cell_idx = find_result.cell_idx;\n-        auto & cell = cells[cell_idx];\n-\n-        /// Set null_value for each attribute\n-        for (auto & attribute : attributes)\n-            setDefaultAttributeValue(attribute, cell_idx);\n-\n-        /// Check if cell had not been occupied before and increment element counter if it hadn't\n-        if (cell.key == StringRef{} && cell_idx != zero_cell_idx)\n-            element_count.fetch_add(1, std::memory_order_relaxed);\n-\n-        if (key == cell.key)\n-            key = cell.key;\n-        else\n-        {\n-            if (cell.key.data)\n-                freeKey(cell.key);\n-\n-            /// copy key from temporary pool\n-            key = copyKey(key);\n-            cell.key = key;\n-        }\n-\n-        cell.hash = hash;\n-\n-        if (dict_lifetime.min_sec != 0 && dict_lifetime.max_sec != 0)\n-            cell.setExpiresAt(std::chrono::system_clock::now() + std::chrono::seconds{distribution(rnd_engine)});\n-        else\n-            cell.setExpiresAt(std::chrono::time_point<std::chrono::system_clock>::max());\n-\n-        cell.setDefault();\n-\n-        /// inform caller that the cell has not been found\n-        on_key_not_found(key, cell_idx);\n-    }\n-\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysRequestedFound, found_num);\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysRequestedMiss, not_found_num);\n-}\n-\n-\n-void ComplexKeyCacheDictionary::createAttributes()\n-{\n-    const auto attributes_size = dict_struct.attributes.size();\n-    attributes.reserve(attributes_size);\n-\n-    bytes_allocated += size * sizeof(CellMetadata);\n-    bytes_allocated += attributes_size * sizeof(attributes.front());\n-\n-    for (const auto & attribute : dict_struct.attributes)\n-    {\n-        attribute_index_by_name.emplace(attribute.name, attributes.size());\n-        attributes.push_back(createAttributeWithType(attribute.underlying_type, attribute.null_value));\n-\n-        if (attribute.hierarchical)\n-            throw Exception{full_name + \": hierarchical attributes not supported for dictionary of type \" + getTypeName(),\n-                            ErrorCodes::TYPE_MISMATCH};\n-    }\n-}\n-\n-ComplexKeyCacheDictionary::Attribute & ComplexKeyCacheDictionary::getAttribute(const std::string & attribute_name) const\n-{\n-    const auto it = attribute_index_by_name.find(attribute_name);\n-    if (it == std::end(attribute_index_by_name))\n-        throw Exception{full_name + \": no such attribute '\" + attribute_name + \"'\", ErrorCodes::BAD_ARGUMENTS};\n-\n-    return attributes[it->second];\n-}\n-\n-void ComplexKeyCacheDictionary::setDefaultAttributeValue(Attribute & attribute, const size_t idx) const\n-{\n-    auto type_call = [&](const auto &dictionary_attribute_type)\n-    {\n-        using Type = std::decay_t<decltype(dictionary_attribute_type)>;\n-        using AttributeType = typename Type::AttributeType;\n-\n-        if constexpr (std::is_same_v<AttributeType, String>)\n-        {\n-            const auto & null_value_ref = std::get<String>(attribute.null_values);\n-            auto & string_ref = std::get<ContainerPtrType<StringRef>>(attribute.arrays)[idx];\n-\n-            if (string_ref.data != null_value_ref.data())\n-            {\n-                if (string_ref.data)\n-                    string_arena->free(const_cast<char *>(string_ref.data), string_ref.size);\n-\n-                string_ref = StringRef{null_value_ref};\n-            }\n-        }\n-        else\n-        {\n-            std::get<ContainerPtrType<AttributeType>>(attribute.arrays)[idx] = std::get<AttributeType>(attribute.null_values);\n-        }\n-    };\n-\n-    callOnDictionaryAttributeType(attribute.type, type_call);\n-}\n-\n-ComplexKeyCacheDictionary::Attribute\n-ComplexKeyCacheDictionary::createAttributeWithType(const AttributeUnderlyingType type, const Field & null_value)\n-{\n-    Attribute attr{type, {}, {}};\n-\n-    auto type_call = [&](const auto &dictionary_attribute_type)\n-    {\n-        using Type = std::decay_t<decltype(dictionary_attribute_type)>;\n-        using AttributeType = typename Type::AttributeType;\n-\n-        if constexpr (std::is_same_v<AttributeType, String>)\n-        {\n-            attr.null_values = null_value.get<String>();\n-            attr.arrays = std::make_unique<ContainerType<StringRef>>(size);\n-            bytes_allocated += size * sizeof(StringRef);\n-            if (!string_arena)\n-                string_arena = std::make_unique<ArenaWithFreeLists>();\n-        }\n-        else\n-        {\n-            attr.null_values = AttributeType(null_value.get<NearestFieldType<AttributeType>>()); /* NOLINT */\n-            attr.arrays = std::make_unique<ContainerType<AttributeType>>(size); /* NOLINT */\n-            bytes_allocated += size * sizeof(AttributeType);\n-        }\n-    };\n-\n-    callOnDictionaryAttributeType(type, type_call);\n-\n-    return attr;\n-}\n-\n-void ComplexKeyCacheDictionary::setAttributeValue(Attribute & attribute, const size_t idx, const Field & value) const\n-{\n-    auto type_call = [&](const auto &dictionary_attribute_type)\n-    {\n-        using Type = std::decay_t<decltype(dictionary_attribute_type)>;\n-        using AttributeType = typename Type::AttributeType;\n-\n-        if constexpr (std::is_same_v<AttributeType, String>)\n-        {\n-            const auto & string = value.get<String>();\n-            auto & string_ref = std::get<ContainerPtrType<StringRef>>(attribute.arrays)[idx];\n-            const auto & null_value_ref = std::get<String>(attribute.null_values);\n-\n-            /// free memory unless it points to a null_value\n-            if (string_ref.data && string_ref.data != null_value_ref.data())\n-                string_arena->free(const_cast<char *>(string_ref.data), string_ref.size);\n-\n-            const auto str_size = string.size();\n-            if (str_size != 0)\n-            {\n-                auto * str_ptr = string_arena->alloc(str_size);\n-                std::copy(string.data(), string.data() + str_size, str_ptr);\n-                string_ref = StringRef{str_ptr, str_size};\n-            }\n-            else\n-                string_ref = {};\n-        }\n-        else\n-        {\n-            std::get<ContainerPtrType<AttributeType>>(attribute.arrays)[idx] = value.get<NearestFieldType<AttributeType>>();\n-        }\n-    };\n-\n-    callOnDictionaryAttributeType(attribute.type, type_call);\n-}\n-\n-StringRef ComplexKeyCacheDictionary::allocKey(const size_t row, const Columns & key_columns, StringRefs & keys) const\n-{\n-    if (key_size_is_fixed)\n-        return placeKeysInFixedSizePool(row, key_columns);\n-\n-    return placeKeysInPool(row, key_columns, keys, *dict_struct.key, *keys_pool);\n-}\n-\n-void ComplexKeyCacheDictionary::freeKey(const StringRef key) const\n-{\n-    if (key_size_is_fixed)\n-        fixed_size_keys_pool->free(const_cast<char *>(key.data));\n-    else\n-        keys_pool->free(const_cast<char *>(key.data), key.size);\n-}\n-\n-template <typename Pool>\n-StringRef ComplexKeyCacheDictionary::placeKeysInPool(\n-    const size_t row, const Columns & key_columns, StringRefs & keys, const std::vector<DictionaryAttribute> & key_attributes, Pool & pool)\n-{\n-    const auto keys_size = key_columns.size();\n-    size_t sum_keys_size{};\n-\n-    for (size_t j = 0; j < keys_size; ++j)\n-    {\n-        keys[j] = key_columns[j]->getDataAt(row);\n-        sum_keys_size += keys[j].size;\n-        if (key_attributes[j].underlying_type == AttributeUnderlyingType::utString)\n-            sum_keys_size += sizeof(size_t) + 1;\n-    }\n-\n-    auto place = pool.alloc(sum_keys_size);\n-\n-    auto key_start = place;\n-    for (size_t j = 0; j < keys_size; ++j)\n-    {\n-        if (key_attributes[j].underlying_type == AttributeUnderlyingType::utString)\n-        {\n-            auto start = key_start;\n-            auto key_size = keys[j].size + 1;\n-            memcpy(key_start, &key_size, sizeof(size_t));\n-            key_start += sizeof(size_t);\n-            memcpy(key_start, keys[j].data, keys[j].size);\n-            key_start += keys[j].size;\n-            *key_start = '\\0';\n-            ++key_start;\n-            keys[j].data = start;\n-            keys[j].size += sizeof(size_t) + 1;\n-        }\n-        else\n-        {\n-            memcpy(key_start, keys[j].data, keys[j].size);\n-            keys[j].data = key_start;\n-            key_start += keys[j].size;\n-        }\n-    }\n-\n-    return {place, sum_keys_size};\n-}\n-\n-/// Explicit instantiations.\n-\n-template StringRef ComplexKeyCacheDictionary::placeKeysInPool<Arena>(\n-    const size_t row,\n-    const Columns & key_columns,\n-    StringRefs & keys,\n-    const std::vector<DictionaryAttribute> & key_attributes,\n-    Arena & pool);\n-\n-template StringRef ComplexKeyCacheDictionary::placeKeysInPool<ArenaWithFreeLists>(\n-    const size_t row,\n-    const Columns & key_columns,\n-    StringRefs & keys,\n-    const std::vector<DictionaryAttribute> & key_attributes,\n-    ArenaWithFreeLists & pool);\n-\n-\n-StringRef ComplexKeyCacheDictionary::placeKeysInFixedSizePool(const size_t row, const Columns & key_columns) const\n-{\n-    auto * res = fixed_size_keys_pool->alloc();\n-    auto * place = res;\n-\n-    for (const auto & key_column : key_columns)\n-    {\n-        const StringRef key = key_column->getDataAt(row);\n-        memcpy(place, key.data, key.size);\n-        place += key.size;\n-    }\n-\n-    return {res, key_size};\n-}\n-\n-StringRef ComplexKeyCacheDictionary::copyIntoArena(StringRef src, Arena & arena)\n-{\n-    char * allocated = arena.alloc(src.size);\n-    memcpy(allocated, src.data, src.size);\n-    return {allocated, src.size};\n-}\n-\n-StringRef ComplexKeyCacheDictionary::copyKey(const StringRef key) const\n-{\n-    auto * res = key_size_is_fixed ? fixed_size_keys_pool->alloc() : keys_pool->alloc(key.size);\n-    memcpy(res, key.data, key.size);\n-\n-    return {res, key.size};\n-}\n-\n-bool ComplexKeyCacheDictionary::isEmptyCell(const UInt64 idx) const\n-{\n-    return (\n-        cells[idx].key == StringRef{}\n-        && (idx != zero_cell_idx || cells[idx].data == ext::safe_bit_cast<CellMetadata::time_point_urep_t>(CellMetadata::time_point_t())));\n-}\n-\n-BlockInputStreamPtr ComplexKeyCacheDictionary::getBlockInputStream(const Names & column_names, size_t max_block_size) const\n-{\n-    std::vector<StringRef> keys;\n-    {\n-        const ProfilingScopedReadRWLock read_lock{rw_lock, ProfileEvents::DictCacheLockReadNs};\n-\n-        for (auto idx : ext::range(0, cells.size()))\n-            if (!isEmptyCell(idx) && !cells[idx].isDefault())\n-                keys.push_back(cells[idx].key);\n-    }\n-\n-    using BlockInputStreamType = DictionaryBlockInputStream<UInt64>;\n-    return std::make_shared<BlockInputStreamType>(shared_from_this(), max_block_size, keys, column_names);\n-}\n-\n-void registerDictionaryComplexKeyCache(DictionaryFactory & factory)\n-{\n-    auto create_layout = [=](const std::string & full_name,\n-                             const DictionaryStructure & dict_struct,\n-                             const Poco::Util::AbstractConfiguration & config,\n-                             const std::string & config_prefix,\n-                             DictionarySourcePtr source_ptr) -> DictionaryPtr\n-    {\n-        if (!dict_struct.key)\n-            throw Exception{\"'key' is required for dictionary of layout 'complex_key_hashed'\", ErrorCodes::BAD_ARGUMENTS};\n-        const auto & layout_prefix = config_prefix + \".layout\";\n-        const auto size = config.getInt(layout_prefix + \".complex_key_cache.size_in_cells\");\n-        if (size == 0)\n-            throw Exception{full_name + \": dictionary of layout 'cache' cannot have 0 cells\", ErrorCodes::TOO_SMALL_BUFFER_SIZE};\n-\n-        const bool require_nonempty = config.getBool(config_prefix + \".require_nonempty\", false);\n-        if (require_nonempty)\n-            throw Exception{full_name + \": dictionary of layout 'cache' cannot have 'require_nonempty' attribute set\",\n-                            ErrorCodes::BAD_ARGUMENTS};\n-\n-        const auto dict_id = StorageID::fromDictionaryConfig(config, config_prefix);\n-        const DictionaryLifetime dict_lifetime{config, config_prefix + \".lifetime\"};\n-        return std::make_unique<ComplexKeyCacheDictionary>(dict_id, dict_struct, std::move(source_ptr), dict_lifetime, size);\n-    };\n-    factory.registerLayout(\"complex_key_cache\", create_layout, true);\n-}\n-\n-\n-}\ndiff --git a/src/Dictionaries/ComplexKeyCacheDictionary.h b/src/Dictionaries/ComplexKeyCacheDictionary.h\ndeleted file mode 100644\nindex f5643fc799c3..000000000000\n--- a/src/Dictionaries/ComplexKeyCacheDictionary.h\n+++ /dev/null\n@@ -1,276 +0,0 @@\n-#pragma once\n-\n-#include <atomic>\n-#include <chrono>\n-#include <map>\n-#include <shared_mutex>\n-#include <variant>\n-#include <vector>\n-#include <Columns/ColumnDecimal.h>\n-#include <Columns/ColumnString.h>\n-#include <pcg_random.hpp>\n-#include <Common/ArenaWithFreeLists.h>\n-#include <Common/HashTable/HashMap.h>\n-#include <Common/ProfilingScopedRWLock.h>\n-#include <Common/SmallObjectPool.h>\n-#include <common/StringRef.h>\n-#include <ext/bit_cast.h>\n-#include <ext/map.h>\n-#include <ext/range.h>\n-#include <ext/size.h>\n-#include <ext/scope_guard.h>\n-#include \"DictionaryStructure.h\"\n-#include \"IDictionary.h\"\n-#include \"IDictionarySource.h\"\n-#include <DataStreams/IBlockInputStream.h>\n-#include \"DictionaryHelpers.h\"\n-\n-namespace ProfileEvents\n-{\n-extern const Event DictCacheKeysRequested;\n-extern const Event DictCacheKeysRequestedMiss;\n-extern const Event DictCacheKeysRequestedFound;\n-extern const Event DictCacheKeysExpired;\n-extern const Event DictCacheKeysNotFound;\n-extern const Event DictCacheKeysHit;\n-extern const Event DictCacheRequestTimeNs;\n-extern const Event DictCacheLockWriteNs;\n-extern const Event DictCacheLockReadNs;\n-}\n-\n-namespace DB\n-{\n-class ComplexKeyCacheDictionary final : public IDictionaryBase\n-{\n-public:\n-    ComplexKeyCacheDictionary(\n-        const StorageID & dict_id_,\n-        const DictionaryStructure & dict_struct_,\n-        DictionarySourcePtr source_ptr_,\n-        const DictionaryLifetime dict_lifetime_,\n-        const size_t size_);\n-\n-    std::string getKeyDescription() const { return key_description; }\n-\n-    std::string getTypeName() const override { return \"ComplexKeyCache\"; }\n-\n-    size_t getBytesAllocated() const override\n-    {\n-        return bytes_allocated + (key_size_is_fixed ? fixed_size_keys_pool->size() : keys_pool->size())\n-            + (string_arena ? string_arena->size() : 0);\n-    }\n-\n-    size_t getQueryCount() const override { return query_count.load(std::memory_order_relaxed); }\n-\n-    double getHitRate() const override\n-    {\n-        return static_cast<double>(hit_count.load(std::memory_order_acquire)) / query_count.load(std::memory_order_relaxed);\n-    }\n-\n-    size_t getElementCount() const override { return element_count.load(std::memory_order_relaxed); }\n-\n-    double getLoadFactor() const override { return static_cast<double>(element_count.load(std::memory_order_relaxed)) / size; }\n-\n-    bool supportUpdates() const override { return false; }\n-\n-    std::shared_ptr<const IExternalLoadable> clone() const override\n-    {\n-        return std::make_shared<ComplexKeyCacheDictionary>(getDictionaryID(), dict_struct, source_ptr->clone(), dict_lifetime, size);\n-    }\n-\n-    const IDictionarySource * getSource() const override { return source_ptr.get(); }\n-\n-    const DictionaryLifetime & getLifetime() const override { return dict_lifetime; }\n-\n-    const DictionaryStructure & getStructure() const override { return dict_struct; }\n-\n-    bool isInjective(const std::string & attribute_name) const override\n-    {\n-        return dict_struct.attributes[&getAttribute(attribute_name) - attributes.data()].injective;\n-    }\n-\n-    DictionaryKeyType getKeyType() const override { return DictionaryKeyType::complex; }\n-\n-    ColumnPtr getColumn(\n-        const std::string& attribute_name,\n-        const DataTypePtr & result_type,\n-        const Columns & key_columns,\n-        const DataTypes & key_types,\n-        const ColumnPtr default_values_column) const override;\n-\n-    ColumnUInt8::Ptr hasKeys(const Columns & key_columns, const DataTypes & key_types) const override;\n-\n-    BlockInputStreamPtr getBlockInputStream(const Names & column_names, size_t max_block_size) const override;\n-\n-private:\n-    template <typename Value>\n-    using MapType = HashMapWithSavedHash<StringRef, Value, StringRefHash>;\n-    template <typename Value>\n-    using ContainerType = Value[];\n-    template <typename Value>\n-    using ContainerPtrType = std::unique_ptr<ContainerType<Value>>;\n-\n-    struct CellMetadata final\n-    {\n-        using time_point_t = std::chrono::system_clock::time_point;\n-        using time_point_rep_t = time_point_t::rep;\n-        using time_point_urep_t = std::make_unsigned_t<time_point_rep_t>;\n-\n-        static constexpr UInt64 EXPIRES_AT_MASK = std::numeric_limits<time_point_rep_t>::max();\n-        static constexpr UInt64 IS_DEFAULT_MASK = ~EXPIRES_AT_MASK;\n-\n-        StringRef key;\n-        decltype(StringRefHash{}(key)) hash;\n-        /// Stores both expiration time and `is_default` flag in the most significant bit\n-        time_point_urep_t data;\n-\n-        /// Sets expiration time, resets `is_default` flag to false\n-        time_point_t expiresAt() const { return ext::safe_bit_cast<time_point_t>(data & EXPIRES_AT_MASK); }\n-        void setExpiresAt(const time_point_t & t) { data = ext::safe_bit_cast<time_point_urep_t>(t); }\n-\n-        bool isDefault() const { return (data & IS_DEFAULT_MASK) == IS_DEFAULT_MASK; }\n-        void setDefault() { data |= IS_DEFAULT_MASK; }\n-    };\n-\n-    struct Attribute final\n-    {\n-        AttributeUnderlyingType type;\n-        std::variant<\n-            UInt8,\n-            UInt16,\n-            UInt32,\n-            UInt64,\n-            UInt128,\n-            Int8,\n-            Int16,\n-            Int32,\n-            Int64,\n-            Decimal32,\n-            Decimal64,\n-            Decimal128,\n-            Float32,\n-            Float64,\n-            String>\n-            null_values;\n-        std::variant<\n-            ContainerPtrType<UInt8>,\n-            ContainerPtrType<UInt16>,\n-            ContainerPtrType<UInt32>,\n-            ContainerPtrType<UInt64>,\n-            ContainerPtrType<UInt128>,\n-            ContainerPtrType<Int8>,\n-            ContainerPtrType<Int16>,\n-            ContainerPtrType<Int32>,\n-            ContainerPtrType<Int64>,\n-            ContainerPtrType<Decimal32>,\n-            ContainerPtrType<Decimal64>,\n-            ContainerPtrType<Decimal128>,\n-            ContainerPtrType<Float32>,\n-            ContainerPtrType<Float64>,\n-            ContainerPtrType<StringRef>>\n-            arrays;\n-    };\n-\n-    void createAttributes();\n-\n-    Attribute createAttributeWithType(const AttributeUnderlyingType type, const Field & null_value);\n-\n-    template <typename AttributeType, typename OutputType, typename DefaultValueExtractor>\n-    void getItemsNumberImpl(\n-        Attribute & attribute,\n-        const Columns & key_columns,\n-        PaddedPODArray<OutputType> & out,\n-        DefaultValueExtractor & default_value_extractor) const;\n-\n-    void getItemsString(\n-        Attribute & attribute,\n-        const Columns & key_columns,\n-        ColumnString * out,\n-        DictionaryDefaultValueExtractor<String> & default_value_extractor) const;\n-\n-    template <typename PresentKeyHandler, typename AbsentKeyHandler>\n-    void update(\n-        const Columns & in_key_columns,\n-        const PODArray<StringRef> & in_keys,\n-        const std::vector<size_t> & in_requested_rows,\n-        PresentKeyHandler && on_cell_updated,\n-        AbsentKeyHandler && on_key_not_found) const;\n-\n-    UInt64 getCellIdx(const StringRef key) const;\n-\n-    void setDefaultAttributeValue(Attribute & attribute, const size_t idx) const;\n-\n-    void setAttributeValue(Attribute & attribute, const size_t idx, const Field & value) const;\n-\n-    Attribute & getAttribute(const std::string & attribute_name) const;\n-\n-    StringRef allocKey(const size_t row, const Columns & key_columns, StringRefs & keys) const;\n-\n-    void freeKey(const StringRef key) const;\n-\n-    template <typename Arena>\n-    static StringRef placeKeysInPool(\n-        const size_t row,\n-        const Columns & key_columns,\n-        StringRefs & keys,\n-        const std::vector<DictionaryAttribute> & key_attributes,\n-        Arena & pool);\n-\n-    StringRef placeKeysInFixedSizePool(const size_t row, const Columns & key_columns) const;\n-\n-    static StringRef copyIntoArena(StringRef src, Arena & arena);\n-    StringRef copyKey(const StringRef key) const;\n-\n-    struct FindResult\n-    {\n-        const size_t cell_idx;\n-        const bool valid;\n-        const bool outdated;\n-    };\n-\n-    FindResult findCellIdx(const StringRef & key, const CellMetadata::time_point_t now, const size_t hash) const;\n-    FindResult findCellIdx(const StringRef & key, const CellMetadata::time_point_t now) const\n-    {\n-        const auto hash = StringRefHash{}(key);\n-        return findCellIdx(key, now, hash);\n-    }\n-\n-    bool isEmptyCell(const UInt64 idx) const;\n-\n-    const DictionaryStructure dict_struct;\n-    const DictionarySourcePtr source_ptr;\n-    const DictionaryLifetime dict_lifetime;\n-    const std::string key_description{dict_struct.getKeyDescription()};\n-\n-    mutable std::shared_mutex rw_lock;\n-\n-    /// Actual size will be increased to match power of 2\n-    const size_t size;\n-\n-    /// all bits to 1  mask (size - 1) (0b1000 - 1 = 0b111)\n-    const size_t size_overlap_mask;\n-\n-    /// Max tries to find cell, overlapped with mask: if size = 16 and start_cell=10: will try cells: 10,11,12,13,14,15,0,1,2,3\n-    static constexpr size_t max_collision_length = 10;\n-\n-    const UInt64 zero_cell_idx{getCellIdx(StringRef{})};\n-    std::map<std::string, size_t> attribute_index_by_name;\n-    mutable std::vector<Attribute> attributes;\n-    mutable std::vector<CellMetadata> cells{size};\n-    const bool key_size_is_fixed{dict_struct.isKeySizeFixed()};\n-    size_t key_size{key_size_is_fixed ? dict_struct.getKeySize() : 0};\n-    std::unique_ptr<ArenaWithFreeLists> keys_pool = key_size_is_fixed ? nullptr : std::make_unique<ArenaWithFreeLists>();\n-    std::unique_ptr<SmallObjectPool> fixed_size_keys_pool = key_size_is_fixed ? std::make_unique<SmallObjectPool>(key_size) : nullptr;\n-    std::unique_ptr<ArenaWithFreeLists> string_arena;\n-\n-    mutable pcg64 rnd_engine;\n-\n-    mutable size_t bytes_allocated = 0;\n-    mutable std::atomic<size_t> element_count{0};\n-    mutable std::atomic<size_t> hit_count{0};\n-    mutable std::atomic<size_t> query_count{0};\n-\n-    const std::chrono::time_point<std::chrono::system_clock> creation_time = std::chrono::system_clock::now();\n-};\n-\n-}\ndiff --git a/src/Dictionaries/ComplexKeyDirectDictionary.cpp b/src/Dictionaries/ComplexKeyDirectDictionary.cpp\nindex 391b5c47980b..eedc24193a25 100644\n--- a/src/Dictionaries/ComplexKeyDirectDictionary.cpp\n+++ b/src/Dictionaries/ComplexKeyDirectDictionary.cpp\n@@ -39,7 +39,7 @@ ColumnPtr ComplexKeyDirectDictionary::getColumn(\n     const DataTypePtr & result_type,\n     const Columns & key_columns,\n     const DataTypes & key_types,\n-    const ColumnPtr default_values_column) const\n+    const ColumnPtr & default_values_column) const\n {\n     dict_struct.validateKeyTypes(key_types);\n \ndiff --git a/src/Dictionaries/ComplexKeyDirectDictionary.h b/src/Dictionaries/ComplexKeyDirectDictionary.h\nindex 0e191321daa5..326ffa2924a7 100644\n--- a/src/Dictionaries/ComplexKeyDirectDictionary.h\n+++ b/src/Dictionaries/ComplexKeyDirectDictionary.h\n@@ -66,7 +66,7 @@ class ComplexKeyDirectDictionary final : public IDictionaryBase\n         const DataTypePtr & result_type,\n         const Columns & key_columns,\n         const DataTypes & key_types,\n-        const ColumnPtr default_values_column) const override;\n+        const ColumnPtr & default_values_column) const override;\n \n     ColumnUInt8::Ptr hasKeys(const Columns & key_columns, const DataTypes & key_types) const override;\n \ndiff --git a/src/Dictionaries/ComplexKeyHashedDictionary.cpp b/src/Dictionaries/ComplexKeyHashedDictionary.cpp\nindex a0784b5a4171..861ce0e768d5 100644\n--- a/src/Dictionaries/ComplexKeyHashedDictionary.cpp\n+++ b/src/Dictionaries/ComplexKeyHashedDictionary.cpp\n@@ -41,7 +41,7 @@ ColumnPtr ComplexKeyHashedDictionary::getColumn(\n     const DataTypePtr & result_type,\n     const Columns & key_columns,\n     const DataTypes & key_types,\n-    const ColumnPtr default_values_column) const\n+    const ColumnPtr & default_values_column) const\n {\n     dict_struct.validateKeyTypes(key_types);\n \n@@ -563,7 +563,13 @@ std::vector<StringRef> ComplexKeyHashedDictionary::getKeys(const Attribute & att\n BlockInputStreamPtr ComplexKeyHashedDictionary::getBlockInputStream(const Names & column_names, size_t max_block_size) const\n {\n     using BlockInputStreamType = DictionaryBlockInputStream<UInt64>;\n-    return std::make_shared<BlockInputStreamType>(shared_from_this(), max_block_size, getKeys(), column_names);\n+    auto vector_keys = getKeys();\n+\n+    PaddedPODArray<StringRef> keys;\n+    keys.reserve(vector_keys.size());\n+    keys.assign(vector_keys.begin(), vector_keys.end());\n+\n+    return std::make_shared<BlockInputStreamType>(shared_from_this(), max_block_size, keys, column_names);\n }\n \n void registerDictionaryComplexKeyHashed(DictionaryFactory & factory)\ndiff --git a/src/Dictionaries/ComplexKeyHashedDictionary.h b/src/Dictionaries/ComplexKeyHashedDictionary.h\nindex ecc720ca0b07..091974bbf435 100644\n--- a/src/Dictionaries/ComplexKeyHashedDictionary.h\n+++ b/src/Dictionaries/ComplexKeyHashedDictionary.h\n@@ -67,7 +67,7 @@ class ComplexKeyHashedDictionary final : public IDictionaryBase\n         const DataTypePtr & result_type,\n         const Columns & key_columns,\n         const DataTypes & key_types,\n-        const ColumnPtr default_values_column) const override;\n+        const ColumnPtr & default_values_column) const override;\n \n     ColumnUInt8::Ptr hasKeys(const Columns & key_columns, const DataTypes & key_types) const override;\n \ndiff --git a/src/Dictionaries/DictionaryBlockInputStream.h b/src/Dictionaries/DictionaryBlockInputStream.h\nindex f045d47c2c21..71615efa7f8e 100644\n--- a/src/Dictionaries/DictionaryBlockInputStream.h\n+++ b/src/Dictionaries/DictionaryBlockInputStream.h\n@@ -21,7 +21,7 @@ namespace ErrorCodes\n     extern const int LOGICAL_ERROR;\n }\n \n-\n+/// TODO: Remove this class\n /* BlockInputStream implementation for external dictionaries\n  * read() returns blocks consisting of the in-memory contents of the dictionaries\n  */\n@@ -30,12 +30,15 @@ class DictionaryBlockInputStream : public DictionaryBlockInputStreamBase\n {\n public:\n     DictionaryBlockInputStream(\n-        std::shared_ptr<const IDictionaryBase> dictionary, UInt64 max_block_size, PaddedPODArray<Key> && ids, const Names & column_names);\n+        std::shared_ptr<const IDictionaryBase> dictionary,\n+        UInt64 max_block_size,\n+        PaddedPODArray<Key> && ids,\n+        const Names & column_names);\n \n     DictionaryBlockInputStream(\n         std::shared_ptr<const IDictionaryBase> dictionary,\n         UInt64 max_block_size,\n-        const std::vector<StringRef> & keys,\n+        const PaddedPODArray<StringRef> & keys,\n         const Names & column_names);\n \n     using GetColumnsFunction = std::function<ColumnsWithTypeAndName(const Columns &, const std::vector<DictionaryAttribute> & attributes)>;\n@@ -55,7 +58,7 @@ class DictionaryBlockInputStream : public DictionaryBlockInputStreamBase\n     String getName() const override { return \"Dictionary\"; }\n \n protected:\n-    Block getBlock(size_t start, size_t size) const override;\n+    Block getBlock(size_t start, size_t length) const override;\n \n private:\n     Block\n@@ -64,7 +67,7 @@ class DictionaryBlockInputStream : public DictionaryBlockInputStreamBase\n     ColumnPtr getColumnFromIds(const PaddedPODArray<Key> & ids_to_fill) const;\n \n     void fillKeyColumns(\n-        const std::vector<StringRef> & keys,\n+        const PaddedPODArray<StringRef> & keys,\n         size_t start,\n         size_t size,\n         const DictionaryStructure & dictionary_structure,\n@@ -105,7 +108,7 @@ template <typename Key>\n DictionaryBlockInputStream<Key>::DictionaryBlockInputStream(\n     std::shared_ptr<const IDictionaryBase> dictionary_,\n     UInt64 max_block_size_,\n-    const std::vector<StringRef> & keys,\n+    const PaddedPODArray<StringRef> & keys,\n     const Names & column_names_)\n     : DictionaryBlockInputStreamBase(keys.size(), max_block_size_)\n     , dictionary(dictionary_)\n@@ -260,7 +263,7 @@ ColumnPtr DictionaryBlockInputStream<Key>::getColumnFromIds(const PaddedPODArray\n \n template <typename Key>\n void DictionaryBlockInputStream<Key>::fillKeyColumns(\n-    const std::vector<StringRef> & keys,\n+    const PaddedPODArray<StringRef> & keys,\n     size_t start,\n     size_t size,\n     const DictionaryStructure & dictionary_structure,\n@@ -275,7 +278,7 @@ void DictionaryBlockInputStream<Key>::fillKeyColumns(\n     for (auto idx : ext::range(start, size))\n     {\n         const auto & key = keys[idx];\n-        auto ptr = key.data;\n+        const auto *ptr = key.data;\n         for (auto & column : columns)\n             ptr = column->deserializeAndInsertFromArena(ptr);\n     }\ndiff --git a/src/Dictionaries/DictionaryHelpers.h b/src/Dictionaries/DictionaryHelpers.h\nindex 0026d8848ca2..5fda5f2599ea 100644\n--- a/src/Dictionaries/DictionaryHelpers.h\n+++ b/src/Dictionaries/DictionaryHelpers.h\n@@ -1,11 +1,13 @@\n #pragma once\n \n+#include <Common/Arena.h>\n #include <Columns/IColumn.h>\n #include <Columns/ColumnDecimal.h>\n #include <Columns/ColumnString.h>\n #include <Columns/ColumnVector.h>\n #include <DataTypes/DataTypesDecimal.h>\n-#include \"DictionaryStructure.h\"\n+#include <Dictionaries/IDictionary.h>\n+#include <Dictionaries/DictionaryStructure.h>\n \n namespace DB\n {\n@@ -13,6 +15,190 @@ namespace DB\n namespace ErrorCodes\n {\n     extern const int TYPE_MISMATCH;\n+    extern const int BAD_ARGUMENTS;\n+}\n+\n+/** Simple helper for getting default.\n+  * Initialized with default value and default values column.\n+  * If default values column is not null default value is taken from column.\n+  * If default value is null default value is taken from initializer.\n+  */\n+class DefaultValueProvider final\n+{\n+public:\n+    explicit DefaultValueProvider(Field default_value_, ColumnPtr default_values_column_ = nullptr)\n+        : default_value(std::move(default_value_))\n+        , default_values_column(default_values_column_)\n+    {\n+    }\n+\n+    inline bool isConstant() const { return default_values_column == nullptr; }\n+\n+    Field getDefaultValue(size_t row) const\n+    {\n+        if (default_values_column)\n+            return (*default_values_column)[row];\n+\n+        return default_value;\n+    }\n+\n+private:\n+    Field default_value;\n+    ColumnPtr default_values_column;\n+};\n+\n+/** Support class for dictionary storages.\n+\n+    The main idea is that during fetch we create all columns, but fill only columns that client requested.\n+\n+    We need to create other columns during fetch, because in case of serialized storage we can skip\n+    unnecessary columns serialized in cache with skipSerializedInArena method.\n+\n+    When result is fetched from the storage client of storage can filterOnlyNecessaryColumns\n+    and get only columns that match attributes_names_to_fetch.\n+ */\n+class DictionaryStorageFetchRequest\n+{\n+public:\n+    DictionaryStorageFetchRequest(const DictionaryStructure & structure, const Strings & attributes_names_to_fetch, Columns attributes_default_values_columns)\n+        : attributes_to_fetch_names_set(attributes_names_to_fetch.begin(), attributes_names_to_fetch.end())\n+        , attributes_to_fetch_filter(structure.attributes.size(), false)\n+    {\n+        assert(attributes_default_values_columns.size() == attributes_names_to_fetch.size());\n+\n+        if (attributes_to_fetch_names_set.size() != attributes_names_to_fetch.size())\n+            throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Attribute names to fetch should be unique\");\n+\n+        size_t attributes_size = structure.attributes.size();\n+        dictionary_attributes_types.reserve(attributes_size);\n+        attributes_default_value_providers.reserve(attributes_to_fetch_names_set.size());\n+\n+        size_t default_values_column_index = 0;\n+        for (size_t i = 0; i < attributes_size; ++i)\n+        {\n+            const auto & dictionary_attribute = structure.attributes[i];\n+            const auto & name = dictionary_attribute.name;\n+            const auto & type = dictionary_attribute.type;\n+            dictionary_attributes_types.emplace_back(type);\n+\n+            if (attributes_to_fetch_names_set.find(name) != attributes_to_fetch_names_set.end())\n+            {\n+                attributes_to_fetch_filter[i] = true;\n+                attributes_default_value_providers.emplace_back(dictionary_attribute.null_value, attributes_default_values_columns[default_values_column_index]);\n+                ++default_values_column_index;\n+            }\n+            else\n+                attributes_default_value_providers.emplace_back(dictionary_attribute.null_value);\n+        }\n+    }\n+\n+    DictionaryStorageFetchRequest() = default;\n+\n+    /// Check requested attributes size\n+    ALWAYS_INLINE size_t attributesSize() const\n+    {\n+        return dictionary_attributes_types.size();\n+    }\n+\n+    /// Check if attribute with attribute_name was requested to fetch\n+    ALWAYS_INLINE bool containsAttribute(const String & attribute_name) const\n+    {\n+        return attributes_to_fetch_names_set.find(attribute_name) != attributes_to_fetch_names_set.end();\n+    }\n+\n+    /// Check if attribute with attribute_index should be filled during fetch\n+    ALWAYS_INLINE bool shouldFillResultColumnWithIndex(size_t attribute_index) const\n+    {\n+        return attributes_to_fetch_filter[attribute_index];\n+    }\n+\n+    const DataTypePtr & dataTypeAtIndex(size_t attribute_index) const\n+    {\n+        return dictionary_attributes_types[attribute_index];\n+    }\n+\n+    const DefaultValueProvider & defaultValueProviderAtIndex(size_t attribute_index) const\n+    {\n+        return attributes_default_value_providers[attribute_index];\n+    }\n+\n+    /// Create columns for each of dictionary attributes\n+    MutableColumns makeAttributesResultColumns() const\n+    {\n+        MutableColumns result;\n+        result.reserve(dictionary_attributes_types.size());\n+\n+        for (const auto & type : dictionary_attributes_types)\n+            result.emplace_back(type->createColumn());\n+\n+        return result;\n+    }\n+\n+    Columns makeAttributesResultColumnsNonMutable() const\n+    {\n+        Columns result;\n+        result.reserve(dictionary_attributes_types.size());\n+\n+        for (const auto & type : dictionary_attributes_types)\n+            result.emplace_back(type->createColumn());\n+\n+        return result;\n+    }\n+\n+    /// Filter only requested columns\n+    Columns filterRequestedColumns(MutableColumns & fetched_mutable_columns) const\n+    {\n+        Columns result;\n+        result.reserve(dictionary_attributes_types.size());\n+\n+        for (size_t fetch_request_index = 0; fetch_request_index < dictionary_attributes_types.size(); ++fetch_request_index)\n+            if (shouldFillResultColumnWithIndex(fetch_request_index))\n+                result.emplace_back(std::move(fetched_mutable_columns[fetch_request_index]));\n+\n+        return result;\n+    }\n+private:\n+    std::unordered_set<String> attributes_to_fetch_names_set;\n+    std::vector<bool> attributes_to_fetch_filter;\n+    std::vector<DefaultValueProvider> attributes_default_value_providers;\n+    DataTypes dictionary_attributes_types;\n+};\n+\n+static inline void insertDefaultValuesIntoColumns(\n+    MutableColumns & columns,\n+    const DictionaryStorageFetchRequest & fetch_request,\n+    size_t row_index)\n+{\n+    size_t columns_size = columns.size();\n+\n+    for (size_t column_index = 0; column_index < columns_size; ++column_index)\n+    {\n+        const auto & column = columns[column_index];\n+        const auto & default_value_provider = fetch_request.defaultValueProviderAtIndex(column_index);\n+\n+        if (fetch_request.shouldFillResultColumnWithIndex(column_index))\n+            column->insert(default_value_provider.getDefaultValue(row_index));\n+    }\n+}\n+\n+/// Deserialize column value and insert it in columns.\n+/// Skip unnecessary columns that were not requested from deserialization.\n+static inline void deserializeAndInsertIntoColumns(\n+    MutableColumns & columns,\n+    const DictionaryStorageFetchRequest & fetch_request,\n+    const char * place_for_serialized_columns)\n+{\n+    size_t columns_size = columns.size();\n+\n+    for (size_t column_index = 0; column_index < columns_size; ++column_index)\n+    {\n+        const auto & column = columns[column_index];\n+\n+        if (fetch_request.shouldFillResultColumnWithIndex(column_index))\n+            place_for_serialized_columns = column->deserializeAndInsertFromArena(place_for_serialized_columns);\n+        else\n+            place_for_serialized_columns = column->skipSerializedInArena(place_for_serialized_columns);\n+    }\n }\n \n /**\n@@ -69,7 +255,7 @@ class DictionaryDefaultValueExtractor\n public:\n     using DefaultValueType = DictionaryValueType<DictionaryAttributeType>;\n \n-    DictionaryDefaultValueExtractor(DictionaryAttributeType attribute_default_value, ColumnPtr default_values_column_ = nullptr)\n+    explicit DictionaryDefaultValueExtractor(DictionaryAttributeType attribute_default_value, ColumnPtr default_values_column_ = nullptr)\n         : default_value(std::move(attribute_default_value))\n     {\n         if (default_values_column_ == nullptr)\n@@ -109,6 +295,76 @@ class DictionaryDefaultValueExtractor\n     bool use_default_value_from_column = false;\n };\n \n+template <DictionaryKeyType key_type>\n+class DictionaryKeysExtractor\n+{\n+public:\n+    using KeyType = std::conditional_t<key_type == DictionaryKeyType::simple, UInt64, StringRef>;\n+    static_assert(key_type != DictionaryKeyType::range, \"Range key type is not supported by DictionaryKeysExtractor\");\n+\n+    explicit DictionaryKeysExtractor(const Columns & key_columns, Arena & existing_arena)\n+    {\n+        assert(!key_columns.empty());\n+\n+        if constexpr (key_type == DictionaryKeyType::simple)\n+            keys = getColumnVectorData(key_columns.front());\n+        else\n+            keys = deserializeKeyColumnsInArena(key_columns, existing_arena);\n+    }\n+\n+\n+    const PaddedPODArray<KeyType> & getKeys() const\n+    {\n+        return keys;\n+    }\n+\n+private:\n+    static PaddedPODArray<UInt64> getColumnVectorData(const ColumnPtr column)\n+    {\n+        PaddedPODArray<UInt64> result;\n+\n+        auto full_column = column->convertToFullColumnIfConst();\n+        const auto *vector_col = checkAndGetColumn<ColumnVector<UInt64>>(full_column.get());\n+\n+        if (!vector_col)\n+            throw Exception{ErrorCodes::TYPE_MISMATCH, \"Column type mismatch for simple key expected UInt64\"};\n+\n+        result.assign(vector_col->getData());\n+\n+        return result;\n+    }\n+\n+    static PaddedPODArray<StringRef> deserializeKeyColumnsInArena(const Columns & key_columns, Arena & temporary_arena)\n+    {\n+        size_t keys_size = key_columns.front()->size();\n+\n+        PaddedPODArray<StringRef> result;\n+        result.reserve(keys_size);\n+\n+        PaddedPODArray<StringRef> temporary_column_data(key_columns.size());\n+\n+        for (size_t key_index = 0; key_index < keys_size; ++key_index)\n+        {\n+            size_t allocated_size_for_columns = 0;\n+            const char * block_start = nullptr;\n+\n+            for (size_t column_index = 0; column_index < key_columns.size(); ++column_index)\n+            {\n+                const auto & column = key_columns[column_index];\n+                temporary_column_data[column_index] = column->serializeValueIntoArena(key_index, temporary_arena, block_start);\n+                allocated_size_for_columns += temporary_column_data[column_index].size;\n+            }\n+\n+            result.push_back(StringRef{block_start, allocated_size_for_columns});\n+        }\n+\n+        return result;\n+    }\n+\n+    PaddedPODArray<KeyType> keys;\n+\n+};\n+\n /**\n  * Returns ColumnVector data as PaddedPodArray.\n \ndiff --git a/src/Dictionaries/DictionaryStructure.cpp b/src/Dictionaries/DictionaryStructure.cpp\nindex 408e4803b1bc..25e29d7e0e82 100644\n--- a/src/Dictionaries/DictionaryStructure.cpp\n+++ b/src/Dictionaries/DictionaryStructure.cpp\n@@ -147,7 +147,7 @@ DictionaryStructure::DictionaryStructure(const Poco::Util::AbstractConfiguration\n         id.emplace(config, structure_prefix + \".id\");\n     else if (has_key)\n     {\n-        key.emplace(getAttributes(config, structure_prefix + \".key\", false, false));\n+        key.emplace(getAttributes(config, structure_prefix + \".key\", true));\n         if (key->empty())\n             throw Exception{\"Empty 'key' supplied\", ErrorCodes::BAD_ARGUMENTS};\n     }\n@@ -196,7 +196,13 @@ DictionaryStructure::DictionaryStructure(const Poco::Util::AbstractConfiguration\n             has_expressions = true;\n     }\n \n-    attributes = getAttributes(config, structure_prefix);\n+    attributes = getAttributes(config, structure_prefix, false);\n+\n+    for (size_t i = 0; i < attributes.size(); ++i)\n+    {\n+        const auto & attribute_name = attributes[i].name;\n+        attribute_name_to_index[attribute_name] = i;\n+    }\n \n     if (attributes.empty())\n         throw Exception{\"Dictionary has no attributes defined\", ErrorCodes::BAD_ARGUMENTS};\n@@ -223,24 +229,25 @@ void DictionaryStructure::validateKeyTypes(const DataTypes & key_types) const\n     }\n }\n \n-const DictionaryAttribute & DictionaryStructure::getAttribute(const String & attribute_name) const\n+const DictionaryAttribute & DictionaryStructure::getAttribute(const std::string & attribute_name) const\n {\n-    auto find_iter\n-        = std::find_if(attributes.begin(), attributes.end(), [&](const auto & attribute) { return attribute.name == attribute_name; });\n-    if (find_iter != attributes.end())\n-        return *find_iter;\n+    auto it = attribute_name_to_index.find(attribute_name);\n \n-    if (key && access_to_key_from_attributes)\n+    if (it == attribute_name_to_index.end())\n     {\n-        find_iter = std::find_if(key->begin(), key->end(), [&](const auto & attribute) { return attribute.name == attribute_name; });\n-        if (find_iter != key->end())\n-            return *find_iter;\n+        if (!access_to_key_from_attributes)\n+            throw Exception{\"No such attribute '\" + attribute_name + \"'\", ErrorCodes::BAD_ARGUMENTS};\n+\n+        for (const auto & key_attribute : *key)\n+            if (key_attribute.name == attribute_name)\n+                return key_attribute;\n     }\n \n-    throw Exception{\"No such attribute '\" + attribute_name + \"'\", ErrorCodes::BAD_ARGUMENTS};\n+    size_t attribute_index = it->second;\n+    return attributes[attribute_index];\n }\n \n-const DictionaryAttribute & DictionaryStructure::getAttribute(const String & attribute_name, const DataTypePtr & type) const\n+const DictionaryAttribute & DictionaryStructure::getAttribute(const std::string & attribute_name, const DataTypePtr & type) const\n {\n     const auto & attribute = getAttribute(attribute_name);\n \n@@ -251,6 +258,14 @@ const DictionaryAttribute & DictionaryStructure::getAttribute(const String & att\n     return attribute;\n }\n \n+size_t DictionaryStructure::getKeysSize() const\n+{\n+    if (id)\n+        return 1;\n+    else\n+        return key->size();\n+}\n+\n std::string DictionaryStructure::getKeyDescription() const\n {\n     if (id)\n@@ -329,9 +344,12 @@ static void checkAttributeKeys(const Poco::Util::AbstractConfiguration::Keys & k\n std::vector<DictionaryAttribute> DictionaryStructure::getAttributes(\n     const Poco::Util::AbstractConfiguration & config,\n     const std::string & config_prefix,\n-    const bool hierarchy_allowed,\n-    const bool allow_null_values)\n+    bool complex_key_attributes)\n {\n+    /// If we request complex key attributes they does not support hierarchy and does not allow null values\n+    const bool hierarchy_allowed = !complex_key_attributes;\n+    const bool allow_null_values = !complex_key_attributes;\n+\n     Poco::Util::AbstractConfiguration::Keys config_elems;\n     config.keys(config_prefix, config_elems);\n     auto has_hierarchy = false;\n@@ -358,7 +376,6 @@ std::vector<DictionaryAttribute> DictionaryStructure::getAttributes(\n         if ((range_min && name == range_min->name) || (range_max && name == range_max->name))\n             continue;\n \n-\n         const auto type_string = config.getString(prefix + \"type\");\n         const auto initial_type = DataTypeFactory::instance().get(type_string);\n         auto type = initial_type;\ndiff --git a/src/Dictionaries/DictionaryStructure.h b/src/Dictionaries/DictionaryStructure.h\nindex 0ff50868e264..08cc49aeb85c 100644\n--- a/src/Dictionaries/DictionaryStructure.h\n+++ b/src/Dictionaries/DictionaryStructure.h\n@@ -1,17 +1,18 @@\n #pragma once\n \n-#include <Core/Field.h>\n-#include <DataTypes/IDataType.h>\n-#include <IO/ReadBufferFromString.h>\n-#include <Interpreters/IExternalLoadable.h>\n-#include <Poco/Util/AbstractConfiguration.h>\n-\n #include <map>\n #include <optional>\n #include <string>\n #include <vector>\n \n \n+#include <Poco/Util/AbstractConfiguration.h>\n+\n+#include <Core/Field.h>\n+#include <IO/ReadBufferFromString.h>\n+#include <DataTypes/IDataType.h>\n+#include <Interpreters/IExternalLoadable.h>\n+\n namespace DB\n {\n \n@@ -45,6 +46,7 @@ using DictionaryLifetime = ExternalLoadableLifetime;\n /** Holds the description of a single dictionary attribute:\n *    - name, used for lookup into dictionary and source;\n *    - type, used in conjunction with DataTypeFactory and getAttributeUnderlyingTypeByname;\n+*    - nested_type, contains nested type of complex type like Nullable, Array\n *    - null_value, used as a default value for non-existent entries in the dictionary,\n *        decimal representation for numeric attributes;\n *    - hierarchical, whether this attribute defines a hierarchy;\n@@ -147,6 +149,7 @@ struct DictionaryStructure final\n     std::optional<DictionarySpecialAttribute> id;\n     std::optional<std::vector<DictionaryAttribute>> key;\n     std::vector<DictionaryAttribute> attributes;\n+    std::unordered_map<std::string, size_t> attribute_name_to_index;\n     std::optional<DictionaryTypedSpecialAttribute> range_min;\n     std::optional<DictionaryTypedSpecialAttribute> range_max;\n     bool has_expressions = false;\n@@ -155,8 +158,11 @@ struct DictionaryStructure final\n     DictionaryStructure(const Poco::Util::AbstractConfiguration & config, const std::string & config_prefix);\n \n     void validateKeyTypes(const DataTypes & key_types) const;\n-    const DictionaryAttribute & getAttribute(const String & attribute_name) const;\n-    const DictionaryAttribute & getAttribute(const String & attribute_name, const DataTypePtr & type) const;\n+\n+    const DictionaryAttribute & getAttribute(const std::string & attribute_name) const;\n+    const DictionaryAttribute & getAttribute(const std::string & attribute_name, const DataTypePtr & type) const;\n+    size_t getKeysSize() const;\n+\n     std::string getKeyDescription() const;\n     bool isKeySizeFixed() const;\n     size_t getKeySize() const;\n@@ -167,8 +173,7 @@ struct DictionaryStructure final\n     std::vector<DictionaryAttribute> getAttributes(\n         const Poco::Util::AbstractConfiguration & config,\n         const std::string & config_prefix,\n-        const bool hierarchy_allowed = true,\n-        const bool allow_null_values = true);\n+        bool complex_key_attributes);\n };\n \n }\ndiff --git a/src/Dictionaries/DirectDictionary.cpp b/src/Dictionaries/DirectDictionary.cpp\nindex b61f256b0cc6..ac995d51f097 100644\n--- a/src/Dictionaries/DirectDictionary.cpp\n+++ b/src/Dictionaries/DirectDictionary.cpp\n@@ -138,7 +138,7 @@ ColumnPtr DirectDictionary::getColumn(\n         const DataTypePtr & result_type,\n         const Columns & key_columns,\n         const DataTypes &,\n-        const ColumnPtr default_values_column) const\n+        const ColumnPtr & default_values_column) const\n {\n     ColumnPtr result;\n \ndiff --git a/src/Dictionaries/DirectDictionary.h b/src/Dictionaries/DirectDictionary.h\nindex c6f4c15556b8..1fb6c8954b26 100644\n--- a/src/Dictionaries/DirectDictionary.h\n+++ b/src/Dictionaries/DirectDictionary.h\n@@ -71,7 +71,7 @@ class DirectDictionary final : public IDictionary\n         const DataTypePtr & result_type,\n         const Columns & key_columns,\n         const DataTypes & key_types,\n-        const ColumnPtr default_values_column) const override;\n+        const ColumnPtr & default_values_column) const override;\n \n     ColumnUInt8::Ptr hasKeys(const Columns & key_columns, const DataTypes & key_types) const override;\n \ndiff --git a/src/Dictionaries/FlatDictionary.cpp b/src/Dictionaries/FlatDictionary.cpp\nindex f4f50a69598a..8cd6b51b65f9 100644\n--- a/src/Dictionaries/FlatDictionary.cpp\n+++ b/src/Dictionaries/FlatDictionary.cpp\n@@ -113,7 +113,7 @@ ColumnPtr FlatDictionary::getColumn(\n         const DataTypePtr & result_type,\n         const Columns & key_columns,\n         const DataTypes &,\n-        const ColumnPtr default_values_column) const\n+        const ColumnPtr & default_values_column) const\n {\n     ColumnPtr result;\n \n@@ -125,7 +125,7 @@ ColumnPtr FlatDictionary::getColumn(\n     const auto & attribute = getAttribute(attribute_name);\n     const auto & dictionary_attribute = dict_struct.getAttribute(attribute_name, result_type);\n \n-    auto type_call = [&](const auto &dictionary_attribute_type)\n+    auto type_call = [&](const auto & dictionary_attribute_type)\n     {\n         using Type = std::decay_t<decltype(dictionary_attribute_type)>;\n         using AttributeType = typename Type::AttributeType;\n@@ -167,7 +167,7 @@ ColumnPtr FlatDictionary::getColumn(\n     if (attribute.nullable_set)\n     {\n         ColumnUInt8::MutablePtr col_null_map_to = ColumnUInt8::create(size, false);\n-        ColumnUInt8::Container& vec_null_map_to = col_null_map_to->getData();\n+        ColumnUInt8::Container & vec_null_map_to = col_null_map_to->getData();\n \n         for (size_t row = 0; row < ids.size(); ++row)\n         {\ndiff --git a/src/Dictionaries/FlatDictionary.h b/src/Dictionaries/FlatDictionary.h\nindex 23bfa3d37b55..f491eb286417 100644\n--- a/src/Dictionaries/FlatDictionary.h\n+++ b/src/Dictionaries/FlatDictionary.h\n@@ -78,7 +78,7 @@ class FlatDictionary final : public IDictionary\n         const DataTypePtr & result_type,\n         const Columns & key_columns,\n         const DataTypes & key_types,\n-        const ColumnPtr default_values_column) const override;\n+        const ColumnPtr & default_values_column) const override;\n \n     ColumnUInt8::Ptr hasKeys(const Columns & key_columns, const DataTypes & key_types) const override;\n \ndiff --git a/src/Dictionaries/HashedDictionary.cpp b/src/Dictionaries/HashedDictionary.cpp\nindex b51f2414142c..ded446a21639 100644\n--- a/src/Dictionaries/HashedDictionary.cpp\n+++ b/src/Dictionaries/HashedDictionary.cpp\n@@ -134,7 +134,7 @@ ColumnPtr HashedDictionary::getColumn(\n     const DataTypePtr & result_type,\n     const Columns & key_columns,\n     const DataTypes &,\n-    const ColumnPtr default_values_column) const\n+    const ColumnPtr & default_values_column) const\n {\n     ColumnPtr result;\n \ndiff --git a/src/Dictionaries/HashedDictionary.h b/src/Dictionaries/HashedDictionary.h\nindex 97b329a8b25f..ab37f1528ca7 100644\n--- a/src/Dictionaries/HashedDictionary.h\n+++ b/src/Dictionaries/HashedDictionary.h\n@@ -75,7 +75,7 @@ class HashedDictionary final : public IDictionary\n         const DataTypePtr & result_type,\n         const Columns & key_columns,\n         const DataTypes & key_types,\n-        const ColumnPtr default_values_column) const override;\n+        const ColumnPtr & default_values_column) const override;\n \n     ColumnUInt8::Ptr hasKeys(const Columns & key_columns, const DataTypes & key_types) const override;\n \ndiff --git a/src/Dictionaries/ICacheDictionaryStorage.h b/src/Dictionaries/ICacheDictionaryStorage.h\nnew file mode 100644\nindex 000000000000..8db2dab536c6\n--- /dev/null\n+++ b/src/Dictionaries/ICacheDictionaryStorage.h\n@@ -0,0 +1,124 @@\n+#pragma once\n+\n+#include <Common/PODArray.h>\n+#include <Common/HashTable/HashMap.h>\n+#include <Columns/IColumn.h>\n+#include <Dictionaries/DictionaryHelpers.h>\n+\n+namespace DB\n+{\n+\n+struct KeyState\n+{\n+    enum State: uint8_t\n+    {\n+        not_found = 2,\n+        expired = 4,\n+        found = 8,\n+    };\n+\n+    KeyState(State state_, size_t fetched_column_index_)\n+        : state(state_)\n+        , fetched_column_index(fetched_column_index_)\n+    {}\n+\n+    KeyState(State state_)\n+        : state(state_)\n+    {}\n+\n+    inline bool isFound() const { return state == State::found; }\n+    inline bool isExpired() const { return state == State::expired; }\n+    inline bool isNotFound() const { return state == State::not_found; }\n+    inline bool isDefault() const { return is_default; }\n+    inline void setDefault() { is_default = true; }\n+    /// Valid only if keyState is found or expired\n+    inline size_t getFetchedColumnIndex() const { return fetched_column_index; }\n+\n+private:\n+    State state = not_found;\n+    size_t fetched_column_index = 0;\n+    bool is_default = false;\n+};\n+\n+/// Result of fetch from CacheDictionaryStorage\n+template <typename KeyType>\n+struct KeysStorageFetchResult\n+{\n+    /// Fetched column values\n+    MutableColumns fetched_columns;\n+\n+    PaddedPODArray<KeyState> key_index_to_state;\n+\n+    size_t expired_keys_size = 0;\n+\n+    size_t found_keys_size = 0;\n+\n+    size_t not_found_keys_size = 0;\n+\n+    size_t default_keys_size = 0;\n+\n+};\n+\n+using SimpleKeysStorageFetchResult = KeysStorageFetchResult<UInt64>;\n+using ComplexKeysStorageFetchResult = KeysStorageFetchResult<StringRef>;\n+\n+class ICacheDictionaryStorage\n+{\n+public:\n+\n+    virtual ~ICacheDictionaryStorage() = default;\n+\n+    /// Necessary if all keys are found we can return result to client without additional aggregation\n+    virtual bool returnsFetchedColumnsInOrderOfRequestedKeys() const = 0;\n+\n+    /// Name of storage\n+    virtual String getName() const = 0;\n+\n+    /// Does storage support simple keys\n+    virtual bool supportsSimpleKeys() const = 0;\n+\n+    /// Fetch columns for keys, this method is not write thread safe\n+    virtual SimpleKeysStorageFetchResult fetchColumnsForKeys(\n+        const PaddedPODArray<UInt64> & keys,\n+        const DictionaryStorageFetchRequest & fetch_request) = 0;\n+\n+    /// Fetch columns for keys, this method is not write thread safe\n+    virtual void insertColumnsForKeys(const PaddedPODArray<UInt64> & keys, Columns columns) = 0;\n+\n+    /// Insert default keys\n+    virtual void insertDefaultKeys(const PaddedPODArray<UInt64> & keys) = 0;\n+\n+    /// Return cached simple keys\n+    virtual PaddedPODArray<UInt64> getCachedSimpleKeys() const = 0;\n+\n+    /// Does storage support complex keys\n+    virtual bool supportsComplexKeys() const = 0;\n+\n+    /// Fetch columns for keys, this method is not write thread safe\n+    virtual ComplexKeysStorageFetchResult fetchColumnsForKeys(\n+        const PaddedPODArray<StringRef> & keys,\n+        const DictionaryStorageFetchRequest & column_fetch_requests) = 0;\n+\n+    /// Fetch columns for keys, this method is not write thread safe\n+    virtual void insertColumnsForKeys(const PaddedPODArray<StringRef> & keys, Columns columns) = 0;\n+\n+    /// Insert default keys\n+    virtual void insertDefaultKeys(const PaddedPODArray<StringRef> & keys) = 0;\n+\n+    /// Return cached simple keys\n+    virtual PaddedPODArray<StringRef> getCachedComplexKeys() const = 0;\n+\n+    /// Return size of keys in storage\n+    virtual size_t getSize() const = 0;\n+\n+    /// Return maximum size of keys in storage\n+    virtual size_t getMaxSize() const = 0;\n+\n+    /// Return bytes allocated in storage\n+    virtual size_t getBytesAllocated() const = 0;\n+\n+};\n+\n+using CacheDictionaryStoragePtr = std::shared_ptr<ICacheDictionaryStorage>;\n+\n+}\ndiff --git a/src/Dictionaries/IDictionary.h b/src/Dictionaries/IDictionary.h\nindex e0e4c7eb880f..4d51747a6523 100644\n--- a/src/Dictionaries/IDictionary.h\n+++ b/src/Dictionaries/IDictionary.h\n@@ -120,7 +120,36 @@ struct IDictionaryBase : public IExternalLoadable\n         const DataTypePtr & result_type,\n         const Columns & key_columns,\n         const DataTypes & key_types,\n-        const ColumnPtr default_values_column) const = 0;\n+        const ColumnPtr & default_values_column) const = 0;\n+\n+    /** Get multiple columns from dictionary.\n+      *\n+      * Default implementation just calls getColumn multiple times.\n+      * Subclasses can provide custom more efficient implementation.\n+      */\n+    virtual Columns getColumns(\n+        const Strings & attribute_names,\n+        const DataTypes & result_types,\n+        const Columns & key_columns,\n+        const DataTypes & key_types,\n+        const Columns & default_values_columns) const\n+    {\n+        size_t attribute_names_size = attribute_names.size();\n+\n+        Columns result;\n+        result.reserve(attribute_names_size);\n+\n+        for (size_t i = 0; i < attribute_names_size; ++i)\n+        {\n+            const auto & attribute_name = attribute_names[i];\n+            const auto & result_type = result_types[i];\n+            const auto & default_values_column = default_values_columns[i];\n+\n+            result.emplace_back(getColumn(attribute_name, result_type, key_columns, key_types, default_values_column));\n+        }\n+\n+        return result;\n+    }\n \n     /** Subclass must validate key columns and key types and return ColumnUInt8 that\n       * is bitmask representation of is key in dictionary or not.\ndiff --git a/src/Dictionaries/IDictionarySource.h b/src/Dictionaries/IDictionarySource.h\nindex 145b2e03dd2b..90f8b7f3a551 100644\n--- a/src/Dictionaries/IDictionarySource.h\n+++ b/src/Dictionaries/IDictionarySource.h\n@@ -10,6 +10,7 @@ namespace DB\n {\n class IDictionarySource;\n using DictionarySourcePtr = std::unique_ptr<IDictionarySource>;\n+using SharedDictionarySourcePtr = std::shared_ptr<IDictionarySource>;\n \n /** Data-provider interface for external dictionaries,\n *    abstracts out the data source (file, MySQL, ClickHouse, external program, network request et cetera)\ndiff --git a/src/Dictionaries/IPAddressDictionary.cpp b/src/Dictionaries/IPAddressDictionary.cpp\nindex 6447c76ee73e..20160d7b431d 100644\n--- a/src/Dictionaries/IPAddressDictionary.cpp\n+++ b/src/Dictionaries/IPAddressDictionary.cpp\n@@ -267,7 +267,7 @@ ColumnPtr IPAddressDictionary::getColumn(\n     const DataTypePtr & result_type,\n     const Columns & key_columns,\n     const DataTypes & key_types,\n-    const ColumnPtr default_values_column) const\n+    const ColumnPtr & default_values_column) const\n {\n     validateKeyTypes(key_types);\n \n@@ -290,7 +290,6 @@ ColumnPtr IPAddressDictionary::getColumn(\n \n         auto column = ColumnProvider::getColumn(dictionary_attribute, size);\n \n-\n         if constexpr (std::is_same_v<AttributeType, String>)\n         {\n             auto * out = column.get();\ndiff --git a/src/Dictionaries/IPAddressDictionary.h b/src/Dictionaries/IPAddressDictionary.h\nindex 6c5cfa765e86..dcfb26c3c964 100644\n--- a/src/Dictionaries/IPAddressDictionary.h\n+++ b/src/Dictionaries/IPAddressDictionary.h\n@@ -67,7 +67,7 @@ class IPAddressDictionary final : public IDictionaryBase\n         const DataTypePtr & result_type,\n         const Columns & key_columns,\n         const DataTypes & key_types,\n-        const ColumnPtr default_values_column) const override;\n+        const ColumnPtr & default_values_column) const override;\n \n     ColumnUInt8::Ptr hasKeys(const Columns & key_columns, const DataTypes & key_types) const override;\n \ndiff --git a/src/Dictionaries/PolygonDictionary.cpp b/src/Dictionaries/PolygonDictionary.cpp\nindex e0d0fa0a0e68..9cf5b47ac2b0 100644\n--- a/src/Dictionaries/PolygonDictionary.cpp\n+++ b/src/Dictionaries/PolygonDictionary.cpp\n@@ -99,7 +99,7 @@ ColumnPtr IPolygonDictionary::getColumn(\n     const DataTypePtr & result_type,\n     const Columns & key_columns,\n     const DataTypes &,\n-    const ColumnPtr default_values_column) const\n+    const ColumnPtr & default_values_column) const\n {\n     ColumnPtr result;\n \ndiff --git a/src/Dictionaries/PolygonDictionary.h b/src/Dictionaries/PolygonDictionary.h\nindex a0ea189c10a7..362342c684be 100644\n--- a/src/Dictionaries/PolygonDictionary.h\n+++ b/src/Dictionaries/PolygonDictionary.h\n@@ -86,7 +86,7 @@ class IPolygonDictionary : public IDictionaryBase\n         const DataTypePtr & result_type,\n         const Columns & key_columns,\n         const DataTypes & key_types,\n-        const ColumnPtr default_values_column) const override;\n+        const ColumnPtr & default_values_column) const override;\n \n     ColumnUInt8::Ptr hasKeys(const Columns & key_columns, const DataTypes & key_types) const override;\n \ndiff --git a/src/Dictionaries/RangeHashedDictionary.cpp b/src/Dictionaries/RangeHashedDictionary.cpp\nindex 9fb1a57a381b..f5be04c120d2 100644\n--- a/src/Dictionaries/RangeHashedDictionary.cpp\n+++ b/src/Dictionaries/RangeHashedDictionary.cpp\n@@ -93,7 +93,7 @@ ColumnPtr RangeHashedDictionary::getColumn(\n     const DataTypePtr & result_type,\n     const Columns & key_columns,\n     const DataTypes & key_types,\n-    const ColumnPtr default_values_column) const\n+    const ColumnPtr & default_values_column) const\n {\n     ColumnPtr result;\n \ndiff --git a/src/Dictionaries/RangeHashedDictionary.h b/src/Dictionaries/RangeHashedDictionary.h\nindex 80cf47eb93b1..1f93fa757758 100644\n--- a/src/Dictionaries/RangeHashedDictionary.h\n+++ b/src/Dictionaries/RangeHashedDictionary.h\n@@ -61,7 +61,7 @@ class RangeHashedDictionary final : public IDictionaryBase\n         const DataTypePtr & result_type,\n         const Columns & key_columns,\n         const DataTypes & key_types,\n-        const ColumnPtr default_values_column) const override;\n+        const ColumnPtr & default_values_column) const override;\n \n     ColumnUInt8::Ptr hasKeys(const Columns & key_columns, const DataTypes & key_types) const override;\n \ndiff --git a/src/Dictionaries/SSDCacheDictionary.cpp b/src/Dictionaries/SSDCacheDictionary.cpp\ndeleted file mode 100644\nindex cbeea39decb8..000000000000\n--- a/src/Dictionaries/SSDCacheDictionary.cpp\n+++ /dev/null\n@@ -1,1667 +0,0 @@\n-#if defined(OS_LINUX) || defined(__FreeBSD__)\n-\n-#include \"SSDCacheDictionary.h\"\n-\n-#include <algorithm>\n-#include <Columns/ColumnsNumber.h>\n-#include <Common/typeid_cast.h>\n-#include <Common/ProfileEvents.h>\n-#include <Common/ProfilingScopedRWLock.h>\n-#include <Common/MemorySanitizer.h>\n-#include <DataStreams/IBlockInputStream.h>\n-#include \"DictionaryBlockInputStream.h\"\n-#include \"DictionaryFactory.h\"\n-#include <IO/AIO.h>\n-#include <IO/ReadHelpers.h>\n-#include <IO/WriteHelpers.h>\n-#include <ext/chrono_io.h>\n-#include <ext/map.h>\n-#include <ext/range.h>\n-#include <ext/size.h>\n-#include <ext/bit_cast.h>\n-#include <filesystem>\n-#include <city.h>\n-#include <fcntl.h>\n-#include <Functions/FunctionHelpers.h>\n-#include <DataTypes/DataTypesDecimal.h>\n-\n-namespace ProfileEvents\n-{\n-    extern const Event DictCacheKeysRequested;\n-    extern const Event DictCacheKeysRequestedMiss;\n-    extern const Event DictCacheKeysRequestedFound;\n-    extern const Event DictCacheKeysExpired;\n-    extern const Event DictCacheKeysNotFound;\n-    extern const Event DictCacheKeysHit;\n-    extern const Event DictCacheRequestTimeNs;\n-    extern const Event DictCacheRequests;\n-    extern const Event DictCacheLockWriteNs;\n-    extern const Event DictCacheLockReadNs;\n-    extern const Event FileOpen;\n-    extern const Event WriteBufferAIOWrite;\n-    extern const Event WriteBufferAIOWriteBytes;\n-}\n-\n-namespace CurrentMetrics\n-{\n-    extern const Metric DictCacheRequests;\n-    extern const Metric Write;\n-}\n-\n-namespace DB\n-{\n-\n-namespace ErrorCodes\n-{\n-    extern const int AIO_READ_ERROR;\n-    extern const int AIO_WRITE_ERROR;\n-    extern const int BAD_ARGUMENTS;\n-    extern const int CACHE_DICTIONARY_UPDATE_FAIL;\n-    extern const int CANNOT_ALLOCATE_MEMORY;\n-    extern const int CANNOT_CREATE_DIRECTORY;\n-    extern const int CANNOT_FSYNC;\n-    extern const int CANNOT_IO_GETEVENTS;\n-    extern const int CANNOT_IO_SUBMIT;\n-    extern const int CANNOT_OPEN_FILE;\n-    extern const int CORRUPTED_DATA;\n-    extern const int FILE_DOESNT_EXIST;\n-    extern const int LOGICAL_ERROR;\n-    extern const int TYPE_MISMATCH;\n-    extern const int UNSUPPORTED_METHOD;\n-}\n-\n-namespace\n-{\n-    constexpr size_t DEFAULT_SSD_BLOCK_SIZE_BYTES = DEFAULT_AIO_FILE_BLOCK_SIZE;\n-    constexpr size_t DEFAULT_FILE_SIZE_BYTES = 4 * 1024 * 1024 * 1024ULL;\n-    constexpr size_t DEFAULT_PARTITIONS_COUNT = 16;\n-    constexpr size_t DEFAULT_READ_BUFFER_SIZE_BYTES = 16 * DEFAULT_SSD_BLOCK_SIZE_BYTES;\n-    constexpr size_t DEFAULT_WRITE_BUFFER_SIZE_BYTES = DEFAULT_SSD_BLOCK_SIZE_BYTES;\n-\n-    constexpr size_t DEFAULT_MAX_STORED_KEYS = 100000;\n-\n-    constexpr size_t BUFFER_ALIGNMENT = DEFAULT_AIO_FILE_BLOCK_SIZE;\n-    constexpr size_t BLOCK_CHECKSUM_SIZE_BYTES = 8;\n-    constexpr size_t BLOCK_SPECIAL_FIELDS_SIZE_BYTES = 4;\n-\n-    constexpr UInt64 KEY_METADATA_EXPIRES_AT_MASK = std::numeric_limits<std::chrono::system_clock::time_point::rep>::max();\n-    constexpr UInt64 KEY_METADATA_IS_DEFAULT_MASK = ~KEY_METADATA_EXPIRES_AT_MASK;\n-\n-    constexpr size_t KEY_IN_MEMORY_BIT = 63;\n-    constexpr size_t KEY_IN_MEMORY = (1ULL << KEY_IN_MEMORY_BIT);\n-    constexpr size_t BLOCK_INDEX_BITS = 32;\n-    constexpr size_t INDEX_IN_BLOCK_BITS = 16;\n-    constexpr size_t INDEX_IN_BLOCK_MASK = (1ULL << INDEX_IN_BLOCK_BITS) - 1;\n-    constexpr size_t BLOCK_INDEX_MASK = ((1ULL << (BLOCK_INDEX_BITS + INDEX_IN_BLOCK_BITS)) - 1) ^ INDEX_IN_BLOCK_MASK;\n-\n-    constexpr size_t NOT_EXISTS = -1;\n-\n-    constexpr UInt8 HAS_NOT_FOUND = 2;\n-\n-    const std::string BIN_FILE_EXT = \".bin\";\n-\n-    int preallocateDiskSpace(int fd, size_t len)\n-    {\n-        #if defined(__FreeBSD__)\n-            return posix_fallocate(fd, 0, len);\n-        #else\n-            return fallocate(fd, 0, 0, len);\n-        #endif\n-    }\n-}\n-\n-SSDCachePartition::Metadata::time_point_t SSDCachePartition::Metadata::expiresAt() const\n-{\n-    return ext::safe_bit_cast<time_point_t>(data & KEY_METADATA_EXPIRES_AT_MASK);\n-}\n-\n-void SSDCachePartition::Metadata::setExpiresAt(const time_point_t & t)\n-{\n-    data = ext::safe_bit_cast<time_point_urep_t>(t);\n-}\n-\n-bool SSDCachePartition::Metadata::isDefault() const\n-{\n-    return (data & KEY_METADATA_IS_DEFAULT_MASK) == KEY_METADATA_IS_DEFAULT_MASK;\n-}\n-void SSDCachePartition::Metadata::setDefault()\n-{\n-    data |= KEY_METADATA_IS_DEFAULT_MASK;\n-}\n-\n-bool SSDCachePartition::Index::inMemory() const\n-{\n-    return (index & KEY_IN_MEMORY) == KEY_IN_MEMORY;\n-}\n-\n-bool SSDCachePartition::Index::exists() const\n-{\n-    return index != NOT_EXISTS;\n-}\n-\n-void SSDCachePartition::Index::setNotExists()\n-{\n-    index = NOT_EXISTS;\n-}\n-\n-void SSDCachePartition::Index::setInMemory(const bool in_memory)\n-{\n-    index = (index & ~KEY_IN_MEMORY) | (static_cast<size_t>(in_memory) << KEY_IN_MEMORY_BIT);\n-}\n-\n-size_t SSDCachePartition::Index::getAddressInBlock() const\n-{\n-    return index & INDEX_IN_BLOCK_MASK;\n-}\n-\n-void SSDCachePartition::Index::setAddressInBlock(const size_t address_in_block)\n-{\n-    index = (index & ~INDEX_IN_BLOCK_MASK) | address_in_block;\n-}\n-\n-size_t SSDCachePartition::Index::getBlockId() const\n-{\n-    return (index & BLOCK_INDEX_MASK) >> INDEX_IN_BLOCK_BITS;\n-}\n-\n-void SSDCachePartition::Index::setBlockId(const size_t block_id)\n-{\n-    index = (index & ~BLOCK_INDEX_MASK) | (block_id << INDEX_IN_BLOCK_BITS);\n-}\n-\n-SSDCachePartition::SSDCachePartition(\n-        const AttributeUnderlyingType & /* key_structure */,\n-        const std::vector<AttributeUnderlyingType> & attributes_structure_,\n-        const std::string & dir_path,\n-        const size_t file_id_,\n-        const size_t max_size_,\n-        const size_t block_size_,\n-        const size_t read_buffer_size_,\n-        const size_t write_buffer_size_,\n-        const size_t max_stored_keys_)\n-    : file_id(file_id_)\n-    , max_size(max_size_)\n-    , block_size(block_size_)\n-    , read_buffer_size(read_buffer_size_)\n-    , write_buffer_size(write_buffer_size_)\n-    , max_stored_keys(max_stored_keys_)\n-    , path(dir_path + \"/\" + std::to_string(file_id))\n-    , key_to_index(max_stored_keys)\n-    , attributes_structure(attributes_structure_)\n-{\n-    keys_buffer.type = AttributeUnderlyingType::utUInt64;\n-    keys_buffer.values = SSDCachePartition::Attribute::Container<UInt64>();\n-\n-    if (!std::filesystem::create_directories(std::filesystem::path{dir_path}))\n-    {\n-        if (std::filesystem::exists(std::filesystem::path{dir_path}))\n-            LOG_INFO(&Poco::Logger::get(\"SSDCachePartition::Constructor\"), \"Using existing directory '{}' for cache-partition\", dir_path);\n-        else\n-            throw Exception{\"Failed to create directories.\", ErrorCodes::CANNOT_CREATE_DIRECTORY};\n-    }\n-\n-    {\n-        ProfileEvents::increment(ProfileEvents::FileOpen);\n-\n-        const std::string filename = path + BIN_FILE_EXT;\n-        fd = ::open(filename.c_str(), O_RDWR | O_CREAT | O_TRUNC | O_DIRECT, 0666);\n-        if (fd == -1)\n-        {\n-            auto error_code = (errno == ENOENT) ? ErrorCodes::FILE_DOESNT_EXIST : ErrorCodes::CANNOT_OPEN_FILE;\n-            throwFromErrnoWithPath(\"Cannot open file \" + filename, filename, error_code);\n-        }\n-\n-        if (preallocateDiskSpace(fd, max_size * block_size) < 0)\n-        {\n-            throwFromErrnoWithPath(\"Cannot preallocate space for the file \" + filename, filename, ErrorCodes::CANNOT_ALLOCATE_MEMORY);\n-        }\n-    }\n-}\n-\n-SSDCachePartition::~SSDCachePartition()\n-{\n-    std::unique_lock lock(rw_lock);\n-    ::close(fd);\n-}\n-\n-size_t SSDCachePartition::appendDefaults(\n-    const Attribute & new_keys, const PaddedPODArray<Metadata> & metadata, const size_t begin)\n-{\n-    return appendBlock(new_keys, Attributes{}, metadata, begin);\n-}\n-\n-size_t SSDCachePartition::appendBlock(\n-    const Attribute & new_keys, const Attributes & new_attributes, const PaddedPODArray<Metadata> & metadata, const size_t begin)\n-{\n-    std::unique_lock lock(rw_lock);\n-    if (!new_attributes.empty() && new_attributes.size() != attributes_structure.size())\n-        throw Exception{\"Wrong columns number in block.\", ErrorCodes::BAD_ARGUMENTS};\n-\n-    const auto & ids = std::get<Attribute::Container<UInt64>>(new_keys.values);\n-    auto & ids_buffer = std::get<Attribute::Container<UInt64>>(keys_buffer.values);\n-\n-    if (!memory)\n-        memory.emplace(block_size * write_buffer_size, BUFFER_ALIGNMENT);\n-\n-    auto init_write_buffer = [&]()\n-    {\n-        write_buffer.emplace(memory->data() + current_memory_block_id * block_size, block_size);\n-        uint64_t tmp = 0;\n-        write_buffer->write(reinterpret_cast<char*>(&tmp), BLOCK_CHECKSUM_SIZE_BYTES);\n-        write_buffer->write(reinterpret_cast<char*>(&tmp), BLOCK_SPECIAL_FIELDS_SIZE_BYTES);\n-        keys_in_block = 0;\n-    };\n-\n-    if (!write_buffer)\n-        init_write_buffer();\n-\n-    bool flushed = false;\n-    auto finish_block = [&]()\n-    {\n-        write_buffer.reset();\n-        std::memcpy(memory->data() + block_size * current_memory_block_id + BLOCK_CHECKSUM_SIZE_BYTES, &keys_in_block, sizeof(keys_in_block)); // set count\n-        uint64_t checksum = CityHash_v1_0_2::CityHash64(memory->data() + block_size * current_memory_block_id + BLOCK_CHECKSUM_SIZE_BYTES, block_size - BLOCK_CHECKSUM_SIZE_BYTES); // checksum\n-        std::memcpy(memory->data() + block_size * current_memory_block_id, &checksum, sizeof(checksum));\n-        if (++current_memory_block_id == write_buffer_size)\n-            flush();\n-        flushed = true;\n-    };\n-\n-    for (size_t index = begin; index < ids.size();)\n-    {\n-        Index cache_index;\n-        cache_index.setInMemory(true);\n-        cache_index.setBlockId(current_memory_block_id);\n-        if (current_memory_block_id >= write_buffer_size)\n-            throw DB::Exception(\"lel \" + std::to_string(current_memory_block_id) + \" \" +\n-                std::to_string(write_buffer_size) + \" \" + std::to_string(index), ErrorCodes::LOGICAL_ERROR);\n-\n-        cache_index.setAddressInBlock(write_buffer->offset());\n-\n-        flushed = false;\n-        if (2 * sizeof(UInt64) > write_buffer->available()) // place for key and metadata\n-        {\n-            finish_block();\n-        }\n-        else\n-        {\n-            writeBinary(ids[index], *write_buffer);\n-            writeBinary(metadata[index].data, *write_buffer);\n-        }\n-\n-        for (const auto & attribute : new_attributes)\n-        {\n-            if (flushed)\n-                break;\n-            switch (attribute.type)\n-            {\n-#define DISPATCH(TYPE) \\\n-            case AttributeUnderlyingType::ut##TYPE: \\\n-                { \\\n-                    if (sizeof(TYPE) > write_buffer->available()) \\\n-                    { \\\n-                        finish_block(); \\\n-                        continue; \\\n-                    } \\\n-                    else \\\n-                    { \\\n-                        const auto & values = std::get<Attribute::Container<TYPE>>(attribute.values); /* NOLINT */ \\\n-                        writeBinary(values[index], *write_buffer); \\\n-                    } \\\n-                } \\\n-                break;\n-\n-                DISPATCH(UInt8)\n-                DISPATCH(UInt16)\n-                DISPATCH(UInt32)\n-                DISPATCH(UInt64)\n-                DISPATCH(UInt128)\n-                DISPATCH(Int8)\n-                DISPATCH(Int16)\n-                DISPATCH(Int32)\n-                DISPATCH(Int64)\n-                DISPATCH(Decimal32)\n-                DISPATCH(Decimal64)\n-                DISPATCH(Decimal128)\n-                DISPATCH(Float32)\n-                DISPATCH(Float64)\n-#undef DISPATCH\n-\n-            case AttributeUnderlyingType::utString:\n-                {\n-                    const auto & value = std::get<Attribute::Container<String>>(attribute.values)[index];\n-                    if (sizeof(UInt64) + value.size() > write_buffer->available())\n-                    {\n-                        finish_block();\n-                        continue;\n-                    }\n-                    else\n-                    {\n-                        writeStringBinary(value, *write_buffer);\n-                    }\n-                }\n-                break;\n-            }\n-        }\n-\n-        if (!flushed)\n-        {\n-            key_to_index.set(ids[index], cache_index);\n-            ids_buffer.push_back(ids[index]);\n-            ++index;\n-            ++keys_in_block;\n-        }\n-        else  // next block in write buffer or flushed to ssd\n-        {\n-            init_write_buffer();\n-        }\n-    }\n-    return ids.size() - begin;\n-}\n-\n-void SSDCachePartition::flush()\n-{\n-    if (current_file_block_id >= max_size)\n-        clearOldestBlocks();\n-\n-    const auto & ids = std::get<Attribute::Container<UInt64>>(keys_buffer.values);\n-    if (ids.empty())\n-        return;\n-    LOG_INFO(&Poco::Logger::get(\"SSDCachePartition::flush()\"), \"Flushing to Disk.\");\n-\n-    AIOContext aio_context{1};\n-\n-    iocb write_request{};\n-    iocb * write_request_ptr{&write_request};\n-\n-#if defined(__FreeBSD__)\n-    write_request.aio.aio_lio_opcode = LIO_WRITE;\n-    write_request.aio.aio_fildes = fd;\n-    write_request.aio.aio_buf = reinterpret_cast<volatile void *>(memory->data());\n-    write_request.aio.aio_nbytes = block_size * write_buffer_size;\n-    write_request.aio.aio_offset = (current_file_block_id % max_size) * block_size;\n-#else\n-    write_request.aio_lio_opcode = IOCB_CMD_PWRITE;\n-    write_request.aio_fildes = fd;\n-    write_request.aio_buf = reinterpret_cast<UInt64>(memory->data());\n-    write_request.aio_nbytes = block_size * write_buffer_size;\n-    write_request.aio_offset = (current_file_block_id % max_size) * block_size;\n-#endif\n-\n-    while (io_submit(aio_context.ctx, 1, &write_request_ptr) < 0)\n-    {\n-        if (errno != EINTR)\n-            throw Exception(\"Cannot submit request for asynchronous IO on file \" + path + BIN_FILE_EXT, ErrorCodes::CANNOT_IO_SUBMIT);\n-    }\n-\n-    CurrentMetrics::Increment metric_increment_write{CurrentMetrics::Write};\n-\n-    io_event event;\n-    while (io_getevents(aio_context.ctx, 1, 1, &event, nullptr) < 0)\n-    {\n-        if (errno != EINTR)\n-            throw Exception(\"Failed to wait for asynchronous IO completion on file \" + path + BIN_FILE_EXT, ErrorCodes::CANNOT_IO_GETEVENTS);\n-    }\n-\n-    // Unpoison the memory returned from an uninstrumented system function.\n-    __msan_unpoison(&event, sizeof(event));\n-\n-    ssize_t  bytes_written;\n-#if defined(__FreeBSD__)\n-    bytes_written = aio_return(reinterpret_cast<struct aiocb *>(event.udata));\n-#else\n-    bytes_written = event.res;\n-#endif\n-\n-    ProfileEvents::increment(ProfileEvents::WriteBufferAIOWrite);\n-    ProfileEvents::increment(ProfileEvents::WriteBufferAIOWriteBytes, bytes_written);\n-\n-    if (bytes_written != static_cast<decltype(bytes_written)>(block_size * write_buffer_size))\n-        throw Exception(\"Not all data was written for asynchronous IO on file \" + path + BIN_FILE_EXT + \". returned: \" + std::to_string(bytes_written), ErrorCodes::AIO_WRITE_ERROR);\n-\n-    if (::fsync(fd) < 0)\n-        throwFromErrnoWithPath(\"Cannot fsync \" + path + BIN_FILE_EXT, path + BIN_FILE_EXT, ErrorCodes::CANNOT_FSYNC);\n-\n-    /// commit changes in index\n-    for (const auto & id : ids)\n-    {\n-        Index index;\n-        if (key_to_index.get(id, index))\n-        {\n-            if (index.inMemory()) // Row can be inserted in the buffer twice, so we need to move to ssd only the last index.\n-            {\n-                index.setInMemory(false);\n-                index.setBlockId((current_file_block_id % max_size) + index.getBlockId());\n-            }\n-            key_to_index.set(id, index);\n-        }\n-    }\n-\n-    current_file_block_id += write_buffer_size;\n-    current_memory_block_id = 0;\n-\n-    /// clear buffer\n-    std::visit([](auto & attr) { attr.clear(); }, keys_buffer.values);\n-}\n-\n-template <typename Out, typename GetDefault>\n-void SSDCachePartition::getValue(const size_t attribute_index, const PaddedPODArray<UInt64> & ids,\n-    ResultArrayType<Out> & out, std::vector<bool> & found, GetDefault & default_value_extractor,\n-    std::chrono::system_clock::time_point now) const\n-{\n-    auto set_value = [&](const size_t index, ReadBuffer & buf)\n-    {\n-        buf.ignore(sizeof(Key)); // key\n-        Metadata metadata;\n-        readBinary(metadata.data, buf);\n-        if (metadata.expiresAt() > now)\n-        {\n-            if (metadata.isDefault())\n-                out[index] = default_value_extractor[index];\n-            else\n-            {\n-                ignoreFromBufferToAttributeIndex(attribute_index, buf);\n-                readBinary(out[index], buf);\n-            }\n-            found[index] = true;\n-        }\n-    };\n-\n-    getImpl(ids, set_value, found);\n-}\n-\n-void SSDCachePartition::getString(const size_t attribute_index, const PaddedPODArray<UInt64> & ids,\n-    StringRefs & refs, ArenaWithFreeLists & arena, std::vector<bool> & found, std::vector<size_t> & default_ids,\n-    std::chrono::system_clock::time_point now) const\n-{\n-    auto set_value = [&](const size_t index, ReadBuffer & buf)\n-    {\n-        buf.ignore(sizeof(Key)); // key\n-        Metadata metadata;\n-        readBinary(metadata.data, buf);\n-\n-        if (metadata.expiresAt() > now)\n-        {\n-            if (metadata.isDefault())\n-                default_ids.push_back(index);\n-            else\n-            {\n-                ignoreFromBufferToAttributeIndex(attribute_index, buf);\n-                size_t size = 0;\n-                readVarUInt(size, buf);\n-                char * string_ptr = arena.alloc(size);\n-                memcpy(string_ptr, buf.position(), size);\n-                refs[index].data = string_ptr;\n-                refs[index].size = size;\n-            }\n-            found[index] = true;\n-        }\n-    };\n-\n-    getImpl(ids, set_value, found);\n-}\n-\n-void SSDCachePartition::has(const PaddedPODArray<UInt64> & ids, ResultArrayType<UInt8> & out,\n-    std::vector<bool> & found, std::chrono::system_clock::time_point now) const\n-{\n-    auto set_value = [&](const size_t index, ReadBuffer & buf)\n-    {\n-        buf.ignore(sizeof(Key)); // key\n-        Metadata metadata;\n-        readBinary(metadata.data, buf);\n-\n-        if (metadata.expiresAt() > now)\n-            out[index] = !metadata.isDefault();\n-    };\n-\n-    getImpl(ids, set_value, found);\n-}\n-\n-template <typename SetFunc>\n-void SSDCachePartition::getImpl(const PaddedPODArray<UInt64> & ids, SetFunc & set,\n-    std::vector<bool> & found) const\n-{\n-    std::shared_lock lock(rw_lock);\n-    PaddedPODArray<Index> indices(ids.size());\n-    for (size_t i = 0; i < ids.size(); ++i)\n-    {\n-        Index index;\n-        if (found[i])\n-            indices[i].setNotExists();\n-        else if (key_to_index.get(ids[i], index))\n-        {\n-            indices[i] = index;\n-        }\n-        else\n-            indices[i].setNotExists();\n-    }\n-\n-    getValueFromMemory(indices, set);\n-    getValueFromStorage(indices, set);\n-}\n-\n-template <typename SetFunc>\n-void SSDCachePartition::getValueFromMemory(const PaddedPODArray<Index> & indices, SetFunc & set) const\n-{\n-    // Do not check checksum while reading from memory.\n-    for (size_t i = 0; i < indices.size(); ++i)\n-    {\n-        const auto & index = indices[i];\n-        if (index.exists() && index.inMemory())\n-        {\n-            const size_t offset = index.getBlockId() * block_size + index.getAddressInBlock();\n-\n-            ReadBufferFromMemory read_buffer(memory->data() + offset, block_size * write_buffer_size - offset);\n-            set(i, read_buffer);\n-        }\n-    }\n-}\n-\n-template <typename SetFunc>\n-void SSDCachePartition::getValueFromStorage(const PaddedPODArray<Index> & indices, SetFunc & set) const\n-{\n-    std::vector<std::pair<Index, size_t>> index_to_out;\n-    for (size_t i = 0; i < indices.size(); ++i)\n-    {\n-        const auto & index = indices[i];\n-        if (index.exists() && !index.inMemory())\n-            index_to_out.emplace_back(index, i);\n-    }\n-    if (index_to_out.empty())\n-        return;\n-\n-    /// sort by (block_id, offset_in_block)\n-    std::sort(std::begin(index_to_out), std::end(index_to_out));\n-\n-    Memory read_buffer(block_size * read_buffer_size, BUFFER_ALIGNMENT);\n-\n-    std::vector<iocb> requests;\n-    std::vector<iocb*> pointers;\n-    std::vector<std::vector<size_t>> blocks_to_indices;\n-    requests.reserve(index_to_out.size());\n-    pointers.reserve(index_to_out.size());\n-    blocks_to_indices.reserve(index_to_out.size());\n-    for (size_t i = 0; i < index_to_out.size(); ++i)\n-    {\n-        #if defined(__FreeBSD__)\n-        const size_t back_offset = requests.empty() ? -1 : static_cast<size_t>(requests.back().aio.aio_offset);\n-        #else\n-        const size_t back_offset = requests.empty() ? -1 : static_cast<size_t>(requests.back().aio_offset);\n-        #endif\n-\n-        if (!requests.empty() && back_offset == index_to_out[i].first.getBlockId() * block_size)\n-        {\n-            blocks_to_indices.back().push_back(i);\n-            continue;\n-        }\n-\n-        iocb request{};\n-#if defined(__FreeBSD__)\n-        request.aio.aio_lio_opcode = LIO_READ;\n-        request.aio.aio_fildes = fd;\n-        request.aio.aio_buf = reinterpret_cast<volatile void *>(\n-            reinterpret_cast<UInt64>(read_buffer.data()) + block_size * (requests.size() % read_buffer_size));\n-        request.aio.aio_nbytes = block_size;\n-        request.aio.aio_offset = index_to_out[i].first.getBlockId() * block_size;\n-        request.aio_data = requests.size();\n-#else\n-        request.aio_lio_opcode = IOCB_CMD_PREAD;\n-        request.aio_fildes = fd;\n-        request.aio_buf = reinterpret_cast<UInt64>(read_buffer.data()) + block_size * (requests.size() % read_buffer_size);\n-        request.aio_nbytes = block_size;\n-        request.aio_offset = index_to_out[i].first.getBlockId() * block_size;\n-        request.aio_data = requests.size();\n-#endif\n-        requests.push_back(request);\n-        pointers.push_back(&requests.back());\n-        blocks_to_indices.emplace_back();\n-        blocks_to_indices.back().push_back(i);\n-    }\n-\n-    AIOContext aio_context(read_buffer_size);\n-\n-    std::vector<bool> processed(requests.size(), false);\n-    std::vector<io_event> events(requests.size());\n-    #if defined(__linux__)\n-    for (auto & event : events)\n-        event.res = -1;\n-    #endif\n-\n-    size_t to_push = 0;\n-    size_t to_pop = 0;\n-    while (to_pop < requests.size())\n-    {\n-        int popped = 0;\n-        while (to_pop < to_push && (popped = io_getevents(aio_context.ctx, to_push - to_pop, to_push - to_pop, &events[to_pop], nullptr)) <= 0)\n-        {\n-            if (errno != EINTR)\n-                throwFromErrno(\"io_getevents: Failed to get an event for asynchronous IO\", ErrorCodes::CANNOT_IO_GETEVENTS);\n-        }\n-\n-        for (size_t i = to_pop; i < to_pop + popped; ++i)\n-        {\n-            const auto request_id = events[i].data;\n-            const auto & request = requests[request_id];\n-\n-            #if defined(__FreeBSD__)\n-            const auto bytes_written = aio_return(reinterpret_cast<struct aiocb *>(events[i].udata));\n-            #else\n-            const auto bytes_written = events[i].res;\n-            #endif\n-\n-            if (bytes_written != static_cast<ssize_t>(block_size))\n-            {\n-                #if defined(__FreeBSD__)\n-                    throw Exception(\"AIO failed to read file \" + path + BIN_FILE_EXT + \".\", ErrorCodes::AIO_READ_ERROR);\n-                #else\n-                    throw Exception(\"AIO failed to read file \" + path + BIN_FILE_EXT + \". \" +\n-                        \"request_id= \" + std::to_string(request.aio_data) + \"/ \" + std::to_string(requests.size()) +\n-                        \", aio_nbytes=\" + std::to_string(request.aio_nbytes) + \", aio_offset=\" + std::to_string(request.aio_offset) +\n-                        \", returned=\" + std::to_string(events[i].res) + \", errno=\" + std::to_string(errno), ErrorCodes::AIO_READ_ERROR);\n-                #endif\n-            }\n-            #if defined(__FreeBSD__)\n-            const char* buf_ptr = reinterpret_cast<char *>(reinterpret_cast<UInt64>(request.aio.aio_buf));\n-            #else\n-            const auto* buf_ptr = reinterpret_cast<char *>(request.aio_buf);\n-            #endif\n-            __msan_unpoison(buf_ptr, block_size);\n-            uint64_t checksum = 0;\n-            ReadBufferFromMemory buf_special(buf_ptr, block_size);\n-            readBinary(checksum, buf_special);\n-            uint64_t calculated_checksum = CityHash_v1_0_2::CityHash64(buf_ptr + BLOCK_CHECKSUM_SIZE_BYTES, block_size - BLOCK_CHECKSUM_SIZE_BYTES);\n-            if (checksum != calculated_checksum)\n-            {\n-                throw Exception(\"Cache data corrupted. From block = \" + std::to_string(checksum) + \" calculated = \" + std::to_string(calculated_checksum) + \".\", ErrorCodes::CORRUPTED_DATA);\n-            }\n-\n-            for (const size_t idx : blocks_to_indices[request_id])\n-            {\n-                const auto & [file_index, out_index] = index_to_out[idx];\n-                ReadBufferFromMemory buf(\n-                        buf_ptr + file_index.getAddressInBlock(),\n-                        block_size - file_index.getAddressInBlock());\n-                set(out_index, buf);\n-            }\n-\n-            processed[request_id] = true;\n-        }\n-\n-        while (to_pop < requests.size() && processed[to_pop])\n-            ++to_pop;\n-\n-        /// add new io tasks\n-        const int new_tasks_count = std::min(read_buffer_size - (to_push - to_pop), requests.size() - to_push);\n-\n-        int pushed = 0;\n-        while (new_tasks_count > 0 && (pushed = io_submit(aio_context.ctx, new_tasks_count, &pointers[to_push])) <= 0)\n-        {\n-            if (errno != EINTR)\n-                throwFromErrno(\"io_submit: Failed to submit a request for asynchronous IO\", ErrorCodes::CANNOT_IO_SUBMIT);\n-        }\n-        to_push += pushed;\n-    }\n-}\n-\n-void SSDCachePartition::clearOldestBlocks()\n-{\n-    // write_buffer_size, because we need to erase the whole buffer.\n-    Memory read_buffer_memory(block_size * write_buffer_size, BUFFER_ALIGNMENT);\n-\n-    iocb request{};\n-#if defined(__FreeBSD__)\n-    request.aio.aio_lio_opcode = LIO_READ;\n-    request.aio.aio_fildes = fd;\n-    request.aio.aio_buf = reinterpret_cast<volatile void *>(reinterpret_cast<UInt64>(read_buffer_memory.data()));\n-    request.aio.aio_nbytes = block_size * write_buffer_size;\n-    request.aio.aio_offset = (current_file_block_id % max_size) * block_size;\n-    request.aio_data = 0;\n-#else\n-    request.aio_lio_opcode = IOCB_CMD_PREAD;\n-    request.aio_fildes = fd;\n-    request.aio_buf = reinterpret_cast<UInt64>(read_buffer_memory.data());\n-    request.aio_nbytes = block_size * write_buffer_size;\n-    request.aio_offset = (current_file_block_id % max_size) * block_size;\n-    request.aio_data = 0;\n-#endif\n-\n-    {\n-        iocb* request_ptr = &request;\n-        io_event event{};\n-        AIOContext aio_context(1);\n-\n-        while (io_submit(aio_context.ctx, 1, &request_ptr) != 1)\n-        {\n-            if (errno != EINTR)\n-                throwFromErrno(\"io_submit: Failed to submit a request for asynchronous IO\", ErrorCodes::CANNOT_IO_SUBMIT);\n-        }\n-\n-        while (io_getevents(aio_context.ctx, 1, 1, &event, nullptr) != 1)\n-        {\n-            if (errno != EINTR)\n-                throwFromErrno(\"io_getevents: Failed to get an event for asynchronous IO\", ErrorCodes::CANNOT_IO_GETEVENTS);\n-        }\n-\n-#if defined(__FreeBSD__)\n-        if (aio_return(reinterpret_cast<struct aiocb *>(event.udata)) != static_cast<ssize_t>(request.aio.aio_nbytes))\n-            throw Exception(\"GC: AIO failed to read file \" + path + BIN_FILE_EXT + \".\", ErrorCodes::AIO_READ_ERROR);\n-#else\n-        if (event.res != static_cast<ssize_t>(request.aio_nbytes))\n-            throw Exception(\"GC: AIO failed to read file \" + path + BIN_FILE_EXT + \". \" +\n-                \"aio_nbytes=\" + std::to_string(request.aio_nbytes) +\n-                \", returned=\" + std::to_string(event.res) + \".\", ErrorCodes::AIO_READ_ERROR);\n-#endif\n-        __msan_unpoison(read_buffer_memory.data(), read_buffer_memory.size());\n-    }\n-\n-    std::vector<UInt64> keys;\n-    keys.reserve(write_buffer_size);\n-\n-    for (size_t i = 0; i < write_buffer_size; ++i)\n-    {\n-        ReadBufferFromMemory read_buffer(read_buffer_memory.data() + i * block_size, block_size);\n-\n-        uint64_t checksum = 0;\n-        readBinary(checksum, read_buffer);\n-        uint64_t calculated_checksum = CityHash_v1_0_2::CityHash64(read_buffer_memory.data() + i * block_size + BLOCK_CHECKSUM_SIZE_BYTES, block_size - BLOCK_CHECKSUM_SIZE_BYTES);\n-        if (checksum != calculated_checksum)\n-        {\n-            throw Exception(\"Cache data corrupted. From block = \" + std::to_string(checksum) + \" calculated = \" + std::to_string(calculated_checksum) + \".\", ErrorCodes::CORRUPTED_DATA);\n-        }\n-\n-        uint32_t keys_in_current_block = 0;\n-        readBinary(keys_in_current_block, read_buffer);\n-\n-        for (uint32_t j = 0; j < keys_in_current_block; ++j)\n-        {\n-            keys.emplace_back();\n-            readBinary(keys.back(), read_buffer);\n-            Metadata metadata;\n-            readBinary(metadata.data, read_buffer);\n-\n-            if (!metadata.isDefault())\n-            {\n-                for (const auto & attribute : attributes_structure)\n-                {\n-                    switch (attribute)\n-                    {\n-            #define DISPATCH(TYPE) \\\n-                    case AttributeUnderlyingType::ut##TYPE: \\\n-                        read_buffer.ignore(sizeof(TYPE)); \\\n-                        break;\n-\n-                        DISPATCH(UInt8)\n-                        DISPATCH(UInt16)\n-                        DISPATCH(UInt32)\n-                        DISPATCH(UInt64)\n-                        DISPATCH(UInt128)\n-                        DISPATCH(Int8)\n-                        DISPATCH(Int16)\n-                        DISPATCH(Int32)\n-                        DISPATCH(Int64)\n-                        DISPATCH(Decimal32)\n-                        DISPATCH(Decimal64)\n-                        DISPATCH(Decimal128)\n-                        DISPATCH(Float32)\n-                        DISPATCH(Float64)\n-            #undef DISPATCH\n-\n-                    case AttributeUnderlyingType::utString:\n-                        {\n-                            size_t size = 0;\n-                            readVarUInt(size, read_buffer);\n-                            read_buffer.ignore(size);\n-                        }\n-                        break;\n-                    }\n-                }\n-            }\n-        }\n-    }\n-\n-    const size_t start_block = current_file_block_id % max_size;\n-    const size_t finish_block = start_block + write_buffer_size;\n-    for (const auto & key : keys)\n-    {\n-        Index index;\n-        if (key_to_index.get(key, index))\n-        {\n-            size_t block_id = index.getBlockId();\n-            if (start_block <= block_id && block_id < finish_block)\n-                key_to_index.erase(key);\n-        }\n-    }\n-}\n-\n-void SSDCachePartition::ignoreFromBufferToAttributeIndex(const size_t attribute_index, ReadBuffer & buf) const\n-{\n-    for (size_t i = 0; i < attribute_index; ++i)\n-    {\n-        switch (attributes_structure[i])\n-        {\n-#define DISPATCH(TYPE) \\\n-        case AttributeUnderlyingType::ut##TYPE: \\\n-            buf.ignore(sizeof(TYPE)); \\\n-            break;\n-\n-            DISPATCH(UInt8)\n-            DISPATCH(UInt16)\n-            DISPATCH(UInt32)\n-            DISPATCH(UInt64)\n-            DISPATCH(UInt128)\n-            DISPATCH(Int8)\n-            DISPATCH(Int16)\n-            DISPATCH(Int32)\n-            DISPATCH(Int64)\n-            DISPATCH(Decimal32)\n-            DISPATCH(Decimal64)\n-            DISPATCH(Decimal128)\n-            DISPATCH(Float32)\n-            DISPATCH(Float64)\n-#undef DISPATCH\n-\n-        case AttributeUnderlyingType::utString:\n-            {\n-                size_t size = 0;\n-                readVarUInt(size, buf);\n-                buf.ignore(size);\n-            }\n-            break;\n-        }\n-    }\n-}\n-\n-size_t SSDCachePartition::getId() const\n-{\n-    return file_id;\n-}\n-\n-double SSDCachePartition::getLoadFactor() const\n-{\n-    std::shared_lock lock(rw_lock);\n-    return static_cast<double>(current_file_block_id) / max_size;\n-}\n-\n-size_t SSDCachePartition::getElementCount() const\n-{\n-    std::shared_lock lock(rw_lock);\n-    return key_to_index.size();\n-}\n-\n-size_t SSDCachePartition::getBytesAllocated() const\n-{\n-    std::shared_lock lock(rw_lock);\n-    return 16.5 * key_to_index.capacity() + (memory ? memory->size() : 0);\n-}\n-\n-PaddedPODArray<SSDCachePartition::Key> SSDCachePartition::getCachedIds(const std::chrono::system_clock::time_point /* now */) const\n-{\n-    std::unique_lock lock(rw_lock); // Begin and end iterators can be changed.\n-    PaddedPODArray<Key> array;\n-    for (const auto & key : key_to_index.keys())\n-        array.push_back(key);\n-    return array;\n-}\n-\n-void SSDCachePartition::remove()\n-{\n-    std::unique_lock lock(rw_lock);\n-    std::filesystem::remove(std::filesystem::path(path + BIN_FILE_EXT));\n-}\n-\n-SSDCacheStorage::SSDCacheStorage(\n-        const AttributeTypes & attributes_structure_,\n-        const std::string & path_,\n-        const size_t max_partitions_count_,\n-        const size_t file_size_,\n-        const size_t block_size_,\n-        const size_t read_buffer_size_,\n-        const size_t write_buffer_size_,\n-        const size_t max_stored_keys_)\n-    : attributes_structure(attributes_structure_)\n-    , path(path_)\n-    , max_partitions_count(max_partitions_count_)\n-    , file_size(file_size_)\n-    , block_size(block_size_)\n-    , read_buffer_size(read_buffer_size_)\n-    , write_buffer_size(write_buffer_size_)\n-    , max_stored_keys(max_stored_keys_)\n-    , log(&Poco::Logger::get(\"SSDCacheStorage\"))\n-{\n-}\n-\n-SSDCacheStorage::~SSDCacheStorage()\n-{\n-    std::unique_lock lock(rw_lock);\n-    partition_delete_queue.splice(std::end(partition_delete_queue), partitions);\n-    collectGarbage();\n-}\n-\n-template <typename Out, typename GetDefault>\n-void SSDCacheStorage::getValue(const size_t attribute_index, const PaddedPODArray<UInt64> & ids,\n-      ResultArrayType<Out> & out, std::unordered_map<Key, std::vector<size_t>> & not_found,\n-      GetDefault & default_value_extractor, std::chrono::system_clock::time_point now) const\n-{\n-    std::vector<bool> found(ids.size(), false);\n-\n-    {\n-        std::shared_lock lock(rw_lock);\n-        for (const auto & partition : partitions)\n-            partition->getValue<Out>(attribute_index, ids, out, found, default_value_extractor, now);\n-    }\n-\n-    for (size_t i = 0; i < ids.size(); ++i)\n-        if (!found[i])\n-            not_found[ids[i]].push_back(i);\n-\n-    query_count.fetch_add(ids.size(), std::memory_order_relaxed);\n-    hit_count.fetch_add(ids.size() - not_found.size(), std::memory_order_release);\n-}\n-\n-void SSDCacheStorage::getString(const size_t attribute_index, const PaddedPODArray<UInt64> & ids,\n-    StringRefs & refs, ArenaWithFreeLists & arena, std::unordered_map<Key, std::vector<size_t>> & not_found,\n-    std::vector<size_t> & default_ids, std::chrono::system_clock::time_point now) const\n-{\n-    std::vector<bool> found(ids.size(), false);\n-\n-    {\n-        std::shared_lock lock(rw_lock);\n-        for (const auto & partition : partitions)\n-            partition->getString(attribute_index, ids, refs, arena, found, default_ids, now);\n-    }\n-\n-    for (size_t i = 0; i < ids.size(); ++i)\n-        if (!found[i])\n-            not_found[ids[i]].push_back(i);\n-\n-    query_count.fetch_add(ids.size(), std::memory_order_relaxed);\n-    hit_count.fetch_add(ids.size() - not_found.size(), std::memory_order_release);\n-}\n-\n-void SSDCacheStorage::has(const PaddedPODArray<UInt64> & ids, ResultArrayType<UInt8> & out,\n-     std::unordered_map<Key, std::vector<size_t>> & not_found, std::chrono::system_clock::time_point now) const\n-{\n-    for (size_t i = 0; i < ids.size(); ++i)\n-        out[i] = HAS_NOT_FOUND;\n-    std::vector<bool> found(ids.size(), false);\n-\n-    {\n-        std::shared_lock lock(rw_lock);\n-        for (const auto & partition : partitions)\n-            partition->has(ids, out, found, now);\n-\n-        for (size_t i = 0; i < ids.size(); ++i)\n-            if (out[i] == HAS_NOT_FOUND)\n-                not_found[ids[i]].push_back(i);\n-    }\n-\n-    query_count.fetch_add(ids.size(), std::memory_order_relaxed);\n-    hit_count.fetch_add(ids.size() - not_found.size(), std::memory_order_release);\n-}\n-\n-namespace\n-{\n-SSDCachePartition::Attributes createAttributesFromBlock(\n-        const Block & block, const size_t begin_column, const std::vector<AttributeUnderlyingType> & structure)\n-{\n-    SSDCachePartition::Attributes attributes;\n-\n-    const auto columns = block.getColumns();\n-    for (size_t i = 0; i < structure.size(); ++i)\n-    {\n-        const auto & column = columns[i + begin_column];\n-        switch (structure[i])\n-        {\n-#define DISPATCH(TYPE) \\\n-        case AttributeUnderlyingType::ut##TYPE: \\\n-            { \\\n-                SSDCachePartition::Attribute::Container<TYPE> values(column->size()); \\\n-                memcpy(&values[0], column->getRawData().data, sizeof(TYPE) * values.size()); \\\n-                attributes.emplace_back(); \\\n-                attributes.back().type = structure[i]; \\\n-                attributes.back().values = std::move(values); \\\n-            } \\\n-            break;\n-\n-            DISPATCH(UInt8)\n-            DISPATCH(UInt16)\n-            DISPATCH(UInt32)\n-            DISPATCH(UInt64)\n-            DISPATCH(UInt128)\n-            DISPATCH(Int8)\n-            DISPATCH(Int16)\n-            DISPATCH(Int32)\n-            DISPATCH(Int64)\n-            DISPATCH(Decimal32)\n-            DISPATCH(Decimal64)\n-            DISPATCH(Decimal128)\n-            DISPATCH(Float32)\n-            DISPATCH(Float64)\n-#undef DISPATCH\n-\n-        case AttributeUnderlyingType::utString:\n-            {\n-                attributes.emplace_back();\n-                SSDCachePartition::Attribute::Container<String> values(column->size());\n-                for (size_t j = 0; j < column->size(); ++j)\n-                {\n-                    const auto ref = column->getDataAt(j);\n-                    values[j].resize(ref.size);\n-                    memcpy(values[j].data(), ref.data, ref.size);\n-                }\n-                attributes.back().type = structure[i];\n-                attributes.back().values = std::move(values);\n-            }\n-            break;\n-        }\n-    }\n-\n-    return attributes;\n-}\n-}\n-\n-template <typename PresentIdHandler, typename AbsentIdHandler>\n-void SSDCacheStorage::update(DictionarySourcePtr & source_ptr, const std::vector<Key> & requested_ids,\n-        PresentIdHandler && on_updated, AbsentIdHandler && on_id_not_found,\n-        const DictionaryLifetime lifetime)\n-{\n-    auto append_block = [this](const SSDCachePartition::Attribute & new_keys,\n-            const SSDCachePartition::Attributes & new_attributes, const PaddedPODArray<SSDCachePartition::Metadata> & metadata)\n-    {\n-        size_t inserted = 0;\n-        while (inserted < metadata.size())\n-        {\n-            if (!partitions.empty())\n-                inserted += partitions.front()->appendBlock(new_keys, new_attributes, metadata, inserted);\n-            if (inserted < metadata.size())\n-            {\n-                partitions.emplace_front(std::make_unique<SSDCachePartition>(\n-                        AttributeUnderlyingType::utUInt64, attributes_structure, path,\n-                        (partitions.empty() ? 0 : partitions.front()->getId() + 1),\n-                        file_size, block_size, read_buffer_size, write_buffer_size, max_stored_keys));\n-            }\n-        }\n-\n-        collectGarbage();\n-    };\n-\n-    CurrentMetrics::Increment metric_increment{CurrentMetrics::DictCacheRequests};\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysRequested, requested_ids.size());\n-\n-    std::unordered_map<Key, UInt8> remaining_ids{requested_ids.size()};\n-    for (const auto id : requested_ids)\n-        remaining_ids.insert({id, 0});\n-\n-    const auto now = std::chrono::system_clock::now();\n-\n-    {\n-        const ProfilingScopedWriteRWLock write_lock{rw_lock, ProfileEvents::DictCacheLockWriteNs};\n-\n-        if (now > backoff_end_time)\n-        {\n-            try\n-            {\n-                if (update_error_count)\n-                {\n-                    /// Recover after error: we have to clone the source here because\n-                    /// it could keep connections which should be reset after error.\n-                    source_ptr = source_ptr->clone();\n-                }\n-\n-                Stopwatch watch;\n-                auto stream = source_ptr->loadIds(requested_ids);\n-                stream->readPrefix();\n-\n-                while (const auto block = stream->read())\n-                {\n-                    const auto new_keys = std::move(createAttributesFromBlock(block, 0, { AttributeUnderlyingType::utUInt64 }).front());\n-                    const auto new_attributes = createAttributesFromBlock(block, 1, attributes_structure);\n-\n-                    const auto & ids = std::get<SSDCachePartition::Attribute::Container<UInt64>>(new_keys.values);\n-\n-                    PaddedPODArray<SSDCachePartition::Metadata> metadata(ids.size());\n-\n-                    for (const auto i : ext::range(0, ids.size()))\n-                    {\n-                        std::uniform_int_distribution<UInt64> distribution{lifetime.min_sec, lifetime.max_sec};\n-                        metadata[i].setExpiresAt(now + std::chrono::seconds(distribution(rnd_engine)));\n-                        /// mark corresponding id as found\n-                        on_updated(ids[i], i, new_attributes);\n-                        remaining_ids[ids[i]] = 1;\n-                    }\n-\n-                    append_block(new_keys, new_attributes, metadata);\n-                }\n-\n-                stream->readSuffix();\n-\n-                update_error_count = 0;\n-                last_update_exception = std::exception_ptr{};\n-                backoff_end_time = std::chrono::system_clock::time_point{};\n-\n-                ProfileEvents::increment(ProfileEvents::DictCacheRequestTimeNs, watch.elapsed());\n-            }\n-            catch (...)\n-            {\n-                ++update_error_count;\n-                last_update_exception = std::current_exception();\n-                backoff_end_time = now + std::chrono::seconds(calculateDurationWithBackoff(rnd_engine, update_error_count));\n-\n-                tryLogException(last_update_exception, log,\n-                        \"Could not update ssd cache dictionary, next update is scheduled at \" + ext::to_string(backoff_end_time));\n-            }\n-        }\n-    }\n-\n-    auto append_defaults = [this](const SSDCachePartition::Attribute & new_keys, const PaddedPODArray<SSDCachePartition::Metadata> & metadata)\n-    {\n-        size_t inserted = 0;\n-        while (inserted < metadata.size())\n-        {\n-            if (!partitions.empty())\n-                inserted += partitions.front()->appendDefaults(new_keys, metadata, inserted);\n-            if (inserted < metadata.size())\n-            {\n-                partitions.emplace_front(std::make_unique<SSDCachePartition>(\n-                        AttributeUnderlyingType::utUInt64, attributes_structure, path,\n-                        (partitions.empty() ? 0 : partitions.front()->getId() + 1),\n-                        file_size, block_size, read_buffer_size, write_buffer_size, max_stored_keys));\n-            }\n-        }\n-\n-        collectGarbage();\n-    };\n-\n-    size_t not_found_num = 0, found_num = 0;\n-    /// Check which ids have not been found and require setting null_value\n-    SSDCachePartition::Attribute new_keys;\n-    new_keys.type = AttributeUnderlyingType::utUInt64;\n-    new_keys.values = SSDCachePartition::Attribute::Container<UInt64>();\n-\n-    PaddedPODArray<SSDCachePartition::Metadata> metadata;\n-    {\n-        const ProfilingScopedWriteRWLock write_lock{rw_lock, ProfileEvents::DictCacheLockWriteNs};\n-\n-        for (const auto & id_found_pair : remaining_ids)\n-        {\n-            if (id_found_pair.second)\n-            {\n-                ++found_num;\n-                continue;\n-            }\n-            ++not_found_num;\n-\n-            const auto id = id_found_pair.first;\n-\n-            if (update_error_count)\n-            {\n-                /// TODO: use old values\n-\n-                // We don't have expired data for that `id` so all we can do is\n-                // to rethrow `last_exception`. We might have to throw the same\n-                // exception for different callers of dictGet() in different\n-                // threads, which might then modify the exception object, so we\n-                // have to throw a copy.\n-                try\n-                {\n-                    std::rethrow_exception(last_update_exception);\n-                }\n-                catch (...)\n-                {\n-                    throw DB::Exception(ErrorCodes::CACHE_DICTIONARY_UPDATE_FAIL,\n-                        \"Update failed for dictionary '{}': {}\",\n-                        getPath(),\n-                        getCurrentExceptionMessage(true /*with stack trace*/,\n-                            true /*check embedded stack trace*/));\n-                }\n-            }\n-\n-            /// Set key\n-            std::get<SSDCachePartition::Attribute::Container<UInt64>>(new_keys.values).push_back(id);\n-\n-            std::uniform_int_distribution<UInt64> distribution{lifetime.min_sec, lifetime.max_sec};\n-            metadata.emplace_back();\n-            metadata.back().setExpiresAt(now + std::chrono::seconds(distribution(rnd_engine)));\n-            metadata.back().setDefault();\n-\n-            /// Inform caller that the cell has not been found\n-            on_id_not_found(id);\n-        }\n-\n-        if (not_found_num)\n-            append_defaults(new_keys, metadata);\n-    }\n-\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysRequestedMiss, not_found_num);\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysRequestedFound, found_num);\n-    ProfileEvents::increment(ProfileEvents::DictCacheRequests);\n-}\n-\n-PaddedPODArray<SSDCachePartition::Key> SSDCacheStorage::getCachedIds() const\n-{\n-    PaddedPODArray<Key> array;\n-\n-    const auto now = std::chrono::system_clock::now();\n-\n-    std::shared_lock lock(rw_lock);\n-    for (const auto & partition : partitions)\n-    {\n-        const auto cached_in_partition = partition->getCachedIds(now);\n-        array.insert(std::begin(cached_in_partition), std::end(cached_in_partition));\n-    }\n-\n-    return array;\n-}\n-\n-double SSDCacheStorage::getLoadFactor() const\n-{\n-    double result = 0;\n-    std::shared_lock lock(rw_lock);\n-    for (const auto & partition : partitions)\n-        result += partition->getLoadFactor();\n-    return result / partitions.size();\n-}\n-\n-size_t SSDCacheStorage::getElementCount() const\n-{\n-    size_t result = 0;\n-    std::shared_lock lock(rw_lock);\n-    for (const auto & partition : partitions)\n-        result += partition->getElementCount();\n-    return result;\n-}\n-\n-size_t SSDCacheStorage::getBytesAllocated() const\n-{\n-    size_t result = 0;\n-    std::shared_lock lock(rw_lock);\n-    for (const auto & partition : partitions)\n-        result += partition->getBytesAllocated();\n-    return result;\n-}\n-\n-void SSDCacheStorage::collectGarbage()\n-{\n-    // add partitions to queue\n-    while (partitions.size() > max_partitions_count)\n-        partition_delete_queue.splice(std::end(partition_delete_queue), partitions, std::prev(std::end(partitions)));\n-\n-    // drop unused partitions\n-    while (!partition_delete_queue.empty() && partition_delete_queue.front().use_count() == 1)\n-    {\n-        partition_delete_queue.front()->remove();\n-        partition_delete_queue.pop_front();\n-    }\n-}\n-\n-SSDCacheDictionary::SSDCacheDictionary(\n-    const StorageID & dict_id_,\n-    const DictionaryStructure & dict_struct_,\n-    DictionarySourcePtr source_ptr_,\n-    const DictionaryLifetime dict_lifetime_,\n-    const std::string & path_,\n-    const size_t max_partitions_count_,\n-    const size_t file_size_,\n-    const size_t block_size_,\n-    const size_t read_buffer_size_,\n-    const size_t write_buffer_size_,\n-    const size_t max_stored_keys_)\n-    : IDictionary(dict_id_)\n-    , dict_struct(dict_struct_)\n-    , source_ptr(std::move(source_ptr_))\n-    , dict_lifetime(dict_lifetime_)\n-    , path(path_)\n-    , max_partitions_count(max_partitions_count_)\n-    , file_size(file_size_)\n-    , block_size(block_size_)\n-    , read_buffer_size(read_buffer_size_)\n-    , write_buffer_size(write_buffer_size_)\n-    , max_stored_keys(max_stored_keys_)\n-    , storage(ext::map<std::vector>(dict_struct.attributes, [](const auto & attribute) { return attribute.underlying_type; }),\n-            path, max_partitions_count, file_size, block_size, read_buffer_size, write_buffer_size, max_stored_keys)\n-    , log(&Poco::Logger::get(\"SSDCacheDictionary\"))\n-{\n-    LOG_INFO(log, \"Using storage path '{}'.\", path);\n-    if (!this->source_ptr->supportsSelectiveLoad())\n-        throw Exception{name + \": source cannot be used with CacheDictionary\", ErrorCodes::UNSUPPORTED_METHOD};\n-\n-    createAttributes();\n-}\n-\n-ColumnPtr SSDCacheDictionary::getColumn(\n-    const std::string & attribute_name,\n-    const DataTypePtr & result_type,\n-    const Columns & key_columns,\n-    const DataTypes &,\n-    const ColumnPtr default_values_column) const\n-{\n-    ColumnPtr result;\n-\n-    PaddedPODArray<Key> backup_storage;\n-    const auto & ids = getColumnVectorData(this, key_columns.front(), backup_storage);\n-    auto keys_size = ids.size();\n-\n-    const auto index = getAttributeIndex(attribute_name);\n-    const auto & dictionary_attribute = dict_struct.getAttribute(attribute_name, result_type);\n-\n-    auto type_call = [&](const auto &dictionary_attribute_type)\n-    {\n-        using Type = std::decay_t<decltype(dictionary_attribute_type)>;\n-        using AttributeType = typename Type::AttributeType;\n-        using ColumnProvider = DictionaryAttributeColumnProvider<AttributeType>;\n-\n-        const auto & null_value = std::get<AttributeType>(null_values[index]);\n-        DictionaryDefaultValueExtractor<AttributeType> default_value_extractor(null_value, default_values_column);\n-\n-        auto column = ColumnProvider::getColumn(dictionary_attribute, keys_size);\n-\n-        if constexpr (std::is_same_v<AttributeType, String>)\n-        {\n-            getItemsStringImpl(index, ids, column.get(), default_value_extractor);\n-        }\n-        else\n-        {\n-            auto & out = column->getData();\n-            getItemsNumberImpl<AttributeType, AttributeType>(index, ids, out, default_value_extractor);\n-        }\n-\n-        result = std::move(column);\n-    };\n-\n-    callOnDictionaryAttributeType(dict_struct.attributes[index].underlying_type, type_call);\n-\n-    return result;\n-}\n-\n-template <typename AttributeType, typename OutputType, typename DefaultGetter>\n-void SSDCacheDictionary::getItemsNumberImpl(\n-        const size_t attribute_index,\n-        const PaddedPODArray<Key> & ids,\n-        ResultArrayType<OutputType> & out,\n-        DefaultGetter & default_value_extractor) const\n-{\n-    const auto now = std::chrono::system_clock::now();\n-\n-    std::unordered_map<Key, std::vector<size_t>> not_found_ids;\n-    storage.getValue<OutputType>(attribute_index, ids, out, not_found_ids, default_value_extractor, now);\n-    if (not_found_ids.empty())\n-        return;\n-\n-    std::vector<Key> required_ids(not_found_ids.size());\n-    std::transform(std::begin(not_found_ids), std::end(not_found_ids), std::begin(required_ids), [](const auto & pair) { return pair.first; });\n-\n-    storage.update(\n-            source_ptr,\n-            required_ids,\n-            [&](const auto id, const auto row, const auto & new_attributes)\n-            {\n-                for (const size_t out_row : not_found_ids[id])\n-                    out[out_row] = std::get<SSDCachePartition::Attribute::Container<OutputType>>(new_attributes[attribute_index].values)[row];\n-            },\n-            [&](const size_t id)\n-            {\n-                for (const size_t row : not_found_ids[id])\n-                    out[row] = default_value_extractor[row];\n-            },\n-            getLifetime());\n-}\n-\n-template <typename DefaultGetter>\n-void SSDCacheDictionary::getItemsStringImpl(\n-    const size_t attribute_index,\n-    const PaddedPODArray<Key> & ids,\n-    ColumnString * out,\n-    DefaultGetter & default_value_extractor) const\n-{\n-    const auto now = std::chrono::system_clock::now();\n-\n-    std::unordered_map<Key, std::vector<size_t>> not_found_ids;\n-\n-    StringRefs refs(ids.size());\n-    ArenaWithFreeLists string_arena;\n-    std::vector<size_t> default_rows;\n-    storage.getString(attribute_index, ids, refs, string_arena, not_found_ids, default_rows, now);\n-    std::sort(std::begin(default_rows), std::end(default_rows));\n-\n-    if (not_found_ids.empty())\n-    {\n-        size_t default_index = 0;\n-        for (size_t row = 0; row < ids.size(); ++row)\n-        {\n-            if (unlikely(default_index != default_rows.size() && default_rows[default_index] == row))\n-            {\n-                auto to_insert = default_value_extractor[row];\n-                out->insertData(to_insert.data, to_insert.size);\n-                ++default_index;\n-            }\n-            else\n-                out->insertData(refs[row].data, refs[row].size);\n-        }\n-        return;\n-    }\n-\n-    std::vector<Key> required_ids(not_found_ids.size());\n-    std::transform(std::begin(not_found_ids), std::end(not_found_ids), std::begin(required_ids), [](const auto & pair) { return pair.first; });\n-\n-    std::unordered_map<Key, String> update_result;\n-\n-    storage.update(\n-            source_ptr,\n-            required_ids,\n-            [&](const auto id, const auto row, const auto & new_attributes)\n-            {\n-                update_result[id] = std::get<SSDCachePartition::Attribute::Container<String>>(new_attributes[attribute_index].values)[row];\n-            },\n-            [&](const size_t) {},\n-            getLifetime());\n-\n-    size_t default_index = 0;\n-    for (size_t row = 0; row < ids.size(); ++row)\n-    {\n-        const auto & id = ids[row];\n-        if (unlikely(default_index != default_rows.size() && default_rows[default_index] == row))\n-        {\n-            auto to_insert = default_value_extractor[row];\n-            out->insertData(to_insert.data, to_insert.size);\n-            ++default_index;\n-        }\n-        else if (auto it = not_found_ids.find(id); it == std::end(not_found_ids))\n-        {\n-            out->insertData(refs[row].data, refs[row].size);\n-        }\n-        else if (auto it_update = update_result.find(id); it_update != std::end(update_result))\n-        {\n-            out->insertData(it_update->second.data(), it_update->second.size());\n-        }\n-        else\n-        {\n-            auto to_insert = default_value_extractor[row];\n-            out->insertData(to_insert.data, to_insert.size);\n-        }\n-    }\n-}\n-\n-ColumnUInt8::Ptr SSDCacheDictionary::hasKeys(const Columns & key_columns, const DataTypes &) const\n-{\n-    PaddedPODArray<Key> backup_storage;\n-    const auto& ids = getColumnVectorData(this, key_columns.front(), backup_storage);\n-\n-    auto result = ColumnUInt8::create(ext::size(ids));\n-    auto& out = result->getData();\n-\n-    const auto rows = ext::size(ids);\n-    for (const auto row : ext::range(0, rows))\n-        out[row] = false;\n-\n-    const auto now = std::chrono::system_clock::now();\n-\n-    std::unordered_map<Key, std::vector<size_t>> not_found_ids;\n-    storage.has(ids, out, not_found_ids, now);\n-    if (not_found_ids.empty())\n-        return result;\n-\n-    std::vector<Key> required_ids(not_found_ids.size());\n-    std::transform(std::begin(not_found_ids), std::end(not_found_ids), std::begin(required_ids), [](const auto & pair) { return pair.first; });\n-\n-    storage.update(\n-            source_ptr,\n-            required_ids,\n-            [&](const auto id, const auto, const auto &)\n-            {\n-                for (const size_t out_row : not_found_ids[id])\n-                    out[out_row] = true;\n-            },\n-            [&](const size_t id)\n-            {\n-                for (const size_t row : not_found_ids[id])\n-                    out[row] = false;\n-            },\n-            getLifetime());\n-\n-    return result;\n-}\n-\n-BlockInputStreamPtr SSDCacheDictionary::getBlockInputStream(const Names & column_names, size_t max_block_size) const\n-{\n-    using BlockInputStreamType = DictionaryBlockInputStream<Key>;\n-    return std::make_shared<BlockInputStreamType>(shared_from_this(), max_block_size, storage.getCachedIds(), column_names);\n-}\n-\n-size_t SSDCacheDictionary::getAttributeIndex(const std::string & attr_name) const\n-{\n-    auto it = attribute_index_by_name.find(attr_name);\n-    if (it == std::end(attribute_index_by_name))\n-        throw  Exception{\"Attribute `\" + name + \"` does not exist.\", ErrorCodes::BAD_ARGUMENTS};\n-    return it->second;\n-}\n-\n-template <typename T>\n-AttributeValueVariant SSDCacheDictionary::createAttributeNullValueWithTypeImpl(const Field & null_value)\n-{\n-    AttributeValueVariant var_null_value = static_cast<T>(null_value.get<NearestFieldType<T>>());\n-    bytes_allocated += sizeof(T);\n-    return var_null_value;\n-}\n-\n-template <>\n-AttributeValueVariant SSDCacheDictionary::createAttributeNullValueWithTypeImpl<String>(const Field & null_value)\n-{\n-    AttributeValueVariant var_null_value = null_value.get<String>();\n-    bytes_allocated += sizeof(StringRef);\n-    return var_null_value;\n-}\n-\n-AttributeValueVariant SSDCacheDictionary::createAttributeNullValueWithType(const AttributeUnderlyingType type, const Field & null_value)\n-{\n-    switch (type)\n-    {\n-#define DISPATCH(TYPE) \\\n-case AttributeUnderlyingType::ut##TYPE: \\\n-    return createAttributeNullValueWithTypeImpl<TYPE>(null_value);\n-\n-        DISPATCH(UInt8)\n-        DISPATCH(UInt16)\n-        DISPATCH(UInt32)\n-        DISPATCH(UInt64)\n-        DISPATCH(UInt128)\n-        DISPATCH(Int8)\n-        DISPATCH(Int16)\n-        DISPATCH(Int32)\n-        DISPATCH(Int64)\n-        DISPATCH(Decimal32)\n-        DISPATCH(Decimal64)\n-        DISPATCH(Decimal128)\n-        DISPATCH(Float32)\n-        DISPATCH(Float64)\n-        DISPATCH(String)\n-#undef DISPATCH\n-    }\n-    throw Exception{\"Unknown attribute type: \" + std::to_string(static_cast<int>(type)), ErrorCodes::TYPE_MISMATCH};\n-}\n-\n-void SSDCacheDictionary::createAttributes()\n-{\n-    null_values.reserve(dict_struct.attributes.size());\n-    for (size_t i = 0; i < dict_struct.attributes.size(); ++i)\n-    {\n-        const auto & attribute = dict_struct.attributes[i];\n-\n-        attribute_index_by_name.emplace(attribute.name, i);\n-        null_values.push_back(createAttributeNullValueWithType(attribute.underlying_type, attribute.null_value));\n-\n-        if (attribute.hierarchical)\n-            throw Exception{name + \": hierarchical attributes not supported for dictionary of type \" + getTypeName(),\n-                            ErrorCodes::TYPE_MISMATCH};\n-    }\n-}\n-\n-void registerDictionarySSDCache(DictionaryFactory & factory)\n-{\n-    auto create_layout = [=](const std::string & name,\n-                             const DictionaryStructure & dict_struct,\n-                             const Poco::Util::AbstractConfiguration & config,\n-                             const std::string & config_prefix,\n-                             DictionarySourcePtr source_ptr) -> DictionaryPtr\n-    {\n-        if (dict_struct.key)\n-            throw Exception{\"'key' is not supported for dictionary of layout 'cache'\", ErrorCodes::UNSUPPORTED_METHOD};\n-\n-        const auto dict_id = StorageID::fromDictionaryConfig(config, config_prefix);\n-\n-        if (dict_struct.range_min || dict_struct.range_max)\n-            throw Exception{name\n-                            + \": elements .structure.range_min and .structure.range_max should be defined only \"\n-                              \"for a dictionary of layout 'range_hashed'\",\n-                            ErrorCodes::BAD_ARGUMENTS};\n-        const auto & layout_prefix = config_prefix + \".layout\";\n-\n-        const auto max_partitions_count = config.getInt(layout_prefix + \".ssd_cache.max_partitions_count\", DEFAULT_PARTITIONS_COUNT);\n-        if (max_partitions_count <= 0)\n-            throw Exception{name + \": dictionary of layout 'ssd_cache' cannot have 0 (or less) max_partitions_count\", ErrorCodes::BAD_ARGUMENTS};\n-\n-        const auto block_size = config.getInt(layout_prefix + \".ssd_cache.block_size\", DEFAULT_SSD_BLOCK_SIZE_BYTES);\n-        if (block_size <= 0)\n-            throw Exception{name + \": dictionary of layout 'ssd_cache' cannot have 0 (or less) block_size\", ErrorCodes::BAD_ARGUMENTS};\n-\n-        const auto file_size = config.getInt64(layout_prefix + \".ssd_cache.file_size\", DEFAULT_FILE_SIZE_BYTES);\n-        if (file_size <= 0)\n-            throw Exception{name + \": dictionary of layout 'ssd_cache' cannot have 0 (or less) file_size\", ErrorCodes::BAD_ARGUMENTS};\n-        if (file_size % block_size != 0)\n-            throw Exception{name + \": file_size must be a multiple of block_size\", ErrorCodes::BAD_ARGUMENTS};\n-\n-        const auto read_buffer_size = config.getInt64(layout_prefix + \".ssd_cache.read_buffer_size\", DEFAULT_READ_BUFFER_SIZE_BYTES);\n-        if (read_buffer_size <= 0)\n-            throw Exception{name + \": dictionary of layout 'ssd_cache' cannot have 0 (or less) read_buffer_size\", ErrorCodes::BAD_ARGUMENTS};\n-        if (read_buffer_size % block_size != 0)\n-            throw Exception{name + \": read_buffer_size must be a multiple of block_size\", ErrorCodes::BAD_ARGUMENTS};\n-\n-        const auto write_buffer_size = config.getInt64(layout_prefix + \".ssd_cache.write_buffer_size\", DEFAULT_WRITE_BUFFER_SIZE_BYTES);\n-        if (write_buffer_size <= 0)\n-            throw Exception{name + \": dictionary of layout 'ssd_cache' cannot have 0 (or less) write_buffer_size\", ErrorCodes::BAD_ARGUMENTS};\n-        if (write_buffer_size % block_size != 0)\n-            throw Exception{name + \": write_buffer_size must be a multiple of block_size\", ErrorCodes::BAD_ARGUMENTS};\n-\n-        auto path = config.getString(layout_prefix + \".ssd_cache.path\");\n-        if (path.empty())\n-            throw Exception{name + \": dictionary of layout 'ssd_cache' cannot have empty path\",\n-                            ErrorCodes::BAD_ARGUMENTS};\n-        if (path.at(0) != '/')\n-            path = std::filesystem::path{config.getString(\"path\")}.concat(path).string();\n-\n-        const auto max_stored_keys = config.getInt64(layout_prefix + \".ssd_cache.max_stored_keys\", DEFAULT_MAX_STORED_KEYS);\n-        if (max_stored_keys <= 0)\n-            throw Exception{name + \": dictionary of layout 'ssd_cache' cannot have 0 (or less) max_stored_keys\", ErrorCodes::BAD_ARGUMENTS};\n-\n-        const DictionaryLifetime dict_lifetime{config, config_prefix + \".lifetime\"};\n-        return std::make_unique<SSDCacheDictionary>(\n-                dict_id, dict_struct, std::move(source_ptr), dict_lifetime, path,\n-                max_partitions_count, file_size / block_size, block_size,\n-                read_buffer_size / block_size, write_buffer_size / block_size,\n-                max_stored_keys);\n-    };\n-    factory.registerLayout(\"ssd_cache\", create_layout, false);\n-}\n-\n-}\n-\n-#endif\ndiff --git a/src/Dictionaries/SSDCacheDictionary.h b/src/Dictionaries/SSDCacheDictionary.h\ndeleted file mode 100644\nindex 4d4d3befa22a..000000000000\n--- a/src/Dictionaries/SSDCacheDictionary.h\n+++ /dev/null\n@@ -1,418 +0,0 @@\n-#pragma once\n-\n-#if defined(__linux__) || defined(__FreeBSD__)\n-\n-#include <atomic>\n-#include <chrono>\n-#include <list>\n-#include <shared_mutex>\n-#include <variant>\n-#include <vector>\n-\n-#include <Poco/Logger.h>\n-\n-#include <Columns/ColumnDecimal.h>\n-#include <Columns/ColumnString.h>\n-#include <Common/ArenaWithFreeLists.h>\n-#include <Common/CurrentMetrics.h>\n-#include <common/logger_useful.h>\n-#include <Compression/CompressedWriteBuffer.h>\n-#include <Core/Block.h>\n-#include <Dictionaries/BucketCache.h>\n-#include <IO/HashingWriteBuffer.h>\n-#include <pcg_random.hpp>\n-#include \"DictionaryStructure.h\"\n-#include \"IDictionary.h\"\n-#include \"IDictionarySource.h\"\n-#include \"DictionaryHelpers.h\"\n-\n-namespace DB\n-{\n-\n-using AttributeValueVariant = std::variant<\n-        UInt8,\n-        UInt16,\n-        UInt32,\n-        UInt64,\n-        UInt128,\n-        Int8,\n-        Int16,\n-        Int32,\n-        Int64,\n-        Decimal32,\n-        Decimal64,\n-        Decimal128,\n-        Float32,\n-        Float64,\n-        String>;\n-\n-\n-/*\n-    Class for operations with cache file and index.\n-    Supports GET/SET operations.\n-*/\n-class SSDCachePartition\n-{\n-public:\n-    struct Index final\n-    {\n-        bool inMemory() const;\n-        void setInMemory(bool in_memory);\n-\n-        bool exists() const;\n-        void setNotExists();\n-\n-        size_t getAddressInBlock() const;\n-        void setAddressInBlock(size_t address_in_block);\n-\n-        size_t getBlockId() const;\n-        void setBlockId(size_t block_id);\n-\n-        bool operator< (const Index & rhs) const { return index < rhs.index; }\n-\n-        /// Stores `is_in_memory` flag, block id, address in uncompressed block\n-        uint64_t index = 0;\n-    };\n-\n-    struct Metadata final\n-    {\n-        using time_point_t = std::chrono::system_clock::time_point;\n-        using time_point_rep_t = time_point_t::rep;\n-        using time_point_urep_t = make_unsigned_t<time_point_rep_t>;\n-\n-        time_point_t expiresAt() const;\n-        void setExpiresAt(const time_point_t & t);\n-\n-        bool isDefault() const;\n-        void setDefault();\n-\n-        /// Stores both expiration time and `is_default` flag in the most significant bit\n-        time_point_urep_t data = 0;\n-    };\n-\n-    using Offset = size_t;\n-    using Offsets = std::vector<Offset>;\n-    using Key = IDictionary::Key;\n-\n-    SSDCachePartition(\n-            const AttributeUnderlyingType & key_structure,\n-            const std::vector<AttributeUnderlyingType> & attributes_structure,\n-            const std::string & dir_path,\n-            size_t file_id,\n-            size_t max_size,\n-            size_t block_size,\n-            size_t read_buffer_size,\n-            size_t write_buffer_size,\n-            size_t max_stored_keys);\n-\n-    ~SSDCachePartition();\n-\n-    template <typename T>\n-    using ResultArrayType = std::conditional_t<IsDecimalNumber<T>, DecimalPaddedPODArray<T>, PaddedPODArray<T>>;\n-\n-    template <typename Out, typename GetDefault>\n-    void getValue(size_t attribute_index, const PaddedPODArray<UInt64> & ids,\n-            ResultArrayType<Out> & out, std::vector<bool> & found, GetDefault & default_value_extractor,\n-            std::chrono::system_clock::time_point now) const;\n-\n-    void getString(size_t attribute_index, const PaddedPODArray<UInt64> & ids,\n-            StringRefs & refs, ArenaWithFreeLists & arena, std::vector<bool> & found,\n-            std::vector<size_t> & default_ids, std::chrono::system_clock::time_point now) const;\n-\n-    void has(const PaddedPODArray<UInt64> & ids, ResultArrayType<UInt8> & out,\n-            std::vector<bool> & found, std::chrono::system_clock::time_point now) const;\n-\n-    struct Attribute\n-    {\n-        template <typename T>\n-        using Container = std::vector<T>;\n-\n-        AttributeUnderlyingType type;\n-        std::variant<\n-                Container<UInt8>,\n-                Container<UInt16>,\n-                Container<UInt32>,\n-                Container<UInt64>,\n-                Container<UInt128>,\n-                Container<Int8>,\n-                Container<Int16>,\n-                Container<Int32>,\n-                Container<Int64>,\n-                Container<Decimal32>,\n-                Container<Decimal64>,\n-                Container<Decimal128>,\n-                Container<Float32>,\n-                Container<Float64>,\n-                Container<String>> values;\n-    };\n-    using Attributes = std::vector<Attribute>;\n-\n-    size_t appendBlock(const Attribute & new_keys, const Attributes & new_attributes,\n-            const PaddedPODArray<Metadata> & metadata, size_t begin);\n-\n-    size_t appendDefaults(const Attribute & new_keys, const PaddedPODArray<Metadata> & metadata, size_t begin);\n-\n-    void flush();\n-\n-    void remove();\n-\n-    size_t getId() const;\n-\n-    PaddedPODArray<Key> getCachedIds(std::chrono::system_clock::time_point now) const;\n-\n-    double getLoadFactor() const;\n-\n-    size_t getElementCount() const;\n-\n-    size_t getBytesAllocated() const;\n-\n-private:\n-    void clearOldestBlocks();\n-\n-    template <typename SetFunc>\n-    void getImpl(const PaddedPODArray<UInt64> & ids, SetFunc & set, std::vector<bool> & found) const;\n-\n-    template <typename SetFunc>\n-    void getValueFromMemory(const PaddedPODArray<Index> & indices, SetFunc & set) const;\n-\n-    template <typename SetFunc>\n-    void getValueFromStorage(const PaddedPODArray<Index> & indices, SetFunc & set) const;\n-\n-    void ignoreFromBufferToAttributeIndex(size_t attribute_index, ReadBuffer & buf) const;\n-\n-    const size_t file_id;\n-    const size_t max_size;\n-    const size_t block_size;\n-    const size_t read_buffer_size;\n-    const size_t write_buffer_size;\n-    const size_t max_stored_keys;\n-    const std::string path;\n-\n-    mutable std::shared_mutex rw_lock;\n-\n-    int fd = -1;\n-\n-    mutable BucketCacheIndex<UInt64, Index, Int64Hasher> key_to_index;\n-\n-    Attribute keys_buffer;\n-    const std::vector<AttributeUnderlyingType> attributes_structure;\n-\n-    std::optional<Memory<>> memory;\n-    std::optional<WriteBuffer> write_buffer;\n-    uint32_t keys_in_block = 0;\n-\n-    size_t current_memory_block_id = 0;\n-    size_t current_file_block_id = 0;\n-};\n-\n-using SSDCachePartitionPtr = std::shared_ptr<SSDCachePartition>;\n-\n-\n-/*\n-    Class for managing SSDCachePartition and getting data from source.\n-*/\n-class SSDCacheStorage\n-{\n-public:\n-    using AttributeTypes = std::vector<AttributeUnderlyingType>;\n-    using Key = SSDCachePartition::Key;\n-\n-    SSDCacheStorage(\n-            const AttributeTypes & attributes_structure,\n-            const std::string & path,\n-            size_t max_partitions_count,\n-            size_t file_size,\n-            size_t block_size,\n-            size_t read_buffer_size,\n-            size_t write_buffer_size,\n-            size_t max_stored_keys);\n-\n-    ~SSDCacheStorage();\n-\n-    template <typename T>\n-    using ResultArrayType = SSDCachePartition::ResultArrayType<T>;\n-\n-    template <typename Out, typename GetDefault>\n-    void getValue(size_t attribute_index, const PaddedPODArray<UInt64> & ids,\n-            ResultArrayType<Out> & out, std::unordered_map<Key, std::vector<size_t>> & not_found,\n-            GetDefault & default_value_extractor, std::chrono::system_clock::time_point now) const;\n-\n-    void getString(size_t attribute_index, const PaddedPODArray<UInt64> & ids,\n-            StringRefs & refs, ArenaWithFreeLists & arena, std::unordered_map<Key, std::vector<size_t>> & not_found,\n-            std::vector<size_t> & default_ids, std::chrono::system_clock::time_point now) const;\n-\n-    void has(const PaddedPODArray<UInt64> & ids, ResultArrayType<UInt8> & out,\n-             std::unordered_map<Key, std::vector<size_t>> & not_found, std::chrono::system_clock::time_point now) const;\n-\n-    template <typename PresentIdHandler, typename AbsentIdHandler>\n-    void update(DictionarySourcePtr & source_ptr, const std::vector<Key> & requested_ids,\n-            PresentIdHandler && on_updated, AbsentIdHandler && on_id_not_found,\n-            DictionaryLifetime lifetime);\n-\n-    PaddedPODArray<Key> getCachedIds() const;\n-\n-    std::exception_ptr getLastException() const { return last_update_exception; }\n-\n-    const std::string & getPath() const { return path; }\n-\n-    size_t getQueryCount() const { return query_count.load(std::memory_order_relaxed); }\n-\n-    size_t getHitCount() const { return hit_count.load(std::memory_order_acquire); }\n-\n-    size_t getElementCount() const;\n-\n-    double getLoadFactor() const;\n-\n-    size_t getBytesAllocated() const;\n-\n-private:\n-    void collectGarbage();\n-\n-    const AttributeTypes attributes_structure;\n-\n-    const std::string path;\n-    const size_t max_partitions_count;\n-    const size_t file_size;\n-    const size_t block_size;\n-    const size_t read_buffer_size;\n-    const size_t write_buffer_size;\n-    const size_t max_stored_keys;\n-\n-    mutable std::shared_mutex rw_lock;\n-    std::list<SSDCachePartitionPtr> partitions;\n-    std::list<SSDCachePartitionPtr> partition_delete_queue;\n-\n-    Poco::Logger * const log;\n-\n-    mutable pcg64 rnd_engine;\n-\n-    mutable std::exception_ptr last_update_exception;\n-    mutable size_t update_error_count = 0;\n-    mutable std::chrono::system_clock::time_point backoff_end_time;\n-\n-    mutable std::atomic<size_t> hit_count{0};\n-    mutable std::atomic<size_t> query_count{0};\n-};\n-\n-\n-/*\n-    Dictionary interface\n-*/\n-class SSDCacheDictionary final : public IDictionary\n-{\n-public:\n-    SSDCacheDictionary(\n-            const StorageID & dict_id_,\n-            const DictionaryStructure & dict_struct_,\n-            DictionarySourcePtr source_ptr_,\n-            DictionaryLifetime dict_lifetime_,\n-            const std::string & path,\n-            size_t max_partitions_count_,\n-            size_t file_size_,\n-            size_t block_size_,\n-            size_t read_buffer_size_,\n-            size_t write_buffer_size_,\n-            size_t max_stored_keys_);\n-\n-    std::string getTypeName() const override { return \"SSDCache\"; }\n-\n-    size_t getBytesAllocated() const override { return storage.getBytesAllocated(); }\n-\n-    size_t getQueryCount() const override { return storage.getQueryCount(); }\n-\n-    double getHitRate() const override\n-    {\n-        return static_cast<double>(storage.getHitCount()) / storage.getQueryCount();\n-    }\n-\n-    size_t getElementCount() const override { return storage.getElementCount(); }\n-\n-    double getLoadFactor() const override { return storage.getLoadFactor(); }\n-\n-    bool supportUpdates() const override { return false; }\n-\n-    std::shared_ptr<const IExternalLoadable> clone() const override\n-    {\n-        return std::make_shared<SSDCacheDictionary>(getDictionaryID(), dict_struct, source_ptr->clone(), dict_lifetime,\n-                path, max_partitions_count, file_size, block_size, read_buffer_size, write_buffer_size, max_stored_keys);\n-    }\n-\n-    const IDictionarySource * getSource() const override { return source_ptr.get(); }\n-\n-    const DictionaryLifetime & getLifetime() const override { return dict_lifetime; }\n-\n-    const DictionaryStructure & getStructure() const override { return dict_struct; }\n-\n-    bool isInjective(const std::string & attribute_name) const override\n-    {\n-        return dict_struct.attributes[getAttributeIndex(attribute_name)].injective;\n-    }\n-\n-    bool hasHierarchy() const override { return false; }\n-\n-    void toParent(const PaddedPODArray<Key> &, PaddedPODArray<Key> &) const override { }\n-\n-    std::exception_ptr getLastException() const override { return storage.getLastException(); }\n-\n-    DictionaryKeyType getKeyType() const override { return DictionaryKeyType::simple; }\n-\n-    ColumnPtr getColumn(\n-        const std::string& attribute_name,\n-        const DataTypePtr & result_type,\n-        const Columns & key_columns,\n-        const DataTypes & key_types,\n-        const ColumnPtr default_values_column) const override;\n-\n-    ColumnUInt8::Ptr hasKeys(const Columns & key_columns, const DataTypes & key_types) const override;\n-\n-    template <typename T>\n-    using ResultArrayType = SSDCacheStorage::ResultArrayType<T>;\n-\n-    BlockInputStreamPtr getBlockInputStream(const Names & column_names, size_t max_block_size) const override;\n-\n-private:\n-    size_t getAttributeIndex(const std::string & attr_name) const;\n-\n-    template <typename T>\n-    AttributeValueVariant createAttributeNullValueWithTypeImpl(const Field & null_value);\n-    AttributeValueVariant createAttributeNullValueWithType(AttributeUnderlyingType type, const Field & null_value);\n-    void createAttributes();\n-\n-    template <typename AttributeType, typename OutputType, typename DefaultGetter>\n-    void getItemsNumberImpl(\n-        size_t attribute_index,\n-        const PaddedPODArray<Key> & ids,\n-        ResultArrayType<OutputType> & out,\n-        DefaultGetter & default_value_extractor) const;\n-\n-    template <typename DefaultGetter>\n-    void getItemsStringImpl(\n-        size_t attribute_index,\n-        const PaddedPODArray<Key> & ids,\n-        ColumnString * out,\n-        DefaultGetter & default_value_extractor) const;\n-\n-    const std::string name;\n-    const DictionaryStructure dict_struct;\n-    mutable DictionarySourcePtr source_ptr;\n-    const DictionaryLifetime dict_lifetime;\n-\n-    const std::string path;\n-    const size_t max_partitions_count;\n-    const size_t file_size;\n-    const size_t block_size;\n-    const size_t read_buffer_size;\n-    const size_t write_buffer_size;\n-    const size_t max_stored_keys;\n-\n-    std::map<std::string, size_t> attribute_index_by_name;\n-    std::vector<AttributeValueVariant> null_values;\n-    mutable SSDCacheStorage storage;\n-    Poco::Logger * const log;\n-\n-    mutable size_t bytes_allocated = 0;\n-};\n-\n-}\n-\n-#endif\ndiff --git a/src/Dictionaries/SSDCacheDictionaryStorage.h b/src/Dictionaries/SSDCacheDictionaryStorage.h\nnew file mode 100644\nindex 000000000000..16a8954de58c\n--- /dev/null\n+++ b/src/Dictionaries/SSDCacheDictionaryStorage.h\n@@ -0,0 +1,1364 @@\n+#pragma once\n+\n+#if defined(__linux__) || defined(__FreeBSD__)\n+\n+#include <chrono>\n+\n+#include <pcg_random.hpp>\n+#include <filesystem>\n+#include <city.h>\n+#include <fcntl.h>\n+#include <absl/container/flat_hash_map.h>\n+#include <absl/container/flat_hash_set.h>\n+\n+#include <common/unaligned.h>\n+#include <Common/Stopwatch.h>\n+#include <Common/randomSeed.h>\n+#include <Common/Arena.h>\n+#include <Common/ArenaWithFreeLists.h>\n+#include <Common/MemorySanitizer.h>\n+#include <Common/HashTable/LRUHashMap.h>\n+#include <IO/AIO.h>\n+#include <Dictionaries/DictionaryStructure.h>\n+#include <Dictionaries/ICacheDictionaryStorage.h>\n+#include <Dictionaries/DictionaryHelpers.h>\n+\n+namespace ProfileEvents\n+{\n+    extern const Event FileOpen;\n+    extern const Event WriteBufferAIOWrite;\n+    extern const Event WriteBufferAIOWriteBytes;\n+}\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int AIO_READ_ERROR;\n+    extern const int AIO_WRITE_ERROR;\n+    extern const int CANNOT_ALLOCATE_MEMORY;\n+    extern const int CANNOT_CREATE_DIRECTORY;\n+    extern const int CANNOT_FSYNC;\n+    extern const int CANNOT_IO_GETEVENTS;\n+    extern const int CANNOT_IO_SUBMIT;\n+    extern const int CANNOT_OPEN_FILE;\n+    extern const int CORRUPTED_DATA;\n+    extern const int FILE_DOESNT_EXIST;\n+    extern const int UNSUPPORTED_METHOD;\n+    extern const int NOT_IMPLEMENTED;\n+}\n+\n+struct SSDCacheDictionaryStorageConfiguration\n+{\n+    const size_t strict_max_lifetime_seconds;\n+    const DictionaryLifetime lifetime;\n+\n+    const std::string file_path;\n+    const size_t max_partitions_count;\n+    const size_t max_stored_keys;\n+    const size_t block_size;\n+    const size_t file_blocks_size;\n+    const size_t read_buffer_blocks_size;\n+    const size_t write_buffer_blocks_size;\n+};\n+\n+\n+/** Simple Key is serialized in block with following structure\n+    key     | data_size | data\n+    8 bytes | 8 bytes   | data_size bytes\n+\n+    Complex Key is serialized in block with following structure\n+    key_size     | key_data       | data_size | data\n+    8 bytes      | key_size bytes | 8 bytes   | data_size bytes\n+*/\n+template <typename TKeyType>\n+struct SSDCacheKey final\n+{\n+    using KeyType = TKeyType;\n+\n+    SSDCacheKey(KeyType key_, size_t size_, const char * data_)\n+        : key(key_)\n+        , size(size_)\n+        , data(data_)\n+    {}\n+\n+    KeyType key;\n+    size_t size;\n+    const char * data;\n+};\n+\n+using SSDCacheSimpleKey = SSDCacheKey<UInt64>;\n+using SSDCacheComplexKey = SSDCacheKey<StringRef>;\n+\n+/** Block is serialized with following structure\n+    check_sum | keys_size | [keys]\n+    8 bytes   | 8 bytes   |\n+*/\n+class SSDCacheBlock final\n+{\n+    static constexpr size_t block_header_check_sum_size = sizeof(size_t);\n+    static constexpr size_t block_header_keys_size = sizeof(size_t);\n+public:\n+\n+    /// Block header size\n+    static constexpr size_t block_header_size = block_header_check_sum_size + block_header_keys_size;\n+\n+    explicit SSDCacheBlock(size_t block_size_)\n+        : block_size(block_size_)\n+    {}\n+\n+    /// Checks if simple key can be written in empty block with block_size\n+    static bool canBeWrittenInEmptyBlock(SSDCacheSimpleKey & simple_key, size_t block_size)\n+    {\n+        static constexpr size_t simple_key_size = sizeof(simple_key.key);\n+\n+        return (block_header_size + simple_key_size + sizeof(simple_key.size) + simple_key.size) <= block_size;\n+    }\n+\n+    /// Checks if complex key can be written in empty block with block_size\n+    static bool canBeWrittenInEmptyBlock(SSDCacheComplexKey & complex_key, size_t block_size)\n+    {\n+        StringRef & key = complex_key.key;\n+        size_t complex_key_size = sizeof(key.size) + key.size;\n+\n+        return (block_header_size + complex_key_size + sizeof(complex_key.size) + complex_key.size) <= block_size;\n+    }\n+\n+    /// Reset block with new block_data\n+    /// block_data must be filled with zeroes if it is new block\n+    ALWAYS_INLINE inline void reset(char * new_block_data)\n+    {\n+        block_data = new_block_data;\n+        current_block_offset = block_header_size;\n+        keys_size = unalignedLoad<size_t>(new_block_data + block_header_check_sum_size);\n+    }\n+\n+    /// Check if it is enough place to write key in block\n+    ALWAYS_INLINE inline bool enoughtPlaceToWriteKey(const SSDCacheSimpleKey & cache_key) const\n+    {\n+        return (current_block_offset + (sizeof(cache_key.key) + sizeof(cache_key.size) + cache_key.size)) <= block_size;\n+    }\n+\n+    /// Check if it is enough place to write key in block\n+    ALWAYS_INLINE inline bool enoughtPlaceToWriteKey(const SSDCacheComplexKey & cache_key) const\n+    {\n+        const StringRef & key = cache_key.key;\n+        size_t complex_key_size = sizeof(key.size) + key.size;\n+\n+        return (current_block_offset + (complex_key_size + sizeof(cache_key.size) + cache_key.size)) <= block_size;\n+    }\n+\n+    /// Write key and returns offset in ssd cache block where data is written\n+    /// It is client responsibility to check if there is enough place in block to write key\n+    /// Returns true if key was written and false if there was not enough place to write key\n+    ALWAYS_INLINE inline bool writeKey(const SSDCacheSimpleKey & cache_key, size_t & offset_in_block)\n+    {\n+        assert(cache_key.size > 0);\n+\n+        if (!enoughtPlaceToWriteKey(cache_key))\n+            return false;\n+\n+        char * current_block_offset_data = block_data + current_block_offset;\n+\n+        /// Write simple key\n+        memcpy(reinterpret_cast<void *>(current_block_offset_data), reinterpret_cast<const void *>(&cache_key.key), sizeof(cache_key.key));\n+        current_block_offset_data += sizeof(cache_key.key);\n+        current_block_offset += sizeof(cache_key.key);\n+\n+        /// Write serialized columns size\n+        memcpy(reinterpret_cast<void *>(current_block_offset_data), reinterpret_cast<const void *>(&cache_key.size), sizeof(cache_key.size));\n+        current_block_offset_data += sizeof(cache_key.size);\n+        current_block_offset += sizeof(cache_key.size);\n+\n+        offset_in_block = current_block_offset;\n+\n+        memcpy(reinterpret_cast<void *>(current_block_offset_data), reinterpret_cast<const void *>(cache_key.data), cache_key.size);\n+        current_block_offset += cache_key.size;\n+\n+        ++keys_size;\n+\n+        return true;\n+    }\n+\n+    ALWAYS_INLINE inline bool writeKey(const SSDCacheComplexKey & cache_key, size_t & offset_in_block)\n+    {\n+        assert(cache_key.size > 0);\n+\n+        if (!enoughtPlaceToWriteKey(cache_key))\n+            return false;\n+\n+        char * current_block_offset_data = block_data + current_block_offset;\n+\n+        const StringRef & key = cache_key.key;\n+\n+        /// Write complex key\n+        memcpy(reinterpret_cast<void *>(current_block_offset_data), reinterpret_cast<const void *>(&key.size), sizeof(key.size));\n+        current_block_offset_data += sizeof(key.size);\n+        current_block_offset += sizeof(key.size);\n+\n+        memcpy(reinterpret_cast<void *>(current_block_offset_data), reinterpret_cast<const void *>(key.data), key.size);\n+        current_block_offset_data += key.size;\n+        current_block_offset += key.size;\n+\n+        /// Write serialized columns size\n+        memcpy(reinterpret_cast<void *>(current_block_offset_data), reinterpret_cast<const void *>(&cache_key.size), sizeof(cache_key.size));\n+        current_block_offset_data += sizeof(cache_key.size);\n+        current_block_offset += sizeof(cache_key.size);\n+\n+        offset_in_block = current_block_offset;\n+\n+        memcpy(reinterpret_cast<void *>(current_block_offset_data), reinterpret_cast<const void *>(cache_key.data), cache_key.size);\n+        current_block_offset += cache_key.size;\n+\n+        ++keys_size;\n+\n+        return true;\n+    }\n+\n+    ALWAYS_INLINE inline size_t getKeysSize() const { return keys_size; }\n+\n+    /// Write keys size into block header\n+    ALWAYS_INLINE inline void writeKeysSize()\n+    {\n+        char * keys_size_offset_data = block_data + block_header_check_sum_size;\n+        std::memcpy(keys_size_offset_data, &keys_size, sizeof(size_t));\n+    }\n+\n+    /// Get check sum from block header\n+    ALWAYS_INLINE inline size_t getCheckSum() const { return unalignedLoad<size_t>(block_data); }\n+\n+    /// Calculate check sum in block\n+    ALWAYS_INLINE inline size_t calculateCheckSum() const\n+    {\n+        size_t calculated_check_sum = static_cast<size_t>(CityHash_v1_0_2::CityHash64(block_data + block_header_check_sum_size, block_size - block_header_check_sum_size));\n+\n+        return calculated_check_sum;\n+    }\n+\n+    /// Check if check sum from block header matched calculated check sum in block\n+    ALWAYS_INLINE inline bool checkCheckSum() const\n+    {\n+        size_t calculated_check_sum = calculateCheckSum();\n+        size_t check_sum = getCheckSum();\n+\n+        return calculated_check_sum == check_sum;\n+    }\n+\n+    /// Write check sum in block header\n+    ALWAYS_INLINE inline void writeCheckSum()\n+    {\n+        size_t check_sum = static_cast<size_t>(CityHash_v1_0_2::CityHash64(block_data + block_header_check_sum_size, block_size - block_header_check_sum_size));\n+        std::memcpy(block_data, &check_sum, sizeof(size_t));\n+    }\n+\n+    ALWAYS_INLINE inline size_t getBlockSize() const { return block_size; }\n+\n+    /// Returns block data\n+    ALWAYS_INLINE inline char * getBlockData() const { return block_data; }\n+\n+    /// Read keys that were serialized in block\n+    /// It is client responsibility to ensure that simple or complex keys were written in block\n+    void readSimpleKeys(PaddedPODArray<UInt64> & simple_keys) const\n+    {\n+        char * block_start = block_data + block_header_size;\n+        char * block_end = block_data + block_size;\n+\n+        static constexpr size_t key_prefix_size = sizeof(UInt64) + sizeof(size_t);\n+\n+        while (block_start + key_prefix_size < block_end)\n+        {\n+            UInt64 key = unalignedLoad<UInt64>(block_start);\n+            block_start += sizeof(UInt64);\n+\n+            size_t allocated_size = unalignedLoad<size_t>(block_start);\n+            block_start += sizeof(size_t);\n+\n+            /// If we read empty allocated size that means it is end of block\n+            if (allocated_size == 0)\n+                break;\n+\n+            simple_keys.emplace_back(key);\n+            block_start += allocated_size;\n+        }\n+    }\n+\n+    void readComplexKeys(PaddedPODArray<StringRef> & complex_keys) const\n+    {\n+        char * block_start = block_data + block_header_size;\n+        char * block_end = block_data + block_size;\n+\n+        static constexpr size_t key_prefix_size = sizeof(size_t) + sizeof(size_t);\n+\n+        while (block_start + key_prefix_size < block_end)\n+        {\n+            size_t key_size = unalignedLoad<size_t>(block_start);\n+            block_start += sizeof(key_size);\n+\n+            StringRef complex_key (block_start, key_size);\n+\n+            block_start += key_size;\n+\n+            size_t allocated_size = unalignedLoad<size_t>(block_start);\n+            block_start += sizeof(size_t);\n+\n+            /// If we read empty allocated size that means it is end of block\n+            if (allocated_size == 0)\n+                break;\n+\n+            complex_keys.emplace_back(complex_key);\n+            block_start += allocated_size;\n+        }\n+    }\n+\n+private:\n+    size_t block_size;\n+    char * block_data = nullptr;\n+\n+    size_t current_block_offset = block_header_size;\n+    size_t keys_size = 0;\n+};\n+\n+struct SSDCacheIndex\n+{\n+    SSDCacheIndex(size_t block_index_, size_t offset_in_block_)\n+        : block_index(block_index_)\n+        , offset_in_block(offset_in_block_)\n+    {}\n+\n+    SSDCacheIndex() = default;\n+\n+    size_t block_index = 0;\n+    size_t offset_in_block = 0;\n+};\n+\n+inline bool operator==(const SSDCacheIndex & lhs, const SSDCacheIndex & rhs)\n+{\n+    return lhs.block_index == rhs.block_index && lhs.offset_in_block == rhs.offset_in_block;\n+}\n+\n+/** SSDCacheMemoryBuffer initialized with block size and memory buffer blocks size.\n+  * Allocate block_size * memory_buffer_blocks_size bytes with page alignment.\n+  * Logically represents multiple memory_buffer_blocks_size blocks and current write block.\n+  * If key cannot be written into current_write_block, current block keys size and check summ is written\n+  * and buffer increase index of current_write_block_index.\n+  * If current_write_block_index == memory_buffer_blocks_size write key will always returns true.\n+  * If reset is called current_write_block_index is set to 0.\n+  */\n+template <typename SSDCacheKeyType>\n+class SSDCacheMemoryBuffer\n+{\n+public:\n+    using KeyType = typename SSDCacheKeyType::KeyType;\n+\n+    explicit SSDCacheMemoryBuffer(size_t block_size_, size_t memory_buffer_blocks_size_)\n+        : block_size(block_size_)\n+        , partition_blocks_size(memory_buffer_blocks_size_)\n+        , buffer(block_size * partition_blocks_size, 4096)\n+        , current_write_block(block_size)\n+    {\n+        current_write_block.reset(buffer.m_data);\n+    }\n+\n+    bool writeKey(const SSDCacheKeyType & key, SSDCacheIndex & index)\n+    {\n+        if (current_block_index == partition_blocks_size)\n+            return false;\n+\n+        size_t block_offset = 0;\n+        bool write_in_current_block = current_write_block.writeKey(key, block_offset);\n+\n+        if (write_in_current_block)\n+        {\n+            index.block_index = current_block_index;\n+            index.offset_in_block = block_offset;\n+            return true;\n+        }\n+\n+        current_write_block.writeKeysSize();\n+        current_write_block.writeCheckSum();\n+\n+        ++current_block_index;\n+\n+        if (current_block_index == partition_blocks_size)\n+            return false;\n+\n+        current_write_block.reset(buffer.m_data + (block_size * current_block_index));\n+\n+        write_in_current_block = current_write_block.writeKey(key, block_offset);\n+        assert(write_in_current_block);\n+\n+        index.block_index = current_block_index;\n+        index.offset_in_block = block_offset;\n+\n+        return write_in_current_block;\n+    }\n+\n+    void writeKeysSizeAndCheckSumForCurrentWriteBlock()\n+    {\n+        current_write_block.writeKeysSize();\n+        current_write_block.writeCheckSum();\n+    }\n+\n+    inline char * getPlace(SSDCacheIndex index) const\n+    {\n+        return buffer.m_data + index.block_index * block_size + index.offset_in_block;\n+    }\n+\n+    inline size_t getCurrentBlockIndex() const { return current_block_index; }\n+\n+    inline const char * getData() const { return buffer.m_data; }\n+\n+    inline size_t getSizeInBytes() const { return block_size * partition_blocks_size; }\n+\n+    void readKeys(PaddedPODArray<KeyType> & keys) const\n+    {\n+        SSDCacheBlock block(block_size);\n+\n+        for (size_t block_index = 0; block_index < partition_blocks_size; ++block_index)\n+        {\n+            block.reset(buffer.m_data + (block_index * block_size));\n+\n+            if constexpr (std::is_same_v<KeyType, UInt64>)\n+                block.readSimpleKeys(keys);\n+            else\n+                block.readComplexKeys(keys);\n+        }\n+    }\n+\n+    inline void reset()\n+    {\n+        current_block_index = 0;\n+        current_write_block.reset(buffer.m_data);\n+    }\n+\n+    const size_t block_size;\n+\n+    const size_t partition_blocks_size;\n+\n+private:\n+    Memory<Allocator<true>> buffer;\n+\n+    SSDCacheBlock current_write_block;\n+\n+    size_t current_block_index = 0;\n+};\n+\n+/// TODO: Add documentation\n+template <typename SSDCacheKeyType>\n+class SSDCacheFileBuffer : private boost::noncopyable\n+{\n+    static constexpr auto BIN_FILE_EXT = \".bin\";\n+\n+public:\n+\n+    using KeyType = typename SSDCacheKeyType::KeyType;\n+\n+    explicit SSDCacheFileBuffer(\n+        const std::string & file_path_,\n+        size_t block_size_,\n+        size_t file_blocks_size_)\n+        : file_path(file_path_ + BIN_FILE_EXT)\n+        , block_size(block_size_)\n+        , file_blocks_size(file_blocks_size_)\n+    {\n+        auto path = std::filesystem::path{file_path};\n+        auto parent_path_directory = path.parent_path();\n+\n+        /// If cache file is in directory that does not exists create it\n+        if (!std::filesystem::exists(parent_path_directory))\n+            if (!std::filesystem::create_directories(parent_path_directory))\n+                throw Exception{\"Failed to create directories.\", ErrorCodes::CANNOT_CREATE_DIRECTORY};\n+\n+        ProfileEvents::increment(ProfileEvents::FileOpen);\n+\n+        file.fd = ::open(file_path.c_str(), O_RDWR | O_CREAT | O_TRUNC | O_DIRECT, 0666);\n+        if (file.fd == -1)\n+        {\n+            auto error_code = (errno == ENOENT) ? ErrorCodes::FILE_DOESNT_EXIST : ErrorCodes::CANNOT_OPEN_FILE;\n+            throwFromErrnoWithPath(\"Cannot open file \" + file_path, file_path, error_code);\n+        }\n+\n+        allocateSizeForNextPartition();\n+    }\n+\n+    void allocateSizeForNextPartition()\n+    {\n+        if (preallocateDiskSpace(file.fd, current_blocks_size * block_size, block_size * file_blocks_size) < 0)\n+            throwFromErrnoWithPath(\"Cannot preallocate space for the file \" + file_path, file_path, ErrorCodes::CANNOT_ALLOCATE_MEMORY);\n+\n+        current_blocks_size += file_blocks_size;\n+    }\n+\n+    bool writeBuffer(const char * buffer, size_t buffer_size_in_blocks)\n+    {\n+        if (current_block_index + buffer_size_in_blocks > current_blocks_size)\n+            return false;\n+\n+        AIOContext aio_context{1};\n+\n+        iocb write_request{};\n+        iocb * write_request_ptr{&write_request};\n+\n+        #if defined(__FreeBSD__)\n+        write_request.aio.aio_lio_opcode = LIO_WRITE;\n+        write_request.aio.aio_fildes = file.fd;\n+        write_request.aio.aio_buf = reinterpret_cast<volatile void *>(const_cast<char *>(buffer));\n+        write_request.aio.aio_nbytes = block_size * buffer_size_in_blocks;\n+        write_request.aio.aio_offset = current_block_index * block_size;\n+        #else\n+        write_request.aio_lio_opcode = IOCB_CMD_PWRITE;\n+        write_request.aio_fildes = file.fd;\n+        write_request.aio_buf = reinterpret_cast<UInt64>(buffer);\n+        write_request.aio_nbytes = block_size * buffer_size_in_blocks;\n+        write_request.aio_offset = current_block_index * block_size;\n+        #endif\n+\n+        while (io_submit(aio_context.ctx, 1, &write_request_ptr) < 0)\n+        {\n+            if (errno != EINTR)\n+                throw Exception(\"Cannot submit request for asynchronous IO on file \" + file_path, ErrorCodes::CANNOT_IO_SUBMIT);\n+        }\n+\n+        // CurrentMetrics::Increment metric_increment_write{CurrentMetrics::Write};\n+\n+        io_event event;\n+\n+        while (io_getevents(aio_context.ctx, 1, 1, &event, nullptr) < 0)\n+        {\n+            if (errno != EINTR)\n+                throw Exception(\"Failed to wait for asynchronous IO completion on file \" + file_path, ErrorCodes::CANNOT_IO_GETEVENTS);\n+        }\n+\n+        // Unpoison the memory returned from an uninstrumented system function.\n+        __msan_unpoison(&event, sizeof(event));\n+\n+        auto bytes_written = eventResult(event);\n+\n+        ProfileEvents::increment(ProfileEvents::WriteBufferAIOWrite);\n+        ProfileEvents::increment(ProfileEvents::WriteBufferAIOWriteBytes, bytes_written);\n+\n+        if (bytes_written != static_cast<decltype(bytes_written)>(block_size * buffer_size_in_blocks))\n+            throw Exception(\"Not all data was written for asynchronous IO on file \" + file_path + \". returned: \" + std::to_string(bytes_written), ErrorCodes::AIO_WRITE_ERROR);\n+\n+        if (::fsync(file.fd) < 0)\n+            throwFromErrnoWithPath(\"Cannot fsync \" + file_path, file_path, ErrorCodes::CANNOT_FSYNC);\n+\n+        current_block_index += buffer_size_in_blocks;\n+\n+        return true;\n+    }\n+\n+    bool readKeys(size_t block_start, size_t blocks_length, PaddedPODArray<KeyType> & out) const\n+    {\n+        if (block_start + blocks_length > current_blocks_size)\n+            return false;\n+\n+        size_t buffer_size_in_bytes = blocks_length * block_size;\n+\n+        Memory read_buffer_memory(block_size * blocks_length, block_size);\n+\n+        iocb request{};\n+        iocb * request_ptr = &request;\n+\n+        #if defined(__FreeBSD__)\n+        request.aio.aio_lio_opcode = LIO_READ;\n+        request.aio.aio_fildes = file.fd;\n+        request.aio.aio_buf = reinterpret_cast<volatile void *>(reinterpret_cast<UInt64>(read_buffer_memory.data()));\n+        request.aio.aio_nbytes = buffer_size_in_bytes;\n+        request.aio.aio_offset = block_start * block_size;\n+        request.aio_data = 0;\n+        #else\n+        request.aio_lio_opcode = IOCB_CMD_PREAD;\n+        request.aio_fildes = file.fd;\n+        request.aio_buf = reinterpret_cast<UInt64>(read_buffer_memory.data());\n+        request.aio_nbytes = buffer_size_in_bytes;\n+        request.aio_offset = block_start * block_size;\n+        request.aio_data = 0;\n+        #endif\n+\n+        io_event event{};\n+        AIOContext aio_context(1);\n+\n+        while (io_submit(aio_context.ctx, 1, &request_ptr) != 1)\n+        {\n+            if (errno != EINTR)\n+                throwFromErrno(\"io_submit: Failed to submit a request for asynchronous IO\", ErrorCodes::CANNOT_IO_SUBMIT);\n+        }\n+\n+        while (io_getevents(aio_context.ctx, 1, 1, &event, nullptr) != 1)\n+        {\n+            if (errno != EINTR)\n+                throwFromErrno(\"io_getevents: Failed to get an event for asynchronous IO\", ErrorCodes::CANNOT_IO_GETEVENTS);\n+        }\n+\n+        auto read_bytes = eventResult(event);\n+\n+        if (read_bytes != static_cast<ssize_t>(buffer_size_in_bytes))\n+            throw Exception(ErrorCodes::AIO_READ_ERROR,\n+                \"GC: AIO failed to read file ({}). Expected bytes ({}). Actual bytes ({})\", file_path, buffer_size_in_bytes, read_bytes);\n+\n+        SSDCacheBlock block(block_size);\n+\n+        for (size_t i = 0; i < blocks_length; ++i)\n+        {\n+            block.reset(read_buffer_memory.data() + (i * block_size));\n+\n+            if constexpr (std::is_same_v<SSDCacheKeyType, SSDCacheSimpleKey>)\n+                block.readSimpleKeys(out);\n+            else\n+                block.readComplexKeys(out);\n+        }\n+\n+        return true;\n+    }\n+\n+    template <typename FetchBlockFunc>\n+    ALWAYS_INLINE void fetchBlocks(char * read_buffer, size_t read_from_file_buffer_blocks_size, const PaddedPODArray<size_t> & blocks_to_fetch, FetchBlockFunc && func) const\n+    {\n+        if (blocks_to_fetch.empty())\n+            return;\n+\n+        size_t blocks_to_fetch_size = blocks_to_fetch.size();\n+\n+        PaddedPODArray<iocb> requests;\n+        PaddedPODArray<iocb *> pointers;\n+\n+        requests.reserve(blocks_to_fetch_size);\n+        pointers.reserve(blocks_to_fetch_size);\n+\n+        for (size_t block_to_fetch_index = 0; block_to_fetch_index < blocks_to_fetch_size; ++block_to_fetch_index)\n+        {\n+            iocb request{};\n+\n+            char * buffer_place = read_buffer + block_size * (block_to_fetch_index % read_from_file_buffer_blocks_size);\n+\n+            #if defined(__FreeBSD__)\n+            request.aio.aio_lio_opcode = LIO_READ;\n+            request.aio.aio_fildes = file.fd;\n+            request.aio.aio_buf = reinterpret_cast<volatile void *>(reinterpret_cast<UInt64>(buffer_place));\n+            request.aio.aio_nbytes = block_size;\n+            request.aio.aio_offset = block_size * blocks_to_fetch[block_to_fetch_index];\n+            request.aio_data = block_to_fetch_index;\n+            #else\n+            request.aio_lio_opcode = IOCB_CMD_PREAD;\n+            request.aio_fildes = file.fd;\n+            request.aio_buf = reinterpret_cast<UInt64>(buffer_place);\n+            request.aio_nbytes = block_size;\n+            request.aio_offset = block_size * blocks_to_fetch[block_to_fetch_index];\n+            request.aio_data = block_to_fetch_index;\n+            #endif\n+\n+            requests.push_back(request);\n+            pointers.push_back(&requests.back());\n+        }\n+\n+        AIOContext aio_context(read_from_file_buffer_blocks_size);\n+\n+        PaddedPODArray<bool> processed(requests.size(), false);\n+        PaddedPODArray<io_event> events;\n+        events.resize_fill(requests.size());\n+\n+        size_t to_push = 0;\n+        size_t to_pop = 0;\n+\n+        while (to_pop < requests.size())\n+        {\n+            int popped = 0;\n+\n+            while (to_pop < to_push && (popped = io_getevents(aio_context.ctx, to_push - to_pop, to_push - to_pop, &events[to_pop], nullptr)) <= 0)\n+            {\n+                if (errno != EINTR)\n+                    throwFromErrno(\"io_getevents: Failed to get an event for asynchronous IO\", ErrorCodes::CANNOT_IO_GETEVENTS);\n+            }\n+\n+            for (size_t i = to_pop; i < to_pop + popped; ++i)\n+            {\n+                size_t block_to_fetch_index = events[i].data;\n+                const auto & request = requests[block_to_fetch_index];\n+\n+                const ssize_t read_bytes = eventResult(events[i]);\n+\n+                if (read_bytes != static_cast<ssize_t>(block_size))\n+                    throw Exception(ErrorCodes::AIO_READ_ERROR,\n+                        \"GC: AIO failed to read file ({}). Expected bytes ({}). Actual bytes ({})\", file_path, block_size, read_bytes);\n+\n+                char * request_buffer = getRequestBuffer(request);\n+\n+                // Unpoison the memory returned from an uninstrumented system function.\n+                __msan_unpoison(request_buffer, block_size);\n+\n+                SSDCacheBlock block(block_size);\n+                block.reset(request_buffer);\n+\n+                if (!block.checkCheckSum())\n+                {\n+                    std::string calculated_check_sum = std::to_string(block.calculateCheckSum());\n+                    std::string check_sum = std::to_string(block.getCheckSum());\n+                    throw Exception(\"Cache data corrupted. Checksum validation failed. Calculated \" +  calculated_check_sum + \" in block \" + check_sum, ErrorCodes::CORRUPTED_DATA);\n+                }\n+\n+                std::forward<FetchBlockFunc>(func)(blocks_to_fetch[block_to_fetch_index], block.getBlockData());\n+\n+                processed[block_to_fetch_index] = true;\n+            }\n+\n+            while (to_pop < requests.size() && processed[to_pop])\n+                ++to_pop;\n+\n+            /// add new io tasks\n+            const int new_tasks_count = std::min(read_from_file_buffer_blocks_size - (to_push - to_pop), requests.size() - to_push);\n+\n+            int pushed = 0;\n+            while (new_tasks_count > 0 && (pushed = io_submit(aio_context.ctx, new_tasks_count, &pointers[to_push])) <= 0)\n+            {\n+                if (errno != EINTR)\n+                    throwFromErrno(\"io_submit: Failed to submit a request for asynchronous IO\", ErrorCodes::CANNOT_IO_SUBMIT);\n+            }\n+\n+            to_push += pushed;\n+        }\n+    }\n+\n+    inline size_t getCurrentBlockIndex() const { return current_block_index; }\n+\n+    inline void reset()\n+    {\n+        current_block_index = 0;\n+    }\n+private:\n+    struct FileDescriptor : private boost::noncopyable\n+    {\n+\n+        FileDescriptor() = default;\n+\n+        FileDescriptor(FileDescriptor && rhs) : fd(rhs.fd) { rhs.fd = -1; }\n+\n+        FileDescriptor & operator=(FileDescriptor && rhs)\n+        {\n+            close(fd);\n+\n+            fd = rhs.fd;\n+            rhs.fd = -1;\n+        }\n+\n+        ~FileDescriptor()\n+        {\n+            if (fd != -1)\n+                close(fd);\n+        }\n+\n+        int fd = -1;\n+    };\n+\n+    ALWAYS_INLINE inline static int preallocateDiskSpace(int fd, size_t offset, size_t len)\n+    {\n+        #if defined(__FreeBSD__)\n+            return posix_fallocate(fd, offset, len);\n+        #else\n+            return fallocate(fd, 0, offset, len);\n+        #endif\n+    }\n+\n+    ALWAYS_INLINE inline static char * getRequestBuffer(const iocb & request)\n+    {\n+        char * result = nullptr;\n+\n+        #if defined(__FreeBSD__)\n+            result = reinterpret_cast<char *>(reinterpret_cast<UInt64>(request.aio.aio_buf));\n+        #else\n+            result = reinterpret_cast<char *>(request.aio_buf);\n+        #endif\n+\n+        return result;\n+    }\n+\n+    ALWAYS_INLINE inline static ssize_t eventResult(io_event & event)\n+    {\n+        ssize_t  bytes_written;\n+\n+        #if defined(__FreeBSD__)\n+            bytes_written = aio_return(reinterpret_cast<struct aiocb *>(event.udata));\n+        #else\n+            bytes_written = event.res;\n+        #endif\n+\n+        return bytes_written;\n+    }\n+\n+    String file_path;\n+    size_t block_size;\n+    size_t file_blocks_size;\n+    FileDescriptor file;\n+\n+    size_t current_block_index = 0;\n+    size_t current_blocks_size = 0;\n+};\n+\n+/// TODO: Add documentation\n+template <DictionaryKeyType dictionary_key_type>\n+class SSDCacheDictionaryStorage final : public ICacheDictionaryStorage\n+{\n+public:\n+    using SSDCacheKeyType = std::conditional_t<dictionary_key_type == DictionaryKeyType::simple, SSDCacheSimpleKey, SSDCacheComplexKey>;\n+    using KeyType = std::conditional_t<dictionary_key_type == DictionaryKeyType::simple, UInt64, StringRef>;\n+\n+    explicit SSDCacheDictionaryStorage(const SSDCacheDictionaryStorageConfiguration & configuration_)\n+        : configuration(configuration_)\n+        , file_buffer(configuration_.file_path, configuration.block_size, configuration.file_blocks_size)\n+        , read_from_file_buffer(configuration_.block_size * configuration_.read_buffer_blocks_size, 4096)\n+        , rnd_engine(randomSeed())\n+        , index(configuration.max_stored_keys, false, { complex_key_arena })\n+    {\n+        memory_buffer_partitions.emplace_back(configuration.block_size, configuration.write_buffer_blocks_size);\n+    }\n+\n+    bool returnsFetchedColumnsInOrderOfRequestedKeys() const override { return false; }\n+\n+    String getName() const override\n+    {\n+        if (dictionary_key_type == DictionaryKeyType::simple)\n+            return \"SSDCache\";\n+        else\n+            return \"SSDComplexKeyCache\";\n+    }\n+\n+    bool supportsSimpleKeys() const override { return dictionary_key_type == DictionaryKeyType::simple; }\n+\n+    SimpleKeysStorageFetchResult fetchColumnsForKeys(\n+        const PaddedPODArray<UInt64> & keys,\n+        const DictionaryStorageFetchRequest & fetch_request) override\n+    {\n+        if constexpr (dictionary_key_type == DictionaryKeyType::simple)\n+            return fetchColumnsForKeysImpl<SimpleKeysStorageFetchResult>(keys, fetch_request);\n+        else\n+            throw Exception(\"Method insertColumnsForKeys is not supported for complex key storage\", ErrorCodes::NOT_IMPLEMENTED);\n+    }\n+\n+    void insertColumnsForKeys(const PaddedPODArray<UInt64> & keys, Columns columns) override\n+    {\n+        if constexpr (dictionary_key_type == DictionaryKeyType::simple)\n+            insertColumnsForKeysImpl(keys, columns);\n+        else\n+            throw Exception(\"Method insertColumnsForKeys is not supported for complex key storage\", ErrorCodes::NOT_IMPLEMENTED);\n+    }\n+\n+    void insertDefaultKeys(const PaddedPODArray<UInt64> & keys) override\n+    {\n+        if constexpr (dictionary_key_type == DictionaryKeyType::simple)\n+            insertDefaultKeysImpl(keys);\n+        else\n+            throw Exception(\"Method insertDefaultKeysImpl is not supported for complex key storage\", ErrorCodes::NOT_IMPLEMENTED);\n+    }\n+\n+    PaddedPODArray<UInt64> getCachedSimpleKeys() const override\n+    {\n+        if constexpr (dictionary_key_type == DictionaryKeyType::simple)\n+            return getCachedKeysImpl();\n+        else\n+            throw Exception(\"Method getCachedSimpleKeys is not supported for complex key storage\", ErrorCodes::NOT_IMPLEMENTED);\n+    }\n+\n+    bool supportsComplexKeys() const override { return dictionary_key_type == DictionaryKeyType::complex; }\n+\n+    ComplexKeysStorageFetchResult fetchColumnsForKeys(\n+        const PaddedPODArray<StringRef> & keys,\n+        const DictionaryStorageFetchRequest & fetch_request) override\n+    {\n+        if constexpr (dictionary_key_type == DictionaryKeyType::complex)\n+            return fetchColumnsForKeysImpl<ComplexKeysStorageFetchResult>(keys, fetch_request);\n+        else\n+            throw Exception(\"Method fetchColumnsForKeys is not supported for simple key storage\", ErrorCodes::NOT_IMPLEMENTED);\n+    }\n+\n+    void insertColumnsForKeys(const PaddedPODArray<StringRef> & keys, Columns columns) override\n+    {\n+        if constexpr (dictionary_key_type == DictionaryKeyType::complex)\n+            insertColumnsForKeysImpl(keys, columns);\n+        else\n+            throw Exception(\"Method insertColumnsForKeys is not supported for simple key storage\", ErrorCodes::NOT_IMPLEMENTED);\n+    }\n+\n+    void insertDefaultKeys(const PaddedPODArray<StringRef> & keys) override\n+    {\n+        if constexpr (dictionary_key_type == DictionaryKeyType::complex)\n+            insertDefaultKeysImpl(keys);\n+        else\n+            throw Exception(\"Method insertDefaultKeysImpl is not supported for simple key storage\", ErrorCodes::NOT_IMPLEMENTED);\n+    }\n+\n+    PaddedPODArray<StringRef> getCachedComplexKeys() const override\n+    {\n+        if constexpr (dictionary_key_type == DictionaryKeyType::complex)\n+            return getCachedKeysImpl();\n+        else\n+            throw Exception(\"Method getCachedSimpleKeys is not supported for simple key storage\", ErrorCodes::NOT_IMPLEMENTED);\n+    }\n+\n+    size_t getSize() const override { return index.size(); }\n+\n+    size_t getMaxSize() const override {return index.getMaxSize(); }\n+\n+    size_t getBytesAllocated() const override\n+    {\n+        size_t memory_partitions_bytes_size = memory_buffer_partitions.size() * configuration.write_buffer_blocks_size * configuration.block_size;\n+        size_t file_partitions_bytes_size = memory_buffer_partitions.size() * configuration.file_blocks_size * configuration.block_size;\n+\n+        return index.getSizeInBytes() + memory_partitions_bytes_size + file_partitions_bytes_size;\n+    }\n+\n+private:\n+\n+    using TimePoint = std::chrono::system_clock::time_point;\n+\n+    struct Cell\n+    {\n+        enum CellState\n+        {\n+            in_memory,\n+            on_disk,\n+            default_value\n+        };\n+\n+        TimePoint deadline;\n+\n+        SSDCacheIndex index;\n+        size_t in_memory_partition_index;\n+        CellState state;\n+\n+        inline bool isInMemory() const { return state == in_memory; }\n+        inline bool isOnDisk() const { return state == on_disk; }\n+        inline bool isDefaultValue() const { return state == default_value; }\n+    };\n+\n+    struct KeyToBlockOffset\n+    {\n+        KeyToBlockOffset(size_t key_index_, size_t offset_in_block_, bool is_expired_)\n+            : key_index(key_index_), offset_in_block(offset_in_block_), is_expired(is_expired_)\n+        {}\n+\n+        size_t key_index = 0;\n+        size_t offset_in_block = 0;\n+        bool is_expired = false;\n+    };\n+\n+    template <typename Result>\n+    Result fetchColumnsForKeysImpl(\n+        const PaddedPODArray<KeyType> & keys,\n+        const DictionaryStorageFetchRequest & fetch_request) const\n+    {\n+        Result result;\n+\n+        result.fetched_columns = fetch_request.makeAttributesResultColumns();\n+        result.key_index_to_state.resize_fill(keys.size(), {KeyState::not_found});\n+\n+        const auto now = std::chrono::system_clock::now();\n+\n+        size_t fetched_columns_index = 0;\n+\n+        using BlockIndexToKeysMap = std::unordered_map<size_t, std::vector<KeyToBlockOffset>, DefaultHash<size_t>>;\n+        BlockIndexToKeysMap block_to_keys_map;\n+        absl::flat_hash_set<size_t, DefaultHash<size_t>> unique_blocks_to_request;\n+        PaddedPODArray<size_t> blocks_to_request;\n+\n+        std::chrono::seconds strict_max_lifetime_seconds(configuration.strict_max_lifetime_seconds);\n+        size_t keys_size = keys.size();\n+\n+        for (size_t key_index = 0; key_index < keys_size; ++key_index)\n+        {\n+            auto key = keys[key_index];\n+\n+            const auto * it = index.find(key);\n+\n+            if (!it)\n+            {\n+                ++result.not_found_keys_size;\n+                continue;\n+            }\n+\n+            const auto & cell = it->getMapped();\n+\n+            bool has_deadline = cellHasDeadline(cell);\n+\n+            if (has_deadline && now > cell.deadline + strict_max_lifetime_seconds)\n+            {\n+                ++result.not_found_keys_size;\n+                continue;\n+            }\n+\n+            bool cell_is_expired = false;\n+            KeyState::State key_state = KeyState::found;\n+\n+            if (has_deadline && now > cell.deadline)\n+            {\n+                cell_is_expired = true;\n+                key_state = KeyState::expired;\n+            }\n+\n+            result.expired_keys_size += cell_is_expired;\n+            result.found_keys_size += !cell_is_expired;\n+\n+            switch (cell.state)\n+            {\n+                case Cell::in_memory:\n+                {\n+                    result.key_index_to_state[key_index] = {key_state, fetched_columns_index};\n+                    ++fetched_columns_index;\n+\n+                    const auto & partition = memory_buffer_partitions[cell.in_memory_partition_index];\n+                    char * serialized_columns_place = partition.getPlace(cell.index);\n+                    deserializeAndInsertIntoColumns(result.fetched_columns, fetch_request, serialized_columns_place);\n+                    break;\n+                }\n+                case Cell::on_disk:\n+                {\n+                    block_to_keys_map[cell.index.block_index].emplace_back(key_index, cell.index.offset_in_block, cell_is_expired);\n+\n+                    if (!unique_blocks_to_request.contains(cell.index.block_index))\n+                    {\n+                        blocks_to_request.emplace_back(cell.index.block_index);\n+                        unique_blocks_to_request.insert(cell.index.block_index);\n+                    }\n+                    break;\n+                }\n+                case Cell::default_value:\n+                {\n+                    result.key_index_to_state[key_index] = {key_state, fetched_columns_index};\n+                    result.key_index_to_state[key_index].setDefault();\n+                    ++fetched_columns_index;\n+                    ++result.default_keys_size;\n+\n+                    insertDefaultValuesIntoColumns(result.fetched_columns, fetch_request, key_index);\n+                    break;\n+                }\n+            }\n+        }\n+\n+        /// Sort blocks by offset before start async io requests\n+        std::sort(blocks_to_request.begin(), blocks_to_request.end());\n+\n+        file_buffer.fetchBlocks(read_from_file_buffer.m_data, configuration.read_buffer_blocks_size, blocks_to_request, [&](size_t block_index, char * block_data)\n+        {\n+            auto & keys_in_block = block_to_keys_map[block_index];\n+\n+            for (auto & key_in_block : keys_in_block)\n+            {\n+                char * key_data = block_data + key_in_block.offset_in_block;\n+                deserializeAndInsertIntoColumns(result.fetched_columns, fetch_request, key_data);\n+\n+                if (key_in_block.is_expired)\n+                    result.key_index_to_state[key_in_block.key_index] = {KeyState::expired, fetched_columns_index};\n+                else\n+                    result.key_index_to_state[key_in_block.key_index] = {KeyState::found, fetched_columns_index};\n+\n+                ++fetched_columns_index;\n+            }\n+        });\n+\n+        return result;\n+    }\n+\n+    void insertColumnsForKeysImpl(const PaddedPODArray<KeyType> & keys, Columns columns)\n+    {\n+        size_t columns_to_serialize_size = columns.size();\n+        PaddedPODArray<StringRef> temporary_column_data(columns_to_serialize_size);\n+\n+        Arena temporary_values_pool;\n+\n+        const auto now = std::chrono::system_clock::now();\n+\n+        for (size_t key_index = 0; key_index < keys.size(); ++key_index)\n+        {\n+            size_t allocated_size_for_columns = 0;\n+            const char * block_start = nullptr;\n+\n+            auto key = keys[key_index];\n+\n+            for (size_t column_index = 0; column_index < columns_to_serialize_size; ++column_index)\n+            {\n+                auto & column = columns[column_index];\n+                temporary_column_data[column_index] = column->serializeValueIntoArena(key_index, temporary_values_pool, block_start);\n+                allocated_size_for_columns += temporary_column_data[column_index].size;\n+            }\n+\n+            SSDCacheKeyType ssd_cache_key { key, allocated_size_for_columns, block_start };\n+\n+            if (!SSDCacheBlock::canBeWrittenInEmptyBlock(ssd_cache_key, configuration.block_size))\n+                throw Exception(\"Serialized columns size is greater than allowed block size and metadata\", ErrorCodes::UNSUPPORTED_METHOD);\n+\n+            /// We cannot reuse place that is already allocated in file or memory cache so we erase key from index\n+            index.erase(key);\n+\n+            Cell cell;\n+            setCellDeadline(cell, now);\n+\n+            if constexpr (dictionary_key_type == DictionaryKeyType::complex)\n+            {\n+                /// Copy complex key into arena and put in cache\n+                size_t key_size = key.size;\n+                char * place_for_key = complex_key_arena.alloc(key_size);\n+                memcpy(reinterpret_cast<void *>(place_for_key), reinterpret_cast<const void *>(key.data), key_size);\n+                KeyType updated_key{place_for_key, key_size};\n+                ssd_cache_key.key = updated_key;\n+            }\n+\n+            insertCell(ssd_cache_key, cell);\n+\n+            temporary_values_pool.rollback(allocated_size_for_columns);\n+        }\n+    }\n+\n+    void insertDefaultKeysImpl(const PaddedPODArray<KeyType> & keys)\n+    {\n+        const auto now = std::chrono::system_clock::now();\n+\n+        for (auto key : keys)\n+        {\n+            /// We cannot reuse place that is already allocated in file or memory cache so we erase key from index\n+            index.erase(key);\n+\n+            Cell cell;\n+\n+            setCellDeadline(cell, now);\n+            cell.index = {0, 0};\n+            cell.in_memory_partition_index = 0;\n+            cell.state = Cell::default_value;\n+\n+\n+            if constexpr (dictionary_key_type == DictionaryKeyType::complex)\n+            {\n+                /// Copy complex key into arena and put in cache\n+                size_t key_size = key.size;\n+                char * place_for_key = complex_key_arena.alloc(key_size);\n+                memcpy(reinterpret_cast<void *>(place_for_key), reinterpret_cast<const void *>(key.data), key_size);\n+                KeyType updated_key{place_for_key, key_size};\n+                key = updated_key;\n+            }\n+\n+            index.insert(key, cell);\n+        }\n+    }\n+\n+    PaddedPODArray<KeyType> getCachedKeysImpl() const\n+    {\n+        PaddedPODArray<KeyType> result;\n+        result.reserve(index.size());\n+\n+        for (auto & node : index)\n+        {\n+            auto & cell = node.getMapped();\n+\n+            if (cell.state == Cell::default_value)\n+                continue;\n+\n+            result.emplace_back(node.getKey());\n+        }\n+\n+        return result;\n+    }\n+\n+    void insertCell(SSDCacheKeyType & ssd_cache_key, Cell & cell)\n+    {\n+        /** InsertCell has following flow\n+\n+            1. We try to write key into current memory buffer, if write succeeded then return.\n+            2. Then if we does not write key into current memory buffer, we try to flush current memory buffer\n+            to disk.\n+\n+            If flush succeeded then reset current memory buffer, write key into it and return.\n+            If flush failed that means that current partition on disk is full, need to allocate new partition\n+            or start reusing old ones.\n+\n+            Retry to step 1.\n+         */\n+\n+        SSDCacheIndex cache_index {0, 0};\n+\n+        while (true)\n+        {\n+            bool started_reusing_old_partitions = memory_buffer_partitions.size() == configuration.max_partitions_count;\n+\n+            auto & current_memory_buffer_partition = memory_buffer_partitions[current_partition_index];\n+\n+            bool write_into_memory_buffer_result = current_memory_buffer_partition.writeKey(ssd_cache_key, cache_index);\n+\n+            if (write_into_memory_buffer_result)\n+            {\n+                cell.state = Cell::in_memory;\n+                cell.index = cache_index;\n+                cell.in_memory_partition_index = current_partition_index;\n+\n+                index.insert(ssd_cache_key.key, cell);\n+                break;\n+            }\n+            else\n+            {\n+                /// Partition memory write buffer if full flush it to disk and retry\n+                size_t block_index_in_file_before_write = file_buffer.getCurrentBlockIndex();\n+\n+                if (started_reusing_old_partitions)\n+                {\n+                    /// If we start reusing old partitions we need to remove old keys on disk from index before writing buffer\n+                    PaddedPODArray<KeyType> old_keys;\n+                    file_buffer.readKeys(block_index_in_file_before_write, configuration.write_buffer_blocks_size, old_keys);\n+\n+                    size_t file_read_end_block_index = block_index_in_file_before_write + configuration.write_buffer_blocks_size;\n+\n+                    for (auto old_key : old_keys)\n+                    {\n+                        auto * it = index.find(old_key);\n+\n+                        if (it)\n+                        {\n+                            const Cell & old_key_cell = it->getMapped();\n+\n+                            size_t old_key_block = old_key_cell.index.block_index;\n+\n+                            /// Check if key in index is key from old partition blocks\n+                            if (old_key_cell.isOnDisk() &&\n+                                old_key_block >= block_index_in_file_before_write &&\n+                                old_key_block < file_read_end_block_index)\n+                                index.erase(old_key);\n+                        }\n+                    }\n+                }\n+\n+                const char * partition_data = current_memory_buffer_partition.getData();\n+\n+                bool flush_to_file_result = file_buffer.writeBuffer(partition_data, configuration.write_buffer_blocks_size);\n+\n+                if (flush_to_file_result)\n+                {\n+                    /// Update index cells keys offset and block index\n+                    PaddedPODArray<KeyType> keys_to_update;\n+                    current_memory_buffer_partition.readKeys(keys_to_update);\n+\n+                    absl::flat_hash_set<KeyType, DefaultHash<KeyType>> updated_keys;\n+\n+                    Int64 keys_to_update_size = static_cast<Int64>(keys_to_update.size());\n+\n+                    /// Start from last to first because there can be multiple keys in same partition.\n+                    /// The valid key is the latest.\n+                    for (Int64 i = keys_to_update_size - 1; i >= 0; --i)\n+                    {\n+                        auto key_to_update = keys_to_update[i];\n+                        auto * it = index.find(key_to_update);\n+\n+                        /// If there are no key to update or key to update not in memory\n+                        if (!it || it->getMapped().state != Cell::in_memory)\n+                            continue;\n+\n+                        /// If there were duplicated keys in memory buffer partition\n+                        if (updated_keys.contains(it->getKey()))\n+                            continue;\n+\n+                        updated_keys.insert(key_to_update);\n+\n+                        Cell & cell_to_update = it->getMapped();\n+\n+                        cell_to_update.state = Cell::on_disk;\n+                        cell_to_update.index.block_index += block_index_in_file_before_write;\n+                    }\n+\n+                    /// Memory buffer partition flushed to disk start reusing it\n+                    current_memory_buffer_partition.reset();\n+                    memset(const_cast<char *>(current_memory_buffer_partition.getData()), 0, current_memory_buffer_partition.getSizeInBytes());\n+\n+                    write_into_memory_buffer_result = current_memory_buffer_partition.writeKey(ssd_cache_key, cache_index);\n+                    assert(write_into_memory_buffer_result);\n+\n+                    cell.state = Cell::in_memory;\n+                    cell.index = cache_index;\n+                    cell.in_memory_partition_index = current_partition_index;\n+\n+                    index.insert(ssd_cache_key.key, cell);\n+                    break;\n+                }\n+                else\n+                {\n+                    /// Partition is full need to try next partition\n+\n+                    if (memory_buffer_partitions.size() < configuration.max_partitions_count)\n+                    {\n+                        /// Try tro create next partition without reusing old partitions\n+                        ++current_partition_index;\n+                        file_buffer.allocateSizeForNextPartition();\n+                        memory_buffer_partitions.emplace_back(configuration.block_size, configuration.write_buffer_blocks_size);\n+                    }\n+                    else\n+                    {\n+                        /// Start reusing old partitions\n+                        current_partition_index = (current_partition_index + 1) % memory_buffer_partitions.size();\n+                        file_buffer.reset();\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    inline static bool cellHasDeadline(const Cell & cell)\n+    {\n+        return cell.deadline != std::chrono::system_clock::from_time_t(0);\n+    }\n+\n+    inline void setCellDeadline(Cell & cell, TimePoint now)\n+    {\n+        if (configuration.lifetime.min_sec == 0 && configuration.lifetime.max_sec == 0)\n+        {\n+            cell.deadline = std::chrono::system_clock::from_time_t(0);\n+            return;\n+        }\n+\n+        size_t min_sec_lifetime = configuration.lifetime.min_sec;\n+        size_t max_sec_lifetime = configuration.lifetime.max_sec;\n+\n+        std::uniform_int_distribution<UInt64> distribution{min_sec_lifetime, max_sec_lifetime};\n+        cell.deadline = now + std::chrono::seconds{distribution(rnd_engine)};\n+    }\n+\n+    template <typename>\n+    friend class ArenaCellKeyDisposer;\n+\n+    SSDCacheDictionaryStorageConfiguration configuration;\n+\n+    SSDCacheFileBuffer<SSDCacheKeyType> file_buffer;\n+\n+    Memory<Allocator<true>> read_from_file_buffer;\n+\n+    std::vector<SSDCacheMemoryBuffer<SSDCacheKeyType>> memory_buffer_partitions;\n+\n+    pcg64 rnd_engine;\n+\n+    class ArenaCellKeyDisposer\n+    {\n+    public:\n+        ArenaWithFreeLists & arena;\n+\n+        template <typename Key, typename Value>\n+        void operator()(const Key & key, const Value &) const\n+        {\n+            /// In case of complex key we keep it in arena\n+            if constexpr (std::is_same_v<Key, StringRef>)\n+                arena.free(const_cast<char *>(key.data), key.size);\n+        }\n+    };\n+\n+    using SimpleKeyLRUHashMap = LRUHashMap<UInt64, Cell, ArenaCellKeyDisposer>;\n+    using ComplexKeyLRUHashMap = LRUHashMapWithSavedHash<StringRef, Cell, ArenaCellKeyDisposer>;\n+\n+    using CacheLRUHashMap = std::conditional_t<\n+        dictionary_key_type == DictionaryKeyType::simple,\n+        SimpleKeyLRUHashMap,\n+        ComplexKeyLRUHashMap>;\n+\n+    ArenaWithFreeLists complex_key_arena;\n+\n+    CacheLRUHashMap index;\n+\n+    size_t current_partition_index = 0;\n+\n+};\n+\n+}\n+\n+#endif\ndiff --git a/src/Dictionaries/SSDComplexKeyCacheDictionary.cpp b/src/Dictionaries/SSDComplexKeyCacheDictionary.cpp\ndeleted file mode 100644\nindex cb22dd2be155..000000000000\n--- a/src/Dictionaries/SSDComplexKeyCacheDictionary.cpp\n+++ /dev/null\n@@ -1,1772 +0,0 @@\n-#if defined(OS_LINUX) || defined(__FreeBSD__)\n-\n-#include \"SSDComplexKeyCacheDictionary.h\"\n-\n-#include <algorithm>\n-#include <Columns/ColumnsNumber.h>\n-#include <Common/typeid_cast.h>\n-#include <Common/ProfileEvents.h>\n-#include <Common/ProfilingScopedRWLock.h>\n-#include <Common/MemorySanitizer.h>\n-#include <DataStreams/IBlockInputStream.h>\n-#include <DataTypes/DataTypesDecimal.h>\n-#include \"DictionaryBlockInputStream.h\"\n-#include \"DictionaryFactory.h\"\n-#include <IO/AIO.h>\n-#include <IO/ReadHelpers.h>\n-#include <IO/WriteHelpers.h>\n-#include <ext/chrono_io.h>\n-#include <ext/map.h>\n-#include <ext/range.h>\n-#include <ext/size.h>\n-#include <ext/bit_cast.h>\n-#include <numeric>\n-#include <filesystem>\n-#include <city.h>\n-#include <fcntl.h>\n-#include <Functions/FunctionHelpers.h>\n-\n-namespace ProfileEvents\n-{\n-    extern const Event DictCacheKeysRequested;\n-    extern const Event DictCacheKeysRequestedMiss;\n-    extern const Event DictCacheKeysRequestedFound;\n-    extern const Event DictCacheKeysExpired;\n-    extern const Event DictCacheKeysNotFound;\n-    extern const Event DictCacheKeysHit;\n-    extern const Event DictCacheRequestTimeNs;\n-    extern const Event DictCacheRequests;\n-    extern const Event DictCacheLockWriteNs;\n-    extern const Event DictCacheLockReadNs;\n-    extern const Event FileOpen;\n-    extern const Event WriteBufferAIOWrite;\n-    extern const Event WriteBufferAIOWriteBytes;\n-}\n-\n-namespace CurrentMetrics\n-{\n-    extern const Metric DictCacheRequests;\n-    extern const Metric Write;\n-}\n-\n-namespace DB\n-{\n-\n-namespace ErrorCodes\n-{\n-    extern const int AIO_READ_ERROR;\n-    extern const int AIO_WRITE_ERROR;\n-    extern const int BAD_ARGUMENTS;\n-    extern const int CACHE_DICTIONARY_UPDATE_FAIL;\n-    extern const int CANNOT_ALLOCATE_MEMORY;\n-    extern const int CANNOT_CREATE_DIRECTORY;\n-    extern const int CANNOT_FSYNC;\n-    extern const int CANNOT_IO_GETEVENTS;\n-    extern const int CANNOT_IO_SUBMIT;\n-    extern const int CANNOT_OPEN_FILE;\n-    extern const int CORRUPTED_DATA;\n-    extern const int FILE_DOESNT_EXIST;\n-    extern const int NOT_IMPLEMENTED;\n-    extern const int TYPE_MISMATCH;\n-    extern const int UNSUPPORTED_METHOD;\n-}\n-\n-namespace\n-{\n-    constexpr size_t DEFAULT_SSD_BLOCK_SIZE_BYTES = DEFAULT_AIO_FILE_BLOCK_SIZE;\n-    constexpr size_t DEFAULT_FILE_SIZE_BYTES = 4 * 1024 * 1024 * 1024ULL;\n-    constexpr size_t DEFAULT_PARTITIONS_COUNT = 16;\n-    constexpr size_t DEFAULT_READ_BUFFER_SIZE_BYTES = 16 * DEFAULT_SSD_BLOCK_SIZE_BYTES;\n-    constexpr size_t DEFAULT_WRITE_BUFFER_SIZE_BYTES = DEFAULT_SSD_BLOCK_SIZE_BYTES;\n-\n-    constexpr size_t DEFAULT_MAX_STORED_KEYS = 100000;\n-\n-    constexpr size_t BUFFER_ALIGNMENT = DEFAULT_AIO_FILE_BLOCK_SIZE;\n-    constexpr size_t BLOCK_CHECKSUM_SIZE_BYTES = 8;\n-    constexpr size_t BLOCK_SPECIAL_FIELDS_SIZE_BYTES = 4;\n-\n-    constexpr UInt64 KEY_METADATA_EXPIRES_AT_MASK = std::numeric_limits<std::chrono::system_clock::time_point::rep>::max();\n-    constexpr UInt64 KEY_METADATA_IS_DEFAULT_MASK = ~KEY_METADATA_EXPIRES_AT_MASK;\n-\n-    constexpr size_t KEY_IN_MEMORY_BIT = 63;\n-    constexpr size_t KEY_IN_MEMORY = (1ULL << KEY_IN_MEMORY_BIT);\n-    constexpr size_t BLOCK_INDEX_BITS = 32;\n-    constexpr size_t INDEX_IN_BLOCK_BITS = 16;\n-    constexpr size_t INDEX_IN_BLOCK_MASK = (1ULL << INDEX_IN_BLOCK_BITS) - 1;\n-    constexpr size_t BLOCK_INDEX_MASK = ((1ULL << (BLOCK_INDEX_BITS + INDEX_IN_BLOCK_BITS)) - 1) ^ INDEX_IN_BLOCK_MASK;\n-\n-    constexpr size_t NOT_EXISTS = -1;\n-\n-    constexpr UInt8 HAS_NOT_FOUND = 2;\n-\n-    const std::string BIN_FILE_EXT = \".bin\";\n-    const std::string IND_FILE_EXT = \".idx\";\n-\n-    int preallocateDiskSpace(int fd, size_t len)\n-    {\n-        #if defined(__FreeBSD__)\n-            return posix_fallocate(fd, 0, len);\n-        #else\n-            return fallocate(fd, 0, 0, len);\n-        #endif\n-    }\n-}\n-\n-SSDComplexKeyCachePartition::Metadata::time_point_t SSDComplexKeyCachePartition::Metadata::expiresAt() const\n-{\n-    return ext::safe_bit_cast<time_point_t>(data & KEY_METADATA_EXPIRES_AT_MASK);\n-}\n-void SSDComplexKeyCachePartition::Metadata::setExpiresAt(const time_point_t & t)\n-{\n-    data = ext::safe_bit_cast<time_point_urep_t>(t);\n-}\n-\n-bool SSDComplexKeyCachePartition::Metadata::isDefault() const\n-{\n-    return (data & KEY_METADATA_IS_DEFAULT_MASK) == KEY_METADATA_IS_DEFAULT_MASK;\n-}\n-void SSDComplexKeyCachePartition::Metadata::setDefault()\n-{\n-    data |= KEY_METADATA_IS_DEFAULT_MASK;\n-}\n-\n-bool SSDComplexKeyCachePartition::Index::inMemory() const\n-{\n-    return (index & KEY_IN_MEMORY) == KEY_IN_MEMORY;\n-}\n-\n-bool SSDComplexKeyCachePartition::Index::exists() const\n-{\n-    return index != NOT_EXISTS;\n-}\n-\n-void SSDComplexKeyCachePartition::Index::setNotExists()\n-{\n-    index = NOT_EXISTS;\n-}\n-\n-void SSDComplexKeyCachePartition::Index::setInMemory(const bool in_memory)\n-{\n-    index = (index & ~KEY_IN_MEMORY) | (static_cast<size_t>(in_memory) << KEY_IN_MEMORY_BIT);\n-}\n-\n-size_t SSDComplexKeyCachePartition::Index::getAddressInBlock() const\n-{\n-    return index & INDEX_IN_BLOCK_MASK;\n-}\n-\n-void SSDComplexKeyCachePartition::Index::setAddressInBlock(const size_t address_in_block)\n-{\n-    index = (index & ~INDEX_IN_BLOCK_MASK) | address_in_block;\n-}\n-\n-size_t SSDComplexKeyCachePartition::Index::getBlockId() const\n-{\n-    return (index & BLOCK_INDEX_MASK) >> INDEX_IN_BLOCK_BITS;\n-}\n-\n-void SSDComplexKeyCachePartition::Index::setBlockId(const size_t block_id)\n-{\n-    index = (index & ~BLOCK_INDEX_MASK) | (block_id << INDEX_IN_BLOCK_BITS);\n-}\n-\n-SSDComplexKeyCachePartition::SSDComplexKeyCachePartition(\n-        const AttributeUnderlyingType & /* key_structure */,\n-        const std::vector<AttributeUnderlyingType> & attributes_structure_,\n-        const std::string & dir_path,\n-        const size_t file_id_,\n-        const size_t max_size_,\n-        const size_t block_size_,\n-        const size_t read_buffer_size_,\n-        const size_t write_buffer_size_,\n-        const size_t max_stored_keys_)\n-    : file_id(file_id_)\n-    , max_size(max_size_)\n-    , block_size(block_size_)\n-    , read_buffer_size(read_buffer_size_)\n-    , write_buffer_size(write_buffer_size_)\n-    , max_stored_keys(max_stored_keys_)\n-    , path(dir_path + \"/\" + std::to_string(file_id))\n-    , key_to_index(max_stored_keys, KeyDeleter(keys_pool))\n-    , attributes_structure(attributes_structure_)\n-{\n-    if (!std::filesystem::create_directories(std::filesystem::path{dir_path}))\n-    {\n-        if (std::filesystem::exists(std::filesystem::path{dir_path}))\n-            LOG_INFO(&Poco::Logger::get(\"SSDComplexKeyCachePartition::Constructor\"), \"Using existing directory '{}' for cache-partition\", dir_path);\n-        else\n-            throw Exception{\"Failed to create directories.\", ErrorCodes::CANNOT_CREATE_DIRECTORY};\n-    }\n-\n-    {\n-        ProfileEvents::increment(ProfileEvents::FileOpen);\n-\n-        const std::string filename = path + BIN_FILE_EXT;\n-        fd = ::open(filename.c_str(), O_RDWR | O_CREAT | O_TRUNC | O_DIRECT, 0666);\n-        if (fd == -1)\n-        {\n-            auto error_code = (errno == ENOENT) ? ErrorCodes::FILE_DOESNT_EXIST : ErrorCodes::CANNOT_OPEN_FILE;\n-            throwFromErrnoWithPath(\"Cannot open file \" + filename, filename, error_code);\n-        }\n-\n-        if (preallocateDiskSpace(fd, max_size * block_size) < 0)\n-            throwFromErrnoWithPath(\"Cannot preallocate space for the file \" + filename, filename, ErrorCodes::CANNOT_ALLOCATE_MEMORY);\n-    }\n-}\n-\n-SSDComplexKeyCachePartition::~SSDComplexKeyCachePartition()\n-{\n-    std::unique_lock lock(rw_lock);\n-    ::close(fd);\n-}\n-\n-size_t SSDComplexKeyCachePartition::appendDefaults(\n-    const KeyRefs & keys_in,\n-    const PaddedPODArray<Metadata> & metadata,\n-    const size_t begin)\n-{\n-    std::unique_lock lock(rw_lock);\n-    KeyRefs keys(keys_in.size());\n-    for (size_t i = 0; i < keys_in.size(); ++i)\n-        keys[i] = keys_pool.copyKeyFrom(keys_in[i]);\n-\n-    return append(keys, Attributes{}, metadata, begin);\n-}\n-\n-size_t SSDComplexKeyCachePartition::appendBlock(\n-    const Columns & key_columns, const DataTypes & /* key_types */,\n-    const Attributes & new_attributes, const PaddedPODArray<Metadata> & metadata, const size_t begin)\n-{\n-    std::unique_lock lock(rw_lock);\n-    if (!new_attributes.empty() && new_attributes.size() != attributes_structure.size())\n-        throw Exception{\"Wrong columns number in block.\", ErrorCodes::BAD_ARGUMENTS};\n-\n-    const auto keys_size = key_columns.size();\n-    KeyRefs keys(key_columns.front()->size());\n-    {\n-        StringRefs tmp_keys_refs(keys_size);\n-        for (size_t i = 0; i < key_columns.front()->size(); ++i)\n-            keys[i] = keys_pool.allocKey(i, key_columns, tmp_keys_refs);\n-    }\n-\n-    return append(keys, new_attributes, metadata, begin);\n-}\n-\n-size_t SSDComplexKeyCachePartition::append(\n-    const KeyRefs & keys,\n-    const Attributes & new_attributes,\n-    const PaddedPODArray<Metadata> & metadata,\n-    const size_t begin)\n-{\n-    if (!memory)\n-        memory.emplace(block_size * write_buffer_size, BUFFER_ALIGNMENT);\n-\n-    auto init_write_buffer = [&]()\n-    {\n-        write_buffer.emplace(memory->data() + current_memory_block_id * block_size, block_size);\n-        uint64_t tmp = 0;\n-        write_buffer->write(reinterpret_cast<char*>(&tmp), BLOCK_CHECKSUM_SIZE_BYTES);\n-        write_buffer->write(reinterpret_cast<char*>(&tmp), BLOCK_SPECIAL_FIELDS_SIZE_BYTES);\n-        keys_in_block = 0;\n-    };\n-\n-    if (!write_buffer)\n-        init_write_buffer();\n-    if (!keys_buffer_pool)\n-        keys_buffer_pool.emplace();\n-\n-    bool flushed = false;\n-    auto finish_block = [&]()\n-    {\n-        write_buffer.reset();\n-        std::memcpy(memory->data() + block_size * current_memory_block_id + BLOCK_CHECKSUM_SIZE_BYTES, &keys_in_block, sizeof(keys_in_block)); // set count\n-        uint64_t checksum = CityHash_v1_0_2::CityHash64(memory->data() + block_size * current_memory_block_id + BLOCK_CHECKSUM_SIZE_BYTES, block_size - BLOCK_CHECKSUM_SIZE_BYTES); // checksum\n-        std::memcpy(memory->data() + block_size * current_memory_block_id, &checksum, sizeof(checksum));\n-        if (++current_memory_block_id == write_buffer_size)\n-            flush();\n-        flushed = true;\n-    };\n-\n-    for (size_t index = begin; index < keys.size();)\n-    {\n-        Index cache_index;\n-        cache_index.setInMemory(true);\n-        cache_index.setBlockId(current_memory_block_id);\n-        cache_index.setAddressInBlock(write_buffer->offset());\n-\n-        flushed = false;\n-        if (keys[index].fullSize() + sizeof(UInt64) > write_buffer->available()) // place for key and metadata\n-        {\n-            finish_block();\n-        }\n-        else\n-        {\n-            keys_pool.writeKey(keys[index], *write_buffer);\n-            writeBinary(metadata[index].data, *write_buffer);\n-        }\n-\n-        for (const auto & attribute : new_attributes)\n-        {\n-            if (flushed)\n-                break;\n-            switch (attribute.type)\n-            {\n-#define DISPATCH(TYPE) \\\n-            case AttributeUnderlyingType::ut##TYPE: \\\n-                { \\\n-                    if (sizeof(TYPE) > write_buffer->available()) \\\n-                    { \\\n-                        finish_block(); \\\n-                        continue; \\\n-                    } \\\n-                    else \\\n-                    { \\\n-                        const auto & values = std::get<Attribute::Container<TYPE>>(attribute.values); /* NOLINT */ \\\n-                        writeBinary(values[index], *write_buffer); \\\n-                    } \\\n-                } \\\n-                break;\n-\n-                DISPATCH(UInt8)\n-                DISPATCH(UInt16)\n-                DISPATCH(UInt32)\n-                DISPATCH(UInt64)\n-                DISPATCH(UInt128)\n-                DISPATCH(Int8)\n-                DISPATCH(Int16)\n-                DISPATCH(Int32)\n-                DISPATCH(Int64)\n-                DISPATCH(Decimal32)\n-                DISPATCH(Decimal64)\n-                DISPATCH(Decimal128)\n-                DISPATCH(Float32)\n-                DISPATCH(Float64)\n-#undef DISPATCH\n-\n-            case AttributeUnderlyingType::utString:\n-                {\n-                    const auto & value = std::get<Attribute::Container<String>>(attribute.values)[index];\n-                    if (sizeof(UInt64) + value.size() > write_buffer->available())\n-                    {\n-                        finish_block();\n-                        continue;\n-                    }\n-                    else\n-                    {\n-                        writeStringBinary(value, *write_buffer);\n-                    }\n-                }\n-                break;\n-            }\n-        }\n-\n-        if (!flushed)\n-        {\n-            key_to_index.setWithDelete(keys[index], cache_index);\n-            keys_buffer.push_back(keys_buffer_pool->copyKeyFrom(keys[index]));\n-            ++index;\n-            ++keys_in_block;\n-        }\n-        else  // next block in write buffer or flushed to ssd\n-        {\n-            init_write_buffer();\n-        }\n-    }\n-    return keys.size() - begin;\n-}\n-\n-void SSDComplexKeyCachePartition::flush()\n-{\n-    if (current_file_block_id >= max_size)\n-        clearOldestBlocks();\n-\n-    if (keys_buffer.empty())\n-        return;\n-\n-    AIOContext aio_context{1};\n-\n-    iocb write_request{};\n-    iocb * write_request_ptr{&write_request};\n-\n-#if defined(__FreeBSD__)\n-    write_request.aio.aio_lio_opcode = LIO_WRITE;\n-    write_request.aio.aio_fildes = fd;\n-    write_request.aio.aio_buf = reinterpret_cast<volatile void *>(memory->data());\n-    write_request.aio.aio_nbytes = block_size * write_buffer_size;\n-    write_request.aio.aio_offset = (current_file_block_id % max_size) * block_size;\n-#else\n-    write_request.aio_lio_opcode = IOCB_CMD_PWRITE;\n-    write_request.aio_fildes = fd;\n-    write_request.aio_buf = reinterpret_cast<UInt64>(memory->data());\n-    write_request.aio_nbytes = block_size * write_buffer_size;\n-    write_request.aio_offset = (current_file_block_id % max_size) * block_size;\n-#endif\n-\n-    while (io_submit(aio_context.ctx, 1, &write_request_ptr) < 0)\n-    {\n-        if (errno != EINTR)\n-            throw Exception(\"Cannot submit request for asynchronous IO on file \" + path + BIN_FILE_EXT, ErrorCodes::CANNOT_IO_SUBMIT);\n-    }\n-\n-    CurrentMetrics::Increment metric_increment_write{CurrentMetrics::Write};\n-\n-    io_event event;\n-    while (io_getevents(aio_context.ctx, 1, 1, &event, nullptr) < 0)\n-    {\n-        if (errno != EINTR)\n-            throw Exception(\"Failed to wait for asynchronous IO completion on file \" + path + BIN_FILE_EXT, ErrorCodes::CANNOT_IO_GETEVENTS);\n-    }\n-\n-    // Unpoison the memory returned from an uninstrumented system function.\n-    __msan_unpoison(&event, sizeof(event));\n-\n-    ssize_t  bytes_written;\n-#if defined(__FreeBSD__)\n-    bytes_written = aio_return(reinterpret_cast<struct aiocb *>(event.udata));\n-#else\n-    bytes_written = event.res;\n-#endif\n-\n-    ProfileEvents::increment(ProfileEvents::WriteBufferAIOWrite);\n-    ProfileEvents::increment(ProfileEvents::WriteBufferAIOWriteBytes, bytes_written);\n-\n-    if (bytes_written != static_cast<decltype(bytes_written)>(block_size * write_buffer_size))\n-        throw Exception(\"Not all data was written for asynchronous IO on file \" + path + BIN_FILE_EXT + \". returned: \" + std::to_string(bytes_written), ErrorCodes::AIO_WRITE_ERROR);\n-\n-    if (::fsync(fd) < 0)\n-        throwFromErrnoWithPath(\"Cannot fsync \" + path + BIN_FILE_EXT, path + BIN_FILE_EXT, ErrorCodes::CANNOT_FSYNC);\n-\n-    /// commit changes in index\n-    for (auto & key : keys_buffer)\n-    {\n-        Index index;\n-        if (key_to_index.getKeyAndValue(key, index))\n-        {\n-            if (index.inMemory()) // Row can be inserted in the buffer twice, so we need to move to ssd only the last index.\n-            {\n-                index.setInMemory(false);\n-                index.setBlockId((current_file_block_id % max_size) + index.getBlockId());\n-            }\n-            key_to_index.set(key, index);\n-        }\n-    }\n-\n-    current_file_block_id += write_buffer_size;\n-    current_memory_block_id = 0;\n-\n-    /// clear buffer\n-    keys_buffer.clear();\n-    keys_buffer_pool.reset();\n-    keys_buffer_pool.emplace();\n-}\n-\n-template <typename Out, typename GetDefault>\n-void SSDComplexKeyCachePartition::getValue(\n-    const size_t attribute_index,\n-    const Columns & key_columns,\n-    const DataTypes & key_types,\n-    ResultArrayType<Out> & out,\n-    std::vector<bool> & found,\n-    GetDefault & default_value_extractor,\n-    std::chrono::system_clock::time_point now) const\n-{\n-    auto set_value = [&](const size_t index, ReadBuffer & buf)\n-    {\n-        keys_pool.ignoreKey(buf);\n-        Metadata metadata;\n-        readVarUInt(metadata.data, buf);\n-\n-        if (metadata.expiresAt() > now)\n-        {\n-            if (metadata.isDefault())\n-                out[index] = default_value_extractor[index];\n-            else\n-            {\n-                ignoreFromBufferToAttributeIndex(attribute_index, buf);\n-                readBinary(out[index], buf);\n-            }\n-            found[index] = true;\n-        }\n-    };\n-\n-    getImpl(key_columns, key_types, set_value, found);\n-}\n-\n-void SSDComplexKeyCachePartition::getString(const size_t attribute_index,\n-    const Columns & key_columns, const DataTypes & key_types,\n-    StringRefs & refs, ArenaWithFreeLists & arena, std::vector<bool> & found,\n-    std::vector<size_t> & default_ids,\n-    std::chrono::system_clock::time_point now) const\n-{\n-    auto set_value = [&](const size_t index, ReadBuffer & buf)\n-    {\n-        keys_pool.ignoreKey(buf);\n-        Metadata metadata;\n-        readBinary(metadata.data, buf);\n-\n-        if (metadata.expiresAt() > now)\n-        {\n-            if (metadata.isDefault())\n-                default_ids.push_back(index);\n-            else\n-            {\n-                ignoreFromBufferToAttributeIndex(attribute_index, buf);\n-                size_t size = 0;\n-                readVarUInt(size, buf);\n-                char * string_ptr = arena.alloc(size);\n-                memcpy(string_ptr, buf.position(), size);\n-                refs[index].data = string_ptr;\n-                refs[index].size = size;\n-            }\n-            found[index] = true;\n-        }\n-    };\n-\n-    getImpl(key_columns, key_types, set_value, found);\n-}\n-\n-void SSDComplexKeyCachePartition::hasKeys(\n-    const Columns & key_columns, const DataTypes & key_types, ResultArrayType<UInt8> & out,\n-    std::vector<bool> & found, std::chrono::system_clock::time_point now) const\n-{\n-    auto set_value = [&](const size_t index, ReadBuffer & buf)\n-    {\n-        keys_pool.ignoreKey(buf);\n-        Metadata metadata;\n-        readBinary(metadata.data, buf);\n-\n-        if (metadata.expiresAt() > now)\n-            out[index] = !metadata.isDefault();\n-    };\n-\n-    getImpl(key_columns, key_types, set_value, found);\n-}\n-\n-template <typename SetFunc>\n-void SSDComplexKeyCachePartition::getImpl(\n-    const Columns & key_columns, const DataTypes & /* key_types */,\n-    SetFunc & set, std::vector<bool> & found) const\n-{\n-    TemporalComplexKeysPool tmp_keys_pool;\n-    StringRefs tmp_refs(key_columns.size());\n-\n-    std::shared_lock lock(rw_lock);\n-    PaddedPODArray<Index> indices(key_columns.front()->size());\n-    for (size_t i = 0; i < key_columns.front()->size(); ++i)\n-    {\n-        auto key = tmp_keys_pool.allocKey(i, key_columns, tmp_refs);\n-        SCOPE_EXIT(tmp_keys_pool.rollback(key));\n-        Index index;\n-        if (found[i])\n-            indices[i].setNotExists();\n-        else if (key_to_index.get(key, index))\n-            indices[i] = index;\n-        else\n-            indices[i].setNotExists();\n-    }\n-\n-    getValueFromMemory(indices, set);\n-    getValueFromStorage(indices, set);\n-}\n-\n-template <typename SetFunc>\n-void SSDComplexKeyCachePartition::getValueFromMemory(const PaddedPODArray<Index> & indices, SetFunc & set) const\n-{\n-    // Do not check checksum while reading from memory.\n-    for (size_t i = 0; i < indices.size(); ++i)\n-    {\n-        const auto & index = indices[i];\n-        if (index.exists() && index.inMemory())\n-        {\n-            const size_t offset = index.getBlockId() * block_size + index.getAddressInBlock();\n-\n-            ReadBufferFromMemory read_buffer(memory->data() + offset, block_size * write_buffer_size - offset);\n-            set(i, read_buffer);\n-        }\n-    }\n-}\n-\n-template <typename SetFunc>\n-void SSDComplexKeyCachePartition::getValueFromStorage(const PaddedPODArray<Index> & indices, SetFunc & set) const\n-{\n-    std::vector<std::pair<Index, size_t>> index_to_out;\n-    for (size_t i = 0; i < indices.size(); ++i)\n-    {\n-        const auto & index = indices[i];\n-        if (index.exists() && !index.inMemory())\n-            index_to_out.emplace_back(index, i);\n-    }\n-    if (index_to_out.empty())\n-        return;\n-\n-    /// sort by (block_id, offset_in_block)\n-    std::sort(std::begin(index_to_out), std::end(index_to_out));\n-\n-    Memory read_buffer(block_size * read_buffer_size, BUFFER_ALIGNMENT);\n-\n-    // TODO: merge requests\n-    std::vector<iocb> requests;\n-    std::vector<iocb*> pointers;\n-    std::vector<std::vector<size_t>> blocks_to_indices;\n-    requests.reserve(index_to_out.size());\n-    pointers.reserve(index_to_out.size());\n-    blocks_to_indices.reserve(index_to_out.size());\n-    for (size_t i = 0; i < index_to_out.size(); ++i)\n-    {\n-        #if defined(__FreeBSD__)\n-        const size_t back_offset = requests.empty() ? -1 : static_cast<size_t>(requests.back().aio.aio_offset);\n-        #else\n-        const size_t back_offset = requests.empty() ? -1 : static_cast<size_t>(requests.back().aio_offset);\n-        #endif\n-\n-        if (!requests.empty() && back_offset == index_to_out[i].first.getBlockId() * block_size)\n-        {\n-            blocks_to_indices.back().push_back(i);\n-            continue;\n-        }\n-\n-        iocb request{};\n-#if defined(__FreeBSD__)\n-        request.aio.aio_lio_opcode = LIO_READ;\n-        request.aio.aio_fildes = fd;\n-        request.aio.aio_buf = reinterpret_cast<volatile void *>(\n-            reinterpret_cast<UInt64>(read_buffer.data()) + block_size * (requests.size() % read_buffer_size));\n-        request.aio.aio_nbytes = block_size;\n-        request.aio.aio_offset = index_to_out[i].first.getBlockId() * block_size;\n-        request.aio_data = requests.size();\n-#else\n-        request.aio_lio_opcode = IOCB_CMD_PREAD;\n-        request.aio_fildes = fd;\n-        request.aio_buf = reinterpret_cast<UInt64>(read_buffer.data()) + block_size * (requests.size() % read_buffer_size);\n-        request.aio_nbytes = block_size;\n-        request.aio_offset = index_to_out[i].first.getBlockId() * block_size;\n-        request.aio_data = requests.size();\n-#endif\n-        requests.push_back(request);\n-        pointers.push_back(&requests.back());\n-        blocks_to_indices.emplace_back();\n-        blocks_to_indices.back().push_back(i);\n-    }\n-\n-    AIOContext aio_context(read_buffer_size);\n-\n-    std::vector<bool> processed(requests.size(), false);\n-    std::vector<io_event> events(requests.size());\n-    #if defined(__linux__)\n-    for (auto & event : events)\n-        event.res = -1;\n-    #endif\n-\n-\n-    size_t to_push = 0;\n-    size_t to_pop = 0;\n-    while (to_pop < requests.size())\n-    {\n-        /// get io tasks from previous iteration\n-        int popped = 0;\n-        while (to_pop < to_push && (popped = io_getevents(aio_context.ctx, to_push - to_pop, to_push - to_pop, &events[to_pop], nullptr)) <= 0)\n-        {\n-            if (errno != EINTR)\n-                throwFromErrno(\"io_getevents: Failed to get an event for asynchronous IO\", ErrorCodes::CANNOT_IO_GETEVENTS);\n-        }\n-\n-        for (size_t i = to_pop; i < to_pop + popped; ++i)\n-        {\n-            const auto request_id = events[i].data;\n-            const auto & request = requests[request_id];\n-\n-            #if defined(__FreeBSD__)\n-            const auto bytes_written = aio_return(reinterpret_cast<struct aiocb *>(events[i].udata));\n-            #else\n-            const auto bytes_written = events[i].res;\n-            #endif\n-\n-            if (bytes_written != static_cast<ssize_t>(block_size))\n-            {\n-                #if defined(__FreeBSD__)\n-                    throw Exception(\"AIO failed to read file \" + path + BIN_FILE_EXT + \".\", ErrorCodes::AIO_READ_ERROR);\n-                #else\n-                    throw Exception(\"AIO failed to read file \" + path + BIN_FILE_EXT + \". \" +\n-                        \"request_id= \" + std::to_string(request.aio_data) + \"/ \" + std::to_string(requests.size()) +\n-                        \", aio_nbytes=\" + std::to_string(request.aio_nbytes) + \", aio_offset=\" + std::to_string(request.aio_offset) +\n-                        \", returned=\" + std::to_string(events[i].res) + \", errno=\" + std::to_string(errno), ErrorCodes::AIO_READ_ERROR);\n-                #endif\n-            }\n-            #if defined(__FreeBSD__)\n-            const char* buf_ptr = reinterpret_cast<char *>(reinterpret_cast<UInt64>(request.aio.aio_buf));\n-            #else\n-            const auto* buf_ptr = reinterpret_cast<char *>(request.aio_buf);\n-            #endif\n-\n-            __msan_unpoison(buf_ptr, block_size);\n-            uint64_t checksum = 0;\n-            ReadBufferFromMemory buf_special(buf_ptr, block_size);\n-            readBinary(checksum, buf_special);\n-            uint64_t calculated_checksum = CityHash_v1_0_2::CityHash64(buf_ptr + BLOCK_CHECKSUM_SIZE_BYTES, block_size - BLOCK_CHECKSUM_SIZE_BYTES);\n-            if (checksum != calculated_checksum)\n-            {\n-                throw Exception(\"Cache data corrupted. From block = \" + std::to_string(checksum) + \" calculated = \" + std::to_string(calculated_checksum) + \".\", ErrorCodes::CORRUPTED_DATA);\n-            }\n-\n-            for (const size_t idx : blocks_to_indices[request_id])\n-            {\n-                const auto & [file_index, out_index] = index_to_out[idx];\n-                ReadBufferFromMemory buf(\n-                        buf_ptr + file_index.getAddressInBlock(),\n-                        block_size - file_index.getAddressInBlock());\n-                set(out_index, buf);\n-            }\n-\n-            processed[request_id] = true;\n-        }\n-\n-        while (to_pop < requests.size() && processed[to_pop])\n-            ++to_pop;\n-\n-        /// add new io tasks\n-        const int new_tasks_count = std::min(read_buffer_size - (to_push - to_pop), requests.size() - to_push);\n-\n-        int pushed = 0;\n-        while (new_tasks_count > 0 && (pushed = io_submit(aio_context.ctx, new_tasks_count, &pointers[to_push])) <= 0)\n-        {\n-            if (errno != EINTR)\n-                throwFromErrno(\"io_submit: Failed to submit a request for asynchronous IO\", ErrorCodes::CANNOT_IO_SUBMIT);\n-        }\n-        to_push += pushed;\n-    }\n-}\n-\n-void SSDComplexKeyCachePartition::clearOldestBlocks()\n-{\n-    // write_buffer_size, because we need to erase the whole buffer.\n-    Memory read_buffer_memory(block_size * write_buffer_size, BUFFER_ALIGNMENT);\n-\n-    iocb request{};\n-#if defined(__FreeBSD__)\n-    request.aio.aio_lio_opcode = LIO_READ;\n-    request.aio.aio_fildes = fd;\n-    request.aio.aio_buf = reinterpret_cast<volatile void *>(reinterpret_cast<UInt64>(read_buffer_memory.data()));\n-    request.aio.aio_nbytes = block_size * write_buffer_size;\n-    request.aio.aio_offset = (current_file_block_id % max_size) * block_size;\n-    request.aio_data = 0;\n-#else\n-    request.aio_lio_opcode = IOCB_CMD_PREAD;\n-    request.aio_fildes = fd;\n-    request.aio_buf = reinterpret_cast<UInt64>(read_buffer_memory.data());\n-    request.aio_nbytes = block_size * write_buffer_size;\n-    request.aio_offset = (current_file_block_id % max_size) * block_size;\n-    request.aio_data = 0;\n-#endif\n-\n-    {\n-        iocb* request_ptr = &request;\n-        io_event event{};\n-        AIOContext aio_context(1);\n-\n-        while (io_submit(aio_context.ctx, 1, &request_ptr) != 1)\n-            if (errno != EINTR)\n-                throwFromErrno(\"io_submit: Failed to submit a request for asynchronous IO\", ErrorCodes::CANNOT_IO_SUBMIT);\n-\n-        while (io_getevents(aio_context.ctx, 1, 1, &event, nullptr) != 1)\n-            if (errno != EINTR)\n-                throwFromErrno(\"io_getevents: Failed to get an event for asynchronous IO\", ErrorCodes::CANNOT_IO_GETEVENTS);\n-\n-#if defined(__FreeBSD__)\n-        if (aio_return(reinterpret_cast<struct aiocb *>(event.udata)) != static_cast<ssize_t>(request.aio.aio_nbytes))\n-            throw Exception(\"GC: AIO failed to read file \" + path + BIN_FILE_EXT + \".\", ErrorCodes::AIO_READ_ERROR);\n-#else\n-        if (event.res != static_cast<ssize_t>(request.aio_nbytes))\n-            throw Exception(\"GC: AIO failed to read file \" + path + BIN_FILE_EXT + \". \" +\n-                \"aio_nbytes=\" + std::to_string(request.aio_nbytes) +\n-                \", returned=\" + std::to_string(event.res) + \".\", ErrorCodes::AIO_READ_ERROR);\n-#endif\n-        __msan_unpoison(read_buffer_memory.data(), read_buffer_memory.size());\n-    }\n-\n-    TemporalComplexKeysPool tmp_keys_pool;\n-    KeyRefs keys;\n-\n-    for (size_t i = 0; i < write_buffer_size; ++i)\n-    {\n-        ReadBufferFromMemory read_buffer(read_buffer_memory.data() + i * block_size, block_size);\n-\n-        uint64_t checksum = 0;\n-        readBinary(checksum, read_buffer);\n-        uint64_t calculated_checksum = CityHash_v1_0_2::CityHash64(read_buffer_memory.data() + i * block_size + BLOCK_CHECKSUM_SIZE_BYTES, block_size - BLOCK_CHECKSUM_SIZE_BYTES);\n-        if (checksum != calculated_checksum)\n-        {\n-            throw Exception(\"Cache data corrupted. From block = \" + std::to_string(checksum) + \" calculated = \" + std::to_string(calculated_checksum) + \".\", ErrorCodes::CORRUPTED_DATA);\n-        }\n-\n-        uint32_t keys_in_current_block = 0;\n-        readBinary(keys_in_current_block, read_buffer);\n-\n-        for (uint32_t j = 0; j < keys_in_current_block; ++j)\n-        {\n-            keys.emplace_back();\n-            tmp_keys_pool.readKey(keys.back(), read_buffer);\n-\n-            Metadata metadata;\n-            readBinary(metadata.data, read_buffer);\n-\n-            if (!metadata.isDefault())\n-            {\n-                for (const auto & attr : attributes_structure)\n-                {\n-                    switch (attr)\n-                    {\n-            #define DISPATCH(TYPE) \\\n-                    case AttributeUnderlyingType::ut##TYPE: \\\n-                        read_buffer.ignore(sizeof(TYPE)); \\\n-                        break;\n-\n-                        DISPATCH(UInt8)\n-                        DISPATCH(UInt16)\n-                        DISPATCH(UInt32)\n-                        DISPATCH(UInt64)\n-                        DISPATCH(UInt128)\n-                        DISPATCH(Int8)\n-                        DISPATCH(Int16)\n-                        DISPATCH(Int32)\n-                        DISPATCH(Int64)\n-                        DISPATCH(Decimal32)\n-                        DISPATCH(Decimal64)\n-                        DISPATCH(Decimal128)\n-                        DISPATCH(Float32)\n-                        DISPATCH(Float64)\n-            #undef DISPATCH\n-\n-                    case AttributeUnderlyingType::utString:\n-                        {\n-                            size_t size = 0;\n-                            readVarUInt(size, read_buffer);\n-                            read_buffer.ignore(size);\n-                        }\n-                        break;\n-                    }\n-                }\n-            }\n-        }\n-    }\n-\n-    const size_t start_block = current_file_block_id % max_size;\n-    const size_t finish_block = start_block + write_buffer_size;\n-    for (const auto& key : keys)\n-    {\n-        Index index;\n-        if (key_to_index.get(key, index))\n-        {\n-            size_t block_id = index.getBlockId();\n-            if (start_block <= block_id && block_id < finish_block)\n-                key_to_index.erase(key);\n-        }\n-    }\n-}\n-\n-void SSDComplexKeyCachePartition::ignoreFromBufferToAttributeIndex(const size_t attribute_index, ReadBuffer & buf) const\n-{\n-    for (size_t i = 0; i < attribute_index; ++i)\n-    {\n-        switch (attributes_structure[i])\n-        {\n-#define DISPATCH(TYPE) \\\n-        case AttributeUnderlyingType::ut##TYPE: \\\n-            buf.ignore(sizeof(TYPE)); \\\n-            break;\n-\n-            DISPATCH(UInt8)\n-            DISPATCH(UInt16)\n-            DISPATCH(UInt32)\n-            DISPATCH(UInt64)\n-            DISPATCH(UInt128)\n-            DISPATCH(Int8)\n-            DISPATCH(Int16)\n-            DISPATCH(Int32)\n-            DISPATCH(Int64)\n-            DISPATCH(Decimal32)\n-            DISPATCH(Decimal64)\n-            DISPATCH(Decimal128)\n-            DISPATCH(Float32)\n-            DISPATCH(Float64)\n-#undef DISPATCH\n-\n-        case AttributeUnderlyingType::utString:\n-            {\n-                size_t size = 0;\n-                readVarUInt(size, buf);\n-                buf.ignore(size);\n-            }\n-            break;\n-        }\n-    }\n-}\n-\n-size_t SSDComplexKeyCachePartition::getId() const\n-{\n-    return file_id;\n-}\n-\n-double SSDComplexKeyCachePartition::getLoadFactor() const\n-{\n-    std::shared_lock lock(rw_lock);\n-    return static_cast<double>(current_file_block_id) / max_size;\n-}\n-\n-size_t SSDComplexKeyCachePartition::getElementCount() const\n-{\n-    std::shared_lock lock(rw_lock);\n-    return key_to_index.size();\n-}\n-\n-size_t SSDComplexKeyCachePartition::getBytesAllocated() const\n-{\n-    std::shared_lock lock(rw_lock);\n-    return 16.5 * key_to_index.capacity() + keys_pool.size() +\n-        (keys_buffer_pool ? keys_buffer_pool->size() : 0) + (memory ? memory->size() : 0);\n-}\n-\n-void SSDComplexKeyCachePartition::remove()\n-{\n-    std::unique_lock lock(rw_lock);\n-    std::filesystem::remove(std::filesystem::path(path + BIN_FILE_EXT));\n-}\n-\n-SSDComplexKeyCacheStorage::SSDComplexKeyCacheStorage(\n-        const AttributeTypes & attributes_structure_,\n-        const std::string & path_,\n-        const size_t max_partitions_count_,\n-        const size_t file_size_,\n-        const size_t block_size_,\n-        const size_t read_buffer_size_,\n-        const size_t write_buffer_size_,\n-        const size_t max_stored_keys_)\n-    : attributes_structure(attributes_structure_)\n-    , path(path_)\n-    , max_partitions_count(max_partitions_count_)\n-    , file_size(file_size_)\n-    , block_size(block_size_)\n-    , read_buffer_size(read_buffer_size_)\n-    , write_buffer_size(write_buffer_size_)\n-    , max_stored_keys(max_stored_keys_)\n-    , log(&Poco::Logger::get(\"SSDComplexKeyCacheStorage\"))\n-{\n-}\n-\n-SSDComplexKeyCacheStorage::~SSDComplexKeyCacheStorage()\n-{\n-    std::unique_lock lock(rw_lock);\n-    partition_delete_queue.splice(std::end(partition_delete_queue), partitions);\n-    collectGarbage();\n-}\n-\n-template <typename Out, typename GetDefault>\n-void SSDComplexKeyCacheStorage::getValue(\n-    const size_t attribute_index, const Columns & key_columns, const DataTypes & key_types,\n-    ResultArrayType<Out> & out, std::unordered_map<KeyRef, std::vector<size_t>> & not_found,\n-    TemporalComplexKeysPool & not_found_pool,\n-    GetDefault & get_default, std::chrono::system_clock::time_point now) const\n-{\n-    size_t n = key_columns.front()->size();\n-    std::vector<bool> found(n, false);\n-\n-    {\n-        std::shared_lock lock(rw_lock);\n-        for (const auto & partition : partitions)\n-            partition->getValue<Out>(attribute_index, key_columns, key_types, out, found, get_default, now);\n-    }\n-\n-    size_t count_not_found = 0;\n-    StringRefs tmp_refs(key_columns.size());\n-    for (size_t i = 0; i < n; ++i)\n-    {\n-        if (!found[i])\n-        {\n-            auto key = not_found_pool.allocKey(i, key_columns, tmp_refs);\n-            not_found[key].push_back(i);\n-            ++count_not_found;\n-        }\n-    }\n-\n-    query_count.fetch_add(n, std::memory_order_relaxed);\n-    hit_count.fetch_add(n - count_not_found, std::memory_order_release);\n-}\n-\n-void SSDComplexKeyCacheStorage::getString(\n-    const size_t attribute_index, const Columns & key_columns, const DataTypes & key_types,\n-    StringRefs & refs, ArenaWithFreeLists & arena,\n-    std::unordered_map<KeyRef, std::vector<size_t>> & not_found,\n-    TemporalComplexKeysPool & not_found_pool,\n-    std::vector<size_t> & default_ids, std::chrono::system_clock::time_point now) const\n-{\n-    size_t n = key_columns.front()->size();\n-    std::vector<bool> found(n, false);\n-\n-    {\n-        std::shared_lock lock(rw_lock);\n-        for (const auto & partition : partitions)\n-            partition->getString(attribute_index, key_columns, key_types, refs, arena, found, default_ids, now);\n-    }\n-\n-    size_t count_not_found = 0;\n-    StringRefs tmp_refs(key_columns.size());\n-    for (size_t i = 0; i < n; ++i)\n-    {\n-        if (!found[i])\n-        {\n-            auto key = not_found_pool.allocKey(i, key_columns, tmp_refs);\n-            not_found[key].push_back(i);\n-            ++count_not_found;\n-        }\n-    }\n-\n-    query_count.fetch_add(n, std::memory_order_relaxed);\n-    hit_count.fetch_add(n - count_not_found, std::memory_order_release);\n-}\n-\n-void SSDComplexKeyCacheStorage::hasKeys(\n-    const Columns & key_columns, const DataTypes & key_types, ResultArrayType<UInt8> & out,\n-    std::unordered_map<KeyRef, std::vector<size_t>> & not_found,\n-    TemporalComplexKeysPool & not_found_pool, std::chrono::system_clock::time_point now) const\n-{\n-    size_t n = key_columns.front()->size();\n-    for (size_t i = 0; i < n; ++i)\n-        out[i] = HAS_NOT_FOUND;\n-    std::vector<bool> found(n, false);\n-\n-    {\n-        std::shared_lock lock(rw_lock);\n-        for (const auto & partition : partitions)\n-            partition->hasKeys(key_columns, key_types, out, found, now);\n-    }\n-\n-    size_t count_not_found = 0;\n-    StringRefs tmp_refs(key_columns.size());\n-    for (size_t i = 0; i < n; ++i)\n-    {\n-        if (out[i] == HAS_NOT_FOUND)\n-        {\n-            auto key = not_found_pool.allocKey(i, key_columns, tmp_refs);\n-            not_found[key].push_back(i);\n-            ++count_not_found;\n-        }\n-    }\n-\n-    query_count.fetch_add(n, std::memory_order_relaxed);\n-    hit_count.fetch_add(n - count_not_found, std::memory_order_release);\n-}\n-\n-namespace\n-{\n-SSDComplexKeyCachePartition::Attributes createAttributesFromBlock(\n-        const Block & block, const size_t begin_column, const std::vector<AttributeUnderlyingType> & structure)\n-{\n-    SSDComplexKeyCachePartition::Attributes attributes;\n-\n-    const auto columns = block.getColumns();\n-    for (size_t i = 0; i < structure.size(); ++i)\n-    {\n-        const auto & column = columns[i + begin_column];\n-        switch (structure[i])\n-        {\n-#define DISPATCH(TYPE) \\\n-        case AttributeUnderlyingType::ut##TYPE: \\\n-            { \\\n-                SSDComplexKeyCachePartition::Attribute::Container<TYPE> values(column->size()); \\\n-                memcpy(&values[0], column->getRawData().data, sizeof(TYPE) * values.size()); \\\n-                attributes.emplace_back(); \\\n-                attributes.back().type = structure[i]; \\\n-                attributes.back().values = std::move(values); \\\n-            } \\\n-            break;\n-\n-            DISPATCH(UInt8)\n-            DISPATCH(UInt16)\n-            DISPATCH(UInt32)\n-            DISPATCH(UInt64)\n-            DISPATCH(UInt128)\n-            DISPATCH(Int8)\n-            DISPATCH(Int16)\n-            DISPATCH(Int32)\n-            DISPATCH(Int64)\n-            DISPATCH(Decimal32)\n-            DISPATCH(Decimal64)\n-            DISPATCH(Decimal128)\n-            DISPATCH(Float32)\n-            DISPATCH(Float64)\n-#undef DISPATCH\n-\n-        case AttributeUnderlyingType::utString:\n-            {\n-                attributes.emplace_back();\n-                SSDComplexKeyCachePartition::Attribute::Container<String> values(column->size());\n-                for (size_t j = 0; j < column->size(); ++j)\n-                {\n-                    const auto ref = column->getDataAt(j);\n-                    values[j].resize(ref.size);\n-                    memcpy(values[j].data(), ref.data, ref.size);\n-                }\n-                attributes.back().type = structure[i];\n-                attributes.back().values = std::move(values);\n-            }\n-            break;\n-        }\n-    }\n-\n-    return attributes;\n-}\n-}\n-\n-template <typename PresentIdHandler, typename AbsentIdHandler>\n-void SSDComplexKeyCacheStorage::update(\n-    DictionarySourcePtr & source_ptr,\n-    const Columns & key_columns,\n-    const DataTypes & key_types,\n-    const KeyRefs & required_keys,\n-    const std::vector<size_t> & required_rows,\n-    TemporalComplexKeysPool & tmp_keys_pool,\n-    PresentIdHandler && on_updated,\n-    AbsentIdHandler && on_key_not_found,\n-    const DictionaryLifetime lifetime)\n-{\n-    assert(key_columns.size() == key_types.size());\n-\n-    auto append_block = [&key_types, this](\n-        const Columns & new_keys,\n-        const SSDComplexKeyCachePartition::Attributes & new_attributes,\n-        const PaddedPODArray<SSDComplexKeyCachePartition::Metadata> & metadata)\n-    {\n-        size_t inserted = 0;\n-        while (inserted < metadata.size())\n-        {\n-            if (!partitions.empty())\n-                inserted += partitions.front()->appendBlock(\n-                    new_keys, key_types, new_attributes, metadata, inserted);\n-            if (inserted < metadata.size())\n-            {\n-                partitions.emplace_front(std::make_unique<SSDComplexKeyCachePartition>(\n-                    AttributeUnderlyingType::utUInt64, attributes_structure, path,\n-                    (partitions.empty() ? 0 : partitions.front()->getId() + 1),\n-                    file_size, block_size, read_buffer_size, write_buffer_size, max_stored_keys));\n-            }\n-        }\n-\n-        collectGarbage();\n-    };\n-\n-    CurrentMetrics::Increment metric_increment{CurrentMetrics::DictCacheRequests};\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysRequested, required_keys.size());\n-\n-    std::unordered_map<KeyRef, UInt8> remaining_keys{required_keys.size()};\n-    for (const auto & key : required_keys)\n-        remaining_keys.insert({key, 0});\n-\n-    const auto now = std::chrono::system_clock::now();\n-\n-    {\n-        const auto keys_size = key_columns.size();\n-        StringRefs keys(keys_size);\n-\n-        const ProfilingScopedWriteRWLock write_lock{rw_lock, ProfileEvents::DictCacheLockWriteNs};\n-\n-        if (now > backoff_end_time)\n-        {\n-            try\n-            {\n-                if (update_error_count)\n-                {\n-                    /// Recover after error: we have to clone the source here because\n-                    /// it could keep connections which should be reset after error.\n-                    source_ptr = source_ptr->clone();\n-                }\n-\n-                Stopwatch watch;\n-                auto stream = source_ptr->loadKeys(key_columns, required_rows);\n-                stream->readPrefix();\n-\n-                while (const auto block = stream->read())\n-                {\n-                    const auto new_key_columns = ext::map<Columns>(\n-                        ext::range(0, keys_size),\n-                        [&](const size_t attribute_idx) { return block.safeGetByPosition(attribute_idx).column; });\n-\n-                    const auto new_attributes = createAttributesFromBlock(block, keys_size, attributes_structure);\n-\n-                    const auto rows_num = block.rows();\n-                    PaddedPODArray<SSDComplexKeyCachePartition::Metadata> metadata(rows_num);\n-\n-                    for (const auto i : ext::range(0, rows_num))\n-                    {\n-                        auto key = tmp_keys_pool.allocKey(i, new_key_columns, keys);\n-                        //SCOPE_EXIT(tmp_keys_pool.rollback(key));\n-\n-                        std::uniform_int_distribution<UInt64> distribution{lifetime.min_sec, lifetime.max_sec};\n-                        metadata[i].setExpiresAt(now + std::chrono::seconds(distribution(rnd_engine)));\n-                        /// mark corresponding id as found\n-                        on_updated(key, i, new_attributes);\n-                        remaining_keys[key] = 1;\n-                    }\n-\n-                    append_block(new_key_columns, new_attributes, metadata);\n-                }\n-\n-                stream->readSuffix();\n-\n-                update_error_count = 0;\n-                last_update_exception = std::exception_ptr{};\n-                backoff_end_time = std::chrono::system_clock::time_point{};\n-\n-                ProfileEvents::increment(ProfileEvents::DictCacheRequestTimeNs, watch.elapsed());\n-            }\n-            catch (...)\n-            {\n-                ++update_error_count;\n-                last_update_exception = std::current_exception();\n-                backoff_end_time = now + std::chrono::seconds(calculateDurationWithBackoff(rnd_engine, update_error_count));\n-\n-                tryLogException(last_update_exception, log,\n-                        \"Could not update ssd cache dictionary, next update is scheduled at \" + ext::to_string(backoff_end_time));\n-            }\n-        }\n-    }\n-\n-    auto append_defaults = [this](\n-        const KeyRefs & new_keys,\n-        const PaddedPODArray<SSDComplexKeyCachePartition::Metadata> & metadata)\n-    {\n-        size_t inserted = 0;\n-        while (inserted < metadata.size())\n-        {\n-            if (!partitions.empty())\n-                inserted += partitions.front()->appendDefaults(\n-                    new_keys, metadata, inserted);\n-            if (inserted < metadata.size())\n-            {\n-                partitions.emplace_front(std::make_unique<SSDComplexKeyCachePartition>(\n-                        AttributeUnderlyingType::utUInt64, attributes_structure, path,\n-                        (partitions.empty() ? 0 : partitions.front()->getId() + 1),\n-                        file_size, block_size, read_buffer_size, write_buffer_size, max_stored_keys));\n-            }\n-        }\n-\n-        collectGarbage();\n-    };\n-\n-    size_t not_found_num = 0, found_num = 0;\n-    /// Check which ids have not been found and require setting null_value\n-    KeyRefs default_keys;\n-\n-    PaddedPODArray<SSDComplexKeyCachePartition::Metadata> metadata;\n-    {\n-        const ProfilingScopedWriteRWLock write_lock{rw_lock, ProfileEvents::DictCacheLockWriteNs};\n-\n-        for (const auto & key_found_pair : remaining_keys)\n-        {\n-            if (key_found_pair.second)\n-            {\n-                ++found_num;\n-                continue;\n-            }\n-            ++not_found_num;\n-\n-            const auto key = key_found_pair.first;\n-\n-            if (update_error_count)\n-            {\n-                /// TODO: use old values.\n-\n-                // We don't have expired data for that `id` so all we can do is\n-                // to rethrow `last_exception`. We might have to throw the same\n-                // exception for different callers of dictGet() in different\n-                // threads, which might then modify the exception object, so we\n-                // have to throw a copy.\n-                try\n-                {\n-                    std::rethrow_exception(last_update_exception);\n-                }\n-                catch (...)\n-                {\n-                    throw DB::Exception(ErrorCodes::CACHE_DICTIONARY_UPDATE_FAIL,\n-                        \"Update failed for dictionary '{}': {}\",\n-                        getPath(),\n-                        getCurrentExceptionMessage(true /*with stack trace*/,\n-                            true /*check embedded stack trace*/));\n-                }\n-            }\n-\n-            std::uniform_int_distribution<UInt64> distribution{lifetime.min_sec, lifetime.max_sec};\n-            metadata.emplace_back();\n-            metadata.back().setExpiresAt(now + std::chrono::seconds(distribution(rnd_engine)));\n-            metadata.back().setDefault();\n-\n-            default_keys.push_back(key);\n-\n-            /// inform caller that the cell has not been found\n-            on_key_not_found(key);\n-        }\n-\n-        if (not_found_num)\n-            append_defaults(default_keys, metadata);\n-    }\n-\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysRequestedMiss, not_found_num);\n-    ProfileEvents::increment(ProfileEvents::DictCacheKeysRequestedFound, found_num);\n-    ProfileEvents::increment(ProfileEvents::DictCacheRequests);\n-}\n-\n-double SSDComplexKeyCacheStorage::getLoadFactor() const\n-{\n-    double result = 0;\n-    std::shared_lock lock(rw_lock);\n-    for (const auto & partition : partitions)\n-        result += partition->getLoadFactor();\n-    return result / partitions.size();\n-}\n-\n-size_t SSDComplexKeyCacheStorage::getElementCount() const\n-{\n-    size_t result = 0;\n-    std::shared_lock lock(rw_lock);\n-    for (const auto & partition : partitions)\n-        result += partition->getElementCount();\n-    return result;\n-}\n-\n-void SSDComplexKeyCacheStorage::collectGarbage()\n-{\n-    // add partitions to queue\n-    while (partitions.size() > max_partitions_count)\n-    {\n-        partition_delete_queue.splice(std::end(partition_delete_queue), partitions, std::prev(std::end(partitions)));\n-    }\n-\n-    // drop unused partitions\n-    while (!partition_delete_queue.empty() && partition_delete_queue.front().use_count() == 1)\n-    {\n-        partition_delete_queue.front()->remove();\n-        partition_delete_queue.pop_front();\n-    }\n-}\n-\n-SSDComplexKeyCacheDictionary::SSDComplexKeyCacheDictionary(\n-    const StorageID & dict_id_,\n-    const DictionaryStructure & dict_struct_,\n-    DictionarySourcePtr source_ptr_,\n-    const DictionaryLifetime dict_lifetime_,\n-    const std::string & path_,\n-    const size_t max_partitions_count_,\n-    const size_t file_size_,\n-    const size_t block_size_,\n-    const size_t read_buffer_size_,\n-    const size_t write_buffer_size_,\n-    const size_t max_stored_keys_)\n-    : IDictionaryBase(dict_id_)\n-    , dict_struct(dict_struct_)\n-    , source_ptr(std::move(source_ptr_))\n-    , dict_lifetime(dict_lifetime_)\n-    , path(path_)\n-    , max_partitions_count(max_partitions_count_)\n-    , file_size(file_size_)\n-    , block_size(block_size_)\n-    , read_buffer_size(read_buffer_size_)\n-    , write_buffer_size(write_buffer_size_)\n-    , max_stored_keys(max_stored_keys_)\n-    , storage(ext::map<std::vector>(dict_struct.attributes, [](const auto & attribute) { return attribute.underlying_type; }),\n-            path, max_partitions_count, file_size, block_size, read_buffer_size, write_buffer_size, max_stored_keys)\n-    , log(&Poco::Logger::get(\"SSDComplexKeyCacheDictionary\"))\n-{\n-    LOG_INFO(log, \"Using storage path '{}'.\", path);\n-    if (!this->source_ptr->supportsSelectiveLoad())\n-        throw Exception{name + \": source cannot be used with CacheDictionary\", ErrorCodes::UNSUPPORTED_METHOD};\n-\n-    createAttributes();\n-}\n-\n-ColumnPtr SSDComplexKeyCacheDictionary::getColumn(\n-    const std::string & attribute_name,\n-    const DataTypePtr & result_type,\n-    const Columns & key_columns,\n-    const DataTypes & key_types,\n-    const ColumnPtr default_values_column) const\n-{\n-    ColumnPtr result;\n-\n-    dict_struct.validateKeyTypes(key_types);\n-\n-    const auto index = getAttributeIndex(attribute_name);\n-    const auto & dictionary_attribute = dict_struct.getAttribute(attribute_name, result_type);\n-\n-    auto keys_size = key_columns.front()->size();\n-\n-    auto type_call = [&](const auto &dictionary_attribute_type)\n-    {\n-        using Type = std::decay_t<decltype(dictionary_attribute_type)>;\n-        using AttributeType = typename Type::AttributeType;\n-        using ColumnProvider = DictionaryAttributeColumnProvider<AttributeType>;\n-\n-        const auto & null_value = std::get<AttributeType>(null_values[index]);\n-        DictionaryDefaultValueExtractor<AttributeType> default_value_extractor(null_value, default_values_column);\n-\n-        auto column = ColumnProvider::getColumn(dictionary_attribute, keys_size);\n-\n-        if constexpr (std::is_same_v<AttributeType, String>)\n-        {\n-            auto * out = column.get();\n-            getItemsStringImpl(index, key_columns, key_types, out, default_value_extractor);\n-        }\n-        else\n-        {\n-            auto & out = column->getData();\n-            getItemsNumberImpl<AttributeType, AttributeType>(\n-                index,\n-                key_columns,\n-                key_types,\n-                out,\n-                default_value_extractor);\n-        }\n-\n-        result = std::move(column);\n-    };\n-\n-    callOnDictionaryAttributeType(dict_struct.attributes[index].underlying_type, type_call);\n-\n-    return result;\n-}\n-\n-template <typename AttributeType, typename OutputType, typename DefaultValueExtractor>\n-void SSDComplexKeyCacheDictionary::getItemsNumberImpl(\n-    const size_t attribute_index,\n-    const Columns & key_columns,\n-    const DataTypes & key_types,\n-    ResultArrayType<OutputType> & out,\n-    DefaultValueExtractor & default_value_extractor) const\n-{\n-    assert(dict_struct.key);\n-    assert(key_columns.size() == key_types.size());\n-\n-    dict_struct.validateKeyTypes(key_types);\n-\n-    const auto now = std::chrono::system_clock::now();\n-\n-    TemporalComplexKeysPool not_found_pool;\n-    std::unordered_map<KeyRef, std::vector<size_t>> not_found_keys;\n-    storage.getValue<OutputType>(attribute_index, key_columns, key_types, out, not_found_keys, not_found_pool, default_value_extractor, now);\n-    if (not_found_keys.empty())\n-        return;\n-\n-    std::vector<KeyRef> required_keys(not_found_keys.size());\n-    std::transform(std::begin(not_found_keys), std::end(not_found_keys), std::begin(required_keys), [](const auto & pair) { return pair.first; });\n-    std::vector<size_t> required_rows;\n-    required_rows.reserve(required_keys.size());\n-    for (const auto & key_ref : required_keys)\n-        required_rows.push_back(not_found_keys[key_ref].front());\n-\n-    TemporalComplexKeysPool tmp_keys_pool;\n-    storage.update(\n-            source_ptr,\n-            key_columns,\n-            key_types,\n-            required_keys,\n-            required_rows,\n-            tmp_keys_pool,\n-            [&](const auto key, const auto row, const auto & new_attributes)\n-            {\n-                for (const size_t out_row : not_found_keys[key])\n-                    out[out_row] = std::get<SSDComplexKeyCachePartition::Attribute::Container<OutputType>>(new_attributes[attribute_index].values)[row];\n-            },\n-            [&](const auto key)\n-            {\n-                for (const size_t row : not_found_keys[key])\n-                    out[row] = default_value_extractor[row];\n-            },\n-            getLifetime());\n-}\n-\n-void SSDComplexKeyCacheDictionary::getItemsStringImpl(\n-    const size_t attribute_index,\n-    const Columns & key_columns,\n-    const DataTypes & key_types,\n-    ColumnString * out,\n-    DictionaryDefaultValueExtractor<String> & default_value_extractor) const\n-{\n-    dict_struct.validateKeyTypes(key_types);\n-\n-    const auto now = std::chrono::system_clock::now();\n-\n-    TemporalComplexKeysPool not_found_pool;\n-    std::unordered_map<KeyRef, std::vector<size_t>> not_found_keys;\n-\n-    const size_t n = key_columns.front()->size();\n-\n-    StringRefs refs(n);\n-    ArenaWithFreeLists string_arena;\n-    std::vector<size_t> default_rows;\n-    storage.getString(\n-        attribute_index, key_columns, key_types,\n-        refs, string_arena, not_found_keys, not_found_pool, default_rows, now);\n-    std::sort(std::begin(default_rows), std::end(default_rows));\n-\n-    if (not_found_keys.empty())\n-    {\n-        size_t default_index = 0;\n-        for (size_t row = 0; row < n; ++row)\n-        {\n-            if (unlikely(default_index != default_rows.size() && default_rows[default_index] == row))\n-            {\n-                auto to_insert = default_value_extractor[row];\n-                out->insertData(to_insert.data, to_insert.size);\n-                ++default_index;\n-            }\n-            else\n-                out->insertData(refs[row].data, refs[row].size);\n-        }\n-        return;\n-    }\n-\n-    std::vector<KeyRef> required_keys(not_found_keys.size());\n-    std::transform(std::begin(not_found_keys), std::end(not_found_keys), std::begin(required_keys), [](const auto & pair) { return pair.first; });\n-\n-    std::unordered_map<KeyRef, String> update_result;\n-\n-    std::vector<size_t> required_rows;\n-    required_rows.reserve(required_keys.size());\n-    for (const auto & key_ref : required_keys)\n-        required_rows.push_back(not_found_keys[key_ref].front());\n-\n-    TemporalComplexKeysPool tmp_keys_pool;\n-    storage.update(\n-            source_ptr,\n-            key_columns,\n-            key_types,\n-            required_keys,\n-            required_rows,\n-            tmp_keys_pool,\n-            [&](const auto key, const auto row, const auto & new_attributes)\n-            {\n-                update_result[key] = std::get<SSDComplexKeyCachePartition::Attribute::Container<String>>(new_attributes[attribute_index].values)[row];\n-            },\n-            [&](const auto) {},\n-            getLifetime());\n-\n-    StringRefs tmp_refs(key_columns.size());\n-    size_t default_index = 0;\n-    for (size_t row = 0; row < n; ++row)\n-    {\n-        const auto key = tmp_keys_pool.allocKey(row, key_columns, tmp_refs);\n-        SCOPE_EXIT(tmp_keys_pool.rollback(key));\n-        if (unlikely(default_index != default_rows.size() && default_rows[default_index] == row))\n-        {\n-            auto to_insert = default_value_extractor[row];\n-            out->insertData(to_insert.data, to_insert.size);\n-            ++default_index;\n-        }\n-        else if (auto it = not_found_keys.find(key); it == std::end(not_found_keys))\n-        {\n-            out->insertData(refs[row].data, refs[row].size);\n-        }\n-        else if (auto it_update = update_result.find(key); it_update != std::end(update_result))\n-        {\n-            out->insertData(it_update->second.data(), it_update->second.size());\n-        }\n-        else\n-        {\n-            auto to_insert = default_value_extractor[row];\n-            out->insertData(to_insert.data, to_insert.size);\n-        }\n-    }\n-}\n-\n-ColumnUInt8::Ptr SSDComplexKeyCacheDictionary::hasKeys(const Columns & key_columns, const DataTypes & key_types) const\n-{\n-    dict_struct.validateKeyTypes(key_types);\n-\n-    const auto rows_num = key_columns.front()->size();\n-\n-    auto result = ColumnUInt8::create(rows_num);\n-    auto& out = result->getData();\n-\n-    for (const auto row : ext::range(0, rows_num))\n-        out[row] = false;\n-\n-    const auto now = std::chrono::system_clock::now();\n-\n-    std::unordered_map<KeyRef, std::vector<size_t>> not_found_keys;\n-    TemporalComplexKeysPool not_found_pool;\n-    storage.hasKeys(key_columns, key_types, out, not_found_keys, not_found_pool, now);\n-    if (not_found_keys.empty())\n-        return result;\n-\n-    std::vector<KeyRef> required_keys(not_found_keys.size());\n-    std::transform(std::begin(not_found_keys), std::end(not_found_keys), std::begin(required_keys), [](const auto & pair) { return pair.first; });\n-\n-    std::vector<size_t> required_rows;\n-    required_rows.reserve(required_keys.size());\n-    for (const auto & key_ref : required_keys)\n-        required_rows.push_back(not_found_keys[key_ref].front());\n-\n-    TemporalComplexKeysPool tmp_keys_pool;\n-    storage.update(\n-            source_ptr,\n-            key_columns,\n-            key_types,\n-            required_keys,\n-            required_rows,\n-            tmp_keys_pool,\n-            [&](const auto key, const auto, const auto &)\n-            {\n-                for (const size_t out_row : not_found_keys[key])\n-                    out[out_row] = true;\n-            },\n-            [&](const auto key)\n-            {\n-                for (const size_t row : not_found_keys[key])\n-                    out[row] = false;\n-            },\n-            getLifetime());\n-\n-    return result;\n-}\n-\n-BlockInputStreamPtr SSDComplexKeyCacheDictionary::getBlockInputStream(\n-    const Names & /* column_names */, size_t /* max_block_size*/) const\n-{\n-    throw DB::Exception(\"Method not supported.\", ErrorCodes::NOT_IMPLEMENTED);\n-}\n-\n-size_t SSDComplexKeyCacheDictionary::getAttributeIndex(const std::string & attr_name) const\n-{\n-    auto it = attribute_index_by_name.find(attr_name);\n-    if (it == std::end(attribute_index_by_name))\n-        throw  Exception{\"Attribute `\" + name + \"` does not exist.\", ErrorCodes::BAD_ARGUMENTS};\n-    return it->second;\n-}\n-\n-template <typename T>\n-AttributeValueVariant SSDComplexKeyCacheDictionary::createAttributeNullValueWithTypeImpl(const Field & null_value)\n-{\n-    AttributeValueVariant var_null_value = static_cast<T>(null_value.get<NearestFieldType<T>>());\n-    bytes_allocated += sizeof(T);\n-    return var_null_value;\n-}\n-\n-template <>\n-AttributeValueVariant SSDComplexKeyCacheDictionary::createAttributeNullValueWithTypeImpl<String>(const Field & null_value)\n-{\n-    AttributeValueVariant var_null_value = null_value.get<String>();\n-    bytes_allocated += sizeof(StringRef);\n-    return var_null_value;\n-}\n-\n-AttributeValueVariant SSDComplexKeyCacheDictionary::createAttributeNullValueWithType(const AttributeUnderlyingType type, const Field & null_value)\n-{\n-    switch (type)\n-    {\n-#define DISPATCH(TYPE) \\\n-case AttributeUnderlyingType::ut##TYPE: \\\n-    return createAttributeNullValueWithTypeImpl<TYPE>(null_value); /* NOLINT */\n-\n-        DISPATCH(UInt8)\n-        DISPATCH(UInt16)\n-        DISPATCH(UInt32)\n-        DISPATCH(UInt64)\n-        DISPATCH(UInt128)\n-        DISPATCH(Int8)\n-        DISPATCH(Int16)\n-        DISPATCH(Int32)\n-        DISPATCH(Int64)\n-        DISPATCH(Decimal32)\n-        DISPATCH(Decimal64)\n-        DISPATCH(Decimal128)\n-        DISPATCH(Float32)\n-        DISPATCH(Float64)\n-        DISPATCH(String)\n-#undef DISPATCH\n-    }\n-    throw Exception{\"Unknown attribute type: \" + std::to_string(static_cast<int>(type)), ErrorCodes::TYPE_MISMATCH};\n-}\n-\n-void SSDComplexKeyCacheDictionary::createAttributes()\n-{\n-    null_values.reserve(dict_struct.attributes.size());\n-    for (size_t i = 0; i < dict_struct.attributes.size(); ++i)\n-    {\n-        const auto & attribute = dict_struct.attributes[i];\n-\n-        attribute_index_by_name.emplace(attribute.name, i);\n-        null_values.push_back(createAttributeNullValueWithType(attribute.underlying_type, attribute.null_value));\n-\n-        if (attribute.hierarchical)\n-            throw Exception{name + \": hierarchical attributes not supported for dictionary of type \" + getTypeName(),\n-                            ErrorCodes::TYPE_MISMATCH};\n-    }\n-}\n-\n-void registerDictionarySSDComplexKeyCache(DictionaryFactory & factory)\n-{\n-    auto create_layout = [=](const std::string & name,\n-                             const DictionaryStructure & dict_struct,\n-                             const Poco::Util::AbstractConfiguration & config,\n-                             const std::string & config_prefix,\n-                             DictionarySourcePtr source_ptr) -> DictionaryPtr\n-    {\n-        const auto dict_id = StorageID::fromDictionaryConfig(config, config_prefix);\n-\n-        if (dict_struct.id)\n-            throw Exception{\"'id' is not supported for dictionary of layout 'complex_key_cache'\", ErrorCodes::UNSUPPORTED_METHOD};\n-\n-        if (dict_struct.range_min || dict_struct.range_max)\n-            throw Exception{name\n-                            + \": elements .structure.range_min and .structure.range_max should be defined only \"\n-                              \"for a dictionary of layout 'range_hashed'\",\n-                            ErrorCodes::BAD_ARGUMENTS};\n-        const auto & layout_prefix = config_prefix + \".layout\";\n-\n-        const auto max_partitions_count = config.getInt(layout_prefix + \".complex_key_ssd_cache.max_partitions_count\", DEFAULT_PARTITIONS_COUNT);\n-        if (max_partitions_count <= 0)\n-            throw Exception{name + \": dictionary of layout 'complex_key_ssd_cache' cannot have 0 (or less) max_partitions_count\", ErrorCodes::BAD_ARGUMENTS};\n-\n-        const auto block_size = config.getInt(layout_prefix + \".complex_key_ssd_cache.block_size\", DEFAULT_SSD_BLOCK_SIZE_BYTES);\n-        if (block_size <= 0)\n-            throw Exception{name + \": dictionary of layout 'complex_key_ssd_cache' cannot have 0 (or less) block_size\", ErrorCodes::BAD_ARGUMENTS};\n-\n-        const auto file_size = config.getInt64(layout_prefix + \".complex_key_ssd_cache.file_size\", DEFAULT_FILE_SIZE_BYTES);\n-        if (file_size <= 0)\n-            throw Exception{name + \": dictionary of layout 'complex_key_ssd_cache' cannot have 0 (or less) file_size\", ErrorCodes::BAD_ARGUMENTS};\n-        if (file_size % block_size != 0)\n-            throw Exception{name + \": file_size must be a multiple of block_size\", ErrorCodes::BAD_ARGUMENTS};\n-\n-        const auto read_buffer_size = config.getInt64(layout_prefix + \".complex_key_ssd_cache.read_buffer_size\", DEFAULT_READ_BUFFER_SIZE_BYTES);\n-        if (read_buffer_size <= 0)\n-            throw Exception{name + \": dictionary of layout 'complex_key_ssd_cache' cannot have 0 (or less) read_buffer_size\", ErrorCodes::BAD_ARGUMENTS};\n-        if (read_buffer_size % block_size != 0)\n-            throw Exception{name + \": read_buffer_size must be a multiple of block_size\", ErrorCodes::BAD_ARGUMENTS};\n-\n-        const auto write_buffer_size = config.getInt64(layout_prefix + \".complex_key_ssd_cache.write_buffer_size\", DEFAULT_WRITE_BUFFER_SIZE_BYTES);\n-        if (write_buffer_size <= 0)\n-            throw Exception{name + \": dictionary of layout 'complex_key_ssd_cache' cannot have 0 (or less) write_buffer_size\", ErrorCodes::BAD_ARGUMENTS};\n-        if (write_buffer_size % block_size != 0)\n-            throw Exception{name + \": write_buffer_size must be a multiple of block_size\", ErrorCodes::BAD_ARGUMENTS};\n-\n-        auto path = config.getString(layout_prefix + \".complex_key_ssd_cache.path\");\n-        if (path.empty())\n-            throw Exception{name + \": dictionary of layout 'complex_key_ssd_cache' cannot have empty path\",\n-                            ErrorCodes::BAD_ARGUMENTS};\n-        if (path.at(0) != '/')\n-            path = std::filesystem::path{config.getString(\"path\")}.concat(path).string();\n-\n-        const auto max_stored_keys = config.getInt64(layout_prefix + \".complex_key_ssd_cache.max_stored_keys\", DEFAULT_MAX_STORED_KEYS);\n-        if (max_stored_keys <= 0)\n-            throw Exception{name + \": dictionary of layout 'complex_key_ssd_cache' cannot have 0 (or less) max_stored_keys\", ErrorCodes::BAD_ARGUMENTS};\n-\n-        const DictionaryLifetime dict_lifetime{config, config_prefix + \".lifetime\"};\n-        return std::make_unique<SSDComplexKeyCacheDictionary>(\n-                dict_id, dict_struct, std::move(source_ptr), dict_lifetime, path,\n-                max_partitions_count, file_size / block_size, block_size,\n-                read_buffer_size / block_size, write_buffer_size / block_size,\n-                max_stored_keys);\n-    };\n-    factory.registerLayout(\"complex_key_ssd_cache\", create_layout, true);\n-}\n-\n-}\n-\n-#endif\ndiff --git a/src/Dictionaries/SSDComplexKeyCacheDictionary.h b/src/Dictionaries/SSDComplexKeyCacheDictionary.h\ndeleted file mode 100644\nindex be65d823e342..000000000000\n--- a/src/Dictionaries/SSDComplexKeyCacheDictionary.h\n+++ /dev/null\n@@ -1,634 +0,0 @@\n-#pragma once\n-\n-#if defined(OS_LINUX) || defined(__FreeBSD__)\n-\n-#include <atomic>\n-#include <chrono>\n-#include <list>\n-#include <shared_mutex>\n-#include <variant>\n-#include <vector>\n-#include <Poco/Logger.h>\n-#include <Columns/ColumnDecimal.h>\n-#include <Columns/ColumnString.h>\n-#include <Common/Arena.h>\n-#include <Common/ArenaWithFreeLists.h>\n-#include <Common/CurrentMetrics.h>\n-#include <common/logger_useful.h>\n-#include <Common/SmallObjectPool.h>\n-#include <Compression/CompressedWriteBuffer.h>\n-#include <Core/Block.h>\n-#include <Dictionaries/BucketCache.h>\n-#include <ext/scope_guard.h>\n-#include <IO/HashingWriteBuffer.h>\n-#include <pcg_random.hpp>\n-#include \"IDictionary.h\"\n-#include \"IDictionarySource.h\"\n-#include \"DictionaryStructure.h\"\n-#include \"DictionaryHelpers.h\"\n-\n-namespace DB\n-{\n-\n-class KeyRef\n-{\n-public:\n-    explicit KeyRef(char * data) : ptr(data) {}\n-\n-    KeyRef() : ptr(nullptr) {}\n-\n-    inline UInt16 size() const\n-    {\n-        UInt16 res;\n-        memcpy(&res, ptr, sizeof(res));\n-        return res;\n-    }\n-\n-    inline size_t fullSize() const\n-    {\n-        return static_cast<size_t>(size()) + sizeof(UInt16);\n-    }\n-\n-    inline bool isNull() const\n-    {\n-        return ptr == nullptr;\n-    }\n-\n-    inline char * data() const\n-    {\n-        return ptr + sizeof(UInt16);\n-    }\n-\n-    inline char * fullData() const\n-    {\n-        return ptr;\n-    }\n-\n-    inline char * fullData()\n-    {\n-        return ptr;\n-    }\n-\n-    inline const StringRef getRef() const\n-    {\n-        return StringRef(data(), size());\n-    }\n-\n-    inline bool operator==(const KeyRef & other) const\n-    {\n-        return getRef() == other.getRef();\n-    }\n-\n-    inline bool operator!=(const KeyRef & other) const\n-    {\n-        return !(*this == other);\n-    }\n-\n-    inline bool operator<(const KeyRef & other) const\n-    {\n-        return getRef() <  other.getRef();\n-    }\n-\n-private:\n-    char * ptr;\n-};\n-\n-using KeyRefs = std::vector<KeyRef>;\n-}\n-\n-namespace std\n-{\n-    template <>\n-    struct hash<DB::KeyRef>\n-    {\n-        size_t operator() (DB::KeyRef key_ref) const\n-        {\n-            return hasher(key_ref.getRef());\n-        }\n-\n-        std::hash<StringRef> hasher;\n-    };\n-}\n-\n-namespace DB\n-{\n-\n-using AttributeValueVariant = std::variant<\n-        UInt8,\n-        UInt16,\n-        UInt32,\n-        UInt64,\n-        UInt128,\n-        Int8,\n-        Int16,\n-        Int32,\n-        Int64,\n-        Decimal32,\n-        Decimal64,\n-        Decimal128,\n-        Float32,\n-        Float64,\n-        String>;\n-\n-/*\n-    The pool for storing complex keys.\n-*/\n-template <typename A>\n-class ComplexKeysPoolImpl\n-{\n-public:\n-    KeyRef allocKey(const size_t row, const Columns & key_columns, StringRefs & keys)\n-    {\n-        const auto keys_size = key_columns.size();\n-        UInt16 sum_keys_size{};\n-\n-        for (size_t j = 0; j < keys_size; ++j)\n-        {\n-            keys[j] = key_columns[j]->getDataAt(row);\n-            sum_keys_size += keys[j].size;\n-            if (!key_columns[j]->valuesHaveFixedSize())  // String\n-                sum_keys_size += sizeof(size_t) + 1;\n-        }\n-\n-        auto place = arena.alloc(sum_keys_size + sizeof(sum_keys_size));\n-\n-        auto key_start = place;\n-        memcpy(key_start, &sum_keys_size, sizeof(sum_keys_size));\n-        key_start += sizeof(sum_keys_size);\n-        for (size_t j = 0; j < keys_size; ++j)\n-        {\n-            if (!key_columns[j]->valuesHaveFixedSize())  // String\n-            {\n-                auto key_size = keys[j].size + 1;\n-                memcpy(key_start, &key_size, sizeof(size_t));\n-                key_start += sizeof(size_t);\n-                memcpy(key_start, keys[j].data, keys[j].size);\n-                key_start += keys[j].size;\n-                *key_start = '\\0';\n-                ++key_start;\n-            }\n-            else\n-            {\n-                memcpy(key_start, keys[j].data, keys[j].size);\n-                key_start += keys[j].size;\n-            }\n-        }\n-\n-        return KeyRef(place);\n-    }\n-\n-    KeyRef copyKeyFrom(const KeyRef & key)\n-    {\n-        char * data = arena.alloc(key.fullSize());\n-        memcpy(data, key.fullData(), key.fullSize());\n-        return KeyRef(data);\n-    }\n-\n-    void freeKey(const KeyRef & key)\n-    {\n-        if constexpr (std::is_same_v<A, ArenaWithFreeLists>)\n-            arena.free(key.fullData(), key.fullSize());\n-    }\n-\n-    void rollback(const KeyRef & key)\n-    {\n-        if constexpr (std::is_same_v<A, Arena>)\n-            arena.rollback(key.fullSize());\n-    }\n-\n-    void writeKey(const KeyRef & key, WriteBuffer & buf)\n-    {\n-        buf.write(key.fullData(), key.fullSize());\n-    }\n-\n-    void readKey(KeyRef & key, ReadBuffer & buf)\n-    {\n-        UInt16 sz;\n-        readBinary(sz, buf);\n-        char * data = nullptr;\n-        if constexpr (std::is_same_v<A, SmallObjectPool>)\n-            data = arena.alloc();\n-        else\n-            data = arena.alloc(sz + sizeof(sz));\n-        memcpy(data, &sz, sizeof(sz));\n-        buf.read(data + sizeof(sz), sz);\n-        key = KeyRef(data);\n-    }\n-\n-    void ignoreKey(ReadBuffer & buf) const\n-    {\n-        UInt16 sz;\n-        readBinary(sz, buf);\n-        buf.ignore(sz);\n-    }\n-\n-    size_t size() const\n-    {\n-        return arena.size();\n-    }\n-\n-private:\n-    A arena;\n-};\n-\n-using TemporalComplexKeysPool = ComplexKeysPoolImpl<Arena>;\n-using ComplexKeysPool = ComplexKeysPoolImpl<ArenaWithFreeLists>;\n-\n-struct KeyDeleter\n-{\n-    KeyDeleter(ComplexKeysPool & keys_pool_) : keys_pool(keys_pool_) {}\n-\n-    void operator()(const KeyRef key) const\n-    {\n-        keys_pool.freeKey(key);\n-    }\n-\n-    ComplexKeysPool & keys_pool;\n-};\n-\n-\n-/*\n-    Class for operations with cache file and index.\n-    Supports GET/SET operations.\n-*/\n-class SSDComplexKeyCachePartition\n-{\n-public:\n-    struct Index final\n-    {\n-        bool inMemory() const;\n-        void setInMemory(const bool in_memory);\n-\n-        bool exists() const;\n-        void setNotExists();\n-\n-        size_t getAddressInBlock() const;\n-        void setAddressInBlock(const size_t address_in_block);\n-\n-        size_t getBlockId() const;\n-        void setBlockId(const size_t block_id);\n-\n-        bool operator< (const Index & rhs) const { return index < rhs.index; }\n-\n-        /// Stores `is_in_memory` flag, block id, address in uncompressed block\n-        uint64_t index = 0;\n-    };\n-\n-    struct Metadata final\n-    {\n-        using time_point_t = std::chrono::system_clock::time_point;\n-        using time_point_rep_t = time_point_t::rep;\n-        using time_point_urep_t = std::make_unsigned_t<time_point_rep_t>;\n-\n-        time_point_t expiresAt() const;\n-        void setExpiresAt(const time_point_t & t);\n-\n-        bool isDefault() const;\n-        void setDefault();\n-\n-        /// Stores both expiration time and `is_default` flag in the most significant bit\n-        time_point_urep_t data = 0;\n-    };\n-\n-    using Offset = size_t;\n-    using Offsets = std::vector<Offset>;\n-\n-\n-    SSDComplexKeyCachePartition(\n-            const AttributeUnderlyingType & key_structure,\n-            const std::vector<AttributeUnderlyingType> & attributes_structure,\n-            const std::string & dir_path,\n-            const size_t file_id,\n-            const size_t max_size,\n-            const size_t block_size,\n-            const size_t read_buffer_size,\n-            const size_t write_buffer_size,\n-            const size_t max_stored_keys);\n-\n-    ~SSDComplexKeyCachePartition();\n-\n-    template <typename T>\n-    using ResultArrayType = std::conditional_t<IsDecimalNumber<T>, DecimalPaddedPODArray<T>, PaddedPODArray<T>>;\n-\n-    template <typename Out, typename GetDefault>\n-    void getValue(const size_t attribute_index,\n-            const Columns & key_columns, const DataTypes & key_types,\n-            ResultArrayType<Out> & out, std::vector<bool> & found, GetDefault & default_value_extractor,\n-            std::chrono::system_clock::time_point now) const;\n-\n-    void getString(const size_t attribute_index,\n-            const Columns & key_columns, const DataTypes & key_types,\n-            StringRefs & refs, ArenaWithFreeLists & arena, std::vector<bool> & found,\n-            std::vector<size_t> & default_ids, std::chrono::system_clock::time_point now) const;\n-\n-    void hasKeys(const Columns & key_columns, const DataTypes & key_types,\n-            ResultArrayType<UInt8> & out, std::vector<bool> & found,\n-            std::chrono::system_clock::time_point now) const;\n-\n-    struct Attribute\n-    {\n-        template <typename T>\n-        using Container = std::vector<T>;\n-\n-        AttributeUnderlyingType type;\n-        std::variant<\n-                Container<UInt8>,\n-                Container<UInt16>,\n-                Container<UInt32>,\n-                Container<UInt64>,\n-                Container<UInt128>,\n-                Container<Int8>,\n-                Container<Int16>,\n-                Container<Int32>,\n-                Container<Int64>,\n-                Container<Decimal32>,\n-                Container<Decimal64>,\n-                Container<Decimal128>,\n-                Container<Float32>,\n-                Container<Float64>,\n-                Container<String>> values;\n-    };\n-    using Attributes = std::vector<Attribute>;\n-\n-    size_t appendBlock(\n-        const Columns & key_columns,\n-        const DataTypes & key_types,\n-        const Attributes & new_attributes,\n-        const PaddedPODArray<Metadata> & metadata,\n-        const size_t begin);\n-\n-    size_t appendDefaults(\n-        const KeyRefs & keys,\n-        const PaddedPODArray<Metadata> & metadata,\n-        const size_t begin);\n-\n-    void clearOldestBlocks();\n-\n-    void flush();\n-\n-    void remove();\n-\n-    size_t getId() const;\n-\n-    double getLoadFactor() const;\n-\n-    size_t getElementCount() const;\n-\n-    size_t getBytesAllocated() const;\n-\n-private:\n-    size_t append(\n-        const KeyRefs & keys,\n-        const Attributes & new_attributes,\n-        const PaddedPODArray<Metadata> & metadata,\n-        const size_t begin);\n-\n-    template <typename SetFunc>\n-    void getImpl(const Columns & key_columns, const DataTypes & key_types,\n-        SetFunc & set, std::vector<bool> & found) const;\n-\n-    template <typename SetFunc>\n-    void getValueFromMemory(const PaddedPODArray<Index> & indices, SetFunc & set) const;\n-\n-    template <typename SetFunc>\n-    void getValueFromStorage(const PaddedPODArray<Index> & indices, SetFunc & set) const;\n-\n-    void ignoreFromBufferToAttributeIndex(const size_t attribute_index, ReadBuffer & buf) const;\n-\n-    const size_t file_id;\n-    const size_t max_size;\n-    const size_t block_size;\n-    const size_t read_buffer_size;\n-    const size_t write_buffer_size;\n-    const size_t max_stored_keys;\n-    const std::string path;\n-\n-    mutable std::shared_mutex rw_lock;\n-\n-    int fd = -1;\n-\n-    ComplexKeysPool keys_pool;\n-    mutable BucketCacheIndex<KeyRef, Index, std::hash<KeyRef>, KeyDeleter> key_to_index;\n-\n-    std::optional<TemporalComplexKeysPool> keys_buffer_pool;\n-    KeyRefs keys_buffer;\n-\n-    const std::vector<AttributeUnderlyingType> attributes_structure;\n-\n-    std::optional<Memory<>> memory;\n-    std::optional<WriteBuffer> write_buffer;\n-    uint32_t keys_in_block = 0;\n-\n-    size_t current_memory_block_id = 0;\n-    size_t current_file_block_id = 0;\n-};\n-\n-using SSDComplexKeyCachePartitionPtr = std::shared_ptr<SSDComplexKeyCachePartition>;\n-\n-\n-/** Class for managing SSDCachePartition and getting data from source.\n-  */\n-class SSDComplexKeyCacheStorage\n-{\n-public:\n-    using AttributeTypes = std::vector<AttributeUnderlyingType>;\n-\n-    SSDComplexKeyCacheStorage(\n-            const AttributeTypes & attributes_structure,\n-            const std::string & path,\n-            const size_t max_partitions_count,\n-            const size_t file_size,\n-            const size_t block_size,\n-            const size_t read_buffer_size,\n-            const size_t write_buffer_size,\n-            const size_t max_stored_keys);\n-\n-    ~SSDComplexKeyCacheStorage();\n-\n-    template <typename T>\n-    using ResultArrayType = SSDComplexKeyCachePartition::ResultArrayType<T>;\n-\n-    template <typename Out, typename GetDefault>\n-    void getValue(const size_t attribute_index, const Columns & key_columns, const DataTypes & key_types,\n-            ResultArrayType<Out> & out, std::unordered_map<KeyRef, std::vector<size_t>> & not_found,\n-            TemporalComplexKeysPool & not_found_pool,\n-            GetDefault & get_default, std::chrono::system_clock::time_point now) const;\n-\n-    void getString(const size_t attribute_index, const Columns & key_columns, const DataTypes & key_types,\n-            StringRefs & refs, ArenaWithFreeLists & arena, std::unordered_map<KeyRef, std::vector<size_t>> & not_found,\n-            TemporalComplexKeysPool & not_found_pool,\n-            std::vector<size_t> & default_ids, std::chrono::system_clock::time_point now) const;\n-\n-    void hasKeys(const Columns & key_columns, const DataTypes & key_types, ResultArrayType<UInt8> & out,\n-            std::unordered_map<KeyRef, std::vector<size_t>> & not_found,\n-            TemporalComplexKeysPool & not_found_pool, std::chrono::system_clock::time_point now) const;\n-\n-    template <typename PresentIdHandler, typename AbsentIdHandler>\n-    void update(DictionarySourcePtr & source_ptr,\n-            const Columns & key_columns, const DataTypes & key_types,\n-            const KeyRefs & required_keys, const std::vector<size_t> & required_rows,\n-            TemporalComplexKeysPool & tmp_keys_pool,\n-            PresentIdHandler && on_updated, AbsentIdHandler && on_key_not_found,\n-            const DictionaryLifetime lifetime);\n-\n-    std::exception_ptr getLastException() const { return last_update_exception; }\n-\n-    const std::string & getPath() const { return path; }\n-\n-    size_t getQueryCount() const { return query_count.load(std::memory_order_relaxed); }\n-\n-    size_t getHitCount() const { return hit_count.load(std::memory_order_acquire); }\n-\n-    size_t getElementCount() const;\n-\n-    double getLoadFactor() const;\n-\n-private:\n-    void collectGarbage();\n-\n-    const AttributeTypes attributes_structure;\n-\n-    const std::string path;\n-    const size_t max_partitions_count;\n-    const size_t file_size;\n-    const size_t block_size;\n-    const size_t read_buffer_size;\n-    const size_t write_buffer_size;\n-    const size_t max_stored_keys;\n-\n-    mutable std::shared_mutex rw_lock;\n-    std::list<SSDComplexKeyCachePartitionPtr> partitions;\n-    std::list<SSDComplexKeyCachePartitionPtr> partition_delete_queue;\n-\n-    Poco::Logger * const log;\n-\n-    mutable pcg64 rnd_engine;\n-\n-    mutable std::exception_ptr last_update_exception;\n-    mutable size_t update_error_count = 0;\n-    mutable std::chrono::system_clock::time_point backoff_end_time;\n-\n-    mutable std::atomic<size_t> hit_count{0};\n-    mutable std::atomic<size_t> query_count{0};\n-};\n-\n-\n-/** Dictionary interface\n-  */\n-class SSDComplexKeyCacheDictionary final : public IDictionaryBase\n-{\n-public:\n-    SSDComplexKeyCacheDictionary(\n-            const StorageID & dict_id_,\n-            const DictionaryStructure & dict_struct_,\n-            DictionarySourcePtr source_ptr_,\n-            const DictionaryLifetime dict_lifetime_,\n-            const std::string & path,\n-            const size_t max_partitions_count_,\n-            const size_t file_size_,\n-            const size_t block_size_,\n-            const size_t read_buffer_size_,\n-            const size_t write_buffer_size_,\n-            const size_t max_stored_keys_);\n-\n-    std::string getKeyDescription() const { return dict_struct.getKeyDescription(); }\n-\n-    std::string getTypeName() const override { return \"SSDComplexKeyCache\"; }\n-\n-    size_t getBytesAllocated() const override { return 0; } // TODO: ?\n-\n-    size_t getQueryCount() const override { return storage.getQueryCount(); }\n-\n-    double getHitRate() const override\n-    {\n-        return static_cast<double>(storage.getHitCount()) / storage.getQueryCount();\n-    }\n-\n-    size_t getElementCount() const override { return storage.getElementCount(); }\n-\n-    double getLoadFactor() const override { return storage.getLoadFactor(); }\n-\n-    bool supportUpdates() const override { return false; }\n-\n-    std::shared_ptr<const IExternalLoadable> clone() const override\n-    {\n-        return std::make_shared<SSDComplexKeyCacheDictionary>(getDictionaryID(), dict_struct, source_ptr->clone(), dict_lifetime, path,\n-                max_partitions_count, file_size, block_size, read_buffer_size, write_buffer_size, max_stored_keys);\n-    }\n-\n-    const IDictionarySource * getSource() const override { return source_ptr.get(); }\n-\n-    const DictionaryLifetime & getLifetime() const override { return dict_lifetime; }\n-\n-    const DictionaryStructure & getStructure() const override { return dict_struct; }\n-\n-    bool isInjective(const std::string & attribute_name) const override\n-    {\n-        return dict_struct.attributes[getAttributeIndex(attribute_name)].injective;\n-    }\n-\n-    std::exception_ptr getLastException() const override { return storage.getLastException(); }\n-\n-    DictionaryKeyType getKeyType() const override { return DictionaryKeyType::complex; }\n-\n-    ColumnPtr getColumn(\n-        const std::string& attribute_name,\n-        const DataTypePtr & result_type,\n-        const Columns & key_columns,\n-        const DataTypes & key_types,\n-        const ColumnPtr default_values_column) const override;\n-\n-    ColumnUInt8::Ptr hasKeys(const Columns & key_columns, const DataTypes & key_types) const override;\n-\n-    template <typename T>\n-    using ResultArrayType = SSDComplexKeyCacheStorage::ResultArrayType<T>;\n-\n-    BlockInputStreamPtr getBlockInputStream(const Names & column_names, size_t max_block_size) const override;\n-\n-private:\n-    size_t getAttributeIndex(const std::string & attr_name) const;\n-\n-    template <typename T>\n-    AttributeValueVariant createAttributeNullValueWithTypeImpl(const Field & null_value);\n-    AttributeValueVariant createAttributeNullValueWithType(const AttributeUnderlyingType type, const Field & null_value);\n-    void createAttributes();\n-\n-    template <typename AttributeType, typename OutputType, typename DefaultValueExtractor>\n-    void getItemsNumberImpl(\n-        const size_t attribute_index,\n-        const Columns & key_columns,\n-        const DataTypes & key_types,\n-        ResultArrayType<OutputType> & out,\n-        DefaultValueExtractor & default_value_extractor) const;\n-\n-    void getItemsStringImpl(\n-        const size_t attribute_index,\n-        const Columns & key_columns,\n-        const DataTypes & key_types,\n-        ColumnString * out,\n-        DictionaryDefaultValueExtractor<String> & default_value_extractor) const;\n-\n-    const std::string name;\n-    const DictionaryStructure dict_struct;\n-    mutable DictionarySourcePtr source_ptr;\n-    const DictionaryLifetime dict_lifetime;\n-\n-    const std::string path;\n-    const size_t max_partitions_count;\n-    const size_t file_size;\n-    const size_t block_size;\n-    const size_t read_buffer_size;\n-    const size_t write_buffer_size;\n-    const size_t max_stored_keys;\n-\n-    std::map<std::string, size_t> attribute_index_by_name;\n-    std::vector<AttributeValueVariant> null_values;\n-    mutable SSDComplexKeyCacheStorage storage;\n-    Poco::Logger * const log;\n-\n-    mutable size_t bytes_allocated = 0;\n-};\n-\n-}\n-\n-#endif\ndiff --git a/src/Dictionaries/benchmark b/src/Dictionaries/benchmark\nnew file mode 100644\nindex 000000000000..37d0d92ac14e\n--- /dev/null\n+++ b/src/Dictionaries/benchmark\n@@ -0,0 +1,154 @@\n+clickhouse-client --query=\"DROP TABLE IF EXISTS simple_cache_dictionary_table_source\";\n+clickhouse-client --query=\"CREATE TABLE simple_cache_dictionary_table_source (id UInt64, value1 String, value2 UInt64, value3 String, value4 Float64, value5 Decimal64(4)) ENGINE=TinyLog;\"\n+clickhouse-client --query=\"INSERT INTO simple_cache_dictionary_table_source SELECT number, concat('Value1 ', toString(number)), number, concat('Value3 ', toString(number)), toFloat64(number), cast(number, 'Decimal64(4)') FROM system.numbers LIMIT 1000000;\"\n+\n+clickhouse-client --multiquery --query=\"CREATE DICTIONARY clickhouse_simple_cache_dictionary (\n+    id UInt64,\n+    value1 String,\n+    value2 UInt64,\n+    value3 String,\n+    value4 Float64,\n+    value5 Decimal64(4)\n+)\n+PRIMARY KEY id\n+SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'simple_cache_dictionary_table_source' PASSWORD '' DB 'default'))\n+LIFETIME(MIN 300 MAX 300)\n+LAYOUT(CACHE(SIZE_IN_CELLS 100000));\"\n+\n+clickhouse-client --multiquery --query=\"CREATE DICTIONARY clickhouse_ssd_simple_cache_dictionary (\n+    id UInt64,\n+    value1 String,\n+    value2 UInt64,\n+    value3 String,\n+    value4 Float64,\n+    value5 Decimal64(4)\n+)\n+PRIMARY KEY id\n+SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'simple_cache_dictionary_table_source' PASSWORD '' DB 'default'))\n+LIFETIME(MIN 300 MAX 300)\n+LAYOUT(SSD_CACHE(BLOCK_SIZE 4096 FILE_SIZE 16777216 READ_BUFFER_SIZE 1048576 WRITE_BUFFER_SIZE 327680 MAX_STORED_KEYS 1048576 PATH '/opt/mkita/ClickHouse/build_release/programs/ssd_cache'));\"\n+\n+clickhouse-client --multiquery --query=\"CREATE DICTIONARY clickhouse_dummy_simple_cache_dictionary (\n+    id UInt64,\n+    value1 String,\n+    value2 UInt64,\n+    value3 String,\n+    value4 Float64,\n+    value5 Decimal64(4)\n+)\n+PRIMARY KEY id\n+SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'simple_cache_dictionary_table_source' PASSWORD '' DB 'default'))\n+LIFETIME(MIN 300 MAX 300)\n+LAYOUT(DUMMY_SIMPLE());\"\n+\n+./clickhouse-benchmark --query=\"SELECT\n+    dictGet('default.clickhouse_dummy_simple_cache_dictionary', 'value1', number),\n+    dictGet('default.clickhouse_dummy_simple_cache_dictionary', 'value2', number),\n+    dictGet('default.clickhouse_dummy_simple_cache_dictionary', 'value3', number),\n+    dictGet('default.clickhouse_dummy_simple_cache_dictionary', 'value4', number),\n+    dictGet('default.clickhouse_dummy_simple_cache_dictionary', 'value5', number)\n+FROM system.numbers\n+LIMIT 10000\n+FORMAT Null\"\n+\n+./clickhouse-benchmark --query=\"SELECT\n+    dictGet('default.clickhouse_simple_cache_dictionary', ('value1', 'value2', 'value3', 'value4', 'value5'), number)\n+FROM system.numbers\n+LIMIT 10000\n+FORMAT Null\"\n+\n+./clickhouse-benchmark --query=\"SELECT dictGet('default.clickhouse_ssd_simple_cache_dictionary', 'value1', number) FROM system.numbers_mt LIMIT 10000 FORMAT Null\"\n+\n+./clickhouse-benchmark --query=\"SELECT\n+    dictGet('default.clickhouse_simple_cache_dictionary', 'value1', number),\n+    dictGet('default.clickhouse_simple_cache_dictionary', 'value2', number),\n+    dictGet('default.clickhouse_simple_cache_dictionary', 'value3', number),\n+    dictGet('default.clickhouse_simple_cache_dictionary', 'value4', number),\n+    dictGet('default.clickhouse_simple_cache_dictionary', 'value5', number)\n+FROM system.numbers\n+LIMIT 10000\n+FORMAT Null\"\n+\n+./clickhouse-benchmark --query=\"SELECT dictGet('default.clickhouse_ssd_simple_cache_dictionary', 'value1', number) FROM system.numbers_mt LIMIT 10000 FORMAT Null\"\n+\n+SELECT\n+    dictGet('default.clickhouse_ssd_simple_cache_dictionary', 'value1', number),\n+    dictGet('default.clickhouse_ssd_simple_cache_dictionary', 'value2', number),\n+    dictGet('default.clickhouse_ssd_simple_cache_dictionary', 'value3', number),\n+    dictGet('default.clickhouse_ssd_simple_cache_dictionary', 'value4', number),\n+    dictGet('default.clickhouse_ssd_simple_cache_dictionary', 'value5', number)\n+FROM system.numbers\n+    LIMIT 10000\n+FORMAT Null\n+\n+SELECT dictGet('default.clickhouse_simple_cache_dictionary', ('value1', 'value2', 'value3', 'value4', 'value5'), number) FROM system.numbers LIMIT 10000 FORMAT Null\n+\n+SELECT dictGet('default.clickhouse_ssd_simple_cache_dictionary', ('value1', 'value2', 'value3', 'value4', 'value5'), number) FROM system.numbers LIMIT 10000\n+FORMAT Null\n+\n+SELECT\n+    dictGet('default.clickhouse_simple_cache_dictionary', ('value1', 'value2', 'value3', 'value4', 'value5'), number)\n+FROM system.numbers\n+    LIMIT 10000\n+FORMAT\n+    Null\n+\n+SELECT\n+    dictGet('default.clickhouse_simple_cache_dictionary', 'value1', number),\n+    dictGet('default.clickhouse_simple_cache_dictionary', 'value2', number),\n+    dictGet('default.clickhouse_simple_cache_dictionary', 'value3', number),\n+    dictGet('default.clickhouse_simple_cache_dictionary', 'value4', number),\n+    dictGet('default.clickhouse_simple_cache_dictionary', 'value5', number)\n+FROM system.numbers\n+    LIMIT 10000\n+FORMAT\n+    Null\n+\n+SELECT\n+    dictGet('default.clickhouse_simple_cache_dictionary', 'value1', number),\n+    dictGet('default.clickhouse_simple_cache_dictionary', 'value2', number)\n+FROM system.numbers\n+LIMIT 10000\n+FORMAT Null\n+\n+SELECT\n+    dictGet('clickhouse_simple_cache_dictionary', 'value1', number)\n+FROM system.numbers\n+LIMIT 100000\n+FORMAT Null\n+\n+SELECT\n+    dictGet('clickhouse_simple_cache_dictionary', 'value2', number)\n+FROM system.numbers\n+LIMIT 100000\n+FORMAT Null\n+\n+SELECT\n+    dictGet('clickhouse_simple_cache_dictionary', 'value3', number)\n+FROM system.numbers\n+LIMIT 100000\n+FORMAT Null\n+\n+SELECT\n+    dictGet('clickhouse_simple_cache_dictionary', 'value4', number)\n+FROM system.numbers\n+LIMIT 100000\n+FORMAT Null\n+\n+SELECT\n+    dictGet('clickhouse_simple_cache_dictionary', 'value5', number)\n+FROM system.numbers\n+LIMIT 100000\n+FORMAT Null\n+\n+SELECT\n+    dictGet('clickhouse_simple_cache_dictionary', 'value1', number),\n+    dictGet('clickhouse_simple_cache_dictionary', 'value2', number),\n+    dictGet('clickhouse_simple_cache_dictionary', 'value3', number),\n+    dictGet('clickhouse_simple_cache_dictionary', 'value4', number),\n+    dictGet('clickhouse_simple_cache_dictionary', 'value5', number)\n+FROM system.numbers\n+LIMIT 100000\n+FORMAT Null\n+\n+SELECT * FROM clickhouse_simple_cache_dictionary_table;\n\\ No newline at end of file\ndiff --git a/src/Dictionaries/registerCacheDictionaries.cpp b/src/Dictionaries/registerCacheDictionaries.cpp\nnew file mode 100644\nindex 000000000000..92e6eb97b63f\n--- /dev/null\n+++ b/src/Dictionaries/registerCacheDictionaries.cpp\n@@ -0,0 +1,309 @@\n+#include \"CacheDictionary.h\"\n+#include \"SSDCacheDictionaryStorage.h\"\n+#include \"CacheDictionaryStorage.h\"\n+#include <Dictionaries/DictionaryFactory.h>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int TOO_SMALL_BUFFER_SIZE;\n+    extern const int UNSUPPORTED_METHOD;\n+    extern const int BAD_ARGUMENTS;\n+}\n+\n+CacheDictionaryStorageConfiguration parseCacheStorageConfiguration(\n+    const String & full_name,\n+    const Poco::Util::AbstractConfiguration & config,\n+    const String & layout_prefix,\n+    const DictionaryLifetime & dict_lifetime,\n+    DictionaryKeyType dictionary_key_type)\n+{\n+    String dictionary_type_prefix = dictionary_key_type == DictionaryKeyType::complex ? \".complex_key_cache.\" : \".cache.\";\n+    String dictionary_configuration_prefix = layout_prefix + dictionary_type_prefix;\n+\n+    const size_t size = config.getUInt64(dictionary_configuration_prefix + \"size_in_cells\");\n+    if (size == 0)\n+        throw Exception(ErrorCodes::TOO_SMALL_BUFFER_SIZE,\n+            \"({}: cache dictionary cannot have 0 cells\",\n+            full_name);\n+\n+    size_t dict_lifetime_seconds = static_cast<size_t>(dict_lifetime.max_sec);\n+    const size_t strict_max_lifetime_seconds = config.getUInt64(dictionary_configuration_prefix + \"strict_max_lifetime_seconds\", dict_lifetime_seconds);\n+\n+    size_t rounded_size = roundUpToPowerOfTwoOrZero(size);\n+\n+    CacheDictionaryStorageConfiguration storage_configuration{rounded_size, strict_max_lifetime_seconds, dict_lifetime};\n+\n+    return storage_configuration;\n+}\n+\n+#if defined(OS_LINUX) || defined(__FreeBSD__)\n+\n+SSDCacheDictionaryStorageConfiguration parseSSDCacheStorageConfiguration(\n+    const String & full_name,\n+    const Poco::Util::AbstractConfiguration & config,\n+    const String & layout_prefix,\n+    const DictionaryLifetime & dict_lifetime,\n+    DictionaryKeyType dictionary_key_type)\n+{\n+    String dictionary_type_prefix = dictionary_key_type == DictionaryKeyType::complex ? \".complex_key_ssd_cache.\" : \".ssd_cache.\";\n+    String dictionary_configuration_prefix = layout_prefix + dictionary_type_prefix;\n+\n+    const size_t strict_max_lifetime_seconds\n+        = config.getUInt64(dictionary_configuration_prefix + \"strict_max_lifetime_seconds\", static_cast<size_t>(dict_lifetime.max_sec));\n+\n+    static constexpr size_t DEFAULT_SSD_BLOCK_SIZE_BYTES = DEFAULT_AIO_FILE_BLOCK_SIZE;\n+    static constexpr size_t DEFAULT_FILE_SIZE_BYTES = 4 * 1024 * 1024 * 1024ULL;\n+    static constexpr size_t DEFAULT_READ_BUFFER_SIZE_BYTES = 16 * DEFAULT_SSD_BLOCK_SIZE_BYTES;\n+    static constexpr size_t DEFAULT_WRITE_BUFFER_SIZE_BYTES = DEFAULT_SSD_BLOCK_SIZE_BYTES;\n+\n+    static constexpr size_t DEFAULT_MAX_STORED_KEYS = 100000;\n+    static constexpr size_t DEFAULT_PARTITIONS_COUNT = 16;\n+\n+    const size_t max_partitions_count\n+        = config.getInt64(dictionary_configuration_prefix + \"ssd_cache.max_partitions_count\", DEFAULT_PARTITIONS_COUNT);\n+\n+    const size_t block_size = config.getInt64(dictionary_configuration_prefix + \"block_size\", DEFAULT_SSD_BLOCK_SIZE_BYTES);\n+    const size_t file_size = config.getInt64(dictionary_configuration_prefix + \"file_size\", DEFAULT_FILE_SIZE_BYTES);\n+    if (file_size % block_size != 0)\n+        throw Exception(ErrorCodes::BAD_ARGUMENTS,\n+            \"({}): file_size must be a multiple of block_size\",\n+            full_name);\n+\n+    const size_t read_buffer_size = config.getInt64(dictionary_configuration_prefix + \"read_buffer_size\", DEFAULT_READ_BUFFER_SIZE_BYTES);\n+    if (read_buffer_size % block_size != 0)\n+        throw Exception(ErrorCodes::BAD_ARGUMENTS,\n+            \"({}): read_buffer_size must be a multiple of block_size\",\n+            full_name);\n+\n+    const size_t write_buffer_size\n+        = config.getInt64(dictionary_configuration_prefix + \"write_buffer_size\", DEFAULT_WRITE_BUFFER_SIZE_BYTES);\n+    if (write_buffer_size % block_size != 0)\n+        throw Exception(ErrorCodes::BAD_ARGUMENTS,\n+            \"({}): write_buffer_size must be a multiple of block_size\",\n+            full_name);\n+\n+    auto directory_path = config.getString(dictionary_configuration_prefix + \"path\");\n+    if (directory_path.empty())\n+        throw Exception(ErrorCodes::BAD_ARGUMENTS,\n+            \"({}): ssd cache dictionary cannot have empty path\",\n+            full_name);\n+\n+    if (directory_path.at(0) != '/')\n+        directory_path = std::filesystem::path{config.getString(\"path\")}.concat(directory_path).string();\n+\n+    const size_t max_stored_keys_in_partition\n+        = config.getInt64(dictionary_configuration_prefix + \"max_stored_keys\", DEFAULT_MAX_STORED_KEYS);\n+    const size_t rounded_size = roundUpToPowerOfTwoOrZero(max_stored_keys_in_partition);\n+\n+    SSDCacheDictionaryStorageConfiguration configuration{\n+        strict_max_lifetime_seconds,\n+        dict_lifetime,\n+        directory_path,\n+        max_partitions_count,\n+        rounded_size,\n+        block_size,\n+        file_size / block_size,\n+        read_buffer_size / block_size,\n+        write_buffer_size / block_size};\n+\n+    return configuration;\n+}\n+\n+#endif\n+\n+CacheDictionaryUpdateQueueConfiguration parseCacheDictionaryUpdateQueueConfiguration(\n+    const String & full_name,\n+    const Poco::Util::AbstractConfiguration & config,\n+    const String & layout_prefix,\n+    DictionaryKeyType key_type)\n+{\n+    String layout_type = key_type == DictionaryKeyType::complex ? \"complex_key_cache\" : \"cache\";\n+\n+    const size_t max_update_queue_size = config.getUInt64(layout_prefix + \".cache.max_update_queue_size\", 100000);\n+    if (max_update_queue_size == 0)\n+        throw Exception(ErrorCodes::TOO_SMALL_BUFFER_SIZE,\n+            \"({}): dictionary of layout '({})' cannot have empty update queue of size 0\",\n+            full_name,\n+            layout_type);\n+\n+    const size_t update_queue_push_timeout_milliseconds\n+        = config.getUInt64(layout_prefix + \".cache.update_queue_push_timeout_milliseconds\", 10);\n+    if (update_queue_push_timeout_milliseconds < 10)\n+        throw Exception(ErrorCodes::BAD_ARGUMENTS,\n+            \"({}): dictionary of layout '({})' have too little update_queue_push_timeout\",\n+            full_name,\n+            layout_type);\n+\n+    const size_t query_wait_timeout_milliseconds = config.getUInt64(layout_prefix + \".cache.query_wait_timeout_milliseconds\", 60000);\n+\n+    const size_t max_threads_for_updates = config.getUInt64(layout_prefix + \".max_threads_for_updates\", 4);\n+    if (max_threads_for_updates == 0)\n+        throw Exception(ErrorCodes::BAD_ARGUMENTS,\n+            \"({}): dictionary of layout) '({})' cannot have zero threads for updates\",\n+            full_name,\n+            layout_type);\n+\n+    CacheDictionaryUpdateQueueConfiguration update_queue_configuration{\n+        max_update_queue_size, max_threads_for_updates, update_queue_push_timeout_milliseconds, query_wait_timeout_milliseconds};\n+\n+    return update_queue_configuration;\n+}\n+\n+template <DictionaryKeyType dictionary_key_type>\n+DictionaryPtr createCacheDictionaryLayout(\n+    const String & full_name,\n+    const DictionaryStructure & dict_struct,\n+    const Poco::Util::AbstractConfiguration & config,\n+    const std::string & config_prefix,\n+    DictionarySourcePtr source_ptr)\n+{\n+    static_assert(dictionary_key_type != DictionaryKeyType::range, \"Range key type is not supported by CacheDictionary\");\n+\n+    if constexpr (dictionary_key_type == DictionaryKeyType::simple)\n+    {\n+        if (dict_struct.key)\n+            throw Exception(ErrorCodes::UNSUPPORTED_METHOD, \"'key' is not supported for dictionary of layout 'cache'\");\n+    }\n+    else if constexpr (dictionary_key_type == DictionaryKeyType::complex)\n+    {\n+        if (dict_struct.id)\n+            throw Exception(ErrorCodes::UNSUPPORTED_METHOD, \"'id' is not supported for dictionary of layout 'complex_key_cache'\");\n+    }\n+\n+    if (dict_struct.range_min || dict_struct.range_max)\n+        throw Exception(ErrorCodes::BAD_ARGUMENTS,\n+            \"({}): elements .structure.range_min and .structure.range_max should be defined only \"\n+                  \"for a dictionary of layout 'range_hashed'\",\n+            full_name);\n+\n+    const bool require_nonempty = config.getBool(config_prefix + \".require_nonempty\", false);\n+    if (require_nonempty)\n+        throw Exception(ErrorCodes::BAD_ARGUMENTS,\n+            \"({}): cache dictionary of layout cannot have 'require_nonempty' attribute set\",\n+            full_name);\n+\n+    const auto & layout_prefix = config_prefix + \".layout\";\n+\n+    const auto dict_id = StorageID::fromDictionaryConfig(config, config_prefix);\n+\n+    const DictionaryLifetime dict_lifetime{config, config_prefix + \".lifetime\"};\n+\n+    const bool allow_read_expired_keys = config.getBool(layout_prefix + \".cache.allow_read_expired_keys\", false);\n+\n+    auto storage_configuration = parseCacheStorageConfiguration(full_name, config, layout_prefix, dict_lifetime, dictionary_key_type);\n+    auto storage = std::make_shared<CacheDictionaryStorage<dictionary_key_type>>(storage_configuration);\n+\n+    auto update_queue_configuration = parseCacheDictionaryUpdateQueueConfiguration(full_name, config, layout_prefix, dictionary_key_type);\n+\n+    return std::make_unique<CacheDictionary<dictionary_key_type>>(\n+        dict_id, dict_struct, std::move(source_ptr), storage, update_queue_configuration, dict_lifetime, allow_read_expired_keys);\n+}\n+\n+#if defined(OS_LINUX) || defined(__FreeBSD__)\n+\n+template <DictionaryKeyType dictionary_key_type>\n+DictionaryPtr createSSDCacheDictionaryLayout(\n+    const String & full_name,\n+    const DictionaryStructure & dict_struct,\n+    const Poco::Util::AbstractConfiguration & config,\n+    const std::string & config_prefix,\n+    DictionarySourcePtr source_ptr)\n+{\n+    static_assert(dictionary_key_type != DictionaryKeyType::range, \"Range key type is not supported by CacheDictionary\");\n+\n+    if constexpr (dictionary_key_type == DictionaryKeyType::simple)\n+    {\n+        if (dict_struct.key)\n+            throw Exception(ErrorCodes::UNSUPPORTED_METHOD, \"'key' is not supported for dictionary of layout 'ssd_cache'\");\n+    }\n+    else if constexpr (dictionary_key_type == DictionaryKeyType::complex)\n+    {\n+        if (dict_struct.id)\n+            throw Exception(ErrorCodes::UNSUPPORTED_METHOD, \"'id' is not supported for dictionary of layout 'complex_key_ssd_cache'\");\n+    }\n+\n+    if (dict_struct.range_min || dict_struct.range_max)\n+        throw Exception(ErrorCodes::BAD_ARGUMENTS,\n+            \"({}): elements .structure.range_min and .structure.range_max should be defined only \"\n+                  \"for a dictionary of layout 'range_hashed'\",\n+            full_name);\n+\n+    const bool require_nonempty = config.getBool(config_prefix + \".require_nonempty\", false);\n+    if (require_nonempty)\n+        throw Exception(ErrorCodes::BAD_ARGUMENTS,\n+            \"({}): cache dictionary of layout cannot have 'require_nonempty' attribute set\",\n+            full_name);\n+\n+    const auto & layout_prefix = config_prefix + \".layout\";\n+\n+    const auto dict_id = StorageID::fromDictionaryConfig(config, config_prefix);\n+\n+    const DictionaryLifetime dict_lifetime{config, config_prefix + \".lifetime\"};\n+\n+    const bool allow_read_expired_keys = config.getBool(layout_prefix + \".cache.allow_read_expired_keys\", false);\n+\n+    auto storage_configuration = parseSSDCacheStorageConfiguration(full_name, config, layout_prefix, dict_lifetime, dictionary_key_type);\n+    auto storage = std::make_shared<SSDCacheDictionaryStorage<dictionary_key_type>>(storage_configuration);\n+\n+    auto update_queue_configuration\n+        = parseCacheDictionaryUpdateQueueConfiguration(full_name, config, layout_prefix, dictionary_key_type);\n+\n+    return std::make_unique<CacheDictionary<dictionary_key_type>>(\n+        dict_id, dict_struct, std::move(source_ptr), storage, update_queue_configuration, dict_lifetime, allow_read_expired_keys);\n+}\n+\n+#endif\n+\n+void registerDictionaryCache(DictionaryFactory & factory)\n+{\n+    auto create_simple_cache_layout = [=](const String & full_name,\n+                                          const DictionaryStructure & dict_struct,\n+                                          const Poco::Util::AbstractConfiguration & config,\n+                                          const std::string & config_prefix,\n+                                          DictionarySourcePtr source_ptr) -> DictionaryPtr\n+    {\n+        return createCacheDictionaryLayout<DictionaryKeyType::simple>(full_name, dict_struct, config, config_prefix, std::move(source_ptr));\n+    };\n+\n+    factory.registerLayout(\"cache\", create_simple_cache_layout, false);\n+\n+    auto create_complex_key_cache_layout = [=](const std::string & full_name,\n+                                               const DictionaryStructure & dict_struct,\n+                                               const Poco::Util::AbstractConfiguration & config,\n+                                               const std::string & config_prefix,\n+                                               DictionarySourcePtr source_ptr) -> DictionaryPtr\n+    {\n+        return createCacheDictionaryLayout<DictionaryKeyType::complex>(full_name, dict_struct, config, config_prefix, std::move(source_ptr));\n+    };\n+\n+    factory.registerLayout(\"complex_key_cache\", create_complex_key_cache_layout, true);\n+\n+#if defined(OS_LINUX) || defined(__FreeBSD__)\n+\n+    auto create_simple_ssd_cache_layout = [=](const std::string & full_name,\n+                                              const DictionaryStructure & dict_struct,\n+                                              const Poco::Util::AbstractConfiguration & config,\n+                                              const std::string & config_prefix,\n+                                              DictionarySourcePtr source_ptr) -> DictionaryPtr\n+    {\n+        return createSSDCacheDictionaryLayout<DictionaryKeyType::simple>(full_name, dict_struct, config, config_prefix, std::move(source_ptr));\n+    };\n+\n+    factory.registerLayout(\"ssd_cache\", create_simple_ssd_cache_layout, false);\n+\n+    auto create_complex_key_ssd_cache_layout = [=](const std::string & full_name,\n+                                                   const DictionaryStructure & dict_struct,\n+                                                   const Poco::Util::AbstractConfiguration & config,\n+                                                   const std::string & config_prefix,\n+                                                   DictionarySourcePtr source_ptr) -> DictionaryPtr {\n+        return createSSDCacheDictionaryLayout<DictionaryKeyType::complex>(full_name, dict_struct, config, config_prefix, std::move(source_ptr));\n+    };\n+\n+    factory.registerLayout(\"complex_key_ssd_cache\", create_complex_key_ssd_cache_layout, true);\n+#endif\n+}\n+\n+}\ndiff --git a/src/Dictionaries/registerDictionaries.cpp b/src/Dictionaries/registerDictionaries.cpp\nindex abcc0ce06ad5..b0bf61a74ef2 100644\n--- a/src/Dictionaries/registerDictionaries.cpp\n+++ b/src/Dictionaries/registerDictionaries.cpp\n@@ -24,16 +24,11 @@ void registerDictionarySourceLibrary(DictionarySourceFactory & source_factory);\n class DictionaryFactory;\n void registerDictionaryRangeHashed(DictionaryFactory & factory);\n void registerDictionaryComplexKeyHashed(DictionaryFactory & factory);\n-void registerDictionaryComplexKeyCache(DictionaryFactory & factory);\n void registerDictionaryComplexKeyDirect(DictionaryFactory & factory);\n void registerDictionaryTrie(DictionaryFactory & factory);\n void registerDictionaryFlat(DictionaryFactory & factory);\n void registerDictionaryHashed(DictionaryFactory & factory);\n void registerDictionaryCache(DictionaryFactory & factory);\n-#if defined(__linux__) || defined(__FreeBSD__)\n-void registerDictionarySSDCache(DictionaryFactory & factory);\n-void registerDictionarySSDComplexKeyCache(DictionaryFactory & factory);\n-#endif\n void registerDictionaryPolygon(DictionaryFactory & factory);\n void registerDictionaryDirect(DictionaryFactory & factory);\n \n@@ -62,16 +57,11 @@ void registerDictionaries()\n         auto & factory = DictionaryFactory::instance();\n         registerDictionaryRangeHashed(factory);\n         registerDictionaryComplexKeyHashed(factory);\n-        registerDictionaryComplexKeyCache(factory);\n         registerDictionaryComplexKeyDirect(factory);\n         registerDictionaryTrie(factory);\n         registerDictionaryFlat(factory);\n         registerDictionaryHashed(factory);\n         registerDictionaryCache(factory);\n-#if defined(OS_LINUX) || defined(__FreeBSD__)\n-        registerDictionarySSDCache(factory);\n-        registerDictionarySSDComplexKeyCache(factory);\n-#endif\n         registerDictionaryPolygon(factory);\n         registerDictionaryDirect(factory);\n     }\ndiff --git a/src/Dictionaries/ya.make b/src/Dictionaries/ya.make\nindex 4f33dc805591..9dbc1edbb09d 100644\n--- a/src/Dictionaries/ya.make\n+++ b/src/Dictionaries/ya.make\n@@ -9,6 +9,7 @@ PEERDIR(\n     contrib/libs/poco/MongoDB\n     contrib/libs/poco/Redis\n     contrib/libs/sparsehash\n+    contrib/restricted/abseil-cpp\n )\n \n IF (USE_ODBC)\n@@ -20,11 +21,11 @@ NO_COMPILER_WARNINGS()\n \n SRCS(\n     CacheDictionary.cpp\n+    CacheDictionaryUpdateQueue.cpp\n     CassandraBlockInputStream.cpp\n     CassandraDictionarySource.cpp\n     CassandraHelpers.cpp\n     ClickHouseDictionarySource.cpp\n-    ComplexKeyCacheDictionary.cpp\n     ComplexKeyDirectDictionary.cpp\n     ComplexKeyHashedDictionary.cpp\n     DictionaryBlockInputStreamBase.cpp\n@@ -58,11 +59,10 @@ SRCS(\n     RangeHashedDictionary.cpp\n     RedisBlockInputStream.cpp\n     RedisDictionarySource.cpp\n-    SSDCacheDictionary.cpp\n-    SSDComplexKeyCacheDictionary.cpp\n     XDBCDictionarySource.cpp\n     getDictionaryConfigurationFromAST.cpp\n     readInvalidateQuery.cpp\n+    registerCacheDictionaries.cpp\n     registerDictionaries.cpp\n     writeParenthesisedString.cpp\n \ndiff --git a/src/Dictionaries/ya.make.in b/src/Dictionaries/ya.make.in\nindex e52b106d0342..aa82fb21ba60 100644\n--- a/src/Dictionaries/ya.make.in\n+++ b/src/Dictionaries/ya.make.in\n@@ -8,6 +8,7 @@ PEERDIR(\n     contrib/libs/poco/MongoDB\n     contrib/libs/poco/Redis\n     contrib/libs/sparsehash\n+    contrib/restricted/abseil-cpp\n )\n \n IF (USE_ODBC)\ndiff --git a/src/Functions/FunctionsExternalDictionaries.h b/src/Functions/FunctionsExternalDictionaries.h\nindex 18b89ff96898..7c88d244815a 100644\n--- a/src/Functions/FunctionsExternalDictionaries.h\n+++ b/src/Functions/FunctionsExternalDictionaries.h\n@@ -24,6 +24,7 @@\n \n #include <Interpreters/Context.h>\n #include <Interpreters/ExternalDictionariesLoader.h>\n+#include <Interpreters/castColumn.h>\n \n #include <Functions/IFunctionImpl.h>\n #include <Functions/FunctionHelpers.h>\n@@ -31,13 +32,7 @@\n #include <Dictionaries/FlatDictionary.h>\n #include <Dictionaries/HashedDictionary.h>\n #include <Dictionaries/CacheDictionary.h>\n-#if defined(OS_LINUX) || defined(__FreeBSD__)\n-#include <Dictionaries/SSDCacheDictionary.h>\n-#include <Dictionaries/SSDComplexKeyCacheDictionary.h>\n-#endif\n #include <Dictionaries/ComplexKeyHashedDictionary.h>\n-#include <Dictionaries/ComplexKeyCacheDictionary.h>\n-#include <Dictionaries/ComplexKeyDirectDictionary.h>\n #include <Dictionaries/RangeHashedDictionary.h>\n #include <Dictionaries/IPAddressDictionary.h>\n #include <Dictionaries/PolygonDictionaryImplementations.h>\n@@ -275,15 +270,21 @@ class FunctionDictGetNoType final : public IFunction\n             throw Exception{\"Illegal type \" + arguments[0].type->getName() + \" of first argument of function \" + getName()\n                 + \", expected a const string.\", ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT};\n \n-        String attribute_name;\n-        if (const auto * name_col = checkAndGetColumnConst<ColumnString>(arguments[1].column.get()))\n-            attribute_name = name_col->getValue<String>();\n-        else\n-            throw Exception{\"Illegal type \" + arguments[1].type->getName() + \" of second argument of function \" + getName()\n-                + \", expected a const string.\", ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT};\n+        Strings attribute_names = getAttributeNamesFromColumn(arguments[1].column, arguments[1].type);\n \n-        /// We're extracting the return type from the dictionary's config, without loading the dictionary.\n-        return helper.getDictionaryStructure(dictionary_name).getAttribute(attribute_name).type;\n+        DataTypes types;\n+\n+        for (auto & attribute_name : attribute_names)\n+        {\n+            /// We're extracting the return type from the dictionary's config, without loading the dictionary.\n+            auto attribute = helper.getDictionaryStructure(dictionary_name).getAttribute(attribute_name);\n+            types.emplace_back(attribute.type);\n+        }\n+\n+        if (types.size() > 1)\n+            return std::make_shared<DataTypeTuple>(types);\n+        else\n+            return types.front();\n     }\n \n     ColumnPtr executeImpl(const ColumnsWithTypeAndName & arguments, const DataTypePtr & result_type, size_t input_rows_count) const override\n@@ -299,13 +300,7 @@ class FunctionDictGetNoType final : public IFunction\n             throw Exception{\"Illegal type \" + arguments[0].type->getName() + \" of first argument of function \" + getName()\n                 + \", expected a const string.\", ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT};\n \n-        String attribute_name;\n-\n-        if (const auto * name_col = checkAndGetColumnConst<ColumnString>(arguments[1].column.get()))\n-            attribute_name = name_col->getValue<String>();\n-        else\n-            throw Exception{\"Illegal type \" + arguments[1].type->getName() + \" of second argument of function \" + getName()\n-                + \", expected a const string.\", ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT};\n+        Strings attribute_names = getAttributeNamesFromColumn(arguments[1].column, arguments[1].type);\n \n         auto dictionary = helper.getDictionary(dictionary_name);\n \n@@ -337,14 +332,54 @@ class FunctionDictGetNoType final : public IFunction\n             ++current_arguments_index;\n         }\n \n-        ColumnPtr default_col = nullptr;\n+        Columns default_cols;\n \n         if (dictionary_get_function_type == DictionaryGetFunctionType::getOrDefault)\n         {\n             if (current_arguments_index >= arguments.size())\n                 throw Exception{\"Wrong argument count for function \" + getName(), ErrorCodes::NUMBER_OF_ARGUMENTS_DOESNT_MATCH};\n \n-            default_col = arguments[current_arguments_index].column;\n+            const auto & column_before_cast = arguments[current_arguments_index];\n+\n+            if (const DataTypeTuple * type_tuple = typeid_cast<const DataTypeTuple *>(column_before_cast.type.get()))\n+            {\n+                const DataTypes & nested_types = type_tuple->getElements();\n+\n+                for (const auto & nested_type : nested_types)\n+                    if (nested_type->isNullable())\n+                        throw Exception(ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT, \"Wrong argument for function ({}) default values column nullable is not supported\", getName());\n+            }\n+            else if (column_before_cast.type->isNullable())\n+                throw Exception(ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT, \"Wrong argument for function ({}) default values column nullable is not supported\", getName());\n+\n+            auto result_type_no_nullable = removeNullable(result_type);\n+\n+            ColumnWithTypeAndName column_to_cast = {column_before_cast.column->convertToFullColumnIfConst(), column_before_cast.type, column_before_cast.name};\n+\n+            auto result = castColumnAccurate(column_to_cast, result_type_no_nullable);\n+\n+            if (attribute_names.size() > 1)\n+            {\n+                const auto * tuple_column = checkAndGetColumn<ColumnTuple>(result.get());\n+\n+                if (!tuple_column)\n+                    throw Exception(ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT,\n+                        \"Wrong argument for function ({}) default values column must be tuple\", getName());\n+\n+                if (tuple_column->tupleSize() != attribute_names.size())\n+                    throw Exception(ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT,\n+                        \"Wrong argument for function ({}) default values tuple column must contain same column size as requested attributes\",\n+                        getName());\n+\n+                default_cols = tuple_column->getColumnsCopy();\n+            }\n+            else\n+                default_cols.emplace_back(result);\n+        }\n+        else\n+        {\n+            for (size_t i = 0; i < attribute_names.size(); ++i)\n+                default_cols.emplace_back(nullptr);\n         }\n \n         ColumnPtr result;\n@@ -354,16 +389,42 @@ class FunctionDictGetNoType final : public IFunction\n \n         if (dictionary_key_type == DictionaryKeyType::simple)\n         {\n-            result = dictionary->getColumn(attribute_name, result_type, {key_column}, {std::make_shared<DataTypeUInt64>()}, default_col);\n+            if (!WhichDataType(key_col_with_type.type).isUInt64())\n+                 throw Exception(\n+                     ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT,\n+                     \"Third argument of function ({}) must be uint64 when dictionary is simple. Actual type ({}).\",\n+                     getName(),\n+                     key_col_with_type.type->getName());\n+\n+            if (attribute_names.size() > 1)\n+            {\n+                const auto & result_tuple_type = assert_cast<const DataTypeTuple &>(*result_type);\n+\n+                Columns result_columns = dictionary->getColumns(\n+                    attribute_names,\n+                    result_tuple_type.getElements(),\n+                    {key_column},\n+                    {std::make_shared<DataTypeUInt64>()},\n+                    default_cols);\n+\n+                result = ColumnTuple::create(std::move(result_columns));\n+            }\n+            else\n+                result = dictionary->getColumn(\n+                    attribute_names[0],\n+                    result_type,\n+                    {key_column},\n+                    {std::make_shared<DataTypeUInt64>()},\n+                    default_cols.front());\n         }\n         else if (dictionary_key_type == DictionaryKeyType::complex)\n         {\n             if (!isTuple(key_col_with_type.type))\n-                throw Exception(\n-                    ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT,\n-                    \"Third argument of function ({}) must be tuple when dictionary is complex. Actual type ({}).\",\n-                    getName(),\n-                    key_col_with_type.type->getName());\n+                 throw Exception(\n+                     ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT,\n+                     \"Third argument of function ({}) must be tuple when dictionary is complex. Actual type ({}).\",\n+                     getName(),\n+                     key_col_with_type.type->getName());\n \n             /// Functions in external dictionaries_loader only support full-value (not constant) columns with keys.\n             ColumnPtr key_column_full = key_col_with_type.column->convertToFullColumnIfConst();\n@@ -371,12 +432,56 @@ class FunctionDictGetNoType final : public IFunction\n             const auto & key_columns = typeid_cast<const ColumnTuple &>(*key_column_full).getColumnsCopy();\n             const auto & key_types = static_cast<const DataTypeTuple &>(*key_col_with_type.type).getElements();\n \n-            result = dictionary->getColumn(attribute_name, result_type, key_columns, key_types, default_col);\n+            if (attribute_names.size() > 1)\n+            {\n+                const auto & result_tuple_type = assert_cast<const DataTypeTuple &>(*result_type);\n+\n+                Columns result_columns = dictionary->getColumns(\n+                    attribute_names,\n+                    result_tuple_type.getElements(),\n+                    key_columns,\n+                    key_types,\n+                    default_cols);\n+\n+                result = ColumnTuple::create(std::move(result_columns));\n+            }\n+            else\n+                result = dictionary->getColumn(\n+                    attribute_names[0],\n+                    result_type,\n+                    key_columns,\n+                    key_types,\n+                    default_cols.front());\n         }\n         else if (dictionary_key_type == DictionaryKeyType::range)\n         {\n-            result = dictionary->getColumn(\n-                attribute_name, result_type, {key_column, range_col}, {std::make_shared<DataTypeUInt64>(), range_col_type}, default_col);\n+            if (!WhichDataType(key_col_with_type.type).isUInt64())\n+                 throw Exception(\n+                     ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT,\n+                     \"Third argument of function ({}) must be uint64 when dictionary is range. Actual type ({}).\",\n+                     getName(),\n+                     key_col_with_type.type->getName());\n+\n+            if (attribute_names.size() > 1)\n+            {\n+                const auto & result_tuple_type = assert_cast<const DataTypeTuple &>(*result_type);\n+\n+                Columns result_columns = dictionary->getColumns(\n+                    attribute_names,\n+                    result_tuple_type.getElements(),\n+                    {key_column, range_col},\n+                    {std::make_shared<DataTypeUInt64>(), range_col_type},\n+                    default_cols);\n+\n+                result = ColumnTuple::create(std::move(result_columns));\n+            }\n+            else\n+                result = dictionary->getColumn(\n+                    attribute_names[0],\n+                    result_type,\n+                    {key_column, range_col},\n+                    {std::make_shared<DataTypeUInt64>(), range_col_type},\n+                    default_cols.front());\n         }\n         else\n             throw Exception{\"Unknown dictionary identifier type\", ErrorCodes::BAD_ARGUMENTS};\n@@ -385,6 +490,45 @@ class FunctionDictGetNoType final : public IFunction\n     }\n \n private:\n+\n+    Strings getAttributeNamesFromColumn(const ColumnPtr & column, const DataTypePtr & type) const\n+    {\n+        Strings attribute_names;\n+\n+        if (const auto * name_col = checkAndGetColumnConst<ColumnString>(column.get()))\n+            attribute_names.emplace_back(name_col->getValue<String>());\n+        else if (const auto * tuple_col_const = checkAndGetColumnConst<ColumnTuple>(column.get()))\n+        {\n+            const ColumnTuple & tuple_col = assert_cast<const ColumnTuple &>(tuple_col_const->getDataColumn());\n+            size_t tuple_size = tuple_col.tupleSize();\n+\n+            if (tuple_size < 1)\n+                throw Exception(ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT,\n+                    \"Tuple second argument of function ({}) must contain multiple constant string columns\");\n+\n+            for (size_t i = 0; i < tuple_col.tupleSize(); ++i)\n+            {\n+                const auto * tuple_column = tuple_col.getColumnPtr(i).get();\n+\n+                const auto * attribute_name_column = checkAndGetColumn<ColumnString>(tuple_column);\n+\n+                if (!attribute_name_column)\n+                    throw Exception(ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT,\n+                        \"Tuple second argument of function ({}) must contain multiple constant string columns\",\n+                        getName());\n+\n+                attribute_names.emplace_back(attribute_name_column->getDataAt(0));\n+            }\n+        }\n+        else\n+            throw Exception(ErrorCodes::ILLEGAL_TYPE_OF_ARGUMENT,\n+                \"Illegal type ({}) of second argument of function ({}), expected a const string or const tuple of const strings.\",\n+                type->getName(),\n+                getName());\n+\n+        return attribute_names;\n+    }\n+\n     mutable FunctionDictHelper helper;\n };\n \n@@ -579,7 +723,7 @@ class FunctionDictGetHierarchy final : public IFunction\n         if (!((res = executeDispatch<FlatDictionary>(arguments, result_type, dict))\n             || (res = executeDispatch<DirectDictionary>(arguments, result_type, dict))\n             || (res = executeDispatch<HashedDictionary>(arguments, result_type, dict))\n-            || (res = executeDispatch<CacheDictionary>(arguments, result_type, dict))))\n+            || (res = executeDispatch<CacheDictionary<DictionaryKeyType::simple>>(arguments, result_type, dict))))\n             throw Exception{\"Unsupported dictionary type \" + dict->getTypeName(), ErrorCodes::UNKNOWN_TYPE};\n \n         return res;\n@@ -732,7 +876,7 @@ class FunctionDictIsIn final : public IFunction\n         if (!((res = executeDispatch<FlatDictionary>(arguments, dict))\n             || (res = executeDispatch<DirectDictionary>(arguments, dict))\n             || (res = executeDispatch<HashedDictionary>(arguments, dict))\n-            || (res = executeDispatch<CacheDictionary>(arguments, dict))))\n+            || (res = executeDispatch<CacheDictionary<DictionaryKeyType::simple>>(arguments, dict))))\n             throw Exception{\"Unsupported dictionary type \" + dict->getTypeName(), ErrorCodes::UNKNOWN_TYPE};\n \n         return res;\ndiff --git a/src/Interpreters/AggregationCommon.h b/src/Interpreters/AggregationCommon.h\nindex e896b0e14dfc..8f15d1c6c56f 100644\n--- a/src/Interpreters/AggregationCommon.h\n+++ b/src/Interpreters/AggregationCommon.h\n@@ -19,11 +19,6 @@\n #include <tmmintrin.h>\n #endif\n \n-\n-template <>\n-struct DefaultHash<StringRef> : public StringRefHash {};\n-\n-\n namespace DB\n {\n namespace ErrorCodes\n",
  "test_patch": "diff --git a/docker/test/fasttest/run.sh b/docker/test/fasttest/run.sh\nindex 1bfc91ecd920..649f9f812e11 100755\n--- a/docker/test/fasttest/run.sh\n+++ b/docker/test/fasttest/run.sh\n@@ -151,6 +151,7 @@ function clone_submodules\n         cd \"$FASTTEST_SOURCE\"\n \n         SUBMODULES_TO_UPDATE=(\n+            contrib/abseil-cpp\n             contrib/antlr4-runtime\n             contrib/boost\n             contrib/zlib-ng\ndiff --git a/src/Dictionaries/tests/gtest_dictionary_ssd_cache_dictionary_storage.cpp b/src/Dictionaries/tests/gtest_dictionary_ssd_cache_dictionary_storage.cpp\nnew file mode 100644\nindex 000000000000..20529e91bd3d\n--- /dev/null\n+++ b/src/Dictionaries/tests/gtest_dictionary_ssd_cache_dictionary_storage.cpp\n@@ -0,0 +1,242 @@\n+#if defined(__linux__) || defined(__FreeBSD__)\n+\n+#include <iostream>\n+\n+#include <gtest/gtest.h>\n+\n+#include <Dictionaries/SSDCacheDictionaryStorage.h>\n+\n+using namespace DB;\n+\n+TEST(SSDCacheDictionaryStorage, SSDCacheBlockWithSSDCacheSimpleKey)\n+{\n+    static constexpr size_t block_data_size = 4096;\n+    std::unique_ptr<char[]> block_data(new char[block_data_size]);\n+    memset(block_data.get(), 0, block_data_size);\n+\n+    {\n+        memset(block_data.get(), 0, block_data_size);\n+\n+        SSDCacheBlock block(block_data_size);\n+\n+        block.reset(block_data.get());\n+\n+        std::unique_ptr<char[]> data_to_insert(new char[4000]);\n+        memset(data_to_insert.get(), 1, 4000);\n+\n+        SSDCacheSimpleKey key(0, 200, data_to_insert.get());\n+        ASSERT_EQ(block.getKeysSize(), 0);\n+\n+        bool write_result = false;\n+        size_t offset_in_block = 0;\n+\n+        ASSERT_TRUE(block.enoughtPlaceToWriteKey(key));\n+        write_result = block.writeKey(key, offset_in_block);\n+        ASSERT_TRUE(write_result);\n+        ASSERT_EQ(block.getKeysSize(), 1);\n+\n+        key.key = 1;\n+        ASSERT_TRUE(block.enoughtPlaceToWriteKey(key));\n+        write_result = block.writeKey(key, offset_in_block);\n+        ASSERT_TRUE(write_result);\n+        ASSERT_EQ(block.getKeysSize(), 2);\n+\n+        key.key = 2;\n+        ASSERT_TRUE(block.enoughtPlaceToWriteKey(key));\n+        write_result = block.writeKey(key, offset_in_block);\n+        ASSERT_TRUE(write_result);\n+        ASSERT_EQ(block.getKeysSize(), 3);\n+\n+        PaddedPODArray<UInt64> expected = {0,1,2};\n+        PaddedPODArray<UInt64> actual;\n+        block.readSimpleKeys(actual);\n+        ASSERT_EQ(actual, expected);\n+    }\n+    {\n+        memset(block_data.get(), 0, block_data_size);\n+        SSDCacheBlock block(block_data_size);\n+        block.reset(block_data.get());\n+\n+        static constexpr size_t block_header_size = SSDCacheBlock::block_header_size;\n+        static constexpr size_t key_metadata_size = sizeof(size_t) * 2;\n+\n+        std::unique_ptr<char[]> data_to_insert(new char[4080]);\n+        memset(data_to_insert.get(), 1, 4000);\n+\n+        SSDCacheSimpleKey key {0, 4064, data_to_insert.get()};\n+\n+        ASSERT_TRUE(SSDCacheBlock::canBeWrittenInEmptyBlock(key, block_data_size));\n+        key.size = 4065;\n+        ASSERT_FALSE(SSDCacheBlock::canBeWrittenInEmptyBlock(key, block_data_size));\n+        key.size = 4064;\n+\n+        size_t offset_in_block;\n+        ASSERT_TRUE(block.enoughtPlaceToWriteKey(key));\n+        ASSERT_TRUE(block.writeKey(key, offset_in_block));\n+        ASSERT_EQ(offset_in_block, block_header_size + key_metadata_size);\n+\n+        ASSERT_FALSE(block.enoughtPlaceToWriteKey({1, 4065, data_to_insert.get()}));\n+        offset_in_block = 0;\n+        ASSERT_FALSE(block.writeKey({1, 4065, data_to_insert.get()}, offset_in_block));\n+        ASSERT_EQ(offset_in_block, 0);\n+\n+        PaddedPODArray<UInt64> expected = {0};\n+        PaddedPODArray<UInt64> actual;\n+        block.readSimpleKeys(actual);\n+        ASSERT_EQ(actual, expected);\n+    }\n+    {\n+        memset(block_data.get(), 0, block_data_size);\n+        SSDCacheBlock block(block_data_size);\n+        block.reset(block_data.get());\n+\n+        PaddedPODArray<UInt64> expected = {};\n+        PaddedPODArray<UInt64> actual;\n+        block.readSimpleKeys(actual);\n+        ASSERT_EQ(actual, expected);\n+    }\n+    {\n+        memset(block_data.get(), 0, block_data_size);\n+        SSDCacheBlock block(block_data_size);\n+        block.reset(block_data.get());\n+\n+        std::unique_ptr<char[]> data_to_insert(new char[4000]);\n+        memset(data_to_insert.get(), 1, 4000);\n+        size_t offset_in_block;\n+        SSDCacheSimpleKey key {0, 200, data_to_insert.get()};\n+        block.writeKey({1, 200, data_to_insert.get()}, offset_in_block);\n+        ASSERT_EQ(block.getKeysSize(), 1);\n+\n+        ASSERT_FALSE(block.checkCheckSum());\n+        block.writeCheckSum();\n+        ASSERT_TRUE(block.checkCheckSum());\n+\n+        SSDCacheBlock other_block(block_data_size);\n+        other_block.reset(block_data.get());\n+        bool write_result = other_block.writeKey({2, 200, data_to_insert.get()}, offset_in_block);\n+        ASSERT_TRUE(write_result);\n+\n+        ASSERT_FALSE(block.checkCheckSum());\n+        block.writeCheckSum();\n+        ASSERT_TRUE(block.checkCheckSum());\n+    }\n+}\n+\n+TEST(SSDCacheDictionaryStorage, SSDCacheBlockWithSSDCachComplexKey)\n+{\n+    static constexpr size_t block_data_size = 4096;\n+    std::unique_ptr<char[]> block_data(new char[block_data_size]);\n+    memset(block_data.get(), 0, block_data_size);\n+\n+    {\n+        memset(block_data.get(), 0, block_data_size);\n+\n+        SSDCacheBlock block(block_data_size);\n+\n+        block.reset(block_data.get());\n+\n+        std::unique_ptr<char[]> data_to_insert(new char[4000]);\n+        memset(data_to_insert.get(), 1, 4000);\n+\n+        String key = \"0\";\n+\n+        SSDCacheComplexKey ssd_cache_key(key, 200, data_to_insert.get());\n+        ASSERT_EQ(block.getKeysSize(), 0);\n+\n+        bool write_result = false;\n+        size_t offset_in_block = 0;\n+\n+        ASSERT_TRUE(block.enoughtPlaceToWriteKey(ssd_cache_key));\n+        write_result = block.writeKey(ssd_cache_key, offset_in_block);\n+        ASSERT_TRUE(write_result);\n+        ASSERT_EQ(block.getKeysSize(), 1);\n+\n+        ssd_cache_key.key = \"1\";\n+        ASSERT_TRUE(block.enoughtPlaceToWriteKey(ssd_cache_key));\n+        write_result = block.writeKey(ssd_cache_key, offset_in_block);\n+        ASSERT_TRUE(write_result);\n+        ASSERT_EQ(block.getKeysSize(), 2);\n+\n+        ssd_cache_key.key = \"2\";\n+        ASSERT_TRUE(block.enoughtPlaceToWriteKey(ssd_cache_key));\n+        write_result = block.writeKey(ssd_cache_key, offset_in_block);\n+        ASSERT_TRUE(write_result);\n+        ASSERT_EQ(block.getKeysSize(), 3);\n+\n+        PaddedPODArray<StringRef> expected = {\"0\",\"1\",\"2\"};\n+        PaddedPODArray<StringRef> actual;\n+\n+        block.readComplexKeys(actual);\n+        ASSERT_EQ(actual, expected);\n+    }\n+    {\n+        memset(block_data.get(), 0, block_data_size);\n+        SSDCacheBlock block(block_data_size);\n+        block.reset(block_data.get());\n+\n+        static constexpr size_t block_header_size = SSDCacheBlock::block_header_size;\n+        static constexpr size_t key_metadata_size = sizeof(size_t) * 2;\n+\n+        std::unique_ptr<char[]> data_to_insert(new char[4080]);\n+        memset(data_to_insert.get(), 1, 4000);\n+\n+        SSDCacheComplexKey key {\"\", 4064, data_to_insert.get()};\n+\n+        ASSERT_TRUE(SSDCacheBlock::canBeWrittenInEmptyBlock(key, block_data_size));\n+        key.size = 4065;\n+        ASSERT_FALSE(SSDCacheBlock::canBeWrittenInEmptyBlock(key, block_data_size));\n+        key.size = 4064;\n+\n+        size_t offset_in_block;\n+        ASSERT_TRUE(block.enoughtPlaceToWriteKey(key));\n+        ASSERT_TRUE(block.writeKey(key, offset_in_block));\n+        ASSERT_EQ(offset_in_block, block_header_size + key_metadata_size);\n+\n+        ASSERT_FALSE(block.enoughtPlaceToWriteKey({1, 4065, data_to_insert.get()}));\n+        offset_in_block = 0;\n+        ASSERT_FALSE(block.writeKey({1, 4065, data_to_insert.get()}, offset_in_block));\n+        ASSERT_EQ(offset_in_block, 0);\n+\n+        PaddedPODArray<UInt64> expected = {0};\n+        PaddedPODArray<UInt64> actual;\n+        block.readSimpleKeys(actual);\n+        ASSERT_EQ(actual, expected);\n+    }\n+    {\n+        memset(block_data.get(), 0, block_data_size);\n+        SSDCacheBlock block(block_data_size);\n+        block.reset(block_data.get());\n+\n+        PaddedPODArray<StringRef> expected = {};\n+        PaddedPODArray<StringRef> actual;\n+        block.readComplexKeys(actual);\n+        ASSERT_EQ(actual, expected);\n+    }\n+    {\n+        memset(block_data.get(), 0, block_data_size);\n+        SSDCacheBlock block(block_data_size);\n+        block.reset(block_data.get());\n+\n+        std::unique_ptr<char[]> data_to_insert(new char[4000]);\n+        memset(data_to_insert.get(), 1, 4000);\n+        size_t offset_in_block;\n+        SSDCacheComplexKey key {\"0\", 200, data_to_insert.get()};\n+        block.writeKey({1, 200, data_to_insert.get()}, offset_in_block);\n+        ASSERT_EQ(block.getKeysSize(), 1);\n+\n+        ASSERT_FALSE(block.checkCheckSum());\n+        block.writeCheckSum();\n+        ASSERT_TRUE(block.checkCheckSum());\n+\n+        SSDCacheBlock other_block(block_data_size);\n+        other_block.reset(block_data.get());\n+        bool write_result = other_block.writeKey({2, 200, data_to_insert.get()}, offset_in_block);\n+        ASSERT_TRUE(write_result);\n+\n+        ASSERT_FALSE(block.checkCheckSum());\n+        block.writeCheckSum();\n+        ASSERT_TRUE(block.checkCheckSum());\n+    }\n+}\n+\n+#endif\ndiff --git a/tests/queries/0_stateless/00950_dict_get.reference b/tests/queries/0_stateless/00950_dict_get.reference\nindex 3010a2989c09..c1b502bf773b 100644\n--- a/tests/queries/0_stateless/00950_dict_get.reference\n+++ b/tests/queries/0_stateless/00950_dict_get.reference\n@@ -1,48 +1,48 @@\n-dictGet\tflat_ints\t1\t1\t1\t1\t1\t1\t1\t1\t1\n-dictGetOrDefault\tflat_ints\t1\t1\t1\t1\t1\t1\t1\t1\t1\n-dictGetOrDefault\tflat_ints\t0\t42\t42\t42\t42\t42\t42\t42\t42\n-dictGet\thashed_ints\t1\t1\t1\t1\t1\t1\t1\t1\t1\n-dictGetOrDefault\thashed_ints\t1\t1\t1\t1\t1\t1\t1\t1\t1\n-dictGetOrDefault\thashed_ints\t0\t42\t42\t42\t42\t42\t42\t42\t42\n-dictGet\thashed_sparse_ints\t1\t1\t1\t1\t1\t1\t1\t1\t1\n-dictGetOrDefault\thashed_sparse_ints\t1\t1\t1\t1\t1\t1\t1\t1\t1\n-dictGetOrDefault\thashed_sparse_ints\t0\t42\t42\t42\t42\t42\t42\t42\t42\n-dictGet\tcache_ints\t1\t1\t1\t1\t1\t1\t1\t1\t1\n-dictGetOrDefault\tcache_ints\t1\t1\t1\t1\t1\t1\t1\t1\t1\n-dictGetOrDefault\tcache_ints\t0\t42\t42\t42\t42\t42\t42\t42\t42\n-dictGet\tcomplex_hashed_ints\t(1)\t1\t1\t1\t1\t1\t1\t1\t1\n-dictGetOrDefault\tcomplex_hashed_ints\t(1)\t1\t1\t1\t1\t1\t1\t1\t1\n-dictGetOrDefault\tcomplex_hashed_ints\t(0)\t42\t42\t42\t42\t42\t42\t42\t42\n-dictGet\tcomplex_cache_ints\t(1)\t1\t1\t1\t1\t1\t1\t1\t1\n-dictGetOrDefault\tcomplex_cache_ints\t(1)\t1\t1\t1\t1\t1\t1\t1\t1\n-dictGetOrDefault\tcomplex_cache_ints\t(0)\t42\t42\t42\t42\t42\t42\t42\t42\n-dictGet\tflat_strings\t1\t1\n-dictGetOrDefault\tflat_strings\t1\t1\n-dictGetOrDefault\tflat_strings\t0\t*\n-dictGet\thashed_strings\t1\t1\n-dictGetOrDefault\thashed_strings\t1\t1\n-dictGetOrDefault\thashed_strings\t0\t*\n-dictGet\tcache_strings\t1\t1\n-dictGetOrDefault\tcache_strings\t1\t1\n-dictGetOrDefault\tcache_strings\t0\t*\n-dictGet\tcomplex_hashed_strings\t1\t1\n-dictGetOrDefault\tcomplex_hashed_strings\t1\t1\n-dictGetOrDefault\tcomplex_hashed_strings\t0\t*\n-dictGet\tcomplex_cache_strings\t1\t1\n-dictGetOrDefault\tcomplex_cache_strings\t1\t1\n-dictGetOrDefault\tcomplex_cache_strings\t0\t*\n-dictGet\tflat_decimals\t1\t1.0000\t1.000000\t1.0\n-dictGetOrDefault\tflat_decimals\t1\t1.0000\t1.000000\t1.0\n-dictGetOrDefault\tflat_decimals\t0\t42.0000\t42.000000\t42.0\n-dictGet\thashed_decimals\t1\t1.0000\t1.000000\t1.0\n-dictGetOrDefault\thashed_decimals\t1\t1.0000\t1.000000\t1.0\n-dictGetOrDefault\thashed_decimals\t0\t42.0000\t42.000000\t42.0\n-dictGet\tcache_decimals\t1\t1.0000\t1.000000\t1.0\n-dictGetOrDefault\tcache_decimals\t1\t1.0000\t1.000000\t1.0\n-dictGetOrDefault\tcache_decimals\t0\t42.0000\t42.000000\t42.0\n-dictGet\tcomplex_hashed_decimals\t(1)\t1.0000\t1.000000\t1.0\n-dictGetOrDefault\tcomplex_hashed_decimals\t(1)\t1.0000\t1.000000\t1.0\n-dictGetOrDefault\tcomplex_hashed_decimals\t(0)\t42.0000\t42.000000\t42.0\n-dictGet\tcomplex_cache_decimals\t(1)\t1.0000\t1.000000\t1.0\n-dictGetOrDefault\tcomplex_cache_decimals\t(1)\t1.0000\t1.000000\t1.0\n-dictGetOrDefault\tcomplex_cache_decimals\t(0)\t42.0000\t42.000000\t42.0\n+dictGet\tflat_ints\t1\t1\t1\t1\t1\t1\t1\t1\t1\t(1,1,1)\n+dictGetOrDefault\tflat_ints\t1\t1\t1\t1\t1\t1\t1\t1\t1\t(1,1,1)\n+dictGetOrDefault\tflat_ints\t0\t42\t42\t42\t42\t42\t42\t42\t42\t(42,42,42)\n+dictGet\thashed_ints\t1\t1\t1\t1\t1\t1\t1\t1\t1\t(1,1,1)\n+dictGetOrDefault\thashed_ints\t1\t1\t1\t1\t1\t1\t1\t1\t1\t(1,1,1)\n+dictGetOrDefault\thashed_ints\t0\t42\t42\t42\t42\t42\t42\t42\t(42,42,42)\n+dictGet\thashed_sparse_ints\t1\t1\t1\t1\t1\t1\t1\t1\t1\t(1,1,1)\n+dictGetOrDefault\thashed_sparse_ints\t1\t1\t1\t1\t1\t1\t1\t1\t1\t(1,1,1)\n+dictGetOrDefault\thashed_sparse_ints\t0\t42\t42\t42\t42\t42\t42\t42\t42\t(42,42,42)\n+dictGet\tcache_ints\t1\t1\t1\t1\t1\t1\t1\t1\t1\t(1,1,1)\n+dictGetOrDefault\tcache_ints\t1\t1\t1\t1\t1\t1\t1\t1\t1\t(1,1,1)\n+dictGetOrDefault\tcache_ints\t0\t42\t42\t42\t42\t42\t42\t42\t42\t(42,42,42)\n+dictGet\tcomplex_hashed_ints\t(1)\t1\t1\t1\t1\t1\t1\t1\t1\t(1,1,1)\n+dictGetOrDefault\tcomplex_hashed_ints\t(1)\t1\t1\t1\t1\t1\t1\t1\t1\t(1,1,1)\n+dictGetOrDefault\tcomplex_hashed_ints\t(0)\t42\t42\t42\t42\t42\t42\t42\t42\t(42,42,42)\n+dictGet\tcomplex_cache_ints\t(1)\t1\t1\t1\t1\t1\t1\t1\t1\t(1,1,1)\n+dictGetOrDefault\tcomplex_cache_ints\t(1)\t1\t1\t1\t1\t1\t1\t1\t1\t(1,1,1)\n+dictGetOrDefault\tcomplex_cache_ints\t(0)\t42\t42\t42\t42\t42\t42\t42\t42\t(42,42,42)\n+dictGet\tflat_strings\t1\t1\t1\n+dictGetOrDefault\tflat_strings\t1\t1\t1\n+dictGetOrDefault\tflat_strings\t0\t*\t*\n+dictGet\thashed_strings\t1\t1\t1\n+dictGetOrDefault\thashed_strings\t1\t1\t1\n+dictGetOrDefault\thashed_strings\t0\t*\t*\n+dictGet\tcache_strings\t1\t1\t1\n+dictGetOrDefault\tcache_strings\t1\t1\t1\n+dictGetOrDefault\tcache_strings\t0\t*\t*\n+dictGet\tcomplex_hashed_strings\t1\t1\t1\n+dictGetOrDefault\tcomplex_hashed_strings\t1\t1\t1\n+dictGetOrDefault\tcomplex_hashed_strings\t0\t*\t*\n+dictGet\tcomplex_cache_strings\t1\t1\t1\n+dictGetOrDefault\tcomplex_cache_strings\t1\t1\t1\n+dictGetOrDefault\tcomplex_cache_strings\t0\t*\t*\n+dictGet\tflat_decimals\t1\t1.0000\t1.000000\t1.0\t(1.0000,1.000000,1.0)\n+dictGetOrDefault\tflat_decimals\t1\t1.0000\t1.000000\t1.0\t(1.0000,1.000000,1.0)\n+dictGetOrDefault\tflat_decimals\t0\t42.0000\t42.000000\t42.0\t(42.0000,42.000000,42.0)\n+dictGet\thashed_decimals\t1\t1.0000\t1.000000\t1.0\t(1.0000,1.000000,1.0)\n+dictGetOrDefault\thashed_decimals\t1\t1.0000\t1.000000\t1.0\t(1.0000,1.000000,1.0)\n+dictGetOrDefault\thashed_decimals\t0\t42.0000\t42.000000\t42.0\t(42.0000,42.000000,42.0)\n+dictGet\tcache_decimals\t1\t1.0000\t1.000000\t1.0\t(1.0000,1.000000,1.0)\n+dictGetOrDefault\tcache_decimals\t1\t1.0000\t1.000000\t1.0\t(1.0000,1.000000,1.0)\n+dictGetOrDefault\tcache_decimals\t0\t42.0000\t42.000000\t42.0\t(42.0000,42.000000,42.0)\n+dictGet\tcomplex_hashed_decimals\t(1)\t1.0000\t1.000000\t1.0\t(1.0000,1.000000,1.0)\n+dictGetOrDefault\tcomplex_hashed_decimals\t(1)\t1.0000\t1.000000\t1.0\t(1.0000,1.000000,1.0)\n+dictGetOrDefault\tcomplex_hashed_decimals\t(0)\t42.0000\t42.000000\t42.0\t(42.0000,42.000000,42.0)\n+dictGet\tcomplex_cache_decimals\t(1)\t1.0000\t1.000000\t1.0\t(1.0000,1.000000,1.0)\n+dictGetOrDefault\tcomplex_cache_decimals\t(1)\t1.0000\t1.000000\t1.0\t(1.0000,1.000000,1.0)\n+dictGetOrDefault\tcomplex_cache_decimals\t(0)\t42.0000\t42.000000\t42.0\t(42.0000,42.000000,42.0)\ndiff --git a/tests/queries/0_stateless/00950_dict_get.sql b/tests/queries/0_stateless/00950_dict_get.sql\nindex 93bb8f9b813e..e49e05fbd27a 100644\n--- a/tests/queries/0_stateless/00950_dict_get.sql\n+++ b/tests/queries/0_stateless/00950_dict_get.sql\n@@ -20,7 +20,8 @@ select 'dictGet', 'flat_ints' as dict_name, toUInt64(1) as k,\n     dictGet(dict_name, 'u8', k),\n     dictGet(dict_name, 'u16', k),\n     dictGet(dict_name, 'u32', k),\n-    dictGet(dict_name, 'u64', k);\n+    dictGet(dict_name, 'u64', k),\n+    dictGet(dict_name, ('i8', 'i16', 'i32'), k);\n select 'dictGetOrDefault', 'flat_ints' as dict_name, toUInt64(1) as k,\n     dictGetOrDefault(dict_name, 'i8', k, toInt8(42)),\n     dictGetOrDefault(dict_name, 'i16', k, toInt16(42)),\n@@ -29,7 +30,8 @@ select 'dictGetOrDefault', 'flat_ints' as dict_name, toUInt64(1) as k,\n     dictGetOrDefault(dict_name, 'u8', k, toUInt8(42)),\n     dictGetOrDefault(dict_name, 'u16', k, toUInt16(42)),\n     dictGetOrDefault(dict_name, 'u32', k, toUInt32(42)),\n-    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42));\n+    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42)),\n+    dictGetOrDefault(dict_name, ('i8', 'i16', 'i32'), k, (toInt8(42), toInt16(42), toInt32(42)));\n select 'dictGetOrDefault', 'flat_ints' as dict_name, toUInt64(0) as k,\n     dictGetOrDefault(dict_name, 'i8', k, toInt8(42)),\n     dictGetOrDefault(dict_name, 'i16', k, toInt16(42)),\n@@ -38,7 +40,8 @@ select 'dictGetOrDefault', 'flat_ints' as dict_name, toUInt64(0) as k,\n     dictGetOrDefault(dict_name, 'u8', k, toUInt8(42)),\n     dictGetOrDefault(dict_name, 'u16', k, toUInt16(42)),\n     dictGetOrDefault(dict_name, 'u32', k, toUInt32(42)),\n-    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42));\n+    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42)),\n+    dictGetOrDefault(dict_name, ('i8', 'i16', 'i32'), k, (toInt8(42), toInt16(42), toInt32(42)));\n \n select 'dictGet', 'hashed_ints' as dict_name, toUInt64(1) as k,\n     dictGet(dict_name, 'i8', k),\n@@ -48,7 +51,8 @@ select 'dictGet', 'hashed_ints' as dict_name, toUInt64(1) as k,\n     dictGet(dict_name, 'u8', k),\n     dictGet(dict_name, 'u16', k),\n     dictGet(dict_name, 'u32', k),\n-    dictGet(dict_name, 'u64', k);\n+    dictGet(dict_name, 'u64', k),\n+    dictGet(dict_name, ('i8', 'i16', 'i32'), k);\n select 'dictGetOrDefault', 'hashed_ints' as dict_name, toUInt64(1) as k,\n     dictGetOrDefault(dict_name, 'i8', k, toInt8(42)),\n     dictGetOrDefault(dict_name, 'i16', k, toInt16(42)),\n@@ -57,7 +61,8 @@ select 'dictGetOrDefault', 'hashed_ints' as dict_name, toUInt64(1) as k,\n     dictGetOrDefault(dict_name, 'u8', k, toUInt8(42)),\n     dictGetOrDefault(dict_name, 'u16', k, toUInt16(42)),\n     dictGetOrDefault(dict_name, 'u32', k, toUInt32(42)),\n-    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42));\n+    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42)),\n+    dictGetOrDefault(dict_name, ('i8', 'i16', 'i32'), k, (toInt8(42), toInt16(42), toInt32(42)));\n select 'dictGetOrDefault', 'hashed_ints' as dict_name, toUInt64(0) as k,\n     dictGetOrDefault(dict_name, 'i8', k, toInt8(42)),\n     dictGetOrDefault(dict_name, 'i16', k, toInt16(42)),\n@@ -66,7 +71,7 @@ select 'dictGetOrDefault', 'hashed_ints' as dict_name, toUInt64(0) as k,\n     dictGetOrDefault(dict_name, 'u8', k, toUInt8(42)),\n     dictGetOrDefault(dict_name, 'u16', k, toUInt16(42)),\n     dictGetOrDefault(dict_name, 'u32', k, toUInt32(42)),\n-    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42));\n+    dictGetOrDefault(dict_name, ('i8', 'i16', 'i32'), k, (toInt8(42), toInt16(42), toInt32(42)));\n \n select 'dictGet', 'hashed_sparse_ints' as dict_name, toUInt64(1) as k,\n     dictGet(dict_name, 'i8', k),\n@@ -76,7 +81,8 @@ select 'dictGet', 'hashed_sparse_ints' as dict_name, toUInt64(1) as k,\n     dictGet(dict_name, 'u8', k),\n     dictGet(dict_name, 'u16', k),\n     dictGet(dict_name, 'u32', k),\n-    dictGet(dict_name, 'u64', k);\n+    dictGet(dict_name, 'u64', k),\n+    dictGet(dict_name, ('i8', 'i16', 'i32'), k);\n select 'dictGetOrDefault', 'hashed_sparse_ints' as dict_name, toUInt64(1) as k,\n     dictGetOrDefault(dict_name, 'i8', k, toInt8(42)),\n     dictGetOrDefault(dict_name, 'i16', k, toInt16(42)),\n@@ -85,7 +91,8 @@ select 'dictGetOrDefault', 'hashed_sparse_ints' as dict_name, toUInt64(1) as k,\n     dictGetOrDefault(dict_name, 'u8', k, toUInt8(42)),\n     dictGetOrDefault(dict_name, 'u16', k, toUInt16(42)),\n     dictGetOrDefault(dict_name, 'u32', k, toUInt32(42)),\n-    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42));\n+    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42)),\n+    dictGetOrDefault(dict_name, ('i8', 'i16', 'i32'), k, (toInt8(42), toInt16(42), toInt32(42)));\n select 'dictGetOrDefault', 'hashed_sparse_ints' as dict_name, toUInt64(0) as k,\n     dictGetOrDefault(dict_name, 'i8', k, toInt8(42)),\n     dictGetOrDefault(dict_name, 'i16', k, toInt16(42)),\n@@ -94,7 +101,8 @@ select 'dictGetOrDefault', 'hashed_sparse_ints' as dict_name, toUInt64(0) as k,\n     dictGetOrDefault(dict_name, 'u8', k, toUInt8(42)),\n     dictGetOrDefault(dict_name, 'u16', k, toUInt16(42)),\n     dictGetOrDefault(dict_name, 'u32', k, toUInt32(42)),\n-    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42));\n+    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42)),\n+    dictGetOrDefault(dict_name, ('i8', 'i16', 'i32'), k, (toInt8(42), toInt16(42), toInt32(42)));\n \n select 'dictGet', 'cache_ints' as dict_name, toUInt64(1) as k,\n     dictGet(dict_name, 'i8', k),\n@@ -104,7 +112,8 @@ select 'dictGet', 'cache_ints' as dict_name, toUInt64(1) as k,\n     dictGet(dict_name, 'u8', k),\n     dictGet(dict_name, 'u16', k),\n     dictGet(dict_name, 'u32', k),\n-    dictGet(dict_name, 'u64', k);\n+    dictGet(dict_name, 'u64', k),\n+    dictGet(dict_name, ('i8', 'i16', 'i32'), k);\n select 'dictGetOrDefault', 'cache_ints' as dict_name, toUInt64(1) as k,\n     dictGetOrDefault(dict_name, 'i8', k, toInt8(42)),\n     dictGetOrDefault(dict_name, 'i16', k, toInt16(42)),\n@@ -113,7 +122,8 @@ select 'dictGetOrDefault', 'cache_ints' as dict_name, toUInt64(1) as k,\n     dictGetOrDefault(dict_name, 'u8', k, toUInt8(42)),\n     dictGetOrDefault(dict_name, 'u16', k, toUInt16(42)),\n     dictGetOrDefault(dict_name, 'u32', k, toUInt32(42)),\n-    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42));\n+    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42)),\n+    dictGetOrDefault(dict_name, ('i8', 'i16', 'i32'), k, (toInt8(42), toInt16(42), toInt32(42)));\n select 'dictGetOrDefault', 'cache_ints' as dict_name, toUInt64(0) as k,\n     dictGetOrDefault(dict_name, 'i8', k, toInt8(42)),\n     dictGetOrDefault(dict_name, 'i16', k, toInt16(42)),\n@@ -122,7 +132,8 @@ select 'dictGetOrDefault', 'cache_ints' as dict_name, toUInt64(0) as k,\n     dictGetOrDefault(dict_name, 'u8', k, toUInt8(42)),\n     dictGetOrDefault(dict_name, 'u16', k, toUInt16(42)),\n     dictGetOrDefault(dict_name, 'u32', k, toUInt32(42)),\n-    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42));\n+    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42)),\n+    dictGetOrDefault(dict_name, ('i8', 'i16', 'i32'), k, (toInt8(42), toInt16(42), toInt32(42)));\n \n select 'dictGet', 'complex_hashed_ints' as dict_name, tuple(toUInt64(1)) as k,\n     dictGet(dict_name, 'i8', k),\n@@ -132,7 +143,8 @@ select 'dictGet', 'complex_hashed_ints' as dict_name, tuple(toUInt64(1)) as k,\n     dictGet(dict_name, 'u8', k),\n     dictGet(dict_name, 'u16', k),\n     dictGet(dict_name, 'u32', k),\n-    dictGet(dict_name, 'u64', k);\n+    dictGet(dict_name, 'u64', k),\n+    dictGet(dict_name, ('i8', 'i16', 'i32'), k);\n select 'dictGetOrDefault', 'complex_hashed_ints' as dict_name, tuple(toUInt64(1)) as k,\n     dictGetOrDefault(dict_name, 'i8', k, toInt8(42)),\n     dictGetOrDefault(dict_name, 'i16', k, toInt16(42)),\n@@ -141,7 +153,8 @@ select 'dictGetOrDefault', 'complex_hashed_ints' as dict_name, tuple(toUInt64(1)\n     dictGetOrDefault(dict_name, 'u8', k, toUInt8(42)),\n     dictGetOrDefault(dict_name, 'u16', k, toUInt16(42)),\n     dictGetOrDefault(dict_name, 'u32', k, toUInt32(42)),\n-    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42));\n+    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42)),\n+    dictGetOrDefault(dict_name, ('i8', 'i16', 'i32'), k, (toInt8(42), toInt16(42), toInt32(42)));\n select 'dictGetOrDefault', 'complex_hashed_ints' as dict_name, tuple(toUInt64(0)) as k,\n     dictGetOrDefault(dict_name, 'i8', k, toInt8(42)),\n     dictGetOrDefault(dict_name, 'i16', k, toInt16(42)),\n@@ -150,7 +163,8 @@ select 'dictGetOrDefault', 'complex_hashed_ints' as dict_name, tuple(toUInt64(0)\n     dictGetOrDefault(dict_name, 'u8', k, toUInt8(42)),\n     dictGetOrDefault(dict_name, 'u16', k, toUInt16(42)),\n     dictGetOrDefault(dict_name, 'u32', k, toUInt32(42)),\n-    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42));\n+    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42)),\n+    dictGetOrDefault(dict_name, ('i8', 'i16', 'i32'), k, (toInt8(42), toInt16(42), toInt32(42)));\n \n select 'dictGet', 'complex_cache_ints' as dict_name, tuple(toUInt64(1)) as k,\n     dictGet(dict_name, 'i8', k),\n@@ -160,7 +174,8 @@ select 'dictGet', 'complex_cache_ints' as dict_name, tuple(toUInt64(1)) as k,\n     dictGet(dict_name, 'u8', k),\n     dictGet(dict_name, 'u16', k),\n     dictGet(dict_name, 'u32', k),\n-    dictGet(dict_name, 'u64', k);\n+    dictGet(dict_name, 'u64', k),\n+    dictGet(dict_name, ('i8', 'i16', 'i32'), k);;\n select 'dictGetOrDefault', 'complex_cache_ints' as dict_name, tuple(toUInt64(1)) as k,\n     dictGetOrDefault(dict_name, 'i8', k, toInt8(42)),\n     dictGetOrDefault(dict_name, 'i16', k, toInt16(42)),\n@@ -169,7 +184,8 @@ select 'dictGetOrDefault', 'complex_cache_ints' as dict_name, tuple(toUInt64(1))\n     dictGetOrDefault(dict_name, 'u8', k, toUInt8(42)),\n     dictGetOrDefault(dict_name, 'u16', k, toUInt16(42)),\n     dictGetOrDefault(dict_name, 'u32', k, toUInt32(42)),\n-    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42));\n+    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42)),\n+    dictGetOrDefault(dict_name, ('i8', 'i16', 'i32'), k, (toInt8(42), toInt16(42), toInt32(42)));\n select 'dictGetOrDefault', 'complex_cache_ints' as dict_name, tuple(toUInt64(0)) as k,\n     dictGetOrDefault(dict_name, 'i8', k, toInt8(42)),\n     dictGetOrDefault(dict_name, 'i16', k, toInt16(42)),\n@@ -178,96 +194,112 @@ select 'dictGetOrDefault', 'complex_cache_ints' as dict_name, tuple(toUInt64(0))\n     dictGetOrDefault(dict_name, 'u8', k, toUInt8(42)),\n     dictGetOrDefault(dict_name, 'u16', k, toUInt16(42)),\n     dictGetOrDefault(dict_name, 'u32', k, toUInt32(42)),\n-    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42));\n+    dictGetOrDefault(dict_name, 'u64', k, toUInt64(42)),\n+    dictGetOrDefault(dict_name, ('i8', 'i16', 'i32'), k, (toInt8(42), toInt16(42), toInt32(42)));\n \n --\n \n-select 'dictGet', 'flat_strings' as dict_name, toUInt64(1) as k, dictGet(dict_name, 'str', k);\n-select 'dictGetOrDefault', 'flat_strings' as dict_name, toUInt64(1) as k, dictGetOrDefault(dict_name, 'str', k, '*');\n-select 'dictGetOrDefault', 'flat_strings' as dict_name, toUInt64(0) as k, dictGetOrDefault(dict_name, 'str', k, '*');\n+select 'dictGet', 'flat_strings' as dict_name, toUInt64(1) as k, dictGet(dict_name, 'str', k), dictGet(dict_name, ('str'), k);\n+select 'dictGetOrDefault', 'flat_strings' as dict_name, toUInt64(1) as k, dictGetOrDefault(dict_name, 'str', k, '*'), dictGetOrDefault(dict_name, ('str'), k, ('*'));\n+select 'dictGetOrDefault', 'flat_strings' as dict_name, toUInt64(0) as k, dictGetOrDefault(dict_name, 'str', k, '*'), dictGetOrDefault(dict_name, ('str'), k, ('*'));\n \n-select 'dictGet', 'hashed_strings' as dict_name, toUInt64(1) as k, dictGet(dict_name, 'str', k);\n-select 'dictGetOrDefault', 'hashed_strings' as dict_name, toUInt64(1) as k, dictGetOrDefault(dict_name, 'str', k, '*');\n-select 'dictGetOrDefault', 'hashed_strings' as dict_name, toUInt64(0) as k, dictGetOrDefault(dict_name, 'str', k, '*');\n+select 'dictGet', 'hashed_strings' as dict_name, toUInt64(1) as k, dictGet(dict_name, 'str', k), dictGet(dict_name, ('str'), k);\n+select 'dictGetOrDefault', 'hashed_strings' as dict_name, toUInt64(1) as k, dictGetOrDefault(dict_name, 'str', k, '*'), dictGetOrDefault(dict_name, ('str'), k, ('*'));\n+select 'dictGetOrDefault', 'hashed_strings' as dict_name, toUInt64(0) as k, dictGetOrDefault(dict_name, 'str', k, '*'), dictGetOrDefault(dict_name, ('str'), k, ('*'));\n \n-select 'dictGet', 'cache_strings' as dict_name, toUInt64(1) as k, dictGet(dict_name, 'str', k);\n-select 'dictGetOrDefault', 'cache_strings' as dict_name, toUInt64(1) as k, dictGetOrDefault(dict_name, 'str', k, '*');\n-select 'dictGetOrDefault', 'cache_strings' as dict_name, toUInt64(0) as k, dictGetOrDefault(dict_name, 'str', k, '*');\n+select 'dictGet', 'cache_strings' as dict_name, toUInt64(1) as k, dictGet(dict_name, 'str', k), dictGet(dict_name, ('str'), k);\n+select 'dictGetOrDefault', 'cache_strings' as dict_name, toUInt64(1) as k, dictGetOrDefault(dict_name, 'str', k, '*'), dictGetOrDefault(dict_name, ('str'), k, ('*'));\n+select 'dictGetOrDefault', 'cache_strings' as dict_name, toUInt64(0) as k, dictGetOrDefault(dict_name, 'str', k, '*'), dictGetOrDefault(dict_name, ('str'), k, ('*'));\n \n-select 'dictGet', 'complex_hashed_strings' as dict_name, toUInt64(1) as k, dictGet(dict_name, 'str', tuple(k));\n-select 'dictGetOrDefault', 'complex_hashed_strings' as dict_name, toUInt64(1) as k, dictGetOrDefault(dict_name, 'str', tuple(k), '*');\n-select 'dictGetOrDefault', 'complex_hashed_strings' as dict_name, toUInt64(0) as k, dictGetOrDefault(dict_name, 'str', tuple(k), '*');\n+select 'dictGet', 'complex_hashed_strings' as dict_name, toUInt64(1) as k, dictGet(dict_name, 'str', tuple(k)), dictGet(dict_name, ('str'), tuple(k));\n+select 'dictGetOrDefault', 'complex_hashed_strings' as dict_name, toUInt64(1) as k, dictGetOrDefault(dict_name, 'str', tuple(k), '*'), dictGetOrDefault(dict_name, ('str'), tuple(k), ('*'));\n+select 'dictGetOrDefault', 'complex_hashed_strings' as dict_name, toUInt64(0) as k, dictGetOrDefault(dict_name, 'str', tuple(k), '*'), dictGetOrDefault(dict_name, ('str'), tuple(k), ('*'));\n \n-select 'dictGet', 'complex_cache_strings' as dict_name, toUInt64(1) as k, dictGet(dict_name, 'str', tuple(k));\n-select 'dictGetOrDefault', 'complex_cache_strings' as dict_name, toUInt64(1) as k, dictGetOrDefault(dict_name, 'str', tuple(k), '*');\n-select 'dictGetOrDefault', 'complex_cache_strings' as dict_name, toUInt64(0) as k, dictGetOrDefault(dict_name, 'str', tuple(k), '*');\n+select 'dictGet', 'complex_cache_strings' as dict_name, toUInt64(1) as k, dictGet(dict_name, 'str', tuple(k)), dictGet(dict_name, ('str'), tuple(k));\n+select 'dictGetOrDefault', 'complex_cache_strings' as dict_name, toUInt64(1) as k, dictGetOrDefault(dict_name, 'str', tuple(k), '*'), dictGetOrDefault(dict_name, ('str'), tuple(k), ('*'));\n+select 'dictGetOrDefault', 'complex_cache_strings' as dict_name, toUInt64(0) as k, dictGetOrDefault(dict_name, 'str', tuple(k), '*'), dictGetOrDefault(dict_name, ('str'), tuple(k), ('*'));\n \n --\n \n select 'dictGet', 'flat_decimals' as dict_name, toUInt64(1) as k,\n     dictGet(dict_name, 'd32', k),\n     dictGet(dict_name, 'd64', k),\n-    dictGet(dict_name, 'd128', k);\n+    dictGet(dict_name, 'd128', k),\n+    dictGet(dict_name, ('d32', 'd64', 'd128'), k);\n select 'dictGetOrDefault', 'flat_decimals' as dict_name, toUInt64(1) as k,\n     dictGetOrDefault(dict_name, 'd32', k, toDecimal32(42, 4)),\n     dictGetOrDefault(dict_name, 'd64', k, toDecimal64(42, 6)),\n-    dictGetOrDefault(dict_name, 'd128', k, toDecimal128(42, 1));\n+    dictGetOrDefault(dict_name, 'd128', k, toDecimal128(42, 1)),\n+    dictGetOrDefault(dict_name, ('d32', 'd64', 'd128'), k, (toDecimal32(42, 4), toDecimal64(42, 6), toDecimal128(42, 1)));\n select 'dictGetOrDefault', 'flat_decimals' as dict_name, toUInt64(0) as k,\n     dictGetOrDefault(dict_name, 'd32', k, toDecimal32(42, 4)),\n     dictGetOrDefault(dict_name, 'd64', k, toDecimal64(42, 6)),\n-    dictGetOrDefault(dict_name, 'd128', k, toDecimal128(42, 1));\n+    dictGetOrDefault(dict_name, 'd128', k, toDecimal128(42, 1)),\n+    dictGetOrDefault(dict_name, ('d32', 'd64', 'd128'), k, (toDecimal32(42, 4), toDecimal64(42, 6), toDecimal128(42, 1)));\n \n select 'dictGet', 'hashed_decimals' as dict_name, toUInt64(1) as k,\n     dictGet(dict_name, 'd32', k),\n     dictGet(dict_name, 'd64', k),\n-    dictGet(dict_name, 'd128', k);\n+    dictGet(dict_name, 'd128', k),\n+    dictGet(dict_name, ('d32', 'd64', 'd128'), k);\n select 'dictGetOrDefault', 'hashed_decimals' as dict_name, toUInt64(1) as k,\n     dictGetOrDefault(dict_name, 'd32', k, toDecimal32(42, 4)),\n     dictGetOrDefault(dict_name, 'd64', k, toDecimal64(42, 6)),\n-    dictGetOrDefault(dict_name, 'd128', k, toDecimal128(42, 1));\n+    dictGetOrDefault(dict_name, 'd128', k, toDecimal128(42, 1)),\n+    dictGetOrDefault(dict_name, ('d32', 'd64', 'd128'), k, (toDecimal32(42, 4), toDecimal64(42, 6), toDecimal128(42, 1)));\n select 'dictGetOrDefault', 'hashed_decimals' as dict_name, toUInt64(0) as k,\n     dictGetOrDefault(dict_name, 'd32', k, toDecimal32(42, 4)),\n     dictGetOrDefault(dict_name, 'd64', k, toDecimal64(42, 6)),\n-    dictGetOrDefault(dict_name, 'd128', k, toDecimal128(42, 1));\n+    dictGetOrDefault(dict_name, 'd128', k, toDecimal128(42, 1)),\n+    dictGetOrDefault(dict_name, ('d32', 'd64', 'd128'), k, (toDecimal32(42, 4), toDecimal64(42, 6), toDecimal128(42, 1)));\n \n select 'dictGet', 'cache_decimals' as dict_name, toUInt64(1) as k,\n     dictGet(dict_name, 'd32', k),\n     dictGet(dict_name, 'd64', k),\n-    dictGet(dict_name, 'd128', k);\n+    dictGet(dict_name, 'd128', k),\n+    dictGet(dict_name, ('d32', 'd64', 'd128'), k);\n select 'dictGetOrDefault', 'cache_decimals' as dict_name, toUInt64(1) as k,\n     dictGetOrDefault(dict_name, 'd32', k, toDecimal32(42, 4)),\n     dictGetOrDefault(dict_name, 'd64', k, toDecimal64(42, 6)),\n-    dictGetOrDefault(dict_name, 'd128', k, toDecimal128(42, 1));\n+    dictGetOrDefault(dict_name, 'd128', k, toDecimal128(42, 1)),\n+    dictGetOrDefault(dict_name, ('d32', 'd64', 'd128'), k, (toDecimal32(42, 4), toDecimal64(42, 6), toDecimal128(42, 1)));\n select 'dictGetOrDefault', 'cache_decimals' as dict_name, toUInt64(0) as k,\n     dictGetOrDefault(dict_name, 'd32', k, toDecimal32(42, 4)),\n     dictGetOrDefault(dict_name, 'd64', k, toDecimal64(42, 6)),\n-    dictGetOrDefault(dict_name, 'd128', k, toDecimal128(42, 1));\n+    dictGetOrDefault(dict_name, 'd128', k, toDecimal128(42, 1)),\n+    dictGetOrDefault(dict_name, ('d32', 'd64', 'd128'), k, (toDecimal32(42, 4), toDecimal64(42, 6), toDecimal128(42, 1)));\n \n select 'dictGet', 'complex_hashed_decimals' as dict_name, tuple(toUInt64(1)) as k,\n     dictGet(dict_name, 'd32', k),\n     dictGet(dict_name, 'd64', k),\n-    dictGet(dict_name, 'd128', k);\n+    dictGet(dict_name, 'd128', k),\n+    dictGet(dict_name, ('d32', 'd64', 'd128'), k);\n select 'dictGetOrDefault', 'complex_hashed_decimals' as dict_name, tuple(toUInt64(1)) as k,\n     dictGetOrDefault(dict_name, 'd32', k, toDecimal32(42, 4)),\n     dictGetOrDefault(dict_name, 'd64', k, toDecimal64(42, 6)),\n-    dictGetOrDefault(dict_name, 'd128', k, toDecimal128(42, 1));\n+    dictGetOrDefault(dict_name, 'd128', k, toDecimal128(42, 1)),\n+    dictGetOrDefault(dict_name, ('d32', 'd64', 'd128'), k, (toDecimal32(42, 4), toDecimal64(42, 6), toDecimal128(42, 1)));\n select 'dictGetOrDefault', 'complex_hashed_decimals' as dict_name, tuple(toUInt64(0)) as k,\n     dictGetOrDefault(dict_name, 'd32', k, toDecimal32(42, 4)),\n     dictGetOrDefault(dict_name, 'd64', k, toDecimal64(42, 6)),\n-    dictGetOrDefault(dict_name, 'd128', k, toDecimal128(42, 1));\n+    dictGetOrDefault(dict_name, 'd128', k, toDecimal128(42, 1)),\n+    dictGetOrDefault(dict_name, ('d32', 'd64', 'd128'), k, (toDecimal32(42, 4), toDecimal64(42, 6), toDecimal128(42, 1)));\n \n select 'dictGet', 'complex_cache_decimals' as dict_name, tuple(toUInt64(1)) as k,\n     dictGet(dict_name, 'd32', k),\n     dictGet(dict_name, 'd64', k),\n-    dictGet(dict_name, 'd128', k);\n+    dictGet(dict_name, 'd128', k),\n+    dictGet(dict_name, ('d32', 'd64', 'd128'), k);\n select 'dictGetOrDefault', 'complex_cache_decimals' as dict_name, tuple(toUInt64(1)) as k,\n     dictGetOrDefault(dict_name, 'd32', k, toDecimal32(42, 4)),\n     dictGetOrDefault(dict_name, 'd64', k, toDecimal64(42, 6)),\n-    dictGetOrDefault(dict_name, 'd128', k, toDecimal128(42, 1));\n+    dictGetOrDefault(dict_name, 'd128', k, toDecimal128(42, 1)),\n+    dictGetOrDefault(dict_name, ('d32', 'd64', 'd128'), k, (toDecimal32(42, 4), toDecimal64(42, 6), toDecimal128(42, 1)));\n select 'dictGetOrDefault', 'complex_cache_decimals' as dict_name, tuple(toUInt64(0)) as k,\n     dictGetOrDefault(dict_name, 'd32', k, toDecimal32(42, 4)),\n     dictGetOrDefault(dict_name, 'd64', k, toDecimal64(42, 6)),\n-    dictGetOrDefault(dict_name, 'd128', k, toDecimal128(42, 1));\n+    dictGetOrDefault(dict_name, 'd128', k, toDecimal128(42, 1)),\n+    dictGetOrDefault(dict_name, ('d32', 'd64', 'd128'), k, (toDecimal32(42, 4), toDecimal64(42, 6), toDecimal128(42, 1)));\n \n --\n -- Keep the tables, so that the dictionaries can be reloaded correctly and\ndiff --git a/tests/queries/0_stateless/01053_ssd_dictionary.sql b/tests/queries/0_stateless/01053_ssd_dictionary.sql\nindex cd4ce95802f2..a23ae7e5e967 100644\n--- a/tests/queries/0_stateless/01053_ssd_dictionary.sql\n+++ b/tests/queries/0_stateless/01053_ssd_dictionary.sql\n@@ -1,12 +1,12 @@\n SET send_logs_level = 'none';\n \n-DROP DATABASE IF EXISTS database_for_dict;\n+DROP DATABASE IF EXISTS 01053_db;\n \n-CREATE DATABASE database_for_dict Engine = Ordinary;\n+CREATE DATABASE 01053_db Engine = Ordinary;\n \n-DROP TABLE IF EXISTS database_for_dict.table_for_dict;\n+DROP TABLE IF EXISTS 01053_db.table_for_dict;\n \n-CREATE TABLE database_for_dict.table_for_dict\n+CREATE TABLE 01053_db.table_for_dict\n (\n   id UInt64,\n   a UInt64,\n@@ -16,16 +16,16 @@ CREATE TABLE database_for_dict.table_for_dict\n ENGINE = MergeTree()\n ORDER BY id;\n \n-INSERT INTO database_for_dict.table_for_dict VALUES (1, 100, -100, 'clickhouse'), (2, 3, 4, 'database'), (5, 6, 7, 'columns'), (10, 9, 8, '');\n-INSERT INTO database_for_dict.table_for_dict SELECT number, 0, -1, 'a' FROM system.numbers WHERE number NOT IN (1, 2, 5, 10) LIMIT 370;\n-INSERT INTO database_for_dict.table_for_dict SELECT number, 0, -1, 'b' FROM system.numbers WHERE number NOT IN (1, 2, 5, 10) LIMIT 370, 370;\n-INSERT INTO database_for_dict.table_for_dict SELECT number, 0, -1, 'c' FROM system.numbers WHERE number NOT IN (1, 2, 5, 10) LIMIT 700, 370;\n+INSERT INTO 01053_db.table_for_dict VALUES (1, 100, -100, 'clickhouse'), (2, 3, 4, 'database'), (5, 6, 7, 'columns'), (10, 9, 8, '');\n+INSERT INTO 01053_db.table_for_dict SELECT number, 0, -1, 'a' FROM system.numbers WHERE number NOT IN (1, 2, 5, 10) LIMIT 370;\n+INSERT INTO 01053_db.table_for_dict SELECT number, 0, -1, 'b' FROM system.numbers WHERE number NOT IN (1, 2, 5, 10) LIMIT 370, 370;\n+INSERT INTO 01053_db.table_for_dict SELECT number, 0, -1, 'c' FROM system.numbers WHERE number NOT IN (1, 2, 5, 10) LIMIT 700, 370;\n \n-DROP DICTIONARY IF EXISTS database_for_dict.ssd_dict;\n+DROP DICTIONARY IF EXISTS 01053_db.ssd_dict;\n \n -- FIXME filesystem error: in create_directory: Permission denied [/var/lib/clickhouse]\n -- Probably we need rewrite it to integration test\n-CREATE DICTIONARY database_for_dict.ssd_dict\n+CREATE DICTIONARY 01053_db.ssd_dict\n (\n     id UInt64,\n     a UInt64 DEFAULT 0,\n@@ -33,40 +33,40 @@ CREATE DICTIONARY database_for_dict.ssd_dict\n     c String DEFAULT 'none'\n )\n PRIMARY KEY id\n-SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'table_for_dict' PASSWORD '' DB 'database_for_dict'))\n+SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'table_for_dict' PASSWORD '' DB '01053_db'))\n LIFETIME(MIN 1000 MAX 2000)\n LAYOUT(SSD_CACHE(FILE_SIZE 8192 PATH '/var/lib/clickhouse/clickhouse_dicts/0d'));\n \n SELECT 'TEST_SMALL';\n-SELECT dictGetInt32('database_for_dict.ssd_dict', 'b', toUInt64(1));\n-SELECT dictGetInt32('database_for_dict.ssd_dict', 'b', toUInt64(4));\n-SELECT dictGetUInt64('database_for_dict.ssd_dict', 'a', toUInt64(5));\n-SELECT dictGetUInt64('database_for_dict.ssd_dict', 'a', toUInt64(6));\n-SELECT dictGetString('database_for_dict.ssd_dict', 'c', toUInt64(2));\n-SELECT dictGetString('database_for_dict.ssd_dict', 'c', toUInt64(3));\n+SELECT dictGetInt32('01053_db.ssd_dict', 'b', toUInt64(1));\n+SELECT dictGetInt32('01053_db.ssd_dict', 'b', toUInt64(4));\n+SELECT dictGetUInt64('01053_db.ssd_dict', 'a', toUInt64(5));\n+SELECT dictGetUInt64('01053_db.ssd_dict', 'a', toUInt64(6));\n+SELECT dictGetString('01053_db.ssd_dict', 'c', toUInt64(2));\n+SELECT dictGetString('01053_db.ssd_dict', 'c', toUInt64(3));\n \n-SELECT * FROM database_for_dict.ssd_dict ORDER BY id;\n-DROP DICTIONARY database_for_dict.ssd_dict;\n+SELECT * FROM 01053_db.ssd_dict ORDER BY id;\n+DROP DICTIONARY 01053_db.ssd_dict;\n \n-DROP TABLE IF EXISTS database_for_dict.keys_table;\n+DROP TABLE IF EXISTS 01053_db.keys_table;\n \n-CREATE TABLE database_for_dict.keys_table\n+CREATE TABLE 01053_db.keys_table\n (\n   id UInt64\n )\n ENGINE = StripeLog();\n \n-INSERT INTO database_for_dict.keys_table VALUES (1);\n-INSERT INTO database_for_dict.keys_table SELECT 11 + intHash64(number) % 1200 FROM system.numbers LIMIT 370;\n-INSERT INTO database_for_dict.keys_table VALUES (2);\n-INSERT INTO database_for_dict.keys_table SELECT 11 + intHash64(number) % 1200 FROM system.numbers LIMIT 370, 370;\n-INSERT INTO database_for_dict.keys_table VALUES (5);\n-INSERT INTO database_for_dict.keys_table SELECT 11 + intHash64(number) % 1200 FROM system.numbers LIMIT 700, 370;\n-INSERT INTO database_for_dict.keys_table VALUES (10);\n+INSERT INTO 01053_db.keys_table VALUES (1);\n+INSERT INTO 01053_db.keys_table SELECT 11 + intHash64(number) % 1200 FROM system.numbers LIMIT 370;\n+INSERT INTO 01053_db.keys_table VALUES (2);\n+INSERT INTO 01053_db.keys_table SELECT 11 + intHash64(number) % 1200 FROM system.numbers LIMIT 370, 370;\n+INSERT INTO 01053_db.keys_table VALUES (5);\n+INSERT INTO 01053_db.keys_table SELECT 11 + intHash64(number) % 1200 FROM system.numbers LIMIT 700, 370;\n+INSERT INTO 01053_db.keys_table VALUES (10);\n \n-DROP DICTIONARY IF EXISTS database_for_dict.ssd_dict;\n+DROP DICTIONARY IF EXISTS 01053_db.ssd_dict;\n \n-CREATE DICTIONARY database_for_dict.ssd_dict\n+CREATE DICTIONARY 01053_db.ssd_dict\n (\n     id UInt64,\n     a UInt64 DEFAULT 0,\n@@ -74,86 +74,86 @@ CREATE DICTIONARY database_for_dict.ssd_dict\n     c String DEFAULT 'none'\n )\n PRIMARY KEY id\n-SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'table_for_dict' PASSWORD '' DB 'database_for_dict'))\n+SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'table_for_dict' PASSWORD '' DB '01053_db'))\n LIFETIME(MIN 1000 MAX 2000)\n LAYOUT(SSD_CACHE(FILE_SIZE 8192 PATH '/var/lib/clickhouse/clickhouse_dicts/1d' BLOCK_SIZE 512 WRITE_BUFFER_SIZE 4096 MAX_STORED_KEYS 1000000));\n \n SELECT 'UPDATE DICTIONARY';\n -- 118\n-SELECT sum(dictGetUInt64('database_for_dict.ssd_dict', 'a', toUInt64(id))) FROM database_for_dict.keys_table;\n+SELECT sum(dictGetUInt64('01053_db.ssd_dict', 'a', toUInt64(id))) FROM 01053_db.keys_table;\n \n SELECT 'VALUE FROM DISK';\n -- -100\n-SELECT dictGetInt32('database_for_dict.ssd_dict', 'b', toUInt64(1));\n+SELECT dictGetInt32('01053_db.ssd_dict', 'b', toUInt64(1));\n \n -- 'clickhouse'\n-SELECT dictGetString('database_for_dict.ssd_dict', 'c', toUInt64(1));\n+SELECT dictGetString('01053_db.ssd_dict', 'c', toUInt64(1));\n \n SELECT 'VALUE FROM RAM BUFFER';\n -- 8\n-SELECT dictGetInt32('database_for_dict.ssd_dict', 'b', toUInt64(10));\n+SELECT dictGetInt32('01053_db.ssd_dict', 'b', toUInt64(10));\n \n -- ''\n-SELECT dictGetString('database_for_dict.ssd_dict', 'c', toUInt64(10));\n+SELECT dictGetString('01053_db.ssd_dict', 'c', toUInt64(10));\n \n SELECT 'VALUES FROM DISK AND RAM BUFFER';\n -- 118\n-SELECT sum(dictGetUInt64('database_for_dict.ssd_dict', 'a', toUInt64(id))) FROM database_for_dict.keys_table;\n+SELECT sum(dictGetUInt64('01053_db.ssd_dict', 'a', toUInt64(id))) FROM 01053_db.keys_table;\n \n SELECT 'HAS';\n -- 1006\n-SELECT count() FROM database_for_dict.keys_table WHERE dictHas('database_for_dict.ssd_dict', toUInt64(id));\n+SELECT count() FROM 01053_db.keys_table WHERE dictHas('01053_db.ssd_dict', toUInt64(id));\n \n SELECT 'VALUES NOT FROM TABLE';\n -- 0 -1 none\n-SELECT dictGetUInt64('database_for_dict.ssd_dict', 'a', toUInt64(1000000)), dictGetInt32('database_for_dict.ssd_dict', 'b', toUInt64(1000000)), dictGetString('database_for_dict.ssd_dict', 'c', toUInt64(1000000));\n-SELECT dictGetUInt64('database_for_dict.ssd_dict', 'a', toUInt64(1000000)), dictGetInt32('database_for_dict.ssd_dict', 'b', toUInt64(1000000)), dictGetString('database_for_dict.ssd_dict', 'c', toUInt64(1000000));\n+SELECT dictGetUInt64('01053_db.ssd_dict', 'a', toUInt64(1000000)), dictGetInt32('01053_db.ssd_dict', 'b', toUInt64(1000000)), dictGetString('01053_db.ssd_dict', 'c', toUInt64(1000000));\n+SELECT dictGetUInt64('01053_db.ssd_dict', 'a', toUInt64(1000000)), dictGetInt32('01053_db.ssd_dict', 'b', toUInt64(1000000)), dictGetString('01053_db.ssd_dict', 'c', toUInt64(1000000));\n \n SELECT 'DUPLICATE KEYS';\n-SELECT arrayJoin([1, 2, 3, 3, 2, 1]) AS id, dictGetInt32('database_for_dict.ssd_dict', 'b', toUInt64(id));\n+SELECT arrayJoin([1, 2, 3, 3, 2, 1]) AS id, dictGetInt32('01053_db.ssd_dict', 'b', toUInt64(id));\n --SELECT\n-DROP DICTIONARY IF EXISTS database_for_dict.ssd_dict;\n+DROP DICTIONARY IF EXISTS 01053_db.ssd_dict;\n \n-DROP TABLE IF EXISTS database_for_dict.keys_table;\n+DROP TABLE IF EXISTS 01053_db.keys_table;\n \n-CREATE TABLE database_for_dict.keys_table\n+CREATE TABLE 01053_db.keys_table\n (\n   id UInt64\n )\n ENGINE = MergeTree()\n ORDER BY id;\n \n-INSERT INTO database_for_dict.keys_table VALUES (1);\n-INSERT INTO database_for_dict.keys_table SELECT intHash64(number) FROM system.numbers LIMIT 370;\n-INSERT INTO database_for_dict.keys_table VALUES (2);\n-INSERT INTO database_for_dict.keys_table SELECT intHash64(number) FROM system.numbers LIMIT 370, 370;\n-INSERT INTO database_for_dict.keys_table VALUES (5);\n-INSERT INTO database_for_dict.keys_table SELECT intHash64(number) FROM system.numbers LIMIT 700, 370;\n-INSERT INTO database_for_dict.keys_table VALUES (10);\n+INSERT INTO 01053_db.keys_table VALUES (1);\n+INSERT INTO 01053_db.keys_table SELECT intHash64(number) FROM system.numbers LIMIT 370;\n+INSERT INTO 01053_db.keys_table VALUES (2);\n+INSERT INTO 01053_db.keys_table SELECT intHash64(number) FROM system.numbers LIMIT 370, 370;\n+INSERT INTO 01053_db.keys_table VALUES (5);\n+INSERT INTO 01053_db.keys_table SELECT intHash64(number) FROM system.numbers LIMIT 700, 370;\n+INSERT INTO 01053_db.keys_table VALUES (10);\n \n-OPTIMIZE TABLE database_for_dict.keys_table;\n+OPTIMIZE TABLE 01053_db.keys_table;\n \n-CREATE DICTIONARY database_for_dict.ssd_dict\n+CREATE DICTIONARY 01053_db.ssd_dict\n (\n     id UInt64,\n     a UInt64 DEFAULT 0,\n     b Int32 DEFAULT -1\n )\n PRIMARY KEY id\n-SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'table_for_dict' PASSWORD '' DB 'database_for_dict'))\n+SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'table_for_dict' PASSWORD '' DB '01053_db'))\n LIFETIME(MIN 1000 MAX 2000)\n LAYOUT(SSD_CACHE(FILE_SIZE 8192 PATH '/var/lib/clickhouse/clickhouse_dicts/2d' BLOCK_SIZE 512 WRITE_BUFFER_SIZE 1024 MAX_STORED_KEYS 10));\n \n SELECT 'UPDATE DICTIONARY (MT)';\n -- 118\n-SELECT sum(dictGetUInt64('database_for_dict.ssd_dict', 'a', toUInt64(id))) FROM database_for_dict.keys_table;\n+SELECT sum(dictGetUInt64('01053_db.ssd_dict', 'a', toUInt64(id))) FROM 01053_db.keys_table;\n \n SELECT 'VALUES FROM DISK AND RAM BUFFER (MT)';\n -- 118\n-SELECT sum(dictGetUInt64('database_for_dict.ssd_dict', 'a', toUInt64(id))) FROM database_for_dict.keys_table;\n+SELECT sum(dictGetUInt64('01053_db.ssd_dict', 'a', toUInt64(id))) FROM 01053_db.keys_table;\n \n-DROP DICTIONARY IF EXISTS database_for_dict.ssd_dict;\n+DROP DICTIONARY IF EXISTS 01053_db.ssd_dict;\n \n-DROP TABLE IF EXISTS database_for_dict.table_for_dict;\n+DROP TABLE IF EXISTS 01053_db.table_for_dict;\n \n-DROP DATABASE IF EXISTS database_for_dict;\n+DROP DATABASE IF EXISTS 01053_db;\ndiff --git a/tests/queries/0_stateless/01280_ssd_complex_key_dictionary.sql b/tests/queries/0_stateless/01280_ssd_complex_key_dictionary.sql\nindex 70e1d2bed291..8c304818602a 100644\n--- a/tests/queries/0_stateless/01280_ssd_complex_key_dictionary.sql\n+++ b/tests/queries/0_stateless/01280_ssd_complex_key_dictionary.sql\n@@ -1,12 +1,12 @@\n SET send_logs_level = 'none';\n \n-DROP DATABASE IF EXISTS database_for_dict;\n+DROP DATABASE IF EXISTS 01280_db;\n \n-CREATE DATABASE database_for_dict Engine = Ordinary;\n+CREATE DATABASE 01280_db Engine = Ordinary;\n \n-DROP TABLE IF EXISTS database_for_dict.table_for_dict;\n+DROP TABLE IF EXISTS 01280_db.table_for_dict;\n \n-CREATE TABLE database_for_dict.table_for_dict\n+CREATE TABLE 01280_db.table_for_dict\n (\n   k1 String,\n   k2 Int32,\n@@ -17,16 +17,16 @@ CREATE TABLE database_for_dict.table_for_dict\n ENGINE = MergeTree()\n ORDER BY (k1, k2);\n \n-INSERT INTO database_for_dict.table_for_dict VALUES (toString(1), 3, 100, -100, 'clickhouse'), (toString(2), -1, 3, 4, 'database'), (toString(5), -3, 6, 7, 'columns'), (toString(10), -20, 9, 8, '');\n-INSERT INTO database_for_dict.table_for_dict SELECT toString(number), number + 1, 0, -1, 'a' FROM system.numbers WHERE number NOT IN (1, 2, 5, 10) LIMIT 370;\n-INSERT INTO database_for_dict.table_for_dict SELECT toString(number), number + 10, 0, -1, 'b' FROM system.numbers WHERE number NOT IN (1, 2, 5, 10) LIMIT 370, 370;\n-INSERT INTO database_for_dict.table_for_dict SELECT toString(number), number + 100, 0, -1, 'c' FROM system.numbers WHERE number NOT IN (1, 2, 5, 10) LIMIT 700, 370;\n+INSERT INTO 01280_db.table_for_dict VALUES (toString(1), 3, 100, -100, 'clickhouse'), (toString(2), -1, 3, 4, 'database'), (toString(5), -3, 6, 7, 'columns'), (toString(10), -20, 9, 8, '');\n+INSERT INTO 01280_db.table_for_dict SELECT toString(number), number + 1, 0, -1, 'a' FROM system.numbers WHERE number NOT IN (1, 2, 5, 10) LIMIT 370;\n+INSERT INTO 01280_db.table_for_dict SELECT toString(number), number + 10, 0, -1, 'b' FROM system.numbers WHERE number NOT IN (1, 2, 5, 10) LIMIT 370, 370;\n+INSERT INTO 01280_db.table_for_dict SELECT toString(number), number + 100, 0, -1, 'c' FROM system.numbers WHERE number NOT IN (1, 2, 5, 10) LIMIT 700, 370;\n \n-DROP DICTIONARY IF EXISTS database_for_dict.ssd_dict;\n+DROP DICTIONARY IF EXISTS 01280_db.ssd_dict;\n \n -- FIXME filesystem error: in create_directory: Permission denied [/var/lib/clickhouse]\n -- Probably we need rewrite it to integration test\n-CREATE DICTIONARY database_for_dict.ssd_dict\n+CREATE DICTIONARY 01280_db.ssd_dict\n (\n     k1 String,\n     k2 Int32,\n@@ -35,7 +35,7 @@ CREATE DICTIONARY database_for_dict.ssd_dict\n     c String DEFAULT 'none'\n )\n PRIMARY KEY k1, k2\n-SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'table_for_dict' PASSWORD '' DB 'database_for_dict'))\n+SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'table_for_dict' PASSWORD '' DB '01280_db'))\n LIFETIME(MIN 1000 MAX 2000)\n LAYOUT(COMPLEX_KEY_SSD_CACHE(FILE_SIZE 8192 PATH '/var/lib/clickhouse/clickhouse_dicts/0d'));\n \n@@ -43,52 +43,52 @@ SELECT 'TEST_SMALL';\n SELECT 'VALUE FROM RAM BUFFER';\n \n -- NUMBER_OF_ARGUMENTS_DOESNT_MATCH\n-SELECT dictHas('database_for_dict.ssd_dict', 'a', tuple('1')); -- { serverError 42 }\n+SELECT dictHas('01280_db.ssd_dict', 'a', tuple('1')); -- { serverError 42 }\n \n-SELECT dictGetUInt64('database_for_dict.ssd_dict', 'a', tuple('1', toInt32(3)));\n-SELECT dictGetInt32('database_for_dict.ssd_dict', 'b', tuple('1', toInt32(3)));\n-SELECT dictGetString('database_for_dict.ssd_dict', 'c', tuple('1', toInt32(3)));\n+SELECT dictGetUInt64('01280_db.ssd_dict', 'a', tuple('1', toInt32(3)));\n+SELECT dictGetInt32('01280_db.ssd_dict', 'b', tuple('1', toInt32(3)));\n+SELECT dictGetString('01280_db.ssd_dict', 'c', tuple('1', toInt32(3)));\n \n-SELECT dictGetUInt64('database_for_dict.ssd_dict', 'a', tuple('1', toInt32(3)));\n-SELECT dictGetInt32('database_for_dict.ssd_dict', 'b', tuple('1', toInt32(3)));\n-SELECT dictGetString('database_for_dict.ssd_dict', 'c', tuple('1', toInt32(3)));\n+SELECT dictGetUInt64('01280_db.ssd_dict', 'a', tuple('1', toInt32(3)));\n+SELECT dictGetInt32('01280_db.ssd_dict', 'b', tuple('1', toInt32(3)));\n+SELECT dictGetString('01280_db.ssd_dict', 'c', tuple('1', toInt32(3)));\n \n-SELECT dictGetUInt64('database_for_dict.ssd_dict', 'a', tuple('2', toInt32(-1)));\n-SELECT dictGetInt32('database_for_dict.ssd_dict', 'b', tuple('2', toInt32(-1)));\n-SELECT dictGetString('database_for_dict.ssd_dict', 'c', tuple('2', toInt32(-1)));\n+SELECT dictGetUInt64('01280_db.ssd_dict', 'a', tuple('2', toInt32(-1)));\n+SELECT dictGetInt32('01280_db.ssd_dict', 'b', tuple('2', toInt32(-1)));\n+SELECT dictGetString('01280_db.ssd_dict', 'c', tuple('2', toInt32(-1)));\n \n-SELECT dictGetUInt64('database_for_dict.ssd_dict', 'a', tuple('5', toInt32(-3)));\n-SELECT dictGetInt32('database_for_dict.ssd_dict', 'b', tuple('5', toInt32(-3)));\n-SELECT dictGetString('database_for_dict.ssd_dict', 'c', tuple('5', toInt32(-3)));\n+SELECT dictGetUInt64('01280_db.ssd_dict', 'a', tuple('5', toInt32(-3)));\n+SELECT dictGetInt32('01280_db.ssd_dict', 'b', tuple('5', toInt32(-3)));\n+SELECT dictGetString('01280_db.ssd_dict', 'c', tuple('5', toInt32(-3)));\n \n-SELECT dictGetUInt64('database_for_dict.ssd_dict', 'a', tuple('10', toInt32(-20)));\n-SELECT dictGetInt32('database_for_dict.ssd_dict', 'b', tuple('10', toInt32(-20)));\n-SELECT dictGetString('database_for_dict.ssd_dict', 'c', tuple('10', toInt32(-20)));\n+SELECT dictGetUInt64('01280_db.ssd_dict', 'a', tuple('10', toInt32(-20)));\n+SELECT dictGetInt32('01280_db.ssd_dict', 'b', tuple('10', toInt32(-20)));\n+SELECT dictGetString('01280_db.ssd_dict', 'c', tuple('10', toInt32(-20)));\n \n-SELECT dictGetUInt64('database_for_dict.ssd_dict', 'a', tuple(toInt32(3))); --{serverError 53}\n+SELECT dictGetUInt64('01280_db.ssd_dict', 'a', tuple(toInt32(3))); --{serverError 53}\n \n-DROP DICTIONARY database_for_dict.ssd_dict;\n+DROP DICTIONARY 01280_db.ssd_dict;\n \n-DROP TABLE IF EXISTS database_for_dict.keys_table;\n+DROP TABLE IF EXISTS 01280_db.keys_table;\n \n-CREATE TABLE database_for_dict.keys_table\n+CREATE TABLE 01280_db.keys_table\n (\n     k1 String,\n     k2 Int32\n )\n ENGINE = StripeLog();\n \n-INSERT INTO database_for_dict.keys_table VALUES ('1', 3);\n-INSERT INTO database_for_dict.keys_table SELECT toString(intHash64(number + 1) % 1200), 11 + intHash64(number) % 1200 FROM system.numbers LIMIT 370;\n-INSERT INTO database_for_dict.keys_table VALUES ('2', -1);\n-INSERT INTO database_for_dict.keys_table SELECT toString(intHash64(number + 1) % 1200), 11 + intHash64(number) % 1200 FROM system.numbers LIMIT 370, 370;\n-INSERT INTO database_for_dict.keys_table VALUES ('5', -3);\n-INSERT INTO database_for_dict.keys_table SELECT toString(intHash64(number + 1) % 1200), 11 + intHash64(number) % 1200 FROM system.numbers LIMIT 700, 370;\n-INSERT INTO database_for_dict.keys_table VALUES ('10', -20);\n+INSERT INTO 01280_db.keys_table VALUES ('1', 3);\n+INSERT INTO 01280_db.keys_table SELECT toString(intHash64(number + 1) % 1200), 11 + intHash64(number) % 1200 FROM system.numbers LIMIT 370;\n+INSERT INTO 01280_db.keys_table VALUES ('2', -1);\n+INSERT INTO 01280_db.keys_table SELECT toString(intHash64(number + 1) % 1200), 11 + intHash64(number) % 1200 FROM system.numbers LIMIT 370, 370;\n+INSERT INTO 01280_db.keys_table VALUES ('5', -3);\n+INSERT INTO 01280_db.keys_table SELECT toString(intHash64(number + 1) % 1200), 11 + intHash64(number) % 1200 FROM system.numbers LIMIT 700, 370;\n+INSERT INTO 01280_db.keys_table VALUES ('10', -20);\n \n-DROP DICTIONARY IF EXISTS database_for_dict.ssd_dict;\n+DROP DICTIONARY IF EXISTS 01280_db.ssd_dict;\n \n-CREATE DICTIONARY database_for_dict.ssd_dict\n+CREATE DICTIONARY 01280_db.ssd_dict\n (\n     k1 String,\n     k2 Int32,\n@@ -97,43 +97,43 @@ CREATE DICTIONARY database_for_dict.ssd_dict\n     c String DEFAULT 'none'\n )\n PRIMARY KEY k1, k2\n-SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'table_for_dict' PASSWORD '' DB 'database_for_dict'))\n+SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'table_for_dict' PASSWORD '' DB '01280_db'))\n LIFETIME(MIN 1000 MAX 2000)\n LAYOUT(COMPLEX_KEY_SSD_CACHE(FILE_SIZE 8192 PATH '/var/lib/clickhouse/clickhouse_dicts/1d' BLOCK_SIZE 512 WRITE_BUFFER_SIZE 4096 MAX_STORED_KEYS 1000000));\n \n SELECT 'UPDATE DICTIONARY';\n -- 118\n-SELECT sum(dictGetUInt64('database_for_dict.ssd_dict', 'a', (k1, k2))) FROM database_for_dict.keys_table;\n+SELECT sum(dictGetUInt64('01280_db.ssd_dict', 'a', (k1, k2))) FROM 01280_db.keys_table;\n \n SELECT 'VALUE FROM DISK';\n -- -100\n-SELECT dictGetInt32('database_for_dict.ssd_dict', 'b', ('1', toInt32(3)));\n+SELECT dictGetInt32('01280_db.ssd_dict', 'b', ('1', toInt32(3)));\n \n -- 'clickhouse'\n-SELECT dictGetString('database_for_dict.ssd_dict', 'c', ('1', toInt32(3)));\n+SELECT dictGetString('01280_db.ssd_dict', 'c', ('1', toInt32(3)));\n \n SELECT 'VALUE FROM RAM BUFFER';\n -- 8\n-SELECT dictGetInt32('database_for_dict.ssd_dict', 'b', ('10', toInt32(-20)));\n+SELECT dictGetInt32('01280_db.ssd_dict', 'b', ('10', toInt32(-20)));\n \n -- ''\n-SELECT dictGetString('database_for_dict.ssd_dict', 'c', ('10', toInt32(-20)));\n+SELECT dictGetString('01280_db.ssd_dict', 'c', ('10', toInt32(-20)));\n \n SELECT 'VALUES FROM DISK AND RAM BUFFER';\n -- 118\n-SELECT sum(dictGetUInt64('database_for_dict.ssd_dict', 'a', (k1, k2))) FROM database_for_dict.keys_table;\n+SELECT sum(dictGetUInt64('01280_db.ssd_dict', 'a', (k1, k2))) FROM 01280_db.keys_table;\n \n SELECT 'HAS';\n -- 6\n-SELECT count() FROM database_for_dict.keys_table WHERE dictHas('database_for_dict.ssd_dict', (k1, k2));\n+SELECT count() FROM 01280_db.keys_table WHERE dictHas('01280_db.ssd_dict', (k1, k2));\n \n SELECT 'VALUES NOT FROM TABLE';\n -- 0 -1 none\n-SELECT dictGetUInt64('database_for_dict.ssd_dict', 'a', ('unknown', toInt32(0))), dictGetInt32('database_for_dict.ssd_dict', 'b', ('unknown', toInt32(0))), dictGetString('database_for_dict.ssd_dict', 'c', ('unknown', toInt32(0)));\n-SELECT dictGetUInt64('database_for_dict.ssd_dict', 'a', ('unknown', toInt32(0))), dictGetInt32('database_for_dict.ssd_dict', 'b', ('unknown', toInt32(0))), dictGetString('database_for_dict.ssd_dict', 'c', ('unknown', toInt32(0)));\n+SELECT dictGetUInt64('01280_db.ssd_dict', 'a', ('unknown', toInt32(0))), dictGetInt32('01280_db.ssd_dict', 'b', ('unknown', toInt32(0))), dictGetString('01280_db.ssd_dict', 'c', ('unknown', toInt32(0)));\n+SELECT dictGetUInt64('01280_db.ssd_dict', 'a', ('unknown', toInt32(0))), dictGetInt32('01280_db.ssd_dict', 'b', ('unknown', toInt32(0))), dictGetString('01280_db.ssd_dict', 'c', ('unknown', toInt32(0)));\n \n SELECT 'DUPLICATE KEYS';\n-SELECT arrayJoin([('1', toInt32(3)), ('2', toInt32(-1)), ('', toInt32(0)), ('', toInt32(0)), ('2', toInt32(-1)), ('1', toInt32(3))]) AS keys, dictGetInt32('database_for_dict.ssd_dict', 'b', keys);\n+SELECT arrayJoin([('1', toInt32(3)), ('2', toInt32(-1)), ('', toInt32(0)), ('', toInt32(0)), ('2', toInt32(-1)), ('1', toInt32(3))]) AS keys, dictGetInt32('01280_db.ssd_dict', 'b', keys);\n \n DROP DICTIONARY IF EXISTS database_for_dict.ssd_dict;\n \ndiff --git a/tests/queries/0_stateless/01681_cache_dictionary_simple_key.reference b/tests/queries/0_stateless/01681_cache_dictionary_simple_key.reference\nnew file mode 100644\nindex 000000000000..b3258e36f745\n--- /dev/null\n+++ b/tests/queries/0_stateless/01681_cache_dictionary_simple_key.reference\n@@ -0,0 +1,66 @@\n+Dictionary cache_dictionary_simple_key_simple_attributes\n+dictGet existing value\n+value_0\tvalue_second_0\n+value_1\tvalue_second_1\n+value_2\tvalue_second_2\n+dictGet with non existing value\n+value_0\tvalue_second_0\n+value_1\tvalue_second_1\n+value_2\tvalue_second_2\n+value_first_default\tvalue_second_default\n+dictGetOrDefault existing value\n+value_0\tvalue_second_0\n+value_1\tvalue_second_1\n+value_2\tvalue_second_2\n+dictGetOrDefault non existing value\n+value_0\tvalue_second_0\n+value_1\tvalue_second_1\n+value_2\tvalue_second_2\n+default\tdefault\n+dictHas\n+1\n+1\n+1\n+0\n+select all values as input stream\n+0\tvalue_0\tvalue_second_0\n+1\tvalue_1\tvalue_second_1\n+2\tvalue_2\tvalue_second_2\n+Dictionary cache_dictionary_simple_key_complex_attributes\n+dictGet existing value\n+value_0\tvalue_second_0\n+value_1\t\\N\n+value_2\tvalue_second_2\n+dictGet with non existing value\n+value_0\tvalue_second_0\n+value_1\t\\N\n+value_2\tvalue_second_2\n+value_first_default\tvalue_second_default\n+dictGetOrDefault existing value\n+value_0\tvalue_second_0\n+value_1\t\\N\n+value_2\tvalue_second_2\n+dictGetOrDefault non existing value\n+value_0\tvalue_second_0\n+value_1\t\\N\n+value_2\tvalue_second_2\n+default\tdefault\n+dictHas\n+1\n+1\n+1\n+0\n+select all values as input stream\n+0\tvalue_0\tvalue_second_0\n+1\tvalue_1\t\\N\n+2\tvalue_2\tvalue_second_2\n+Dictionary cache_dictionary_simple_key_hierarchy\n+dictGet\n+0\n+0\n+1\n+1\n+2\n+dictGetHierarchy\n+[1]\n+[4,2,1]\ndiff --git a/tests/queries/0_stateless/01681_cache_dictionary_simple_key.sql b/tests/queries/0_stateless/01681_cache_dictionary_simple_key.sql\nnew file mode 100644\nindex 000000000000..ee2cde963d77\n--- /dev/null\n+++ b/tests/queries/0_stateless/01681_cache_dictionary_simple_key.sql\n@@ -0,0 +1,123 @@\n+DROP DATABASE IF EXISTS 01681_database_for_cache_dictionary;\n+CREATE DATABASE 01681_database_for_cache_dictionary;\n+\n+CREATE TABLE 01681_database_for_cache_dictionary.simple_key_simple_attributes_source_table\n+(\n+   id UInt64,\n+   value_first String,\n+   value_second String\n+)\n+ENGINE = TinyLog;\n+\n+INSERT INTO 01681_database_for_cache_dictionary.simple_key_simple_attributes_source_table VALUES(0, 'value_0', 'value_second_0');\n+INSERT INTO 01681_database_for_cache_dictionary.simple_key_simple_attributes_source_table VALUES(1, 'value_1', 'value_second_1');\n+INSERT INTO 01681_database_for_cache_dictionary.simple_key_simple_attributes_source_table VALUES(2, 'value_2', 'value_second_2');\n+\n+CREATE DICTIONARY 01681_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes\n+(\n+   id UInt64,\n+   value_first String DEFAULT 'value_first_default',\n+   value_second String DEFAULT 'value_second_default'\n+)\n+PRIMARY KEY id\n+SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'simple_key_simple_attributes_source_table'))\n+LIFETIME(MIN 1 MAX 1000)\n+LAYOUT(CACHE(SIZE_IN_CELLS 10));\n+\n+SELECT 'Dictionary cache_dictionary_simple_key_simple_attributes';\n+SELECT 'dictGet existing value';\n+SELECT dictGet('01681_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes', 'value_first', number) as value_first,\n+    dictGet('01681_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes', 'value_second', number) as value_second FROM system.numbers LIMIT 3;\n+SELECT 'dictGet with non existing value';\n+SELECT dictGet('01681_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes', 'value_first', number) as value_first,\n+    dictGet('01681_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes', 'value_second', number) as value_second FROM system.numbers LIMIT 4;\n+SELECT 'dictGetOrDefault existing value';\n+SELECT dictGetOrDefault('01681_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes', 'value_first', number, toString('default')) as value_first,\n+    dictGetOrDefault('01681_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes', 'value_second', number, toString('default')) as value_second FROM system.numbers LIMIT 3;\n+SELECT 'dictGetOrDefault non existing value';\n+SELECT dictGetOrDefault('01681_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes', 'value_first', number, toString('default')) as value_first,\n+    dictGetOrDefault('01681_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes', 'value_second', number, toString('default')) as value_second FROM system.numbers LIMIT 4;\n+SELECT 'dictHas';\n+SELECT dictHas('01681_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes', number) FROM system.numbers LIMIT 4;\n+SELECT 'select all values as input stream';\n+SELECT * FROM 01681_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes;\n+\n+DROP DICTIONARY 01681_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes;\n+DROP TABLE 01681_database_for_cache_dictionary.simple_key_simple_attributes_source_table;\n+\n+CREATE TABLE 01681_database_for_cache_dictionary.simple_key_complex_attributes_source_table\n+(\n+   id UInt64,\n+   value_first String,\n+   value_second Nullable(String)\n+)\n+ENGINE = TinyLog;\n+\n+INSERT INTO 01681_database_for_cache_dictionary.simple_key_complex_attributes_source_table VALUES(0, 'value_0', 'value_second_0');\n+INSERT INTO 01681_database_for_cache_dictionary.simple_key_complex_attributes_source_table VALUES(1, 'value_1', NULL);\n+INSERT INTO 01681_database_for_cache_dictionary.simple_key_complex_attributes_source_table VALUES(2, 'value_2', 'value_second_2');\n+\n+CREATE DICTIONARY 01681_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes\n+(\n+   id UInt64,\n+   value_first String DEFAULT 'value_first_default',\n+   value_second Nullable(String) DEFAULT 'value_second_default'\n+)\n+PRIMARY KEY id\n+SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'simple_key_complex_attributes_source_table'))\n+LIFETIME(MIN 1 MAX 1000)\n+LAYOUT(CACHE(SIZE_IN_CELLS 10));\n+\n+SELECT 'Dictionary cache_dictionary_simple_key_complex_attributes';\n+SELECT 'dictGet existing value';\n+SELECT dictGet('01681_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes', 'value_first', number) as value_first,\n+    dictGet('01681_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes', 'value_second', number) as value_second FROM system.numbers LIMIT 3;\n+SELECT 'dictGet with non existing value';\n+SELECT dictGet('01681_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes', 'value_first', number) as value_first,\n+    dictGet('01681_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes', 'value_second', number) as value_second FROM system.numbers LIMIT 4;\n+SELECT 'dictGetOrDefault existing value';\n+SELECT dictGetOrDefault('01681_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes', 'value_first', number, toString('default')) as value_first,\n+    dictGetOrDefault('01681_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes', 'value_second', number, toString('default')) as value_second FROM system.numbers LIMIT 3;\n+SELECT 'dictGetOrDefault non existing value';\n+SELECT dictGetOrDefault('01681_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes', 'value_first', number, toString('default')) as value_first,\n+    dictGetOrDefault('01681_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes', 'value_second', number, toString('default')) as value_second FROM system.numbers LIMIT 4;\n+SELECT 'dictHas';\n+SELECT dictHas('01681_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes', number) FROM system.numbers LIMIT 4;\n+SELECT 'select all values as input stream';\n+SELECT * FROM 01681_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes;\n+\n+DROP DICTIONARY 01681_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes;\n+DROP TABLE 01681_database_for_cache_dictionary.simple_key_complex_attributes_source_table;\n+\n+CREATE TABLE 01681_database_for_cache_dictionary.simple_key_hierarchy_table\n+(\n+    id UInt64,\n+    parent_id UInt64\n+) ENGINE = TinyLog();\n+\n+INSERT INTO 01681_database_for_cache_dictionary.simple_key_hierarchy_table VALUES (1, 0);\n+INSERT INTO 01681_database_for_cache_dictionary.simple_key_hierarchy_table VALUES (2, 1);\n+INSERT INTO 01681_database_for_cache_dictionary.simple_key_hierarchy_table VALUES (3, 1);\n+INSERT INTO 01681_database_for_cache_dictionary.simple_key_hierarchy_table VALUES (4, 2);\n+\n+CREATE DICTIONARY 01681_database_for_cache_dictionary.cache_dictionary_simple_key_hierarchy\n+(\n+   id UInt64,\n+   parent_id UInt64 HIERARCHICAL\n+)\n+PRIMARY KEY id\n+SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'simple_key_hierarchy_table'))\n+LIFETIME(MIN 1 MAX 1000)\n+LAYOUT(CACHE(SIZE_IN_CELLS 10));\n+\n+SELECT 'Dictionary cache_dictionary_simple_key_hierarchy';\n+SELECT 'dictGet';\n+SELECT dictGet('01681_database_for_cache_dictionary.cache_dictionary_simple_key_hierarchy', 'parent_id', number) FROM system.numbers LIMIT 5;\n+SELECT 'dictGetHierarchy';\n+SELECT dictGetHierarchy('01681_database_for_cache_dictionary.cache_dictionary_simple_key_hierarchy', toUInt64(1));\n+SELECT dictGetHierarchy('01681_database_for_cache_dictionary.cache_dictionary_simple_key_hierarchy', toUInt64(4));\n+\n+DROP DICTIONARY 01681_database_for_cache_dictionary.cache_dictionary_simple_key_hierarchy;\n+DROP TABLE 01681_database_for_cache_dictionary.simple_key_hierarchy_table;\n+\n+DROP DATABASE 01681_database_for_cache_dictionary;\ndiff --git a/tests/queries/0_stateless/01682_cache_dictionary_complex_key.reference b/tests/queries/0_stateless/01682_cache_dictionary_complex_key.reference\nnew file mode 100644\nindex 000000000000..a0f99ca13cc9\n--- /dev/null\n+++ b/tests/queries/0_stateless/01682_cache_dictionary_complex_key.reference\n@@ -0,0 +1,56 @@\n+Dictionary cache_dictionary_complex_key_simple_attributes\n+dictGet existing value\n+value_0\tvalue_second_0\n+value_1\tvalue_second_1\n+value_2\tvalue_second_2\n+dictGet with non existing value\n+value_0\tvalue_second_0\n+value_1\tvalue_second_1\n+value_2\tvalue_second_2\n+value_first_default\tvalue_second_default\n+dictGetOrDefault existing value\n+value_0\tvalue_second_0\n+value_1\tvalue_second_1\n+value_2\tvalue_second_2\n+dictGetOrDefault non existing value\n+value_0\tvalue_second_0\n+value_1\tvalue_second_1\n+value_2\tvalue_second_2\n+default\tdefault\n+dictHas\n+1\n+1\n+1\n+0\n+select all values as input stream\n+0\tid_key_0\tvalue_0\tvalue_second_0\n+1\tid_key_1\tvalue_1\tvalue_second_1\n+2\tid_key_2\tvalue_2\tvalue_second_2\n+Dictionary cache_dictionary_complex_key_complex_attributes\n+dictGet existing value\n+value_0\tvalue_second_0\n+value_1\t\\N\n+value_2\tvalue_second_2\n+dictGet with non existing value\n+value_0\tvalue_second_0\n+value_1\t\\N\n+value_2\tvalue_second_2\n+value_first_default\tvalue_second_default\n+dictGetOrDefault existing value\n+value_0\tvalue_second_0\n+value_1\t\\N\n+value_2\tvalue_second_2\n+dictGetOrDefault non existing value\n+value_0\tvalue_second_0\n+value_1\t\\N\n+value_2\tvalue_second_2\n+default\tdefault\n+dictHas\n+1\n+1\n+1\n+0\n+select all values as input stream\n+0\tid_key_0\tvalue_0\tvalue_second_0\n+1\tid_key_1\tvalue_1\t\\N\n+2\tid_key_2\tvalue_2\tvalue_second_2\ndiff --git a/tests/queries/0_stateless/01682_cache_dictionary_complex_key.sql b/tests/queries/0_stateless/01682_cache_dictionary_complex_key.sql\nnew file mode 100644\nindex 000000000000..65c56090c47f\n--- /dev/null\n+++ b/tests/queries/0_stateless/01682_cache_dictionary_complex_key.sql\n@@ -0,0 +1,97 @@\n+DROP DATABASE IF EXISTS 01682_database_for_cache_dictionary;\n+CREATE DATABASE 01682_database_for_cache_dictionary;\n+\n+CREATE TABLE 01682_database_for_cache_dictionary.complex_key_simple_attributes_source_table\n+(\n+   id UInt64,\n+   id_key String,\n+   value_first String,\n+   value_second String\n+)\n+ENGINE = TinyLog;\n+\n+INSERT INTO 01682_database_for_cache_dictionary.complex_key_simple_attributes_source_table VALUES(0, 'id_key_0', 'value_0', 'value_second_0');\n+INSERT INTO 01682_database_for_cache_dictionary.complex_key_simple_attributes_source_table VALUES(1, 'id_key_1', 'value_1', 'value_second_1');\n+INSERT INTO 01682_database_for_cache_dictionary.complex_key_simple_attributes_source_table VALUES(2, 'id_key_2', 'value_2', 'value_second_2');\n+\n+CREATE DICTIONARY 01682_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes\n+(\n+   id UInt64,\n+   id_key String,\n+   value_first String DEFAULT 'value_first_default',\n+   value_second String DEFAULT 'value_second_default'\n+)\n+PRIMARY KEY id, id_key\n+SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'complex_key_simple_attributes_source_table' DB '01682_database_for_cache_dictionary'))\n+LIFETIME(MIN 1 MAX 1000)\n+LAYOUT(COMPLEX_KEY_CACHE(SIZE_IN_CELLS 10));\n+\n+SELECT 'Dictionary cache_dictionary_complex_key_simple_attributes';\n+SELECT 'dictGet existing value';\n+SELECT dictGet('01682_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes', 'value_first', (number, concat('id_key_', toString(number)))) as value_first,\n+    dictGet('01682_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes', 'value_second', (number, concat('id_key_', toString(number)))) as value_second FROM system.numbers LIMIT 3;\n+SELECT 'dictGet with non existing value';\n+SELECT dictGet('01682_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes', 'value_first', (number, concat('id_key_', toString(number)))) as value_first,\n+    dictGet('01682_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes', 'value_second', (number, concat('id_key_', toString(number)))) as value_second FROM system.numbers LIMIT 4;\n+SELECT 'dictGetOrDefault existing value';\n+SELECT dictGetOrDefault('01682_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes', 'value_first', (number, concat('id_key_', toString(number))), toString('default')) as value_first,\n+    dictGetOrDefault('01682_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes', 'value_second', (number, concat('id_key_', toString(number))), toString('default')) as value_second FROM system.numbers LIMIT 3;\n+SELECT 'dictGetOrDefault non existing value';\n+SELECT dictGetOrDefault('01682_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes', 'value_first', (number, concat('id_key_', toString(number))), toString('default')) as value_first,\n+    dictGetOrDefault('01682_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes', 'value_second', (number, concat('id_key_', toString(number))), toString('default')) as value_second FROM system.numbers LIMIT 4;\n+SELECT 'dictHas';\n+SELECT dictHas('01682_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes', (number, concat('id_key_', toString(number)))) FROM system.numbers LIMIT 4;\n+SELECT 'select all values as input stream';\n+SELECT * FROM 01682_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes;\n+\n+DROP DICTIONARY 01682_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes;\n+DROP TABLE 01682_database_for_cache_dictionary.complex_key_simple_attributes_source_table;\n+\n+CREATE TABLE 01682_database_for_cache_dictionary.complex_key_complex_attributes_source_table\n+(\n+   id UInt64,\n+   id_key String,\n+   value_first String,\n+   value_second Nullable(String)\n+)\n+ENGINE = TinyLog;\n+\n+INSERT INTO 01682_database_for_cache_dictionary.complex_key_complex_attributes_source_table VALUES(0, 'id_key_0', 'value_0', 'value_second_0');\n+INSERT INTO 01682_database_for_cache_dictionary.complex_key_complex_attributes_source_table VALUES(1, 'id_key_1', 'value_1', NULL);\n+INSERT INTO 01682_database_for_cache_dictionary.complex_key_complex_attributes_source_table VALUES(2, 'id_key_2', 'value_2', 'value_second_2');\n+\n+CREATE DICTIONARY 01682_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes\n+(\n+    id UInt64,\n+    id_key String,\n+\n+    value_first String DEFAULT 'value_first_default',\n+    value_second Nullable(String) DEFAULT 'value_second_default'\n+)\n+PRIMARY KEY id, id_key\n+SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'complex_key_complex_attributes_source_table' DB '01682_database_for_cache_dictionary'))\n+LIFETIME(MIN 1 MAX 1000)\n+LAYOUT(COMPLEX_KEY_CACHE(SIZE_IN_CELLS 10));\n+\n+SELECT 'Dictionary cache_dictionary_complex_key_complex_attributes';\n+SELECT 'dictGet existing value';\n+SELECT dictGet('01682_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes', 'value_first', (number, concat('id_key_', toString(number)))) as value_first,\n+    dictGet('01682_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes', 'value_second', (number, concat('id_key_', toString(number)))) as value_second FROM system.numbers LIMIT 3;\n+SELECT 'dictGet with non existing value';\n+SELECT dictGet('01682_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes', 'value_first', (number, concat('id_key_', toString(number)))) as value_first,\n+    dictGet('01682_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes', 'value_second', (number, concat('id_key_', toString(number)))) as value_second FROM system.numbers LIMIT 4;\n+SELECT 'dictGetOrDefault existing value';\n+SELECT dictGetOrDefault('01682_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes', 'value_first', (number, concat('id_key_', toString(number))), toString('default')) as value_first,\n+    dictGetOrDefault('01682_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes', 'value_second', (number, concat('id_key_', toString(number))), toString('default')) as value_second FROM system.numbers LIMIT 3;\n+SELECT 'dictGetOrDefault non existing value';\n+SELECT dictGetOrDefault('01682_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes', 'value_first', (number, concat('id_key_', toString(number))), toString('default')) as value_first,\n+    dictGetOrDefault('01682_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes', 'value_second', (number, concat('id_key_', toString(number))), toString('default')) as value_second FROM system.numbers LIMIT 4;\n+SELECT 'dictHas';\n+SELECT dictHas('01682_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes', (number, concat('id_key_', toString(number)))) FROM system.numbers LIMIT 4;\n+SELECT 'select all values as input stream';\n+SELECT * FROM 01682_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes;\n+\n+DROP DICTIONARY 01682_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes;\n+DROP TABLE 01682_database_for_cache_dictionary.complex_key_complex_attributes_source_table;\n+\n+DROP DATABASE 01682_database_for_cache_dictionary;\ndiff --git a/tests/queries/0_stateless/01683_flat_dictionary.reference b/tests/queries/0_stateless/01683_flat_dictionary.reference\nnew file mode 100644\nindex 000000000000..b2a24a017381\n--- /dev/null\n+++ b/tests/queries/0_stateless/01683_flat_dictionary.reference\n@@ -0,0 +1,58 @@\n+Dictionary flat_dictionary_simple_key_simple_attributes\n+dictGet existing value\n+value_0\tvalue_second_0\n+value_1\tvalue_second_1\n+value_2\tvalue_second_2\n+dictGet with non existing value\n+value_0\tvalue_second_0\n+value_1\tvalue_second_1\n+value_2\tvalue_second_2\n+value_first_default\tvalue_second_default\n+dictGetOrDefault existing value\n+value_0\tvalue_second_0\n+value_1\tvalue_second_1\n+value_2\tvalue_second_2\n+dictGetOrDefault non existing value\n+value_0\tvalue_second_0\n+value_1\tvalue_second_1\n+value_2\tvalue_second_2\n+default\tdefault\n+dictHas\n+1\n+1\n+1\n+0\n+Dictionary flat_dictionary_simple_key_complex_attributes\n+dictGet existing value\n+value_0\tvalue_second_0\n+value_1\t\\N\n+value_2\tvalue_second_2\n+dictGet with non existing value\n+value_0\tvalue_second_0\n+value_1\t\\N\n+value_2\tvalue_second_2\n+value_first_default\tvalue_second_default\n+dictGetOrDefault existing value\n+value_0\tvalue_second_0\n+value_1\t\\N\n+value_2\tvalue_second_2\n+dictGetOrDefault non existing value\n+value_0\tvalue_second_0\n+value_1\t\\N\n+value_2\tvalue_second_2\n+default\tdefault\n+dictHas\n+1\n+1\n+1\n+0\n+Dictionary flat_dictionary_simple_key_hierarchy\n+dictGet\n+0\n+0\n+1\n+1\n+2\n+dictGetHierarchy\n+[1]\n+[4,2,1]\ndiff --git a/tests/queries/0_stateless/01683_flat_dictionary.sql b/tests/queries/0_stateless/01683_flat_dictionary.sql\nnew file mode 100644\nindex 000000000000..c9f864c6a168\n--- /dev/null\n+++ b/tests/queries/0_stateless/01683_flat_dictionary.sql\n@@ -0,0 +1,119 @@\n+DROP DATABASE IF EXISTS 01681_database_for_flat_dictionary;\n+CREATE DATABASE 01681_database_for_flat_dictionary;\n+\n+CREATE TABLE 01681_database_for_flat_dictionary.simple_key_simple_attributes_source_table\n+(\n+   id UInt64,\n+   value_first String,\n+   value_second String\n+)\n+ENGINE = TinyLog;\n+\n+INSERT INTO 01681_database_for_flat_dictionary.simple_key_simple_attributes_source_table VALUES(0, 'value_0', 'value_second_0');\n+INSERT INTO 01681_database_for_flat_dictionary.simple_key_simple_attributes_source_table VALUES(1, 'value_1', 'value_second_1');\n+INSERT INTO 01681_database_for_flat_dictionary.simple_key_simple_attributes_source_table VALUES(2, 'value_2', 'value_second_2');\n+\n+CREATE DICTIONARY 01681_database_for_flat_dictionary.flat_dictionary_simple_key_simple_attributes\n+(\n+   id UInt64,\n+   value_first String DEFAULT 'value_first_default',\n+   value_second String DEFAULT 'value_second_default'\n+)\n+PRIMARY KEY id\n+SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'simple_key_simple_attributes_source_table'))\n+LIFETIME(MIN 1 MAX 1000)\n+LAYOUT(FLAT());\n+\n+SELECT 'Dictionary flat_dictionary_simple_key_simple_attributes';\n+SELECT 'dictGet existing value';\n+SELECT dictGet('01681_database_for_flat_dictionary.flat_dictionary_simple_key_simple_attributes', 'value_first', number) as value_first,\n+    dictGet('01681_database_for_flat_dictionary.flat_dictionary_simple_key_simple_attributes', 'value_second', number) as value_second FROM system.numbers LIMIT 3;\n+SELECT 'dictGet with non existing value';\n+SELECT dictGet('01681_database_for_flat_dictionary.flat_dictionary_simple_key_simple_attributes', 'value_first', number) as value_first,\n+    dictGet('01681_database_for_flat_dictionary.flat_dictionary_simple_key_simple_attributes', 'value_second', number) as value_second FROM system.numbers LIMIT 4;\n+SELECT 'dictGetOrDefault existing value';\n+SELECT dictGetOrDefault('01681_database_for_flat_dictionary.flat_dictionary_simple_key_simple_attributes', 'value_first', number, toString('default')) as value_first,\n+    dictGetOrDefault('01681_database_for_flat_dictionary.flat_dictionary_simple_key_simple_attributes', 'value_second', number, toString('default')) as value_second FROM system.numbers LIMIT 3;\n+SELECT 'dictGetOrDefault non existing value';\n+SELECT dictGetOrDefault('01681_database_for_flat_dictionary.flat_dictionary_simple_key_simple_attributes', 'value_first', number, toString('default')) as value_first,\n+    dictGetOrDefault('01681_database_for_flat_dictionary.flat_dictionary_simple_key_simple_attributes', 'value_second', number, toString('default')) as value_second FROM system.numbers LIMIT 4;\n+SELECT 'dictHas';\n+SELECT dictHas('01681_database_for_flat_dictionary.flat_dictionary_simple_key_simple_attributes', number) FROM system.numbers LIMIT 4;\n+\n+DROP DICTIONARY 01681_database_for_flat_dictionary.flat_dictionary_simple_key_simple_attributes;\n+DROP TABLE 01681_database_for_flat_dictionary.simple_key_simple_attributes_source_table;\n+\n+CREATE TABLE 01681_database_for_flat_dictionary.simple_key_complex_attributes_source_table\n+(\n+   id UInt64,\n+   value_first String,\n+   value_second Nullable(String)\n+)\n+ENGINE = TinyLog;\n+\n+INSERT INTO 01681_database_for_flat_dictionary.simple_key_complex_attributes_source_table VALUES(0, 'value_0', 'value_second_0');\n+INSERT INTO 01681_database_for_flat_dictionary.simple_key_complex_attributes_source_table VALUES(1, 'value_1', NULL);\n+INSERT INTO 01681_database_for_flat_dictionary.simple_key_complex_attributes_source_table VALUES(2, 'value_2', 'value_second_2');\n+\n+CREATE DICTIONARY 01681_database_for_flat_dictionary.flat_dictionary_simple_key_complex_attributes\n+(\n+   id UInt64,\n+   value_first String DEFAULT 'value_first_default',\n+   value_second Nullable(String) DEFAULT 'value_second_default'\n+)\n+PRIMARY KEY id\n+SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'simple_key_complex_attributes_source_table'))\n+LIFETIME(MIN 1 MAX 1000)\n+LAYOUT(FLAT());\n+\n+SELECT 'Dictionary flat_dictionary_simple_key_complex_attributes';\n+SELECT 'dictGet existing value';\n+SELECT dictGet('01681_database_for_flat_dictionary.flat_dictionary_simple_key_complex_attributes', 'value_first', number) as value_first,\n+    dictGet('01681_database_for_flat_dictionary.flat_dictionary_simple_key_complex_attributes', 'value_second', number) as value_second FROM system.numbers LIMIT 3;\n+SELECT 'dictGet with non existing value';\n+SELECT dictGet('01681_database_for_flat_dictionary.flat_dictionary_simple_key_complex_attributes', 'value_first', number) as value_first,\n+    dictGet('01681_database_for_flat_dictionary.flat_dictionary_simple_key_complex_attributes', 'value_second', number) as value_second FROM system.numbers LIMIT 4;\n+SELECT 'dictGetOrDefault existing value';\n+SELECT dictGetOrDefault('01681_database_for_flat_dictionary.flat_dictionary_simple_key_complex_attributes', 'value_first', number, toString('default')) as value_first,\n+    dictGetOrDefault('01681_database_for_flat_dictionary.flat_dictionary_simple_key_complex_attributes', 'value_second', number, toString('default')) as value_second FROM system.numbers LIMIT 3;\n+SELECT 'dictGetOrDefault non existing value';\n+SELECT dictGetOrDefault('01681_database_for_flat_dictionary.flat_dictionary_simple_key_complex_attributes', 'value_first', number, toString('default')) as value_first,\n+    dictGetOrDefault('01681_database_for_flat_dictionary.flat_dictionary_simple_key_complex_attributes', 'value_second', number, toString('default')) as value_second FROM system.numbers LIMIT 4;\n+SELECT 'dictHas';\n+SELECT dictHas('01681_database_for_flat_dictionary.flat_dictionary_simple_key_complex_attributes', number) FROM system.numbers LIMIT 4;\n+\n+DROP DICTIONARY 01681_database_for_flat_dictionary.flat_dictionary_simple_key_complex_attributes;\n+DROP TABLE 01681_database_for_flat_dictionary.simple_key_complex_attributes_source_table;\n+\n+CREATE TABLE 01681_database_for_flat_dictionary.simple_key_hierarchy_table\n+(\n+    id UInt64,\n+    parent_id UInt64\n+) ENGINE = TinyLog();\n+\n+INSERT INTO 01681_database_for_flat_dictionary.simple_key_hierarchy_table VALUES (1, 0);\n+INSERT INTO 01681_database_for_flat_dictionary.simple_key_hierarchy_table VALUES (2, 1);\n+INSERT INTO 01681_database_for_flat_dictionary.simple_key_hierarchy_table VALUES (3, 1);\n+INSERT INTO 01681_database_for_flat_dictionary.simple_key_hierarchy_table VALUES (4, 2);\n+\n+CREATE DICTIONARY 01681_database_for_flat_dictionary.flat_dictionary_simple_key_hierarchy\n+(\n+   id UInt64,\n+   parent_id UInt64 HIERARCHICAL\n+)\n+PRIMARY KEY id\n+SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'simple_key_hierarchy_table'))\n+LIFETIME(MIN 1 MAX 1000)\n+LAYOUT(FLAT());\n+\n+SELECT 'Dictionary flat_dictionary_simple_key_hierarchy';\n+SELECT 'dictGet';\n+SELECT dictGet('01681_database_for_flat_dictionary.flat_dictionary_simple_key_hierarchy', 'parent_id', number) FROM system.numbers LIMIT 5;\n+SELECT 'dictGetHierarchy';\n+SELECT dictGetHierarchy('01681_database_for_flat_dictionary.flat_dictionary_simple_key_hierarchy', toUInt64(1));\n+SELECT dictGetHierarchy('01681_database_for_flat_dictionary.flat_dictionary_simple_key_hierarchy', toUInt64(4));\n+\n+DROP DICTIONARY 01681_database_for_flat_dictionary.flat_dictionary_simple_key_hierarchy;\n+DROP TABLE 01681_database_for_flat_dictionary.simple_key_hierarchy_table;\n+\n+DROP DATABASE 01681_database_for_flat_dictionary;\ndiff --git a/tests/queries/0_stateless/01684_ssd_cache_dictionary_simple_key.reference b/tests/queries/0_stateless/01684_ssd_cache_dictionary_simple_key.reference\nnew file mode 100644\nindex 000000000000..b3258e36f745\n--- /dev/null\n+++ b/tests/queries/0_stateless/01684_ssd_cache_dictionary_simple_key.reference\n@@ -0,0 +1,66 @@\n+Dictionary cache_dictionary_simple_key_simple_attributes\n+dictGet existing value\n+value_0\tvalue_second_0\n+value_1\tvalue_second_1\n+value_2\tvalue_second_2\n+dictGet with non existing value\n+value_0\tvalue_second_0\n+value_1\tvalue_second_1\n+value_2\tvalue_second_2\n+value_first_default\tvalue_second_default\n+dictGetOrDefault existing value\n+value_0\tvalue_second_0\n+value_1\tvalue_second_1\n+value_2\tvalue_second_2\n+dictGetOrDefault non existing value\n+value_0\tvalue_second_0\n+value_1\tvalue_second_1\n+value_2\tvalue_second_2\n+default\tdefault\n+dictHas\n+1\n+1\n+1\n+0\n+select all values as input stream\n+0\tvalue_0\tvalue_second_0\n+1\tvalue_1\tvalue_second_1\n+2\tvalue_2\tvalue_second_2\n+Dictionary cache_dictionary_simple_key_complex_attributes\n+dictGet existing value\n+value_0\tvalue_second_0\n+value_1\t\\N\n+value_2\tvalue_second_2\n+dictGet with non existing value\n+value_0\tvalue_second_0\n+value_1\t\\N\n+value_2\tvalue_second_2\n+value_first_default\tvalue_second_default\n+dictGetOrDefault existing value\n+value_0\tvalue_second_0\n+value_1\t\\N\n+value_2\tvalue_second_2\n+dictGetOrDefault non existing value\n+value_0\tvalue_second_0\n+value_1\t\\N\n+value_2\tvalue_second_2\n+default\tdefault\n+dictHas\n+1\n+1\n+1\n+0\n+select all values as input stream\n+0\tvalue_0\tvalue_second_0\n+1\tvalue_1\t\\N\n+2\tvalue_2\tvalue_second_2\n+Dictionary cache_dictionary_simple_key_hierarchy\n+dictGet\n+0\n+0\n+1\n+1\n+2\n+dictGetHierarchy\n+[1]\n+[4,2,1]\ndiff --git a/tests/queries/0_stateless/01684_ssd_cache_dictionary_simple_key.sql b/tests/queries/0_stateless/01684_ssd_cache_dictionary_simple_key.sql\nnew file mode 100644\nindex 000000000000..3b327257fc4f\n--- /dev/null\n+++ b/tests/queries/0_stateless/01684_ssd_cache_dictionary_simple_key.sql\n@@ -0,0 +1,123 @@\n+DROP DATABASE IF EXISTS 01684_database_for_cache_dictionary;\n+CREATE DATABASE 01684_database_for_cache_dictionary;\n+\n+CREATE TABLE 01684_database_for_cache_dictionary.simple_key_simple_attributes_source_table\n+(\n+   id UInt64,\n+   value_first String,\n+   value_second String\n+)\n+ENGINE = TinyLog;\n+\n+INSERT INTO 01684_database_for_cache_dictionary.simple_key_simple_attributes_source_table VALUES(0, 'value_0', 'value_second_0');\n+INSERT INTO 01684_database_for_cache_dictionary.simple_key_simple_attributes_source_table VALUES(1, 'value_1', 'value_second_1');\n+INSERT INTO 01684_database_for_cache_dictionary.simple_key_simple_attributes_source_table VALUES(2, 'value_2', 'value_second_2');\n+\n+CREATE DICTIONARY 01684_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes\n+(\n+   id UInt64,\n+   value_first String DEFAULT 'value_first_default',\n+   value_second String DEFAULT 'value_second_default'\n+)\n+PRIMARY KEY id\n+SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'simple_key_simple_attributes_source_table'))\n+LIFETIME(MIN 1 MAX 1000)\n+LAYOUT(SSD_CACHE(BLOCK_SIZE 4096 FILE_SIZE 8192 PATH '/var/lib/clickhouse/clickhouse_dicts/0d'));\n+\n+SELECT 'Dictionary cache_dictionary_simple_key_simple_attributes';\n+SELECT 'dictGet existing value';\n+SELECT dictGet('01684_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes', 'value_first', number) as value_first,\n+    dictGet('01684_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes', 'value_second', number) as value_second FROM system.numbers LIMIT 3;\n+SELECT 'dictGet with non existing value';\n+SELECT dictGet('01684_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes', 'value_first', number) as value_first,\n+    dictGet('01684_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes', 'value_second', number) as value_second FROM system.numbers LIMIT 4;\n+SELECT 'dictGetOrDefault existing value';\n+SELECT dictGetOrDefault('01684_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes', 'value_first', number, toString('default')) as value_first,\n+    dictGetOrDefault('01684_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes', 'value_second', number, toString('default')) as value_second FROM system.numbers LIMIT 3;\n+SELECT 'dictGetOrDefault non existing value';\n+SELECT dictGetOrDefault('01684_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes', 'value_first', number, toString('default')) as value_first,\n+    dictGetOrDefault('01684_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes', 'value_second', number, toString('default')) as value_second FROM system.numbers LIMIT 4;\n+SELECT 'dictHas';\n+SELECT dictHas('01684_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes', number) FROM system.numbers LIMIT 4;\n+SELECT 'select all values as input stream';\n+SELECT * FROM 01684_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes;\n+\n+DROP DICTIONARY 01684_database_for_cache_dictionary.cache_dictionary_simple_key_simple_attributes;\n+DROP TABLE 01684_database_for_cache_dictionary.simple_key_simple_attributes_source_table;\n+\n+CREATE TABLE 01684_database_for_cache_dictionary.simple_key_complex_attributes_source_table\n+(\n+   id UInt64,\n+   value_first String,\n+   value_second Nullable(String)\n+)\n+ENGINE = TinyLog;\n+\n+INSERT INTO 01684_database_for_cache_dictionary.simple_key_complex_attributes_source_table VALUES(0, 'value_0', 'value_second_0');\n+INSERT INTO 01684_database_for_cache_dictionary.simple_key_complex_attributes_source_table VALUES(1, 'value_1', NULL);\n+INSERT INTO 01684_database_for_cache_dictionary.simple_key_complex_attributes_source_table VALUES(2, 'value_2', 'value_second_2');\n+\n+CREATE DICTIONARY 01684_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes\n+(\n+   id UInt64,\n+   value_first String DEFAULT 'value_first_default',\n+   value_second Nullable(String) DEFAULT 'value_second_default'\n+)\n+PRIMARY KEY id\n+SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'simple_key_complex_attributes_source_table'))\n+LIFETIME(MIN 1 MAX 1000)\n+LAYOUT(SSD_CACHE(BLOCK_SIZE 4096 FILE_SIZE 8192 PATH '/var/lib/clickhouse/clickhouse_dicts/1d'));\n+\n+SELECT 'Dictionary cache_dictionary_simple_key_complex_attributes';\n+SELECT 'dictGet existing value';\n+SELECT dictGet('01684_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes', 'value_first', number) as value_first,\n+    dictGet('01684_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes', 'value_second', number) as value_second FROM system.numbers LIMIT 3;\n+SELECT 'dictGet with non existing value';\n+SELECT dictGet('01684_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes', 'value_first', number) as value_first,\n+    dictGet('01684_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes', 'value_second', number) as value_second FROM system.numbers LIMIT 4;\n+SELECT 'dictGetOrDefault existing value';\n+SELECT dictGetOrDefault('01684_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes', 'value_first', number, toString('default')) as value_first,\n+    dictGetOrDefault('01684_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes', 'value_second', number, toString('default')) as value_second FROM system.numbers LIMIT 3;\n+SELECT 'dictGetOrDefault non existing value';\n+SELECT dictGetOrDefault('01684_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes', 'value_first', number, toString('default')) as value_first,\n+    dictGetOrDefault('01684_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes', 'value_second', number, toString('default')) as value_second FROM system.numbers LIMIT 4;\n+SELECT 'dictHas';\n+SELECT dictHas('01684_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes', number) FROM system.numbers LIMIT 4;\n+SELECT 'select all values as input stream';\n+SELECT * FROM 01684_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes;\n+\n+DROP DICTIONARY 01684_database_for_cache_dictionary.cache_dictionary_simple_key_complex_attributes;\n+DROP TABLE 01684_database_for_cache_dictionary.simple_key_complex_attributes_source_table;\n+\n+CREATE TABLE 01684_database_for_cache_dictionary.simple_key_hierarchy_table\n+(\n+    id UInt64,\n+    parent_id UInt64\n+) ENGINE = TinyLog();\n+\n+INSERT INTO 01684_database_for_cache_dictionary.simple_key_hierarchy_table VALUES (1, 0);\n+INSERT INTO 01684_database_for_cache_dictionary.simple_key_hierarchy_table VALUES (2, 1);\n+INSERT INTO 01684_database_for_cache_dictionary.simple_key_hierarchy_table VALUES (3, 1);\n+INSERT INTO 01684_database_for_cache_dictionary.simple_key_hierarchy_table VALUES (4, 2);\n+\n+CREATE DICTIONARY 01684_database_for_cache_dictionary.cache_dictionary_simple_key_hierarchy\n+(\n+   id UInt64,\n+   parent_id UInt64 HIERARCHICAL\n+)\n+PRIMARY KEY id\n+SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'simple_key_hierarchy_table'))\n+LIFETIME(MIN 1 MAX 1000)\n+LAYOUT(SSD_CACHE(BLOCK_SIZE 4096 FILE_SIZE 8192 PATH '/var/lib/clickhouse/clickhouse_dicts/2d'));\n+\n+SELECT 'Dictionary cache_dictionary_simple_key_hierarchy';\n+SELECT 'dictGet';\n+SELECT dictGet('01684_database_for_cache_dictionary.cache_dictionary_simple_key_hierarchy', 'parent_id', number) FROM system.numbers LIMIT 5;\n+SELECT 'dictGetHierarchy';\n+SELECT dictGetHierarchy('01684_database_for_cache_dictionary.cache_dictionary_simple_key_hierarchy', toUInt64(1));\n+SELECT dictGetHierarchy('01684_database_for_cache_dictionary.cache_dictionary_simple_key_hierarchy', toUInt64(4));\n+\n+DROP DICTIONARY 01684_database_for_cache_dictionary.cache_dictionary_simple_key_hierarchy;\n+DROP TABLE 01684_database_for_cache_dictionary.simple_key_hierarchy_table;\n+\n+DROP DATABASE 01684_database_for_cache_dictionary;\ndiff --git a/tests/queries/0_stateless/01685_ssd_cache_dictionary_complex_key.reference b/tests/queries/0_stateless/01685_ssd_cache_dictionary_complex_key.reference\nnew file mode 100644\nindex 000000000000..a0f99ca13cc9\n--- /dev/null\n+++ b/tests/queries/0_stateless/01685_ssd_cache_dictionary_complex_key.reference\n@@ -0,0 +1,56 @@\n+Dictionary cache_dictionary_complex_key_simple_attributes\n+dictGet existing value\n+value_0\tvalue_second_0\n+value_1\tvalue_second_1\n+value_2\tvalue_second_2\n+dictGet with non existing value\n+value_0\tvalue_second_0\n+value_1\tvalue_second_1\n+value_2\tvalue_second_2\n+value_first_default\tvalue_second_default\n+dictGetOrDefault existing value\n+value_0\tvalue_second_0\n+value_1\tvalue_second_1\n+value_2\tvalue_second_2\n+dictGetOrDefault non existing value\n+value_0\tvalue_second_0\n+value_1\tvalue_second_1\n+value_2\tvalue_second_2\n+default\tdefault\n+dictHas\n+1\n+1\n+1\n+0\n+select all values as input stream\n+0\tid_key_0\tvalue_0\tvalue_second_0\n+1\tid_key_1\tvalue_1\tvalue_second_1\n+2\tid_key_2\tvalue_2\tvalue_second_2\n+Dictionary cache_dictionary_complex_key_complex_attributes\n+dictGet existing value\n+value_0\tvalue_second_0\n+value_1\t\\N\n+value_2\tvalue_second_2\n+dictGet with non existing value\n+value_0\tvalue_second_0\n+value_1\t\\N\n+value_2\tvalue_second_2\n+value_first_default\tvalue_second_default\n+dictGetOrDefault existing value\n+value_0\tvalue_second_0\n+value_1\t\\N\n+value_2\tvalue_second_2\n+dictGetOrDefault non existing value\n+value_0\tvalue_second_0\n+value_1\t\\N\n+value_2\tvalue_second_2\n+default\tdefault\n+dictHas\n+1\n+1\n+1\n+0\n+select all values as input stream\n+0\tid_key_0\tvalue_0\tvalue_second_0\n+1\tid_key_1\tvalue_1\t\\N\n+2\tid_key_2\tvalue_2\tvalue_second_2\ndiff --git a/tests/queries/0_stateless/01685_ssd_cache_dictionary_complex_key.sql b/tests/queries/0_stateless/01685_ssd_cache_dictionary_complex_key.sql\nnew file mode 100644\nindex 000000000000..1757b136d3ed\n--- /dev/null\n+++ b/tests/queries/0_stateless/01685_ssd_cache_dictionary_complex_key.sql\n@@ -0,0 +1,98 @@\n+DROP DATABASE IF EXISTS 01685_database_for_cache_dictionary;\n+CREATE DATABASE 01685_database_for_cache_dictionary;\n+\n+CREATE TABLE 01685_database_for_cache_dictionary.complex_key_simple_attributes_source_table\n+(\n+   id UInt64,\n+   id_key String,\n+   value_first String,\n+   value_second String\n+)\n+ENGINE = TinyLog;\n+\n+INSERT INTO 01685_database_for_cache_dictionary.complex_key_simple_attributes_source_table VALUES(0, 'id_key_0', 'value_0', 'value_second_0');\n+INSERT INTO 01685_database_for_cache_dictionary.complex_key_simple_attributes_source_table VALUES(1, 'id_key_1', 'value_1', 'value_second_1');\n+INSERT INTO 01685_database_for_cache_dictionary.complex_key_simple_attributes_source_table VALUES(2, 'id_key_2', 'value_2', 'value_second_2');\n+\n+CREATE DICTIONARY 01685_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes\n+(\n+   id UInt64,\n+   id_key String,\n+   value_first String DEFAULT 'value_first_default',\n+   value_second String DEFAULT 'value_second_default'\n+)\n+PRIMARY KEY id, id_key\n+SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'complex_key_simple_attributes_source_table' DB '01685_database_for_cache_dictionary'))\n+LIFETIME(MIN 1 MAX 1000)\n+LAYOUT(COMPLEX_KEY_SSD_CACHE(BLOCK_SIZE 4096 FILE_SIZE 8192 PATH '/var/lib/clickhouse/clickhouse_dicts/0d'));\n+\n+SELECT 'Dictionary cache_dictionary_complex_key_simple_attributes';\n+SELECT 'dictGet existing value';\n+SELECT dictGet('01685_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes', 'value_first', (number, concat('id_key_', toString(number)))) as value_first,\n+    dictGet('01685_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes', 'value_second', (number, concat('id_key_', toString(number)))) as value_second FROM system.numbers LIMIT 3;\n+SELECT 'dictGet with non existing value';\n+SELECT dictGet('01685_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes', 'value_first', (number, concat('id_key_', toString(number)))) as value_first,\n+    dictGet('01685_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes', 'value_second', (number, concat('id_key_', toString(number)))) as value_second FROM system.numbers LIMIT 4;\n+SELECT 'dictGetOrDefault existing value';\n+SELECT dictGetOrDefault('01685_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes', 'value_first', (number, concat('id_key_', toString(number))), toString('default')) as value_first,\n+    dictGetOrDefault('01685_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes', 'value_second', (number, concat('id_key_', toString(number))), toString('default')) as value_second FROM system.numbers LIMIT 3;\n+SELECT 'dictGetOrDefault non existing value';\n+SELECT dictGetOrDefault('01685_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes', 'value_first', (number, concat('id_key_', toString(number))), toString('default')) as value_first,\n+    dictGetOrDefault('01685_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes', 'value_second', (number, concat('id_key_', toString(number))), toString('default')) as value_second FROM system.numbers LIMIT 4;\n+SELECT 'dictHas';\n+SELECT dictHas('01685_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes', (number, concat('id_key_', toString(number)))) FROM system.numbers LIMIT 4;\n+SELECT 'select all values as input stream';\n+SELECT * FROM 01685_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes;\n+\n+DROP DICTIONARY 01685_database_for_cache_dictionary.cache_dictionary_complex_key_simple_attributes;\n+DROP TABLE 01685_database_for_cache_dictionary.complex_key_simple_attributes_source_table;\n+\n+CREATE TABLE 01685_database_for_cache_dictionary.complex_key_complex_attributes_source_table\n+(\n+   id UInt64,\n+   id_key String,\n+   value_first String,\n+   value_second Nullable(String)\n+)\n+ENGINE = TinyLog;\n+\n+INSERT INTO 01685_database_for_cache_dictionary.complex_key_complex_attributes_source_table VALUES(0, 'id_key_0', 'value_0', 'value_second_0');\n+INSERT INTO 01685_database_for_cache_dictionary.complex_key_complex_attributes_source_table VALUES(1, 'id_key_1', 'value_1', NULL);\n+INSERT INTO 01685_database_for_cache_dictionary.complex_key_complex_attributes_source_table VALUES(2, 'id_key_2', 'value_2', 'value_second_2');\n+\n+CREATE DICTIONARY 01685_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes\n+(\n+    id UInt64,\n+    id_key String,\n+\n+    value_first String DEFAULT 'value_first_default',\n+    value_second Nullable(String) DEFAULT 'value_second_default'\n+)\n+PRIMARY KEY id, id_key\n+SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'complex_key_complex_attributes_source_table' DB '01685_database_for_cache_dictionary'))\n+LIFETIME(MIN 1 MAX 1000)\n+LAYOUT(COMPLEX_KEY_SSD_CACHE(BLOCK_SIZE 4096 FILE_SIZE 8192 PATH '/var/lib/clickhouse/clickhouse_dicts/1d'));\n+\n+SELECT 'Dictionary cache_dictionary_complex_key_complex_attributes';\n+SELECT 'dictGet existing value';\n+SELECT dictGet('01685_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes', 'value_first', (number, concat('id_key_', toString(number)))) as value_first,\n+    dictGet('01685_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes', 'value_second', (number, concat('id_key_', toString(number)))) as value_second FROM system.numbers LIMIT 3;\n+SELECT 'dictGet with non existing value';\n+SELECT dictGet('01685_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes', 'value_first', (number, concat('id_key_', toString(number)))) as value_first,\n+    dictGet('01685_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes', 'value_second', (number, concat('id_key_', toString(number)))) as value_second FROM system.numbers LIMIT 4;\n+SELECT 'dictGetOrDefault existing value';\n+SELECT dictGetOrDefault('01685_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes', 'value_first', (number, concat('id_key_', toString(number))), toString('default')) as value_first,\n+    dictGetOrDefault('01685_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes', 'value_second', (number, concat('id_key_', toString(number))), toString('default')) as value_second FROM system.numbers LIMIT 3;\n+SELECT 'dictGetOrDefault non existing value';\n+SELECT dictGetOrDefault('01685_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes', 'value_first', (number, concat('id_key_', toString(number))), toString('default')) as value_first,\n+    dictGetOrDefault('01685_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes', 'value_second', (number, concat('id_key_', toString(number))), toString('default')) as value_second FROM system.numbers LIMIT 4;\n+SELECT 'dictHas';\n+SELECT dictHas('01685_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes', (number, concat('id_key_', toString(number)))) FROM system.numbers LIMIT 4;\n+SELECT 'select all values as input stream';\n+SELECT * FROM 01685_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes;\n+\n+DROP DICTIONARY 01685_database_for_cache_dictionary.cache_dictionary_complex_key_complex_attributes;\n+DROP TABLE 01685_database_for_cache_dictionary.complex_key_complex_attributes_source_table;\n+\n+DROP DATABASE 01685_database_for_cache_dictionary;\n+                                                                                                                                          \ndiff --git a/tests/queries/0_stateless/arcadia_skip_list.txt b/tests/queries/0_stateless/arcadia_skip_list.txt\nindex c1e991ff6b25..6926f16e0274 100644\n--- a/tests/queries/0_stateless/arcadia_skip_list.txt\n+++ b/tests/queries/0_stateless/arcadia_skip_list.txt\n@@ -212,3 +212,9 @@\n 01017_uniqCombined_memory_usage\n 01747_join_view_filter_dictionary\n 01748_dictionary_table_dot\n+00950_dict_get,\n+01683_flat_dictionary\n+01681_cache_dictionary_simple_key\n+01682_cache_dictionary_complex_key\n+01684_ssd_cache_dictionary_simple_key\n+01685_ssd_cache_dictionary_complex_key\ndiff --git a/tests/queries/skip_list.json b/tests/queries/skip_list.json\nindex 1200d8f54360..e14ea250b8d4 100644\n--- a/tests/queries/skip_list.json\n+++ b/tests/queries/skip_list.json\n@@ -760,6 +760,12 @@\n         \"polygon_dicts\", // they use an explicitly specified database\n         \"01658_read_file_to_stringcolumn\",\n         \"01721_engine_file_truncate_on_insert\", // It's ok to execute in parallel but not several instances of the same test.\n-        \"01748_dictionary_table_dot\" // creates database\n+        \"01748_dictionary_table_dot\", // creates database\n+        \"00950_dict_get\",\n+        \"01683_flat_dictionary\",\n+        \"01681_cache_dictionary_simple_key\",\n+        \"01682_cache_dictionary_complex_key\",\n+        \"01684_ssd_cache_dictionary_simple_key\",\n+        \"01685_ssd_cache_dictionary_complex_key\"\n     ]\n }\n",
  "problem_statement": "Ideas on how to rework cache dictionaries.\nThe code of cache dictionaries is unnesessarily complicated.\r\n\r\n1. Template specialization on types (all integers and strings) is unnecessary.\r\n2. It does not make sense to have separate `cache` and `complex_key_cache` - the performance of generic case won't be lower.\r\n3. It stores all attributes in separate hash tables that leads to higher complexity and higher memory usage. Instead, let's serialize all attributes of every record into single contiguous memory region (in ArenaWithFreeLists) along with offsets to every column. This will also allow to support more data types. Serialize and desirialize with IColumn methods.\r\n4. Intrusive LRU linked list may work better than cache table and as we collect all attributes together, memory overhead won't be high.\r\n\r\nWe can create new type of dictionary layout, then provide for `cache` and `complex_key_cached` and remove their code.\nCache dictionary weird bytes allocated behaviour\nMy code runs on a 128Gb of RAM machine. I generate a CSV file, create a corresponding dictionary caching the entire table so that I can run a query returning the entire table (for benchmarking purposes). I record the number of rows, the time to load the dictionary, and number of bytes allocated. There are two bizarre things to note:\r\n\r\n- Sometimes the bytes allocated get stuck, i.e. 3400000 and 3500000 rows have the same number of bytes allocated despite the number of cached entries being different.\r\n- Between 3300000 and 3400000 the number of bytes allocated almost doubles while the number of cached entries only increases slightly.\r\nKeep in mind `cachedEntries = numberOfRows*28`. \r\n\r\n```\r\nConsecutive: Number of rows,Loading time,Mean,Variance,Skewness,Bytes Allocated\r\n3000000,14.04,6.445,0.08519,0.1359,16.06 GiB\r\n3100000,17.78,9.309,0.5572,0.2411,16.19 GiB\r\n3200000,30.44,6.821,0.04326,0.09151,16.31 GiB\r\n3300000,31.37,8.631,0.2122,0.1603,16.31 GiB\r\n3400000,38.62,9.87,0.2573,0.1543,30.25 GiB\r\n3500000,21.65,9.046,0.2258,0.1577,30.25 GiB\r\n3600000,41.43,12.62,0.03201,0.04252,30.37 GiB\r\n3700000,25.55,10.25,0.3695,0.1781,30.50 GiB\r\n```\nAddressSanitizer: heap-buffer-overflow DB::SSDComplexKeyCachePartition::append\n**Describe the bug**\r\nhttps://clickhouse-test-reports.s3.yandex.net/19702/6fc39b10d30be05d183f77ea7013901cbc425a5d/fuzzer_asan/report.html#fail1\r\n\r\n**Error message and/or stacktrace**\r\n``` sql\r\nSELECT 2., dictGetInt32('database_for_dict.ssd_dict', 'b', ('1010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010', toInt32(9223372036854775807)))\r\n```\r\n\r\n```\r\n2021.01.27 15:42:44.233048 [ 177 ] {} <Fatal> BaseDaemon: ########################################\r\n2021.01.27 15:42:44.233149 [ 177 ] {} <Fatal> BaseDaemon: (version 21.2.1.5823, build id: 6582B09C21ECF70ADDA3E772ACAA4BCDF9378F88) (from thread 91) (query_id: d918ad4b-6fa2-4e73-839b-233653f1297e) Received signal Unknown signal (-3)\r\n2021.01.27 15:42:44.233223 [ 177 ] {} <Fatal> BaseDaemon: Sanitizer trap.\r\n2021.01.27 15:42:44.233337 [ 177 ] {} <Fatal> BaseDaemon: Stack trace: 0xa9ddce7 0xadf69bd 0xa916496 0xa8fdec4 0xa8ff97e 0xa8f938c 0xa984105 0x19937ad7 0x19933946 0x1998d9b0 0x11494d8b 0x11481b46 0xf756235 0xf755dfc 0xf832797 0xf8311c1 0xf8326c6 0xf8338d4 0x1e31158d 0x1e30faeb 0x1f0ab4f3 0x1f0b6983 0x1f0c0fe9 0x1f07919f 0x1f04c631 0x1f0637c9 0x1f06ed8a 0x1ec41f02 0x1ec37bc5 0x1ec2ca27 0x1ec291b7 0x1f101913\r\n2021.01.27 15:42:44.233694 [ 177 ] {} <Fatal> BaseDaemon: 0. ./obj-x86_64-linux-gnu/../src/Common/StackTrace.cpp:293: StackTrace::StackTrace() @ 0xa9ddce7 in /workspace/clickhouse\r\n2021.01.27 15:42:44.234679 [ 177 ] {} <Fatal> BaseDaemon: 1. ./obj-x86_64-linux-gnu/../src/Common/CurrentThread.h:81: sanitizerDeathCallback() @ 0xadf69bd in /workspace/clickhouse\r\n2021.01.27 15:42:44.236450 [ 177 ] {} <Fatal> BaseDaemon: 2. __sanitizer::Die() @ 0xa916496 in /workspace/clickhouse\r\n2021.01.27 15:42:44.238154 [ 177 ] {} <Fatal> BaseDaemon: 3. ? @ 0xa8fdec4 in /workspace/clickhouse\r\n2021.01.27 15:42:44.239825 [ 177 ] {} <Fatal> BaseDaemon: 4. __asan::ReportGenericError(unsigned long, unsigned long, unsigned long, unsigned long, bool, unsigned long, unsigned int, bool) @ 0xa8ff97e in /workspace/clickhouse\r\n2021.01.27 15:42:44.241581 [ 177 ] {} <Fatal> BaseDaemon: 5. __asan_memcpy @ 0xa8f938c in /workspace/clickhouse\r\n2021.01.27 15:42:44.245370 [ 177 ] {} <Fatal> BaseDaemon: 6. ./obj-x86_64-linux-gnu/../src/IO/WriteBuffer.h:81: DB::WriteBuffer::write(char const*, unsigned long) @ 0xa984105 in /workspace/clickhouse\r\n2021.01.27 15:42:44.246290 [ 177 ] {} <Fatal> BaseDaemon: 7. ./obj-x86_64-linux-gnu/../src/Dictionaries/SSDComplexKeyCacheDictionary.cpp:268: DB::SSDComplexKeyCachePartition::append(std::__1::vector<DB::KeyRef, std::__1::allocator<DB::KeyRef> > const&, std::__1::vector<DB::SSDComplexKeyCachePartition::Attribute, std::__1::allocator<DB::SSDComplexKeyCachePartition::Attribute> > const&, DB::PODArray<DB::SSDComplexKeyCachePartition::Metadata, 4096ul, Allocator<false, false>, 15ul, 16ul> const&, unsigned long) @ 0x19937ad7 in /workspace/clickhouse\r\n2021.01.27 15:42:44.246984 [ 177 ] {} <Fatal> BaseDaemon: 8. ./obj-x86_64-linux-gnu/../src/Dictionaries/SSDComplexKeyCacheDictionary.cpp:0: DB::SSDComplexKeyCachePartition::appendDefaults(std::__1::vector<DB::KeyRef, std::__1::allocator<DB::KeyRef> > const&, DB::PODArray<DB::SSDComplexKeyCachePartition::Metadata, 4096ul, Allocator<false, false>, 15ul, 16ul> const&, unsigned long) @ 0x19933946 in /workspace/clickhouse\r\n2021.01.27 15:42:44.252187 [ 177 ] {} <Fatal> BaseDaemon: 9. ./obj-x86_64-linux-gnu/../src/Dictionaries/SSDComplexKeyCacheDictionary.cpp:1235: DB::SSDComplexKeyCacheDictionary::getInt32(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::PODArray<int, 4096ul, Allocator<false, false>, 15ul, 16ul>&) const @ 0x1998d9b0 in /workspace/clickhouse\r\n2021.01.27 15:42:44.253899 [ 177 ] {} <Fatal> BaseDaemon: 10. COW<DB::IColumn>::immutable_ptr<DB::IColumn> DB::FunctionDictGet<DB::DataTypeNumber<int>, DB::NameDictGetInt32>::executeDispatchComplex<DB::SSDComplexKeyCacheDictionary>(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDictionaryBase const> const&) const @ 0x11494d8b in /workspace/clickhouse\r\n2021.01.27 15:42:44.255575 [ 177 ] {} <Fatal> BaseDaemon: 11. DB::FunctionDictGet<DB::DataTypeNumber<int>, DB::NameDictGetInt32>::executeImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const @ 0x11481b46 in /workspace/clickhouse\r\n2021.01.27 15:42:44.257224 [ 177 ] {} <Fatal> BaseDaemon: 12. DB::IFunction::executeImplDryRun(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const @ 0xf756235 in /workspace/clickhouse\r\n2021.01.27 15:42:44.258863 [ 177 ] {} <Fatal> BaseDaemon: 13. DB::DefaultExecutable::executeDryRun(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const @ 0xf755dfc in /workspace/clickhouse\r\n2021.01.27 15:42:44.260545 [ 177 ] {} <Fatal> BaseDaemon: 14. DB::ExecutableFunctionAdaptor::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0xf832797 in /workspace/clickhouse\r\n2021.01.27 15:42:44.262139 [ 177 ] {} <Fatal> BaseDaemon: 15. DB::ExecutableFunctionAdaptor::defaultImplementationForConstantArguments(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0xf8311c1 in /workspace/clickhouse\r\n2021.01.27 15:42:44.263847 [ 177 ] {} <Fatal> BaseDaemon: 16. DB::ExecutableFunctionAdaptor::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0xf8326c6 in /workspace/clickhouse\r\n2021.01.27 15:42:44.265523 [ 177 ] {} <Fatal> BaseDaemon: 17. DB::ExecutableFunctionAdaptor::execute(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0xf8338d4 in /workspace/clickhouse\r\n2021.01.27 15:42:44.266568 [ 177 ] {} <Fatal> BaseDaemon: 18. ./obj-x86_64-linux-gnu/../contrib/boost/boost/smart_ptr/intrusive_ptr.hpp:194: DB::ActionsDAG::addFunction(std::__1::shared_ptr<DB::IFunctionOverloadResolver> const&, std::__1::vector<DB::ActionsDAG::Node*, std::__1::allocator<DB::ActionsDAG::Node*> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, bool) @ 0x1e31158d in /workspace/clickhouse\r\n2021.01.27 15:42:44.267542 [ 177 ] {} <Fatal> BaseDaemon: 19. ./obj-x86_64-linux-gnu/../src/Interpreters/ActionsDAG.cpp:0: DB::ActionsDAG::addFunction(std::__1::shared_ptr<DB::IFunctionOverloadResolver> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::Context const&) @ 0x1e30faeb in /workspace/clickhouse\r\n2021.01.27 15:42:44.269271 [ 177 ] {} <Fatal> BaseDaemon: 20. ./obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:0: DB::ScopeStack::addFunction(std::__1::shared_ptr<DB::IFunctionOverloadResolver> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) @ 0x1f0ab4f3 in /workspace/clickhouse\r\n2021.01.27 15:42:44.271427 [ 177 ] {} <Fatal> BaseDaemon: 21. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/string:1444: DB::ActionsMatcher::visit(DB::ASTFunction const&, std::__1::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) @ 0x1f0b6983 in /workspace/clickhouse\r\n2021.01.27 15:42:44.274172 [ 177 ] {} <Fatal> BaseDaemon: 22. ./obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:0: DB::ActionsMatcher::visit(DB::ASTExpressionList&, std::__1::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) @ 0x1f0c0fe9 in /workspace/clickhouse\r\n2021.01.27 15:42:44.277875 [ 177 ] {} <Fatal> BaseDaemon: 23. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2844: DB::InDepthNodeVisitor<DB::ActionsMatcher, true, std::__1::shared_ptr<DB::IAST> const>::visit(std::__1::shared_ptr<DB::IAST> const&) @ 0x1f07919f in /workspace/clickhouse\r\n2021.01.27 15:42:44.279240 [ 177 ] {} <Fatal> BaseDaemon: 24. ./obj-x86_64-linux-gnu/../src/Interpreters/ExpressionAnalyzer.cpp:410: DB::ExpressionAnalyzer::getRootActions(std::__1::shared_ptr<DB::IAST> const&, bool, std::__1::shared_ptr<DB::ActionsDAG>&, bool) @ 0x1f04c631 in /workspace/clickhouse\r\n2021.01.27 15:42:44.281664 [ 177 ] {} <Fatal> BaseDaemon: 25. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:3211: DB::SelectQueryExpressionAnalyzer::appendSelect(DB::ExpressionActionsChain&, bool) @ 0x1f0637c9 in /workspace/clickhouse\r\n2021.01.27 15:42:44.284638 [ 177 ] {} <Fatal> BaseDaemon: 26. ./obj-x86_64-linux-gnu/../src/Interpreters/ExpressionAnalyzer.cpp:0: DB::ExpressionAnalysisResult::ExpressionAnalysisResult(DB::SelectQueryExpressionAnalyzer&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, bool, bool, bool, std::__1::shared_ptr<DB::FilterInfo> const&, DB::Block const&) @ 0x1f06ed8a in /workspace/clickhouse\r\n2021.01.27 15:42:44.286693 [ 177 ] {} <Fatal> BaseDaemon: 27. ./obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:567: DB::InterpreterSelectQuery::getSampleBlockImpl() @ 0x1ec41f02 in /workspace/clickhouse\r\n2021.01.27 15:42:44.288396 [ 177 ] {} <Fatal> BaseDaemon: 28. ./obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:461: DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, std::__1::shared_ptr<DB::IBlockInputStream> const&, std::__1::optional<DB::Pipe>, std::__1::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&)::$_2::operator()(bool) const @ 0x1ec37bc5 in /workspace/clickhouse\r\n2021.01.27 15:42:44.289529 [ 177 ] {} <Fatal> BaseDaemon: 29. ./obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:0: DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, std::__1::shared_ptr<DB::IBlockInputStream> const&, std::__1::optional<DB::Pipe>, std::__1::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&) @ 0x1ec2ca27 in /workspace/clickhouse\r\n2021.01.27 15:42:44.290532 [ 177 ] {} <Fatal> BaseDaemon: 30. ./obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:154: DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0x1ec291b7 in /workspace/clickhouse\r\n2021.01.27 15:42:44.291527 [ 177 ] {} <Fatal> BaseDaemon: 31. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:0: DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::__1::shared_ptr<DB::IAST> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0x1f101913 in /workspace/clickhouse\r\n2021.01.27 15:42:44.788310 [ 177 ] {} <Fatal> BaseDaemon: Calculated checksum of the binary: 610FC81A2CB78A9147920C9EDBBBDD7F. There is no information about the reference checksum.\r\n```\r\n\r\n```\r\n=================================================================\r\n==86==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x6270007c0000 at pc 0x00000a8f936a bp 0x7f91d51b3320 sp 0x7f91d51b2ae8\r\nWRITE of size 8 at 0x6270007c0000 thread T3 (TCPHandler)\r\n    #0 0xa8f9369 in __asan_memcpy (/workspace/clickhouse+0xa8f9369)\r\n    #1 0xa984104 in DB::WriteBuffer::write(char const*, unsigned long) obj-x86_64-linux-gnu/../src/IO/WriteBuffer.h:80:13\r\n    #2 0x19937ad6 in DB::SSDComplexKeyCachePartition::append(std::__1::vector<DB::KeyRef, std::__1::allocator<DB::KeyRef> > const&, std::__1::vector<DB::SSDComplexKeyCachePartition::Attribute, std::__1::allocator<DB::SSDComplexKeyCachePartition::Attribute> > const&, DB::PODArray<DB::SSDComplexKeyCachePartition::Metadata, 4096ul, Allocator<false, false>, 15ul, 16ul> const&, unsigned long)::$_2::operator()() const obj-x86_64-linux-gnu/../src/Dictionaries/SSDComplexKeyCacheDictionary.cpp:267:23\r\n    #3 0x19937ad6 in DB::SSDComplexKeyCachePartition::append(std::__1::vector<DB::KeyRef, std::__1::allocator<DB::KeyRef> > const&, std::__1::vector<DB::SSDComplexKeyCachePartition::Attribute, std::__1::allocator<DB::SSDComplexKeyCachePartition::Attribute> > const&, DB::PODArray<DB::SSDComplexKeyCachePartition::Metadata, 4096ul, Allocator<false, false>, 15ul, 16ul> const&, unsigned long) obj-x86_64-linux-gnu/../src/Dictionaries/SSDComplexKeyCacheDictionary.cpp:371:13\r\n    #4 0x19933945 in DB::SSDComplexKeyCachePartition::appendDefaults(std::__1::vector<DB::KeyRef, std::__1::allocator<DB::KeyRef> > const&, DB::PODArray<DB::SSDComplexKeyCachePartition::Metadata, 4096ul, Allocator<false, false>, 15ul, 16ul> const&, unsigned long) obj-x86_64-linux-gnu/../src/Dictionaries/SSDComplexKeyCacheDictionary.cpp:232:12\r\n    #5 0x1998d9af in void DB::SSDComplexKeyCacheStorage::update<void DB::SSDComplexKeyCacheDictionary::getItemsNumberImpl<int, int, DB::SSDComplexKeyCacheDictionary::getInt32(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::PODArray<int, 4096ul, Allocator<false, false>, 15ul, 16ul>&) const::$_13>(unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, std::__1::conditional<IsDecimalNumber<int>, DB::DecimalPaddedPODArray<int>, DB::PODArray<int, 4096ul, Allocator<false, false>, 15ul, 16ul> >::type&, DB::SSDComplexKeyCacheDictionary::getInt32(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::PODArray<int, 4096ul, Allocator<false, false>, 15ul, 16ul>&) const::$_13&&) const::'lambda'(auto, int, DB::SSDComplexKeyCacheDictionary::getInt32(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::PODArray<int, 4096ul, Allocator<false, false>, 15ul, 16ul>&) const::$_13 const&), void DB::SSDComplexKeyCacheDictionary::getItemsNumberImpl<int, int, DB::SSDComplexKeyCacheDictionary::getInt32(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::PODArray<int, 4096ul, Allocator<false, false>, 15ul, 16ul>&) const::$_13>(unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, std::__1::conditional<IsDecimalNumber<int>, DB::DecimalPaddedPODArray<int>, DB::PODArray<int, 4096ul, Allocator<false, false>, 15ul, 16ul> >::type&, DB::SSDComplexKeyCacheDictionary::getInt32(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::PODArray<int, 4096ul, Allocator<false, false>, 15ul, 16ul>&) const::$_13&&) const::'lambda'(auto)>(std::__1::unique_ptr<DB::IDictionarySource, std::__1::default_delete<DB::IDictionarySource> >&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, std::__1::vector<DB::KeyRef, std::__1::allocator<DB::KeyRef> > const&, std::__1::vector<unsigned long, std::__1::allocator<unsigned long> > const&, DB::ComplexKeysPoolImpl<DB::Arena>&, auto&&, int&&, DB::ExternalLoadableLifetime)::'lambda'(std::__1::vector<DB::KeyRef, std::__1::allocator<DB::KeyRef> > const&, DB::PODArray<DB::SSDComplexKeyCachePartition::Metadata, 4096ul, Allocator<false, false>, 15ul, 16ul> const&)::operator()(std::__1::vector<DB::KeyRef, std::__1::allocator<DB::KeyRef> > const&, DB::PODArray<DB::SSDComplexKeyCachePartition::Metadata, 4096ul, Allocator<false, false>, 15ul, 16ul> const&) const obj-x86_64-linux-gnu/../src/Dictionaries/SSDComplexKeyCacheDictionary.cpp:1235:49\r\n    #6 0x1998d9af in void DB::SSDComplexKeyCacheStorage::update<void DB::SSDComplexKeyCacheDictionary::getItemsNumberImpl<int, int, DB::SSDComplexKeyCacheDictionary::getInt32(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::PODArray<int, 4096ul, Allocator<false, false>, 15ul, 16ul>&) const::$_13>(unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, std::__1::conditional<IsDecimalNumber<int>, DB::DecimalPaddedPODArray<int>, DB::PODArray<int, 4096ul, Allocator<false, false>, 15ul, 16ul> >::type&, DB::SSDComplexKeyCacheDictionary::getInt32(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::PODArray<int, 4096ul, Allocator<false, false>, 15ul, 16ul>&) const::$_13&&) const::'lambda'(auto, int, DB::SSDComplexKeyCacheDictionary::getInt32(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::PODArray<int, 4096ul, Allocator<false, false>, 15ul, 16ul>&) const::$_13 const&), void DB::SSDComplexKeyCacheDictionary::getItemsNumberImpl<int, int, DB::SSDComplexKeyCacheDictionary::getInt32(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::PODArray<int, 4096ul, Allocator<false, false>, 15ul, 16ul>&) const::$_13>(unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, std::__1::conditional<IsDecimalNumber<int>, DB::DecimalPaddedPODArray<int>, DB::PODArray<int, 4096ul, Allocator<false, false>, 15ul, 16ul> >::type&, DB::SSDComplexKeyCacheDictionary::getInt32(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::PODArray<int, 4096ul, Allocator<false, false>, 15ul, 16ul>&) const::$_13&&) const::'lambda'(auto)>(std::__1::unique_ptr<DB::IDictionarySource, std::__1::default_delete<DB::IDictionarySource> >&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, std::__1::vector<DB::KeyRef, std::__1::allocator<DB::KeyRef> > const&, std::__1::vector<unsigned long, std::__1::allocator<unsigned long> > const&, DB::ComplexKeysPoolImpl<DB::Arena>&, auto&&, int&&, DB::ExternalLoadableLifetime) obj-x86_64-linux-gnu/../src/Dictionaries/SSDComplexKeyCacheDictionary.cpp:1303:13\r\n    #7 0x1998d9af in void DB::SSDComplexKeyCacheDictionary::getItemsNumberImpl<int, int, DB::SSDComplexKeyCacheDictionary::getInt32(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::PODArray<int, 4096ul, Allocator<false, false>, 15ul, 16ul>&) const::$_13>(unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, std::__1::conditional<IsDecimalNumber<int>, DB::DecimalPaddedPODArray<int>, DB::PODArray<int, 4096ul, Allocator<false, false>, 15ul, 16ul> >::type&, DB::SSDComplexKeyCacheDictionary::getInt32(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::PODArray<int, 4096ul, Allocator<false, false>, 15ul, 16ul>&) const::$_13&&) const obj-x86_64-linux-gnu/../src/Dictionaries/SSDComplexKeyCacheDictionary.cpp:1491:13\r\n    #8 0x1998d9af in DB::SSDComplexKeyCacheDictionary::getInt32(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::PODArray<int, 4096ul, Allocator<false, false>, 15ul, 16ul>&) const obj-x86_64-linux-gnu/../src/Dictionaries/SSDComplexKeyCacheDictionary.cpp:1399:5\r\n    #9 0x11494d8a in COW<DB::IColumn>::immutable_ptr<DB::IColumn> DB::FunctionDictGet<DB::DataTypeNumber<int>, DB::NameDictGetInt32>::executeDispatchComplex<DB::SSDComplexKeyCacheDictionary>(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDictionaryBase const> const&) const (/workspace/clickhouse+0x11494d8a)\r\n    #10 0x11481b45 in DB::FunctionDictGet<DB::DataTypeNumber<int>, DB::NameDictGetInt32>::executeImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const (/workspace/clickhouse+0x11481b45)\r\n    #11 0xf756234 in DB::IFunction::executeImplDryRun(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const (/workspace/clickhouse+0xf756234)\r\n    #12 0xf755dfb in DB::DefaultExecutable::executeDryRun(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const (/workspace/clickhouse+0xf755dfb)\r\n    #13 0xf832796 in DB::ExecutableFunctionAdaptor::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const (/workspace/clickhouse+0xf832796)\r\n    #14 0xf8311c0 in DB::ExecutableFunctionAdaptor::defaultImplementationForConstantArguments(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const (/workspace/clickhouse+0xf8311c0)\r\n    #15 0xf8326c5 in DB::ExecutableFunctionAdaptor::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const (/workspace/clickhouse+0xf8326c5)\r\n    #16 0xf8338d3 in DB::ExecutableFunctionAdaptor::execute(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const (/workspace/clickhouse+0xf8338d3)\r\n    #17 0x1e31158c in DB::ActionsDAG::addFunction(std::__1::shared_ptr<DB::IFunctionOverloadResolver> const&, std::__1::vector<DB::ActionsDAG::Node*, std::__1::allocator<DB::ActionsDAG::Node*> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, bool) obj-x86_64-linux-gnu/../src/Interpreters/ActionsDAG.cpp:211:35\r\n    #18 0x1e30faea in DB::ActionsDAG::addFunction(std::__1::shared_ptr<DB::IFunctionOverloadResolver> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::Context const&) obj-x86_64-linux-gnu/../src/Interpreters/ActionsDAG.cpp:165:12\r\n    #19 0x1f0ab4f2 in DB::ScopeStack::addFunction(std::__1::shared_ptr<DB::IFunctionOverloadResolver> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:501:51\r\n    #20 0x1f0b6982 in DB::ActionsMatcher::Data::addFunction(std::__1::shared_ptr<DB::IFunctionOverloadResolver> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.h:155:27\r\n    #21 0x1f0b6982 in DB::ActionsMatcher::visit(DB::ASTFunction const&, std::__1::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:971:14\r\n    #22 0x1f0c0fe8 in DB::ActionsMatcher::visit(DB::ASTExpressionList&, std::__1::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:654:17\r\n    #23 0x1f07919e in DB::InDepthNodeVisitor<DB::ActionsMatcher, true, std::__1::shared_ptr<DB::IAST> const>::visit(std::__1::shared_ptr<DB::IAST> const&) obj-x86_64-linux-gnu/../src/Interpreters/InDepthNodeVisitor.h:34:13\r\n    #24 0x1f04c630 in DB::ExpressionAnalyzer::getRootActions(std::__1::shared_ptr<DB::IAST> const&, bool, std::__1::shared_ptr<DB::ActionsDAG>&, bool) obj-x86_64-linux-gnu/../src/Interpreters/ExpressionAnalyzer.cpp:410:48\r\n    #25 0x1f0637c8 in DB::SelectQueryExpressionAnalyzer::appendSelect(DB::ExpressionActionsChain&, bool) obj-x86_64-linux-gnu/../src/Interpreters/ExpressionAnalyzer.cpp:1083:5\r\n    #26 0x1f06ed89 in DB::ExpressionAnalysisResult::ExpressionAnalysisResult(DB::SelectQueryExpressionAnalyzer&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, bool, bool, bool, std::__1::shared_ptr<DB::FilterInfo> const&, DB::Block const&) obj-x86_64-linux-gnu/../src/Interpreters/ExpressionAnalyzer.cpp:1467:24\r\n    #27 0x1ec41f01 in DB::InterpreterSelectQuery::getSampleBlockImpl() obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:567:23\r\n    #28 0x1ec37bc4 in DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, std::__1::shared_ptr<DB::IBlockInputStream> const&, std::__1::optional<DB::Pipe>, std::__1::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&)::$_2::operator()(bool) const obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:461:25\r\n    #29 0x1ec2ca26 in DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, std::__1::shared_ptr<DB::IBlockInputStream> const&, std::__1::optional<DB::Pipe>, std::__1::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:464:5\r\n    #30 0x1ec291b6 in DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:154:7\r\n    #31 0x1f101912 in std::__1::__unique_if<DB::InterpreterSelectQuery>::__unique_single std::__1::make_unique<DB::InterpreterSelectQuery, std::__1::shared_ptr<DB::IAST> const&, DB::Context&, DB::SelectQueryOptions&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&>(std::__1::shared_ptr<DB::IAST> const&, DB::Context&, DB::SelectQueryOptions&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2068:32\r\n    #32 0x1f101912 in DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::__1::shared_ptr<DB::IAST> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectWithUnionQuery.cpp:327:16\r\n    #33 0x1f0fdce2 in DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectWithUnionQuery.cpp:249:13\r\n    #34 0x1eb730c0 in std::__1::__unique_if<DB::InterpreterSelectWithUnionQuery>::__unique_single std::__1::make_unique<DB::InterpreterSelectWithUnionQuery, std::__1::shared_ptr<DB::IAST>&, DB::Context&, DB::SelectQueryOptions const&>(std::__1::shared_ptr<DB::IAST>&, DB::Context&, DB::SelectQueryOptions const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2068:32\r\n    #35 0x1eb730c0 in DB::InterpreterFactory::get(std::__1::shared_ptr<DB::IAST>&, DB::Context&, DB::SelectQueryOptions const&) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterFactory.cpp:110:16\r\n    #36 0x1f51301e in DB::executeQueryImpl(char const*, char const*, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer*) obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:520:28\r\n    #37 0x1f510297 in DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool) obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:900:30\r\n    #38 0x20777e0c in DB::TCPHandler::runImpl() obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:260:24\r\n    #39 0x2079a86c in DB::TCPHandler::run() obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:1417:9\r\n    #40 0x2672c48e in Poco::Net::TCPServerConnection::start() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:43:3\r\n    #41 0x2672cfab in Poco::Net::TCPServerDispatcher::run() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerDispatcher.cpp:112:19\r\n    #42 0x269ee7d4 in Poco::PooledThread::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:199:14\r\n    #43 0x269e8d26 in Poco::ThreadImpl::runnableEntry(void*) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:345:27\r\n    #44 0x7f92b127b608 in start_thread /build/glibc-ZN95T4/glibc-2.31/nptl/pthread_create.c:477:8\r\n    #45 0x7f92b11a2292 in clone /build/glibc-ZN95T4/glibc-2.31/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:95\r\n\r\n0x6270007c0000 is located 0 bytes to the right of 8192-byte region [0x6270007be000,0x6270007c0000)\r\nallocated by thread T3 (TCPHandler) here:\r\n    #0 0xa8fa917 in posix_memalign (/workspace/clickhouse+0xa8fa917)\r\n    #1 0xaa28445 in Allocator<false, false>::allocNoTrack(unsigned long, unsigned long) obj-x86_64-linux-gnu/../src/Common/Allocator.h:227:27\r\n    #2 0xaa2815c in Allocator<false, false>::alloc(unsigned long, unsigned long) obj-x86_64-linux-gnu/../src/Common/Allocator.h:96:16\r\n    #3 0xaa2815c in DB::Memory<Allocator<false, false> >::alloc() obj-x86_64-linux-gnu/../src/IO/BufferWithOwnMemory.h:117:49\r\n    #4 0x19934018 in DB::Memory<Allocator<false, false> >::Memory(unsigned long, unsigned long) obj-x86_64-linux-gnu/../src/IO/BufferWithOwnMemory.h:43:9\r\n    #5 0x19934018 in void std::__1::__optional_storage_base<DB::Memory<Allocator<false, false> >, false>::__construct<unsigned long, unsigned long const&>(unsigned long&&, unsigned long const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/optional:324:55\r\n    #6 0x19934018 in DB::Memory<Allocator<false, false> >& std::__1::optional<DB::Memory<Allocator<false, false> > >::emplace<unsigned long, unsigned long const&, void>(unsigned long&&, unsigned long const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/optional:830:15\r\n    #7 0x19934018 in DB::SSDComplexKeyCachePartition::append(std::__1::vector<DB::KeyRef, std::__1::allocator<DB::KeyRef> > const&, std::__1::vector<DB::SSDComplexKeyCachePartition::Attribute, std::__1::allocator<DB::SSDComplexKeyCachePartition::Attribute> > const&, DB::PODArray<DB::SSDComplexKeyCachePartition::Metadata, 4096ul, Allocator<false, false>, 15ul, 16ul> const&, unsigned long) obj-x86_64-linux-gnu/../src/Dictionaries/SSDComplexKeyCacheDictionary.cpp:261:16\r\n    #8 0x19933945 in DB::SSDComplexKeyCachePartition::appendDefaults(std::__1::vector<DB::KeyRef, std::__1::allocator<DB::KeyRef> > const&, DB::PODArray<DB::SSDComplexKeyCachePartition::Metadata, 4096ul, Allocator<false, false>, 15ul, 16ul> const&, unsigned long) obj-x86_64-linux-gnu/../src/Dictionaries/SSDComplexKeyCacheDictionary.cpp:232:12\r\n    #9 0x19a9136e in void DB::SSDComplexKeyCacheStorage::update<void DB::SSDComplexKeyCacheDictionary::getItemsStringImpl<DB::SSDComplexKeyCacheDictionary::getString(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*) const::$_48>(unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*, DB::SSDComplexKeyCacheDictionary::getString(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*) const::$_48&&) const::'lambda'(DB::SSDComplexKeyCacheDictionary::getString(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*) const::$_48, auto, auto const&), void DB::SSDComplexKeyCacheDictionary::getItemsStringImpl<DB::SSDComplexKeyCacheDictionary::getString(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*) const::$_48>(unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*, DB::SSDComplexKeyCacheDictionary::getString(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*) const::$_48&&) const::'lambda'(DB::SSDComplexKeyCacheDictionary::getString(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*) const::$_48)>(std::__1::unique_ptr<DB::IDictionarySource, std::__1::default_delete<DB::IDictionarySource> >&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, std::__1::vector<DB::KeyRef, std::__1::allocator<DB::KeyRef> > const&, std::__1::vector<unsigned long, std::__1::allocator<unsigned long> > const&, DB::ComplexKeysPoolImpl<DB::Arena>&, DB::SSDComplexKeyCacheDictionary::getString(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*) const::$_48&&, auto&&, DB::ExternalLoadableLifetime)::'lambda'(std::__1::vector<DB::KeyRef, std::__1::allocator<DB::KeyRef> > const&, DB::PODArray<DB::SSDComplexKeyCachePartition::Metadata, 4096ul, Allocator<false, false>, 15ul, 16ul> const&)::operator()(std::__1::vector<DB::KeyRef, std::__1::allocator<DB::KeyRef> > const&, DB::PODArray<DB::SSDComplexKeyCachePartition::Metadata, 4096ul, Allocator<false, false>, 15ul, 16ul> const&) const obj-x86_64-linux-gnu/../src/Dictionaries/SSDComplexKeyCacheDictionary.cpp:1235:49\r\n    #10 0x19a9136e in void DB::SSDComplexKeyCacheStorage::update<void DB::SSDComplexKeyCacheDictionary::getItemsStringImpl<DB::SSDComplexKeyCacheDictionary::getString(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*) const::$_48>(unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*, DB::SSDComplexKeyCacheDictionary::getString(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*) const::$_48&&) const::'lambda'(DB::SSDComplexKeyCacheDictionary::getString(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*) const::$_48, auto, auto const&), void DB::SSDComplexKeyCacheDictionary::getItemsStringImpl<DB::SSDComplexKeyCacheDictionary::getString(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*) const::$_48>(unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*, DB::SSDComplexKeyCacheDictionary::getString(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*) const::$_48&&) const::'lambda'(DB::SSDComplexKeyCacheDictionary::getString(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*) const::$_48)>(std::__1::unique_ptr<DB::IDictionarySource, std::__1::default_delete<DB::IDictionarySource> >&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, std::__1::vector<DB::KeyRef, std::__1::allocator<DB::KeyRef> > const&, std::__1::vector<unsigned long, std::__1::allocator<unsigned long> > const&, DB::ComplexKeysPoolImpl<DB::Arena>&, DB::SSDComplexKeyCacheDictionary::getString(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*) const::$_48&&, auto&&, DB::ExternalLoadableLifetime) obj-x86_64-linux-gnu/../src/Dictionaries/SSDComplexKeyCacheDictionary.cpp:1303:13\r\n    #11 0x19a9136e in void DB::SSDComplexKeyCacheDictionary::getItemsStringImpl<DB::SSDComplexKeyCacheDictionary::getString(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*) const::$_48>(unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*, DB::SSDComplexKeyCacheDictionary::getString(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*) const::$_48&&) const obj-x86_64-linux-gnu/../src/Dictionaries/SSDComplexKeyCacheDictionary.cpp:1600:13\r\n    #12 0x19a9136e in DB::SSDComplexKeyCacheDictionary::getString(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, DB::ColumnString*) const obj-x86_64-linux-gnu/../src/Dictionaries/SSDComplexKeyCacheDictionary.cpp:1520:5\r\n    #13 0x115673c3 in COW<DB::IColumn>::immutable_ptr<DB::IColumn> DB::FunctionDictGetString::executeDispatchComplex<DB::SSDComplexKeyCacheDictionary>(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDictionaryBase const> const&) const (/workspace/clickhouse+0x115673c3)\r\n    #14 0x11556095 in DB::FunctionDictGetString::executeImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const (/workspace/clickhouse+0x11556095)\r\n    #15 0xf756234 in DB::IFunction::executeImplDryRun(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const (/workspace/clickhouse+0xf756234)\r\n    #16 0xf755dfb in DB::DefaultExecutable::executeDryRun(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const (/workspace/clickhouse+0xf755dfb)\r\n    #17 0xf832796 in DB::ExecutableFunctionAdaptor::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const (/workspace/clickhouse+0xf832796)\r\n    #18 0xf8311c0 in DB::ExecutableFunctionAdaptor::defaultImplementationForConstantArguments(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const (/workspace/clickhouse+0xf8311c0)\r\n    #19 0xf8326c5 in DB::ExecutableFunctionAdaptor::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const (/workspace/clickhouse+0xf8326c5)\r\n    #20 0xf8338d3 in DB::ExecutableFunctionAdaptor::execute(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const (/workspace/clickhouse+0xf8338d3)\r\n    #21 0x1e31158c in DB::ActionsDAG::addFunction(std::__1::shared_ptr<DB::IFunctionOverloadResolver> const&, std::__1::vector<DB::ActionsDAG::Node*, std::__1::allocator<DB::ActionsDAG::Node*> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, bool) obj-x86_64-linux-gnu/../src/Interpreters/ActionsDAG.cpp:211:35\r\n    #22 0x1e30faea in DB::ActionsDAG::addFunction(std::__1::shared_ptr<DB::IFunctionOverloadResolver> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::Context const&) obj-x86_64-linux-gnu/../src/Interpreters/ActionsDAG.cpp:165:12\r\n    #23 0x1f0ab4f2 in DB::ScopeStack::addFunction(std::__1::shared_ptr<DB::IFunctionOverloadResolver> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:501:51\r\n    #24 0x1f0b6982 in DB::ActionsMatcher::Data::addFunction(std::__1::shared_ptr<DB::IFunctionOverloadResolver> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.h:155:27\r\n    #25 0x1f0b6982 in DB::ActionsMatcher::visit(DB::ASTFunction const&, std::__1::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:971:14\r\n    #26 0x1f0b0f7b in DB::ActionsMatcher::visit(DB::ASTFunction const&, std::__1::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:882:17\r\n    #27 0x1f0c0fe8 in DB::ActionsMatcher::visit(DB::ASTExpressionList&, std::__1::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:654:17\r\n    #28 0x1f07919e in DB::InDepthNodeVisitor<DB::ActionsMatcher, true, std::__1::shared_ptr<DB::IAST> const>::visit(std::__1::shared_ptr<DB::IAST> const&) obj-x86_64-linux-gnu/../src/Interpreters/InDepthNodeVisitor.h:34:13\r\n    #29 0x1f04c630 in DB::ExpressionAnalyzer::getRootActions(std::__1::shared_ptr<DB::IAST> const&, bool, std::__1::shared_ptr<DB::ActionsDAG>&, bool) obj-x86_64-linux-gnu/../src/Interpreters/ExpressionAnalyzer.cpp:410:48\r\n    #30 0x1f0637c8 in DB::SelectQueryExpressionAnalyzer::appendSelect(DB::ExpressionActionsChain&, bool) obj-x86_64-linux-gnu/../src/Interpreters/ExpressionAnalyzer.cpp:1083:5\r\n    #31 0x1f06ed89 in DB::ExpressionAnalysisResult::ExpressionAnalysisResult(DB::SelectQueryExpressionAnalyzer&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, bool, bool, bool, std::__1::shared_ptr<DB::FilterInfo> const&, DB::Block const&) obj-x86_64-linux-gnu/../src/Interpreters/ExpressionAnalyzer.cpp:1467:24\r\n    #32 0x1ec41f01 in DB::InterpreterSelectQuery::getSampleBlockImpl() obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:567:23\r\n    #33 0x1ec37bc4 in DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, std::__1::shared_ptr<DB::IBlockInputStream> const&, std::__1::optional<DB::Pipe>, std::__1::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&)::$_2::operator()(bool) const obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:461:25\r\n    #34 0x1ec2ca26 in DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, std::__1::shared_ptr<DB::IBlockInputStream> const&, std::__1::optional<DB::Pipe>, std::__1::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:464:5\r\n    #35 0x1ec291b6 in DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:154:7\r\n    #36 0x1f101912 in std::__1::__unique_if<DB::InterpreterSelectQuery>::__unique_single std::__1::make_unique<DB::InterpreterSelectQuery, std::__1::shared_ptr<DB::IAST> const&, DB::Context&, DB::SelectQueryOptions&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&>(std::__1::shared_ptr<DB::IAST> const&, DB::Context&, DB::SelectQueryOptions&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2068:32\r\n    #37 0x1f101912 in DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::__1::shared_ptr<DB::IAST> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectWithUnionQuery.cpp:327:16\r\n    #38 0x1f0fdce2 in DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectWithUnionQuery.cpp:249:13\r\n\r\nThread T3 (TCPHandler) created by T0 here:\r\n    #0 0xa8e488a in pthread_create (/workspace/clickhouse+0xa8e488a)\r\n    #1 0x269e80bf in Poco::ThreadImpl::startImpl(Poco::SharedPtr<Poco::Runnable, Poco::ReferenceCounter, Poco::ReleasePolicy<Poco::Runnable> >) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:202:6\r\n    #2 0x269eaf3a in Poco::Thread::start(Poco::Runnable&) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:128:2\r\n    #3 0x269eed78 in Poco::PooledThread::start() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:85:10\r\n    #4 0x269eed78 in Poco::ThreadPool::ThreadPool(int, int, int, int) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:252:12\r\n    #5 0xa94a623 in DB::Server::main(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) obj-x86_64-linux-gnu/../programs/server/Server.cpp:828:22\r\n    #6 0x2675c595 in Poco::Util::Application::run() obj-x86_64-linux-gnu/../contrib/poco/Util/src/Application.cpp:334:8\r\n    #7 0xa93518a in DB::Server::run() obj-x86_64-linux-gnu/../programs/server/Server.cpp:337:25\r\n    #8 0x26799b97 in Poco::Util::ServerApplication::run(int, char**) obj-x86_64-linux-gnu/../contrib/poco/Util/src/ServerApplication.cpp:611:9\r\n    #9 0xa931791 in mainEntryClickHouseServer(int, char**) obj-x86_64-linux-gnu/../programs/server/Server.cpp:129:20\r\n    #10 0xa92d6ba in main obj-x86_64-linux-gnu/../programs/main.cpp:368:12\r\n    #11 0x7f92b10a70b2 in __libc_start_main /build/glibc-ZN95T4/glibc-2.31/csu/../csu/libc-start.c:308:16\r\n\r\nSUMMARY: AddressSanitizer: heap-buffer-overflow (/workspace/clickhouse+0xa8f9369) in __asan_memcpy\r\nShadow bytes around the buggy address:\r\n  0x0c4e800effb0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\r\n  0x0c4e800effc0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\r\n  0x0c4e800effd0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\r\n  0x0c4e800effe0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\r\n  0x0c4e800efff0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\r\n=>0x0c4e800f0000:[fa]fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c4e800f0010: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c4e800f0020: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c4e800f0030: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c4e800f0040: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c4e800f0050: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\nShadow byte legend (one shadow byte represents 8 application bytes):\r\n  Addressable:           00\r\n  Partially addressable: 01 02 03 04 05 06 07\r\n  Heap left redzone:       fa\r\n  Freed heap region:       fd\r\n  Stack left redzone:      f1\r\n  Stack mid redzone:       f2\r\n  Stack right redzone:     f3\r\n  Stack after return:      f5\r\n  Stack use after scope:   f8\r\n  Global redzone:          f9\r\n  Global init order:       f6\r\n  Poisoned by user:        f7\r\n  Container overflow:      fc\r\n  Array cookie:            ac\r\n  Intra object redzone:    bb\r\n  ASan internal:           fe\r\n  Left alloca redzone:     ca\r\n  Right alloca redzone:    cb\r\n  Shadow gap:              cc\r\n==86==ABORTING\r\n```\nDictionaries add dictGet, dictGetOrDefault overload that returns multiple columns as tuple\nCurrently for cache and direct dictionaries if we need to get multiple columns we have to go into cache or into source multiple times. Solution is to add overload for dictGet, dictGetOrDefault functions to support multiple attributes names passes into function as tuple. For dictGetOrDefault additionally we need pass tuple of default columns.\r\nSyntax: \r\n1. dictGet('dict_name', ('1_attr_name', ...), keys_type, keys_column_expression).\r\n2. dictGetOrDefault('dict_name', ('1_attr_name', ...), keys_type, keys_column_expression, (1_attr_default_expr, ...)).\r\n\nOOM error when setting up cache dictionaries\nRunning a centos 7.6 VM with 16Gb of RAM I get the following error\r\n```\r\nCode: 32. DB::Exception: Attempt to read after eof: while receiving packet from localhost:9000\r\n```\r\nwhen querying a cache dictionary with 75 columns and 100000 rows (126Mb). Is this to be expected as the dictionary seems quite small. I'm using `clickhouse-client --query` to launch the query:\r\n```\r\nSELECT  dictGet('CacheDictionary', 'date', toUInt64(number)) AS date, SUM(dictGet('CacheDictionary', 'filterColumn', toUInt64(number))) AS val, AVG(dictGet('CacheDictionary', 'filterColumn', toUInt64(number))) AS avg FROM system.numbers(1, 100000) GROUP BY date\r\n```\r\nChecking dmesg I get the correspond OOM error.\r\n```\r\n[613700.447158] CPU: 1 PID: 5859 Comm: node Not tainted 3.10.0-957.5.1.el7.x86_64 #1\r\n[613700.449266] Hardware name: Scaleway SCW-GP1-S, BIOS 0.0.0 02/06/2015\r\n[613700.451020] Call Trace:\r\n[613700.451708]  [<ffffffff9cf61e41>] dump_stack+0x19/0x1b\r\n[613700.452792]  [<ffffffff9cf5c86a>] dump_header+0x90/0x229\r\n[613700.453875]  [<ffffffff9cb00f3b>] ? cred_has_capability+0x6b/0x120\r\n[613700.455220]  [<ffffffff9c9ba524>] oom_kill_process+0x254/0x3d0\r\n[613700.456475]  [<ffffffff9cb0101e>] ? selinux_capable+0x2e/0x40\r\n[613700.457641]  [<ffffffff9c9bad66>] out_of_memory+0x4b6/0x4f0\r\n[613700.458844]  [<ffffffff9cf5d36e>] __alloc_pages_slowpath+0x5d6/0x724\r\n[613700.459965]  [<ffffffff9c9c1145>] __alloc_pages_nodemask+0x405/0x420\r\n[613700.460773]  [<ffffffff9ca0e0a8>] alloc_pages_current+0x98/0x110\r\n[613700.461521]  [<ffffffff9c9b6387>] __page_cache_alloc+0x97/0xb0\r\n[613700.462156]  [<ffffffff9c9b8fe8>] filemap_fault+0x298/0x490\r\n[613700.462849]  [<ffffffffc0461186>] ext4_filemap_fault+0x36/0x50 [ext4]\r\n[613700.463623]  [<ffffffff9c9e458a>] __do_fault.isra.59+0x8a/0x100\r\n[613700.464350]  [<ffffffff9c9e4b3c>] do_read_fault.isra.61+0x4c/0x1b0\r\n[613700.465079]  [<ffffffff9c9e94e4>] handle_pte_fault+0x2f4/0xd10\r\n[613700.465756]  [<ffffffff9c9ec01d>] handle_mm_fault+0x39d/0x9b0\r\n[613700.466454]  [<ffffffff9cf6f5e3>] __do_page_fault+0x203/0x500\r\n[613700.467136]  [<ffffffff9c9f1a37>] ? do_munmap+0x327/0x480\r\n[613700.467749]  [<ffffffff9cf6f9c6>] trace_do_page_fault+0x56/0x150\r\n[613700.468410]  [<ffffffff9cf6ef42>] do_async_page_fault+0x22/0xf0\r\n[613700.469126]  [<ffffffff9cf6b788>] async_page_fault+0x28/0x30\r\n[613700.469766] Mem-Info:\r\n[613700.470040] active_anon:5266300 inactive_anon:2788185 isolated_anon:0\r\n active_file:184 inactive_file:53 isolated_file:0\r\n unevictable:0 dirty:11 writeback:223 unstable:0\r\n slab_reclaimable:9023 slab_unreclaimable:9736\r\n mapped:1319 shmem:1310 pagetables:18040 bounce:0\r\n free:49178 free_pcp:61 free_cma:0\r\n[613700.473741] Node 0 DMA free:14912kB min:28kB low:32kB high:40kB active_anon:0kB inactive_anon:0kB active_file:0kB inactive_file:0kB unevictable:0kB isolated(anon):0kB isolated(file):0kB present:15004kB managed:14912kB mlocked:0kB dirty:0kB writeback:0kB mapped:0kB shmem:0kB slab_reclaimable:0kB slab_unreclaimable:0kB kernel_stack:0kB pagetables:0kB unstable:0kB bounce:0kB free_pcp:0kB local_pcp:0kB free_cma:0kB writeback_tmp:0kB pages_scanned:0 all_unreclaimable? yes\r\n[613700.478645] lowmem_reserve[]: 0 2827 31990 31990\r\n[613700.479345] Node 0 DMA32 free:122332kB min:5788kB low:7232kB high:8680kB active_anon:1511568kB inactive_anon:1243404kB active_file:84kB inactive_file:140kB unevictable:0kB isolated(anon):0kB isolated(file):0kB present:3126684kB managed:2895228kB mlocked:0kB dirty:24kB writeback:272kB mapped:148kB shmem:216kB slab_reclaimable:3228kB slab_unreclaimable:3372kB kernel_stack:144kB pagetables:7784kB unstable:0kB bounce:0kB free_pcp:0kB local_pcp:0kB free_cma:0kB writeback_tmp:0kB pages_scanned:47688 all_unreclaimable? yes\r\n[613700.484885] lowmem_reserve[]: 0 0 29163 29163\r\n[613700.485457] Node 0 Normal free:59468kB min:59716kB low:74644kB high:89572kB active_anon:19553632kB inactive_anon:9909336kB active_file:652kB inactive_file:72kB unevictable:0kB isolated(anon):0kB isolated(file):0kB present:30408704kB managed:29866176kB mlocked:0kB dirty:20kB writeback:620kB mapped:5128kB shmem:5024kB slab_reclaimable:32864kB slab_unreclaimable:35572kB kernel_stack:3728kB pagetables:64376kB unstable:0kB bounce:0kB free_pcp:244kB local_pcp:0kB free_cma:0kB writeback_tmp:0kB pages_scanned:7605 all_unreclaimable? yes\r\n[613700.490897] lowmem_reserve[]: 0 0 0 0\r\n[613700.491483] Node 0 DMA: 2*4kB (U) 1*8kB (U) 1*16kB (U) 3*32kB (UM) 1*64kB (U) 1*128kB (U) 1*256kB (U) 0*512kB 2*1024kB (UM) 2*2048kB (M) 2*4096kB (M) = 14912kB\r\n[613700.493471] Node 0 DMA32: 134*4kB (UEM) 83*8kB (UEM) 149*16kB (UEM) 74*32kB (UE) 41*64kB (UE) 27*128kB (UEM) 13*256kB (UE) 8*512kB (UEM) 101*1024kB (UM) 0*2048kB 0*4096kB = 122880kB\r\n[613700.495884] Node 0 Normal: 408*4kB (UE) 320*8kB (UE) 301*16kB (UE) 252*32kB (UE) 184*64kB (UEM) 130*128kB (UE) 34*256kB (UE) 2*512kB (UE) 5*1024kB (M) 0*2048kB 0*4096kB = 60336kB\r\n[613700.498284] Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=1048576kB\r\n[613700.499258] Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=2048kB\r\n[613700.500155] 152719 total pagecache pages\r\n[613700.500582] 151262 pages in swap cache\r\n[613700.500989] Swap cache stats: add 25089927, delete 24939368, find 1630185/1802893\r\n[613700.501787] Free swap  = 0kB\r\n[613700.502119] Total swap = 1048572kB\r\n[613700.502538] 8387598 pages RAM\r\n[613700.502891] 0 pages HighMem/MovableOnly\r\n[613700.503352] 193519 pages reserved\r\n[613700.503741] [ pid ]   uid  tgid total_vm      rss nr_ptes swapents oom_score_adj name\r\n[613700.504600] [ 1773]     0  1773    63151     1390     100      153             0 systemd-journal\r\n[613700.505791] [ 1812]     0  1812     9945        2      25      538         -1000 systemd-udevd\r\n[613700.506886] [ 2179]     0  2179     6594       41      19       35             0 systemd-logind\r\n[613700.507899] [ 2185]    81  2185    16581       57      32       80          -900 dbus-daemon\r\n[613700.508887] [ 3808]     0  3808    26865        2      50      498             0 dhclient\r\n[613700.509845] [ 3965]     0  3965    22931       12      45      251             0 master\r\n[613700.510888] [ 3967]    89  3967    22974       12      45      246             0 qmgr\r\n[613700.511796] [ 4041]     0  4041    90366      307     113      577             0 rsyslogd\r\n[613700.512756] [ 4060]     0  4060     2476        1      10       32             0 agetty\r\n[613700.513715] [ 4061]     0  4061     2476        1      10       31             0 agetty\r\n[613700.514736] [ 4064]     0  4064     6476        1      18       52             0 atd\r\n[613700.515708] [ 4065]     0  4065     6533      105      18       65             0 crond\r\n[613700.516641] [ 4110]     0  4110    28215       13      57      243         -1000 sshd\r\n[613700.517563] [ 5851]     0  5851     3247        0      11       47             0 sh\r\n[613700.518476] [ 5859]     0  5859   380762     1460     310    10577             0 node\r\n[613700.519397] [ 6163]     0  6163     3841       52      12       65             0 bash\r\n[613700.520320] [24945]     0 24945     3813        1      12      103             0 bash\r\n[613700.521241] [26835]     0 26835    38743       56      77      850             0 sshd\r\n[613700.522116] [26838]     0 26838     3779        2      12       65             0 bash\r\n[613700.523063] [26848]     0 26848     3314       40      13       98             0 bash\r\n[613700.523971] [26898]     0 26898   208799     8732     320     5172             0 node\r\n[613700.524890] [26914]     0 26914   218819     1071     149     2602             0 node\r\n[613700.525827] [30635]    89 30635    22438       10      46      240             0 pickup\r\n[613700.526717] [31250]     0 31250     5768        1      16       56             0 anacron\r\n[613700.527670] [32757]     0 32757   413109      921     220    12462             0 python3\r\n[613700.528622] [ 1083]   999  1014 11454714  7881670   16173   207603             0 TCPHandler\r\n[613700.529600] [ 1174]     0  1174     1941       18       9        0             0 sleep\r\n[613700.530531] [ 1344]     0  1344   123706     6927     125        0             0 clickhouse-clie\r\n[613700.531556] Out of memory: Kill process 1083 (TCPHandler) score 958 or sacrifice child\r\n[613700.532483] Killed process 1083 (TCPHandler) total-vm:45818856kB, anon-rss:31526680kB, file-rss:0kB, shmem-rss:0kB\r\n```\r\nand the corresponding `stderr.log` (no output to `clickhouse-server.err.log`)\r\n```\r\nProcessing configuration file '/etc/clickhouse-server/users.xml'.\r\nInclude not found: networks\r\nSaved preprocessed configuration to '/var/lib/clickhouse//preprocessed_configs/users.xml'.\r\nProcessing configuration file '/etc/clickhouse-server/dicts/benchmark_dictionary.xml'.\r\nSaved preprocessed configuration to '/var/lib/clickhouse//preprocessed_configs/dicts_benchmark_dictionary.xml'.\r\nProcessing configuration file '/etc/clickhouse-server/config.xml'.\r\nInclude not found: clickhouse_remote_servers\r\nInclude not found: clickhouse_compression\r\nSaved preprocessed configuration to '/var/lib/clickhouse//preprocessed_configs/config.xml'.\r\nProcessing configuration file '/etc/clickhouse-server/dicts/benchmark_dictionary.xml'.\r\nSaved preprocessed configuration to '/var/lib/clickhouse//preprocessed_configs/dicts_benchmark_dictionary.xml'.\r\nProcessing configuration file '/etc/clickhouse-server/dicts/benchmark_dictionary.xml'.\r\nSaved preprocessed configuration to '/var/lib/clickhouse//preprocessed_configs/dicts_benchmark_dictionary.xml'.\r\nStatus file /var/run/clickhouse-server/clickhouse-server.pid already exists - unclean restart. Contents:\r\n26053\r\nLogging trace to /var/log/clickhouse-server/clickhouse-server.log\r\nLogging errors to /var/log/clickhouse-server/clickhouse-server.err.log\r\nStatus file /var/lib/clickhouse/status already exists - unclean restart. Contents:\r\nPID: 26053\r\nStarted at: 2021-02-08 03:26:05\r\nRevision: 54438\r\n```\r\nHere is the dictionary xml file:\r\n```\r\n<yandex>\r\n        <dictionary> \r\n        <name>CacheDictionary</name> \r\n        <source>\r\n        <executable>\r\n            <command>\r\n            awk 'BEGIN { FS=\",\"; OFS=\",\"} { $1=$1; print }' /var/lib/clickhouse/user_files/testCache.csv \r\n            </command> \r\n            <format>CSV</format>\r\n        </executable>\r\n        </source>\r\n        <layout>\r\n        <cache>\r\n        <size_in_cells>7500000</size_in_cells>\r\n        </cache>\r\n        </layout>\r\n        <lifetime>0</lifetime>\r\n        <structure>\r\n            <id>\r\n                <name>referentialKey</name>\r\n            </id>\r\n\r\n            <attribute>\r\n                <name>date</name>\r\n                <type>Date</type>\r\n                <null_value></null_value>\r\n            </attribute>\r\n            <attribute>\r\n                <name>integers</name>\r\n                <type>UInt64</type>\r\n                <null_value></null_value>\r\n            </attribute>\r\n            <attribute>\r\n                <name>filterColumn</name>\r\n                <type>Float64</type>\r\n                <null_value></null_value>\r\n            </attribute>\r\n            \r\n            <attribute>\r\n                <name>random0</name>\r\n                <type>String</type>\r\n                <null_value></null_value>\r\n            </attribute>\r\n\r\n            ...\r\n            \r\n            <attribute>\r\n                <name>random74</name>\r\n                <type>String</type>\r\n                <null_value></null_value>\r\n            </attribute>\r\n            \r\n        </structure>\r\n        </dictionary>\r\n        </yandex>\r\n        \r\n```\n",
  "hints_text": "I looked throw some articles. Here are the best \r\n\r\n[High-Throughput, Thread-Safe, LRU Caching from Ebay](https://tech.ebayinc.com/engineering/high-throughput-thread-safe-lru-caching/)\r\n\r\n[High Concurrency LRU Caching written in Go](https://www.openmymind.net/High-Concurrency-LRU-Caching/)\r\n\r\n[Just cool idea](https://www.codeproject.com/Articles/23396/A-High-Performance-Multi-Threaded-LRU-Cache)\nWill fix https://github.com/ClickHouse/ClickHouse/issues/20194\n\nhttps://clickhouse-test-reports.s3.yandex.net/20012/5e350bb7619bcac0d07c728a0ff4797128255d7c/fuzzer_msan/server.log\n\nI've tried creating swap files but I don't really understand how they work, is there a tradeoff between swap files and available cache for example? If so what would be the optimal sizings for 16Gb Ram and 256Gb of storage.\nPlease share your dict definition. What is your size_in_cells? \n@filimonov  Size in cells would be 7500000 since I'd like to find the maximum number of items that I can run my query on. If I'm only using three of the columns should I instead be using 300000?\nsuch dictionary may eat 100GB RAM. \r\ncheck system.dictionaries.bytes_allocated with `<size_in_cells>10000`\r\n\r\nhttps://github.com/ClickHouse/ClickHouse/issues/2738\nIt says 9.7Gb allocated for 7500000 cells and 20MiB for 10000 cells, both are still beneath 16Gb. I also have a 1Gb swap file if that means anything.\n@MaxJPowers  Cache, ComplexKeyCache, SSDCache, SSDComplexKeyCache currently under big refactoring because we see a lot of problems and issues with memory consumption and performance, after refactoring they will be much more usable, related to your current problem it seems that reducing size of cells is the only workaround.\n@kitaisreal Thanks, is there a way to link this to the refactoring so once updates are made I can be notified?\n@MaxJPowers linked issue with dictionaries, I will try to make it part of our https://github.com/ClickHouse/ClickHouse/issues/20182 release.",
  "created_at": "2021-02-16T21:36:48Z"
}