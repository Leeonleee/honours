You will be provided with a partial code base and an issue statement explaining a problem to resolve.

<issue>
Segfault merging MT table with TTL and projection over `_part_offset`
### Company or project name

_No response_

### Describe what's wrong

A while after ingesting some data into a table with TTL of 1 day, I come back to the ClickHouse server to notice that it's not running, and when I try to restart it, it quickly segfaults with an error that appears to occur during merging projections.

### Does it reproduce on the most recent release?

Yes

### How to reproduce

* Server + Client version: 25.6.2.5
* Default settings

Here's the schema for my table:
```
CREATE TABLE otel.spans_local
(
    `span_id` FixedString(8) CODEC(NONE),
    `trace_id` FixedString(16) CODEC(ZSTD(1)),
    `parent_id` FixedString(8) CODEC(ZSTD(1)),
    `service` LowCardinality(String) CODEC(ZSTD(1)),
    `name` LowCardinality(String) CODEC(ZSTD(1)),
    `start_time` DateTime64(6) CODEC(Delta(8), ZSTD(1)),
    `end_time` DateTime64(6) CODEC(Delta(8), ZSTD(1)),
    `duration` UInt64 CODEC(ZSTD(1)),
    `logs.timestamps` Array(DateTime64(6)) CODEC(ZSTD(1)),
    `logs.labels.names` Array(Array(LowCardinality(String))) CODEC(ZSTD(1)),
    `logs.labels.values` Array(Array(String)) CODEC(ZSTD(1)),
    `references.trace_ids` Array(FixedString(16)) CODEC(ZSTD(1)),
    `references.span_ids` Array(FixedString(8)) CODEC(ZSTD(1)),
    `references.ref_types` Array(Enum8('CHILD_OF' = 0, 'FOLLOWS_FROM' = 1)) CODEC(ZSTD(1)),
    `labels.names` Array(LowCardinality(String)) CODEC(ZSTD(1)),
    `labels.values` Array(String) CODEC(ZSTD(1)),
    `process.labels` JSON(label1 LowCardinality(String), label2 LowCardinality(String), label3 LowCardinality(String), label4 LowCardinality(String), label5 LowCardinality(String), label6 LowCardinality(String), label7 LowCardinality(String), label8 LowCardinality(String), label10 LowCardinality(String), label11 LowCardinality(String), label12 LowCardinality(String), label13 LowCardinality(String), label14 LowCardinality(String)) CODEC(ZSTD(1)),
    `warnings` Array(String) CODEC(ZSTD(1)),
    `status` Enum8('UNSET' = 0, 'OK' = 1, 'ERROR' = 2) CODEC(ZSTD(1)),
    PROJECTION by_trace_id (SELECT _part_offset ORDER BY trace_id)
)
ENGINE = MergeTree
PARTITION BY toStartOfHour(start_time)
ORDER BY (service, name, toDateTime(start_time), trace_id, duration)
TTL toStartOfHour(start_time) + toIntervalDay(1)
SETTINGS index_granularity = 8192, ttl_only_drop_parts = 1
```
Notably. this schema is using some of the new projection index features contributed by @amosbird (Thank you!) -- could this be related, since the exception was thrown during projection part merging?


### Expected behavior

I expected the part merges to succeed, the TTL-expired parts to be dropped, and the projection parts' `_parent_part_offset` column to be updated accordingly.

### Error message and/or stacktrace

```
2025.07.03 11:56:31.070792 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> otel.spans_local (f698c668-b6d9-488b-9c05-5dc117635b99) (TTLTransform): Removed all rows from part 1751392800_66_21096_109_6576 due to expired TTL
2025.07.03 11:56:31.079903 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> MergeTask::MergeProjectionsStage: Merge sorted 73169 rows, containing 19 columns (19 merged, 0 gathered) in 0.162754486 sec., 449566.71731923876 rows/sec., 253.03 MiB/sec.
2025.07.03 11:56:31.080034 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> MergeTask::MergeProjectionsStage: Selected 6 projection_parts from by_trace_id to by_trace_id
2025.07.03 11:56:31.080157 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> MergeTask::PrepareStage: Merging 6 parts: from by_trace_id to by_trace_id into Wide with storage Full
2025.07.03 11:56:31.080263 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> MergeTask::PrepareStage: Selected MergeAlgorithm: Horizontal
2025.07.03 11:56:31.080419 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> MergeTreeSequentialSource: Reading 109 marks from part by_trace_id, total 877057 rows starting from the beginning of the part
2025.07.03 11:56:31.080495 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> MergeTreeSequentialSource: Reading 84 marks from part by_trace_id, total 679048 rows starting from the beginning of the part
2025.07.03 11:56:31.080543 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> MergeTreeSequentialSource: Reading 13 marks from part by_trace_id, total 92453 rows starting from the beginning of the part
2025.07.03 11:56:31.080585 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> MergeTreeSequentialSource: Reading 9 marks from part by_trace_id, total 58725 rows starting from the beginning of the part
2025.07.03 11:56:31.080624 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> MergeTreeSequentialSource: Reading 2 marks from part by_trace_id, total 171 rows starting from the beginning of the part
2025.07.03 11:56:31.080663 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> MergeTreeSequentialSource: Reading 2 marks from part by_trace_id, total 1 rows starting from the beginning of the part
2025.07.03 11:56:31.081570 [ 775284 ] {} <Trace> BaseDaemon: Received signal 11
2025.07.03 11:56:31.081970 [ 776029 ] {} <Fatal> BaseDaemon: ########## Short fault info ############
2025.07.03 11:56:31.082093 [ 776029 ] {} <Fatal> BaseDaemon: (version 25.6.2.5 (official build), build id: 55568BFED59571C903992F5F302516519AC535BB, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, architecture: x86_64) (from thread 775886) Received signal 11
2025.07.03 11:56:31.082122 [ 776029 ] {} <Fatal> BaseDaemon: Signal description: Segmentation fault
2025.07.03 11:56:31.082141 [ 776029 ] {} <Fatal> BaseDaemon: Address: NULL pointer. Access: read. Unknown si_code.
2025.07.03 11:56:31.082163 [ 776029 ] {} <Fatal> BaseDaemon: Stack trace: 0x000000001506006a 0x000000001563121e 0x0000000015630e27 0x000000001564d422 0x0000000015640f18 0x00000000156402f2 0x0000000015652e42 0x0000000015653059 0x0000000014e214d5 0x0000000014e20d8e 0x0000000014e2b222 0x0000000014e2b180 0x0000000014e2bcee 0x0000000014e2b222 0x0000000015298e2e 0x0000000014e5762d 0x000000000fd3ad2b 0x000000000fd4193d 0x000000000fd37f52 0x000000000fd3f41a 0x00007feb982081ea 0x00007feb97e39953
2025.07.03 11:56:31.082189 [ 776029 ] {} <Fatal> BaseDaemon: ########################################
2025.07.03 11:56:31.082214 [ 776029 ] {} <Fatal> BaseDaemon: (version 25.6.2.5 (official build), build id: 55568BFED59571C903992F5F302516519AC535BB, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d) (from thread 775886) (query_id: f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576) (query: ) Received signal Segmentation fault (11)
2025.07.03 11:56:31.082232 [ 776029 ] {} <Fatal> BaseDaemon: Address: NULL pointer. Access: read. Unknown si_code.
2025.07.03 11:56:31.082246 [ 776029 ] {} <Fatal> BaseDaemon: Stack trace: 0x000000001506006a 0x000000001563121e 0x0000000015630e27 0x000000001564d422 0x0000000015640f18 0x00000000156402f2 0x0000000015652e42 0x0000000015653059 0x0000000014e214d5 0x0000000014e20d8e 0x0000000014e2b222 0x0000000014e2b180 0x0000000014e2bcee 0x0000000014e2b222 0x0000000015298e2e 0x0000000014e5762d 0x000000000fd3ad2b 0x000000000fd4193d 0x000000000fd37f52 0x000000000fd3f41a 0x00007feb982081ea 0x00007feb97e39953
2025.07.03 11:56:31.082319 [ 776029 ] {} <Fatal> BaseDaemon: 2. DB::MergeTreeSequentialSource::generate() @ 0x000000001506006a
2025.07.03 11:56:31.082361 [ 776029 ] {} <Fatal> BaseDaemon: 3. DB::ISource::tryGenerate() @ 0x000000001563121e
2025.07.03 11:56:31.082381 [ 776029 ] {} <Fatal> BaseDaemon: 4. DB::ISource::work() @ 0x0000000015630e27
2025.07.03 11:56:31.082404 [ 776029 ] {} <Fatal> BaseDaemon: 5. DB::ExecutionThreadContext::executeTask() @ 0x000000001564d422
2025.07.03 11:56:31.082429 [ 776029 ] {} <Fatal> BaseDaemon: 6. DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*) @ 0x0000000015640f18
2025.07.03 11:56:31.082450 [ 776029 ] {} <Fatal> BaseDaemon: 7. DB::PipelineExecutor::executeStep(std::atomic<bool>*) @ 0x00000000156402f2
2025.07.03 11:56:31.082472 [ 776029 ] {} <Fatal> BaseDaemon: 8. DB::PullingPipelineExecutor::pull(DB::Chunk&) @ 0x0000000015652e42
2025.07.03 11:56:31.082490 [ 776029 ] {} <Fatal> BaseDaemon: 9. DB::PullingPipelineExecutor::pull(DB::Block&) @ 0x0000000015653059
2025.07.03 11:56:31.082511 [ 776029 ] {} <Fatal> BaseDaemon: 10. DB::MergeTask::ExecuteAndFinalizeHorizontalPart::executeImpl() const @ 0x0000000014e214d5
2025.07.03 11:56:31.082530 [ 776029 ] {} <Fatal> BaseDaemon: 11. DB::MergeTask::ExecuteAndFinalizeHorizontalPart::execute() @ 0x0000000014e20d8e
2025.07.03 11:56:31.082549 [ 776029 ] {} <Fatal> BaseDaemon: 12. DB::MergeTask::execute() @ 0x0000000014e2b222
2025.07.03 11:56:31.082571 [ 776029 ] {} <Fatal> BaseDaemon: 13. DB::MergeTask::MergeProjectionsStage::executeProjections() const @ 0x0000000014e2b180
2025.07.03 11:56:31.082590 [ 776029 ] {} <Fatal> BaseDaemon: 14. DB::MergeTask::MergeProjectionsStage::execute() @ 0x0000000014e2bcee
2025.07.03 11:56:31.082611 [ 776029 ] {} <Fatal> BaseDaemon: 15. DB::MergeTask::execute() @ 0x0000000014e2b222
2025.07.03 11:56:31.082631 [ 776029 ] {} <Fatal> BaseDaemon: 16. DB::MergePlainMergeTreeTask::executeStep() @ 0x0000000015298e2e
```

### Additional context

_No response_
Column TTL can generate part with incorrect skip indices and projections
### Company or project name

.

### Describe what's wrong

This occurs in wide part merges when fully expired columns are skipped.

### Does it reproduce on the most recent release?

Yes

### How to reproduce

https://fiddle.clickhouse.com/c4109b1b-9bef-4d2e-a5b7-f1a4720c169a

### Expected behavior

_No response_

### Error message and/or stacktrace

_No response_

### Additional context

_No response_
Min(col) vs Min(distinct col) return different value
### Company or project name

_No response_

### Describe what's wrong

![Image](https://github.com/user-attachments/assets/d1183c2e-2a52-4ce6-9627-6e860812a2fd)

How it possible?

The table engine is ReplicatedMergeTree and clickhouse version is 24.5.1.1763


EXPLAIN description = 1, indexes = 1, actions = 1
SELECT min(DateKey)
FROM HourlyImpressionShareOrderItemUsage

Query id: 85adce85-de7a-48f1-8c9e-b12f7c50ef7e

    ┌─explain─────────────────────────────────────────────────────────────────────────────────────────┐
 1. │ Expression ((Project names + Projection))                                                       │
 2. │ Actions: INPUT : 0 -> min(__table1.DateKey) Int32 : 0                                           │
 3. │          ALIAS min(__table1.DateKey) :: 0 -> min(DateKey) Int32 : 1                             │
 4. │ Positions: 1                                                                                    │
 5. │   Aggregating                                                                                   │
 6. │   Keys:                                                                                         │
 7. │   Aggregates:                                                                                   │
 8. │       min(__table1.DateKey)                                                                     │
 9. │         Function: min(Int32) → Int32                                                            │
10. │         Arguments: __table1.DateKey                                                             │
11. │   Skip merging: 0                                                                               │
12. │     Expression                                                                                  │
13. │     Actions: INPUT : 0 -> min(DateKey) AggregateFunction(min, Int32) : 0                        │
14. │              ALIAS min(DateKey) :: 0 -> min(__table1.DateKey) AggregateFunction(min, Int32) : 1 │
15. │     Positions: 1                                                                                │
16. │       ReadFromPreparedSource (_minmax_count_projection)                                         │
    └─────────────────────────────────────────────────────────────────────────────────────────────────┘



EXPLAIN description = 1, indexes = 1, actions = 1
SELECT minDistinct(DateKey)
FROM HourlyImpressionShareOrderItemUsage

Query id: ce4fb5da-063f-4f2e-ad2c-c820edb4e674

    ┌─explain─────────────────────────────────────────────────────────────────────────────┐
 1. │ Expression ((Project names + Projection))                                           │
 2. │ Actions: INPUT : 0 -> minDistinct(__table1.DateKey) Int32 : 0                       │
 3. │          ALIAS minDistinct(__table1.DateKey) :: 0 -> minDistinct(DateKey) Int32 : 1 │
 4. │ Positions: 1                                                                        │
 5. │   Aggregating                                                                       │
 6. │   Keys:                                                                             │
 7. │   Aggregates:                                                                       │
 8. │       minDistinct(__table1.DateKey)                                                 │
 9. │         Function: minDistinct(Int32) → Int32                                        │
10. │         Arguments: __table1.DateKey                                                 │
11. │   Skip merging: 0                                                                   │
12. │     Expression ((Before GROUP BY + Change column names to column identifiers))      │
13. │     Actions: INPUT : 0 -> DateKey Int32 : 0                                         │
14. │              ALIAS DateKey :: 0 -> __table1.DateKey Int32 : 1                       │
15. │     Positions: 1                                                                    │
16. │       ReadFromMergeTree (AdvertiserBI.HourlyImpressionShareOrderItemUsage)          │
17. │       ReadType: Default                                                             │
18. │       Parts: 14                                                                     │
19. │       Granules: 8372                                                                │
20. │       Indexes:                                                                      │
21. │         MinMax                                                                      │
22. │           Condition: true                                                           │
23. │           Parts: 14/14                                                              │
24. │           Granules: 8372/8372                                                       │
25. │         Partition                                                                   │
26. │           Condition: true                                                           │
27. │           Parts: 14/14                                                              │
28. │           Granules: 8372/8372                                                       │
29. │         PrimaryKey                                                                  │
30. │           Condition: true                                                           │
31. │           Parts: 14/14                                                              │
32. │           Granules: 8372/8372                                                       │
    └─────────────────────────────────────────────────────────────────────────────────────┘

### Does it reproduce on the most recent release?

Yes

### How to reproduce

NA

### Expected behavior

_No response_

### Error message and/or stacktrace

_No response_

### Additional context

_No response_
</issue>

I need you to solve the provided issue by generating a code fix that can be applied directly to the repository

Respond below:
