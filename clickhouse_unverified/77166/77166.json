{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 77166,
  "instance_id": "ClickHouse__ClickHouse-77166",
  "issue_numbers": [
    "83169",
    "77152",
    "77091"
  ],
  "base_commit": "ddbff557d1101ef967b7ded342809c134f3172f7",
  "patch": "diff --git a/docs/en/sql-reference/table-functions/mergeTreeIndex.md b/docs/en/sql-reference/table-functions/mergeTreeIndex.md\nindex 7a1db20ea5e9..59899dec4091 100644\n--- a/docs/en/sql-reference/table-functions/mergeTreeIndex.md\n+++ b/docs/en/sql-reference/table-functions/mergeTreeIndex.md\n@@ -14,20 +14,21 @@ Represents the contents of index and marks files of MergeTree tables. It can be\n ## Syntax {#syntax}\n \n ```sql\n-mergeTreeIndex(database, table, [with_marks = true])\n+mergeTreeIndex(database, table [, with_marks = true] [, with_minmax = true])\n ```\n \n ## Arguments {#arguments}\n \n-| Argument     | Description                                       |\n-|--------------|---------------------------------------------------|\n-| `database`   | The database name to read index and marks from.   |\n-| `table`      | The table name to read index and marks from.      |\n-| `with_marks` | Whether include columns with marks to the result. |\n+| Argument      | Description                                       |\n+|---------------|---------------------------------------------------|\n+| `database`    | The database name to read index and marks from.   |\n+| `table`       | The table name to read index and marks from.      |\n+| `with_marks`  | Whether include columns with marks to the result. |\n+| `with_minmax` | Whether include min-max index to the result.      |\n \n ## Returned value {#returned_value}\n \n-A table object with columns with values of primary index of source table, columns with values of marks (if enabled) for all possible files in data parts of source table and virtual columns:\n+A table object with columns with values of primary index and min-max index (if enabled) of source table, columns with values of marks (if enabled) for all possible files in data parts of source table and virtual columns:\n \n - `part_name` - The name of data part.\n - `mark_number` - The number of current mark in data part.\ndiff --git a/src/Processors/TTL/TTLColumnAlgorithm.cpp b/src/Processors/TTL/TTLColumnAlgorithm.cpp\nindex 4b8f6b6f9ba2..d454dc373181 100644\n--- a/src/Processors/TTL/TTLColumnAlgorithm.cpp\n+++ b/src/Processors/TTL/TTLColumnAlgorithm.cpp\n@@ -42,9 +42,17 @@ void TTLColumnAlgorithm::execute(Block & block)\n     if (!isMinTTLExpired())\n         return;\n \n-    /// Later drop full column\n+    auto & column_with_type = block.getByName(column_name);\n+\n+    /// Reset the column to its default state so that dependent skip indices or\n+    /// projections can correctly recalculate using it.\n     if (isMaxTTLExpired() && !is_compact_part)\n+    {\n+        auto empty_column = column_with_type.column->cloneEmpty();\n+        empty_column->insertManyDefaults(block.rows());\n+        column_with_type.column = std::move(empty_column);\n         return;\n+    }\n \n     auto default_column = executeExpressionAndGetColumn(default_expression, block, default_column_name);\n     if (default_column)\n@@ -52,7 +60,6 @@ void TTLColumnAlgorithm::execute(Block & block)\n \n     auto ttl_column = executeExpressionAndGetColumn(ttl_expressions.expression, block, description.result_column);\n \n-    auto & column_with_type = block.getByName(column_name);\n     const IColumn * values_column = column_with_type.column.get();\n     MutableColumnPtr result_column = values_column->cloneEmpty();\n     result_column->reserve(block.rows());\ndiff --git a/src/Storages/MergeTree/Compaction/MergePredicates/MergeTreeMergePredicate.cpp b/src/Storages/MergeTree/Compaction/MergePredicates/MergeTreeMergePredicate.cpp\nindex acccdf971aa6..588daddf5b08 100644\n--- a/src/Storages/MergeTree/Compaction/MergePredicates/MergeTreeMergePredicate.cpp\n+++ b/src/Storages/MergeTree/Compaction/MergePredicates/MergeTreeMergePredicate.cpp\n@@ -40,8 +40,11 @@ std::expected<void, PreformattedMessage> MergeTreeMergePredicate::canMergeParts(\n     if (left.projection_names != right.projection_names)\n     {\n         return std::unexpected(PreformattedMessage::create(\n-            \"Parts have different projection sets: {{}} in {} and {{}} in {}\",\n-            fmt::join(left.projection_names, \", \"), left.name, fmt::join(right.projection_names, \", \"), right.name));\n+            \"Parts have different projection sets: {{{}}} in {} and {{{}}} in {}\",\n+            fmt::join(left.projection_names, \", \"),\n+            left.name,\n+            fmt::join(right.projection_names, \", \"),\n+            right.name));\n     }\n \n     {\ndiff --git a/src/Storages/MergeTree/MergeTask.cpp b/src/Storages/MergeTree/MergeTask.cpp\nindex 902362e169ce..6d4d0b73746d 100644\n--- a/src/Storages/MergeTree/MergeTask.cpp\n+++ b/src/Storages/MergeTree/MergeTask.cpp\n@@ -118,6 +118,7 @@ namespace MergeTreeSetting\n     extern const MergeTreeSettingsBool prewarm_mark_cache;\n     extern const MergeTreeSettingsBool use_const_adaptive_granularity;\n     extern const MergeTreeSettingsUInt64 max_merge_delayed_streams_for_parallel_write;\n+    extern const MergeTreeSettingsBool ttl_only_drop_parts;\n }\n \n namespace ErrorCodes\n@@ -307,6 +308,13 @@ void MergeTask::ExecuteAndFinalizeHorizontalPart::extractMergingAndGatheringColu\n     if (key_columns.empty())\n         key_columns.emplace(global_ctx->storage_columns.front().name);\n \n+    /// Recalculate the min-max index for partition columns if the merge might reduce rows.\n+    if (global_ctx->merge_may_reduce_rows)\n+    {\n+        auto minmax_columns = MergeTreeData::getMinMaxColumnsNames(global_ctx->metadata_snapshot->getPartitionKey());\n+        key_columns.insert(minmax_columns.begin(), minmax_columns.end());\n+    }\n+\n     const auto & skip_indexes = global_ctx->metadata_snapshot->getSecondaryIndices();\n \n     for (const auto & index : skip_indexes)\n@@ -446,10 +454,86 @@ bool MergeTask::ExecuteAndFinalizeHorizontalPart::prepare() const\n     extendObjectColumns(global_ctx->storage_columns, object_columns, false);\n     global_ctx->storage_snapshot = std::make_shared<StorageSnapshot>(*global_ctx->data, global_ctx->metadata_snapshot, std::move(object_columns));\n \n+    ctx->need_remove_expired_values = false;\n+    ctx->force_ttl = false;\n+    for (const auto & part : global_ctx->future_part->parts)\n+    {\n+        global_ctx->new_data_part->ttl_infos.update(part->ttl_infos);\n+\n+        if (global_ctx->metadata_snapshot->hasAnyTTL() && !part->checkAllTTLCalculated(global_ctx->metadata_snapshot))\n+        {\n+            LOG_INFO(ctx->log, \"Some TTL values were not calculated for part {}. Will calculate them forcefully during merge.\", part->name);\n+            ctx->need_remove_expired_values = true;\n+            ctx->force_ttl = true;\n+        }\n+    }\n+\n+    const auto & local_part_min_ttl = global_ctx->new_data_part->ttl_infos.part_min_ttl;\n+    if (local_part_min_ttl && local_part_min_ttl <= global_ctx->time_of_merge)\n+        ctx->need_remove_expired_values = true;\n+\n+    if (ctx->need_remove_expired_values && global_ctx->ttl_merges_blocker->isCancelled())\n+    {\n+        LOG_INFO(ctx->log, \"Part {} has values with expired TTL, but merges with TTL are cancelled.\", global_ctx->new_data_part->name);\n+        ctx->need_remove_expired_values = false;\n+    }\n+\n+    const auto & patch_parts = global_ctx->future_part->patch_parts;\n+\n+    /// Skip fully expired columns manually, since in case of\n+    /// need_remove_expired_values is not set, TTLTransform will not be used,\n+    /// and columns that had been removed by TTL (via TTLColumnAlgorithm) will\n+    /// be added again with default values.\n+    ///\n+    /// Also note, that it is better to do this here, since in other places it\n+    /// will be too late (i.e. they will be written, and we will burn CPU/disk\n+    /// resources for this).\n+    if (!ctx->need_remove_expired_values)\n+    {\n+        for (auto & [column_name, ttl] : global_ctx->new_data_part->ttl_infos.columns_ttl)\n+        {\n+            if (ttl.finished())\n+            {\n+                global_ctx->new_data_part->expired_columns.insert(column_name);\n+                LOG_TRACE(ctx->log, \"Adding expired column {} for part {}\", column_name, global_ctx->new_data_part->name);\n+            }\n+        }\n+    }\n+\n+    /// Determine whether projections and minmax indexes need to be updated during merge,\n+    /// which typically happens when the number of rows may be reduced.\n+    ///\n+    /// This is necessary in cases such as TTL expiration, cleanup merges, deduplication,\n+    /// or special merge modes like Collapsing/Replacing.\n+    global_ctx->merge_may_reduce_rows =\n+        ctx->need_remove_expired_values ||\n+        !patch_parts.empty() ||\n+        global_ctx->cleanup ||\n+        global_ctx->deduplicate ||\n+        global_ctx->merging_params.mode == MergeTreeData::MergingParams::Collapsing ||\n+        global_ctx->merging_params.mode == MergeTreeData::MergingParams::Replacing ||\n+        global_ctx->merging_params.mode == MergeTreeData::MergingParams::VersionedCollapsing;\n+\n     prepareProjectionsToMergeAndRebuild();\n \n     extractMergingAndGatheringColumns();\n \n+    const auto & expired_columns = global_ctx->new_data_part->expired_columns;\n+    if (!expired_columns.empty())\n+    {\n+        auto part_serialization_infos = global_ctx->new_data_part->getSerializationInfos();\n+        for (const auto & expired_column : expired_columns)\n+            part_serialization_infos.erase(expired_column);\n+\n+        global_ctx->gathering_columns = global_ctx->gathering_columns.eraseNames(expired_columns);\n+        global_ctx->merging_columns = global_ctx->merging_columns.eraseNames(expired_columns);\n+        global_ctx->storage_columns = global_ctx->storage_columns.eraseNames(expired_columns);\n+        global_ctx->new_data_part->setColumns(\n+            global_ctx->storage_columns,\n+            part_serialization_infos,\n+            global_ctx->metadata_snapshot->getMetadataVersion());\n+    }\n+\n     global_ctx->new_data_part->uuid = global_ctx->future_part->uuid;\n     global_ctx->new_data_part->partition.assign(global_ctx->future_part->getPartition());\n     global_ctx->new_data_part->is_temp = global_ctx->parent_part == nullptr;\n@@ -459,17 +543,12 @@ bool MergeTask::ExecuteAndFinalizeHorizontalPart::prepare() const\n     /// The blobs have to be removed along with the part, this temporary part owns them and does not share them yet.\n     global_ctx->new_data_part->remove_tmp_policy = IMergeTreeDataPart::BlobsRemovalPolicyForTemporaryParts::REMOVE_BLOBS;\n \n-    ctx->need_remove_expired_values = false;\n-    ctx->force_ttl = false;\n-\n     if (enabledBlockNumberColumn(global_ctx))\n         addGatheringColumn(global_ctx, BlockNumberColumn::name, BlockNumberColumn::type);\n \n     if (enabledBlockOffsetColumn(global_ctx))\n         addGatheringColumn(global_ctx, BlockOffsetColumn::name, BlockOffsetColumn::type);\n \n-    const auto & patch_parts = global_ctx->future_part->patch_parts;\n-\n     MergeTreeData::IMutationsSnapshot::Params params\n     {\n         .metadata_version = global_ctx->metadata_snapshot->getMetadataVersion(),\n@@ -506,15 +585,6 @@ bool MergeTask::ExecuteAndFinalizeHorizontalPart::prepare() const\n \n     for (const auto & part : global_ctx->future_part->parts)\n     {\n-        global_ctx->new_data_part->ttl_infos.update(part->ttl_infos);\n-\n-        if (global_ctx->metadata_snapshot->hasAnyTTL() && !part->checkAllTTLCalculated(global_ctx->metadata_snapshot))\n-        {\n-            LOG_INFO(ctx->log, \"Some TTL values were not calculated for part {}. Will calculate them forcefully during merge.\", part->name);\n-            ctx->need_remove_expired_values = true;\n-            ctx->force_ttl = true;\n-        }\n-\n         if (!info_settings.isAlwaysDefault())\n         {\n             auto part_infos = part->getSerializationInfos();\n@@ -540,16 +610,6 @@ bool MergeTask::ExecuteAndFinalizeHorizontalPart::prepare() const\n \n     global_ctx->new_data_part->setColumns(global_ctx->storage_columns, infos, global_ctx->metadata_snapshot->getMetadataVersion());\n \n-    const auto & local_part_min_ttl = global_ctx->new_data_part->ttl_infos.part_min_ttl;\n-    if (local_part_min_ttl && local_part_min_ttl <= global_ctx->time_of_merge)\n-        ctx->need_remove_expired_values = true;\n-\n-    if (ctx->need_remove_expired_values && global_ctx->ttl_merges_blocker->isCancelled())\n-    {\n-        LOG_INFO(ctx->log, \"Part {} has values with expired TTL, but merges with TTL are cancelled.\", global_ctx->new_data_part->name);\n-        ctx->need_remove_expired_values = false;\n-    }\n-\n     ctx->sum_input_rows_upper_bound = global_ctx->merge_list_element_ptr->total_rows_count;\n     ctx->sum_compressed_bytes_upper_bound = global_ctx->merge_list_element_ptr->total_size_bytes_compressed;\n     ctx->sum_uncompressed_bytes_upper_bound = global_ctx->merge_list_element_ptr->total_size_bytes_uncompressed;\n@@ -615,43 +675,6 @@ bool MergeTask::ExecuteAndFinalizeHorizontalPart::prepare() const\n     /// Merged stream will be created and available as merged_stream variable\n     createMergedStream();\n \n-    /// Skip fully expired columns manually, since in case of\n-    /// need_remove_expired_values is not set, TTLTransform will not be used,\n-    /// and columns that had been removed by TTL (via TTLColumnAlgorithm) will\n-    /// be added again with default values.\n-    ///\n-    /// Also note, that it is better to do this here, since in other places it\n-    /// will be too late (i.e. they will be written, and we will burn CPU/disk\n-    /// resources for this).\n-    if (!ctx->need_remove_expired_values)\n-    {\n-        auto part_serialization_infos = global_ctx->new_data_part->getSerializationInfos();\n-\n-        NameSet columns_to_remove;\n-        for (auto & [column_name, ttl] : global_ctx->new_data_part->ttl_infos.columns_ttl)\n-        {\n-            if (ttl.finished())\n-            {\n-                global_ctx->new_data_part->expired_columns.insert(column_name);\n-                LOG_TRACE(ctx->log, \"Adding expired column {} for part {}\", column_name, global_ctx->new_data_part->name);\n-                columns_to_remove.insert(column_name);\n-                part_serialization_infos.erase(column_name);\n-            }\n-        }\n-\n-        if (!columns_to_remove.empty())\n-        {\n-            global_ctx->gathering_columns = global_ctx->gathering_columns.eraseNames(columns_to_remove);\n-            global_ctx->merging_columns = global_ctx->merging_columns.eraseNames(columns_to_remove);\n-            global_ctx->storage_columns = global_ctx->storage_columns.eraseNames(columns_to_remove);\n-\n-            global_ctx->new_data_part->setColumns(\n-                global_ctx->storage_columns,\n-                part_serialization_infos,\n-                global_ctx->metadata_snapshot->getMetadataVersion());\n-        }\n-    }\n-\n     auto index_granularity_ptr = createMergeTreeIndexGranularity(\n         ctx->sum_input_rows_upper_bound,\n         ctx->sum_uncompressed_bytes_upper_bound,\n@@ -783,20 +806,25 @@ void MergeTask::ExecuteAndFinalizeHorizontalPart::prepareProjectionsToMergeAndRe\n         && (mode == DeduplicateMergeProjectionMode::THROW || mode == DeduplicateMergeProjectionMode::DROP))\n         return;\n \n-    /// These merging modes may or may not reduce number of rows. It's not known until the horizontal stage is finished.\n-    const bool merge_may_reduce_rows =\n-        global_ctx->cleanup ||\n-        global_ctx->deduplicate ||\n-        global_ctx->merging_params.mode == MergeTreeData::MergingParams::Collapsing ||\n-        global_ctx->merging_params.mode == MergeTreeData::MergingParams::Replacing ||\n-        global_ctx->merging_params.mode == MergeTreeData::MergingParams::VersionedCollapsing;\n-\n     const auto & projections = global_ctx->metadata_snapshot->getProjections();\n \n     for (const auto & projection : projections)\n     {\n-        /// Checking IGNORE here is just for compatibility.\n-        if (merge_may_reduce_rows && mode != DeduplicateMergeProjectionMode::IGNORE)\n+        const auto & required_columns = projection.getRequiredColumns();\n+        bool some_source_column_expired = std::any_of(\n+            required_columns.begin(),\n+            required_columns.end(),\n+            [&](const String & name) { return global_ctx->new_data_part->expired_columns.contains(name); });\n+\n+        /// The IGNORE mode is checked here purely for backward compatibility.\n+        /// However, if the projection contains `_parent_part_offset`, it must still be rebuilt,\n+        /// since offset correctness cannot be ignored even in IGNORE mode.\n+        if (global_ctx->merge_may_reduce_rows && (mode != DeduplicateMergeProjectionMode::IGNORE || projection.with_parent_part_offset))\n+        {\n+            global_ctx->projections_to_rebuild.push_back(&projection);\n+            continue;\n+        }\n+        else if (some_source_column_expired && mode != DeduplicateMergeProjectionMode::IGNORE)\n         {\n             global_ctx->projections_to_rebuild.push_back(&projection);\n             continue;\n@@ -951,6 +979,12 @@ bool MergeTask::ExecuteAndFinalizeHorizontalPart::executeImpl() const\n         global_ctx->rows_written += block.rows();\n         const_cast<MergedBlockOutputStream &>(*global_ctx->to).write(block);\n \n+        if (global_ctx->merge_may_reduce_rows)\n+        {\n+            global_ctx->new_data_part->minmax_idx->update(\n+                block, MergeTreeData::getMinMaxColumnsNames(global_ctx->metadata_snapshot->getPartitionKey()));\n+        }\n+\n         calculateProjections(block);\n \n         UInt64 result_rows = 0;\n@@ -1316,14 +1350,17 @@ bool MergeTask::VerticalMergeStage::finalizeVerticalMergeForAllColumns() const\n \n bool MergeTask::MergeProjectionsStage::mergeMinMaxIndexAndPrepareProjections() const\n {\n-    for (const auto & part : global_ctx->future_part->parts)\n+    if (!global_ctx->merge_may_reduce_rows)\n     {\n-        /// Skip empty parts,\n-        /// (that can be created in StorageReplicatedMergeTree::createEmptyPartInsteadOfLost())\n-        /// since they can incorrectly set min,\n-        /// that will be changed after one more merge/OPTIMIZE.\n-        if (!part->isEmpty())\n-            global_ctx->new_data_part->minmax_idx->merge(*part->minmax_idx);\n+        for (const auto & part : global_ctx->future_part->parts)\n+        {\n+            /// Skip empty parts,\n+            /// (that can be created in StorageReplicatedMergeTree::createEmptyPartInsteadOfLost())\n+            /// since they can incorrectly set min,\n+            /// that will be changed after one more merge/OPTIMIZE.\n+            if (!part->isEmpty())\n+                global_ctx->new_data_part->minmax_idx->merge(*part->minmax_idx);\n+        }\n     }\n \n     /// Print overall profiling info. NOTE: it may duplicates previous messages\n@@ -1356,6 +1393,16 @@ bool MergeTask::MergeProjectionsStage::mergeMinMaxIndexAndPrepareProjections() c\n             projection_parts.front()->name,\n             projection_parts.back()->name);\n \n+        /// Skip parts with empty parent parts.\n+        chassert(global_ctx->future_part->parts.size() == projection_parts.size());\n+        std::erase_if(\n+            projection_parts,\n+            [&](const auto & part)\n+            {\n+                size_t index = &part - projection_parts.data();\n+                return global_ctx->future_part->parts[index]->isEmpty();\n+            });\n+\n         auto projection_future_part = std::make_shared<FutureMergedMutatedPart>();\n         projection_future_part->assign(std::move(projection_parts), /*patch_parts_=*/ {});\n         projection_future_part->name = projection->name;\ndiff --git a/src/Storages/MergeTree/MergeTask.h b/src/Storages/MergeTree/MergeTask.h\nindex 51a63ae42265..d280a13fc5f0 100644\n--- a/src/Storages/MergeTree/MergeTask.h\n+++ b/src/Storages/MergeTree/MergeTask.h\n@@ -239,6 +239,9 @@ class MergeTask\n \n         UInt64 prev_elapsed_ms{0};\n \n+        /// Current merge may or may not reduce number of rows. It's not known until the horizontal stage is finished.\n+        bool merge_may_reduce_rows{false};\n+\n         // will throw an exception if merge was cancelled in any way.\n         void checkOperationIsNotCanceled() const;\n         bool isCancelled() const;\ndiff --git a/src/Storages/StorageMergeTreeIndex.cpp b/src/Storages/StorageMergeTreeIndex.cpp\nindex 479825e954ff..0a9354c5b4f7 100644\n--- a/src/Storages/StorageMergeTreeIndex.cpp\n+++ b/src/Storages/StorageMergeTreeIndex.cpp\n@@ -16,6 +16,7 @@\n #include <Access/Common/AccessFlags.h>\n #include <Common/HashTable/HashSet.h>\n #include <Common/escapeForFileName.h>\n+#include <Interpreters/ExpressionActions.h>\n #include <Processors/QueryPlan/QueryPlan.h>\n #include <Processors/QueryPlan/SourceStepWithFilter.h>\n #include <Processors/ISource.h>\n@@ -28,6 +29,7 @@ namespace DB\n namespace ErrorCodes\n {\n     extern const int BAD_ARGUMENTS;\n+    extern const int CORRUPTED_DATA;\n     extern const int NO_SUCH_COLUMN_IN_TABLE;\n     extern const int NOT_IMPLEMENTED;\n }\n@@ -38,6 +40,7 @@ class MergeTreeIndexSource : public ISource, WithContext\n     MergeTreeIndexSource(\n         Block header_,\n         Block index_header_,\n+        Block minmax_header_,\n         MergeTreeData::DataPartsVector data_parts_,\n         ContextPtr context_,\n         bool with_marks_)\n@@ -45,6 +48,7 @@ class MergeTreeIndexSource : public ISource, WithContext\n         , WithContext(context_)\n         , header(std::move(header_))\n         , index_header(std::move(index_header_))\n+        , minmax_header(std::move(minmax_header_))\n         , data_parts(std::move(data_parts_))\n         , with_marks(with_marks_)\n     {\n@@ -104,6 +108,22 @@ class MergeTreeIndexSource : public ISource, WithContext\n                     result_columns[pos] = index_column->convertToFullColumnIfConst();\n                 }\n             }\n+            else if (minmax_header.has(column_name))\n+            {\n+                size_t minmax_pos = minmax_header.getPositionByName(column_name);\n+                if (minmax_pos >= part->minmax_idx->hyperrectangle.size())\n+                    throw Exception(\n+                        ErrorCodes::CORRUPTED_DATA,\n+                        \"Part {} has broken minmax_idx: size = {} but {} has pos = {}\",\n+                        part->name,\n+                        part->minmax_idx->hyperrectangle.size(),\n+                        column_name,\n+                        minmax_pos);\n+\n+                auto column = column_type->createColumnConst(\n+                    num_rows, Tuple{part->minmax_idx->hyperrectangle[minmax_pos].left, part->minmax_idx->hyperrectangle[minmax_pos].right});\n+                result_columns[pos] = column->convertToFullColumnIfConst();\n+            }\n             else if (column_name == part_name_column.name)\n             {\n                 auto column = column_type->createColumnConst(num_rows, part->name);\n@@ -236,6 +256,7 @@ class MergeTreeIndexSource : public ISource, WithContext\n \n     Block header;\n     Block index_header;\n+    Block minmax_header;\n     MergeTreeData::DataPartsVector data_parts;\n     bool with_marks;\n \n@@ -251,18 +272,33 @@ StorageMergeTreeIndex::StorageMergeTreeIndex(\n     const StorageID & table_id_,\n     const StoragePtr & source_table_,\n     const ColumnsDescription & columns,\n-    bool with_marks_)\n+    bool with_marks_,\n+    bool with_minmax_)\n     : IStorage(table_id_)\n     , source_table(source_table_)\n     , with_marks(with_marks_)\n+    , with_minmax(with_minmax_)\n {\n     const auto * merge_tree = dynamic_cast<const MergeTreeData *>(source_table.get());\n     if (!merge_tree)\n         throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Storage MergeTreeIndex expected MergeTree table, got: {}\", source_table->getName());\n \n     data_parts = merge_tree->getDataPartsVectorForInternalUsage();\n+    std::erase_if(data_parts, [](const MergeTreeData::DataPartPtr & part) { return part->isEmpty(); });\n+\n     key_sample_block = merge_tree->getInMemoryMetadataPtr()->getPrimaryKey().sample_block;\n \n+    if (with_minmax)\n+    {\n+        const auto & partition_key = merge_tree->getInMemoryMetadataPtr()->getPartitionKey();\n+        if (!partition_key.column_names.empty() && partition_key.expression)\n+        {\n+            for (const auto & column : partition_key.expression->getRequiredColumnsWithTypes())\n+                minmax_sample_block.insert(\n+                    {nullptr, std::make_shared<DataTypeTuple>(DataTypes{column.type, column.type}), fmt::format(\"minmax_{}\", column.name)});\n+        }\n+    }\n+\n     StorageInMemoryMetadata storage_metadata;\n     storage_metadata.setColumns(columns);\n     setInMemoryMetadata(storage_metadata);\n@@ -368,12 +404,19 @@ void ReadFromMergeTreeIndex::initializePipeline(QueryPipelineBuilder & pipeline,\n {\n     auto filtered_parts = storage->getFilteredDataParts(virtual_columns_filter);\n \n-    LOG_DEBUG(log, \"Reading index{}from {} parts of table {}\",\n-        storage->with_marks ? \" with marks \" : \" \",\n+    LOG_DEBUG(log, \"Reading index{}{} from {} parts of table {}\",\n+        storage->with_marks ? \" with marks\" : \"\",\n+        storage->with_minmax ? \" with minmax_idx\" : \"\",\n         filtered_parts.size(),\n         storage->source_table->getStorageID().getNameForLogs());\n \n-    pipeline.init(Pipe(std::make_shared<MergeTreeIndexSource>(getOutputHeader(), storage->key_sample_block, std::move(filtered_parts), context, storage->with_marks)));\n+    pipeline.init(Pipe(std::make_shared<MergeTreeIndexSource>(\n+        getOutputHeader(),\n+        storage->key_sample_block,\n+        storage->minmax_sample_block,\n+        std::move(filtered_parts),\n+        context,\n+        storage->with_marks)));\n }\n \n MergeTreeData::DataPartsVector StorageMergeTreeIndex::getFilteredDataParts(const ExpressionActionsPtr & virtual_columns_filter) const\ndiff --git a/src/Storages/StorageMergeTreeIndex.h b/src/Storages/StorageMergeTreeIndex.h\nindex ed8274d7d92e..04deefabe5e2 100644\n--- a/src/Storages/StorageMergeTreeIndex.h\n+++ b/src/Storages/StorageMergeTreeIndex.h\n@@ -19,7 +19,8 @@ class StorageMergeTreeIndex final : public IStorage\n         const StorageID & table_id_,\n         const StoragePtr & source_table_,\n         const ColumnsDescription & columns,\n-        bool with_marks_);\n+        bool with_marks_,\n+        bool with_minmax_);\n \n     void read(\n         QueryPlan & query_plan,\n@@ -40,9 +41,11 @@ class StorageMergeTreeIndex final : public IStorage\n \n     StoragePtr source_table;\n     bool with_marks;\n+    bool with_minmax;\n \n     MergeTreeData::DataPartsVector data_parts;\n     Block key_sample_block;\n+    Block minmax_sample_block;\n };\n \n }\ndiff --git a/src/TableFunctions/TableFunctionMergeTreeIndex.cpp b/src/TableFunctions/TableFunctionMergeTreeIndex.cpp\nindex b8ffcf86136b..863427af4790 100644\n--- a/src/TableFunctions/TableFunctionMergeTreeIndex.cpp\n+++ b/src/TableFunctions/TableFunctionMergeTreeIndex.cpp\n@@ -1,6 +1,7 @@\n #include <Storages/StorageMergeTreeIndex.h>\n #include <TableFunctions/ITableFunction.h>\n #include <Interpreters/DatabaseCatalog.h>\n+#include <Interpreters/ExpressionActions.h>\n #include <Interpreters/evaluateConstantExpression.h>\n #include <Storages/checkAndGetLiteralArgument.h>\n #include <TableFunctions/TableFunctionFactory.h>\n@@ -43,6 +44,7 @@ class TableFunctionMergeTreeIndex : public ITableFunction\n \n     StorageID source_table_id{StorageID::createEmpty()};\n     bool with_marks = false;\n+    bool with_minmax = false;\n };\n \n void TableFunctionMergeTreeIndex::parseArguments(const ASTPtr & ast_function, ContextPtr context)\n@@ -66,20 +68,29 @@ void TableFunctionMergeTreeIndex::parseArguments(const ASTPtr & ast_function, Co\n     if (!rest_args.empty())\n     {\n         auto params = getParamsMapFromAST(rest_args, context);\n-        auto param = params.extract(\"with_marks\");\n \n-        if (!param.empty())\n+        auto extract_flag = [&](auto param, const String & param_name) -> UInt64\n         {\n-            auto & value = param.mapped();\n-            if (value.getType() != Field::Types::Bool && value.getType() != Field::Types::UInt64)\n-                throw Exception(ErrorCodes::BAD_ARGUMENTS,\n-                    \"Table function '{}' expected bool flag for 'with_marks' argument\", getName());\n-\n-            if (value.getType() == Field::Types::Bool)\n-                with_marks = value.safeGet<bool>();\n+            if (!param.empty())\n+            {\n+                auto & value = param.mapped();\n+                if (value.getType() != Field::Types::Bool && value.getType() != Field::Types::UInt64)\n+                    throw Exception(ErrorCodes::BAD_ARGUMENTS,\n+                                    \"Table function '{}' expected bool flag for '{}' argument\", getName(), param_name);\n+\n+                if (value.getType() == Field::Types::Bool)\n+                    return value.template safeGet<bool>();\n+                else\n+                    return value.template safeGet<UInt64>();\n+            }\n             else\n-                with_marks = value.safeGet<UInt64>();\n-        }\n+            {\n+                return 0;\n+            }\n+        };\n+\n+        with_marks = extract_flag(params.extract(\"with_marks\"), \"with_marks\");\n+        with_minmax = extract_flag(params.extract(\"with_minmax\"), \"with_minmax\");\n \n         if (!params.empty())\n         {\n@@ -99,7 +110,7 @@ static NameSet getAllPossibleStreamNames(\n     NameSet all_streams;\n \n     /// Add the stream with the name of column\n-    /// because it may be abcent in serialization streams (e.g. for Tuple type)\n+    /// because it may be absent in serialization streams (e.g. for Tuple type)\n     /// but in compact parts we write only marks for whole columns, not subsubcolumns.\n     auto main_stream_name = escapeForFileName(column.name);\n     all_streams.insert(Nested::concatenateName(main_stream_name, \"mark\"));\n@@ -140,6 +151,16 @@ ColumnsDescription TableFunctionMergeTreeIndex::getActualTableStructure(ContextP\n     for (const auto & column : StorageMergeTreeIndex::virtuals_sample_block)\n         columns.add({column.name, column.type});\n \n+    if (with_minmax)\n+    {\n+        const auto & partition_key = metadata_snapshot->getPartitionKey();\n+        if (!partition_key.column_names.empty() && partition_key.expression)\n+        {\n+            for (const auto & column : partition_key.expression->getRequiredColumnsWithTypes())\n+                columns.add({fmt::format(\"minmax_{}\", column.name), std::make_shared<DataTypeTuple>(DataTypes{column.type, column.type})});\n+        }\n+    }\n+\n     for (const auto & column : metadata_snapshot->getPrimaryKey().sample_block)\n         columns.add({column.name, column.type});\n \n@@ -183,7 +204,8 @@ StoragePtr TableFunctionMergeTreeIndex::executeImpl(\n     auto columns = getActualTableStructure(context, is_insert_query);\n \n     StorageID storage_id(getDatabaseName(), table_name);\n-    auto res = std::make_shared<StorageMergeTreeIndex>(std::move(storage_id), std::move(source_table), std::move(columns), with_marks);\n+    auto res = std::make_shared<StorageMergeTreeIndex>(\n+        std::move(storage_id), std::move(source_table), std::move(columns), with_marks, with_minmax);\n \n     res->startup();\n     return res;\n@@ -196,7 +218,7 @@ void registerTableFunctionMergeTreeIndex(TableFunctionFactory & factory)\n         .documentation =\n         {\n             .description = \"Represents the contents of index and marks files of MergeTree tables. It can be used for introspection\",\n-            .examples = {{\"mergeTreeIndex\", \"SELECT * FROM mergeTreeIndex(currentDatabase(), mt_table, with_marks = true)\", \"\"}},\n+            .examples = {{\"mergeTreeIndex\", \"SELECT * FROM mergeTreeIndex(currentDatabase(), mt_table, with_marks = true, with_minmax = true)\", \"\"}},\n             .category = FunctionDocumentation::Category::TableFunction\n         },\n         .allow_readonly = true,\n",
  "test_patch": "diff --git a/tests/queries/0_stateless/03364_ttl_should_recalculate_minmax_index.reference b/tests/queries/0_stateless/03364_ttl_should_recalculate_minmax_index.reference\nnew file mode 100644\nindex 000000000000..2f19e2ca4d9c\n--- /dev/null\n+++ b/tests/queries/0_stateless/03364_ttl_should_recalculate_minmax_index.reference\n@@ -0,0 +1,14 @@\n+-- { echoOn }\n+\n+drop table if exists x;\n+create table x (dt DateTime, i Int32) engine MergeTree partition by indexHint(dt) order by dt TTL dt + toIntervalDay(15) settings index_granularity = 8192;\n+insert into x values (now(), 1), (now() - toIntervalDay(30), 2);\n+optimize table x final;\n+select i from x;\n+1\n+select minmax_dt.1 == minmax_dt.2 from mergeTreeIndex(currentDatabase(), x, with_minmax = 1);\n+1\n+1\n+select (select min(dt) from x) == (select minDistinct(dt) from x);\n+1\n+drop table x;\ndiff --git a/tests/queries/0_stateless/03364_ttl_should_recalculate_minmax_index.sql b/tests/queries/0_stateless/03364_ttl_should_recalculate_minmax_index.sql\nnew file mode 100644\nindex 000000000000..3d62de3e0a4b\n--- /dev/null\n+++ b/tests/queries/0_stateless/03364_ttl_should_recalculate_minmax_index.sql\n@@ -0,0 +1,17 @@\n+-- { echoOn }\n+\n+drop table if exists x;\n+\n+create table x (dt DateTime, i Int32) engine MergeTree partition by indexHint(dt) order by dt TTL dt + toIntervalDay(15) settings index_granularity = 8192;\n+\n+insert into x values (now(), 1), (now() - toIntervalDay(30), 2);\n+\n+optimize table x final;\n+\n+select i from x;\n+\n+select minmax_dt.1 == minmax_dt.2 from mergeTreeIndex(currentDatabase(), x, with_minmax = 1);\n+\n+select (select min(dt) from x) == (select minDistinct(dt) from x);\n+\n+drop table x;\ndiff --git a/tests/queries/0_stateless/03365_column_ttl_should_rebuild_skp_idx_and_proj.reference b/tests/queries/0_stateless/03365_column_ttl_should_rebuild_skp_idx_and_proj.reference\nnew file mode 100644\nindex 000000000000..a08e8a1d440f\n--- /dev/null\n+++ b/tests/queries/0_stateless/03365_column_ttl_should_rebuild_skp_idx_and_proj.reference\n@@ -0,0 +1,4 @@\n+0\t0\n+0\t0\n+0\t0\n+0\t0\ndiff --git a/tests/queries/0_stateless/03365_column_ttl_should_rebuild_skp_idx_and_proj.sh b/tests/queries/0_stateless/03365_column_ttl_should_rebuild_skp_idx_and_proj.sh\nnew file mode 100755\nindex 000000000000..d00ade755997\n--- /dev/null\n+++ b/tests/queries/0_stateless/03365_column_ttl_should_rebuild_skp_idx_and_proj.sh\n@@ -0,0 +1,30 @@\n+#!/usr/bin/env bash\n+# Tags: no-parallel-replicas\n+\n+CUR_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+# shellcheck source=../shell_config.sh\n+. \"$CUR_DIR\"/../shell_config.sh\n+\n+${CLICKHOUSE_CLIENT} --query \"drop table if exists tbl;\"\n+${CLICKHOUSE_CLIENT} --query \"create table tbl (timestamp DateTime, x UInt32 TTL timestamp + INTERVAL 1 MONTH, y UInt32 TTL timestamp + INTERVAL 1 DAY, index i x type minmax granularity 1, projection p (select x order by y)) engine MergeTree order by () settings min_bytes_for_wide_part = 1, index_granularity = 1;\"\n+${CLICKHOUSE_CLIENT} --query \"insert into tbl select today() - 100, 1, 2 union all select today() - 50, 2, 4;\"\n+\n+# Wait for column TTL to take effect\n+i=0\n+retries=300\n+while [[ $i -lt $retries ]]; do\n+    res=$($CLICKHOUSE_CLIENT -q \"select x from tbl limit 1 format TSVRaw\")\n+    if [[ $res -eq 0 ]]; then\n+        ${CLICKHOUSE_CLIENT} --query \"select x, y from tbl;\"\n+        ${CLICKHOUSE_CLIENT} --query \"select x, y from tbl where x = 0;\"\n+        ${CLICKHOUSE_CLIENT} --query \"select x, y from tbl where y = 2 settings force_optimize_projection_name = 'p';\"\n+\n+        ${CLICKHOUSE_CLIENT} --query \"drop table tbl;\"\n+        exit 0\n+    fi\n+    ((++i))\n+    sleep 1\n+done\n+\n+echo \"Timeout waiting for column TTL to take effect\" >&2\n+exit 1\ndiff --git a/tests/queries/0_stateless/03550_projection_with_part_offset_ttl.reference b/tests/queries/0_stateless/03550_projection_with_part_offset_ttl.reference\nnew file mode 100644\nindex 000000000000..dcabb3873028\n--- /dev/null\n+++ b/tests/queries/0_stateless/03550_projection_with_part_offset_ttl.reference\n@@ -0,0 +1,19 @@\n+-- { echo ON }\n+\n+DROP TABLE IF EXISTS test;\n+CREATE TABLE test\n+(\n+    `order` int,\n+    `indexed` int,\n+    PROJECTION proj\n+    (\n+        SELECT _part_offset\n+        ORDER BY indexed\n+    )\n+)\n+ENGINE = MergeTree\n+ORDER BY order\n+TTL if(order = 1, '1970-01-02T00:00:00'::DateTime, '2030-01-01T00:00:00'::DateTime);\n+INSERT INTO test SELECT 1, 10;\n+OPTIMIZE TABLE test final;\n+DROP TABLE test;\ndiff --git a/tests/queries/0_stateless/03550_projection_with_part_offset_ttl.sql b/tests/queries/0_stateless/03550_projection_with_part_offset_ttl.sql\nnew file mode 100644\nindex 000000000000..89872a28e6da\n--- /dev/null\n+++ b/tests/queries/0_stateless/03550_projection_with_part_offset_ttl.sql\n@@ -0,0 +1,23 @@\n+-- { echo ON }\n+\n+DROP TABLE IF EXISTS test;\n+\n+CREATE TABLE test\n+(\n+    `order` int,\n+    `indexed` int,\n+    PROJECTION proj\n+    (\n+        SELECT _part_offset\n+        ORDER BY indexed\n+    )\n+)\n+ENGINE = MergeTree\n+ORDER BY order\n+TTL if(order = 1, '1970-01-02T00:00:00'::DateTime, '2030-01-01T00:00:00'::DateTime);\n+\n+INSERT INTO test SELECT 1, 10;\n+\n+OPTIMIZE TABLE test final;\n+\n+DROP TABLE test;\n",
  "problem_statement": "Segfault merging MT table with TTL and projection over `_part_offset`\n### Company or project name\n\n_No response_\n\n### Describe what's wrong\n\nA while after ingesting some data into a table with TTL of 1 day, I come back to the ClickHouse server to notice that it's not running, and when I try to restart it, it quickly segfaults with an error that appears to occur during merging projections.\n\n### Does it reproduce on the most recent release?\n\nYes\n\n### How to reproduce\n\n* Server + Client version: 25.6.2.5\n* Default settings\n\nHere's the schema for my table:\n```\nCREATE TABLE otel.spans_local\n(\n    `span_id` FixedString(8) CODEC(NONE),\n    `trace_id` FixedString(16) CODEC(ZSTD(1)),\n    `parent_id` FixedString(8) CODEC(ZSTD(1)),\n    `service` LowCardinality(String) CODEC(ZSTD(1)),\n    `name` LowCardinality(String) CODEC(ZSTD(1)),\n    `start_time` DateTime64(6) CODEC(Delta(8), ZSTD(1)),\n    `end_time` DateTime64(6) CODEC(Delta(8), ZSTD(1)),\n    `duration` UInt64 CODEC(ZSTD(1)),\n    `logs.timestamps` Array(DateTime64(6)) CODEC(ZSTD(1)),\n    `logs.labels.names` Array(Array(LowCardinality(String))) CODEC(ZSTD(1)),\n    `logs.labels.values` Array(Array(String)) CODEC(ZSTD(1)),\n    `references.trace_ids` Array(FixedString(16)) CODEC(ZSTD(1)),\n    `references.span_ids` Array(FixedString(8)) CODEC(ZSTD(1)),\n    `references.ref_types` Array(Enum8('CHILD_OF' = 0, 'FOLLOWS_FROM' = 1)) CODEC(ZSTD(1)),\n    `labels.names` Array(LowCardinality(String)) CODEC(ZSTD(1)),\n    `labels.values` Array(String) CODEC(ZSTD(1)),\n    `process.labels` JSON(label1 LowCardinality(String), label2 LowCardinality(String), label3 LowCardinality(String), label4 LowCardinality(String), label5 LowCardinality(String), label6 LowCardinality(String), label7 LowCardinality(String), label8 LowCardinality(String), label10 LowCardinality(String), label11 LowCardinality(String), label12 LowCardinality(String), label13 LowCardinality(String), label14 LowCardinality(String)) CODEC(ZSTD(1)),\n    `warnings` Array(String) CODEC(ZSTD(1)),\n    `status` Enum8('UNSET' = 0, 'OK' = 1, 'ERROR' = 2) CODEC(ZSTD(1)),\n    PROJECTION by_trace_id (SELECT _part_offset ORDER BY trace_id)\n)\nENGINE = MergeTree\nPARTITION BY toStartOfHour(start_time)\nORDER BY (service, name, toDateTime(start_time), trace_id, duration)\nTTL toStartOfHour(start_time) + toIntervalDay(1)\nSETTINGS index_granularity = 8192, ttl_only_drop_parts = 1\n```\nNotably. this schema is using some of the new projection index features contributed by @amosbird (Thank you!) -- could this be related, since the exception was thrown during projection part merging?\n\n\n### Expected behavior\n\nI expected the part merges to succeed, the TTL-expired parts to be dropped, and the projection parts' `_parent_part_offset` column to be updated accordingly.\n\n### Error message and/or stacktrace\n\n```\n2025.07.03 11:56:31.070792 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> otel.spans_local (f698c668-b6d9-488b-9c05-5dc117635b99) (TTLTransform): Removed all rows from part 1751392800_66_21096_109_6576 due to expired TTL\n2025.07.03 11:56:31.079903 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> MergeTask::MergeProjectionsStage: Merge sorted 73169 rows, containing 19 columns (19 merged, 0 gathered) in 0.162754486 sec., 449566.71731923876 rows/sec., 253.03 MiB/sec.\n2025.07.03 11:56:31.080034 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> MergeTask::MergeProjectionsStage: Selected 6 projection_parts from by_trace_id to by_trace_id\n2025.07.03 11:56:31.080157 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> MergeTask::PrepareStage: Merging 6 parts: from by_trace_id to by_trace_id into Wide with storage Full\n2025.07.03 11:56:31.080263 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> MergeTask::PrepareStage: Selected MergeAlgorithm: Horizontal\n2025.07.03 11:56:31.080419 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> MergeTreeSequentialSource: Reading 109 marks from part by_trace_id, total 877057 rows starting from the beginning of the part\n2025.07.03 11:56:31.080495 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> MergeTreeSequentialSource: Reading 84 marks from part by_trace_id, total 679048 rows starting from the beginning of the part\n2025.07.03 11:56:31.080543 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> MergeTreeSequentialSource: Reading 13 marks from part by_trace_id, total 92453 rows starting from the beginning of the part\n2025.07.03 11:56:31.080585 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> MergeTreeSequentialSource: Reading 9 marks from part by_trace_id, total 58725 rows starting from the beginning of the part\n2025.07.03 11:56:31.080624 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> MergeTreeSequentialSource: Reading 2 marks from part by_trace_id, total 171 rows starting from the beginning of the part\n2025.07.03 11:56:31.080663 [ 775886 ] {f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576} <Debug> MergeTreeSequentialSource: Reading 2 marks from part by_trace_id, total 1 rows starting from the beginning of the part\n2025.07.03 11:56:31.081570 [ 775284 ] {} <Trace> BaseDaemon: Received signal 11\n2025.07.03 11:56:31.081970 [ 776029 ] {} <Fatal> BaseDaemon: ########## Short fault info ############\n2025.07.03 11:56:31.082093 [ 776029 ] {} <Fatal> BaseDaemon: (version 25.6.2.5 (official build), build id: 55568BFED59571C903992F5F302516519AC535BB, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d, architecture: x86_64) (from thread 775886) Received signal 11\n2025.07.03 11:56:31.082122 [ 776029 ] {} <Fatal> BaseDaemon: Signal description: Segmentation fault\n2025.07.03 11:56:31.082141 [ 776029 ] {} <Fatal> BaseDaemon: Address: NULL pointer. Access: read. Unknown si_code.\n2025.07.03 11:56:31.082163 [ 776029 ] {} <Fatal> BaseDaemon: Stack trace: 0x000000001506006a 0x000000001563121e 0x0000000015630e27 0x000000001564d422 0x0000000015640f18 0x00000000156402f2 0x0000000015652e42 0x0000000015653059 0x0000000014e214d5 0x0000000014e20d8e 0x0000000014e2b222 0x0000000014e2b180 0x0000000014e2bcee 0x0000000014e2b222 0x0000000015298e2e 0x0000000014e5762d 0x000000000fd3ad2b 0x000000000fd4193d 0x000000000fd37f52 0x000000000fd3f41a 0x00007feb982081ea 0x00007feb97e39953\n2025.07.03 11:56:31.082189 [ 776029 ] {} <Fatal> BaseDaemon: ########################################\n2025.07.03 11:56:31.082214 [ 776029 ] {} <Fatal> BaseDaemon: (version 25.6.2.5 (official build), build id: 55568BFED59571C903992F5F302516519AC535BB, git hash: 51a12888a03cc2b211c90e16e4154761f43b889d) (from thread 775886) (query_id: f698c668-b6d9-488b-9c05-5dc117635b99::1751392800_66_21096_109_6576) (query: ) Received signal Segmentation fault (11)\n2025.07.03 11:56:31.082232 [ 776029 ] {} <Fatal> BaseDaemon: Address: NULL pointer. Access: read. Unknown si_code.\n2025.07.03 11:56:31.082246 [ 776029 ] {} <Fatal> BaseDaemon: Stack trace: 0x000000001506006a 0x000000001563121e 0x0000000015630e27 0x000000001564d422 0x0000000015640f18 0x00000000156402f2 0x0000000015652e42 0x0000000015653059 0x0000000014e214d5 0x0000000014e20d8e 0x0000000014e2b222 0x0000000014e2b180 0x0000000014e2bcee 0x0000000014e2b222 0x0000000015298e2e 0x0000000014e5762d 0x000000000fd3ad2b 0x000000000fd4193d 0x000000000fd37f52 0x000000000fd3f41a 0x00007feb982081ea 0x00007feb97e39953\n2025.07.03 11:56:31.082319 [ 776029 ] {} <Fatal> BaseDaemon: 2. DB::MergeTreeSequentialSource::generate() @ 0x000000001506006a\n2025.07.03 11:56:31.082361 [ 776029 ] {} <Fatal> BaseDaemon: 3. DB::ISource::tryGenerate() @ 0x000000001563121e\n2025.07.03 11:56:31.082381 [ 776029 ] {} <Fatal> BaseDaemon: 4. DB::ISource::work() @ 0x0000000015630e27\n2025.07.03 11:56:31.082404 [ 776029 ] {} <Fatal> BaseDaemon: 5. DB::ExecutionThreadContext::executeTask() @ 0x000000001564d422\n2025.07.03 11:56:31.082429 [ 776029 ] {} <Fatal> BaseDaemon: 6. DB::PipelineExecutor::executeStepImpl(unsigned long, std::atomic<bool>*) @ 0x0000000015640f18\n2025.07.03 11:56:31.082450 [ 776029 ] {} <Fatal> BaseDaemon: 7. DB::PipelineExecutor::executeStep(std::atomic<bool>*) @ 0x00000000156402f2\n2025.07.03 11:56:31.082472 [ 776029 ] {} <Fatal> BaseDaemon: 8. DB::PullingPipelineExecutor::pull(DB::Chunk&) @ 0x0000000015652e42\n2025.07.03 11:56:31.082490 [ 776029 ] {} <Fatal> BaseDaemon: 9. DB::PullingPipelineExecutor::pull(DB::Block&) @ 0x0000000015653059\n2025.07.03 11:56:31.082511 [ 776029 ] {} <Fatal> BaseDaemon: 10. DB::MergeTask::ExecuteAndFinalizeHorizontalPart::executeImpl() const @ 0x0000000014e214d5\n2025.07.03 11:56:31.082530 [ 776029 ] {} <Fatal> BaseDaemon: 11. DB::MergeTask::ExecuteAndFinalizeHorizontalPart::execute() @ 0x0000000014e20d8e\n2025.07.03 11:56:31.082549 [ 776029 ] {} <Fatal> BaseDaemon: 12. DB::MergeTask::execute() @ 0x0000000014e2b222\n2025.07.03 11:56:31.082571 [ 776029 ] {} <Fatal> BaseDaemon: 13. DB::MergeTask::MergeProjectionsStage::executeProjections() const @ 0x0000000014e2b180\n2025.07.03 11:56:31.082590 [ 776029 ] {} <Fatal> BaseDaemon: 14. DB::MergeTask::MergeProjectionsStage::execute() @ 0x0000000014e2bcee\n2025.07.03 11:56:31.082611 [ 776029 ] {} <Fatal> BaseDaemon: 15. DB::MergeTask::execute() @ 0x0000000014e2b222\n2025.07.03 11:56:31.082631 [ 776029 ] {} <Fatal> BaseDaemon: 16. DB::MergePlainMergeTreeTask::executeStep() @ 0x0000000015298e2e\n```\n\n### Additional context\n\n_No response_\nColumn TTL can generate part with incorrect skip indices and projections\n### Company or project name\n\n.\n\n### Describe what's wrong\n\nThis occurs in wide part merges when fully expired columns are skipped.\n\n### Does it reproduce on the most recent release?\n\nYes\n\n### How to reproduce\n\nhttps://fiddle.clickhouse.com/c4109b1b-9bef-4d2e-a5b7-f1a4720c169a\n\n### Expected behavior\n\n_No response_\n\n### Error message and/or stacktrace\n\n_No response_\n\n### Additional context\n\n_No response_\nMin(col) vs Min(distinct col) return different value\n### Company or project name\n\n_No response_\n\n### Describe what's wrong\n\n![Image](https://github.com/user-attachments/assets/d1183c2e-2a52-4ce6-9627-6e860812a2fd)\n\nHow it possible?\n\nThe table engine is ReplicatedMergeTree and clickhouse version is 24.5.1.1763\n\n\nEXPLAIN description = 1, indexes = 1, actions = 1\nSELECT min(DateKey)\nFROM HourlyImpressionShareOrderItemUsage\n\nQuery id: 85adce85-de7a-48f1-8c9e-b12f7c50ef7e\n\n    \u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n 1. \u2502 Expression ((Project names + Projection))                                                       \u2502\n 2. \u2502 Actions: INPUT : 0 -> min(__table1.DateKey) Int32 : 0                                           \u2502\n 3. \u2502          ALIAS min(__table1.DateKey) :: 0 -> min(DateKey) Int32 : 1                             \u2502\n 4. \u2502 Positions: 1                                                                                    \u2502\n 5. \u2502   Aggregating                                                                                   \u2502\n 6. \u2502   Keys:                                                                                         \u2502\n 7. \u2502   Aggregates:                                                                                   \u2502\n 8. \u2502       min(__table1.DateKey)                                                                     \u2502\n 9. \u2502         Function: min(Int32) \u2192 Int32                                                            \u2502\n10. \u2502         Arguments: __table1.DateKey                                                             \u2502\n11. \u2502   Skip merging: 0                                                                               \u2502\n12. \u2502     Expression                                                                                  \u2502\n13. \u2502     Actions: INPUT : 0 -> min(DateKey) AggregateFunction(min, Int32) : 0                        \u2502\n14. \u2502              ALIAS min(DateKey) :: 0 -> min(__table1.DateKey) AggregateFunction(min, Int32) : 1 \u2502\n15. \u2502     Positions: 1                                                                                \u2502\n16. \u2502       ReadFromPreparedSource (_minmax_count_projection)                                         \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\nEXPLAIN description = 1, indexes = 1, actions = 1\nSELECT minDistinct(DateKey)\nFROM HourlyImpressionShareOrderItemUsage\n\nQuery id: ce4fb5da-063f-4f2e-ad2c-c820edb4e674\n\n    \u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n 1. \u2502 Expression ((Project names + Projection))                                           \u2502\n 2. \u2502 Actions: INPUT : 0 -> minDistinct(__table1.DateKey) Int32 : 0                       \u2502\n 3. \u2502          ALIAS minDistinct(__table1.DateKey) :: 0 -> minDistinct(DateKey) Int32 : 1 \u2502\n 4. \u2502 Positions: 1                                                                        \u2502\n 5. \u2502   Aggregating                                                                       \u2502\n 6. \u2502   Keys:                                                                             \u2502\n 7. \u2502   Aggregates:                                                                       \u2502\n 8. \u2502       minDistinct(__table1.DateKey)                                                 \u2502\n 9. \u2502         Function: minDistinct(Int32) \u2192 Int32                                        \u2502\n10. \u2502         Arguments: __table1.DateKey                                                 \u2502\n11. \u2502   Skip merging: 0                                                                   \u2502\n12. \u2502     Expression ((Before GROUP BY + Change column names to column identifiers))      \u2502\n13. \u2502     Actions: INPUT : 0 -> DateKey Int32 : 0                                         \u2502\n14. \u2502              ALIAS DateKey :: 0 -> __table1.DateKey Int32 : 1                       \u2502\n15. \u2502     Positions: 1                                                                    \u2502\n16. \u2502       ReadFromMergeTree (AdvertiserBI.HourlyImpressionShareOrderItemUsage)          \u2502\n17. \u2502       ReadType: Default                                                             \u2502\n18. \u2502       Parts: 14                                                                     \u2502\n19. \u2502       Granules: 8372                                                                \u2502\n20. \u2502       Indexes:                                                                      \u2502\n21. \u2502         MinMax                                                                      \u2502\n22. \u2502           Condition: true                                                           \u2502\n23. \u2502           Parts: 14/14                                                              \u2502\n24. \u2502           Granules: 8372/8372                                                       \u2502\n25. \u2502         Partition                                                                   \u2502\n26. \u2502           Condition: true                                                           \u2502\n27. \u2502           Parts: 14/14                                                              \u2502\n28. \u2502           Granules: 8372/8372                                                       \u2502\n29. \u2502         PrimaryKey                                                                  \u2502\n30. \u2502           Condition: true                                                           \u2502\n31. \u2502           Parts: 14/14                                                              \u2502\n32. \u2502           Granules: 8372/8372                                                       \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n### Does it reproduce on the most recent release?\n\nYes\n\n### How to reproduce\n\nNA\n\n### Expected behavior\n\n_No response_\n\n### Error message and/or stacktrace\n\n_No response_\n\n### Additional context\n\n_No response_\n",
  "hints_text": "\n\nWhat's your table schema? Can you provide sample data to reproduce the issue?  In the meantime, you can disable `optimize_use_implicit_projections`.  \n\nThanks @amosbird .  disable optimize_use_implicit_projections works.\n\nHere is the table schema.\nCREATE TABLE AdvertiserBI.HourlyImpressionShareAccountUsage\n(\n    `AccountId` Int64 CODEC(ZSTD(1)),\n    `DateKey` Int32 CODEC(ZSTD(1)),\n    `HourNum` UInt8 CODEC(ZSTD(1)),\n    `LoadTime` DateTime CODEC(ZSTD(1)),\n\t...\n)\nENGINE = ReplicatedMergeTree('/clickhouse/{installation}/{cluster}/tables/{shard}/HourlyImpressionShareAccountUsage', '{replica}')\nPARTITION BY toInt32(DateKey / 100)\nORDER BY (DateKey, AccountId)\nTTL parseDateTimeBestEffort(toString(DateKey)) + toIntervalDay(7)\nSETTINGS allow_nullable_key = 1, index_granularity = 8192, materialize_ttl_recalculate_only = 1\n\nI cannot repro it by creating a total new table. \n\nWhat reason leads to projections inconsistent with data? Is there any way to recover e.g. force refresh the projection?\n\nI've successfully reproduced the issue. It's caused by TTL not rewriting `minmax_DateKey.idx`, which relaxes the min-max range.  \n\nThis reduces the efficiency of pruning algorithms and breaks algorithms that rely on precise metadata, such as implicit projections.\n\n> Is there any way to recover e.g. force refresh the projection?\n\nThere's nothing to refresh\u2014the implicit projection is, by definition, implicit.  \n\nYou can try setting `ttl_only_drop_parts = 1`, though this makes TTL much less aggressive. Alternatively, you can disable `optimize_use_implicit_projections` until the issue is resolved.\n\n",
  "created_at": "2025-03-05T13:56:10Z"
}