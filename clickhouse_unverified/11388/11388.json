{
  "repo": "ClickHouse/ClickHouse",
  "pull_number": 11388,
  "instance_id": "ClickHouse__ClickHouse-11388",
  "issue_numbers": [
    "8056",
    "11308",
    "4116"
  ],
  "base_commit": "29376cb8006b80ccef203c39324df12f96d7cf8e",
  "patch": "diff --git a/src/Core/Settings.h b/src/Core/Settings.h\nindex 17d15a6643d9..cd9de5abec3b 100644\n--- a/src/Core/Settings.h\n+++ b/src/Core/Settings.h\n@@ -46,7 +46,7 @@ struct Settings : public SettingsCollection<Settings>\n       * A setting is \"IMPORTANT\" if it affects the results of queries and can't be ignored by older versions.\n       */\n \n-#define LIST_OF_SETTINGS(M)                                            \\\n+#define COMMON_SETTINGS(M)                                            \\\n     M(SettingUInt64, min_compress_block_size, 65536, \"The actual size of the block to compress, if the uncompressed data less than max_compress_block_size is no less than this value and no less than the volume of data for one mark.\", 0) \\\n     M(SettingUInt64, max_compress_block_size, 1048576, \"The maximum size of blocks of uncompressed data before compressing for writing to a table.\", 0) \\\n     M(SettingUInt64, max_block_size, DEFAULT_BLOCK_SIZE, \"Maximum block size for reading\", 0) \\\n@@ -185,40 +185,10 @@ struct Settings : public SettingsCollection<Settings>\n     \\\n     M(SettingString, count_distinct_implementation, \"uniqExact\", \"What aggregate function to use for implementation of count(DISTINCT ...)\", 0) \\\n     \\\n-    M(SettingBool, output_format_enable_streaming, false, \"Enable streaming in output formats that support it.\", 0) \\\n-    M(SettingBool, output_format_write_statistics, true, \"Write statistics about read rows, bytes, time elapsed in suitable output formats.\", 0) \\\n-    \\\n     M(SettingBool, add_http_cors_header, false, \"Write add http CORS header.\", 0) \\\n     \\\n     M(SettingUInt64, max_http_get_redirects, 0, \"Max number of http GET redirects hops allowed. Make sure additional security measures are in place to prevent a malicious server to redirect your requests to unexpected services.\", 0) \\\n     \\\n-    M(SettingBool, input_format_skip_unknown_fields, false, \"Skip columns with unknown names from input data (it works for JSONEachRow, CSVWithNames, TSVWithNames and TSKV formats).\", 0) \\\n-    M(SettingBool, input_format_with_names_use_header, true, \"For TSVWithNames and CSVWithNames input formats this controls whether format parser is to assume that column data appear in the input exactly as they are specified in the header.\", 0) \\\n-    M(SettingBool, input_format_import_nested_json, false, \"Map nested JSON data to nested tables (it works for JSONEachRow format).\", 0) \\\n-    M(SettingBool, input_format_defaults_for_omitted_fields, true, \"For input data calculate default expressions for omitted fields (it works for JSONEachRow, CSV and TSV formats).\", IMPORTANT) \\\n-    M(SettingBool, input_format_tsv_empty_as_default, false, \"Treat empty fields in TSV input as default values.\", 0) \\\n-    M(SettingBool, input_format_null_as_default, false, \"For text input formats initialize null fields with default values if data type of this field is not nullable\", 0) \\\n-    \\\n-    M(SettingBool, input_format_values_interpret_expressions, true, \"For Values format: if the field could not be parsed by streaming parser, run SQL parser and try to interpret it as SQL expression.\", 0) \\\n-    M(SettingBool, input_format_values_deduce_templates_of_expressions, true, \"For Values format: if the field could not be parsed by streaming parser, run SQL parser, deduce template of the SQL expression, try to parse all rows using template and then interpret expression for all rows.\", 0) \\\n-    M(SettingBool, input_format_values_accurate_types_of_literals, true, \"For Values format: when parsing and interpreting expressions using template, check actual type of literal to avoid possible overflow and precision issues.\", 0) \\\n-    M(SettingURI, format_avro_schema_registry_url, {}, \"For AvroConfluent format: Confluent Schema Registry URL.\", 0) \\\n-    \\\n-    M(SettingBool, output_format_json_quote_64bit_integers, true, \"Controls quoting of 64-bit integers in JSON output format.\", 0) \\\n-    \\\n-    M(SettingBool, output_format_json_quote_denormals, false, \"Enables '+nan', '-nan', '+inf', '-inf' outputs in JSON output format.\", 0) \\\n-    \\\n-    M(SettingBool, output_format_json_escape_forward_slashes, true, \"Controls escaping forward slashes for string outputs in JSON output format. This is intended for compatibility with JavaScript. Don't confuse with backslashes that are always escaped.\", 0) \\\n-    \\\n-    M(SettingUInt64, output_format_pretty_max_rows, 10000, \"Rows limit for Pretty formats.\", 0) \\\n-    M(SettingUInt64, output_format_pretty_max_column_pad_width, 250, \"Maximum width to pad all values in a column in Pretty formats.\", 0) \\\n-    M(SettingUInt64, output_format_pretty_max_value_width, 10000, \"Maximum width of value to display in Pretty formats. If greater - it will be cut.\", 0) \\\n-    M(SettingBool, output_format_pretty_color, true, \"Use ANSI escape sequences to paint colors in Pretty formats\", 0) \\\n-    M(SettingUInt64, output_format_parquet_row_group_size, 1000000, \"Row group size in rows.\", 0) \\\n-    M(SettingString, output_format_avro_codec, \"\", \"Compression codec used for output. Possible values: 'null', 'deflate', 'snappy'.\", 0) \\\n-    M(SettingUInt64, output_format_avro_sync_interval, 16 * 1024, \"Sync interval in bytes.\", 0) \\\n-    M(SettingBool, output_format_tsv_crlf_end_of_line, false, \"If it is set true, end of line in TSV format will be \\\\r\\\\n instead of \\\\n.\", 0) \\\n-    \\\n     M(SettingBool, use_client_time_zone, false, \"Use client timezone for interpreting DateTime string values, instead of adopting server timezone.\", 0) \\\n     \\\n     M(SettingBool, send_progress_in_http_headers, false, \"Send progress notifications using X-ClickHouse-Progress headers. Some clients do not support high amount of HTTP headers (Python requests in particular), so it is disabled by default.\", 0) \\\n@@ -227,9 +197,6 @@ struct Settings : public SettingsCollection<Settings>\n     \\\n     M(SettingBool, fsync_metadata, 1, \"Do fsync after changing metadata for tables and databases (.sql files). Could be disabled in case of poor latency on server with high load of DDL queries and high load of disk subsystem.\", 0) \\\n     \\\n-    M(SettingUInt64, input_format_allow_errors_num, 0, \"Maximum absolute amount of errors while reading text formats (like CSV, TSV). In case of error, if at least absolute or relative amount of errors is lower than corresponding value, will skip until next line and continue.\", 0) \\\n-    M(SettingFloat, input_format_allow_errors_ratio, 0, \"Maximum relative amount of errors while reading text formats (like CSV, TSV). In case of error, if at least absolute or relative amount of errors is lower than corresponding value, will skip until next line and continue.\", 0) \\\n-    \\\n     M(SettingBool, join_use_nulls, 0, \"Use NULLs for non-joined rows of outer JOINs for types that can be inside Nullable. If false, use default value of corresponding columns data type.\", IMPORTANT) \\\n     \\\n     M(SettingJoinStrictness, join_default_strictness, JoinStrictness::ALL, \"Set default strictness in JOIN query. Possible values: empty string, 'ANY', 'ALL'. If empty, query without strictness will throw exception.\", 0) \\\n@@ -247,23 +214,6 @@ struct Settings : public SettingsCollection<Settings>\n     M(SettingMilliseconds, stream_flush_interval_ms, 7500, \"Timeout for flushing data from streaming storages.\", 0) \\\n     M(SettingMilliseconds, stream_poll_timeout_ms, 500, \"Timeout for polling data from/to streaming storages.\", 0) \\\n     \\\n-    M(SettingString, format_schema, \"\", \"Schema identifier (used by schema-based formats)\", 0) \\\n-    M(SettingString, format_template_resultset, \"\", \"Path to file which contains format string for result set (for Template format)\", 0) \\\n-    M(SettingString, format_template_row, \"\", \"Path to file which contains format string for rows (for Template format)\", 0) \\\n-    M(SettingString, format_template_rows_between_delimiter, \"\\n\", \"Delimiter between rows (for Template format)\", 0) \\\n-    \\\n-    M(SettingString, format_custom_escaping_rule, \"Escaped\", \"Field escaping rule (for CustomSeparated format)\", 0) \\\n-    M(SettingString, format_custom_field_delimiter, \"\\t\", \"Delimiter between fields (for CustomSeparated format)\", 0) \\\n-    M(SettingString, format_custom_row_before_delimiter, \"\", \"Delimiter before field of the first column (for CustomSeparated format)\", 0) \\\n-    M(SettingString, format_custom_row_after_delimiter, \"\\n\", \"Delimiter after field of the last column (for CustomSeparated format)\", 0) \\\n-    M(SettingString, format_custom_row_between_delimiter, \"\", \"Delimiter between rows (for CustomSeparated format)\", 0) \\\n-    M(SettingString, format_custom_result_before_delimiter, \"\", \"Prefix before result set (for CustomSeparated format)\", 0) \\\n-    M(SettingString, format_custom_result_after_delimiter, \"\", \"Suffix after result set (for CustomSeparated format)\", 0) \\\n-    \\\n-    M(SettingString, format_regexp, \"\", \"Regular expression (for Regexp format)\", 0) \\\n-    M(SettingString, format_regexp_escaping_rule, \"Escaped\", \"Field escaping rule (for Regexp format)\", 0) \\\n-    M(SettingBool, format_regexp_skip_unmatched, false, \"Skip lines unmatched by regular expression (for Regexp format\", 0) \\\n-    \\\n     M(SettingBool, insert_allow_materialized_columns, 0, \"If setting is enabled, Allow materialized columns in INSERT.\", 0) \\\n     M(SettingSeconds, http_connection_timeout, DEFAULT_HTTP_READ_BUFFER_CONNECTION_TIMEOUT, \"HTTP connection timeout.\", 0) \\\n     M(SettingSeconds, http_send_timeout, DEFAULT_HTTP_READ_BUFFER_TIMEOUT, \"HTTP send timeout\", 0) \\\n@@ -361,13 +311,7 @@ struct Settings : public SettingsCollection<Settings>\n     M(SettingUInt64, max_network_bytes, 0, \"The maximum number of bytes (compressed) to receive or transmit over the network for execution of the query.\", 0) \\\n     M(SettingUInt64, max_network_bandwidth_for_user, 0, \"The maximum speed of data exchange over the network in bytes per second for all concurrently running user queries. Zero means unlimited.\", 0)\\\n     M(SettingUInt64, max_network_bandwidth_for_all_users, 0, \"The maximum speed of data exchange over the network in bytes per second for all concurrently running queries. Zero means unlimited.\", 0) \\\n-    M(SettingChar, format_csv_delimiter, ',', \"The character to be considered as a delimiter in CSV data. If setting with a string, a string has to have a length of 1.\", 0) \\\n-    M(SettingBool, format_csv_allow_single_quotes, 1, \"If it is set to true, allow strings in single quotes.\", 0) \\\n-    M(SettingBool, format_csv_allow_double_quotes, 1, \"If it is set to true, allow strings in double quotes.\", 0) \\\n-    M(SettingBool, output_format_csv_crlf_end_of_line, false, \"If it is set true, end of line in CSV format will be \\\\r\\\\n instead of \\\\n.\", 0) \\\n-    M(SettingBool, input_format_csv_unquoted_null_literal_as_null, false, \"Consider unquoted NULL literal as \\\\N\", 0) \\\n     \\\n-    M(SettingDateTimeInputFormat, date_time_input_format, FormatSettings::DateTimeInputFormat::Basic, \"Method to read DateTime from text input formats. Possible values: 'basic' and 'best_effort'.\", 0) \\\n     M(SettingBool, log_profile_events, true, \"Log query performance statistics into the query_log and query_thread_log.\", 0) \\\n     M(SettingBool, log_query_settings, true, \"Log query settings into the query_log.\", 0) \\\n     M(SettingBool, log_query_threads, true, \"Log query threads into system.query_thread_log table. This setting have effect only when 'log_queries' is true.\", 0) \\\n@@ -446,7 +390,69 @@ struct Settings : public SettingsCollection<Settings>\n     M(SettingBool, partial_merge_join, false, \"Obsolete. Use join_algorithm='prefer_partial_merge' instead.\", 0) \\\n     M(SettingUInt64, max_memory_usage_for_all_queries, 0, \"Obsolete. Will be removed after 2020-10-20\", 0) \\\n     \\\n-    M(SettingBool, experimental_use_processors, true, \"Obsolete setting, does nothing. Will be removed after 2020-11-29.\", 0) \\\n+    M(SettingBool, experimental_use_processors, true, \"Obsolete setting, does nothing. Will be removed after 2020-11-29.\", 0)\n+\n+#define FORMAT_FACTORY_SETTINGS(M)                                            \\\n+    M(SettingChar, format_csv_delimiter, ',', \"The character to be considered as a delimiter in CSV data. If setting with a string, a string has to have a length of 1.\", 0) \\\n+    M(SettingBool, format_csv_allow_single_quotes, 1, \"If it is set to true, allow strings in single quotes.\", 0) \\\n+    M(SettingBool, format_csv_allow_double_quotes, 1, \"If it is set to true, allow strings in double quotes.\", 0) \\\n+    M(SettingBool, output_format_csv_crlf_end_of_line, false, \"If it is set true, end of line in CSV format will be \\\\r\\\\n instead of \\\\n.\", 0) \\\n+    M(SettingBool, input_format_csv_unquoted_null_literal_as_null, false, \"Consider unquoted NULL literal as \\\\N\", 0) \\\n+    M(SettingBool, input_format_skip_unknown_fields, false, \"Skip columns with unknown names from input data (it works for JSONEachRow, CSVWithNames, TSVWithNames and TSKV formats).\", 0) \\\n+    M(SettingBool, input_format_with_names_use_header, true, \"For TSVWithNames and CSVWithNames input formats this controls whether format parser is to assume that column data appear in the input exactly as they are specified in the header.\", 0) \\\n+    M(SettingBool, input_format_import_nested_json, false, \"Map nested JSON data to nested tables (it works for JSONEachRow format).\", 0) \\\n+    M(SettingBool, input_format_defaults_for_omitted_fields, true, \"For input data calculate default expressions for omitted fields (it works for JSONEachRow, CSV and TSV formats).\", IMPORTANT) \\\n+    M(SettingBool, input_format_tsv_empty_as_default, false, \"Treat empty fields in TSV input as default values.\", 0) \\\n+    M(SettingBool, input_format_null_as_default, false, \"For text input formats initialize null fields with default values if data type of this field is not nullable\", 0) \\\n+    \\\n+    M(SettingDateTimeInputFormat, date_time_input_format, FormatSettings::DateTimeInputFormat::Basic, \"Method to read DateTime from text input formats. Possible values: 'basic' and 'best_effort'.\", 0) \\\n+    \\\n+    M(SettingBool, input_format_values_interpret_expressions, true, \"For Values format: if the field could not be parsed by streaming parser, run SQL parser and try to interpret it as SQL expression.\", 0) \\\n+    M(SettingBool, input_format_values_deduce_templates_of_expressions, true, \"For Values format: if the field could not be parsed by streaming parser, run SQL parser, deduce template of the SQL expression, try to parse all rows using template and then interpret expression for all rows.\", 0) \\\n+    M(SettingBool, input_format_values_accurate_types_of_literals, true, \"For Values format: when parsing and interpreting expressions using template, check actual type of literal to avoid possible overflow and precision issues.\", 0) \\\n+    M(SettingURI, format_avro_schema_registry_url, {}, \"For AvroConfluent format: Confluent Schema Registry URL.\", 0) \\\n+    \\\n+    M(SettingBool, output_format_json_quote_64bit_integers, true, \"Controls quoting of 64-bit integers in JSON output format.\", 0) \\\n+    \\\n+    M(SettingBool, output_format_json_quote_denormals, false, \"Enables '+nan', '-nan', '+inf', '-inf' outputs in JSON output format.\", 0) \\\n+    \\\n+    M(SettingBool, output_format_json_escape_forward_slashes, true, \"Controls escaping forward slashes for string outputs in JSON output format. This is intended for compatibility with JavaScript. Don't confuse with backslashes that are always escaped.\", 0) \\\n+    \\\n+    M(SettingUInt64, output_format_pretty_max_rows, 10000, \"Rows limit for Pretty formats.\", 0) \\\n+    M(SettingUInt64, output_format_pretty_max_column_pad_width, 250, \"Maximum width to pad all values in a column in Pretty formats.\", 0) \\\n+    M(SettingUInt64, output_format_pretty_max_value_width, 10000, \"Maximum width of value to display in Pretty formats. If greater - it will be cut.\", 0) \\\n+    M(SettingBool, output_format_pretty_color, true, \"Use ANSI escape sequences to paint colors in Pretty formats\", 0) \\\n+    M(SettingUInt64, output_format_parquet_row_group_size, 1000000, \"Row group size in rows.\", 0) \\\n+    M(SettingString, output_format_avro_codec, \"\", \"Compression codec used for output. Possible values: 'null', 'deflate', 'snappy'.\", 0) \\\n+    M(SettingUInt64, output_format_avro_sync_interval, 16 * 1024, \"Sync interval in bytes.\", 0) \\\n+    M(SettingBool, output_format_tsv_crlf_end_of_line, false, \"If it is set true, end of line in TSV format will be \\\\r\\\\n instead of \\\\n.\", 0) \\\n+    \\\n+    M(SettingUInt64, input_format_allow_errors_num, 0, \"Maximum absolute amount of errors while reading text formats (like CSV, TSV). In case of error, if at least absolute or relative amount of errors is lower than corresponding value, will skip until next line and continue.\", 0) \\\n+    M(SettingFloat, input_format_allow_errors_ratio, 0, \"Maximum relative amount of errors while reading text formats (like CSV, TSV). In case of error, if at least absolute or relative amount of errors is lower than corresponding value, will skip until next line and continue.\", 0) \\\n+    \\\n+    M(SettingString, format_schema, \"\", \"Schema identifier (used by schema-based formats)\", 0) \\\n+    M(SettingString, format_template_resultset, \"\", \"Path to file which contains format string for result set (for Template format)\", 0) \\\n+    M(SettingString, format_template_row, \"\", \"Path to file which contains format string for rows (for Template format)\", 0) \\\n+    M(SettingString, format_template_rows_between_delimiter, \"\\n\", \"Delimiter between rows (for Template format)\", 0) \\\n+    \\\n+    M(SettingString, format_custom_escaping_rule, \"Escaped\", \"Field escaping rule (for CustomSeparated format)\", 0) \\\n+    M(SettingString, format_custom_field_delimiter, \"\\t\", \"Delimiter between fields (for CustomSeparated format)\", 0) \\\n+    M(SettingString, format_custom_row_before_delimiter, \"\", \"Delimiter before field of the first column (for CustomSeparated format)\", 0) \\\n+    M(SettingString, format_custom_row_after_delimiter, \"\\n\", \"Delimiter after field of the last column (for CustomSeparated format)\", 0) \\\n+    M(SettingString, format_custom_row_between_delimiter, \"\", \"Delimiter between rows (for CustomSeparated format)\", 0) \\\n+    M(SettingString, format_custom_result_before_delimiter, \"\", \"Prefix before result set (for CustomSeparated format)\", 0) \\\n+    M(SettingString, format_custom_result_after_delimiter, \"\", \"Suffix after result set (for CustomSeparated format)\", 0) \\\n+    \\\n+    M(SettingString, format_regexp, \"\", \"Regular expression (for Regexp format)\", 0) \\\n+    M(SettingString, format_regexp_escaping_rule, \"Escaped\", \"Field escaping rule (for Regexp format)\", 0) \\\n+    M(SettingBool, format_regexp_skip_unmatched, false, \"Skip lines unmatched by regular expression (for Regexp format\", 0) \\\n+    \\\n+    M(SettingBool, output_format_enable_streaming, false, \"Enable streaming in output formats that support it.\", 0) \\\n+    M(SettingBool, output_format_write_statistics, true, \"Write statistics about read rows, bytes, time elapsed in suitable output formats.\", 0)\n+\n+    #define LIST_OF_SETTINGS(M)    \\\n+        COMMON_SETTINGS(M)         \\\n+        FORMAT_FACTORY_SETTINGS(M)\n \n     DECLARE_SETTINGS_COLLECTION(LIST_OF_SETTINGS)\n \ndiff --git a/src/Storages/Kafka/KafkaBlockInputStream.cpp b/src/Storages/Kafka/KafkaBlockInputStream.cpp\nindex 3e4533f8bb28..3edfcc7b9d2e 100644\n--- a/src/Storages/Kafka/KafkaBlockInputStream.cpp\n+++ b/src/Storages/Kafka/KafkaBlockInputStream.cpp\n@@ -13,7 +13,7 @@ namespace ErrorCodes\n     extern const int LOGICAL_ERROR;\n }\n KafkaBlockInputStream::KafkaBlockInputStream(\n-    StorageKafka & storage_, const Context & context_, const Names & columns, size_t max_block_size_, bool commit_in_suffix_)\n+    StorageKafka & storage_, const std::shared_ptr<Context> & context_, const Names & columns, size_t max_block_size_, bool commit_in_suffix_)\n     : storage(storage_)\n     , context(context_)\n     , column_names(columns)\n@@ -22,12 +22,6 @@ KafkaBlockInputStream::KafkaBlockInputStream(\n     , non_virtual_header(storage.getSampleBlockNonMaterialized())\n     , virtual_header(storage.getSampleBlockForColumns({\"_topic\", \"_key\", \"_offset\", \"_partition\", \"_timestamp\",\"_timestamp_ms\",\"_headers.name\",\"_headers.value\"}))\n {\n-    context.setSetting(\"input_format_skip_unknown_fields\", 1u); // Always skip unknown fields regardless of the context (JSON or TSKV)\n-    context.setSetting(\"input_format_allow_errors_ratio\", 0.);\n-    context.setSetting(\"input_format_allow_errors_num\", storage.skipBroken());\n-\n-    if (!storage.getSchemaName().empty())\n-        context.setSetting(\"format_schema\", storage.getSchemaName());\n }\n \n KafkaBlockInputStream::~KafkaBlockInputStream()\n@@ -48,7 +42,7 @@ Block KafkaBlockInputStream::getHeader() const\n \n void KafkaBlockInputStream::readPrefixImpl()\n {\n-    auto timeout = std::chrono::milliseconds(context.getSettingsRef().kafka_max_wait_ms.totalMilliseconds());\n+    auto timeout = std::chrono::milliseconds(context->getSettingsRef().kafka_max_wait_ms.totalMilliseconds());\n     buffer = storage.popReadBuffer(timeout);\n \n     if (!buffer)\n@@ -73,7 +67,7 @@ Block KafkaBlockInputStream::readImpl()\n     MutableColumns virtual_columns = virtual_header.cloneEmptyColumns();\n \n     auto input_format = FormatFactory::instance().getInputFormat(\n-        storage.getFormatName(), *buffer, non_virtual_header, context, max_block_size);\n+        storage.getFormatName(), *buffer, non_virtual_header, *context, max_block_size);\n \n     InputPort port(input_format->getPort().getHeader(), input_format.get());\n     connect(input_format->getPort(), port);\ndiff --git a/src/Storages/Kafka/KafkaBlockInputStream.h b/src/Storages/Kafka/KafkaBlockInputStream.h\nindex e30521228940..387f5088721d 100644\n--- a/src/Storages/Kafka/KafkaBlockInputStream.h\n+++ b/src/Storages/Kafka/KafkaBlockInputStream.h\n@@ -14,7 +14,7 @@ class KafkaBlockInputStream : public IBlockInputStream\n {\n public:\n     KafkaBlockInputStream(\n-        StorageKafka & storage_, const Context & context_, const Names & columns, size_t max_block_size_, bool commit_in_suffix = true);\n+        StorageKafka & storage_, const std::shared_ptr<Context> & context_, const Names & columns, size_t max_block_size_, bool commit_in_suffix = true);\n     ~KafkaBlockInputStream() override;\n \n     String getName() const override { return storage.getName(); }\n@@ -29,7 +29,7 @@ class KafkaBlockInputStream : public IBlockInputStream\n \n private:\n     StorageKafka & storage;\n-    Context context;\n+    const std::shared_ptr<Context> context;\n     Names column_names;\n     UInt64 max_block_size;\n \ndiff --git a/src/Storages/Kafka/KafkaBlockOutputStream.cpp b/src/Storages/Kafka/KafkaBlockOutputStream.cpp\nindex fe8aa207c939..17ef5aa104cb 100644\n--- a/src/Storages/Kafka/KafkaBlockOutputStream.cpp\n+++ b/src/Storages/Kafka/KafkaBlockOutputStream.cpp\n@@ -11,7 +11,7 @@ namespace ErrorCodes\n     extern const int CANNOT_CREATE_IO_BUFFER;\n }\n \n-KafkaBlockOutputStream::KafkaBlockOutputStream(StorageKafka & storage_, const Context & context_) : storage(storage_), context(context_)\n+KafkaBlockOutputStream::KafkaBlockOutputStream(StorageKafka & storage_, const std::shared_ptr<Context> & context_) : storage(storage_), context(context_)\n {\n }\n \n@@ -26,7 +26,7 @@ void KafkaBlockOutputStream::writePrefix()\n     if (!buffer)\n         throw Exception(\"Failed to create Kafka producer!\", ErrorCodes::CANNOT_CREATE_IO_BUFFER);\n \n-    child = FormatFactory::instance().getOutput(storage.getFormatName(), *buffer, getHeader(), context, [this](const Columns & columns, size_t row){ buffer->countRow(columns, row); });\n+    child = FormatFactory::instance().getOutput(storage.getFormatName(), *buffer, getHeader(), *context, [this](const Columns & columns, size_t row){ buffer->countRow(columns, row); });\n }\n \n void KafkaBlockOutputStream::write(const Block & block)\ndiff --git a/src/Storages/Kafka/KafkaBlockOutputStream.h b/src/Storages/Kafka/KafkaBlockOutputStream.h\nindex f3eb3dae0ba5..7a973724f1b3 100644\n--- a/src/Storages/Kafka/KafkaBlockOutputStream.h\n+++ b/src/Storages/Kafka/KafkaBlockOutputStream.h\n@@ -10,7 +10,7 @@ namespace DB\n class KafkaBlockOutputStream : public IBlockOutputStream\n {\n public:\n-    explicit KafkaBlockOutputStream(StorageKafka & storage_, const Context & context_);\n+    explicit KafkaBlockOutputStream(StorageKafka & storage_, const std::shared_ptr<Context> & context_);\n \n     Block getHeader() const override;\n \n@@ -22,7 +22,7 @@ class KafkaBlockOutputStream : public IBlockOutputStream\n \n private:\n     StorageKafka & storage;\n-    Context context;\n+    const std::shared_ptr<Context> context;\n     ProducerBufferPtr buffer;\n     BlockOutputStreamPtr child;\n };\ndiff --git a/src/Storages/Kafka/KafkaSettings.h b/src/Storages/Kafka/KafkaSettings.h\nindex 43984f81e054..e65522b3606f 100644\n--- a/src/Storages/Kafka/KafkaSettings.h\n+++ b/src/Storages/Kafka/KafkaSettings.h\n@@ -1,7 +1,7 @@\n #pragma once\n \n #include <Core/SettingsCollection.h>\n-\n+#include <Core/Settings.h>\n \n namespace DB\n {\n@@ -15,18 +15,34 @@ struct KafkaSettings : public SettingsCollection<KafkaSettings>\n {\n \n \n-#define LIST_OF_KAFKA_SETTINGS(M)                                      \\\n+#define KAFKA_RELATED_SETTINGS(M)                                      \\\n     M(SettingString, kafka_broker_list, \"\", \"A comma-separated list of brokers for Kafka engine.\", 0) \\\n     M(SettingString, kafka_topic_list, \"\", \"A list of Kafka topics.\", 0) \\\n-    M(SettingString, kafka_group_name, \"\", \"A group of Kafka consumers.\", 0) \\\n-    M(SettingString, kafka_client_id, \"\", \"A client id of Kafka consumer.\", 0) \\\n+    M(SettingString, kafka_group_name, \"\", \"Client group id string. All Kafka consumers sharing the same group.id belong to the same group.\", 0) \\\n+    M(SettingString, kafka_client_id, \"\", \"Client identifier.\", 0) \\\n+    M(SettingUInt64, kafka_num_consumers, 1, \"The number of consumers per table for Kafka engine.\", 0) \\\n+    M(SettingBool, kafka_commit_every_batch, false, \"Commit every consumed and handled batch instead of a single commit after writing a whole block\", 0) \\\n+    /* default is stream_poll_timeout_ms */ \\\n+    M(SettingMilliseconds, kafka_poll_timeout_ms, 0, \"Timeout for single poll from Kafka.\", 0) \\\n+    /* default is min(max_block_size, kafka_max_block_size)*/ \\\n+    M(SettingUInt64, kafka_poll_max_batch_size, 0, \"Maximum amount of messages to be polled in a single Kafka poll.\", 0) \\\n+    /* default is = min_insert_block_size / kafka_num_consumers  */ \\\n+    M(SettingUInt64, kafka_max_block_size, 0, \"Number of row collected by poll(s) for flushing data from Kafka.\", 0) \\\n+    /* default is stream_flush_interval_ms */ \\\n+    M(SettingMilliseconds, kafka_flush_interval_ms, 0, \"Timeout for flushing data from Kafka.\", 0) \\\n+    /* those are mapped to format factory settings */ \\\n     M(SettingString, kafka_format, \"\", \"The message format for Kafka engine.\", 0) \\\n     M(SettingChar, kafka_row_delimiter, '\\0', \"The character to be considered as a delimiter in Kafka message.\", 0) \\\n     M(SettingString, kafka_schema, \"\", \"Schema identifier (used by schema-based formats) for Kafka engine\", 0) \\\n-    M(SettingUInt64, kafka_num_consumers, 1, \"The number of consumers per table for Kafka engine.\", 0) \\\n-    M(SettingUInt64, kafka_max_block_size, 0, \"The maximum batch size for poll.\", 0) \\\n-    M(SettingUInt64, kafka_skip_broken_messages, 0, \"Skip at least this number of broken messages from Kafka topic per block\", 0) \\\n-    M(SettingUInt64, kafka_commit_every_batch, 0, \"Commit every consumed and handled batch instead of a single commit after writing a whole block\", 0)\n+    M(SettingUInt64, kafka_skip_broken_messages, 0, \"Skip at least this number of broken messages from Kafka topic per block\", 0)\n+\n+    /** TODO: */\n+    /* https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md */\n+    /* https://github.com/edenhill/librdkafka/blob/v1.4.2/src/rdkafka_conf.c */\n+\n+#define LIST_OF_KAFKA_SETTINGS(M) \\\n+    KAFKA_RELATED_SETTINGS(M) \\\n+    FORMAT_FACTORY_SETTINGS(M)\n \n     DECLARE_SETTINGS_COLLECTION(LIST_OF_KAFKA_SETTINGS)\n \ndiff --git a/src/Storages/Kafka/StorageKafka.cpp b/src/Storages/Kafka/StorageKafka.cpp\nindex fc83fd848849..bb721417c5b3 100644\n--- a/src/Storages/Kafka/StorageKafka.cpp\n+++ b/src/Storages/Kafka/StorageKafka.cpp\n@@ -119,39 +119,74 @@ StorageKafka::StorageKafka(\n     const StorageID & table_id_,\n     Context & context_,\n     const ColumnsDescription & columns_,\n-    const String & brokers_,\n-    const String & group_,\n-    const String & client_id_,\n-    const Names & topics_,\n-    const String & format_name_,\n-    char row_delimiter_,\n-    const String & schema_name_,\n-    size_t num_consumers_,\n-    UInt64 max_block_size_,\n-    size_t skip_broken_,\n-    bool intermediate_commit_)\n+    std::unique_ptr<KafkaSettings> kafka_settings_)\n     : IStorage(table_id_)\n     , global_context(context_.getGlobalContext())\n-    , kafka_context(Context(global_context))\n-    , topics(global_context.getMacros()->expand(topics_))\n-    , brokers(global_context.getMacros()->expand(brokers_))\n-    , group(global_context.getMacros()->expand(group_))\n-    , client_id(client_id_.empty() ? getDefaultClientId(table_id_) : global_context.getMacros()->expand(client_id_))\n-    , format_name(global_context.getMacros()->expand(format_name_))\n-    , row_delimiter(row_delimiter_)\n-    , schema_name(global_context.getMacros()->expand(schema_name_))\n-    , num_consumers(num_consumers_)\n-    , max_block_size(max_block_size_)\n+    , kafka_context(std::make_shared<Context>(global_context))\n+    , kafka_settings(std::move(kafka_settings_))\n+    , topics(parseTopics(global_context.getMacros()->expand(kafka_settings->kafka_topic_list.value)))\n+    , brokers(global_context.getMacros()->expand(kafka_settings->kafka_broker_list.value))\n+    , group(global_context.getMacros()->expand(kafka_settings->kafka_group_name.value))\n+    , client_id(kafka_settings->kafka_client_id.value.empty() ? getDefaultClientId(table_id_) : global_context.getMacros()->expand(kafka_settings->kafka_client_id.value))\n+    , format_name(global_context.getMacros()->expand(kafka_settings->kafka_format.value))\n+    , row_delimiter(kafka_settings->kafka_row_delimiter.value)\n+    , schema_name(global_context.getMacros()->expand(kafka_settings->kafka_schema.value))\n+    , num_consumers(kafka_settings->kafka_num_consumers.value)\n     , log(&Poco::Logger::get(\"StorageKafka (\" + table_id_.table_name + \")\"))\n-    , semaphore(0, num_consumers_)\n-    , skip_broken(skip_broken_)\n-    , intermediate_commit(intermediate_commit_)\n+    , semaphore(0, num_consumers)\n+    , intermediate_commit(kafka_settings->kafka_commit_every_batch.value)\n+    , settings_adjustments(createSettingsAdjustments())\n {\n-    kafka_context.makeQueryContext();\n-\n     setColumns(columns_);\n     task = global_context.getSchedulePool().createTask(log->name(), [this]{ threadFunc(); });\n     task->deactivate();\n+\n+    kafka_context->makeQueryContext();\n+    kafka_context->applySettingsChanges(settings_adjustments);\n+}\n+\n+SettingsChanges StorageKafka::createSettingsAdjustments()\n+{\n+    SettingsChanges result;\n+    // Needed for backward compatibility\n+    if (!kafka_settings->input_format_skip_unknown_fields.changed)\n+    {\n+        // Always skip unknown fields regardless of the context (JSON or TSKV)\n+        kafka_settings->input_format_skip_unknown_fields = true;\n+    }\n+\n+    if (!kafka_settings->input_format_allow_errors_ratio.changed)\n+    {\n+        kafka_settings->input_format_allow_errors_ratio = 0.;\n+    }\n+\n+    if (!kafka_settings->input_format_allow_errors_num.changed)\n+    {\n+        kafka_settings->input_format_allow_errors_num = kafka_settings->kafka_skip_broken_messages.value;\n+    }\n+\n+    if (!schema_name.empty())\n+        result.emplace_back(\"format_schema\", schema_name);\n+\n+    for (auto & it : *kafka_settings)\n+    {\n+        if (it.isChanged() && it.getName().toString().rfind(\"kafka_\",0) == std::string::npos)\n+        {\n+            result.emplace_back(it.getName().toString(), it.getValueAsString());\n+        }\n+    }\n+    return result;\n+}\n+\n+Names StorageKafka::parseTopics(String topic_list)\n+{\n+    Names result;\n+    boost::split(result,topic_list,[](char c){ return c == ','; });\n+    for (String & topic : result)\n+    {\n+        boost::trim(topic);\n+    }\n+    return result;\n }\n \n String StorageKafka::getDefaultClientId(const StorageID & table_id_)\n@@ -176,6 +211,8 @@ Pipes StorageKafka::read(\n     /// Always use all consumers at once, otherwise SELECT may not read messages from all partitions.\n     Pipes pipes;\n     pipes.reserve(num_created_consumers);\n+    auto modified_context = std::make_shared<Context>(context);\n+    modified_context->applySettingsChanges(settings_adjustments);\n \n     // Claim as many consumers as requested, but don't block\n     for (size_t i = 0; i < num_created_consumers; ++i)\n@@ -184,7 +221,7 @@ Pipes StorageKafka::read(\n         /// TODO: probably that leads to awful performance.\n         /// FIXME: seems that doesn't help with extra reading and committing unprocessed messages.\n         /// TODO: rewrite KafkaBlockInputStream to KafkaSource. Now it is used in other place.\n-        pipes.emplace_back(std::make_shared<SourceFromInputStream>(std::make_shared<KafkaBlockInputStream>(*this, context, column_names, 1)));\n+        pipes.emplace_back(std::make_shared<SourceFromInputStream>(std::make_shared<KafkaBlockInputStream>(*this, modified_context, column_names, 1)));\n     }\n \n     LOG_DEBUG(log, \"Starting reading {} streams\", pipes.size());\n@@ -194,9 +231,12 @@ Pipes StorageKafka::read(\n \n BlockOutputStreamPtr StorageKafka::write(const ASTPtr &, const Context & context)\n {\n+    auto modified_context = std::make_shared<Context>(context);\n+    modified_context->applySettingsChanges(settings_adjustments);\n+\n     if (topics.size() > 1)\n         throw Exception(\"Can't write to Kafka table with multiple topics!\", ErrorCodes::NOT_IMPLEMENTED);\n-    return std::make_shared<KafkaBlockOutputStream>(*this, context);\n+    return std::make_shared<KafkaBlockOutputStream>(*this, modified_context);\n }\n \n \n@@ -268,13 +308,14 @@ ConsumerBufferPtr StorageKafka::popReadBuffer(std::chrono::milliseconds timeout)\n     return buffer;\n }\n \n-\n ProducerBufferPtr StorageKafka::createWriteBuffer(const Block & header)\n {\n     cppkafka::Configuration conf;\n     conf.set(\"metadata.broker.list\", brokers);\n     conf.set(\"group.id\", group);\n     conf.set(\"client.id\", client_id);\n+    conf.set(\"client.software.name\", VERSION_NAME);\n+    conf.set(\"client.software.version\", VERSION_DESCRIBE);\n     // TODO: fill required settings\n     updateConfiguration(conf);\n \n@@ -303,9 +344,16 @@ ConsumerBufferPtr StorageKafka::createReadBuffer(const size_t consumer_number)\n     {\n         conf.set(\"client.id\", client_id);\n     }\n-\n+    conf.set(\"client.software.name\", VERSION_NAME);\n+    conf.set(\"client.software.version\", VERSION_DESCRIBE);\n     conf.set(\"auto.offset.reset\", \"smallest\");     // If no offset stored for this group, read all messages from the start\n \n+    // that allows to prevent fast draining of the librdkafka queue\n+    // during building of single insert block. Improves performance\n+    // significantly, but may lead to bigger memory consumption.\n+    size_t default_queued_min_messages = 100000; // we don't want to decrease the default\n+    conf.set(\"queued.min.messages\", std::max(getMaxBlockSize(),default_queued_min_messages));\n+\n     updateConfiguration(conf);\n \n     // those settings should not be changed by users.\n@@ -317,17 +365,32 @@ ConsumerBufferPtr StorageKafka::createReadBuffer(const size_t consumer_number)\n     auto consumer = std::make_shared<cppkafka::Consumer>(conf);\n     consumer->set_destroy_flags(RD_KAFKA_DESTROY_F_NO_CONSUMER_CLOSE);\n \n-    // Limit the number of batched messages to allow early cancellations\n-    const Settings & settings = global_context.getSettingsRef();\n-    size_t batch_size = max_block_size;\n-    if (!batch_size)\n-        batch_size = settings.max_block_size.value;\n-    size_t poll_timeout = settings.stream_poll_timeout_ms.totalMilliseconds();\n-\n     /// NOTE: we pass |stream_cancelled| by reference here, so the buffers should not outlive the storage.\n-    return std::make_shared<ReadBufferFromKafkaConsumer>(consumer, log, batch_size, poll_timeout, intermediate_commit, stream_cancelled, getTopics());\n+    return std::make_shared<ReadBufferFromKafkaConsumer>(consumer, log, getPollMaxBatchSize(), getPollTimeoutMillisecond(), intermediate_commit, stream_cancelled, topics);\n+}\n+\n+size_t StorageKafka::getMaxBlockSize() const\n+{\n+    return kafka_settings->kafka_max_block_size.changed\n+        ? kafka_settings->kafka_max_block_size.value\n+        : (global_context.getSettingsRef().max_insert_block_size.value / num_consumers);\n }\n \n+size_t StorageKafka::getPollMaxBatchSize() const\n+{\n+    size_t batch_size = kafka_settings->kafka_poll_max_batch_size.changed\n+                        ? kafka_settings->kafka_poll_max_batch_size.value\n+                        : global_context.getSettingsRef().max_block_size.value;\n+\n+    return std::min(batch_size,getMaxBlockSize());\n+}\n+\n+size_t StorageKafka::getPollTimeoutMillisecond() const\n+{\n+    return kafka_settings->kafka_poll_timeout_ms.changed\n+        ? kafka_settings->kafka_poll_timeout_ms.totalMilliseconds()\n+        : global_context.getSettingsRef().stream_poll_timeout_ms.totalMilliseconds();\n+}\n \n void StorageKafka::updateConfiguration(cppkafka::Configuration & conf)\n {\n@@ -458,19 +521,17 @@ bool StorageKafka::streamToViews()\n     auto insert = std::make_shared<ASTInsertQuery>();\n     insert->table_id = table_id;\n \n-    const Settings & settings = global_context.getSettingsRef();\n-    size_t block_size = max_block_size;\n-    if (block_size == 0)\n-        block_size = settings.max_block_size;\n+    size_t block_size = getMaxBlockSize();\n \n     // Create a stream for each consumer and join them in a union stream\n     // Only insert into dependent views and expect that input blocks contain virtual columns\n-    InterpreterInsertQuery interpreter(insert, kafka_context, false, true, true);\n+    InterpreterInsertQuery interpreter(insert, *kafka_context, false, true, true);\n     auto block_io = interpreter.execute();\n \n     // Create a stream for each consumer and join them in a union stream\n     BlockInputStreams streams;\n     streams.reserve(num_created_consumers);\n+\n     for (size_t i = 0; i < num_created_consumers; ++i)\n     {\n         auto stream\n@@ -479,7 +540,11 @@ bool StorageKafka::streamToViews()\n \n         // Limit read batch to maximum block size to allow DDL\n         IBlockInputStream::LocalLimits limits;\n-        limits.speed_limits.max_execution_time = settings.stream_flush_interval_ms;\n+\n+        limits.speed_limits.max_execution_time = kafka_settings->kafka_flush_interval_ms.changed\n+                                                 ? kafka_settings->kafka_flush_interval_ms\n+                                                 : global_context.getSettingsRef().stream_flush_interval_ms;\n+\n         limits.timeout_overflow_mode = OverflowMode::BREAK;\n         stream->setLimits(limits);\n     }\n@@ -514,17 +579,61 @@ void registerStorageKafka(StorageFactory & factory)\n         size_t args_count = engine_args.size();\n         bool has_settings = args.storage_def->settings;\n \n-        KafkaSettings kafka_settings;\n+        auto kafka_settings = std::make_unique<KafkaSettings>();\n         if (has_settings)\n         {\n-            kafka_settings.loadFromQuery(*args.storage_def);\n+            kafka_settings->loadFromQuery(*args.storage_def);\n         }\n \n+        // Check arguments and settings\n+        #define CHECK_KAFKA_STORAGE_ARGUMENT(ARG_NUM, PAR_NAME, EVAL)       \\\n+            /* One of the four required arguments is not specified */       \\\n+            if (args_count < (ARG_NUM) && (ARG_NUM) <= 4 &&                 \\\n+                !kafka_settings->PAR_NAME.changed)                          \\\n+            {                                                               \\\n+                throw Exception(                                            \\\n+                    \"Required parameter '\" #PAR_NAME \"' \"                   \\\n+                    \"for storage Kafka not specified\",                      \\\n+                    ErrorCodes::NUMBER_OF_ARGUMENTS_DOESNT_MATCH);          \\\n+            }                                                               \\\n+            if (args_count >= (ARG_NUM))                                    \\\n+            {                                                               \\\n+                /* The same argument is given in two places */              \\\n+                if (has_settings &&                                         \\\n+                    kafka_settings->PAR_NAME.changed)                       \\\n+                {                                                           \\\n+                    throw Exception(                                        \\\n+                        \"The argument \u2116\" #ARG_NUM \" of storage Kafka \"      \\\n+                        \"and the parameter '\" #PAR_NAME \"' \"                \\\n+                        \"in SETTINGS cannot be specified at the same time\", \\\n+                        ErrorCodes::BAD_ARGUMENTS);                         \\\n+                }                                                           \\\n+                /* move engine args to settings */                          \\\n+                else                                                        \\\n+                {                                                           \\\n+                    if ((EVAL) == 1)                                        \\\n+                    {                                                       \\\n+                        engine_args[(ARG_NUM)-1] =                          \\\n+                            evaluateConstantExpressionAsLiteral(            \\\n+                                engine_args[(ARG_NUM)-1],                   \\\n+                                args.local_context);                        \\\n+                    }                                                       \\\n+                    if ((EVAL) == 2)                                        \\\n+                    {                                                       \\\n+                        engine_args[(ARG_NUM)-1] =                          \\\n+                           evaluateConstantExpressionOrIdentifierAsLiteral( \\\n+                                engine_args[(ARG_NUM)-1],                   \\\n+                                args.local_context);                        \\\n+                    }                                                       \\\n+                    kafka_settings->PAR_NAME.set(                           \\\n+                        engine_args[(ARG_NUM)-1]->as<ASTLiteral &>().value);\\\n+                }                                                           \\\n+            }\n+\n         /** Arguments of engine is following:\n           * - Kafka broker list\n           * - List of topics\n           * - Group ID (may be a constaint expression with a string result)\n-          * - Client ID\n           * - Message format (string)\n           * - Row delimiter\n           * - Schema (optional, if the format supports it)\n@@ -534,209 +643,32 @@ void registerStorageKafka(StorageFactory & factory)\n           * - Do intermediate commits when the batch consumed and handled\n           */\n \n-        // Check arguments and settings\n-        #define CHECK_KAFKA_STORAGE_ARGUMENT(ARG_NUM, PAR_NAME)            \\\n-            /* One of the four required arguments is not specified */      \\\n-            if (args_count < (ARG_NUM) && (ARG_NUM) <= 4 &&                    \\\n-                !kafka_settings.PAR_NAME.changed)                          \\\n-            {                                                              \\\n-                throw Exception(                                           \\\n-                    \"Required parameter '\" #PAR_NAME \"' \"                  \\\n-                    \"for storage Kafka not specified\",                     \\\n-                    ErrorCodes::NUMBER_OF_ARGUMENTS_DOESNT_MATCH);         \\\n-            }                                                              \\\n-            /* The same argument is given in two places */                 \\\n-            if (has_settings &&                                            \\\n-                kafka_settings.PAR_NAME.changed &&                         \\\n-                args_count >= (ARG_NUM))                                     \\\n-            {                                                              \\\n-                throw Exception(                                           \\\n-                    \"The argument \u2116\" #ARG_NUM \" of storage Kafka \"         \\\n-                    \"and the parameter '\" #PAR_NAME \"' \"                   \\\n-                    \"in SETTINGS cannot be specified at the same time\",    \\\n-                    ErrorCodes::BAD_ARGUMENTS);                            \\\n-            }\n-\n-        CHECK_KAFKA_STORAGE_ARGUMENT(1, kafka_broker_list)\n-        CHECK_KAFKA_STORAGE_ARGUMENT(2, kafka_topic_list)\n-        CHECK_KAFKA_STORAGE_ARGUMENT(3, kafka_group_name)\n-        CHECK_KAFKA_STORAGE_ARGUMENT(4, kafka_format)\n-        CHECK_KAFKA_STORAGE_ARGUMENT(5, kafka_row_delimiter)\n-        CHECK_KAFKA_STORAGE_ARGUMENT(6, kafka_schema)\n-        CHECK_KAFKA_STORAGE_ARGUMENT(7, kafka_num_consumers)\n-        CHECK_KAFKA_STORAGE_ARGUMENT(8, kafka_max_block_size)\n-        CHECK_KAFKA_STORAGE_ARGUMENT(9, kafka_skip_broken_messages)\n-        CHECK_KAFKA_STORAGE_ARGUMENT(10, kafka_commit_every_batch)\n+        /* 0 = raw, 1 = evaluateConstantExpressionAsLiteral, 2=evaluateConstantExpressionOrIdentifierAsLiteral */\n+        CHECK_KAFKA_STORAGE_ARGUMENT(1, kafka_broker_list, 0)\n+        CHECK_KAFKA_STORAGE_ARGUMENT(2, kafka_topic_list, 1)\n+        CHECK_KAFKA_STORAGE_ARGUMENT(3, kafka_group_name, 2)\n+        CHECK_KAFKA_STORAGE_ARGUMENT(4, kafka_format, 2)\n+        CHECK_KAFKA_STORAGE_ARGUMENT(5, kafka_row_delimiter, 2)\n+        CHECK_KAFKA_STORAGE_ARGUMENT(6, kafka_schema, 2)\n+        CHECK_KAFKA_STORAGE_ARGUMENT(7, kafka_num_consumers, 0)\n+        CHECK_KAFKA_STORAGE_ARGUMENT(8, kafka_max_block_size, 0)\n+        CHECK_KAFKA_STORAGE_ARGUMENT(9, kafka_skip_broken_messages, 0)\n+        CHECK_KAFKA_STORAGE_ARGUMENT(10, kafka_commit_every_batch, 0)\n \n         #undef CHECK_KAFKA_STORAGE_ARGUMENT\n \n-        // Get and check broker list\n-        String brokers = kafka_settings.kafka_broker_list;\n-        if (args_count >= 1)\n-        {\n-            const auto * ast = engine_args[0]->as<ASTLiteral>();\n-            if (ast && ast->value.getType() == Field::Types::String)\n-            {\n-                brokers = safeGet<String>(ast->value);\n-            }\n-            else\n-            {\n-                throw Exception(String(\"Kafka broker list must be a string\"), ErrorCodes::BAD_ARGUMENTS);\n-            }\n-        }\n+        auto num_consumers = kafka_settings->kafka_num_consumers.value;\n \n-        // Get and check topic list\n-        String topic_list = kafka_settings.kafka_topic_list.value;\n-        if (args_count >= 2)\n+        if (num_consumers > 16)\n         {\n-            engine_args[1] = evaluateConstantExpressionAsLiteral(engine_args[1], args.local_context);\n-            topic_list = engine_args[1]->as<ASTLiteral &>().value.safeGet<String>();\n+            throw Exception(\"Number of consumers can not be bigger than 16\", ErrorCodes::BAD_ARGUMENTS);\n         }\n-\n-        Names topics;\n-        boost::split(topics, topic_list , [](char c){ return c == ','; });\n-        for (String & topic : topics)\n-        {\n-            boost::trim(topic);\n-        }\n-\n-        // Get and check group name\n-        String group = kafka_settings.kafka_group_name.value;\n-        if (args_count >= 3)\n+        else if (num_consumers < 1)\n         {\n-            engine_args[2] = evaluateConstantExpressionOrIdentifierAsLiteral(engine_args[2], args.local_context);\n-            group = engine_args[2]->as<ASTLiteral &>().value.safeGet<String>();\n+            throw Exception(\"Number of consumers can not be lower than 1\", ErrorCodes::BAD_ARGUMENTS);\n         }\n \n-        // Get and check message format name\n-        String format = kafka_settings.kafka_format.value;\n-        if (args_count >= 4)\n-        {\n-            engine_args[3] = evaluateConstantExpressionOrIdentifierAsLiteral(engine_args[3], args.local_context);\n-\n-            const auto * ast = engine_args[3]->as<ASTLiteral>();\n-            if (ast && ast->value.getType() == Field::Types::String)\n-            {\n-                format = safeGet<String>(ast->value);\n-            }\n-            else\n-            {\n-                throw Exception(\"Format must be a string\", ErrorCodes::BAD_ARGUMENTS);\n-            }\n-        }\n-\n-        // Parse row delimiter (optional)\n-        char row_delimiter = kafka_settings.kafka_row_delimiter;\n-        if (args_count >= 5)\n-        {\n-            engine_args[4] = evaluateConstantExpressionOrIdentifierAsLiteral(engine_args[4], args.local_context);\n-\n-            const auto * ast = engine_args[4]->as<ASTLiteral>();\n-            String arg;\n-            if (ast && ast->value.getType() == Field::Types::String)\n-            {\n-                arg = safeGet<String>(ast->value);\n-            }\n-            else\n-            {\n-                throw Exception(\"Row delimiter must be a char\", ErrorCodes::BAD_ARGUMENTS);\n-            }\n-            if (arg.size() > 1)\n-            {\n-                throw Exception(\"Row delimiter must be a char\", ErrorCodes::BAD_ARGUMENTS);\n-            }\n-            else if (arg.empty())\n-            {\n-                row_delimiter = '\\0';\n-            }\n-            else\n-            {\n-                row_delimiter = arg[0];\n-            }\n-        }\n-\n-        // Parse format schema if supported (optional)\n-        String schema = kafka_settings.kafka_schema.value;\n-        if (args_count >= 6)\n-        {\n-            engine_args[5] = evaluateConstantExpressionOrIdentifierAsLiteral(engine_args[5], args.local_context);\n-\n-            const auto * ast = engine_args[5]->as<ASTLiteral>();\n-            if (ast && ast->value.getType() == Field::Types::String)\n-            {\n-                schema = safeGet<String>(ast->value);\n-            }\n-            else\n-            {\n-                throw Exception(\"Format schema must be a string\", ErrorCodes::BAD_ARGUMENTS);\n-            }\n-        }\n-\n-        // Parse number of consumers (optional)\n-        UInt64 num_consumers = kafka_settings.kafka_num_consumers;\n-        if (args_count >= 7)\n-        {\n-            const auto * ast = engine_args[6]->as<ASTLiteral>();\n-            if (ast && ast->value.getType() == Field::Types::UInt64)\n-            {\n-                num_consumers = safeGet<UInt64>(ast->value);\n-            }\n-            else\n-            {\n-                throw Exception(\"Number of consumers must be a positive integer\", ErrorCodes::BAD_ARGUMENTS);\n-            }\n-        }\n-\n-        // Parse max block size (optional)\n-        UInt64 max_block_size = static_cast<size_t>(kafka_settings.kafka_max_block_size);\n-        if (args_count >= 8)\n-        {\n-            const auto * ast = engine_args[7]->as<ASTLiteral>();\n-            if (ast && ast->value.getType() == Field::Types::UInt64)\n-            {\n-                max_block_size = static_cast<size_t>(safeGet<UInt64>(ast->value));\n-            }\n-            else\n-            {\n-                // TODO: no check if the integer is really positive\n-                throw Exception(\"Maximum block size must be a positive integer\", ErrorCodes::BAD_ARGUMENTS);\n-            }\n-        }\n-\n-        size_t skip_broken = static_cast<size_t>(kafka_settings.kafka_skip_broken_messages);\n-        if (args_count >= 9)\n-        {\n-            const auto * ast = engine_args[8]->as<ASTLiteral>();\n-            if (ast && ast->value.getType() == Field::Types::UInt64)\n-            {\n-                skip_broken = static_cast<size_t>(safeGet<UInt64>(ast->value));\n-            }\n-            else\n-            {\n-                throw Exception(\"Number of broken messages to skip must be a non-negative integer\", ErrorCodes::BAD_ARGUMENTS);\n-            }\n-        }\n-\n-        bool intermediate_commit = static_cast<bool>(kafka_settings.kafka_commit_every_batch);\n-        if (args_count >= 10)\n-        {\n-            const auto * ast = engine_args[9]->as<ASTLiteral>();\n-            if (ast && ast->value.getType() == Field::Types::UInt64)\n-            {\n-                intermediate_commit = static_cast<bool>(safeGet<UInt64>(ast->value));\n-            }\n-            else\n-            {\n-                throw Exception(\"Flag for committing every batch must be 0 or 1\", ErrorCodes::BAD_ARGUMENTS);\n-            }\n-        }\n-\n-        // Get and check client id\n-        String client_id = kafka_settings.kafka_client_id.value;\n-\n-        return StorageKafka::create(\n-            args.table_id, args.context, args.columns,\n-            brokers, group, client_id, topics, format, row_delimiter, schema, num_consumers, max_block_size, skip_broken, intermediate_commit);\n+        return StorageKafka::create(args.table_id, args.context, args.columns, std::move(kafka_settings));\n     };\n \n     factory.registerStorage(\"Kafka\", creator_fn, StorageFactory::StorageFeatures{ .supports_settings = true, });\ndiff --git a/src/Storages/Kafka/StorageKafka.h b/src/Storages/Kafka/StorageKafka.h\nindex 1ea7d6dcad70..be3f89687fef 100644\n--- a/src/Storages/Kafka/StorageKafka.h\n+++ b/src/Storages/Kafka/StorageKafka.h\n@@ -3,6 +3,7 @@\n #include <Core/BackgroundSchedulePool.h>\n #include <Storages/IStorage.h>\n #include <Storages/Kafka/Buffer_fwd.h>\n+#include <Storages/Kafka/KafkaSettings.h>\n #include <Interpreters/Context.h>\n \n #include <Poco/Semaphore.h>\n@@ -54,10 +55,7 @@ class StorageKafka final : public ext::shared_ptr_helper<StorageKafka>, public I\n \n     ProducerBufferPtr createWriteBuffer(const Block & header);\n \n-    const auto & getTopics() const { return topics; }\n     const auto & getFormatName() const { return format_name; }\n-    const auto & getSchemaName() const { return schema_name; }\n-    const auto & skipBroken() const { return skip_broken; }\n \n     NamesAndTypesList getVirtuals() const override;\n protected:\n@@ -65,58 +63,53 @@ class StorageKafka final : public ext::shared_ptr_helper<StorageKafka>, public I\n         const StorageID & table_id_,\n         Context & context_,\n         const ColumnsDescription & columns_,\n-        const String & brokers_,\n-        const String & group_,\n-        const String & client_id_,\n-        const Names & topics_,\n-        const String & format_name_,\n-        char row_delimiter_,\n-        const String & schema_name_,\n-        size_t num_consumers_,\n-        UInt64 max_block_size_,\n-        size_t skip_broken,\n-        bool intermediate_commit_);\n+        std::unique_ptr<KafkaSettings> kafka_settings_);\n \n private:\n     // Configuration and state\n-    Context global_context;\n-    Context kafka_context;\n-    Names topics;\n+    Context & global_context;\n+    std::shared_ptr<Context> kafka_context;\n+    std::unique_ptr<KafkaSettings> kafka_settings;\n+    const Names topics;\n     const String brokers;\n     const String group;\n     const String client_id;\n     const String format_name;\n-    char row_delimiter; /// optional row delimiter for generating char delimited stream in order to make various input stream parsers happy.\n+    const char row_delimiter; /// optional row delimiter for generating char delimited stream in order to make various input stream parsers happy.\n     const String schema_name;\n-    size_t num_consumers; /// total number of consumers\n-    UInt64 max_block_size; /// maximum block size for insertion into this table\n+    const size_t num_consumers; /// total number of consumers\n+    Poco::Logger * log;\n+    Poco::Semaphore semaphore;\n+    const bool intermediate_commit;\n+    const SettingsChanges settings_adjustments;\n \n     /// Can differ from num_consumers in case of exception in startup() (or if startup() hasn't been called).\n     /// In this case we still need to be able to shutdown() properly.\n     size_t num_created_consumers = 0; /// number of actually created consumers.\n \n-    Poco::Logger * log;\n-\n-    // Consumer list\n-    Poco::Semaphore semaphore;\n-    std::mutex mutex;\n     std::vector<ConsumerBufferPtr> buffers; /// available buffers for Kafka consumers\n \n-    size_t skip_broken;\n-\n-    bool intermediate_commit;\n+    std::mutex mutex;\n \n     // Stream thread\n     BackgroundSchedulePool::TaskHolder task;\n     std::atomic<bool> stream_cancelled{false};\n \n+    SettingsChanges createSettingsAdjustments();\n     ConsumerBufferPtr createReadBuffer(const size_t consumer_number);\n \n     // Update Kafka configuration with values from CH user configuration.\n-    void updateConfiguration(cppkafka::Configuration & conf);\n \n+    void updateConfiguration(cppkafka::Configuration & conf);\n     void threadFunc();\n+\n+    size_t getPollMaxBatchSize() const;\n+    size_t getMaxBlockSize() const;\n+    size_t getPollTimeoutMillisecond() const;\n+\n+    static Names parseTopics(String topic_list);\n     static String getDefaultClientId(const StorageID & table_id_);\n+\n     bool streamToViews();\n     bool checkDependencies(const StorageID & table_id);\n };\n",
  "test_patch": "diff --git a/tests/integration/test_storage_kafka/test.py b/tests/integration/test_storage_kafka/test.py\nindex d2a2532bb9a0..2a1b42f8e0e3 100644\n--- a/tests/integration/test_storage_kafka/test.py\n+++ b/tests/integration/test_storage_kafka/test.py\n@@ -233,6 +233,83 @@ def test_kafka_settings_new_syntax(kafka_cluster):\n     members = describe_consumer_group('new')\n     assert members[0]['client_id'] == u'instance test 1234'\n \n+\n+@pytest.mark.timeout(180)\n+def test_kafka_issue11308(kafka_cluster):\n+    # Check that matview does respect Kafka SETTINGS\n+    kafka_produce('issue11308', ['{\"t\": 123, \"e\": {\"x\": \"woof\"} }', '{\"t\": 123, \"e\": {\"x\": \"woof\"} }', '{\"t\": 124, \"e\": {\"x\": \"test\"} }'])\n+\n+    instance.query('''\n+        CREATE TABLE test.persistent_kafka (\n+            time UInt64,\n+            some_string String\n+        )\n+        ENGINE = MergeTree()\n+        ORDER BY time;\n+\n+        CREATE TABLE test.kafka (t UInt64, `e.x` String)\n+            ENGINE = Kafka\n+            SETTINGS kafka_broker_list = 'kafka1:19092',\n+                     kafka_topic_list = 'issue11308',\n+                     kafka_group_name = 'issue11308',\n+                     kafka_format = 'JSONEachRow',\n+                     kafka_row_delimiter = '\\\\n',\n+                     kafka_flush_interval_ms=1000,\n+                     input_format_import_nested_json = 1;\n+\n+        CREATE MATERIALIZED VIEW test.persistent_kafka_mv TO test.persistent_kafka AS\n+        SELECT\n+            `t` AS `time`,\n+            `e.x` AS `some_string`\n+        FROM test.kafka;\n+        ''')\n+\n+    time.sleep(9)\n+\n+    result = instance.query('SELECT * FROM test.persistent_kafka ORDER BY time;')\n+\n+    instance.query('''\n+        DROP TABLE test.persistent_kafka;\n+        DROP TABLE test.persistent_kafka_mv;\n+    ''')\n+\n+    expected = '''\\\n+123\twoof\n+123\twoof\n+124\ttest\n+'''\n+    assert TSV(result) == TSV(expected)\n+\n+\n+@pytest.mark.timeout(180)\n+def test_kafka_issue4116(kafka_cluster):\n+    # Check that format_csv_delimiter parameter works now - as part of all available format settings.\n+    kafka_produce('issue4116', ['1|foo', '2|bar', '42|answer','100|multi\\n101|row\\n103|message'])\n+\n+    instance.query('''\n+        CREATE TABLE test.kafka (a UInt64, b String)\n+            ENGINE = Kafka\n+            SETTINGS kafka_broker_list = 'kafka1:19092',\n+                     kafka_topic_list = 'issue4116',\n+                     kafka_group_name = 'issue4116',\n+                     kafka_format = 'CSV',\n+                     kafka_row_delimiter = '\\\\n',\n+                     format_csv_delimiter = '|';\n+        ''')\n+\n+    result = instance.query('SELECT * FROM test.kafka ORDER BY a;')\n+\n+    expected = '''\\\n+1\tfoo\n+2\tbar\n+42\tanswer\n+100\tmulti\n+101\trow\n+103\tmessage\n+'''\n+    assert TSV(result) == TSV(expected)\n+\n+\n @pytest.mark.timeout(180)\n def test_kafka_consumer_hang(kafka_cluster):\n \n",
  "problem_statement": "'Template' format currently can't be used with Kafka\n'Template' currently can't be used with Kafka, as you can't pass required parameters \r\n\r\n(DB::Exception: Unknown setting format_template_row for storage Kafka).\nMaterialized view pulling from Kafka topic ignores `SETTINGS`\nWhen using materialized views to pump events from a Kafka topic into another CH table, the `SETTINGS` clause of the MV's SELECT appears to be ignored. At least for the `input_format_import_nested_json` setting -- I haven't tested any other settings.\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use: 20.3.10.75\r\n* `CREATE TABLE` statements for all tables involved\r\n\r\n```sql\r\nCREATE TABLE kafka (\r\n    `t` UInt64,\r\n    `e.x` String\r\n)\r\nENGINE = Kafka()\r\nSETTINGS\r\n    kafka_broker_list = 'broker:29092',\r\n    kafka_topic_list = 'topic-name',\r\n    kafka_group_name = 'group-name',\r\n    kafka_format = 'JSONEachRow';\r\n\r\nCREATE TABLE persistent_kafka (\r\n    time UInt64,\r\n    some_string String\r\n)\r\nENGINE = MergeTree()\r\nORDER BY time;\r\n\r\nCREATE MATERIALIZED VIEW persistent_kafka_mv TO persistent_kafka AS\r\nSELECT * FROM (\r\n    SELECT\r\n        `t` AS `time`,\r\n        `e.x` AS `some_string`\r\n    FROM kafka\r\n    SETTINGS input_format_import_nested_json = 1\r\n)\r\n```\r\n\r\nInsert a few test messages to Kafka:\r\n```\r\nkafkacat -P -b 127.0.0.1:29092 -t topic-name\r\n{\"t\": 123, \"e\": {\"x\": \"woof\"}}\r\n{\"t\": 123, \"e\": {\"x\": \"woof\"}}\r\n{\"t\": 123, \"e\": {\"x\": \"woof\"}}\r\n{\"t\": 123, \"e\": {\"x\": \"woof\"}}\r\n{\"t\": 123, \"e\": {\"x\": \"woof\"}}\r\n```\r\n\r\nWhen querying the `kafka` table directly, without the MV, it works as expected:\r\n\r\n```\r\nSELECT\r\n    t AS time,\r\n    `e.x` AS some_string\r\nFROM kafka\r\nSETTINGS input_format_import_nested_json = 1\r\n\r\n\u250c\u2500time\u2500\u252c\u2500some_string\u2500\u2510\r\n\u2502  123 \u2502 woof        \u2502\r\n\u2502  123 \u2502 woof        \u2502\r\n\u2502  123 \u2502 woof        \u2502\r\n\u2502  123 \u2502 woof        \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nIn the `persistent_kafka` filled from the MV however, the `SETTINGS` clause is ignored.\r\n```\r\n:) select * from persistent_kafka;\r\n\r\nSELECT *\r\nFROM persistent_kafka\r\n\r\n\u250c\u2500time\u2500\u252c\u2500some_string\u2500\u2510\r\n\u2502  123 \u2502             \u2502\r\n\u2502  123 \u2502             \u2502\r\n\u2502  123 \u2502             \u2502\r\n\u2502  123 \u2502             \u2502\r\n\u2502  123 \u2502             \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n5 rows in set. Elapsed: 0.009 sec.\r\n```\r\n\r\nWhen setting `input_format_import_nested_json=1` globally in `users.xml`, the MV works fine. I also tried the MV without the subquery -- same result.\r\n\r\nAs a more general critique, I feel like it would be smarter if one could set `input_format_import_nested_json = 1` on the kafka table directly rather than having to specify it for each query, but I guess that should probably be discussed elsewhere.\nKafka engine CSV format_csv_delimiter parameter\nIs there way to set format_csv_delimiter parameter for CSV type in kafka engine?\r\n\r\nIn documentary of CK described default value fo CSV input as \",\". In our files the delimiter is \"|\", so we need to change the default value. But i didn't find any documented possibility for set format_csv_delimiter in kafka engine table for csv input type.\n",
  "hints_text": "We should enable all format-related settings for Kafka.\nThat should be supported by Kafka table itself, not by Materialized View. \r\n\r\nWorking on it right now.\nyou can try to set that in default profile (may be server restart / or Kafka table recreate can be needed).\r\n\r\nBTW: May be user_profile should be added to Kafka settings? \nThe same as #8056 ",
  "created_at": "2020-06-02T17:27:05Z"
}